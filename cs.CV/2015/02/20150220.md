# Arxiv Papers in cs.CV on 2015-02-20
### Visual object tracking performance measures revisited
- **Arxiv ID**: http://arxiv.org/abs/1502.05803v3
- **DOI**: 10.1109/TIP.2016.2520370
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.05803v3)
- **Published**: 2015-02-20 08:57:31+00:00
- **Updated**: 2016-03-07 14:29:56+00:00
- **Authors**: Luka Čehovin, Aleš Leonardis, Matej Kristan
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing (March 2016), 1261 - 1274
- **Summary**: The problem of visual tracking evaluation is sporting a large variety of performance measures, and largely suffers from lack of consensus about which measures should be used in experiments. This makes the cross-paper tracker comparison difficult. Furthermore, as some measures may be less effective than others, the tracking results may be skewed or biased towards particular tracking aspects. In this paper we revisit the popular performance measures and tracker performance visualizations and analyze them theoretically and experimentally. We show that several measures are equivalent from the point of information they provide for tracker comparison and, crucially, that some are more brittle than the others. Based on our analysis we narrow down the set of potential measures to only two complementary ones, describing accuracy and robustness, thus pushing towards homogenization of the tracker evaluation methodology. These two measures can be intuitively interpreted and visualized and have been employed by the recent Visual Object Tracking (VOT) challenges as the foundation for the evaluation methodology.



### A General Multi-Graph Matching Approach via Graduated Consistency-regularized Boosting
- **Arxiv ID**: http://arxiv.org/abs/1502.05840v1
- **DOI**: 10.1109/TPAMI.2015.2477832
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.05840v1)
- **Published**: 2015-02-20 11:45:25+00:00
- **Updated**: 2015-02-20 11:45:25+00:00
- **Authors**: Junchi Yan, Minsu Cho, Hongyuan Zha, Xiaokang Yang, Stephen Chu
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  38(6) 2016, page 1228 - 1242
- **Summary**: This paper addresses the problem of matching $N$ weighted graphs referring to an identical object or category. More specifically, matching the common node correspondences among graphs. This multi-graph matching problem involves two ingredients affecting the overall accuracy: i) the local pairwise matching affinity score among graphs; ii) the global matching consistency that measures the uniqueness of the pairwise matching results by different chaining orders. Previous studies typically either enforce the matching consistency constraints in the beginning of iterative optimization, which may propagate matching error both over iterations and across graph pairs; or separate affinity optimizing and consistency regularization in two steps. This paper is motivated by the observation that matching consistency can serve as a regularizer in the affinity objective function when the function is biased due to noises or inappropriate modeling. We propose multi-graph matching methods to incorporate the two aspects by boosting the affinity score, meanwhile gradually infusing the consistency as a regularizer. Furthermore, we propose a node-wise consistency/affinity-driven mechanism to elicit the common inlier nodes out of the irrelevant outliers. Extensive results on both synthetic and public image datasets demonstrate the competency of the proposed algorithms.



### Learning Descriptors for Object Recognition and 3D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1502.05908v2
- **DOI**: 10.1109/CVPR.2015.7298930
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.05908v2)
- **Published**: 2015-02-20 15:39:42+00:00
- **Updated**: 2015-04-13 13:53:07+00:00
- **Authors**: Paul Wohlhart, Vincent Lepetit
- **Comment**: CVPR 2015
- **Journal**: None
- **Summary**: Detecting poorly textured objects and estimating their 3D pose reliably is still a very challenging problem. We introduce a simple but powerful approach to computing descriptors for object views that efficiently capture both the object identity and 3D pose. By contrast with previous manifold-based approaches, we can rely on the Euclidean distance to evaluate the similarity between descriptors, and therefore use scalable Nearest Neighbor search methods to efficiently handle a large number of objects under a large range of poses. To achieve this, we train a Convolutional Neural Network to compute these descriptors by enforcing simple similarity and dissimilarity constraints between the descriptors. We show that our constraints nicely untangle the images from different objects and different views into clusters that are not only well-separated but also structured as the corresponding sets of poses: The Euclidean distance between descriptors is large when the descriptors are from different objects, and directly related to the distance between the poses when the descriptors are from the same object. These important properties allow us to outperform state-of-the-art object views representations on challenging RGB and RGB-D data.



### Supervised Dictionary Learning and Sparse Representation-A Review
- **Arxiv ID**: http://arxiv.org/abs/1502.05928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.05928v1)
- **Published**: 2015-02-20 16:51:19+00:00
- **Updated**: 2015-02-20 16:51:19+00:00
- **Authors**: Mehrdad J. Gangeh, Ahmed K. Farahat, Ali Ghodsi, Mohamed S. Kamel
- **Comment**: None
- **Journal**: None
- **Summary**: Dictionary learning and sparse representation (DLSR) is a recent and successful mathematical model for data representation that achieves state-of-the-art performance in various fields such as pattern recognition, machine learning, computer vision, and medical imaging. The original formulation for DLSR is based on the minimization of the reconstruction error between the original signal and its sparse representation in the space of the learned dictionary. Although this formulation is optimal for solving problems such as denoising, inpainting, and coding, it may not lead to optimal solution in classification tasks, where the ultimate goal is to make the learned dictionary and corresponding sparse representation as discriminative as possible. This motivated the emergence of a new category of techniques, which is appropriately called supervised dictionary learning and sparse representation (S-DLSR), leading to more optimal dictionary and sparse representation in classification tasks. Despite many research efforts for S-DLSR, the literature lacks a comprehensive view of these techniques, their connections, advantages and shortcomings. In this paper, we address this gap and provide a review of the recently proposed algorithms for S-DLSR. We first present a taxonomy of these algorithms into six categories based on the approach taken to include label information into the learning of the dictionary and/or sparse representation. For each category, we draw connections between the algorithms in this category and present a unified framework for them. We then provide guidelines for applied researchers on how to represent and learn the building blocks of an S-DLSR solution based on the problem at hand. This review provides a broad, yet deep, view of the state-of-the-art methods for S-DLSR and allows for the advancement of research and development in this emerging area of research.



### Web Similarity in Sets of Search Terms using Database Queries
- **Arxiv ID**: http://arxiv.org/abs/1502.05957v2
- **DOI**: 10.1007/s42979-020-00148-5
- **Categories**: **cs.IR**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1502.05957v2)
- **Published**: 2015-02-20 17:55:58+00:00
- **Updated**: 2020-07-23 16:27:48+00:00
- **Authors**: Andrew R. Cohen, Paul M. B. Vitanyi
- **Comment**: LaTeX 18 pages, 3 tables. A precursor is arXiv:1308.3177
- **Journal**: SN COMPUT. SCI. 1, 161(2020)
- **Summary**: Normalized web distance (NWD) is a similarity or normalized semantic distance based on the World Wide Web or another large electronic database, for instance Wikipedia, and a search engine that returns reliable aggregate page counts. For sets of search terms the NWD gives a common similarity (common semantics) on a scale from 0 (identical) to 1 (completely different). The NWD approximates the similarity of members of a set according to all (upper semi)computable properties. We develop the theory and give applications of classifying using Amazon, Wikipedia, and the NCBI website from the National Institutes of Health. The last gives new correlations between health hazards. A restriction of the NWD to a set of two yields the earlier normalized google distance (NGD) but no combination of the NGD's of pairs in a set can extract the information the NWD extracts from the set. The NWD enables a new contextual (different databases) learning approachbased on Kolmogorov complexity theory that incorporates knowledge from these databases.



