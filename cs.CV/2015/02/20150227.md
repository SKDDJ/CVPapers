# Arxiv Papers in cs.CV on 2015-02-27
### Modelling Local Deep Convolutional Neural Network Features to Improve Fine-Grained Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1502.07802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.07802v1)
- **Published**: 2015-02-27 02:04:57+00:00
- **Updated**: 2015-02-27 02:04:57+00:00
- **Authors**: ZongYuan Ge, Chris McCool, Conrad Sanderson, Peter Corke
- **Comment**: 5 pages, three figures
- **Journal**: None
- **Summary**: We propose a local modelling approach using deep convolutional neural networks (CNNs) for fine-grained image classification. Recently, deep CNNs trained from large datasets have considerably improved the performance of object recognition. However, to date there has been limited work using these deep CNNs as local feature extractors. This partly stems from CNNs having internal representations which are high dimensional, thereby making such representations difficult to model using stochastic models. To overcome this issue, we propose to reduce the dimensionality of one of the internal fully connected layers, in conjunction with layer-restricted retraining to avoid retraining the entire network. The distribution of low-dimensional features obtained from the modified layer is then modelled using a Gaussian mixture model. Comparative experiments show that considerable performance improvements can be achieved on the challenging Fish and UEC FOOD-100 datasets.



### Puzzle Imaging: Using Large-scale Dimensionality Reduction Algorithms for Localization
- **Arxiv ID**: http://arxiv.org/abs/1502.07816v3
- **DOI**: 10.1371/journal.pone.0131593
- **Categories**: **q-bio.NC**, cs.CE, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1502.07816v3)
- **Published**: 2015-02-27 04:55:54+00:00
- **Updated**: 2015-06-21 19:17:03+00:00
- **Authors**: Joshua I. Glaser, Bradley M. Zamft, George M. Church, Konrad P. Kording
- **Comment**: None
- **Journal**: None
- **Summary**: Current high-resolution imaging techniques require an intact sample that preserves spatial relationships. We here present a novel approach, "puzzle imaging," that allows imaging a spatially scrambled sample. This technique takes many spatially disordered samples, and then pieces them back together using local properties embedded within the sample. We show that puzzle imaging can efficiently produce high-resolution images using dimensionality reduction algorithms. We demonstrate the theoretical capabilities of puzzle imaging in three biological scenarios, showing that (1) relatively precise 3-dimensional brain imaging is possible; (2) the physical structure of a neural network can often be recovered based only on the neural connectivity matrix; and (3) a chemical map could be reproduced using bacteria with chemosensitive DNA and conjugative transfer. The ability to reconstruct scrambled images promises to enable imaging based on DNA sequencing of homogenized tissue samples.



### Hybrid coding of visual content and local image features
- **Arxiv ID**: http://arxiv.org/abs/1502.07828v1
- **DOI**: 10.1109/ICIP.2015.7351258
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1502.07828v1)
- **Published**: 2015-02-27 08:44:21+00:00
- **Updated**: 2015-02-27 08:44:21+00:00
- **Authors**: Luca Baroffio, Matteo Cesana, Alessandro Redondi, Marco Tagliasacchi, Stefano Tubaro
- **Comment**: submitted to IEEE International Conference on Image Processing
- **Journal**: None
- **Summary**: Distributed visual analysis applications, such as mobile visual search or Visual Sensor Networks (VSNs) require the transmission of visual content on a bandwidth-limited network, from a peripheral node to a processing unit. Traditionally, a Compress-Then-Analyze approach has been pursued, in which sensing nodes acquire and encode the pixel-level representation of the visual content, that is subsequently transmitted to a sink node in order to be processed. This approach might not represent the most effective solution, since several analysis applications leverage a compact representation of the content, thus resulting in an inefficient usage of network resources. Furthermore, coding artifacts might significantly impact the accuracy of the visual task at hand. To tackle such limitations, an orthogonal approach named Analyze-Then-Compress has been proposed. According to such a paradigm, sensing nodes are responsible for the extraction of visual features, that are encoded and transmitted to a sink node for further processing. In spite of improved task efficiency, such paradigm implies the central processing node not being able to reconstruct a pixel-level representation of the visual content. In this paper we propose an effective compromise between the two paradigms, namely Hybrid-Analyze-Then-Compress (HATC) that aims at jointly encoding visual content and local image features. Furthermore, we show how a target tradeoff between image quality and task accuracy might be achieved by accurately allocating the bitrate to either visual content or local features.



### Error-Correcting Factorization
- **Arxiv ID**: http://arxiv.org/abs/1502.07976v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1502.07976v2)
- **Published**: 2015-02-27 17:22:53+00:00
- **Updated**: 2015-03-05 17:49:16+00:00
- **Authors**: Miguel Angel Bautista, Oriol Pujol, Fernando de la Torre, Sergio Escalera
- **Comment**: Under review at TPAMI
- **Journal**: None
- **Summary**: Error Correcting Output Codes (ECOC) is a successful technique in multi-class classification, which is a core problem in Pattern Recognition and Machine Learning. A major advantage of ECOC over other methods is that the multi- class problem is decoupled into a set of binary problems that are solved independently. However, literature defines a general error-correcting capability for ECOCs without analyzing how it distributes among classes, hindering a deeper analysis of pair-wise error-correction. To address these limitations this paper proposes an Error-Correcting Factorization (ECF) method, our contribution is three fold: (I) We propose a novel representation of the error-correction capability, called the design matrix, that enables us to build an ECOC on the basis of allocating correction to pairs of classes. (II) We derive the optimal code length of an ECOC using rank properties of the design matrix. (III) ECF is formulated as a discrete optimization problem, and a relaxed solution is found using an efficient constrained block coordinate descent approach. (IV) Enabled by the flexibility introduced with the design matrix we propose to allocate the error-correction on classes that are prone to confusion. Experimental results in several databases show that when allocating the error-correction to confusable classes ECF outperforms state-of-the-art approaches.



### Describing Videos by Exploiting Temporal Structure
- **Arxiv ID**: http://arxiv.org/abs/1502.08029v5
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1502.08029v5)
- **Published**: 2015-02-27 19:30:40+00:00
- **Updated**: 2015-10-01 00:12:46+00:00
- **Authors**: Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville
- **Comment**: Accepted to ICCV15. This version comes with code release and
  supplementary material
- **Journal**: None
- **Summary**: Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.



### Probabilistic Zero-shot Classification with Semantic Rankings
- **Arxiv ID**: http://arxiv.org/abs/1502.08039v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1502.08039v1)
- **Published**: 2015-02-27 20:00:53+00:00
- **Updated**: 2015-02-27 20:00:53+00:00
- **Authors**: Jihun Hamm, Mikhail Belkin
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a non-metric ranking-based representation of semantic similarity that allows natural aggregation of semantic information from multiple heterogeneous sources. We apply the ranking-based representation to zero-shot learning problems, and present deterministic and probabilistic zero-shot classifiers which can be built from pre-trained classifiers without retraining. We demonstrate their the advantages on two large real-world image datasets. In particular, we show that aggregating different sources of semantic information, including crowd-sourcing, leads to more accurate classification.



### DistancePPG: Robust non-contact vital signs monitoring using a camera
- **Arxiv ID**: http://arxiv.org/abs/1502.08040v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.08040v2)
- **Published**: 2015-02-27 20:03:06+00:00
- **Updated**: 2015-03-24 02:31:18+00:00
- **Authors**: Mayank Kumar, Ashok Veeraraghavan, Ashutosh Sabharval
- **Comment**: 24 pages, 11 figures
- **Journal**: None
- **Summary**: Vital signs such as pulse rate and breathing rate are currently measured using contact probes. But, non-contact methods for measuring vital signs are desirable both in hospital settings (e.g. in NICU) and for ubiquitous in-situ health tracking (e.g. on mobile phone and computers with webcams). Recently, camera-based non-contact vital sign monitoring have been shown to be feasible. However, camera-based vital sign monitoring is challenging for people with darker skin tone, under low lighting conditions, and/or during movement of an individual in front of the camera. In this paper, we propose distancePPG, a new camera-based vital sign estimation algorithm which addresses these challenges. DistancePPG proposes a new method of combining skin-color change signals from different tracked regions of the face using a weighted average, where the weights depend on the blood perfusion and incident light intensity in the region, to improve the signal-to-noise ratio (SNR) of camera-based estimate. One of our key contributions is a new automatic method for determining the weights based only on the video recording of the subject. The gains in SNR of camera-based PPG estimated using distancePPG translate into reduction of the error in vital sign estimation, and thus expand the scope of camera-based vital sign monitoring to potentially challenging scenarios. Further, a dataset will be released, comprising of synchronized video recordings of face and pulse oximeter based ground truth recordings from the earlobe for people with different skin tones, under different lighting conditions and for various motion scenarios.



### Image Segmentation in Liquid Argon Time Projection Chamber Detector
- **Arxiv ID**: http://arxiv.org/abs/1502.08046v1
- **DOI**: None
- **Categories**: **cs.CV**, hep-ex
- **Links**: [PDF](http://arxiv.org/pdf/1502.08046v1)
- **Published**: 2015-02-27 20:32:35+00:00
- **Updated**: 2015-02-27 20:32:35+00:00
- **Authors**: Piotr Płoński, Dorota Stefan, Robert Sulej, Krzysztof Zaremba
- **Comment**: 10 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: The Liquid Argon Time Projection Chamber (LAr-TPC) detectors provide excellent imaging and particle identification ability for studying neutrinos. An efficient and automatic reconstruction procedures are required to exploit potential of this imaging technology. Herein, a novel method for segmentation of images from LAr-TPC detectors is presented. The proposed approach computes a feature descriptor for each pixel in the image, which characterizes amplitude distribution in pixel and its neighbourhood. The supervised classifier is employed to distinguish between pixels representing particle's track and noise. The classifier is trained and evaluated on the hand-labeled dataset. The proposed approach can be a preprocessing step for reconstructing algorithms working directly on detector images.



