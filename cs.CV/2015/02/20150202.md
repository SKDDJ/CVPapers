# Arxiv Papers in cs.CV on 2015-02-02
### Discriminatively Trained And-Or Graph Models for Object Shape Detection
- **Arxiv ID**: http://arxiv.org/abs/1502.00341v1
- **DOI**: 10.1109/TPAMI.2014.2359888
- **Categories**: **cs.CV**, 68U01
- **Links**: [PDF](http://arxiv.org/pdf/1502.00341v1)
- **Published**: 2015-02-02 02:04:01+00:00
- **Updated**: 2015-02-02 02:04:01+00:00
- **Authors**: Liang Lin, Xiaolong Wang, Wei Yang, Jian-Huang Lai
- **Comment**: 15 pages, 14 figures, TPAMI 2014
- **Journal**: Pattern Analysis and Machine Intelligence, IEEE Transactions on ,
  vol.PP, no.99, pp.1,1, 2014
- **Summary**: In this paper, we investigate a novel reconfigurable part-based model, namely And-Or graph model, to recognize object shapes in images. Our proposed model consists of four layers: leaf-nodes at the bottom are local classifiers for detecting contour fragments; or-nodes above the leaf-nodes function as the switches to activate their child leaf-nodes, making the model reconfigurable during inference; and-nodes in a higher layer capture holistic shape deformations; one root-node on the top, which is also an or-node, activates one of its child and-nodes to deal with large global variations (e.g. different poses and views). We propose a novel structural optimization algorithm to discriminatively train the And-Or model from weakly annotated data. This algorithm iteratively determines the model structures (e.g. the nodes and their layouts) along with the parameter learning. On several challenging datasets, our model demonstrates the effectiveness to perform robust shape-based object detection against background clutter and outperforms the other state-of-the-art approaches. We also release a new shape database with annotations, which includes more than 1500 challenging shape instances, for recognition and detection.



### Complex Background Subtraction by Pursuing Dynamic Spatio-Temporal Models
- **Arxiv ID**: http://arxiv.org/abs/1502.00344v1
- **DOI**: 10.1109/TIP.2014.2326776
- **Categories**: **cs.CV**, 68U01
- **Links**: [PDF](http://arxiv.org/pdf/1502.00344v1)
- **Published**: 2015-02-02 03:04:01+00:00
- **Updated**: 2015-02-02 03:04:01+00:00
- **Authors**: Liang Lin, Yuanlu Xu, Xiaodan Liang, Jianhuang Lai
- **Comment**: 12 pages, 7 figures
- **Journal**: Image Processing, IEEE Transactions on , vol.23, no.7,
  pp.3191,3202, July 2014
- **Summary**: Although it has been widely discussed in video surveillance, background subtraction is still an open problem in the context of complex scenarios, e.g., dynamic backgrounds, illumination variations, and indistinct foreground objects. To address these challenges, we propose an effective background subtraction method by learning and maintaining an array of dynamic texture models within the spatio-temporal representations. At any location of the scene, we extract a sequence of regular video bricks, i.e. video volumes spanning over both spatial and temporal domain. The background modeling is thus posed as pursuing subspaces within the video bricks while adapting the scene variations. For each sequence of video bricks, we pursue the subspace by employing the ARMA (Auto Regressive Moving Average) Model that jointly characterizes the appearance consistency and temporal coherence of the observations. During online processing, we incrementally update the subspaces to cope with disturbances from foreground objects and scene changes. In the experiments, we validate the proposed method in several complex scenarios, and show superior performances over other state-of-the-art approaches of background subtraction. The empirical studies of parameter setting and component analysis are presented as well.



### Iterated Support Vector Machines for Distance Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1502.00363v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1502.00363v1)
- **Published**: 2015-02-02 05:30:44+00:00
- **Updated**: 2015-02-02 05:30:44+00:00
- **Authors**: Wangmeng Zuo, Faqiang Wang, David Zhang, Liang Lin, Yuchi Huang, Deyu Meng, Lei Zhang
- **Comment**: 14 pages, 10 figures
- **Journal**: None
- **Summary**: Distance metric learning aims to learn from the given training data a valid distance metric, with which the similarity between data samples can be more effectively evaluated for classification. Metric learning is often formulated as a convex or nonconvex optimization problem, while many existing metric learning algorithms become inefficient for large scale problems. In this paper, we formulate metric learning as a kernel classification problem, and solve it by iterated training of support vector machines (SVM). The new formulation is easy to implement, efficient in training, and tractable for large-scale problems. Two novel metric learning models, namely Positive-semidefinite Constrained Metric Learning (PCML) and Nonnegative-coefficient Constrained Metric Learning (NCML), are developed. Both PCML and NCML can guarantee the global optimality of their solutions. Experimental results on UCI dataset classification, handwritten digit recognition, face verification and person re-identification demonstrate that the proposed metric learning methods achieve higher classification accuracy than state-of-the-art methods and they are significantly more efficient in training.



### Adaptive Scene Category Discovery with Generative Learning and Compositional Sampling
- **Arxiv ID**: http://arxiv.org/abs/1502.00374v1
- **DOI**: 10.1109/TCSVT.2014.2313897
- **Categories**: **cs.CV**, 68U01
- **Links**: [PDF](http://arxiv.org/pdf/1502.00374v1)
- **Published**: 2015-02-02 06:33:51+00:00
- **Updated**: 2015-02-02 06:33:51+00:00
- **Authors**: Liang Lin, Ruimao Zhang, Xiaohua Duan
- **Comment**: 11 pages, 7 figures
- **Journal**: Circuits and Systems for Video Technology, IEEE Transactions on ,
  vol.PP, no.99, pp.1,1, 2014
- **Summary**: This paper investigates a general framework to discover categories of unlabeled scene images according to their appearances (i.e., textures and structures). We jointly solve the two coupled tasks in an unsupervised manner: (i) classifying images without pre-determining the number of categories, and (ii) pursuing generative model for each category. In our method, each image is represented by two types of image descriptors that are effective to capture image appearances from different aspects. By treating each image as a graph vertex, we build up an graph, and pose the image categorization as a graph partition process. Specifically, a partitioned sub-graph can be regarded as a category of scenes, and we define the probabilistic model of graph partition by accumulating the generative models of all separated categories. For efficient inference with the graph, we employ a stochastic cluster sampling algorithm, which is designed based on the Metropolis-Hasting mechanism. During the iterations of inference, the model of each category is analytically updated by a generative learning algorithm. In the experiments, our approach is validated on several challenging databases, and it outperforms other popular state-of-the-art methods. The implementation details and empirical analysis are presented as well.



### Integrating Graph Partitioning and Matching for Trajectory Analysis in Video Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1502.00377v1
- **DOI**: 10.1109/TIP.2012.2211373
- **Categories**: **cs.CV**, 68U01
- **Links**: [PDF](http://arxiv.org/pdf/1502.00377v1)
- **Published**: 2015-02-02 06:52:47+00:00
- **Updated**: 2015-02-02 06:52:47+00:00
- **Authors**: Liang Lin, Yongyi Lu, Yan Pan, Xiaowu Chen
- **Comment**: 10 pages, 12 figures
- **Journal**: Image Processing, IEEE Transactions on , vol.21, no.12,
  pp.4844-4857, 2012
- **Summary**: In order to track the moving objects in long range against occlusion, interruption, and background clutter, this paper proposes a unified approach for global trajectory analysis. Instead of the traditional frame-by-frame tracking, our method recovers target trajectories based on a short sequence of video frames, e.g. $15$ frames. We initially calculate a foreground map at each frame, as obtained from a state-of-the-art background model. An attribute graph is then extracted from the foreground map, where the graph vertices are image primitives represented by the composite features. With this graph representation, we pose trajectory analysis as a joint task of spatial graph partitioning and temporal graph matching. The task can be formulated by maximizing a posteriori under the Bayesian framework, in which we integrate the spatio-temporal contexts and the appearance models. The probabilistic inference is achieved by a data-driven Markov Chain Monte Carlo (MCMC) algorithm. Given a peroid of observed frames, the algorithm simulates a ergodic and aperiodic Markov Chain, and it visits a sequence of solution states in the joint space of spatial graph partitioning and temporal graph matching. In the experiments, our method is tested on several challenging videos from the public datasets of visual surveillance, and it outperforms the state-of-the-art methods.



### Towards a solid solution of real-time fire and flame detection
- **Arxiv ID**: http://arxiv.org/abs/1502.00416v1
- **DOI**: 10.1007/s11042-014-2106-z
- **Categories**: **cs.CV**, 68U01
- **Links**: [PDF](http://arxiv.org/pdf/1502.00416v1)
- **Published**: 2015-02-02 09:34:01+00:00
- **Updated**: 2015-02-02 09:34:01+00:00
- **Authors**: Bo Jiang, Yongyi Lu, Xiying Li, Liang Lin
- **Comment**: 14 pages, 6 figures
- **Journal**: Multimedia Tools and Applications, pp. 1380-7501, 2014
- **Summary**: Although the object detection and recognition has received growing attention for decades, a robust fire and flame detection method is rarely explored. This paper presents an empirical study, towards a general and solid approach to fast detect fire and flame in videos, with the applications in video surveillance and event retrieval. Our system consists of three cascaded steps: (1) candidate regions proposing by a background model, (2) fire region classifying with color-texture features and a dictionary of visual words, and (3) temporal verifying. The experimental evaluation and analysis are done for each step. We believe that it is a useful service to both academic research and real-world application. In addition, we release the software of the proposed system with the source code, as well as a public benchmark and data set, including 64 video clips covered both indoor and outdoor scenes under different conditions. We achieve an 82% Recall with 93% Precision on the data set, and greatly improve the performance by state-of-the-arts methods.



### Structured Occlusion Coding for Robust Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1502.00478v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.00478v2)
- **Published**: 2015-02-02 13:48:46+00:00
- **Updated**: 2015-07-25 06:24:26+00:00
- **Authors**: Yandong Wen, Weiyang Liu, Meng Yang, Yuli Fu, Youjun Xiang, Rui Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Occlusion in face recognition is a common yet challenging problem. While sparse representation based classification (SRC) has been shown promising performance in laboratory conditions (i.e. noiseless or random pixel corrupted), it performs much worse in practical scenarios. In this paper, we consider the practical face recognition problem, where the occlusions are predictable and available for sampling. We propose the structured occlusion coding (SOC) to address occlusion problems. The structured coding here lies in two folds. On one hand, we employ a structured dictionary for recognition. On the other hand, we propose to use the structured sparsity in this formulation. Specifically, SOC simultaneously separates the occlusion and classifies the image. In this way, the problem of recognizing an occluded image is turned into seeking a structured sparse solution on occlusion-appended dictionary. In order to construct a well-performing occlusion dictionary, we propose an occlusion mask estimating technique via locality constrained dictionary (LCD), showing striking improvement in occlusion sample. On a category-specific occlusion dictionary, we replace norm sparsity with the structured sparsity which is shown more robust, further enhancing the robustness of our approach. Moreover, SOC achieves significant improvement in handling large occlusion in real world. Extensive experiments are conducted on public data sets to validate the superiority of the proposed algorithm.



### Fast and Robust Feature Matching for RGB-D Based Localization
- **Arxiv ID**: http://arxiv.org/abs/1502.00500v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1502.00500v1)
- **Published**: 2015-02-02 15:02:10+00:00
- **Updated**: 2015-02-02 15:02:10+00:00
- **Authors**: Miguel Heredia, Felix Endres, Wolfram Burgard, Rafael Sanz
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a novel approach to global localization using an RGB-D camera in maps of visual features. For large maps, the performance of pure image matching techniques decays in terms of robustness and computational cost. Particularly, repeated occurrences of similar features due to repeating structure in the world (e.g., doorways, chairs, etc.) or missing associations between observations pose critical challenges to visual localization. We address these challenges using a two-step approach. We first estimate a candidate pose using few correspondences between features of the current camera frame and the feature map. The initial set of correspondences is established by proximity in feature space. The initial pose estimate is used in the second step to guide spatial matching of features in 3D, i.e., searching for associations where the image features are expected to be found in the map. A RANSAC algorithm is used to compute a fine estimation of the pose from the correspondences. Our approach clearly outperforms localization based on feature matching exclusively in feature space, both in terms of estimation accuracy and robustness to failure and allows for global localization in real time (30Hz).



### An Expressive Deep Model for Human Action Parsing from A Single Image
- **Arxiv ID**: http://arxiv.org/abs/1502.00501v1
- **DOI**: 10.1109/ICME.2014.6890158
- **Categories**: **cs.CV**, 68U01
- **Links**: [PDF](http://arxiv.org/pdf/1502.00501v1)
- **Published**: 2015-02-02 15:03:25+00:00
- **Updated**: 2015-02-02 15:03:25+00:00
- **Authors**: Zhujin Liang, Xiaolong Wang, Rui Huang, Liang Lin
- **Comment**: 6 pages, 8 figures, ICME 2014
- **Journal**: None
- **Summary**: This paper aims at one newly raising task in vision and multimedia research: recognizing human actions from still images. Its main challenges lie in the large variations in human poses and appearances, as well as the lack of temporal motion information. Addressing these problems, we propose to develop an expressive deep model to naturally integrate human layout and surrounding contexts for higher level action understanding from still images. In particular, a Deep Belief Net is trained to fuse information from different noisy sources such as body part detection and object detection. To bridge the semantic gap, we used manually labeled data to greatly improve the effectiveness and efficiency of the pre-training and fine-tuning stages of the DBN training. The resulting framework is shown to be robust to sometimes unreliable inputs (e.g., imprecise detections of human parts and objects), and outperforms the state-of-the-art approaches.



### Complex-Valued Hough Transforms for Circles
- **Arxiv ID**: http://arxiv.org/abs/1502.00558v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.00558v2)
- **Published**: 2015-02-02 17:22:26+00:00
- **Updated**: 2015-02-09 19:38:58+00:00
- **Authors**: Marcelo Cicconet, Davi Geiger, Michael Werman
- **Comment**: The paper has been withdrawn since the authors concluded a more
  comprehensive study on the choice of parameters needs to be performed
- **Journal**: None
- **Summary**: This paper advocates the use of complex variables to represent votes in the Hough transform for circle detection. Replacing the positive numbers classically used in the parameter space of the Hough transforms by complex numbers allows cancellation effects when adding up the votes. Cancellation and the computation of shape likelihood via a complex number's magnitude square lead to more robust solutions than the "classic" algorithms, as shown by computational experiments on synthetic and real datasets.



### Quantum Pairwise Symmetry: Applications in 2D Shape Analysis
- **Arxiv ID**: http://arxiv.org/abs/1502.00561v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.00561v2)
- **Published**: 2015-02-02 17:33:55+00:00
- **Updated**: 2015-02-09 19:38:39+00:00
- **Authors**: Marcelo Cicconet, Davi Geiger, Michael Werman
- **Comment**: The paper has been withdrawn since the authors concluded a more
  comprehensive study on the choice of parameters needs to be performed
- **Journal**: None
- **Summary**: A pair of rooted tangents -- defining a quantum triangle -- with an associated quantum wave of spin 1/2 is proposed as the primitive to represent and compute symmetry. Measures of the spin characterize how "isosceles" or how "degenerate" these triangles are -- which corresponds to their mirror or parallel symmetry. We also introduce a complex-valued kernel to model probability errors in the parameter space, which is more robust to noise and clutter than the classical model.



### A Class of DCT Approximations Based on the Feig-Winograd Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1502.00592v2
- **DOI**: 10.1016/j.sigpro.2015.01.011
- **Categories**: **stat.ME**, cs.CV, cs.MM, cs.NA, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1502.00592v2)
- **Published**: 2015-02-02 19:39:46+00:00
- **Updated**: 2016-07-15 21:25:18+00:00
- **Authors**: C. J. Tablada, F. M. Bayer, R. J. Cintra
- **Comment**: 26 pages, 4 figures, 5 tables, fixed arithmetic complexity in Table
  IV
- **Journal**: Signal Processing, vol. 113, pp. 38-51, August 2015
- **Summary**: A new class of matrices based on a parametrization of the Feig-Winograd factorization of 8-point DCT is proposed. Such parametrization induces a matrix subspace, which unifies a number of existing methods for DCT approximation. By solving a comprehensive multicriteria optimization problem, we identified several new DCT approximations. Obtained solutions were sought to possess the following properties: (i) low multiplierless computational complexity, (ii) orthogonality or near orthogonality, (iii) low complexity invertibility, and (iv) close proximity and performance to the exact DCT. Proposed approximations were submitted to assessment in terms of proximity to the DCT, coding performance, and suitability for image compression. Considering Pareto efficiency, particular new proposed approximations could outperform various existing methods archived in literature.



### Learning the Matching Function
- **Arxiv ID**: http://arxiv.org/abs/1502.00652v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.00652v1)
- **Published**: 2015-02-02 21:20:43+00:00
- **Updated**: 2015-02-02 21:20:43+00:00
- **Authors**: Ľubor Ladický, Christian Häne, Marc Pollefeys
- **Comment**: rejected from ACCV 2014 and probably from CVPR 2015
- **Journal**: None
- **Summary**: The matching function for the problem of stereo reconstruction or optical flow has been traditionally designed as a function of the distance between the features describing matched pixels. This approach works under assumption, that the appearance of pixels in two stereo cameras or in two consecutive video frames does not change dramatically. However, this might not be the case, if we try to match pixels over a large interval of time.   In this paper we propose a method, which learns the matching function, that automatically finds the space of allowed changes in visual appearance, such as due to the motion blur, chromatic distortions, different colour calibration or seasonal changes. Furthermore, it automatically learns the importance of matching scores of contextual features at different relative locations and scales. Proposed classifier gives reliable estimations of pixel disparities already without any form of regularization.   We evaluated our method on two standard problems - stereo matching on KITTI outdoor dataset, optical flow on Sintel data set, and on newly introduced TimeLapse change detection dataset. Our algorithm obtained very promising results comparable to the state-of-the-art.



