# Arxiv Papers in cs.CV on 2015-02-09
### Tensor Canonical Correlation Analysis for Multi-view Dimension Reduction
- **Arxiv ID**: http://arxiv.org/abs/1502.02330v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1502.02330v1)
- **Published**: 2015-02-09 01:58:27+00:00
- **Updated**: 2015-02-09 01:58:27+00:00
- **Authors**: Yong Luo, Dacheng Tao, Yonggang Wen, Kotagiri Ramamohanarao, Chao Xu
- **Comment**: 20 pages, 10 figures
- **Journal**: None
- **Summary**: Canonical correlation analysis (CCA) has proven an effective tool for two-view dimension reduction due to its profound theoretical foundation and success in practical applications. In respect of multi-view learning, however, it is limited by its capability of only handling data represented by two-view features, while in many real-world applications, the number of views is frequently many more. Although the ad hoc way of simultaneously exploring all possible pairs of features can numerically deal with multi-view data, it ignores the high order statistics (correlation information) which can only be discovered by simultaneously exploring all features.   Therefore, in this work, we develop tensor CCA (TCCA) which straightforwardly yet naturally generalizes CCA to handle the data of an arbitrary number of views by analyzing the covariance tensor of the different views. TCCA aims to directly maximize the canonical correlation of multiple (more than two) views. Crucially, we prove that the multi-view canonical correlation maximization problem is equivalent to finding the best rank-1 approximation of the data covariance tensor, which can be solved efficiently using the well-known alternating least squares (ALS) algorithm. As a consequence, the high order correlation information contained in the different views is explored and thus a more reliable common subspace shared by all features can be obtained. In addition, a non-linear extension of TCCA is presented. Experiments on various challenge tasks, including large scale biometric structure prediction, internet advertisement classification and web image annotation, demonstrate the effectiveness of the proposed method.



### Out-of-sample generalizations for supervised manifold learning for classification
- **Arxiv ID**: http://arxiv.org/abs/1502.02410v1
- **DOI**: 10.1109/TIP.2016.2520368
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1502.02410v1)
- **Published**: 2015-02-09 09:56:57+00:00
- **Updated**: 2015-02-09 09:56:57+00:00
- **Authors**: Elif Vural, Christine Guillemot
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised manifold learning methods for data classification map data samples residing in a high-dimensional ambient space to a lower-dimensional domain in a structure-preserving way, while enhancing the separation between different classes in the learned embedding. Most nonlinear supervised manifold learning methods compute the embedding of the manifolds only at the initially available training points, while the generalization of the embedding to novel points, known as the out-of-sample extension problem in manifold learning, becomes especially important in classification applications. In this work, we propose a semi-supervised method for building an interpolation function that provides an out-of-sample extension for general supervised manifold learning algorithms studied in the context of classification. The proposed algorithm computes a radial basis function (RBF) interpolator that minimizes an objective function consisting of the total embedding error of unlabeled test samples, defined as their distance to the embeddings of the manifolds of their own class, as well as a regularization term that controls the smoothness of the interpolation function in a direction-dependent way. The class labels of test data and the interpolation function parameters are estimated jointly with a progressive procedure. Experimental results on face and object images demonstrate the potential of the proposed out-of-sample extension algorithm for the classification of manifold-modeled data sets.



### Deep Neural Networks for Anatomical Brain Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1502.02445v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1502.02445v2)
- **Published**: 2015-02-09 11:48:42+00:00
- **Updated**: 2015-06-25 16:19:44+00:00
- **Authors**: Alexandre de Brebisson, Giovanni Montana
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach to automatically segment magnetic resonance (MR) images of the human brain into anatomical regions. Our methodology is based on a deep artificial neural network that assigns each voxel in an MR image of the brain to its corresponding anatomical region. The inputs of the network capture information at different scales around the voxel of interest: 3D and orthogonal 2D intensity patches capture the local spatial context while large, compressed 2D orthogonal patches and distances to the regional centroids enforce global spatial consistency. Contrary to commonly used segmentation methods, our technique does not require any non-linear registration of the MR images. To benchmark our model, we used the dataset provided for the MICCAI 2012 challenge on multi-atlas labelling, which consists of 35 manually segmented MR images of the brain. We obtained competitive results (mean dice coefficient 0.725, error rate 0.163) showing the potential of our approach. To our knowledge, our technique is the first to tackle the anatomical segmentation of the whole brain using deep neural networks.



### Efficient batchwise dropout training using submatrices
- **Arxiv ID**: http://arxiv.org/abs/1502.02478v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1502.02478v1)
- **Published**: 2015-02-09 13:29:48+00:00
- **Updated**: 2015-02-09 13:29:48+00:00
- **Authors**: Ben Graham, Jeremy Reizenstein, Leigh Robinson
- **Comment**: None
- **Journal**: None
- **Summary**: Dropout is a popular technique for regularizing artificial neural networks. Dropout networks are generally trained by minibatch gradient descent with a dropout mask turning off some of the units---a different pattern of dropout is applied to every sample in the minibatch. We explore a very simple alternative to the dropout mask. Instead of masking dropped out units by setting them to zero, we perform matrix multiplication using a submatrix of the weight matrix---unneeded hidden units are never calculated. Performing dropout batchwise, so that one pattern of dropout is used for each sample in a minibatch, we can substantially reduce training times. Batchwise dropout can be used with fully-connected and convolutional neural networks.



### Predicting Alzheimer's disease: a neuroimaging study with 3D convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1502.02506v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1502.02506v1)
- **Published**: 2015-02-09 14:46:40+00:00
- **Updated**: 2015-02-09 14:46:40+00:00
- **Authors**: Adrien Payan, Giovanni Montana
- **Comment**: None
- **Journal**: None
- **Summary**: Pattern recognition methods using neuroimaging data for the diagnosis of Alzheimer's disease have been the subject of extensive research in recent years. In this paper, we use deep learning methods, and in particular sparse autoencoders and 3D convolutional neural networks, to build an algorithm that can predict the disease status of a patient, based on an MRI scan of the brain. We report on experiments using the ADNI data set involving 2,265 historical scans. We demonstrate that 3D convolutional neural networks outperform several other classifiers reported in the literature and produce state-of-art results.



### Analysis of classifiers' robustness to adversarial perturbations
- **Arxiv ID**: http://arxiv.org/abs/1502.02590v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1502.02590v4)
- **Published**: 2015-02-09 18:20:00+00:00
- **Updated**: 2016-03-28 22:50:52+00:00
- **Authors**: Alhussein Fawzi, Omar Fawzi, Pascal Frossard
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this paper is to analyze an intriguing phenomenon recently discovered in deep networks, namely their instability to adversarial perturbations (Szegedy et. al., 2014). We provide a theoretical framework for analyzing the robustness of classifiers to adversarial perturbations, and show fundamental upper bounds on the robustness of classifiers. Specifically, we establish a general upper bound on the robustness of classifiers to adversarial perturbations, and then illustrate the obtained upper bound on the families of linear and quadratic classifiers. In both cases, our upper bound depends on a distinguishability measure that captures the notion of difficulty of the classification task. Our results for both classes imply that in tasks involving small distinguishability, no classifier in the considered set will be robust to adversarial perturbations, even if a good accuracy is achieved. Our theoretical framework moreover suggests that the phenomenon of adversarial instability is due to the low flexibility of classifiers, compared to the difficulty of the classification task (captured by the distinguishability). Moreover, we show the existence of a clear distinction between the robustness of a classifier to random noise and its robustness to adversarial perturbations. Specifically, the former is shown to be larger than the latter by a factor that is proportional to \sqrt{d} (with d being the signal dimension) for linear classifiers. This result gives a theoretical explanation for the discrepancy between the two robustness properties in high dimensional problems, which was empirically observed in the context of neural networks. To the best of our knowledge, our results provide the first theoretical work that addresses the phenomenon of adversarial instability recently observed for deep networks. Our analysis is complemented by experimental results on controlled and real-world data.



### Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1502.02734v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.02734v3)
- **Published**: 2015-02-09 23:38:45+00:00
- **Updated**: 2015-10-05 23:29:28+00:00
- **Authors**: George Papandreou, Liang-Chieh Chen, Kevin Murphy, Alan L. Yuille
- **Comment**: Accepted to ICCV 2015
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently significantly pushed the state-of-art in semantic image segmentation. We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop Expectation-Maximization (EM) methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering competitive results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort. We share source code implementing the proposed system at https://bitbucket.org/deeplab/deeplab-public.



