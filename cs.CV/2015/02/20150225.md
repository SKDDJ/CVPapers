# Arxiv Papers in cs.CV on 2015-02-25
### Building with Drones: Accurate 3D Facade Reconstruction using MAVs
- **Arxiv ID**: http://arxiv.org/abs/1502.07019v1
- **DOI**: 10.1109/ICRA.2015.7139681
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1502.07019v1)
- **Published**: 2015-02-25 00:52:11+00:00
- **Updated**: 2015-02-25 00:52:11+00:00
- **Authors**: Shreyansh Daftry, Christof Hoppe, Horst Bischof
- **Comment**: 8 Pages, 2015 IEEE International Conference on Robotics and
  Automation (ICRA '15), Seattle, WA, USA
- **Journal**: None
- **Summary**: Automatic reconstruction of 3D models from images using multi-view Structure-from-Motion methods has been one of the most fruitful outcomes of computer vision. These advances combined with the growing popularity of Micro Aerial Vehicles as an autonomous imaging platform, have made 3D vision tools ubiquitous for large number of Architecture, Engineering and Construction applications among audiences, mostly unskilled in computer vision. However, to obtain high-resolution and accurate reconstructions from a large-scale object using SfM, there are many critical constraints on the quality of image data, which often become sources of inaccuracy as the current 3D reconstruction pipelines do not facilitate the users to determine the fidelity of input data during the image acquisition. In this paper, we present and advocate a closed-loop interactive approach that performs incremental reconstruction in real-time and gives users an online feedback about the quality parameters like Ground Sampling Distance (GSD), image redundancy, etc on a surface mesh. We also propose a novel multi-scale camera network design to prevent scene drift caused by incremental map building, and release the first multi-scale image sequence dataset as a benchmark. Further, we evaluate our system on real outdoor scenes, and show that our interactive pipeline combined with a multi-scale camera network approach provides compelling accuracy in multi-view reconstruction tasks when compared against the state-of-the-art methods.



### Describing Colors, Textures and Shapes for Content Based Image Retrieval - A Survey
- **Arxiv ID**: http://arxiv.org/abs/1502.07041v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1502.07041v1)
- **Published**: 2015-02-25 03:51:43+00:00
- **Updated**: 2015-02-25 03:51:43+00:00
- **Authors**: Jamil Ahmad, Muhammad Sajjad, Irfan Mehmood, Seungmin Rho, Sung Wook Baik
- **Comment**: None
- **Journal**: (2014), Journal of Platform Technology 2(4): 34-48
- **Summary**: Visual media has always been the most enjoyed way of communication. From the advent of television to the modern day hand held computers, we have witnessed the exponential growth of images around us. Undoubtedly it's a fact that they carry a lot of information in them which needs be utilized in an effective manner. Hence intense need has been felt to efficiently index and store large image collections for effective and on- demand retrieval. For this purpose low-level features extracted from the image contents like color, texture and shape has been used. Content based image retrieval systems employing these features has proven very successful. Image retrieval has promising applications in numerous fields and hence has motivated researchers all over the world. New and improved ways to represent visual content are being developed each day. Tremendous amount of research has been carried out in the last decade. In this paper we will present a detailed overview of some of the powerful color, texture and shape descriptors for content based image retrieval. A comparative analysis will also be carried out for providing an insight into outstanding challenges in this field.



### Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1502.07058v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1502.07058v1)
- **Published**: 2015-02-25 05:58:43+00:00
- **Updated**: 2015-02-25 05:58:43+00:00
- **Authors**: Adam W. Harley, Alex Ufkes, Konstantinos G. Derpanis
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular hand-crafted alternatives. Experiments also show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories, useful for training new CNNs for document analysis.



### Exploiting Feature and Class Relationships in Video Categorization with Regularized Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1502.07209v2
- **DOI**: 10.1109/TPAMI.2017.2670560
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1502.07209v2)
- **Published**: 2015-02-25 15:41:48+00:00
- **Updated**: 2018-02-21 20:37:34+00:00
- **Authors**: Yu-Gang Jiang, Zuxuan Wu, Jun Wang, Xiangyang Xue, Shih-Fu Chang
- **Comment**: Please cite the officially published IEEE TPAMI version if you find
  this work helpful
- **Journal**: IEEE TPAMI 40.2 (2018): 352-364
- **Summary**: In this paper, we study the challenging problem of categorizing videos according to high-level semantics such as the existence of a particular human action or a complex event. Although extensive efforts have been devoted in recent years, most existing works combined multiple video features using simple fusion strategies and neglected the utilization of inter-class semantic relationships. This paper proposes a novel unified framework that jointly exploits the feature relationships and the class relationships for improved categorization performance. Specifically, these two types of relationships are estimated and utilized by rigorously imposing regularizations in the learning process of a deep neural network (DNN). Such a regularized DNN (rDNN) can be efficiently realized using a GPU-based implementation with an affordable training cost. Through arming the DNN with better capability of harnessing both the feature and the class relationships, the proposed rDNN is more suitable for modeling video semantics. With extensive experimental evaluations, we show that rDNN produces superior performance over several state-of-the-art approaches. On the well-known Hollywood2 and Columbia Consumer Video benchmarks, we obtain very competitive results: 66.9\% and 73.5\% respectively in terms of mean average precision. In addition, to substantially evaluate our rDNN and stimulate future research on large scale video categorization, we collect and release a new benchmark dataset, called FCVID, which contains 91,223 Internet videos and 239 manually annotated categories.



### Proceedings of the DATE Friday Workshop on Heterogeneous Architectures and Design Methods for Embedded Image Systems (HIS 2015)
- **Arxiv ID**: http://arxiv.org/abs/1502.07241v2
- **DOI**: None
- **Categories**: **cs.AR**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1502.07241v2)
- **Published**: 2015-02-25 16:52:56+00:00
- **Updated**: 2015-02-26 06:00:09+00:00
- **Authors**: Frank Hannig, Dietmar Fey, Anton Lokhmotov
- **Comment**: Website of the workshop: https://www12.cs.fau.de/ws/his2015/
- **Journal**: None
- **Summary**: This volume contains the papers accepted at the DATE Friday Workshop on Heterogeneous Architectures and Design Methods for Embedded Image Systems (HIS 2015), held in Grenoble, France, March 13, 2015. HIS 2015 was co-located with the Conference on Design, Automation and Test in Europe (DATE).



### Highly corrupted image inpainting through hypoelliptic diffusion
- **Arxiv ID**: http://arxiv.org/abs/1502.07331v4
- **DOI**: 10.1007/s10851-018-0810-4
- **Categories**: **cs.CV**, math.AP
- **Links**: [PDF](http://arxiv.org/pdf/1502.07331v4)
- **Published**: 2015-02-25 20:36:08+00:00
- **Updated**: 2018-04-05 16:52:34+00:00
- **Authors**: Ugo Boscain, Roman Chertovskih, Jean-Paul Gauthier, Dario Prandi, Alexey Remizov
- **Comment**: 15 pages, 10 figures
- **Journal**: Journal of Mathematical Imaging and Vision, 2018.
  https://rdcu.be/KFww
- **Summary**: We present a new image inpainting algorithm, the Averaging and Hypoelliptic Evolution (AHE) algorithm, inspired by the one presented in [SIAM J. Imaging Sci., vol. 7, no. 2, pp. 669--695, 2014] and based upon a semi-discrete variation of the Citti-Petitot-Sarti model of the primary visual cortex V1. The AHE algorithm is based on a suitable combination of sub-Riemannian hypoelliptic diffusion and ad-hoc local averaging techniques. In particular, we focus on reconstructing highly corrupted images (i.e. where more than the 80% of the image is missing), for which we obtain reconstructions comparable with the state-of-the-art.



