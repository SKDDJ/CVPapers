# Arxiv Papers in cs.CV on 2015-08-09
### Digging Deep into the layers of CNNs: In Search of How CNNs Achieve View Invariance
- **Arxiv ID**: http://arxiv.org/abs/1508.01983v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1508.01983v4)
- **Published**: 2015-08-09 04:02:51+00:00
- **Updated**: 2016-06-20 10:05:15+00:00
- **Authors**: Amr Bakry, Mohamed Elhoseiny, Tarek El-Gaaly, Ahmed Elgammal
- **Comment**: This paper accepted in ICLR 2016 main conference
- **Journal**: None
- **Summary**: This paper is focused on studying the view-manifold structure in the feature spaces implied by the different layers of Convolutional Neural Networks (CNN). There are several questions that this paper aims to answer: Does the learned CNN representation achieve viewpoint invariance? How does it achieve viewpoint invariance? Is it achieved by collapsing the view manifolds, or separating them while preserving them? At which layer is view invariance achieved? How can the structure of the view manifold at each layer of a deep convolutional neural network be quantified experimentally? How does fine-tuning of a pre-trained CNN on a multi-view dataset affect the representation at each layer of the network? In order to answer these questions we propose a methodology to quantify the deformation and degeneracy of view manifolds in CNN layers. We apply this methodology and report interesting results in this paper that answer the aforementioned questions.



### Image Representations and New Domains in Neural Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1508.02091v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1508.02091v1)
- **Published**: 2015-08-09 22:52:10+00:00
- **Updated**: 2015-08-09 22:52:10+00:00
- **Authors**: Jack Hessel, Nicolas Savva, Michael J. Wilber
- **Comment**: 11 Pages, 5 Images, To appear at EMNLP 2015's Vision + Learning
  workshop
- **Journal**: None
- **Summary**: We examine the possibility that recent promising results in automatic caption generation are due primarily to language models. By varying image representation quality produced by a convolutional neural network, we find that a state-of-the-art neural captioning algorithm is able to produce quality captions even when provided with surprisingly poor image representations. We replicate this result in a new, fine-grained, transfer learned captioning domain, consisting of 66K recipe image/title pairs. We also provide some experiments regarding the appropriateness of datasets for automatic captioning, and find that having multiple captions per image is beneficial, but not an absolute requirement.



