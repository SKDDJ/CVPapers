# Arxiv Papers in cs.CV on 2015-08-04
### Recognition of Emotions using Kinects
- **Arxiv ID**: http://arxiv.org/abs/1508.00761v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1508.00761v1)
- **Published**: 2015-08-04 13:03:27+00:00
- **Updated**: 2015-08-04 13:03:27+00:00
- **Authors**: Shun Li, Changye Zhu, Liqing Cui, Nan Zhao, Baobin Li, Tingshao Zhu
- **Comment**: 15 pages, 4 figures
- **Journal**: None
- **Summary**: Psychological studies indicate that emotional states are expressed in the way people walk and the human gait is investigated in terms of its ability to reveal a person's emotional state. And Microsoft Kinect is a rapidly developing, inexpensive, portable and no-marker motion capture system. This paper gives a new referable method to do emotion recognition, by using Microsoft Kinect to do gait pattern analysis, which has not been reported. $59$ subjects are recruited in this study and their gait patterns are record by two Kinect cameras. Significant joints selecting, Coordinate system transforming, Slider window gauss filter, Differential operation, and Data segmentation are used in data preprocessing. Feature extracting is based on Fourier transformation. By using the NaiveBayes, RandomForests, libSVM and SMO classification, the recognition rate of natural and unnatural emotions can reach above 70%.It is concluded that using the Kinect system can be a new method in recognition of emotions.



### Online Domain Adaptation for Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1508.00776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1508.00776v1)
- **Published**: 2015-08-04 14:01:55+00:00
- **Updated**: 2015-08-04 14:01:55+00:00
- **Authors**: Adrien Gaidon, Eleonora Vig
- **Comment**: To appear at BMVC 2015
- **Journal**: None
- **Summary**: Automatically detecting, labeling, and tracking objects in videos depends first and foremost on accurate category-level object detectors. These might, however, not always be available in practice, as acquiring high-quality large scale labeled training datasets is either too costly or impractical for all possible real-world application scenarios. A scalable solution consists in re-using object detectors pre-trained on generic datasets. This work is the first to investigate the problem of on-line domain adaptation of object detectors for causal multi-object tracking (MOT). We propose to alleviate the dataset bias by adapting detectors from category to instances, and back: (i) we jointly learn all target models by adapting them from the pre-trained one, and (ii) we also adapt the pre-trained model on-line. We introduce an on-line multi-task learning algorithm to efficiently share parameters and reduce drift, while gradually improving recall. Our approach is applicable to any linear object detector, and we evaluate both cheap "mini-Fisher Vectors" and expensive "off-the-shelf" ConvNet features. We quantitatively measure the benefit of our domain adaptation strategy on the KITTI tracking benchmark and on a new dataset (PASCAL-to-KITTI) we introduce to study the domain mismatch problem in MOT.



### Semantic Pose using Deep Networks Trained on Synthetic RGB-D
- **Arxiv ID**: http://arxiv.org/abs/1508.00835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1508.00835v1)
- **Published**: 2015-08-04 17:07:51+00:00
- **Updated**: 2015-08-04 17:07:51+00:00
- **Authors**: Jeremie Papon, Markus Schoeler
- **Comment**: ICCV 2015 Submission
- **Journal**: None
- **Summary**: In this work we address the problem of indoor scene understanding from RGB-D images. Specifically, we propose to find instances of common furniture classes, their spatial extent, and their pose with respect to generalized class models. To accomplish this, we use a deep, wide, multi-output convolutional neural network (CNN) that predicts class, pose, and location of possible objects simultaneously. To overcome the lack of large annotated RGB-D training sets (especially those with pose), we use an on-the-fly rendering pipeline that generates realistic cluttered room scenes in parallel to training. We then perform transfer learning on the relatively small amount of publicly available annotated RGB-D data, and find that our model is able to successfully annotate even highly challenging real scenes. Importantly, our trained network is able to understand noisy and sparse observations of highly cluttered scenes with a remarkable degree of accuracy, inferring class and pose from a very limited set of cues. Additionally, our neural network is only moderately deep and computes class, pose and position in tandem, so the overall run-time is significantly faster than existing methods, estimating all output parameters simultaneously in parallel on a GPU in seconds.



