# Arxiv Papers in cs.CV on 2015-08-28
### Discrete Hashing with Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1508.07148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1508.07148v1)
- **Published**: 2015-08-28 09:38:05+00:00
- **Updated**: 2015-08-28 09:38:05+00:00
- **Authors**: Thanh-Toan Do, Anh-Zung Doan, Ngai-Man Cheung
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of learning binary hash codes for large scale image search by proposing a novel hashing method based on deep neural network. The advantage of our deep model over previous deep model used in hashing is that our model contains necessary criteria for producing good codes such as similarity preserving, balance and independence. Another advantage of our method is that instead of relaxing the binary constraint of codes during the learning process as most previous works, in this paper, by introducing the auxiliary variable, we reformulate the optimization into two sub-optimization steps allowing us to efficiently solve binary constraints without any relaxation.   The proposed method is also extended to the supervised hashing by leveraging the label information such that the learned binary codes preserve the pairwise label of inputs.   The experimental results on three benchmark datasets show the proposed methods outperform state-of-the-art hashing methods.



### Bilevel parameter learning for higher-order total variation regularisation models
- **Arxiv ID**: http://arxiv.org/abs/1508.07243v1
- **DOI**: 10.1007/s10851-016-0662-8
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1508.07243v1)
- **Published**: 2015-08-28 15:36:15+00:00
- **Updated**: 2015-08-28 15:36:15+00:00
- **Authors**: J. C. De los Reyes, C. -B. Sch√∂nlieb, T. Valkonen
- **Comment**: None
- **Journal**: None
- **Summary**: We consider a bilevel optimisation approach for parameter learning in higher-order total variation image reconstruction models. Apart from the least squares cost functional, naturally used in bilevel learning, we propose and analyse an alternative cost, based on a Huber regularised TV-seminorm. Differentiability properties of the solution operator are verified and a first-order optimality system is derived. Based on the adjoint information, a quasi-Newton algorithm is proposed for the numerical solution of the bilevel problems. Numerical experiments are carried out to show the suitability of our approach and the improved performance of the new cost functional. Thanks to the bilevel optimisation framework, also a detailed comparison between TGV$^2$ and ICTV is carried out, showing the advantages and shortcomings of both regularisers, depending on the structure of the processed images and their noise level.



