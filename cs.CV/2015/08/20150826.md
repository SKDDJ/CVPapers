# Arxiv Papers in cs.CV on 2015-08-26
### SPF-CellTracker: Tracking multiple cells with strongly-correlated moves using a spatial particle filter
- **Arxiv ID**: http://arxiv.org/abs/1508.06464v3
- **DOI**: 10.1109/TCBB.2017.2782255
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1508.06464v3)
- **Published**: 2015-08-26 12:27:17+00:00
- **Updated**: 2016-05-27 19:52:44+00:00
- **Authors**: Osamu Hirose, Shotaro Kawaguchi, Terumasa Tokunaga, Yu Toyoshima, Takayuki Teramoto, Sayuri Kuge, Takeshi Ishihara, Yuichi Iino, Ryo Yoshida
- **Comment**: 14 pages, 6 figures
- **Journal**: IEEE/ACM Trans.Comput.Biol.Bioinform. 2017
- **Summary**: Tracking many cells in time-lapse 3D image sequences is an important challenging task of bioimage informatics. Motivated by a study of brain-wide 4D imaging of neural activity in C. elegans, we present a new method of multi-cell tracking. Data types to which the method is applicable are characterized as follows: (i) cells are imaged as globular-like objects, (ii) it is difficult to distinguish cells based only on shape and size, (iii) the number of imaged cells ranges in several hundreds, (iv) moves of nearly-located cells are strongly correlated and (v) cells do not divide. We developed a tracking software suite which we call SPF-CellTracker. Incorporating dependency on cells' moves into prediction model is the key to reduce the tracking errors: cell-switching and coalescence of tracked positions. We model target cells' correlated moves as a Markov random field and we also derive a fast computation algorithm, which we call spatial particle filter. With the live-imaging data of nuclei of C. elegans neurons in which approximately 120 nuclei of neurons are imaged, we demonstrate an advantage of the proposed method over the standard particle filter and a method developed by Tokunaga et al. (2014).



### Deep Convolutional Neural Networks for Smile Recognition
- **Arxiv ID**: http://arxiv.org/abs/1508.06535v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1508.06535v1)
- **Published**: 2015-08-26 15:39:09+00:00
- **Updated**: 2015-08-26 15:39:09+00:00
- **Authors**: Patrick O. Glauner
- **Comment**: MSc thesis
- **Journal**: None
- **Summary**: This thesis describes the design and implementation of a smile detector based on deep convolutional neural networks. It starts with a summary of neural networks, the difficulties of training them and new training methods, such as Restricted Boltzmann Machines or autoencoders. It then provides a literature review of convolutional neural networks and recurrent neural networks. In order to select databases for smile recognition, comprehensive statistics of databases popular in the field of facial expression recognition were generated and are summarized in this thesis. It then proposes a model for smile detection, of which the main part is implemented. The experimental results are discussed in this thesis and justified based on a comprehensive model selection performed. All experiments were run on a Tesla K40c GPU benefiting from a speedup of up to factor 10 over the computations on a CPU. A smile detection test accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action (DISFA) database, significantly outperforming existing approaches with accuracies ranging from 65.55% to 79.67%. This experiment is re-run under various variations, such as retaining less neutral images or only the low or high intensities, of which the results are extensively compared.



### A Neural Algorithm of Artistic Style
- **Arxiv ID**: http://arxiv.org/abs/1508.06576v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1508.06576v2)
- **Published**: 2015-08-26 17:14:42+00:00
- **Updated**: 2015-09-02 08:24:59+00:00
- **Authors**: Leon A. Gatys, Alexander S. Ecker, Matthias Bethge
- **Comment**: None
- **Journal**: None
- **Summary**: In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.



### Towards universal neural nets: Gibbs machines and ACE
- **Arxiv ID**: http://arxiv.org/abs/1508.06585v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, I.2.6; I.4; I.5; I.6
- **Links**: [PDF](http://arxiv.org/pdf/1508.06585v5)
- **Published**: 2015-08-26 17:43:08+00:00
- **Updated**: 2016-06-30 06:26:34+00:00
- **Authors**: Galin Georgiev
- **Comment**: v5: added thermodynamic identities and variational error estimation;
  expanded references
- **Journal**: None
- **Summary**: We study from a physics viewpoint a class of generative neural nets, Gibbs machines, designed for gradual learning. While including variational auto-encoders, they offer a broader universal platform for incrementally adding newly learned features, including physical symmetries. Their direct connection to statistical physics and information geometry is established. A variational Pythagorean theorem justifies invoking the exponential/Gibbs class of probabilities for creating brand new objects. Combining these nets with classifiers, gives rise to a brand of universal generative neural nets - stochastic auto-classifier-encoders (ACE). ACE have state-of-the-art performance in their class, both for classification and density estimation for the MNIST data set.



