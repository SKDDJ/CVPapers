# Arxiv Papers in cs.CV on 2015-08-17
### Pose-Guided Human Parsing with Deep Learned Features
- **Arxiv ID**: http://arxiv.org/abs/1508.03881v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1508.03881v2)
- **Published**: 2015-08-17 00:05:38+00:00
- **Updated**: 2015-11-25 02:07:11+00:00
- **Authors**: Fangting Xia, Jun Zhu, Peng Wang, Alan Yuille
- **Comment**: 12 pages, 10 figures, a shortened version of this paper was accepted
  by AAAI 2016
- **Journal**: None
- **Summary**: Parsing human body into semantic regions is crucial to human-centric analysis. In this paper, we propose a segment-based parsing pipeline that explores human pose information, i.e. the joint location of a human model, which improves the part proposal, accelerates the inference and regularizes the parsing process at the same time. Specifically, we first generate part segment proposals with respect to human joints predicted by a deep model, then part- specific ranking models are trained for segment selection using both pose-based features and deep-learned part potential features. Finally, the best ensemble of the proposed part segments are inferred though an And-Or Graph.   We evaluate our approach on the popular Penn-Fudan pedestrian parsing dataset, and demonstrate the effectiveness of using the pose information for each stage of the parsing pipeline. Finally, we show that our approach yields superior part segmentation accuracy comparing to the state-of-the-art methods.



### LCNN: Low-level Feature Embedded CNN for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1508.03928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1508.03928v1)
- **Published**: 2015-08-17 05:45:12+00:00
- **Updated**: 2015-08-17 05:45:12+00:00
- **Authors**: Hongyang Li, Huchuan Lu, Zhe Lin, Xiaohui Shen, Brian Price
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel deep neural network framework embedded with low-level features (LCNN) for salient object detection in complex images. We utilise the advantage of convolutional neural networks to automatically learn the high-level features that capture the structured information and semantic context in the image. In order to better adapt a CNN model into the saliency task, we redesign the network architecture based on the small-scale datasets. Several low-level features are extracted, which can effectively capture contrast and spatial information in the salient regions, and incorporated to compensate with the learned high-level features at the output of the last fully connected layer. The concatenated feature vector is further fed into a hinge-loss SVM detector in a joint discriminative learning manner and the final saliency score of each region within the bounding box is obtained by the linear combination of the detector's weights. Experiments on three challenging benchmark (MSRA-5000, PASCAL-S, ECCSD) demonstrate our algorithm to be effective and superior than most low-level oriented state-of-the-arts in terms of P-R curves, F-measure and mean absolute errors.



### Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1508.03929v4
- **DOI**: 10.1038/srep32672
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1508.03929v4)
- **Published**: 2015-08-17 05:46:24+00:00
- **Updated**: 2016-06-28 10:55:18+00:00
- **Authors**: Saeed Reza Kheradpisheh, Masoud Ghodrati, Mohammad Ganjtabesh, Timoth√©e Masquelier
- **Comment**: None
- **Journal**: Scientific Reports (2016) 6: 32672
- **Summary**: Deep convolutional neural networks (DCNNs) have attracted much attention recently, and have shown to be able to recognize thousands of object categories in natural image databases. Their architecture is somewhat similar to that of the human visual system: both use restricted receptive fields, and a hierarchy of layers which progressively extract more and more abstracted features. Yet it is unknown whether DCNNs match human performance at the task of view-invariant object recognition, whether they make similar errors and use similar representations for this task, and whether the answers depend on the magnitude of the viewpoint variations. To investigate these issues, we benchmarked eight state-of-the-art DCNNs, the HMAX model, and a baseline shallow model and compared their results to those of humans with backward masking. Unlike in all previous DCNN studies, we carefully controlled the magnitude of the viewpoint variations to demonstrate that shallow nets can outperform deep nets and humans when variations are weak. When facing larger variations, however, more layers were needed to match human performance and error distributions, and to have representations that are consistent with human behavior. A very deep net with 18 layers even outperformed humans at the highest variation level, using the most human-like representations.



### Sense Beyond Expressions: Cuteness
- **Arxiv ID**: http://arxiv.org/abs/1508.03953v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1508.03953v1)
- **Published**: 2015-08-17 08:48:54+00:00
- **Updated**: 2015-08-17 08:48:54+00:00
- **Authors**: Kang Wang, Tam V. Nguyen, Jiashi Feng, Jose Sepulveda
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: With the development of Internet culture, cuteness has become a popular concept. Many people are curious about what factors making a person look cute. However, there is rare research to answer this interesting question. In this work, we construct a dataset of personal images with comprehensively annotated cuteness scores and facial attributes to investigate this high-level concept in depth. Based on this dataset, through an automatic attributes mining process, we find several critical attributes determining the cuteness of a person. We also develop a novel Continuous Latent Support Vector Machine (C-LSVM) method to predict the cuteness score of one person given only his image. Extensive evaluations validate the effectiveness of the proposed method for cuteness prediction.



### Owl and Lizard: Patterns of Head Pose and Eye Pose in Driver Gaze Classification
- **Arxiv ID**: http://arxiv.org/abs/1508.04028v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1508.04028v3)
- **Published**: 2015-08-17 13:54:05+00:00
- **Updated**: 2016-11-20 04:46:26+00:00
- **Authors**: Lex Fridman, Joonbum Lee, Bryan Reimer, Trent Victor
- **Comment**: Accepted for Publication in IET Computer Vision. arXiv admin note:
  text overlap with arXiv:1507.04760
- **Journal**: None
- **Summary**: Accurate, robust, inexpensive gaze tracking in the car can help keep a driver safe by facilitating the more effective study of how to improve (1) vehicle interfaces and (2) the design of future Advanced Driver Assistance Systems. In this paper, we estimate head pose and eye pose from monocular video using methods developed extensively in prior work and ask two new interesting questions. First, how much better can we classify driver gaze using head and eye pose versus just using head pose? Second, are there individual-specific gaze strategies that strongly correlate with how much gaze classification improves with the addition of eye pose information? We answer these questions by evaluating data drawn from an on-road study of 40 drivers. The main insight of the paper is conveyed through the analogy of an "owl" and "lizard" which describes the degree to which the eyes and the head move when shifting gaze. When the head moves a lot ("owl"), not much classification improvement is attained by estimating eye pose on top of head pose. On the other hand, when the head stays still and only the eyes move ("lizard"), classification accuracy increases significantly from adding in eye pose. We characterize how that accuracy varies between people, gaze strategies, and gaze regions.



### A Generative Model for Multi-Dialect Representation
- **Arxiv ID**: http://arxiv.org/abs/1508.04035v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1508.04035v1)
- **Published**: 2015-08-17 14:05:44+00:00
- **Updated**: 2015-08-17 14:05:44+00:00
- **Authors**: Emmanuel N. Osegi
- **Comment**: 19 pages, 3 figures, 2 tables, Appendix
- **Journal**: None
- **Summary**: In the era of deep learning several unsupervised models have been developed to capture the key features in unlabeled handwritten data. Popular among them is the Restricted Boltzmann Machines RBM. However, due to the novelty in handwritten multidialect data, the RBM may fail to generate an efficient representation. In this paper we propose a generative model, the Mode Synthesizing Machine MSM for on-line representation of real life handwritten multidialect language data. The MSM takes advantage of the hierarchical representation of the modes of a data distribution using a two-point error update to learn a sequence of representative multidialects in a generative way. Experiments were performed to evaluate the performance of the MSM over the RBM with the former attaining much lower error values than the latter on both independent and mixed data set.



