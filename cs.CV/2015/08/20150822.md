# Arxiv Papers in cs.CV on 2015-08-22
### StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity
- **Arxiv ID**: http://arxiv.org/abs/1508.05463v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1508.05463v4)
- **Published**: 2015-08-22 03:36:43+00:00
- **Updated**: 2015-11-10 20:30:05+00:00
- **Authors**: Mohammad Javad Shafiee, Parthipan Siva, Alexander Wong
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Deep neural networks is a branch in machine learning that has seen a meteoric rise in popularity due to its powerful abilities to represent and model high-level abstractions in highly complex data. One area in deep neural networks that is ripe for exploration is neural connectivity formation. A pivotal study on the brain tissue of rats found that synaptic formation for specific functional connectivity in neocortical neural microcircuits can be surprisingly well modeled and predicted as a random formation. Motivated by this intriguing finding, we introduce the concept of StochasticNet, where deep neural networks are formed via stochastic connectivity between neurons. As a result, any type of deep neural networks can be formed as a StochasticNet by allowing the neuron connectivity to be stochastic. Stochastic synaptic formations, in a deep neural network architecture, can allow for efficient utilization of neurons for performing specific tasks. To evaluate the feasibility of such a deep neural network architecture, we train a StochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, and STL-10). Experimental results show that a StochasticNet, using less than half the number of neural connections as a conventional deep neural network, achieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST and SVHN dataset. Interestingly, StochasticNet with less than half the number of neural connections, achieved a higher accuracy (relative improvement in test error rate of ~6% compared to ConvNet) on the STL-10 dataset than a conventional deep neural network. Finally, StochasticNets have faster operational speeds while achieving better or similar accuracy performances.



### Gaussian Mixture Reduction Using Reverse Kullback-Leibler Divergence
- **Arxiv ID**: http://arxiv.org/abs/1508.05514v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.RO, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1508.05514v1)
- **Published**: 2015-08-22 13:41:17+00:00
- **Updated**: 2015-08-22 13:41:17+00:00
- **Authors**: Tohid Ardeshiri, Umut Orguner, Emre Ã–zkan
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a greedy mixture reduction algorithm which is capable of pruning mixture components as well as merging them based on the Kullback-Leibler divergence (KLD). The algorithm is distinct from the well-known Runnalls' KLD based method since it is not restricted to merging operations. The capability of pruning (in addition to merging) gives the algorithm the ability of preserving the peaks of the original mixture during the reduction. Analytical approximations are derived to circumvent the computational intractability of the KLD which results in a computationally efficient method. The proposed algorithm is compared with Runnalls' and Williams' methods in two numerical examples, using both simulated and real world data. The results indicate that the performance and computational complexity of the proposed approach make it an efficient alternative to existing mixture reduction methods.



