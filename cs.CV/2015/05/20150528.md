# Arxiv Papers in cs.CV on 2015-05-28
### Like Partying? Your Face Says It All. Predicting the Ambiance of Places with Profile Pictures
- **Arxiv ID**: http://arxiv.org/abs/1505.07522v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1505.07522v1)
- **Published**: 2015-05-28 01:23:05+00:00
- **Updated**: 2015-05-28 01:23:05+00:00
- **Authors**: Miriam Redi, Daniele Quercia, Lindsay T. Graham, Samuel D. Gosling
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: To choose restaurants and coffee shops, people are increasingly relying on social-networking sites. In a popular site such as Foursquare or Yelp, a place comes with descriptions and reviews, and with profile pictures of people who frequent them. Descriptions and reviews have been widely explored in the research area of data mining. By contrast, profile pictures have received little attention. Previous work showed that people are able to partly guess a place's ambiance, clientele, and activities not only by observing the place itself but also by observing the profile pictures of its visitors. Here we further that work by determining which visual cues people may have relied upon to make their guesses; showing that a state-of-the-art algorithm could make predictions more accurately than humans at times; and demonstrating that the visual cues people relied upon partly differ from those of the algorithm.



### Visual Search at Pinterest
- **Arxiv ID**: http://arxiv.org/abs/1505.07647v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.07647v3)
- **Published**: 2015-05-28 11:17:39+00:00
- **Updated**: 2017-03-08 05:02:00+00:00
- **Authors**: Yushi Jing, David Liu, Dmitry Kislyuk, Andrew Zhai, Jiajing Xu, Jeff Donahue, Sarah Tavel
- **Comment**: in Proceedings of the 21th ACM SIGKDD International Conference on
  Knowledge and Discovery and Data Mining, 2015
- **Journal**: None
- **Summary**: We demonstrate that, with the availability of distributed computation platforms such as Amazon Web Services and open-source tools, it is possible for a small engineering team to build, launch and maintain a cost-effective, large-scale visual search system with widely available tools. We also demonstrate, through a comprehensive set of live experiments at Pinterest, that content recommendation powered by visual search improve user engagement. By sharing our implementation details and the experiences learned from launching a commercial visual search engines from scratch, we hope visual search are more widely incorporated into today's commercial applications.



### A Generative Model of Natural Texture Surrogates
- **Arxiv ID**: http://arxiv.org/abs/1505.07672v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.07672v1)
- **Published**: 2015-05-28 12:37:15+00:00
- **Updated**: 2015-05-28 12:37:15+00:00
- **Authors**: Niklas Ludtke, Debapriya Das, Lucas Theis, Matthias Bethge
- **Comment**: 34 pages, 9 figures
- **Journal**: None
- **Summary**: Natural images can be viewed as patchworks of different textures, where the local image statistics is roughly stationary within a small neighborhood but otherwise varies from region to region. In order to model this variability, we first applied the parametric texture algorithm of Portilla and Simoncelli to image patches of 64X64 pixels in a large database of natural images such that each image patch is then described by 655 texture parameters which specify certain statistics, such as variances and covariances of wavelet coefficients or coefficient magnitudes within that patch.   To model the statistics of these texture parameters, we then developed suitable nonlinear transformations of the parameters that allowed us to fit their joint statistics with a multivariate Gaussian distribution. We find that the first 200 principal components contain more than 99% of the variance and are sufficient to generate textures that are perceptually extremely close to those generated with all 655 components. We demonstrate the usefulness of the model in several ways: (1) We sample ensembles of texture patches that can be directly compared to samples of patches from the natural image database and can to a high degree reproduce their perceptual appearance. (2) We further developed an image compression algorithm which generates surprisingly accurate images at bit rates as low as 0.14 bits/pixel. Finally, (3) We demonstrate how our approach can be used for an efficient and objective evaluation of samples generated with probabilistic models of natural images.



### Improved Deep Convolutional Neural Network For Online Handwritten Chinese Character Recognition using Domain-Specific Knowledge
- **Arxiv ID**: http://arxiv.org/abs/1505.07675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.07675v1)
- **Published**: 2015-05-28 12:43:22+00:00
- **Updated**: 2015-05-28 12:43:22+00:00
- **Authors**: Weixin Yang, Lianwen Jin, Zecheng Xie, Ziyong Feng
- **Comment**: 5 pages, 4 figures, 3 tables. Accepted to appear at ICDAR 2015
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) have achieved great success in various computer vision and pattern recognition applications, including those for handwritten Chinese character recognition (HCCR). However, most current DCNN-based HCCR approaches treat the handwritten sample simply as an image bitmap, ignoring some vital domain-specific information that may be useful but that cannot be learnt by traditional networks. In this paper, we propose an enhancement of the DCNN approach to online HCCR by incorporating a variety of domain-specific knowledge, including deformation, non-linear normalization, imaginary strokes, path signature, and 8-directional features. Our contribution is twofold. First, these domain-specific technologies are investigated and integrated with a DCNN to form a composite network to achieve improved performance. Second, the resulting DCNNs with diversity in their domain knowledge are combined using a hybrid serial-parallel (HSP) strategy. Consequently, we achieve a promising accuracy of 97.20% and 96.87% on CASIA-OLHWDB1.0 and CASIA-OLHWDB1.1, respectively, outperforming the best results previously reported in the literature.



### Invertible Orientation Scores of 3D Images
- **Arxiv ID**: http://arxiv.org/abs/1505.07690v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1505.07690v1)
- **Published**: 2015-05-28 13:52:41+00:00
- **Updated**: 2015-05-28 13:52:41+00:00
- **Authors**: Michiel Janssen, Remco Duits, Marcel Breeuwer
- **Comment**: ssvm 2015 published version in LNCS contains a mistake (a switch
  notation spherical angles) that is corrected in this arxiv version
- **Journal**: None
- **Summary**: The enhancement and detection of elongated structures in noisy image data is relevant for many biomedical applications. To handle complex crossing structures in 2D images, 2D orientation scores were introduced, which already showed their use in a variety of applications. Here we extend this work to 3D orientation scores. First, we construct the orientation score from a given dataset, which is achieved by an invertible coherent state type of transform. For this transformation we introduce 3D versions of the 2D cake-wavelets, which are complex wavelets that can simultaneously detect oriented structures and oriented edges. For efficient implementation of the different steps in the wavelet creation we use a spherical harmonic transform. Finally, we show some first results of practical applications of 3D orientation scores.



### Query by String word spotting based on character bi-gram indexing
- **Arxiv ID**: http://arxiv.org/abs/1505.07778v1
- **DOI**: 10.1109/ICDAR.2015.7333888
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.07778v1)
- **Published**: 2015-05-28 17:59:24+00:00
- **Updated**: 2015-05-28 17:59:24+00:00
- **Authors**: Suman K. Ghosh, Ernest Valveny
- **Comment**: To be published in ICDAR2015
- **Journal**: None
- **Summary**: In this paper we propose a segmentation-free query by string word spotting method. Both the documents and query strings are encoded using a recently proposed word representa- tion that projects images and strings into a common atribute space based on a pyramidal histogram of characters(PHOC). These attribute models are learned using linear SVMs over the Fisher Vector representation of the images along with the PHOC labels of the corresponding strings. In order to search through the whole page, document regions are indexed per character bi- gram using a similar attribute representation. On top of that, we propose an integral image representation of the document using a simplified version of the attribute model for efficient computation. Finally we introduce a re-ranking step in order to boost retrieval performance. We show state-of-the-art results for segmentation-free query by string word spotting in single-writer and multi-writer standard datasets



