# Arxiv Papers in cs.CV on 2015-05-07
### Webly Supervised Learning of Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1505.01554v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.01554v2)
- **Published**: 2015-05-07 00:56:15+00:00
- **Updated**: 2015-10-07 21:53:16+00:00
- **Authors**: Xinlei Chen, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach to utilize large amounts of web data for learning CNNs. Specifically inspired by curriculum learning, we present a two-step approach for CNN training. First, we use easy images to train an initial visual representation. We then use this initial CNN and adapt it to harder, more realistic images by leveraging the structure of data and categories. We demonstrate that our two-stage CNN outperforms a fine-tuned CNN trained on ImageNet on Pascal VOC 2012. We also demonstrate the strength of webly supervised learning by localizing objects in web images and training a R-CNN style detector. It achieves the best performance on VOC 2007 where no VOC training data is used. Finally, we show our approach is quite robust to noise and performs comparably even when we use image search results from March 2013 (pre-CNN image search era).



### Adaptive Nonparametric Image Parsing
- **Arxiv ID**: http://arxiv.org/abs/1505.01560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.01560v1)
- **Published**: 2015-05-07 02:28:32+00:00
- **Updated**: 2015-05-07 02:28:32+00:00
- **Authors**: Tam V. Nguyen, Canyi Lu, Jose Sepulveda, Shuicheng Yan
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: In this paper, we present an adaptive nonparametric solution to the image parsing task, namely annotating each image pixel with its corresponding category label. For a given test image, first, a locality-aware retrieval set is extracted from the training data based on super-pixel matching similarities, which are augmented with feature extraction for better differentiation of local super-pixels. Then, the category of each super-pixel is initialized by the majority vote of the $k$-nearest-neighbor super-pixels in the retrieval set. Instead of fixing $k$ as in traditional non-parametric approaches, here we propose a novel adaptive nonparametric approach which determines the sample-specific k for each test image. In particular, $k$ is adaptively set to be the number of the fewest nearest super-pixels which the images in the retrieval set can use to get the best category prediction. Finally, the initial super-pixel labels are further refined by contextual smoothing. Extensive experiments on challenging datasets demonstrate the superiority of the new solution over other state-of-the-art nonparametric solutions.



### Shadow Optimization from Structured Deep Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/1505.01589v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.01589v2)
- **Published**: 2015-05-07 05:07:11+00:00
- **Updated**: 2015-05-08 06:42:48+00:00
- **Authors**: Li Shen, Teck Wee Chua, Karianto Leman
- **Comment**: 8 pages. CVPR 2015
- **Journal**: None
- **Summary**: Local structures of shadow boundaries as well as complex interactions of image regions remain largely unexploited by previous shadow detection approaches. In this paper, we present a novel learning-based framework for shadow region recovery from a single image. We exploit the local structures of shadow edges by using a structured CNN learning framework. We show that using the structured label information in the classification can improve the local consistency of the results and avoid spurious labelling. We further propose and formulate a shadow/bright measure to model the complex interactions among image regions. The shadow and bright measures of each patch are computed from the shadow edges detected in the image. Using the global interaction constraints on patches, we formulate a least-square optimization problem for shadow recovery that can be solved efficiently. Our shadow recovery method achieves state-of-the-art results on the major shadow benchmark databases collected under various conditions.



### Learning to See by Moving
- **Arxiv ID**: http://arxiv.org/abs/1505.01596v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1505.01596v2)
- **Published**: 2015-05-07 06:03:01+00:00
- **Updated**: 2015-09-14 16:59:36+00:00
- **Authors**: Pulkit Agrawal, Joao Carreira, Jitendra Malik
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: The dominant paradigm for feature learning in computer vision relies on training neural networks for the task of object recognition using millions of hand labelled images. Is it possible to learn useful features for a diverse set of visual tasks using any other form of supervision? In biology, living organisms developed the ability of visual perception for the purpose of moving and acting in the world. Drawing inspiration from this observation, in this work we investigate if the awareness of egomotion can be used as a supervisory signal for feature learning. As opposed to the knowledge of class labels, information about egomotion is freely available to mobile agents. We show that given the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt using class-label as supervision on visual tasks of scene recognition, object recognition, visual odometry and keypoint matching.



### Filter characteristics in image decomposition with singular spectrum analysis
- **Arxiv ID**: http://arxiv.org/abs/1505.01599v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1505.01599v1)
- **Published**: 2015-05-07 06:21:15+00:00
- **Updated**: 2015-05-07 06:21:15+00:00
- **Authors**: Kenji Kume, Naoko Nose-Togawa
- **Comment**: None
- **Journal**: None
- **Summary**: Singular spectrum analysis is developed as a nonparametric spectral decomposition of a time series. It can be easily extended to the decomposition of multidimensional lattice-like data through the filtering interpretation. In this viewpoint, the singular spectrum analysis can be understood as the adaptive and optimal generation of the filters and their two-step point-symmetric operation to the original data. In this paper, we point out that, when applied to the multidimensional data, the adaptively generated filters exhibit symmetry properties resulting from the bisymmetric nature of the lag-covariance matrices. The eigenvectors of the lag-covariance matrix are either symmetric or antisymmetric, and for the 2D image data, these lead to the differential-type filters with even- or odd-order derivatives. The dominant filter is a smoothing filter, reflecting the dominance of low-frequency components of the photo images. The others are the edge-enhancement or the noise filters corresponding to the band-pass or the high-pass filters. The implication of the decomposition to the image denoising is briefly discussed.



### Data Fusion of Objects Using Techniques Such as Laser Scanning, Structured Light and Photogrammetry for Cultural Heritage Applications
- **Arxiv ID**: http://arxiv.org/abs/1505.01631v1
- **DOI**: 10.1007/978-3-319-15979-9_20
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.01631v1)
- **Published**: 2015-05-07 09:03:20+00:00
- **Updated**: 2015-05-07 09:03:20+00:00
- **Authors**: Citlalli Gamez Serna, Ruven Pillay, Alain Tremeau
- **Comment**: None
- **Journal**: Computational Color Imaging, Lecture Notes in Computer Science,
  Springer, 2015, pp. 208-224
- **Summary**: In this paper we present a semi-automatic 2D-3D local registration pipeline capable of coloring 3D models obtained from 3D scanners by using uncalibrated images. The proposed pipeline exploits the Structure from Motion (SfM) technique in order to reconstruct a sparse representation of the 3D object and obtain the camera parameters from image feature matches. We then coarsely register the reconstructed 3D model to the scanned one through the Scale Iterative Closest Point (SICP) algorithm. SICP provides the global scale, rotation and translation parameters, using minimal manual user intervention. In the final processing stage, a local registration refinement algorithm optimizes the color projection of the aligned photos on the 3D object removing the blurring/ghosting artefacts introduced due to small inaccuracies during the registration. The proposed pipeline is capable of handling real world cases with a range of characteristics from objects with low level geometric features to complex ones.



### Integrating K-means with Quadratic Programming Feature Selection
- **Arxiv ID**: http://arxiv.org/abs/1505.01728v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1505.01728v2)
- **Published**: 2015-05-07 14:45:11+00:00
- **Updated**: 2015-08-11 18:06:36+00:00
- **Authors**: Yamuna Prasad, K. K. Biswas
- **Comment**: 17 pages, 11 figures
- **Journal**: None
- **Summary**: Several data mining problems are characterized by data in high dimensions. One of the popular ways to reduce the dimensionality of the data is to perform feature selection, i.e, select a subset of relevant and non-redundant features. Recently, Quadratic Programming Feature Selection (QPFS) has been proposed which formulates the feature selection problem as a quadratic program. It has been shown to outperform many of the existing feature selection methods for a variety of applications. Though, better than many existing approaches, the running time complexity of QPFS is cubic in the number of features, which can be quite computationally expensive even for moderately sized datasets. In this paper we propose a novel method for feature selection by integrating k-means clustering with QPFS. The basic variant of our approach runs k-means to bring down the number of features which need to be passed on to QPFS. We then enhance this idea, wherein we gradually refine the feature space from a very coarse clustering to a fine-grained one, by interleaving steps of QPFS with k-means clustering. Every step of QPFS helps in identifying the clusters of irrelevant features (which can then be thrown away), whereas every step of k-means further refines the clusters which are potentially relevant. We show that our iterative refinement of clusters is guaranteed to converge. We provide bounds on the number of distance computations involved in the k-means algorithm. Further, each QPFS run is now cubic in number of clusters, which can be much smaller than actual number of features. Experiments on eight publicly available datasets show that our approach gives significant computational gains (both in time and memory), over standard QPFS as well as other state of the art feature selection methods, even while improving the overall accuracy.



### Fast Spectral Unmixing based on Dykstra's Alternating Projection
- **Arxiv ID**: http://arxiv.org/abs/1505.01740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.01740v1)
- **Published**: 2015-05-07 15:22:33+00:00
- **Updated**: 2015-05-07 15:22:33+00:00
- **Authors**: Qi Wei, Jose Bioucas-Dias, Nicolas Dobigeon, Jean-Yves Tourneret
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: This paper presents a fast spectral unmixing algorithm based on Dykstra's alternating projection. The proposed algorithm formulates the fully constrained least squares optimization problem associated with the spectral unmixing task as an unconstrained regression problem followed by a projection onto the intersection of several closed convex sets. This projection is achieved by iteratively projecting onto each of the convex sets individually, following Dyktra's scheme. The sequence thus obtained is guaranteed to converge to the sought projection. Thanks to the preliminary matrix decomposition and variable substitution, the projection is implemented intrinsically in a subspace, whose dimension is very often much lower than the number of bands. A benefit of this strategy is that the order of the computational complexity for each projection is decreased from quadratic to linear time. Numerical experiments considering diverse spectral unmixing scenarios provide evidence that the proposed algorithm competes with the state-of-the-art, namely when the number of endmembers is relatively small, a circumstance often observed in real hyperspectral applications.



### Object detection via a multi-region & semantic segmentation-aware CNN model
- **Arxiv ID**: http://arxiv.org/abs/1505.01749v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1505.01749v3)
- **Published**: 2015-05-07 15:42:07+00:00
- **Updated**: 2015-09-23 22:24:42+00:00
- **Authors**: Spyros Gidaris, Nikos Komodakis
- **Comment**: Extended technical report -- short version to appear at ICCV 2015
- **Journal**: None
- **Summary**: We propose an object detection system that relies on a multi-region deep convolutional neural network (CNN) that also encodes semantic segmentation-aware features. The resulting CNN-based representation aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. We exploit the above properties of our recognition module by integrating it on an iterative localization mechanism that alternates between scoring a box proposal and refining its location with a deep CNN regression model. Thanks to the efficient use of our modules, we detect objects with very high localization accuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we achieve mAP of 78.2% and 73.9% correspondingly, surpassing any other published work by a significant margin.



### Language Models for Image Captioning: The Quirks and What Works
- **Arxiv ID**: http://arxiv.org/abs/1505.01809v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1505.01809v3)
- **Published**: 2015-05-07 18:36:14+00:00
- **Updated**: 2015-10-14 22:03:40+00:00
- **Authors**: Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell
- **Comment**: See http://research.microsoft.com/en-us/projects/image_captioning for
  project information
- **Journal**: None
- **Summary**: Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.



### Jointly Modeling Embedding and Translation to Bridge Video and Language
- **Arxiv ID**: http://arxiv.org/abs/1505.01861v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1505.01861v3)
- **Published**: 2015-05-07 20:13:33+00:00
- **Updated**: 2015-06-04 07:17:06+00:00
- **Authors**: Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, Yong Rui
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically describing video content with natural language is a fundamental challenge of multimedia. Recurrent Neural Networks (RNN), which models sequence dynamics, has attracted increasing attention on visual interpretation. However, most existing approaches generate a word locally with given previous words and the visual content, while the relationship between sentence semantics and visual content is not holistically exploited. As a result, the generated sentences may be contextually correct but the semantics (e.g., subjects, verbs or objects) are not true.   This paper presents a novel unified framework, named Long Short-Term Memory with visual-semantic Embedding (LSTM-E), which can simultaneously explore the learning of LSTM and visual-semantic embedding. The former aims to locally maximize the probability of generating the next word given previous words and visual content, while the latter is to create a visual-semantic embedding space for enforcing the relationship between the semantics of the entire sentence and visual content. Our proposed LSTM-E consists of three components: a 2-D and/or 3-D deep convolutional neural networks for learning powerful video representation, a deep RNN for generating sentences, and a joint embedding model for exploring the relationships between visual content and sentence semantics. The experiments on YouTube2Text dataset show that our proposed LSTM-E achieves to-date the best reported performance in generating natural sentences: 45.3% and 31.0% in terms of BLEU@4 and METEOR, respectively. We also demonstrate that LSTM-E is superior in predicting Subject-Verb-Object (SVO) triplets to several state-of-the-art techniques.



