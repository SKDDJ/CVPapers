# Arxiv Papers in cs.CV on 2015-05-20
### Image aesthetic evaluation using paralleled deep convolution neural network
- **Arxiv ID**: http://arxiv.org/abs/1505.05225v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, 68U10, I.4.7
- **Links**: [PDF](http://arxiv.org/pdf/1505.05225v1)
- **Published**: 2015-05-20 02:03:23+00:00
- **Updated**: 2015-05-20 02:03:23+00:00
- **Authors**: Guo Lihua, Li Fudi
- **Comment**: 7 pages, 6 figures, 9 tables
- **Journal**: None
- **Summary**: Image aesthetic evaluation has attracted much attention in recent years. Image aesthetic evaluation methods heavily depend on the effective aesthetic feature. Traditional meth-ods always extract hand-crafted features. However, these hand-crafted features are always designed to adapt particu-lar datasets, and extraction of them needs special design. Rather than extracting hand-crafted features, an automati-cally learn of aesthetic features based on deep convolutional neural network (DCNN) is first adopt in this paper. As we all know, when the training dataset is given, the DCNN architecture with high complexity may meet the over-fitting problem. On the other side, the DCNN architecture with low complexity would not efficiently extract effective features. For these reasons, we further propose a paralleled convolutional neural network (PDCNN) with multi-level structures to automatically adapt to the training dataset. Experimental results show that our proposed PDCNN architecture achieves better performance than other traditional methods.



### Multi-scale recognition with DAG-CNNs
- **Arxiv ID**: http://arxiv.org/abs/1505.05232v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05232v1)
- **Published**: 2015-05-20 02:52:07+00:00
- **Updated**: 2015-05-20 02:52:07+00:00
- **Authors**: Songfan Yang, Deva Ramanan
- **Comment**: None
- **Journal**: None
- **Summary**: We explore multi-scale convolutional neural nets (CNNs) for image classification. Contemporary approaches extract features from a single output layer. By extracting features from multiple layers, one can simultaneously reason about high, mid, and low-level features during classification. The resulting multi-scale architecture can itself be seen as a feed-forward model that is structured as a directed acyclic graph (DAG-CNNs). We use DAG-CNNs to learn a set of multiscale features that can be effectively shared between coarse and fine-grained classification tasks. While fine-tuning such models helps performance, we show that even "off-the-self" multiscale features perform quite well. We present extensive analysis and demonstrate state-of-the-art classification performance on three standard scene benchmarks (SUN397, MIT67, and Scene15). In terms of the heavily benchmarked MIT67 and Scene15 datasets, our results reduce the lowest previously-reported error by 23.9% and 9.5%, respectively.



### Visual Understanding via Multi-Feature Shared Learning with Global Consistency
- **Arxiv ID**: http://arxiv.org/abs/1505.05233v2
- **DOI**: 10.1109/TMM.2015.2510509
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1505.05233v2)
- **Published**: 2015-05-20 03:01:08+00:00
- **Updated**: 2015-09-09 10:07:11+00:00
- **Authors**: Lei Zhang, David Zhang
- **Comment**: 13 pages,6 figures, this paper is accepted for publication in IEEE
  Transactions on Multimedia
- **Journal**: None
- **Summary**: Image/video data is usually represented with multiple visual features. Fusion of multi-source information for establishing the attributes has been widely recognized. Multi-feature visual recognition has recently received much attention in multimedia applications. This paper studies visual understanding via a newly proposed l_2-norm based multi-feature shared learning framework, which can simultaneously learn a global label matrix and multiple sub-classifiers with the labeled multi-feature data. Additionally, a group graph manifold regularizer composed of the Laplacian and Hessian graph is proposed for better preserving the manifold structure of each feature, such that the label prediction power is much improved through the semi-supervised learning with global label consistency. For convenience, we call the proposed approach Global-Label-Consistent Classifier (GLCC). The merits of the proposed method include: 1) the manifold structure information of each feature is exploited in learning, resulting in a more faithful classification owing to the global label consistency; 2) a group graph manifold regularizer based on the Laplacian and Hessian regularization is constructed; 3) an efficient alternative optimization method is introduced as a fast solver owing to the convex sub-problems. Experiments on several benchmark visual datasets for multimedia understanding, such as the 17-category Oxford Flower dataset, the challenging 101-category Caltech dataset, the YouTube & Consumer Videos dataset and the large-scale NUS-WIDE dataset, demonstrate that the proposed approach compares favorably with the state-of-the-art algorithms. An extensive experiment on the deep convolutional activation features also show the effectiveness of the proposed approach. The code is available on http://www.escience.cn/people/lei/index.html



### Benchmarking KAZE and MCM for Multiclass Classification
- **Arxiv ID**: http://arxiv.org/abs/1505.05240v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, I.4.7; I.5.4; I.4.8; I.4.9; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/1505.05240v1)
- **Published**: 2015-05-20 04:09:47+00:00
- **Updated**: 2015-05-20 04:09:47+00:00
- **Authors**: Siddharth Srivastava, Prerana Mukherjee, Brejesh Lall
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach for feature generation by appropriately fusing KAZE and SIFT features. We then use this feature set along with Minimal Complexity Machine(MCM) for object classification. We show that KAZE and SIFT features are complementary. Experimental results indicate that an elementary integration of these techniques can outperform the state-of-the-art approaches.



### Live Video Synopsis for Multiple Cameras
- **Arxiv ID**: http://arxiv.org/abs/1505.05254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05254v1)
- **Published**: 2015-05-20 06:03:48+00:00
- **Updated**: 2015-05-20 06:03:48+00:00
- **Authors**: Yedid Hoshen, Shmuel Peleg
- **Comment**: To be presented in ICIP 2015
- **Journal**: Proc. ICIP'15, Quebec City, Sept. 2015, pp. 212-216
- **Summary**: Video surveillance cameras generate most of recorded video, and there is far more recorded video than operators can watch. Much progress has recently been made using summarization of recorded video, but such techniques do not have much impact on live video surveillance.   We assume a camera hierarchy where a Master camera observes the decision-critical region, and one or more Slave cameras observe regions where past activity is important for making the current decision. We propose that when people appear in the live Master camera, the Slave cameras will display their past activities, and the operator could use past information for real-time decision making.   The basic units of our method are action tubes, representing objects and their trajectories over time. Our object-based method has advantages over frame based methods, as it can handle multiple people, multiple activities for each person, and can address re-identification uncertainty.



### Measuring Visibility using Atmospheric Transmission and Digital Surface Model
- **Arxiv ID**: http://arxiv.org/abs/1505.05286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05286v1)
- **Published**: 2015-05-20 09:02:32+00:00
- **Updated**: 2015-05-20 09:02:32+00:00
- **Authors**: Jean-Philippe Andreu, Stefan Mayer, Karlheinz Gutjahr, Harald Ganster
- **Comment**: Presented at OAGM Workshop, 2015 (arXiv:1505.01065)
- **Journal**: None
- **Summary**: Reliable and exact assessment of visibility is essential for safe air traffic. In order to overcome the drawbacks of the currently subjective reports from human observers, we present an approach to automatically derive visibility measures by means of image processing. It first exploits image based estimation of the atmospheric transmission describing the portion of the light that is not scattered by atmospheric phenomena (e.g., haze, fog, smoke) and reaches the camera. Once the atmospheric transmission is estimated, a 3D representation of the vicinity (digital surface model: DMS) is used to compute depth measurements for the haze-free pixels and then derive a global visibility estimation for the airport. Results on foggy images demonstrate the validity of the proposed method.



### Algorithmic Analysis of Edge Ranking and Profiling for MTF Determination of an Imaging System
- **Arxiv ID**: http://arxiv.org/abs/1505.05338v1
- **DOI**: 10.14445/22312803/IJCTT-V23P110
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05338v1)
- **Published**: 2015-05-20 12:12:48+00:00
- **Updated**: 2015-05-20 12:12:48+00:00
- **Authors**: Poorna Banerjee Dasgupta
- **Comment**: 3 pages, Published with International Journal of Computer Trends and
  Technology (IJCTT), Volume-23 Number-1, 2015
- **Journal**: International Journal of Computer Trends and Technology (IJCTT)
  V23(1):46-48, May 2015
- **Summary**: Edge detection is one of the most principal techniques for detecting discontinuities in the gray levels of image pixels. The Modulation Transfer Function (MTF) is one of the main criteria for assessing imaging quality and is a parameter frequently used for measuring the sharpness of an imaging system. In order to determine the MTF, it is essential to determine the best edge from the target image so that an edge profile can be developed and then the line spread function and hence the MTF, can be computed accordingly. For regular image sizes, the human visual system is adept enough to identify suitable edges from the image. But considering huge image datasets, such as those obtained from satellites, the image size may range in few gigabytes and in such a case, manual inspection of images for determination of the best suitable edge is not plausible and hence, edge profiling tasks have to be automated. This paper presents a novel, yet simple, algorithm for edge ranking and detection from image data-sets for MTF computation, which is ideal for automation on vectorised graphical processing units.



### DropSample: A New Training Method to Enhance Deep Convolutional Neural Networks for Large-Scale Unconstrained Handwritten Chinese Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1505.05354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05354v1)
- **Published**: 2015-05-20 13:08:57+00:00
- **Updated**: 2015-05-20 13:08:57+00:00
- **Authors**: Weixin Yang, Lianwen Jin, Dacheng Tao, Zecheng Xie, Ziyong Feng
- **Comment**: 18 pages, 8 figures, 5 tables
- **Journal**: None
- **Summary**: Inspired by the theory of Leitners learning box from the field of psychology, we propose DropSample, a new method for training deep convolutional neural networks (DCNNs), and apply it to large-scale online handwritten Chinese character recognition (HCCR). According to the principle of DropSample, each training sample is associated with a quota function that is dynamically adjusted on the basis of the classification confidence given by the DCNN softmax output. After a learning iteration, samples with low confidence will have a higher probability of being selected as training data in the next iteration; in contrast, well-trained and well-recognized samples with very high confidence will have a lower probability of being involved in the next training iteration and can be gradually eliminated. As a result, the learning process becomes more efficient as it progresses. Furthermore, we investigate the use of domain-specific knowledge to enhance the performance of DCNN by adding a domain knowledge layer before the traditional CNN. By adopting DropSample together with different types of domain-specific knowledge, the accuracy of HCCR can be improved efficiently. Experiments on the CASIA-OLHDWB 1.0, CASIA-OLHWDB 1.1, and ICDAR 2013 online HCCR competition datasets yield outstanding recognition rates of 97.33%, 97.06%, and 97.51% respectively, all of which are significantly better than the previous best results reported in the literature.



### Kinect Range Sensing: Structured-Light versus Time-of-Flight Kinect
- **Arxiv ID**: http://arxiv.org/abs/1505.05459v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05459v1)
- **Published**: 2015-05-20 17:33:13+00:00
- **Updated**: 2015-05-20 17:33:13+00:00
- **Authors**: Hamed Sarbolandi, Damien Lefloch, Andreas Kolb
- **Comment**: 58 pages, 23 figures. Accepted for publication in Computer Vision and
  Image Understanding (CVIU)
- **Journal**: None
- **Summary**: Recently, the new Kinect One has been issued by Microsoft, providing the next generation of real-time range sensing devices based on the Time-of-Flight (ToF) principle. As the first Kinect version was using a structured light approach, one would expect various differences in the characteristics of the range data delivered by both devices. This paper presents a detailed and in-depth comparison between both devices. In order to conduct the comparison, we propose a framework of seven different experimental setups, which is a generic basis for evaluating range cameras such as Kinect. The experiments have been designed with the goal to capture individual effects of the Kinect devices as isolatedly as possible and in a way, that they can also be adopted, in order to apply them to any other range sensing device. The overall goal of this paper is to provide a solid insight into the pros and cons of either device. Thus, scientists that are interested in using Kinect range sensing cameras in their specific application scenario can directly assess the expected, specific benefits and potential problem of either device.



### A Sparse Gaussian Process Framework for Photometric Redshift Estimation
- **Arxiv ID**: http://arxiv.org/abs/1505.05489v3
- **DOI**: 10.1093/mnras/stv2425
- **Categories**: **astro-ph.IM**, astro-ph.GA, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1505.05489v3)
- **Published**: 2015-05-20 19:08:54+00:00
- **Updated**: 2015-10-19 16:43:19+00:00
- **Authors**: Ibrahim A. Almosallam, Sam N. Lindsay, Matt J. Jarvis, Stephen J. Roberts
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate photometric redshifts are a lynchpin for many future experiments to pin down the cosmological model and for studies of galaxy evolution. In this study, a novel sparse regression framework for photometric redshift estimation is presented. Simulated and real data from SDSS DR12 were used to train and test the proposed models. We show that approaches which include careful data preparation and model design offer a significant improvement in comparison with several competing machine learning algorithms. Standard implementations of most regression algorithms have as the objective the minimization of the sum of squared errors. For redshift inference, however, this induces a bias in the posterior mean of the output distribution, which can be problematic. In this paper we directly target minimizing $\Delta z = (z_\textrm{s} - z_\textrm{p})/(1+z_\textrm{s})$ and address the bias problem via a distribution-based weighting scheme, incorporated as part of the optimization objective. The results are compared with other machine learning algorithms in the field such as Artificial Neural Networks (ANN), Gaussian Processes (GPs) and sparse GPs. The proposed framework reaches a mean absolute $\Delta z = 0.0026(1+z_\textrm{s})$, over the redshift range of $0 \le z_\textrm{s} \le 2$ on the simulated data, and $\Delta z = 0.0178(1+z_\textrm{s})$ over the entire redshift range on the SDSS DR12 survey, outperforming the standard ANNz used in the literature. We also investigate how the relative size of the training set affects the photometric redshift accuracy. We find that a training set of \textgreater 30 per cent of total sample size, provides little additional constraint on the photometric redshifts, and note that our GP formalism strongly outperforms ANNz in the sparse data regime for the simulated data set.



