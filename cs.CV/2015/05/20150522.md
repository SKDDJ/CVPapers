# Arxiv Papers in cs.CV on 2015-05-22
### Joint Inference of Groups, Events and Human Roles in Aerial Videos
- **Arxiv ID**: http://arxiv.org/abs/1505.05957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05957v1)
- **Published**: 2015-05-22 05:59:18+00:00
- **Updated**: 2015-05-22 05:59:18+00:00
- **Authors**: Tianmin Shu, Dan Xie, Brandon Rothrock, Sinisa Todorovic, Song-Chun Zhu
- **Comment**: CVPR 2015 Oral Presentation
- **Journal**: None
- **Summary**: With the advent of drones, aerial video analysis becomes increasingly important; yet, it has received scant attention in the literature. This paper addresses a new problem of parsing low-resolution aerial videos of large spatial areas, in terms of 1) grouping, 2) recognizing events and 3) assigning roles to people engaged in events. We propose a novel framework aimed at conducting joint inference of the above tasks, as reasoning about each in isolation typically fails in our setting. Given noisy tracklets of people and detections of large objects and scene surfaces (e.g., building, grass), we use a spatiotemporal AND-OR graph to drive our joint inference, using Markov Chain Monte Carlo and dynamic programming. We also introduce a new formalism of spatiotemporal templates characterizing latent sub-events. For evaluation, we have collected and released a new aerial videos dataset using a hex-rotor flying over picnic areas rich with group events. Our results demonstrate that we successfully address above inference tasks under challenging conditions.



### Weakly-Supervised Alignment of Video With Text
- **Arxiv ID**: http://arxiv.org/abs/1505.06027v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1505.06027v2)
- **Published**: 2015-05-22 11:08:39+00:00
- **Updated**: 2015-12-21 14:57:40+00:00
- **Authors**: Piotr Bojanowski, Rémi Lajugie, Edouard Grave, Francis Bach, Ivan Laptev, Jean Ponce, Cordelia Schmid
- **Comment**: ICCV 2015 - IEEE International Conference on Computer Vision, Dec
  2015, Santiago, Chile
- **Journal**: None
- **Summary**: Suppose that we are given a set of videos, along with natural language descriptions in the form of multiple sentences (e.g., manual annotations, movie scripts, sport summaries etc.), and that these sentences appear in the same temporal order as their visual counterparts. We propose in this paper a method for aligning the two modalities, i.e., automatically providing a time stamp for every sentence. Given vectorial features for both video and text, we propose to cast this task as a temporal assignment problem, with an implicit linear mapping between the two feature modalities. We formulate this problem as an integer quadratic program, and solve its continuous convex relaxation using an efficient conditional gradient algorithm. Several rounding procedures are proposed to construct the final integer solution. After demonstrating significant improvements over the state of the art on the related task of aligning video with symbolic labels [7], we evaluate our method on a challenging dataset of videos with associated textual descriptions [36], using both bag-of-words and continuous representations for text.



### Diffusion Methods for Classification with Pairwise Relationships
- **Arxiv ID**: http://arxiv.org/abs/1505.06072v4
- **DOI**: None
- **Categories**: **cs.DS**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1505.06072v4)
- **Published**: 2015-05-22 13:36:58+00:00
- **Updated**: 2019-05-14 18:10:26+00:00
- **Authors**: Pedro F. Felzenszwalb, Benar F. Svaiter
- **Comment**: To appear in the Quarterly of Applied Mathematics
- **Journal**: None
- **Summary**: We define two algorithms for propagating information in classification problems with pairwise relationships. The algorithms are based on contraction maps and are related to non-linear diffusion and random walks on graphs. The approach is also related to message passing algorithms, including belief propagation and mean field methods. The algorithms we describe are guaranteed to converge on graphs with arbitrary topology. Moreover they always converge to a unique fixed point, independent of initialization. We prove that the fixed points of the algorithms under consideration define lower-bounds on the energy function and the max-marginals of a Markov random field. The theoretical results also illustrate a relationship between message passing algorithms and value iteration for an infinite horizon Markov decision process. We illustrate the practical application of the algorithms under study with numerical experiments in image restoration, stereo depth estimation and binary classification on a grid.



### Robust Rotation Synchronization via Low-rank and Sparse Matrix Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1505.06079v2
- **DOI**: 10.1016/j.cviu.2018.08.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.06079v2)
- **Published**: 2015-05-22 13:48:10+00:00
- **Updated**: 2017-07-12 13:58:54+00:00
- **Authors**: Federica Arrigoni, Andrea Fusiello, Beatrice Rossi, Pasqualina Fragneto
- **Comment**: The material contained in this paper is part of a manuscript
  submitted to CVIU
- **Journal**: In Computer Vision and Image Understanding, 174: 95-113, 2018
- **Summary**: This paper deals with the rotation synchronization problem, which arises in global registration of 3D point-sets and in structure from motion. The problem is formulated in an unprecedented way as a "low-rank and sparse" matrix decomposition that handles both outliers and missing data. A minimization strategy, dubbed R-GoDec, is also proposed and evaluated experimentally against state-of-the-art algorithms on simulated and real data. The results show that R-GoDec is the fastest among the robust algorithms.



### Design and Implementation of Real-time Algorithms for Eye Tracking and PERCLOS Measurement for on board Estimation of Alertness of Drivers
- **Arxiv ID**: http://arxiv.org/abs/1505.06162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.06162v1)
- **Published**: 2015-05-22 17:51:53+00:00
- **Updated**: 2015-05-22 17:51:53+00:00
- **Authors**: Anjith George, Aurobinda Routray
- **Comment**: Thesis
- **Journal**: None
- **Summary**: The alertness level of drivers can be estimated with the use of computer vision based methods. The level of fatigue can be found from the value of PERCLOS. It is the ratio of closed eye frames to the total frames processed. The main objective of the thesis is the design and implementation of real-time algorithms for measurement of PERCLOS. In this work we have developed a real-time system which is able to process the video onboard and to alarm the driver in case the driver is in alert. For accurate estimation of PERCLOS the frame rate should be greater than 4 and accuracy should be greater than 90%. For eye detection we have used mainly two approaches Haar classifier based method and Principal Component Analysis (PCA) based method for day time. During night time active Near Infra Red (NIR) illumination is used. Local Binary Pattern (LBP) histogram based method is used for the detection of eyes at night time. The accuracy rate of the algorithms was found to be more than 90% at frame rates more than 5 fps which was suitable for the application.



### Direct Variational Perspective Shape from Shading with Cartesian Depth Parametrisation
- **Arxiv ID**: http://arxiv.org/abs/1505.06163v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.06163v2)
- **Published**: 2015-05-22 17:52:11+00:00
- **Updated**: 2015-07-13 11:03:25+00:00
- **Authors**: Yong Chul Ju, Daniel Maurer, Michael Breuß, Andrés Bruhn
- **Comment**: None
- **Journal**: None
- **Summary**: Most of today's state-of-the-art methods for perspective shape from shading are modelled in terms of partial differential equations (PDEs) of Hamilton-Jacobi type. To improve the robustness of such methods w.r.t. noise and missing data, first approaches have recently been proposed that seek to embed the underlying PDE into a variational framework with data and smoothness term. So far, however, such methods either make use of a radial depth parametrisation that makes the regularisation hard to interpret from a geometrical viewpoint or they consider indirect smoothness terms that require additional consistency constraints to provide valid solutions. Moreover the minimisation of such frameworks is an intricate task, since the underlying energy is typically non-convex. In our paper we address all three of the aforementioned issues. First, we propose a novel variational model that operates directly on the Cartesian depth. In this context, we also point out a common mistake in the derivation of the surface normal. Moreover, we employ a direct second-order regulariser with edge-preservation property. This direct regulariser yields by construction valid solutions without requiring additional consistency constraints. Finally, we also propose a novel coarse-to-fine minimisation framework based on an alternating explicit scheme. This framework allows us to avoid local minima during the minimisation and thus to improve the accuracy of the reconstruction. Experiments show the good quality of our model as well as the usefulness of the proposed numerical scheme.



### A Bottom-up Approach for Pancreas Segmentation using Cascaded Superpixels and (Deep) Image Patch Labeling
- **Arxiv ID**: http://arxiv.org/abs/1505.06236v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.06236v2)
- **Published**: 2015-05-22 21:59:45+00:00
- **Updated**: 2016-03-07 18:24:43+00:00
- **Authors**: Amal Farag, Le Lu, Holger R. Roth, Jiamin Liu, Evrim Turkbey, Ronald M. Summers
- **Comment**: 14 pages, 14 figures, 2 tables
- **Journal**: None
- **Summary**: Robust automated organ segmentation is a prerequisite for computer-aided diagnosis (CAD), quantitative imaging analysis and surgical assistance. For high-variability organs such as the pancreas, previous approaches report undesirably low accuracies. We present a bottom-up approach for pancreas segmentation in abdominal CT scans that is based on a hierarchy of information propagation by classifying image patches at different resolutions; and cascading superpixels. There are four stages: 1) decomposing CT slice images as a set of disjoint boundary-preserving superpixels; 2) computing pancreas class probability maps via dense patch labeling; 3) classifying superpixels by pooling both intensity and probability features to form empirical statistics in cascaded random forest frameworks; and 4) simple connectivity based post-processing. The dense image patch labeling are conducted by: efficient random forest classifier on image histogram, location and texture features; and more expensive (but with better specificity) deep convolutional neural network classification on larger image windows (with more spatial contexts). Evaluation of the approach is performed on a database of 80 manually segmented CT volumes in six-fold cross-validation (CV). Our achieved results are comparable, or better than the state-of-the-art methods (evaluated by "leave-one-patient-out"), with Dice 70.7% and Jaccard 57.9%. The computational efficiency has been drastically improved in the order of 6~8 minutes, comparing with others of ~10 hours per case. Finally, we implement a multi-atlas label fusion (MALF) approach for pancreas segmentation using the same datasets. Under six-fold CV, our bottom-up segmentation method significantly outperforms its MALF counterpart: (70.7 +/- 13.0%) versus (52.5 +/- 20.8%) in Dice. Deep CNN patch labeling confidences offer more numerical stability, reflected by smaller standard deviations.



### Tunnel Surface 3D Reconstruction from Unoriented Image Sequences
- **Arxiv ID**: http://arxiv.org/abs/1505.06237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.06237v1)
- **Published**: 2015-05-22 22:00:55+00:00
- **Updated**: 2015-05-22 22:00:55+00:00
- **Authors**: Arnold Bauer, Karlheinz Gutjahr, Gerhard Paar, Heiner Kontrus, Robert Glatzl
- **Comment**: Presented at OAGM Workshop, 2015 (arXiv:1505.01065)
- **Journal**: None
- **Summary**: The 3D documentation of the tunnel surface during construction requires fast and robust measurement systems. In the solution proposed in this paper, during tunnel advance a single camera is taking pictures of the tunnel surface from several positions. The recorded images are automatically processed to gain a 3D tunnel surface model. Image acquisition is realized by the tunneling/advance/driving personnel close to the tunnel face (= the front end of the advance). Based on the following fully automatic analysis/evaluation, a decision on the quality of the outbreak can be made within a few minutes. This paper describes the image recording system and conditions as well as the stereo-photogrammetry based workflow for the continuously merged dense 3D reconstruction of the entire advance region. Geo-reference is realized by means of signalized targets that are automatically detected in the images. We report on the results of recent testing under real construction conditions, and conclude with prospects for further development in terms of on-site performance.



### Efficient Large Scale Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1505.06250v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1505.06250v1)
- **Published**: 2015-05-22 23:45:32+00:00
- **Updated**: 2015-05-22 23:45:32+00:00
- **Authors**: Balakrishnan Varadarajan, George Toderici, Sudheendra Vijayanarasimhan, Apostol Natsev
- **Comment**: None
- **Journal**: None
- **Summary**: Video classification has advanced tremendously over the recent years. A large part of the improvements in video classification had to do with the work done by the image classification community and the use of deep convolutional networks (CNNs) which produce competitive results with hand- crafted motion features. These networks were adapted to use video frames in various ways and have yielded state of the art classification results. We present two methods that build on this work, and scale it up to work with millions of videos and hundreds of thousands of classes while maintaining a low computational cost. In the context of large scale video processing, training CNNs on video frames is extremely time consuming, due to the large number of frames involved. We propose to avoid this problem by training CNNs on either YouTube thumbnails or Flickr images, and then using these networks' outputs as features for other higher level classifiers. We discuss the challenges of achieving this and propose two models for frame-level and video-level classification. The first is a highly efficient mixture of experts while the latter is based on long short term memory neural networks. We present results on the Sports-1M video dataset (1 million videos, 487 classes) and on a new dataset which has 12 million videos and 150,000 labels.



