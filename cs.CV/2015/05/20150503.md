# Arxiv Papers in cs.CV on 2015-05-03
### Detail-preserving and Content-aware Variational Multi-view Stereo Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1505.00389v1
- **DOI**: 10.1109/TIP.2015.2507400
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.00389v1)
- **Published**: 2015-05-03 03:03:49+00:00
- **Updated**: 2015-05-03 03:03:49+00:00
- **Authors**: Zhaoxin Li, Kuanquan Wang, Wangmeng Zuo, Deyu Meng, Lei Zhang
- **Comment**: 14 pages,16 figures. Submitted to IEEE Transaction on image
  processing
- **Journal**: None
- **Summary**: Accurate recovery of 3D geometrical surfaces from calibrated 2D multi-view images is a fundamental yet active research area in computer vision. Despite the steady progress in multi-view stereo reconstruction, most existing methods are still limited in recovering fine-scale details and sharp features while suppressing noises, and may fail in reconstructing regions with few textures. To address these limitations, this paper presents a Detail-preserving and Content-aware Variational (DCV) multi-view stereo method, which reconstructs the 3D surface by alternating between reprojection error minimization and mesh denoising. In reprojection error minimization, we propose a novel inter-image similarity measure, which is effective to preserve fine-scale details of the reconstructed surface and builds a connection between guided image filtering and image registration. In mesh denoising, we propose a content-aware $\ell_{p}$-minimization algorithm by adaptively estimating the $p$ value and regularization parameters based on the current input. It is much more promising in suppressing noise while preserving sharp features than conventional isotropic mesh smoothing. Experimental results on benchmark datasets demonstrate that our DCV method is capable of recovering more surface details, and obtains cleaner and more accurate reconstructions than state-of-the-art methods. In particular, our method achieves the best results among all published methods on the Middlebury dino ring and dino sparse ring datasets in terms of both completeness and accuracy.



### ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1505.00393v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.00393v3)
- **Published**: 2015-05-03 04:58:53+00:00
- **Updated**: 2015-07-23 17:11:04+00:00
- **Authors**: Francesco Visin, Kyle Kastner, Kyunghyun Cho, Matteo Matteucci, Aaron Courville, Yoshua Bengio
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a deep neural network architecture for object recognition based on recurrent neural networks. The proposed network, called ReNet, replaces the ubiquitous convolution+pooling layer of the deep convolutional neural network with four recurrent neural networks that sweep horizontally and vertically in both directions across the image. We evaluate the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and SVHN. The result suggests that ReNet is a viable alternative to the deep convolutional neural network, and that further investigation is needed.



### On a fast bilateral filtering formulation using functional rearrangements
- **Arxiv ID**: http://arxiv.org/abs/1505.00412v1
- **DOI**: 10.1007/s10851-015-0583-y
- **Categories**: **cs.CV**, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1505.00412v1)
- **Published**: 2015-05-03 09:50:33+00:00
- **Updated**: 2015-05-03 09:50:33+00:00
- **Authors**: Gonzalo Galiano, Julián Velasco
- **Comment**: 29 pages, Journal of Mathematical Imaging and Vision, 2015. arXiv
  admin note: substantial text overlap with arXiv:1406.7128
- **Journal**: None
- **Summary**: We introduce an exact reformulation of a broad class of neighborhood filters, among which the bilateral filters, in terms of two functional rearrangements: the decreasing and the relative rearrangements.   Independently of the image spatial dimension (one-dimensional signal, image, volume of images, etc.), we reformulate these filters as integral operators defined in a one-dimensional space corresponding to the level sets measures.   We prove the equivalence between the usual pixel-based version and the rearranged version of the filter. When restricted to the discrete setting, our reformulation of bilateral filters extends previous results for the so-called fast bilateral filtering. We, in addition, prove that the solution of the discrete setting, understood as constant-wise interpolators, converges to the solution of the continuous setting.   Finally, we numerically illustrate computational aspects concerning quality approximation and execution time provided by the rearranged formulation.



### Electron Neutrino Classification in Liquid Argon Time Projection Chamber Detector
- **Arxiv ID**: http://arxiv.org/abs/1505.00424v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/1505.00424v1)
- **Published**: 2015-05-03 12:52:22+00:00
- **Updated**: 2015-05-03 12:52:22+00:00
- **Authors**: Piotr Płoński, Dorota Stefan, Robert Sulej, Krzysztof Zaremba
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Neutrinos are one of the least known elementary particles. The detection of neutrinos is an extremely difficult task since they are affected only by weak sub-atomic force or gravity. Therefore large detectors are constructed to reveal neutrino's properties. Among them the Liquid Argon Time Projection Chamber (LAr-TPC) detectors provide excellent imaging and particle identification ability for studying neutrinos. The computerized methods for automatic reconstruction and identification of particles are needed to fully exploit the potential of the LAr-TPC technique. Herein, the novel method for electron neutrino classification is presented. The method constructs a feature descriptor from images of observed event. It characterizes the signal distribution propagated from vertex of interest, where the particle interacts with the detector medium. The classifier is learned with a constructed feature descriptor to decide whether the images represent the electron neutrino or cascade produced by photons. The proposed approach assumes that the position of primary interaction vertex is known. The method's performance in dependency to the noise in a primary vertex position and deposited energy of particles is studied.



### Object Class Detection and Classification using Multi Scale Gradient and Corner Point based Shape Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1505.00432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.00432v1)
- **Published**: 2015-05-03 14:06:08+00:00
- **Updated**: 2015-05-03 14:06:08+00:00
- **Authors**: Basura Fernando, Sezer Karaoglu, Sajib Kumar Saha
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: This paper presents a novel multi scale gradient and a corner point based shape descriptors. The novel multi scale gradient based shape descriptor is combined with generic Fourier descriptors to extract contour and region based shape information. Shape information based object class detection and classification technique with a random forest classifier has been optimized. Proposed integrated descriptor in this paper is robust to rotation, scale, translation, affine deformations, noisy contours and noisy shapes. The new corner point based interpolated shape descriptor has been exploited for fast object detection and classification with higher accuracy.



### VQA: Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1505.00468v7
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1505.00468v7)
- **Published**: 2015-05-03 20:07:39+00:00
- **Updated**: 2016-10-27 03:50:19+00:00
- **Authors**: Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, Devi Parikh
- **Comment**: The first three authors contributed equally. International Conference
  on Computer Vision (ICCV) 2015
- **Journal**: None
- **Summary**: We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ~0.25M images, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa).



### Sequence to Sequence -- Video to Text
- **Arxiv ID**: http://arxiv.org/abs/1505.00487v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.00487v3)
- **Published**: 2015-05-03 22:32:00+00:00
- **Updated**: 2015-10-19 18:01:06+00:00
- **Authors**: Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko
- **Comment**: ICCV 2015 camera-ready. Includes code, project page and LSMDC
  challenge results
- **Journal**: None
- **Summary**: Real-world videos often have complex dynamics; and methods for generating open-domain video descriptions should be sensitive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem, we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).



