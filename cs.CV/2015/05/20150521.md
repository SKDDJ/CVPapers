# Arxiv Papers in cs.CV on 2015-05-21
### Why Regularized Auto-Encoders learn Sparse Representation?
- **Arxiv ID**: http://arxiv.org/abs/1505.05561v5
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1505.05561v5)
- **Published**: 2015-05-21 00:10:46+00:00
- **Updated**: 2016-06-17 23:01:20+00:00
- **Authors**: Devansh Arpit, Yingbo Zhou, Hung Ngo, Venu Govindaraju
- **Comment**: 8 pages of content, 1 page of reference, 4 pages of supplementary.
  ICML 2016; bug fix in lemma 1
- **Journal**: None
- **Summary**: While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks-- \textit{Internal Covariate Shift}-- the current solution has certain drawbacks. For instance, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate due to shifting parameter values (especially during initial training epochs). Another fundamental problem with BN is that it cannot be used with batch-size $ 1 $ during training. We address these drawbacks of BN by proposing a non-adaptive normalization technique for removing covariate shift, that we call \textit{Normalization Propagation}. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers.



### Unsupervised Segmentation of Overlapping Cervical Cell Cytoplasm
- **Arxiv ID**: http://arxiv.org/abs/1505.05601v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05601v1)
- **Published**: 2015-05-21 04:24:48+00:00
- **Updated**: 2015-05-21 04:24:48+00:00
- **Authors**: S L Happy, Swarnadip Chatterjee, Debdoot Sheet
- **Comment**: 2 pages, 2 figures
- **Journal**: None
- **Summary**: Overlapping of cervical cells and poor contrast of cell cytoplasm are the major issues in accurate detection and segmentation of cervical cells. An unsupervised cell segmentation approach is presented here. Cell clump segmentation was carried out using the extended depth of field (EDF) image created from the images of different focal planes. A modified Otsu method with prior class weights is proposed for accurate segmentation of nuclei from the cell clumps. The cell cytoplasm was further segmented from cell clump depending upon the number of nucleus detected in that cell clump. Level set model was used for cytoplasm segmentation.



### Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1505.05612v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, I.2.6; I.2.7; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1505.05612v3)
- **Published**: 2015-05-21 06:09:36+00:00
- **Updated**: 2015-11-02 21:12:15+00:00
- **Authors**: Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu
- **Comment**: Dataset released on the project page, see
  http://idl.baidu.com/FM-IQA.html ; NIPS 2015 camera ready version
- **Journal**: None
- **Summary**: In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long Short-Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, an LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset is evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human). The details of this work, including the FM-IQA dataset, can be found on the project page: http://idl.baidu.com/FM-IQA.html



### Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views
- **Arxiv ID**: http://arxiv.org/abs/1505.05641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05641v1)
- **Published**: 2015-05-21 08:16:06+00:00
- **Updated**: 2015-05-21 08:16:06+00:00
- **Authors**: Hao Su, Charles R. Qi, Yangyan Li, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: Object viewpoint estimation from 2D images is an essential task in computer vision. However, two issues hinder its progress: scarcity of training data with viewpoint annotations, and a lack of powerful features. Inspired by the growing availability of 3D models, we propose a framework to address both issues by combining render-based image synthesis and CNNs. We believe that 3D models have the potential in generating a large number of images of high variation, which can be well exploited by deep CNN with a high learning capacity. Towards this goal, we propose a scalable and overfit-resistant image synthesis pipeline, together with a novel CNN specifically tailored for the viewpoint estimation task. Experimentally, we show that the viewpoint estimation from our pipeline can significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark.



### Object Modelling with a Handheld RGB-D Camera
- **Arxiv ID**: http://arxiv.org/abs/1505.05643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05643v1)
- **Published**: 2015-05-21 08:24:21+00:00
- **Updated**: 2015-05-21 08:24:21+00:00
- **Authors**: Aitor Aldoma, Johann Prankl, Alexander Svejda, Markus Vincze
- **Comment**: Presented at OAGM Workshop, 2015 (arXiv:1505.01065)
- **Journal**: None
- **Summary**: This work presents a flexible system to reconstruct 3D models of objects captured with an RGB-D sensor. A major advantage of the method is that our reconstruction pipeline allows the user to acquire a full 3D model of the object. This is achieved by acquiring several partial 3D models in different sessions that are automatically merged together to reconstruct a full model. In addition, the 3D models acquired by our system can be directly used by state-of-the-art object instance recognition and object tracking modules, providing object-perception capabilities for different applications, such as human-object interaction analysis or robot grasping. The system does not impose constraints in the appearance of objects (textured, untextured) nor in the modelling setup (moving camera with static object or a turn-table setup). The proposed reconstruction system has been used to model a large number of objects resulting in metrically accurate and visually appealing 3D models.



### Graph edit distance : a new binary linear programming formulation
- **Arxiv ID**: http://arxiv.org/abs/1505.05740v1
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1505.05740v1)
- **Published**: 2015-05-21 13:57:40+00:00
- **Updated**: 2015-05-21 13:57:40+00:00
- **Authors**: Julien Lerouge, Zeina Abu-Aisheh, Romain Raveaux, Pierre Héroux, Sébastien Adam
- **Comment**: None
- **Journal**: None
- **Summary**: Graph edit distance (GED) is a powerful and flexible graph matching paradigm that can be used to address different tasks in structural pattern recognition, machine learning, and data mining. In this paper, some new binary linear programming formulations for computing the exact GED between two graphs are proposed. A major strength of the formulations lies in their genericity since the GED can be computed between directed or undirected fully attributed graphs (i.e. with attributes on both vertices and edges). Moreover, a relaxation of the domain constraints in the formulations provides efficient lower bound approximations of the GED. A complete experimental study comparing the proposed formulations with 4 state-of-the-art algorithms for exact and approximate graph edit distances is provided. By considering both the quality of the proposed solution and the efficiency of the algorithms as performance criteria, the results show that none of the compared methods dominates the others in the Pareto sense. As a consequence, faced to a given real-world problem, a trade-off between quality and efficiency has to be chosen w.r.t. the application constraints. In this context, this paper provides a guide that can be used to choose the appropriate method.



### GazeDPM: Early Integration of Gaze Information in Deformable Part Models
- **Arxiv ID**: http://arxiv.org/abs/1505.05753v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1505.05753v1)
- **Published**: 2015-05-21 14:39:51+00:00
- **Updated**: 2015-05-21 14:39:51+00:00
- **Authors**: Iaroslav Shcherbatyi, Andreas Bulling, Mario Fritz
- **Comment**: None
- **Journal**: None
- **Summary**: An increasing number of works explore collaborative human-computer systems in which human gaze is used to enhance computer vision systems. For object detection these efforts were so far restricted to late integration approaches that have inherent limitations, such as increased precision without increase in recall. We propose an early integration approach in a deformable part model, which constitutes a joint formulation over gaze and visual data. We show that our GazeDPM method improves over the state-of-the-art DPM baseline by 4% and a recent method for gaze-supported object detection by 3% on the public POET dataset. Our approach additionally provides introspection of the learnt models, can reveal salient image structures, and allows us to investigate the interplay between gaze attracting and repelling areas, the importance of view-specific models, as well as viewers' personal biases in gaze patterns. We finally study important practical aspects of our approach, such as the impact of using saliency maps instead of real fixations, the impact of the number of fixations, as well as robustness to gaze estimation error.



### Watch and Learn: Semi-Supervised Learning of Object Detectors from Videos
- **Arxiv ID**: http://arxiv.org/abs/1505.05769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05769v1)
- **Published**: 2015-05-21 15:34:30+00:00
- **Updated**: 2015-05-21 15:34:30+00:00
- **Authors**: Ishan Misra, Abhinav Shrivastava, Martial Hebert
- **Comment**: To appear in CVPR 2015
- **Journal**: None
- **Summary**: We present a semi-supervised approach that localizes multiple unknown object instances in long videos. We start with a handful of labeled boxes and iteratively learn and label hundreds of thousands of object instances. We propose criteria for reliable object detection and tracking for constraining the semi-supervised learning process and minimizing semantic drift. Our approach does not assume exhaustive labeling of each object instance in any single frame, or any explicit annotation of negative data. Working in such a generic setting allow us to tackle multiple object instances in video, many of which are static. In contrast, existing approaches either do not consider multiple object instances per video, or rely heavily on the motion of the objects present. The experiments demonstrate the effectiveness of our approach by evaluating the automatically labeled data on a variety of metrics like quality, coverage (recall), diversity, and relevance to training an object detector.



### Object-Proposal Evaluation Protocol is 'Gameable'
- **Arxiv ID**: http://arxiv.org/abs/1505.05836v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05836v4)
- **Published**: 2015-05-21 18:53:45+00:00
- **Updated**: 2015-11-23 07:16:16+00:00
- **Authors**: Neelima Chavali, Harsh Agrawal, Aroma Mahendru, Dhruv Batra
- **Comment**: 15 pages, 11 figures, 4 tables
- **Journal**: None
- **Summary**: Object proposals have quickly become the de-facto pre-processing step in a number of vision pipelines (for object detection, object discovery, and other tasks). Their performance is usually evaluated on partially annotated datasets. In this paper, we argue that the choice of using a partially annotated dataset for evaluation of object proposals is problematic -- as we demonstrate via a thought experiment, the evaluation protocol is 'gameable', in the sense that progress under this protocol does not necessarily correspond to a "better" category independent object proposal algorithm.   To alleviate this problem, we: (1) Introduce a nearly-fully annotated version of PASCAL VOC dataset, which serves as a test-bed to check if object proposal techniques are overfitting to a particular list of categories. (2) Perform an exhaustive evaluation of object proposal methods on our introduced nearly-fully annotated PASCAL dataset and perform cross-dataset generalization experiments; and (3) Introduce a diagnostic experiment to detect the bias capacity in an object proposal algorithm. This tool circumvents the need to collect a densely annotated dataset, which can be expensive and cumbersome to collect. Finally, we plan to release an easy-to-use toolbox which combines various publicly available implementations of object proposal algorithms which standardizes the proposal generation and evaluation so that new methods can be added and evaluated on different datasets. We hope that the results presented in the paper will motivate the community to test the category independence of various object proposal methods by carefully choosing the evaluation protocol.



### Randomized Robust Subspace Recovery for High Dimensional Data Matrices
- **Arxiv ID**: http://arxiv.org/abs/1505.05901v2
- **DOI**: 10.1109/TSP.2016.2645515
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1505.05901v2)
- **Published**: 2015-05-21 21:04:33+00:00
- **Updated**: 2016-04-08 22:25:06+00:00
- **Authors**: Mostafa Rahmani, George Atia
- **Comment**: None
- **Journal**: IEEE Transactions on Signal Processing ( Volume: 65, Issue: 6,
  March15, 15 2017 )
- **Summary**: This paper explores and analyzes two randomized designs for robust Principal Component Analysis (PCA) employing low-dimensional data sketching. In one design, a data sketch is constructed using random column sampling followed by low dimensional embedding, while in the other, sketching is based on random column and row sampling. Both designs are shown to bring about substantial savings in complexity and memory requirements for robust subspace learning over conventional approaches that use the full scale data. A characterization of the sample and computational complexity of both designs is derived in the context of two distinct outlier models, namely, sparse and independent outlier models. The proposed randomized approach can provably recover the correct subspace with computational and sample complexity that are almost independent of the size of the data. The results of the mathematical analysis are confirmed through numerical simulations using both synthetic and real data.



### A Multi-scale Multiple Instance Video Description Network
- **Arxiv ID**: http://arxiv.org/abs/1505.05914v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05914v3)
- **Published**: 2015-05-21 21:47:08+00:00
- **Updated**: 2016-03-19 02:27:58+00:00
- **Authors**: Huijuan Xu, Subhashini Venugopalan, Vasili Ramanishka, Marcus Rohrbach, Kate Saenko
- **Comment**: ICCV15 workshop on Closing the Loop Between Vision and Language
- **Journal**: None
- **Summary**: Generating natural language descriptions for in-the-wild videos is a challenging task. Most state-of-the-art methods for solving this problem borrow existing deep convolutional neural network (CNN) architectures (AlexNet, GoogLeNet) to extract a visual representation of the input video. However, these deep CNN architectures are designed for single-label centered-positioned object classification. While they generate strong semantic features, they have no inherent structure allowing them to detect multiple objects of different sizes and locations in the frame. Our paper tries to solve this problem by integrating the base CNN into several fully convolutional neural networks (FCNs) to form a multi-scale network that handles multiple receptive field sizes in the original image. FCNs, previously applied to image segmentation, can generate class heat-maps efficiently compared to sliding window mechanisms, and can easily handle multiple scales. To further handle the ambiguity over multiple objects and locations, we incorporate the Multiple Instance Learning mechanism (MIL) to consider objects in different positions and at different scales simultaneously. We integrate our multi-scale multi-instance architecture with a sequence-to-sequence recurrent neural network to generate sentence descriptions based on the visual representation. Ours is the first end-to-end trainable architecture that is capable of multi-scale region processing. Evaluation on a Youtube video dataset shows the advantage of our approach compared to the original single-scale whole frame CNN model. Our flexible and efficient architecture can potentially be extended to support other video processing tasks.



### Rendering of Eyes for Eye-Shape Registration and Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/1505.05916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05916v1)
- **Published**: 2015-05-21 22:12:31+00:00
- **Updated**: 2015-05-21 22:12:31+00:00
- **Authors**: Erroll Wood, Tadas Baltrusaitis, Xucong Zhang, Yusuke Sugano, Peter Robinson, Andreas Bulling
- **Comment**: None
- **Journal**: None
- **Summary**: Images of the eye are key in several computer vision problems, such as shape registration and gaze estimation. Recent large-scale supervised methods for these problems require time-consuming data collection and manual annotation, which can be unreliable. We propose synthesizing perfectly labelled photo-realistic training data in a fraction of the time. We used computer graphics techniques to build a collection of dynamic eye-region models from head scan geometry. These were randomly posed to synthesize close-up eye images for a wide range of head poses, gaze directions, and illumination conditions. We used our model's controllability to verify the importance of realistic illumination and shape variations in eye-region training data. Finally, we demonstrate the benefits of our synthesized training data (SynthesEyes) by out-performing state-of-the-art methods for eye-shape registration as well as cross-dataset appearance-based gaze estimation in the wild.



