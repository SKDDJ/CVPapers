# Arxiv Papers in cs.CV on 2015-05-15
### Using Ensemble Models in the Histological Examination of Tissue Abnormalities
- **Arxiv ID**: http://arxiv.org/abs/1505.03932v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CE, cs.LG, H.2.8; I.5.3; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1505.03932v1)
- **Published**: 2015-05-15 00:59:48+00:00
- **Updated**: 2015-05-15 00:59:48+00:00
- **Authors**: Giancarlo Crocetti, Michael Coakley, Phil Dressner, Wanda Kellum, Tamba Lamin
- **Comment**: 4 pages, 4 tables, 3 figures. Proceedings of 12th Annual Research
  Day, 2014 - Pace University
- **Journal**: None
- **Summary**: Classification models for the automatic detection of abnormalities on histological samples do exists, with an active debate on the cost associated with false negative diagnosis (underdiagnosis) and false positive diagnosis (overdiagnosis). Current models tend to underdiagnose, failing to recognize a potentially fatal disease.   The objective of this study is to investigate the possibility of automatically identifying abnormalities in tissue samples through the use of an ensemble model on data generated by histological examination and to minimize the number of false negative cases.



### Automatic Facial Expression Recognition Using Features of Salient Facial Patches
- **Arxiv ID**: http://arxiv.org/abs/1505.04026v1
- **DOI**: 10.1109/TAFFC.2014.2386334
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04026v1)
- **Published**: 2015-05-15 11:07:39+00:00
- **Updated**: 2015-05-15 11:07:39+00:00
- **Authors**: S L Happy, Aurobinda Routray
- **Comment**: None
- **Journal**: IEEE Transactions on Affective Computing, vol. 6, no. 1, pp. 1-12,
  2015
- **Summary**: Extraction of discriminative features from salient facial patches plays a vital role in effective facial expression recognition. The accurate detection of facial landmarks improves the localization of the salient patches on face images. This paper proposes a novel framework for expression recognition by using appearance features of selected facial patches. A few prominent facial patches, depending on the position of facial landmarks, are extracted which are active during emotion elicitation. These active patches are further processed to obtain the salient patches which contain discriminative features for classification of each pair of expressions, thereby selecting different facial patches as salient for different pair of expression classes. One-against-one classification method is adopted using these features. In addition, an automated learning-free facial landmark detection technique has been proposed, which achieves similar performances as that of other state-of-art landmark detection methods, yet requires significantly less execution time. The proposed method is found to perform well consistently in different resolutions, hence, providing a solution for expression recognition in low resolution images. Experiments on CK+ and JAFFE facial expression databases show the effectiveness of the proposed system.



### Biometric Matching and Fusion System for Fingerprints from Non-Distal Phalanges
- **Arxiv ID**: http://arxiv.org/abs/1505.04028v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04028v2)
- **Published**: 2015-05-15 11:09:58+00:00
- **Updated**: 2015-05-20 08:10:57+00:00
- **Authors**: Mehmet Kayaoglu, Berkay Topcu, Umut Uludag
- **Comment**: 22 pages, 14 figures
- **Journal**: None
- **Summary**: Market research indicates that fingerprints are still the most popular biometric modality for personal authentication. Even with the onset of new modalities (e.g. vein matching), many applications within different domains (e-ID, banking, border control...) and geographies rely on fingerprints obtained from the distal phalanges (a.k.a. sections, digits) of the human hand structure. Motivated by the problem of poor quality distal fingerprint images affecting a non-trivial portion of the population (which decreases associated authentication accuracy), we designed and tested a multifinger, multiphalanx fusion scheme, that combines minutiae matching scores originating from non-distal (ie. middle and proximal) phalanges based on (i) simple sum fusion, (ii) NFIQ image-quality-based fusion, and (iii) phalanx-type-based fusion. Utilizing a medium-size (50 individuals, 400 unique fingers, 1600 distinct images) database collected in our laboratory with a commercial optical fingerprint sensor, and a commercial minutiae extractor & matcher (without any modification), allowed us to simulate a real-world fingerprint authentication setting. Detailed analyses including ROC curves with statistical confidence intervals show that the proposed system can be a viable alternative for cases where (i) distal phalanx images are not usable (e.g. due to missing digits, or low quality finger surface due to manual labor), and (ii) switching to a new biometric modality (e.g. iris) is not possible due to economical or infrastructure limits. Further, we show that when distal phalanx images are in fact usable, combining them with images from other phalanges increases accuracy as well.



### Robust Facial Expression Classification Using Shape and Appearance Features
- **Arxiv ID**: http://arxiv.org/abs/1505.04030v1
- **DOI**: 10.1109/ICAPR.2015.7050661
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04030v1)
- **Published**: 2015-05-15 11:15:18+00:00
- **Updated**: 2015-05-15 11:15:18+00:00
- **Authors**: S. L. Happy, Aurobinda Routray
- **Comment**: Proceedings of 8th International Conference of Advances in Pattern
  Recognition, 2015
- **Journal**: None
- **Summary**: Facial expression recognition has many potential applications which has attracted the attention of researchers in the last decade. Feature extraction is one important step in expression analysis which contributes toward fast and accurate expression recognition. This paper represents an approach of combining the shape and appearance features to form a hybrid feature vector. We have extracted Pyramid of Histogram of Gradients (PHOG) as shape descriptors and Local Binary Patterns (LBP) as appearance features. The proposed framework involves a novel approach of extracting hybrid features from active facial patches. The active facial patches are located on the face regions which undergo a major change during different expressions. After detection of facial landmarks, the active patches are localized and hybrid features are calculated from these patches. The use of small parts of face instead of the whole face for extracting features reduces the computational cost and prevents the over-fitting of the features for classification. By using linear discriminant analysis, the dimensionality of the feature is reduced which is further classified by using the support vector machine (SVM). The experimental results on two publicly available databases show promising accuracy in recognizing all expression classes.



### A Video Database of Human Faces under Near Infra-Red Illumination for Human Computer Interaction Aplications
- **Arxiv ID**: http://arxiv.org/abs/1505.04055v1
- **DOI**: 10.1109/IHCI.2012.6481868
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04055v1)
- **Published**: 2015-05-15 13:24:14+00:00
- **Updated**: 2015-05-15 13:24:14+00:00
- **Authors**: S L Happy, Anirban Dasgupta, Anjith George, Aurobinda Routray
- **Comment**: None
- **Journal**: IEEE Proceedings of 4th International Conference on Intelligent
  Human Computer Interaction, 2012
- **Summary**: Human Computer Interaction (HCI) is an evolving area of research for coherent communication between computers and human beings. Some of the important applications of HCI as reported in literature are face detection, face pose estimation, face tracking and eye gaze estimation. Development of algorithms for these applications is an active field of research. However, availability of standard database to validate such algorithms is insufficient. This paper discusses the creation of such a database created under Near Infra-Red (NIR) illumination. NIR illumination has gained its popularity for night mode applications since prolonged exposure to Infra-Red (IR) lighting may lead to many health issues. The database contains NIR videos of 60 subjects in different head orientations and with different facial expressions, facial occlusions and illumination variation. This new database can be a very valuable resource for development and evaluation of algorithms on face detection, eye detection, head tracking, eye gaze tracking etc. in NIR lighting.



### A Real Time Facial Expression Classification System Using Local Binary Patterns
- **Arxiv ID**: http://arxiv.org/abs/1505.04058v1
- **DOI**: 10.1109/IHCI.2012.6481802
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04058v1)
- **Published**: 2015-05-15 13:33:10+00:00
- **Updated**: 2015-05-15 13:33:10+00:00
- **Authors**: S. L. Happy, Anjith George, Aurobinda Routray
- **Comment**: IEEE Proceedings of 4th International Conference on Intelligent Human
  Computer Interaction, 2012
- **Journal**: None
- **Summary**: Facial expression analysis is one of the popular fields of research in human computer interaction (HCI). It has several applications in next generation user interfaces, human emotion analysis, behavior and cognitive modeling. In this paper, a facial expression classification algorithm is proposed which uses Haar classifier for face detection purpose, Local Binary Patterns (LBP) histogram of different block sizes of a face image as feature vectors and classifies various facial expressions using Principal Component Analysis (PCA). The algorithm is implemented in real time for expression classification since the computational complexity of the algorithm is small. A customizable approach is proposed for facial expression analysis, since the various expressions and intensity of expressions vary from person to person. The system uses grayscale frontal face images of a person to classify six basic emotions namely happiness, sadness, disgust, fear, surprise and anger.



### Discovering Attribute Shades of Meaning with the Crowd
- **Arxiv ID**: http://arxiv.org/abs/1505.04117v1
- **DOI**: 10.1007/s11263-014-0798-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04117v1)
- **Published**: 2015-05-15 16:43:08+00:00
- **Updated**: 2015-05-15 16:43:08+00:00
- **Authors**: Adriana Kovashka, Kristen Grauman
- **Comment**: Published in the International Journal of Computer Vision (IJCV),
  January 2015. The final publication is available at Springer via
  http://dx.doi.org/10.1007/s11263-014-0798-1
- **Journal**: International Journal of Computer Vision 1573-1405 (2015,
  Springer)
- **Summary**: To learn semantic attributes, existing methods typically train one discriminative model for each word in a vocabulary of nameable properties. However, this "one model per word" assumption is problematic: while a word might have a precise linguistic definition, it need not have a precise visual definition. We propose to discover shades of attribute meaning. Given an attribute name, we use crowdsourced image labels to discover the latent factors underlying how different annotators perceive the named concept. We show that structure in those latent factors helps reveal shades, that is, interpretations for the attribute shared by some group of annotators. Using these shades, we train classifiers to capture the primary (often subtle) variants of the attribute. The resulting models are both semantic and visually precise. By catering to users' interpretations, they improve attribute prediction accuracy on novel images. Shades also enable more successful attribute-based image search, by providing robust personalized models for retrieving multi-attribute query results. They are widely applicable to tasks that involve describing visual content, such as zero-shot category learning and organization of photo collections.



### WhittleSearch: Interactive Image Search with Relative Attribute Feedback
- **Arxiv ID**: http://arxiv.org/abs/1505.04141v2
- **DOI**: 10.1007/s11263-015-0814-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04141v2)
- **Published**: 2015-05-15 18:03:12+00:00
- **Updated**: 2015-05-18 13:52:40+00:00
- **Authors**: Adriana Kovashka, Devi Parikh, Kristen Grauman
- **Comment**: Published in the International Journal of Computer Vision (IJCV),
  April 2015. The final publication is available at Springer via
  http://dx.doi.org/10.1007/s11263-015-0814-0
- **Journal**: International Journal of Computer Vision, 1573-1405 (2015,
  Springer)
- **Summary**: We propose a novel mode of feedback for image search, where a user describes which properties of exemplar images should be adjusted in order to more closely match his/her mental model of the image sought. For example, perusing image results for a query "black shoes", the user might state, "Show me shoe images like these, but sportier." Offline, our approach first learns a set of ranking functions, each of which predicts the relative strength of a nameable attribute in an image (e.g., sportiness). At query time, the system presents the user with a set of exemplar images, and the user relates them to his/her target image with comparative statements. Using a series of such constraints in the multi-dimensional attribute space, our method iteratively updates its relevance function and re-ranks the database of images. To determine which exemplar images receive feedback from the user, we present two variants of the approach: one where the feedback is user-initiated and another where the feedback is actively system-initiated. In either case, our approach allows a user to efficiently "whittle away" irrelevant portions of the visual feature space, using semantic language to precisely communicate her preferences to the system. We demonstrate our technique for refining image search for people, products, and scenes, and we show that it outperforms traditional binary relevance feedback in terms of search speed and accuracy. In addition, the ordinal nature of relative attributes helps make our active approach efficient -- both computationally for the machine when selecting the reference images, and for the user by requiring less user interaction than conventional passive and active methods.



### Dense Semantic Correspondence where Every Pixel is a Classifier
- **Arxiv ID**: http://arxiv.org/abs/1505.04143v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04143v1)
- **Published**: 2015-05-15 18:04:07+00:00
- **Updated**: 2015-05-15 18:04:07+00:00
- **Authors**: Hilton Bristow, Jack Valmadre, Simon Lucey
- **Comment**: ICCV 2015 Submission
- **Journal**: None
- **Summary**: Determining dense semantic correspondences across objects and scenes is a difficult problem that underpins many higher-level computer vision algorithms. Unlike canonical dense correspondence problems which consider images that are spatially or temporally adjacent, semantic correspondence is characterized by images that share similar high-level structures whose exact appearance and geometry may differ.   Motivated by object recognition literature and recent work on rapidly estimating linear classifiers, we treat semantic correspondence as a constrained detection problem, where an exemplar LDA classifier is learned for each pixel. LDA classifiers have two distinct benefits: (i) they exhibit higher average precision than similarity metrics typically used in correspondence problems, and (ii) unlike exemplar SVM, can output globally interpretable posterior probabilities without calibration, whilst also being significantly faster to train.   We pose the correspondence problem as a graphical model, where the unary potentials are computed via convolution with the set of exemplar classifiers, and the joint potentials enforce smoothly varying correspondence assignment.



