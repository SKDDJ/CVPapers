# Arxiv Papers in cs.CV on 2015-05-05
### Empirical Evaluation of Rectified Activations in Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1505.00853v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1505.00853v2)
- **Published**: 2015-05-05 01:16:39+00:00
- **Updated**: 2015-11-27 06:58:14+00:00
- **Authors**: Bing Xu, Naiyan Wang, Tianqi Chen, Mu Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68\% accuracy on CIFAR-100 test set without multiple test or ensemble.



### Large-scale Classification of Fine-Art Paintings: Learning The Right Metric on The Right Feature
- **Arxiv ID**: http://arxiv.org/abs/1505.00855v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1505.00855v1)
- **Published**: 2015-05-05 01:25:26+00:00
- **Updated**: 2015-05-05 01:25:26+00:00
- **Authors**: Babak Saleh, Ahmed Elgammal
- **Comment**: 21 pages
- **Journal**: None
- **Summary**: In the past few years, the number of fine-art collections that are digitized and publicly available has been growing rapidly. With the availability of such large collections of digitized artworks comes the need to develop multimedia systems to archive and retrieve this pool of data. Measuring the visual similarity between artistic items is an essential step for such multimedia systems, which can benefit more high-level multimedia tasks. In order to model this similarity between paintings, we should extract the appropriate visual features for paintings and find out the best approach to learn the similarity metric based on these features. We investigate a comprehensive list of visual features and metric learning approaches to learn an optimized similarity measure between paintings. We develop a machine that is able to make aesthetic-related semantic-level judgments, such as predicting a painting's style, genre, and artist, as well as providing similarity measures optimized based on the knowledge available in the domain of art historical interpretation. Our experiments show the value of using this similarity measure for the aforementioned prediction tasks.



### Adaptive diffusion constrained total variation scheme with application to `cartoon + texture + edge' image decomposition
- **Arxiv ID**: http://arxiv.org/abs/1505.00866v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1505.00866v1)
- **Published**: 2015-05-05 02:14:15+00:00
- **Updated**: 2015-05-05 02:14:15+00:00
- **Authors**: Juan C. Moreno, V. B. Surya Prasath, D. Vorotnikov, H. Proenca, K. Palaniappan
- **Comment**: None
- **Journal**: None
- **Summary**: We consider an image decomposition model involving a variational (minimization) problem and an evolutionary partial differential equation (PDE). We utilize a linear inhomogenuous diffusion constrained and weighted total variation (TV) scheme for image adaptive decomposition. An adaptive weight along with TV regularization splits a given image into three components representing the geometrical (cartoon), textural (small scale - microtextures), and edges (big scale - macrotextures). We study the wellposedness of the coupled variational-PDE scheme along with an efficient numerical scheme based on Chambolle's dual minimization method. We provide extensive experimental results in cartoon-texture-edges decomposition, and denoising as well compare with other related variational, coupled anisotropic diffusion PDE based methods.



### Multi-view Convolutional Neural Networks for 3D Shape Recognition
- **Arxiv ID**: http://arxiv.org/abs/1505.00880v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1505.00880v3)
- **Published**: 2015-05-05 04:51:19+00:00
- **Updated**: 2015-09-27 20:42:16+00:00
- **Authors**: Hang Su, Subhransu Maji, Evangelos Kalogerakis, Erik Learned-Miller
- **Comment**: v1: Initial version. v2: An updated ModelNet40 training/test split is
  used; results with low-rank Mahalanobis metric learning are added. v3 (ICCV
  2015): A second camera setup without the upright orientation assumption is
  added; some accuracy and mAP numbers are changed slightly because a small
  issue in mesh rendering related to specularities is fixed
- **Journal**: None
- **Summary**: A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.



### Fast Guided Filter
- **Arxiv ID**: http://arxiv.org/abs/1505.00996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.00996v1)
- **Published**: 2015-05-05 13:10:53+00:00
- **Updated**: 2015-05-05 13:10:53+00:00
- **Authors**: Kaiming He, Jian Sun
- **Comment**: Technical report
- **Journal**: None
- **Summary**: The guided filter is a technique for edge-aware image filtering. Because of its nice visual quality, fast speed, and ease of implementation, the guided filter has witnessed various applications in real products, such as image editing apps in phones and stereo reconstruction, and has been included in official MATLAB and OpenCV. In this note, we remind that the guided filter can be simply sped up from O(N) time to O(N/s^2) time for a subsampling ratio s. In a variety of applications, this leads to a speedup of >10x with almost no visible degradation. We hope this acceleration will improve performance of current applications and further popularize this filter. Code is released.



### In Defense of the Direct Perception of Affordances
- **Arxiv ID**: http://arxiv.org/abs/1505.01085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.01085v1)
- **Published**: 2015-05-05 17:11:26+00:00
- **Updated**: 2015-05-05 17:11:26+00:00
- **Authors**: David F. Fouhey, Xiaolong Wang, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: The field of functional recognition or affordance estimation from images has seen a revival in recent years. As originally proposed by Gibson, the affordances of a scene were directly perceived from the ambient light: in other words, functional properties like sittable were estimated directly from incoming pixels. Recent work, however, has taken a mediated approach in which affordances are derived by first estimating semantics or geometry and then reasoning about the affordances. In a tribute to Gibson, this paper explores his theory of affordances as originally proposed. We propose two approaches for direct perception of affordances and show that they obtain good results and can out-perform mediated approaches. We hope this paper can rekindle discussion around direct perception and its implications in the long term.



### Ask Your Neurons: A Neural-based Approach to Answering Questions about Images
- **Arxiv ID**: http://arxiv.org/abs/1505.01121v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1505.01121v3)
- **Published**: 2015-05-05 18:39:29+00:00
- **Updated**: 2015-10-01 12:13:20+00:00
- **Authors**: Mateusz Malinowski, Marcus Rohrbach, Mario Fritz
- **Comment**: ICCV'15 (Oral)
- **Journal**: None
- **Summary**: We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus.



### Visual Summary of Egocentric Photostreams by Representative Keyframes
- **Arxiv ID**: http://arxiv.org/abs/1505.01130v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1505.01130v2)
- **Published**: 2015-05-05 19:14:23+00:00
- **Updated**: 2015-05-06 14:00:23+00:00
- **Authors**: Marc Bolaños, Ricard Mestre, Estefanía Talavera, Xavier Giró-i-Nieto, Petia Radeva
- **Comment**: Paper accepted in the IEEE First International Workshop on Wearable
  and Ego-vision Systems for Augmented Experience (WEsAX). Turin, Italy. July
  3, 2015
- **Journal**: None
- **Summary**: Building a visual summary from an egocentric photostream captured by a lifelogging wearable camera is of high interest for different applications (e.g. memory reinforcement). In this paper, we propose a new summarization method based on keyframes selection that uses visual features extracted by means of a convolutional neural network. Our method applies an unsupervised clustering for dividing the photostreams into events, and finally extracts the most relevant keyframe for each event. We assess the results by applying a blind-taste test on a group of 20 people who assessed the quality of the summaries.



### Deep Learning for Object Saliency Detection and Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1505.01173v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.01173v1)
- **Published**: 2015-05-05 20:03:07+00:00
- **Updated**: 2015-05-05 20:03:07+00:00
- **Authors**: Hengyue Pan, Bo Wang, Hui Jiang
- **Comment**: 9 pages, 126 figures, technical report
- **Journal**: None
- **Summary**: In this paper, we propose several novel deep learning methods for object saliency detection based on the powerful convolutional neural networks. In our approach, we use a gradient descent method to iteratively modify an input image based on the pixel-wise gradients to reduce a cost function measuring the class-specific objectness of the image. The pixel-wise gradients can be efficiently computed using the back-propagation algorithm. The discrepancy between the modified image and the original one may be used as a saliency map for the image. Moreover, we have further proposed several new training methods to learn saliency-specific convolutional nets for object saliency detection, in order to leverage the available pixel-wise segmentation information. Our methods are extremely computationally efficient (processing 20-40 images per second in one GPU). In this work, we use the computed saliency maps for image segmentation. Experimental results on two benchmark tasks, namely Microsoft COCO and Pascal VOC 2012, have shown that our proposed methods can generate high-quality salience maps, clearly outperforming many existing methods. In particular, our approaches excel in handling many difficult images, which contain complex background, highly-variable salient objects, multiple objects, and/or very small salient objects.



### Contextual Action Recognition with R*CNN
- **Arxiv ID**: http://arxiv.org/abs/1505.01197v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.01197v3)
- **Published**: 2015-05-05 21:56:10+00:00
- **Updated**: 2016-03-25 01:06:01+00:00
- **Authors**: Georgia Gkioxari, Ross Girshick, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: There are multiple cues in an image which reveal what action a person is performing. For example, a jogger has a pose that is characteristic for jogging, but the scene (e.g. road, trail) and the presence of other joggers can be an additional source of information. In this work, we exploit the simple observation that actions are accompanied by contextual cues to build a strong action recognition system. We adapt RCNN to use more than one region for classification while still maintaining the ability to localize the action. We call our system R*CNN. The action-specific models and the feature maps are trained jointly, allowing for action specific representations to emerge. R*CNN achieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all other approaches in the field by a significant margin. Last, we show that R*CNN is not limited to action recognition. In particular, R*CNN can also be used to tackle fine-grained tasks such as attribute classification. We validate this claim by reporting state-of-the-art performance on the Berkeley Attributes of People dataset.



### Learning Style Similarity for Searching Infographics
- **Arxiv ID**: http://arxiv.org/abs/1505.01214v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.HC, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1505.01214v1)
- **Published**: 2015-05-05 22:59:32+00:00
- **Updated**: 2015-05-05 22:59:32+00:00
- **Authors**: Babak Saleh, Mira Dontcheva, Aaron Hertzmann, Zhicheng Liu
- **Comment**: 6 pages, to appear in the 41st annual conference on Graphics
  Interface (GI) 2015,
- **Journal**: None
- **Summary**: Infographics are complex graphic designs integrating text, images, charts and sketches. Despite the increasing popularity of infographics and the rapid growth of online design portfolios, little research investigates how we can take advantage of these design resources. In this paper we present a method for measuring the style similarity between infographics. Based on human perception data collected from crowdsourced experiments, we use computer vision and machine learning algorithms to learn a style similarity metric for infographic designs. We evaluate different visual features and learning algorithms and find that a combination of color histograms and Histograms-of-Gradients (HoG) features is most effective in characterizing the style of infographics. We demonstrate our similarity metric on a preliminary image retrieval test.



