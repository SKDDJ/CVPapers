# Arxiv Papers in cs.CV on 2015-05-18
### Reproducible Evaluation of Pan-Tilt-Zoom Tracking
- **Arxiv ID**: http://arxiv.org/abs/1505.04502v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04502v1)
- **Published**: 2015-05-18 03:52:52+00:00
- **Updated**: 2015-05-18 03:52:52+00:00
- **Authors**: Gengjie Chen, Pierre-Luc St-Charles, Wassim Bouachir, Thomas Joeisseint, Guillaume-Alexandre Bilodeau, Robert Bergevin
- **Comment**: This is an extended version of the 2015 ICIP paper "Reproducible
  Evaluation of Pan-Tilt-Zoom Tracking"
- **Journal**: None
- **Summary**: Tracking with a Pan-Tilt-Zoom (PTZ) camera has been a research topic in computer vision for many years. However, it is very difficult to assess the progress that has been made on this topic because there is no standard evaluation methodology. The difficulty in evaluating PTZ tracking algorithms arises from their dynamic nature. In contrast to other forms of tracking, PTZ tracking involves both locating the target in the image and controlling the motors of the camera to aim it so that the target stays in its field of view. This type of tracking can only be performed online. In this paper, we propose a new evaluation framework based on a virtual PTZ camera. With this framework, tracking scenarios do not change for each experiment and we are able to replicate online PTZ camera control and behavior including camera positioning delays, tracker processing delays, and numerical zoom. We tested our evaluation framework with the Camshift tracker to show its viability and to establish baseline results.



### Place Recognition with Event-based Cameras and a Neural Implementation of SeqSLAM
- **Arxiv ID**: http://arxiv.org/abs/1505.04548v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1505.04548v1)
- **Published**: 2015-05-18 08:33:15+00:00
- **Updated**: 2015-05-18 08:33:15+00:00
- **Authors**: Michael Milford, Hanme Kim, Michael Mangan, Stefan Leutenegger, Tom Stone, Barbara Webb, Andrew Davison
- **Comment**: Paper accepted for presentation at the "Innovative Sensing for
  Robotics: Focus on Neuromorphic Sensors" workshop at the 2015 IEEE
  International Conference on Robotics and Automation, 8 pages, 10 figures
- **Journal**: None
- **Summary**: Event-based cameras offer much potential to the fields of robotics and computer vision, in part due to their large dynamic range and extremely high "frame rates". These attributes make them, at least in theory, particularly suitable for enabling tasks like navigation and mapping on high speed robotic platforms under challenging lighting conditions, a task which has been particularly challenging for traditional algorithms and camera sensors. Before these tasks become feasible however, progress must be made towards adapting and innovating current RGB-camera-based algorithms to work with event-based cameras. In this paper we present ongoing research investigating two distinct approaches to incorporating event-based cameras for robotic navigation: the investigation of suitable place recognition / loop closure techniques, and the development of efficient neural implementations of place recognition techniques that enable the possibility of place recognition using event-based cameras at very high frame rates using neuromorphic computing hardware.



### Global Variational Method for Fingerprint Segmentation by Three-part Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1505.04585v1
- **DOI**: 10.1049/iet-bmt.2015.0010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04585v1)
- **Published**: 2015-05-18 10:40:38+00:00
- **Updated**: 2015-05-18 10:40:38+00:00
- **Authors**: Duy Hoang Thai, Carsten Gottschlich
- **Comment**: None
- **Journal**: IET Biometrics, vol. 5, no. 2, pp. 120-130, June 2016
- **Summary**: Verifying an identity claim by fingerprint recognition is a commonplace experience for millions of people in their daily life, e.g. for unlocking a tablet computer or smartphone. The first processing step after fingerprint image acquisition is segmentation, i.e. dividing a fingerprint image into a foreground region which contains the relevant features for the comparison algorithm, and a background region. We propose a novel segmentation method by global three-part decomposition (G3PD). Based on global variational analysis, the G3PD method decomposes a fingerprint image into cartoon, texture and noise parts. After decomposition, the foreground region is obtained from the non-zero coefficients in the texture image using morphological processing. The segmentation performance of the G3PD method is compared to five state-of-the-art methods on a benchmark which comprises manually marked ground truth segmentation for 10560 images. Performance evaluations show that the G3PD method consistently outperforms existing methods in terms of segmentation accuracy.



### U-Net: Convolutional Networks for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1505.04597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04597v1)
- **Published**: 2015-05-18 11:28:37+00:00
- **Updated**: 2015-05-18 11:28:37+00:00
- **Authors**: Olaf Ronneberger, Philipp Fischer, Thomas Brox
- **Comment**: conditionally accepted at MICCAI 2015
- **Journal**: None
- **Summary**: There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .



### Joint Representation Classification for Collective Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1505.04617v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1505.04617v1)
- **Published**: 2015-05-18 13:04:04+00:00
- **Updated**: 2015-05-18 13:04:04+00:00
- **Authors**: Liping Wang, Songcan Chen
- **Comment**: 20 pages, 5 figures, 3 tables; 4 algorithms, 2 lemmas and 2 theorems
- **Journal**: None
- **Summary**: Sparse representation based classification (SRC) is popularly used in many applications such as face recognition, and implemented in two steps: representation coding and classification. For a given set of testing images, SRC codes every image over the base images as a sparse representation then classifies it to the class with the least representation error. This scheme utilizes an individual representation rather than the collective one to classify such a set of images, doing so obviously ignores the correlation among the given images. In this paper, a joint representation classification (JRC) for collective face recognition is proposed. JRC takes the correlation of multiple images as well as a single representation into account. Under the assumption that the given face images are generally related to each other, JRC codes all the testing images over the base images simultaneously to facilitate recognition. To this end, the testing inputs are aligned into a matrix and the joint representation coding is formulated to a generalized $l_{2,q}-l_{2,p}$-minimization problem. To uniformly solve the induced optimization problems for any $q\in[1,2]$ and $p\in (0,2]$, an iterative quadratic method (IQM) is developed. IQM is proved to be a strict descent algorithm with convergence to the optimal solution. Moreover, a more practical IQM is proposed for large-scale case. Experimental results on three public databases show that the JRC with practical IQM no only saves much computational cost but also achieves better performance in collective face recognition than the state-of-the-arts.



### Predicting Important Objects for Egocentric Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1505.04803v1
- **DOI**: 10.1007/s11263-014-0794-5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04803v1)
- **Published**: 2015-05-18 20:07:20+00:00
- **Updated**: 2015-05-18 20:07:20+00:00
- **Authors**: Yong Jae Lee, Kristen Grauman
- **Comment**: Published in the International Journal of Computer Vision (IJCV),
  January 2015
- **Journal**: None
- **Summary**: We present a video summarization approach for egocentric or "wearable" camera data. Given hours of video, the proposed method produces a compact storyboard summary of the camera wearer's day. In contrast to traditional keyframe selection techniques, the resulting summary focuses on the most important objects and people with which the camera wearer interacts. To accomplish this, we develop region cues indicative of high-level saliency in egocentric video---such as the nearness to hands, gaze, and frequency of occurrence---and learn a regressor to predict the relative importance of any new region based on these cues. Using these predictions and a simple form of temporal event detection, our method selects frames for the storyboard that reflect the key object-driven happenings. We adjust the compactness of the final summary given either an importance selection criterion or a length budget; for the latter, we design an efficient dynamic programming solution that accounts for importance, visual uniqueness, and temporal displacement. Critically, the approach is neither camera-wearer-specific nor object-specific; that means the learned importance metric need not be trained for a given user or context, and it can predict the importance of objects and people that have never been seen previously. Our results on two egocentric video datasets show the method's promise relative to existing techniques for saliency and summarization.



