# Arxiv Papers in cs.CV on 2015-05-02
### Dense Optical Flow Prediction from a Static Image
- **Arxiv ID**: http://arxiv.org/abs/1505.00295v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.00295v2)
- **Published**: 2015-05-02 00:02:32+00:00
- **Updated**: 2015-12-17 05:00:47+00:00
- **Authors**: Jacob Walker, Abhinav Gupta, Martial Hebert
- **Comment**: None
- **Journal**: None
- **Summary**: Given a scene, what is going to move, and in what direction will it move? Such a question could be considered a non-semantic form of action prediction. In this work, we present a convolutional neural network (CNN) based approach for motion prediction. Given a static image, this CNN predicts the future motion of each and every pixel in the image in terms of optical flow. Our CNN model leverages the data in tens of thousands of realistic videos to train our model. Our method relies on absolutely no human labeling and is able to predict motion based on the context of the scene. Because our CNN model makes no assumptions about the underlying scene, it can predict future optical flow on a diverse set of scenarios. We outperform all previous approaches by large margins.



### Object-Scene Convolutional Neural Networks for Event Recognition in Images
- **Arxiv ID**: http://arxiv.org/abs/1505.00296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.00296v1)
- **Published**: 2015-05-02 01:26:42+00:00
- **Updated**: 2015-05-02 01:26:42+00:00
- **Authors**: Limin Wang, Zhe Wang, Wenbin Du, Yu Qiao
- **Comment**: CVPR, ChaLearn Looking at People (LAP) challenge, 2015
- **Journal**: None
- **Summary**: Event recognition from still images is of great importance for image understanding. However, compared with event recognition in videos, there are much fewer research works on event recognition in images. This paper addresses the issue of event recognition from images and proposes an effective method with deep neural networks. Specifically, we design a new architecture, called Object-Scene Convolutional Neural Network (OS-CNN). This architecture is decomposed into object net and scene net, which extract useful information for event understanding from the perspective of objects and scene context, respectively. Meanwhile, we investigate different network architectures for OS-CNN design, and adapt the deep (AlexNet) and very-deep (GoogLeNet) networks to the task of event recognition. Furthermore, we find that the deep and very-deep networks are complementary to each other. Finally, based on the proposed OS-CNN and comparative study of different network architectures, we come up with a solution of five-stream CNN for the track of cultural event recognition at the ChaLearn Looking at People (LAP) challenge 2015. Our method obtains the performance of 85.5% and ranks the $1^{st}$ place in this challenge.



### Multi-Object Classification and Unsupervised Scene Understanding Using Deep Learning Features and Latent Tree Probabilistic Models
- **Arxiv ID**: http://arxiv.org/abs/1505.00308v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1505.00308v1)
- **Published**: 2015-05-02 03:23:46+00:00
- **Updated**: 2015-05-02 03:23:46+00:00
- **Authors**: Tejaswi Nimmagadda, Anima Anandkumar
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has shown state-of-art classification performance on datasets such as ImageNet, which contain a single object in each image. However, multi-object classification is far more challenging. We present a unified framework which leverages the strengths of multiple machine learning methods, viz deep learning, probabilistic models and kernel methods to obtain state-of-art performance on Microsoft COCO, consisting of non-iconic images. We incorporate contextual information in natural images through a conditional latent tree probabilistic model (CLTM), where the object co-occurrences are conditioned on the extracted fc7 features from pre-trained Imagenet CNN as input. We learn the CLTM tree structure using conditional pairwise probabilities for object co-occurrences, estimated through kernel methods, and we learn its node and edge potentials by training a new 3-layer neural network, which takes fc7 features as input. Object classification is carried out via inference on the learnt conditional tree model, and we obtain significant gain in precision-recall and F-measures on MS-COCO, especially for difficult object categories. Moreover, the latent variables in the CLTM capture scene information: the images with top activations for a latent node have common themes such as being a grasslands or a food scene, and on on. In addition, we show that a simple k-means clustering of the inferred latent nodes alone significantly improves scene classification performance on the MIT-Indoor dataset, without the need for any retraining, and without using scene labels during training. Thus, we present a unified framework for multi-object classification and unsupervised scene understanding.



### Learning Temporal Embeddings for Complex Video Analysis
- **Arxiv ID**: http://arxiv.org/abs/1505.00315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.00315v1)
- **Published**: 2015-05-02 06:43:28+00:00
- **Updated**: 2015-05-02 06:43:28+00:00
- **Authors**: Vignesh Ramanathan, Kevin Tang, Greg Mori, Li Fei-Fei
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose to learn temporal embeddings of video frames for complex video analysis. Large quantities of unlabeled video data can be easily obtained from the Internet. These videos possess the implicit weak label that they are sequences of temporally and semantically coherent images. We leverage this information to learn temporal embeddings for video frames by associating frames with the temporal context that they appear in. To do this, we propose a scheme for incorporating temporal context based on past and future frames in videos, and compare this to other contextual representations. In addition, we show how data augmentation using multi-resolution samples and hard negatives helps to significantly improve the quality of the learned embeddings. We evaluate various design decisions for learning temporal embeddings, and show that our embeddings can improve performance for multiple video tasks such as retrieval, classification, and temporal order recovery in unconstrained Internet video.



### Joint Multi-Leaf Segmentation, Alignment and Tracking from Fluorescence Plant Videos
- **Arxiv ID**: http://arxiv.org/abs/1505.00353v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.00353v2)
- **Published**: 2015-05-02 16:37:57+00:00
- **Updated**: 2017-05-09 04:07:12+00:00
- **Authors**: Xi Yin, Xiaoming Liu, Jin Chen, David M. Kramer
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel framework for fluorescence plant video processing. The plant research community is interested in the leaf-level photosynthetic analysis within a plant. A prerequisite for such analysis is to segment all leaves, estimate their structures, and track them over time. We identify this as a joint multi-leaf segmentation, alignment, and tracking problem. First, leaf segmentation and alignment are applied on the last frame of a plant video to find a number of well-aligned leaf candidates. Second, leaf tracking is applied on the remaining frames with leaf candidate transformation from the previous frame. We form two optimization problems with shared terms in their objective functions for leaf alignment and tracking respectively. A quantitative evaluation framework is formulated to evaluate the performance of our algorithm with four metrics. Two models are learned to predict the alignment accuracy and detect tracking failure respectively in order to provide guidance for subsequent plant biology analysis. The limitation of our algorithm is also studied. Experimental results show the effectiveness, efficiency, and robustness of the proposed method.



