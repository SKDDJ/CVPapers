# Arxiv Papers in cs.CV on 2015-05-19
### Multi-Image Matching via Fast Alternating Minimization
- **Arxiv ID**: http://arxiv.org/abs/1505.04845v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04845v2)
- **Published**: 2015-05-19 01:14:26+00:00
- **Updated**: 2015-12-02 17:08:34+00:00
- **Authors**: Xiaowei Zhou, Menglong Zhu, Kostas Daniilidis
- **Comment**: In ICCV'2015
- **Journal**: None
- **Summary**: In this paper we propose a global optimization-based approach to jointly matching a set of images. The estimated correspondences simultaneously maximize pairwise feature affinities and cycle consistency across multiple images. Unlike previous convex methods relying on semidefinite programming, we formulate the problem as a low-rank matrix recovery problem and show that the desired semidefiniteness of a solution can be spontaneously fulfilled. The low-rank formulation enables us to derive a fast alternating minimization algorithm in order to handle practical problems with thousands of features. Both simulation and real experiments demonstrate that the proposed algorithm can achieve a competitive performance with an order of magnitude speedup compared to the state-of-the-art algorithm. In the end, we demonstrate the applicability of the proposed method to match the images of different object instances and as a result the potential to reconstruct category-specific object models from those images.



### Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1505.04868v1
- **DOI**: 10.1109/CVPR.2015.7299059
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04868v1)
- **Published**: 2015-05-19 04:36:42+00:00
- **Updated**: 2015-05-19 04:36:42+00:00
- **Authors**: Limin Wang, Yu Qiao, Xiaoou Tang
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2015
- **Journal**: None
- **Summary**: Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features and deep-learned features. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMDB51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features and deep-learned features. Our method also achieves superior performance to the state of the art on these datasets (HMDB51 65.9%, UCF101 91.5%).



### Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models
- **Arxiv ID**: http://arxiv.org/abs/1505.04870v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1505.04870v4)
- **Published**: 2015-05-19 04:46:03+00:00
- **Updated**: 2016-09-19 20:20:42+00:00
- **Authors**: Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, Svetlana Lazebnik
- **Comment**: None
- **Journal**: None
- **Summary**: The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. Such annotations are essential for continued progress in automatic image description and grounded language understanding. They enable us to define a new benchmark for localization of textual entity mentions in an image. We present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards selecting larger objects. While our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research.



### Have a Look at What I See
- **Arxiv ID**: http://arxiv.org/abs/1505.04873v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04873v1)
- **Published**: 2015-05-19 04:49:46+00:00
- **Updated**: 2015-05-19 04:49:46+00:00
- **Authors**: Lior Talker, Yael Moses, Ilan Shimshoni
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method for guiding a photographer to rotate her/his smartphone camera to obtain an image that overlaps with another image of the same scene. The other image is taken by another photographer from a different viewpoint. Our method is applicable even when the images do not have overlapping fields of view. Straightforward applications of our method include sharing attention to regions of interest for social purposes, or adding missing images to improve structure for motion results. Our solution uses additional images of the scene, which are often available since many people use their smartphone cameras regularly. These images may be available online from other photographers who are present at the scene. Our method avoids 3D scene reconstruction; it relies instead on a new representation that consists of the spatial orders of the scene points on two axes, x and y. This representation allows a sequence of points to be chosen efficiently and projected onto the photographers images, using epipolar point transfer. Overlaying these epipolar lines on the live preview of the camera produces a convenient interface to guide the user. The method was tested on challenging datasets of images and succeeded in guiding a photographer from one view to a non-overlapping destination view.



### Character-level Chinese Writer Identification using Path Signature Feature, DropStroke and Deep CNN
- **Arxiv ID**: http://arxiv.org/abs/1505.04922v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04922v1)
- **Published**: 2015-05-19 09:25:46+00:00
- **Updated**: 2015-05-19 09:25:46+00:00
- **Authors**: Weixin Yang, Lianwen Jin, Manfei Liu
- **Comment**: 5 pages, 4 figures, 2 tables. Manuscript is accepted to appear in
  ICDAR 2015
- **Journal**: None
- **Summary**: Most existing online writer-identification systems require that the text content is supplied in advance and rely on separately designed features and classifiers. The identifications are based on lines of text, entire paragraphs, or entire documents; however, these materials are not always available. In this paper, we introduce a path-signature feature to an end-to-end text-independent writer-identification system with a deep convolutional neural network (DCNN). Because deep models require a considerable amount of data to achieve good performance, we propose a data-augmentation method named DropStroke to enrich personal handwriting. Experiments were conducted on online handwritten Chinese characters from the CASIA-OLHWDB1.0 dataset, which consists of 3,866 classes from 420 writers. For each writer, we only used 200 samples for training and the remaining 3,666. The results reveal that the path-signature feature is useful for writer identification, and the proposed DropStroke technique enhances the generalization and significantly improves performance.



### High Performance Offline Handwritten Chinese Character Recognition Using GoogLeNet and Directional Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/1505.04925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04925v1)
- **Published**: 2015-05-19 09:32:54+00:00
- **Updated**: 2015-05-19 09:32:54+00:00
- **Authors**: Zhuoyao Zhong, Lianwen Jin, Zecheng Xie
- **Comment**: 5 pages, 4 figures, 2 tables. Manuscript is accepted to appear in
  ICDAR 2015
- **Journal**: None
- **Summary**: Just like its great success in solving many computer vision problems, the convolutional neural networks (CNN) provided new end-to-end approach to handwritten Chinese character recognition (HCCR) with very promising results in recent years. However, previous CNNs so far proposed for HCCR were neither deep enough nor slim enough. We show in this paper that, a deeper architecture can benefit HCCR a lot to achieve higher performance, meanwhile can be designed with less parameters. We also show that the traditional feature extraction methods, such as Gabor or gradient feature maps, are still useful for enhancing the performance of CNN. We design a streamlined version of GoogLeNet [13], which was original proposed for image classification in recent years with very deep architecture, for HCCR (denoted as HCCR-GoogLeNet). The HCCR-GoogLeNet we used is 19 layers deep but involves with only 7.26 million parameters. Experiments were conducted using the ICDAR 2013 offline HCCR competition dataset. It has been shown that with the proper incorporation with traditional directional feature maps, the proposed single and ensemble HCCR-GoogLeNet models achieve new state of the art recognition accuracy of 96.35% and 96.74%, respectively, outperforming previous best result with significant gap.



### Convective regularization for optical flow
- **Arxiv ID**: http://arxiv.org/abs/1505.04938v2
- **DOI**: 10.1515/9783110430394-005
- **Categories**: **math.OC**, cs.CV, 49N45, 68T45, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1505.04938v2)
- **Published**: 2015-05-19 10:02:53+00:00
- **Updated**: 2015-10-13 11:24:36+00:00
- **Authors**: José A. Iglesias, Clemens Kirisits
- **Comment**: None
- **Journal**: Variational Methods in Imaging and Geometric Control, Radon Series
  on Computational and Applied Mathematics 18, pp. 184-201, 2017
- **Summary**: We argue that the time derivative in a fixed coordinate frame may not be the most appropriate measure of time regularity of an optical flow field. Instead, for a given velocity field $v$ we consider the convective acceleration $v_t + \nabla v v$ which describes the acceleration of objects moving according to $v$. Consequently we investigate the suitability of the nonconvex functional $\|v_t + \nabla v v\|^2_{L^2}$ as a regularization term for optical flow. We demonstrate that this term acts as both a spatial and a temporal regularizer and has an intrinsic edge-preserving property. We incorporate it into a contrast invariant and time-regularized variant of the Horn-Schunck functional, prove existence of minimizers and verify experimentally that it addresses some of the problems of basic quadratic models. For the minimization we use an iterative scheme that approximates the original nonlinear problem with a sequence of linear ones. We believe that the convective acceleration may be gainfully introduced in a variety of optical flow models.



### Image Reconstruction from Bag-of-Visual-Words
- **Arxiv ID**: http://arxiv.org/abs/1505.05190v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1505.05190v1)
- **Published**: 2015-05-19 21:12:15+00:00
- **Updated**: 2015-05-19 21:12:15+00:00
- **Authors**: Hiroharu Kato, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: The objective of this work is to reconstruct an original image from Bag-of-Visual-Words (BoVW). Image reconstruction from features can be a means of identifying the characteristics of features. Additionally, it enables us to generate novel images via features. Although BoVW is the de facto standard feature for image recognition and retrieval, successful image reconstruction from BoVW has not been reported yet. What complicates this task is that BoVW lacks the spatial information for including visual words. As described in this paper, to estimate an original arrangement, we propose an evaluation function that incorporates the naturalness of local adjacency and the global position, with a method to obtain related parameters using an external image database. To evaluate the performance of our method, we reconstruct images of objects of 101 kinds. Additionally, we apply our method to analyze object classifiers and to generate novel images via BoVW.



### Unsupervised Visual Representation Learning by Context Prediction
- **Arxiv ID**: http://arxiv.org/abs/1505.05192v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05192v3)
- **Published**: 2015-05-19 21:18:17+00:00
- **Updated**: 2016-01-16 22:09:45+00:00
- **Authors**: Carl Doersch, Abhinav Gupta, Alexei A. Efros
- **Comment**: Oral paper at ICCV 2015
- **Journal**: None
- **Summary**: This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.



### Barcode Annotations for Medical Image Retrieval: A Preliminary Investigation
- **Arxiv ID**: http://arxiv.org/abs/1505.05212v1
- **DOI**: 10.1109/ICIP.2015.7350913
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.05212v1)
- **Published**: 2015-05-19 23:48:24+00:00
- **Updated**: 2015-05-19 23:48:24+00:00
- **Authors**: Hamid R. Tizhoosh
- **Comment**: To be published in proceedings of The IEEE International Conference
  on Image Processing (ICIP 2015), September 27-30, 2015, Quebec City, Canada
- **Journal**: None
- **Summary**: This paper proposes to generate and to use barcodes to annotate medical images and/or their regions of interest such as organs, tumors and tissue types. A multitude of efficient feature-based image retrieval methods already exist that can assign a query image to a certain image class. Visual annotations may help to increase the retrieval accuracy if combined with existing feature-based classification paradigms. Whereas with annotations we usually mean textual descriptions, in this paper barcode annotations are proposed. In particular, Radon barcodes (RBC) are introduced. As well, local binary patterns (LBP) and local Radon binary patterns (LRBP) are implemented as barcodes. The IRMA x-ray dataset with 12,677 training images and 1,733 test images is used to verify how barcodes could facilitate image retrieval.



