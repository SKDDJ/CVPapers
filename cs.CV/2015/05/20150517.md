# Arxiv Papers in cs.CV on 2015-05-17
### Salient Structure Detection by Context-Guided Visual Search
- **Arxiv ID**: http://arxiv.org/abs/1505.04364v1
- **DOI**: 10.1109/TIP.2016.2572600
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04364v1)
- **Published**: 2015-05-17 07:15:25+00:00
- **Updated**: 2015-05-17 07:15:25+00:00
- **Authors**: Kai-Fu Yang, Hui Li, Chao-Yi Li, Yong-Jie Li
- **Comment**: 13 pages, 15 figures
- **Journal**: IEEE Transactions on Image Processing (TIP), 2016
- **Summary**: We define the task of salient structure (SS) detection to unify the saliency-related tasks like fixation prediction, salient object detection, and other detection of structures of interest. In this study, we propose a unified framework for SS detection by modeling the two-pathway-based guided search strategy of biological vision. Firstly, context-based spatial prior (CBSP) is extracted based on the layout of edges in the given scene along a fast visual pathway, called non-selective pathway. This is a rough and non-selective estimation of the locations where the potential SSs present. Secondly, another flow of local feature extraction is executed in parallel along the selective pathway. Finally, Bayesian inference is used to integrate local cues guided by CBSP, and to predict the exact locations of SSs in the input scene. The proposed model is invariant to size and features of objects. Experimental results on four datasets (two fixation prediction datasets and two salient object datasets) demonstrate that our system achieves competitive performance for SS detection (i.e., both the tasks of fixation prediction and salient object detection) comparing to the state-of-the-art methods.



### Learning Deconvolution Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1505.04366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04366v1)
- **Published**: 2015-05-17 07:33:28+00:00
- **Updated**: 2015-05-17 07:33:28+00:00
- **Authors**: Hyeonwoo Noh, Seunghoon Hong, Bohyung Han
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained with no external data through ensemble with the fully convolutional network.



### Evolutionary Cost-sensitive Extreme Learning Machine
- **Arxiv ID**: http://arxiv.org/abs/1505.04373v2
- **DOI**: 10.1109/TNNLS.2016.2607757
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04373v2)
- **Published**: 2015-05-17 09:04:47+00:00
- **Updated**: 2016-09-22 09:55:31+00:00
- **Authors**: Lei Zhang, David Zhang
- **Comment**: This paper has been accepted for publication in IEEE Transactions on
  Neural Networks and Learning Systems
- **Journal**: None
- **Summary**: Conventional extreme learning machines solve a Moore-Penrose generalized inverse of hidden layer activated matrix and analytically determine the output weights to achieve generalized performance, by assuming the same loss from different types of misclassification. The assumption may not hold in cost-sensitive recognition tasks, such as face recognition based access control system, where misclassifying a stranger as a family member may result in more serious disaster than misclassifying a family member as a stranger. Though recent cost-sensitive learning can reduce the total loss with a given cost matrix that quantifies how severe one type of mistake against another, in many realistic cases the cost matrix is unknown to users. Motivated by these concerns, this paper proposes an evolutionary cost-sensitive extreme learning machine (ECSELM), with the following merits: 1) to our best knowledge, it is the first proposal of ELM in evolutionary cost-sensitive classification scenario; 2) it well addresses the open issue of how to define the cost matrix in cost-sensitive learning tasks; 3) an evolutionary backtracking search algorithm is induced for adaptive cost matrix optimization. Experiments in a variety of cost-sensitive tasks well demonstrate the effectiveness of the proposed approaches, with about 5%~10% improvements.



### Robust Visual Knowledge Transfer via EDA
- **Arxiv ID**: http://arxiv.org/abs/1505.04382v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04382v2)
- **Published**: 2015-05-17 11:23:12+00:00
- **Updated**: 2016-08-09 07:22:34+00:00
- **Authors**: Lei Zhang, David Zhang
- **Comment**: This paper has been accepted for publication in IEEE Transactions on
  Image Processing
- **Journal**: None
- **Summary**: We address the problem of visual knowledge adaptation by leveraging labeled patterns from source domain and a very limited number of labeled instances in target domain to learn a robust classifier for visual categorization. This paper proposes a new extreme learning machine based cross-domain network learning framework, that is called Extreme Learning Machine (ELM) based Domain Adaptation (EDA). It allows us to learn a category transformation and an ELM classifier with random projection by minimizing the l_(2,1)-norm of the network output weights and the learning error simultaneously. The unlabeled target data, as useful knowledge, is also integrated as a fidelity term to guarantee the stability during cross domain learning. It minimizes the matching error between the learned classifier and a base classifier, such that many existing classifiers can be readily incorporated as base classifiers. The network output weights cannot only be analytically determined, but also transferrable. Additionally, a manifold regularization with Laplacian graph is incorporated, such that it is beneficial to semi-supervised learning. Extensively, we also propose a model of multiple views, referred as MvEDA. Experiments on benchmark visual datasets for video event recognition and object recognition, demonstrate that our EDA methods outperform existing cross-domain learning methods.



### Improved Microaneurysm Detection using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1505.04424v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1505.04424v2)
- **Published**: 2015-05-17 17:37:14+00:00
- **Updated**: 2016-07-17 10:07:40+00:00
- **Authors**: Mrinal Haloi
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a novel microaneurysm (MA) detection for early diabetic retinopathy screening using color fundus images. Since MA usually the first lesions to appear as an indicator of diabetic retinopathy, accurate detection of MA is necessary for treatment. Each pixel of the image is classified as either MA or non-MA using a deep neural network with dropout training procedure using maxout activation function. No preprocessing step or manual feature extraction is required. Substantial improvements over standard MA detection method based on the pipeline of preprocessing, feature extraction, classification followed by post processing is achieved. The presented method is evaluated in publicly available Retinopathy Online Challenge (ROC) and Diaretdb1v2 database and achieved state-of-the-art accuracy.



### The Best of Both Worlds: Combining Data-independent and Data-driven Approaches for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1505.04427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04427v1)
- **Published**: 2015-05-17 17:54:38+00:00
- **Updated**: 2015-05-17 17:54:38+00:00
- **Authors**: Zhenzhong Lan, Dezhong Yao, Ming Lin, Shoou-I Yu, Alexander Hauptmann
- **Comment**: None
- **Journal**: None
- **Summary**: Motivated by the success of data-driven convolutional neural networks (CNNs) in object recognition on static images, researchers are working hard towards developing CNN equivalents for learning video features. However, learning video features globally has proven to be quite a challenge due to its high dimensionality, the lack of labelled data and the difficulty in processing large-scale video data. Therefore, we propose to leverage effective techniques from both data-driven and data-independent approaches to improve action recognition system.   Our contribution is three-fold. First, we propose a two-stream Stacked Convolutional Independent Subspace Analysis (ConvISA) architecture to show that unsupervised learning methods can significantly boost the performance of traditional local features extracted from data-independent models. Second, we demonstrate that by learning on video volumes detected by Improved Dense Trajectory (IDT), we can seamlessly combine our novel local descriptors with hand-crafted descriptors. Thus we can utilize available feature enhancing techniques developed for hand-crafted descriptors. Finally, similar to multi-class classification framework in CNNs, we propose a training-free re-ranking technique that exploits the relationship among action classes to improve the overall performance. Our experimental results on four benchmark action recognition datasets show significantly improved performance.



### Exploring Nearest Neighbor Approaches for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1505.04467v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04467v1)
- **Published**: 2015-05-17 22:14:27+00:00
- **Updated**: 2015-05-17 22:14:27+00:00
- **Authors**: Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick
- **Comment**: None
- **Journal**: None
- **Summary**: We explore a variety of nearest neighbor baseline approaches for image captioning. These approaches find a set of nearest neighbor images in the training set from which a caption may be borrowed for the query image. We select a caption for the query image by finding the caption that best represents the "consensus" of the set of candidate captions gathered from the nearest neighbor images. When measured by automatic evaluation metrics on the MS COCO caption evaluation server, these approaches perform as well as many recent approaches that generate novel captions. However, human studies show that a method that generates novel captions is still preferred over the nearest neighbor approach.



### Visual Semantic Role Labeling
- **Arxiv ID**: http://arxiv.org/abs/1505.04474v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.04474v1)
- **Published**: 2015-05-17 23:21:35+00:00
- **Updated**: 2015-05-17 23:21:35+00:00
- **Authors**: Saurabh Gupta, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce the problem of Visual Semantic Role Labeling: given an image we want to detect people doing actions and localize the objects of interaction. Classical approaches to action recognition either study the task of action classification at the image or video clip level or at best produce a bounding box around the person doing the action. We believe such an output is inadequate and a complete understanding can only come when we are able to associate objects in the scene to the different semantic roles of the action. To enable progress towards this goal, we annotate a dataset of 16K people instances in 10K images with actions they are doing and associate objects in the scene with different semantic roles for each action. Finally, we provide a set of baseline algorithms for this task and analyze error modes providing directions for future work.



