# Arxiv Papers in cs.CV on 2015-05-27
### Inner and Inter Label Propagation: Salient Object Detection in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1505.07192v1
- **DOI**: 10.1109/TIP.2015.2440174
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.07192v1)
- **Published**: 2015-05-27 05:24:03+00:00
- **Updated**: 2015-05-27 05:24:03+00:00
- **Authors**: Hongyang Li, Huchuan Lu, Zhe Lin, Xiaohui Shen, Brian Price
- **Comment**: The full version of the TIP 2015 publication
- **Journal**: None
- **Summary**: In this paper, we propose a novel label propagation based method for saliency detection. A key observation is that saliency in an image can be estimated by propagating the labels extracted from the most certain background and object regions. For most natural images, some boundary superpixels serve as the background labels and the saliency of other superpixels are determined by ranking their similarities to the boundary labels based on an inner propagation scheme. For images of complex scenes, we further deploy a 3-cue-center-biased objectness measure to pick out and propagate foreground labels. A co-transduction algorithm is devised to fuse both boundary and objectness labels based on an inter propagation scheme. The compactness criterion decides whether the incorporation of objectness labels is necessary, thus greatly enhancing computational efficiency. Results on five benchmark datasets with pixel-wise accurate annotations show that the proposed method achieves superior performance compared with the newest state-of-the-arts in terms of different evaluation metrics.



### New characterizations of minimum spanning trees and of saliency maps based on quasi-flat zones
- **Arxiv ID**: http://arxiv.org/abs/1505.07203v1
- **DOI**: 10.1007/978-3-319-18720-4_18
- **Categories**: **cs.CV**, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1505.07203v1)
- **Published**: 2015-05-27 06:36:10+00:00
- **Updated**: 2015-05-27 06:36:10+00:00
- **Authors**: Jean Cousty, Laurent Najman, Yukiko Kenmochi, Silvio GuimarÃ£es
- **Comment**: None
- **Journal**: 12th International Symposium on Mathematical Morphology (ISMM),
  May 2015, Reykjavik, Iceland. Lecture Notes in Computer Science (LNCS), 9082,
  pp.205-216, Mathematical Morphology and Its Applications to Signal and Image
  Processing
- **Summary**: We study three representations of hierarchies of partitions: dendrograms (direct representations), saliency maps, and minimum spanning trees. We provide a new bijection between saliency maps and hierarchies based on quasi-flat zones as used in image processing and characterize saliency maps and minimum spanning trees as solutions to constrained minimization problems where the constraint is quasi-flat zones preservation. In practice, these results form a toolkit for new hierarchical methods where one can choose the most convenient representation. They also invite us to process non-image data with morphological hierarchies.



### SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling
- **Arxiv ID**: http://arxiv.org/abs/1505.07293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.07293v1)
- **Published**: 2015-05-27 12:54:17+00:00
- **Updated**: 2015-05-27 12:54:17+00:00
- **Authors**: Vijay Badrinarayanan, Ankur Handa, Roberto Cipolla
- **Comment**: This version was first submitted to CVPR' 15 on November 14, 2014
  with paper Id 1468. A similar architecture was proposed more recently on May
  17, 2015, see http://arxiv.org/pdf/1505.04366.pdf
- **Journal**: None
- **Summary**: We propose a novel deep architecture, SegNet, for semantic pixel wise image labelling. SegNet has several attractive properties; (i) it only requires forward evaluation of a fully learnt function to obtain smooth label predictions, (ii) with increasing depth, a larger context is considered for pixel labelling which improves accuracy, and (iii) it is easy to visualise the effect of feature activation(s) in the pixel label space at any depth. SegNet is composed of a stack of encoders followed by a corresponding decoder stack which feeds into a soft-max classification layer. The decoders help map low resolution feature maps at the output of the encoder stack to full input image size feature maps. This addresses an important drawback of recent deep learning approaches which have adopted networks designed for object categorization for pixel wise labelling. These methods lack a mechanism to map deep layer feature maps to input dimensions. They resort to ad hoc methods to upsample features, e.g. by replication. This results in noisy predictions and also restricts the number of pooling layers in order to avoid too much upsampling and thus reduces spatial context. SegNet overcomes these problems by learning to map encoder outputs to image pixel labels. We test the performance of SegNet on outdoor RGB scenes from CamVid, KITTI and indoor scenes from the NYU dataset. Our results show that SegNet achieves state-of-the-art performance even without use of additional cues such as depth, video frames or post-processing with CRF models.



### Texture Synthesis Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1505.07376v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1505.07376v3)
- **Published**: 2015-05-27 15:29:52+00:00
- **Updated**: 2015-11-06 13:55:09+00:00
- **Authors**: Leon A. Gatys, Alexander S. Ecker, Matthias Bethge
- **Comment**: Revision for NIPS 2015 Camera Ready. In line with reviewer's comments
  we now focus on the texture model and texture synthesis performance. We limit
  the relationship of our texture model to the ventral stream and its potential
  use for neuroscience to the discussion of the paper
- **Journal**: None
- **Summary**: Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.



### Improving Spatial Codification in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1505.07409v1
- **DOI**: 10.1109/ICIP.2015.7351476
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1505.07409v1)
- **Published**: 2015-05-27 17:26:39+00:00
- **Updated**: 2015-05-27 17:26:39+00:00
- **Authors**: Carles Ventura, Xavier Giró-i-Nieto, Verónica Vilaplana, Kevin McGuinness, Ferran Marqués, Noel E. O'Connor
- **Comment**: Paper accepted at the IEEE International Conference on Image
  Processing, ICIP 2015. Quebec City, 27-30 September. Project page:
  https://imatge.upc.edu/web/publications/improving-spatial-codification-semantic-segmentation
- **Journal**: None
- **Summary**: This paper explores novel approaches for improving the spatial codification for the pooling of local descriptors to solve the semantic segmentation problem. We propose to partition the image into three regions for each object to be described: Figure, Border and Ground. This partition aims at minimizing the influence of the image context on the object description and vice versa by introducing an intermediate zone around the object contour. Furthermore, we also propose a richer visual descriptor of the object by applying a Spatial Pyramid over the Figure region. Two novel Spatial Pyramid configurations are explored: Cartesian-based and crown-based Spatial Pyramids. We test these approaches with state-of-the-art techniques and show that they improve the Figure-Ground based pooling in the Pascal VOC 2011 and 2012 semantic segmentation challenges.



### PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization
- **Arxiv ID**: http://arxiv.org/abs/1505.07427v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1505.07427v4)
- **Published**: 2015-05-27 18:18:42+00:00
- **Updated**: 2016-02-18 13:52:18+00:00
- **Authors**: Alex Kendall, Matthew Grimes, Roberto Cipolla
- **Comment**: 9 pages, 13 figures; Corrected numerical error in orientation results
- **Journal**: None
- **Summary**: We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 6 degree accuracy for large scale outdoor scenes and 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples. PoseNet code, dataset and an online demonstration is available on our project webpage, at http://mi.eng.cam.ac.uk/projects/relocalisation/



### Training a Convolutional Neural Network for Appearance-Invariant Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1505.07428v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1505.07428v1)
- **Published**: 2015-05-27 18:21:54+00:00
- **Updated**: 2015-05-27 18:21:54+00:00
- **Authors**: Ruben Gomez-Ojeda, Manuel Lopez-Antequera, Nicolai Petkov, Javier Gonzalez-Jimenez
- **Comment**: None
- **Journal**: None
- **Summary**: Place recognition is one of the most challenging problems in computer vision, and has become a key part in mobile robotics and autonomous driving applications for performing loop closure in visual SLAM systems. Moreover, the difficulty of recognizing a revisited location increases with appearance changes caused, for instance, by weather or illumination variations, which hinders the long-term application of such algorithms in real environments. In this paper we present a convolutional neural network (CNN), trained for the first time with the purpose of recognizing revisited locations under severe appearance changes, which maps images to a low dimensional space where Euclidean distances represent place dissimilarity. In order for the network to learn the desired invariances, we train it with triplets of images selected from datasets which present a challenging variability in visual appearance. The triplets are selected in such way that two samples are from the same location and the third one is taken from a different place. We validate our system through extensive experimentation, where we demonstrate better performance than state-of-art algorithms in a number of popular datasets.



