# Arxiv Papers in cs.CV on 2015-09-01
### Discovery Radiomics for Multi-Parametric MRI Prostate Cancer Detection
- **Arxiv ID**: http://arxiv.org/abs/1509.00111v3
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1509.00111v3)
- **Published**: 2015-09-01 01:40:23+00:00
- **Updated**: 2015-10-20 01:49:05+00:00
- **Authors**: Audrey G. Chung, Mohammad Javad Shafiee, Devinder Kumar, Farzad Khalvati, Masoom A. Haider, Alexander Wong
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Prostate cancer is the most diagnosed form of cancer in Canadian men, and is the third leading cause of cancer death. Despite these statistics, prognosis is relatively good with a sufficiently early diagnosis, making fast and reliable prostate cancer detection crucial. As imaging-based prostate cancer screening, such as magnetic resonance imaging (MRI), requires an experienced medical professional to extensively review the data and perform a diagnosis, radiomics-driven methods help streamline the process and has the potential to significantly improve diagnostic accuracy and efficiency, and thus improving patient survival rates. These radiomics-driven methods currently rely on hand-crafted sets of quantitative imaging-based features, which are selected manually and can limit their ability to fully characterize unique prostate cancer tumour phenotype. In this study, we propose a novel \textit{discovery radiomics} framework for generating custom radiomic sequences tailored for prostate cancer detection. Discovery radiomics aims to uncover abstract imaging-based features that capture highly unique tumour traits and characteristics beyond what can be captured using predefined feature models. In this paper, we discover new custom radiomic sequencers for generating new prostate radiomic sequences using multi-parametric MRI data. We evaluated the performance of the discovered radiomic sequencer against a state-of-the-art hand-crafted radiomic sequencer for computer-aided prostate cancer detection with a feedforward neural network using real clinical prostate multi-parametric MRI data. Results for the discovered radiomic sequencer demonstrate good performance in prostate cancer detection and clinical decision support relative to the hand-crafted radiomic sequencer. The use of discovery radiomics shows potential for more efficient and reliable automatic prostate cancer detection.



### FlatCam: Thin, Bare-Sensor Cameras using Coded Aperture and Computation
- **Arxiv ID**: http://arxiv.org/abs/1509.00116v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.00116v2)
- **Published**: 2015-09-01 01:59:59+00:00
- **Updated**: 2016-01-27 20:09:57+00:00
- **Authors**: M. Salman Asif, Ali Ayremlou, Aswin Sankaranarayanan, Ashok Veeraraghavan, Richard Baraniuk
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: FlatCam is a thin form-factor lensless camera that consists of a coded mask placed on top of a bare, conventional sensor array. Unlike a traditional, lens-based camera where an image of the scene is directly recorded on the sensor pixels, each pixel in FlatCam records a linear combination of light from multiple scene elements. A computational algorithm is then used to demultiplex the recorded measurements and reconstruct an image of the scene. FlatCam is an instance of a coded aperture imaging system; however, unlike the vast majority of related work, we place the coded mask extremely close to the image sensor that can enable a thin system. We employ a separable mask to ensure that both calibration and image reconstruction are scalable in terms of memory requirements and computational complexity. We demonstrate the potential of the FlatCam design using two prototypes: one at visible wavelengths and one at infrared wavelengths.



### Discovery Radiomics for Pathologically-Proven Computed Tomography Lung Cancer Prediction
- **Arxiv ID**: http://arxiv.org/abs/1509.00117v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.00117v3)
- **Published**: 2015-09-01 02:00:56+00:00
- **Updated**: 2017-03-28 02:01:31+00:00
- **Authors**: Devinder Kumar, Mohammad Javad Shafiee, Audrey G. Chung, Farzad Khalvati, Masoom A. Haider, Alexander Wong
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Lung cancer is the leading cause for cancer related deaths. As such, there is an urgent need for a streamlined process that can allow radiologists to provide diagnosis with greater efficiency and accuracy. A powerful tool to do this is radiomics: a high-dimension imaging feature set. In this study, we take the idea of radiomics one step further by introducing the concept of discovery radiomics for lung cancer prediction using CT imaging data. In this study, we realize these custom radiomic sequencers as deep convolutional sequencers using a deep convolutional neural network learning architecture. To illustrate the prognostic power and effectiveness of the radiomic sequences produced by the discovered sequencer, we perform cancer prediction between malignant and benign lesions from 97 patients using the pathologically-proven diagnostic data from the LIDC-IDRI dataset. Using the clinically provided pathologically-proven data as ground truth, the proposed framework provided an average accuracy of 77.52% via 10-fold cross-validation with a sensitivity of 79.06% and specificity of 76.11%, surpassing the state-of-the art method.



### Learning A Task-Specific Deep Architecture For Clustering
- **Arxiv ID**: http://arxiv.org/abs/1509.00151v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1509.00151v3)
- **Published**: 2015-09-01 06:12:29+00:00
- **Updated**: 2015-10-16 06:38:37+00:00
- **Authors**: Zhangyang Wang, Shiyu Chang, Jiayu Zhou, Meng Wang, Thomas S. Huang
- **Comment**: None
- **Journal**: None
- **Summary**: While sparse coding-based clustering methods have shown to be successful, their bottlenecks in both efficiency and scalability limit the practical usage. In recent years, deep learning has been proved to be a highly effective, efficient and scalable feature learning tool. In this paper, we propose to emulate the sparse coding-based clustering pipeline in the context of deep learning, leading to a carefully crafted deep model benefiting from both. A feed-forward network structure, named TAGnet, is constructed based on a graph-regularized sparse coding algorithm. It is then trained with task-specific loss functions from end to end. We discover that connecting deep learning to sparse coding benefits not only the model performance, but also its initialization and interpretation. Moreover, by introducing auxiliary clustering tasks to the intermediate feature hierarchy, we formulate DTAGnet and obtain a further performance boost. Extensive experiments demonstrate that the proposed model gains remarkable margins over several state-of-the-art methods.



### Tumor Motion Tracking in Liver Ultrasound Images Using Mean Shift and Active Contour
- **Arxiv ID**: http://arxiv.org/abs/1509.00154v4
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1509.00154v4)
- **Published**: 2015-09-01 06:21:44+00:00
- **Updated**: 2015-10-14 23:15:38+00:00
- **Authors**: Jalil Rasekhi
- **Comment**: This paper has been withdrawn by the author due to a crucial sign
  error in equation 5,6
- **Journal**: None
- **Summary**: In this paper we present a new method for motion tracking of tumors in liver ultrasound image sequences. Our algorithm has two main steps. In the first step, we apply mean shift algorithm with multiple features to estimate the center of the target in each frame. Target in the first frame is defined using an ellipse. Edge, texture, and intensity features are extracted from the first frame, and then mean shift algorithm is applied to each feature separately to find the center of ellipse related to that feature in the next frame. The center of ellipse will be the weighted average of these centers. By using mean shift actually we estimate the target movement between two consecutive frames. Once the correct ellipsoid in each frame is known, in the second step we apply the Dynamic Directional Gradient Vector Flow (DDGVF) version of active contour models, in order to find the correct boundary of tumors. We sample a few points on the boundary of active contour then translate those points based on the translation of the center of ellipsoid in two consecutive frames to determine the target movement. We use these translated sample points as an initial guess for active contour in the next frame. Our experimental results show that, the suggested method provides a reliable performance for liver tumor tracking in ultrasound image sequences.



### Robust Face Recognition via Multimodal Deep Face Representation
- **Arxiv ID**: http://arxiv.org/abs/1509.00244v1
- **DOI**: 10.1109/TMM.2015.2477042
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.00244v1)
- **Published**: 2015-09-01 12:09:06+00:00
- **Updated**: 2015-09-01 12:09:06+00:00
- **Authors**: Changxing Ding, Dacheng Tao
- **Comment**: To appear in IEEE Trans. Multimedia
- **Journal**: None
- **Summary**: Face images appeared in multimedia applications, e.g., social networks and digital entertainment, usually exhibit dramatic pose, illumination, and expression variations, resulting in considerable performance degradation for traditional face recognition algorithms. This paper proposes a comprehensive deep learning framework to jointly learn face representation using multimodal information. The proposed deep learning structure is composed of a set of elaborately designed convolutional neural networks (CNNs) and a three-layer stacked auto-encoder (SAE). The set of CNNs extracts complementary facial features from multimodal data. Then, the extracted features are concatenated to form a high-dimensional feature vector, whose dimension is compressed by SAE. All the CNNs are trained using a subset of 9,000 subjects from the publicly available CASIA-WebFace database, which ensures the reproducibility of this work. Using the proposed single CNN architecture and limited training data, 98.43% verification rate is achieved on the LFW database. Benefited from the complementary information contained in multimodal data, our small ensemble system achieves higher than 99.0% recognition rate on LFW using publicly available training set.



### Fast Randomized Singular Value Thresholding for Low-rank Optimization
- **Arxiv ID**: http://arxiv.org/abs/1509.00296v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1509.00296v2)
- **Published**: 2015-09-01 13:49:11+00:00
- **Updated**: 2016-08-22 17:43:05+00:00
- **Authors**: Tae-Hyun Oh, Yasuyuki Matsushita, Yu-Wing Tai, In So Kweon
- **Comment**: Appeared in CVPR 2015, and under major revision of TPAMI. Source code
  is available on http://thoh.kaist.ac.kr
- **Journal**: None
- **Summary**: Rank minimization can be converted into tractable surrogate problems, such as Nuclear Norm Minimization (NNM) and Weighted NNM (WNNM). The problems related to NNM, or WNNM, can be solved iteratively by applying a closed-form proximal operator, called Singular Value Thresholding (SVT), or Weighted SVT, but they suffer from high computational cost of Singular Value Decomposition (SVD) at each iteration. We propose a fast and accurate approximation method for SVT, that we call fast randomized SVT (FRSVT), with which we avoid direct computation of SVD. The key idea is to extract an approximate basis for the range of the matrix from its compressed matrix. Given the basis, we compute partial singular values of the original matrix from the small factored matrix. In addition, by developping a range propagation method, our method further speeds up the extraction of approximate basis at each iteration. Our theoretical analysis shows the relationship between the approximation bound of SVD and its effect to NNM via SVT. Along with the analysis, our empirical results quantitatively and qualitatively show that our approximation rarely harms the convergence of the host algorithms. We assess the efficiency and accuracy of the proposed method on various computer vision problems, e.g., subspace clustering, weather artifact removal, and simultaneous multi-image alignment and rectification.



### Iterative hypothesis testing for multi-object tracking in presence of features with variable reliability
- **Arxiv ID**: http://arxiv.org/abs/1509.00313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.00313v1)
- **Published**: 2015-09-01 14:27:50+00:00
- **Updated**: 2015-09-01 14:27:50+00:00
- **Authors**: Amit Kumar K. C., Damien Delannay, Christophe De Vleeschouwer
- **Comment**: 21 pages, 8 figures, submitted to CVIU: Special Issue on Visual
  Tracking
- **Journal**: None
- **Summary**: This paper assumes prior detections of multiple targets at each time instant, and uses a graph-based approach to connect those detections across time, based on their position and appearance estimates. In contrast to most earlier works in the field, our framework has been designed to exploit the appearance features, even when they are only sporadically available, or affected by a non-stationary noise, along the sequence of detections. This is done by implementing an iterative hypothesis testing strategy to progressively aggregate the detections into short trajectories, named tracklets. Specifically, each iteration considers a node, named key-node, and investigates how to link this key-node with other nodes in its neighborhood, under the assumption that the target appearance is defined by the key-node appearance estimate. This is done through shortest path computation in a temporal neighborhood of the key-node. The approach is conservative in that it only aggregates the shortest paths that are sufficiently better compared to alternative paths. It is also multi-scale in that the size of the investigated neighborhood is increased proportionally to the number of detections already aggregated into the key-node. The multi-scale nature of the process and the progressive relaxation of its conservativeness makes it both computationally efficient and effective.   Experimental validations are performed extensively on a toy example, a 15 minutes long multi-view basketball dataset, and other monocular pedestrian datasets.



