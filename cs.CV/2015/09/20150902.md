# Arxiv Papers in cs.CV on 2015-09-02
### DAG-Recurrent Neural Networks For Scene Labeling
- **Arxiv ID**: http://arxiv.org/abs/1509.00552v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.00552v2)
- **Published**: 2015-09-02 03:09:23+00:00
- **Updated**: 2015-11-23 12:27:52+00:00
- **Authors**: Bing Shuai, Zhen Zuo, Gang Wang, Bing Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In image labeling, local representations for image units are usually generated from their surrounding image patches, thus long-range contextual information is not effectively encoded. In this paper, we introduce recurrent neural networks (RNNs) to address this issue. Specifically, directed acyclic graph RNNs (DAG-RNNs) are proposed to process DAG-structured images, which enables the network to model long-range semantic dependencies among image units. Our DAG-RNNs are capable of tremendously enhancing the discriminative power of local representations, which significantly benefits the local classification. Meanwhile, we propose a novel class weighting function that attends to rare classes, which phenomenally boosts the recognition accuracy for non-frequent classes. Integrating with convolution and deconvolution layers, our DAG-RNNs achieve new state-of-the-art results on the challenging SiftFlow, CamVid and Barcelona benchmarks.



### Exploring Online Ad Images Using a Deep Convolutional Neural Network Approach
- **Arxiv ID**: http://arxiv.org/abs/1509.00568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.00568v1)
- **Published**: 2015-09-02 06:18:27+00:00
- **Updated**: 2015-09-02 06:18:27+00:00
- **Authors**: Michael Fire, Jonathan Schler
- **Comment**: None
- **Journal**: None
- **Summary**: Online advertising is a huge, rapidly growing advertising market in today's world. One common form of online advertising is using image ads. A decision is made (often in real time) every time a user sees an ad, and the advertiser is eager to determine the best ad to display. Consequently, many algorithms have been developed that calculate the optimal ad to show to the current user at the present time. Typically, these algorithms focus on variations of the ad, optimizing among different properties such as background color, image size, or set of images. However, there is a more fundamental layer. Our study looks at new qualities of ads that can be determined before an ad is shown (rather than online optimization) and defines which ads are most likely to be successful.   We present a set of novel algorithms that utilize deep-learning image processing, machine learning, and graph theory to investigate online advertising and to construct prediction models which can foresee an image ad's success. We evaluated our algorithms on a dataset with over 260,000 ad images, as well as a smaller dataset specifically related to the automotive industry, and we succeeded in constructing regression models for ad image click rate prediction. The obtained results emphasize the great potential of using deep-learning algorithms to effectively and efficiently analyze image ads and to create better and more innovative online ads. Moreover, the algorithms presented in this paper can help predict ad success and can be applied to analyze other large-scale image corpora.



### Manipulated Object Proposal: A Discriminative Object Extraction and Feature Fusion Framework for First-Person Daily Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1509.00651v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.00651v3)
- **Published**: 2015-09-02 11:51:48+00:00
- **Updated**: 2016-05-31 12:26:08+00:00
- **Authors**: Changzhi Luo, Bingbing Ni, Jun Yuan, Jianfeng Wang, Shuicheng Yan, Meng Wang
- **Comment**: This paper has been withdrawn by the author due to incomplete
  experiments
- **Journal**: None
- **Summary**: Detecting and recognizing objects interacting with humans lie in the center of first-person (egocentric) daily activity recognition. However, due to noisy camera motion and frequent changes in viewpoint and scale, most of the previous egocentric action recognition methods fail to capture and model highly discriminative object features. In this work, we propose a novel pipeline for first-person daily activity recognition, aiming at more discriminative object feature representation and object-motion feature fusion. Our object feature extraction and representation pipeline is inspired by the recent success of object hypotheses and deep convolutional neural network based detection frameworks. Our key contribution is a simple yet effective manipulated object proposal generation scheme. This scheme leverages motion cues such as motion boundary and motion magnitude (in contrast, camera motion is usually considered as "noise" for most previous methods) to generate a more compact and discriminative set of object proposals, which are more closely related to the objects which are being manipulated. Then, we learn more discriminative object detectors from these manipulated object proposals based on region-based convolutional neural network (R-CNN). Meanwhile, we develop a network based feature fusion scheme which better combines object and motion features. We show in experiments that the proposed framework significantly outperforms the state-of-the-art recognition performance on a challenging first-person daily activity benchmark.



### Dictionary based Approach to Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/1509.00714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.00714v1)
- **Published**: 2015-09-02 14:09:13+00:00
- **Updated**: 2015-09-02 14:09:13+00:00
- **Authors**: Nitish Chandra, Kedar Khare
- **Comment**: None
- **Journal**: None
- **Summary**: Edge detection is a very essential part of image processing, as quality and accuracy of detection determines the success of further processing. We have developed a new self learning technique for edge detection using dictionary comprised of eigenfilters constructed using features of the input image. The dictionary based method eliminates the need of pre or post processing of the image and accounts for noise, blurriness, class of image and variation of illumination during the detection process itself. Since, this method depends on the characteristics of the image, the new technique can detect edges more accurately and capture greater detail than existing algorithms such as Sobel, Prewitt Laplacian of Gaussian, Canny method etc which use generic filters and operators. We have demonstrated its application on various classes of images such as text, face, barcodes, traffic and cell images. An application of this technique to cell counting in a microscopic image is also presented.



### On Transitive Consistency for Linear Invertible Transformations between Euclidean Coordinate Systems
- **Arxiv ID**: http://arxiv.org/abs/1509.00728v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.MA, cs.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1509.00728v1)
- **Published**: 2015-09-02 14:57:16+00:00
- **Updated**: 2015-09-02 14:57:16+00:00
- **Authors**: Johan Thunberg, Florian Bernard, Jorge Goncalves
- **Comment**: 25 pages
- **Journal**: None
- **Summary**: Transitive consistency is an intrinsic property for collections of linear invertible transformations between Euclidean coordinate frames. In practice, when the transformations are estimated from data, this property is lacking. This work addresses the problem of synchronizing transformations that are not transitively consistent. Once the transformations have been synchronized, they satisfy the transitive consistency condition - a transformation from frame $A$ to frame $C$ is equal to the composite transformation of first transforming A to B and then transforming B to C. The coordinate frames correspond to nodes in a graph and the transformations correspond to edges in the same graph. Two direct or centralized synchronization methods are presented for different graph topologies; the first one for quasi-strongly connected graphs, and the second one for connected graphs. As an extension of the second method, an iterative Gauss-Newton method is presented, which is later adapted to the case of affine and Euclidean transformations. Two distributed synchronization methods are also presented for orthogonal matrices, which can be seen as distributed versions of the two direct or centralized methods; they are similar in nature to standard consensus protocols used for distributed averaging. When the transformations are orthogonal matrices, a bound on the optimality gap can be computed. Simulations show that the gap is almost right, even for noise large in magnitude. This work also contributes on a theoretical level by providing linear algebraic relationships for transitively consistent transformations. One of the benefits of the proposed methods is their simplicity - basic linear algebraic methods are used, e.g., the Singular Value Decomposition (SVD). For a wide range of parameter settings, the methods are numerically validated.



### Depth Fields: Extending Light Field Techniques to Time-of-Flight Imaging
- **Arxiv ID**: http://arxiv.org/abs/1509.00816v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.00816v1)
- **Published**: 2015-09-02 18:49:18+00:00
- **Updated**: 2015-09-02 18:49:18+00:00
- **Authors**: Suren Jayasuriya, Adithya Pediredla, Sriram Sivaramakrishnan, Alyosha Molnar, Ashok Veeraraghavan
- **Comment**: 9 pages, 8 figures, Accepted to 3DV 2015
- **Journal**: None
- **Summary**: A variety of techniques such as light field, structured illumination, and time-of-flight (TOF) are commonly used for depth acquisition in consumer imaging, robotics and many other applications. Unfortunately, each technique suffers from its individual limitations preventing robust depth sensing. In this paper, we explore the strengths and weaknesses of combining light field and time-of-flight imaging, particularly the feasibility of an on-chip implementation as a single hybrid depth sensor. We refer to this combination as depth field imaging. Depth fields combine light field advantages such as synthetic aperture refocusing with TOF imaging advantages such as high depth resolution and coded signal processing to resolve multipath interference. We show applications including synthesizing virtual apertures for TOF imaging, improved depth mapping through partial and scattering occluders, and single frequency TOF phase unwrapping. Utilizing space, angle, and temporal coding, depth fields can improve depth sensing in the wild and generate new insights into the dimensions of light's plenoptic function.



