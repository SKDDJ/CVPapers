# Arxiv Papers in cs.CV on 2015-09-21
### Fusing Multi-Stream Deep Networks for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1509.06086v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1509.06086v2)
- **Published**: 2015-09-21 00:38:54+00:00
- **Updated**: 2015-11-11 01:29:44+00:00
- **Authors**: Zuxuan Wu, Yu-Gang Jiang, Xi Wang, Hao Ye, Xiangyang Xue, Jun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies deep network architectures to address the problem of video classification. A multi-stream framework is proposed to fully utilize the rich multimodal information in videos. Specifically, we first train three Convolutional Neural Networks to model spatial, short-term motion and audio clues respectively. Long Short Term Memory networks are then adopted to explore long-term temporal dynamics. With the outputs of the individual streams, we propose a simple and effective fusion method to generate the final predictions, where the optimal fusion weights are learned adaptively for each class, and the learning process is regularized by automatically estimated class relationships. Our contributions are two-fold. First, the proposed multi-stream framework is able to exploit multimodal features that are more comprehensive than those previously attempted. Second, we demonstrate that the adaptive fusion method using the class relationship as a regularizer outperforms traditional alternatives that estimate the weights in a "free" fashion. Our framework produces significantly better results than the state of the arts on two popular benchmarks, 92.2\% on UCF-101 (without using audio) and 84.9\% on Columbia Consumer Videos.



### Deep Spatial Autoencoders for Visuomotor Learning
- **Arxiv ID**: http://arxiv.org/abs/1509.06113v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1509.06113v3)
- **Published**: 2015-09-21 06:15:12+00:00
- **Updated**: 2016-03-01 17:24:50+00:00
- **Authors**: Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, Pieter Abbeel
- **Comment**: Published in the International Conference on Robotics and Automation
  (ICRA)
- **Journal**: None
- **Summary**: Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot's arm.



### Vision System and Depth Processing for DRC-HUBO+
- **Arxiv ID**: http://arxiv.org/abs/1509.06114v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1509.06114v2)
- **Published**: 2015-09-21 06:17:21+00:00
- **Updated**: 2015-09-28 15:05:37+00:00
- **Authors**: Inwook Shim, Seunghak Shin, Yunsu Bok, Kyungdon Joo, Dong-Geol Choi, Joon-Young Lee, Jaesik Park, Jun-Ho Oh, In So Kweon
- **Comment**: submitted in ICRA 2016
- **Journal**: None
- **Summary**: This paper presents a vision system and a depth processing algorithm for DRC-HUBO+, the winner of the DRC finals 2015. Our system is designed to reliably capture 3D information of a scene and objects robust to challenging environment conditions. We also propose a depth-map upsampling method that produces an outliers-free depth map by explicitly handling depth outliers. Our system is suitable for an interactive robot with real-world that requires accurate object detection and pose estimation. We evaluate our depth processing algorithm over state-of-the-art algorithms on several synthetic and real-world datasets.



### On 3D Face Reconstruction via Cascaded Regression in Shape Space
- **Arxiv ID**: http://arxiv.org/abs/1509.06161v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.06161v3)
- **Published**: 2015-09-21 09:29:38+00:00
- **Updated**: 2017-04-18 02:57:38+00:00
- **Authors**: Feng Liu, Dan Zeng, Jing Li, Qijun Zhao
- **Comment**: 11 pages, 11 figures
- **Journal**: None
- **Summary**: Cascaded regression has been recently applied to reconstructing 3D faces from single 2D images directly in shape space, and achieved state-of-the-art performance. This paper investigates thoroughly such cascaded regression based 3D face reconstruction approaches from four perspectives that are not well studied yet: (i) The impact of the number of 2D landmarks; (ii) the impact of the number of 3D vertices; (iii) the way of using standalone automated landmark detection methods; and (iv) the convergence property. To answer these questions, a simplified cascaded regression based 3D face reconstruction method is devised, which can be integrated with standalone automated landmark detection methods and reconstruct 3D face shapes that have the same pose and expression as the input face images, rather than normalized pose and expression. Moreover, an effective training method is proposed by disturbing the automatically detected landmarks. Comprehensive evaluation experiments have been done with comparison to other 3D face reconstruction methods. The results not only deepen the understanding of cascaded regression based 3D face reconstruction approaches, but also prove the effectiveness of proposed method.



### LEWIS: Latent Embeddings for Word Images and their Semantics
- **Arxiv ID**: http://arxiv.org/abs/1509.06243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.06243v1)
- **Published**: 2015-09-21 14:32:43+00:00
- **Updated**: 2015-09-21 14:32:43+00:00
- **Authors**: Albert Gordo, Jon Almazan, Naila Murray, Florent Perronnin
- **Comment**: Accepted for publication at the International Conference on Computer
  Vision (ICCV) 2015
- **Journal**: None
- **Summary**: The goal of this work is to bring semantics into the tasks of text recognition and retrieval in natural images. Although text recognition and retrieval have received a lot of attention in recent years, previous works have focused on recognizing or retrieving exactly the same word used as a query, without taking the semantics into consideration.   In this paper, we ask the following question: \emph{can we predict semantic concepts directly from a word image, without explicitly trying to transcribe the word image or its characters at any point?} For this goal we propose a convolutional neural network (CNN) with a weighted ranking loss objective that ensures that the concepts relevant to the query image are ranked ahead of those that are not relevant. This can also be interpreted as learning a Euclidean space where word images and concepts are jointly embedded. This model is learned in an end-to-end manner, from image pixels to semantic concepts, using a dataset of synthetically generated word images and concepts mined from a lexical database (WordNet). Our results show that, despite the complexity of the task, word images and concepts can indeed be associated with a high degree of accuracy



### Evaluating the visualization of what a Deep Neural Network has learned
- **Arxiv ID**: http://arxiv.org/abs/1509.06321v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.06321v1)
- **Published**: 2015-09-21 17:36:22+00:00
- **Updated**: 2015-09-21 17:36:22+00:00
- **Authors**: Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian Bach, Klaus-Robert Müller
- **Comment**: 13 pages, 8 Figures
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multi-layer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the ''importance'' of individual pixels wrt the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012 and MIT Places data sets. Our main result is that the recently proposed Layer-wise Relevance Propagation (LRP) algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of neural network performance.



