# Arxiv Papers in cs.CV on 2015-09-27
### Deep Trans-layer Unsupervised Networks for Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1509.08038v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1509.08038v1)
- **Published**: 2015-09-27 00:46:08+00:00
- **Updated**: 2015-09-27 00:46:08+00:00
- **Authors**: Wentao Zhu, Jun Miao, Laiyun Qing, Xilin Chen
- **Comment**: 21 pages, 3 figures
- **Journal**: None
- **Summary**: Learning features from massive unlabelled data is a vast prevalent topic for high-level tasks in many machine learning applications. The recent great improvements on benchmark data sets achieved by increasingly complex unsupervised learning methods and deep learning models with lots of parameters usually requires many tedious tricks and much expertise to tune. However, filters learned by these complex architectures are quite similar to standard hand-crafted features visually. In this paper, unsupervised learning methods, such as PCA or auto-encoder, are employed as the building block to learn filter banks at each layer. The lower layer responses are transferred to the last layer (trans-layer) to form a more complete representation retaining more information. In addition, some beneficial methods such as local contrast normalization and whitening are added to the proposed deep trans-layer networks to further boost performance. The trans-layer representations are followed by block histograms with binary encoder schema to learn translation and rotation invariant representations, which are utilized to do high-level tasks such as recognition and classification. Compared to traditional deep learning methods, the implemented feature learning method has much less parameters and is validated in several typical experiments, such as digit recognition on MNIST and MNIST variations, object recognition on Caltech 101 dataset and face verification on LFW dataset. The deep trans-layer unsupervised learning achieves 99.45% accuracy on MNIST dataset, 67.11% accuracy on 15 samples per class and 75.98% accuracy on 30 samples per class on Caltech 101 dataset, 87.10% on LFW dataset.



### Online Object Tracking, Learning and Parsing with And-Or Graphs
- **Arxiv ID**: http://arxiv.org/abs/1509.08067v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1509.08067v6)
- **Published**: 2015-09-27 08:14:57+00:00
- **Updated**: 2016-09-03 00:26:58+00:00
- **Authors**: Tianfu Wu, Yang Lu, Song-Chun Zhu
- **Comment**: 17 pages, Reproducibility: The source code is released with this
  paper for reproducing all results, which is available at
  https://github.com/tfwu/RGM-AOGTracker
- **Journal**: None
- **Summary**: This paper presents a method, called AOGTracker, for simultaneously tracking, learning and parsing (TLP) of unknown objects in video sequences with a hierarchical and compositional And-Or graph (AOG) representation. %The AOG captures both structural and appearance variations of a target object in a principled way. The TLP method is formulated in the Bayesian framework with a spatial and a temporal dynamic programming (DP) algorithms inferring object bounding boxes on-the-fly. During online learning, the AOG is discriminatively learned using latent SVM to account for appearance (e.g., lighting and partial occlusion) and structural (e.g., different poses and viewpoints) variations of a tracked object, as well as distractors (e.g., similar objects) in background. Three key issues in online inference and learning are addressed: (i) maintaining purity of positive and negative examples collected online, (ii) controling model complexity in latent structure learning, and (iii) identifying critical moments to re-learn the structure of AOG based on its intrackability. The intrackability measures uncertainty of an AOG based on its score maps in a frame. In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks, and the VOT benchmarks --- VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking). In the former, our AOGTracker outperforms state-of-the-art tracking algorithms including two trackers based on deep convolutional network. In the latter, our AOGTracker outperforms all other trackers in VOT2013 and is comparable to the state-of-the-art methods in VOT2014, 2015 and TIR2015.



### Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing
- **Arxiv ID**: http://arxiv.org/abs/1509.08075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.08075v1)
- **Published**: 2015-09-27 10:01:42+00:00
- **Updated**: 2015-09-27 10:01:42+00:00
- **Authors**: Hamid Izadinia, Fereshteh Sadeghi, Santosh Kumar Divvala, Yejin Choi, Ali Farhadi
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: We introduce Segment-Phrase Table (SPT), a large collection of bijective associations between textual phrases and their corresponding segmentations. Leveraging recent progress in object recognition and natural language semantics, we show how we can successfully build a high-quality segment-phrase table using minimal human supervision. More importantly, we demonstrate the unique value unleashed by this rich bimodal resource, for both vision as well as natural language understanding. First, we show that fine-grained textual labels facilitate contextual reasoning that helps in satisfying semantic constraints across image segments. This feature enables us to achieve state-of-the-art segmentation results on benchmark datasets. Next, we show that the association of high-quality segmentations to textual phrases aids in richer semantic understanding and reasoning of these textual phrases. Leveraging this feature, we motivate the problem of visual entailment and visual paraphrasing, and demonstrate its utility on a large dataset.



### Multivariate Median Filters and Partial Differential Equations
- **Arxiv ID**: http://arxiv.org/abs/1509.08082v2
- **DOI**: 10.1007/s10851-016-0645-9
- **Categories**: **cs.CV**, I.4.3; G.1.8
- **Links**: [PDF](http://arxiv.org/pdf/1509.08082v2)
- **Published**: 2015-09-27 11:02:31+00:00
- **Updated**: 2016-03-18 15:11:37+00:00
- **Authors**: Martin Welk
- **Comment**: v2: Minor revision; a few equations, some text, and one reference
  added; typos corrected
- **Journal**: Journal of Mathematical Imaging and Vision, 56 (2016) 320-351
- **Summary**: Multivariate median filters have been proposed as generalisations of the well-established median filter for grey-value images to multi-channel images. As multivariate median, most of the recent approaches use the $L^1$ median, i.e.\ the minimiser of an objective function that is the sum of distances to all input points. Many properties of univariate median filters generalise to such a filter. However, the famous result by Guichard and Morel about approximation of the mean curvature motion PDE by median filtering does not have a comparably simple counterpart for $L^1$ multivariate median filtering. We discuss the affine equivariant Oja median and the affine equivariant transformation--retransformation $L^1$ median as alternatives to $L^1$ median filtering. We analyse multivariate median filters in a space-continuous setting, including the formulation of a space-continuous version of the transformation--retransformation $L^1$ median, and derive PDEs approximated by these filters in the cases of bivariate planar images, three-channel volume images and three-channel planar images. The PDEs for the affine equivariant filters can be interpreted geometrically as combinations of a diffusion and a principal-component-wise curvature motion contribution with a cross-effect term based on torsions of principal components. Numerical experiments are presented that demonstrate the validity of the approximation results.



### Amodal Completion and Size Constancy in Natural Scenes
- **Arxiv ID**: http://arxiv.org/abs/1509.08147v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.08147v2)
- **Published**: 2015-09-27 21:39:42+00:00
- **Updated**: 2015-10-01 04:34:06+00:00
- **Authors**: Abhishek Kar, Shubham Tulsiani, Jo√£o Carreira, Jitendra Malik
- **Comment**: Accepted to ICCV 2015
- **Journal**: None
- **Summary**: We consider the problem of enriching current object detection systems with veridical object sizes and relative depth estimates from a single image. There are several technical challenges to this, such as occlusions, lack of calibration data and the scale ambiguity between object size and distance. These have not been addressed in full generality in previous work. Here we propose to tackle these issues by building upon advances in object recognition and using recently created large-scale datasets. We first introduce the task of amodal bounding box completion, which aims to infer the the full extent of the object instances in the image. We then propose a probabilistic framework for learning category-specific object size distributions from available annotations and leverage these in conjunction with amodal completion to infer veridical sizes in novel images. Finally, we introduce a focal length prediction approach that exploits scene recognition to overcome inherent scaling ambiguities and we demonstrate qualitative results on challenging real-world scenes.



