# Arxiv Papers in cs.CV on 2015-09-04
### Semantic Amodal Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1509.01329v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.01329v2)
- **Published**: 2015-09-04 02:20:13+00:00
- **Updated**: 2016-12-14 19:49:24+00:00
- **Authors**: Yan Zhu, Yuandong Tian, Dimitris Mexatas, Piotr Doll√°r
- **Comment**: major update including new COCO data, metrics, and baselines
- **Journal**: None
- **Summary**: Common visual recognition tasks such as classification, object detection, and semantic segmentation are rapidly reaching maturity, and given the recent rate of progress, it is not unreasonable to conjecture that techniques for many of these problems will approach human levels of performance in the next few years. In this paper we look to the future: what is the next frontier in visual recognition?   We offer one possible answer to this question. We propose a detailed image annotation that captures information beyond the visible pixels and requires complex reasoning about full scene structure. Specifically, we create an amodal segmentation of each image: the full extent of each region is marked, not just the visible pixels. Annotators outline and name all salient regions in the image and specify a partial depth order. The result is a rich scene structure, including visible and occluded portions of each region, figure-ground edge information, semantic labels, and object overlap.   We create two datasets for semantic amodal segmentation. First, we label 500 images in the BSDS dataset with multiple annotators per image, allowing us to study the statistics of human annotations. We show that the proposed full scene annotation is surprisingly consistent between annotators, including for regions and edges. Second, we annotate 5000 images from COCO. This larger dataset allows us to explore a number of algorithmic ideas for amodal segmentation and depth ordering. We introduce novel metrics for these tasks, and along with our strong baselines, define concrete new challenges for the community.



### Learning Temporal Alignment Uncertainty for Efficient Event Detection
- **Arxiv ID**: http://arxiv.org/abs/1509.01343v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.01343v1)
- **Published**: 2015-09-04 05:33:42+00:00
- **Updated**: 2015-09-04 05:33:42+00:00
- **Authors**: Iman Abbasnejad, Sridha Sridharan, Simon Denman, Clinton Fookes, Simon Lucey
- **Comment**: Appeared in DICTA 2015, 8 pages
- **Journal**: None
- **Summary**: In this paper we tackle the problem of efficient video event detection. We argue that linear detection functions should be preferred in this regard due to their scalability and efficiency during estimation and evaluation. A popular approach in this regard is to represent a sequence using a bag of words (BOW) representation due to its: (i) fixed dimensionality irrespective of the sequence length, and (ii) its ability to compactly model the statistics in the sequence. A drawback to the BOW representation, however, is the intrinsic destruction of the temporal ordering information. In this paper we propose a new representation that leverages the uncertainty in relative temporal alignments between pairs of sequences while not destroying temporal ordering. Our representation, like BOW, is of a fixed dimensionality making it easily integrated with a linear detection function. Extensive experiments on CK+, 6DMG, and UvA-NEMO databases show significant performance improvements across both isolated and continuous event detection tasks.



### CNN Based Hashing for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1509.01354v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.6; H.3.1
- **Links**: [PDF](http://arxiv.org/pdf/1509.01354v1)
- **Published**: 2015-09-04 07:08:44+00:00
- **Updated**: 2015-09-04 07:08:44+00:00
- **Authors**: Jinma Guo, Jianmin Li
- **Comment**: 16 pages, 6 figures
- **Journal**: None
- **Summary**: Along with data on the web increasing dramatically, hashing is becoming more and more popular as a method of approximate nearest neighbor search. Previous supervised hashing methods utilized similarity/dissimilarity matrix to get semantic information. But the matrix is not easy to construct for a new dataset. Rather than to reconstruct the matrix, we proposed a straightforward CNN-based hashing method, i.e. binarilizing the activations of a fully connected layer with threshold 0 and taking the binary result as hash codes. This method achieved the best performance on CIFAR-10 and was comparable with the state-of-the-art on MNIST. And our experiments on CIFAR-10 suggested that the signs of activations may carry more information than the relative values of activations between samples, and that the co-adaption between feature extractor and hash functions is important for hashing.



### A statistical shape space model of the palate surface trained on 3D MRI scans of the vocal tract
- **Arxiv ID**: http://arxiv.org/abs/1602.07679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1602.07679v1)
- **Published**: 2015-09-04 07:24:53+00:00
- **Updated**: 2015-09-04 07:24:53+00:00
- **Authors**: Alexander Hewer, Ingmar Steiner, Timo Bolkart, Stefanie Wuhrer, Korin Richmond
- **Comment**: Proceedings of the 18th International Congress of Phonetic Sciences,
  Aug 2015, Glasgow, United Kingdom. 2015, http://www.icphs2015.info/
- **Journal**: None
- **Summary**: We describe a minimally-supervised method for computing a statistical shape space model of the palate surface. The model is created from a corpus of volumetric magnetic resonance imaging (MRI) scans collected from 12 speakers. We extract a 3D mesh of the palate from each speaker, then train the model using principal component analysis (PCA). The palate model is then tested using 3D MRI from another corpus and evaluated using a high-resolution optical scan. We find that the error is low even when only a handful of measured coordinates are available. In both cases, our approach yields promising results. It can be applied to extract the palate shape from MRI data, and could be useful to other analysis modalities, such as electromagnetic articulography (EMA) and ultrasound tongue imaging (UTI).



### Coordinate Descent Methods for Symmetric Nonnegative Matrix Factorization
- **Arxiv ID**: http://arxiv.org/abs/1509.01404v2
- **DOI**: 10.1109/TSP.2016.2591510
- **Categories**: **cs.NA**, cs.CV, cs.LG, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1509.01404v2)
- **Published**: 2015-09-04 11:19:35+00:00
- **Updated**: 2016-05-31 12:50:38+00:00
- **Authors**: Arnaud Vandaele, Nicolas Gillis, Qi Lei, Kai Zhong, Inderjit Dhillon
- **Comment**: 25 pages, 5 figures, 7 tables. Main changes: comparison with another
  symNMF algorithm (namely, BetaSNMF), and correction of an error in the
  convergence proof
- **Journal**: IEEE Transactions on Signal Processing 64 (21), pp. 5571-5584,
  2016
- **Summary**: Given a symmetric nonnegative matrix $A$, symmetric nonnegative matrix factorization (symNMF) is the problem of finding a nonnegative matrix $H$, usually with much fewer columns than $A$, such that $A \approx HH^T$. SymNMF can be used for data analysis and in particular for various clustering tasks. In this paper, we propose simple and very efficient coordinate descent schemes to solve this problem, and that can handle large and sparse input matrices. The effectiveness of our methods is illustrated on synthetic and real-world data sets, and we show that they perform favorably compared to recent state-of-the-art methods.



### NoSPaM Manual - A Tool for Node-Specific Triad Pattern Mining
- **Arxiv ID**: http://arxiv.org/abs/1509.03503v1
- **DOI**: None
- **Categories**: **cs.SI**, cs.CV, cs.DS, physics.data-an, physics.soc-ph
- **Links**: [PDF](http://arxiv.org/pdf/1509.03503v1)
- **Published**: 2015-09-04 14:57:08+00:00
- **Updated**: 2015-09-04 14:57:08+00:00
- **Authors**: Marco Winkler
- **Comment**: None
- **Journal**: None
- **Summary**: The detection of triadic subgraph motifs is a common methodology in complex-networks research. The procedure usually applied in order to detect motifs evaluates whether a certain subgraph pattern is overrepresented in a network as a whole. However, motifs do not necessarily appear frequently in every region of a graph. For this reason, we recently introduced the framework of Node-Specific Pattern Mining (NoSPaM). This work is a manual for an implementation of NoSPaM which can be downloaded from www.mwinkler.eu.



### EM Algorithms for Weighted-Data Clustering with Application to Audio-Visual Scene Analysis
- **Arxiv ID**: http://arxiv.org/abs/1509.01509v2
- **DOI**: 10.1109/TPAMI.2016.2522425
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1509.01509v2)
- **Published**: 2015-09-04 15:51:17+00:00
- **Updated**: 2016-01-25 11:17:13+00:00
- **Authors**: Israel D. Gebru, Xavier Alameda-Pineda, Florence Forbes, Radu Horaud
- **Comment**: 14 pages, 4 figures, 4 tables
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  volume 38, number 12, 2402 - 2415, 2016
- **Summary**: Data clustering has received a lot of attention and numerous methods, algorithms and software packages are available. Among these techniques, parametric finite-mixture models play a central role due to their interesting mathematical properties and to the existence of maximum-likelihood estimators based on expectation-maximization (EM). In this paper we propose a new mixture model that associates a weight with each observed point. We introduce the weighted-data Gaussian mixture and we derive two EM algorithms. The first one considers a fixed weight for each observation. The second one treats each weight as a random variable following a gamma distribution. We propose a model selection method based on a minimum message length criterion, provide a weight initialization strategy, and validate the proposed algorithms by comparing them with several state of the art parametric and non-parametric clustering techniques. We also demonstrate the effectiveness and robustness of the proposed clustering technique in the presence of heterogeneous data, namely audio-visual scene analysis.



### Conjugate Gradient Acceleration of Non-Linear Smoothing Filters
- **Arxiv ID**: http://arxiv.org/abs/1509.01514v1
- **DOI**: 10.1109/GlobalSIP.2015.7418194
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.01514v1)
- **Published**: 2015-09-04 16:06:38+00:00
- **Updated**: 2015-09-04 16:06:38+00:00
- **Authors**: Andrew Knyazev, Alexander Malyshev
- **Comment**: 5 pages, 5 figures, IEEE Conference GlobalSIP 2015
- **Journal**: 2015 IEEE Global Conference on Signal and Information Processing
  (GlobalSIP), Orlando, FL, 2015, pp. 245-249
- **Summary**: The most efficient signal edge-preserving smoothing filters, e.g., for denoising, are non-linear. Thus, their acceleration is challenging and is often performed in practice by tuning filter parameters, such as by increasing the width of the local smoothing neighborhood, resulting in more aggressive smoothing of a single sweep at the cost of increased edge blurring. We propose an alternative technology, accelerating the original filters without tuning, by running them through a special conjugate gradient method, not affecting their quality. The filter non-linearity is dealt with by careful freezing and restarting. Our initial numerical experiments on toy one-dimensional signals demonstrate 20x acceleration of the classical bilateral filter and 3-5x acceleration of the recently developed guided filter.



### An On-line Variational Bayesian Model for Multi-Person Tracking from Cluttered Scenes
- **Arxiv ID**: http://arxiv.org/abs/1509.01520v3
- **DOI**: 10.1016/j.cviu.2016.07.006
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1509.01520v3)
- **Published**: 2015-09-04 16:16:42+00:00
- **Updated**: 2016-06-30 08:50:42+00:00
- **Authors**: Sileye Ba, Xavier Alameda-Pineda, Alessio Xompero, Radu Horaud
- **Comment**: 21 pages, 9 figures, 4 tables
- **Journal**: Computer Vision and Image Understanding, volume 153, December
  2016, pages 64-76
- **Summary**: Object tracking is an ubiquitous problem that appears in many applications such as remote sensing, audio processing, computer vision, human-machine interfaces, human-robot interaction, etc. Although thoroughly investigated in computer vision, tracking a time-varying number of persons remains a challenging open problem. In this paper, we propose an on-line variational Bayesian model for multi-person tracking from cluttered visual observations provided by person detectors. The contributions of this paper are the followings. First, we propose a variational Bayesian framework for tracking an unknown and varying number of persons. Second, our model results in a variational expectation-maximization (VEM) algorithm with closed-form expressions for the posterior distributions of the latent variables and for the estimation of the model parameters. Third, the proposed model exploits observations from multiple detectors, and it is therefore multimodal by nature. Finally, we propose to embed both object-birth and object-visibility processes in an effort to robustly handle person appearances and disappearances over time. Evaluated on classical multiple person tracking datasets, our method shows competitive results with respect to state-of-the-art multiple-object tracking models, such as the probability hypothesis density (PHD) filter among others.



### Object Recognition from Short Videos for Robotic Perception
- **Arxiv ID**: http://arxiv.org/abs/1509.01602v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1509.01602v1)
- **Published**: 2015-09-04 20:48:23+00:00
- **Updated**: 2015-09-04 20:48:23+00:00
- **Authors**: Ivan Bogun, Anelia Angelova, Navdeep Jaitly
- **Comment**: 7 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: Deep neural networks have become the primary learning technique for object recognition. Videos, unlike still images, are temporally coherent which makes the application of deep networks non-trivial. Here, we investigate how motion can aid object recognition in short videos. Our approach is based on Long Short-Term Memory (LSTM) deep networks. Unlike previous applications of LSTMs, we implement each gate as a convolution. We show that convolutional-based LSTM models are capable of learning motion dependencies and are able to improve the recognition accuracy when more frames in a sequence are available. We evaluate our approach on the Washington RGBD Object dataset and on the Washington RGBD Scenes dataset. Our approach outperforms deep nets applied to still images and sets a new state-of-the-art in this domain.



### Semantic Video Segmentation : Exploring Inference Efficiency
- **Arxiv ID**: http://arxiv.org/abs/1509.02441v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.02441v1)
- **Published**: 2015-09-04 22:03:40+00:00
- **Updated**: 2015-09-04 22:03:40+00:00
- **Authors**: Subarna Tripathi, Serge Belongie, Youngbae Hwang, Truong Nguyen
- **Comment**: To appear in proc of ISOCC 2015
- **Journal**: None
- **Summary**: We explore the efficiency of the CRF inference beyond image level semantic segmentation and perform joint inference in video frames. The key idea is to combine best of two worlds: semantic co-labeling and more expressive models. Our formulation enables us to perform inference over ten thousand images within seconds and makes the system amenable to perform video semantic segmentation most effectively. On CamVid dataset, with TextonBoost unaries, our proposed method achieves up to 8% improvement in accuracy over individual semantic image segmentation without additional time overhead. The source code is available at https://github.com/subtri/video_inference



### Chebyshev and Conjugate Gradient Filters for Graph Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1509.01624v1
- **DOI**: 10.1109/ICMEW.2014.6890711
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.01624v1)
- **Published**: 2015-09-04 22:22:25+00:00
- **Updated**: 2015-09-04 22:22:25+00:00
- **Authors**: Dong Tian, Hassan Mansour, Andrew Knyazev, Anthony Vetro
- **Comment**: 6 pages, 6 figures, accepted to 2014 IEEE International Conference on
  Multimedia and Expo Workshops (ICMEW)
- **Journal**: Multimedia and Expo Workshops (ICMEW), 2014 IEEE International
  Conference on, vol., no., pp.1-6, 14-18 July 2014
- **Summary**: In 3D image/video acquisition, different views are often captured with varying noise levels across the views. In this paper, we propose a graph-based image enhancement technique that uses a higher quality view to enhance a degraded view. A depth map is utilized as auxiliary information to match the perspectives of the two views. Our method performs graph-based filtering of the noisy image by directly computing a projection of the image to be filtered onto a lower dimensional Krylov subspace of the graph Laplacian. We discuss two graph spectral denoising methods: first using Chebyshev polynomials, and second using iterations of the conjugate gradient algorithm. Our framework generalizes previously known polynomial graph filters, and we demonstrate through numerical simulations that our proposed technique produces subjectively cleaner images with about 1-3 dB improvement in PSNR over existing polynomial graph filters.



