# Arxiv Papers in cs.CV on 2015-09-17
### Accelerated Distance Computation with Encoding Tree for High Dimensional Data
- **Arxiv ID**: http://arxiv.org/abs/1509.05186v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.05186v2)
- **Published**: 2015-09-17 09:54:33+00:00
- **Updated**: 2015-09-18 06:40:22+00:00
- **Authors**: Shicong Liu, Junru Shao, Hongtao Lu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel distance to calculate distance between high dimensional vector pairs, utilizing vector quantization generated encodings. Vector quantization based methods are successful in handling large scale high dimensional data. These methods compress vectors into short encodings, and allow efficient distance computation between an uncompressed vector and compressed dataset without decompressing explicitly. However for large datasets, these distance computing methods perform excessive computations. We avoid excessive computations by storing the encodings on an Encoding Tree(E-Tree), interestingly the memory consumption is also lowered. We also propose Encoding Forest(E-Forest) to further lower the computation cost. E-Tree and E-Forest is compatible with various existing quantization-based methods. We show by experiments our methods speed-up distance computing for high dimensional data drastically, and various existing algorithms can benefit from our methods.



### HCLAE: High Capacity Locally Aggregating Encodings for Approximate Nearest Neighbor Search
- **Arxiv ID**: http://arxiv.org/abs/1509.05194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.05194v1)
- **Published**: 2015-09-17 10:18:05+00:00
- **Updated**: 2015-09-17 10:18:05+00:00
- **Authors**: Shicong Liu, Junru Shao, Hongtao Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Vector quantization-based approaches are successful to solve Approximate Nearest Neighbor (ANN) problems which are critical to many applications. The idea is to generate effective encodings to allow fast distance approximation. We propose quantization-based methods should partition the data space finely and exhibit locality of the dataset to allow efficient non-exhaustive search. In this paper, we introduce the concept of High Capacity Locality Aggregating Encodings (HCLAE) to this end, and propose Dictionary Annealing (DA) to learn HCLAE by a simulated annealing procedure. The quantization error is lower than other state-of-the-art. The algorithms of DA can be easily extended to an online learning scheme, allowing effective handle of large scale data. Further, we propose Aggregating-Tree (A-Tree), a non-exhaustive search method using HCLAE to perform efficient ANN-Search. A-Tree achieves magnitudes of speed-up on ANN-Search tasks, compared to the state-of-the-art.



### Improved Residual Vector Quantization for High-dimensional Approximate Nearest Neighbor Search
- **Arxiv ID**: http://arxiv.org/abs/1509.05195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.05195v1)
- **Published**: 2015-09-17 10:19:37+00:00
- **Updated**: 2015-09-17 10:19:37+00:00
- **Authors**: Shicong Liu, Hongtao Lu, Junru Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Quantization methods have been introduced to perform large scale approximate nearest search tasks. Residual Vector Quantization (RVQ) is one of the effective quantization methods. RVQ uses a multi-stage codebook learning scheme to lower the quantization error stage by stage. However, there are two major limitations for RVQ when applied to on high-dimensional approximate nearest neighbor search: 1. The performance gain diminishes quickly with added stages. 2. Encoding a vector with RVQ is actually NP-hard. In this paper, we propose an improved residual vector quantization (IRVQ) method, our IRVQ learns codebook with a hybrid method of subspace clustering and warm-started k-means on each stage to prevent performance gain from dropping, and uses a multi-path encoding scheme to encode a vector with lower distortion. Experimental results on the benchmark datasets show that our method gives substantially improves RVQ and delivers better performance compared to the state-of-the-art.



### Hand-held Video Deblurring via Efficient Fourier Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1509.05251v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.05251v3)
- **Published**: 2015-09-17 13:37:39+00:00
- **Updated**: 2015-12-04 15:22:25+00:00
- **Authors**: Mauricio Delbracio, Guillermo Sapiro
- **Comment**: None
- **Journal**: None
- **Summary**: Videos captured with hand-held cameras often suffer from a significant amount of blur, mainly caused by the inevitable natural tremor of the photographer's hand. In this work, we present an algorithm that removes blur due to camera shake by combining information in the Fourier domain from nearby frames in a video. The dynamic nature of typical videos with the presence of multiple moving objects and occlusions makes this problem of camera shake removal extremely challenging, in particular when low complexity is needed. Given an input video frame, we first create a consistent registered version of temporally adjacent frames. Then, the set of consistently registered frames is block-wise fused in the Fourier domain with weights depending on the Fourier spectrum magnitude. The method is motivated from the physiological fact that camera shake blur has a random nature and therefore, nearby video frames are generally blurred differently. Experiments with numerous videos recorded in the wild, along with extensive comparisons, show that the proposed algorithm achieves state-of-the-art results while at the same time being much faster than its competitors.



### Deep Multi-task Learning for Railway Track Inspection
- **Arxiv ID**: http://arxiv.org/abs/1509.05267v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.05267v1)
- **Published**: 2015-09-17 14:16:46+00:00
- **Updated**: 2015-09-17 14:16:46+00:00
- **Authors**: Xavier Gibert, Vishal M. Patel, Rama Chellappa
- **Comment**: Submitted to IEEE Trans. on Pattern Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: Railroad tracks need to be periodically inspected and monitored to ensure safe transportation. Automated track inspection using computer vision and pattern recognition methods have recently shown the potential to improve safety by allowing for more frequent inspections while reducing human errors. Achieving full automation is still very challenging due to the number of different possible failure modes as well as the broad range of image variations that can potentially trigger false alarms. Also, the number of defective components is very small, so not many training examples are available for the machine to learn a robust anomaly detector. In this paper, we show that detection performance can be improved by combining multiple detectors within a multi-task learning framework. We show that this approach results in better accuracy in detecting defects on railway ties and fasteners.



### Humans Are Easily Fooled by Digital Images
- **Arxiv ID**: http://arxiv.org/abs/1509.05301v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1509.05301v1)
- **Published**: 2015-09-17 15:47:25+00:00
- **Updated**: 2015-09-17 15:47:25+00:00
- **Authors**: Victor Schetinger, Manuel M. Oliveira, Roberto da Silva, Tiago J. Carvalho
- **Comment**: None
- **Journal**: None
- **Summary**: Digital images are ubiquitous in our modern lives, with uses ranging from social media to news, and even scientific papers. For this reason, it is crucial evaluate how accurate people are when performing the task of identify doctored images. In this paper, we performed an extensive user study evaluating subjects capacity to detect fake images. After observing an image, users have been asked if it had been altered or not. If the user answered the image has been altered, he had to provide evidence in the form of a click on the image. We collected 17,208 individual answers from 383 users, using 177 images selected from public forensic databases. Different from other previously studies, our method propose different ways to avoid lucky guess when evaluating users answers. Our results indicate that people show inaccurate skills at differentiating between altered and non-altered images, with an accuracy of 58%, and only identifying the modified images 46.5% of the time. We also track user features such as age, answering time, confidence, providing deep analysis of how such variables influence on the users' performance.



### Recurrent Spatial Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/1509.05329v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.05329v1)
- **Published**: 2015-09-17 16:50:29+00:00
- **Updated**: 2015-09-17 16:50:29+00:00
- **Authors**: Søren Kaae Sønderby, Casper Kaae Sønderby, Lars Maaløe, Ole Winther
- **Comment**: None
- **Journal**: None
- **Summary**: We integrate the recently proposed spatial transformer network (SPN) [Jaderberg et. al 2015] into a recurrent neural network (RNN) to form an RNN-SPN model. We use the RNN-SPN to classify digits in cluttered MNIST sequences. The proposed model achieves a single digit error of 1.5% compared to 2.9% for a convolutional networks and 2.0% for convolutional networks with SPN layers. The SPN outputs a zoomed, rotated and skewed version of the input image. We investigate different down-sampling factors (ratio of pixel in input and output) for the SPN and show that the RNN-SPN model is able to down-sample the input images without deteriorating performance. The down-sampling in RNN-SPN can be thought of as adaptive down-sampling that minimizes the information loss in the regions of interest. We attribute the superior performance of the RNN-SPN to the fact that it can attend to a sequence of regions of interest.



### Geometry-aware Deep Transform
- **Arxiv ID**: http://arxiv.org/abs/1509.05360v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.05360v2)
- **Published**: 2015-09-17 18:30:10+00:00
- **Updated**: 2015-10-18 19:28:25+00:00
- **Authors**: Jiaji Huang, Qiang Qiu, Robert Calderbank, Guillermo Sapiro
- **Comment**: to appear in ICCV2015, updated with minor revision
- **Journal**: None
- **Summary**: Many recent efforts have been devoted to designing sophisticated deep learning structures, obtaining revolutionary results on benchmark datasets. The success of these deep learning methods mostly relies on an enormous volume of labeled training samples to learn a huge number of parameters in a network; therefore, understanding the generalization ability of a learned deep network cannot be overlooked, especially when restricted to a small training set, which is the case for many applications. In this paper, we propose a novel deep learning objective formulation that unifies both the classification and metric learning criteria. We then introduce a geometry-aware deep transform to enable a non-linear discriminative and robust feature transform, which shows competitive performance on small training sets for both synthetic and real-world data. We further support the proposed framework with a formal $(K,\epsilon)$-robustness analysis.



### Facial Descriptors for Human Interaction Recognition In Still Images
- **Arxiv ID**: http://arxiv.org/abs/1509.05366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.05366v1)
- **Published**: 2015-09-17 18:40:15+00:00
- **Updated**: 2015-09-17 18:40:15+00:00
- **Authors**: Gokhan Tanisik, Cemil Zalluhoglu, Nazli Ikizler-Cinbis
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel approach in a rarely studied area of computer vision: Human interaction recognition in still images. We explore whether the facial regions and their spatial configurations contribute to the recognition of interactions. In this respect, our method involves extraction of several visual features from the facial regions, as well as incorporation of scene characteristics and deep features to the recognition. Extracted multiple features are utilized within a discriminative learning framework for recognizing interactions between people. Our designed facial descriptors are based on the observation that relative positions, size and locations of the faces are likely to be important for characterizing human interactions. Since there is no available dataset in this relatively new domain, a comprehensive new dataset which includes several images of human interactions is collected. Our experimental results show that faces and scene characteristics contain important information to recognize interactions between people.



### DeXpression: Deep Convolutional Neural Network for Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1509.05371v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1509.05371v2)
- **Published**: 2015-09-17 18:49:10+00:00
- **Updated**: 2016-08-17 19:34:55+00:00
- **Authors**: Peter Burkert, Felix Trier, Muhammad Zeshan Afzal, Andreas Dengel, Marcus Liwicki
- **Comment**: Under consideration for publication in Pattern Recognition Letters
- **Journal**: None
- **Summary**: We propose a convolutional neural network (CNN) architecture for facial expression recognition. The proposed architecture is independent of any hand-crafted feature extraction and performs better than the earlier proposed convolutional neural network based approaches. We visualize the automatically extracted features which have been learned by the network in order to provide a better understanding. The standard datasets, i.e. Extended Cohn-Kanade (CKP) and MMI Facial Expression Databse are used for the quantitative evaluation. On the CKP set the current state of the art approach, using CNNs, achieves an accuracy of 99.2%. For the MMI dataset, currently the best accuracy for emotion recognition is 93.33%. The proposed architecture achieves 99.6% for CKP and 98.63% for MMI, therefore performing better than the state of the art using CNNs. Automatic facial expression recognition has a broad spectrum of applications such as human-computer interaction and safety systems. This is due to the fact that non-verbal cues are important forms of communication and play a pivotal role in interpersonal communication. The performance of the proposed architecture endorses the efficacy and reliable usage of the proposed work for real world applications.



### Learning from Synthetic Data Using a Stacked Multichannel Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1509.05463v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1509.05463v2)
- **Published**: 2015-09-17 22:26:30+00:00
- **Updated**: 2015-09-21 19:28:12+00:00
- **Authors**: Xi Zhang, Yanwei Fu, Shanshan Jiang, Leonid Sigal, Gady Agam
- **Comment**: None
- **Journal**: None
- **Summary**: Learning from synthetic data has many important and practical applications. An example of application is photo-sketch recognition. Using synthetic data is challenging due to the differences in feature distributions between synthetic and real data, a phenomenon we term synthetic gap. In this paper, we investigate and formalize a general framework-Stacked Multichannel Autoencoder (SMCAE) that enables bridging the synthetic gap and learning from synthetic data more efficiently. In particular, we show that our SMCAE can not only transform and use synthetic data on the challenging face-sketch recognition task, but that it can also help simulate real images, which can be used for training classifiers for recognition. Preliminary experiments validate the effectiveness of the framework.



