# Arxiv Papers in cs.CV on 2015-09-16
### Group Membership Prediction
- **Arxiv ID**: http://arxiv.org/abs/1509.04783v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1509.04783v1)
- **Published**: 2015-09-16 01:22:40+00:00
- **Updated**: 2015-09-16 01:22:40+00:00
- **Authors**: Ziming Zhang, Yuting Chen, Venkatesh Saligrama
- **Comment**: accepted for ICCV 2015
- **Journal**: None
- **Summary**: The group membership prediction (GMP) problem involves predicting whether or not a collection of instances share a certain semantic property. For instance, in kinship verification given a collection of images, the goal is to predict whether or not they share a {\it familial} relationship. In this context we propose a novel probability model and introduce latent {\em view-specific} and {\em view-shared} random variables to jointly account for the view-specific appearance and cross-view similarities among data instances. Our model posits that data from each view is independent conditioned on the shared variables. This postulate leads to a parametric probability model that decomposes group membership likelihood into a tensor product of data-independent parameters and data-dependent factors. We propose learning the data-independent parameters in a discriminative way with bilinear classifiers, and test our prediction algorithm on challenging visual recognition tasks such as multi-camera person re-identification and kinship verification. On most benchmark datasets, our method can significantly outperform the current state-of-the-art.



### SPECFACE - A Dataset of Human Faces Wearing Spectacles
- **Arxiv ID**: http://arxiv.org/abs/1509.04853v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.04853v2)
- **Published**: 2015-09-16 08:43:53+00:00
- **Updated**: 2016-08-11 05:15:48+00:00
- **Authors**: Anirban Dasgupta, Shubhobrata Bhattacharya, Aurobinda Routray
- **Comment**: 5 pages, 9 figures, 1 Table. arXiv admin note: text overlap with
  arXiv:1505.04055
- **Journal**: 2016 IEEE Students' Technology Symposium (IEEE TechSym 2016)
- **Summary**: This paper presents a database of human faces for persons wearing spectacles. The database consists of images of faces having significant variations with respect to illumination, head pose, skin color, facial expressions and sizes, and nature of spectacles. The database contains data of 60 subjects. This database is expected to be a precious resource for the development and evaluation of algorithms for face detection, eye detection, head tracking, eye gaze tracking, etc., for subjects wearing spectacles. As such, this can be a valuable contribution to the computer vision community.



### Fast Template Matching by Subsampled Circulant Matrix
- **Arxiv ID**: http://arxiv.org/abs/1509.04863v1
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1509.04863v1)
- **Published**: 2015-09-16 09:38:29+00:00
- **Updated**: 2015-09-16 09:38:29+00:00
- **Authors**: Sung-Hsien Hsieh, Chun-Shien Lu, and Soo-Chang Pei
- **Comment**: 7 pages, 1 figure, 2 tables
- **Journal**: None
- **Summary**: Template matching is widely used for many applications in image and signal processing and usually is time-critical. Traditional methods usually focus on how to reduce the search locations by coarse-to-fine strategy or full search combined with pruning strategy. However, the computation cost of those methods is easily dominated by the size of signal N instead of that of template K. This paper proposes a probabilistic and fast matching scheme, which computation costs requires O(N) additions and O(K \log K) multiplications, based on cross-correlation. The nuclear idea is to first downsample signal, which size becomes O(K), and then subsequent operations only involves downsampled signals. The probability of successful match depends on cross-correlation between signal and the template. We show the sufficient condition for successful match and prove that the probability is high for binary signals with K^2/log K >= O(N). The experiments shows this proposed scheme is fast and efficient and supports the theoretical results.



### DenseBox: Unifying Landmark Localization with End to End Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1509.04874v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.04874v3)
- **Published**: 2015-09-16 10:30:37+00:00
- **Updated**: 2015-09-19 02:36:04+00:00
- **Authors**: Lichao Huang, Yi Yang, Yafeng Deng, Yinan Yu
- **Comment**: None
- **Journal**: None
- **Summary**: How can a single fully convolutional neural network (FCN) perform on object detection? We introduce DenseBox, a unified end-to-end FCN framework that directly predicts bounding boxes and object class confidences through all locations and scales of an image. Our contribution is two-fold. First, we show that a single FCN, if designed and optimized carefully, can detect multiple different objects extremely accurately and efficiently. Second, we show that when incorporating with landmark localization during multi-task learning, DenseBox further improves object detection accuray. We present experimental results on public benchmark datasets including MALF face detection and KITTI car detection, that indicate our DenseBox is the state-of-the-art system for detecting challenging objects such as faces and cars.



### An Improved Algorithm for Eye Corner Detection
- **Arxiv ID**: http://arxiv.org/abs/1509.04887v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.04887v2)
- **Published**: 2015-09-16 11:19:59+00:00
- **Updated**: 2016-11-08 08:46:38+00:00
- **Authors**: Anirban Dasgupta, Anshit Mandloi, Anjith George, Aurobinda Routray
- **Comment**: None
- **Journal**: IEEE Conference on Signal Processing and Communications SPCOM,
  2016
- **Summary**: In this paper, a modified algorithm for the detection of nasal and temporal eye corners is presented. The algorithm is a modification of the Santos and Proenka Method. In the first step, we detect the face and the eyes using classifiers based on Haar-like features. We then segment out the sclera, from the detected eye region. From the segmented sclera, we segment out an approximate eyelid contour. Eye corner candidates are obtained using Harris and Stephens corner detector. We introduce a post-pruning of the Eye corner candidates to locate the eye corners, finally. The algorithm has been tested on Yale, JAFFE databases as well as our created database.



### Projection Bank: From High-dimensional Data to Medium-length Binary Codes
- **Arxiv ID**: http://arxiv.org/abs/1509.04916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.04916v1)
- **Published**: 2015-09-16 13:42:42+00:00
- **Updated**: 2015-09-16 13:42:42+00:00
- **Authors**: Li Liu, Mengyang Yu, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, very high-dimensional feature representations, e.g., Fisher Vector, have achieved excellent performance for visual recognition and retrieval. However, these lengthy representations always cause extremely heavy computational and storage costs and even become unfeasible in some large-scale applications. A few existing techniques can transfer very high-dimensional data into binary codes, but they still require the reduced code length to be relatively long to maintain acceptable accuracies. To target a better balance between computational efficiency and accuracies, in this paper, we propose a novel embedding method called Binary Projection Bank (BPB), which can effectively reduce the very high-dimensional representations to medium-dimensional binary codes without sacrificing accuracies. Instead of using conventional single linear or bilinear projections, the proposed method learns a bank of small projections via the max-margin constraint to optimally preserve the intrinsic data similarity. We have systematically evaluated the proposed method on three datasets: Flickr 1M, ILSVR2010 and UCF101, showing competitive retrieval and recognition accuracies compared with state-of-the-art approaches, but with a significantly smaller memory footprint and lower coding complexity.



### Guiding Long-Short Term Memory for Image Caption Generation
- **Arxiv ID**: http://arxiv.org/abs/1509.04942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.04942v1)
- **Published**: 2015-09-16 15:02:30+00:00
- **Updated**: 2015-09-16 15:02:30+00:00
- **Authors**: Xu Jia, Efstratios Gavves, Basura Fernando, Tinne Tuytelaars
- **Comment**: accepted by ICCV 2015
- **Journal**: None
- **Summary**: In this work we focus on the problem of image caption generation. We propose an extension of the long short term memory (LSTM) model, which we coin gLSTM for short. In particular, we add semantic information extracted from the image as extra input to each unit of the LSTM block, with the aim of guiding the model towards solutions that are more tightly coupled to the image content. Additionally, we explore different length normalization strategies for beam search in order to prevent from favoring short sentences. On various benchmark datasets such as Flickr8K, Flickr30K and MS COCO, we obtain results that are on par with or even outperform the current state-of-the-art.



### Human and Sheep Facial Landmarks Localisation by Triplet Interpolated Features
- **Arxiv ID**: http://arxiv.org/abs/1509.04954v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.04954v1)
- **Published**: 2015-09-16 15:50:01+00:00
- **Updated**: 2015-09-16 15:50:01+00:00
- **Authors**: Heng Yang, Renqiao Zhang, Peter Robinson
- **Comment**: submitted to WACV2016
- **Journal**: None
- **Summary**: In this paper we present a method for localisation of facial landmarks on human and sheep. We introduce a new feature extraction scheme called triplet-interpolated feature used at each iteration of the cascaded shape regression framework. It is able to extract features from similar semantic location given an estimated shape, even when head pose variations are large and the facial landmarks are very sparsely distributed. Furthermore, we study the impact of training data imbalance on model performance and propose a training sample augmentation scheme that produces more initialisations for training samples from the minority. More specifically, the augmentation number for a training sample is made to be negatively correlated to the value of the fitted probability density function at the sample's position. We evaluate the proposed scheme on both human and sheep facial landmarks localisation. On the benchmark 300w human face dataset, we demonstrate the benefits of our proposed methods and show very competitive performance when comparing to other methods. On a newly created sheep face dataset, we get very good performance despite the fact that we only have a limited number of training samples and a set of sparse landmarks are annotated.



### Recurrent Neural Networks for Driver Activity Anticipation via Sensory-Fusion Architecture
- **Arxiv ID**: http://arxiv.org/abs/1509.05016v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1509.05016v1)
- **Published**: 2015-09-16 19:49:24+00:00
- **Updated**: 2015-09-16 19:49:24+00:00
- **Authors**: Ashesh Jain, Avi Singh, Hema S Koppula, Shane Soh, Ashutosh Saxena
- **Comment**: Follow-up of ICCV 2015 Brain4Cars http://www.brain4cars.com
- **Journal**: None
- **Summary**: Anticipating the future actions of a human is a widely studied problem in robotics that requires spatio-temporal reasoning. In this work we propose a deep learning approach for anticipation in sensory-rich robotics applications. We introduce a sensory-fusion architecture which jointly learns to anticipate and fuse information from multiple sensory streams. Our architecture consists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM) units to capture long temporal dependencies. We train our architecture in a sequence-to-sequence prediction manner, and it explicitly learns to predict the future given only a partial temporal context. We further introduce a novel loss layer for anticipation which prevents over-fitting and encourages early anticipation. We use our architecture to anticipate driving maneuvers several seconds before they happen on a natural driving data set of 1180 miles. The context for maneuver anticipation comes from multiple sensors installed on the vehicle. Our approach shows significant improvement over the state-of-the-art in maneuver anticipation by increasing the precision from 77.4% to 90.5% and recall from 71.2% to 87.4%.



### Overcomplete Dictionary Learning with Jacobi Atom Updates
- **Arxiv ID**: http://arxiv.org/abs/1509.05054v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.05054v1)
- **Published**: 2015-09-16 20:23:06+00:00
- **Updated**: 2015-09-16 20:23:06+00:00
- **Authors**: Paul Irofti, Bogdan Dumitrescu
- **Comment**: None
- **Journal**: None
- **Summary**: Dictionary learning for sparse representations is traditionally approached with sequential atom updates, in which an optimized atom is used immediately for the optimization of the next atoms. We propose instead a Jacobi version, in which groups of atoms are updated independently, in parallel. Extensive numerical evidence for sparse image representation shows that the parallel algorithms, especially when all atoms are updated simultaneously, give better dictionaries than their sequential counterparts.



### Exact simultaneous recovery of locations and structure from known orientations and corrupted point correspondences
- **Arxiv ID**: http://arxiv.org/abs/1509.05064v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, math.CO, math.IT, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1509.05064v1)
- **Published**: 2015-09-16 20:56:53+00:00
- **Updated**: 2015-09-16 20:56:53+00:00
- **Authors**: Paul Hand, Choongbum Lee, Vladislav Voroninski
- **Comment**: arXiv admin note: text overlap with arXiv:1506.01437
- **Journal**: None
- **Summary**: Let $t_1,\ldots,t_{n_l} \in \mathbb{R}^d$ and $p_1,\ldots,p_{n_s} \in \mathbb{R}^d$ and consider the bipartite location recovery problem: given a subset of pairwise direction observations $\{(t_i - p_j) / \|t_i - p_j\|_2\}_{i,j \in [n_l] \times [n_s]}$, where a constant fraction of these observations are arbitrarily corrupted, find $\{t_i\}_{i \in [n_ll]}$ and $\{p_j\}_{j \in [n_s]}$ up to a global translation and scale. We study the recently introduced ShapeFit algorithm as a method for solving this bipartite location recovery problem. In this case, ShapeFit consists of a simple convex program over $d(n_l + n_s)$ real variables. We prove that this program recovers a set of $n_l+n_s$ i.i.d. Gaussian locations exactly and with high probability if the observations are given by a bipartite Erd\H{o}s-R\'{e}nyi graph, $d$ is large enough, and provided that at most a constant fraction of observations involving any particular location are adversarially corrupted. This recovery theorem is based on a set of deterministic conditions that we prove are sufficient for exact recovery. Finally, we propose a modified pipeline for the Structure for Motion problem, based on this bipartite location recovery problem.



