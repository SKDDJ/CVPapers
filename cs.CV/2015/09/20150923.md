# Arxiv Papers in cs.CV on 2015-09-23
### Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours
- **Arxiv ID**: http://arxiv.org/abs/1509.06825v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1509.06825v1)
- **Published**: 2015-09-23 02:08:02+00:00
- **Updated**: 2015-09-23 02:08:02+00:00
- **Authors**: Lerrel Pinto, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Current learning-based robot grasping approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping.



### New Fuzzy LBP Features for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1509.06853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.06853v1)
- **Published**: 2015-09-23 06:14:26+00:00
- **Updated**: 2015-09-23 06:14:26+00:00
- **Authors**: Abdullah Gubbi, Mohammed Fazle Azeem, Zahid Ansari
- **Comment**: None
- **Journal**: None
- **Summary**: There are many Local texture features each very in way they implement and each of the Algorithm trying improve the performance. An attempt is made in this paper to represent a theoretically very simple and computationally effective approach for face recognition. In our implementation the face image is divided into 3x3 sub-regions from which the features are extracted using the Local Binary Pattern (LBP) over a window, fuzzy membership function and at the central pixel. The LBP features possess the texture discriminative property and their computational cost is very low. By utilising the information from LBP, membership function, and central pixel, the limitations of traditional LBP is eliminated. The bench mark database like ORL and Sheffield Databases are used for the evaluation of proposed features with SVM classifier. For the proposed approach K-fold and ROC curves are obtained and results are compared.



### Robust Object Tracking with a Hierarchical Ensemble Framework
- **Arxiv ID**: http://arxiv.org/abs/1509.06925v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.06925v2)
- **Published**: 2015-09-23 11:23:56+00:00
- **Updated**: 2016-08-10 01:04:34+00:00
- **Authors**: Mengmeng Wang, Yong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous robots enjoy a wide popularity nowadays and have been applied in many applications, such as home security, entertainment, delivery, navigation and guidance. It is vital to robots to track objects accurately in these applications, so it is necessary to focus on tracking algorithms to improve the robustness and accuracy. In this paper, we propose a robust object tracking algorithm based on a hierarchical ensemble framework which can incorporate information including individual pixel features, local patches and holistic target models. The framework combines multiple ensemble models simultaneously instead of using a single ensemble model individually. A discriminative model which accounts for the matching degree of local patches is adopted via a bottom ensemble layer, and a generative model which exploits holistic templates is used to search for the object through the middle ensemble layer as well as an adaptive Kalman filter. We test the proposed tracker on challenging benchmark image sequences. Both qualitative and quantitative evaluations demonstrate that the proposed tracker performs superiorly against several state-of-the-art algorithms, especially when the appearance changes dramatically and the occlusions occur.



### Enabling Depth-driven Visual Attention on the iCub Humanoid Robot: Instructions for Use and New Perspectives
- **Arxiv ID**: http://arxiv.org/abs/1509.06939v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1509.06939v1)
- **Published**: 2015-09-23 12:18:01+00:00
- **Updated**: 2015-09-23 12:18:01+00:00
- **Authors**: Giulia Pasquale, Tanis Mar, Carlo Ciliberto, Lorenzo Rosasco, Lorenzo Natale
- **Comment**: None
- **Journal**: None
- **Summary**: The importance of depth perception in the interactions that humans have within their nearby space is a well established fact. Consequently, it is also well known that the possibility of exploiting good stereo information would ease and, in many cases, enable, a large variety of attentional and interactive behaviors on humanoid robotic platforms. However, the difficulty of computing real-time and robust binocular disparity maps from moving stereo cameras often prevents from relying on this kind of cue to visually guide robots' attention and actions in real-world scenarios. The contribution of this paper is two-fold: first, we show that the Efficient Large-scale Stereo Matching algorithm (ELAS) by A. Geiger et al. 2010 for computation of the disparity map is well suited to be used on a humanoid robotic platform as the iCub robot; second, we show how, provided with a fast and reliable stereo system, implementing relatively challenging visual behaviors in natural settings can require much less effort. As a case of study we consider the common situation where the robot is asked to focus the attention on one object close in the scene, showing how a simple but effective disparity-based segmentation solves the problem in this case. Indeed this example paves the way to a variety of other similar applications.



### Is Image Super-resolution Helpful for Other Vision Tasks?
- **Arxiv ID**: http://arxiv.org/abs/1509.07009v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1509.07009v2)
- **Published**: 2015-09-23 14:31:02+00:00
- **Updated**: 2016-01-28 15:09:11+00:00
- **Authors**: Dengxin Dai, Yujian Wang, Yuhua Chen, Luc Van Gool
- **Comment**: 1. Super-Resolution Forest added 2. Scene Recognition task added 3.
  Title changed 4. More work cited, WACV 2016
- **Journal**: None
- **Summary**: Despite the great advances made in the field of image super-resolution (ISR) during the last years, the performance has merely been evaluated perceptually. Thus, it is still unclear whether ISR is helpful for other vision tasks. In this paper, we present the first comprehensive study and analysis of the usefulness of ISR for other vision applications. In particular, six ISR methods are evaluated on four popular vision tasks, namely edge detection, semantic image segmentation, digit recognition, and scene recognition. We show that applying ISR to input images of other vision systems does improve their performance when the input images are of low-resolution. We also study the correlation between four standard perceptual evaluation criteria (namely PSNR, SSIM, IFC, and NQM) and the usefulness of ISR to the vision tasks. Experiments show that they correlate well with each other in general, but perceptual criteria are still not accurate enough to be used as full proxies for the usefulness. We hope this work will inspire the community to evaluate ISR methods also in real vision applications, and to adopt ISR as a pre-processing step of other vision tasks if the resolution of their input images is low.



### 3D Scan Registration using Curvelet Features in Planetary Environments
- **Arxiv ID**: http://arxiv.org/abs/1509.07075v1
- **DOI**: 10.1002/rob.21616
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1509.07075v1)
- **Published**: 2015-09-23 17:51:03+00:00
- **Updated**: 2015-09-23 17:51:03+00:00
- **Authors**: Siddhant Ahuja, Peter Iles, Steven L. Waslander
- **Comment**: 27 pages in Journal of Field Robotics, 2015
- **Journal**: None
- **Summary**: Topographic mapping in planetary environments relies on accurate 3D scan registration methods. However, most global registration algorithms relying on features such as FPFH and Harris-3D show poor alignment accuracy in these settings due to the poor structure of the Mars-like terrain and variable resolution, occluded, sparse range data that is hard to register without some a-priori knowledge of the environment. In this paper, we propose an alternative approach to 3D scan registration using the curvelet transform that performs multi-resolution geometric analysis to obtain a set of coefficients indexed by scale (coarsest to finest), angle and spatial position. Features are detected in the curvelet domain to take advantage of the directional selectivity of the transform. A descriptor is computed for each feature by calculating the 3D spatial histogram of the image gradients, and nearest neighbor based matching is used to calculate the feature correspondences. Correspondence rejection using Random Sample Consensus identifies inliers, and a locally optimal Singular Value Decomposition-based estimation of the rigid-body transformation aligns the laser scans given the re-projected correspondences in the metric space. Experimental results on a publicly available data-set of planetary analogue indoor facility, as well as simulated and real-world scans from Neptec Design Group's IVIGMS 3D laser rangefinder at the outdoor CSA Mars yard demonstrates improved performance over existing methods in the challenging sparse Mars-like terrain.



