# Arxiv Papers in cs.CV on 2015-01-19
### Instance Significance Guided Multiple Instance Boosting for Robust Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1501.04378v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1501.04378v5)
- **Published**: 2015-01-19 03:04:16+00:00
- **Updated**: 2020-03-17 07:48:30+00:00
- **Authors**: Jinwu Liu, Yao Lu, Tianfei Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple Instance Learning (MIL) recently provides an appealing way to alleviate the drifting problem in visual tracking. Following the tracking-by-detection framework, an online MILBoost approach is developed that sequentially chooses weak classifiers by maximizing the bag likelihood. In this paper, we extend this idea towards incorporating the instance significance estimation into the online MILBoost framework. First, instead of treating all instances equally, with each instance we associate a significance-coefficient that represents its contribution to the bag likelihood. The coefficients are estimated by a simple Bayesian formula that jointly considers the predictions from several standard MILBoost classifiers. Next, we follow the online boosting framework, and propose a new criterion for the selection of weak classifiers. Experiments with challenging public datasets show that the proposed method outperforms both existing MIL based and boosting based trackers.



### Robust Visual Tracking via Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1501.04505v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1501.04505v2)
- **Published**: 2015-01-19 14:39:51+00:00
- **Updated**: 2015-08-24 06:07:22+00:00
- **Authors**: Kaihua Zhang, Qingshan Liu, Yi Wu, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep networks have been successfully applied to visual tracking by learning a generic representation offline from numerous training images. However the offline training is time-consuming and the learned generic representation may be less discriminative for tracking specific objects. In this paper we present that, even without offline training with a large amount of auxiliary data, simple two-layer convolutional networks can be powerful enough to develop a robust representation for visual tracking. In the first frame, we employ the k-means algorithm to extract a set of normalized patches from the target region as fixed filters, which integrate a series of adaptive contextual filters surrounding the target to define a set of feature maps in the subsequent frames. These maps measure similarities between each filter and the useful local intensity patterns across the target, thereby encoding its local structural information. Furthermore, all the maps form together a global representation, which is built on mid-level features, thereby remaining close to image-level information, and hence the inner geometric layout of the target is also well preserved. A simple soft shrinkage method with an adaptive threshold is employed to de-noise the global representation, resulting in a robust sparse representation. The representation is updated via a simple and effective online strategy, allowing it to robustly adapt to target appearance variations. Our convolution networks have surprisingly lightweight structure, yet perform favorably against several state-of-the-art methods on the CVPR2013 tracking benchmark dataset with 50 challenging videos.



### Coupled Depth Learning
- **Arxiv ID**: http://arxiv.org/abs/1501.04537v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1501.04537v6)
- **Published**: 2015-01-19 16:18:48+00:00
- **Updated**: 2016-02-09 16:27:35+00:00
- **Authors**: Mohammad Haris Baig, Lorenzo Torresani
- **Comment**: 10 pages, 3 Figures, 4 Tables with quantitative evaluations
- **Journal**: None
- **Summary**: In this paper we propose a method for estimating depth from a single image using a coarse to fine approach. We argue that modeling the fine depth details is easier after a coarse depth map has been computed. We express a global (coarse) depth map of an image as a linear combination of a depth basis learned from training examples. The depth basis captures spatial and statistical regularities and reduces the problem of global depth estimation to the task of predicting the input-specific coefficients in the linear combination. This is formulated as a regression problem from a holistic representation of the image. Crucially, the depth basis and the regression function are {\bf coupled} and jointly optimized by our learning scheme. We demonstrate that this results in a significant improvement in accuracy compared to direct regression of depth pixel values or approaches learning the depth basis disjointly from the regression function. The global depth estimate is then used as a guidance by a local refinement method that introduces depth details that were not captured at the global level. Experiments on the NYUv2 and KITTI datasets show that our method outperforms the existing state-of-the-art at a considerably lower computational cost for both training and testing.



### Transductive Multi-view Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1501.04560v2
- **DOI**: 10.1109/TPAMI.2015.2408354
- **Categories**: **cs.CV**, cs.DS, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1501.04560v2)
- **Published**: 2015-01-19 17:04:11+00:00
- **Updated**: 2015-03-03 04:43:44+00:00
- **Authors**: Yanwei Fu, Timothy M. Hospedales, Tao Xiang, Shaogang Gong
- **Comment**: accepted by IEEE TPAMI, more info and longer report will be available
  in :http://www.eecs.qmul.ac.uk/~yf300/embedding/index.html
- **Journal**: None
- **Summary**: Most existing zero-shot learning approaches exploit transfer learning via an intermediate-level semantic representation shared between an annotated auxiliary dataset and a target dataset with different classes and no annotation. A projection from a low-level feature space to the semantic representation space is learned from the auxiliary dataset and is applied without adaptation to the target dataset. In this paper we identify two inherent limitations with these approaches. First, due to having disjoint and potentially unrelated classes, the projection functions learned from the auxiliary dataset/domain are biased when applied directly to the target dataset/domain. We call this problem the projection domain shift problem and propose a novel framework, transductive multi-view embedding, to solve it. The second limitation is the prototype sparsity problem which refers to the fact that for each target class, only a single prototype is available for zero-shot learning given a semantic representation. To overcome this problem, a novel heterogeneous multi-view hypergraph label propagation method is formulated for zero-shot learning in the transductive embedding space. It effectively exploits the complementary information offered by different semantic representations and takes advantage of the manifold structures of multiple representation spaces in a coherent manner. We demonstrate through extensive experiments that the proposed approach (1) rectifies the projection shift between the auxiliary and target domains, (2) exploits the complementarity of multiple semantic representations, (3) significantly outperforms existing methods for both zero-shot and N-shot recognition on three image and video benchmark datasets, and (4) enables novel cross-view annotation tasks.



### Transferring Rich Feature Hierarchies for Robust Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1501.04587v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1501.04587v2)
- **Published**: 2015-01-19 18:54:34+00:00
- **Updated**: 2015-04-23 06:18:09+00:00
- **Authors**: Naiyan Wang, Siyi Li, Abhinav Gupta, Dit-Yan Yeung
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural network (CNN) models have demonstrated great success in various computer vision tasks including image classification and object detection. However, some equally important tasks such as visual tracking remain relatively unexplored. We believe that a major hurdle that hinders the application of CNN to visual tracking is the lack of properly labeled training data. While existing applications that liberate the power of CNN often need an enormous amount of training data in the order of millions, visual tracking applications typically have only one labeled example in the first frame of each video. We address this research issue here by pre-training a CNN offline and then transferring the rich feature hierarchies learned to online tracking. The CNN is also fine-tuned during online tracking to adapt to the appearance of the tracked target specified in the first video frame. To fit the characteristics of object tracking, we first pre-train the CNN to recognize what is an object, and then propose to generate a probability map instead of producing a simple class label. Using two challenging open benchmarks for performance evaluation, our proposed tracker has demonstrated substantial improvement over other state-of-the-art trackers.



### Microscopic Advances with Large-Scale Learning: Stochastic Optimization for Cryo-EM
- **Arxiv ID**: http://arxiv.org/abs/1501.04656v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1501.04656v2)
- **Published**: 2015-01-19 22:07:27+00:00
- **Updated**: 2015-01-27 21:13:28+00:00
- **Authors**: Ali Punjani, Marcus A. Brubaker
- **Comment**: Presented at NIPS 2014 Workshop on Machine Learning in Computational
  Biology http://mlcb.org
- **Journal**: None
- **Summary**: Determining the 3D structures of biological molecules is a key problem for both biology and medicine. Electron Cryomicroscopy (Cryo-EM) is a promising technique for structure estimation which relies heavily on computational methods to reconstruct 3D structures from 2D images. This paper introduces the challenging Cryo-EM density estimation problem as a novel application for stochastic optimization techniques. Structure discovery is formulated as MAP estimation in a probabilistic latent-variable model, resulting in an optimization problem to which an array of seven stochastic optimization methods are applied. The methods are tested on both real and synthetic data, with some methods recovering reasonable structures in less than one epoch from a random initialization. Complex quasi-Newton methods are found to converge more slowly than simple gradient-based methods, but all stochastic methods are found to converge to similar optima. This method represents a major improvement over existing methods as it is significantly faster and is able to converge from a random initialization.



