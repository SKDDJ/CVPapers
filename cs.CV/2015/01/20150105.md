# Arxiv Papers in cs.CV on 2015-01-05
### A Deep-structured Conditional Random Field Model for Object Silhouette Tracking
- **Arxiv ID**: http://arxiv.org/abs/1501.00752v2
- **DOI**: 10.1371/journal.pone.0133036
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1501.00752v2)
- **Published**: 2015-01-05 03:09:34+00:00
- **Updated**: 2015-08-04 18:27:20+00:00
- **Authors**: Mohammad Shafiee, Zohreh Azimifar, Alexander Wong
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: In this work, we introduce a deep-structured conditional random field (DS-CRF) model for the purpose of state-based object silhouette tracking. The proposed DS-CRF model consists of a series of state layers, where each state layer spatially characterizes the object silhouette at a particular point in time. The interactions between adjacent state layers are established by inter-layer connectivity dynamically determined based on inter-frame optical flow. By incorporate both spatial and temporal context in a dynamic fashion within such a deep-structured probabilistic graphical model, the proposed DS-CRF model allows us to develop a framework that can accurately and efficiently track object silhouettes that can change greatly over time, as well as under different situations such as occlusion and multiple targets within the scene. Experiment results using video surveillance datasets containing different scenarios such as occlusion and multiple targets showed that the proposed DS-CRF approach provides strong object silhouette tracking performance when compared to baseline methods such as mean-shift tracking, as well as state-of-the-art methods such as context tracking and boosted particle filtering.



### Hashing with binary autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1501.00756v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1501.00756v1)
- **Published**: 2015-01-05 03:49:02+00:00
- **Updated**: 2015-01-05 03:49:02+00:00
- **Authors**: Miguel Á. Carreira-Perpiñán, Ramin Raziperchikolaei
- **Comment**: 22 pages, 11 figures
- **Journal**: None
- **Summary**: An attractive approach for fast search in image databases is binary hashing, where each high-dimensional, real-valued image is mapped onto a low-dimensional, binary vector and the search is done in this binary space. Finding the optimal hash function is difficult because it involves binary constraints, and most approaches approximate the optimization by relaxing the constraints and then binarizing the result. Here, we focus on the binary autoencoder model, which seeks to reconstruct an image from the binary code produced by the hash function. We show that the optimization can be simplified with the method of auxiliary coordinates. This reformulates the optimization as alternating two easier steps: one that learns the encoder and decoder separately, and one that optimizes the code for each image. Image retrieval experiments, using precision/recall and a measure of code utilization, show the resulting hash function outperforms or is competitive with state-of-the-art methods for binary hashing.



### Sparse Deep Stacking Network for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1501.00777v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1501.00777v1)
- **Published**: 2015-01-05 08:07:31+00:00
- **Updated**: 2015-01-05 08:07:31+00:00
- **Authors**: Jun Li, Heyou Chang, Jian Yang
- **Comment**: 8 pages, 3 figures, AAAI-2015
- **Journal**: None
- **Summary**: Sparse coding can learn good robust representation to noise and model more higher-order representation for image classification. However, the inference algorithm is computationally expensive even though the supervised signals are used to learn compact and discriminative dictionaries in sparse coding techniques. Luckily, a simplified neural network module (SNNM) has been proposed to directly learn the discriminative dictionaries for avoiding the expensive inference. But the SNNM module ignores the sparse representations. Therefore, we propose a sparse SNNM module by adding the mixed-norm regularization (l1/l2 norm). The sparse SNNM modules are further stacked to build a sparse deep stacking network (S-DSN). In the experiments, we evaluate S-DSN with four databases, including Extended YaleB, AR, 15 scene and Caltech101. Experimental results show that our model outperforms related classification methods with only a linear classifier. It is worth noting that we reach 98.8% recognition accuracy on 15 scene.



### Group $K$-Means
- **Arxiv ID**: http://arxiv.org/abs/1501.00825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1501.00825v1)
- **Published**: 2015-01-05 11:43:26+00:00
- **Updated**: 2015-01-05 11:43:26+00:00
- **Authors**: Jianfeng Wang, Shuicheng Yan, Yi Yang, Mohan S Kankanhalli, Shipeng Li, Jingdong Wang
- **Comment**: The developed algorithm is similar with "Christopher F. Barnes, A new
  multiple path search technique for residual vector quantizers, 1994", but we
  conduct the research independently and apply it in data/feature compression
  and image retrieval
- **Journal**: None
- **Summary**: We study how to learn multiple dictionaries from a dataset, and approximate any data point by the sum of the codewords each chosen from the corresponding dictionary. Although theoretically low approximation errors can be achieved by the global solution, an effective solution has not been well studied in practice. To solve the problem, we propose a simple yet effective algorithm \textit{Group $K$-Means}. Specifically, we take each dictionary, or any two selected dictionaries, as a group of $K$-means cluster centers, and then deal with the approximation issue by minimizing the approximation errors. Besides, we propose a hierarchical initialization for such a non-convex problem. Experimental results well validate the effectiveness of the approach.



### Inverse Renormalization Group Transformation in Bayesian Image Segmentations
- **Arxiv ID**: http://arxiv.org/abs/1501.00834v1
- **DOI**: 10.7566/JPSJ.84.045001
- **Categories**: **cs.CV**, cond-mat.stat-mech, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1501.00834v1)
- **Published**: 2015-01-05 12:20:09+00:00
- **Updated**: 2015-01-05 12:20:09+00:00
- **Authors**: Kazuyuki Tanaka, Shun Kataoka, Muneki Yasuda, Masayuki Ohzeki
- **Comment**: 6 pages, 2 figures
- **Journal**: Journal of the Physical Society of Japan 84 (2015) 045001
- **Summary**: A new Bayesian image segmentation algorithm is proposed by combining a loopy belief propagation with an inverse real space renormalization group transformation to reduce the computational time. In results of our experiment, we observe that the proposed method can reduce the computational time to less than one-tenth of that taken by conventional Bayesian approaches.



### Fast forward feature selection for the nonlinear classification of hyperspectral images
- **Arxiv ID**: http://arxiv.org/abs/1501.00857v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1501.00857v1)
- **Published**: 2015-01-05 13:37:37+00:00
- **Updated**: 2015-01-05 13:37:37+00:00
- **Authors**: Mathieu Fauvel, Clement Dechesne, Anthony Zullo, Frédéric Ferraty
- **Comment**: None
- **Journal**: None
- **Summary**: A fast forward feature selection algorithm is presented in this paper. It is based on a Gaussian mixture model (GMM) classifier. GMM are used for classifying hyperspectral images. The algorithm selects iteratively spectral features that maximizes an estimation of the classification rate. The estimation is done using the k-fold cross validation. In order to perform fast in terms of computing time, an efficient implementation is proposed. First, the GMM can be updated when the estimation of the classification rate is computed, rather than re-estimate the full model. Secondly, using marginalization of the GMM, sub models can be directly obtained from the full model learned with all the spectral features. Experimental results for two real hyperspectral data sets show that the method performs very well in terms of classification accuracy and processing time. Furthermore, the extracted model contains very few spectral channels.



### Learning to Recognize Pedestrian Attribute
- **Arxiv ID**: http://arxiv.org/abs/1501.00901v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1501.00901v2)
- **Published**: 2015-01-05 15:53:01+00:00
- **Updated**: 2015-04-29 06:35:50+00:00
- **Authors**: Yubin Deng, Ping Luo, Chen Change Loy, Xiaoou Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Learning to recognize pedestrian attributes at far distance is a challenging problem in visual surveillance since face and body close-shots are hardly available; instead, only far-view image frames of pedestrian are given. In this study, we present an alternative approach that exploits the context of neighboring pedestrian images for improved attribute inference compared to the conventional SVM-based method. In addition, we conduct extensive experiments to evaluate the informativeness of background and foreground features for attribute recognition. Experiments are based on our newly released pedestrian attribute dataset, which is by far the largest and most diverse of its kind.



### Adaptive Objectness for Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1501.00909v1
- **DOI**: 10.1109/LSP.2016.2556706
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1501.00909v1)
- **Published**: 2015-01-05 16:24:37+00:00
- **Updated**: 2015-01-05 16:24:37+00:00
- **Authors**: Pengpeng Liang, Chunyuan Liao, Xue Mei, Haibin Ling
- **Comment**: None
- **Journal**: None
- **Summary**: Object tracking is a long standing problem in vision. While great efforts have been spent to improve tracking performance, a simple yet reliable prior knowledge is left unexploited: the target object in tracking must be an object other than non-object. The recently proposed and popularized objectness measure provides a natural way to model such prior in visual tracking. Thus motivated, in this paper we propose to adapt objectness for visual object tracking. Instead of directly applying an existing objectness measure that is generic and handles various objects and environments, we adapt it to be compatible to the specific tracking sequence and object. More specifically, we use the newly proposed BING objectness as the base, and then train an object-adaptive objectness for each tracking task. The training is implemented by using an adaptive support vector machine that integrates information from the specific tracking target into the BING measure. We emphasize that the benefit of the proposed adaptive objectness, named ADOBING, is generic. To show this, we combine ADOBING with seven top performed trackers in recent evaluations. We run the ADOBING-enhanced trackers with their base trackers on two popular benchmarks, the CVPR2013 benchmark (50 sequences) and the Princeton Tracking Benchmark (100 sequences). On both benchmarks, our methods not only consistently improve the base trackers, but also achieve the best known performances. Noting that the way we integrate objectness in visual tracking is generic and straightforward, we expect even more improvement by using tracker-specific objectness.



### Salient Object Detection: A Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1501.02741v2
- **DOI**: 10.1109/TIP.2015.2487833
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1501.02741v2)
- **Published**: 2015-01-05 20:24:01+00:00
- **Updated**: 2018-02-27 06:24:39+00:00
- **Authors**: Ali Borji, Ming-Ming Cheng, Huaizu Jiang, Jia Li
- **Comment**: None
- **Journal**: Image Processing, IEEE Transactions on (Volume:24, Issue: 12),
  2015
- **Summary**: We extensively compare, qualitatively and quantitatively, 40 state-of-the-art models (28 salient object detection, 10 fixation prediction, 1 objectness, and 1 baseline) over 6 challenging datasets for the purpose of benchmarking salient object detection and segmentation methods. From the results obtained so far, our evaluation shows a consistent rapid progress over the last few years in terms of both accuracy and running time. The top contenders in this benchmark significantly outperform the models identified as the best in the previous benchmark conducted just two years ago. We find that the models designed specifically for salient object detection generally work better than models in closely related areas, which in turn provides a precise definition and suggests an appropriate treatment of this problem that distinguishes it from other problems. In particular, we analyze the influences of center bias and scene complexity in model performance, which, along with the hard cases for state-of-the-art models, provide useful hints towards constructing more challenging large scale datasets and better saliency models. Finally, we propose probable solutions for tackling several open problems such as evaluation scores and dataset bias, which also suggest future research directions in the rapidly-growing field of salient object detection.



