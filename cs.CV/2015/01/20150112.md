# Arxiv Papers in cs.CV on 2015-01-12
### A Dataset for Movie Description
- **Arxiv ID**: http://arxiv.org/abs/1501.02530v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1501.02530v1)
- **Published**: 2015-01-12 03:31:33+00:00
- **Updated**: 2015-01-12 03:31:33+00:00
- **Authors**: Anna Rohrbach, Marcus Rohrbach, Niket Tandon, Bernt Schiele
- **Comment**: None
- **Journal**: None
- **Summary**: Descriptive video service (DVS) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed DVS, which is temporally aligned to full length HD movies. In addition we also collected the aligned movie scripts which have been used in prior work and compare the two different sources of descriptions. In total the Movie Description dataset contains a parallel corpus of over 54,000 sentences and video snippets from 72 HD movies. We characterize the dataset by benchmarking different approaches for generating video descriptions. Comparing DVS to scripts, we find that DVS is far more visual and describes precisely what is shown rather than what should happen according to the scripts created prior to movie production.



### Tri-Subject Kinship Verification: Understanding the Core of A Family
- **Arxiv ID**: http://arxiv.org/abs/1501.02555v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1501.02555v3)
- **Published**: 2015-01-12 07:32:45+00:00
- **Updated**: 2015-07-21 11:50:41+00:00
- **Authors**: Xiaoqian Qin, Xiaoyang Tan, Songcan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: One major challenge in computer vision is to go beyond the modeling of individual objects and to investigate the bi- (one-versus-one) or tri- (one-versus-two) relationship among multiple visual entities, answering such questions as whether a child in a photo belongs to given parents. The child-parents relationship plays a core role in a family and understanding such kin relationship would have fundamental impact on the behavior of an artificial intelligent agent working in the human world. In this work, we tackle the problem of one-versus-two (tri-subject) kinship verification and our contributions are three folds: 1) a novel relative symmetric bilinear model (RSBM) introduced to model the similarity between the child and the parents, by incorporating the prior knowledge that a child may resemble a particular parent more than the other; 2) a spatially voted method for feature selection, which jointly selects the most discriminative features for the child-parents pair, while taking local spatial information into account; 3) a large scale tri-subject kinship database characterized by over 1,000 child-parents families. Extensive experiments on KinFaceW, Family101 and our newly released kinship database show that the proposed method outperforms several previous state of the art methods, while could also be used to significantly boost the performance of one-versus-one kinship verification when the information about both parents are available.



### EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1501.02565v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1501.02565v2)
- **Published**: 2015-01-12 08:19:09+00:00
- **Updated**: 2015-05-19 14:46:16+00:00
- **Authors**: Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach for optical flow estimation , targeted at large displacements with significant oc-clusions. It consists of two steps: i) dense matching by edge-preserving interpolation from a sparse set of matches; ii) variational energy minimization initialized with the dense matches. The sparse-to-dense interpolation relies on an appropriate choice of the distance, namely an edge-aware geodesic distance. This distance is tailored to handle occlusions and motion boundaries -- two common and difficult issues for optical flow computation. We also propose an approximation scheme for the geodesic distance to allow fast computation without loss of performance. Subsequent to the dense interpolation step, standard one-level variational energy minimization is carried out on the dense matches to obtain the final flow estimation. The proposed approach, called Edge-Preserving Interpolation of Correspondences (EpicFlow) is fast and robust to large displacements. It significantly outperforms the state of the art on MPI-Sintel and performs on par on Kitti and Middlebury.



### Combining Language and Vision with a Multimodal Skip-gram Model
- **Arxiv ID**: http://arxiv.org/abs/1501.02598v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1501.02598v3)
- **Published**: 2015-01-12 10:48:32+00:00
- **Updated**: 2015-03-12 09:47:33+00:00
- **Authors**: Angeliki Lazaridou, Nghia The Pham, Marco Baroni
- **Comment**: accepted at NAACL 2015, camera ready version, 11 pages
- **Journal**: None
- **Summary**: We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual information into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM) build vector-based word representations by learning to predict linguistic contexts in text corpora. However, for a restricted set of words, the models are also exposed to visual representations of the objects they denote (extracted from natural images), and must predict linguistic and visual features jointly. The MMSKIP-GRAM models achieve good performance on a variety of semantic benchmarks. Moreover, since they propagate visual information to all words, we use them to improve image labeling and retrieval in the zero-shot setup, where the test concepts are never seen during model training. Finally, the MMSKIP-GRAM models discover intriguing visual properties of abstract words, paving the way to realistic implementations of embodied theories of meaning.



### Texture Retrieval via the Scattering Transform
- **Arxiv ID**: http://arxiv.org/abs/1501.02655v4
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1501.02655v4)
- **Published**: 2015-01-12 14:22:28+00:00
- **Updated**: 2015-06-01 10:44:19+00:00
- **Authors**: Alexander Sagel, Dominik Meyer, Hao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: This work studies the problem of content-based image retrieval, specifically, texture retrieval. It focuses on feature extraction and similarity measure for texture images. Our approach employs a recently developed method, the so-called Scattering transform, for the process of feature extraction in texture retrieval. It shares a distinctive property of providing a robust representation, which is stable with respect to spatial deformations. Recent work has demonstrated its capability for texture classification, and hence as a promising candidate for the problem of texture retrieval.   Moreover, we adopt a common approach of measuring the similarity of textures by comparing the subband histograms of a filterbank transform. To this end we derive a similarity measure based on the popular Bhattacharyya Kernel. Despite the popularity of describing histograms using parametrized probability density functions, such as the Generalized Gaussian Distribution, it is unfortunately not applicable for describing most of the Scattering transform subbands, due to the complex modulus performed on each one of them. In this work, we propose to use the Weibull distribution to model the Scattering subbands of descendant layers.   Our numerical experiments demonstrated the effectiveness of the proposed approach, in comparison with several state of the arts.



### From Visual Attributes to Adjectives through Decompositional Distributional Semantics
- **Arxiv ID**: http://arxiv.org/abs/1501.02714v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1501.02714v2)
- **Published**: 2015-01-12 16:48:19+00:00
- **Updated**: 2015-03-24 12:32:05+00:00
- **Authors**: Angeliki Lazaridou, Georgiana Dinu, Adam Liska, Marco Baroni
- **Comment**: accepted at Transactions of the Association for Computational
  Linguistics (TACL), 3/2015
- **Journal**: None
- **Summary**: As automated image analysis progresses, there is increasing interest in richer linguistic annotation of pictures, with attributes of objects (e.g., furry, brown...) attracting most attention. By building on the recent "zero-shot learning" approach, and paying attention to the linguistic nature of attributes as noun modifiers, and specifically adjectives, we show that it is possible to tag images with attribute-denoting adjectives even when no training data containing the relevant annotation are available. Our approach relies on two key observations. First, objects can be seen as bundles of attributes, typically expressed as adjectival modifiers (a dog is something furry, brown, etc.), and thus a function trained to map visual representations of objects to nominal labels can implicitly learn to map attributes to adjectives. Second, objects and attributes come together in pictures (the same thing is a dog and it is brown). We can thus achieve better attribute (and object) label retrieval by treating images as "visual phrases", and decomposing their linguistic representation into an attribute-denoting adjective and an object-denoting noun. Our approach performs comparably to a method exploiting manual attribute annotation, it outperforms various competitive alternatives in both attribute and object annotation, and it automatically constructs attribute-centric representations that significantly improve performance in supervised object recognition.



### A Survey on Recent Advances of Computer Vision Algorithms for Egocentric Video
- **Arxiv ID**: http://arxiv.org/abs/1501.02825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1501.02825v1)
- **Published**: 2015-01-12 21:14:56+00:00
- **Updated**: 2015-01-12 21:14:56+00:00
- **Authors**: Sven Bambach
- **Comment**: None
- **Journal**: None
- **Summary**: Recent technological advances have made lightweight, head mounted cameras both practical and affordable and products like Google Glass show first approaches to introduce the idea of egocentric (first-person) video to the mainstream. Interestingly, the computer vision community has only recently started to explore this new domain of egocentric vision, where research can roughly be categorized into three areas: Object recognition, activity detection/recognition, video summarization. In this paper, we try to give a broad overview about the different problems that have been addressed and collect and compare evaluation results. Moreover, along with the emergence of this new domain came the introduction of numerous new and versatile benchmark datasets, which we summarize and compare as well.



