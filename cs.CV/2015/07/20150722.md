# Arxiv Papers in cs.CV on 2015-07-22
### Banzhaf Random Forests
- **Arxiv ID**: http://arxiv.org/abs/1507.06105v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1507.06105v1)
- **Published**: 2015-07-22 09:10:15+00:00
- **Updated**: 2015-07-22 09:10:15+00:00
- **Authors**: Jianyuan Sun, Guoqiang Zhong, Junyu Dong, Yajuan Cai
- **Comment**: arXiv admin note: text overlap with arXiv:1302.4853 by other authors
- **Journal**: None
- **Summary**: Random forests are a type of ensemble method which makes predictions by combining the results of several independent trees. However, the theory of random forests has long been outpaced by their application. In this paper, we propose a novel random forests algorithm based on cooperative game theory. Banzhaf power index is employed to evaluate the power of each feature by traversing possible feature coalitions. Unlike the previously used information gain rate of information theory, which simply chooses the most informative feature, the Banzhaf power index can be considered as a metric of the importance of each feature on the dependency among a group of features. More importantly, we have proved the consistency of the proposed algorithm, named Banzhaf random forests (BRF). This theoretical analysis takes a step towards narrowing the gap between the theory and practice of random forests for classification problems. Experiments on several UCI benchmark data sets show that BRF is competitive with state-of-the-art classifiers and dramatically outperforms previous consistent random forests. Particularly, it is much more efficient than previous consistent random forests.



### Towards Storytelling from Visual Lifelogging: An Overview
- **Arxiv ID**: http://arxiv.org/abs/1507.06120v5
- **DOI**: 10.1109/THMS.2016.2616296
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.06120v5)
- **Published**: 2015-07-22 10:23:50+00:00
- **Updated**: 2016-07-20 07:38:01+00:00
- **Authors**: Marc Bola√±os, Mariella Dimiccoli, Petia Radeva
- **Comment**: 16 pages, 11 figures, Submitted to IEEE Transactions on Human-Machine
  Systems
- **Journal**: None
- **Summary**: Visual lifelogging consists of acquiring images that capture the daily experiences of the user by wearing a camera over a long period of time. The pictures taken offer considerable potential for knowledge mining concerning how people live their lives, hence, they open up new opportunities for many potential applications in fields including healthcare, security, leisure and the quantified self. However, automatically building a story from a huge collection of unstructured egocentric data presents major challenges. This paper provides a thorough review of advances made so far in egocentric data analysis, and in view of the current state of the art, indicates new lines of research to move us towards storytelling from visual lifelogging.



### Data-free parameter pruning for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1507.06149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.06149v1)
- **Published**: 2015-07-22 12:18:21+00:00
- **Updated**: 2015-07-22 12:18:21+00:00
- **Authors**: Suraj Srinivas, R. Venkatesh Babu
- **Comment**: BMVC 2015
- **Journal**: None
- **Summary**: Deep Neural nets (NNs) with millions of parameters are at the heart of many state-of-the-art computer vision systems today. However, recent works have shown that much smaller models can achieve similar levels of performance. In this work, we address the problem of pruning parameters in a trained NN model. Instead of removing individual weights one at a time as done in previous works, we remove one neuron at a time. We show how similar neurons are redundant, and propose a systematic way to remove them. Our experiments in pruning the densely connected layers show that we can remove upto 85\% of the total parameters in an MNIST-trained network, and about 35\% for AlexNet without significantly affecting performance. Our method can be applied on top of most networks with a fully connected layer to give a smaller network.



### Bayesian Time-of-Flight for Realtime Shape, Illumination and Albedo
- **Arxiv ID**: http://arxiv.org/abs/1507.06173v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.06173v1)
- **Published**: 2015-07-22 13:18:14+00:00
- **Updated**: 2015-07-22 13:18:14+00:00
- **Authors**: Amit Adam, Christoph Dann, Omer Yair, Shai Mazor, Sebastian Nowozin
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a computational model for shape, illumination and albedo inference in a pulsed time-of-flight (TOF) camera. In contrast to TOF cameras based on phase modulation, our camera enables general exposure profiles. This results in added flexibility and requires novel computational approaches.   To address this challenge we propose a generative probabilistic model that accurately relates latent imaging conditions to observed camera responses. While principled, realtime inference in the model turns out to be infeasible, and we propose to employ efficient non-parametric regression trees to approximate the model outputs. As a result we are able to provide, for each pixel, at video frame rate, estimates and uncertainty for depth, effective albedo, and ambient light intensity. These results we present are state-of-the-art in depth imaging.   The flexibility of our approach allows us to easily enrich our generative model. We demonstrate that by extending the original single-path model to a two-path model, capable of describing some multipath effects. The new model is seamlessly integrated in the system at no additional computational cost.   Our work also addresses the important question of optimal exposure design in pulsed TOF systems. Finally, for benchmark purposes and to obtain realistic empirical priors of multipath and insights into this phenomena, we propose a physically accurate simulation of multipath phenomena.



### Particle detection and tracking in fluorescence time-lapse imaging: a contrario approach
- **Arxiv ID**: http://arxiv.org/abs/1507.06266v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.06266v5)
- **Published**: 2015-07-22 17:43:54+00:00
- **Updated**: 2016-12-22 13:05:45+00:00
- **Authors**: Mariella Dimiccoli, Jean-Pascal Jacob, Lionel Moisan
- **Comment**: Published in Journal of Machine Vision and Applications
- **Journal**: None
- **Summary**: This paper proposes a probabilistic approach for the detection and the tracking of particles in fluorescent time-lapse imaging. In the presence of a very noised and poor-quality data, particles and trajectories can be characterized by an a contrario model, that estimates the probability of observing the structures of interest in random data. This approach, first introduced in the modeling of human visual perception and then successfully applied in many image processing tasks, leads to algorithms that neither require a previous learning stage, nor a tedious parameter tuning and are very robust to noise. Comparative evaluations against a well-established baseline show that the proposed approach outperforms the state of the art.



### Part Localization using Multi-Proposal Consensus for Fine-Grained Categorization
- **Arxiv ID**: http://arxiv.org/abs/1507.06332v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.06332v1)
- **Published**: 2015-07-22 20:21:59+00:00
- **Updated**: 2015-07-22 20:21:59+00:00
- **Authors**: Kevin J. Shih, Arun Mallya, Saurabh Singh, Derek Hoiem
- **Comment**: BMVC 2015
- **Journal**: None
- **Summary**: We present a simple deep learning framework to simultaneously predict keypoint locations and their respective visibilities and use those to achieve state-of-the-art performance for fine-grained classification. We show that by conditioning the predictions on object proposals with sufficient image support, our method can do well without complicated spatial reasoning. Instead, inference methods with robustness to outliers, yield state-of-the-art for keypoint localization. We demonstrate the effectiveness of our accurate keypoint localization and visibility prediction on the fine-grained bird recognition task with and without ground truth bird bounding boxes, and outperform existing state-of-the-art methods by over 2%.



