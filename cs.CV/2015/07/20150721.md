# Arxiv Papers in cs.CV on 2015-07-21
### Bottom-Up and Top-Down Reasoning with Hierarchical Rectified Gaussians
- **Arxiv ID**: http://arxiv.org/abs/1507.05699v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.05699v5)
- **Published**: 2015-07-21 04:00:44+00:00
- **Updated**: 2016-05-04 22:50:35+00:00
- **Authors**: Peiyun Hu, Deva Ramanan
- **Comment**: To appear in CVPR 2016
- **Journal**: None
- **Summary**: Convolutional neural nets (CNNs) have demonstrated remarkable performance in recent history. Such approaches tend to work in a unidirectional bottom-up feed-forward fashion. However, practical experience and biological evidence tells us that feedback plays a crucial role, particularly for detailed spatial understanding tasks. This work explores bidirectional architectures that also reason with top-down feedback: neural units are influenced by both lower and higher-level units.   We do so by treating units as rectified latent variables in a quadratic energy function, which can be seen as a hierarchical Rectified Gaussian model (RGs). We show that RGs can be optimized with a quadratic program (QP), that can in turn be optimized with a recurrent neural network (with rectified linear units). This allows RGs to be trained with GPU-optimized gradient descent. From a theoretical perspective, RGs help establish a connection between CNNs and hierarchical probabilistic models. From a practical perspective, RGs are well suited for detailed spatial tasks that can benefit from top-down reasoning. We illustrate them on the challenging task of keypoint localization under occlusions, where local bottom-up evidence may be misleading. We demonstrate state-of-the-art results on challenging benchmarks.



### An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1507.05717v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.05717v1)
- **Published**: 2015-07-21 06:26:32+00:00
- **Updated**: 2015-07-21 06:26:32+00:00
- **Authors**: Baoguang Shi, Xiang Bai, Cong Yao
- **Comment**: 5 figures
- **Journal**: None
- **Summary**: Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.



### Rule Of Thumb: Deep derotation for improved fingertip detection
- **Arxiv ID**: http://arxiv.org/abs/1507.05726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.05726v1)
- **Published**: 2015-07-21 07:23:45+00:00
- **Updated**: 2015-07-21 07:23:45+00:00
- **Authors**: Aaron Wetzler, Ron Slossberg, Ron Kimmel
- **Comment**: To be published in proceedings of BMVC 2015
- **Journal**: None
- **Summary**: We investigate a novel global orientation regression approach for articulated objects using a deep convolutional neural network. This is integrated with an in-plane image derotation scheme, DeROT, to tackle the problem of per-frame fingertip detection in depth images. The method reduces the complexity of learning in the space of articulated poses which is demonstrated by using two distinct state-of-the-art learning based hand pose estimation methods applied to fingertip detection. Significant classification improvements are shown over the baseline implementation. Our framework involves no tracking, kinematic constraints or explicit prior model of the articulated object in hand. To support our approach we also describe a new pipeline for high accuracy magnetic annotation and labeling of objects imaged by a depth camera.



### Online Metric-Weighted Linear Representations for Robust Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1507.05737v1
- **DOI**: 10.1109/TPAMI.2015.2469276
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.05737v1)
- **Published**: 2015-07-21 08:02:36+00:00
- **Updated**: 2015-07-21 08:02:36+00:00
- **Authors**: Xi Li, Chunhua Shen, Anthony Dick, Zhongfei Zhang, Yueting Zhuang
- **Comment**: 51 pages. Appearing in IEEE Transactions on Pattern Analysis and
  Machine Intelligence
- **Journal**: None
- **Summary**: In this paper, we propose a visual tracker based on a metric-weighted linear representation of appearance. In order to capture the interdependence of different feature dimensions, we develop two online distance metric learning methods using proximity comparison information and structured output learning. The learned metric is then incorporated into a linear representation of appearance.   We show that online distance metric learning significantly improves the robustness of the tracker, especially on those sequences exhibiting drastic appearance changes. In order to bound growth in the number of training samples, we design a time-weighted reservoir sampling method.   Moreover, we enable our tracker to automatically perform object identification during the process of object tracking, by introducing a collection of static template samples belonging to several object classes of interest. Object identification results for an entire video sequence are achieved by systematically combining the tracking information and visual recognition at each frame. Experimental results on challenging video sequences demonstrate the effectiveness of the method for both inter-frame tracking and object identification.



### Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos
- **Arxiv ID**: http://arxiv.org/abs/1507.05738v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.05738v3)
- **Published**: 2015-07-21 08:07:50+00:00
- **Updated**: 2017-06-09 10:42:09+00:00
- **Authors**: Serena Yeung, Olga Russakovsky, Ning Jin, Mykhaylo Andriluka, Greg Mori, Li Fei-Fei
- **Comment**: To appear in IJCV
- **Journal**: None
- **Summary**: Every moment counts in action recognition. A comprehensive understanding of human activity in video requires labeling every frame according to the actions occurring, placing multiple labels densely over a video sequence. To study this problem we extend the existing THUMOS dataset and introduce MultiTHUMOS, a new dataset of dense labels over unconstrained internet videos. Modeling multiple, dense labels benefits from temporal relations within and across classes. We define a novel variant of long short-term memory (LSTM) deep networks for modeling these temporal relations via multiple input and output connections. We show that this model improves action labeling accuracy and further enables deeper understanding tasks ranging from structured retrieval to action prediction.



### Compression of Fully-Connected Layer in Neural Network by Kronecker Product
- **Arxiv ID**: http://arxiv.org/abs/1507.05775v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1507.05775v2)
- **Published**: 2015-07-21 10:29:11+00:00
- **Updated**: 2015-07-22 11:59:08+00:00
- **Authors**: Shuchang Zhou, Jia-Nan Wu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose and study a technique to reduce the number of parameters and computation time in fully-connected layers of neural networks using Kronecker product, at a mild cost of the prediction quality. The technique proceeds by replacing Fully-Connected layers with so-called Kronecker Fully-Connected layers, where the weight matrices of the FC layers are approximated by linear combinations of multiple Kronecker products of smaller matrices. In particular, given a model trained on SVHN dataset, we are able to construct a new KFC model with 73\% reduction in total number of parameters, while the error only rises mildly. In contrast, using low-rank method can only achieve 35\% reduction in total number of parameters given similar quality degradation allowance. If we only compare the KFC layer with its counterpart fully-connected layer, the reduction in the number of parameters exceeds 99\%. The amount of computation is also reduced as we replace matrix product of the large matrices in FC layers with matrix products of a few smaller matrices in KFC layers. Further experiments on MNIST, SVHN and some Chinese Character recognition models also demonstrate effectiveness of our technique.



### The Cumulative Distribution Transform and Linear Pattern Classification
- **Arxiv ID**: http://arxiv.org/abs/1507.05936v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.05936v3)
- **Published**: 2015-07-21 18:19:31+00:00
- **Updated**: 2017-02-14 17:17:10+00:00
- **Authors**: Se Rim Park, Soheil Kolouri, Shinjini Kundu, Gustavo Rohde
- **Comment**: None
- **Journal**: None
- **Summary**: Discriminating data classes emanating from sensors is an important problem with many applications in science and technology. We describe a new transform for pattern identification that interprets patterns as probability density functions, and has special properties with regards to classification. The transform, which we denote as the Cumulative Distribution Transform (CDT) is invertible, with well defined forward and inverse operations. We show that it can be useful in `parsing out' variations (confounds) that are `Lagrangian' (displacement and intensity variations) by converting these to `Eulerian' (intensity variations) in transform space. This conversion is the basis for our main result that describes when the CDT can allow for linear classification to be possible in transform space. We also describe several properties of the transform and show, with computational experiments that used both real and simulated data, that the CDT can help render a variety of real world problems simpler to solve.



