# Arxiv Papers in cs.CV on 2015-07-17
### RBIR Based on Signature Graph
- **Arxiv ID**: http://arxiv.org/abs/1507.04816v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.04816v1)
- **Published**: 2015-07-17 01:56:44+00:00
- **Updated**: 2015-07-17 01:56:44+00:00
- **Authors**: Thanh The Van, Thanh Manh Le
- **Comment**: 4 pages, 4 figures
- **Journal**: None
- **Summary**: This paper approaches the image retrieval system on the base of visual features local region RBIR (region-based image retrieval). First of all, the paper presents a method for extracting the interest points based on Harris-Laplace to create the feature region of the image. Next, in order to reduce the storage space and speed up query image, the paper builds the binary signature structure to describe the visual content of image. Based on the image's binary signature, the paper builds the SG (signature graph) to classify and store image's binary signatures. Since then, the paper builds the image retrieval algorithm on SG through the similar measure EMD (earth mover's distance) between the image's binary signatures. Last but not least, the paper gives an image retrieval model RBIR, experiments and assesses the image retrieval method on Corel image database over 10,000 images.



### Deep Multimodal Speaker Naming
- **Arxiv ID**: http://arxiv.org/abs/1507.04831v1
- **DOI**: 10.1145/2733373.2806293
- **Categories**: **cs.CV**, cs.LG, cs.MM, cs.SD, H.3
- **Links**: [PDF](http://arxiv.org/pdf/1507.04831v1)
- **Published**: 2015-07-17 04:13:12+00:00
- **Updated**: 2015-07-17 04:13:12+00:00
- **Authors**: Yongtao Hu, Jimmy Ren, Jingwen Dai, Chang Yuan, Li Xu, Wenping Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic speaker naming is the problem of localizing as well as identifying each speaking character in a TV/movie/live show video. This is a challenging problem mainly attributes to its multimodal nature, namely face cue alone is insufficient to achieve good performance. Previous multimodal approaches to this problem usually process the data of different modalities individually and merge them using handcrafted heuristics. Such approaches work well for simple scenes, but fail to achieve high performance for speakers with large appearance variations. In this paper, we propose a novel convolutional neural networks (CNN) based learning framework to automatically learn the fusion function of both face and audio cues. We show that without using face tracking, facial landmark localization or subtitle/transcript, our system with robust multimodal feature extraction is able to achieve state-of-the-art speaker naming performance evaluated on two diverse TV series. The dataset and implementation of our algorithm are publicly available online.



### Multiscale Adaptive Representation of Signals: I. The Basic Framework
- **Arxiv ID**: http://arxiv.org/abs/1507.04835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.04835v1)
- **Published**: 2015-07-17 05:24:45+00:00
- **Updated**: 2015-07-17 05:24:45+00:00
- **Authors**: Cheng Tai, Weinan E
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a framework for designing multi-scale, adaptive, shift-invariant frames and bi-frames for representing signals. The new framework, called AdaFrame, improves over dictionary learning-based techniques in terms of computational efficiency at inference time. It improves classical multi-scale basis such as wavelet frames in terms of coding efficiency. It provides an attractive alternative to dictionary learning-based techniques for low level signal processing tasks, such as compression and denoising, as well as high level tasks, such as feature extraction for object recognition. Connections with deep convolutional networks are also discussed. In particular, the proposed framework reveals a drawback in the commonly used approach for visualizing the activations of the intermediate layers in convolutional networks, and suggests a natural alternative.



### Learning Robust Deep Face Representation
- **Arxiv ID**: http://arxiv.org/abs/1507.04844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.04844v1)
- **Published**: 2015-07-17 06:21:31+00:00
- **Updated**: 2015-07-17 06:21:31+00:00
- **Authors**: Xiang Wu
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of convolution neural network, more and more researchers focus their attention on the advantage of CNN for face recognition task. In this paper, we propose a deep convolution network for learning a robust face representation. The deep convolution net is constructed by 4 convolution layers, 4 max pooling layers and 2 fully connected layers, which totally contains about 4M parameters. The Max-Feature-Map activation function is used instead of ReLU because the ReLU might lead to the loss of information due to the sparsity while the Max-Feature-Map can get the compact and discriminative feature vectors. The model is trained on CASIA-WebFace dataset and evaluated on LFW dataset. The result on LFW achieves 97.77% on unsupervised setting for single net.



### Analysis of the South Slavic Scripts by Run-Length Features of the Image Texture
- **Arxiv ID**: http://arxiv.org/abs/1507.04908v1
- **DOI**: 10.5755/j01.eee.21.4.12785
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1507.04908v1)
- **Published**: 2015-07-17 10:34:23+00:00
- **Updated**: 2015-07-17 10:34:23+00:00
- **Authors**: Darko Brodic, Zoran N. Milivojevic, Alessia Amelio
- **Comment**: 9 pages, 9 figures, In Electronics 2015, Elektronika IR
  Elektrotechnika, ISSN 1392-1215 (in press)
- **Journal**: Elektronika Ir Elektrotechnika, ISSN 1392-1215, VOL. 21, NO. 4,
  2015
- **Summary**: The paper proposes an algorithm for the script recognition based on the texture characteristics. The image texture is achieved by coding each letter with the equivalent script type (number code) according to its position in the text line. Each code is transformed into equivalent gray level pixel creating an 1-D image. Then, the image texture is subjected to the run-length analysis. This analysis extracts the run-length features, which are classified to make a distinction between the scripts under consideration. In the experiment, a custom oriented database is subject to the proposed algorithm. The database consists of some text documents written in Cyrillic, Latin and Glagolitic scripts. Furthermore, it is divided into training and test parts. The results of the experiment show that 3 out of 5 run-length features can be used for effective differentiation between the analyzed South Slavic scripts.



### Tree-based Visualization and Optimization for Image Collection
- **Arxiv ID**: http://arxiv.org/abs/1507.04913v1
- **DOI**: 10.1109/TCYB.2015.2448236
- **Categories**: **cs.MM**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1507.04913v1)
- **Published**: 2015-07-17 10:45:26+00:00
- **Updated**: 2015-07-17 10:45:26+00:00
- **Authors**: Xintong Han, Chongyang Zhang, Weiyao Lin, Mingliang Xu, Bin Sheng, Tao Mei
- **Comment**: This manuscript is the accepted version for T-CYB (IEEE Transactions
  on Cybernetics) IEEE Trans. Cybernetics, 2015
- **Journal**: None
- **Summary**: The visualization of an image collection is the process of displaying a collection of images on a screen under some specific layout requirements. This paper focuses on an important problem that is not well addressed by the previous methods: visualizing image collections into arbitrary layout shapes while arranging images according to user-defined semantic or visual correlations (e.g., color or object category). To this end, we first propose a property-based tree construction scheme to organize images of a collection into a tree structure according to user-defined properties. In this way, images can be adaptively placed with the desired semantic or visual correlations in the final visualization layout. Then, we design a two-step visualization optimization scheme to further optimize image layouts. As a result, multiple layout effects including layout shape and image overlap ratio can be effectively controlled to guarantee a satisfactory visualization. Finally, we also propose a tree-transfer scheme such that visualization layouts can be adaptively changed when users select different "images of interest". We demonstrate the effectiveness of our proposed approach through the comparisons with state-of-the-art visualization techniques.



### Classification of Complex Wishart Matrices with a Diffusion-Reaction System guided by Stochastic Distances
- **Arxiv ID**: http://arxiv.org/abs/1507.05033v1
- **DOI**: 10.1098/rsta.2015.0118
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.05033v1)
- **Published**: 2015-07-17 17:10:47+00:00
- **Updated**: 2015-07-17 17:10:47+00:00
- **Authors**: Luis Gomez, Luis Alvarez, Luis Mazorra, Alejandro C. Frery
- **Comment**: Accepted for publication in Philosophical Transactions A
- **Journal**: None
- **Summary**: We propose a new method for PolSAR (Polarimetric Synthetic Aperture Radar) imagery classification based on stochastic distances in the space of random matrices obeying complex Wishart distributions. Given a collection of prototypes $\{Z_m\}_{m=1}^M$ and a stochastic distance $d(.,.)$, we classify any random matrix $X$ using two criteria in an iterative setup. Firstly, we associate $X$ to the class which minimizes the weighted stochastic distance $w_md(X,Z_m)$, where the positive weights $w_m$ are computed to maximize the class discrimination power. Secondly, we improve the result by embedding the classification problem into a diffusion-reaction partial differential system where the diffusion term smooths the patches within the image, and the reaction term tends to move the pixel values towards the closest class prototype. In particular, the method inherits the benefits of speckle reduction by diffusion-like methods. Results on synthetic and real PolSAR data show the performance of the method.



### Massively Deep Artificial Neural Networks for Handwritten Digit Recognition
- **Arxiv ID**: http://arxiv.org/abs/1507.05053v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1507.05053v1)
- **Published**: 2015-07-17 17:48:49+00:00
- **Updated**: 2015-07-17 17:48:49+00:00
- **Authors**: Keiron O'Shea
- **Comment**: 2 pages, 1 figure
- **Journal**: None
- **Summary**: Greedy Restrictive Boltzmann Machines yield an fairly low 0.72% error rate on the famous MNIST database of handwritten digits. All that was required to achieve this result was a high number of hidden layers consisting of many neurons, and a graphics card to greatly speed up the rate of learning.



### Estimating Absolute-Phase Maps Using ESPIRiT and Virtual Conjugate Coils
- **Arxiv ID**: http://arxiv.org/abs/1509.03557v2
- **DOI**: 10.1002/mrm.26191
- **Categories**: **cs.CV**, cs.CE, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1509.03557v2)
- **Published**: 2015-07-17 22:35:14+00:00
- **Updated**: 2016-01-05 09:02:37+00:00
- **Authors**: Martin Uecker, Michael Lustig
- **Comment**: 15 pages, 5 figures
- **Journal**: Magnetic Resonance in Medicine 77 (2017) 1201-1207
- **Summary**: Purpose: To develop an ESPIRiT-based method to estimate coil sensitivities with image phase as a building block for efficient and robust image reconstruction with phase constraints. Theory and Methods: ESPIRiT is a new framework for calibration of the coil sensitivities and reconstruction in parallel Magnetic Resonance Imaging (MRI). Applying ESPIRiT to a combined set of physical and virtual conjugate coils (VCC-ESPIRiT) implicitly exploits conjugate symmetry in k-space similar to VCC-GRAPPA. Based on this method, a new post-processing step is proposed for the explicit computation of coil sensitivities that include the absolute phase of the image. The accuracy of the computed maps is directly validated using a test based on projection onto fully sampled coil images and also indirectly in phase-constrained parallel-imaging reconstructions. Results: The proposed method can estimate accurate sensitivities which include low-resolution image phase. In case of high-frequency phase variations VCC-ESPIRiT yields an additional set of maps that indicates the existence of a high-frequency phase component. Taking this additional set of maps into account can improve the robustness of phase-constrained parallel imaging. Conclusion: The extended VCC-ESPIRiT is a useful tool for phase-constrained imaging.



