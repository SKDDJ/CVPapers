# Arxiv Papers in cs.CV on 2015-07-09
### Neural Network Classifiers for Natural Food Products
- **Arxiv ID**: http://arxiv.org/abs/1507.02346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.02346v1)
- **Published**: 2015-07-09 02:09:19+00:00
- **Updated**: 2015-07-09 02:09:19+00:00
- **Authors**: Jaderick P. Pabico, Alona V. De Grano, Alan L. Zarsuela
- **Comment**: 8 pages, 5 figures, appeared in H.N. Adorna, R.E.O. Roxas, and A.L.
  Sioson (eds.) Proceedings of the 12th Philippine Computing Science Congress
  (PCSC 2012), De La Salle Canlubang, Bi\~nan, Laguna, Philippines, 01-03 March
  2012
- **Journal**: None
- **Summary**: Two cheap, off-the-shelf machine vision systems (MVS), each using an artificial neural network (ANN) as classifier, were developed, improved and evaluated to automate the classification of tomato ripeness and acceptability of eggs, respectively. Six thousand color images of human-graded tomatoes and 750 images of human-graded eggs were used to train, test, and validate several multi-layered ANNs. The ANNs output the corresponding grade of the produce by accepting as inputs the spectral patterns of the background-less image. In both MVS, the ANN with the highest validation rate was automatically chosen by a heuristic and its performance compared to that of the human graders'. Using the validation set, the MVS correctly graded 97.00\% and 86.00\% of the tomato and egg data, respectively. The human grader's, however, were measured to perform at a daily average of 92.65\% and 72.67\% for tomato and egg grading, respectively. This results show that an ANN-based MVS is a potential alternative to manual grading.



### The Shadows of a Cycle Cannot All Be Paths
- **Arxiv ID**: http://arxiv.org/abs/1507.02355v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV, math.MG
- **Links**: [PDF](http://arxiv.org/pdf/1507.02355v1)
- **Published**: 2015-07-09 02:47:50+00:00
- **Updated**: 2015-07-09 02:47:50+00:00
- **Authors**: Prosenjit Bose, Jean-Lou De Carufel, Michael G. Dobbins, Heuna Kim, Giovanni Viglietta
- **Comment**: 6 pages, 10 figures
- **Journal**: None
- **Summary**: A "shadow" of a subset $S$ of Euclidean space is an orthogonal projection of $S$ into one of the coordinate hyperplanes. In this paper we show that it is not possible for all three shadows of a cycle (i.e., a simple closed curve) in $\mathbb R^3$ to be paths (i.e., simple open curves).   We also show two contrasting results: the three shadows of a path in $\mathbb R^3$ can all be cycles (although not all convex) and, for every $d\geq 1$, there exists a $d$-sphere embedded in $\mathbb R^{d+2}$ whose $d+2$ shadows have no holes (i.e., they deformation-retract onto a point).



### Understanding Intra-Class Knowledge Inside CNN
- **Arxiv ID**: http://arxiv.org/abs/1507.02379v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.02379v2)
- **Published**: 2015-07-09 05:20:43+00:00
- **Updated**: 2015-07-21 10:18:57+00:00
- **Authors**: Donglai Wei, Bolei Zhou, Antonio Torrabla, William Freeman
- **Comment**: tech report for: http://vision03.csail.mit.edu/cnn_art/index.html
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) has been successful in image recognition tasks, and recent works shed lights on how CNN separates different classes with the learned inter-class knowledge through visualization. In this work, we instead visualize the intra-class knowledge inside CNN to better understand how an object class is represented in the fully-connected layers.   To invert the intra-class knowledge into more interpretable images, we propose a non-parametric patch prior upon previous CNN visualization models. With it, we show how different "styles" of templates for an object class are organized by CNN in terms of location and content, and represented in a hierarchical and ensemble way. Moreover, such intra-class knowledge can be used in many interesting applications, e.g. style-based image retrieval and style-based object completion.



### Learning Structured Ordinal Measures for Video based Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1507.02380v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.02380v1)
- **Published**: 2015-07-09 05:34:36+00:00
- **Updated**: 2015-07-09 05:34:36+00:00
- **Authors**: Ran He, Tieniu Tan, Larry Davis, Zhenan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a structured ordinal measure method for video-based face recognition that simultaneously learns ordinal filters and structured ordinal features. The problem is posed as a non-convex integer program problem that includes two parts. The first part learns stable ordinal filters to project video data into a large-margin ordinal space. The second seeks self-correcting and discrete codes by balancing the projected data and a rank-one ordinal matrix in a structured low-rank way. Unsupervised and supervised structures are considered for the ordinal matrix. In addition, as a complement to hierarchical structures, deep feature representations are integrated into our method to enhance coding stability. An alternating minimization method is employed to handle the discrete and low-rank constraints, yielding high-quality codes that capture prior structures well. Experimental results on three commonly used face video databases show that our method with a simple voting classifier can achieve state-of-the-art recognition rates using fewer features and samples.



### Towards Effective Codebookless Model for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1507.02385v2
- **DOI**: 10.1016/j.patcog.2016.03.004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.02385v2)
- **Published**: 2015-07-09 06:01:34+00:00
- **Updated**: 2015-07-20 05:48:55+00:00
- **Authors**: Qilong Wang, Peihua Li, Lei Zhang, Wangmeng Zuo
- **Comment**: None
- **Journal**: Pattern Recognition 59 (2016) 63-71
- **Summary**: The bag-of-features (BoF) model for image classification has been thoroughly studied over the last decade. Different from the widely used BoF methods which modeled images with a pre-trained codebook, the alternative codebook free image modeling method, which we call Codebookless Model (CLM), attracted little attention. In this paper, we present an effective CLM that represents an image with a single Gaussian for classification. By embedding Gaussian manifold into a vector space, we show that the simple incorporation of our CLM into a linear classifier achieves very competitive accuracy compared with state-of-the-art BoF methods (e.g., Fisher Vector). Since our CLM lies in a high dimensional Riemannian manifold, we further propose a joint learning method of low-rank transformation with support vector machine (SVM) classifier on the Gaussian manifold, in order to reduce computational and storage cost. To study and alleviate the side effect of background clutter on our CLM, we also present a simple yet effective partial background removal method based on saliency detection. Experiments are extensively conducted on eight widely used databases to demonstrate the effectiveness and efficiency of our CLM method.



### Planar Ultrametric Rounding for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1507.02407v3
- **DOI**: None
- **Categories**: **cs.DS**, cs.CG, cs.CV, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1507.02407v3)
- **Published**: 2015-07-09 08:07:34+00:00
- **Updated**: 2015-09-10 03:32:55+00:00
- **Authors**: Julian Yarkony, Charless C. Fowlkes
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of hierarchical clustering on planar graphs. We formulate this in terms of an LP relaxation of ultrametric rounding. To solve this LP efficiently we introduce a dual cutting plane scheme that uses minimum cost perfect matching as a subroutine in order to efficiently explore the space of planar partitions. We apply our algorithm to the problem of hierarchical image segmentation.



### Generalized Video Deblurring for Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/1507.02438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.02438v1)
- **Published**: 2015-07-09 09:59:40+00:00
- **Updated**: 2015-07-09 09:59:40+00:00
- **Authors**: Tae Hyun Kim, Kyoung Mu Lee
- **Comment**: CVPR 2015 oral
- **Journal**: None
- **Summary**: Several state-of-the-art video deblurring methods are based on a strong assumption that the captured scenes are static. These methods fail to deblur blurry videos in dynamic scenes. We propose a video deblurring method to deal with general blurs inherent in dynamic scenes, contrary to other methods. To handle locally varying and general blurs caused by various sources, such as camera shake, moving objects, and depth variation in a scene, we approximate pixel-wise kernel with bidirectional optical flows. Therefore, we propose a single energy model that simultaneously estimates optical flows and latent frames to solve our deblurring problem. We also provide a framework and efficient solvers to optimize the energy model. By minimizing the proposed energy function, we achieve significant improvements in removing blurs and estimating accurate optical flows in blurry frames. Extensive experimental results demonstrate the superiority of the proposed method in real and challenging videos that state-of-the-art methods fail in either deblurring or optical flow estimation.



### Multi-Type Activity Recognition in Robot-Centric Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1507.02558v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.02558v2)
- **Published**: 2015-07-09 15:33:40+00:00
- **Updated**: 2016-04-12 01:33:06+00:00
- **Authors**: Ilaria Gori, J. K. Aggarwal, Larry Matthies, Michael S. Ryoo
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters (RA-L), 1(1):593-600, 2016
- **Summary**: Activity recognition is very useful in scenarios where robots interact with, monitor or assist humans. In the past years many types of activities -- single actions, two persons interactions or ego-centric activities, to name a few -- have been analyzed. Whereas traditional methods treat such types of activities separately, an autonomous robot should be able to detect and recognize multiple types of activities to effectively fulfill its tasks. We propose a method that is intrinsically able to detect and recognize activities of different types that happen in sequence or concurrently. We present a new unified descriptor, called Relation History Image (RHI), which can be extracted from all the activity types we are interested in. We then formulate an optimization procedure to detect and recognize activities of different types. We apply our approach to a new dataset recorded from a robot-centric perspective and systematically evaluate its quality compared to multiple baselines. Finally, we show the efficacy of the RHI descriptor on publicly available datasets performing extensive comparisons.



### Deep filter banks for texture recognition, description, and segmentation
- **Arxiv ID**: http://arxiv.org/abs/1507.02620v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.02620v2)
- **Published**: 2015-07-09 17:55:30+00:00
- **Updated**: 2015-11-18 23:10:52+00:00
- **Authors**: Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Andrea Vedaldi
- **Comment**: 29 pages; 13 figures; 8 tables
- **Journal**: None
- **Summary**: Visual textures have played a key role in image understanding because they convey important semantics of images, and because texture representations that pool local image descriptors in an orderless manner have had a tremendous impact in diverse applications. In this paper we make several contributions to texture understanding. First, instead of focusing on texture instance and material category recognition, we propose a human-interpretable vocabulary of texture attributes to describe common texture patterns, complemented by a new describable texture dataset for benchmarking. Second, we look at the problem of recognizing materials and texture attributes in realistic imaging conditions, including when textures appear in clutter, developing corresponding benchmarks on top of the recently proposed OpenSurfaces dataset. Third, we revisit classic texture representations, including bag-of-visual-words and the Fisher vectors, in the context of deep learning and show that these have excellent efficiency and generalization properties if the convolutional layers of a deep model are used as filter banks. We obtain in this manner state-of-the-art performance in numerous datasets well beyond textures, an efficient method to apply deep features to image regions, as well as benefit in transferring features from one domain to another.



### Robot In a Room: Toward Perfect Object Recognition in Closed Environments
- **Arxiv ID**: http://arxiv.org/abs/1507.02703v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.02703v1)
- **Published**: 2015-07-09 20:40:58+00:00
- **Updated**: 2015-07-09 20:40:58+00:00
- **Authors**: Shuran Song, Linguang Zhang, Jianxiong Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: While general object recognition is still far from being solved, this paper proposes a way for a robot to recognize every object at an almost human-level accuracy. Our key observation is that many robots will stay in a relatively closed environment (e.g. a house or an office). By constraining a robot to stay in a limited territory, we can ensure that the robot has seen most objects before and the speed of introducing a new object is slow. Furthermore, we can build a 3D map of the environment to reliably subtract the background to make recognition easier. We propose extremely robust algorithms to obtain a 3D map and enable humans to collectively annotate objects. During testing time, our algorithm can recognize all objects very reliably, and query humans from crowd sourcing platform if confidence is low or new objects are identified. This paper explains design decisions in building such a system, and constructs a benchmark for extensive evaluation. Experiments suggest that making robot vision appear to be working from an end user's perspective is a reachable goal today, as long as the robot stays in a closed environment. By formulating this task, we hope to lay the foundation of a new direction in vision for robotics. Code and data will be available upon acceptance.



