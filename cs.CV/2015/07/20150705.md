# Arxiv Papers in cs.CV on 2015-07-05
### Parsimonious Labeling
- **Arxiv ID**: http://arxiv.org/abs/1507.01208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.01208v1)
- **Published**: 2015-07-05 11:59:43+00:00
- **Updated**: 2015-07-05 11:59:43+00:00
- **Authors**: Puneet K. Dokania, M. Pawan Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new family of discrete energy minimization problems, which we call parsimonious labeling. Specifically, our energy functional consists of unary potentials and high-order clique potentials. While the unary potentials are arbitrary, the clique potentials are proportional to the {\em diversity} of set of the unique labels assigned to the clique. Intuitively, our energy functional encourages the labeling to be parsimonious, that is, use as few labels as possible. This in turn allows us to capture useful cues for important computer vision applications such as stereo correspondence and image denoising. Furthermore, we propose an efficient graph-cuts based algorithm for the parsimonious labeling problem that provides strong theoretical guarantees on the quality of the solution. Our algorithm consists of three steps. First, we approximate a given diversity using a mixture of a novel hierarchical $P^n$ Potts model. Second, we use a divide-and-conquer approach for each mixture component, where each subproblem is solved using an effficient $\alpha$-expansion algorithm. This provides us with a small number of putative labelings, one for each mixture component. Third, we choose the best putative labeling in terms of the energy value. Using both sythetic and standard real datasets, we show that our algorithm significantly outperforms other graph-cuts based approaches.



### TV News Commercials Detection using Success based Locally Weighted Kernel Combination
- **Arxiv ID**: http://arxiv.org/abs/1507.01209v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1507.01209v1)
- **Published**: 2015-07-05 12:01:34+00:00
- **Updated**: 2015-07-05 12:01:34+00:00
- **Authors**: Raghvendra Kannao, Prithwijit Guha
- **Comment**: None
- **Journal**: None
- **Summary**: Commercial detection in news broadcast videos involves judicious selection of meaningful audio-visual feature combinations and efficient classifiers. And, this problem becomes much simpler if these combinations can be learned from the data. To this end, we propose an Multiple Kernel Learning based method for boosting successful kernel functions while ignoring the irrelevant ones. We adopt a intermediate fusion approach where, a SVM is trained with a weighted linear combination of different kernel functions instead of single kernel function. Each kernel function is characterized by a feature set and kernel type. We identify the feature sub-space locations of the prediction success of a particular classifier trained only with particular kernel function. We propose to estimate a weighing function using support vector regression (with RBF kernel) for each kernel function which has high values (near 1.0) where the classifier learned on kernel function succeeded and lower values (nearly 0.0) otherwise. Second contribution of this work is TV News Commercials Dataset of 150 Hours of News videos. Classifier trained with our proposed scheme has outperformed the baseline methods on 6 of 8 benchmark dataset and our own TV commercials dataset.



### Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit
- **Arxiv ID**: http://arxiv.org/abs/1507.01238v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1507.01238v3)
- **Published**: 2015-07-05 16:29:31+00:00
- **Updated**: 2016-05-05 20:16:54+00:00
- **Authors**: Chong You, Daniel P. Robinson, Rene Vidal
- **Comment**: 13 pages, 1 figure, 2 tables. Accepted to CVPR 2016 as an oral
  presentation
- **Journal**: None
- **Summary**: Subspace clustering methods based on $\ell_1$, $\ell_2$ or nuclear norm regularization have become very popular due to their simplicity, theoretical guarantees and empirical success. However, the choice of the regularizer can greatly impact both theory and practice. For instance, $\ell_1$ regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad conditions (e.g., arbitrary subspaces and corrupted data). However, it requires solving a large scale convex optimization problem. On the other hand, $\ell_2$ and nuclear norm regularization provide efficient closed form solutions, but require very strong assumptions to guarantee a subspace-preserving affinity, e.g., independent subspaces and uncorrupted data. In this paper we study a subspace clustering method based on orthogonal matching pursuit. We show that the method is both computationally efficient and guaranteed to give a subspace-preserving affinity under broad conditions. Experiments on synthetic data verify our theoretical analysis, and applications in handwritten digit and face clustering show that our approach achieves the best trade off between accuracy and efficiency.



### Autoencoding the Retrieval Relevance of Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1507.01251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.01251v1)
- **Published**: 2015-07-05 18:40:14+00:00
- **Updated**: 2015-07-05 18:40:14+00:00
- **Authors**: Zehra Camlica, H. R. Tizhoosh, Farzad Khalvati
- **Comment**: To appear in proceedings of The 5th International Conference on Image
  Processing Theory, Tools and Applications (IPTA'15), Nov 10-13, 2015,
  Orleans, France
- **Journal**: None
- **Summary**: Content-based image retrieval (CBIR) of medical images is a crucial task that can contribute to a more reliable diagnosis if applied to big data. Recent advances in feature extraction and classification have enormously improved CBIR results for digital images. However, considering the increasing accessibility of big data in medical imaging, we are still in need of reducing both memory requirements and computational expenses of image retrieval systems. This work proposes to exclude the features of image blocks that exhibit a low encoding error when learned by a $n/p/n$ autoencoder ($p\!<\!n$). We examine the histogram of autoendcoding errors of image blocks for each image class to facilitate the decision which image regions, or roughly what percentage of an image perhaps, shall be declared relevant for the retrieval task. This leads to reduction of feature dimensionality and speeds up the retrieval process. To validate the proposed scheme, we employ local binary patterns (LBP) and support vector machines (SVM) which are both well-established approaches in CBIR research community. As well, we use IRMA dataset with 14,410 x-ray images as test data. The results show that the dimensionality of annotated feature vectors can be reduced by up to 50% resulting in speedups greater than 27% at expense of less than 1% decrease in the accuracy of retrieval when validating the precision and recall of the top 20 hits.



