# Arxiv Papers in cs.CV on 2015-07-15
### Unsupervised Decision Forest for Data Clustering and Density Estimation
- **Arxiv ID**: http://arxiv.org/abs/1507.04060v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.04060v1)
- **Published**: 2015-07-15 00:50:06+00:00
- **Updated**: 2015-07-15 00:50:06+00:00
- **Authors**: Hayder Albehadili, Naz Islam
- **Comment**: None
- **Journal**: None
- **Summary**: An algorithm to improve performance parameter for unsupervised decision forest clustering and density estimation is presented. Specifically, a dual assignment parameter is introduced as a density estimator by combining Random Forest and Gaussian Mixture Model. The Random Forest method has been specifically applied to construct a robust affinity graph that provides information on the underlying structure of data objects used in clustering. The proposed algorithm differs from the commonly used spectral clustering methods where the computed distance metric is used to find similarities between data points. Experiments were conducted using five datasets. A comparison with six other state-of-the-art methods shows that our model is superior to existing approaches. Efficiency of the proposed model is in capturing the underlying structure for a given set of data points. The proposed method is also robust, and can discriminate between the complex features of data points among different clusters.



### Untangling AdaBoost-based Cost-Sensitive Classification. Part I: Theoretical Perspective
- **Arxiv ID**: http://arxiv.org/abs/1507.04125v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1507.04125v2)
- **Published**: 2015-07-15 08:50:09+00:00
- **Updated**: 2016-07-22 17:44:11+00:00
- **Authors**: Iago Landesa-Vázquez, José Luis Alba-Castro
- **Comment**: Extended version of paper submitted to Pattern Recognition (Revised
  in July 2016)
- **Journal**: None
- **Summary**: Boosting algorithms have been widely used to tackle a plethora of problems. In the last few years, a lot of approaches have been proposed to provide standard AdaBoost with cost-sensitive capabilities, each with a different focus. However, for the researcher, these algorithms shape a tangled set with diffuse differences and properties, lacking a unifying analysis to jointly compare, classify, evaluate and discuss those approaches on a common basis. In this series of two papers we aim to revisit the various proposals, both from theoretical (Part I) and practical (Part II) perspectives, in order to analyze their specific properties and behavior, with the final goal of identifying the algorithm providing the best and soundest results.



### Untangling AdaBoost-based Cost-Sensitive Classification. Part II: Empirical Analysis
- **Arxiv ID**: http://arxiv.org/abs/1507.04126v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1507.04126v2)
- **Published**: 2015-07-15 08:51:18+00:00
- **Updated**: 2016-07-22 17:44:33+00:00
- **Authors**: Iago Landesa-Vázquez, José Luis Alba-Castro
- **Comment**: Extended version of paper submitted to Pattern Recognition (Revised
  in July 2016)
- **Journal**: None
- **Summary**: A lot of approaches, each following a different strategy, have been proposed in the literature to provide AdaBoost with cost-sensitive properties. In the first part of this series of two papers, we have presented these algorithms in a homogeneous notational framework, proposed a clustering scheme for them and performed a thorough theoretical analysis of those approaches with a fully theoretical foundation. The present paper, in order to complete our analysis, is focused on the empirical study of all the algorithms previously presented over a wide range of heterogeneous classification problems. The results of our experiments, confirming the theoretical conclusions, seem to reveal that the simplest approach, just based on cost-sensitive weight initialization, is the one showing the best and soundest results, despite having been recurrently overlooked in the literature.



