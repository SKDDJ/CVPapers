# Arxiv Papers in cs.CV on 2015-07-27
### Discovery of Shared Semantic Spaces for Multi-Scene Video Query and Summarization
- **Arxiv ID**: http://arxiv.org/abs/1507.07458v1
- **DOI**: 10.1109/TCSVT.2016.2532719
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.07458v1)
- **Published**: 2015-07-27 15:50:03+00:00
- **Updated**: 2015-07-27 15:50:03+00:00
- **Authors**: Xun Xu, Timothy Hospedales, Shaogang Gong
- **Comment**: Multi-Scene Traffic Behaviour Analysis ---- Accepted at IEEE
  Transactions on Circuits and Systems for Video Technology
- **Journal**: None
- **Summary**: The growing rate of public space CCTV installations has generated a need for automated methods for exploiting video surveillance data including scene understanding, query, behaviour annotation and summarization. For this reason, extensive research has been performed on surveillance scene understanding and analysis. However, most studies have considered single scenes, or groups of adjacent scenes. The semantic similarity between different but related scenes (e.g., many different traffic scenes of similar layout) is not generally exploited to improve any automated surveillance tasks and reduce manual effort. Exploiting commonality, and sharing any supervised annotations, between different scenes is however challenging due to: Some scenes are totally un-related -- and thus any information sharing between them would be detrimental; while others may only share a subset of common activities -- and thus information sharing is only useful if it is selective. Moreover, semantically similar activities which should be modelled together and shared across scenes may have quite different pixel-level appearance in each scene. To address these issues we develop a new framework for distributed multiple-scene global understanding that clusters surveillance scenes by their ability to explain each other's behaviours; and further discovers which subset of activities are shared versus scene-specific within each cluster. We show how to use this structured representation of multiple scenes to improve common surveillance tasks including scene activity understanding, cross-scene query-by-example, behaviour classification with reduced supervised labelling requirements, and video summarization. In each case we demonstrate how our multi-scene model improves on a collection of standard single scene models and a flat model of all scenes.



### Real-time 2D/3D Registration via CNN Regression
- **Arxiv ID**: http://arxiv.org/abs/1507.07505v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.07505v3)
- **Published**: 2015-07-27 18:13:52+00:00
- **Updated**: 2016-04-25 06:49:42+00:00
- **Authors**: Shun Miao, Z. Jane Wang, Rui Liao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a Convolutional Neural Network (CNN) regression approach for real-time 2-D/3-D registration. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the Digitally Reconstructed Radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. The CNN regressors are trained for local zones and applied in a hierarchical manner to break down the complex regression task into simpler sub-tasks that can be learned separately. Our experiment results demonstrate the advantage of the proposed method in computational efficiency with negligible degradation of registration accuracy compared to intensity-based methods.



### Occlusion-Aware Object Localization, Segmentation and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1507.07882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.07882v1)
- **Published**: 2015-07-27 18:16:35+00:00
- **Updated**: 2015-07-27 18:16:35+00:00
- **Authors**: Samarth Brahmbhatt, Heni Ben Amor, Henrik Christensen
- **Comment**: British Machine Vision Conference 2015 (poster)
- **Journal**: None
- **Summary**: We present a learning approach for localization and segmentation of objects in an image in a manner that is robust to partial occlusion. Our algorithm produces a bounding box around the full extent of the object and labels pixels in the interior that belong to the object. Like existing segmentation aware detection approaches, we learn an appearance model of the object and consider regions that do not fit this model as potential occlusions. However, in addition to the established use of pairwise potentials for encouraging local consistency, we use higher order potentials which capture information at the level of im- age segments. We also propose an efficient loss function that targets both localization and segmentation performance. Our algorithm achieves 13.52% segmentation error and 0.81 area under the false-positive per image vs. recall curve on average over the challenging CMU Kitchen Occlusion Dataset. This is a 42.44% decrease in segmentation error and a 16.13% increase in localization performance compared to the state-of-the-art. Finally, we show that the visibility labelling produced by our algorithm can make full 3D pose estimation from a single image robust to occlusion.



### Fast Segmentation of Left Ventricle in CT Images by Explicit Shape Regression using Random Pixel Difference Features
- **Arxiv ID**: http://arxiv.org/abs/1507.07508v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.07508v2)
- **Published**: 2015-07-27 18:17:55+00:00
- **Updated**: 2015-07-28 14:07:05+00:00
- **Authors**: Peng Sun, Haoyin Zhou, Devon Lundine, James K. Min, Guanglei Xiong
- **Comment**: 8 pages, link to a video demo
- **Journal**: None
- **Summary**: Recently, machine learning has been successfully applied to model-based left ventricle (LV) segmentation. The general framework involves two stages, which starts with LV localization and is followed by boundary delineation. Both are driven by supervised learning techniques. When compared to previous non-learning-based methods, several advantages have been shown, including full automation and improved accuracy. However, the speed is still slow, in the order of several seconds, for applications involving a large number of cases or case loads requiring real-time performance. In this paper, we propose a fast LV segmentation algorithm by joint localization and boundary delineation via training explicit shape regressor with random pixel difference features. Tested on 3D cardiac computed tomography (CT) image volumes, the average running time of the proposed algorithm is 1.2 milliseconds per case. On a dataset consisting of 139 CT volumes, a 5-fold cross validation shows the segmentation error is $1.21 \pm 0.11$ for LV endocardium and $1.23 \pm 0.11$ millimeters for epicardium. Compared with previous work, the proposed method is more stable (lower standard deviation) without significant compromise to the accuracy.



### Mapping Auto-context Decision Forests to Deep ConvNets for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1507.07583v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.07583v3)
- **Published**: 2015-07-27 20:44:51+00:00
- **Updated**: 2018-08-13 12:43:01+00:00
- **Authors**: David L. Richmond, Dagmar Kainmueller, Michael Y. Yang, Eugene W. Myers, Carsten Rother
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the task of pixel-wise semantic segmentation given a small set of labeled training images. Among two of the most popular techniques to address this task are Decision Forests (DF) and Neural Networks (NN). In this work, we explore the relationship between two special forms of these techniques: stacked DFs (namely Auto-context) and deep Convolutional Neural Networks (ConvNet). Our main contribution is to show that Auto-context can be mapped to a deep ConvNet with novel architecture, and thereby trained end-to-end. This mapping can be used as an initialization of a deep ConvNet, enabling training even in the face of very limited amounts of training data. We also demonstrate an approximate mapping back from the refined ConvNet to a second stacked DF, with improved performance over the original. We experimentally verify that these mappings outperform stacked DFs for two different applications in computer vision and biology: Kinect-based body part labeling from depth images, and somite segmentation in microscopy images of developing zebrafish. Finally, we revisit the core mapping from a Decision Tree (DT) to a NN, and show that it is also possible to map a fuzzy DT, with sigmoidal split decisions, to a NN. This addresses multiple limitations of the previous mapping, and yields new insights into the popular Rectified Linear Unit (ReLU), and more recently proposed concatenated ReLU (CReLU), activation functions.



