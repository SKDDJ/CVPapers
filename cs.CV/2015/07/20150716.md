# Arxiv Papers in cs.CV on 2015-07-16
### A Deep Hashing Learning Network
- **Arxiv ID**: http://arxiv.org/abs/1507.04437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.04437v1)
- **Published**: 2015-07-16 02:57:59+00:00
- **Updated**: 2015-07-16 02:57:59+00:00
- **Authors**: Guoqiang Zhong, Pan Yang, Sijiang Wang, Junyu Dong
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: Hashing-based methods seek compact and efficient binary codes that preserve the neighborhood structure in the original data space. For most existing hashing methods, an image is first encoded as a vector of hand-crafted visual feature, followed by a hash projection and quantization step to get the compact binary vector. Most of the hand-crafted features just encode the low-level information of the input, the feature may not preserve the semantic similarities of images pairs. Meanwhile, the hashing function learning process is independent with the feature representation, so the feature may not be optimal for the hashing projection. In this paper, we propose a supervised hashing method based on a well designed deep convolutional neural network, which tries to learn hashing code and compact representations of data simultaneously. The proposed model learn the binary codes by adding a compact sigmoid layer before the loss layer. Experiments on several image data sets show that the proposed model outperforms other state-of-the-art methods.



### Diagnosing State-Of-The-Art Object Proposal Methods
- **Arxiv ID**: http://arxiv.org/abs/1507.04512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.04512v1)
- **Published**: 2015-07-16 10:00:30+00:00
- **Updated**: 2015-07-16 10:00:30+00:00
- **Authors**: Hongyuan Zhu, Shijian Lu, Jianfei Cai, Quangqing Lee
- **Comment**: Accepted to BMVC 2015
- **Journal**: None
- **Summary**: Object proposal has become a popular paradigm to replace exhaustive sliding window search in current top-performing methods in PASCAL VOC and ImageNet. Recently, Hosang et al. conduct the first unified study of existing methods' in terms of various image-level degradations. On the other hand, the vital question "what object-level characteristics really affect existing methods' performance?" is not yet answered. Inspired by Hoiem et al.'s work in categorical object detection, this paper conducts the first meta-analysis of various object-level characteristics' impact on state-of-the-art object proposal methods. Specifically, we examine the effects of object size, aspect ratio, iconic view, color contrast, shape regularity and texture. We also analyse existing methods' localization accuracy and latency for various PASCAL VOC object classes. Our study reveals the limitations of existing methods in terms of non-iconic view, small object size, low color contrast, shape regularity etc. Based on our observations, lessons are also learned and shared with respect to the selection of existing object proposal technologies as well as the design of the future ones.



### Multi-Face Tracking by Extended Bag-of-Tracklets in Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/1507.04576v2
- **DOI**: 10.1016/j.cviu.2016.02.013
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.04576v2)
- **Published**: 2015-07-16 13:51:47+00:00
- **Updated**: 2016-01-13 12:26:09+00:00
- **Authors**: Maedeh Aghaei, Mariella Dimiccoli, Petia Radeva
- **Comment**: 27 pages, 18 figures, submitted to computer vision and image
  understanding journal
- **Journal**: None
- **Summary**: Wearable cameras offer a hands-free way to record egocentric images of daily experiences, where social events are of special interest. The first step towards detection of social events is to track the appearance of multiple persons involved in it. In this paper, we propose a novel method to find correspondences of multiple faces in low temporal resolution egocentric videos acquired through a wearable camera. This kind of photo-stream imposes additional challenges to the multi-tracking problem with respect to conventional videos. Due to the free motion of the camera and to its low temporal resolution, abrupt changes in the field of view, in illumination condition and in the target location are highly frequent. To overcome such difficulties, we propose a multi-face tracking method that generates a set of tracklets through finding correspondences along the whole sequence for each detected face and takes advantage of the tracklets redundancy to deal with unreliable ones. Similar tracklets are grouped into the so called extended bag-of-tracklets (eBoT), which is aimed to correspond to a specific person. Finally, a prototype tracklet is extracted for each eBoT, where the occurred occlusions are estimated by relying on a new measure of confidence. We validated our approach over an extensive dataset of egocentric photo-streams and compared it to state of the art methods, demonstrating its effectiveness and robustness.



### Driver Gaze Region Estimation Without Using Eye Movement
- **Arxiv ID**: http://arxiv.org/abs/1507.04760v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.04760v2)
- **Published**: 2015-07-16 20:16:20+00:00
- **Updated**: 2016-03-01 17:21:25+00:00
- **Authors**: Lex Fridman, Philipp Langhans, Joonbum Lee, Bryan Reimer
- **Comment**: Accepted for Publication in IEEE Intelligent Systems
- **Journal**: None
- **Summary**: Automated estimation of the allocation of a driver's visual attention may be a critical component of future Advanced Driver Assistance Systems. In theory, vision-based tracking of the eye can provide a good estimate of gaze location. In practice, eye tracking from video is challenging because of sunglasses, eyeglass reflections, lighting conditions, occlusions, motion blur, and other factors. Estimation of head pose, on the other hand, is robust to many of these effects, but cannot provide as fine-grained of a resolution in localizing the gaze. However, for the purpose of keeping the driver safe, it is sufficient to partition gaze into regions. In this effort, we propose a system that extracts facial features and classifies their spatial configuration into six regions in real-time. Our proposed method achieves an average accuracy of 91.4% at an average decision rate of 11 Hz on a dataset of 50 drivers from an on-road study.



