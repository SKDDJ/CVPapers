# Arxiv Papers in cs.CV on 2015-07-08
### DCTNet : A Simple Learning-free Approach for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1507.02049v3
- **DOI**: 10.1109/APSIPA.2015.7415375
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.02049v3)
- **Published**: 2015-07-08 07:15:41+00:00
- **Updated**: 2015-09-29 11:38:10+00:00
- **Authors**: Cong Jie Ng, Andrew Beng Jin Teoh
- **Comment**: APSIPA ASC 2015
- **Journal**: None
- **Summary**: PCANet was proposed as a lightweight deep learning network that mainly leverages Principal Component Analysis (PCA) to learn multistage filter banks followed by binarization and block-wise histograming. PCANet was shown worked surprisingly well in various image classification tasks. However, PCANet is data-dependence hence inflexible. In this paper, we proposed a data-independence network, dubbed DCTNet for face recognition in which we adopt Discrete Cosine Transform (DCT) as filter banks in place of PCA. This is motivated by the fact that 2D DCT basis is indeed a good approximation for high ranked eigenvectors of PCA. Both 2D DCT and PCA resemble a kind of modulated sine-wave patterns, which can be perceived as a bandpass filter bank. DCTNet is free from learning as 2D DCT bases can be computed in advance. Besides that, we also proposed an effective method to regulate the block-wise histogram feature vector of DCTNet for robustness. It is shown to provide surprising performance boost when the probe image is considerably different in appearance from the gallery image. We evaluate the performance of DCTNet extensively on a number of benchmark face databases and being able to achieve on par with or often better accuracy performance than PCANet.



### Shedding Light on the Asymmetric Learning Capability of AdaBoost
- **Arxiv ID**: http://arxiv.org/abs/1507.02084v1
- **DOI**: 10.1016/j.patrec.2011.10.022
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1507.02084v1)
- **Published**: 2015-07-08 09:58:06+00:00
- **Updated**: 2015-07-08 09:58:06+00:00
- **Authors**: Iago Landesa-Vázquez, José Luis Alba-Castro
- **Comment**: None
- **Journal**: Pattern Recognition Letters 33 (2012) 247-255
- **Summary**: In this paper, we propose a different insight to analyze AdaBoost. This analysis reveals that, beyond some preconceptions, AdaBoost can be directly used as an asymmetric learning algorithm, preserving all its theoretical properties. A novel class-conditional description of AdaBoost, which models the actual asymmetric behavior of the algorithm, is presented.



### Spotlight the Negatives: A Generalized Discriminative Latent Model
- **Arxiv ID**: http://arxiv.org/abs/1507.02144v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.02144v1)
- **Published**: 2015-07-08 13:26:13+00:00
- **Updated**: 2015-07-08 13:26:13+00:00
- **Authors**: Hossein Azizpour, Mostafa Arefiyan, Sobhan Naderi Parizi, Stefan Carlsson
- **Comment**: Published in proceedings of BMVC 2015
- **Journal**: None
- **Summary**: Discriminative latent variable models (LVM) are frequently applied to various visual recognition tasks. In these systems the latent (hidden) variables provide a formalism for modeling structured variation of visual features. Conventionally, latent variables are de- fined on the variation of the foreground (positive) class. In this work we augment LVMs to include negative latent variables corresponding to the background class. We formalize the scoring function of such a generalized LVM (GLVM). Then we discuss a framework for learning a model based on the GLVM scoring function. We theoretically showcase how some of the current visual recognition methods can benefit from this generalization. Finally, we experiment on a generalized form of Deformable Part Models with negative latent variables and show significant improvements on two different detection tasks.



### SAR Imaging of Moving Target based on Knowledge-aided Two-dimensional Autofocus
- **Arxiv ID**: http://arxiv.org/abs/1507.02150v1
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1507.02150v1)
- **Published**: 2015-07-08 13:34:25+00:00
- **Updated**: 2015-07-08 13:34:25+00:00
- **Authors**: Xinhua Mao
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Due to uncertainty on target's motion, the range cell migration (RCM) and azimuth phase error (APE) of moving targets can't be completely compensated in synthetic aperture radar (SAR) processing. Therefore, moving targets often appear two-dimensional (2-D) defocused in SAR images. In this paper, a 2-D autofocus method for refocusing defocused moving targets in SAR images is presented. The new method only requires a direct estimate of APE, while the residual 2-D phase error ( or RCM) is computed from the estimated APE by exploiting the analytical relationship between the 2-D phase error ( or RCM) and APE. Because the parameter estimation is performed in the reduced-dimension space by exploiting prior knowledge on phase error structure, the proposed approach offers clear advantages in both computational efficiency and estimation accuracy.



### Double-Base Asymmetric AdaBoost
- **Arxiv ID**: http://arxiv.org/abs/1507.02154v1
- **DOI**: 10.1016/j.neucom.2013.02.019
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1507.02154v1)
- **Published**: 2015-07-08 13:44:34+00:00
- **Updated**: 2015-07-08 13:44:34+00:00
- **Authors**: Iago Landesa-Vázquez, José Luis Alba-Castro
- **Comment**: None
- **Journal**: Neurocomputing 118 (2013) 101-114
- **Summary**: Based on the use of different exponential bases to define class-dependent error bounds, a new and highly efficient asymmetric boosting scheme, coined as AdaBoostDB (Double-Base), is proposed. Supported by a fully theoretical derivation procedure, unlike most of the other approaches in the literature, our algorithm preserves all the formal guarantees and properties of original (cost-insensitive) AdaBoost, similarly to the state-of-the-art Cost-Sensitive AdaBoost algorithm. However, the key advantage of AdaBoostDB is that our novel derivation scheme enables an extremely efficient conditional search procedure, dramatically improving and simplifying the training phase of the algorithm. Experiments, both over synthetic and real datasets, reveal that AdaBoostDB is able to save over 99% training time with regard to Cost-Sensitive AdaBoost, providing the same cost-sensitive results. This computational advantage of AdaBoostDB can make a difference in problems managing huge pools of weak classifiers in which boosting techniques are commonly used.



### Towards Good Practices for Very Deep Two-Stream ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1507.02159v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.02159v1)
- **Published**: 2015-07-08 14:00:35+00:00
- **Updated**: 2015-07-08 14:00:35+00:00
- **Authors**: Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional networks have achieved great success for object recognition in still images. However, for action recognition in videos, the improvement of deep convolutional networks is not so evident. We argue that there are two reasons that could probably explain this result. First the current network architectures (e.g. Two-stream ConvNets) are relatively shallow compared with those very deep models in image domain (e.g. VGGNet, GoogLeNet), and therefore their modeling capacity is constrained by their depth. Second, probably more importantly, the training dataset of action recognition is extremely small compared with the ImageNet dataset, and thus it will be easy to over-fit on the training dataset.   To address these issues, this report presents very deep two-stream ConvNets for action recognition, by adapting recent very deep architectures into video domain. However, this extension is not easy as the size of action recognition is quite small. We design several good practices for the training of very deep two-stream ConvNets, namely (i) pre-training for both spatial and temporal nets, (ii) smaller learning rates, (iii) more data augmentation techniques, (iv) high drop out ratio. Meanwhile, we extend the Caffe toolbox into Multi-GPU implementation with high computational efficiency and low memory consumption. We verify the performance of very deep two-stream ConvNets on the dataset of UCF101 and it achieves the recognition accuracy of $91.4\%$.



### Iris Recognition Using Scattering Transform and Textural Features
- **Arxiv ID**: http://arxiv.org/abs/1507.02177v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.02177v1)
- **Published**: 2015-07-08 14:37:24+00:00
- **Updated**: 2015-07-08 14:37:24+00:00
- **Authors**: Shervin Minaee, AmirAli Abdolrashidi, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Iris recognition has drawn a lot of attention since the mid-twentieth century. Among all biometric features, iris is known to possess a rich set of features. Different features have been used to perform iris recognition in the past. In this paper, two powerful sets of features are introduced to be used for iris recognition: scattering transform-based features and textural features. PCA is also applied on the extracted features to reduce the dimensionality of the feature vector while preserving most of the information of its initial value. Minimum distance classifier is used to perform template matching for each new test sample. The proposed scheme is tested on a well-known iris database, and showed promising results with the best accuracy rate of 99.2%.



### Feature Representation in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1507.02313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.02313v1)
- **Published**: 2015-07-08 21:13:26+00:00
- **Updated**: 2015-07-08 21:13:26+00:00
- **Authors**: Ben Athiwaratkun, Keegan Kang
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are powerful models that achieve impressive results for image classification. In addition, pre-trained CNNs are also useful for other computer vision tasks as generic feature extractors. This paper aims to gain insight into the feature aspect of CNN and demonstrate other uses of CNN features. Our results show that CNN feature maps can be used with Random Forests and SVM to yield classification results that outperforms the original CNN. A CNN that is less than optimal (e.g. not fully trained or overfitting) can also extract features for Random Forest/SVM that yield competitive classification accuracy. In contrast to the literature which uses the top-layer activations as feature representation of images for other tasks, using lower-layer features can yield better results for classification.



