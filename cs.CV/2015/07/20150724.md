# Arxiv Papers in cs.CV on 2015-07-24
### Multimodal Deep Learning for Robust RGB-D Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1507.06821v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1507.06821v2)
- **Published**: 2015-07-24 12:20:19+00:00
- **Updated**: 2015-08-18 13:04:29+00:00
- **Authors**: Andreas Eitel, Jost Tobias Springenberg, Luciano Spinello, Martin Riedmiller, Wolfram Burgard
- **Comment**: Final version submitted to IROS'2015, results unchanged,
  reformulation of some text passages in abstract and introduction
- **Journal**: None
- **Summary**: Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset and show recognition in challenging RGB-D real-world noisy settings.



### Descriptors and regions of interest fusion for gender classification in the wild. Comparison and combination with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1507.06838v2
- **DOI**: 10.1016/j.imavis.2016.10.004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.06838v2)
- **Published**: 2015-07-24 13:22:29+00:00
- **Updated**: 2016-02-19 12:44:13+00:00
- **Authors**: M. Castrillón-Santana, J. Lorenzo-Navarro, E. Ramón-Balmaseda
- **Comment**: Revised version containing 12 pages. This revision includes newer
  referenes, results with CNN, fusion of local descriptors amd CNN and corrects
  different typos
- **Journal**: None
- **Summary**: Gender classification (GC) has achieved high accuracy in different experimental evaluations based mostly on inner facial details. However, these results do not generalize well in unrestricted datasets and particularly in cross-database experiments, where the performance drops drastically. In this paper, we analyze the state-of-the-art GC accuracy on three large datasets: MORPH, LFW and GROUPS. We discuss their respective difficulties and bias, concluding that the most challenging and wildest complexity is present in GROUPS. This dataset covers hard conditions such as low resolution imagery and cluttered background. Firstly, we analyze in depth the performance of different descriptors extracted from the face and its local context on this dataset. Selecting the bests and studying their most suitable combination allows us to design a solution that beats any previously published results for GROUPS with the Dago's protocol, reaching an accuracy over 94.2%, reducing the gap with other simpler datasets. The chosen solution based on local descriptors is later evaluated in a cross-database scenario with the three mentioned datasets, and full dataset 5-fold cross validation. The achieved results are compared with a Convolutional Neural Network approach, achieving rather similar marks. Finally, a solution is proposed combining both focuses, exhibiting great complementarity, boosting GC performance to beat previously published results in GC both cross-database, and full in-database evaluations.



