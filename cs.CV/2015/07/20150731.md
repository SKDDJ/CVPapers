# Arxiv Papers in cs.CV on 2015-07-31
### Beyond Gauss: Image-Set Matching on the Riemannian Manifold of PDFs
- **Arxiv ID**: http://arxiv.org/abs/1507.08711v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.08711v1)
- **Published**: 2015-07-31 00:11:11+00:00
- **Updated**: 2015-07-31 00:11:11+00:00
- **Authors**: Mehrtash Harandi, Mathieu Salzmann, Mahsa Baktashmotlagh
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art image-set matching techniques typically implicitly model each image-set with a Gaussian distribution. Here, we propose to go beyond these representations and model image-sets as probability distribution functions (PDFs) using kernel density estimators. To compare and match image-sets, we exploit Csiszar f-divergences, which bear strong connections to the geodesic distance defined on the space of PDFs, i.e., the statistical manifold. Furthermore, we introduce valid positive definite kernels on the statistical manifolds, which let us make use of more powerful classification schemes to match image-sets. Finally, we introduce a supervised dimensionality reduction technique that learns a latent space where f-divergences reflect the class labels of the data. Our experiments on diverse problems, such as video-based face recognition and dynamic texture classification, evidence the benefits of our approach over the state-of-the-art image-set matching methods.



### A Sinc Wavelet Describes the Receptive Fields of Neurons in the Motion Cortex
- **Arxiv ID**: http://arxiv.org/abs/1507.08736v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.IT, math.IT, physics.bio-ph
- **Links**: [PDF](http://arxiv.org/pdf/1507.08736v1)
- **Published**: 2015-07-31 02:55:54+00:00
- **Updated**: 2015-07-31 02:55:54+00:00
- **Authors**: Stephen G. Odaibo
- **Comment**: This work was presented in part at the 44th Annual Meeting of the
  Society for Neuroscience in Washington, DC
- **Journal**: None
- **Summary**: Visual perception results from a systematic transformation of the information flowing through the visual system. In the neuronal hierarchy, the response properties of single neurons are determined by neurons located one level below, and in turn, determine the responses of neurons located one level above. Therefore in modeling receptive fields, it is essential to ensure that the response properties of neurons in a given level can be generated by combining the response models of neurons in its input levels. However, existing response models of neurons in the motion cortex do not inherently yield the temporal frequency filtering gradient (TFFG) property that is known to emerge along the primary visual cortex (V1) to middle temporal (MT) motion processing stream. TFFG is the change from predominantly lowpass to predominantly bandpass temporal frequency filtering character along the V1 to MT pathway (Foster et al 1985; DeAngelis et al 1993; Hawken et al 1996). We devised a new model, the sinc wavelet model (Odaibo, 2014), which logically and efficiently generates the TFFG. The model replaces the Gabor function's sine wave carrier with a sinc (sin(x)/x) function, and has the same or fewer number of parameters as existing models. Because of its logical consistency with the emergent network property of TFFG, we conclude that the sinc wavelet is a better model for the receptive fields of motion cortex neurons. This model will provide new physiological insights into how the brain represents visual information.



### Action-Conditional Video Prediction using Deep Networks in Atari Games
- **Arxiv ID**: http://arxiv.org/abs/1507.08750v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1507.08750v2)
- **Published**: 2015-07-31 04:43:30+00:00
- **Updated**: 2015-12-22 04:26:54+00:00
- **Authors**: Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, Satinder Singh
- **Comment**: Published at NIPS 2015 (Advances in Neural Information Processing
  Systems 28)
- **Journal**: None
- **Summary**: Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.



### Flip-Rotate-Pooling Convolution and Split Dropout on Convolution Neural Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1507.08754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.08754v1)
- **Published**: 2015-07-31 05:02:56+00:00
- **Updated**: 2015-07-31 05:02:56+00:00
- **Authors**: Fa Wu, Peijun Hu, Dexing Kong
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new version of Dropout called Split Dropout (sDropout) and rotational convolution techniques to improve CNNs' performance on image classification. The widely used standard Dropout has advantage of preventing deep neural networks from overfitting by randomly dropping units during training. Our sDropout randomly splits the data into two subsets and keeps both rather than discards one subset. We also introduce two rotational convolution techniques, i.e. rotate-pooling convolution (RPC) and flip-rotate-pooling convolution (FRPC) to boost CNNs' performance on the robustness for rotation transformation. These two techniques encode rotation invariance into the network without adding extra parameters. Experimental evaluations on ImageNet2012 classification task demonstrate that sDropout not only enhances the performance but also converges faster. Additionally, RPC and FRPC make CNNs more robust for rotation transformations. Overall, FRPC together with sDropout bring $1.18\%$ (model of Zeiler and Fergus~\cite{zeiler2013visualizing}, 10-view, top-1) accuracy increase in ImageNet 2012 classification task compared to the original network.



### Multimodal Multipart Learning for Action Recognition in Depth Videos
- **Arxiv ID**: http://arxiv.org/abs/1507.08761v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.08761v1)
- **Published**: 2015-07-31 06:02:56+00:00
- **Updated**: 2015-07-31 06:02:56+00:00
- **Authors**: Amir Shahroudy, Gang Wang, Tian-Tsong Ng, Qingxiong Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The articulated and complex nature of human actions makes the task of action recognition difficult. One approach to handle this complexity is dividing it to the kinetics of body parts and analyzing the actions based on these partial descriptors. We propose a joint sparse regression based learning method which utilizes the structured sparsity to model each action as a combination of multimodal features from a sparse set of body parts. To represent dynamics and appearance of parts, we employ a heterogeneous set of depth and skeleton based features. The proper structure of multimodal multipart features are formulated into the learning framework via the proposed hierarchical mixed norm, to regularize the structured features of each part and to apply sparsity between them, in favor of a group feature selection. Our experimental results expose the effectiveness of the proposed learning method in which it outperforms other methods in all three tested datasets while saturating one of them by achieving perfect accuracy.



### A Visual Embedding for the Unsupervised Extraction of Abstract Semantics
- **Arxiv ID**: http://arxiv.org/abs/1507.08818v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1507.08818v6)
- **Published**: 2015-07-31 10:16:42+00:00
- **Updated**: 2016-12-16 13:58:59+00:00
- **Authors**: D. Garcia-Gasulla, J. Béjar, U. Cortés, E. Ayguadé, J. Labarta, T. Suzumura, R. Chen
- **Comment**: 14 pages, 5 figures, accepted at Cognitive Systems Research
- **Journal**: None
- **Summary**: Vector-space word representations obtained from neural network models have been shown to enable semantic operations based on vector arithmetic. In this paper, we explore the existence of similar information on vector representations of images. For that purpose we define a methodology to obtain large, sparse vector representations of image classes, and generate vectors through the state-of-the-art deep learning architecture GoogLeNet for 20K images obtained from ImageNet. We first evaluate the resultant vector-space semantics through its correlation with WordNet distances, and find vector distances to be strongly correlated with linguistic semantics. We then explore the location of images within the vector space, finding elements close in WordNet to be clustered together, regardless of significant visual variances (e.g. 118 dog types). More surprisingly, we find that the space unsupervisedly separates complex classes without prior knowledge (e.g. living things). Afterwards, we consider vector arithmetics. Although we are unable to obtain meaningful results on this regard, we discuss the various problem we encountered, and how we consider to solve them. Finally, we discuss the impact of our research for cognitive systems, focusing on the role of the architecture being used.



### A novel multivariate performance optimization method based on sparse coding and hyper-predictor learning
- **Arxiv ID**: http://arxiv.org/abs/1507.08847v1
- **DOI**: 10.1016/j.neunet.2015.07.011
- **Categories**: **cs.LG**, cs.CV, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1507.08847v1)
- **Published**: 2015-07-31 12:14:35+00:00
- **Updated**: 2015-07-31 12:14:35+00:00
- **Authors**: Jiachen Yanga, Zhiyong Dinga, Fei Guoa, Huogen Wanga, Nick Hughesb
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate the problem of optimization multivariate performance measures, and propose a novel algorithm for it. Different from traditional machine learning methods which optimize simple loss functions to learn prediction function, the problem studied in this paper is how to learn effective hyper-predictor for a tuple of data points, so that a complex loss function corresponding to a multivariate performance measure can be minimized. We propose to present the tuple of data points to a tuple of sparse codes via a dictionary, and then apply a linear function to compare a sparse code against a give candidate class label. To learn the dictionary, sparse codes, and parameter of the linear function, we propose a joint optimization problem. In this problem, the both the reconstruction error and sparsity of sparse code, and the upper bound of the complex loss function are minimized. Moreover, the upper bound of the loss function is approximated by the sparse codes and the linear function parameter. To optimize this problem, we develop an iterative algorithm based on descent gradient methods to learn the sparse codes and hyper-predictor parameter alternately. Experiment results on some benchmark data sets show the advantage of the proposed methods over other state-of-the-art algorithms.



### Mobile Multi-View Object Image Search
- **Arxiv ID**: http://arxiv.org/abs/1507.08861v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1507.08861v2)
- **Published**: 2015-07-31 13:02:23+00:00
- **Updated**: 2018-04-30 14:23:01+00:00
- **Authors**: Fatih Calisir, Muhammet Bastan, Ozgur Ulusoy, Ugur Gudukbay
- **Comment**: Multimedia Tools and Applications, 2017
- **Journal**: None
- **Summary**: High user interaction capability of mobile devices can help improve the accuracy of mobile visual search systems. At query time, it is possible to capture multiple views of an object from different viewing angles and at different scales with the mobile device camera to obtain richer information about the object compared to a single view and hence return more accurate results. Motivated by this, we developed a mobile multi-view object image search system, using a client-server architecture. Multi-view images of objects acquired by the mobile clients are processed and local features are sent to the server, which combines the query image representations with early/late fusion methods based on bag-of-visual-words and sends back the query results. We performed a comprehensive analysis of early and late fusion approaches using various similarity functions, on an existing single view and a new multi-view object image database. The experimental results show that multi-view search provides significantly better retrieval accuracy compared to single view search.



### Deep Networks for Image Super-Resolution with Sparse Prior
- **Arxiv ID**: http://arxiv.org/abs/1507.08905v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1507.08905v4)
- **Published**: 2015-07-31 14:56:30+00:00
- **Updated**: 2015-10-15 19:02:26+00:00
- **Authors**: Zhaowen Wang, Ding Liu, Jianchao Yang, Wei Han, Thomas Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning techniques have been successfully applied in many areas of computer vision, including low-level image restoration problems. For image super-resolution, several models based on deep neural networks have been recently proposed and attained superior performance that overshadows all previous handcrafted models. The question then arises whether large-capacity and data-driven models have become the dominant solution to the ill-posed super-resolution problem. In this paper, we argue that domain expertise represented by the conventional sparse coding model is still valuable, and it can be combined with the key ingredients of deep learning to achieve further improved results. We show that a sparse coding model particularly designed for super-resolution can be incarnated as a neural network, and trained in a cascaded structure from end to end. The interpretation of the network based on sparse coding leads to much more efficient and effective training, as well as a reduced model size. Our model is evaluated on a wide range of images, and shows clear advantage over existing state-of-the-art methods in terms of both restoration accuracy and human subjective quality.



### SnowWatch: Snow Monitoring through Acquisition and Analysis of User-Generated Content
- **Arxiv ID**: http://arxiv.org/abs/1507.08958v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1507.08958v1)
- **Published**: 2015-07-31 17:41:55+00:00
- **Updated**: 2015-07-31 17:41:55+00:00
- **Authors**: Roman Fedorov, Piero Fraternali, Chiara Pasini, Marco Tagliasacchi
- **Comment**: IEEE International Conference on Multimedia and Expo, ICME 2015.
  Accepted and presented technical demo proposal
- **Journal**: None
- **Summary**: We present a system for complementing snow phenomena monitoring with virtual measurements extracted from public visual content. The proposed system integrates an automatic acquisition and analysis of photographs and webcam images depicting Alpine mountains. In particular, the technical demonstration consists in a web portal that interfaces the whole system with the population. It acts as an entertaining photo-sharing social web site, acquiring at the same time visual content necessary for environmental monitoring.



