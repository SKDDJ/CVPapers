# Arxiv Papers in cs.CV on 2015-10-31
### Bioinspired Visual Motion Estimation
- **Arxiv ID**: http://arxiv.org/abs/1511.00096v1
- **DOI**: 10.1109/JPROC.2014.2346763
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.00096v1)
- **Published**: 2015-10-31 07:46:59+00:00
- **Updated**: 2015-10-31 07:46:59+00:00
- **Authors**: Garrick Orchard, Ralph Etienne-Cummings
- **Comment**: 16 pages, 11 figures, 1 table
- **Journal**: Proceedings of the IEEE, vol.102, no.10, pp.1520-1536, Oct. 2014
- **Summary**: Visual motion estimation is a computationally intensive, but important task for sighted animals. Replicating the robustness and efficiency of biological visual motion estimation in artificial systems would significantly enhance the capabilities of future robotic agents. 25 years ago, in this very journal, Carver Mead outlined his argument for replicating biological processing in silicon circuits. His vision served as the foundation for the field of neuromorphic engineering, which has experienced a rapid growth in interest over recent years as the ideas and technologies mature. Replicating biological visual sensing was one of the first tasks attempted in the neuromorphic field. In this paper we focus specifically on the task of visual motion estimation. We describe the task itself, present the progression of works from the early first attempts through to the modern day state-of-the-art, and provide an outlook for future directions in the field.



### Semantic Cross-View Matching
- **Arxiv ID**: http://arxiv.org/abs/1511.00098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.00098v1)
- **Published**: 2015-10-31 08:50:19+00:00
- **Updated**: 2015-10-31 08:50:19+00:00
- **Authors**: Francesco Castaldo, Amir Zamir, Roland Angst, Francesco Palmieri, Silvio Savarese
- **Comment**: None
- **Journal**: None
- **Summary**: Matching cross-view images is challenging because the appearance and viewpoints are significantly different. While low-level features based on gradient orientations or filter responses can drastically vary with such changes in viewpoint, semantic information of images however shows an invariant characteristic in this respect. Consequently, semantically labeled regions can be used for performing cross-view matching. In this paper, we therefore explore this idea and propose an automatic method for detecting and representing the semantic information of an RGB image with the goal of performing cross-view matching with a (non-RGB) geographic information system (GIS). A segmented image forms the input to our system with segments assigned to semantic concepts such as traffic signs, lakes, roads, foliage, etc. We design a descriptor to robustly capture both, the presence of semantic concepts and the spatial layout of those segments. Pairwise distances between the descriptors extracted from the GIS map and the query image are then used to generate a shortlist of the most promising locations with similar semantic concepts in a consistent spatial layout. An experimental evaluation with challenging query images and a large urban area shows promising results.



### Sketch-based Image Retrieval from Millions of Images under Rotation, Translation and Scale Variations
- **Arxiv ID**: http://arxiv.org/abs/1511.00099v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1511.00099v1)
- **Published**: 2015-10-31 08:50:43+00:00
- **Updated**: 2015-10-31 08:50:43+00:00
- **Authors**: Sarthak Parui, Anurag Mittal
- **Comment**: submitted to IJCV, April 2015
- **Journal**: None
- **Summary**: Proliferation of touch-based devices has made sketch-based image retrieval practical. While many methods exist for sketch-based object detection/image retrieval on small datasets, relatively less work has been done on large (web)-scale image retrieval. In this paper, we present an efficient approach for image retrieval from millions of images based on user-drawn sketches. Unlike existing methods for this problem which are sensitive to even translation or scale variations, our method handles rotation, translation, scale (i.e. a similarity transformation) and small deformations. The object boundaries are represented as chains of connected segments and the database images are pre-processed to obtain such chains that have a high chance of containing the object. This is accomplished using two approaches in this work: a) extracting long chains in contour segment networks and b) extracting boundaries of segmented object proposals. These chains are then represented by similarity-invariant variable length descriptors. Descriptor similarities are computed by a fast Dynamic Programming-based partial matching algorithm. This matching mechanism is used to generate a hierarchical k-medoids based indexing structure for the extracted chains of all database images in an offline process which is used to efficiently retrieve a small set of possible matched images for query chains. Finally, a geometric verification step is employed to test geometric consistency of multiple chain matches to improve results. Qualitative and quantitative results clearly demonstrate superiority of the approach over existing methods.



### Fast Neuromimetic Object Recognition using FPGA Outperforms GPU Implementations
- **Arxiv ID**: http://arxiv.org/abs/1511.00100v1
- **DOI**: 10.1109/TNNLS.2013.2253563
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.00100v1)
- **Published**: 2015-10-31 08:59:03+00:00
- **Updated**: 2015-10-31 08:59:03+00:00
- **Authors**: Garrick Orchard, Jacob G. Martin, R. Jacob Vogelstein, Ralph Etienne-Cummings
- **Comment**: 14 pages, 8 figures, 5 tables
- **Journal**: Neural Networks and Learning Systems, IEEE Transactions on,
  vol.24, no.8, pp.1239-1252, 2013
- **Summary**: Recognition of objects in still images has traditionally been regarded as a difficult computational problem. Although modern automated methods for visual object recognition have achieved steadily increasing recognition accuracy, even the most advanced computational vision approaches are unable to obtain performance equal to that of humans. This has led to the creation of many biologically-inspired models of visual object recognition, among them the HMAX model. HMAX is traditionally known to achieve high accuracy in visual object recognition tasks at the expense of significant computational complexity. Increasing complexity, in turn, increases computation time, reducing the number of images that can be processed per unit time. In this paper we describe how the computationally intensive, biologically inspired HMAX model for visual object recognition can be modified for implementation on a commercial Field Programmable Gate Array, specifically the Xilinx Virtex 6 ML605 evaluation board with XC6VLX240T FPGA. We show that with minor modifications to the traditional HMAX model we can perform recognition on images of size 128x128 pixels at a rate of 190 images per second with a less than 1% loss in recognition accuracy in both binary and multi-class visual object recognition tasks.



### Regional Active Contours based on Variational level sets and Machine Learning for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1511.00111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.00111v1)
- **Published**: 2015-10-31 11:17:15+00:00
- **Updated**: 2015-10-31 11:17:15+00:00
- **Authors**: M. Abdelsamea
- **Comment**: IMT PhD thesis, 2015
- **Journal**: None
- **Summary**: Image segmentation is the problem of partitioning an image into different subsets, where each subset may have a different characterization in terms of color, intensity, texture, and/or other features. Segmentation is a fundamental component of image processing, and plays a significant role in computer vision, object recognition, and object tracking. Active Contour Models (ACMs) constitute a powerful energy-based minimization framework for image segmentation, which relies on the concept of contour evolution. Starting from an initial guess, the contour is evolved with the aim of approximating better and better the actual object boundary. Handling complex images in an efficient, effective, and robust way is a real challenge, especially in the presence of intensity inhomogeneity, overlap between the foreground/background intensity distributions, objects characterized by many different intensities, and/or additive noise. In this thesis, to deal with these challenges, we propose a number of image segmentation models relying on variational level set methods and specific kinds of neural networks, to handle complex images in both supervised and unsupervised ways. Experimental results demonstrate the high accuracy of the segmentation results, obtained by the proposed models on various benchmark synthetic and real images compared with state-of-the-art active contour models.



### Color Space Transformation Network
- **Arxiv ID**: http://arxiv.org/abs/1511.01064v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.01064v2)
- **Published**: 2015-10-31 13:25:20+00:00
- **Updated**: 2015-12-11 21:17:27+00:00
- **Authors**: Alexandros Karargyris
- **Comment**: Report
- **Journal**: None
- **Summary**: Deep networks have become very popular over the past few years. The main reason for this widespread use is their excellent ability to learn and predict knowledge in a very easy and efficient way. Convolutional neural networks and auto-encoders have become the normal in the area of imaging and computer vision achieving unprecedented accuracy levels in many applications. The most common strategy is to build and train networks with many layers by tuning their hyper-parameters. While this approach has proven to be a successful way to build robust deep learning schemes it suffers from high complexity. In this paper we introduce a module that learns color space transformations within a network. Given a large dataset of colored images the color space transformation module tries to learn color space transformations that increase overall classification accuracy. This module has shown to increase overall accuracy for the same network design and to achieve faster convergence. It is part of a broader family of image transformations (e.g. spatial transformer network).



### FireCaffe: near-linear acceleration of deep neural network training on compute clusters
- **Arxiv ID**: http://arxiv.org/abs/1511.00175v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.00175v2)
- **Published**: 2015-10-31 21:13:30+00:00
- **Updated**: 2016-01-08 06:50:36+00:00
- **Authors**: Forrest N. Iandola, Khalid Ashraf, Matthew W. Moskewicz, Kurt Keutzer
- **Comment**: Version 2: Added results on 128 GPUs
- **Journal**: None
- **Summary**: Long training times for high-accuracy deep neural networks (DNNs) impede research into new DNN architectures and slow the development of high-accuracy DNNs. In this paper we present FireCaffe, which successfully scales deep neural network training across a cluster of GPUs. We also present a number of best practices to aid in comparing advancements in methods for scaling and accelerating the training of deep neural networks. The speed and scalability of distributed algorithms is almost always limited by the overhead of communicating between servers; DNN training is not an exception to this rule. Therefore, the key consideration here is to reduce communication overhead wherever possible, while not degrading the accuracy of the DNN models that we train. Our approach has three key pillars. First, we select network hardware that achieves high bandwidth between GPU servers -- Infiniband or Cray interconnects are ideal for this. Second, we consider a number of communication algorithms, and we find that reduction trees are more efficient and scalable than the traditional parameter server approach. Third, we optionally increase the batch size to reduce the total quantity of communication during DNN training, and we identify hyperparameters that allow us to reproduce the small-batch accuracy while training with large batch sizes. When training GoogLeNet and Network-in-Network on ImageNet, we achieve a 47x and 39x speedup, respectively, when training on a cluster of 128 GPUs.



