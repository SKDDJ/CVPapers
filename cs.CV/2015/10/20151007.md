# Arxiv Papers in cs.CV on 2015-10-07
### Event-based Camera Pose Tracking using a Generative Event Model
- **Arxiv ID**: http://arxiv.org/abs/1510.01972v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1510.01972v1)
- **Published**: 2015-10-07 14:52:08+00:00
- **Updated**: 2015-10-07 14:52:08+00:00
- **Authors**: Guillermo Gallego, Christian Forster, Elias Mueggler, Davide Scaramuzza
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: Event-based vision sensors mimic the operation of biological retina and they represent a major paradigm shift from traditional cameras. Instead of providing frames of intensity measurements synchronously, at artificially chosen rates, event-based cameras provide information on brightness changes asynchronously, when they occur. Such non-redundant pieces of information are called "events". These sensors overcome some of the limitations of traditional cameras (response time, bandwidth and dynamic range) but require new methods to deal with the data they output. We tackle the problem of event-based camera localization in a known environment, without additional sensing, using a probabilistic generative event model in a Bayesian filtering framework. Our main contribution is the design of the likelihood function used in the filter to process the observed events. Based on the physical characteristics of the sensor and on empirical evidence of the Gaussian-like distribution of spiked events with respect to the brightness change, we propose to use the contrast residual as a measure of how well the estimated pose of the event-based camera and the environment explain the observed events. The filter allows for localization in the general case of six degrees-of-freedom motions.



### Diverse Large-Scale ITS Dataset Created from Continuous Learning for Real-Time Vehicle Detection
- **Arxiv ID**: http://arxiv.org/abs/1510.02055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.02055v1)
- **Published**: 2015-10-07 18:34:36+00:00
- **Updated**: 2015-10-07 18:34:36+00:00
- **Authors**: Justin A. Eichel, Akshaya Mishra, Nicholas Miller, Nicholas Jankovic, Mohan A. Thomas, Tyler Abbott, Douglas Swanson, Joel Keller
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: In traffic engineering, vehicle detectors are trained on limited datasets resulting in poor accuracy when deployed in real world applications. Annotating large-scale high quality datasets is challenging. Typically, these datasets have limited diversity; they do not reflect the real-world operating environment. There is a need for a large-scale, cloud based positive and negative mining (PNM) process and a large-scale learning and evaluation system for the application of traffic event detection. The proposed positive and negative mining process addresses the quality of crowd sourced ground truth data through machine learning review and human feedback mechanisms. The proposed learning and evaluation system uses a distributed cloud computing framework to handle data-scaling issues associated with large numbers of samples and a high-dimensional feature space. The system is trained using AdaBoost on $1,000,000$ Haar-like features extracted from $70,000$ annotated video frames. The trained real-time vehicle detector achieves an accuracy of at least $95\%$ for $1/2$ and about $78\%$ for $19/20$ of the time when tested on approximately $7,500,000$ video frames. At the end of 2015, the dataset is expect to have over one billion annotated video frames.



### Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1510.02071v1
- **DOI**: 10.1109/CVPR.2013.338
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.02071v1)
- **Published**: 2015-10-07 19:37:11+00:00
- **Updated**: 2015-10-07 19:37:11+00:00
- **Authors**: Vinay Bettadapura, Grant Schindler, Thomaz Plotz, Irfan Essa
- **Comment**: 8 pages
- **Journal**: Proceedings of the 2013 IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR 2013) -- Pages 2619 - 2626
- **Summary**: We present data-driven techniques to augment Bag of Words (BoW) models, which allow for more robust modeling and recognition of complex long-term activities, especially when the structure and topology of the activities are not known a priori. Our approach specifically addresses the limitations of standard BoW approaches, which fail to represent the underlying temporal and causal information that is inherent in activity streams. In addition, we also propose the use of randomly sampled regular expressions to discover and encode patterns in activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets.



### Egocentric Field-of-View Localization Using First-Person Point-of-View Devices
- **Arxiv ID**: http://arxiv.org/abs/1510.02073v1
- **DOI**: 10.1109/WACV.2015.89
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.02073v1)
- **Published**: 2015-10-07 19:39:26+00:00
- **Updated**: 2015-10-07 19:39:26+00:00
- **Authors**: Vinay Bettadapura, Irfan Essa, Caroline Pantofaru
- **Comment**: 8 pages in Proceedings of the 2015 IEEE Winter Conference on
  Applications of Computer Vision (WACV 2015)
- **Journal**: None
- **Summary**: We present a technique that uses images, videos and sensor data taken from first-person point-of-view devices to perform egocentric field-of-view (FOV) localization. We define egocentric FOV localization as capturing the visual information from a person's field-of-view in a given environment and transferring this information onto a reference corpus of images and videos of the same space, hence determining what a person is attending to. Our method matches images and video taken from the first-person perspective with the reference corpus and refines the results using the first-person's head orientation information obtained using the device sensors. We demonstrate single and multi-user egocentric FOV localization in different indoor and outdoor environments with applications in augmented reality, event understanding and studying social interactions.



### Leveraging Context to Support Automated Food Recognition in Restaurants
- **Arxiv ID**: http://arxiv.org/abs/1510.02078v1
- **DOI**: 10.1109/WACV.2015.83
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.02078v1)
- **Published**: 2015-10-07 19:51:23+00:00
- **Updated**: 2015-10-07 19:51:23+00:00
- **Authors**: Vinay Bettadapura, Edison Thomaz, Aman Parnami, Gregory Abowd, Irfan Essa
- **Comment**: 8 pages in Proceedings of the 2015 IEEE Winter Conference on
  Applications of Computer Vision (WACV 2015)
- **Journal**: None
- **Summary**: The pervasiveness of mobile cameras has resulted in a dramatic increase in food photos, which are pictures reflecting what people eat. In this paper, we study how taking pictures of what we eat in restaurants can be used for the purpose of automating food journaling. We propose to leverage the context of where the picture was taken, with additional information about the restaurant, available online, coupled with state-of-the-art computer vision techniques to recognize the food being consumed. To this end, we demonstrate image-based recognition of foods eaten in restaurants by training a classifier with images from restaurant's online menu databases. We evaluate the performance of our system in unconstrained, real-world settings with food images taken in 10 restaurants across 5 different types of food (American, Indian, Italian, Mexican and Thai).



### DeepLogo: Hitting Logo Recognition with the Deep Neural Network Hammer
- **Arxiv ID**: http://arxiv.org/abs/1510.02131v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.02131v1)
- **Published**: 2015-10-07 21:01:34+00:00
- **Updated**: 2015-10-07 21:01:34+00:00
- **Authors**: Forrest N. Iandola, Anting Shen, Peter Gao, Kurt Keutzer
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, there has been a flurry of industrial activity around logo recognition, such as Ditto's service for marketers to track their brands in user-generated images, and LogoGrab's mobile app platform for logo recognition. However, relatively little academic or open-source logo recognition progress has been made in the last four years. Meanwhile, deep convolutional neural networks (DCNNs) have revolutionized a broad range of object recognition applications. In this work, we apply DCNNs to logo recognition. We propose several DCNN architectures, with which we surpass published state-of-art accuracy on a popular logo recognition dataset.



