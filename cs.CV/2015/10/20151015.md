# Arxiv Papers in cs.CV on 2015-10-15
### Scatter Component Analysis: A Unified Framework for Domain Adaptation and Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/1510.04373v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML, I.2.6; I.4
- **Links**: [PDF](http://arxiv.org/pdf/1510.04373v2)
- **Published**: 2015-10-15 01:41:12+00:00
- **Updated**: 2016-07-26 21:35:08+00:00
- **Authors**: Muhammad Ghifary, David Balduzzi, W. Bastiaan Kleijn, Mengjie Zhang
- **Comment**: to appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: This paper addresses classification tasks on a particular target domain in which labeled training data are only available from source domains different from (but related to) the target. Two closely related frameworks, domain adaptation and domain generalization, are concerned with such tasks, where the only difference between those frameworks is the availability of the unlabeled target data: domain adaptation can leverage unlabeled target information, while domain generalization cannot. We propose Scatter Component Analyis (SCA), a fast representation learning algorithm that can be applied to both domain adaptation and domain generalization. SCA is based on a simple geometrical measure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA finds a representation that trades between maximizing the separability of classes, minimizing the mismatch between domains, and maximizing the separability of data; each of which is quantified through scatter. The optimization problem of SCA can be reduced to a generalized eigenvalue problem, which results in a fast and exact solution. Comprehensive experiments on benchmark cross-domain object recognition datasets verify that SCA performs much faster than several state-of-the-art algorithms and also provides state-of-the-art classification accuracy in both domain adaptation and domain generalization. We also show that scatter can be used to establish a theoretical generalization bound in the case of domain adaptation.



### Sketch-based Manga Retrieval using Manga109 Dataset
- **Arxiv ID**: http://arxiv.org/abs/1510.04389v1
- **DOI**: 10.1007/s11042-016-4020-z
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1510.04389v1)
- **Published**: 2015-10-15 03:47:46+00:00
- **Updated**: 2015-10-15 03:47:46+00:00
- **Authors**: Yusuke Matsui, Kota Ito, Yuji Aramaki, Toshihiko Yamasaki, Kiyoharu Aizawa
- **Comment**: 13 pages
- **Journal**: Multimedia Tools and Applications, Volume 76, Issue 20, 2017
- **Summary**: Manga (Japanese comics) are popular worldwide. However, current e-manga archives offer very limited search support, including keyword-based search by title or author, or tag-based categorization. To make the manga search experience more intuitive, efficient, and enjoyable, we propose a content-based manga retrieval system. First, we propose a manga-specific image-describing framework. It consists of efficient margin labeling, edge orientation histogram feature description, and approximate nearest-neighbor search using product quantization. Second, we propose a sketch-based interface as a natural way to interact with manga content. The interface provides sketch-based querying, relevance feedback, and query retouch. For evaluation, we built a novel dataset of manga images, Manga109, which consists of 109 comic books of 21,142 pages drawn by professional manga artists. To the best of our knowledge, Manga109 is currently the biggest dataset of manga images available for research. We conducted a comparative study, a localization evaluation, and a large-scale qualitative study. From the experiments, we verified that: (1) the retrieval accuracy of the proposed method is higher than those of previous methods; (2) the proposed method can localize an object instance with reasonable runtime and accuracy; and (3) sketch querying is useful for manga search.



### Dual Principal Component Pursuit
- **Arxiv ID**: http://arxiv.org/abs/1510.04390v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1510.04390v5)
- **Published**: 2015-10-15 03:50:01+00:00
- **Updated**: 2019-11-07 11:11:23+00:00
- **Authors**: Manolis C. Tsakiris, Rene Vidal
- **Comment**: fixed two typos in section 7.3
- **Journal**: Journal of Machine Learning Research 19 (2018) 1-49
- **Summary**: We consider the problem of learning a linear subspace from data corrupted by outliers. Classical approaches are typically designed for the case in which the subspace dimension is small relative to the ambient dimension. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement; as such, it is particularly suitable for subspaces whose dimension is close to the ambient dimension (subspaces of high relative dimension). We pose the problem of computing normal vectors to the inlier subspace as a non-convex $\ell_1$ minimization problem on the sphere, which we call Dual Principal Component Pursuit (DPCP) problem. We provide theoretical guarantees under which every global solution to DPCP is a vector in the orthogonal complement of the inlier subspace. Moreover, we relax the non-convex DPCP problem to a recursion of linear programs whose solutions are shown to converge in a finite number of steps to a vector orthogonal to the subspace. In particular, when the inlier subspace is a hyperplane, the solutions to the recursion of linear programs converge to the global minimum of the non-convex DPCP problem in a finite number of steps. We also propose algorithms based on alternating minimization and iteratively re-weighted least squares, which are suitable for dealing with large-scale data. Experiments on synthetic data show that the proposed methods are able to handle more outliers and higher relative dimensions than current state-of-the-art methods, while experiments in the context of the three-view geometry problem in computer vision suggest that the proposed methods can be a useful or even superior alternative to traditional RANSAC-based approaches for computer vision and other applications.



### Filtrated Spectral Algebraic Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1510.04396v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1510.04396v1)
- **Published**: 2015-10-15 04:12:37+00:00
- **Updated**: 2015-10-15 04:12:37+00:00
- **Authors**: Manolis C. Tsakiris, Rene Vidal
- **Comment**: None
- **Journal**: None
- **Summary**: Algebraic Subspace Clustering (ASC) is a simple and elegant method based on polynomial fitting and differentiation for clustering noiseless data drawn from an arbitrary union of subspaces. In practice, however, ASC is limited to equi-dimensional subspaces because the estimation of the subspace dimension via algebraic methods is sensitive to noise. This paper proposes a new ASC algorithm that can handle noisy data drawn from subspaces of arbitrary dimensions. The key ideas are (1) to construct, at each point, a decreasing sequence of subspaces containing the subspace passing through that point; (2) to use the distances from any other point to each subspace in the sequence to construct a subspace clustering affinity, which is superior to alternative affinities both in theory and in practice. Experiments on the Hopkins 155 dataset demonstrate the superiority of the proposed method with respect to sparse and low rank subspace clustering methods.



### A Novel Approach for Human Action Recognition from Silhouette Images
- **Arxiv ID**: http://arxiv.org/abs/1510.04437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.04437v1)
- **Published**: 2015-10-15 08:10:42+00:00
- **Updated**: 2015-10-15 08:10:42+00:00
- **Authors**: Satyabrata Maity, Debotosh Bhattacharjee, Amlan Chakrabarti
- **Comment**: Manuscript
- **Journal**: None
- **Summary**: In this paper, a novel human action recognition technique from video is presented. Any action of human is a combination of several micro action sequences performed by one or more body parts of the human. The proposed approach uses spatio-temporal body parts movement (STBPM) features extracted from foreground silhouette of the human objects. The newly proposed STBPM feature estimates the movements of different body parts for any given time segment to classify actions. We also proposed a rule based logic named rule action classifier (RAC), which uses a series of condition action rules based on prior knowledge and hence does not required training to classify any action. Since we don't require training to classify actions, the proposed approach is view independent. The experimental results on publicly available Wizeman and MuHVAi datasets are compared with that of the related research work in terms of accuracy in the human action detection, and proposed technique outperforms the others.



### DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers
- **Arxiv ID**: http://arxiv.org/abs/1510.04445v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.04445v1)
- **Published**: 2015-10-15 08:34:54+00:00
- **Updated**: 2015-10-15 08:34:54+00:00
- **Authors**: Amir Ghodrati, Ali Diba, Marco Pedersoli, Tinne Tuytelaars, Luc Van Gool
- **Comment**: ICCV 2015
- **Journal**: None
- **Summary**: In this paper we evaluate the quality of the activation layers of a convolutional neural network (CNN) for the gen- eration of object proposals. We generate hypotheses in a sliding-window fashion over different activation layers and show that the final convolutional layers can find the object of interest with high recall but poor localization due to the coarseness of the feature maps. Instead, the first layers of the network can better localize the object of interest but with a reduced recall. Based on this observation we design a method for proposing object locations that is based on CNN features and that combines the best of both worlds. We build an inverse cascade that, going from the final to the initial convolutional layers of the CNN, selects the most promising object locations and refines their boxes in a coarse-to-fine manner. The method is efficient, because i) it uses the same features extracted for detection, ii) it aggregates features using integral images, and iii) it avoids a dense evaluation of the proposals due to the inverse coarse-to-fine cascade. The method is also accurate; it outperforms most of the previously proposed object proposals approaches and when plugged into a CNN-based detector produces state-of-the- art detection performance.



### Sparsity-aware Possibilistic Clustering Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1510.04493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.04493v1)
- **Published**: 2015-10-15 12:18:58+00:00
- **Updated**: 2015-10-15 12:18:58+00:00
- **Authors**: Spyridoula D. Xenaki, Konstantinos D. Koutroumbas, Athanasios A. Rontogiannis
- **Comment**: arXiv admin note: text overlap with arXiv:1412.3613
- **Journal**: None
- **Summary**: In this paper two novel possibilistic clustering algorithms are presented, which utilize the concept of sparsity. The first one, called sparse possibilistic c-means, exploits sparsity and can deal well with closely located clusters that may also be of significantly different densities. The second one, called sparse adaptive possibilistic c-means, is an extension of the first, where now the involved parameters are dynamically adapted. The latter can deal well with even more challenging cases, where, in addition to the above, clusters may be of significantly different variances. More specifically, it provides improved estimates of the cluster representatives, while, in addition, it has the ability to estimate the actual number of clusters, given an overestimate of it. Extensive experimental results on both synthetic and real data sets support the previous statements.



### Elasticity-based Matching by Minimizing the Symmetric Difference of Shapes
- **Arxiv ID**: http://arxiv.org/abs/1510.04563v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1510.04563v1)
- **Published**: 2015-10-15 14:51:35+00:00
- **Updated**: 2015-10-15 14:51:35+00:00
- **Authors**: Konrad Simon, Ronen Basri
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of matching two shapes assuming these shapes are related by an elastic deformation. Using linearized elasticity theory and the finite element method we seek an elastic deformation that is caused by simple external boundary forces and accounts for the difference between the two shapes. Our main contribution is in proposing a cost function and an optimization procedure to minimize the symmetric difference between the deformed and the target shapes as an alternative to point matches that guide the matching in other techniques. We show how to approximate the nonlinear optimization problem by a sequence of convex problems. We demonstrate the utility of our method in experiments and compare it to an ICP-like matching algorithm.



### Beyond Spatial Pyramid Matching: Space-time Extended Descriptor for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1510.04565v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.04565v1)
- **Published**: 2015-10-15 14:57:37+00:00
- **Updated**: 2015-10-15 14:57:37+00:00
- **Authors**: Zhenzhong Lan, Alexander G. Hauptmann
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of generating video features for action recognition. The spatial pyramid and its variants have been very popular feature models due to their success in balancing spatial location encoding and spatial invariance. Although it seems straightforward to extend spatial pyramid to the temporal domain (spatio-temporal pyramid), the large spatio-temporal diversity of unconstrained videos and the resulting significantly higher dimensional representations make it less appealing. This paper introduces the space-time extended descriptor, a simple but efficient alternative way to include the spatio-temporal location into the video features. Instead of only coding motion information and leaving the spatio-temporal location to be represented at the pooling stage, location information is used as part of the encoding step. This method is a much more effective and efficient location encoding method as compared to the fixed grid model because it avoids the danger of over committing to artificial boundaries and its dimension is relatively low. Experimental results on several benchmark datasets show that, despite its simplicity, this method achieves comparable or better results than spatio-temporal pyramid.



### A Brief Survey of Image Processing Algorithms in Electrical Capacitance Tomography
- **Arxiv ID**: http://arxiv.org/abs/1510.04585v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.04585v1)
- **Published**: 2015-10-15 15:36:03+00:00
- **Updated**: 2015-10-15 15:36:03+00:00
- **Authors**: Kezhi Li
- **Comment**: Internal Report, MRRC, University of Cambridge
- **Journal**: None
- **Summary**: To study the fundamental physics of complex multiphase flow systems using advanced measurement techniques, especially the electrical capacitance tomography (ECT) approach, this article carries out an initial literature review of the ECT method from a point of view of signal processing and algorithm design. After introducing the physical laws governing the ECT system, we will focus on various reconstruction techniques that are capable to recover the image of the internal characteristics of a specified region based on the measuring capacitances of multi-electrode sensors surrounding the region. Each technique has its own advantages and limitations, and many algorithms have been examined by simulations or experiments. Future researches in 3D reconstruction and other potential improvements of the system are discussed in the end.



### A Picture is Worth a Billion Bits: Real-Time Image Reconstruction from Dense Binary Pixels
- **Arxiv ID**: http://arxiv.org/abs/1510.04601v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.04601v2)
- **Published**: 2015-10-15 16:05:06+00:00
- **Updated**: 2015-12-05 10:13:16+00:00
- **Authors**: Tal Remez, Or Litany, Alex Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: The pursuit of smaller pixel sizes at ever increasing resolution in digital image sensors is mainly driven by the stringent price and form-factor requirements of sensors and optics in the cellular phone market. Recently, Eric Fossum proposed a novel concept of an image sensor with dense sub-diffraction limit one-bit pixels jots, which can be considered a digital emulation of silver halide photographic film. This idea has been recently embodied as the EPFL Gigavision camera. A major bottleneck in the design of such sensors is the image reconstruction process, producing a continuous high dynamic range image from oversampled binary measurements. The extreme quantization of the Poisson statistics is incompatible with the assumptions of most standard image processing and enhancement frameworks. The recently proposed maximum-likelihood (ML) approach addresses this difficulty, but suffers from image artifacts and has impractically high computational complexity. In this work, we study a variant of a sensor with binary threshold pixels and propose a reconstruction algorithm combining an ML data fitting term with a sparse synthesis prior. We also show an efficient hardware-friendly real-time approximation of this inverse operator.Promising results are shown on synthetic data as well as on HDR data emulated using multiple exposures of a regular CMOS sensor.



### Layer-Specific Adaptive Learning Rates for Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1510.04609v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1510.04609v1)
- **Published**: 2015-10-15 16:31:46+00:00
- **Updated**: 2015-10-15 16:31:46+00:00
- **Authors**: Bharat Singh, Soham De, Yangmuzi Zhang, Thomas Goldstein, Gavin Taylor
- **Comment**: ICMLA 2015, deep learning, adaptive learning rates for training,
  layer specific learning rate
- **Journal**: None
- **Summary**: The increasing complexity of deep learning architectures is resulting in training time requiring weeks or even months. This slow training is due in part to vanishing gradients, in which the gradients used by back-propagation are extremely large for weights connecting deep layers (layers near the output layer), and extremely small for shallow layers (near the input layer); this results in slow learning in the shallow layers. Additionally, it has also been shown that in highly non-convex problems, such as deep neural networks, there is a proliferation of high-error low curvature saddle points, which slows down learning dramatically. In this paper, we attempt to overcome the two above problems by proposing an optimization method for training deep neural networks which uses learning rates which are both specific to each layer in the network and adaptive to the curvature of the function, increasing the learning rate at low curvature points. This enables us to speed up learning in the shallow layers of the network and quickly escape high-error low curvature saddle points. We test our method on standard image classification datasets such as MNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy as well as reduces the required training time over standard algorithms.



### Shape Complexes in Continuous Max-Flow Hierarchical Multi-Labeling Problems
- **Arxiv ID**: http://arxiv.org/abs/1510.04706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.04706v1)
- **Published**: 2015-10-15 20:10:49+00:00
- **Updated**: 2015-10-15 20:10:49+00:00
- **Authors**: John S. H. Baxter, Jing Yuan, Terry M. Peters
- **Comment**: 9 pages, 1 figure
- **Journal**: None
- **Summary**: Although topological considerations amongst multiple labels have been previously investigated in the context of continuous max-flow image segmentation, similar investigations have yet to be made about shape considerations in a general and extendable manner. This paper presents shape complexes for segmentation, which capture more complex shapes by combining multiple labels and super-labels constrained by geodesic star convexity. Shape complexes combine geodesic star convexity constraints with hierarchical label organization, which together allow for more complex shapes to be represented. This framework avoids the use of co-ordinate system warping techniques to convert shape constraints into topological constraints, which may be ambiguous or ill-defined for certain segmentation problems.



### Multilingual Image Description with Neural Sequence Models
- **Arxiv ID**: http://arxiv.org/abs/1510.04709v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1510.04709v2)
- **Published**: 2015-10-15 20:29:21+00:00
- **Updated**: 2015-11-18 17:04:35+00:00
- **Authors**: Desmond Elliott, Stella Frank, Eva Hasler
- **Comment**: Under review as a conference paper at ICLR 2016
- **Journal**: None
- **Summary**: In this paper we present an approach to multi-language image description bringing together insights from neural machine translation and neural image description. To create a description of an image for a given target language, our sequence generation models condition on feature vectors from the image, the description from the source language, and/or a multimodal vector computed over the image and a description in the source language. In image description experiments on the IAPR-TC12 dataset of images aligned with English and German sentences, we find significant and substantial improvements in BLEU4 and Meteor scores for models trained over multiple languages, compared to a monolingual baseline.



