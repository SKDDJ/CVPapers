# Arxiv Papers in cs.CV on 2015-10-06
### On the Existence of Epipolar Matrices
- **Arxiv ID**: http://arxiv.org/abs/1510.01401v1
- **DOI**: None
- **Categories**: **cs.CV**, math.AG
- **Links**: [PDF](http://arxiv.org/pdf/1510.01401v1)
- **Published**: 2015-10-06 00:00:27+00:00
- **Updated**: 2015-10-06 00:00:27+00:00
- **Authors**: Sameer Agarwal, Hon-Leung Lee, Bernd Sturmfels, Rekha R. Thomas
- **Comment**: 19 pages, 2 figures; This paper is related to our previous paper
  arXiv:1407.5367. However, the two papers differ enough in their focus and
  results that they merit being archived separately
- **Journal**: None
- **Summary**: This paper considers the foundational question of the existence of a fundamental (resp. essential) matrix given $m$ point correspondences in two views. We present a complete answer for the existence of fundamental matrices for any value of $m$. Using examples we disprove the widely held beliefs that fundamental matrices always exist whenever $m \leq 7$. At the same time, we prove that they exist unconditionally when $m \leq 5$. Under a mild genericity condition, we show that an essential matrix always exists when $m \leq 4$. We also characterize the six and seven point configurations in two views for which all matrices satisfying the epipolar constraint have rank at most one.



### SentiCap: Generating Image Descriptions with Sentiments
- **Arxiv ID**: http://arxiv.org/abs/1510.01431v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, I.2.10; I.2.7; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1510.01431v2)
- **Published**: 2015-10-06 04:57:47+00:00
- **Updated**: 2015-12-13 23:03:23+00:00
- **Authors**: Alexander Mathews, Lexing Xie, Xuming He
- **Comment**: None
- **Journal**: None
- **Summary**: The recent progress on image recognition and language modeling is making automatic description of image content a reality. However, stylized, non-factual aspects of the written description are missing from the current systems. One such style is descriptions with emotions, which is commonplace in everyday communication, and influences decision-making and interpersonal relationships. We design a system to describe an image with emotions, and present a model that automatically generates captions with positive or negative sentiments. We propose a novel switching recurrent neural network with word-level regularization, which is able to produce emotional image captions using only 2000+ training sentences containing sentiments. We evaluate the captions with different automatic and crowd-sourcing metrics. Our model compares favourably in common quality metrics for image captioning. In 84.6% of cases the generated positive captions were judged as being at least as descriptive as the factual captions. Of these positive captions 88% were confirmed by the crowd-sourced workers as having the appropriate sentiment.



### Harvesting Discriminative Meta Objects with Deep CNN Features for Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/1510.01440v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.01440v1)
- **Published**: 2015-10-06 05:59:11+00:00
- **Updated**: 2015-10-06 05:59:11+00:00
- **Authors**: Ruobing Wu, Baoyuan Wang, Wenping Wang, Yizhou Yu
- **Comment**: To Appear in ICCV 2015
- **Journal**: None
- **Summary**: Recent work on scene classification still makes use of generic CNN features in a rudimentary manner. In this ICCV 2015 paper, we present a novel pipeline built upon deep CNN features to harvest discriminative visual objects and parts for scene classification. We first use a region proposal technique to generate a set of high-quality patches potentially containing objects, and apply a pre-trained CNN to extract generic deep features from these patches. Then we perform both unsupervised and weakly supervised learning to screen these patches and discover discriminative ones representing category-specific objects and parts. We further apply discriminative clustering enhanced with local CNN fine-tuning to aggregate similar objects and parts into groups, called meta objects. A scene image representation is constructed by pooling the feature response maps of all the learned meta objects at multiple spatial scales. We have confirmed that the scene image representation obtained using this new pipeline is capable of delivering state-of-the-art performance on two popular scene benchmark datasets, MIT Indoor 67~\cite{MITIndoor67} and Sun397~\cite{Sun397}



### Unsupervised Extraction of Video Highlights Via Robust Recurrent Auto-encoders
- **Arxiv ID**: http://arxiv.org/abs/1510.01442v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.01442v1)
- **Published**: 2015-10-06 06:07:50+00:00
- **Updated**: 2015-10-06 06:07:50+00:00
- **Authors**: Huan Yang, Baoyuan Wang, Stephen Lin, David Wipf, Minyi Guo, Baining Guo
- **Comment**: To Appear in ICCV 2015
- **Journal**: None
- **Summary**: With the growing popularity of short-form video sharing platforms such as \em{Instagram} and \em{Vine}, there has been an increasing need for techniques that automatically extract highlights from video. Whereas prior works have approached this problem with heuristic rules or supervised learning, we present an unsupervised learning approach that takes advantage of the abundance of user-edited videos on social media websites such as YouTube. Based on the idea that the most significant sub-events within a video class are commonly present among edited videos while less interesting ones appear less frequently, we identify the significant sub-events via a robust recurrent auto-encoder trained on a collection of user-edited videos queried for each particular class of interest. The auto-encoder is trained using a proposed shrinking exponential loss function that makes it robust to noise in the web-crawled training data, and is configured with bidirectional long short term memory (LSTM)~\cite{LSTM:97} cells to better model the temporal structure of highlight segments. Different from supervised techniques, our method can infer highlights using only a set of downloaded edited videos, without also needing their pre-edited counterparts which are rarely available online. Extensive experiments indicate the promise of our proposed solution in this challenging unsupervised settin



### Directional Global Three-part Image Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1510.01490v1
- **DOI**: 10.1186/s13640-016-0110-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.01490v1)
- **Published**: 2015-10-06 09:11:47+00:00
- **Updated**: 2015-10-06 09:11:47+00:00
- **Authors**: Duy Hoang Thai, Carsten Gottschlich
- **Comment**: None
- **Journal**: EURASIP Journal on Image and Video Processing, vol. 2016, no. 12,
  pp. 1-20, Mar. 2016
- **Summary**: We consider the task of image decomposition and we introduce a new model coined directional global three-part decomposition (DG3PD) for solving it. As key ingredients of the DG3PD model, we introduce a discrete multi-directional total variation norm and a discrete multi-directional G-norm. Using these novel norms, the proposed discrete DG3PD model can decompose an image into two parts or into three parts. Existing models for image decomposition by Vese and Osher, by Aujol and Chambolle, by Starck et al., and by Thai and Gottschlich are included as special cases in the new model. Decomposition of an image by DG3PD results in a cartoon image, a texture image and a residual image. Advantages of the DG3PD model over existing ones lie in the properties enforced on the cartoon and texture images. The geometric objects in the cartoon image have a very smooth surface and sharp edges. The texture image yields oscillating patterns on a defined scale which is both smooth and sparse. Moreover, the DG3PD method achieves the goal of perfect reconstruction by summation of all components better than the other considered methods. Relevant applications of DG3PD are a novel way of image compression as well as feature extraction for applications such as latent fingerprint processing and optical character recognition.



### Active Transfer Learning with Zero-Shot Priors: Reusing Past Datasets for Future Tasks
- **Arxiv ID**: http://arxiv.org/abs/1510.01544v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.01544v1)
- **Published**: 2015-10-06 12:06:19+00:00
- **Updated**: 2015-10-06 12:06:19+00:00
- **Authors**: Efstratios Gavves, Thomas Mensink, Tatiana Tommasi, Cees G. M. Snoek, Tinne Tuytelaars
- **Comment**: None
- **Journal**: None
- **Summary**: How can we reuse existing knowledge, in the form of available datasets, when solving a new and apparently unrelated target task from a set of unlabeled data? In this work we make a first contribution to answer this question in the context of image classification. We frame this quest as an active learning problem and use zero-shot classifiers to guide the learning process by linking the new task to the existing classifiers. By revisiting the dual formulation of adaptive SVM, we reveal two basic conditions to choose greedily only the most relevant samples to be annotated. On this basis we propose an effective active learning algorithm which learns the best possible target classification model with minimum human labeling effort. Extensive experiments on two challenging datasets show the value of our approach compared to the state-of-the-art active learning methodologies, as well as its potential to reuse past datasets with minimal effort for future tasks.



### Learning Deep Representations of Appearance and Motion for Anomalous Event Detection
- **Arxiv ID**: http://arxiv.org/abs/1510.01553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.01553v1)
- **Published**: 2015-10-06 12:42:55+00:00
- **Updated**: 2015-10-06 12:42:55+00:00
- **Authors**: Dan Xu, Elisa Ricci, Yan Yan, Jingkuan Song, Nicu Sebe
- **Comment**: Oral paper in BMVC 2015
- **Journal**: None
- **Summary**: We present a novel unsupervised deep learning framework for anomalous event detection in complex video scenes. While most existing works merely use hand-crafted appearance and motion features, we propose Appearance and Motion DeepNet (AMDN) which utilizes deep neural networks to automatically learn feature representations. To exploit the complementary information of both appearance and motion patterns, we introduce a novel double fusion framework, combining both the benefits of traditional early fusion and late fusion strategies. Specifically, stacked denoising autoencoders are proposed to separately learn both appearance and motion features as well as a joint representation (early fusion). Based on the learned representations, multiple one-class SVM models are used to predict the anomaly scores of each input, which are then integrated with a late fusion strategy for final anomaly detection. We evaluate the proposed method on two publicly available video surveillance datasets, showing competitive performance with respect to state of the art approaches.



### Predicting Daily Activities From Egocentric Images Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1510.01576v1
- **DOI**: 10.1145/2802083.2808398
- **Categories**: **cs.CV**, I.5; J.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1510.01576v1)
- **Published**: 2015-10-06 13:56:50+00:00
- **Updated**: 2015-10-06 13:56:50+00:00
- **Authors**: Daniel Castro, Steven Hickson, Vinay Bettadapura, Edison Thomaz, Gregory Abowd, Henrik Christensen, Irfan Essa
- **Comment**: 8 pages
- **Journal**: ISWC '15 Proceedings of the 2015 ACM International Symposium on
  Wearable Computers - Pages 75-82
- **Summary**: We present a method to analyze images taken from a passive egocentric wearable camera along with the contextual information, such as time and day of week, to learn and predict everyday activities of an individual. We collected a dataset of 40,103 egocentric images over a 6 month period with 19 activity classes and demonstrate the benefit of state-of-the-art deep learning techniques for learning and predicting daily activities. Classification is conducted using a Convolutional Neural Network (CNN) with a classification method we introduce called a late fusion ensemble. This late fusion ensemble incorporates relevant contextual information and increases our classification accuracy. Our technique achieves an overall accuracy of 83.07% in predicting a person's activity across the 19 activity classes. We also demonstrate some promising results from two additional users by fine-tuning the classifier with one day of training data.



### Large-scale subspace clustering using sketching and validation
- **Arxiv ID**: http://arxiv.org/abs/1510.01628v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1510.01628v1)
- **Published**: 2015-10-06 15:34:32+00:00
- **Updated**: 2015-10-06 15:34:32+00:00
- **Authors**: Panagiotis A. Traganitis, Konstantinos Slavakis, Georgios B. Giannakis
- **Comment**: None
- **Journal**: None
- **Summary**: The nowadays massive amounts of generated and communicated data present major challenges in their processing. While capable of successfully classifying nonlinearly separable objects in various settings, subspace clustering (SC) methods incur prohibitively high computational complexity when processing large-scale data. Inspired by the random sampling and consensus (RANSAC) approach to robust regression, the present paper introduces a randomized scheme for SC, termed sketching and validation (SkeVa-)SC, tailored for large-scale data. At the heart of SkeVa-SC lies a randomized scheme for approximating the underlying probability density function of the observed data by kernel smoothing arguments. Sparsity in data representations is also exploited to reduce the computational burden of SC, while achieving high clustering accuracy. Performance analysis as well as extensive numerical tests on synthetic and real data corroborate the potential of SkeVa-SC and its competitive performance relative to state-of-the-art scalable SC approaches. Keywords: Subspace clustering, big data, kernel smoothing, randomization, sketching, validation, sparsity.



### A Latent Source Model for Patch-Based Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1510.01648v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.01648v1)
- **Published**: 2015-10-06 16:28:44+00:00
- **Updated**: 2015-10-06 16:28:44+00:00
- **Authors**: George Chen, Devavrat Shah, Polina Golland
- **Comment**: International Conference on Medical Image Computing and Computer
  Assisted Interventions 2015
- **Journal**: None
- **Summary**: Despite the popularity and empirical success of patch-based nearest-neighbor and weighted majority voting approaches to medical image segmentation, there has been no theoretical development on when, why, and how well these nonparametric methods work. We bridge this gap by providing a theoretical performance guarantee for nearest-neighbor and weighted majority voting segmentation under a new probabilistic model for patch-based image segmentation. Our analysis relies on a new local property for how similar nearby patches are, and fuses existing lines of work on modeling natural imagery patches and theory for nonparametric classification. We use the model to derive a new patch-based segmentation algorithm that iterates between inferring local label patches and merging these local segmentations to produce a globally consistent image segmentation. Many existing patch-based algorithms arise as special cases of the new algorithm.



### Euclidean Auto Calibration of Camera Networks: Baseline Constraint Removes Scale Ambiguity
- **Arxiv ID**: http://arxiv.org/abs/1510.01663v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.01663v1)
- **Published**: 2015-10-06 16:56:22+00:00
- **Updated**: 2015-10-06 16:56:22+00:00
- **Authors**: Kiran Kumar Vupparaboina, Kamala Raghavan, Soumya Jana
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Metric auto calibration of a camera network from multiple views has been reported by several authors. Resulting 3D reconstruction recovers shape faithfully, but not scale. However, preservation of scale becomes critical in applications, such as multi-party telepresence, where multiple 3D scenes need to be fused into a single coordinate system. In this context, we propose a camera network configuration that includes a stereo pair with known baseline separation, and analytically demonstrate Euclidean auto calibration of such network under mild conditions. Further, we experimentally validate our theory using a four-camera network. Importantly, our method not only recovers scale, but also compares favorably with the well known Zhang and Pollefeys methods in terms of shape recovery.



### Structured Transforms for Small-Footprint Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1510.01722v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1510.01722v1)
- **Published**: 2015-10-06 19:42:22+00:00
- **Updated**: 2015-10-06 19:42:22+00:00
- **Authors**: Vikas Sindhwani, Tara N. Sainath, Sanjiv Kumar
- **Comment**: To appear in NIPS 2015; 9 pages
- **Journal**: None
- **Summary**: We consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices. We propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank. Our structured transforms admit fast function and gradient evaluation, and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured. Experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training, and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques. In keyword spotting applications in mobile speech recognition, our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models, while providing more than 3.5-fold compression.



