# Arxiv Papers in cs.CV on 2015-10-01
### Transfer Learning from Deep Features for Remote Sensing and Poverty Mapping
- **Arxiv ID**: http://arxiv.org/abs/1510.00098v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1510.00098v2)
- **Published**: 2015-10-01 03:04:29+00:00
- **Updated**: 2016-02-27 23:21:48+00:00
- **Authors**: Michael Xie, Neal Jean, Marshall Burke, David Lobell, Stefano Ermon
- **Comment**: In Proc. 30th AAAI Conference on Artificial Intelligence
- **Journal**: None
- **Summary**: The lack of reliable data in developing countries is a major obstacle to sustainable development, food security, and disaster relief. Poverty data, for example, is typically scarce, sparse in coverage, and labor-intensive to obtain. Remote sensing data such as high-resolution satellite imagery, on the other hand, is becoming increasingly available and inexpensive. Unfortunately, such data is highly unstructured and currently no techniques exist to automatically extract useful insights to inform policy decisions and help direct humanitarian efforts. We propose a novel machine learning approach to extract large-scale socioeconomic indicators from high-resolution satellite imagery. The main challenge is that training data is very scarce, making it difficult to apply modern techniques such as Convolutional Neural Networks (CNN). We therefore propose a transfer learning approach where nighttime light intensities are used as a data-rich proxy. We train a fully convolutional CNN model to predict nighttime lights from daytime imagery, simultaneously learning features that are useful for poverty prediction. The model learns filters identifying different terrains and man-made structures, including roads, buildings, and farmlands, without any supervision beyond nighttime lights. We demonstrate that these learned features are highly informative for poverty mapping, even approaching the predictive performance of survey data collected in the field.



### Fast Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1510.00143v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.00143v3)
- **Published**: 2015-10-01 08:47:28+00:00
- **Updated**: 2016-05-02 13:40:30+00:00
- **Authors**: Ningning Zhao, Qi Wei, Adrian Basarab, Nicolas Dobigeon, Denis Kouame, Jean-Yves Tourneret
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of single image super-resolution (SR), which consists of recovering a high resolution image from its blurred, decimated and noisy version. The existing algorithms for single image SR use different strategies to handle the decimation and blurring operators. In addition to the traditional first-order gradient methods, recent techniques investigate splitting-based methods dividing the SR problem into up-sampling and deconvolution steps that can be easily solved. Instead of following this splitting strategy, we propose to deal with the decimation and blurring operators simultaneously by taking advantage of their particular properties in the frequency domain, leading to a new fast SR approach. Specifically, an analytical solution can be obtained and implemented efficiently for the Gaussian prior or any other regularization that can be formulated into an $\ell_2$-regularized quadratic model, i.e., an $\ell_2$-$\ell_2$ optimization problem. Furthermore, the flexibility of the proposed SR scheme is shown through the use of various priors/regularizations, ranging from generic image priors to learning-based approaches. In the case of non-Gaussian priors, we show how the analytical solution derived from the Gaussian case can be embedded intotraditional splitting frameworks, allowing the computation cost of existing algorithms to be decreased significantly. Simulation results conducted on several images with different priors illustrate the effectiveness of our fast SR approach compared with the existing techniques.



### Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding
- **Arxiv ID**: http://arxiv.org/abs/1510.00149v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1510.00149v5)
- **Published**: 2015-10-01 09:03:44+00:00
- **Updated**: 2016-02-15 06:25:40+00:00
- **Authors**: Song Han, Huizi Mao, William J. Dally
- **Comment**: Published as a conference paper at ICLR 2016 (oral)
- **Journal**: None
- **Summary**: Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.



### Data Association for an Adaptive Multi-target Particle Filter Tracking System
- **Arxiv ID**: http://arxiv.org/abs/1510.00203v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1510.00203v1)
- **Published**: 2015-10-01 12:40:18+00:00
- **Updated**: 2015-10-01 12:40:18+00:00
- **Authors**: R. Alampay, K. Teknomo
- **Comment**: None
- **Journal**: Philippine Computing Journal Vol 7 (1) August 2012, p. 16-25
- **Summary**: This paper presents a novel approach to improve the accuracy of tracking multiple objects in a static scene using a particle filter system by introducing a data association step, a state queue for the collection of tracked objects and adaptive parameters to the system. The data association step makes use of the object detection phase and appearance model to determine if the approximated targets given by the particle filter step match the given set of detected objects. The remaining detected objects are used as information to instantiate new objects for tracking. State queues are also used for each tracked object to deal with occlusion events and occlusion recovery. Finally we present how the parameters adjust to occlusion events. The adaptive property of the system is also used for possible occlusion recovery. Results of the system are then compared to a ground truth data set for performance evaluation. Our system produced accurate results and was able to handle partially occluded objects as well as proper occlusion recovery from tracking multiple objects



### Off-the-Grid Recovery of Piecewise Constant Images from Few Fourier Samples
- **Arxiv ID**: http://arxiv.org/abs/1510.00384v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.00384v2)
- **Published**: 2015-10-01 20:01:37+00:00
- **Updated**: 2016-04-21 21:25:20+00:00
- **Authors**: Greg Ongie, Mathews Jacob
- **Comment**: Accepted for publication in SIAM Journal on Imaging Sciences
- **Journal**: None
- **Summary**: We introduce a method to recover a continuous domain representation of a piecewise constant two-dimensional image from few low-pass Fourier samples. Assuming the edge set of the image is localized to the zero set of a trigonometric polynomial, we show the Fourier coefficients of the partial derivatives of the image satisfy a linear annihilation relation. We present necessary and sufficient conditions for unique recovery of the image from finite low-pass Fourier samples using the annihilation relation. We also propose a practical two-stage recovery algorithm which is robust to model-mismatch and noise. In the first stage we estimate a continuous domain representation of the edge set of the image. In the second stage we perform an extrapolation in Fourier domain by a least squares two-dimensional linear prediction, which recovers the exact Fourier coefficients of the underlying image. We demonstrate our algorithm on the super-resolution recovery of MRI phantoms and real MRI data from low-pass Fourier samples, which shows benefits over standard approaches for single-image super-resolution MRI.



