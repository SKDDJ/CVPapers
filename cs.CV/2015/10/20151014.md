# Arxiv Papers in cs.CV on 2015-10-14
### Better Exploiting OS-CNNs for Better Event Recognition in Images
- **Arxiv ID**: http://arxiv.org/abs/1510.03979v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.03979v1)
- **Published**: 2015-10-14 06:56:54+00:00
- **Updated**: 2015-10-14 06:56:54+00:00
- **Authors**: Limin Wang, Zhe Wang, Sheng Guo, Yu Qiao
- **Comment**: 8 pages. This work is following our previous work:
  http://arxiv.org/abs/1505.00296
- **Journal**: None
- **Summary**: Event recognition from still images is one of the most important problems for image understanding. However, compared with object recognition and scene recognition, event recognition has received much less research attention in computer vision community. This paper addresses the problem of cultural event recognition in still images and focuses on applying deep learning methods on this problem. In particular, we utilize the successful architecture of Object-Scene Convolutional Neural Networks (OS-CNNs) to perform event recognition. OS-CNNs are composed of object nets and scene nets, which transfer the learned representations from the pre-trained models on large-scale object and scene recognition datasets, respectively. We propose four types of scenarios to explore OS-CNNs for event recognition by treating them as either "end-to-end event predictors" or "generic feature extractors". Our experimental results demonstrate that the global and local representations of OS-CNNs are complementary to each other. Finally, based on our investigation of OS-CNNs, we come up with a solution for the cultural event recognition track at the ICCV ChaLearn Looking at People (LAP) challenge 2015. Our team secures the third place at this challenge and our result is very close to the best performance.



### Multiresolution Search of the Rigid Motion Space for Intensity Based Registration
- **Arxiv ID**: http://arxiv.org/abs/1510.04004v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.04004v1)
- **Published**: 2015-10-14 08:47:12+00:00
- **Updated**: 2015-10-14 08:47:12+00:00
- **Authors**: Behrooz Nasihatkon, Fredrik Kahl
- **Comment**: None
- **Journal**: None
- **Summary**: We study the relation between the target functions of low-resolution and high-resolution intensity-based registration for the class of rigid transformations. Our results show that low resolution target values can tightly bound the high-resolution target function in natural images. This can help with analyzing and better understanding the process of multiresolution image registration. It also gives a guideline for designing multiresolution algorithms in which the search space in higher resolution registration is restricted given the fitness values for lower resolution image pairs. To demonstrate this, we incorporate our multiresolution technique into a Lipschitz global optimization framework. We show that using the multiresolution scheme can result in large gains in the efficiency of such algorithms. The method is evaluated by applying to 2D and 3D registration problems as well as the detection of reflective symmetry in 2D and 3D images.



### Fine-Grained Product Class Recognition for Assisted Shopping
- **Arxiv ID**: http://arxiv.org/abs/1510.04074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.04074v1)
- **Published**: 2015-10-14 13:07:05+00:00
- **Updated**: 2015-10-14 13:07:05+00:00
- **Authors**: Marian George, Dejan Mircic, Gábor Sörös, Christian Floerkemeier, Friedemann Mattern
- **Comment**: Accepted at ICCV Workshop on Assistive Computer Vision and Robotics
  (ICCV-ACVR) 2015
- **Journal**: None
- **Summary**: Assistive solutions for a better shopping experience can improve the quality of life of people, in particular also of visually impaired shoppers. We present a system that visually recognizes the fine-grained product classes of items on a shopping list, in shelves images taken with a smartphone in a grocery store. Our system consists of three components: (a) We automatically recognize useful text on product packaging, e.g., product name and brand, and build a mapping of words to product classes based on the large-scale GroceryProducts dataset. When the user populates the shopping list, we automatically infer the product class of each entered word. (b) We perform fine-grained product class recognition when the user is facing a shelf. We discover discriminative patches on product packaging to differentiate between visually similar product classes and to increase the robustness against continuous changes in product design. (c) We continuously improve the recognition accuracy through active learning. Our experiments show the robustness of the proposed method against cross-domain challenges, and the scalability to an increasing number of products with minimal re-training.



### Dynamical spectral unmixing of multitemporal hyperspectral images
- **Arxiv ID**: http://arxiv.org/abs/1510.04238v1
- **DOI**: 10.1109/TIP.2016.2562562
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.04238v1)
- **Published**: 2015-10-14 18:51:51+00:00
- **Updated**: 2015-10-14 18:51:51+00:00
- **Authors**: Simon Henrot, Jocelyn Chanussot, Christian Jutten
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: In this paper, we consider the problem of unmixing a time series of hyperspectral images. We propose a dynamical model based on linear mixing processes at each time instant. The spectral signatures and fractional abundances of the pure materials in the scene are seen as latent variables, and assumed to follow a general dynamical structure. Based on a simplified version of this model, we derive an efficient spectral unmixing algorithm to estimate the latent variables by performing alternating minimizations. The performance of the proposed approach is demonstrated on synthetic and real multitemporal hyperspectral images.



