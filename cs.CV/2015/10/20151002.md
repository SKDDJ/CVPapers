# Arxiv Papers in cs.CV on 2015-10-02
### Learning a Discriminative Model for the Perception of Realism in Composite Images
- **Arxiv ID**: http://arxiv.org/abs/1510.00477v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.00477v1)
- **Published**: 2015-10-02 03:06:34+00:00
- **Updated**: 2015-10-02 03:06:34+00:00
- **Authors**: Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, Alexei A. Efros
- **Comment**: International Conference on Computer Vision (ICCV) 2015
- **Journal**: None
- **Summary**: What makes an image appear realistic? In this work, we are answering this question from a data-driven perspective by learning the perception of visual realism directly from large amounts of data. In particular, we train a Convolutional Neural Network (CNN) model that distinguishes natural photographs from automatically generated composite images. The model learns to predict visual realism of a scene in terms of color, lighting and texture compatibility, without any human annotations pertaining to it. Our model outperforms previous works that rely on hand-crafted heuristics, for the task of classifying realistic vs. unrealistic photos. Furthermore, we apply our learned model to compute optimal parameters of a compositing method, to maximize the visual realism score predicted by our CNN model. We demonstrate its advantage against existing methods via a human perception study.



### Effective Object Tracking in Unstructured Crowd Scenes
- **Arxiv ID**: http://arxiv.org/abs/1510.00479v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.00479v1)
- **Published**: 2015-10-02 03:19:16+00:00
- **Updated**: 2015-10-02 03:19:16+00:00
- **Authors**: Ishan Jindal, Shanmuganathan Raman
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we are presenting a rotation variant Oriented Texture Curve (OTC) descriptor based mean shift algorithm for tracking an object in an unstructured crowd scene. The proposed algorithm works by first obtaining the OTC features for a manually selected object target, then a visual vocabulary is created by using all the OTC features of the target. The target histogram is obtained using codebook encoding method which is then used in mean shift framework to perform similarity search. Results are obtained on different videos of challenging scenes and the comparison of the proposed approach with several state-of-the-art approaches are provided. The analysis shows the advantages and limitations of the proposed approach for tracking an object in unstructured crowd scenes.



### Local Higher-Order Statistics (LHS) describing images with statistics of local non-binarized pixel patterns
- **Arxiv ID**: http://arxiv.org/abs/1510.00542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.00542v1)
- **Published**: 2015-10-02 09:41:39+00:00
- **Updated**: 2015-10-02 09:41:39+00:00
- **Authors**: Gaurav Sharma, Frederic Jurie
- **Comment**: CVIU preprint
- **Journal**: None
- **Summary**: We propose a new image representation for texture categorization and facial analysis, relying on the use of higher-order local differential statistics as features. It has been recently shown that small local pixel pattern distributions can be highly discriminative while being extremely efficient to compute, which is in contrast to the models based on the global structure of images. Motivated by such works, we propose to use higher-order statistics of local non-binarized pixel patterns for the image description. The proposed model does not require either (i) user specified quantization of the space (of pixel patterns) or (ii) any heuristics for discarding low occupancy volumes of the space. We propose to use a data driven soft quantization of the space, with parametric mixture models, combined with higher-order statistics, based on Fisher scores. We demonstrate that this leads to a more expressive representation which, when combined with discriminatively learned classifiers and metrics, achieves state-of-the-art performance on challenging texture and facial analysis datasets, in low complexity setup. Further, it is complementary to higher complexity features and when combined with them improves performance.



### Human Action Recognition using Factorized Spatio-Temporal Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1510.00562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.00562v1)
- **Published**: 2015-10-02 11:24:04+00:00
- **Updated**: 2015-10-02 11:24:04+00:00
- **Authors**: Lin Sun, Kui Jia, Dit-Yan Yeung, Bertram E. Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Human actions in video sequences are three-dimensional (3D) spatio-temporal signals characterizing both the visual appearance and motion dynamics of the involved humans and objects. Inspired by the success of convolutional neural networks (CNN) for image classification, recent attempts have been made to learn 3D CNNs for recognizing human actions in videos. However, partly due to the high complexity of training 3D convolution kernels and the need for large quantities of training videos, only limited success has been reported. This has triggered us to investigate in this paper a new deep architecture which can handle 3D signals more effectively. Specifically, we propose factorized spatio-temporal convolutional networks (FstCN) that factorize the original 3D convolution kernel learning as a sequential process of learning 2D spatial kernels in the lower layers (called spatial convolutional layers), followed by learning 1D temporal kernels in the upper layers (called temporal convolutional layers). We introduce a novel transformation and permutation operator to make factorization in FstCN possible. Moreover, to address the issue of sequence alignment, we propose an effective training and inference strategy based on sampling multiple video clips from a given action video sequence. We have tested FstCN on two commonly used benchmark datasets (UCF-101 and HMDB-51). Without using auxiliary training videos to boost the performance, FstCN outperforms existing CNN based methods and achieves comparable performance with a recent method that benefits from using auxiliary training videos.



### WHOI-Plankton- A Large Scale Fine Grained Visual Recognition Benchmark Dataset for Plankton Classification
- **Arxiv ID**: http://arxiv.org/abs/1510.00745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.00745v1)
- **Published**: 2015-10-02 22:06:52+00:00
- **Updated**: 2015-10-02 22:06:52+00:00
- **Authors**: Eric C. Orenstein, Oscar Beijbom, Emily E. Peacock, Heidi M. Sosik
- **Comment**: 2 pages, 1 figure, presented at the Third Workshop on Fine-Grained
  Visual Categorization at CVPR 2015
- **Journal**: None
- **Summary**: Planktonic organisms are of fundamental importance to marine ecosystems: they form the basis of the food web, provide the link between the atmosphere and the deep ocean, and influence global-scale biogeochemical cycles. Scientists are increasingly using imaging-based technologies to study these creatures in their natural habit. Images from such systems provide an unique opportunity to model and understand plankton ecosystems, but the collected datasets can be enormous. The Imaging FlowCytobot (IFCB) at Woods Hole Oceanographic Institution, for example, is an \emph{in situ} system that has been continuously imaging plankton since 2006. To date, it has generated more than 700 million samples. Manual classification of such a vast image collection is impractical due to the size of the data set. In addition, the annotation task is challenging due to the large space of relevant classes, intra-class variability, and inter-class similarity. Methods for automated classification exist, but the accuracy is often below that of human experts. Here we introduce WHOI-Plankton: a large scale, fine-grained visual recognition dataset for plankton classification, which comprises over 3.4 million expert-labeled images across 70 classes. The labeled image set is complied from over 8 years of near continuous data collection with the IFCB at the Martha's Vineyard Coastal Observatory (MVCO). We discuss relevant metrics for evaluation of classification performance and provide results for a traditional method based on hand-engineered features and two methods based on convolutional neural networks.



