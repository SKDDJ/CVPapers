# Arxiv Papers in cs.CV on 2015-10-26
### Pan-Tilt Camera and PIR Sensor Fusion Based Moving Object Detection for Mobile Security Robots
- **Arxiv ID**: http://arxiv.org/abs/1510.07390v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1510.07390v1)
- **Published**: 2015-10-26 07:40:14+00:00
- **Updated**: 2015-10-26 07:40:14+00:00
- **Authors**: YongChol Sin, MyongSong Choe, GyongIl Ryang
- **Comment**: 13 pages,5 figures
- **Journal**: None
- **Summary**: One of fundamental issues for security robots is to detect and track people in the surroundings. The main problems of this task are real-time constraints, a changing background, varying illumination conditions and a non-rigid shape of the person to be tracked. In this paper, we propose a solution for tracking with a pan-tilt camera and a passive infrared range (PIR) sensor to detect the moving object based on consecutive frame difference. The proposed method is excellent in real-time performance because it requires only a little memory and computation. Experiment results show that this method can detect the moving object such as human efficiently and accurately in non-stationary and complex indoor environment.



### Vehicle Color Recognition using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1510.07391v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.07391v3)
- **Published**: 2015-10-26 07:41:32+00:00
- **Updated**: 2018-08-15 06:49:37+00:00
- **Authors**: Reza Fuad Rachmadi, I Ketut Eddy Purnama
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle color information is one of the important elements in ITS (Intelligent Traffic System). In this paper, we present a vehicle color recognition method using convolutional neural network (CNN). Naturally, CNN is designed to learn classification method based on shape information, but we proved that CNN can also learn classification based on color distribution. In our method, we convert the input image to two different color spaces, HSV and CIE Lab, and run it to some CNN architecture. The training process follow procedure introduce by Krizhevsky, that learning rate is decreasing by factor of 10 after some iterations. To test our method, we use publicly vehicle color recognition dataset provided by Chen. The results, our model outperform the original system provide by Chen with 2% higher overall accuracy.



### A Markov Random Field and Active Contour Image Segmentation Model for Animal Spots Patterns
- **Arxiv ID**: http://arxiv.org/abs/1510.07474v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.07474v1)
- **Published**: 2015-10-26 13:26:46+00:00
- **Updated**: 2015-10-26 13:26:46+00:00
- **Authors**: Alexander Gómez, German Díez, Jhony Giraldo, Augusto Salazar, Juan M. Daza
- **Comment**: 11th International Symposium on Visual Computing
- **Journal**: None
- **Summary**: Non-intrusive biometrics of animals using images allows to analyze phenotypic populations and individuals with patterns like stripes and spots without affecting the studied subjects. However, non-intrusive biometrics demand a well trained subject or the development of computer vision algorithms that ease the identification task. In this work, an analysis of classic segmentation approaches that require a supervised tuning of their parameters such as threshold, adaptive threshold, histogram equalization, and saturation correction is presented. In contrast, a general unsupervised algorithm using Markov Random Fields (MRF) for segmentation of spots patterns is proposed. Active contours are used to boost results using MRF output as seeds. As study subject the Diploglossus millepunctatus lizard is used. The proposed method achieved a maximum efficiency of $91.11\%$.



### Aggregating Deep Convolutional Features for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1510.07493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.07493v1)
- **Published**: 2015-10-26 14:35:26+00:00
- **Updated**: 2015-10-26 14:35:26+00:00
- **Authors**: Artem Babenko, Victor Lempitsky
- **Comment**: accepted for ICCV 2015
- **Journal**: None
- **Summary**: Several recent works have shown that image descriptors produced by deep convolutional neural networks provide state-of-the-art performance for image classification and retrieval problems. It has also been shown that the activations from the convolutional layers can be interpreted as local features describing particular image regions. These local features can be aggregated using aggregation approaches developed for local features (e.g. Fisher vectors), thus providing new powerful global descriptors.   In this paper we investigate possible ways to aggregate local deep features to produce compact global descriptors for image retrieval. First, we show that deep features and traditional hand-engineered features have quite different distributions of pairwise similarities, hence existing aggregation methods have to be carefully re-evaluated. Such re-evaluation reveals that in contrast to shallow features, the simple aggregation method based on sum pooling provides arguably the best performance for deep convolutional features. This method is efficient, has few parameters, and bears little risk of overfitting when e.g. learning the PCA matrix. Overall, the new compact global descriptor improves the state-of-the-art on four common benchmarks considerably.



### Generalized Regressive Motion: a Visual Cue to Collision
- **Arxiv ID**: http://arxiv.org/abs/1510.07573v1
- **DOI**: 10.1088/1748-3190/11/4/046008
- **Categories**: **cs.RO**, cs.CV, cs.MA, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1510.07573v1)
- **Published**: 2015-10-26 18:07:27+00:00
- **Updated**: 2015-10-26 18:07:27+00:00
- **Authors**: Krzysztof Chalupka, Michael Dickinson, Pietro Perona
- **Comment**: None
- **Journal**: None
- **Summary**: Brains and sensory systems evolved to guide motion. Central to this task is controlling the approach to stationary obstacles and detecting moving organisms. Looming has been proposed as the main monocular visual cue for detecting the approach of other animals and avoiding collisions with stationary obstacles. Elegant neural mechanisms for looming detection have been found in the brain of insects and vertebrates. However, looming has not been analyzed in the context of collisions between two moving animals. We propose an alternative strategy, Generalized Regressive Motion (GRM), which is consistent with recently observed behavior in fruit flies. Geometric analysis proves that GRM is a reliable cue to collision among conspecifics, whereas agent-based modeling suggests that GRM is a better cue than looming as a means to detect approach, prevent collisions and maintain mobility.



### Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1510.07712v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.07712v2)
- **Published**: 2015-10-26 22:47:00+00:00
- **Updated**: 2016-04-06 02:24:35+00:00
- **Authors**: Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, Wei Xu
- **Comment**: In CVPR2016
- **Journal**: None
- **Summary**: We present an approach that exploits hierarchical Recurrent Neural Networks (RNNs) to tackle the video captioning problem, i.e., generating one or multiple sentences to describe a realistic video. Our hierarchical framework contains a sentence generator and a paragraph generator. The sentence generator produces one simple short sentence that describes a specific short video interval. It exploits both temporal- and spatial-attention mechanisms to selectively focus on visual elements during generation. The paragraph generator captures the inter-sentence dependency by taking as input the sentential embedding produced by the sentence generator, combining it with the paragraph history, and outputting the new initial state for the sentence generator. We evaluate our approach on two large-scale benchmark datasets: YouTubeClips and TACoS-MultiLevel. The experiments demonstrate that our approach significantly outperforms the current state-of-the-art methods with BLEU@4 scores 0.499 and 0.305 respectively.



