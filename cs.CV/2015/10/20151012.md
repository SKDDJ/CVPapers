# Arxiv Papers in cs.CV on 2015-10-12
### Fast detection of multiple objects in traffic scenes with a common detection framework
- **Arxiv ID**: http://arxiv.org/abs/1510.03125v1
- **DOI**: 10.1109/TITS.2015.2496795
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.03125v1)
- **Published**: 2015-10-12 02:30:22+00:00
- **Updated**: 2015-10-12 02:30:22+00:00
- **Authors**: Qichang Hu, Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel, Fatih Porikli
- **Comment**: Appearing in IEEE Transactions on Intelligent Transportation Systems
- **Journal**: None
- **Summary**: Traffic scene perception (TSP) aims to real-time extract accurate on-road environment information, which in- volves three phases: detection of objects of interest, recognition of detected objects, and tracking of objects in motion. Since recognition and tracking often rely on the results from detection, the ability to detect objects of interest effectively plays a crucial role in TSP. In this paper, we focus on three important classes of objects: traffic signs, cars, and cyclists. We propose to detect all the three important objects in a single learning based detection framework. The proposed framework consists of a dense feature extractor and detectors of three important classes. Once the dense features have been extracted, these features are shared with all detectors. The advantage of using one common framework is that the detection speed is much faster, since all dense features need only to be evaluated once in the testing phase. In contrast, most previous works have designed specific detectors using different features for each of these objects. To enhance the feature robustness to noises and image deformations, we introduce spatially pooled features as a part of aggregated channel features. In order to further improve the generalization performance, we propose an object subcategorization method as a means of capturing intra-class variation of objects. We experimentally demonstrate the effectiveness and efficiency of the proposed framework in three detection applications: traffic sign detection, car detection, and cyclist detection. The proposed framework achieves the competitive performance with state-of- the-art approaches on several benchmark datasets.



### Interactive multiclass segmentation using superpixel classification
- **Arxiv ID**: http://arxiv.org/abs/1510.03199v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.03199v1)
- **Published**: 2015-10-12 09:36:53+00:00
- **Updated**: 2015-10-12 09:36:53+00:00
- **Authors**: Bérengère Mathieu, Alain Crouzil, Jean-Baptiste Puel
- **Comment**: None
- **Journal**: None
- **Summary**: This paper adresses the problem of interactive multiclass segmentation. We propose a fast and efficient new interactive segmentation method called Superpixel Classification-based Interactive Segmentation (SCIS). From a few strokes drawn by a human user over an image, this method extracts relevant semantic objects. To get a fast calculation and an accurate segmentation, SCIS uses superpixel over-segmentation and support vector machine classification. In this paper, we demonstrate that SCIS significantly outperfoms competing algorithms by evaluating its performances on the reference benchmarks of McGuinness and Santner.



### Using Anatomical Markers for Left Ventricular Segmentation of Long Axis Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/1510.03250v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.03250v1)
- **Published**: 2015-10-12 12:20:23+00:00
- **Updated**: 2015-10-12 12:20:23+00:00
- **Authors**: Yael Petrank, Nahum Smirin, Yossi Tsadok, Zvi Friedman, Peter Lysiansky, Dan Adam
- **Comment**: 11 pages, 17 figures
- **Journal**: None
- **Summary**: Left ventricular segmentation is essential for measuring left ventricular function indices. Segmentation of one or several images requires an initial guess of the contour. It is hypothesized here that creating an initial guess by first detecting anatomical markers, would lead to correct detection of the endocardium. The first step of the algorithm presented here includes automatic detection of the mitral valve. Next, the apex is detected in the same frame. The valve is then tracked throughout the cardiac cycle. Contours passing from the apex to each valve corner are then found using a dynamic programming algorithm. The resulting contour is used as an input to an active contour algorithm. The algorithm was tested on 21 long axis ultrasound clips and showed good agreement with manually traced contours. Thus, this study demonstrates that detection of anatomic markers leads to a reliable initial guess of the left ventricle border.



### Text-Attentional Convolutional Neural Networks for Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1510.03283v2
- **DOI**: 10.1109/TIP.2016.2547588
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.03283v2)
- **Published**: 2015-10-12 13:53:13+00:00
- **Updated**: 2016-03-24 23:25:52+00:00
- **Authors**: Tong He, Weilin Huang, Yu Qiao, Jian Yao
- **Comment**: To appear in IEEE Trans. on Image Processing, 2016
- **Journal**: None
- **Summary**: Recent deep learning models have demonstrated strong capabilities for classifying text and non-text components in natural images. They extract a high-level feature computed globally from a whole image component (patch), where the cluttered background information may dominate true text features in the deep representation. This leads to less discriminative power and poorer robustness. In this work, we present a new system for scene text detection by proposing a novel Text-Attentional Convolutional Neural Network (Text-CNN) that particularly focuses on extracting text-related regions and features from the image components. We develop a new learning mechanism to train the Text-CNN with multi-level and rich supervised information, including text region mask, character label, and binary text/nontext information. The rich supervision information enables the Text-CNN with a strong capability for discriminating ambiguous texts, and also increases its robustness against complicated background components. The training process is formulated as a multi-task learning problem, where low-level supervised information greatly facilitates main task of text/non-text classification. In addition, a powerful low-level detector called Contrast- Enhancement Maximally Stable Extremal Regions (CE-MSERs) is developed, which extends the widely-used MSERs by enhancing intensity contrast between text patterns and background. This allows it to detect highly challenging text patterns, resulting in a higher recall. Our approach achieved promising results on the ICDAR 2013 dataset, with a F-measure of 0.82, improving the state-of-the-art results substantially.



