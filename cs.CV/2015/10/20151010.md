# Arxiv Papers in cs.CV on 2015-10-10
### Wavelet Frame Based Image Restoration Using Sparsity, Nonlocal and Support Prior of Frame Coefficients
- **Arxiv ID**: http://arxiv.org/abs/1510.02866v1
- **DOI**: None
- **Categories**: **cs.CV**, 90-08, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/1510.02866v1)
- **Published**: 2015-10-10 02:52:48+00:00
- **Updated**: 2015-10-10 02:52:48+00:00
- **Authors**: Liangtian He, Yilun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The wavelet frame systems have been widely investigated and applied for image restoration and many other image processing problems over the past decades, attributing to their good capability of sparsely approximating piece-wise smooth functions such as images. Most wavelet frame based models exploit the $l_1$ norm of frame coefficients for a sparsity constraint in the past. The authors in \cite{ZhangY2013, Dong2013} proposed an $l_0$ minimization model, where the $l_0$ norm of wavelet frame coefficients is penalized instead, and have demonstrated that significant improvements can be achieved compared to the commonly used $l_1$ minimization model. Very recently, the authors in \cite{Chen2015} proposed $l_0$-$l_2$ minimization model, where the nonlocal prior of frame coefficients is incorporated. This model proved to outperform the single $l_0$ minimization based model in terms of better recovered image quality. In this paper, we propose a truncated $l_0$-$l_2$ minimization model which combines sparsity, nonlocal and support prior of the frame coefficients. The extensive experiments have shown that the recovery results from the proposed regularization method performs better than existing state-of-the-art wavelet frame based methods, in terms of edge enhancement and texture preserving performance.



### Learn to Evaluate Image Perceptual Quality Blindly from Statistics of Self-similarity
- **Arxiv ID**: http://arxiv.org/abs/1510.02884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.02884v1)
- **Published**: 2015-10-10 07:23:30+00:00
- **Updated**: 2015-10-10 07:23:30+00:00
- **Authors**: Wufeng Xue, Xuanqin Mou, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Among the various image quality assessment (IQA) tasks, blind IQA (BIQA) is particularly challenging due to the absence of knowledge about the reference image and distortion type. Features based on natural scene statistics (NSS) have been successfully used in BIQA, while the quality relevance of the feature plays an essential role to the quality prediction performance. Motivated by the fact that the early processing stage in human visual system aims to remove the signal redundancies for efficient visual coding, we propose a simple but very effective BIQA method by computing the statistics of self-similarity (SOS) in an image. Specifically, we calculate the inter-scale similarity and intra-scale similarity of the distorted image, extract the SOS features from these similarities, and learn a regression model to map the SOS features to the subjective quality score. Extensive experiments demonstrate very competitive quality prediction performance and generalization ability of the proposed SOS based BIQA method.



### TagBook: A Semantic Video Representation without Supervision for Event Detection
- **Arxiv ID**: http://arxiv.org/abs/1510.02899v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1510.02899v2)
- **Published**: 2015-10-10 09:28:56+00:00
- **Updated**: 2016-04-23 13:23:03+00:00
- **Authors**: Masoud Mazloom, Xirong Li, Cees G. M. Snoek
- **Comment**: accepted for publication as a regular paper in the IEEE Transactions
  on Multimedia
- **Journal**: None
- **Summary**: We consider the problem of event detection in video for scenarios where only few, or even zero examples are available for training. For this challenging setting, the prevailing solutions in the literature rely on a semantic video representation obtained from thousands of pre-trained concept detectors. Different from existing work, we propose a new semantic video representation that is based on freely available social tagged videos only, without the need for training any intermediate concept detectors. We introduce a simple algorithm that propagates tags from a video's nearest neighbors, similar in spirit to the ones used for image retrieval, but redesign it for video event detection by including video source set refinement and varying the video tag assignment. We call our approach TagBook and study its construction, descriptiveness and detection performance on the TRECVID 2013 and 2014 multimedia event detection datasets and the Columbia Consumer Video dataset. Despite its simple nature, the proposed TagBook video representation is remarkably effective for few-example and zero-example event detection, even outperforming very recent state-of-the-art alternatives building on supervised representations.



### Temporal Dynamic Appearance Modeling for Online Multi-Person Tracking
- **Arxiv ID**: http://arxiv.org/abs/1510.02906v1
- **DOI**: 10.1016/j.cviu.2016.05.003
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.02906v1)
- **Published**: 2015-10-10 10:13:04+00:00
- **Updated**: 2015-10-10 10:13:04+00:00
- **Authors**: Min Yang, Yunde Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Robust online multi-person tracking requires the correct associations of online detection responses with existing trajectories. We address this problem by developing a novel appearance modeling approach to provide accurate appearance affinities to guide data association. In contrast to most existing algorithms that only consider the spatial structure of human appearances, we exploit the temporal dynamic characteristics within temporal appearance sequences to discriminate different persons. The temporal dynamic makes a sufficient complement to the spatial structure of varying appearances in the feature space, which significantly improves the affinity measurement between trajectories and detections. We propose a feature selection algorithm to describe the appearance variations with mid-level semantic features, and demonstrate its usefulness in terms of temporal dynamic appearance modeling. Moreover, the appearance model is learned incrementally by alternatively evaluating newly-observed appearances and adjusting the model parameters to be suitable for online tracking. Reliable tracking of multiple persons in complex scenes is achieved by incorporating the learned model into an online tracking-by-detection framework. Our experiments on the challenging benchmark MOTChallenge 2015 demonstrate that our method outperforms the state-of-the-art multi-person tracking algorithms.



### On 1-Laplacian Elliptic Equations Modeling Magnetic Resonance Image Rician Denoising
- **Arxiv ID**: http://arxiv.org/abs/1510.02923v2
- **DOI**: 10.1007/s10851-016-0675-3
- **Categories**: **math.AP**, cs.CV, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1510.02923v2)
- **Published**: 2015-10-10 13:11:57+00:00
- **Updated**: 2016-07-12 09:19:28+00:00
- **Authors**: Adrian Martin, Emanuele Schiavi, Sergio Segura de Leon
- **Comment**: None
- **Journal**: None
- **Summary**: Modeling magnitude Magnetic Resonance Images (MRI) rician denoising in a Bayesian or generalized Tikhonov framework using Total Variation (TV) leads naturally to the consideration of nonlinear elliptic equations. These involve the so called $1$-Laplacian operator and special care is needed to properly formulate the problem. The rician statistics of the data are introduced through a singular equation with a reaction term defined in terms of modified first order Bessel functions. An existence theory is provided here together with other qualitative properties of the solutions. Remarkably, each positive global minimum of the associated functional is one of such solutions. Moreover, we directly solve this non--smooth non--convex minimization problem using a convergent Proximal Point Algorithm. Numerical results based on synthetic and real MRI demonstrate a better performance of the proposed method when compared to previous TV based models for rician denoising which regularize or convexify the problem. Finally, an application on real Diffusion Tensor Images, a strongly affected by rician noise MRI modality, is presented and discussed.



### DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations
- **Arxiv ID**: http://arxiv.org/abs/1510.02927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.02927v1)
- **Published**: 2015-10-10 13:36:31+00:00
- **Updated**: 2015-10-10 13:36:31+00:00
- **Authors**: Srinivas S. S. Kruthiventi, Kumar Ayush, R. Venkatesh Babu
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding and predicting the human visual attentional mechanism is an active area of research in the fields of neuroscience and computer vision. In this work, we propose DeepFix, a first-of-its-kind fully convolutional neural network for accurate saliency prediction. Unlike classical works which characterize the saliency map using various hand-crafted features, our model automatically learns features in a hierarchical fashion and predicts saliency map in an end-to-end manner. DeepFix is designed to capture semantics at multiple scales while taking global context into account using network layers with very large receptive fields. Generally, fully convolutional nets are spatially invariant which prevents them from modeling location dependent patterns (e.g. centre-bias). Our network overcomes this limitation by incorporating a novel Location Biased Convolutional layer. We evaluate our model on two challenging eye fixation datasets -- MIT300, CAT2000 and show that it outperforms other recent approaches by a significant margin.



### Fast and Accurate Poisson Denoising with Optimized Nonlinear Diffusion
- **Arxiv ID**: http://arxiv.org/abs/1510.02930v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.02930v1)
- **Published**: 2015-10-10 13:44:47+00:00
- **Updated**: 2015-10-10 13:44:47+00:00
- **Authors**: Wensen Feng, Yunjin Chen
- **Comment**: 11 pages, 12 figures, technical report
- **Journal**: None
- **Summary**: The degradation of the acquired signal by Poisson noise is a common problem for various imaging applications, such as medical imaging, night vision and microscopy. Up to now, many state-of-the-art Poisson denoising techniques mainly concentrate on achieving utmost performance, with little consideration for the computation efficiency. Therefore, in this study we aim to propose an efficient Poisson denoising model with both high computational efficiency and recovery quality. To this end, we exploit the newly-developed trainable nonlinear reaction diffusion model which has proven an extremely fast image restoration approach with performance surpassing recent state-of-the-arts. We retrain the model parameters, including the linear filters and influence functions by taking into account the Poisson noise statistics, and end up with an optimized nonlinear diffusion model specialized for Poisson denoising. The trained model provides strongly competitive results against state-of-the-art approaches, meanwhile bearing the properties of simple structure and high efficiency. Furthermore, our proposed model comes along with an additional advantage, that the diffusion process is well-suited for parallel computation on GPUs. For images of size $512 \times 512$, our GPU implementation takes less than 0.1 seconds to produce state-of-the-art Poisson denoising performance.



### Tract Orientation and Angular Dispersion Deviation Indicator (TOADDI): A framework for single-subject analysis in diffusion tensor imaging
- **Arxiv ID**: http://arxiv.org/abs/1510.02934v2
- **DOI**: 10.1016/j.neuroimage.2015.11.046
- **Categories**: **physics.med-ph**, cs.CV, stat.AP, stat.CO, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/1510.02934v2)
- **Published**: 2015-10-10 13:54:07+00:00
- **Updated**: 2015-11-20 01:35:14+00:00
- **Authors**: Cheng Guan Koay, Ping-Hong Yeh, John M. Ollinger, M. Okan İrfanoğlu, Carlo Pierpaoli, Peter J. Basser, Terrence R. Oakes, Gerard Riedy
- **Comment**: 49 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: The purpose of this work is to develop a framework for single-subject analysis of diffusion tensor imaging (DTI) data. This framework (termed TOADDI) is capable of testing whether an individual tract as represented by the major eigenvector of the diffusion tensor and its corresponding angular dispersion are significantly different from a group of tracts on a voxel-by-voxel basis. This work develops two complementary statistical tests based on the elliptical cone of uncertainty (COU), which is a model of uncertainty or dispersion of the major eigenvector of the diffusion tensor. The orientation deviation test examines whether the major eigenvector from a single subject is within the average elliptical COU formed by a collection of elliptical COUs. The shape deviation test is based on the two-tailed Wilcoxon-Mann-Whitney two-sample test between the normalized shape measures (area and circumference) of the elliptical cones of uncertainty of the single subject against a group of controls. The False Discovery Rate (FDR) and False Non-discovery Rate (FNR) were incorporated in the orientation deviation test. The shape deviation test uses FDR only. TOADDI was found to be numerically accurate and statistically effective. Clinical data from two Traumatic Brain Injury (TBI) patients and one non-TBI subject were tested against the data obtained from a group of 45 non-TBI controls to illustrate the application of the proposed framework in single-subject analysis. The frontal portion of the superior longitudinal fasciculus seemed to be implicated in both tests as significantly different from that of the control group. The TBI patients and the single non-TBI subject were well separated under the shape deviation test at the chosen FDR level of 0.0005. TOADDI is a simple but novel geometrically based statistical framework for analyzing DTI data.



### Evaluation of Joint Multi-Instance Multi-Label Learning For Breast Cancer Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1510.02942v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1510.02942v1)
- **Published**: 2015-10-10 14:30:25+00:00
- **Updated**: 2015-10-10 14:30:25+00:00
- **Authors**: Baris Gecer, Ozge Yalcinkaya, Onur Tasar, Selim Aksoy
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-instance multi-label (MIML) learning is a challenging problem in many aspects. Such learning approaches might be useful for many medical diagnosis applications including breast cancer detection and classification. In this study subset of digiPATH dataset (whole slide digital breast cancer histopathology images) are used for training and evaluation of six state-of-the-art MIML methods.   At the end, performance comparison of these approaches are given by means of effective evaluation metrics. It is shown that MIML-kNN achieve the best performance that is %65.3 average precision, where most of other methods attain acceptable results as well.



### Spatial Semantic Regularisation for Large Scale Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1510.02949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1510.02949v1)
- **Published**: 2015-10-10 15:15:45+00:00
- **Updated**: 2015-10-10 15:15:45+00:00
- **Authors**: Damian Mrowca, Marcus Rohrbach, Judy Hoffman, Ronghang Hu, Kate Saenko, Trevor Darrell
- **Comment**: accepted at ICCV 2015
- **Journal**: None
- **Summary**: Large scale object detection with thousands of classes introduces the problem of many contradicting false positive detections, which have to be suppressed. Class-independent non-maximum suppression has traditionally been used for this step, but it does not scale well as the number of classes grows. Traditional non-maximum suppression does not consider label- and instance-level relationships nor does it allow an exploitation of the spatial layout of detection proposals. We propose a new multi-class spatial semantic regularisation method based on affinity propagation clustering, which simultaneously optimises across all categories and all proposed locations in the image, to improve both the localisation and categorisation of selected detection proposals. Constraints are shared across the labels through the semantic WordNet hierarchy. Our approach proves to be especially useful in large scale settings with thousands of classes, where spatial and semantic interactions are very frequent and only weakly supervised detectors can be built due to a lack of bounding box annotations. Detection experiments are conducted on the ImageNet and COCO dataset, and in settings with thousands of detected categories. Our method provides a significant precision improvement by reducing false positives, while simultaneously improving the recall.



### Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?
- **Arxiv ID**: http://arxiv.org/abs/1510.02969v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1510.02969v3)
- **Published**: 2015-10-10 18:53:21+00:00
- **Updated**: 2017-03-16 03:07:21+00:00
- **Authors**: Pooya Khorrami, Tom Le Paine, Thomas S. Huang
- **Comment**: Accepted at ICCV 2015 CV4AC Workshop. Corrected numbers in Tables 2
  and 3
- **Journal**: None
- **Summary**: Despite being the appearance-based classifier of choice in recent years, relatively few works have examined how much convolutional neural networks (CNNs) can improve performance on accepted expression recognition benchmarks and, more importantly, examine what it is they actually learn. In this work, not only do we show that CNNs can achieve strong performance, but we also introduce an approach to decipher which portions of the face influence the CNN's predictions. First, we train a zero-bias CNN on facial expression data and achieve, to our knowledge, state-of-the-art performance on two expression recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD). We then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble Facial Action Units (FAUs). Finally, we use the FAU labels provided in the CK+ dataset to verify that the FAUs observed in our filter visualizations indeed align with the subject's facial movements.



### Optimal Piecewise Linear Function Approximation for GPU-based Applications
- **Arxiv ID**: http://arxiv.org/abs/1510.02975v1
- **DOI**: 10.1109/TCYB.2015.2482365
- **Categories**: **math.OC**, cs.CV, cs.DC, cs.NA, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1510.02975v1)
- **Published**: 2015-10-10 20:49:17+00:00
- **Updated**: 2015-10-10 20:49:17+00:00
- **Authors**: Daniel Berjón, Guillermo Gallego, Carlos Cuevas, Francisco Morán, Narciso García
- **Comment**: 12 pages, 12 figures, post-print, IEEE Transactions on Cybernetics,
  Oct. 2015
- **Journal**: IEEE Transactions on Cybernetics, vol. 46, no. 11, pp. 2584-2595,
  Nov. 2016
- **Summary**: Many computer vision and human-computer interaction applications developed in recent years need evaluating complex and continuous mathematical functions as an essential step toward proper operation. However, rigorous evaluation of this kind of functions often implies a very high computational cost, unacceptable in real-time applications. To alleviate this problem, functions are commonly approximated by simpler piecewise-polynomial representations. Following this idea, we propose a novel, efficient, and practical technique to evaluate complex and continuous functions using a nearly optimal design of two types of piecewise linear approximations in the case of a large budget of evaluation subintervals. To this end, we develop a thorough error analysis that yields asymptotically tight bounds to accurately quantify the approximation performance of both representations. It provides an improvement upon previous error estimates and allows the user to control the trade-off between the approximation error and the number of evaluation subintervals. To guarantee real-time operation, the method is suitable for, but not limited to, an efficient implementation in modern Graphics Processing Units (GPUs), where it outperforms previous alternative approaches by exploiting the fixed-function interpolation routines present in their texture units. The proposed technique is a perfect match for any application requiring the evaluation of continuous functions, we have measured in detail its quality and efficiency on several functions, and, in particular, the Gaussian function because it is extensively used in many areas of computer vision and cybernetics, and it is expensive to evaluate.



