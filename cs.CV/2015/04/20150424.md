# Arxiv Papers in cs.CV on 2015-04-24
### Holistically-Nested Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/1504.06375v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06375v2)
- **Published**: 2015-04-24 02:12:15+00:00
- **Updated**: 2015-10-04 02:15:38+00:00
- **Authors**: Saining Xie, Zhuowen Tu
- **Comment**: v2 Add appendix A for updated results (ODS=0.790) on BSDS-500 in a
  new experiment setting. Fix typos and reorganize formulations. Add Table 2 to
  discuss the role of deep supervision. Add links to publicly available
  repository for code, models and data
- **Journal**: None
- **Summary**: We develop a new edge detection algorithm that tackles two important issues in this long-standing vision problem: (1) holistic image training and prediction; and (2) multi-scale and multi-level feature learning. Our proposed method, holistically-nested edge detection (HED), performs image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets. HED automatically learns rich hierarchical representations (guided by deep supervision on side responses) that are important in order to approach the human ability resolve the challenging ambiguity in edge and object boundary detection. We significantly advance the state-of-the-art on the BSD500 dataset (ODS F-score of .782) and the NYU Depth dataset (ODS F-score of .746), and do so with an improved speed (0.4 second per image) that is orders of magnitude faster than some recent CNN-based edge detection algorithms.



### Depth-based hand pose estimation: methods, data, and challenges
- **Arxiv ID**: http://arxiv.org/abs/1504.06378v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06378v2)
- **Published**: 2015-04-24 02:37:37+00:00
- **Updated**: 2015-05-06 20:31:57+00:00
- **Authors**: James Steven Supancic III, Gregory Rogez, Yi Yang, Jamie Shotton, Deva Ramanan
- **Comment**: None
- **Journal**: None
- **Summary**: Hand pose estimation has matured rapidly in recent years. The introduction of commodity depth sensors and a multitude of practical applications have spurred new advances. We provide an extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame. To do so, we have implemented a considerable number of systems, and will release all software and evaluation code. We summarize important conclusions here: (1) Pose estimation appears roughly solved for scenes with isolated hands. However, methods still struggle to analyze cluttered scenes where hands may be interacting with nearby objects and surfaces. To spur further progress we introduce a challenging new dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves with disparate criteria, making comparisons difficult. We define a consistent evaluation criteria, rigorously motivated by human experiments. (3) We introduce a simple nearest-neighbor baseline that outperforms most existing systems. This implies that most systems do not generalize beyond their training sets. This also reinforces the under-appreciated point that training data is as important as the model itself. We conclude with directions for future progress.



### Situational Object Boundary Detection
- **Arxiv ID**: http://arxiv.org/abs/1504.06434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06434v1)
- **Published**: 2015-04-24 09:15:33+00:00
- **Updated**: 2015-04-24 09:15:33+00:00
- **Authors**: Jasper Uijlings, Vittorio Ferrari
- **Comment**: None
- **Journal**: None
- **Summary**: Intuitively, the appearance of true object boundaries varies from image to image. Hence the usual monolithic approach of training a single boundary predictor and applying it to all images regardless of their content is bound to be suboptimal. In this paper we therefore propose situational object boundary detection: We first define a variety of situations and train a specialized object boundary detector for each of them using [Dollar and Zitnick 2013]. Then given a test image, we classify it into these situations using its context, which we model by global image appearance. We apply the corresponding situational object boundary detectors, and fuse them based on the classification probabilities. In experiments on ImageNet, Microsoft COCO, and Pascal VOC 2012 segmentation we show that our situational object boundary detection gives significant improvements over a monolithic approach. Additionally, our method substantially outperforms [Hariharan et al. 2011] on semantic contour detection on their SBD dataset.



### Local Variation as a Statistical Hypothesis Test
- **Arxiv ID**: http://arxiv.org/abs/1504.06507v1
- **DOI**: 10.1007/s11263-015-0855-4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06507v1)
- **Published**: 2015-04-24 13:45:37+00:00
- **Updated**: 2015-04-24 13:45:37+00:00
- **Authors**: Michael Baltaxe, Peter Meer, Michael Lindenbaum
- **Comment**: None
- **Journal**: Journal of Computer Vision 117 (2016)
- **Summary**: The goal of image oversegmentation is to divide an image into several pieces, each of which should ideally be part of an object. One of the simplest and yet most effective oversegmentation algorithms is known as local variation (LV) (Felzenszwalb and Huttenlocher 2004). In this work, we study this algorithm and show that algorithms similar to LV can be devised by applying different statistical models and decisions, thus providing further theoretical justification and a well-founded explanation for the unexpected high performance of the LV approach. Some of these algorithms are based on statistics of natural images and on a hypothesis testing decision; we denote these algorithms probabilistic local variation (pLV). The best pLV algorithm, which relies on censored estimation, presents state-of-the-art results while keeping the same computational complexity of the LV algorithm.



### Cultural Event Recognition with Visual ConvNets and Temporal Models
- **Arxiv ID**: http://arxiv.org/abs/1504.06567v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1504.06567v1)
- **Published**: 2015-04-24 17:00:44+00:00
- **Updated**: 2015-04-24 17:00:44+00:00
- **Authors**: Amaia Salvador, Matthias Zeppelzauer, Daniel Manchon-Vizuete, Andrea Calafell, Xavier Giro-i-Nieto
- **Comment**: Initial version of the paper accepted at the CVPR Workshop ChaLearn
  Looking at People 2015
- **Journal**: None
- **Summary**: This paper presents our contribution to the ChaLearn Challenge 2015 on Cultural Event Classification. The challenge in this task is to automatically classify images from 50 different cultural events. Our solution is based on the combination of visual features extracted from convolutional neural networks with temporal information using a hierarchical classifier scheme. We extract visual features from the last three fully connected layers of both CaffeNet (pretrained with ImageNet) and our fine tuned version for the ChaLearn challenge. We propose a late fusion strategy that trains a separate low-level SVM on each of the extracted neural codes. The class predictions of the low-level SVMs form the input to a higher level SVM, which gives the final event scores. We achieve our best result by adding a temporal refinement step into our classification scheme, which is applied directly to the output of each low-level SVM. Our approach penalizes high classification scores based on visual features when their time stamp does not match well an event-specific temporal distribution learned from the training and validation data. Our system achieved the second best result in the ChaLearn Challenge 2015 on Cultural Event Classification with a mean average precision of 0.767 on the test set.



### Semantic Motion Segmentation Using Dense CRF Formulation
- **Arxiv ID**: http://arxiv.org/abs/1504.06587v1
- **DOI**: 10.1145/2683483.2683539
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06587v1)
- **Published**: 2015-04-24 18:06:50+00:00
- **Updated**: 2015-04-24 18:06:50+00:00
- **Authors**: N. Dinesh Reddy, Prateek Singhal, K. Madhava Krishna
- **Comment**: None
- **Journal**: None
- **Summary**: While the literature has been fairly dense in the areas of scene understanding and semantic labeling there have been few works that make use of motion cues to embellish semantic performance and vice versa. In this paper, we address the problem of semantic motion segmentation, and show how semantic and motion priors augments performance. We pro- pose an algorithm that jointly infers the semantic class and motion labels of an object. Integrating semantic, geometric and optical ow based constraints into a dense CRF-model we infer both the object class as well as motion class, for each pixel. We found improvement in performance using a fully connected CRF as compared to a standard clique-based CRFs. For inference, we use a Mean Field approximation based algorithm. Our method outperforms recently pro- posed motion detection algorithms and also improves the semantic labeling compared to the state-of-the-art Automatic Labeling Environment algorithm on the challenging KITTI dataset especially for object classes such as pedestrians and cars that are critical to an outdoor robotic navigation scenario.



### Object Level Deep Feature Pooling for Compact Image Representation
- **Arxiv ID**: http://arxiv.org/abs/1504.06591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06591v1)
- **Published**: 2015-04-24 18:27:25+00:00
- **Updated**: 2015-04-24 18:27:25+00:00
- **Authors**: Konda Reddy Mopuri, R. Venkatesh Babu
- **Comment**: Deep Vision 2015
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) features have been successfully employed in recent works as an image descriptor for various vision tasks. But the inability of the deep CNN features to exhibit invariance to geometric transformations and object compositions poses a great challenge for image search. In this work, we demonstrate the effectiveness of the objectness prior over the deep CNN features of image regions for obtaining an invariant image representation. The proposed approach represents the image as a vector of pooled CNN features describing the underlying objects. This representation provides robustness to spatial layout of the objects in the scene and achieves invariance to general geometric transformations, such as translation, rotation and scaling. The proposed approach also leads to a compact representation of the scene, making each image occupy a smaller memory footprint. Experiments show that the proposed representation achieves state of the art retrieval results on a set of challenging benchmark image datasets, while maintaining a compact representation.



### WxBS: Wide Baseline Stereo Generalizations
- **Arxiv ID**: http://arxiv.org/abs/1504.06603v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06603v2)
- **Published**: 2015-04-24 19:19:04+00:00
- **Updated**: 2015-05-12 14:42:53+00:00
- **Authors**: Dmytro Mishkin, Jiri Matas, Michal Perdoch, Karel Lenc
- **Comment**: Descriptor and detector evaluation expanded
- **Journal**: None
- **Summary**: We have presented a new problem -- the wide multiple baseline stereo (WxBS) -- which considers matching of images that simultaneously differ in more than one image acquisition factor such as viewpoint, illumination, sensor type or where object appearance changes significantly, e.g. over time. A new dataset with the ground truth for evaluation of matching algorithms has been introduced and will be made public.   We have extensively tested a large set of popular and recent detectors and descriptors and show than the combination of RootSIFT and HalfRootSIFT as descriptors with MSER and Hessian-Affine detectors works best for many different nuisance factors. We show that simple adaptive thresholding improves Hessian-Affine, DoG, MSER (and possibly other) detectors and allows to use them on infrared and low contrast images.   A novel matching algorithm for addressing the WxBS problem has been introduced. We have shown experimentally that the WxBS-M matcher dominantes the state-of-the-art methods both on both the new and existing datasets.



