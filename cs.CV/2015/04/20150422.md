# Arxiv Papers in cs.CV on 2015-04-22
### Median and Mode Ellipse Parameterization for Robust Contour Fitting
- **Arxiv ID**: http://arxiv.org/abs/1504.05623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.05623v1)
- **Published**: 2015-04-22 00:12:07+00:00
- **Updated**: 2015-04-22 00:12:07+00:00
- **Authors**: Michael A. Greminger
- **Comment**: None
- **Journal**: None
- **Summary**: Problems that require the parameterization of closed contours arise frequently in computer vision applications. This article introduces a new curve parameterization algorithm that is able to fit a closed curve to a set of points while being robust to the presence of outliers and occlusions in the data. This robustness property makes this algorithm applicable to computer vision applications where misclassification of features may lead to outliers. The algorithm starts by fitting ellipses to numerous five point subsets from the source data. The closed curve is parameterized by determining the median perimeter of the set of ellipses. The resulting curve is not an ellipse, allowing arbitrary closed contours to be parameterized. The use of the modal perimeter rather than the median perimeter is also explored. A detailed comparison is made between the proposed curve fitting algorithm and existing robust ellipse fitting algorithms. Finally, the utility of the algorithm for computer vision applications is demonstrated through the parameterization of the boundary of fuel droplets during combustion. The performance of the proposed algorithm and the performance of existing algorithms are compared to a ground truth segmentation of the fuel droplet images, which demonstrates improved performance for both area quantification and edge deviation.



### Self-Tuned Deep Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1504.05632v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1504.05632v1)
- **Published**: 2015-04-22 02:01:36+00:00
- **Updated**: 2015-04-22 02:01:36+00:00
- **Authors**: Zhangyang Wang, Yingzhen Yang, Zhaowen Wang, Shiyu Chang, Wei Han, Jianchao Yang, Thomas S. Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been successfully applied to image super resolution (SR). In this paper, we propose a deep joint super resolution (DJSR) model to exploit both external and self similarities for SR. A Stacked Denoising Convolutional Auto Encoder (SDCAE) is first pre-trained on external examples with proper data augmentations. It is then fine-tuned with multi-scale self examples from each input, where the reliability of self examples is explicitly taken into account. We also enhance the model performance by sub-model training and selection. The DJSR model is extensively evaluated and compared with state-of-the-arts, and show noticeable performance improvements both quantitatively and perceptually on a wide range of images.



### Combining local regularity estimation and total variation optimization for scale-free texture segmentation
- **Arxiv ID**: http://arxiv.org/abs/1504.05776v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.05776v3)
- **Published**: 2015-04-22 13:01:12+00:00
- **Updated**: 2016-06-24 08:22:00+00:00
- **Authors**: Nelly Pustelnik, Herwig Wendt, Patrice Abry, Nicolas Dobigeon
- **Comment**: None
- **Journal**: None
- **Summary**: Texture segmentation constitutes a standard image processing task, crucial to many applications. The present contribution focuses on the particular subset of scale-free textures and its originality resides in the combination of three key ingredients: First, texture characterization relies on the concept of local regularity ; Second, estimation of local regularity is based on new multiscale quantities referred to as wavelet leaders ; Third, segmentation from local regularity faces a fundamental bias variance trade-off: In nature, local regularity estimation shows high variability that impairs the detection of changes, while a posteriori smoothing of regularity estimates precludes from locating correctly changes. Instead, the present contribution proposes several variational problem formulations based on total variation and proximal resolutions that effectively circumvent this trade-off. Estimation and segmentation performance for the proposed procedures are quantified and compared on synthetic as well as on real-world textures.



### LOAD: Local Orientation Adaptive Descriptor for Texture and Material Classification
- **Arxiv ID**: http://arxiv.org/abs/1504.05809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.05809v1)
- **Published**: 2015-04-22 13:59:49+00:00
- **Updated**: 2015-04-22 13:59:49+00:00
- **Authors**: Xianbiao Qi, Guoying Zhao, Linlin Shen, Qingquan Li, Matti Pietikainen
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: In this paper, we propose a novel local feature, called Local Orientation Adaptive Descriptor (LOAD), to capture regional texture in an image. In LOAD, we proposed to define point description on an Adaptive Coordinate System (ACS), adopt a binary sequence descriptor to capture relationships between one point and its neighbors and use multi-scale strategy to enhance the discriminative power of the descriptor. The proposed LOAD enjoys not only discriminative power to capture the texture information, but also has strong robustness to illumination variation and image rotation. Extensive experiments on benchmark data sets of texture classification and real-world material recognition show that the proposed LOAD yields the state-of-the-art performance. It is worth to mention that we achieve a 65.4\% classification accuracy-- which is, to the best of our knowledge, the highest record by far --on Flickr Material Database by using a single feature. Moreover, by combining LOAD with the feature extracted by Convolutional Neural Networks (CNN), we obtain significantly better performance than both the LOAD and CNN. This result confirms that the LOAD is complementary to the learning-based features.



### Exploit Bounding Box Annotations for Multi-label Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1504.05843v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1504.05843v2)
- **Published**: 2015-04-22 15:01:29+00:00
- **Updated**: 2016-06-03 09:44:35+00:00
- **Authors**: Hao Yang, Joey Tianyi Zhou, Yu Zhang, Bin-Bin Gao, Jianxin Wu, Jianfei Cai
- **Comment**: Accepted in CVPR 2016
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have shown great performance as general feature representations for object recognition applications. However, for multi-label images that contain multiple objects from different categories, scales and locations, global CNN features are not optimal. In this paper, we incorporate local information to enhance the feature discriminative power. In particular, we first extract object proposals from each image. With each image treated as a bag and object proposals extracted from it treated as instances, we transform the multi-label recognition problem into a multi-class multi-instance learning problem. Then, in addition to extracting the typical CNN feature representation from each proposal, we propose to make use of ground-truth bounding box annotations (strong labels) to add another level of local information by using nearest-neighbor relationships of local regions to form a multi-view pipeline. The proposed multi-view multi-instance framework utilizes both weak and strong labels effectively, and more importantly it has the generalization ability to even boost the performance of unseen categories by partial strong labels from other categories. Our framework is extensively compared with state-of-the-art hand-crafted feature based methods and CNN based methods on two multi-label benchmark datasets. The experimental results validate the discriminative power and the generalization ability of the proposed framework. With strong labels, our framework is able to achieve state-of-the-art results in both datasets.



