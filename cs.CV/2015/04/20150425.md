# Arxiv Papers in cs.CV on 2015-04-25
### Differential Recurrent Neural Networks for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1504.06678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06678v1)
- **Published**: 2015-04-25 03:59:14+00:00
- **Updated**: 2015-04-25 03:59:14+00:00
- **Authors**: Vivek Veeriah, Naifan Zhuang, Guo-Jun Qi
- **Comment**: None
- **Journal**: None
- **Summary**: The long short-term memory (LSTM) neural network is capable of processing complex sequential information since it utilizes special gating schemes for learning representations from long input sequences. It has the potential to model any sequential time-series data, where the current hidden state has to be considered in the context of the past hidden states. This property makes LSTM an ideal choice to learn the complex dynamics of various actions. Unfortunately, the conventional LSTMs do not consider the impact of spatio-temporal dynamics corresponding to the given salient motion patterns, when they gate the information that ought to be memorized through time. To address this problem, we propose a differential gating scheme for the LSTM neural network, which emphasizes on the change in information gain caused by the salient motions between the successive frames. This change in information gain is quantified by Derivative of States (DoS), and thus the proposed LSTM model is termed as differential Recurrent Neural Network (dRNN). We demonstrate the effectiveness of the proposed model by automatically recognizing actions from the real-world 2D and 3D human action datasets. Our study is one of the first works towards demonstrating the potential of learning complex time-series representations via high-order derivatives of states.



### Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images
- **Arxiv ID**: http://arxiv.org/abs/1504.06692v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, I.2.6; I.2.7; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1504.06692v2)
- **Published**: 2015-04-25 06:45:35+00:00
- **Updated**: 2015-10-02 02:36:05+00:00
- **Authors**: Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille
- **Comment**: ICCV 2015 camera ready version. We add much more novel visual
  concepts in the NVC dataset and have released it, see
  http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html
- **Journal**: None
- **Summary**: In this paper, we address the task of learning novel visual concepts, and their interactions with other concepts, from a few images with sentence descriptions. Using linguistic context and visual features, our method is able to efficiently hypothesize the semantic meaning of new words and add them to its word dictionary so that they can be used to describe images which contain these novel concepts. Our method has an image captioning module based on m-RNN with several improvements. In particular, we propose a transposed weight sharing scheme, which not only improves performance on image captioning, but also makes the model more suitable for the novel concept learning task. We propose methods to prevent overfitting the new concepts. In addition, three novel concept datasets are constructed for this new task. In the experiments, we show that our method effectively learns novel visual concepts from a few examples without disturbing the previously learned concepts. The project page is http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html



### Adaptive Locally Affine-Invariant Shape Matching
- **Arxiv ID**: http://arxiv.org/abs/1504.06719v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06719v1)
- **Published**: 2015-04-25 12:01:30+00:00
- **Updated**: 2015-04-25 12:01:30+00:00
- **Authors**: Smit Marvaniya, Raj Gupta, Anurag Mittal
- **Comment**: submitted to Image and Vision Computing
- **Journal**: None
- **Summary**: Matching deformable objects using their shapes is an important problem in computer vision since shape is perhaps the most distinguishable characteristic of an object. The problem is difficult due to many factors such as intra-class variations, local deformations, articulations, viewpoint changes and missed and extraneous contour portions due to errors in shape extraction. While small local deformations has been handled in the literature by allowing some leeway in the matching of individual contour points via methods such as Chamfer distance and Hausdorff distance, handling more severe deformations and articulations has been done by applying local geometric corrections such as similarity or affine. However, determining which portions of the shape should be used for the geometric corrections is very hard, although some methods have been tried. In this paper, we address this problem by an efficient search for the group of contour segments to be clustered together for a geometric correction using Dynamic Programming by essentially searching for the segmentations of two shapes that lead to the best matching between them. At the same time, we allow portions of the contours to remain unmatched to handle missing and extraneous contour portions. Experiments indicate that our method outperforms other algorithms, especially when the shapes to be matched are more complex.



### SIFT Vs SURF: Quantifying the Variation in Transformations
- **Arxiv ID**: http://arxiv.org/abs/1504.06740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06740v1)
- **Published**: 2015-04-25 15:26:38+00:00
- **Updated**: 2015-04-25 15:26:38+00:00
- **Authors**: Siddharth Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies the robustness of SIFT and SURF against different image transforms (rigid body, similarity, affine and projective) by quantitatively analyzing the variations in the extent of transformations. Previous studies have been comparing the two techniques on absolute transformations rather than the specific amount of deformation caused by the transformation. The paper establishes an exhaustive empirical analysis of such deformations and matching capability of SIFT and SURF with variations in matching parameters and the amount of tolerance. This is helpful in choosing the specific use case for applying these techniques.



### TurkerGaze: Crowdsourcing Saliency with Webcam based Eye Tracking
- **Arxiv ID**: http://arxiv.org/abs/1504.06755v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06755v2)
- **Published**: 2015-04-25 19:26:47+00:00
- **Updated**: 2015-05-20 18:51:23+00:00
- **Authors**: Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R. Kulkarni, Jianxiong Xiao
- **Comment**: 9 pages, 14 figures
- **Journal**: None
- **Summary**: Traditional eye tracking requires specialized hardware, which means collecting gaze data from many observers is expensive, tedious and slow. Therefore, existing saliency prediction datasets are order-of-magnitudes smaller than typical datasets for other vision recognition tasks. The small size of these datasets limits the potential for training data intensive algorithms, and causes overfitting in benchmark evaluation. To address this deficiency, this paper introduces a webcam-based gaze tracking system that supports large-scale, crowdsourced eye tracking deployed on Amazon Mechanical Turk (AMTurk). By a combination of careful algorithm and gaming protocol design, our system obtains eye tracking data for saliency prediction comparable to data gathered in a traditional lab setting, with relatively lower cost and less effort on the part of the researchers. Using this tool, we build a saliency dataset for a large number of natural images. We will open-source our tool and provide a web server where researchers can upload their images to get eye tracking results from AMTurk.



