# Arxiv Papers in cs.CV on 2015-04-29
### Projected Iterative Soft-thresholding Algorithm for Tight Frames in Compressed Sensing Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/1504.07786v2
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1504.07786v2)
- **Published**: 2015-04-29 09:35:31+00:00
- **Updated**: 2015-10-03 14:21:59+00:00
- **Authors**: Yunsong Liu, Zhifang Zhan, Jian-Feng Cai, Di Guo, Zhong Chen, Xiaobo Qu
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: Compressed sensing has shown great potentials in accelerating magnetic resonance imaging. Fast image reconstruction and high image quality are two main issues faced by this new technology. It has been shown that, redundant image representations, e.g. tight frames, can significantly improve the image quality. But how to efficiently solve the reconstruction problem with these redundant representation systems is still challenging. This paper attempts to address the problem of applying iterative soft-thresholding algorithm (ISTA) to tight frames based magnetic resonance image reconstruction. By introducing the canonical dual frame to construct the orthogonal projection operator on the range of the analysis sparsity operator, we propose a projected iterative soft-thresholding algorithm (pISTA) and further accelerate it by incorporating the strategy proposed by Beck and Teboulle in 2009. We theoretically prove that pISTA converges to the minimum of a function with a balanced tight frame sparsity. Experimental results demonstrate that the proposed algorithm achieves better reconstruction than the widely used synthesis sparse model and the accelerated pISTA converges faster or comparable to the state-of-art smoothing FISTA. One major advantage of pISTA is that only one extra parameter, the step size, is introduced and the numerical solution is stable to it in terms of image reconstruction errors, thus allowing easily setting in many fast magnetic resonance imaging applications.



### Probabilistic Depth Image Registration incorporating Nonvisual Information
- **Arxiv ID**: http://arxiv.org/abs/1504.07857v2
- **DOI**: 10.1109/ICRA.2012.6225179
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1504.07857v2)
- **Published**: 2015-04-29 13:45:20+00:00
- **Updated**: 2015-05-01 19:02:13+00:00
- **Authors**: Manuel Wüthrich, Peter Pastor, Ludovic Righetti, Aude Billard, Stefan Schaal
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we derive a probabilistic registration algorithm for object modeling and tracking. In many robotics applications, such as manipulation tasks, nonvisual information about the movement of the object is available, which we will combine with the visual information. Furthermore we do not only consider observations of the object, but we also take space into account which has been observed to not be part of the object. Furthermore we are computing a posterior distribution over the relative alignment and not a point estimate as typically done in for example Iterative Closest Point (ICP). To our knowledge no existing algorithm meets these three conditions and we thus derive a novel registration algorithm in a Bayesian framework. Experimental results suggest that the proposed methods perform favorably in comparison to PCL implementations of feature mapping and ICP, especially if nonvisual information is available.



### Intelligent Health Recommendation System for Computer Users
- **Arxiv ID**: http://arxiv.org/abs/1504.07858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.07858v1)
- **Published**: 2015-04-29 13:51:04+00:00
- **Updated**: 2015-04-29 13:51:04+00:00
- **Authors**: Qi Guo, Zixuan Wang, Ming Li, Hamid Aghajan
- **Comment**: None
- **Journal**: None
- **Summary**: The time people spend in front of computers has been increasing steadily due to the role computers play in modern society. Individuals who sit in front of computers for an extended period of time, specifically with improper postures may incur various health issues. In this work, individuals' behaviors in front of computers are studied using web cameras. By means of non-rigid face tracking system, data are analyzed to determine the 3D head pose, blink rate and yawn frequency of computer users. When combining these visual cues, a system of intelligent personal assistants for computer users is proposed.



### Visual Information Retrieval in Endoscopic Video Archives
- **Arxiv ID**: http://arxiv.org/abs/1504.07874v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1504.07874v1)
- **Published**: 2015-04-29 14:35:35+00:00
- **Updated**: 2015-04-29 14:35:35+00:00
- **Authors**: Jennifer Roldan-Carlos, Mathias Lux, Xavier Giró-i-Nieto, Pia Muñoz, Nektarios Anagnostopoulos
- **Comment**: Paper accepted at the IEEE/ACM 13th International Workshop on
  Content-Based Multimedia Indexing (CBMI) in Prague (Czech Republic) between
  10 and 12 June 2015
- **Journal**: None
- **Summary**: In endoscopic procedures, surgeons work with live video streams from the inside of their subjects. A main source for documentation of procedures are still frames from the video, identified and taken during the surgery. However, with growing demands and technical means, the streams are saved to storage servers and the surgeons need to retrieve parts of the videos on demand. In this submission we present a demo application allowing for video retrieval based on visual features and late fusion, which allows surgeons to re-find shots taken during the procedure.



### Bilinear CNNs for Fine-grained Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1504.07889v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.07889v6)
- **Published**: 2015-04-29 15:23:58+00:00
- **Updated**: 2017-06-01 04:24:01+00:00
- **Authors**: Tsung-Yu Lin, Aruni RoyChowdhury, Subhransu Maji
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple and effective architecture for fine-grained visual recognition called Bilinear Convolutional Neural Networks (B-CNNs). These networks represent an image as a pooled outer product of features derived from two CNNs and capture localized feature interactions in a translationally invariant manner. B-CNNs belong to the class of orderless texture representations but unlike prior work they can be trained in an end-to-end manner. Our most accurate model obtains 84.1%, 79.4%, 86.9% and 91.3% per-image accuracy on the Caltech-UCSD birds [67], NABirds [64], FGVC aircraft [42], and Stanford cars [33] dataset respectively and runs at 30 frames-per-second on a NVIDIA Titan X GPU. We then present a systematic analysis of these networks and show that (1) the bilinear features are highly redundant and can be reduced by an order of magnitude in size without significant loss in accuracy, (2) are also effective for other image classification tasks such as texture and scene recognition, and (3) can be trained from scratch on the ImageNet dataset offering consistent improvements over the baseline architecture. Finally, we present visualizations of these models on various datasets using top activations of neural units and gradient-based inversion techniques. The source code for the complete system is available at http://vis-www.cs.umass.edu/bcnn.



### Comparative study of image registration techniques for bladder video-endoscopy
- **Arxiv ID**: http://arxiv.org/abs/1504.07901v1
- **DOI**: 10.1117/12.831772
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.07901v1)
- **Published**: 2015-04-29 15:48:22+00:00
- **Updated**: 2015-04-29 15:48:22+00:00
- **Authors**: Achraf Ben-Hamadou, Charles Soussen, Walter Blondel, Christian Daul, Didier Wolf
- **Comment**: 7 pages, 5 figures
- **Journal**: Novel Optical Instrumentation for Biomedical Applications, 737118
  (10 July 2009)
- **Summary**: Bladder cancer is widely spread in the world. Many adequate diagnosis techniques exist. Video-endoscopy remains the standard clinical procedure for visual exploration of the bladder internal surface. However, video-endoscopy presents the limit that the imaged area for each image is about nearly 1cm2. And, lesions are, typically, spread over several images. The aim of this contribution is to assess the performance of two mosaicing algorithms leading to the construction of panoramic maps (one unique image) of bladder walls. The quantitative comparison study is performed on a set of real endoscopic exam data and on simulated data relative to bladder phantom.



### A Flexible Tensor Block Coordinate Ascent Scheme for Hypergraph Matching
- **Arxiv ID**: http://arxiv.org/abs/1504.07907v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.07907v2)
- **Published**: 2015-04-29 16:02:56+00:00
- **Updated**: 2015-04-30 19:01:11+00:00
- **Authors**: Quynh Nguyen, Antoine Gautier, Matthias Hein
- **Comment**: CVPR 2015 (Long version - All proofs included)
- **Journal**: None
- **Summary**: The estimation of correspondences between two images resp. point sets is a core problem in computer vision. One way to formulate the problem is graph matching leading to the quadratic assignment problem which is NP-hard. Several so called second order methods have been proposed to solve this problem. In recent years hypergraph matching leading to a third order problem became popular as it allows for better integration of geometric information. For most of these third order algorithms no theoretical guarantees are known. In this paper we propose a general framework for tensor block coordinate ascent methods for hypergraph matching. We propose two algorithms which both come along with the guarantee of monotonic ascent in the matching score on the set of discrete assignment matrices. In the experiments we show that our new algorithms outperform previous work both in terms of achieving better matching scores and matching accuracy. This holds in particular for very challenging settings where one has a high number of outliers and other forms of noise.



### Robust hyperspectral image classification with rejection fields
- **Arxiv ID**: http://arxiv.org/abs/1504.07918v1
- **DOI**: None
- **Categories**: **cs.CV**, 68
- **Links**: [PDF](http://arxiv.org/pdf/1504.07918v1)
- **Published**: 2015-04-29 16:30:45+00:00
- **Updated**: 2015-04-29 16:30:45+00:00
- **Authors**: Filipe Condessa, Jose Bioucas-Dias, Jelena Kovacevic
- **Comment**: This paper was submitted to IEEE WHISPERS 2015: 7th Workshop on
  Hyperspectral Image and Signal Processing: Evolution on Remote Sensing. 5
  pages, 1 figure, 2 tables
- **Journal**: None
- **Summary**: In this paper we present a novel method for robust hyperspectral image classification using context and rejection. Hyperspectral image classification is generally an ill-posed image problem where pixels may belong to unknown classes, and obtaining representative and complete training sets is costly. Furthermore, the need for high classification accuracies is frequently greater than the need to classify the entire image.   We approach this problem with a robust classification method that combines classification with context with classification with rejection. A rejection field that will guide the rejection is derived from the classification with contextual information obtained by using the SegSALSA algorithm. We validate our method in real hyperspectral data and show that the performance gains obtained from the rejection fields are equivalent to an increase the dimension of the training sets.



### Patch-based Convolutional Neural Network for Whole Slide Tissue Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1504.07947v5
- **DOI**: None
- **Categories**: **cs.CV**, J.3; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1504.07947v5)
- **Published**: 2015-04-29 18:15:22+00:00
- **Updated**: 2016-03-09 14:26:16+00:00
- **Authors**: Le Hou, Dimitris Samaras, Tahsin M. Kurc, Yi Gao, James E. Davis, Joel H. Saltz
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) are state-of-the-art models for many image classification tasks. However, to recognize cancer subtypes automatically, training a CNN on gigapixel resolution Whole Slide Tissue Images (WSI) is currently computationally impossible. The differentiation of cancer subtypes is based on cellular-level visual features observed on image patch scale. Therefore, we argue that in this situation, training a patch-level classifier on image patches will perform better than or similar to an image-level classifier. The challenge becomes how to intelligently combine patch-level classification results and model the fact that not all patches will be discriminative. We propose to train a decision fusion model to aggregate patch-level predictions given by patch-level CNNs, which to the best of our knowledge has not been shown before. Furthermore, we formulate a novel Expectation-Maximization (EM) based method that automatically locates discriminative patches robustly by utilizing the spatial relationships of patches. We apply our method to the classification of glioma and non-small-cell lung carcinoma cases into subtypes. The classification accuracy of our method is similar to the inter-observer agreement between pathologists. Although it is impossible to train CNNs on WSIs, we experimentally demonstrate using a comparable non-cancer dataset of smaller images that a patch-based CNN can outperform an image-based CNN.



### Exploring Integral Image Word Length Reduction Techniques for SURF Detector
- **Arxiv ID**: http://arxiv.org/abs/1504.07958v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.07958v1)
- **Published**: 2015-04-29 18:43:06+00:00
- **Updated**: 2015-04-29 18:43:06+00:00
- **Authors**: Shoaib Ehsan, Klaus D. McDonald-Maier
- **Comment**: ICCEE 2009
- **Journal**: None
- **Summary**: Speeded Up Robust Features (SURF) is a state of the art computer vision algorithm that relies on integral image representation for performing fast detection and description of image features that are scale and rotation invariant. Integral image representation, however, has major draw back of large binary word length that leads to substantial increase in memory size. When designing a dedicated hardware to achieve real-time performance for the SURF algorithm, it is imperative to consider the adverse effects of integral image on memory size, bus width and computational resources. With the objective of minimizing hardware resources, this paper presents a novel implementation concept of a reduced word length integral image based SURF detector. It evaluates two existing word length reduction techniques for the particular case of SURF detector and extends one of these to achieve more reduction in word length. This paper also introduces a novel method to achieve integral image word length reduction for SURF detector.



### Hardware based Scale- and Rotation-Invariant Feature Extraction: A Retrospective Analysis and Future Directions
- **Arxiv ID**: http://arxiv.org/abs/1504.07962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.07962v1)
- **Published**: 2015-04-29 18:52:37+00:00
- **Updated**: 2015-04-29 18:52:37+00:00
- **Authors**: Shoaib Ehsan, Adrian F. Clark, Klaus D. McDonald-Maier
- **Comment**: ICCEE 2009
- **Journal**: None
- **Summary**: Computer Vision techniques represent a class of algorithms that are highly computation and data intensive in nature. Generally, performance of these algorithms in terms of execution speed on desktop computers is far from real-time. Since real-time performance is desirable in many applications, special-purpose hardware is required in most cases to achieve this goal. Scale- and rotation-invariant local feature extraction is a low level computer vision task with very high computational complexity. The state-of-the-art algorithms that currently exist in this domain, like SIFT and SURF, suffer from slow execution speeds and at best can only achieve rates of 2-3 Hz on modern desktop computers. Hardware-based scale- and rotation-invariant local feature extraction is an emerging trend enabling real-time performance for these computationally complex algorithms. This paper takes a retrospective look at the advances made so far in this field, discusses the hardware design strategies employed and results achieved, identifies current research gaps and suggests future research directions.



### Improved repeatability measures for evaluating performance of feature detectors
- **Arxiv ID**: http://arxiv.org/abs/1504.07967v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1504.07967v1)
- **Published**: 2015-04-29 19:01:30+00:00
- **Updated**: 2015-04-29 19:01:30+00:00
- **Authors**: Shoaib Ehsan, Nadia Kanwal, Adrian F. Clark, Klaus D. McDonald-Maier
- **Comment**: None
- **Journal**: Electronics Letters 8th July 2010 Vol. 46 No. 14
- **Summary**: The most frequently employed measure for performance characterisation of local feature detectors is repeatability, but it has been observed that this does not necessarily mirror actual performance. Presented are improved repeatability formulations which correlate much better with the true performance of feature detectors. Comparative results for several state-of-the-art feature detectors are presented using these measures; it is found that Hessian-based detectors are generally superior at identifying features when images are subject to various geometric and photometric transformations.



### Anticipating Visual Representations from Unlabeled Video
- **Arxiv ID**: http://arxiv.org/abs/1504.08023v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.08023v2)
- **Published**: 2015-04-29 21:01:51+00:00
- **Updated**: 2016-11-30 03:49:34+00:00
- **Authors**: Carl Vondrick, Hamed Pirsiavash, Antonio Torralba
- **Comment**: CVPR 2016
- **Journal**: None
- **Summary**: Anticipating actions and objects before they start or appear is a difficult problem in computer vision with several real-world applications. This task is challenging partly because it requires leveraging extensive knowledge of the world that is difficult to write down. We believe that a promising resource for efficiently learning this knowledge is through readily available unlabeled video. We present a framework that capitalizes on temporal structure in unlabeled video to learn to anticipate human actions and objects. The key idea behind our approach is that we can train deep networks to predict the visual representation of images in the future. Visual representations are a promising prediction target because they encode images at a higher semantic level than pixels yet are automatic to compute. We then apply recognition algorithms on our predicted representation to anticipate objects and actions. We experimentally validate this idea on two datasets, anticipating actions one second in the future and objects five seconds in the future.



