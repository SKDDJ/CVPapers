# Arxiv Papers in cs.CV on 2015-04-28
### Convolutional Channel Features
- **Arxiv ID**: http://arxiv.org/abs/1504.07339v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.07339v3)
- **Published**: 2015-04-28 03:44:39+00:00
- **Updated**: 2015-09-24 17:22:41+00:00
- **Authors**: Bin Yang, Junjie Yan, Zhen Lei, Stan Z. Li
- **Comment**: 9 pages, 5 figures, 6 tables; ICCV 2015 camera-ready version
- **Journal**: None
- **Summary**: Deep learning methods are powerful tools but often suffer from expensive computation and limited flexibility. An alternative is to combine light-weight models with deep representations. As successful cases exist in several visual problems, a unified framework is absent. In this paper, we revisit two widely used approaches in computer vision, namely filtered channel features and Convolutional Neural Networks (CNN), and absorb merits from both by proposing an integrated method called Convolutional Channel Features (CCF). CCF transfers low-level features from pre-trained CNN models to feed the boosting forest model. With the combination of CNN features and boosting forest, CCF benefits from the richer capacity in feature representation compared with channel features, as well as lower cost in computation and storage compared with end-to-end CNN methods. We show that CCF serves as a good way of tailoring pre-trained CNN models to diverse tasks without fine-tuning the whole network to each task by achieving state-of-the-art performances in pedestrian detection, face detection, edge detection and object proposal generation.



### Embedded Platforms for Computer Vision-based Advanced Driver Assistance Systems: a Survey
- **Arxiv ID**: http://arxiv.org/abs/1504.07442v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9; I.2.10; D.4.7; C.3
- **Links**: [PDF](http://arxiv.org/pdf/1504.07442v1)
- **Published**: 2015-04-28 12:19:45+00:00
- **Updated**: 2015-04-28 12:19:45+00:00
- **Authors**: Gorka Velez, Oihana Otaegui
- **Comment**: 10 pages. To be published in ITS World Congress 2015
- **Journal**: None
- **Summary**: Computer Vision, either alone or combined with other technologies such as radar or Lidar, is one of the key technologies used in Advanced Driver Assistance Systems (ADAS). Its role understanding and analysing the driving scene is of great importance as it can be noted by the number of ADAS applications that use this technology. However, porting a vision algorithm to an embedded automotive system is still very challenging, as there must be a trade-off between several design requisites. Furthermore, there is not a standard implementation platform, so different alternatives have been proposed by both the scientific community and the industry. This paper aims to review the requisites and the different embedded implementation platforms that can be used for Computer Vision-based ADAS, with a critical analysis and an outlook to future trends.



### Identifying Reliable Annotations for Large Scale Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1504.07460v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.07460v1)
- **Published**: 2015-04-28 13:19:21+00:00
- **Updated**: 2015-04-28 13:19:21+00:00
- **Authors**: Alexander Kolesnikov, Christoph H. Lampert
- **Comment**: None
- **Journal**: None
- **Summary**: Challenging computer vision tasks, in particular semantic image segmentation, require large training sets of annotated images. While obtaining the actual images is often unproblematic, creating the necessary annotation is a tedious and costly process. Therefore, one often has to work with unreliable annotation sources, such as Amazon Mechanical Turk or (semi-)automatic algorithmic techniques. In this work, we present a Gaussian process (GP) based technique for simultaneously identifying which images of a training set have unreliable annotation and learning a segmentation model in which the negative effect of these images is suppressed. Alternatively, the model can also just be used to identify the most reliably annotated images from the training set, which can then be used for training any other segmentation method. By relying on "deep features" in combination with a linear covariance function, our GP can be learned and its hyperparameter determined efficiently using only matrix operations and gradient-based optimization. This makes our method scalable even to large datasets with several million training instances.



### Compact CNN for Indexing Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/1504.07469v2
- **DOI**: 10.1109/WACV.2016.7477708
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.07469v2)
- **Published**: 2015-04-28 13:41:16+00:00
- **Updated**: 2015-11-24 21:13:18+00:00
- **Authors**: Yair Poleg, Ariel Ephrat, Shmuel Peleg, Chetan Arora
- **Comment**: None
- **Journal**: IEEE WACV'16, March 2016, pp. 1-9
- **Summary**: While egocentric video is becoming increasingly popular, browsing it is very difficult. In this paper we present a compact 3D Convolutional Neural Network (CNN) architecture for long-term activity recognition in egocentric videos. Recognizing long-term activities enables us to temporally segment (index) long and unstructured egocentric videos. Existing methods for this task are based on hand tuned features derived from visible objects, location of hands, as well as optical flow.   Given a sparse optical flow volume as input, our CNN classifies the camera wearer's activity. We obtain classification accuracy of 89%, which outperforms the current state-of-the-art by 19%. Additional evaluation is performed on an extended egocentric video dataset, classifying twice the amount of categories than current state-of-the-art. Furthermore, our CNN is able to recognize whether a video is egocentric or not with 99.2% accuracy, up by 24% from current state-of-the-art. To better understand what the network actually learns, we propose a novel visualization of CNN kernels as flow fields.



### Speeding Up Neural Networks for Large Scale Classification using WTA Hashing
- **Arxiv ID**: http://arxiv.org/abs/1504.07488v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.07488v1)
- **Published**: 2015-04-28 14:17:24+00:00
- **Updated**: 2015-04-28 14:17:24+00:00
- **Authors**: Amir H. Bakhtiary, Agata Lapedriza, David Masip
- **Comment**: 9 pages, 9 figures, 1 table
- **Journal**: None
- **Summary**: In this paper we propose to use the Winner Takes All hashing technique to speed up forward propagation and backward propagation in fully connected layers in convolutional neural networks. The proposed technique reduces significantly the computational complexity, which in turn, allows us to train layers with a large number of kernels with out the associated time penalty.   As a consequence we are able to train convolutional neural network on a very large number of output classes with only a small increase in the computational cost. To show the effectiveness of the technique we train a new output layer on a pretrained network using both the regular multiplicative approach and our proposed hashing methodology. Our results showed no drop in performance and demonstrate, with our implementation, a 7 fold speed up during the training.



### Becoming the Expert - Interactive Multi-Class Machine Teaching
- **Arxiv ID**: http://arxiv.org/abs/1504.07575v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1504.07575v1)
- **Published**: 2015-04-28 17:22:29+00:00
- **Updated**: 2015-04-28 17:22:29+00:00
- **Authors**: Edward Johns, Oisin Mac Aodha, Gabriel J. Brostow
- **Comment**: CVPR 2015
- **Journal**: None
- **Summary**: Compared to machines, humans are extremely good at classifying images into categories, especially when they possess prior knowledge of the categories at hand. If this prior information is not available, supervision in the form of teaching images is required. To learn categories more quickly, people should see important and representative images first, followed by less important images later - or not at all. However, image-importance is individual-specific, i.e. a teaching image is important to a student if it changes their overall ability to discriminate between classes. Further, students keep learning, so while image-importance depends on their current knowledge, it also varies with time.   In this work we propose an Interactive Machine Teaching algorithm that enables a computer to teach challenging visual concepts to a human. Our adaptive algorithm chooses, online, which labeled images from a teaching set should be shown to the student as they learn. We show that a teaching strategy that probabilistically models the student's ability and progress, based on their correct and incorrect answers, produces better 'experts'. We present results using real human participants across several varied and challenging real-world datasets.



### A Robust Lane Detection and Departure Warning System
- **Arxiv ID**: http://arxiv.org/abs/1504.07590v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1504.07590v1)
- **Published**: 2015-04-28 18:14:02+00:00
- **Updated**: 2015-04-28 18:14:02+00:00
- **Authors**: Mrinal Haloi, Dinesh Babu Jayagopi
- **Comment**: The Intelligent Vehicles Symposium (IV2015). arXiv admin note: text
  overlap with arXiv:1503.06648
- **Journal**: None
- **Summary**: In this work, we have developed a robust lane detection and departure warning technique. Our system is based on single camera sensor. For lane detection a modified Inverse Perspective Mapping using only a few extrinsic camera parameters and illuminant Invariant techniques is used. Lane markings are represented using a combination of 2nd and 4th order steerable filters, robust to shadowing. Effect of shadowing and extra sun light are removed using Lab color space, and illuminant invariant representation. Lanes are assumed to be cubic curves and fitted using robust RANSAC. This method can reliably detect lanes of the road and its boundary. This method has been experimented in Indian road conditions under different challenging situations and the result obtained were very good. For lane departure angle an optical flow based method were used.



### A novel variational model for image registration using Gaussian curvature
- **Arxiv ID**: http://arxiv.org/abs/1504.07643v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, 65F10, 68U10, 62H35
- **Links**: [PDF](http://arxiv.org/pdf/1504.07643v1)
- **Published**: 2015-04-28 20:06:35+00:00
- **Updated**: 2015-04-28 20:06:35+00:00
- **Authors**: Mazlinda Ibrahim, Ke Chen, Carlos Brito-Loeza
- **Comment**: 23 pages, 5 figures. Key words: Image registration, Non-parametric
  image registration, Regularisation, Gaussian curvature, surface mapping
- **Journal**: None
- **Summary**: Image registration is one important task in many image processing applications. It aims to align two or more images so that useful information can be extracted through comparison, combination or superposition. This is achieved by constructing an optimal trans- formation which ensures that the template image becomes similar to a given reference image. Although many models exist, designing a model capable of modelling large and smooth deformation field continues to pose a challenge. This paper proposes a novel variational model for image registration using the Gaussian curvature as a regulariser. The model is motivated by the surface restoration work in geometric processing [Elsey and Esedoglu, Multiscale Model. Simul., (2009), pp. 1549-1573]. An effective numerical solver is provided for the model using an augmented Lagrangian method. Numerical experiments can show that the new model outperforms three competing models based on, respectively, a linear curvature [Fischer and Modersitzki, J. Math. Imaging Vis., (2003), pp. 81- 85], the mean curvature [Chumchob, Chen and Brito, Multiscale Model. Simul., (2011), pp. 89-128] and the diffeomorphic demon model [Vercauteren at al., NeuroImage, (2009), pp. 61-72] in terms of robustness and accuracy.



