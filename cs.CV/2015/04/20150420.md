# Arxiv Papers in cs.CV on 2015-04-20
### Learning discriminative trajectorylet detector sets for accurate skeleton-based action recognition
- **Arxiv ID**: http://arxiv.org/abs/1504.04923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.04923v1)
- **Published**: 2015-04-20 02:41:03+00:00
- **Updated**: 2015-04-20 02:41:03+00:00
- **Authors**: Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, Anton von den Hengel
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: The introduction of low-cost RGB-D sensors has promoted the research in skeleton-based human action recognition. Devising a representation suitable for characterising actions on the basis of noisy skeleton sequences remains a challenge, however. We here provide two insights into this challenge. First, we show that the discriminative information of a skeleton sequence usually resides in a short temporal interval and we propose a simple-but-effective local descriptor called trajectorylet to capture the static and kinematic information within this interval. Second, we further propose to encode each trajectorylet with a discriminative trajectorylet detector set which is selected from a large number of candidate detectors trained through exemplar-SVMs. The action-level representation is obtained by pooling trajectorylet encodings. Evaluating on standard datasets acquired from the Kinect sensor, it is demonstrated that our method obtains superior results over existing approaches under various experimental setups.



### Weakly Supervised Fine-Grained Image Categorization
- **Arxiv ID**: http://arxiv.org/abs/1504.04943v1
- **DOI**: 10.1109/TIP.2016.2531289
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.04943v1)
- **Published**: 2015-04-20 05:58:21+00:00
- **Updated**: 2015-04-20 05:58:21+00:00
- **Authors**: Yu Zhang, Xiu-shen Wei, Jianxin Wu, Jianfei Cai, Jiangbo Lu, Viet-Anh Nguyen, Minh N. Do
- **Comment**: None
- **Journal**: An extended version in IEEE Trans Image Processing, 25(4), 2016:
  pp. 1713-1725
- **Summary**: In this paper, we categorize fine-grained images without using any object / part annotation neither in the training nor in the testing stage, a step towards making it suitable for deployments. Fine-grained image categorization aims to classify objects with subtle distinctions. Most existing works heavily rely on object / part detectors to build the correspondence between object parts by using object or object part annotations inside training images. The need for expensive object annotations prevents the wide usage of these methods. Instead, we propose to select useful parts from multi-scale part proposals in objects, and use them to compute a global image representation for categorization. This is specially designed for the annotation-free fine-grained categorization task, because useful parts have shown to play an important role in existing annotation-dependent works but accurate part detectors can be hardly acquired. With the proposed image representation, we can further detect and visualize the key (most discriminative) parts in objects of different classes. In the experiment, the proposed annotation-free method achieves better accuracy than that of state-of-the-art annotation-free and most existing annotation-dependent methods on two challenging datasets, which shows that it is not always necessary to use accurate object / part annotations in fine-grained image categorization.



### F-SVM: Combination of Feature Transformation and SVM Learning via Convex Relaxation
- **Arxiv ID**: http://arxiv.org/abs/1504.05035v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1504.05035v1)
- **Published**: 2015-04-20 12:36:50+00:00
- **Updated**: 2015-04-20 12:36:50+00:00
- **Authors**: Xiaohe Wu, Wangmeng Zuo, Yuanyuan Zhu, Liang Lin
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: The generalization error bound of support vector machine (SVM) depends on the ratio of radius and margin, while standard SVM only considers the maximization of the margin but ignores the minimization of the radius. Several approaches have been proposed to integrate radius and margin for joint learning of feature transformation and SVM classifier. However, most of them either require the form of the transformation matrix to be diagonal, or are non-convex and computationally expensive. In this paper, we suggest a novel approximation for the radius of minimum enclosing ball (MEB) in feature space, and then propose a convex radius-margin based SVM model for joint learning of feature transformation and SVM classifier, i.e., F-SVM. An alternating minimization method is adopted to solve the F-SVM model, where the feature transformation is updatedvia gradient descent and the classifier is updated by employing the existing SVM solver. By incorporating with kernel principal component analysis, F-SVM is further extended for joint learning of nonlinear transformation and classifier. Experimental results on the UCI machine learning datasets and the LFW face datasets show that F-SVM outperforms the standard SVM and the existing radius-margin based SVMs, e.g., RMM, R-SVM+ and R-SVM+{\mu}.



### Exploiting Local Features from Deep Networks for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1504.05133v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.05133v2)
- **Published**: 2015-04-20 17:41:46+00:00
- **Updated**: 2015-04-30 03:36:25+00:00
- **Authors**: Joe Yue-Hei Ng, Fan Yang, Larry S. Davis
- **Comment**: CVPR DeepVision Workshop 2015
- **Journal**: None
- **Summary**: Deep convolutional neural networks have been successfully applied to image classification tasks. When these same networks have been applied to image retrieval, the assumption has been made that the last layers would give the best performance, as they do in classification. We show that for instance-level image retrieval, lower layers often perform better than the last layers in convolutional neural networks. We present an approach for extracting convolutional features from different layers of the networks, and adopt VLAD encoding to encode features into a single vector for each image. We investigate the effect of different layers and scales of input images on the performance of convolutional features using the recent deep networks OxfordNet and GoogLeNet. Experiments demonstrate that intermediate layers or higher layers with finer scales produce better results for image retrieval, compared to the last layer. When using compressed 128-D VLAD descriptors, our method obtains state-of-the-art results and outperforms other VLAD and CNN based approaches on two out of three test datasets. Our work provides guidance for transferring deep networks trained on image classification to image retrieval tasks.



### Convolutional Neural Network-Based Image Representation for Visual Loop Closure Detection
- **Arxiv ID**: http://arxiv.org/abs/1504.05241v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1504.05241v1)
- **Published**: 2015-04-20 21:43:45+00:00
- **Updated**: 2015-04-20 21:43:45+00:00
- **Authors**: Yi Hou, Hong Zhang, Shilin Zhou
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNN) have recently been shown in many computer vision and pattern recog- nition applications to outperform by a significant margin state- of-the-art solutions that use traditional hand-crafted features. However, this impressive performance is yet to be fully exploited in robotics. In this paper, we focus one specific problem that can benefit from the recent development of the CNN technology, i.e., we focus on using a pre-trained CNN model as a method of generating an image representation appropriate for visual loop closure detection in SLAM (simultaneous localization and mapping). We perform a comprehensive evaluation of the outputs at the intermediate layers of a CNN as image descriptors, in comparison with state-of-the-art image descriptors, in terms of their ability to match images for detecting loop closures. The main conclusions of our study include: (a) CNN-based image representations perform comparably to state-of-the-art hand- crafted competitors in environments without significant lighting change, (b) they outperform state-of-the-art competitors when lighting changes significantly, and (c) they are also significantly faster to extract than the state-of-the-art hand-crafted features even on a conventional CPU and are two orders of magnitude faster on an entry-level GPU.



