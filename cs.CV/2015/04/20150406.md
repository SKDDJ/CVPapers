# Arxiv Papers in cs.CV on 2015-04-06
### Matching-CNN Meets KNN: Quasi-Parametric Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/1504.01220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.01220v1)
- **Published**: 2015-04-06 07:20:02+00:00
- **Updated**: 2015-04-06 07:20:02+00:00
- **Authors**: Si Liu, Xiaodan Liang, Luoqi Liu, Xiaohui Shen, Jianchao Yang, Changsheng Xu, Liang Lin, Xiaochun Cao, Shuicheng Yan
- **Comment**: This manuscript is the accepted version for CVPR 2015
- **Journal**: None
- **Summary**: Both parametric and non-parametric approaches have demonstrated encouraging performances in the human parsing task, namely segmenting a human image into several semantic regions (e.g., hat, bag, left arm, face). In this work, we aim to develop a new solution with the advantages of both methodologies, namely supervision from annotated data and the flexibility to use newly annotated (possibly uncommon) images, and present a quasi-parametric human parsing model. Under the classic K Nearest Neighbor (KNN)-based nonparametric framework, the parametric Matching Convolutional Neural Network (M-CNN) is proposed to predict the matching confidence and displacements of the best matched region in the testing image for a particular semantic region in one KNN image. Given a testing image, we first retrieve its KNN images from the annotated/manually-parsed human image corpus. Then each semantic region in each KNN image is matched with confidence to the testing image using M-CNN, and the matched regions from all KNN images are further fused, followed by a superpixel smoothing procedure to obtain the ultimate human parsing result. The M-CNN differs from the classic CNN in that the tailored cross image matching filters are introduced to characterize the matching between the testing image and the semantic region of a KNN image. The cross image matching filters are defined at different convolutional layers, each aiming to capture a particular range of displacements. Comprehensive evaluations over a large dataset with 7,700 annotated human images well demonstrate the significant performance gain from the quasi-parametric model over the state-of-the-arts, for the human parsing task.



### Knowledge driven Offline to Online Script Conversion
- **Arxiv ID**: http://arxiv.org/abs/1504.01420v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.01420v1)
- **Published**: 2015-04-06 21:27:45+00:00
- **Updated**: 2015-04-06 21:27:45+00:00
- **Authors**: Sunil Kopparapu, Devanuj, Akhilesh Srivastava, P. V. S. Rao
- **Comment**: 4 pages, 5 figures, KBCS 2004
- **Journal**: None
- **Summary**: The problem of offline to online script conversion is a challenging and an ill-posed problem. The interest in offline to online conversion exists because there are a plethora of robust algorithms in online script literature which can not be used on offline scripts. In this paper, we propose a method, based on heuristics, to extract online script information from offline bitmap image. We show the performance of the proposed method on a real sample signature offline image, whose online information is known.



