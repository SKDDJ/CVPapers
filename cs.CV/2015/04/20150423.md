# Arxiv Papers in cs.CV on 2015-04-23
### Edge Detection Based on Global and Local Parameters of the Image
- **Arxiv ID**: http://arxiv.org/abs/1504.06036v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1504.06036v1)
- **Published**: 2015-04-23 04:11:41+00:00
- **Updated**: 2015-04-23 04:11:41+00:00
- **Authors**: Andrew F. C. Brustolin
- **Comment**: 13 pages, 26 figures
- **Journal**: None
- **Summary**: This paper presents an edge detection method based on global and local parameters of the image, which produces satisfactory results on the edge detection of complex images and has a simple structure for execution. The local and global parameters of the image are arithmetic means and standard deviations, the former acquired from a three sized window representing five pixels, the latter acquired from the entire row or column. We obtain the differences of grayscale intensities between two adjacent pixels and the sum of the modulus of these differences from the horizontal and vertical scans of the image. Using these obtained values, we calculate the local and global parameters. After the gathering of the local and global parameters, we compare each sum of the modulus of differences with its own local and global parameter. In the case of the comparison is true, the consecutive pixel to the modulus sum of differences index is marked as an edge. We present the results of the tests with grayscale images using different parameters and discuss the advantages and disadvantages of each parameter value and algorithm structure chosen on the edge processing. There is a comparison of results between this papers detector and Canny, where we evaluate the quality of the presented detector.



### Understanding and Diagnosing Visual Tracking Systems
- **Arxiv ID**: http://arxiv.org/abs/1504.06055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06055v1)
- **Published**: 2015-04-23 06:37:29+00:00
- **Updated**: 2015-04-23 06:37:29+00:00
- **Authors**: Naiyan Wang, Jianping Shi, Dit-Yan Yeung, Jiaya Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Several benchmark datasets for visual tracking research have been proposed in recent years. Despite their usefulness, whether they are sufficient for understanding and diagnosing the strengths and weaknesses of different trackers remains questionable. To address this issue, we propose a framework by breaking a tracker down into five constituent parts, namely, motion model, feature extractor, observation model, model updater, and ensemble post-processor. We then conduct ablative experiments on each component to study how it affects the overall result. Surprisingly, our findings are discrepant with some common beliefs in the visual tracking research community. We find that the feature extractor plays the most important role in a tracker. On the other hand, although the observation model is the focus of many studies, we find that it often brings no significant improvement. Moreover, the motion model and model updater contain many details that could affect the result. Also, the ensemble post-processor can improve the result substantially when the constituent trackers have high diversity. Based on our findings, we put together some very elementary building blocks to give a basic tracker which is competitive in performance to the state-of-the-art trackers. We believe our framework can provide a solid baseline when conducting controlled experiments for visual tracking research.



### Multimodal Convolutional Neural Networks for Matching Image and Sentence
- **Arxiv ID**: http://arxiv.org/abs/1504.06063v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1504.06063v5)
- **Published**: 2015-04-23 07:10:13+00:00
- **Updated**: 2015-08-29 09:35:09+00:00
- **Authors**: Lin Ma, Zhengdong Lu, Lifeng Shang, Hang Li
- **Comment**: Accepted by ICCV 2015
- **Journal**: None
- **Summary**: In this paper, we propose multimodal convolutional neural networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convolutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content, and one matching CNN learning the joint representation of image and sentence. The matching CNN composes words to different semantic fragments and learns the inter-modal relations between image and the composed fragments at different levels, thus fully exploit the matching relations between image and sentence. Experimental results on benchmark databases of bidirectional image and sentence retrieval demonstrate that the proposed m-CNNs can effectively capture the information necessary for image and sentence matching. Specifically, our proposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K and Microsoft COCO databases achieve the state-of-the-art performances.



### Object Detection Networks on Convolutional Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/1504.06066v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06066v2)
- **Published**: 2015-04-23 07:15:10+00:00
- **Updated**: 2016-08-17 15:51:13+00:00
- **Authors**: Shaoqing Ren, Kaiming He, Ross Girshick, Xiangyu Zhang, Jian Sun
- **Comment**: To appear in TPAMI; substantial re-writing over the original post at
  arXiv of April 2015. COCO competition results included
- **Journal**: None
- **Summary**: Most object detectors contain two important components: a feature extractor and an object classifier. The feature extractor has rapidly evolved with significant research efforts leading to better deep convolutional architectures. The object classifier, however, has not received much attention and many recent systems (like SPPnet and Fast/Faster R-CNN) use simple multi-layer perceptrons. This paper demonstrates that carefully designing deep networks for object classification is just as important. We experiment with region-wise classifier networks that use shared, region-independent convolutional features. We call them "Networks on Convolutional feature maps" (NoCs). We discover that aside from deep feature maps, a deep and convolutional per-region classifier is of particular importance for object detection, whereas latest superior image classification models (such as ResNets and GoogLeNets) do not directly lead to good detection accuracy without using such a per-region classifier. We show by experiments that despite the effective ResNets and Faster R-CNN systems, the design of NoCs is an essential element for the 1st-place winning entries in ImageNet and MS COCO challenges 2015.



### Online Adaptive Hidden Markov Model for Multi-Tracker Fusion
- **Arxiv ID**: http://arxiv.org/abs/1504.06103v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06103v2)
- **Published**: 2015-04-23 09:34:59+00:00
- **Updated**: 2016-03-04 14:52:18+00:00
- **Authors**: Tomas Vojir, Jiri Matas, Jana Noskova
- **Comment**: 27 pages, 9 figures, submitted to CVIU journal
- **Journal**: None
- **Summary**: In this paper, we propose a novel method for visual object tracking called HMMTxD. The method fuses observations from complementary out-of-the box trackers and a detector by utilizing a hidden Markov model whose latent states correspond to a binary vector expressing the failure of individual trackers. The Markov model is trained in an unsupervised way, relying on an online learned detector to provide a source of tracker-independent information for a modified Baum- Welch algorithm that updates the model w.r.t. the partially annotated data.   We show the effectiveness of the proposed method on combination of two and three tracking algorithms. The performance of HMMTxD is evaluated on two standard benchmarks (CVPR2013 and VOT) and on a rich collection of 77 publicly available sequences. The HMMTxD outperforms the state-of-the-art, often significantly, on all datasets in almost all criteria.



### Sparse Radial Sampling LBP for Writer Identification
- **Arxiv ID**: http://arxiv.org/abs/1504.06133v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06133v1)
- **Published**: 2015-04-23 11:51:53+00:00
- **Updated**: 2015-04-23 11:51:53+00:00
- **Authors**: Anguelos Nicolaou, Andrew D. Bagdanov, Marcus Liwicki, Dimosthenis Karatzas
- **Comment**: Submitted to the 13th International Conference on Document Analysis
  and Recognition (ICDAR 2015)
- **Journal**: None
- **Summary**: In this paper we present the use of Sparse Radial Sampling Local Binary Patterns, a variant of Local Binary Patterns (LBP) for text-as-texture classification. By adapting and extending the standard LBP operator to the particularities of text we get a generic text-as-texture classification scheme and apply it to writer identification. In experiments on CVL and ICDAR 2013 datasets, the proposed feature-set demonstrates State-Of-the-Art (SOA) performance. Among the SOA, the proposed method is the only one that is based on dense extraction of a single local feature descriptor. This makes it fast and applicable at the earliest stages in a DIA pipeline without the need for segmentation, binarization, or extraction of multiple features.



### Robust Principal Component Analysis on Graphs
- **Arxiv ID**: http://arxiv.org/abs/1504.06151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06151v1)
- **Published**: 2015-04-23 12:39:40+00:00
- **Updated**: 2015-04-23 12:39:40+00:00
- **Authors**: Nauman Shahid, Vassilis Kalofolias, Xavier Bresson, Michael Bronstein, Pierre Vandergheynst
- **Comment**: None
- **Journal**: None
- **Summary**: Principal Component Analysis (PCA) is the most widely used tool for linear dimensionality reduction and clustering. Still it is highly sensitive to outliers and does not scale well with respect to the number of data samples. Robust PCA solves the first issue with a sparse penalty term. The second issue can be handled with the matrix factorization model, which is however non-convex. Besides, PCA based clustering can also be enhanced by using a graph of data similarity. In this article, we introduce a new model called "Robust PCA on Graphs" which incorporates spectral graph regularization into the Robust PCA framework. Our proposed model benefits from 1) the robustness of principal components to occlusions and missing values, 2) enhanced low-rank recovery, 3) improved clustering property due to the graph smoothness assumption on the low-rank matrix, and 4) convexity of the resulting optimization problem. Extensive experiments on 8 benchmark, 3 video and 2 artificial datasets with corruptions clearly reveal that our model outperforms 10 other state-of-the-art models in its clustering and low-rank recovery tasks.



### High-for-Low and Low-for-High: Efficient Boundary Detection from Deep Object Features and its Applications to High-Level Vision
- **Arxiv ID**: http://arxiv.org/abs/1504.06201v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06201v3)
- **Published**: 2015-04-23 14:35:12+00:00
- **Updated**: 2015-09-21 17:48:23+00:00
- **Authors**: Gedas Bertasius, Jianbo Shi, Lorenzo Torresani
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the current boundary detection systems rely exclusively on low-level features, such as color and texture. However, perception studies suggest that humans employ object-level reasoning when judging if a particular pixel is a boundary. Inspired by this observation, in this work we show how to predict boundaries by exploiting object-level features from a pretrained object-classification network. Our method can be viewed as a "High-for-Low" approach where high-level object features inform the low-level boundary detection process. Our model achieves state-of-the-art performance on an established boundary detection benchmark and it is efficient to run.   Additionally, we show that due to the semantic nature of our boundaries we can use them to aid a number of high-level vision tasks. We demonstrate that using our boundaries we improve the performance of state-of-the-art methods on the problems of semantic boundary labeling, semantic segmentation and object proposal generation. We can view this process as a "Low-for-High" scheme, where low-level boundaries aid high-level vision tasks.   Thus, our contributions include a boundary detection system that is accurate, efficient, generalizes well to multiple datasets, and is also shown to improve existing state-of-the-art high-level vision methods on three distinct tasks.



### An Elastic Image Registration Approach for Wireless Capsule Endoscope Localization
- **Arxiv ID**: http://arxiv.org/abs/1504.06206v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06206v1)
- **Published**: 2015-04-23 14:41:07+00:00
- **Updated**: 2015-04-23 14:41:07+00:00
- **Authors**: Isabel N. Figueiredo, Carlos Leal, Luís Pinto, Pedro N. Figueiredo, Richard Tsai
- **Comment**: None
- **Journal**: None
- **Summary**: Wireless Capsule Endoscope (WCE) is an innovative imaging device that permits physicians to examine all the areas of the Gastrointestinal (GI) tract. It is especially important for the small intestine, where traditional invasive endoscopies cannot reach. Although WCE represents an extremely important advance in medical imaging, a major drawback that remains unsolved is the WCE precise location in the human body during its operating time. This is mainly due to the complex physiological environment and the inherent capsule effects during its movement. When an abnormality is detected, in the WCE images, medical doctors do not know precisely where this abnormality is located relative to the intestine and therefore they can not proceed efficiently with the appropriate therapy. The primary objective of the present paper is to give a contribution to WCE localization, using image-based methods. The main focus of this work is on the description of a multiscale elastic image registration approach, its experimental application on WCE videos, and comparison with a multiscale affine registration. The proposed approach includes registrations that capture both rigid-like and non-rigid deformations, due respectively to the rigid-like WCE movement and the elastic deformation of the small intestine originated by the GI peristaltic movement. Under this approach a qualitative information about the WCE speed can be obtained, as well as the WCE location and orientation via projective geometry. The results of the experimental tests with real WCE video frames show the good performance of the proposed approach, when elastic deformations of the small intestine are involved in successive frames, and its superiority with respect to a multiscale affine image registration, which accounts for rigid-like deformations only and discards elastic deformations.



### Person Re-identification with Correspondence Structure Learning
- **Arxiv ID**: http://arxiv.org/abs/1504.06243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06243v1)
- **Published**: 2015-04-23 16:24:43+00:00
- **Updated**: 2015-04-23 16:24:43+00:00
- **Authors**: Yang Shen, Weiyao Lin, Junchi Yan, Mingliang Xu, Jianxin Wu, Jingdong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of handling spatial misalignments due to camera-view changes or human-pose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or human-pose variation in individual images. We further introduce a global-based matching process. It integrates a global matching constraint over the learned correspondence structure to exclude cross-view misalignments during the image patch matching process, hence achieving a more reliable matching score between images. Experimental results on various datasets demonstrate the effectiveness of our approach.



### Evolving Fuzzy Image Segmentation with Self-Configuration
- **Arxiv ID**: http://arxiv.org/abs/1504.06266v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1504.06266v1)
- **Published**: 2015-04-23 17:23:09+00:00
- **Updated**: 2015-04-23 17:23:09+00:00
- **Authors**: Ahmed Othman, Hamid R. Tizhoosh, Farzad Khalvati
- **Comment**: Benchmark data (35 breast ultrasound images with gold standard
  segments) available; 11 pages, 4 algorithms, 6 figures, 5 tables;
- **Journal**: None
- **Summary**: Current image segmentation techniques usually require that the user tune several parameters in order to obtain maximum segmentation accuracy, a computationally inefficient approach, especially when a large number of images must be processed sequentially in daily practice. The use of evolving fuzzy systems for designing a method that automatically adjusts parameters to segment medical images according to the quality expectation of expert users has been proposed recently (Evolving fuzzy image segmentation EFIS). However, EFIS suffers from a few limitations when used in practice mainly due to some fixed parameters. For instance, EFIS depends on auto-detection of the object of interest for feature calculation, a task that is highly application-dependent. This shortcoming limits the applicability of EFIS, which was proposed with the ultimate goal of offering a generic but adjustable segmentation scheme. In this paper, a new version of EFIS is proposed to overcome these limitations. The new EFIS, called self-configuring EFIS (SC-EFIS), uses available training data to self-estimate the parameters that are fixed in EFIS. As well, the proposed SC-EFIS relies on a feature selection process that does not require auto-detection of an ROI. The proposed SC-EFIS was evaluated using the same segmentation algorithms and the same dataset as for EFIS. The results show that SC-EFIS can provide the same results as EFIS but with a higher level of automation.



