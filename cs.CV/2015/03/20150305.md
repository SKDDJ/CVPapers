# Arxiv Papers in cs.CV on 2015-03-05
### Do We Need More Training Data?
- **Arxiv ID**: http://arxiv.org/abs/1503.01508v1
- **DOI**: 10.1007/s11263-015-0812-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.01508v1)
- **Published**: 2015-03-05 01:51:12+00:00
- **Updated**: 2015-03-05 01:51:12+00:00
- **Authors**: Xiangxin Zhu, Carl Vondrick, Charless Fowlkes, Deva Ramanan
- **Comment**: None
- **Journal**: None
- **Summary**: Datasets for training object recognition systems are steadily increasing in size. This paper investigates the question of whether existing detectors will continue to improve as data grows, or saturate in performance due to limited model complexity and the Bayes risk associated with the feature spaces in which they operate. We focus on the popular paradigm of discriminatively trained templates defined on oriented gradient features. We investigate the performance of mixtures of templates as the number of mixture components and the amount of training data grows. Surprisingly, even with proper treatment of regularization and "outliers", the performance of classic mixture models appears to saturate quickly ($\sim$10 templates and $\sim$100 positive training examples per template). This is not a limitation of the feature space as compositional mixtures that share template parameters via parts and that can synthesize new templates not encountered during training yield significantly better performance. Based on our analysis, we conjecture that the greatest gains in detection performance will continue to derive from improved representations and learning algorithms that can make efficient use of large datasets.



### Jointly Learning Multiple Measures of Similarities from Triplet Comparisons
- **Arxiv ID**: http://arxiv.org/abs/1503.01521v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1503.01521v3)
- **Published**: 2015-03-05 02:57:19+00:00
- **Updated**: 2015-10-06 21:42:55+00:00
- **Authors**: Liwen Zhang, Subhransu Maji, Ryota Tomioka
- **Comment**: None
- **Journal**: None
- **Summary**: Similarity between objects is multi-faceted and it can be easier for human annotators to measure it when the focus is on a specific aspect. We consider the problem of mapping objects into view-specific embeddings where the distance between them is consistent with the similarity comparisons of the form "from the t-th view, object A is more similar to B than to C". Our framework jointly learns view-specific embeddings exploiting correlations between views. Experiments on a number of datasets, including one of multi-view crowdsourced comparison on bird images, show the proposed method achieves lower triplet generalization error when compared to both learning embeddings independently for each view and all views pooled into one view. Our method can also be used to learn multiple measures of similarity over input features taking class labels into account and compares favorably to existing approaches for multi-task metric learning on the ISOLET dataset.



### Spectral Clustering by Ellipsoid and Its Connection to Separable Nonnegative Matrix Factorization
- **Arxiv ID**: http://arxiv.org/abs/1503.01531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.01531v1)
- **Published**: 2015-03-05 04:07:46+00:00
- **Updated**: 2015-03-05 04:07:46+00:00
- **Authors**: Tomohiko Mizutani
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a variant of the normalized cut algorithm for spectral clustering. Although the normalized cut algorithm applies the K-means algorithm to the eigenvectors of a normalized graph Laplacian for finding clusters, our algorithm instead uses a minimum volume enclosing ellipsoid for them. We show that the algorithm shares similarity with the ellipsoidal rounding algorithm for separable nonnegative matrix factorization. Our theoretical insight implies that the algorithm can serve as a bridge between spectral clustering and separable NMF. The K-means algorithm has the issues in that the choice of initial points affects the construction of clusters and certain choices result in poor clustering performance. The normalized cut algorithm inherits these issues since K-means is incorporated in it, whereas the algorithm proposed here does not. An empirical study is presented to examine the performance of the algorithm.



### Deep Temporal Appearance-Geometry Network for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1503.01532v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.01532v1)
- **Published**: 2015-03-05 04:07:56+00:00
- **Updated**: 2015-03-05 04:07:56+00:00
- **Authors**: Heechul Jung, Sihaeng Lee, Sunjeong Park, Injae Lee, Chunghyun Ahn, Junmo Kim
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Temporal information can provide useful features for recognizing facial expressions. However, to manually design useful features requires a lot of effort. In this paper, to reduce this effort, a deep learning technique which is regarded as a tool to automatically extract useful features from raw data, is adopted. Our deep network is based on two different models. The first deep network extracts temporal geometry features from temporal facial landmark points, while the other deep network extracts temporal appearance features from image sequences . These two models are combined in order to boost the performance of the facial expression recognition. Through several experiments, we showed that the two models cooperate with each other. As a result, we achieved superior performance to other state-of-the-art methods in CK+ and Oulu-CASIA databases. Furthermore, one of the main contributions of this paper is that our deep network catches the facial action points automatically.



### Pyrcca: regularized kernel canonical correlation analysis in Python and its applications to neuroimaging
- **Arxiv ID**: http://arxiv.org/abs/1503.01538v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1503.01538v1)
- **Published**: 2015-03-05 04:57:22+00:00
- **Updated**: 2015-03-05 04:57:22+00:00
- **Authors**: Natalia Y. Bilenko, Jack L. Gallant
- **Comment**: None
- **Journal**: None
- **Summary**: Canonical correlation analysis (CCA) is a valuable method for interpreting cross-covariance across related datasets of different dimensionality. There are many potential applications of CCA to neuroimaging data analysis. For instance, CCA can be used for finding functional similarities across fMRI datasets collected from multiple subjects without resampling individual datasets to a template anatomy. In this paper, we introduce Pyrcca, an open-source Python module for executing CCA between two or more datasets. Pyrcca can be used to implement CCA with or without regularization, and with or without linear or a Gaussian kernelization of the datasets. We demonstrate an application of CCA implemented with Pyrcca to neuroimaging data analysis. We use CCA to find a data-driven set of functional response patterns that are similar across individual subjects in a natural movie experiment. We then demonstrate how this set of response patterns discovered by CCA can be used to accurately predict subject responses to novel natural movie stimuli.



### Learning to rank in person re-identification with metric ensembles
- **Arxiv ID**: http://arxiv.org/abs/1503.01543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.01543v1)
- **Published**: 2015-03-05 05:25:57+00:00
- **Updated**: 2015-03-05 05:25:57+00:00
- **Authors**: Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We propose an effective structured learning based approach to the problem of person re-identification which outperforms the current state-of-the-art on most benchmark data sets evaluated. Our framework is built on the basis of multiple low-level hand-crafted and high-level visual features. We then formulate two optimization algorithms, which directly optimize evaluation measures commonly used in person re-identification, also known as the Cumulative Matching Characteristic (CMC) curve. Our new approach is practical to many real-world surveillance applications as the re-identification performance can be concentrated in the range of most practical importance. The combination of these factors leads to a person re-identification system which outperforms most existing algorithms. More importantly, we advance state-of-the-art results on person re-identification by improving the rank-$1$ recognition rates from $40\%$ to $50\%$ on the iLIDS benchmark, $16\%$ to $18\%$ on the PRID2011 benchmark, $43\%$ to $46\%$ on the VIPeR benchmark, $34\%$ to $53\%$ on the CUHK01 benchmark and $21\%$ to $62\%$ on the CUHK03 benchmark.



### Supervised Discrete Hashing
- **Arxiv ID**: http://arxiv.org/abs/1503.01557v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.01557v3)
- **Published**: 2015-03-05 07:06:02+00:00
- **Updated**: 2015-04-19 02:42:16+00:00
- **Authors**: Fumin Shen, Chunhua Shen, Wei Liu, Heng Tao Shen
- **Comment**: This paper has been withdrawn by the authour since the algorithm is
  being used for patent application
- **Journal**: None
- **Summary**: This paper has been withdrawn by the authour.



### What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision
- **Arxiv ID**: http://arxiv.org/abs/1503.01558v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1503.01558v3)
- **Published**: 2015-03-05 07:07:48+00:00
- **Updated**: 2015-03-13 18:55:22+00:00
- **Authors**: Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew Rabinovich, Kevin Murphy
- **Comment**: To appear in NAACL 2015
- **Journal**: None
- **Summary**: We present a novel method for aligning a sequence of instructions to a video of someone carrying out a task. In particular, we focus on the cooking domain, where the instructions correspond to the recipe. Our technique relies on an HMM to align the recipe steps to the (automatically generated) speech transcript. We then refine this alignment using a state-of-the-art visual food detector, based on a deep convolutional neural network. We show that our technique outperforms simpler techniques based on keyword spotting. It also enables interesting applications, such as automatically illustrating recipes with keyframes, and searching within a video for events of interest.



### Convex Optimization for Parallel Energy Minimization
- **Arxiv ID**: http://arxiv.org/abs/1503.01563v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1503.01563v1)
- **Published**: 2015-03-05 07:40:56+00:00
- **Updated**: 2015-03-05 07:40:56+00:00
- **Authors**: K. S. Sesh Kumar, Alvaro Barbero, Stefanie Jegelka, Suvrit Sra, Francis Bach
- **Comment**: None
- **Journal**: None
- **Summary**: Energy minimization has been an intensely studied core problem in computer vision. With growing image sizes (2D and 3D), it is now highly desirable to run energy minimization algorithms in parallel. But many existing algorithms, in particular, some efficient combinatorial algorithms, are difficult to par-allelize. By exploiting results from convex and submodular theory, we reformulate the quadratic energy minimization problem as a total variation denoising problem, which, when viewed geometrically, enables the use of projection and reflection based convex methods. The resulting min-cut algorithm (and code) is conceptually very simple, and solves a sequence of TV denoising problems. We perform an extensive empirical evaluation comparing state-of-the-art combinatorial algorithms and convex optimization techniques. On small problems the iterative convex methods match the combinatorial max-flow algorithms, while on larger problems they offer other flexibility and important gains: (a) their memory footprint is small; (b) their straightforward parallelizability fits multi-core platforms; (c) they can easily be warm-started; and (d) they quickly reach approximately good solutions, thereby enabling faster "inexact" solutions. A key consequence of our approach based on submodularity and convexity is that it is allows to combine any arbitrary combinatorial or convex methods as subroutines, which allows one to obtain hybrid combinatorial and convex optimization algorithms that benefit from the strengths of both.



### Inference of hidden structures in complex physical systems by multi-scale clustering
- **Arxiv ID**: http://arxiv.org/abs/1503.01626v2
- **DOI**: 10.1007/978-3-319-23871-5_6
- **Categories**: **cond-mat.mtrl-sci**, cond-mat.stat-mech, cs.CV, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1503.01626v2)
- **Published**: 2015-03-05 12:40:38+00:00
- **Updated**: 2016-01-14 18:19:21+00:00
- **Authors**: Z. Nussinov, P. Ronhovde, Dandan Hu, S. Chakrabarty, M. Sahu, Bo Sun, N. A. Mauro, K. K. Sahu
- **Comment**: 25 pages, 16 Figures; a review of earlier works
- **Journal**: None
- **Summary**: We survey the application of a relatively new branch of statistical physics--"community detection"-- to data mining. In particular, we focus on the diagnosis of materials and automated image segmentation. Community detection describes the quest of partitioning a complex system involving many elements into optimally decoupled subsets or communities of such elements. We review a multiresolution variant which is used to ascertain structures at different spatial and temporal scales. Significant patterns are obtained by examining the correlations between different independent solvers. Similar to other combinatorial optimization problems in the NP complexity class, community detection exhibits several phases. Typically, illuminating orders are revealed by choosing parameters that lead to extremal information theory correlations.



### BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1503.01640v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.01640v2)
- **Published**: 2015-03-05 14:06:53+00:00
- **Updated**: 2015-05-18 09:00:40+00:00
- **Authors**: Jifeng Dai, Kaiming He, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Recent leading approaches to semantic segmentation rely on deep convolutional networks trained with human-annotated, pixel-level segmentation masks. Such pixel-accurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data. In this paper, we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks. These two steps gradually recover segmentation masks for improving the networks, and vise versa. Our method, called BoxSup, produces competitive results supervised by boxes only, on par with strong baselines fully supervised by masks under the same setting. By leveraging a large amount of bounding boxes, BoxSup further unleashes the power of deep convolutional networks and yields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT.



### Video-Based Facial Expression Recognition Using Local Directional Binary Pattern
- **Arxiv ID**: http://arxiv.org/abs/1503.01646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.01646v1)
- **Published**: 2015-03-05 14:33:58+00:00
- **Updated**: 2015-03-05 14:33:58+00:00
- **Authors**: Sahar Hooshmand, Ali Jamali Avilaq, Amir Hossein Rezaie
- **Comment**: 9 pages, 8 figures, 3 tables in IJASCSE 2015 volume 4 issue 2
- **Journal**: None
- **Summary**: Automatic facial expression analysis is a challenging issue and influenced so many areas such as human computer interaction. Due to the uncertainties of the light intensity and light direction, the face gray shades are uneven and the expression recognition rate under simple Local Binary Pattern is not ideal and promising. In this paper we propose two state-of-the-art descriptors for person-independent facial expression recognition. First the face regions of the whole images in a video sequence are modeled with Volume Local Directional Binary pattern (VLDBP), which is an extended version of the LDBP operator, incorporating movement and appearance together. To make the survey computationally simple and easy to expand, only the co-occurrences of the Local Directional Binary Pattern on three orthogonal planes (LDBP-TOP) are debated. After extracting the feature vectors the K-Nearest Neighbor classifier was used to recognize the expressions. The proposed methods are applied to the videos of the Extended Cohn-Kanade database (CK+) and the experimental outcomes demonstrate that the offered techniques achieve more accuracy in comparison with the classic and traditional algorithms.



### Color Image Classification via Quaternion Principal Component Analysis Network
- **Arxiv ID**: http://arxiv.org/abs/1503.01657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.01657v1)
- **Published**: 2015-03-05 15:12:28+00:00
- **Updated**: 2015-03-05 15:12:28+00:00
- **Authors**: Rui Zeng, Jiasong Wu, Zhuhong Shao, Yang Chen, Lotfi Senhadji, Huazhong Shu
- **Comment**: 9 figures,5 tables
- **Journal**: None
- **Summary**: The Principal Component Analysis Network (PCANet), which is one of the recently proposed deep learning architectures, achieves the state-of-the-art classification accuracy in various databases. However, the performance of PCANet may be degraded when dealing with color images. In this paper, a Quaternion Principal Component Analysis Network (QPCANet), which is an extension of PCANet, is proposed for color images classification. Compared to PCANet, the proposed QPCANet takes into account the spatial distribution information of color images and ensures larger amount of intra-class invariance of color images. Experiments conducted on different color image datasets such as Caltech-101, UC Merced Land Use, Georgia Tech face and CURet have revealed that the proposed QPCANet achieves higher classification accuracy than PCANet.



### EmoNets: Multimodal deep learning approaches for emotion recognition in video
- **Arxiv ID**: http://arxiv.org/abs/1503.01800v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1503.01800v2)
- **Published**: 2015-03-05 22:03:26+00:00
- **Updated**: 2015-03-30 00:55:02+00:00
- **Authors**: Samira Ebrahimi Kahou, Xavier Bouthillier, Pascal Lamblin, Caglar Gulcehre, Vincent Michalski, Kishore Konda, Sébastien Jean, Pierre Froumenty, Yann Dauphin, Nicolas Boulanger-Lewandowski, Raul Chandias Ferrari, Mehdi Mirza, David Warde-Farley, Aaron Courville, Pascal Vincent, Roland Memisevic, Christopher Pal, Yoshua Bengio
- **Comment**: None
- **Journal**: None
- **Summary**: The task of the emotion recognition in the wild (EmotiW) Challenge is to assign one of seven emotions to short video clips extracted from Hollywood style movies. The videos depict acted-out emotions under realistic conditions with a large degree of variation in attributes such as pose and illumination, making it worthwhile to explore approaches which consider combinations of features from multiple modalities for label assignment. In this paper we present our approach to learning several specialist models using deep learning techniques, each focusing on one modality. Among these are a convolutional neural network, focusing on capturing visual information in detected faces, a deep belief net focusing on the representation of the audio stream, a K-Means based "bag-of-mouths" model, which extracts visual features around the mouth region and a relational autoencoder, which addresses spatio-temporal aspects of videos. We explore multiple methods for the combination of cues from these modalities into one common classifier. This achieves a considerably greater accuracy than predictions from our strongest single-modality classifier. Our method was the winning submission in the 2013 EmotiW challenge and achieved a test set accuracy of 47.67% on the 2014 dataset.



### Frequency Domain TOF: Encoding Object Depth in Modulation Frequency
- **Arxiv ID**: http://arxiv.org/abs/1503.01804v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1503.01804v1)
- **Published**: 2015-03-05 22:15:33+00:00
- **Updated**: 2015-03-05 22:15:33+00:00
- **Authors**: Achuta Kadambi, Vage Taamazyan, Suren Jayasuriya, Ramesh Raskar
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Time of flight cameras may emerge as the 3-D sensor of choice. Today, time of flight sensors use phase-based sampling, where the phase delay between emitted and received, high-frequency signals encodes distance. In this paper, we present a new time of flight architecture that relies only on frequency---we refer to this technique as frequency-domain time of flight (FD-TOF). Inspired by optical coherence tomography (OCT), FD-TOF excels when frequency bandwidth is high. With the increasing frequency of TOF sensors, new challenges to time of flight sensing continue to emerge. At high frequencies, FD-TOF offers several potential benefits over phase-based time of flight methods.



