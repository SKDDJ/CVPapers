# Arxiv Papers in cs.CV on 2015-03-12
### Training Binary Multilayer Neural Networks for Image Classification using Expectation Backpropagation
- **Arxiv ID**: http://arxiv.org/abs/1503.03562v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1503.03562v3)
- **Published**: 2015-03-12 02:24:31+00:00
- **Updated**: 2015-03-22 21:47:56+00:00
- **Authors**: Zhiyong Cheng, Daniel Soudry, Zexi Mao, Zhenzhong Lan
- **Comment**: 8 pages with 1 figures and 4 tables
- **Journal**: None
- **Summary**: Compared to Multilayer Neural Networks with real weights, Binary Multilayer Neural Networks (BMNNs) can be implemented more efficiently on dedicated hardware. BMNNs have been demonstrated to be effective on binary classification tasks with Expectation BackPropagation (EBP) algorithm on high dimensional text datasets. In this paper, we investigate the capability of BMNNs using the EBP algorithm on multiclass image classification tasks. The performances of binary neural networks with multiple hidden layers and different numbers of hidden units are examined on MNIST. We also explore the effectiveness of image spatial filters and the dropout technique in BMNNs. Experimental results on MNIST dataset show that EBP can obtain 2.12% test error with binary weights and 1.66% test error with real weights, which is comparable to the results of standard BackPropagation algorithm on fully connected MNNs.



### Low-Level Features for Image Retrieval Based on Extraction of Directional Binary Patterns and Its Oriented Gradients Histogram
- **Arxiv ID**: http://arxiv.org/abs/1503.03606v1
- **DOI**: 10.5121/caij.2015.2102
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1503.03606v1)
- **Published**: 2015-03-12 06:45:01+00:00
- **Updated**: 2015-03-12 06:45:01+00:00
- **Authors**: Nagaraja S., Prabhakar C. J.
- **Comment**: 7 Figures, 5 Tables 16 Pages in Computer Applications: An
  International Journal (CAIJ), Vol.2, No.1, February 2015
- **Journal**: None
- **Summary**: In this paper, we present a novel approach for image retrieval based on extraction of low level features using techniques such as Directional Binary Code, Haar Wavelet transform and Histogram of Oriented Gradients. The DBC texture descriptor captures the spatial relationship between any pair of neighbourhood pixels in a local region along a given direction, while Local Binary Patterns descriptor considers the relationship between a given pixel and its surrounding neighbours. Therefore, DBC captures more spatial information than LBP and its variants, also it can extract more edge information than LBP. Hence, we employ DBC technique in order to extract grey level texture feature from each RGB channels individually and computed texture maps are further combined which represents colour texture features of an image. Then, we decomposed the extracted colour texture map and original image using Haar wavelet transform. Finally, we encode the shape and local features of wavelet transformed images using Histogram of Oriented Gradients for content based image retrieval. The performance of proposed method is compared with existing methods on two databases such as Wang's corel image and Caltech 256. The evaluation results show that our approach outperforms the existing methods for image retrieval.



### Designing A Composite Dictionary Adaptively From Joint Examples
- **Arxiv ID**: http://arxiv.org/abs/1503.03621v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.03621v2)
- **Published**: 2015-03-12 08:09:13+00:00
- **Updated**: 2015-09-08 19:04:30+00:00
- **Authors**: Zhangyang Wang, Yingzhen Yang, Jianchao Yang, Thomas S. Huang
- **Comment**: None
- **Journal**: None
- **Summary**: We study the complementary behaviors of external and internal examples in image restoration, and are motivated to formulate a composite dictionary design framework. The composite dictionary consists of the global part learned from external examples, and the sample-specific part learned from internal examples. The dictionary atoms in both parts are further adaptively weighted to emphasize their model statistics. Experiments demonstrate that the joint utilization of external and internal examples leads to substantial improvements, with successful applications in image denoising and super resolution.



### Single image super-resolution by approximated Heaviside functions
- **Arxiv ID**: http://arxiv.org/abs/1503.03630v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, math.IT, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1503.03630v1)
- **Published**: 2015-03-12 08:54:54+00:00
- **Updated**: 2015-03-12 08:54:54+00:00
- **Authors**: Liang-Jian Deng, Weihong Guo, Ting-Zhu Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Image super-resolution is a process to enhance image resolution. It is widely used in medical imaging, satellite imaging, target recognition, etc. In this paper, we conduct continuous modeling and assume that the unknown image intensity function is defined on a continuous domain and belongs to a space with a redundant basis. We propose a new iterative model for single image super-resolution based on an observation: an image is consisted of smooth components and non-smooth components, and we use two classes of approximated Heaviside functions (AHFs) to represent them respectively. Due to sparsity of the non-smooth components, a $L_{1}$ model is employed. In addition, we apply the proposed iterative model to image patches to reduce computation and storage. Comparisons with some existing competitive methods show the effectiveness of the proposed method.



### On Computing the Translations Norm in the Epipolar Graph
- **Arxiv ID**: http://arxiv.org/abs/1503.03637v3
- **DOI**: 10.1109/3DV.2015.41
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.03637v3)
- **Published**: 2015-03-12 09:16:11+00:00
- **Updated**: 2015-09-04 11:33:39+00:00
- **Authors**: Federica Arrigoni, Beatrice Rossi, Andrea Fusiello
- **Comment**: Accepted at 3DV 2015
- **Journal**: Proceedings of the 2015 International Conference on 3D Vision, pp.
  300-308
- **Summary**: This paper deals with the problem of recovering the unknown norm of relative translations between cameras based on the knowledge of relative rotations and translation directions. We provide theoretical conditions for the solvability of such a problem, and we propose a two-stage method to solve it. First, a cycle basis for the epipolar graph is computed, then all the scaling factors are recovered simultaneously by solving a homogeneous linear system. We demonstrate the accuracy of our solution by means of synthetic and real experiments.



### Starting engagement detection towards a companion robot using multimodal features
- **Arxiv ID**: http://arxiv.org/abs/1503.03732v1
- **DOI**: 10.1016/j.robot.2015.01.004
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1503.03732v1)
- **Published**: 2015-03-12 14:19:40+00:00
- **Updated**: 2015-03-12 14:19:40+00:00
- **Authors**: Dominique Vaufreydaz, Wafa Johal, Claudine Combe
- **Comment**: None
- **Journal**: Robotics and Autonomous Systems, Elsevier, 2015, Robotics and
  Autonomous Systems, pp.25
- **Summary**: Recognition of intentions is a subconscious cognitive process vital to human communication. This skill enables anticipation and increases the quality of interactions between humans. Within the context of engagement, non-verbal signals are used to communicate the intention of starting the interaction with a partner. In this paper, we investigated methods to detect these signals in order to allow a robot to know when it is about to be addressed. Originality of our approach resides in taking inspiration from social and cognitive sciences to perform our perception task. We investigate meaningful features, i.e. human readable features, and elicit which of these are important for recognizing someone's intention of starting an interaction. Classically, spatial information like the human position and speed, the human-robot distance are used to detect the engagement. Our approach integrates multimodal features gathered using a companion robot equipped with a Kinect. The evaluation on our corpus collected in spontaneous conditions highlights its robustness and validates the use of such a technique in a real environment. Experimental validation shows that multimodal features set gives better precision and recall than using only spatial and speed features. We also demonstrate that 7 selected features are sufficient to provide a good starting engagement detection score. In our last investigation, we show that among our full 99 features set, the space reduction is not a solved task. This result opens new researches perspectives on multimodal engagement detection.



### 2D Face Recognition System Based on Selected Gabor Filters and Linear Discriminant Analysis LDA
- **Arxiv ID**: http://arxiv.org/abs/1503.03741v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.03741v1)
- **Published**: 2015-03-12 14:30:51+00:00
- **Updated**: 2015-03-12 14:30:51+00:00
- **Authors**: Samir F. Hafez, Mazen M. Selim, Hala H. Zayed
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new approach for face recognition system. The method is based on 2D face image features using subset of non-correlated and Orthogonal Gabor Filters instead of using the whole Gabor Filter Bank, then compressing the output feature vector using Linear Discriminant Analysis (LDA). The face image has been enhanced using multi stage image processing technique to normalize it and compensate for illumination variation. Experimental results show that the proposed system is effective for both dimension reduction and good recognition performance when compared to the complete Gabor filter bank. The system has been tested using CASIA, ORL and Cropped YaleB 2D face images Databases and achieved average recognition rate of 98.9 %.



### Learning to Detect Vehicles by Clustering Appearance Patterns
- **Arxiv ID**: http://arxiv.org/abs/1503.03771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.03771v1)
- **Published**: 2015-03-12 15:33:09+00:00
- **Updated**: 2015-03-12 15:33:09+00:00
- **Authors**: Eshed Ohn-Bar, Mohan M. Trivedi
- **Comment**: Preprint version of our T-ITS 2015 paper
- **Journal**: None
- **Summary**: This paper studies efficient means for dealing with intra-category diversity in object detection. Strategies for occlusion and orientation handling are explored by learning an ensemble of detection models from visual and geometrical clusters of object instances. An AdaBoost detection scheme is employed with pixel lookup features for fast detection. The analysis provides insight into the design of a robust vehicle detection system, showing promise in terms of detection performance and orientation estimation accuracy.



### FaceNet: A Unified Embedding for Face Recognition and Clustering
- **Arxiv ID**: http://arxiv.org/abs/1503.03832v3
- **DOI**: 10.1109/CVPR.2015.7298682
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.03832v3)
- **Published**: 2015-03-12 18:10:53+00:00
- **Updated**: 2015-06-17 23:35:47+00:00
- **Authors**: Florian Schroff, Dmitry Kalenichenko, James Philbin
- **Comment**: Also published, in Proceedings of the IEEE Computer Society
  Conference on Computer Vision and Pattern Recognition 2015
- **Journal**: None
- **Summary**: Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.   Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face.   On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result by 30% on both datasets.   We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.



### Diagnosing Heterogeneous Dynamics for CT Scan Images of Human Brain in Wavelet and MFDFA domain
- **Arxiv ID**: http://arxiv.org/abs/1503.03913v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.03913v1)
- **Published**: 2015-03-12 23:20:30+00:00
- **Updated**: 2015-03-12 23:20:30+00:00
- **Authors**: Sabyasachi Mukhopadhyay, Soham Mandal, Nandan K Das, Subhadip Dey, Asish Mitra, Nirmalya Ghosh, Prasanta K Panigrahi
- **Comment**: None
- **Journal**: None
- **Summary**: CT scan images of human brain of a particular patient in different cross sections are taken, on which wavelet transform and multi-fractal analysis are applied. The vertical and horizontal unfolding of images are done before analyzing these images. A systematic investigation of de-noised CT scan images of human brain in different cross-sections are carried out through wavelet normalized energy and wavelet semi-log plots, which clearly points out the mismatch between results of vertical and horizontal unfolding. The mismatch of results confirms the heterogeneity in spatial domain. Using the multi-fractal de-trended fluctuation analysis (MFDFA), the mismatch between the values of Hurst exponent and width of singularity spectrum by vertical and horizontal unfolding confirms the same.



