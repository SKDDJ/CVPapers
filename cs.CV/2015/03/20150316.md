# Arxiv Papers in cs.CV on 2015-03-16
### Statistical Analysis of Loopy Belief Propagation in Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1503.04585v3
- **DOI**: 10.1103/PhysRevE.92.042120
- **Categories**: **stat.ML**, cond-mat.dis-nn, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1503.04585v3)
- **Published**: 2015-03-16 10:08:01+00:00
- **Updated**: 2015-09-13 09:17:43+00:00
- **Authors**: Muneki Yasuda, Shun Kataoka, Kazuyuki Tanaka
- **Comment**: None
- **Journal**: Phys. Rev. E 92, 042120 (2015)
- **Summary**: Loopy belief propagation (LBP), which is equivalent to the Bethe approximation in statistical mechanics, is a message-passing-type inference method that is widely used to analyze systems based on Markov random fields (MRFs). In this paper, we propose a message-passing-type method to analytically evaluate the quenched average of LBP in random fields by using the replica cluster variation method. The proposed analytical method is applicable to general pair-wise MRFs with random fields whose distributions differ from each other and can give the quenched averages of the Bethe free energies over random fields, which are consistent with numerical results. The order of its computational cost is equivalent to that of standard LBP. In the latter part of this paper, we describe the application of the proposed method to Bayesian image restoration, in which we observed that our theoretical results are in good agreement with the numerical results for natural images.



### Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1503.04596v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1503.04596v3)
- **Published**: 2015-03-16 10:41:30+00:00
- **Updated**: 2015-08-15 13:02:08+00:00
- **Authors**: Mark D. McDonnell, Tony Vladusich
- **Comment**: 7 pages, 2 figures, Paper at IJCNN 2015 (International Joint
  Conference on Neural Networks, 2015)
- **Journal**: None
- **Summary**: We present a neural network architecture and training method designed to enable very rapid training and low implementation complexity. Due to its training speed and very few tunable parameters, the method has strong potential for applications requiring frequent retraining or online training. The approach is characterized by (a) convolutional filters based on biologically inspired visual processing filters, (b) randomly-valued classifier-stage input weights, (c) use of least squares regression to train the classifier output weights in a single batch, and (d) linear classifier-stage output units. We demonstrate the efficacy of the method by applying it to image classification. Our results match existing state-of-the-art results on the MNIST (0.37% error) and NORB-small (2.2% error) image classification databases, but with very fast training times compared to standard deep network approaches. The network's performance on the Google Street View House Number (SVHN) (4% error) database is also competitive with state-of-the art methods.



### PiMPeR: Piecewise Dense 3D Reconstruction from Multi-View and Multi-Illumination Images
- **Arxiv ID**: http://arxiv.org/abs/1503.04598v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.04598v2)
- **Published**: 2015-03-16 10:51:08+00:00
- **Updated**: 2015-03-17 12:59:24+00:00
- **Authors**: Reza Sabzevari, Vittori Murino, Alessio Del Bue
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of dense 3D reconstruction from multiple view images subject to strong lighting variations. In this regard, a new piecewise framework is proposed to explicitly take into account the change of illumination across several wide-baseline images. Unlike multi-view stereo and multi-view photometric stereo methods, this pipeline deals with wide-baseline images that are uncalibrated, in terms of both camera parameters and lighting conditions. Such a scenario is meant to avoid use of any specific imaging setup and provide a tool for normal users without any expertise. To the best of our knowledge, this paper presents the first work that deals with such unconstrained setting. We propose a coarse-to-fine approach, in which a coarse mesh is first created using a set of geometric constraints and, then, fine details are recovered by exploiting photometric properties of the scene. Augmenting the fine details on the coarse mesh is done via a final optimization step. Note that the method does not provide a generic solution for multi-view photometric stereo problem but it relaxes several common assumptions of this problem. The approach scales very well in size given its piecewise nature, dealing with large scale optimization and with severe missing data. Experiments on a benchmark dataset Robot data-set show the method performance against 3D ground truth.



### Template-based Monocular 3D Shape Recovery using Laplacian Meshes
- **Arxiv ID**: http://arxiv.org/abs/1503.04643v2
- **DOI**: 10.1109/TPAMI.2015.2435739
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.04643v2)
- **Published**: 2015-03-16 13:42:09+00:00
- **Updated**: 2015-05-06 11:49:04+00:00
- **Authors**: Dat Tien Ngo, Jonas Ostlund, Pascal Fua
- **Comment**: Article
- **Journal**: None
- **Summary**: We show that by extending the Laplacian formalism, which was first introduced in the Graphics community to regularize 3D meshes, we can turn the monocular 3D shape reconstruction of a deformable surface given correspondences with a reference image into a much better-posed problem. This allows us to quickly and reliably eliminate outliers by simply solving a linear least squares problem. This yields an initial 3D shape estimate, which is not necessarily accurate, but whose 2D projections are. The initial shape is then refined by a constrained optimization problem to output the final surface reconstruction.   Our approach allows us to reduce the dimensionality of the surface reconstruction problem without sacrificing accuracy, thus allowing for real-time implementations.



### Skilled Impostor Attacks Against Fingerprint Verification Systems And Its Remedy
- **Arxiv ID**: http://arxiv.org/abs/1503.04729v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1503.04729v1)
- **Published**: 2015-03-16 17:03:30+00:00
- **Updated**: 2015-03-16 17:03:30+00:00
- **Authors**: Carsten Gottschlich
- **Comment**: None
- **Journal**: None
- **Summary**: Fingerprint verification systems are becoming ubiquitous in everyday life. This trend is propelled especially by the proliferation of mobile devices with fingerprint sensors such as smartphones and tablet computers, and fingerprint verification is increasingly applied for authenticating financial transactions. In this study we describe a novel attack vector against fingerprint verification systems which we coin skilled impostor attack. We show that existing protocols for performance evaluation of fingerprint verification systems are flawed and as a consequence of this, the system's real vulnerability is systematically underestimated. We examine a scenario in which a fingerprint verification system is tuned to operate at false acceptance rate of 0.1% using the traditional verification protocols with random impostors (zero-effort attacks). We demonstrate that an active and intelligent attacker can achieve a chance of success in the area of 89% or more against this system by performing skilled impostor attacks. We describe a new protocol for evaluating fingerprint verification performance in order to improve the assessment of potential and limitations of fingerprint recognition systems. This new evaluation protocol enables a more informed decision concerning the operating threshold in practical applications and the respective trade-off between security (low false acceptance rates) and usability (low false rejection rates). The skilled impostor attack is a general attack concept which is independent of specific databases or comparison algorithms. The proposed protocol relying on skilled impostor attacks can directly be applied for evaluating the verification performance of other biometric modalities such as e.g. iris, face, ear, finger vein, gait or speaker recognition.



### Phase and TV Based Convex Sets for Blind Deconvolution of Microscopic Images
- **Arxiv ID**: http://arxiv.org/abs/1503.04776v1
- **DOI**: 10.1109/JSTSP.2015.2502541
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1503.04776v1)
- **Published**: 2015-03-16 19:18:10+00:00
- **Updated**: 2015-03-16 19:18:10+00:00
- **Authors**: Mohammad Tofighi, Onur Yorulmaz, A. Enis Cetin
- **Comment**: Submitted to IEEE Selected Topics in Signal Processing
- **Journal**: None
- **Summary**: In this article, two closed and convex sets for blind deconvolution problem are proposed. Most blurring functions in microscopy are symmetric with respect to the origin. Therefore, they do not modify the phase of the Fourier transform (FT) of the original image. As a result blurred image and the original image have the same FT phase. Therefore, the set of images with a prescribed FT phase can be used as a constraint set in blind deconvolution problems. Another convex set that can be used during the image reconstruction process is the epigraph set of Total Variation (TV) function. This set does not need a prescribed upper bound on the total variation of the image. The upper bound is automatically adjusted according to the current image of the restoration process. Both of these two closed and convex sets can be used as a part of any blind deconvolution algorithm. Simulation examples are presented.



