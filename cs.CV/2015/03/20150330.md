# Arxiv Papers in cs.CV on 2015-03-30
### Fast Optimal Transport Averaging of Neuroimaging Data
- **Arxiv ID**: http://arxiv.org/abs/1503.08596v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.08596v2)
- **Published**: 2015-03-30 08:48:08+00:00
- **Updated**: 2015-04-10 01:47:02+00:00
- **Authors**: Alexandre Gramfort, Gabriel Peyr√©, Marco Cuturi
- **Comment**: Information Processing in Medical Imaging (IPMI), Jun 2015, Isle of
  Skye, United Kingdom. Springer, 2015
- **Journal**: None
- **Summary**: Knowing how the Human brain is anatomically and functionally organized at the level of a group of healthy individuals or patients is the primary goal of neuroimaging research. Yet computing an average of brain imaging data defined over a voxel grid or a triangulation remains a challenge. Data are large, the geometry of the brain is complex and the between subjects variability leads to spatially or temporally non-overlapping effects of interest. To address the problem of variability, data are commonly smoothed before group linear averaging. In this work we build on ideas originally introduced by Kantorovich to propose a new algorithm that can average efficiently non-normalized data defined over arbitrary discrete domains using transportation metrics. We show how Kantorovich means can be linked to Wasserstein barycenters in order to take advantage of an entropic smoothing approach. It leads to a smooth convex optimization problem and an algorithm with strong convergence guarantees. We illustrate the versatility of this tool and its empirical behavior on functional neuroimaging data, functional MRI and magnetoencephalography (MEG) source estimates, defined on voxel grids and triangulations of the folded cortical surface.



### Visual Saliency Based on Multiscale Deep Features
- **Arxiv ID**: http://arxiv.org/abs/1503.08663v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.08663v3)
- **Published**: 2015-03-30 13:21:09+00:00
- **Updated**: 2015-04-10 06:40:46+00:00
- **Authors**: Guanbin Li, Yizhou Yu
- **Comment**: To appear in CVPR 2015
- **Journal**: None
- **Summary**: Visual saliency is a fundamental problem in both cognitive and computational sciences, including computer vision. In this CVPR 2015 paper, we discover that a high-quality visual saliency model can be trained with multiscale features extracted using a popular deep learning architecture, convolutional neural networks (CNNs), which have had many successes in visual recognition tasks. For learning such saliency models, we introduce a neural network architecture, which has fully connected layers on top of CNNs responsible for extracting features at three different scales. We then propose a refinement method to enhance the spatial coherence of our saliency results. Finally, aggregating multiple saliency maps computed for different levels of image segmentation can further boost the performance, yielding saliency maps better than those generated from a single segmentation. To promote further research and evaluation of visual saliency models, we also construct a new large database of 4447 challenging images and their pixelwise saliency annotation. Experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance on all public benchmarks, improving the F-Measure by 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset (HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively on these two datasets.



### Label-Embedding for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1503.08677v2
- **DOI**: 10.1109/TPAMI.2015.2487986
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.08677v2)
- **Published**: 2015-03-30 14:04:34+00:00
- **Updated**: 2015-10-01 10:48:38+00:00
- **Authors**: Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid
- **Comment**: IEEE TPAMI preprint
- **Journal**: None
- **Summary**: Attributes act as intermediate representations that enable parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function that measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. Label embedding enjoys a built-in ability to leverage alternative sources of information instead of or in addition to attributes, such as e.g. class hierarchies or textual descriptions. Moreover, label embedding encompasses the whole range of learning settings from zero-shot learning to regular learning with a large number of labeled examples.



### Globally Tuned Cascade Pose Regression via Back Propagation with Application in 2D Face Pose Estimation and Heart Segmentation in 3D CT Images
- **Arxiv ID**: http://arxiv.org/abs/1503.08843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.08843v1)
- **Published**: 2015-03-30 20:17:23+00:00
- **Updated**: 2015-03-30 20:17:23+00:00
- **Authors**: Peng Sun, James K. Min, Guanglei Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, a successful pose estimation algorithm, called Cascade Pose Regression (CPR), was proposed in the literature. Trained over Pose Index Feature, CPR is a regressor ensemble that is similar to Boosting. In this paper we show how CPR can be represented as a Neural Network. Specifically, we adopt a Graph Transformer Network (GTN) representation and accordingly train CPR with Back Propagation (BP) that permits globally tuning. In contrast, previous CPR literature only took a layer wise training without any post fine tuning. We empirically show that global training with BP outperforms layer-wise (pre-)training. Our CPR-GTN adopts a Multi Layer Percetron as the regressor, which utilized sparse connection to learn local image feature representation. We tested the proposed CPR-GTN on 2D face pose estimation problem as in previous CPR literature. Besides, we also investigated the possibility of extending CPR-GTN to 3D pose estimation by doing experiments using 3D Computed Tomography dataset for heart segmentation.



### Reconciling saliency and object center-bias hypotheses in explaining free-viewing fixations
- **Arxiv ID**: http://arxiv.org/abs/1503.08853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.08853v1)
- **Published**: 2015-03-30 21:07:53+00:00
- **Updated**: 2015-03-30 21:07:53+00:00
- **Authors**: Ali Borji, James Tanner
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting where people look in natural scenes has attracted a lot of interest in computer vision and computational neuroscience over the past two decades. Two seemingly contrasting categories of cues have been proposed to influence where people look: \textit{low-level image saliency} and \textit{high-level semantic information}. Our first contribution is to take a detailed look at these cues to confirm the hypothesis proposed by Henderson~\cite{henderson1993eye} and Nuthmann \& Henderson~\cite{nuthmann2010object} that observers tend to look at the center of objects. We analyzed fixation data for scene free-viewing over 17 observers on 60 fully annotated images with various types of objects. Images contained different types of scenes, such as natural scenes, line drawings, and 3D rendered scenes. Our second contribution is to propose a simple combined model of low-level saliency and object center-bias that outperforms each individual component significantly over our data, as well as on the OSIE dataset by Xu et al.~\cite{xu2014predicting}. The results reconcile saliency with object center-bias hypotheses and highlight that both types of cues are important in guiding fixations. Our work opens new directions to understand strategies that humans use in observing scenes and objects, and demonstrates the construction of combined models of low-level saliency and high-level object-based information.



