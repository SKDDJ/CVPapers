# Arxiv Papers in cs.CV on 2015-03-23
### A Comparative Analysis of Tensor Decomposition Models Using Hyper Spectral Image
- **Arxiv ID**: http://arxiv.org/abs/1503.06561v1
- **DOI**: None
- **Categories**: **cs.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1503.06561v1)
- **Published**: 2015-03-23 09:08:50+00:00
- **Updated**: 2015-03-23 09:08:50+00:00
- **Authors**: Ankit Gupta, Ashish Oberoi
- **Comment**: 7 pages, 3 figures,1 table
- **Journal**: International Journal of Computer Science Trends and Technology
  (IJCST) V3(2): Page(5-11) Mar-Apr 2015. ISSN: 2347-8578
- **Summary**: Hyper spectral imaging is a remote sensing technology, providing variety of applications such as material identification, space object identification, planetary exploitation etc. It deals with capturing continuum of images of the earth surface from different angles. Due to the multidimensional nature of the image, multi-way arrays are one of the possible solutions for analyzing hyper spectral data. This multi-way array is called tensor. Our approach deals with implementing three decomposition models LMLRA, BTD and CPD to the sample data for choosing the best decomposition of the data set. The results have proved that Block Term Decomposition (BTD) is the best tensor model for decomposing the hyper spectral image in to resultant factor matrices.



### Superpixelizing Binary MRF for Image Labeling Problems
- **Arxiv ID**: http://arxiv.org/abs/1503.06642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.06642v1)
- **Published**: 2015-03-23 14:09:09+00:00
- **Updated**: 2015-03-23 14:09:09+00:00
- **Authors**: Junyan Wang, Sai-Kit Yeung
- **Comment**: None
- **Journal**: None
- **Summary**: Superpixels have become prevalent in computer vision. They have been used to achieve satisfactory performance at a significantly smaller computational cost for various tasks. People have also combined superpixels with Markov random field (MRF) models. However, it often takes additional effort to formulate MRF on superpixel-level, and to the best of our knowledge there exists no principled approach to obtain this formulation. In this paper, we show how generic pixel-level binary MRF model can be solved in the superpixel space. As the main contribution of this paper, we show that a superpixel-level MRF can be derived from the pixel-level MRF by substituting the superpixel representation of the pixelwise label into the original pixel-level MRF energy. The resultant superpixel-level MRF energy also remains submodular for a submodular pixel-level MRF. The derived formula hence gives us a handy way to formulate MRF energy in superpixel-level. In the experiments, we demonstrate the efficacy of our approach on several computer vision problems.



### A novel pLSA based Traffic Signs Classification System
- **Arxiv ID**: http://arxiv.org/abs/1503.06643v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1503.06643v1)
- **Published**: 2015-03-23 14:10:44+00:00
- **Updated**: 2015-03-23 14:10:44+00:00
- **Authors**: Mrinal Haloi
- **Comment**: APMediaCast-2015, Bali, Indonesia
- **Journal**: None
- **Summary**: In this work we developed a novel and fast traffic sign recognition system, a very important part for advanced driver assistance system and for autonomous driving. Traffic signs play a very vital role in safe driving and avoiding accident. We have used image processing and topic discovery model pLSA to tackle this challenging multiclass classification problem. Our algorithm is consist of two parts, shape classification and sign classification for improved accuracy. For processing and representation of image we have used bag of features model with SIFT local descriptor. Where a visual vocabulary of size 300 words are formed using k-means codebook formation algorithm. We exploited the concept that every image is a collection of visual topics and images having same topics will belong to same category. Our algorithm is tested on German traffic sign recognition benchmark (GTSRB) and gives very promising result near to existing state of the art techniques.



### Vehicle Local Position Estimation System
- **Arxiv ID**: http://arxiv.org/abs/1503.06648v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1503.06648v1)
- **Published**: 2015-03-23 14:14:10+00:00
- **Updated**: 2015-03-23 14:14:10+00:00
- **Authors**: Mrinal Haloi, Dinesh Babu Jayagopi
- **Comment**: Accepted in ICVES-2014, Hyderabad, India
- **Journal**: None
- **Summary**: In this paper, a robust vehicle local position estimation with the help of single camera sensor and GPS is presented. A modified Inverse Perspective Mapping, illuminant Invariant techniques and object detection based approach is used to localize the vehicle in the road. Vehicles current lane, its position from road boundary and other cars are used to define its local position. For this purpose Lane markings are detected using a Laplacian edge feature, robust to shadowing. Effect of shadowing and extra sun light are removed using Lab color space and illuminant invariant techniques. Lanes are assumed to be as parabolic model and fitted using robust RANSAC. This method can reliably detect all lanes of the road, estimate lane departure angle and local position of vehicle relative to lanes, road boundary and other cars. Different type of obstacle like pedestrians, vehicles are detected using HOG feature based deformable part model.



### Video-Based Action Recognition Using Rate-Invariant Analysis of Covariance Trajectories
- **Arxiv ID**: http://arxiv.org/abs/1503.06699v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.06699v2)
- **Published**: 2015-03-23 16:08:08+00:00
- **Updated**: 2015-04-09 20:18:46+00:00
- **Authors**: Zhengwu Zhang, Jingyong Su, Eric Klassen, Huiling Le, Anuj Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Statistical classification of actions in videos is mostly performed by extracting relevant features, particularly covariance features, from image frames and studying time series associated with temporal evolutions of these features. A natural mathematical representation of activity videos is in form of parameterized trajectories on the covariance manifold, i.e. the set of symmetric, positive-definite matrices (SPDMs). The variable execution-rates of actions implies variable parameterizations of the resulting trajectories, and complicates their classification. Since action classes are invariant to execution rates, one requires rate-invariant metrics for comparing trajectories. A recent paper represented trajectories using their transported square-root vector fields (TSRVFs), defined by parallel translating scaled-velocity vectors of trajectories to a reference tangent space on the manifold. To avoid arbitrariness of selecting the reference and to reduce distortion introduced during this mapping, we develop a purely intrinsic approach where SPDM trajectories are represented by redefining their TSRVFs at the starting points of the trajectories, and analyzed as elements of a vector bundle on the manifold. Using a natural Riemannain metric on vector bundles of SPDMs, we compute geodesic paths and geodesic distances between trajectories in the quotient space of this vector bundle, with respect to the re-parameterization group. This makes the resulting comparison of trajectories invariant to their re-parameterization. We demonstrate this framework on two applications involving video classification: visual speech recognition or lip-reading and hand-gesture recognition. In both cases we achieve results either comparable to or better than the current literature.



### Non-contact transmittance photoplethysmographic imaging (PPGI) for long-distance cardiovascular monitoring
- **Arxiv ID**: http://arxiv.org/abs/1503.06775v1
- **DOI**: None
- **Categories**: **physics.optics**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1503.06775v1)
- **Published**: 2015-03-23 19:19:40+00:00
- **Updated**: 2015-03-23 19:19:40+00:00
- **Authors**: Robert Amelard, Christian Scharfenberger, Farnoud Kazemzadeh, Kaylen J. Pfisterer, Bill S. Lin, Alexander Wong, David A. Clausi
- **Comment**: 13 pages, 6 figures, submitted to Nature Scientific Reports, for
  associated video files see
  http://vip.uwaterloo.ca/publications/non-contact-transmittance-photoplethysmographic-imaging-ppgi-long-distance
- **Journal**: None
- **Summary**: Photoplethysmography (PPG) devices are widely used for monitoring cardiovascular function. However, these devices require skin contact, which restrict their use to at-rest short-term monitoring using single-point measurements. Photoplethysmographic imaging (PPGI) has been recently proposed as a non-contact monitoring alternative by measuring blood pulse signals across a spatial region of interest. Existing systems operate in reflectance mode, of which many are limited to short-distance monitoring and are prone to temporal changes in ambient illumination. This paper is the first study to investigate the feasibility of long-distance non-contact cardiovascular monitoring at the supermeter level using transmittance PPGI. For this purpose, a novel PPGI system was designed at the hardware and software level using ambient correction via temporally coded illumination (TCI) and signal processing for PPGI signal extraction. Experimental results show that the processing steps yield a substantially more pulsatile PPGI signal than the raw acquired signal, resulting in statistically significant increases in correlation to ground-truth PPG in both short- ($p \in [<0.0001, 0.040]$) and long-distance ($p \in [<0.0001, 0.056]$) monitoring. The results support the hypothesis that long-distance heart rate monitoring is feasible using transmittance PPGI, allowing for new possibilities of monitoring cardiovascular function in a non-contact manner.



### Factorization of View-Object Manifolds for Joint Object Recognition and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1503.06813v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.06813v2)
- **Published**: 2015-03-23 20:05:36+00:00
- **Updated**: 2015-04-13 02:59:41+00:00
- **Authors**: Haopeng Zhang, Tarek El-Gaaly, Ahmed Elgammal, Zhiguo Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Due to large variations in shape, appearance, and viewing conditions, object recognition is a key precursory challenge in the fields of object manipulation and robotic/AI visual reasoning in general. Recognizing object categories, particular instances of objects and viewpoints/poses of objects are three critical subproblems robots must solve in order to accurately grasp/manipulate objects and reason about their environments. Multi-view images of the same object lie on intrinsic low-dimensional manifolds in descriptor spaces (e.g. visual/depth descriptor spaces). These object manifolds share the same topology despite being geometrically different. Each object manifold can be represented as a deformed version of a unified manifold. The object manifolds can thus be parameterized by its homeomorphic mapping/reconstruction from the unified manifold. In this work, we develop a novel framework to jointly solve the three challenging recognition sub-problems, by explicitly modeling the deformations of object manifolds and factorizing it in a view-invariant space for recognition. We perform extensive experiments on several challenging datasets and achieve state-of-the-art results.



