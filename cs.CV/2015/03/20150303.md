# Arxiv Papers in cs.CV on 2015-03-03
### Context Forest for efficient object detection with large mixture models
- **Arxiv ID**: http://arxiv.org/abs/1503.00787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.00787v1)
- **Published**: 2015-03-03 00:20:58+00:00
- **Updated**: 2015-03-03 00:20:58+00:00
- **Authors**: Davide Modolo, Alexander Vezhnevets, Vittorio Ferrari
- **Comment**: None
- **Journal**: None
- **Summary**: We present Context Forest (ConF), a technique for predicting properties of the objects in an image based on its global appearance. Compared to standard nearest-neighbour techniques, ConF is more accurate, fast and memory efficient. We train ConF to predict which aspects of an object class are likely to appear in a given image (e.g. which viewpoint). This enables to speed-up multi-component object detectors, by automatically selecting the most relevant components to run on that image. This is particularly useful for detectors trained from large datasets, which typically need many components to fully absorb the data and reach their peak performance. ConF provides a speed-up of 2x for the DPM detector [1] and of 10x for the EE-SVM detector [2]. To show ConF's generality, we also train it to predict at which locations objects are likely to appear in an image. Incorporating this information in the detector score improves mAP performance by about 2% by removing false positive detections in unlikely locations.



### A Survey On Video Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/1503.00843v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1503.00843v1)
- **Published**: 2015-03-03 07:17:46+00:00
- **Updated**: 2015-03-03 07:17:46+00:00
- **Authors**: Sowmya K. N., H. R. Chennamma
- **Comment**: 11 pages, 3 figures, International Journal of Computer Engineering
  and Applications, Volume IX, Issue II, February 2015
- **Journal**: International Journal of Computer Engineering and Applications,
  Volume IX, Issue II, pp. 17-27, February 2015
- **Summary**: The Digital Forgeries though not visibly identifiable to human perception it may alter or meddle with underlying natural statistics of digital content. Tampering involves fiddling with video content in order to cause damage or make unauthorized alteration/modification. Tampering detection in video is cumbersome compared to image when considering the properties of the video. Tampering impacts need to be studied and the applied technique/method is used to establish the factual information for legal course in judiciary. In this paper we give an overview of the prior literature and challenges involved in video forgery detection where passive approach is found.



### Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation
- **Arxiv ID**: http://arxiv.org/abs/1503.00848v4
- **DOI**: 10.1109/TPAMI.2016.2537320
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.00848v4)
- **Published**: 2015-03-03 07:58:22+00:00
- **Updated**: 2016-03-01 09:00:09+00:00
- **Authors**: Jordi Pont-Tuset, Pablo Arbelaez, Jonathan T. Barron, Ferran Marques, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a unified approach for bottom-up hierarchical image segmentation and object proposal generation for recognition, called Multiscale Combinatorial Grouping (MCG). For this purpose, we first develop a fast normalized cuts algorithm. We then propose a high-performance hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a grouping strategy that combines our multiscale regions into highly-accurate object proposals by exploring efficiently their combinatorial space. We also present Single-scale Combinatorial Grouping (SCG), a faster version of MCG that produces competitive proposals in under five second per image. We conduct an extensive and comprehensive empirical validation on the BSDS500, SegVOC12, SBD, and COCO datasets, showing that MCG produces state-of-the-art contours, hierarchical regions, and object proposals.



### Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning
- **Arxiv ID**: http://arxiv.org/abs/1503.00949v3
- **DOI**: 10.1109/TPAMI.2016.2535231
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.00949v3)
- **Published**: 2015-03-03 14:06:02+00:00
- **Updated**: 2016-02-22 20:26:43+00:00
- **Authors**: Ramazan Gokberk Cinbis, Jakob Verbeek, Cordelia Schmid
- **Comment**: To appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)
- **Journal**: None
- **Summary**: Object category localization is a challenging problem in computer vision. Standard supervised training requires bounding box annotations of object instances. This time-consuming annotation process is sidestepped in weakly supervised learning. In this case, the supervised information is restricted to binary labels that indicate the absence/presence of object instances in the image, without their locations. We follow a multiple-instance learning approach that iteratively trains the detector and infers the object locations in the positive training images. Our main contribution is a multi-fold multiple instance learning procedure, which prevents training from prematurely locking onto erroneous object locations. This procedure is particularly important when using high-dimensional representations, such as Fisher vectors and convolutional neural network features. We also propose a window refinement method, which improves the localization accuracy by incorporating an objectness prior. We present a detailed experimental evaluation using the PASCAL VOC 2007 dataset, which verifies the effectiveness of our approach.



### Anisotropic Diffusion in ITK
- **Arxiv ID**: http://arxiv.org/abs/1503.00992v1
- **DOI**: None
- **Categories**: **cs.CV**, math.AP
- **Links**: [PDF](http://arxiv.org/pdf/1503.00992v1)
- **Published**: 2015-03-03 16:17:16+00:00
- **Updated**: 2015-03-03 16:17:16+00:00
- **Authors**: Jean-Marie Mirebeau, Jérôme Fehrenbach, Laurent Risser, Shaza Tobji
- **Comment**: None
- **Journal**: None
- **Summary**: Anisotropic Non-Linear Diffusion is a powerful image processing technique, which allows to simultaneously remove the noise and enhance sharp features in two or three dimensional images. Anisotropic Diffusion is understood here in the sense of Weickert, meaning that diffusion tensors are anisotropic and reflect the local orientation of image features. This is in contrast with the non-linear diffusion filter of Perona and Malik, which only involves scalar diffusion coefficients, in other words isotropic diffusion tensors. In this paper, we present an anisotropic non-linear diffusion technique we implemented in ITK. This technique is based on a recent adaptive scheme making the diffusion stable and requiring limited numerical resources. (See supplementary data.)



### Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research
- **Arxiv ID**: http://arxiv.org/abs/1503.01070v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1503.01070v1)
- **Published**: 2015-03-03 19:22:01+00:00
- **Updated**: 2015-03-03 19:22:01+00:00
- **Authors**: Atousa Torabi, Christopher Pal, Hugo Larochelle, Aaron Courville
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: In this work, we introduce a dataset of video annotated with high quality natural language phrases describing the visual content in a given segment of time. Our dataset is based on the Descriptive Video Service (DVS) that is now encoded on many digital media products such as DVDs. DVS is an audio narration describing the visual elements and actions in a movie for the visually impaired. It is temporally aligned with the movie and mixed with the original movie soundtrack. We describe an automatic DVS segmentation and alignment method for movies, that enables us to scale up the collection of a DVS-derived dataset with minimal human intervention. Using this method, we have collected the largest DVS-derived dataset for video description of which we are aware. Our dataset currently includes over 84.6 hours of paired video/sentences from 92 DVDs and is growing.



### Learning Super-Resolution Jointly from External and Internal Examples
- **Arxiv ID**: http://arxiv.org/abs/1503.01138v3
- **DOI**: 10.1109/TIP.2015.2462113
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.01138v3)
- **Published**: 2015-03-03 21:48:48+00:00
- **Updated**: 2015-06-16 07:19:19+00:00
- **Authors**: Zhangyang Wang, Yingzhen Yang, Zhaowen Wang, Shiyu Chang, Jianchao Yang, Thomas S. Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Single image super-resolution (SR) aims to estimate a high-resolution (HR) image from a lowresolution (LR) input. Image priors are commonly learned to regularize the otherwise seriously ill-posed SR problem, either using external LR-HR pairs or internal similar patterns. We propose joint SR to adaptively combine the advantages of both external and internal SR methods. We define two loss functions using sparse coding based external examples, and epitomic matching based on internal examples, as well as a corresponding adaptive weight to automatically balance their contributions according to their reconstruction errors. Extensive SR results demonstrate the effectiveness of the proposed method over the existing state-of-the-art methods, and is also verified by our subjective evaluation study.



