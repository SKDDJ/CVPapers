# Arxiv Papers in cs.CV on 2015-03-09
### Fully Connected Deep Structured Networks
- **Arxiv ID**: http://arxiv.org/abs/1503.02351v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1503.02351v1)
- **Published**: 2015-03-09 01:08:00+00:00
- **Updated**: 2015-03-09 01:08:00+00:00
- **Authors**: Alexander G. Schwing, Raquel Urtasun
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks with many layers have recently been shown to achieve excellent results on many high-level tasks such as image classification, object detection and more recently also semantic segmentation. Particularly for semantic segmentation, a two-stage procedure is often employed. Hereby, convolutional networks are trained to provide good local pixel-wise features for the second step being traditionally a more global graphical model. In this work we unify this two-stage process into a single joint training algorithm. We demonstrate our method on the semantic image segmentation task and show encouraging results on the challenging PASCAL VOC 2012 dataset.



### Deep Human Parsing with Active Template Regression
- **Arxiv ID**: http://arxiv.org/abs/1503.02391v1
- **DOI**: 10.1109/TPAMI.2015.2408360
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.02391v1)
- **Published**: 2015-03-09 08:14:12+00:00
- **Updated**: 2015-03-09 08:14:12+00:00
- **Authors**: Xiaodan Liang, Si Liu, Xiaohui Shen, Jianchao Yang, Luoqi Liu, Jian Dong, Liang Lin, Shuicheng Yan
- **Comment**: This manuscript is the accepted version for IEEE Transactions on
  Pattern Analysis and Machine Intelligence (TPAMI) 2015
- **Journal**: None
- **Summary**: In this work, the human parsing task, namely decomposing a human image into semantic fashion/body regions, is formulated as an Active Template Regression (ATR) problem, where the normalized mask of each fashion/body item is expressed as the linear combination of the learned mask templates, and then morphed to a more precise mask with the active shape parameters, including position, scale and visibility of each semantic region. The mask template coefficients and the active shape parameters together can generate the human parsing results, and are thus called the structure outputs for human parsing. The deep Convolutional Neural Network (CNN) is utilized to build the end-to-end relation between the input human image and the structure outputs for human parsing. More specifically, the structure outputs are predicted by two separate networks. The first CNN network is with max-pooling, and designed to predict the template coefficients for each label mask, while the second CNN network is without max-pooling to preserve sensitivity to label mask position and accurately predict the active shape parameters. For a new image, the structure outputs of the two networks are fused to generate the probability of each label for each pixel, and super-pixel smoothing is finally used to refine the human parsing result. Comprehensive evaluations on a large dataset well demonstrate the significant superiority of the ATR framework over other state-of-the-arts for human parsing. In particular, the F1-score reaches $64.38\%$ by our ATR framework, significantly higher than $44.76\%$ based on the state-of-the-art algorithm.



### Representation Learning with Deep Extreme Learning Machines for Efficient Image Set Classification
- **Arxiv ID**: http://arxiv.org/abs/1503.02445v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.02445v3)
- **Published**: 2015-03-09 12:14:42+00:00
- **Updated**: 2015-04-01 10:29:09+00:00
- **Authors**: Muhammad Uzair, Faisal Shafait, Bernard Ghanem, Ajmal Mian
- **Comment**: None
- **Journal**: None
- **Summary**: Efficient and accurate joint representation of a collection of images, that belong to the same class, is a major research challenge for practical image set classification. Existing methods either make prior assumptions about the data structure, or perform heavy computations to learn structure from the data itself. In this paper, we propose an efficient image set representation that does not make any prior assumptions about the structure of the underlying data. We learn the non-linear structure of image sets with Deep Extreme Learning Machines (DELM) that are very efficient and generalize well even on a limited number of training samples. Extensive experiments on a broad range of public datasets for image set classification (Honda/UCSD, CMU Mobo, YouTube Celebrities, Celebrity-1000, ETH-80) show that the proposed algorithm consistently outperforms state-of-the-art image set classification methods both in terms of speed and accuracy.



### Brain Tumor Segmentation: A Comparative Analysis
- **Arxiv ID**: http://arxiv.org/abs/1503.02466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.02466v1)
- **Published**: 2015-03-09 13:15:05+00:00
- **Updated**: 2015-03-09 13:15:05+00:00
- **Authors**: Muhammad Ali Qadar, Yan Zhaowen
- **Comment**: 8 Pages, 8 Figues, International Journal of Computer Science (IJCSI)
- **Journal**: Muhammad Ali Qadar, Yan Zhaowen, Brain Tumor Segmentation: A
  Comparative Analysis, IJCSI International Journal of Computer Science Issues,
  Volume 11, Issue 6, No 1, November 2014,1694-0784
- **Summary**: Five different threshold segmentation based approaches have been reviewed and compared over here to extract the tumor from set of brain images. This research focuses on the analysis of image segmentation methods, a comparison of five semi-automated methods have been undertaken for evaluating their relative performance in the segmentation of tumor. Consequently, results are compared on the basis of quantitative and qualitative analysis of respective methods. The purpose of this study was to analytically identify the methods, most suitable for application for a particular genre of problems. The results show that of the region growing segmentation performed better than rest in most cases.



### MODS: Fast and Robust Method for Two-View Matching
- **Arxiv ID**: http://arxiv.org/abs/1503.02619v2
- **DOI**: 10.1016/j.cviu.2015.08.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.02619v2)
- **Published**: 2015-03-09 18:59:18+00:00
- **Updated**: 2016-05-01 14:44:35+00:00
- **Authors**: Dmytro Mishkin, Jiri Matas, Michal Perdoch
- **Comment**: Version accepted to CVIU. arXiv admin note: text overlap with
  arXiv:1306.3855
- **Journal**: None
- **Summary**: A novel algorithm for wide-baseline matching called MODS - Matching On Demand with view Synthesis - is presented. The MODS algorithm is experimentally shown to solve a broader range of wide-baseline problems than the state of the art while being nearly as fast as standard matchers on simple problems. The apparent robustness vs. speed trade-off is finessed by the use of progressively more time-consuming feature detectors and by on-demand generation of synthesized images that is performed until a reliable estimate of geometry is obtained.   We introduce an improved method for tentative correspondence selection, applicable both with and without view synthesis. A modification of the standard first to second nearest distance rule increases the number of correct matches by 5-20% at no additional computational cost.   Performance of the MODS algorithm is evaluated on several standard publicly available datasets, and on a new set of geometrically challenging wide baseline problems that is made public together with the ground truth. Experiments show that the MODS outperforms the state-of-the-art in robustness and speed. Moreover, MODS performs well on other classes of difficult two-view problems like matching of images from different modalities, with wide temporal baseline or with significant lighting changes.



### Global 6DOF Pose Estimation from Untextured 2D City Models
- **Arxiv ID**: http://arxiv.org/abs/1503.02675v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.3.7; I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/1503.02675v2)
- **Published**: 2015-03-09 20:18:19+00:00
- **Updated**: 2015-03-18 12:11:35+00:00
- **Authors**: Clemens Arth, Christian Pirchheim, Jonathan Ventura, Vincent Lepetit
- **Comment**: 9 pages excluding supplementary material
- **Journal**: None
- **Summary**: We propose a method for estimating the 3D pose for the camera of a mobile device in outdoor conditions, using only an untextured 2D model. Previous methods compute only a relative pose using a SLAM algorithm, or require many registered images, which are cumbersome to acquire. By contrast, our method returns an accurate, absolute camera pose in an absolute referential using simple 2D+height maps, which are broadly available, to refine a first estimate of the pose provided by the device's sensors. We show how to first estimate the camera absolute orientation from straight line segments, and then how to estimate the translation by aligning the 2D map with a semantic segmentation of the input image. We demonstrate the robustness and accuracy of our approach on a challenging dataset.



### Deep Hierarchical Parsing for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1503.02725v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.02725v2)
- **Published**: 2015-03-09 23:05:26+00:00
- **Updated**: 2015-03-30 20:03:01+00:00
- **Authors**: Abhishek Sharma, Oncel Tuzel, David W. Jacobs
- **Comment**: IEEE CVPR 2015
- **Journal**: None
- **Summary**: This paper proposes a learning-based approach to scene parsing inspired by the deep Recursive Context Propagation Network (RCPN). RCPN is a deep feed-forward neural network that utilizes the contextual information from the entire image, through bottom-up followed by top-down context propagation via random binary parse trees. This improves the feature representation of every super-pixel in the image for better classification into semantic categories. We analyze RCPN and propose two novel contributions to further improve the model. We first analyze the learning of RCPN parameters and discover the presence of bypass error paths in the computation graph of RCPN that can hinder contextual propagation. We propose to tackle this problem by including the classification loss of the internal nodes of the random parse trees in the original RCPN loss function. Secondly, we use an MRF on the parse tree nodes to model the hierarchical dependency present in the output. Both modifications provide performance boosts over the original RCPN and the new system achieves state-of-the-art performance on Stanford Background, SIFT-Flow and Daimler urban datasets.



### Video Compressive Sensing for Spatial Multiplexing Cameras using Motion-Flow Models
- **Arxiv ID**: http://arxiv.org/abs/1503.02727v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.02727v2)
- **Published**: 2015-03-09 23:08:44+00:00
- **Updated**: 2015-08-05 17:00:42+00:00
- **Authors**: Aswin C. Sankaranarayanan, Lina Xu, Christoph Studer, Yun Li, Kevin Kelly, Richard G. Baraniuk
- **Comment**: in SIAM Journal on Imaging Sciences, 2015
- **Journal**: None
- **Summary**: Spatial multiplexing cameras (SMCs) acquire a (typically static) scene through a series of coded projections using a spatial light modulator (e.g., a digital micro-mirror device) and a few optical sensors. This approach finds use in imaging applications where full-frame sensors are either too expensive (e.g., for short-wave infrared wavelengths) or unavailable. Existing SMC systems reconstruct static scenes using techniques from compressive sensing (CS). For videos, however, existing acquisition and recovery methods deliver poor quality. In this paper, we propose the CS multi-scale video (CS-MUVI) sensing and recovery framework for high-quality video acquisition and recovery using SMCs. Our framework features novel sensing matrices that enable the efficient computation of a low-resolution video preview, while enabling high-resolution video recovery using convex optimization. To further improve the quality of the reconstructed videos, we extract optical-flow estimates from the low-resolution previews and impose them as constraints in the recovery procedure. We demonstrate the efficacy of our CS-MUVI framework for a host of synthetic and real measured SMC video data, and we show that high-quality videos can be recovered at roughly $60\times$ compression.



