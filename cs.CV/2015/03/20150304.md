# Arxiv Papers in cs.CV on 2015-03-04
### Temporal Pyramid Pooling Based Convolutional Neural Networks for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1503.01224v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.01224v2)
- **Published**: 2015-03-04 05:18:18+00:00
- **Updated**: 2015-04-16 12:18:46+00:00
- **Authors**: Peng Wang, Yuanzhouhan Cao, Chunhua Shen, Lingqiao Liu, Heng Tao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Encouraged by the success of Convolutional Neural Networks (CNNs) in image classification, recently much effort is spent on applying CNNs to video based action recognition problems. One challenge is that video contains a varying number of frames which is incompatible to the standard input format of CNNs. Existing methods handle this issue either by directly sampling a fixed number of frames or bypassing this issue by introducing a 3D convolutional layer which conducts convolution in spatial-temporal domain.   To solve this issue, here we propose a novel network structure which allows an arbitrary number of frames as the network input. The key of our solution is to introduce a module consisting of an encoding layer and a temporal pyramid pooling layer. The encoding layer maps the activation from previous layers to a feature vector suitable for pooling while the temporal pyramid pooling layer converts multiple frame-level activations into a fixed-length video-level representation. In addition, we adopt a feature concatenation layer which combines appearance information and motion information. Compared with the frame sampling strategy, our method avoids the risk of missing any important frames. Compared with the 3D convolutional method which requires a huge video dataset for network training, our model can be learned on a small target dataset because we can leverage the off-the-shelf image-level CNN for model parameter initialization. Experiments on two challenging datasets, Hollywood2 and HMDB51, demonstrate that our method achieves superior performance over state-of-the-art methods while requiring much fewer training data.



### Bethe Learning of Conditional Random Fields via MAP Decoding
- **Arxiv ID**: http://arxiv.org/abs/1503.01228v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1503.01228v1)
- **Published**: 2015-03-04 05:41:29+00:00
- **Updated**: 2015-03-04 05:41:29+00:00
- **Authors**: Kui Tang, Nicholas Ruozzi, David Belanger, Tony Jebara
- **Comment**: 19 pages (9 supplementary), 10 figures (3 supplementary)
- **Journal**: None
- **Summary**: Many machine learning tasks can be formulated in terms of predicting structured outputs. In frameworks such as the structured support vector machine (SVM-Struct) and the structured perceptron, discriminative functions are learned by iteratively applying efficient maximum a posteriori (MAP) decoding. However, maximum likelihood estimation (MLE) of probabilistic models over these same structured spaces requires computing partition functions, which is generally intractable. This paper presents a method for learning discrete exponential family models using the Bethe approximation to the MLE. Remarkably, this problem also reduces to iterative (MAP) decoding. This connection emerges by combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on a convex dual objective which circumvents the intractable partition function. The result is a new single loop algorithm MLE-Struct, which is substantially more efficient than previous double-loop methods for approximate maximum likelihood estimation. Our algorithm outperforms existing methods in experiments involving image segmentation, matching problems from vision, and a new dataset of university roommate assignments.



### A Novel Performance Evaluation Methodology for Single-Target Trackers
- **Arxiv ID**: http://arxiv.org/abs/1503.01313v3
- **DOI**: 10.1109/TPAMI.2016.2516982
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1503.01313v3)
- **Published**: 2015-03-04 14:12:17+00:00
- **Updated**: 2016-01-08 15:27:11+00:00
- **Authors**: Matej Kristan, Jiri Matas, Ales Leonardis, Tomas Vojir, Roman Pflugfelder, Gustavo Fernandez, Georg Nebehay, Fatih Porikli, Luka Cehovin
- **Comment**: Final version (Accepted), IEEE Pattern Analysis and Machine
  Intelligence, 2016
- **Journal**: None
- **Summary**: This paper addresses the problem of single-target tracker performance evaluation. We consider the performance measures, the dataset and the evaluation system to be the most important components of tracker evaluation and propose requirements for each of them. The requirements are the basis of a new evaluation methodology that aims at a simple and easily interpretable tracker comparison. The ranking-based methodology addresses tracker equivalence in terms of statistical significance and practical differences. A fully-annotated dataset with per-frame annotations with several visual attributes is introduced. The diversity of its visual properties is maximized in a novel way by clustering a large number of videos according to their visual attributes. This makes it the most sophistically constructed and annotated dataset to date. A multi-platform evaluation system allowing easy integration of third-party trackers is presented as well. The proposed evaluation methodology was tested on the VOT2014 challenge on the new dataset and 38 trackers, making it the largest benchmark to date. Most of the tested trackers are indeed state-of-the-art since they outperform the standard baselines, resulting in a highly-challenging benchmark. An exhaustive analysis of the dataset from the perspective of tracking difficulty is carried out. To facilitate tracker comparison a new performance visualization technique is proposed.



### A Hierarchical Approach for Joint Multi-view Object Pose Estimation and Categorization
- **Arxiv ID**: http://arxiv.org/abs/1503.01393v1
- **DOI**: 10.1109/ICRA.2014.6907665
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1503.01393v1)
- **Published**: 2015-03-04 17:17:48+00:00
- **Updated**: 2015-03-04 17:17:48+00:00
- **Authors**: Mete Ozay, Krzysztof Walas, Ales Leonardis
- **Comment**: 7 Figures
- **Journal**: Proceedings of IEEE International Conference on Robotics and
  Automation (ICRA), pp. 5480 - 5487, Hong Kong, 2014
- **Summary**: We propose a joint object pose estimation and categorization approach which extracts information about object poses and categories from the object parts and compositions constructed at different layers of a hierarchical object representation algorithm, namely Learned Hierarchy of Parts (LHOP). In the proposed approach, we first employ the LHOP to learn hierarchical part libraries which represent entity parts and compositions across different object categories and views. Then, we extract statistical and geometric features from the part realizations of the objects in the images in order to represent the information about object pose and category at each different layer of the hierarchy. Unlike the traditional approaches which consider specific layers of the hierarchies in order to extract information to perform specific tasks, we combine the information extracted at different layers to solve a joint object pose estimation and categorization problem using distributed optimization algorithms. We examine the proposed generative-discriminative learning approach and the algorithms on two benchmark 2-D multi-view image datasets. The proposed approach and the algorithms outperform state-of-the-art classification, regression and feature extraction algorithms. In addition, the experimental results shed light on the relationship between object categorization, pose estimation and the part realizations observed at different layers of the hierarchy.



### Partial Sum Minimization of Singular Values in Robust PCA: Algorithm and Applications
- **Arxiv ID**: http://arxiv.org/abs/1503.01444v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1503.01444v2)
- **Published**: 2015-03-04 20:14:35+00:00
- **Updated**: 2015-08-13 12:51:08+00:00
- **Authors**: Tae-Hyun Oh, Yu-Wing Tai, Jean-Charles Bazin, Hyeongwoo Kim, In So Kweon
- **Comment**: Accepted in Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI). To appear
- **Journal**: None
- **Summary**: Robust Principal Component Analysis (RPCA) via rank minimization is a powerful tool for recovering underlying low-rank structure of clean data corrupted with sparse noise/outliers. In many low-level vision problems, not only it is known that the underlying structure of clean data is low-rank, but the exact rank of clean data is also known. Yet, when applying conventional rank minimization for those problems, the objective function is formulated in a way that does not fully utilize a priori target rank information about the problems. This observation motivates us to investigate whether there is a better alternative solution when using rank minimization. In this paper, instead of minimizing the nuclear norm, we propose to minimize the partial sum of singular values, which implicitly encourages the target rank constraint. Our experimental analyses show that, when the number of samples is deficient, our approach leads to a higher success rate than conventional rank minimization, while the solutions obtained by the two approaches are almost identical when the number of samples is more than sufficient. We apply our approach to various low-level vision problems, e.g. high dynamic range imaging, motion edge detection, photometric stereo, image alignment and recovery, and show that our results outperform those obtained by the conventional nuclear norm rank minimization method.



