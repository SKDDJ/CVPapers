# Arxiv Papers in cs.CV on 2015-11-18
### Competitive Multi-scale Convolution
- **Arxiv ID**: http://arxiv.org/abs/1511.05635v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.05635v1)
- **Published**: 2015-11-18 01:19:00+00:00
- **Updated**: 2015-11-18 01:19:00+00:00
- **Authors**: Zhibin Liao, Gustavo Carneiro
- **Comment**: None
- **Journal**: Pattern Recognition 71 (2017), 94-105
- **Summary**: In this paper, we introduce a new deep convolutional neural network (ConvNet) module that promotes competition among a set of multi-scale convolutional filters. This new module is inspired by the inception module, where we replace the original collaborative pooling stage (consisting of a concatenation of the multi-scale filter outputs) by a competitive pooling represented by a maxout activation unit. This extension has the following two objectives: 1) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model, which has been shown to facilitate the training of complex learning problems; and 2) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters. We show that the use of our proposed module in typical deep ConvNets produces classification results that are either better than or comparable to the state of the art on the following benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and SVHN.



### A New Smooth Approximation to the Zero One Loss with a Probabilistic Interpretation
- **Arxiv ID**: http://arxiv.org/abs/1511.05643v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.05643v1)
- **Published**: 2015-11-18 02:31:16+00:00
- **Updated**: 2015-11-18 02:31:16+00:00
- **Authors**: Md Kamrul Hasan, Christopher J. Pal
- **Comment**: 32 pages, 7 figures, 15 tables
- **Journal**: None
- **Summary**: We examine a new form of smooth approximation to the zero one loss in which learning is performed using a reformulation of the widely used logistic function. Our approach is based on using the posterior mean of a novel generalized Beta-Bernoulli formulation. This leads to a generalized logistic function that approximates the zero one loss, but retains a probabilistic formulation conferring a number of useful properties. The approach is easily generalized to kernel logistic regression and easily integrated into methods for structured prediction. We present experiments in which we learn such models using an optimization method consisting of a combination of gradient descent and coordinate descent using localized grid search so as to escape from local minima. Our experiments indicate that optimization quality is improved when learning meta-parameters are themselves optimized using a validation set. Our experiments show improved performance relative to widely used logistic and hinge loss methods on a wide variety of problems ranging from standard UC Irvine and libSVM evaluation datasets to product review predictions and a visual information extraction task. We observe that the approach: 1) is more robust to outliers compared to the logistic and hinge losses; 2) outperforms comparable logistic and max margin models on larger scale benchmark problems; 3) when combined with Gaussian- Laplacian mixture prior on parameters the kernelized version of our formulation yields sparser solutions than Support Vector Machine classifiers; and 4) when integrated into a probabilistic structured prediction technique our approach provides more accurate probabilities yielding improved inference and increasing information extraction performance.



### Super-Resolution with Deep Convolutional Sufficient Statistics
- **Arxiv ID**: http://arxiv.org/abs/1511.05666v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05666v4)
- **Published**: 2015-11-18 06:24:00+00:00
- **Updated**: 2016-03-01 18:26:13+00:00
- **Authors**: Joan Bruna, Pablo Sprechmann, Yann LeCun
- **Comment**: None
- **Journal**: None
- **Summary**: Inverse problems in image and audio, and super-resolution in particular, can be seen as high-dimensional structured prediction problems, where the goal is to characterize the conditional distribution of a high-resolution output given its low-resolution corrupted observation. When the scaling ratio is small, point estimates achieve impressive performance, but soon they suffer from the regression-to-the-mean problem, result of their inability to capture the multi-modality of this conditional distribution. Modeling high-dimensional image and audio distributions is a hard task, requiring both the ability to model complex geometrical structures and textured regions. In this paper, we propose to use as conditional model a Gibbs distribution, where its sufficient statistics are given by deep convolutional neural networks. The features computed by the network are stable to local deformation, and have reduced variance when the input is a stationary texture. These properties imply that the resulting sufficient statistics minimize the uncertainty of the target signals given the degraded observations, while being highly informative. The filters of the CNN are initialized by multiscale complex wavelets, and then we propose an algorithm to fine-tune them by estimating the gradient of the conditional log-likelihood, which bears some similarities with Generative Adversarial Networks. We evaluate experimentally the proposed approach in the image super-resolution task, but the approach is general and could be used in other challenging ill-posed problems such as audio bandwidth extension.



### Compositional Memory for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1511.05676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05676v1)
- **Published**: 2015-11-18 07:25:16+00:00
- **Updated**: 2015-11-18 07:25:16+00:00
- **Authors**: Aiwen Jiang, Fang Wang, Fatih Porikli, Yi Li
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) emerges as one of the most fascinating topics in computer vision recently. Many state of the art methods naively use holistic visual features with language features into a Long Short-Term Memory (LSTM) module, neglecting the sophisticated interaction between them. This coarse modeling also blocks the possibilities of exploring finer-grained local features that contribute to the question answering dynamically over time.   This paper addresses this fundamental problem by directly modeling the temporal dynamics between language and all possible local image patches. When traversing the question words sequentially, our end-to-end approach explicitly fuses the features associated to the words and the ones available at multiple local patches in an attention mechanism, and further combines the fused information to generate dynamic messages, which we call episode. We then feed the episodes to a standard question answering module together with the contextual visual information and linguistic information. Motivated by recent practices in deep learning, we use auxiliary loss functions during training to improve the performance. Our experiments on two latest public datasets suggest that our method has a superior performance. Notably, on the DARQUAR dataset we advanced the state of the art by 6$\%$, and we also evaluated our approach on the most recent MSCOCO-VQA dataset.



### Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction
- **Arxiv ID**: http://arxiv.org/abs/1511.05756v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.05756v1)
- **Published**: 2015-11-18 12:30:57+00:00
- **Updated**: 2015-11-18 12:30:57+00:00
- **Authors**: Hyeonwoo Noh, Paul Hongsuck Seo, Bohyung Han
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle image question answering (ImageQA) problem by learning a convolutional neural network (CNN) with a dynamic parameter layer whose weights are determined adaptively based on questions. For the adaptive parameter prediction, we employ a separate parameter prediction network, which consists of gated recurrent unit (GRU) taking a question as its input and a fully-connected layer generating a set of candidate weights as its output. However, it is challenging to construct a parameter prediction network for a large number of parameters in the fully-connected dynamic parameter layer of the CNN. We reduce the complexity of this problem by incorporating a hashing technique, where the candidate weights given by the parameter prediction network are selected using a predefined hash function to determine individual weights in the dynamic parameter layer. The proposed network---joint network with the CNN for ImageQA and the parameter prediction network---is trained end-to-end through back-propagation, where its weights are initialized using a pre-trained CNN and GRU. The proposed algorithm illustrates the state-of-the-art performance on all available public ImageQA benchmarks.



### Labeled pupils in the wild: A dataset for studying pupil detection in unconstrained environments
- **Arxiv ID**: http://arxiv.org/abs/1511.05768v1
- **DOI**: 10.1145/2857491.2857520
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05768v1)
- **Published**: 2015-11-18 13:30:55+00:00
- **Updated**: 2015-11-18 13:30:55+00:00
- **Authors**: Marc Tonsen, Xucong Zhang, Yusuke Sugano, Andreas Bulling
- **Comment**: None
- **Journal**: None
- **Summary**: We present labelled pupils in the wild (LPW), a novel dataset of 66 high-quality, high-speed eye region videos for the development and evaluation of pupil detection algorithms. The videos in our dataset were recorded from 22 participants in everyday locations at about 95 FPS using a state-of-the-art dark-pupil head-mounted eye tracker. They cover people with different ethnicities, a diverse set of everyday indoor and outdoor illumination environments, as well as natural gaze direction distributions. The dataset also includes participants wearing glasses, contact lenses, as well as make-up. We benchmark five state-of-the-art pupil detection algorithms on our dataset with respect to robustness and accuracy. We further study the influence of image resolution, vision aids, as well as recording location (indoor, outdoor) on pupil detection performance. Our evaluations provide valuable insights into the general pupil detection problem and allow us to identify key challenges for robust pupil detection on head-mounted eye trackers.



### From Pose to Activity: Surveying Datasets and Introducing CONVERSE
- **Arxiv ID**: http://arxiv.org/abs/1511.05788v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05788v2)
- **Published**: 2015-11-18 14:03:55+00:00
- **Updated**: 2015-11-19 13:04:09+00:00
- **Authors**: Michael Edwards, Jingjing Deng, Xianghua Xie
- **Comment**: Presentation of pose-based conversational human interaction dataset,
  review of current appearance and depth based action recognition datasets,
  public dataset, 38 pages
- **Journal**: None
- **Summary**: We present a review on the current state of publicly available datasets within the human action recognition community; highlighting the revival of pose based methods and recent progress of understanding person-person interaction modeling. We categorize datasets regarding several key properties for usage as a benchmark dataset; including the number of class labels, ground truths provided, and application domain they occupy. We also consider the level of abstraction of each dataset; grouping those that present actions, interactions and higher level semantic activities. The survey identifies key appearance and pose based datasets, noting a tendency for simplistic, emphasized, or scripted action classes that are often readily definable by a stable collection of sub-action gestures. There is a clear lack of datasets that provide closely related actions, those that are not implicitly identified via a series of poses and gestures, but rather a dynamic set of interactions. We therefore propose a novel dataset that represents complex conversational interactions between two individuals via 3D pose. 8 pairwise interactions describing 7 separate conversation based scenarios were collected using two Kinect depth sensors. The intention is to provide events that are constructed from numerous primitive actions, interactions and motions, over a period of time; providing a set of subtle action classes that are more representative of the real world, and a challenge to currently developed recognition methodologies. We believe this is among one of the first datasets devoted to conversational interaction classification using 3D pose features and the attributed papers show this task is indeed possible. The full dataset is made publicly available to the research community at www.csvision.swansea.ac.uk/converse.



### Eigenspectra optoacoustic tomography achieves quantitative blood oxygenation imaging deep in tissues
- **Arxiv ID**: http://arxiv.org/abs/1511.05846v1
- **DOI**: 10.1038/ncomms12121
- **Categories**: **physics.med-ph**, cs.CV, physics.optics, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1511.05846v1)
- **Published**: 2015-11-18 15:56:13+00:00
- **Updated**: 2015-11-18 15:56:13+00:00
- **Authors**: Stratis Tzoumas, Antonio Nunes, Ivan Olefir, Stefan Stangl, Panagiotis Symvoulidis, Sarah Glasl, Christine Bayer, Gabriele Multhoff, Vasilis Ntziachristos
- **Comment**: None
- **Journal**: None
- **Summary**: Light propagating in tissue attains a spectrum that varies with location due to wavelength-dependent fluence attenuation by tissue optical properties, an effect that causes spectral corruption. Predictions of the spectral variations of light fluence in tissue are challenging since the spatial distribution of optical properties in tissue cannot be resolved in high resolution or with high accuracy by current methods. Spectral corruption has fundamentally limited the quantification accuracy of optical and optoacoustic methods and impeded the long sought-after goal of imaging blood oxygen saturation (sO2) deep in tissues; a critical but still unattainable target for the assessment of oxygenation in physiological processes and disease. We discover a new principle underlying light fluence in tissues, which describes the wavelength dependence of light fluence as an affine function of a few reference base spectra, independently of the specific distribution of tissue optical properties. This finding enables the introduction of a previously undocumented concept termed eigenspectra Multispectral Optoacoustic Tomography (eMSOT) that can effectively account for wavelength dependent light attenuation without explicit knowledge of the tissue optical properties. We validate eMSOT in more than 2000 simulations and with phantom and animal measurements. We find that eMSOT can quantitatively image tissue sO2 reaching in many occasions a better than 10-fold improved accuracy over conventional spectral optoacoustic methods. Then, we show that eMSOT can spatially resolve sO2 in muscle and tumor; revealing so far unattainable tissue physiology patterns. Last, we related eMSOT readings to cancer hypoxia and found congruence between eMSOT tumor sO2 images and tissue perfusion and hypoxia maps obtained by correlative histological analysis.



### Particular object retrieval with integral max-pooling of CNN activations
- **Arxiv ID**: http://arxiv.org/abs/1511.05879v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05879v2)
- **Published**: 2015-11-18 17:02:59+00:00
- **Updated**: 2016-02-24 15:14:34+00:00
- **Authors**: Giorgos Tolias, Ronan Sicre, Hervé Jégou
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, image representation built upon Convolutional Neural Network (CNN) has been shown to provide effective descriptors for image search, outperforming pre-CNN features as short-vector representations. Yet such models are not compatible with geometry-aware re-ranking methods and still outperformed, on some particular object retrieval benchmarks, by traditional image search systems relying on precise descriptor matching, geometric re-ranking, or query expansion. This work revisits both retrieval stages, namely initial search and re-ranking, by employing the same primitive information derived from the CNN. We build compact feature vectors that encode several image regions without the need to feed multiple inputs to the network. Furthermore, we extend integral images to handle max-pooling on convolutional layer activations, allowing us to efficiently localize matching objects. The resulting bounding box is finally used for image re-ranking. As a result, this paper significantly improves existing CNN-based recognition pipeline: We report for the first time results competing with traditional methods on the challenging Oxford5k and Paris6k datasets.



### Dense Human Body Correspondences Using Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.05904v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1511.05904v2)
- **Published**: 2015-11-18 18:36:54+00:00
- **Updated**: 2016-06-26 01:06:43+00:00
- **Authors**: Lingyu Wei, Qixing Huang, Duygu Ceylan, Etienne Vouga, Hao Li
- **Comment**: CVPR 2016 oral presentation
- **Journal**: None
- **Summary**: We propose a deep learning approach for finding dense correspondences between 3D scans of people. Our method requires only partial geometric information in the form of two depth maps or partial reconstructed surfaces, works for humans in arbitrary poses and wearing any clothing, does not require the two people to be scanned from similar viewpoints, and runs in real time. We use a deep convolutional neural network to train a feature descriptor on depth map pixels, but crucially, rather than training the network to solve the shape correspondence problem directly, we train it to solve a body region classification problem, modified to increase the smoothness of the learned descriptors near region boundaries. This approach ensures that nearby points on the human body are nearby in feature space, and vice versa, rendering the feature descriptor suitable for computing dense correspondences between the scans. We validate our method on real and synthetic data for both clothed and unclothed humans, and show that our correspondences are more robust than is possible with state-of-the-art unsupervised methods, and more accurate than those found using methods that require full watertight 3D geometry.



### Collecting and Annotating the Large Continuous Action Dataset
- **Arxiv ID**: http://arxiv.org/abs/1511.05914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05914v1)
- **Published**: 2015-11-18 19:16:58+00:00
- **Updated**: 2015-11-18 19:16:58+00:00
- **Authors**: Daniel Paul Barrett, Ran Xu, Haonan Yu, Jeffrey Mark Siskind
- **Comment**: None
- **Journal**: None
- **Summary**: We make available to the community a new dataset to support action-recognition research. This dataset is different from prior datasets in several key ways. It is significantly larger. It contains streaming video with long segments containing multiple action occurrences that often overlap in space and/or time. All actions were filmed in the same collection of backgrounds so that background gives little clue as to action class. We had five humans replicate the annotation of temporal extent of action occurrences labeled with their class and measured a surprisingly low level of intercoder agreement. A baseline experiment shows that recent state-of-the-art methods perform poorly on this dataset. This suggests that this will be a challenging dataset to foster advances in action-recognition research. This manuscript serves to describe the novel content and characteristics of the LCA dataset, present the design decisions made when filming the dataset, and document the novel methods employed to annotate the dataset.



### ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1511.05960v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05960v2)
- **Published**: 2015-11-18 20:59:50+00:00
- **Updated**: 2016-04-03 22:47:38+00:00
- **Authors**: Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, Ram Nevatia
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel attention based deep learning architecture for visual question answering task (VQA). Given an image and an image related natural language question, VQA generates the natural language answer for the question. Generating the correct answers requires the model's attention to focus on the regions corresponding to the question, because different questions inquire about the attributes of different image regions. We introduce an attention based configurable convolutional neural network (ABC-CNN) to learn such question-guided attention. ABC-CNN determines an attention map for an image-question pair by convolving the image feature map with configurable convolutional kernels derived from the question's semantics. We evaluate the ABC-CNN architecture on three benchmark VQA datasets: Toronto COCO-QA, DAQUAR, and VQA dataset. ABC-CNN model achieves significant improvements over state-of-the-art methods on these datasets. The question-guided attention generated by ABC-CNN is also shown to reflect the regions that are highly relevant to the questions.



### Active Object Localization with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1511.06015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06015v1)
- **Published**: 2015-11-18 22:55:46+00:00
- **Updated**: 2015-11-18 22:55:46+00:00
- **Authors**: Juan C. Caicedo, Svetlana Lazebnik
- **Comment**: IEEE ICCV 2015
- **Journal**: None
- **Summary**: We present an active detection model for localizing objects in scenes. The model is class-specific and allows an agent to focus attention on candidate regions for identifying the correct location of a target object. This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.



