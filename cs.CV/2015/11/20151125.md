# Arxiv Papers in cs.CV on 2015-11-25
### Principal Basis Analysis in Sparse Representation
- **Arxiv ID**: http://arxiv.org/abs/1511.07927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07927v1)
- **Published**: 2015-11-25 01:11:24+00:00
- **Updated**: 2015-11-25 01:11:24+00:00
- **Authors**: Hong Sun, Cheng-Wei Sang, Chen-Guang Liu
- **Comment**: The text propose a Principal Basis Analysis in Sparse Representation
  and apply the principal basis analysis to image denoising corrupted by
  Gaussian and non-Gaussian noises, showing better performances than some
  reference methods at suppressing strong noise and at preserving signal
  details;including 8 pages, 4 figures prepared using pdf according to the
  instructions to Authors
- **Journal**: None
- **Summary**: This article introduces a new signal analysis method, which can be interpreted as a principal component analysis in sparse decomposition of the signal. The method, called principal basis analysis, is based on a novel criterion: reproducibility of component which is an intrinsic characteristic of regularity in natural signals. We show how to measure reproducibility. Then we present the principal basis analysis method, which chooses, in a sparse representation of the signal, the components optimizing the reproducibility degree to build the so-called principal basis. With this principal basis, we show that the underlying signal pattern could be effectively extracted from corrupted data. As illustration, we apply the principal basis analysis to image denoising corrupted by Gaussian and non-Gaussian noises, showing better performances than some reference methods at suppressing strong noise and at preserving signal details.



### Video Tracking Using Learned Hierarchical Features
- **Arxiv ID**: http://arxiv.org/abs/1511.07940v1
- **DOI**: 10.1109/TIP.2015.2403231
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07940v1)
- **Published**: 2015-11-25 02:58:42+00:00
- **Updated**: 2015-11-25 02:58:42+00:00
- **Authors**: Li Wang, Ting Liu, Gang Wang, Kap Luk Chan, Qingxiong Yang
- **Comment**: 12 pages, 7 figures
- **Journal**: IEEE Transactions on Image Processing, vol. 24, no. 4, April 2015
- **Summary**: In this paper, we propose an approach to learn hierarchical features for visual object tracking. First, we offline learn features robust to diverse motion patterns from auxiliary video sequences. The hierarchical features are learned via a two-layer convolutional neural network. Embedding the temporal slowness constraint in the stacked architecture makes the learned features robust to complicated motion transformations, which is important for visual object tracking. Then, given a target video sequence, we propose a domain adaptation module to online adapt the pre-learned features according to the specific target object. The adaptation is conducted in both layers of the deep feature learning module so as to include appearance information of the specific target object. As a result, the learned hierarchical features can be robust to both complicated motion transformations and appearance changes of target objects. We integrate our feature learning algorithm into three tracking methods. Experimental results demonstrate that significant improvement can be achieved using our learned hierarchical features, especially on video sequences with complicated motion transformations.



### PASCAL Boundaries: A Class-Agnostic Semantic Boundary Dataset
- **Arxiv ID**: http://arxiv.org/abs/1511.07951v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07951v1)
- **Published**: 2015-11-25 05:12:38+00:00
- **Updated**: 2015-11-25 05:12:38+00:00
- **Authors**: Vittal Premachandran, Boyan Bonev, Alan L. Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the boundary detection task motivated by the ambiguities in current definition of edge detection. To this end, we generate a large database consisting of more than 10k images (which is 20x bigger than existing edge detection databases) along with ground truth boundaries between 459 semantic classes including both foreground objects and different types of background, and call it the PASCAL Boundaries dataset, which will be released to the community. In addition, we propose a novel deep network-based multi-scale semantic boundary detector and name it Multi-scale Deep Semantic Boundary Detector (M-DSBD). We provide baselines using models that were trained on edge detection and show that they transfer reasonably to the task of boundary detection. Finally, we point to various important research problems that this dataset can be used for.



### Calculate distance to object in the area where car, using video analysis
- **Arxiv ID**: http://arxiv.org/abs/1511.07963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07963v1)
- **Published**: 2015-11-25 06:09:42+00:00
- **Updated**: 2015-11-25 06:09:42+00:00
- **Authors**: Elena Legchekova, Oleg Titov
- **Comment**: 5 pages, in Russian
- **Journal**: None
- **Summary**: The method of using video cameras installed on the car, to calculate the distance to the object in its area of movement.



### A Short Survey on Data Clustering Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1511.09123v1
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV, cs.LG, stat.CO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1511.09123v1)
- **Published**: 2015-11-25 08:02:37+00:00
- **Updated**: 2015-11-25 08:02:37+00:00
- **Authors**: Ka-Chun Wong
- **Comment**: None
- **Journal**: None
- **Summary**: With rapidly increasing data, clustering algorithms are important tools for data analytics in modern research. They have been successfully applied to a wide range of domains; for instance, bioinformatics, speech recognition, and financial analysis. Formally speaking, given a set of data instances, a clustering algorithm is expected to divide the set of data instances into the subsets which maximize the intra-subset similarity and inter-subset dissimilarity, where a similarity measure is defined beforehand. In this work, the state-of-the-arts clustering algorithms are reviewed from design concept to methodology; Different clustering paradigms are discussed. Advanced clustering algorithms are also discussed. After that, the existing clustering evaluation metrics are reviewed. A summary with future insights is provided at the end.



### Learning to detect video events from zero or very few video examples
- **Arxiv ID**: http://arxiv.org/abs/1511.08032v1
- **DOI**: 10.1016/j.imavis.2015.09.005
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.08032v1)
- **Published**: 2015-11-25 12:17:50+00:00
- **Updated**: 2015-11-25 12:17:50+00:00
- **Authors**: Christos Tzelepis, Damianos Galanopoulos, Vasileios Mezaris, Ioannis Patras
- **Comment**: Image and Vision Computing Journal, Elsevier, 2015, accepted for
  publication
- **Journal**: Image and Vision Computing Journal, Elsevier, 2015
- **Summary**: In this work we deal with the problem of high-level event detection in video. Specifically, we study the challenging problems of i) learning to detect video events from solely a textual description of the event, without using any positive video examples, and ii) additionally exploiting very few positive training samples together with a small number of ``related'' videos. For learning only from an event's textual description, we first identify a general learning framework and then study the impact of different design choices for various stages of this framework. For additionally learning from example videos, when true positive training samples are scarce, we employ an extension of the Support Vector Machine that allows us to exploit ``related'' event videos by automatically introducing different weights for subsets of the videos in the overall training set. Experimental evaluations performed on the large-scale TRECVID MED 2014 video dataset provide insight on the effectiveness of the proposed methods.



### Pedestrian Detection Inspired by Appearance Constancy and Shape Symmetry
- **Arxiv ID**: http://arxiv.org/abs/1511.08058v1
- **DOI**: 10.1109/TIP.2016.2609807
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.08058v1)
- **Published**: 2015-11-25 13:49:13+00:00
- **Updated**: 2015-11-25 13:49:13+00:00
- **Authors**: Jiale Cao, Yanwei Pang, Xuelong Li
- **Comment**: 9 pages,17 figures
- **Journal**: None
- **Summary**: The discrimination and simplicity of features are very important for effective and efficient pedestrian detection. However, most state-of-the-art methods are unable to achieve good tradeoff between accuracy and efficiency. Inspired by some simple inherent attributes of pedestrians (i.e., appearance constancy and shape symmetry), we propose two new types of non-neighboring features (NNF): side-inner difference features (SIDF) and symmetrical similarity features (SSF). SIDF can characterize the difference between the background and pedestrian and the difference between the pedestrian contour and its inner part. SSF can capture the symmetrical similarity of pedestrian shape. However, it's difficult for neighboring features to have such above characterization abilities. Finally, we propose to combine both non-neighboring and neighboring features for pedestrian detection. It's found that non-neighboring features can further decrease the average miss rate by 4.44%. Experimental results on INRIA and Caltech pedestrian datasets demonstrate the effectiveness and efficiency of the proposed method. Compared to the state-of-the-art methods without using CNN, our method achieves the best detection performance on Caltech, outperforming the second best method (i.e., Checkboards) by 1.63%.



### Higher Order Conditional Random Fields in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.08119v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.08119v4)
- **Published**: 2015-11-25 17:02:31+00:00
- **Updated**: 2016-07-29 18:16:18+00:00
- **Authors**: Anurag Arnab, Sadeep Jayasumana, Shuai Zheng, Philip Torr
- **Comment**: ECCV 2016
- **Journal**: None
- **Summary**: We address the problem of semantic segmentation using deep learning. Most segmentation systems include a Conditional Random Field (CRF) to produce a structured output that is consistent with the image's visual features. Recent deep learning approaches have incorporated CRFs into Convolutional Neural Networks (CNNs), with some even training the CRF end-to-end with the rest of the network. However, these approaches have not employed higher order potentials, which have previously been shown to significantly improve segmentation performance. In this paper, we demonstrate that two types of higher order potential, based on object detections and superpixels, can be included in a CRF embedded within a deep network. We design these higher order potentials to allow inference with the differentiable mean field algorithm. As a result, all the parameters of our richer CRF model can be learned end-to-end with our pixelwise CNN classifier. We achieve state-of-the-art segmentation performance on the PASCAL VOC benchmark with these trainable higher order potentials.



### Unsupervised Deep Feature Extraction for Remote Sensing Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1511.08131v1
- **DOI**: 10.1109/TGRS.2015.2478379
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.08131v1)
- **Published**: 2015-11-25 17:36:28+00:00
- **Updated**: 2015-11-25 17:36:28+00:00
- **Authors**: Adriana Romero, Carlo Gatta, Gustau Camps-Valls
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, Volume:PP ,
  Issue: 99, 2015
- **Summary**: This paper introduces the use of single layer and deep convolutional networks for remote sensing data analysis. Direct application to multi- and hyper-spectral imagery of supervised (shallow or deep) convolutional networks is very challenging given the high input data dimensionality and the relatively small amount of available labeled data. Therefore, we propose the use of greedy layer-wise unsupervised pre-training coupled with a highly efficient algorithm for unsupervised learning of sparse features. The algorithm is rooted on sparse representations and enforces both population and lifetime sparsity of the extracted features, simultaneously. We successfully illustrate the expressive power of the extracted representations in several scenarios: classification of aerial scenes, as well as land-use classification in very high resolution (VHR), or land-cover classification from multi- and hyper-spectral images. The proposed algorithm clearly outperforms standard Principal Component Analysis (PCA) and its kernel counterpart (kPCA), as well as current state-of-the-art algorithms of aerial classification, while being extremely computationally efficient at learning representations of data. Results show that single layer convolutional networks can extract powerful discriminative features only when the receptive field accounts for neighboring pixels, and are preferred when the classification requires high resolution and detailed results. However, deep architectures significantly outperform single layers variants, capturing increasing levels of abstraction and complexity throughout the feature hierarchy.



### Tracking Motion and Proxemics using Thermal-sensor Array
- **Arxiv ID**: http://arxiv.org/abs/1511.08166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.08166v1)
- **Published**: 2015-11-25 19:14:49+00:00
- **Updated**: 2015-11-25 19:14:49+00:00
- **Authors**: Chandrayee Basu, Anthony Rowe
- **Comment**: 6 pages, 6 figures, Machine Learning for Signal Processing Class
  project
- **Journal**: None
- **Summary**: Indoor tracking has all-pervasive applications beyond mere surveillance, for example in education, health monitoring, marketing, energy management and so on. Image and video based tracking systems are intrusive. Thermal array sensors on the other hand can provide coarse-grained tracking while preserving privacy of the subjects. The goal of the project is to facilitate motion detection and group proxemics modeling using an 8 x 8 infrared sensor array. Each of the 8 x 8 pixels is a temperature reading in Fahrenheit. We refer to each 8 x 8 matrix as a scene. We collected approximately 902 scenes with different configurations of human groups and different walking directions. We infer direction of motion of a subject across a set of scenes as left-to-right, right-to-left, up-to-down and down-to-up using cross-correlation analysis. We used features from connected component analysis of each background subtracted scene and performed Support Vector Machine classification to estimate number of instances of human subjects in the scene.



### Exploring Person Context and Local Scene Context for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1511.08177v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.08177v1)
- **Published**: 2015-11-25 19:45:03+00:00
- **Updated**: 2015-11-25 19:45:03+00:00
- **Authors**: Saurabh Gupta, Bharath Hariharan, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we explore two ways of using context for object detection. The first model focusses on people and the objects they commonly interact with, such as fashion and sports accessories. The second model considers more general object detection and uses the spatial relationships between objects and between objects and scenes. Our models are able to capture precise spatial relationships between the context and the object of interest, and make effective use of the appearance of the contextual region. On the newly released COCO dataset, our models provide relative improvements of up to 5% over CNN-based state-of-the-art detectors, with the gains concentrated on hard cases such as small objects (10% relative improvement).



### Recurrent Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1511.08250v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1511.08250v3)
- **Published**: 2015-11-25 23:28:14+00:00
- **Updated**: 2016-10-24 23:57:19+00:00
- **Authors**: Bernardino Romera-Paredes, Philip H. S. Torr
- **Comment**: 14 pages (main paper). 24 pages including references and appendix
- **Journal**: ECCV 2016. 14th European Conference on Computer Vision
- **Summary**: Instance segmentation is the problem of detecting and delineating each distinct object of interest appearing in an image. Current instance segmentation approaches consist of ensembles of modules that are trained independently of each other, thus missing opportunities for joint learning. Here we propose a new instance segmentation paradigm consisting in an end-to-end method that learns how to segment instances sequentially. The model is based on a recurrent neural network that sequentially finds objects and their segmentations one at a time. This net is provided with a spatial memory that keeps track of what pixels have been explained and allows occlusion handling. In order to train the model we designed a principled loss function that accurately represents the properties of the instance segmentation problem. In the experiments carried out, we found that our method outperforms recent approaches on multiple person segmentation, and all state of the art approaches on the Plant Phenotyping dataset for leaf counting.



