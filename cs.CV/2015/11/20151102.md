# Arxiv Papers in cs.CV on 2015-11-02
### BinaryConnect: Training Deep Neural Networks with binary weights during propagations
- **Arxiv ID**: http://arxiv.org/abs/1511.00363v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.00363v3)
- **Published**: 2015-11-02 02:50:05+00:00
- **Updated**: 2016-04-18 13:11:45+00:00
- **Authors**: Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David
- **Comment**: Accepted at NIPS 2015, 9 pages, 3 figures
- **Journal**: None
- **Summary**: Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.



### Towards Reading Hidden Emotions: A comparative Study of Spontaneous Micro-expression Spotting and Recognition Methods
- **Arxiv ID**: http://arxiv.org/abs/1511.00423v2
- **DOI**: 10.1109/TAFFC.2017.2667642
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.00423v2)
- **Published**: 2015-11-02 09:51:06+00:00
- **Updated**: 2017-02-08 12:40:34+00:00
- **Authors**: Xiaobai Li, Xiaopeng Hong, Antti Moilanen, Xiaohua Huang, Tomas Pfister, Guoying Zhao, Matti Pietikäinen
- **Comment**: None
- **Journal**: None
- **Summary**: Micro-expressions (MEs) are rapid, involuntary facial expressions which reveal emotions that people do not intend to show. Studying MEs is valuable as recognizing them has many important applications, particularly in forensic science and psychotherapy. However, analyzing spontaneous MEs is very challenging due to their short duration and low intensity. Automatic ME analysis includes two tasks: ME spotting and ME recognition. For ME spotting, previous studies have focused on posed rather than spontaneous videos. For ME recognition, the performance of previous studies is low. To address these challenges, we make the following contributions: (i)We propose the first method for spotting spontaneous MEs in long videos (by exploiting feature difference contrast). This method is training free and works on arbitrary unseen videos. (ii)We present an advanced ME recognition framework, which outperforms previous work by a large margin on two challenging spontaneous ME databases (SMIC and CASMEII). (iii)We propose the first automatic ME analysis system (MESR), which can spot and recognize MEs from spontaneous video data. Finally, we show our method outperforms humans in the ME recognition task by a large margin, and achieves comparable performance to humans at the very challenging task of spotting and then recognizing spontaneous MEs.



### Semantic Summarization of Egocentric Photo Stream Events
- **Arxiv ID**: http://arxiv.org/abs/1511.00438v3
- **DOI**: 10.1145/3133202.3133204
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.00438v3)
- **Published**: 2015-11-02 10:41:34+00:00
- **Updated**: 2017-08-18 16:59:38+00:00
- **Authors**: Aniol Lidon, Marc Bolaños, Mariella Dimiccoli, Petia Radeva, Maite Garolera, Xavier Giró-i-Nieto
- **Comment**: Oral paper at the ACM Multimedia 2017 Workshop on Lifelogging Tools
  and Applications (LTA), Mountain View, California USA.
  http://lta2017.computing.dcu.ie/
- **Journal**: None
- **Summary**: With the rapid increase of users of wearable cameras in recent years and of the amount of data they produce, there is a strong need for automatic retrieval and summarization techniques. This work addresses the problem of automatically summarizing egocentric photo streams captured through a wearable camera by taking an image retrieval perspective. After removing non-informative images by a new CNN-based filter, images are ranked by relevance to ensure semantic diversity and finally re-ranked by a novelty criterion to reduce redundancy. To assess the results, a new evaluation metric is proposed which takes into account the non-uniqueness of the solution. Experimental results applied on a database of 7,110 images from 6 different subjects and evaluated by experts gave 95.74% of experts satisfaction and a Mean Opinion Score of 4.57 out of 5.0. Source code is available at https://github.com/imatge-upc/egocentric-2017-lta



### Circle detection using isosceles triangles sampling
- **Arxiv ID**: http://arxiv.org/abs/1511.00461v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1511.00461v1)
- **Published**: 2015-11-02 11:55:30+00:00
- **Updated**: 2015-11-02 11:55:30+00:00
- **Authors**: Hanqing Zhang, Krister Wiklund, Magnus Andersson
- **Comment**: Manuscript, 31 pages, 11 figures
- **Journal**: None
- **Summary**: Detection of circular objects in digital images is an important problem in several vision applications. Circle detection using randomized sampling has been developed in recent years to reduce the computational intensity. Randomized sampling, however, is sensitive to noise that can lead to reduced accuracy and false-positive candidates. This paper presents a new circle detection method based upon randomized isosceles triangles sampling to improve the robustness of randomized circle detection in noisy conditions. It is shown that the geometrical property of isosceles triangles provide a robust criterion to find relevant edge pixels and thereby efficiently provide an estimation of the circle center and radii. The estimated results given by the isosceles triangles sampling from each connected component of edge map were analyzed using a simple clustering approach for efficiency. To further improve on the accuracy we applied a two-step refinement process using chords and linear error compensation with gradient information of the edge pixels. Extensive experiments using both synthetic and real images were presented and results were compared to leading state-of-the-art algorithms and showed that the proposed algorithm: are efficient in finding circles with a low number of iterations; has high rejection rate of false-positive circle candidates; and has high robustness against noise, making it adaptive and useful in many vision applications.



### Water Detection through Spatio-Temporal Invariant Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1511.00472v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.00472v2)
- **Published**: 2015-11-02 12:28:05+00:00
- **Updated**: 2015-11-03 08:39:44+00:00
- **Authors**: Pascal Mettes, Robby T. Tan, Remco C. Veltkamp
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we aim to segment and detect water in videos. Water detection is beneficial for appllications such as video search, outdoor surveillance, and systems such as unmanned ground vehicles and unmanned aerial vehicles. The specific problem, however, is less discussed compared to general texture recognition. Here, we analyze several motion properties of water. First, we describe a video pre-processing step, to increase invariance against water reflections and water colours. Second, we investigate the temporal and spatial properties of water and derive corresponding local descriptors. The descriptors are used to locally classify the presence of water and a binary water detection mask is generated through spatio-temporal Markov Random Field regularization of the local classifications. Third, we introduce the Video Water Database, containing several hours of water and non-water videos, to validate our algorithm. Experimental evaluation on the Video Water Database and the DynTex database indicates the effectiveness of the proposed algorithm, outperforming multiple algorithms for dynamic texture recognition and material recognition by ca. 5% and 15% respectively.



### Pixel-wise Segmentation of Street with Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.00513v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.00513v1)
- **Published**: 2015-11-02 14:23:22+00:00
- **Updated**: 2015-11-02 14:23:22+00:00
- **Authors**: Sebastian Bittel, Vitali Kaiser, Marvin Teichmann, Martin Thoma
- **Comment**: None
- **Journal**: None
- **Summary**: Pixel-wise street segmentation of photographs taken from a drivers perspective is important for self-driving cars and can also support other object recognition tasks. A framework called SST was developed to examine the accuracy and execution time of different neural networks. The best neural network achieved an $F_1$-score of 89.5% with a simple feedforward neural network which trained to solve a regression task.



### SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1511.00561v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.00561v3)
- **Published**: 2015-11-02 15:51:03+00:00
- **Updated**: 2016-10-10 21:11:59+00:00
- **Authors**: Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance.   SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.



