# Arxiv Papers in cs.CV on 2015-11-13
### UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1511.04136v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04136v4)
- **Published**: 2015-11-13 01:37:39+00:00
- **Updated**: 2020-01-24 22:57:15+00:00
- **Authors**: Longyin Wen, Dawei Du, Zhaowei Cai, Zhen Lei, Ming-Ching Chang, Honggang Qi, Jongwoo Lim, Ming-Hsuan Yang, Siwei Lyu
- **Comment**: 18 pages, 11 figures, accepted by CVIU
- **Journal**: None
- **Summary**: In recent years, numerous effective multi-object tracking (MOT) methods are developed because of the wide range of applications. Existing performance evaluations of MOT methods usually separate the object tracking step from the object detection step by using the same fixed object detection results for comparisons. In this work, we perform a comprehensive quantitative study on the effects of object detection accuracy to the overall MOT performance, using the new large-scale University at Albany DETection and tRACking (UA-DETRAC) benchmark dataset. The UA-DETRAC benchmark dataset consists of 100 challenging video sequences captured from real-world traffic scenes (over 140,000 frames with rich annotations, including occlusion, weather, vehicle category, truncation, and vehicle bounding boxes) for object detection, object tracking and MOT system. We evaluate complete MOT systems constructed from combinations of state-of-the-art object detection and object tracking methods. Our analysis shows the complex effects of object detection accuracy on MOT system performance. Based on these observations, we propose new evaluation tools and metrics for MOT systems that consider both object detection and object tracking for comprehensive analysis.



### Deep Mean Maps
- **Arxiv ID**: http://arxiv.org/abs/1511.04150v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.04150v2)
- **Published**: 2015-11-13 03:36:51+00:00
- **Updated**: 2021-01-14 06:24:14+00:00
- **Authors**: Junier B. Oliva, Danica J. Sutherland, Barnabás Póczos, Jeff Schneider
- **Comment**: None
- **Journal**: None
- **Summary**: The use of distributions and high-level features from deep architecture has become commonplace in modern computer vision. Both of these methodologies have separately achieved a great deal of success in many computer vision tasks. However, there has been little work attempting to leverage the power of these to methodologies jointly. To this end, this paper presents the Deep Mean Maps (DMMs) framework, a novel family of methods to non-parametrically represent distributions of features in convolutional neural network models.   DMMs are able to both classify images using the distribution of top-level features, and to tune the top-level features for performing this task. We show how to implement DMMs using a special mean map layer composed of typical CNN operations, making both forward and backward propagation simple.   We illustrate the efficacy of DMMs at analyzing distributional patterns in image data in a synthetic data experiment. We also show that we extending existing deep architectures with DMMs improves the performance of existing CNNs on several challenging real-world datasets.



### Adaptive Affinity Matrix for Unsupervised Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1511.04153v2
- **DOI**: 10.1109/ICME.2016.7552887
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.04153v2)
- **Published**: 2015-11-13 03:59:14+00:00
- **Updated**: 2016-09-11 13:58:06+00:00
- **Authors**: Yaoyi Li, Junxuan Chen, Hongtao Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Spectral clustering is one of the most popular clustering approaches with the capability to handle some challenging clustering problems. Most spectral clustering methods provide a nonlinear map from the data manifold to a subspace. Only a little work focuses on the explicit linear map which can be viewed as the unsupervised distance metric learning. In practice, the selection of the affinity matrix exhibits a tremendous impact on the unsupervised learning. While much success of affinity learning has been achieved in recent years, some issues such as noise reduction remain to be addressed. In this paper, we propose a novel method, dubbed Adaptive Affinity Matrix (AdaAM), to learn an adaptive affinity matrix and derive a distance metric from the affinity. We assume the affinity matrix to be positive semidefinite with ability to quantify the pairwise dissimilarity. Our method is based on posing the optimization of objective function as a spectral decomposition problem. We yield the affinity from both the original data distribution and the widely-used heat kernel. The provided matrix can be regarded as the optimal representation of pairwise relationship on the manifold. Extensive experiments on a number of real-world data sets show the effectiveness and efficiency of AdaAM.



### Natural Language Object Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1511.04164v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1511.04164v3)
- **Published**: 2015-11-13 05:53:37+00:00
- **Updated**: 2016-04-11 03:36:58+00:00
- **Authors**: Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, Trevor Darrell
- **Comment**: Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition, 2016
- **Journal**: None
- **Summary**: In this paper, we address the task of natural language object retrieval, to localize a target object within a given image based on a natural language query of the object. Natural language object retrieval differs from text-based image retrieval task as it involves spatial information about objects within the scene and global scene context. To address this issue, we propose a novel Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate boxes for object retrieval, integrating spatial configurations and global scene-level contextual information into the network. Our model processes query text, local image descriptors, spatial configurations and global context features through a recurrent network, outputs the probability of the query text conditioned on each candidate box as a score for the box, and can transfer visual-linguistic knowledge from image captioning domain to our task. Experimental results demonstrate that our method effectively utilizes both local and global information, outperforming previous baseline methods significantly on different datasets and scenarios, and can exploit large scale vision and language datasets for knowledge transfer.



### Unsupervised Learning of Edges
- **Arxiv ID**: http://arxiv.org/abs/1511.04166v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04166v2)
- **Published**: 2015-11-13 06:09:00+00:00
- **Updated**: 2016-04-10 21:55:43+00:00
- **Authors**: Yin Li, Manohar Paluri, James M. Rehg, Piotr Dollár
- **Comment**: Camera ready version for CVPR 2016
- **Journal**: None
- **Summary**: Data-driven approaches for edge detection have proven effective and achieve top results on modern benchmarks. However, all current data-driven edge detectors require manual supervision for training in the form of hand-labeled region segments or object boundaries. Specifically, human annotators mark semantically meaningful edges which are subsequently used for training. Is this form of strong, high-level supervision actually necessary to learn to accurately detect edges? In this work we present a simple yet effective approach for training edge detectors without human supervision. To this end we utilize motion, and more specifically, the only input to our method is noisy semi-dense matches between frames. We begin with only a rudimentary knowledge of edges (in the form of image gradients), and alternate between improving motion estimation and edge detection in turn. Using a large corpus of video data, we show that edge detectors trained using our unsupervised scheme approach the performance of the same methods trained with full supervision (within 3-5%). Finally, we show that when using a deep network for the edge detector, our approach provides a novel pre-training scheme for object detection.



### Sequence to Sequence Learning for Optical Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1511.04176v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04176v2)
- **Published**: 2015-11-13 06:33:22+00:00
- **Updated**: 2015-12-27 13:55:02+00:00
- **Authors**: Devendra Kumar Sahu, Mohak Sukhwani
- **Comment**: 9 pages (including reference), 6 figures (including subfigures), 5
  tables
- **Journal**: None
- **Summary**: We propose an end-to-end recurrent encoder-decoder based sequence learning approach for printed text Optical Character Recognition (OCR). In contrast to present day existing state-of-art OCR solution which uses connectionist temporal classification (CTC) output layer, our approach makes minimalistic assumptions on the structure and length of the sequence. We use a two step encoder-decoder approach -- (a) A recurrent encoder reads a variable length printed text word image and encodes it to a fixed dimensional embedding. (b) This fixed dimensional embedding is subsequently comprehended by decoder structure which converts it into a variable length text output. Our architecture gives competitive performance relative to connectionist temporal classification (CTC) output layer while being executed in more natural settings. The learnt deep word image embedding from encoder can be used for printed text based retrieval systems. The expressive fixed dimensional embedding for any variable length input expedites the task of retrieval and makes it more efficient which is not possible with other recurrent neural network architectures. We empirically investigate the expressiveness and the learnability of long short term memory (LSTMs) in the sequence to sequence learning regime by training our network for prediction tasks in segmentation free printed text OCR. The utility of the proposed architecture for printed text is demonstrated by quantitative and qualitative evaluation of two tasks -- word prediction and retrieval.



### DISC: Deep Image Saliency Computing via Progressive Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1511.04192v2
- **DOI**: 10.1109/TNNLS.2015.2506664
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04192v2)
- **Published**: 2015-11-13 07:14:13+00:00
- **Updated**: 2015-12-10 13:11:23+00:00
- **Authors**: Tianshui Chen, Liang Lin, Lingbo Liu, Xiaonan Luo, Xuelong Li
- **Comment**: This manuscript is the accepted version for IEEE Transactions on
  Neural Networks and Learning Systems (T-NNLS), 2015
- **Journal**: None
- **Summary**: Salient object detection increasingly receives attention as an important component or step in several pattern recognition and image processing tasks. Although a variety of powerful saliency models have been intensively proposed, they usually involve heavy feature (or model) engineering based on priors (or assumptions) about the properties of objects and backgrounds. Inspired by the effectiveness of recently developed feature learning, we provide a novel Deep Image Saliency Computing (DISC) framework for fine-grained image saliency computing. In particular, we model the image saliency from both the coarse- and fine-level observations, and utilize the deep convolutional neural network (CNN) to learn the saliency representation in a progressive manner. Specifically, our saliency model is built upon two stacked CNNs. The first CNN generates a coarse-level saliency map by taking the overall image as the input, roughly identifying saliency regions in the global context. Furthermore, we integrate superpixel-based local context information in the first CNN to refine the coarse-level saliency map. Guided by the coarse saliency map, the second CNN focuses on the local context to produce fine-grained and accurate saliency map while preserving object details. For a testing image, the two CNNs collaboratively conduct the saliency computing in one shot. Our DISC framework is capable of uniformly highlighting the objects-of-interest from complex background while preserving well object details. Extensive experiments on several standard benchmarks suggest that DISC outperforms other state-of-the-art methods and it also generalizes well across datasets without additional training. The executable version of DISC is available online: http://vision.sysu.edu.cn/projects/DISC.



### Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1511.04196v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04196v2)
- **Published**: 2015-11-13 07:58:38+00:00
- **Updated**: 2016-04-12 17:28:18+00:00
- **Authors**: Zhiwei Deng, Arash Vahdat, Hexiang Hu, Greg Mori
- **Comment**: CVPR 2016
- **Journal**: None
- **Summary**: Rich semantic relations are important in a variety of visual recognition problems. As a concrete example, group activity recognition involves the interactions and relative spatial relations of a set of people in a scene. State of the art recognition methods center on deep learning approaches for training highly effective, complex classifiers for interpreting images. However, bridging the relatively low-level concepts output by these methods to interpret higher-level compositional scenes remains a challenge. Graphical models are a standard tool for this task. In this paper, we propose a method to integrate graphical models and deep neural networks into a joint framework. Instead of using a traditional inference method, we use a sequential inference modeled by a recurrent neural network. Beyond this, the appropriate structure for inference can be learned by imposing gates on edges between nodes. Empirical results on group activity recognition demonstrate the potential of this model to handle highly structured learning tasks.



### An Adaptive Data Representation for Robust Point-Set Registration and Merging
- **Arxiv ID**: http://arxiv.org/abs/1511.04240v1
- **DOI**: 10.1109/ICCV.2015.488
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04240v1)
- **Published**: 2015-11-13 11:23:40+00:00
- **Updated**: 2015-11-13 11:23:40+00:00
- **Authors**: Dylan Campbell, Lars Petersson
- **Comment**: Manuscript in press 2015 IEEE International Conference on Computer
  Vision
- **Journal**: None
- **Summary**: This paper presents a framework for rigid point-set registration and merging using a robust continuous data representation. Our point-set representation is constructed by training a one-class support vector machine with a Gaussian radial basis function kernel and subsequently approximating the output function with a Gaussian mixture model. We leverage the representation's sparse parametrisation and robustness to noise, outliers and occlusions in an efficient registration algorithm that minimises the L2 distance between our support vector--parametrised Gaussian mixtures. In contrast, existing techniques, such as Iterative Closest Point and Gaussian mixture approaches, manifest a narrower region of convergence and are less robust to occlusions and missing data, as demonstrated in the evaluation on a range of 2D and 3D datasets. Finally, we present a novel algorithm, GMMerge, that parsimoniously and equitably merges aligned mixture models, allowing the framework to be used for reconstruction and mapping.



### Volume-based Semantic Labeling with Signed Distance Functions
- **Arxiv ID**: http://arxiv.org/abs/1511.04242v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04242v1)
- **Published**: 2015-11-13 11:25:50+00:00
- **Updated**: 2015-11-13 11:25:50+00:00
- **Authors**: Tommaso Cavallari, Luigi Di Stefano
- **Comment**: Submitted to PSIVT2015
- **Journal**: None
- **Summary**: Research works on the two topics of Semantic Segmentation and SLAM (Simultaneous Localization and Mapping) have been following separate tracks. Here, we link them quite tightly by delineating a category label fusion technique that allows for embedding semantic information into the dense map created by a volume-based SLAM algorithm such as KinectFusion. Accordingly, our approach is the first to provide a semantically labeled dense reconstruction of the environment from a stream of RGB-D images. We validate our proposal using a publicly available semantically annotated RGB-D dataset and a) employing ground truth labels, b) corrupting such annotations with synthetic noise, c) deploying a state of the art semantic segmentation algorithm based on Convolutional Neural Networks.



### Learning to Assign Orientations to Feature Points
- **Arxiv ID**: http://arxiv.org/abs/1511.04273v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04273v2)
- **Published**: 2015-11-13 13:23:09+00:00
- **Updated**: 2016-04-11 14:03:54+00:00
- **Authors**: Kwang Moo Yi, Yannick Verdie, Pascal Fua, Vincent Lepetit
- **Comment**: Accepted as Oral presentation in Computer Vision and Pattern
  Recognition, 2016
- **Journal**: None
- **Summary**: We show how to train a Convolutional Neural Network to assign a canonical orientation to feature points given an image patch centered on the feature point. Our method improves feature point matching upon the state-of-the art and can be used in conjunction with any existing rotation sensitive descriptors. To avoid the tedious and almost impossible task of finding a target orientation to learn, we propose to use Siamese networks which implicitly find the optimal orientations during training. We also propose a new type of activation function for Neural Networks that generalizes the popular ReLU, maxout, and PReLU activation functions. This novel activation performs better for our task. We validate the effectiveness of our method extensively with four existing datasets, including two non-planar datasets, as well as our own dataset. We show that we outperform the state-of-the-art without the need of retraining for each dataset.



### Standard methods for inexpensive pollen loads authentication by means of computer vision and machine learning
- **Arxiv ID**: http://arxiv.org/abs/1511.04320v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04320v1)
- **Published**: 2015-11-13 15:35:22+00:00
- **Updated**: 2015-11-13 15:35:22+00:00
- **Authors**: Manuel Chica, Pascual Campoy
- **Comment**: 24 pages. Book chapter to appear
- **Journal**: None
- **Summary**: We present a complete methodology for authenticating local bee pollen against fraudulent samples using image processing and machine learning techniques. The proposed standard methods do not need expensive equipment such as advanced microscopes and can be used for a preliminary fast rejection of unknown pollen types. The system is able to rapidly reject the non-local pollen samples with inexpensive hardware and without the need to send the product to the laboratory. Methods are based on the color properties of bee pollen loads images and the use of one-class classifiers which are appropriate to reject unknown pollen samples when there is limited data about them. The validation of the method is carried out by authenticating Spanish bee pollen types. Experimentation shows that the proposed methods can obtain an overall authentication accuracy of 94%. We finally illustrate the user interaction with the software in some practical cases by showing the developed application prototype.



### Learning Dense Convolutional Embeddings for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1511.04377v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04377v3)
- **Published**: 2015-11-13 17:32:11+00:00
- **Updated**: 2016-01-08 01:16:52+00:00
- **Authors**: Adam W. Harley, Konstantinos G. Derpanis, Iasonas Kokkinos
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a new deep convolutional neural network (DCNN) architecture that learns pixel embeddings, such that pairwise distances between the embeddings can be used to infer whether or not the pixels lie on the same region. That is, for any two pixels on the same object, the embeddings are trained to be similar; for any pair that straddles an object boundary, the embeddings are trained to be dissimilar. Experimental results show that when this embedding network is used in conjunction with a DCNN trained on semantic segmentation, there is a systematic improvement in per-pixel classification accuracy. Our contributions are integrated in the popular Caffe deep learning framework, and consist in straightforward modifications to convolution routines. As such, they can be exploited for any task involving convolution layers.



### Deep Reflectance Maps
- **Arxiv ID**: http://arxiv.org/abs/1511.04384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04384v1)
- **Published**: 2015-11-13 18:06:32+00:00
- **Updated**: 2015-11-13 18:06:32+00:00
- **Authors**: Konstantinos Rematas, Tobias Ritschel, Mario Fritz, Efstratios Gavves, Tinne Tuytelaars
- **Comment**: project page: http://homes.esat.kuleuven.be/~krematas/DRM/
- **Journal**: None
- **Summary**: Undoing the image formation process and therefore decomposing appearance into its intrinsic properties is a challenging task due to the under-constraint nature of this inverse problem. While significant progress has been made on inferring shape, materials and illumination from images only, progress in an unconstrained setting is still limited. We propose a convolutional neural architecture to estimate reflectance maps of specular materials in natural lighting conditions. We achieve this in an end-to-end learning formulation that directly predicts a reflectance map from the image itself. We show how to improve estimates by facilitating additional supervision in an indirect scheme that first predicts surface orientation and afterwards predicts the reflectance map by a learning-based sparse data interpolation.   In order to analyze performance on this difficult task, we propose a new challenge of Specular MAterials on SHapes with complex IllumiNation (SMASHINg) using both synthetic and real images. Furthermore, we show the application of our method to a range of image-based editing tasks on real images.



### Similarity-based Text Recognition by Deeply Supervised Siamese Network
- **Arxiv ID**: http://arxiv.org/abs/1511.04397v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.04397v5)
- **Published**: 2015-11-13 18:46:01+00:00
- **Updated**: 2016-07-05 01:21:08+00:00
- **Authors**: Ehsan Hosseini-Asl, Angshuman Guha
- **Comment**: Accepted for presenting at Future Technologies Conference - (FTC
  2016) San Francisco, December 6-7, 2016
- **Journal**: None
- **Summary**: In this paper, we propose a new text recognition model based on measuring the visual similarity of text and predicting the content of unlabeled texts. First a Siamese convolutional network is trained with deep supervision on a labeled training dataset. This network projects texts into a similarity manifold. The Deeply Supervised Siamese network learns visual similarity of texts. Then a K-nearest neighbor classifier is used to predict unlabeled text based on similarity distance to labeled texts. The performance of the model is evaluated on three datasets of machine-print and hand-written text combined. We demonstrate that the model reduces the cost of human estimation by $50\%-85\%$. The error of the system is less than $0.5\%$. The proposed model outperform conventional Siamese network by finding visually-similar barely-readable and readable text, e.g. machine-printed, handwritten, due to deep supervision. The results also demonstrate that the predicted labels are sometimes better than human labels e.g. spelling correction.



### Symbol Grounding Association in Multimodal Sequences with Missing Elements
- **Arxiv ID**: http://arxiv.org/abs/1511.04401v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.04401v5)
- **Published**: 2015-11-13 18:59:36+00:00
- **Updated**: 2017-12-07 10:14:23+00:00
- **Authors**: Federico Raue, Andreas Dengel, Thomas M. Breuel, Marcus Liwicki
- **Comment**: Under review on Journal of Artificial Intelligence Research (JAIR) --
  Special Track on Deep Learning, Knowledge Representation, and Reasoning
- **Journal**: None
- **Summary**: In this paper, we extend a symbolic association framework for being able to handle missing elements in multimodal sequences. The general scope of the work is the symbolic associations of object-word mappings as it happens in language development in infants. In other words, two different representations of the same abstract concepts can associate in both directions. This scenario has been long interested in Artificial Intelligence, Psychology, and Neuroscience. In this work, we extend a recent approach for multimodal sequences (visual and audio) to also cope with missing elements in one or both modalities. Our method uses two parallel Long Short-Term Memories (LSTMs) with a learning rule based on EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We propose to include an extra step for the combination with the max operation for exploiting the common elements between both sequences. The motivation behind is that the combination acts as a condition selector for choosing the best representation from both LSTMs. We evaluated the proposed extension in the following scenarios: missing elements in one modality (visual or audio) and missing elements in both modalities (visual and sound). The performance of our extension reaches better results than the original model and similar results to individual LSTM trained in each modality.



### Robust Face Alignment Using a Mixture of Invariant Experts
- **Arxiv ID**: http://arxiv.org/abs/1511.04404v2
- **DOI**: 10.1007/978-3-319-46454-1_50
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04404v2)
- **Published**: 2015-11-13 19:14:51+00:00
- **Updated**: 2016-10-23 18:31:06+00:00
- **Authors**: Oncel Tuzel, Tim K. Marks, Salil Tambe
- **Comment**: 17 pages, 6 figures
- **Journal**: Proceedings of 14th European Conference on Computer Vision (ECCV),
  Amsterdam, The Netherlands, October 11-14, 2016, pp 825-841
- **Summary**: Face alignment, which is the task of finding the locations of a set of facial landmark points in an image of a face, is useful in widespread application areas. Face alignment is particularly challenging when there are large variations in pose (in-plane and out-of-plane rotations) and facial expression. To address this issue, we propose a cascade in which each stage consists of a mixture of regression experts. Each expert learns a customized regression model that is specialized to a different subset of the joint space of pose and expressions. The system is invariant to a predefined class of transformations (e.g., affine), because the input is transformed to match each expert's prototype shape before the regression is applied. We also present a method to include deformation constraints within the discriminative alignment framework, which makes our algorithm more robust. Our algorithm significantly outperforms previous methods on publicly available face alignment datasets.



### Transductive Zero-Shot Action Recognition by Word-Vector Embedding
- **Arxiv ID**: http://arxiv.org/abs/1511.04458v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04458v2)
- **Published**: 2015-11-13 21:05:20+00:00
- **Updated**: 2016-12-02 07:17:03+00:00
- **Authors**: Xun Xu, Timothy Hospedales, Shaogang Gong
- **Comment**: Accepted by IJCV
- **Journal**: None
- **Summary**: The number of categories for action recognition is growing rapidly and it has become increasingly hard to label sufficient training data for learning conventional models for all categories. Instead of collecting ever more data and labelling them exhaustively for all categories, an attractive alternative approach is zero-shot learning" (ZSL). To that end, in this study we construct a mapping between visual features and a semantic descriptor of each action category, allowing new categories to be recognised in the absence of any visual training data. Existing ZSL studies focus primarily on still images, and attribute-based semantic representations. In this work, we explore word-vectors as the shared semantic space to embed videos and category labels for ZSL action recognition. This is a more challenging problem than existing ZSL of still images and/or attributes, because the mapping between video spacetime features of actions and the semantic space is more complex and harder to learn for the purpose of generalising over any cross-category domain shift. To solve this generalisation problem in ZSL action recognition, we investigate a series of synergistic strategies to improve upon the standard ZSL pipeline. Most of these strategies are transductive in nature which means access to testing data in the training phase.



### Solving Jigsaw Puzzles with Linear Programming
- **Arxiv ID**: http://arxiv.org/abs/1511.04472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04472v1)
- **Published**: 2015-11-13 22:15:54+00:00
- **Updated**: 2015-11-13 22:15:54+00:00
- **Authors**: Rui Yu, Chris Russell, Lourdes Agapito
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel Linear Program (LP) based formula- tion for solving jigsaw puzzles. We formulate jigsaw solving as a set of successive global convex relaxations of the stan- dard NP-hard formulation, that can describe both jigsaws with pieces of unknown position and puzzles of unknown po- sition and orientation. The main contribution and strength of our approach comes from the LP assembly strategy. In contrast to existing greedy methods, our LP solver exploits all the pairwise matches simultaneously, and computes the position of each piece/component globally. The main ad- vantages of our LP approach include: (i) a reduced sensi- tivity to local minima compared to greedy approaches, since our successive approximations are global and convex and (ii) an increased robustness to the presence of mismatches in the pairwise matches due to the use of a weighted L1 penalty. To demonstrate the effectiveness of our approach, we test our algorithm on public jigsaw datasets and show that it outperforms state-of-the-art methods.



