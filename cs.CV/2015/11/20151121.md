# Arxiv Papers in cs.CV on 2015-11-21
### Learning visual groups from co-occurrences in space and time
- **Arxiv ID**: http://arxiv.org/abs/1511.06811v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06811v1)
- **Published**: 2015-11-21 01:33:12+00:00
- **Updated**: 2015-11-21 01:33:12+00:00
- **Authors**: Phillip Isola, Daniel Zoran, Dilip Krishnan, Edward H. Adelson
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories.



### An Immersive Telepresence System using RGB-D Sensors and Head Mounted Display
- **Arxiv ID**: http://arxiv.org/abs/1511.06815v1
- **DOI**: 10.1109/ISM.2015.108
- **Categories**: **cs.CV**, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1511.06815v1)
- **Published**: 2015-11-21 01:57:47+00:00
- **Updated**: 2015-11-21 01:57:47+00:00
- **Authors**: Xinzhong Lu, Ju Shen, Saverio Perugini, Jianjun Yang
- **Comment**: IEEE International Symposium on Multimedia 2015
- **Journal**: None
- **Summary**: We present a tele-immersive system that enables people to interact with each other in a virtual world using body gestures in addition to verbal communication. Beyond the obvious applications, including general online conversations and gaming, we hypothesize that our proposed system would be particularly beneficial to education by offering rich visual contents and interactivity. One distinct feature is the integration of egocentric pose recognition that allows participants to use their gestures to demonstrate and manipulate virtual objects simultaneously. This functionality enables the instructor to ef- fectively and efficiently explain and illustrate complex concepts or sophisticated problems in an intuitive manner. The highly interactive and flexible environment can capture and sustain more student attention than the traditional classroom setting and, thus, delivers a compelling experience to the students. Our main focus here is to investigate possible solutions for the system design and implementation and devise strategies for fast, efficient computation suitable for visual data processing and network transmission. We describe the technique and experiments in details and provide quantitative performance results, demonstrating our system can be run comfortably and reliably for different application scenarios. Our preliminary results are promising and demonstrate the potential for more compelling directions in cyberlearning.



### Ground-truth dataset and baseline evaluations for image base-detail separation algorithms
- **Arxiv ID**: http://arxiv.org/abs/1511.06830v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06830v2)
- **Published**: 2015-11-21 04:04:39+00:00
- **Updated**: 2016-02-18 22:59:13+00:00
- **Authors**: Xuan Dong, Boyan Bonev, Weixin Li, Weichao Qiu, Xianjie Chen, Alan Yuille
- **Comment**: This paper has been withdrawn by the author due to some un-proper
  examples
- **Journal**: None
- **Summary**: Base-detail separation is a fundamental computer vision problem consisting of modeling a smooth base layer with the coarse structures, and a detail layer containing the texture-like structures. One of the challenges of estimating the base is to preserve sharp boundaries between objects or parts to avoid halo artifacts. Many methods have been proposed to address this problem, but there is no ground-truth dataset of real images for quantitative evaluation. We proposed a procedure to construct such a dataset, and provide two datasets: Pascal Base-Detail and Fashionista Base-Detail, containing 1000 and 250 images, respectively. Our assumption is that the base is piecewise smooth and we label the appearance of each piece by a polynomial model. The pieces are objects and parts of objects, obtained from human annotations. Finally, we proposed a way to evaluate methods with our base-detail ground-truth and we compared the performances of seven state-of-the-art algorithms.



### Fidelity-Naturalness Evaluation of Single Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1511.06834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06834v1)
- **Published**: 2015-11-21 04:40:59+00:00
- **Updated**: 2015-11-21 04:40:59+00:00
- **Authors**: Xuan Dong, Yu Zhu, Weixin Li, Lingxi Xie, Alex Wong, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of evaluating super resolution methods. Traditional evaluation methods usually judge the quality of super resolved images based on a single measure of their difference with the original high resolution images. In this paper, we proposed to use both fidelity (the difference with original images) and naturalness (human visual perception of super resolved images) for evaluation. For fidelity evaluation, a new metric is proposed to solve the bias problem of traditional evaluation. For naturalness evaluation, we let humans label preference of super resolution results using pair-wise comparison, and test the correlation between human labeling results and image quality assessment metrics' outputs. Experimental results show that our fidelity-naturalness method is better than the traditional evaluation method for super resolution methods, which could help future research on single-image super resolution.



### Mapping Images to Sentiment Adjective Noun Pairs with Factorized Neural Nets
- **Arxiv ID**: http://arxiv.org/abs/1511.06838v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1511.06838v1)
- **Published**: 2015-11-21 04:58:46+00:00
- **Updated**: 2015-11-21 04:58:46+00:00
- **Authors**: Takuya Narihira, Damian Borth, Stella X. Yu, Karl Ni, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the visual sentiment task of mapping an image to an adjective noun pair (ANP) such as "cute baby". To capture the two-factor structure of our ANP semantics as well as to overcome annotation noise and ambiguity, we propose a novel factorized CNN model which learns separate representations for adjectives and nouns but optimizes the classification performance over their product. Our experiments on the publicly available SentiBank dataset show that our model significantly outperforms not only independent ANP classifiers on unseen ANPs and on retrieving images of novel ANPs, but also image captioning models which capture word semantics from co-occurrence of natural text; the latter turn out to be surprisingly poor at capturing the sentiment evoked by pure visual experience. That is, our factorized ANP CNN not only trains better from noisy labels, generalizes better to new images, but can also expands the ANP vocabulary on its own.



### TransCut: Transparent Object Segmentation from a Light-Field Image
- **Arxiv ID**: http://arxiv.org/abs/1511.06853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06853v1)
- **Published**: 2015-11-21 08:33:18+00:00
- **Updated**: 2015-11-21 08:33:18+00:00
- **Authors**: Yichao Xu, Hajime Nagahara, Atsushi Shimada, Rin-ichiro Taniguchi
- **Comment**: 9 pages, 14 figures, 2 tables, ICCV 2015
- **Journal**: None
- **Summary**: The segmentation of transparent objects can be very useful in computer vision applications. However, because they borrow texture from their background and have a similar appearance to their surroundings, transparent objects are not handled well by regular image segmentation methods. We propose a method that overcomes these problems using the consistency and distortion properties of a light-field image. Graph-cut optimization is applied for the pixel labeling problem. The light-field linearity is used to estimate the likelihood of a pixel belonging to the transparent object or Lambertian background, and the occlusion detector is used to find the occlusion boundary. We acquire a light field dataset for the transparent object, and use this dataset to evaluate our method. The results demonstrate that the proposed method successfully segments transparent objects from the background.



### Unsupervised learning of object semantic parts from internal states of CNNs by population encoding
- **Arxiv ID**: http://arxiv.org/abs/1511.06855v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06855v3)
- **Published**: 2015-11-21 09:02:21+00:00
- **Updated**: 2016-11-12 13:37:07+00:00
- **Authors**: Jianyu Wang, Zhishuai Zhang, Cihang Xie, Vittal Premachandran, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: We address the key question of how object part representations can be found from the internal states of CNNs that are trained for high-level tasks, such as object classification. This work provides a new unsupervised method to learn semantic parts and gives new understanding of the internal representations of CNNs. Our technique is based on the hypothesis that semantic parts are represented by populations of neurons rather than by single filters. We propose a clustering technique to extract part representations, which we call Visual Concepts. We show that visual concepts are semantically coherent in that they represent semantic parts, and visually coherent in that corresponding image patches appear very similar. Also, visual concepts provide full spatial coverage of the parts of an object, rather than a few sparse parts as is typically found in keypoint annotations. Furthermore, We treat single visual concept as part detector and evaluate it for keypoint detection using the PASCAL3D+ dataset and for part detection using our newly annotated ImageNetPart dataset. The experiments demonstrate that visual concepts can be used to detect parts. We also show that some visual concepts respond to several semantic parts, provided these parts are visually similar. Thus visual concepts have the essential properties: semantic meaning and detection capability. Note that our ImageNetPart dataset gives rich part annotations which cover the whole object, making it useful for other part-related applications.



### Data-dependent Initializations of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.06856v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06856v3)
- **Published**: 2015-11-21 09:07:08+00:00
- **Updated**: 2016-09-22 22:14:17+00:00
- **Authors**: Philipp Krähenbühl, Carl Doersch, Jeff Donahue, Trevor Darrell
- **Comment**: ICLR 2016
- **Journal**: None
- **Summary**: Convolutional Neural Networks spread through computer vision like a wildfire, impacting almost all visual tasks imaginable. Despite this, few researchers dare to train their models from scratch. Most work builds on one of a handful of ImageNet pre-trained models, and fine-tunes or adapts these for specific tasks. This is in large part due to the difficulty of properly initializing these networks from scratch. A small miscalibration of the initial weights leads to vanishing or exploding gradients, as well as poor convergence properties. In this work we present a fast and simple data-dependent initialization procedure, that sets the weights of a network such that all units in the network train at roughly the same rate, avoiding vanishing or exploding gradients. Our initialization matches the current state-of-the-art unsupervised or self-supervised pre-training methods on standard computer vision tasks, such as image classification and object detection, while being roughly three orders of magnitude faster. When combined with pre-training methods, our initialization significantly outperforms prior work, narrowing the gap between supervised and unsupervised pre-training.



### Convex Sparse Spectral Clustering: Single-view to Multi-view
- **Arxiv ID**: http://arxiv.org/abs/1511.06860v3
- **DOI**: 10.1109/TIP.2016.2553459
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06860v3)
- **Published**: 2015-11-21 09:38:31+00:00
- **Updated**: 2018-05-27 06:02:40+00:00
- **Authors**: Canyi Lu, Shuicheng Yan, Zhouchen Lin
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing (TIP), vol. 25, pp.
  2833-2843, 2016
- **Summary**: Spectral Clustering (SC) is one of the most widely used methods for data clustering. It first finds a low-dimensonal embedding $U$ of data by computing the eigenvectors of the normalized Laplacian matrix, and then performs k-means on $U^\top$ to get the final clustering result. In this work, we observe that, in the ideal case, $UU^\top$ should be block diagonal and thus sparse. Therefore we propose the Sparse Spectral Clustering (SSC) method which extends SC with sparse regularization on $UU^\top$. To address the computational issue of the nonconvex SSC model, we propose a novel convex relaxation of SSC based on the convex hull of the fixed rank projection matrices. Then the convex SSC model can be efficiently solved by the Alternating Direction Method of \canyi{Multipliers} (ADMM). Furthermore, we propose the Pairwise Sparse Spectral Clustering (PSSC) which extends SSC to boost the clustering performance by using the multi-view information of data. Experimental comparisons with several baselines on real-world datasets testify to the efficacy of our proposed methods.



### Zoom Better to See Clearer: Human and Object Parsing with Hierarchical Auto-Zoom Net
- **Arxiv ID**: http://arxiv.org/abs/1511.06881v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06881v5)
- **Published**: 2015-11-21 13:32:26+00:00
- **Updated**: 2016-03-28 21:53:31+00:00
- **Authors**: Fangting Xia, Peng Wang, Liang-Chieh Chen, Alan L. Yuille
- **Comment**: A shortened version has been submitted to ECCV 2016
- **Journal**: None
- **Summary**: Parsing articulated objects, e.g. humans and animals, into semantic parts (e.g. body, head and arms, etc.) from natural images is a challenging and fundamental problem for computer vision. A big difficulty is the large variability of scale and location for objects and their corresponding parts. Even limited mistakes in estimating scale and location will degrade the parsing output and cause errors in boundary details. To tackle these difficulties, we propose a "Hierarchical Auto-Zoom Net" (HAZN) for object part parsing which adapts to the local scales of objects and parts. HAZN is a sequence of two "Auto-Zoom Net" (AZNs), each employing fully convolutional networks that perform two tasks: (1) predict the locations and scales of object instances (the first AZN) or their parts (the second AZN); (2) estimate the part scores for predicted object instance or part regions. Our model can adaptively "zoom" (resize) predicted image regions into their proper scales to refine the parsing.   We conduct extensive experiments over the PASCAL part datasets on humans, horses, and cows. For humans, our approach significantly outperforms the state-of-the-arts by 5% mIOU and is especially better at segmenting small instances and small parts. We obtain similar improvements for parsing cows and horses over alternative methods. In summary, our strategy of first zooming into objects and then zooming into parts is very effective. It also enables us to process different regions of the image at different scales adaptively so that, for example, we do not need to waste computational resources scaling the entire image.



### Screen Content Image Segmentation Using Sparse-Smooth Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1511.06911v1
- **DOI**: 10.1109/ACSSC.2015.7421331
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06911v1)
- **Published**: 2015-11-21 17:55:14+00:00
- **Updated**: 2015-11-21 17:55:14+00:00
- **Authors**: Shervin Minaee, Amirali Abdolrashidi, Yao Wang
- **Comment**: Asilomar Conference on Signals, Systems and Computers, IEEE, 2015,
  (to Appear)
- **Journal**: None
- **Summary**: Sparse decomposition has been extensively used for different applications including signal compression and denoising and document analysis. In this paper, sparse decomposition is used for image segmentation. The proposed algorithm separates the background and foreground using a sparse-smooth decomposition technique such that the smooth and sparse components correspond to the background and foreground respectively. This algorithm is tested on several test images from HEVC test sequences and is shown to have superior performance over other methods, such as the hierarchical k-means clustering in DjVu. This segmentation algorithm can also be used for text extraction, video compression and medical image segmentation.



### Semantic Segmentation of Colon Glands with Deep Convolutional Neural Networks and Total Variation Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1511.06919v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06919v2)
- **Published**: 2015-11-21 20:13:24+00:00
- **Updated**: 2017-10-10 11:44:54+00:00
- **Authors**: Philipp Kainz, Michael Pfeiffer, Martin Urschler
- **Comment**: An extended version of this work has been published in PeerJ
  (https://doi.org/10.7717/peerj.3874), so please cite our journal version
  instead of this preprint
- **Journal**: None
- **Summary**: Segmentation of histopathology sections is an ubiquitous requirement in digital pathology and due to the large variability of biological tissue, machine learning techniques have shown superior performance over standard image processing methods. As part of the GlaS@MICCAI2015 colon gland segmentation challenge, we present a learning-based algorithm to segment glands in tissue of benign and malignant colorectal cancer. Images are preprocessed according to the Hematoxylin-Eosin staining protocol and two deep convolutional neural networks (CNN) are trained as pixel classifiers. The CNN predictions are then regularized using a figure-ground segmentation based on weighted total variation to produce the final segmentation result. On two test sets, our approach achieves a tissue classification accuracy of 98% and 94%, making use of the inherent capability of our system to distinguish between benign and malignant tissue.



### Real-Time Anomalous Behavior Detection and Localization in Crowded Scenes
- **Arxiv ID**: http://arxiv.org/abs/1511.07425v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07425v2)
- **Published**: 2015-11-21 22:42:53+00:00
- **Updated**: 2016-01-02 06:10:47+00:00
- **Authors**: Mohammad Sabokrou, Mahmood Fathy, Mojtaba Hosseini
- **Comment**: This paper has been withdrawn by the author due to some error in
  experimental result. There are some mistakes
- **Journal**: None
- **Summary**: In this paper, we propose an accurate and real-time anomaly detection and localization in crowded scenes, and two descriptors for representing anomalous behavior in video are proposed. We consider a video as being a set of cubic patches. Based on the low likelihood of an anomaly occurrence, and the redundancy of structures in normal patches in videos, two (global and local) views are considered for modeling the video. Our algorithm has two components, for (1) representing the patches using local and global descriptors, and for (2) modeling the training patches using a new representation. We have two Gaussian models for all training patches respect to global and local descriptors. The local and global features are based on structure similarity between adjacent patches and the features that are learned in an unsupervised way. We propose a fusion strategy to combine the two descriptors as the output of our system. Experimental results show that our algorithm performs like a state-of-the-art method on several standard datasets, but even is more time-efficient.



### Real-Time Anomaly Detection and Localization in Crowded Scenes
- **Arxiv ID**: http://arxiv.org/abs/1511.06936v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06936v1)
- **Published**: 2015-11-21 23:17:55+00:00
- **Updated**: 2015-11-21 23:17:55+00:00
- **Authors**: Mohammad Sabokrou, Mahmood Fathy, Mojtaba Hosseini, Reinhard Klette
- **Comment**: CVPRw 2015
- **Journal**: None
- **Summary**: In this paper, we propose a method for real-time anomaly detection and localization in crowded scenes. Each video is defined as a set of non-overlapping cubic patches, and is described using two local and global descriptors. These descriptors capture the video properties from different aspects. By incorporating simple and cost-effective Gaussian classifiers, we can distinguish normal activities and anomalies in videos. The local and global features are based on structure similarity between adjacent patches and the features learned in an unsupervised way, using a sparse auto- encoder. Experimental results show that our algorithm is comparable to a state-of-the-art procedure on UCSD ped2 and UMN benchmarks, but even more time-efficient. The experiments confirm that our system can reliably detect and localize anomalies as soon as they happen in a video.



