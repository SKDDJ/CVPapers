# Arxiv Papers in cs.CV on 2015-11-06
### Enhanced Low-Rank Matrix Approximation
- **Arxiv ID**: http://arxiv.org/abs/1511.01966v4
- **DOI**: 10.1109/LSP.2016.2535227
- **Categories**: **cs.CV**, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1511.01966v4)
- **Published**: 2015-11-06 01:19:18+00:00
- **Updated**: 2016-04-12 21:21:48+00:00
- **Authors**: Ankit Parekh, Ivan W. Selesnick
- **Comment**: 5 pages, 2 figures. MATLAB code available at https://goo.gl/xAi85N
- **Journal**: IEEE Signal Processing Letters, vol. 23, no. 4, pp.493-497, Apr.
  2016
- **Summary**: This letter proposes to estimate low-rank matrices by formulating a convex optimization problem with non-convex regularization. We employ parameterized non-convex penalty functions to estimate the non-zero singular values more accurately than the nuclear norm. A closed-form solution for the global optimum of the proposed objective function (sum of data fidelity and the non-convex regularizer) is also derived. The solution reduces to singular value thresholding method as a special case. The proposed method is demonstrated for image denoising.



### Next Generation Multicuts for Semi-Planar Graphs
- **Arxiv ID**: http://arxiv.org/abs/1511.01994v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1511.01994v1)
- **Published**: 2015-11-06 07:18:56+00:00
- **Updated**: 2015-11-06 07:18:56+00:00
- **Authors**: Julian Yarkony
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of multicut segmentation. We introduce modified versions of the Semi-PlanarCC based on bounding Lagrange multipliers. We apply our work to natural image segmentation.



### Facial Expression Recognition Using Sparse Gaussian Conditional Random Field
- **Arxiv ID**: http://arxiv.org/abs/1511.02023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.02023v1)
- **Published**: 2015-11-06 10:29:09+00:00
- **Updated**: 2015-11-06 10:29:09+00:00
- **Authors**: Mohammadamin Abbasnejad, Mohammad Ali Masnadi-Shirazi
- **Comment**: http://waset.org/abstracts/computer-and-information-engineering/26245. arXiv
  admin note: text overlap with arXiv:1509.01343 by other authors
- **Journal**: None
- **Summary**: The analysis of expression and facial Action Units (AUs) detection are very important tasks in fields of computer vision and Human Computer Interaction (HCI) due to the wide range of applications in human life. Many works has been done during the past few years which has their own advantages and disadvantages. In this work we present a new model based on Gaussian Conditional Random Field. We solve our objective problem using ADMM and we show how well the proposed model works. We train and test our work on two facial expression datasets, CK+ and RU-FACS. Experimental evaluation shows that our proposed approach outperform state of the art expression recognition.



### Pooling the Convolutional Layers in Deep ConvNets for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1511.02126v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.02126v1)
- **Published**: 2015-11-06 15:51:07+00:00
- **Updated**: 2015-11-06 15:51:07+00:00
- **Authors**: Shichao Zhao, Yanbin Liu, Yahong Han, Richang Hong
- **Comment**: None
- **Journal**: None
- **Summary**: Deep ConvNets have shown its good performance in image classification tasks. However it still remains as a problem in deep video representation for action recognition. The problem comes from two aspects: on one hand, current video ConvNets are relatively shallow compared with image ConvNets, which limits its capability of capturing the complex video action information; on the other hand, temporal information of videos is not properly utilized to pool and encode the video sequences. Towards these issues, in this paper, we utilize two state-of-the-art ConvNets, i.e., the very deep spatial net (VGGNet) and the temporal net from Two-Stream ConvNets, for action representation. The convolutional layers and the proposed new layer, called frame-diff layer, are extracted and pooled with two temporal pooling strategy: Trajectory pooling and line pooling. The pooled local descriptors are then encoded with VLAD to form the video representations. In order to verify the effectiveness of the proposed framework, we conduct experiments on UCF101 and HMDB51 datasets. It achieves the accuracy of 93.78\% on UCF101 which is the state-of-the-art and the accuracy of 65.62\% on HMDB51 which is comparable to the state-of-the-art.



### Seven ways to improve example-based single image super resolution
- **Arxiv ID**: http://arxiv.org/abs/1511.02228v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.02228v1)
- **Published**: 2015-11-06 20:58:20+00:00
- **Updated**: 2015-11-06 20:58:20+00:00
- **Authors**: Radu Timofte, Rasmus Rothe, Luc Van Gool
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: In this paper we present seven techniques that everybody should know to improve example-based single image super resolution (SR): 1) augmentation of data, 2) use of large dictionaries with efficient search structures, 3) cascading, 4) image self-similarities, 5) back projection refinement, 6) enhanced prediction by consistency check, and 7) context reasoning. We validate our seven techniques on standard SR benchmarks (i.e. Set5, Set14, B100) and methods (i.e. A+, SRCNN, ANR, Zeyde, Yang) and achieve substantial improvements.The techniques are widely applicable and require no changes or only minor adjustments of the SR methods. Moreover, our Improved A+ (IA) method sets new state-of-the-art results outperforming A+ by up to 0.9dB on average PSNR whilst maintaining a low time complexity.



### Learning Visual Features from Large Weakly Supervised Data
- **Arxiv ID**: http://arxiv.org/abs/1511.02251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.02251v1)
- **Published**: 2015-11-06 22:08:37+00:00
- **Updated**: 2015-11-06 22:08:37+00:00
- **Authors**: Armand Joulin, Laurens van der Maaten, Allan Jabri, Nicolas Vasilache
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional networks trained on large supervised dataset produce visual features which form the basis for the state-of-the-art in many computer-vision problems. Further improvements of these visual features will likely require even larger manually labeled data sets, which severely limits the pace at which progress can be made. In this paper, we explore the potential of leveraging massive, weakly-labeled image collections for learning good visual features. We train convolutional networks on a dataset of 100 million Flickr photos and captions, and show that these networks produce features that perform well in a range of vision problems. We also show that the networks appropriately capture word similarity, and learn correspondences between different languages.



