# Arxiv Papers in cs.CV on 2015-11-24
### Constrained Deep Metric Learning for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1511.07545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07545v1)
- **Published**: 2015-11-24 02:46:35+00:00
- **Updated**: 2015-11-24 02:46:35+00:00
- **Authors**: Hailin Shi, Xiangyu Zhu, Shengcai Liao, Zhen Lei, Yang Yang, Stan Z. Li
- **Comment**: 11 pages, 16 figures
- **Journal**: None
- **Summary**: Person re-identification aims to re-identify the probe image from a given set of images under different camera views. It is challenging due to large variations of pose, illumination, occlusion and camera view. Since the convolutional neural networks (CNN) have excellent capability of feature extraction, certain deep learning methods have been recently applied in person re-identification. However, in person re-identification, the deep networks often suffer from the over-fitting problem. In this paper, we propose a novel CNN-based method to learn a discriminative metric with good robustness to the over-fitting problem in person re-identification. Firstly, a novel deep architecture is built where the Mahalanobis metric is learned with a weight constraint. This weight constraint is used to regularize the learning, so that the learned metric has a better generalization ability. Secondly, we find that the selection of intra-class sample pairs is crucial for learning but has received little attention. To cope with the large intra-class variations in pedestrian images, we propose a novel training strategy named moderate positive mining to prevent the training process from over-fitting to the extreme samples in intra-class pairs. Experiments show that our approach significantly outperforms state-of-the-art methods on several benchmarks of person re-identification.



### DenseCap: Fully Convolutional Localization Networks for Dense Captioning
- **Arxiv ID**: http://arxiv.org/abs/1511.07571v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.07571v1)
- **Published**: 2015-11-24 05:13:54+00:00
- **Updated**: 2015-11-24 05:13:54+00:00
- **Authors**: Justin Johnson, Andrej Karpathy, Li Fei-Fei
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.



### Fine-Grain Annotation of Cricket Videos
- **Arxiv ID**: http://arxiv.org/abs/1511.07607v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.07607v2)
- **Published**: 2015-11-24 08:34:20+00:00
- **Updated**: 2017-09-27 10:48:11+00:00
- **Authors**: Rahul Anand Sharma, Pramod Sankar K, CV Jawahar
- **Comment**: ACPR 2015
- **Journal**: None
- **Summary**: The recognition of human activities is one of the key problems in video understanding. Action recognition is challenging even for specific categories of videos, such as sports, that contain only a small set of actions. Interestingly, sports videos are accompanied by detailed commentaries available online, which could be used to perform action annotation in a weakly-supervised setting. For the specific case of Cricket videos, we address the challenge of temporal segmentation and annotation of ctions with semantic descriptions. Our solution consists of two stages. In the first stage, the video is segmented into "scenes", by utilizing the scene category information extracted from text-commentary. The second stage consists of classifying video-shots as well as the phrases in the textual description into various categories. The relevant phrases are then suitably mapped to the video-shots. The novel aspect of this work is the fine temporal scale at which semantic information is assigned to the video. As a result of our approach, we enable retrieval of specific actions that last only a few seconds, from several hours of video. This solution yields a large number of labeled exemplars, with no manual effort, that could be used by machine learning algorithms to learn complex actions.



### Picking a Conveyor Clean by an Autonomously Learning Robot
- **Arxiv ID**: http://arxiv.org/abs/1511.07608v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.07608v1)
- **Published**: 2015-11-24 08:35:49+00:00
- **Updated**: 2015-11-24 08:35:49+00:00
- **Authors**: Janne V. Kujala, Tuomas J. Lukka, Harri Holopainen
- **Comment**: 6 pages, 8 figures
- **Journal**: None
- **Summary**: We present a research picking prototype related to our company's industrial waste sorting application. The goal of the prototype is to be as autonomous as possible and it both calibrates itself and improves its picking with minimal human intervention. The system learns to pick objects better based on a feedback sensor in its gripper and uses machine learning to choosing the best proposal from a random sample produced by simple hard-coded geometric models. We show experimentally the system improving its picking autonomously by measuring the pick success rate as function of time. We also show how this system can pick a conveyor belt clean, depositing 70 out of 80 objects in a difficult to manipulate pile of novel objects into the correct chute. We discuss potential improvements and next steps in this direction.



### Mouse Pose Estimation From Depth Images
- **Arxiv ID**: http://arxiv.org/abs/1511.07611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07611v1)
- **Published**: 2015-11-24 08:42:01+00:00
- **Updated**: 2015-11-24 08:42:01+00:00
- **Authors**: Ashwin Nanjappa, Li Cheng, Wei Gao, Chi Xu, Adam Claridge-Chang, Zoe Bichler
- **Comment**: None
- **Journal**: None
- **Summary**: We focus on the challenging problem of efficient mouse 3D pose estimation based on static images, and especially single depth images. We introduce an approach to discriminatively train the split nodes of trees in random forest to improve their performance on estimation of 3D joint positions of mouse. Our algorithm is capable of working with different types of rodents and with different types of depth cameras and imaging setups. In particular, it is demonstrated in this paper that when a top-mounted depth camera is combined with a bottom-mounted color camera, the final system is capable of delivering full-body pose estimation including four limbs and the paws. Empirical examinations on synthesized and real-world depth images confirm the applicability of our approach on mouse pose estimation, as well as the closely related task of part-based labeling of mouse.



### Searching for Objects using Structure in Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/1511.07710v1
- **DOI**: 10.5244/C.29.53
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1511.07710v1)
- **Published**: 2015-11-24 14:05:28+00:00
- **Updated**: 2015-11-24 14:05:28+00:00
- **Authors**: Varun K. Nagaraja, Vlad I. Morariu, Larry S. Davis
- **Comment**: Appeared in British Machine Vision Conference (BMVC) 2015
- **Journal**: None
- **Summary**: To identify the location of objects of a particular class, a passive computer vision system generally processes all the regions in an image to finally output few regions. However, we can use structure in the scene to search for objects without processing the entire image. We propose a search technique that sequentially processes image regions such that the regions that are more likely to correspond to the query class object are explored earlier. We frame the problem as a Markov decision process and use an imitation learning algorithm to learn a search strategy. Since structure in the scene is essential for search, we work with indoor scene images as they contain both unary scene context information and object-object context in the scene. We perform experiments on the NYU-depth v2 dataset and show that the unary scene context features alone can achieve a significantly high average precision while processing only 20-25\% of the regions for classes like bed and sofa. By considering object-object context along with the scene context features, the performance is further improved for classes like counter, lamp, pillow and sofa.



### Bayesian Identification of Fixations, Saccades, and Smooth Pursuits
- **Arxiv ID**: http://arxiv.org/abs/1511.07732v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.1; I.6.4; J.7
- **Links**: [PDF](http://arxiv.org/pdf/1511.07732v1)
- **Published**: 2015-11-24 14:40:05+00:00
- **Updated**: 2015-11-24 14:40:05+00:00
- **Authors**: Thiago Santini, Wolfgang Fuhl, Thomas KÃ¼bler, Enkelejda Kasneci
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Smooth pursuit eye movements provide meaningful insights and information on subject's behavior and health and may, in particular situations, disturb the performance of typical fixation/saccade classification algorithms. Thus, an automatic and efficient algorithm to identify these eye movements is paramount for eye-tracking research involving dynamic stimuli. In this paper, we propose the Bayesian Decision Theory Identification (I-BDT) algorithm, a novel algorithm for ternary classification of eye movements that is able to reliably separate fixations, saccades, and smooth pursuits in an online fashion, even for low-resolution eye trackers. The proposed algorithm is evaluated on four datasets with distinct mixtures of eye movements, including fixations, saccades, as well as straight and circular smooth pursuits; data was collected with a sample rate of 30 Hz from six subjects, totaling 24 evaluation datasets. The algorithm exhibits high and consistent performance across all datasets and movements relative to a manual annotation by a domain expert (recall: \mu = 91.42%, \sigma = 9.52%; precision: \mu = 95.60%, \sigma = 5.29%; specificity \mu = 95.41%, \sigma = 7.02%) and displays a significant improvement when compared to I-VDT, an state-of-the-art algorithm (recall: \mu = 87.67%, \sigma = 14.73%; precision: \mu = 89.57%, \sigma = 8.05%; specificity \mu = 92.10%, \sigma = 11.21%). For algorithm implementation and annotated datasets, please contact the first author.



### LocNet: Improving Localization Accuracy for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1511.07763v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.07763v2)
- **Published**: 2015-11-24 15:42:01+00:00
- **Updated**: 2016-04-07 15:09:15+00:00
- **Authors**: Spyros Gidaris, Nikos Komodakis
- **Comment**: Extended technical report -- short version to appear as oral paper on
  CVPR 2016. Code: https://github.com/gidariss/LocNet/
- **Journal**: None
- **Summary**: We propose a novel object localization methodology with the purpose of boosting the localization accuracy of state-of-the-art object detection systems. Our model, given a search region, aims at returning the bounding box of an object of interest inside this region. To accomplish its goal, it relies on assigning conditional probabilities to each row and column of this region, where these probabilities provide useful information regarding the location of the boundaries of the object inside the search region and allow the accurate inference of the object bounding box under a simple probabilistic framework.   For implementing our localization model, we make use of a convolutional neural network architecture that is properly adapted for this task, called LocNet. We show experimentally that LocNet achieves a very significant improvement on the mAP for high IoU thresholds on PASCAL VOC2007 test set and that it can be very easily coupled with recent state-of-the-art object detection systems, helping them to boost their performance. Finally, we demonstrate that our detection approach can achieve high detection accuracy even when it is given as input a set of sliding windows, thus proving that it is independent of box proposal methods.



### Weakly Supervised Object Boundaries
- **Arxiv ID**: http://arxiv.org/abs/1511.07803v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07803v1)
- **Published**: 2015-11-24 16:54:58+00:00
- **Updated**: 2015-11-24 16:54:58+00:00
- **Authors**: Anna Khoreva, Rodrigo Benenson, Mohamed Omran, Matthias Hein, Bernt Schiele
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art learning based boundary detection methods require extensive training data. Since labelling object boundaries is one of the most expensive types of annotations, there is a need to relax the requirement to carefully annotate images to make both the training more affordable and to extend the amount of training data. In this paper we propose a technique to generate weakly supervised annotations and show that bounding box annotations alone suffice to reach high-quality object boundaries without using any object-specific boundary annotations. With the proposed weak supervision techniques we achieve the top performance on the object boundary detection task, outperforming by a large margin the current fully supervised state-of-the-art methods.



### Shape and Symmetry Induction for 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/1511.07845v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07845v2)
- **Published**: 2015-11-24 19:48:42+00:00
- **Updated**: 2015-11-25 01:43:51+00:00
- **Authors**: Shubham Tulsiani, Abhishek Kar, Qixing Huang, JoÃ£o Carreira, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: Actions as simple as grasping an object or navigating around it require a rich understanding of that object's 3D shape from a given viewpoint. In this paper we repurpose powerful learning machinery, originally developed for object classification, to discover image cues relevant for recovering the 3D shape of potentially unfamiliar objects. We cast the problem as one of local prediction of surface normals and global detection of 3D reflection symmetry planes, which open the door for extrapolating occluded surfaces from visible ones. We demonstrate that our method is able to recover accurate 3D shape information for classes of objects it was not trained on, in both synthetic and real images.



### Context-aware CNNs for person head detection
- **Arxiv ID**: http://arxiv.org/abs/1511.07917v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.07917v1)
- **Published**: 2015-11-24 23:23:18+00:00
- **Updated**: 2015-11-24 23:23:18+00:00
- **Authors**: Tuan-Hung Vu, Anton Osokin, Ivan Laptev
- **Comment**: To appear in International Conference on Computer Vision (ICCV), 2015
- **Journal**: None
- **Summary**: Person detection is a key problem for many computer vision tasks. While face detection has reached maturity, detecting people under a full variation of camera view-points, human poses, lighting conditions and occlusions is still a difficult challenge. In this work we focus on detecting human heads in natural scenes. Starting from the recent local R-CNN object detector, we extend it with two types of contextual cues. First, we leverage person-scene relations and propose a Global CNN model trained to predict positions and scales of heads directly from the full image. Second, we explicitly model pairwise relations among objects and train a Pairwise CNN model using a structured-output surrogate loss. The Local, Global and Pairwise models are combined into a joint CNN framework. To train and test our full model, we introduce a large dataset composed of 369,846 human heads annotated in 224,740 movie frames. We evaluate our method and demonstrate improvements of person head detection against several recent baselines in three datasets. We also show improvements of the detection speed provided by our model.



