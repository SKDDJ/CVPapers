# Arxiv Papers in cs.CV on 2015-11-17
### Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1511.05234v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.05234v2)
- **Published**: 2015-11-17 01:00:04+00:00
- **Updated**: 2016-03-19 03:06:58+00:00
- **Authors**: Huijuan Xu, Kate Saenko
- **Comment**: include test-standard result on VQA full release (V1.0) dataset
- **Journal**: None
- **Summary**: We address the problem of Visual Question Answering (VQA), which requires joint image and language understanding to answer a question about a given photograph. Recent approaches have applied deep image captioning methods based on convolutional-recurrent networks to this problem, but have failed to model spatial inference. To remedy this, we propose a model we call the Spatial Memory Network and apply it to the VQA task. Memory networks are recurrent neural networks with an explicit attention mechanism that selects certain parts of the information stored in memory. Our Spatial Memory Network stores neuron activations from different spatial regions of the image in its memory, and uses the question to choose relevant regions for computing the answer, a process of which constitutes a single "hop" in the network. We propose a novel spatial attention architecture that aligns words with image patches in the first hop, and obtain improved results by adding a second attention hop which considers the whole question to choose visual evidence based on the results of the first hop. To better understand the inference process learned by the network, we design synthetic questions that specifically require spatial inference and visualize the attention weights. We evaluate our model on two published visual question answering datasets, DAQUAR [1] and VQA [2], and obtain improved results compared to a strong deep baseline model (iBOWIMG) which concatenates image and question features to predict the answer [3].



### Robust PCA via Nonconvex Rank Approximation
- **Arxiv ID**: http://arxiv.org/abs/1511.05261v1
- **DOI**: 10.1109/ICDM.2015.15
- **Categories**: **cs.CV**, cs.LG, cs.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1511.05261v1)
- **Published**: 2015-11-17 03:00:30+00:00
- **Updated**: 2015-11-17 03:00:30+00:00
- **Authors**: Zhao Kang, Chong Peng, Qiang Cheng
- **Comment**: IEEE International Conference on Data Mining
- **Journal**: None
- **Summary**: Numerous applications in data mining and machine learning require recovering a matrix of minimal rank. Robust principal component analysis (RPCA) is a general framework for handling this kind of problems. Nuclear norm based convex surrogate of the rank function in RPCA is widely investigated. Under certain assumptions, it can recover the underlying true low rank matrix with high probability. However, those assumptions may not hold in real-world applications. Since the nuclear norm approximates the rank by adding all singular values together, which is essentially a $\ell_1$-norm of the singular values, the resulting approximation error is not trivial and thus the resulting matrix estimator can be significantly biased. To seek a closer approximation and to alleviate the above-mentioned limitations of the nuclear norm, we propose a nonconvex rank approximation. This approximation to the matrix rank is tighter than the nuclear norm. To solve the associated nonconvex minimization problem, we develop an efficient augmented Lagrange multiplier based optimization algorithm. Experimental results demonstrate that our method outperforms current state-of-the-art algorithms in both accuracy and efficiency.



### Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data
- **Arxiv ID**: http://arxiv.org/abs/1511.05284v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1511.05284v2)
- **Published**: 2015-11-17 06:44:48+00:00
- **Updated**: 2016-04-27 23:40:55+00:00
- **Authors**: Lisa Anne Hendricks, Subhashini Venugopalan, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: While recent deep neural network models have achieved promising results on the image captioning task, they rely largely on the availability of corpora with paired image and sentence captions to describe objects in context. In this work, we propose the Deep Compositional Captioner (DCC) to address the task of generating descriptions of novel objects which are not present in paired image-sentence datasets. Our method achieves this by leveraging large object recognition datasets and external text corpora and by transferring knowledge between semantically similar concepts. Current deep caption models can only describe objects contained in paired image-sentence corpora, despite the fact that they are pre-trained with large object recognition datasets, namely ImageNet. In contrast, our model can compose sentences that describe novel objects and their interactions with other objects. We demonstrate our model's ability to describe novel concepts by empirically evaluating its performance on MSCOCO and show qualitative results on ImageNet images of objects for which no paired image-caption data exist. Further, we extend our approach to generate descriptions of objects in video clips. Our results show that DCC has distinct advantages over existing image and video captioning approaches for generating descriptions of new objects in context.



### Classifying and Segmenting Microscopy Images Using Convolutional Multiple Instance Learning
- **Arxiv ID**: http://arxiv.org/abs/1511.05286v1
- **DOI**: 10.1093/bioinformatics/btw252
- **Categories**: **cs.CV**, q-bio.SC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1511.05286v1)
- **Published**: 2015-11-17 06:55:58+00:00
- **Updated**: 2015-11-17 06:55:58+00:00
- **Authors**: Oren Z. Kraus, Lei Jimmy Ba, Brendan Frey
- **Comment**: None
- **Journal**: Bioinformatics (2016) 32 (12): i52-i59
- **Summary**: Convolutional neural networks (CNN) have achieved state of the art performance on both classification and segmentation tasks. Applying CNNs to microscopy images is challenging due to the lack of datasets labeled at the single cell level. We extend the application of CNNs to microscopy image classification and segmentation using multiple instance learning (MIL). We present the adaptive Noisy-AND MIL pooling function, a new MIL operator that is robust to outliers. Combining CNNs with MIL enables training CNNs using full resolution microscopy images with global labels. We base our approach on the similarity between the aggregation function used in MIL and pooling layers used in CNNs. We show that training MIL CNNs end-to-end outperforms several previous methods on both mammalian and yeast microscopy images without requiring any segmentation steps.



### Hierarchical Spatial Sum-Product Networks for Action Recognition in Still Images
- **Arxiv ID**: http://arxiv.org/abs/1511.05292v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05292v3)
- **Published**: 2015-11-17 07:21:20+00:00
- **Updated**: 2016-07-08 01:41:41+00:00
- **Authors**: Jinghua Wang, Gang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing actions from still images is popularly studied recently. In this paper, we model an action class as a flexible number of spatial configurations of body parts by proposing a new spatial SPN (Sum-Product Networks). First, we discover a set of parts in image collections via unsupervised learning. Then, our new spatial SPN is applied to model the spatial relationship and also the high-order correlations of parts. To learn robust networks, we further develop a hierarchical spatial SPN method, which models pairwise spatial relationship between parts inside sub-images and models the correlation of sub-images via extra layers of SPN. Our method is shown to be effective on two benchmark datasets.



### Towards Predicting the Likeability of Fashion Images
- **Arxiv ID**: http://arxiv.org/abs/1511.05296v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05296v2)
- **Published**: 2015-11-17 07:31:36+00:00
- **Updated**: 2015-11-23 07:26:25+00:00
- **Authors**: Jinghua Wang, Abrar Abdul Nabi, Gang Wang, Chengde Wan, Tian-Tsong Ng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a method for ranking fashion images to find the ones which might be liked by more people. We collect two new datasets from image sharing websites (Pinterest and Polyvore). We represent fashion images based on attributes: semantic attributes and data-driven attributes. To learn semantic attributes from limited training data, we use an algorithm on multi-task convolutional neural networks to share visual knowledge among different semantic attribute categories. To discover data-driven attributes unsupervisedly, we propose an algorithm to simultaneously discover visual clusters and learn fashion-specific feature representations. Given attributes as representations, we propose to learn a ranking SPN (sum product networks) to rank pairs of fashion images. The proposed ranking SPN can capture the high-order correlations of the attributes. We show the effectiveness of our method on our two newly collected datasets.



### Structural-RNN: Deep Learning on Spatio-Temporal Graphs
- **Arxiv ID**: http://arxiv.org/abs/1511.05298v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1511.05298v3)
- **Published**: 2015-11-17 07:49:58+00:00
- **Updated**: 2016-04-11 19:00:24+00:00
- **Authors**: Ashesh Jain, Amir R. Zamir, Silvio Savarese, Ashutosh Saxena
- **Comment**: CVPR 2016 (Oral)
- **Journal**: None
- **Summary**: Deep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can benefit from it. Spatio-temporal graphs are a popular tool for imposing such high-level intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks~(RNNs). We develop a scalable method for casting an arbitrary spatio-temporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well defined steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks.



### Deep multi-scale video prediction beyond mean square error
- **Arxiv ID**: http://arxiv.org/abs/1511.05440v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1511.05440v6)
- **Published**: 2015-11-17 15:36:32+00:00
- **Updated**: 2016-02-26 22:10:30+00:00
- **Authors**: Michael Mathieu, Camille Couprie, Yann LeCun
- **Comment**: None
- **Journal**: None
- **Summary**: Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset



### Learning Neural Network Architectures using Backpropagation
- **Arxiv ID**: http://arxiv.org/abs/1511.05497v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.05497v2)
- **Published**: 2015-11-17 18:26:11+00:00
- **Updated**: 2016-08-02 11:46:48+00:00
- **Authors**: Suraj Srinivas, R. Venkatesh Babu
- **Comment**: BMVC 2016 ; Title modified from 'Learning the Architecture of Deep
  Neural Networks'
- **Journal**: None
- **Summary**: Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We introduce a new trainable parameter called tri-state ReLU, which helps in eliminating unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with a considerably small number of parameters without affecting prediction accuracy.



### Moral Lineage Tracing
- **Arxiv ID**: http://arxiv.org/abs/1511.05512v2
- **DOI**: 10.1109/CVPR.2016.638
- **Categories**: **cs.CV**, cs.DM
- **Links**: [PDF](http://arxiv.org/pdf/1511.05512v2)
- **Published**: 2015-11-17 19:18:16+00:00
- **Updated**: 2016-11-08 10:42:12+00:00
- **Authors**: Florian Jug, Evgeny Levinkov, Corinna Blasse, Eugene W. Myers, Bjoern Andres
- **Comment**: 11 pages
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2016, pp. 5926-5935
- **Summary**: Lineage tracing, the tracking of living cells as they move and divide, is a central problem in biological image analysis. Solutions, called lineage forests, are key to understanding how the structure of multicellular organisms emerges. We propose an integer linear program (ILP) whose feasible solutions define a decomposition of each image in a sequence into cells (segmentation), and a lineage forest of cells across images (tracing). Unlike previous formulations, we do not constrain the set of decompositions, except by contracting pixels to superpixels. The main challenge, as we show, is to enforce the morality of lineages, i.e., the constraint that cells do not merge. To enforce morality, we introduce path-cut inequalities. To find feasible solutions of the NP-hard ILP, with certified bounds to the global optimum, we define efficient separation procedures and apply these as part of a branch-and-cut algorithm. We show the effectiveness of this approach by analyzing feasible solutions for real microscopy data in terms of bounds and run-time, and by their weighted edit distance to ground truth lineage forests traced by humans.



### Learning Articulated Motion Models from Visual and Lingual Signals
- **Arxiv ID**: http://arxiv.org/abs/1511.05526v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.05526v2)
- **Published**: 2015-11-17 19:55:34+00:00
- **Updated**: 2016-07-01 14:53:28+00:00
- **Authors**: Zhengyang Wu, Mohit Bansal, Matthew R. Walter
- **Comment**: None
- **Journal**: None
- **Summary**: In order for robots to operate effectively in homes and workplaces, they must be able to manipulate the articulated objects common within environments built for and by humans. Previous work learns kinematic models that prescribe this manipulation from visual demonstrations. Lingual signals, such as natural language descriptions and instructions, offer a complementary means of conveying knowledge of such manipulation models and are suitable to a wide range of interactions (e.g., remote manipulation). In this paper, we present a multimodal learning framework that incorporates both visual and lingual information to estimate the structure and parameters that define kinematic models of articulated objects. The visual signal takes the form of an RGB-D image stream that opportunistically captures object motion in an unprepared scene. Accompanying natural language descriptions of the motion constitute the lingual signal. We present a probabilistic language model that uses word embeddings to associate lingual verbs with their corresponding kinematic structures. By exploiting the complementary nature of the visual and lingual input, our method infers correct kinematic structures for various multiple-part objects on which the previous state-of-the-art, visual-only system fails. We evaluate our multimodal learning framework on a dataset comprised of a variety of household objects, and demonstrate a 36% improvement in model accuracy over the vision-only baseline.



### Return of Frustratingly Easy Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1511.05547v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.05547v2)
- **Published**: 2015-11-17 20:53:26+00:00
- **Updated**: 2015-12-09 05:39:43+00:00
- **Authors**: Baochen Sun, Jiashi Feng, Kate Saenko
- **Comment**: Fixed typos. Full paper to appear in AAAI-16. Extended Abstract of
  the full paper to appear in TASK-CV 2015 workshop
- **Journal**: None
- **Summary**: Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being "frustratingly easy" to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.



### Identifying the Absorption Bump with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1511.05607v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.05607v2)
- **Published**: 2015-11-17 22:27:05+00:00
- **Updated**: 2015-11-20 14:20:46+00:00
- **Authors**: Min Li, Sudeep Gaddam, Xiaolin Li, Yinan Zhao, Jingzhe Ma, Jian Ge
- **Comment**: None
- **Journal**: None
- **Summary**: The pervasive interstellar dust grains provide significant insights to understand the formation and evolution of the stars, planetary systems, and the galaxies, and may harbor the building blocks of life. One of the most effective way to analyze the dust is via their interaction with the light from background sources. The observed extinction curves and spectral features carry the size and composition information of dust. The broad absorption bump at 2175 Angstrom is the most prominent feature in the extinction curves. Traditionally, statistical methods are applied to detect the existence of the absorption bump. These methods require heavy preprocessing and the co-existence of other reference features to alleviate the influence from the noises. In this paper, we apply Deep Learning techniques to detect the broad absorption bump. We demonstrate the key steps for training the selected models and their results. The success of Deep Learning based method inspires us to generalize a common methodology for broader science discovery problems. We present our on-going work to build the DeepDis system for such kind of applications.



### Learning Structured Inference Neural Networks with Label Relations
- **Arxiv ID**: http://arxiv.org/abs/1511.05616v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.05616v4)
- **Published**: 2015-11-17 23:22:25+00:00
- **Updated**: 2016-10-24 18:20:20+00:00
- **Authors**: Hexiang Hu, Guang-Tong Zhou, Zhiwei Deng, Zicheng Liao, Greg Mori
- **Comment**: Conference on Computer Vision and Pattern Recognition(CVPR) 2016
- **Journal**: None
- **Summary**: Images of scenes have various objects as well as abundant attributes, and diverse levels of visual categorization are possible. A natural image could be assigned with fine-grained labels that describe major components, coarse-grained labels that depict high level abstraction or a set of labels that reveal attributes. Such categorization at different concept layers can be modeled with label graphs encoding label information. In this paper, we exploit this rich information with a state-of-art deep learning framework, and propose a generic structured model that leverages diverse label relations to improve image classification performance. Our approach employs a novel stacked label prediction neural network, capturing both inter-level and intra-level label semantics. We evaluate our method on benchmark image datasets, and empirical results illustrate the efficacy of our model.



### Predicting distributions with Linearizing Belief Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.05622v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.05622v4)
- **Published**: 2015-11-17 23:50:35+00:00
- **Updated**: 2016-05-02 03:22:01+00:00
- **Authors**: Yann N. Dauphin, David Grangier
- **Comment**: None
- **Journal**: None
- **Summary**: Conditional belief networks introduce stochastic binary variables in neural networks. Contrary to a classical neural network, a belief network can predict more than the expected value of the output $Y$ given the input $X$. It can predict a distribution of outputs $Y$ which is useful when an input can admit multiple outputs whose average is not necessarily a valid answer. Such networks are particularly relevant to inverse problems such as image prediction for denoising, or text to speech. However, traditional sigmoid belief networks are hard to train and are not suited to continuous problems. This work introduces a new family of networks called linearizing belief nets or LBNs. A LBN decomposes into a deep linear network where each linear unit can be turned on or off by non-deterministic binary latent units. It is a universal approximator of real-valued conditional distributions and can be trained using gradient descent. Moreover, the linear pathways efficiently propagate continuous information and they act as multiplicative skip-connections that help optimization by removing gradient diffusion. This yields a model which trains efficiently and improves the state-of-the-art on image denoising and facial expression generation with the Toronto faces dataset.



