# Arxiv Papers in cs.CV on 2015-11-19
### Stochastic gradient method with accelerated stochastic dynamics
- **Arxiv ID**: http://arxiv.org/abs/1511.06036v1
- **DOI**: 10.1088/1742-6596/699/1/012019
- **Categories**: **stat.ML**, cond-mat.dis-nn, cond-mat.stat-mech, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06036v1)
- **Published**: 2015-11-19 01:01:59+00:00
- **Updated**: 2015-11-19 01:01:59+00:00
- **Authors**: Masayuki Ohzeki
- **Comment**: 12 pages, proceedings for International Meeting on High-Dimensional
  Data Driven Science (HD3-2015)
  (http://www.sparse-modeling.jp/HD3-2015/index_e.html)
- **Journal**: None
- **Summary**: In this paper, we propose a novel technique to implement stochastic gradient methods, which are beneficial for learning from large datasets, through accelerated stochastic dynamics. A stochastic gradient method is based on mini-batch learning for reducing the computational cost when the amount of data is large. The stochasticity of the gradient can be mitigated by the injection of Gaussian noise, which yields the stochastic Langevin gradient method; this method can be used for Bayesian posterior sampling. However, the performance of the stochastic Langevin gradient method depends on the mixing rate of the stochastic dynamics. In this study, we propose violating the detailed balance condition to enhance the mixing rate. Recent studies have revealed that violating the detailed balance condition accelerates the convergence to a stationary state and reduces the correlation time between the samplings. We implement this violation of the detailed balance condition in the stochastic gradient Langevin method and test our method for a simple model to demonstrate its performance.



### A Hierarchical Deep Temporal Model for Group Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1511.06040v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06040v2)
- **Published**: 2015-11-19 01:33:35+00:00
- **Updated**: 2016-04-05 20:43:53+00:00
- **Authors**: Moustafa Ibrahim, Srikanth Muralidharan, Zhiwei Deng, Arash Vahdat, Greg Mori
- **Comment**: cs.cv Accepted to CVPR 2016
- **Journal**: None
- **Summary**: In group activity recognition, the temporal dynamics of the whole activity can be inferred based on the dynamics of the individual people representing the activity. We build a deep model to capture these dynamics based on LSTM (long-short term memory) models. To make use of these ob- servations, we present a 2-stage deep temporal model for the group activity recognition problem. In our model, a LSTM model is designed to represent action dynamics of in- dividual people in a sequence and another LSTM model is designed to aggregate human-level information for whole activity understanding. We evaluate our model over two datasets: the collective activity dataset and a new volley- ball dataset. Experimental results demonstrate that our proposed model improves group activity recognition perfor- mance with compared to baseline methods.



### What Objective Does Self-paced Learning Indeed Optimize?
- **Arxiv ID**: http://arxiv.org/abs/1511.06049v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06049v2)
- **Published**: 2015-11-19 02:55:18+00:00
- **Updated**: 2016-11-01 13:59:27+00:00
- **Authors**: Deyu Meng, Qian Zhao, Lu Jiang
- **Comment**: 25 pages, 1 figures
- **Journal**: None
- **Summary**: Self-paced learning (SPL) is a recently raised methodology designed through simulating the learning principle of humans/animals. A variety of SPL realization schemes have been designed for different computer vision and pattern recognition tasks, and empirically substantiated to be effective in these applications. However, the investigation on its theoretical insight is still a blank. To this issue, this study attempts to provide some new theoretical understanding under the SPL scheme. Specifically, we prove that the solving strategy on SPL accords with a majorization minimization algorithm implemented on a latent objective function. Furthermore, we find that the loss function contained in this latent objective has a similar configuration with non-convex regularized penalty (NSPR) known in statistics and machine learning. Such connection inspires us discovering more intrinsic relationship between SPL regimes and NSPR forms, like SCAD, LOG and EXP. The robustness insight under SPL can then be finely explained. We also analyze the capability of SPL on its easy loss prior embedding property, and provide an insightful interpretation to the effectiveness mechanism under previous SPL variations. Besides, we design a group-partial-order loss prior, which is especially useful to weakly labeled large-scale data processing tasks. Through applying SPL with this loss prior to the FCVID dataset, which is currently one of the biggest manually annotated video dataset, our method achieves state-of-the-art performance beyond previous methods, which further helps supports the proposed theoretical arguments.



### Compact Bilinear Pooling
- **Arxiv ID**: http://arxiv.org/abs/1511.06062v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06062v2)
- **Published**: 2015-11-19 05:34:35+00:00
- **Updated**: 2016-04-12 01:59:15+00:00
- **Authors**: Yang Gao, Oscar Beijbom, Ning Zhang, Trevor Darrell
- **Comment**: Camera ready version for CVPR
- **Journal**: None
- **Summary**: Bilinear models has been shown to achieve impressive performance on a wide range of visual tasks, such as semantic segmentation, fine grained recognition and face recognition. However, bilinear features are high dimensional, typically on the order of hundreds of thousands to a few million, which makes them impractical for subsequent analysis. We propose two compact bilinear representations with the same discriminative power as the full bilinear representation but with only a few thousand dimensions. Our compact representations allow back-propagation of classification errors enabling an end-to-end optimization of the visual recognition system. The compact bilinear representations are derived through a novel kernelized analysis of bilinear pooling which provide insights into the discriminative power of bilinear pooling, and a platform for further research in compact pooling methods. Experimentation illustrate the utility of the proposed representations for image classification and few-shot learning across several datasets.



### Deep Learning for Tactile Understanding From Visual and Haptic Data
- **Arxiv ID**: http://arxiv.org/abs/1511.06065v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06065v2)
- **Published**: 2015-11-19 05:52:15+00:00
- **Updated**: 2016-04-12 00:16:21+00:00
- **Authors**: Yang Gao, Lisa Anne Hendricks, Katherine J. Kuchenbecker, Trevor Darrell
- **Comment**: Camera ready version for ICRA 2016
- **Journal**: None
- **Summary**: Robots which interact with the physical world will benefit from a fine-grained tactile understanding of objects and surfaces. Additionally, for certain tasks, robots may need to know the haptic properties of an object before touching it. To enable better tactile understanding for robots, we propose a method of classifying surfaces with haptic adjectives (e.g., compressible or smooth) from both visual and physical interaction data. Humans typically combine visual predictions and feedback from physical interactions to accurately predict haptic properties and interact with the world. Inspired by this cognitive pattern, we propose and explore a purely visual haptic prediction model. Purely visual models enable a robot to "feel" without physical interaction. Furthermore, we demonstrate that using both visual and physical interaction signals together yields more accurate haptic classification. Our models take advantage of recent advances in deep neural networks by employing a unified approach to learning features for physical interaction and visual observations. Even though we employ little domain specific knowledge, our model still achieves better results than methods based on hand-designed features.



### Convolutional neural networks with low-rank regularization
- **Arxiv ID**: http://arxiv.org/abs/1511.06067v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1511.06067v3)
- **Published**: 2015-11-19 06:13:55+00:00
- **Updated**: 2016-02-14 03:46:09+00:00
- **Authors**: Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, Weinan E
- **Comment**: None
- **Journal**: None
- **Summary**: Large CNNs have delivered impressive performance in various computer vision applications. But the storage and computation requirements make it problematic for deploying these models on mobile devices. Recently, tensor decompositions have been used for speeding up CNNs. In this paper, we further develop the tensor decomposition technique. We propose a new algorithm for computing the low-rank tensor decomposition for removing the redundancy in the convolution kernels. The algorithm finds the exact global optimizer of the decomposition and is more effective than iterative methods. Based on the decomposition, we further propose a new method for training low-rank constrained CNNs from scratch. Interestingly, while achieving a significant speedup, sometimes the low-rank constrained CNNs delivers significantly better performance than their non-constrained counterparts. On the CIFAR-10 dataset, the proposed low-rank NIN model achieves $91.31\%$ accuracy (without data augmentation), which also improves upon state-of-the-art result. We evaluated the proposed method on CIFAR-10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet, NIN, VGG and GoogleNet with success. For example, the forward time of VGG-16 is reduced by half while the performance is still comparable. Empirical success suggests that low-rank tensor decompositions can be a very useful tool for speeding up large CNNs.



### Structured Depth Prediction in Challenging Monocular Video Sequences
- **Arxiv ID**: http://arxiv.org/abs/1511.06070v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06070v1)
- **Published**: 2015-11-19 06:42:45+00:00
- **Updated**: 2015-11-19 06:42:45+00:00
- **Authors**: Miaomiao Liu, Mathieu Salzmann, Xuming He
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of estimating the depth of a scene from a monocular video sequence. In particular, we handle challenging scenarios, such as non-translational camera motion and dynamic scenes, where traditional structure from motion and motion stereo methods do not apply. To this end, we first study the problem of depth estimation from a single image. In this context, we exploit the availability of a pool of images for which the depth is known, and formulate monocular depth estimation as a discrete-continuous optimization problem, where the continuous variables encode the depth of the superpixels in the input image, and the discrete ones represent relationships between neighboring superpixels. The solution to this discrete-continuous optimization problem is obtained by performing inference in a graphical model using particle belief propagation. To handle video sequences, we then extend our single image model to a two-frame one that naturally encodes short-range temporal consistency and inherently handles dynamic objects. Based on the prediction of this model, we then introduce a fully-connected pairwise CRF that accounts for longer range spatio-temporal interactions throughout a video. We demonstrate the effectiveness of our model in both the indoor and outdoor scenarios.



### Learning Deep Structure-Preserving Image-Text Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1511.06078v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06078v2)
- **Published**: 2015-11-19 07:17:49+00:00
- **Updated**: 2016-04-14 03:10:04+00:00
- **Authors**: Liwei Wang, Yin Li, Svetlana Lazebnik
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a method for learning joint embeddings of images and text using a two-branch neural network with multiple layers of linear projections followed by nonlinearities. The network is trained using a large margin objective that combines cross-view ranking constraints with within-view neighborhood structure preservation constraints inspired by metric learning literature. Extensive experiments show that our approach gains significant improvements in accuracy for image-to-text and text-to-image retrieval. Our method achieves new state-of-the-art results on the Flickr30K and MSCOCO image-sentence datasets and shows promise on the new task of phrase localization on the Flickr30K Entities dataset.



### Variable Rate Image Compression with Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.06085v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.06085v5)
- **Published**: 2015-11-19 07:50:46+00:00
- **Updated**: 2016-03-01 22:13:44+00:00
- **Authors**: George Toderici, Sean M. O'Malley, Sung Jin Hwang, Damien Vincent, David Minnen, Shumeet Baluja, Michele Covell, Rahul Sukthankar
- **Comment**: Under review as a conference paper at ICLR 2016
- **Journal**: None
- **Summary**: A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will significantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32$\times$32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10% or more.



### Principled Parallel Mean-Field Inference for Discrete Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1511.06103v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06103v2)
- **Published**: 2015-11-19 09:44:20+00:00
- **Updated**: 2015-12-03 10:26:03+00:00
- **Authors**: Pierre Baqué, Timur Bagautdinov, François Fleuret, Pascal Fua
- **Comment**: The first two authors contributed equally
- **Journal**: None
- **Summary**: Mean-field variational inference is one of the most popular approaches to inference in discrete random fields. Standard mean-field optimization is based on coordinate descent and in many situations can be impractical. Thus, in practice, various parallel techniques are used, which either rely on ad-hoc smoothing with heuristically set parameters, or put strong constraints on the type of models. In this paper, we propose a novel proximal gradient-based approach to optimizing the variational objective. It is naturally parallelizable and easy to implement. We prove its convergence, and then demonstrate that, in practice, it yields faster convergence and often finds better optima than more traditional mean-field optimization techniques. Moreover, our method is less sensitive to the choice of parameters.



### Semi-supervised Learning for Convolutional Neural Networks via Online Graph Construction
- **Arxiv ID**: http://arxiv.org/abs/1511.06104v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06104v2)
- **Published**: 2015-11-19 09:44:57+00:00
- **Updated**: 2016-01-19 00:56:08+00:00
- **Authors**: Sheng-Yi Bai, Sebastian Agethen, Ting-Hsuan Chao, Winston Hsu
- **Comment**: As the original submission of iclr is withdrawn, the arxiv submission
  should be withdrawn as well
- **Journal**: None
- **Summary**: The recent promising achievements of deep learning rely on the large amount of labeled data. Considering the abundance of data on the web, most of them do not have labels at all. Therefore, it is important to improve generalization performance using unlabeled data on supervised tasks with few labeled instances. In this work, we revisit graph-based semi-supervised learning algorithms and propose an online graph construction technique which suits deep convolutional neural network better. We consider an EM-like algorithm for semi-supervised learning on deep neural networks: In forward pass, the graph is constructed based on the network output, and the graph is then used for loss calculation to help update the network by back propagation in the backward pass. We demonstrate the strength of our online approach compared to the conventional ones whose graph is constructed on static but not robust enough feature representations beforehand.



### Quantitative Analysis of Particles Segregation
- **Arxiv ID**: http://arxiv.org/abs/1511.06106v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06106v2)
- **Published**: 2015-11-19 09:57:14+00:00
- **Updated**: 2015-11-27 03:09:58+00:00
- **Authors**: Ting Peng, Aiping Qu, Xiaoling Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Segregation is a popular phenomenon. It has considerable effects on material performance. To the author's knowledge, there is still no automated objective quantitative indicator for segregation. In order to full fill this task, segregation of particles is analyzed. Edges of the particles are extracted from the digital picture. Then, the whole picture of particles is splintered to small rectangles with the same shape. Statistical index of the edges in each rectangle is calculated. Accordingly, segregation between the indexes corresponding to the rectangles is evaluated. The results show coincident with subjective evaluated results. Further more, it can be implemented as an automated system, which would facilitate the materials quality control mechanism during production process.



### Coreset-Based Adaptive Tracking
- **Arxiv ID**: http://arxiv.org/abs/1511.06147v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06147v1)
- **Published**: 2015-11-19 12:59:20+00:00
- **Updated**: 2015-11-19 12:59:20+00:00
- **Authors**: Abhimanyu Dubey, Nikhil Naik, Dan Raviv, Rahul Sukthankar, Ramesh Raskar
- **Comment**: 8 pages, 5 figures, In submission to IEEE TPAMI (Transactions on
  Pattern Analysis and Machine Intelligence)
- **Journal**: None
- **Summary**: We propose a method for learning from streaming visual data using a compact, constant size representation of all the data that was seen until a given moment. Specifically, we construct a 'coreset' representation of streaming data using a parallelized algorithm, which is an approximation of a set with relation to the squared distances between this set and all other points in its ambient space. We learn an adaptive object appearance model from the coreset tree in constant time and logarithmic space and use it for object tracking by detection. Our method obtains excellent results for object tracking on three standard datasets over more than 100 videos. The ability to summarize data efficiently makes our method ideally suited for tracking in long videos in presence of space and time constraints. We demonstrate this ability by outperforming a variety of algorithms on the TLD dataset with 2685 frames on average. This coreset based learning approach can be applied for both real-time learning of small, varied data and fast learning of big data.



### What Players do with the Ball: A Physically Constrained Interaction Modeling
- **Arxiv ID**: http://arxiv.org/abs/1511.06181v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06181v2)
- **Published**: 2015-11-19 14:25:04+00:00
- **Updated**: 2015-12-01 10:56:34+00:00
- **Authors**: Andrii Maksai, Xinchao Wang, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Tracking the ball is critical for video-based analysis of team sports. However, it is difficult, especially in low-resolution images, due to the small size of the ball, its speed that creates motion blur, and its often being occluded by players. In this paper, we propose a generic and principled approach to modeling the interaction between the ball and the players while also imposing appropriate physical constraints on the ball's trajectory. We show that our approach, formulated in terms of a Mixed Integer Program, is more robust and more accurate than several state-of-the-art approaches on real-life volleyball, basketball, and soccer sequences.



### Automatically selecting inference algorithms for discrete energy minimisation
- **Arxiv ID**: http://arxiv.org/abs/1511.06214v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06214v2)
- **Published**: 2015-11-19 15:45:02+00:00
- **Updated**: 2016-08-09 16:56:57+00:00
- **Authors**: Paul Henderson, Vittorio Ferrari
- **Comment**: None
- **Journal**: None
- **Summary**: Minimisation of discrete energies defined over factors is an important problem in computer vision, and a vast number of MAP inference algorithms have been proposed. Different inference algorithms perform better on factor graph models (GMs) from different underlying problem classes, and in general it is difficult to know which algorithm will yield the lowest energy for a given GM. To mitigate this difficulty, survey papers advise the practitioner on what algorithms perform well on what classes of models. We take the next step forward, and present a technique to automatically select the best inference algorithm for an input GM. We validate our method experimentally on an extended version of the OpenGM2 benchmark, containing a diverse set of vision problems. On average, our method selects an inference algorithm yielding labellings with 96% of variables the same as the best available algorithm.



### Towards Open Set Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.06233v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06233v1)
- **Published**: 2015-11-19 16:13:55+00:00
- **Updated**: 2015-11-19 16:13:55+00:00
- **Authors**: Abhijit Bendale, Terrance Boult
- **Comment**: None
- **Journal**: None
- **Summary**: Deep networks have produced significant gains for various visual recognition problems, leading to high impact academic and commercial applications. Recent work in deep networks highlighted that it is easy to generate images that humans would never classify as a particular object class, yet networks classify such images high confidence as that given class - deep network are easily fooled with images humans do not consider meaningful. The closed set nature of deep networks forces them to choose from one of the known classes leading to such artifacts. Recognition in the real world is open set, i.e. the recognition system should reject unknown/unseen classes at test time. We present a methodology to adapt deep networks for open set recognition, by introducing a new model layer, OpenMax, which estimates the probability of an input being from an unknown class. A key element of estimating the unknown probability is adapting Meta-Recognition concepts to the activation patterns in the penultimate layer of the network. OpenMax allows rejection of "fooling" and unrelated open set images presented to the system; OpenMax greatly reduces the number of obvious errors made by a deep network. We prove that the OpenMax concept provides bounded open space risk, thereby formally providing an open set recognition solution. We evaluate the resulting open set deep networks using pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation data, and thousands of fooling and open set images. The proposed OpenMax model significantly outperforms open set recognition accuracy of basic deep networks as well as deep networks with thresholding of SoftMax probabilities.



### Multimodal sparse representation learning and applications
- **Arxiv ID**: http://arxiv.org/abs/1511.06238v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1511.06238v3)
- **Published**: 2015-11-19 16:26:24+00:00
- **Updated**: 2016-03-02 19:22:48+00:00
- **Authors**: Miriam Cha, Youngjune Gwon, H. T. Kung
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised methods have proven effective for discriminative tasks in a single-modality scenario. In this paper, we present a multimodal framework for learning sparse representations that can capture semantic correlation between modalities. The framework can model relationships at a higher level by forcing the shared sparse representation. In particular, we propose the use of joint dictionary learning technique for sparse coding and formulate the joint representation for concision, cross-modal representations (in case of a missing modality), and union of the cross-modal representations. Given the accelerated growth of multimodal data posted on the Web such as YouTube, Wikipedia, and Twitter, learning good multimodal features is becoming increasingly important. We show that the shared representations enabled by our framework substantially improve the classification performance under both unimodal and multimodal settings. We further show how deep architectures built on the proposed framework are effective for the case of highly nonlinear correlations between modalities. The effectiveness of our approach is demonstrated experimentally in image denoising, multimedia event detection and retrieval on the TRECVID dataset (audio-video), category classification on the Wikipedia dataset (image-text), and sentiment classification on PhotoTweet (image-text).



### Convolutional Clustering for Unsupervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1511.06241v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06241v2)
- **Published**: 2015-11-19 16:31:46+00:00
- **Updated**: 2016-02-16 16:46:53+00:00
- **Authors**: Aysegul Dundar, Jonghoon Jin, Eugenio Culurciello
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: The task of labeling data for training deep neural networks is daunting and tedious, requiring millions of labels to achieve the current state-of-the-art results. Such reliance on large amounts of labeled data can be relaxed by exploiting hierarchical features via unsupervised learning techniques. In this work, we propose to train a deep convolutional network based on an enhanced version of the k-means clustering algorithm, which reduces the number of correlated parameters in the form of similar filters, and thus increases test categorization accuracy. We call our algorithm convolutional k-means clustering. We further show that learning the connection between the layers of a deep convolutional neural network improves its ability to be trained on a smaller amount of labeled data. Our experiments show that the proposed algorithm outperforms other techniques that learn filters unsupervised. Specifically, we obtained a test accuracy of 74.1% on STL-10 and a test error of 0.5% on MNIST.



### Faster method for Deep Belief Network based Object classification using DWT
- **Arxiv ID**: http://arxiv.org/abs/1511.06276v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06276v1)
- **Published**: 2015-11-19 17:41:08+00:00
- **Updated**: 2015-11-19 17:41:08+00:00
- **Authors**: Saurabh Sihag, Pranab Kumar Dutta
- **Comment**: None
- **Journal**: None
- **Summary**: A Deep Belief Network (DBN) requires large, multiple hidden layers with high number of hidden units to learn good features from the raw pixels of large images. This implies more training time as well as computational complexity. By integrating DBN with Discrete Wavelet Transform (DWT), both training time and computational complexity can be reduced. The low resolution images obtained after application of DWT are used to train multiple DBNs. The results obtained from these DBNs are combined using a weighted voting algorithm. The performance of this method is found to be competent and faster in comparison with that of traditional DBNs.



### Density Modeling of Images using a Generalized Normalization Transformation
- **Arxiv ID**: http://arxiv.org/abs/1511.06281v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06281v4)
- **Published**: 2015-11-19 17:52:01+00:00
- **Updated**: 2016-02-29 21:07:30+00:00
- **Authors**: Johannes Ballé, Valero Laparra, Eero P. Simoncelli
- **Comment**: published as a conference paper at ICLR 2016
- **Journal**: Int'l Conf on Learning Representations (ICLR), San Juan, Puerto
  Rico, May 2016
- **Summary**: We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.



### Foveation-based Mechanisms Alleviate Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1511.06292v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06292v3)
- **Published**: 2015-11-19 18:35:07+00:00
- **Updated**: 2016-01-19 18:15:28+00:00
- **Authors**: Yan Luo, Xavier Boix, Gemma Roig, Tomaso Poggio, Qi Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: We show that adversarial examples, i.e., the visually imperceptible perturbations that result in Convolutional Neural Networks (CNNs) fail, can be alleviated with a mechanism based on foveations---applying the CNN in different image regions. To see this, first, we report results in ImageNet that lead to a revision of the hypothesis that adversarial perturbations are a consequence of CNNs acting as a linear classifier: CNNs act locally linearly to changes in the image regions with objects recognized by the CNN, and in other regions the CNN may act non-linearly. Then, we corroborate that when the neural responses are linear, applying the foveation mechanism to the adversarial example tends to significantly reduce the effect of the perturbation. This is because, hypothetically, the CNNs for ImageNet are robust to changes of scale and translation of the object produced by the foveation, but this property does not generalize to transformations of the perturbation. As a result, the accuracy after a foveation is almost the same as the accuracy of the CNN without the adversarial perturbation, even if the adversarial perturbation is calculated taking into account a foveation.



### Robust Convolutional Neural Networks under Adversarial Noise
- **Arxiv ID**: http://arxiv.org/abs/1511.06306v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06306v2)
- **Published**: 2015-11-19 18:51:08+00:00
- **Updated**: 2016-02-25 16:30:04+00:00
- **Authors**: Jonghoon Jin, Aysegul Dundar, Eugenio Culurciello
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Recent studies have shown that Convolutional Neural Networks (CNNs) are vulnerable to a small perturbation of input called "adversarial examples". In this work, we propose a new feedforward CNN that improves robustness in the presence of adversarial noise. Our model uses stochastic additive noise added to the input image and to the CNN models. The proposed model operates in conjunction with a CNN trained with either standard or adversarial objective function. In particular, convolution, max-pooling, and ReLU layers are modified to benefit from the noise model. Our feedforward model is parameterized by only a mean and variance per pixel which simplifies computations and makes our method scalable to a deep architecture. From CIFAR-10 and ImageNet test, the proposed model outperforms other methods and the improvement is more evident for difficult classification tasks or stronger adversarial noise.



### Spatio-temporal video autoencoder with differentiable memory
- **Arxiv ID**: http://arxiv.org/abs/1511.06309v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06309v5)
- **Published**: 2015-11-19 19:06:28+00:00
- **Updated**: 2016-09-01 11:36:40+00:00
- **Authors**: Viorica Patraucean, Ankur Handa, Roberto Cipolla
- **Comment**: The experiments section has been extended and a direct application to
  weakly-supervised video segmentation through label propagation has been
  included
- **Journal**: None
- **Summary**: We describe a new spatio-temporal video autoencoder, based on a classic spatial image autoencoder and a novel nested temporal autoencoder. The temporal encoder is represented by a differentiable visual memory composed of convolutional long short-term memory (LSTM) cells that integrate changes over time. Here we target motion changes and use as temporal decoder a robust optical flow prediction module together with an image sampler serving as built-in feedback loop. The architecture is end-to-end differentiable. At each time step, the system receives as input a video frame, predicts the optical flow based on the current observation and the LSTM memory state as a dense transformation map, and applies it to the current frame to generate the next frame. By minimising the reconstruction error between the predicted next frame and the corresponding ground truth next frame, we train the whole system to extract features useful for motion estimation without any supervision effort. We present one direct application of the proposed framework in weakly-supervised semantic segmentation of videos through label propagation using optical flow.



### Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.06314v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.06314v1)
- **Published**: 2015-11-19 19:19:58+00:00
- **Updated**: 2015-11-19 19:19:58+00:00
- **Authors**: Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, Dhruv Batra
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks have achieved state-of-the-art performance on a wide range of tasks. Most benchmarks are led by ensembles of these powerful learners, but ensembling is typically treated as a post-hoc procedure implemented by averaging independently trained models with model variation induced by bagging or random initialization. In this paper, we rigorously treat ensembling as a first-class problem to explicitly address the question: what are the best strategies to create an ensemble? We first compare a large number of ensembling strategies, and then propose and evaluate novel strategies, such as parameter sharing (through a new family of models we call TreeNets) as well as training under ensemble-aware and diversity-encouraging losses. We demonstrate that TreeNets can improve ensemble performance and that diverse ensembles can be trained end-to-end under a unified loss, achieving significantly higher "oracle" accuracies than classical ensembles.



### face anti-spoofing based on color texture analysis
- **Arxiv ID**: http://arxiv.org/abs/1511.06316v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06316v1)
- **Published**: 2015-11-19 19:28:20+00:00
- **Updated**: 2015-11-19 19:28:20+00:00
- **Authors**: Zinelabidine Boulkenafet, Jukka Komulainen, Abdenour Hadid
- **Comment**: None
- **Journal**: None
- **Summary**: Research on face spoofing detection has mainly been focused on analyzing the luminance of the face images, hence discarding the chrominance information which can be useful for discriminating fake faces from genuine ones. In this work, we propose a new face anti-spoofing method based on color texture analysis. We analyze the joint color-texture information from the luminance and the chrominance channels using a color local binary pattern descriptor. More specifically, the feature histograms are extracted from each image band separately. Extensive experiments on two benchmark datasets, namely CASIA face anti-spoofing and Replay-Attack databases, showed excellent results compared to the state-of-the-art. Most importantly, our inter-database evaluation depicts that the proposed approach showed very promising generalization capabilities.



### Unsupervised Deep Embedding for Clustering Analysis
- **Arxiv ID**: http://arxiv.org/abs/1511.06335v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06335v2)
- **Published**: 2015-11-19 20:06:14+00:00
- **Updated**: 2016-05-24 22:27:35+00:00
- **Authors**: Junyuan Xie, Ross Girshick, Ali Farhadi
- **Comment**: icml2016
- **Journal**: None
- **Summary**: Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.



### Robust Classification by Pre-conditioned LASSO and Transductive Diffusion Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/1511.06340v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.ST, stat.ML, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1511.06340v2)
- **Published**: 2015-11-19 20:13:51+00:00
- **Updated**: 2019-12-25 02:06:46+00:00
- **Authors**: Yanwei Fu, De-An Huang, Leonid Sigal
- **Comment**: we will significantly change the content of this paper which makes it
  another paper. In order not to misleading, we decided to withdraw it. The
  updated version can not be shared currently, for some reason. We will update
  it once it is OK to be shared
- **Journal**: None
- **Summary**: Modern machine learning-based recognition approaches require large-scale datasets with large number of labelled training images. However, such datasets are inherently difficult and costly to collect and annotate. Hence there is a great and growing interest in automatic dataset collection methods that can leverage the web. % which are collected % in a cheap, efficient and yet unreliable way. Collecting datasets in this way, however, requires robust and efficient ways for detecting and excluding outliers that are common and prevalent. % Outliers are thus a % prominent treat of using these dataset. So far, there have been a limited effort in machine learning community to directly detect outliers for robust classification. Inspired by the recent work on Pre-conditioned LASSO, this paper formulates the outlier detection task using Pre-conditioned LASSO and employs \red{unsupervised} transductive diffusion component analysis to both integrate the topological structure of the data manifold, from labeled and unlabeled instances, and reduce the feature dimensionality. Synthetic experiments as well as results on two real-world classification tasks show that our framework can robustly detect the outliers and improve classification.



### How much data is needed to train a medical image deep learning system to achieve necessary high accuracy?
- **Arxiv ID**: http://arxiv.org/abs/1511.06348v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.06348v2)
- **Published**: 2015-11-19 20:38:43+00:00
- **Updated**: 2016-01-07 21:08:10+00:00
- **Authors**: Junghwan Cho, Kyewook Lee, Ellie Shin, Garry Choy, Synho Do
- **Comment**: None
- **Journal**: None
- **Summary**: The use of Convolutional Neural Networks (CNN) in natural image classification systems has produced very impressive results. Combined with the inherent nature of medical images that make them ideal for deep-learning, further application of such systems to medical image classification holds much promise. However, the usefulness and potential impact of such a system can be completely negated if it does not reach a target accuracy. In this paper, we present a study on determining the optimum size of the training data set necessary to achieve high classification accuracy with low variance in medical image classification systems. The CNN was applied to classify axial Computed Tomography (CT) images into six anatomical classes. We trained the CNN using six different sizes of training data set (5, 10, 20, 50, 100, and 200) and then tested the resulting system with a total of 6000 CT images. All images were acquired from the Massachusetts General Hospital (MGH) Picture Archiving and Communication System (PACS). Using this data, we employ the learning curve approach to predict classification accuracy at a given training sample size. Our research will present a general methodology for determining the training data set size necessary to achieve a certain target classification accuracy that can be easily applied to other problems within such systems.



### FRIST - Flipping and Rotation Invariant Sparsifying Transform Learning and Applications
- **Arxiv ID**: http://arxiv.org/abs/1511.06359v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06359v4)
- **Published**: 2015-11-19 20:55:49+00:00
- **Updated**: 2017-10-16 02:42:20+00:00
- **Authors**: Bihan Wen, Saiprasad Ravishankar, Yoram Bresler
- **Comment**: Published in Inverse Problems
- **Journal**: None
- **Summary**: Features based on sparse representation, especially using the synthesis dictionary model, have been heavily exploited in signal processing and computer vision. However, synthesis dictionary learning typically involves NP-hard sparse coding and expensive learning steps. Recently, sparsifying transform learning received interest for its cheap computation and its optimal updates in the alternating algorithms. In this work, we develop a methodology for learning Flipping and Rotation Invariant Sparsifying Transforms, dubbed FRIST, to better represent natural images that contain textures with various geometrical directions. The proposed alternating FRIST learning algorithm involves efficient optimal updates. We provide a convergence guarantee, and demonstrate the empirical convergence behavior of the proposed FRIST learning approach. Preliminary experiments show the promising performance of FRIST learning for sparse image representation, segmentation, denoising, robust inpainting, and compressed sensing-based magnetic resonance image reconstruction.



### Order-Embeddings of Images and Language
- **Arxiv ID**: http://arxiv.org/abs/1511.06361v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06361v6)
- **Published**: 2015-11-19 20:56:14+00:00
- **Updated**: 2016-03-01 08:23:50+00:00
- **Authors**: Ivan Vendrov, Ryan Kiros, Sanja Fidler, Raquel Urtasun
- **Comment**: ICLR camera-ready version
- **Journal**: None
- **Summary**: Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.



### Efficient inference in occlusion-aware generative models of images
- **Arxiv ID**: http://arxiv.org/abs/1511.06362v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06362v2)
- **Published**: 2015-11-19 20:56:27+00:00
- **Updated**: 2016-02-16 07:22:02+00:00
- **Authors**: Jonathan Huang, Kevin Murphy
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We present a generative model of images based on layering, in which image layers are individually generated, then composited from front to back. We are thus able to factor the appearance of an image into the appearance of individual objects within the image --- and additionally for each individual object, we can factor content from pose. Unlike prior work on layered models, we learn a shape prior for each object/layer, allowing the model to tease out which object is in front by looking for a consistent shape, without needing access to motion cues or any labeled data. We show that ordinary stochastic gradient variational bayes (SGVB), which optimizes our fully differentiable lower-bound on the log-likelihood, is sufficient to learn an interpretable representation of images. Finally we present experiments demonstrating the effectiveness of the model for inferring foreground and background objects in images.



### Unsupervised Learning of Visual Structure using Predictive Generative Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.06380v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1511.06380v2)
- **Published**: 2015-11-19 21:10:17+00:00
- **Updated**: 2016-01-20 05:50:46+00:00
- **Authors**: William Lotter, Gabriel Kreiman, David Cox
- **Comment**: under review as conference paper at ICLR 2016
- **Journal**: None
- **Summary**: The ability to predict future states of the environment is a central pillar of intelligence. At its core, effective prediction requires an internal model of the world and an understanding of the rules by which the world changes. Here, we explore the internal models developed by deep neural networks trained using a loss based on predicting future frames in synthetic video sequences, using a CNN-LSTM-deCNN framework. We first show that this architecture can achieve excellent performance in visual sequence prediction tasks, including state-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever et al., 2009). Using a weighted mean-squared error and adversarial loss (Goodfellow et al., 2014), the same architecture successfully extrapolates out-of-the-plane rotations of computer-generated faces. Furthermore, despite being trained end-to-end to predict only pixel-level information, our Predictive Generative Networks learn a representation of the latent structure of the underlying three-dimensional objects themselves. Importantly, we find that this representation is naturally tolerant to object transformations, and generalizes well to new tasks, such as classification of static images. Similar models trained solely with a reconstruction loss fail to generalize as effectively. We argue that prediction can serve as a powerful unsupervised loss for learning rich internal representations of high-level object features.



### Manifold Regularized Deep Neural Networks using Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1511.06381v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06381v2)
- **Published**: 2015-11-19 21:10:29+00:00
- **Updated**: 2016-01-14 16:35:11+00:00
- **Authors**: Taehoon Lee, Minsuk Choi, Sungroh Yoon
- **Comment**: Figure 2, 5, 7, and several descriptions revised
- **Journal**: None
- **Summary**: Learning meaningful representations using deep neural networks involves designing efficient training schemes and well-structured networks. Currently, the method of stochastic gradient descent that has a momentum with dropout is one of the most popular training protocols. Based on that, more advanced methods (i.e., Maxout and Batch Normalization) have been proposed in recent years, but most still suffer from performance degradation caused by small perturbations, also known as adversarial examples. To address this issue, we propose manifold regularized networks (MRnet) that utilize a novel training objective function that minimizes the difference between multi-layer embedding results of samples and those adversarial. Our experimental results demonstrated that MRnet is more resilient to adversarial examples and helps us to generalize representations on manifolds. Furthermore, combining MRnet and dropout allowed us to achieve competitive classification performances for three well-known benchmarks: MNIST, CIFAR-10, and SVHN.



### Geodesics of learned representations
- **Arxiv ID**: http://arxiv.org/abs/1511.06394v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06394v4)
- **Published**: 2015-11-19 21:40:13+00:00
- **Updated**: 2016-02-22 17:42:25+00:00
- **Authors**: Olivier J. Hénaff, Eero P. Simoncelli
- **Comment**: Published as a conference paper at ICLR 2016
- **Journal**: Presented at: Int'l Conf on Learning Representations (ICLR), San
  Juan, Puerto Rico, May 2016
- **Summary**: We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a "representational geodesic"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation. We use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.



### Feature-based Attention in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.06408v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06408v2)
- **Published**: 2015-11-19 21:57:27+00:00
- **Updated**: 2015-12-09 19:31:12+00:00
- **Authors**: Grace W. Lindsay
- **Comment**: 9 pages (plus 3 page Appendix), 7 figures total, submitted to ICLR
  2016
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have proven effective for image processing tasks, such as object recognition and classification. Recently, CNNs have been enhanced with concepts of attention, similar to those found in biology. Much of this work on attention has focused on effective serial spatial processing. In this paper, I introduce a simple procedure for applying feature-based attention (FBA) to CNNs and compare multiple implementation options. FBA is a top-down signal applied globally to an input image which aides in detecting chosen objects in cluttered or noisy settings. The concept of FBA and the implementation details tested here were derived from what is known (and debated) about biological object- and feature-based attention. The implementations of FBA described here increase performance on challenging object detection tasks using a procedure that is simple, fast, and does not require additional iterative training. Furthermore, the comparisons performed here suggest that a proposed model of biological FBA (the "feature similarity gain model") is effective in increasing performance.



### Learning to Generate Images with Perceptual Similarity Metrics
- **Arxiv ID**: http://arxiv.org/abs/1511.06409v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06409v3)
- **Published**: 2015-11-19 21:57:46+00:00
- **Updated**: 2017-01-24 02:03:41+00:00
- **Authors**: Jake Snell, Karl Ridgeway, Renjie Liao, Brett D. Roads, Michael C. Mozer, Richard S. Zemel
- **Comment**: None
- **Journal**: None
- **Summary**: Deep networks are increasingly being applied to problems involving image synthesis, e.g., generating images from textual descriptions and reconstructing an input image from a compact representation. Supervised training of image-synthesis networks typically uses a pixel-wise loss (PL) to indicate the mismatch between a generated image and its corresponding target image. We propose instead to use a loss function that is better calibrated to human perceptual judgments of image quality: the multiscale structural-similarity score (MS-SSIM). Because MS-SSIM is differentiable, it is easily incorporated into gradient-descent learning. We compare the consequences of using MS-SSIM versus PL loss on training deterministic and stochastic autoencoders. For three different architectures, we collected human judgments of the quality of image reconstructions. Observers reliably prefer images synthesized by MS-SSIM-optimized models over those synthesized by PL-optimized models, for two distinct PL measures ($\ell_1$ and $\ell_2$ distances). We also explore the effect of training objective on image encoding and analyze conditions under which perceptually-optimized representations yield better performance on image classification. Finally, we demonstrate the superiority of perceptually-optimized networks for super-resolution imaging. Just as computer vision has advanced through the use of convolutional architectures that mimic the structure of the mammalian visual system, we argue that significant additional advances can be made in modeling images through the use of training objectives that are well aligned to characteristics of human perception.



### QBDC: Query by dropout committee for training deep supervised architecture
- **Arxiv ID**: http://arxiv.org/abs/1511.06412v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06412v2)
- **Published**: 2015-11-19 22:03:14+00:00
- **Updated**: 2015-11-26 14:19:01+00:00
- **Authors**: Melanie Ducoffe, Frederic Precioso
- **Comment**: Submitted to ICLR2016
- **Journal**: None
- **Summary**: While the current trend is to increase the depth of neural networks to increase their performance, the size of their training database has to grow accordingly. We notice an emergence of tremendous databases, although providing labels to build a training set still remains a very expensive task. We tackle the problem of selecting the samples to be labelled in an online fashion. In this paper, we present an active learning strategy based on query by committee and dropout technique to train a Convolutional Neural Network (CNN). We derive a commmittee of partial CNNs resulting from batchwise dropout runs on the initial CNN. We evaluate our active learning strategy for CNN on MNIST benchmark, showing in particular that selecting less than 30 % from the annotated database is enough to get similar error rate as using the full training set on MNIST. We also studied the robustness of our method against adversarial examples.



### Deep Manifold Traversal: Changing Labels with Convolutional Features
- **Arxiv ID**: http://arxiv.org/abs/1511.06421v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1511.06421v3)
- **Published**: 2015-11-19 22:17:20+00:00
- **Updated**: 2016-03-17 17:57:55+00:00
- **Authors**: Jacob R. Gardner, Paul Upchurch, Matt J. Kusner, Yixuan Li, Kilian Q. Weinberger, Kavita Bala, John E. Hopcroft
- **Comment**: None
- **Journal**: None
- **Summary**: Many tasks in computer vision can be cast as a "label changing" problem, where the goal is to make a semantic change to the appearance of an image or some subject in an image in order to alter the class membership. Although successful task-specific methods have been developed for some label changing applications, to date no general purpose method exists. Motivated by this we propose deep manifold traversal, a method that addresses the problem in its most general form: it first approximates the manifold of natural images then morphs a test image along a traversal path away from a source class and towards a target class while staying near the manifold throughout. The resulting algorithm is surprisingly effective and versatile. It is completely data driven, requiring only an example set of images from the desired source and target domains. We demonstrate deep manifold traversal on highly diverse label changing tasks: changing an individual's appearance (age and hair color), changing the season of an outdoor image, and transforming a city skyline towards nighttime.



### First Step toward Model-Free, Anonymous Object Tracking with Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.06425v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06425v2)
- **Published**: 2015-11-19 22:24:15+00:00
- **Updated**: 2015-11-25 19:44:15+00:00
- **Authors**: Quan Gan, Qipeng Guo, Zheng Zhang, Kyunghyun Cho
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose and study a novel visual object tracking approach based on convolutional networks and recurrent networks. The proposed approach is distinct from the existing approaches to visual object tracking, such as filtering-based ones and tracking-by-detection ones, in the sense that the tracking system is explicitly trained off-line to track anonymous objects in a noisy environment. The proposed visual tracking model is end-to-end trainable, minimizing any adversarial effect from mismatches in object representation and between the true underlying dynamics and learning dynamics. We empirically show that the proposed tracking approach works well in various scenarios by generating artificial video sequences with varying conditions; the number of objects, amount of noise and the match between the training shapes and test shapes.



### A Controller-Recognizer Framework: How necessary is recognition for control?
- **Arxiv ID**: http://arxiv.org/abs/1511.06428v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06428v4)
- **Published**: 2015-11-19 22:38:53+00:00
- **Updated**: 2016-02-09 20:58:21+00:00
- **Authors**: Marcin Moczulski, Kelvin Xu, Aaron Courville, Kyunghyun Cho
- **Comment**: None
- **Journal**: None
- **Summary**: Recently there has been growing interest in building active visual object recognizers, as opposed to the usual passive recognizers which classifies a given static image into a predefined set of object categories. In this paper we propose to generalize these recently proposed end-to-end active visual recognizers into a controller-recognizer framework. A model in the controller-recognizer framework consists of a controller, which interfaces with an external manipulator, and a recognizer which classifies the visual input adjusted by the manipulator. We describe two most recently proposed controller-recognizer models: recurrent attention model and spatial transformer network as representative examples of controller-recognizer models. Based on this description we observe that most existing end-to-end controller-recognizers tightly, or completely, couple a controller and recognizer. We ask a question whether this tight coupling is necessary, and try to answer this empirically by building a controller-recognizer model with a decoupled controller and recognizer. Our experiments revealed that it is not always necessary to tightly couple them and that by decoupling a controller and recognizer, there is a possibility of building a generic controller that is pretrained and works together with any subsequent recognizer.



### Delving Deeper into Convolutional Networks for Learning Video Representations
- **Arxiv ID**: http://arxiv.org/abs/1511.06432v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.06432v4)
- **Published**: 2015-11-19 22:46:13+00:00
- **Updated**: 2016-03-01 18:54:11+00:00
- **Authors**: Nicolas Ballas, Li Yao, Chris Pal, Aaron Courville
- **Comment**: ICLR 2016
- **Journal**: None
- **Summary**: We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call "percepts" using Gated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts that are extracted from all level of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts can leads to high-dimensionality video representations. To mitigate this effect and control the model number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations.   We empirically validate our approach on both Human Action Recognition and Video Captioning tasks. In particular, we achieve results equivalent to state-of-art on the YouTube2Text dataset using a simpler text-decoder model and without extra 3D CNN features.



### Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.06434v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06434v2)
- **Published**: 2015-11-19 22:50:32+00:00
- **Updated**: 2016-01-07 23:09:39+00:00
- **Authors**: Alec Radford, Luke Metz, Soumith Chintala
- **Comment**: Under review as a conference paper at ICLR 2016
- **Journal**: None
- **Summary**: In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.



### A convnet for non-maximum suppression
- **Arxiv ID**: http://arxiv.org/abs/1511.06437v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06437v3)
- **Published**: 2015-11-19 22:56:18+00:00
- **Updated**: 2016-01-08 00:00:21+00:00
- **Authors**: Jan Hosang, Rodrigo Benenson, Bernt Schiele
- **Comment**: Included comments from reviewers
- **Journal**: None
- **Summary**: Non-maximum suppression (NMS) is used in virtually all state-of-the-art object detection pipelines. While essential object detection ingredients such as features, classifiers, and proposal methods have been extensively researched surprisingly little work has aimed to systematically address NMS. The de-facto standard for NMS is based on greedy clustering with a fixed distance threshold, which forces to trade-off recall versus precision. We propose a convnet designed to perform NMS of a given set of detections. We report experiments on a synthetic setup, and results on crowded pedestrian detection scenes. Our approach overcomes the intrinsic limitations of greedy NMS, obtaining better recall and precision.



### Fast Metric Learning For Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.06442v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1511.06442v5)
- **Published**: 2015-11-19 23:10:00+00:00
- **Updated**: 2016-04-05 07:29:48+00:00
- **Authors**: Henry Gouk, Bernhard Pfahringer, Michael Cree
- **Comment**: None
- **Journal**: None
- **Summary**: Similarity metrics are a core component of many information retrieval and machine learning systems. In this work we propose a method capable of learning a similarity metric from data equipped with a binary relation. By considering only the similarity constraints, and initially ignoring the features, we are able to learn target vectors for each instance using one of several appropriately designed loss functions. A regression model can then be constructed that maps novel feature vectors to the same target vector space, resulting in a feature extractor that computes vectors for which a predefined metric is a meaningful measure of similarity. We present results on both multiclass and multi-label classification datasets that demonstrate considerably faster convergence, as well as higher accuracy on the majority of the intrinsic evaluation tasks and all extrinsic evaluation tasks.



### Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.06448v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.06448v3)
- **Published**: 2015-11-19 23:29:55+00:00
- **Updated**: 2016-02-29 21:33:45+00:00
- **Authors**: Pouya Bashivan, Irina Rish, Mohammed Yeasin, Noel Codella
- **Comment**: To be published as a conference paper at ICLR 2016
- **Journal**: None
- **Summary**: One of the challenges in modeling cognitive events from electroencephalogram (EEG) data is finding representations that are invariant to inter- and intra-subject differences, as well as to inherent noise associated with such data. Herein, we propose a novel approach for learning such representations from multi-channel EEG time-series, and demonstrate its advantages in the context of mental load classification task. First, we transform EEG activities into a sequence of topology-preserving multi-spectral images, as opposed to standard EEG analysis techniques that ignore such spatial information. Next, we train a deep recurrent-convolutional network inspired by state-of-the-art video classification to learn robust representations from the sequence of images. The proposed approach is designed to preserve the spatial, spectral, and temporal structure of EEG which leads to finding features that are less sensitive to variations and distortions within each dimension. Empirical evaluation on the cognitive load classification task demonstrated significant improvements in classification accuracy over current state-of-the-art approaches in this field.



### Learning to decompose for object detection and instance segmentation
- **Arxiv ID**: http://arxiv.org/abs/1511.06449v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06449v3)
- **Published**: 2015-11-19 23:30:06+00:00
- **Updated**: 2016-05-11 02:55:29+00:00
- **Authors**: Eunbyung Park, Alexander C. Berg
- **Comment**: ICLR 2016 Workshop
- **Journal**: None
- **Summary**: Although deep convolutional neural networks(CNNs) have achieved remarkable results on object detection and segmentation, pre- and post-processing steps such as region proposals and non-maximum suppression(NMS), have been required. These steps result in high computational complexity and sensitivity to hyperparameters, e.g. thresholds for NMS. In this work, we propose a novel end-to-end trainable deep neural network architecture, which consists of convolutional and recurrent layers, that generates the correct number of object instances and their bounding boxes (or segmentation masks) given an image, using only a single network evaluation without any pre- or post-processing steps. We have tested on detecting digits in multi-digit images synthesized using MNIST, automatically segmenting digits in these images, and detecting cars in the KITTI benchmark dataset. The proposed approach outperforms a strong CNN baseline on the synthesized digits datasets and shows promising results on KITTI car detection.



### Deep Metric Learning via Lifted Structured Feature Embedding
- **Arxiv ID**: http://arxiv.org/abs/1511.06452v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06452v1)
- **Published**: 2015-11-19 23:41:11+00:00
- **Updated**: 2015-11-19 23:41:11+00:00
- **Authors**: Hyun Oh Song, Yu Xiang, Stefanie Jegelka, Silvio Savarese
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Learning the distance metric between pairs of examples is of great importance for learning and visual recognition. With the remarkable success from the state of the art convolutional neural networks, recent works have shown promising results on discriminatively training the networks to learn semantic feature embeddings where similar examples are mapped close to each other and dissimilar examples are mapped farther apart. In this paper, we describe an algorithm for taking full advantage of the training batches in the neural network training by lifting the vector of pairwise distances within the batch to the matrix of pairwise distances. This step enables the algorithm to learn the state of the art feature embedding by optimizing a novel structured prediction objective on the lifted problem. Additionally, we collected Online Products dataset: 120k images of 23k classes of online products for metric learning. Our experiments on the CUB-200-2011, CARS196, and Online Products datasets demonstrate significant improvement over existing deep feature embedding methods on all experimented embedding sizes with the GoogLeNet network.



