# Arxiv Papers in cs.CV on 2015-11-16
### Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization
- **Arxiv ID**: http://arxiv.org/abs/1511.04798v2
- **DOI**: 10.1109/TAFFC.2016.2622690
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1511.04798v2)
- **Published**: 2015-11-16 01:40:15+00:00
- **Updated**: 2018-02-20 06:02:45+00:00
- **Authors**: Baohan Xu, Yanwei Fu, Yu-Gang Jiang, Boyang Li, Leonid Sigal
- **Comment**: 13 pages, 11 figures. Published at the IEEE Transactions on Affective
  Computing
- **Journal**: IEEE Transactions on Affective Computing. 2016
- **Summary**: Emotion is a key element in user-generated videos. However, it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user-generated content and the sparsity of video frames expressing emotion. In this paper, for the first time, we study the problem of transferring knowledge from heterogeneous external sources, including image and textual data, to facilitate three related tasks in understanding video emotion: emotion recognition, emotion attribution and emotion-oriented summarization. Specifically, our framework (1) learns a video encoding from an auxiliary emotional image dataset in order to improve supervised video emotion recognition, and (2) transfers knowledge from an auxiliary textual corpora for zero-shot recognition of emotion classes unseen during training. The proposed technique for knowledge transfer facilitates novel applications of emotion attribution and emotion-oriented summarization. A comprehensive set of experiments on multiple datasets demonstrate the effectiveness of our framework.



### Learning Mid-level Words on Riemannian Manifold for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1511.04808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04808v1)
- **Published**: 2015-11-16 03:18:06+00:00
- **Updated**: 2015-11-16 03:18:06+00:00
- **Authors**: Mengyi Liu, Ruiping Wang, Shiguang Shan, Xilin Chen
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Human action recognition remains a challenging task due to the various sources of video data and large intra-class variations. It thus becomes one of the key issues in recent research to explore effective and robust representation to handle such challenges. In this paper, we propose a novel representation approach by constructing mid-level words in videos and encoding them on Riemannian manifold. Specifically, we first conduct a global alignment on the densely extracted low-level features to build a bank of corresponding feature groups, each of which can be statistically modeled as a mid-level word lying on some specific Riemannian manifold. Based on these mid-level words, we construct intrinsic Riemannian codebooks by employing K-Karcher-means clustering and Riemannian Gaussian Mixture Model, and consequently extend the Riemannian manifold version of three well studied encoding methods in Euclidean space, i.e. Bag of Visual Words (BoVW), Vector of Locally Aggregated Descriptors (VLAD), and Fisher Vector (FV), to obtain the final action video representations. Our method is evaluated in two tasks on four popular realistic datasets: action recognition on YouTube, UCF50, HMDB51 databases, and action similarity labeling on ASLAN database. In all cases, the reported results achieve very competitive performance with those most recent state-of-the-art works.



### Deep learning is a good steganalysis tool when embedding key is reused for different images, even if there is a cover source-mismatch
- **Arxiv ID**: http://arxiv.org/abs/1511.04855v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.04855v2)
- **Published**: 2015-11-16 07:59:14+00:00
- **Updated**: 2018-01-12 07:49:46+00:00
- **Authors**: Lionel Pibre, Pasquet Jérôme, Dino Ienco, Marc Chaumont
- **Comment**: IS&T. Media Watermarking, Security, and Forensics, Part of IS&T
  International Symposium on Electronic Imaging, EI'2016, Feb 2015, San
  Fransisco, United States
- **Journal**: None
- **Summary**: Since the BOSS competition, in 2010, most steganalysis approaches use a learning methodology involving two steps: feature extraction, such as the Rich Models (RM), for the image representation, and use of the Ensemble Classifier (EC) for the learning step. In 2015, Qian et al. have shown that the use of a deep learning approach that jointly learns and computes the features, is very promising for the steganalysis. In this paper, we follow-up the study of Qian et al., and show that, due to intrinsic joint minimization, the results obtained from a Convolutional Neural Network (CNN) or a Fully Connected Neural Network (FNN), if well parameterized, surpass the conventional use of a RM with an EC. First, numerous experiments were conducted in order to find the best " shape " of the CNN. Second, experiments were carried out in the clairvoyant scenario in order to compare the CNN and FNN to an RM with an EC. The results show more than 16% reduction in the classification error with our CNN or FNN. Third, experiments were also performed in a cover-source mismatch setting. The results show that the CNN and FNN are naturally robust to the mismatch problem. In Addition to the experiments, we provide discussions on the internal mechanisms of a CNN, and weave links with some previously stated ideas, in order to understand the impressive results we obtained.



### Sherlock: Scalable Fact Learning in Images
- **Arxiv ID**: http://arxiv.org/abs/1511.04891v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.04891v4)
- **Published**: 2015-11-16 09:56:04+00:00
- **Updated**: 2016-04-02 05:26:39+00:00
- **Authors**: Mohamed Elhoseiny, Scott Cohen, Walter Chang, Brian Price, Ahmed Elgammal
- **Comment**: Jan 7 Update
- **Journal**: None
- **Summary**: We study scalable and uniform understanding of facts in images. Existing visual recognition systems are typically modeled differently for each fact type such as objects, actions, and interactions. We propose a setting where all these facts can be modeled simultaneously with a capacity to understand unbounded number of facts in a structured way. The training data comes as structured facts in images, including (1) objects (e.g., $<$boy$>$), (2) attributes (e.g., $<$boy, tall$>$), (3) actions (e.g., $<$boy, playing$>$), and (4) interactions (e.g., $<$boy, riding, a horse $>$). Each fact has a semantic language view (e.g., $<$ boy, playing$>$) and a visual view (an image with this fact). We show that learning visual facts in a structured way enables not only a uniform but also generalizable visual understanding. We propose and investigate recent and strong approaches from the multiview learning literature and also introduce two learning representation models as potential baselines. We applied the investigated methods on several datasets that we augmented with structured facts and a large scale dataset of more than 202,000 facts and 814,000 images. Our experiments show the advantage of relating facts by the structure by the proposed models compared to the designed baselines on bidirectional fact retrieval.



### Fast clustering for scalable statistical analysis on structured images
- **Arxiv ID**: http://arxiv.org/abs/1511.04898v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.04898v1)
- **Published**: 2015-11-16 10:26:18+00:00
- **Updated**: 2015-11-16 10:26:18+00:00
- **Authors**: Bertrand Thirion, Andrés Hoyos-Idrobo, Jonas Kahn, Gael Varoquaux
- **Comment**: ICML Workshop on Statistics, Machine Learning and Neuroscience
  (Stamlins 2015), Jul 2015, Lille, France
- **Journal**: None
- **Summary**: The use of brain images as markers for diseases or behavioral differences is challenged by the small effects size and the ensuing lack of power, an issue that has incited researchers to rely more systematically on large cohorts. Coupled with resolution increases, this leads to very large datasets. A striking example in the case of brain imaging is that of the Human Connectome Project: 20 Terabytes of data and growing. The resulting data deluge poses severe challenges regarding the tractability of some processing steps (discriminant analysis, multivariate models) due to the memory demands posed by these data. In this work, we revisit dimension reduction approaches, such as random projections, with the aim of replacing costly function evaluations by cheaper ones while decreasing the memory requirements. Specifically, we investigate the use of alternate schemes, based on fast clustering, that are well suited for signals exhibiting a strong spatial structure, such as anatomical and functional brain images. Our contribution is twofold: i) we propose a linear-time clustering scheme that bypasses the percolation issues inherent in these algorithms and thus provides compressions nearly as good as traditional quadratic-complexity variance-minimizing clustering schemes, ii) we show that cluster-based compression can have the virtuous effect of removing high-frequency noise, actually improving subsequent estimations steps. As a consequence, the proposed approach yields very accurate models on several large-scale problems yet with impressive gains in computational efficiency, making it possible to analyze large datasets.



### Coarse-to-fine Face Alignment with Multi-Scale Local Patch Regression
- **Arxiv ID**: http://arxiv.org/abs/1511.04901v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04901v1)
- **Published**: 2015-11-16 10:31:18+00:00
- **Updated**: 2015-11-16 10:31:18+00:00
- **Authors**: Zhiao Huang, Erjin Zhou, Zhimin Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Facial landmark localization plays an important role in face recognition and analysis applications. In this paper, we give a brief introduction to a coarse-to-fine pipeline with neural networks and sequential regression. First, a global convolutional network is applied to the holistic facial image to give an initial landmark prediction. A pyramid of multi-scale local image patches is then cropped to feed to a new network for each landmark to refine the prediction. As the refinement network outputs a more accurate position estimation than the input, such procedure could be repeated several times until the estimation converges. We evaluate our system on the 300-W dataset [11] and it outperforms the recent state-of-the-arts.



### Graph-based denoising for time-varying point clouds
- **Arxiv ID**: http://arxiv.org/abs/1511.04902v1
- **DOI**: 10.1109/3DTV.2015.7169366
- **Categories**: **cs.CV**, cs.GR, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1511.04902v1)
- **Published**: 2015-11-16 10:34:25+00:00
- **Updated**: 2015-11-16 10:34:25+00:00
- **Authors**: Yann Schoenenberger, Johan Paratte, Pierre Vandergheynst
- **Comment**: 4 pages, 3 figures, 3DTV-Con 2015
- **Journal**: 3DTV-Conference: The True Vision - Capture, Transmission and
  Display of 3D Video (3DTV-CON) (2015) 1-4
- **Summary**: Noisy 3D point clouds arise in many applications. They may be due to errors when constructing a 3D model from images or simply to imprecise depth sensors. Point clouds can be given geometrical structure using graphs created from the similarity information between points. This paper introduces a technique that uses this graph structure and convex optimization methods to denoise 3D point clouds. A short discussion presents how those methods naturally generalize to time-varying inputs such as 3D point cloud time series.



### Performing Highly Accurate Predictions Through Convolutional Networks for Actual Telecommunication Challenges
- **Arxiv ID**: http://arxiv.org/abs/1511.04906v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.04906v3)
- **Published**: 2015-11-16 10:42:08+00:00
- **Updated**: 2016-07-14 10:21:47+00:00
- **Authors**: Jaime Zaratiegui, Ana Montoro, Federico Castanedo
- **Comment**: 11 pages, 6 figures, accepted by IJCAI-16 Workshop on Deep Learning
  for Artificial Intelligence (DLAI)
- **Journal**: None
- **Summary**: We investigated how the application of deep learning, specifically the use of convolutional networks trained with GPUs, can help to build better predictive models in telecommunication business environments, and fill this gap. In particular, we focus on the non-trivial problem of predicting customer churn in telecommunication operators. Our model, called WiseNet, consists of a convolutional network and a novel encoding method that transforms customer activity data and Call Detail Records (CDRs) into images. Experimental evaluation with several machine learning classifiers supports the ability of WiseNet for learning features when using structured input data. For this type of telecommunication business problems, we found that WiseNet outperforms machine learning models with hand-crafted features, and does not require the labor-intensive step of feature engineering. Furthermore, the same model has been applied without retraining to a different market, achieving consistent results. This confirms the generalization property of WiseNet and the ability to extract useful representations.



### Identification and Counting White Blood Cells and Red Blood Cells using Image Processing Case Study of Leukemia
- **Arxiv ID**: http://arxiv.org/abs/1511.04934v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04934v1)
- **Published**: 2015-11-16 12:41:06+00:00
- **Updated**: 2015-11-16 12:41:06+00:00
- **Authors**: Esti Suryani, Wiharto Wiharto, Nizomjon Polvonov
- **Comment**: None
- **Journal**: None
- **Summary**: Leukemia is diagnosed with complete blood counts which is by calculating all blood cells and compare the number of white blood cells (White Blood Cells / WBC) and red blood cells (Red Blood Cells / RBC). Information obtained from a complete blood count, has become a cornerstone in the hematology laboratory for diagnostic purposes and monitoring of hematological disorders. However, the traditional procedure for counting blood cells manually requires effort and a long time, therefore this method is one of the most expensive routine tests in laboratory hematology clinic. Solution for such kind of time consuming task and necessity of data tracability can be found in image processing techniques based on blood cell morphology . This study aims to identify Acute Lymphocytic Leukemia (ALL) and Acute Myeloid Leukemia type M3 (AML M3) using Fuzzy Rule Based System based on morphology of white blood cells. Characteristic parameters witch extractedare WBC Area, Nucleus and Granule Ratio of white blood cells. Image processing algorithms such as thresholding, Canny edge detection and color identification filters are used.Then for identification of ALL, AML M3 and Healthy cells used Fuzzy Rule Based System with Sugeno method. In the testing process used 104 images out of which 29 ALL - Positive, 50 AML M3 - Positive and 25 Healthy cells. Test results showed 83.65 % accuracy .



### Sample and Filter: Nonparametric Scene Parsing via Efficient Filtering
- **Arxiv ID**: http://arxiv.org/abs/1511.04960v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04960v2)
- **Published**: 2015-11-16 14:07:47+00:00
- **Updated**: 2016-03-15 01:29:03+00:00
- **Authors**: Mohammad Najafi, Sarah Taghavi Namin, Mathieu Salzmann, Lars Petersson
- **Comment**: Please refer to the CVPR-2016 version of this manuscript
- **Journal**: None
- **Summary**: Scene parsing has attracted a lot of attention in computer vision. While parametric models have proven effective for this task, they cannot easily incorporate new training data. By contrast, nonparametric approaches, which bypass any learning phase and directly transfer the labels from the training data to the query images, can readily exploit new labeled samples as they become available. Unfortunately, because of the computational cost of their label transfer procedures, state-of-the-art nonparametric methods typically filter out most training images to only keep a few relevant ones to label the query. As such, these methods throw away many images that still contain valuable information and generally obtain an unbalanced set of labeled samples. In this paper, we introduce a nonparametric approach to scene parsing that follows a sample-and-filter strategy. More specifically, we propose to sample labeled superpixels according to an image similarity score, which allows us to obtain a balanced set of samples. We then formulate label transfer as an efficient filtering procedure, which lets us exploit more labeled samples than existing techniques. Our experiments evidence the benefits of our approach over state-of-the-art nonparametric methods on two benchmark datasets.



### Handcrafted Local Features are Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.05045v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05045v2)
- **Published**: 2015-11-16 17:17:28+00:00
- **Updated**: 2015-11-19 20:25:09+00:00
- **Authors**: Zhenzhong Lan, Shoou-I Yu, Ming Lin, Bhiksha Raj, Alexander G. Hauptmann
- **Comment**: None
- **Journal**: None
- **Summary**: Image and video classification research has made great progress through the development of handcrafted local features and learning based features. These two architectures were proposed roughly at the same time and have flourished at overlapping stages of history. However, they are typically viewed as distinct approaches. In this paper, we emphasize their structural similarities and show how such a unified view helps us in designing features that balance efficiency and effectiveness. As an example, we study the problem of designing efficient video feature learning algorithms for action recognition.   We approach this problem by first showing that local handcrafted features and Convolutional Neural Networks (CNNs) share the same convolution-pooling network structure. We then propose a two-stream Convolutional ISA (ConvISA) that adopts the convolution-pooling structure of the state-of-the-art handcrafted video feature with greater modeling capacities and a cost-effective training algorithm. Through custom designed network structures for pixels and optical flow, our method also reflects distinctive characteristics of these two data sources.   Our experimental results on standard action recognition benchmarks show that by focusing on the structure of CNNs, rather than end-to-end training methods, we are able to design an efficient and powerful video feature learning algorithm.



### An Empirical Study of Recent Face Alignment Methods
- **Arxiv ID**: http://arxiv.org/abs/1511.05049v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05049v1)
- **Published**: 2015-11-16 17:26:27+00:00
- **Updated**: 2015-11-16 17:26:27+00:00
- **Authors**: Heng Yang, Xuhui Jia, Chen Change Loy, Peter Robinson
- **Comment**: under review of a conference. Project page:
  https://www.cl.cam.ac.uk/~hy306/FaceAlignment.html
- **Journal**: None
- **Summary**: The problem of face alignment has been intensively studied in the past years. A large number of novel methods have been proposed and reported very good performance on benchmark dataset such as 300W. However, the differences in the experimental setting and evaluation metric, missing details in the description of the methods make it hard to reproduce the results reported and evaluate the relative merits. For instance, most recent face alignment methods are built on top of face detection but from different face detectors. In this paper, we carry out a rigorous evaluation of these methods by making the following contributions: 1) we proposes a new evaluation metric for face alignment on a set of images, i.e., area under error distribution curve within a threshold, AUC$_\alpha$, given the fact that the traditional evaluation measure (mean error) is very sensitive to big alignment error. 2) we extend the 300W database with more practical face detections to make fair comparison possible. 3) we carry out face alignment sensitivity analysis w.r.t. face detection, on both synthetic and real data, using both off-the-shelf and re-retrained models. 4) we study factors that are particularly important to achieve good performance and provide suggestions for practical applications. Most of the conclusions drawn from our comparative analysis cannot be inferred from the original publications.



### Proposal Flow
- **Arxiv ID**: http://arxiv.org/abs/1511.05065v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05065v3)
- **Published**: 2015-11-16 17:54:45+00:00
- **Updated**: 2016-07-08 18:32:37+00:00
- **Authors**: Bumsub Ham, Minsu Cho, Cordelia Schmid, Jean Ponce
- **Comment**: None
- **Journal**: None
- **Summary**: Finding image correspondences remains a challenging problem in the presence of intra-class variations and large changes in scene layout.~Semantic flow methods are designed to handle images depicting different instances of the same object or scene category. We introduce a novel approach to semantic flow, dubbed proposal flow, that establishes reliable correspondences using object proposals. Unlike prevailing semantic flow approaches that operate on pixels or regularly sampled local regions, proposal flow benefits from the characteristics of modern object proposals, that exhibit high repeatability at multiple scales, and can take advantage of both local and geometric consistency constraints among proposals. We also show that proposal flow can effectively be transformed into a conventional dense flow field. We introduce a new dataset that can be used to evaluate both general semantic flow techniques and region-based approaches such as proposal flow. We use this benchmark to compare different matching algorithms, object proposals, and region features within proposal flow, to the state of the art in semantic flow. This comparison, along with experiments on standard datasets, demonstrates that proposal flow significantly outperforms existing semantic flow methods in various settings.



### Joint Training of Generic CNN-CRF Models with Stochastic Optimization
- **Arxiv ID**: http://arxiv.org/abs/1511.05067v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05067v3)
- **Published**: 2015-11-16 17:59:14+00:00
- **Updated**: 2016-09-14 11:52:49+00:00
- **Authors**: Alexander Kirillov, Dmitrij Schlesinger, Shuai Zheng, Bogdan Savchynskyy, Philip H. S. Torr, Carsten Rother
- **Comment**: ACCV2016
- **Journal**: None
- **Summary**: We propose a new CNN-CRF end-to-end learning framework, which is based on joint stochastic optimization with respect to both Convolutional Neural Network (CNN) and Conditional Random Field (CRF) parameters. While stochastic gradient descent is a standard technique for CNN training, it was not used for joint models so far. We show that our learning method is (i) general, i.e. it applies to arbitrary CNN and CRF architectures and potential functions; (ii) scalable, i.e. it has a low memory footprint and straightforwardly parallelizes on GPUs; (iii) easy in implementation. Additionally, the unified CNN-CRF optimization approach simplifies a potential hardware implementation. We empirically evaluate our method on the task of semantic labeling of body parts in depth images and show that it compares favorably to competing techniques.



### Understanding learned CNN features through Filter Decoding with Substitution
- **Arxiv ID**: http://arxiv.org/abs/1511.05084v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05084v2)
- **Published**: 2015-11-16 18:50:24+00:00
- **Updated**: 2015-11-17 10:15:48+00:00
- **Authors**: Ivet Rafegas, Maria Vanrell
- **Comment**: 10 pages, 7 figures (including supplementary material). Submitted for
  review for CVPR 2016
- **Journal**: None
- **Summary**: In parallel with the success of CNNs to solve vision problems, there is a growing interest in developing methodologies to understand and visualize the internal representations of these networks. How the responses of a trained CNN encode the visual information is a fundamental question both for computer and human vision research. Image representations provided by the first convolutional layer as well as the resolution change provided by the max-polling operation are easy to understand, however, as soon as a second and further convolutional layers are added in the representation, any intuition is lost. A usual way to deal with this problem has been to define deconvolutional networks that somehow allow to explore the internal representations of the most important activations towards the image space, where deconvolution is assumed as a convolution with the transposed filter. However, this assumption is not the best approximation of an inverse convolution. In this paper we propose a new assumption based on filter substitution to reverse the encoding of a convolutional layer. This provides us with a new tool to directly visualize any CNN single neuron as a filter in the first layer, this is in terms of the image space.



### Yin and Yang: Balancing and Answering Binary Visual Questions
- **Arxiv ID**: http://arxiv.org/abs/1511.05099v5
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.05099v5)
- **Published**: 2015-11-16 19:38:14+00:00
- **Updated**: 2016-04-19 19:30:00+00:00
- **Authors**: Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, Devi Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: The complex compositional structure of language makes problems at the intersection of vision and language challenging. But language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content. This can hinder progress in pushing state of art in the computer vision aspects of multi-modal AI. In this paper, we address binary Visual Question Answering (VQA) on abstract scenes. We formulate this problem as visual verification of concepts inquired in the questions. Specifically, we convert the question to a tuple that concisely summarizes the visual concept to be detected in the image. If the concept can be found in the image, the answer to the question is "yes", and otherwise "no". Abstract scenes play two roles (1) They allow us to focus on the high-level semantics of the VQA task as opposed to the low-level recognition problems, and perhaps more importantly, (2) They provide us the modality to balance the dataset such that language priors are controlled, and the role of vision is essential. In particular, we collect fine-grained pairs of scenes for every question, such that the answer to the question is "yes" for one scene, and "no" for the other for the exact same question. Indeed, language priors alone do not perform better than chance on our balanced dataset. Moreover, our proposed approach matches the performance of a state-of-the-art VQA approach on the unbalanced dataset, and outperforms it on the balanced dataset.



### Adversarial Manipulation of Deep Representations
- **Arxiv ID**: http://arxiv.org/abs/1511.05122v9
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.05122v9)
- **Published**: 2015-11-16 20:48:20+00:00
- **Updated**: 2016-03-04 20:21:24+00:00
- **Authors**: Sara Sabour, Yanshuai Cao, Fartash Faghri, David J. Fleet
- **Comment**: Accepted as a conference paper at ICLR 2016
- **Journal**: None
- **Summary**: We show that the representation of an image in a deep neural network (DNN) can be manipulated to mimic those of other natural images, with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image perturbations designed to produce erroneous class labels, while we concentrate on the internal layers of DNN representations. In this way our new class of adversarial images differs qualitatively from others. While the adversary is perceptually similar to one image, its internal representation appears remarkably similar to a different image, one from a different class, bearing little if any apparent similarity to the input; they appear generic and consistent with the space of natural images. This phenomenon raises questions about DNN representations, as well as the properties of natural images themselves.



### Nonlinear Local Metric Learning for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1511.05169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05169v1)
- **Published**: 2015-11-16 21:02:31+00:00
- **Updated**: 2015-11-16 21:02:31+00:00
- **Authors**: Siyuan Huang, Jiwen Lu, Jie Zhou, Anil K. Jain
- **Comment**: Submitted to CVPR 2016
- **Journal**: None
- **Summary**: Person re-identification aims at matching pedestrians observed from non-overlapping camera views. Feature descriptor and metric learning are two significant problems in person re-identification. A discriminative metric learning method should be capable of exploiting complex nonlinear transformations due to the large variations in feature space. In this paper, we propose a nonlinear local metric learning (NLML) method to improve the state-of-the-art performance of person re-identification on public datasets. Motivated by the fact that local metric learning has been introduced to handle the data which varies locally and deep neural network has presented outstanding capability in exploiting the nonlinearity of samples, we utilize the merits of both local metric learning and deep neural network to learn multiple sets of nonlinear transformations. By enforcing a margin between the distances of positive pedestrian image pairs and distances of negative pairs in the transformed feature subspace, discriminative information can be effectively exploited in the developed neural networks. Our experiments show that the proposed NLML method achieves the state-of-the-art results on the widely used VIPeR, GRID, and CUHK 01 datasets.



### Cross-scale predictive dictionaries
- **Arxiv ID**: http://arxiv.org/abs/1511.05174v3
- **DOI**: 10.1109/TIP.2018.2869719
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1511.05174v3)
- **Published**: 2015-11-16 21:07:38+00:00
- **Updated**: 2018-09-04 03:25:13+00:00
- **Authors**: Vishwanath Saragadam, Xin Li, Aswin Sankaranarayanan
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Sparse representations using data dictionaries provide an efficient model particularly for signals that do not enjoy alternate analytic sparsifying transformations. However, solving inverse problems with sparsifying dictionaries can be computationally expensive, especially when the dictionary under consideration has a large number of atoms. In this paper, we incorporate additional structure on to dictionary-based sparse representations for visual signals to enable speedups when solving sparse approximation problems. The specific structure that we endow onto sparse models is that of a multi-scale modeling where the sparse representation at each scale is constrained by the sparse representation at coarser scales. We show that this cross-scale predictive model delivers significant speedups, often in the range of 10-60$\times$, with little loss in accuracy for linear inverse problems associated with images, videos, and light fields.



### Convolutional Models for Joint Object Categorization and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1511.05175v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.05175v6)
- **Published**: 2015-11-16 21:08:22+00:00
- **Updated**: 2016-04-19 17:56:34+00:00
- **Authors**: Mohamed Elhoseiny, Tarek El-Gaaly, Amr Bakry, Ahmed Elgammal
- **Comment**: only for workshop presentation at ICLR
- **Journal**: None
- **Summary**: In the task of Object Recognition, there exists a dichotomy between the categorization of objects and estimating object pose, where the former necessitates a view-invariant representation, while the latter requires a representation capable of capturing pose information over different categories of objects. With the rise of deep architectures, the prime focus has been on object category recognition. Deep learning methods have achieved wide success in this task. In contrast, object pose regression using these approaches has received relatively much less attention. In this paper we show how deep architectures, specifically Convolutional Neural Networks (CNN), can be adapted to the task of simultaneous categorization and pose estimation of objects. We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations of CNNs represent object pose information and how this contradicts with object category representations. We extensively experiment on two recent large and challenging multi-view datasets. Our models achieve better than state-of-the-art performance on both datasets.



### Visualizing and Understanding Deep Texture Representations
- **Arxiv ID**: http://arxiv.org/abs/1511.05197v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05197v2)
- **Published**: 2015-11-16 22:01:16+00:00
- **Updated**: 2016-04-12 16:37:46+00:00
- **Authors**: Tsung-Yu Lin, Subhransu Maji
- **Comment**: None
- **Journal**: None
- **Summary**: A number of recent approaches have used deep convolutional neural networks (CNNs) to build texture representations. Nevertheless, it is still unclear how these models represent texture and invariances to categorical variations. This work conducts a systematic evaluation of recent CNN-based texture descriptors for recognition and attempts to understand the nature of invariances captured by these representations. First we show that the recently proposed bilinear CNN model [25] is an excellent general-purpose texture descriptor and compares favorably to other CNN-based descriptors on various texture and scene recognition benchmarks. The model is translationally invariant and obtains better accuracy on the ImageNet dataset without requiring spatial jittering of data compared to corresponding models trained with spatial jittering. Based on recent work [13, 28] we propose a technique to visualize pre-images, providing a means for understanding categorical properties that are captured by these representations. Finally, we show preliminary results on how a unified parametric model of texture analysis and synthesis can be used for attribute-based image manipulation, e.g. to make an image more swirly, honeycombed, or knitted. The source code and additional visualizations are available at http://vis-www.cs.umass.edu/texture



### Learning Expressionlets via Universal Manifold Model for Dynamic Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1511.05204v1
- **DOI**: 10.1109/TIP.2016.2615424
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.05204v1)
- **Published**: 2015-11-16 22:19:11+00:00
- **Updated**: 2015-11-16 22:19:11+00:00
- **Authors**: Mengyi Liu, Shiguang Shan, Ruiping Wang, Xilin Chen
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Facial expression is temporally dynamic event which can be decomposed into a set of muscle motions occurring in different facial regions over various time intervals. For dynamic expression recognition, two key issues, temporal alignment and semantics-aware dynamic representation, must be taken into account. In this paper, we attempt to solve both problems via manifold modeling of videos based on a novel mid-level representation, i.e. \textbf{expressionlet}. Specifically, our method contains three key stages: 1) each expression video clip is characterized as a spatial-temporal manifold (STM) formed by dense low-level features; 2) a Universal Manifold Model (UMM) is learned over all low-level features and represented as a set of local modes to statistically unify all the STMs. 3) the local modes on each STM can be instantiated by fitting to UMM, and the corresponding expressionlet is constructed by modeling the variations in each local mode. With above strategy, expression videos are naturally aligned both spatially and temporally. To enhance the discriminative power, the expressionlet-based STM representation is further processed with discriminant embedding. Our method is evaluated on four public expression databases, CK+, MMI, Oulu-CASIA, and FERA. In all cases, our method outperforms the known state-of-the-art by a large margin.



