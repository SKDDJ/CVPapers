# Arxiv Papers in cs.CV on 2015-11-14
### Deeply-Recursive Convolutional Network for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1511.04491v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.04491v2)
- **Published**: 2015-11-14 02:21:50+00:00
- **Updated**: 2016-11-11 08:40:53+00:00
- **Authors**: Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee
- **Comment**: CVPR 2016 Oral
- **Journal**: None
- **Summary**: We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN). Our network has a very deep recursive layer (up to 16 recursions). Increasing recursion depth can improve performance without introducing new parameters for additional convolutions. Albeit advantages, learning a DRCN is very hard with a standard gradient descent method due to exploding/vanishing gradients. To ease the difficulty of training, we propose two extensions: recursive-supervision and skip-connection. Our method outperforms previous methods by a large margin.



### Semantic Object Parsing with Local-Global Long Short-Term Memory
- **Arxiv ID**: http://arxiv.org/abs/1511.04510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04510v1)
- **Published**: 2015-11-14 05:42:50+00:00
- **Updated**: 2015-11-14 05:42:50+00:00
- **Authors**: Xiaodan Liang, Xiaohui Shen, Donglai Xiang, Jiashi Feng, Liang Lin, Shuicheng Yan
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Semantic object parsing is a fundamental task for understanding objects in detail in computer vision community, where incorporating multi-level contextual information is critical for achieving such fine-grained pixel-level recognition. Prior methods often leverage the contextual information through post-processing predicted confidence maps. In this work, we propose a novel deep Local-Global Long Short-Term Memory (LG-LSTM) architecture to seamlessly incorporate short-distance and long-distance spatial dependencies into the feature learning over all pixel positions. In each LG-LSTM layer, local guidance from neighboring positions and global guidance from the whole image are imposed on each position to better exploit complex local and global contextual information. Individual LSTMs for distinct spatial dimensions are also utilized to intrinsically capture various spatial layouts of semantic parts in the images, yielding distinct hidden and memory cells of each position for each dimension. In our parsing approach, several LG-LSTM layers are stacked and appended to the intermediate convolutional layers to directly enhance visual features, allowing network parameters to be learned in an end-to-end way. The long chains of sequential computation by stacked LG-LSTM layers also enable each pixel to sense a much larger region for inference benefiting from the memorization of previous dependencies in all positions along all dimensions. Comprehensive evaluations on three public datasets well demonstrate the significant superiority of our LG-LSTM over other state-of-the-art methods.



### Sequential Optimization for Efficient High-Quality Object Proposal Generation
- **Arxiv ID**: http://arxiv.org/abs/1511.04511v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04511v3)
- **Published**: 2015-11-14 05:45:47+00:00
- **Updated**: 2017-05-22 17:23:07+00:00
- **Authors**: Ziming Zhang, Yun Liu, Xi Chen, Yanjun Zhu, Ming-Ming Cheng, Venkatesh Saligrama, Philip H. S. Torr
- **Comment**: Accepted by TPAMI
- **Journal**: None
- **Summary**: We are motivated by the need for a generic object proposal generation algorithm which achieves good balance between object detection recall, proposal localization quality and computational efficiency. We propose a novel object proposal algorithm, BING++, which inherits the virtue of good computational efficiency of BING but significantly improves its proposal localization quality. At high level we formulate the problem of object proposal generation from a novel probabilistic perspective, based on which our BING++ manages to improve the localization quality by employing edges and segments to estimate object boundaries and update the proposals sequentially. We propose learning the parameters efficiently by searching for approximate solutions in a quantized parameter space for complexity reduction. We demonstrate the generalization of BING++ with the same fixed parameters across different object classes and datasets. Empirically our BING++ can run at half speed of BING on CPU, but significantly improve the localization quality by 18.5% and 16.7% on both VOC2007 and Microhsoft COCO datasets, respectively. Compared with other state-of-the-art approaches, BING++ can achieve comparable performance, but run significantly faster.



### Zero-Shot Learning via Joint Latent Similarity Embedding
- **Arxiv ID**: http://arxiv.org/abs/1511.04512v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04512v3)
- **Published**: 2015-11-14 05:53:30+00:00
- **Updated**: 2016-08-17 16:29:51+00:00
- **Authors**: Ziming Zhang, Venkatesh Saligrama
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot recognition (ZSR) deals with the problem of predicting class labels for target domain instances based on source domain side information (e.g. attributes) of unseen classes. We formulate ZSR as a binary prediction problem. Our resulting classifier is class-independent. It takes an arbitrary pair of source and target domain instances as input and predicts whether or not they come from the same class, i.e. whether there is a match. We model the posterior probability of a match since it is a sufficient statistic and propose a latent probabilistic model in this context. We develop a joint discriminative learning framework based on dictionary learning to jointly learn the parameters of our model for both domains, which ultimately leads to our class-independent classifier. Many of the existing embedding methods can be viewed as special cases of our probabilistic model. On ZSR our method shows 4.90\% improvement over the state-of-the-art in accuracy averaged across four benchmark datasets. We also adapt ZSR method for zero-shot retrieval and show 22.45\% improvement accordingly in mean average precision (mAP).



### Reversible Recursive Instance-level Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1511.04517v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04517v2)
- **Published**: 2015-11-14 06:10:41+00:00
- **Updated**: 2015-11-18 06:49:18+00:00
- **Authors**: Xiaodan Liang, Yunchao Wei, Xiaohui Shen, Zequn Jie, Jiashi Feng, Liang Lin, Shuicheng Yan
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: In this work, we propose a novel Reversible Recursive Instance-level Object Segmentation (R2-IOS) framework to address the challenging instance-level object segmentation task. R2-IOS consists of a reversible proposal refinement sub-network that predicts bounding box offsets for refining the object proposal locations, and an instance-level segmentation sub-network that generates the foreground mask of the dominant object instance in each proposal. By being recursive, R2-IOS iteratively optimizes the two sub-networks during joint training, in which the refined object proposals and improved segmentation predictions are alternately fed into each other to progressively increase the network capabilities. By being reversible, the proposal refinement sub-network adaptively determines an optimal number of refinement iterations required for each proposal during both training and testing. Furthermore, to handle multiple overlapped instances within a proposal, an instance-aware denoising autoencoder is introduced into the segmentation sub-network to distinguish the dominant object from other distracting instances. Extensive experiments on the challenging PASCAL VOC 2012 benchmark well demonstrate the superiority of R2-IOS over other state-of-the-art methods. In particular, the $\text{AP}^r$ over $20$ classes at $0.5$ IoU achieves $66.7\%$, which significantly outperforms the results of $58.7\%$ by PFN~\cite{PFN} and $46.3\%$ by~\cite{liu2015multi}.



### Efficient Training of Very Deep Neural Networks for Supervised Hashing
- **Arxiv ID**: http://arxiv.org/abs/1511.04524v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.04524v2)
- **Published**: 2015-11-14 07:35:01+00:00
- **Updated**: 2016-04-21 21:49:21+00:00
- **Authors**: Ziming Zhang, Yuting Chen, Venkatesh Saligrama
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose training very deep neural networks (DNNs) for supervised learning of hash codes. Existing methods in this context train relatively "shallow" networks limited by the issues arising in back propagation (e.e. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into independent layer-wise local updates through auxiliary variables. Empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks. Empirically we manage to train DNNs with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single GPU. Our proposed very deep supervised hashing (VDSH) method significantly outperforms the state-of-the-art on several benchmark datasets.



### Learning Fine-grained Features via a CNN Tree for Large-scale Classification
- **Arxiv ID**: http://arxiv.org/abs/1511.04534v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04534v2)
- **Published**: 2015-11-14 09:19:44+00:00
- **Updated**: 2017-09-22 07:59:12+00:00
- **Authors**: Zhenhua Wang, Xingxing Wang, Gang Wang
- **Comment**: Neurocomputing 2017
- **Journal**: None
- **Summary**: We propose a novel approach to enhance the discriminability of Convolutional Neural Networks (CNN). The key idea is to build a tree structure that could progressively learn fine-grained features to distinguish a subset of classes, by learning features only among these classes. Such features are expected to be more discriminative, compared to features learned for all the classes. We develop a new algorithm to effectively learn the tree structure from a large number of classes. Experiments on large-scale image classification tasks demonstrate that our method could boost the performance of a given basic CNN model. Our method is quite general, hence it can potentially be used in combination with many other deep learning models.



### Accurate Image Super-Resolution Using Very Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.04587v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.04587v2)
- **Published**: 2015-11-14 17:36:45+00:00
- **Updated**: 2016-11-11 08:40:47+00:00
- **Authors**: Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee
- **Comment**: CVPR 2016 Oral
- **Journal**: None
- **Summary**: We present a highly accurate single-image super-resolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classification \cite{simonyan2015very}. We find increasing our network depth shows a significant improvement in accuracy. Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual information over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates ($10^4$ times higher than SRCNN \cite{dong2015image}) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable.



### Oracle performance for visual captioning
- **Arxiv ID**: http://arxiv.org/abs/1511.04590v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1511.04590v5)
- **Published**: 2015-11-14 18:02:39+00:00
- **Updated**: 2016-09-14 16:55:29+00:00
- **Authors**: Li Yao, Nicolas Ballas, Kyunghyun Cho, John R. Smith, Yoshua Bengio
- **Comment**: BMVC2016 (Oral paper)
- **Journal**: None
- **Summary**: The task of associating images and videos with a natural language description has attracted a great amount of attention recently. Rapid progress has been made in terms of both developing novel algorithms and releasing new datasets. Indeed, the state-of-the-art results on some of the standard datasets have been pushed into the regime where it has become more and more difficult to make significant improvements. Instead of proposing new models, this work investigates the possibility of empirically establishing performance upper bounds on various visual captioning datasets without extra data labelling effort or human evaluation. In particular, it is assumed that visual captioning is decomposed into two steps: from visual inputs to visual concepts, and from visual concepts to natural language descriptions. One would be able to obtain an upper bound when assuming the first step is perfect and only requiring training a conditional language model for the second step. We demonstrate the construction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combination of M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we used for visual concept extraction in the first step and the simplicity of the language model for the second step, we show that current state-of-the-art models fall short when being compared with the learned upper bounds. Furthermore, with such a bound, we quantify several important factors concerning image and video captioning: the number of visual concepts captured by different models, the trade-off between the amount of visual elements captured and their accuracy, and the intrinsic difficulty and blessing of different datasets.



### DeepFool: a simple and accurate method to fool deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1511.04599v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.04599v3)
- **Published**: 2015-11-14 18:50:00+00:00
- **Updated**: 2016-07-04 04:49:44+00:00
- **Authors**: Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard
- **Comment**: In Proceedings of IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2016
- **Journal**: None
- **Summary**: State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.



### Jointly Learning Non-negative Projection and Dictionary with Discriminative Graph Constraints for Classification
- **Arxiv ID**: http://arxiv.org/abs/1511.04601v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04601v4)
- **Published**: 2015-11-14 19:11:13+00:00
- **Updated**: 2016-08-05 10:56:20+00:00
- **Authors**: Weiyang Liu, Zhiding Yu, Yandong Wen, Rongmei Lin, Meng Yang
- **Comment**: To appear in BMVC 2016
- **Journal**: None
- **Summary**: Sparse coding with dictionary learning (DL) has shown excellent classification performance. Despite the considerable number of existing works, how to obtain features on top of which dictionaries can be better learned remains an open and interesting question. Many current prevailing DL methods directly adopt well-performing crafted features. While such strategy may empirically work well, it ignores certain intrinsic relationship between dictionaries and features. We propose a framework where features and dictionaries are jointly learned and optimized. The framework, named joint non-negative projection and dictionary learning (JNPDL), enables interaction between the input features and the dictionaries. The non-negative projection leads to discriminative parts-based object features while DL seeks a more suitable representation. Discriminative graph constraints are further imposed to simultaneously maximize intra-class compactness and inter-class separability. Experiments on both image and image set classification show the excellent performance of JNPDL by outperforming several state-of-the-art approaches.



