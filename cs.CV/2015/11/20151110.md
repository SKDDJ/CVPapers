# Arxiv Papers in cs.CV on 2015-11-10
### Experimental robustness of Fourier Ptychography phase retrieval algorithms
- **Arxiv ID**: http://arxiv.org/abs/1511.02986v2
- **DOI**: 10.1364/OE.23.033214
- **Categories**: **physics.optics**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.02986v2)
- **Published**: 2015-11-10 03:45:02+00:00
- **Updated**: 2015-12-18 07:33:10+00:00
- **Authors**: Li-Hao Yeh, Jonathan Dong, Jingshan Zhong, Lei Tian, Michael Chen, Gongguo Tang, Mahdi Soltanolkotabi, Laura Waller
- **Comment**: None
- **Journal**: Opt. Express 23, 33214-33240 (2015)
- **Summary**: Fourier ptychography is a new computational microscopy technique that provides gigapixel-scale intensity and phase images with both wide field-of-view and high resolution. By capturing a stack of low-resolution images under different illumination angles, a nonlinear inverse algorithm can be used to computationally reconstruct the high-resolution complex field. Here, we compare and classify multiple proposed inverse algorithms in terms of experimental robustness. We find that the main sources of error are noise, aberrations and mis-calibration (i.e. model mis-match). Using simulations and experiments, we demonstrate that the choice of cost function plays a critical role, with amplitude-based cost functions performing better than intensity-based ones. The reason for this is that Fourier ptychography datasets consist of images from both brightfield and darkfield illumination, representing a large range of measured intensities. Both noise (e.g. Poisson noise) and model mis-match errors are shown to scale with intensity. Hence, algorithms that use an appropriate cost function will be more tolerant to both noise and model mis-match. Given these insights, we propose a global Newton's method algorithm which is robust and computationally efficient. Finally, we discuss the impact of procedures for algorithmic correction of aberrations and mis-calibration.



### Traffic Sign Classification Using Deep Inception Based Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.02992v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1511.02992v2)
- **Published**: 2015-11-10 05:07:03+00:00
- **Updated**: 2016-07-17 11:05:22+00:00
- **Authors**: Mrinal Haloi
- **Comment**: modifications: Accepted version of 2016 IEEE Intelligent Vehicles
  Symposium (IV 2016)
- **Journal**: None
- **Summary**: In this work, we propose a novel deep network for traffic sign classification that achieves outstanding performance on GTSRB surpassing all previous methods. Our deep network consists of spatial transformer layers and a modified version of inception module specifically designed for capturing local and global features together. This features adoption allows our network to classify precisely intraclass samples even under deformations. Use of spatial transformer layer makes this network more robust to deformations such as translation, rotation, scaling of input images. Unlike existing approaches that are developed with hand-crafted features, multiple deep networks with huge parameters and data augmentations, our method addresses the concern of exploding parameters and augmentations. We have achieved the state-of-the-art performance of 99.81\% on GTSRB dataset.



### Improvised Salient Object Detection and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1511.02999v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1511.02999v1)
- **Published**: 2015-11-10 06:08:42+00:00
- **Updated**: 2015-11-10 06:08:42+00:00
- **Authors**: Abhishek Maity
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: In case of salient subject recognition, computer algorithms have been heavily relied on scanning of images from top-left to bottom-right systematically and apply brute-force when attempting to locate objects of interest. Thus, the process turns out to be quite time consuming. Here a novel approach and a simple solution to the above problem is discussed. In this paper, we implement an approach to object manipulation and detection through segmentation map, which would help to desaturate or, in other words, wash out the background of the image. Evaluation for the performance is carried out using the Jaccard index against the well-known Ground-truth target box technique.



### Deep Representation of Facial Geometric and Photometric Attributes for Automatic 3D Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1511.03015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03015v1)
- **Published**: 2015-11-10 08:30:31+00:00
- **Updated**: 2015-11-10 08:30:31+00:00
- **Authors**: Huibin Li, Jian Sun, Dong Wang, Zongben Xu, Liming Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel approach to automatic 3D Facial Expression Recognition (FER) based on deep representation of facial 3D geometric and 2D photometric attributes. A 3D face is firstly represented by its geometric and photometric attributes, including the geometry map, normal maps, normalized curvature map and texture map. These maps are then fed into a pre-trained deep convolutional neural network to generate the deep representation. Then the facial expression prediction is simplyachieved by training linear SVMs over the deep representation for different maps and fusing these SVM scores. The visualizations show that the deep representation provides a complete and highly discriminative coding scheme for 3D faces. Comprehensive experiments on the BU-3DFE database demonstrate that the proposed deep representation can outperform the widely used hand-crafted descriptors (i.e., LBP, SIFT, HOG, Gabor) and the state-of-art approaches under the same experimental protocols.



### 3D Time-lapse Reconstruction from Internet Photos
- **Arxiv ID**: http://arxiv.org/abs/1511.03019v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03019v2)
- **Published**: 2015-11-10 08:42:40+00:00
- **Updated**: 2020-02-22 03:21:40+00:00
- **Authors**: Ricardo Martin-Brualla, David Gallup, Steven M. Seitz
- **Comment**: To appear in ICCV'15. Supplementary video at:
  http://grail.cs.washington.edu/projects/timelapse3d/
- **Journal**: None
- **Summary**: Given an Internet photo collection of a landmark, we compute a 3D time-lapse video sequence where a virtual camera moves continuously in time and space. While previous work assumed a static camera, the addition of camera motion during the time-lapse creates a very compelling impression of parallax. Achieving this goal, however, requires addressing multiple technical challenges, including solving for time-varying depth maps, regularizing 3D point color profiles over time, and reconstructing high quality, hole-free images at every frame from the projected profiles. Our results show photorealistic time-lapses of skylines and natural scenes over many years, with dramatic parallax effects.



### Online Action Recognition based on Incremental Learning of Weighted Covariance Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1511.03028v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03028v4)
- **Published**: 2015-11-10 09:18:30+00:00
- **Updated**: 2017-07-06 11:22:38+00:00
- **Authors**: Chang Tang, Pichao Wang, Wanqing Li
- **Comment**: None
- **Journal**: None
- **Summary**: Different from traditional action recognition based on video segments, online action recognition aims to recognize actions from unsegmented streams of data in a continuous manner. One way for online recognition is based on the evidence accumulation over time to make predictions from stream videos. This paper presents a fast yet effective method to recognize actions from stream of noisy skeleton data, and a novel weighted covariance descriptor is adopted to accumulate evidence. In particular, a fast incremental updating method for the weighted covariance descriptor is developed for accumulation of temporal information and online prediction. The weighted covariance descriptor takes the following principles into consideration: past frames have less contribution for recognition and recent and informative frames such as key frames contribute more to the recognition. The online recognition is achieved using a simple nearest neighbor search against a set of offline trained action models. Experimental results on MSC-12 Kinect Gesture dataset and our newly constructed online action recognition dataset have demonstrated the efficacy of the proposed method.



### Analyzing Stability of Convolutional Neural Networks in the Frequency Domain
- **Arxiv ID**: http://arxiv.org/abs/1511.03042v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03042v2)
- **Published**: 2015-11-10 09:54:20+00:00
- **Updated**: 2015-11-16 08:42:10+00:00
- **Authors**: Elnaz J. Heravi, Hamed H. Aghdam, Domenec Puig
- **Comment**: Under review as a conference paper at ICLR2016, minor changes in the
  text
- **Journal**: None
- **Summary**: Understanding the internal process of ConvNets is commonly done using visualization techniques. However, these techniques do not usually provide a tool for estimating the stability of a ConvNet against noise. In this paper, we show how to analyze a ConvNet in the frequency domain using a 4-dimensional visualization technique. Using the frequency domain analysis, we show the reason that a ConvNet might be sensitive to a very low magnitude additive noise. Our experiments on a few ConvNets trained on different datasets revealed that convolution kernels of a trained ConvNet usually pass most of the frequencies and they are not able to effectively eliminate the effect of high frequencies. Our next experiments shows that a convolution kernel which has a more concentrated frequency response could be more stable. Finally, we show that fine-tuning a ConvNet using a training set augmented with noisy images can produce more stable ConvNets.



### Tiny Descriptors for Image Retrieval with Unsupervised Triplet Hashing
- **Arxiv ID**: http://arxiv.org/abs/1511.03055v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG, 68P20, H.3.3; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1511.03055v1)
- **Published**: 2015-11-10 10:38:37+00:00
- **Updated**: 2015-11-10 10:38:37+00:00
- **Authors**: Jie Lin, Olivier Morère, Julie Petta, Vijay Chandrasekhar, Antoine Veillard
- **Comment**: None
- **Journal**: None
- **Summary**: A typical image retrieval pipeline starts with the comparison of global descriptors from a large database to find a short list of candidate matches. A good image descriptor is key to the retrieval pipeline and should reconcile two contradictory requirements: providing recall rates as high as possible and being as compact as possible for fast matching. Following the recent successes of Deep Convolutional Neural Networks (DCNN) for large scale image classification, descriptors extracted from DCNNs are increasingly used in place of the traditional hand crafted descriptors such as Fisher Vectors (FV) with better retrieval performances. Nevertheless, the dimensionality of a typical DCNN descriptor --extracted either from the visual feature pyramid or the fully-connected layers-- remains quite high at several thousands of scalar values. In this paper, we propose Unsupervised Triplet Hashing (UTH), a fully unsupervised method to compute extremely compact binary hashes --in the 32-256 bits range-- from high-dimensional global descriptors. UTH consists of two successive deep learning steps. First, Stacked Restricted Boltzmann Machines (SRBM), a type of unsupervised deep neural nets, are used to learn binary embedding functions able to bring the descriptor size down to the desired bitrate. SRBMs are typically able to ensure a very high compression rate at the expense of loosing some desirable metric properties of the original DCNN descriptor space. Then, triplet networks, a rank learning scheme based on weight sharing nets is used to fine-tune the binary embedding functions to retain as much as possible of the useful metric properties of the original space. A thorough empirical evaluation conducted on multiple publicly available dataset using DCNN descriptors shows that our method is able to significantly outperform state-of-the-art unsupervised schemes in the target bit range.



### Dynamic Belief Fusion for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1511.03183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03183v1)
- **Published**: 2015-11-10 17:03:55+00:00
- **Updated**: 2015-11-10 17:03:55+00:00
- **Authors**: Hyungtae Lee, Heesung Kwon, Ryan M. Robinson, William d. Nothwang, Amar M. Marathe
- **Comment**: 8 pages, 6 figures, 28 references. arXiv admin note: text overlap
  with arXiv:1502.07643
- **Journal**: None
- **Summary**: A novel approach for the fusion of heterogeneous object detection methods is proposed. In order to effectively integrate the outputs of multiple detectors, the level of ambiguity in each individual detection score is estimated using the precision/recall relationship of the corresponding detector. The main contribution of the proposed work is a novel fusion method, called Dynamic Belief Fusion (DBF), which dynamically assigns probabilities to hypotheses (target, non-target, intermediate state (target or non-target)) based on confidence levels in the detection results conditioned on the prior performance of individual detectors. In DBF, a joint basic probability assignment, optimally fusing information from all detectors, is determined by the Dempster's combination rule, and is easily reduced to a single fused detection score. Experiments on ARL and PASCAL VOC 07 datasets demonstrate that the detection accuracy of DBF is considerably greater than conventional fusion approaches as well as individual detectors used for the fusion.



### The Radon cumulative distribution transform and its application to image classification
- **Arxiv ID**: http://arxiv.org/abs/1511.03206v1
- **DOI**: 10.1109/TIP.2015.2509419
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03206v1)
- **Published**: 2015-11-10 18:05:45+00:00
- **Updated**: 2015-11-10 18:05:45+00:00
- **Authors**: Soheil Kolouri, Se Rim Park, Gustavo K. Rohde
- **Comment**: None
- **Journal**: None
- **Summary**: Invertible image representation methods (transforms) are routinely employed as low-level image processing operations based on which feature extraction and recognition algorithms are developed. Most transforms in current use (e.g. Fourier, Wavelet, etc.) are linear transforms, and, by themselves, are unable to substantially simplify the representation of image classes for classification. Here we describe a nonlinear, invertible, low-level image processing transform based on combining the well known Radon transform for image data, and the 1D Cumulative Distribution Transform proposed earlier. We describe a few of the properties of this new transform, and with both theoretical and experimental results show that it can often render certain problems linearly separable in transform space.



### Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer
- **Arxiv ID**: http://arxiv.org/abs/1511.03240v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03240v2)
- **Published**: 2015-11-10 19:56:01+00:00
- **Updated**: 2016-04-12 07:08:11+00:00
- **Authors**: Jun Xie, Martin Kiefel, Ming-Ting Sun, Andreas Geiger
- **Comment**: 10 pages in Conference on Computer Vision and Pattern Recognition
  (CVPR), 2016
- **Journal**: None
- **Summary**: Semantic annotations are vital for training models for object recognition, semantic segmentation or scene understanding. Unfortunately, pixelwise annotation of images at very large scale is labor-intensive and only little labeled data is available, particularly at instance level and for street scenes. In this paper, we propose to tackle this problem by lifting the semantic instance labeling task from 2D into 3D. Given reconstructions from stereo or laser data, we annotate static 3D scene elements with rough bounding primitives and develop a model which transfers this information into the image domain. We leverage our method to obtain 2D labels for a novel suburban video dataset which we have collected, resulting in 400k semantic and instance image annotations. A comparison of our method to state-of-the-art label transfer baselines reveals that 3D information enables more efficient annotation while at the same time resulting in improved accuracy and time-coherent labels.



### TemplateNet for Depth-Based Object Instance Recognition
- **Arxiv ID**: http://arxiv.org/abs/1511.03244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03244v1)
- **Published**: 2015-11-10 20:03:36+00:00
- **Updated**: 2015-11-10 20:03:36+00:00
- **Authors**: Ujwal Bonde, Vijay Badrinarayanan, Roberto Cipolla, Minh-Tri Pham
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: We present a novel deep architecture termed templateNet for depth based object instance recognition. Using an intermediate template layer we exploit prior knowledge of an object's shape to sparsify the feature maps. This has three advantages: (i) the network is better regularised resulting in structured filters; (ii) the sparse feature maps results in intuitive features been learnt which can be visualized as the output of the template layer and (iii) the resulting network achieves state-of-the-art performance. The network benefits from this without any additional parametrization from the template layer. We derive the weight updates needed to efficiently train this network in an end-to-end manner. We benchmark the templateNet for depth based object instance recognition using two publicly available datasets. The datasets present multiple challenges of clutter, large pose variations and similar looking distractors. Through our experiments we show that with the addition of a template layer, a depth based CNN is able to outperform existing state-of-the-art methods in the field.



### Online Supervised Hashing for Ever-Growing Datasets
- **Arxiv ID**: http://arxiv.org/abs/1511.03257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03257v1)
- **Published**: 2015-11-10 20:37:41+00:00
- **Updated**: 2015-11-10 20:37:41+00:00
- **Authors**: Fatih Cakir, Sarah Adel Bargal, Stan Sclaroff
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised hashing methods are widely-used for nearest neighbor search in computer vision applications. Most state-of-the-art supervised hashing approaches employ batch-learners. Unfortunately, batch-learning strategies can be inefficient when confronted with large training datasets. Moreover, with batch-learners, it is unclear how to adapt the hash functions as a dataset continues to grow and diversify over time. Yet, in many practical scenarios the dataset grows and diversifies; thus, both the hash functions and the indexing must swiftly accommodate these changes. To address these issues, we propose an online hashing method that is amenable to changes and expansions of the datasets. Since it is an online algorithm, our approach offers linear complexity with the dataset size. Our solution is supervised, in that we incorporate available label information to preserve the semantic neighborhood. Such an adaptive hashing method is attractive; but it requires recomputing the hash table as the hash functions are updated. If the frequency of update is high, then recomputing the hash table entries may cause inefficiencies in the system, especially for large indexes. Thus, we also propose a framework to reduce hash table updates. We compare our method to state-of-the-art solutions on two benchmarks and demonstrate significant improvements over previous work.



### From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge
- **Arxiv ID**: http://arxiv.org/abs/1511.03292v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1511.03292v1)
- **Published**: 2015-11-10 21:14:51+00:00
- **Updated**: 2015-11-10 21:14:51+00:00
- **Authors**: Somak Aditya, Yezhou Yang, Chitta Baral, Cornelia Fermuller, Yiannis Aloimonos
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning is applied on (a) detections obtained from existing perception methods on given images, (b) a "commonsense" knowledge base constructed using natural language processing of image annotations and (c) lexical ontological knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most cases, sentences auto-constructed from SDGs obtained by our method give a more relevant and thorough description of an image than a recent state-of-the-art image caption based approach. Our Image-Sentence Alignment Evaluation results are also comparable to that of the recent state-of-the art approaches.



### The Fast Bilateral Solver
- **Arxiv ID**: http://arxiv.org/abs/1511.03296v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03296v2)
- **Published**: 2015-11-10 21:19:34+00:00
- **Updated**: 2016-07-22 19:12:04+00:00
- **Authors**: Jonathan T. Barron, Ben Poole
- **Comment**: None
- **Journal**: None
- **Summary**: We present the bilateral solver, a novel algorithm for edge-aware smoothing that combines the flexibility and speed of simple filtering approaches with the accuracy of domain-specific optimization algorithms. Our technique is capable of matching or improving upon state-of-the-art results on several different computer vision tasks (stereo, depth superresolution, colorization, and semantic segmentation) while being 10-1000 times faster than competing approaches. The bilateral solver is fast, robust, straightforward to generalize to new domains, and simple to integrate into deep learning pipelines.



### Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform
- **Arxiv ID**: http://arxiv.org/abs/1511.03328v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03328v2)
- **Published**: 2015-11-10 22:54:13+00:00
- **Updated**: 2016-06-02 02:11:07+00:00
- **Authors**: Liang-Chieh Chen, Jonathan T. Barron, George Papandreou, Kevin Murphy, Alan L. Yuille
- **Comment**: 14 pages. Accepted to appear at CVPR 2016
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) are the backbone of state-of-art semantic image segmentation systems. Recent work has shown that complementing CNNs with fully-connected conditional random fields (CRFs) can significantly enhance their object localization accuracy, yet dense CRF inference is computationally expensive. We propose replacing the fully-connected CRF with domain transform (DT), a modern edge-preserving filtering method in which the amount of smoothing is controlled by a reference edge map. Domain transform filtering is several times faster than dense CRF inference and we show that it yields comparable semantic segmentation results, accurately capturing object boundaries. Importantly, our formulation allows learning the reference edge map from intermediate CNN features instead of using the image gradient magnitude as in standard DT filtering. This produces task-specific edges in an end-to-end trainable system optimizing the target semantic segmentation quality.



### Attention to Scale: Scale-aware Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1511.03339v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03339v2)
- **Published**: 2015-11-10 23:53:57+00:00
- **Updated**: 2016-06-02 02:02:21+00:00
- **Authors**: Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, Alan L. Yuille
- **Comment**: 14 pages. Accepted to appear at CVPR 2016
- **Journal**: None
- **Summary**: Incorporating multi-scale features in fully convolutional neural networks (FCNs) has been a key element to achieving state-of-the-art performance on semantic image segmentation. One common way to extract multi-scale features is to feed multiple resized input images to a shared deep network and then merge the resulting features for pixelwise classification. In this work, we propose an attention mechanism that learns to softly weight the multi-scale features at each pixel location. We adapt a state-of-the-art semantic image segmentation model, which we jointly train with multi-scale input images and the attention model. The proposed attention model not only outperforms average- and max-pooling, but allows us to diagnostically visualize the importance of features at different positions and scales. Moreover, we show that adding extra supervision to the output at each scale is essential to achieving excellent performance when merging multi-scale features. We demonstrate the effectiveness of our model with extensive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.



