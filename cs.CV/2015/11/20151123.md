# Arxiv Papers in cs.CV on 2015-11-23
### Multi-Volume High Resolution RGB-D Mapping with Dynamic Volume Placement
- **Arxiv ID**: http://arxiv.org/abs/1511.07106v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.07106v1)
- **Published**: 2015-11-23 04:01:00+00:00
- **Updated**: 2015-11-23 04:01:00+00:00
- **Authors**: Michael Salvato, Ross Finman, John Leonard
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel RGB-D mapping system for generating 3D maps over spatially extended regions with higher resolution than current methods using multiple, dynamically placed mapping volumes. Our method takes in RGB-D frames and dynamically assigns multiple mapping volumes to the environment, exchanging mapping volumes between the CPU and GPU. Mapping volumes are added or removed as needed to allow for spatially extended, high resolution mapping. Our system is designed to maximize the resolution possible for such volumetric methods, while working on an unbounded space.



### Adapting Deep Visuomotor Representations with Weak Pairwise Constraints
- **Arxiv ID**: http://arxiv.org/abs/1511.07111v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07111v5)
- **Published**: 2015-11-23 05:07:15+00:00
- **Updated**: 2017-05-25 21:51:55+00:00
- **Authors**: Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Pieter Abbeel, Sergey Levine, Kate Saenko, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world robotics problems often occur in domains that differ significantly from the robot's prior training environment. For many robotic control tasks, real world experience is expensive to obtain, but data is easy to collect in either an instrumented environment or in simulation. We propose a novel domain adaptation approach for robot perception that adapts visual representations learned on a large easy-to-obtain source dataset (e.g. synthetic images) to a target real-world domain, without requiring expensive manual data annotation of real world data before policy search. Supervised domain adaptation methods minimize cross-domain differences using pairs of aligned images that contain the same object or scene in both the source and target domains, thus learning a domain-invariant representation. However, they require manual alignment of such image pairs. Fully unsupervised adaptation methods rely on minimizing the discrepancy between the feature distributions across domains. We propose a novel, more powerful combination of both distribution and pairwise image alignment, and remove the requirement for expensive annotation by using weakly aligned pairs of images in the source and target domains. Focusing on adapting from simulation to real world data using a PR2 robot, we evaluate our approach on a manipulation task and show that by using weakly paired images, our method compensates for domain shift more effectively than previous techniques, enabling better robot performance in the real world.



### Multi-Scale Context Aggregation by Dilated Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1511.07122v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07122v3)
- **Published**: 2015-11-23 07:32:14+00:00
- **Updated**: 2016-04-30 18:19:37+00:00
- **Authors**: Fisher Yu, Vladlen Koltun
- **Comment**: Published as a conference paper at ICLR 2016
- **Journal**: None
- **Summary**: State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.



### What Happened to My Dog in That Network: Unraveling Top-down Generators in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.07125v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1511.07125v1)
- **Published**: 2015-11-23 07:48:01+00:00
- **Updated**: 2015-11-23 07:48:01+00:00
- **Authors**: Patrick W. Gallagher, Shuai Tang, Zhuowen Tu
- **Comment**: None
- **Journal**: None
- **Summary**: Top-down information plays a central role in human perception, but plays relatively little role in many current state-of-the-art deep networks, such as Convolutional Neural Networks (CNNs). This work seeks to explore a path by which top-down information can have a direct impact within current deep networks. We explore this path by learning and using "generators" corresponding to the network internal effects of three types of transformation (each a restriction of a general affine transformation): rotation, scaling, and translation. We demonstrate how these learned generators can be used to transfer top-down information to novel settings, as mediated by the "feature flows" that the transformations (and the associated generators) correspond to inside the network. Specifically, we explore three aspects: 1) using generators as part of a method for synthesizing transformed images --- given a previously unseen image, produce versions of that image corresponding to one or more specified transformations, 2) "zero-shot learning" --- when provided with a feature flow corresponding to the effect of a transformation of unknown amount, leverage learned generators as part of a method by which to perform an accurate categorization of the amount of transformation, even for amounts never observed during training, and 3) (inside-CNN) "data augmentation" --- improve the classification performance of an existing network by using the learned generators to directly provide additional training "inside the CNN".



### DeePM: A Deep Part-Based Model for Object Detection and Semantic Part Localization
- **Arxiv ID**: http://arxiv.org/abs/1511.07131v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07131v3)
- **Published**: 2015-11-23 08:24:18+00:00
- **Updated**: 2016-01-26 09:14:31+00:00
- **Authors**: Jun Zhu, Xianjie Chen, Alan L. Yuille
- **Comment**: the final revision to ICLR 2016, in which some color errors in the
  figures are fixed
- **Journal**: None
- **Summary**: In this paper, we propose a deep part-based model (DeePM) for symbiotic object detection and semantic part localization. For this purpose, we annotate semantic parts for all 20 object categories on the PASCAL VOC 2012 dataset, which provides information on object pose, occlusion, viewpoint and functionality. DeePM is a latent graphical model based on the state-of-the-art R-CNN framework, which learns an explicit representation of the object-part configuration with flexible type sharing (e.g., a sideview horse head can be shared by a fully-visible sideview horse and a highly truncated sideview horse with head and neck only). For comparison, we also present an end-to-end Object-Part (OP) R-CNN which learns an implicit feature representation for jointly mapping an image ROI to the object and part bounding boxes. We evaluate the proposed methods for both the object and part detection performance on PASCAL VOC 2012, and show that DeePM consistently outperforms OP R-CNN in detecting objects and parts. In addition, it obtains superior performance to Fast and Faster R-CNNs in object detection.



### Face Alignment Across Large Poses: A 3D Solution
- **Arxiv ID**: http://arxiv.org/abs/1511.07212v1
- **DOI**: 10.1109/TPAMI.2017.2778152
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07212v1)
- **Published**: 2015-11-23 13:23:19+00:00
- **Updated**: 2015-11-23 13:23:19+00:00
- **Authors**: Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, Stan Z. Li
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: Face alignment, which fits a face model to an image and extracts the semantic meanings of facial pixels, has been an important topic in CV community. However, most algorithms are designed for faces in small to medium poses (below 45 degree), lacking the ability to align faces in large poses up to 90 degree. The challenges are three-fold: Firstly, the commonly used landmark-based face model assumes that all the landmarks are visible and is therefore not suitable for profile views. Secondly, the face appearance varies more dramatically across large poses, ranging from frontal view to profile view. Thirdly, labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed. In this paper, we propose a solution to the three problems in an new alignment framework, called 3D Dense Face Alignment (3DDFA), in which a dense 3D face model is fitted to the image via convolutional neutral network (CNN). We also propose a method to synthesize large-scale training samples in profile views to solve the third problem of data labelling. Experiments on the challenging AFLW database show that our approach achieves significant improvements over state-of-the-art methods.



### NetVLAD: CNN architecture for weakly supervised place recognition
- **Arxiv ID**: http://arxiv.org/abs/1511.07247v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.07247v3)
- **Published**: 2015-11-23 14:51:51+00:00
- **Updated**: 2016-05-02 15:42:41+00:00
- **Authors**: Relja Arandjelović, Petr Gronat, Akihiko Torii, Tomas Pajdla, Josef Sivic
- **Comment**: Appears in: IEEE Computer Vision and Pattern Recognition (CVPR) 2016
- **Journal**: None
- **Summary**: We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the "Vector of Locally Aggregated Descriptors" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks.



### Rendering refraction and reflection of eyeglasses for synthetic eye tracker images
- **Arxiv ID**: http://arxiv.org/abs/1511.07299v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07299v2)
- **Published**: 2015-11-23 16:28:14+00:00
- **Updated**: 2015-11-24 15:01:57+00:00
- **Authors**: Thomas C. Kübler, Tobias Rittig, Judith Ungewiss, Christina Krauss, Enkelejda Kasneci
- **Comment**: None
- **Journal**: None
- **Summary**: While for the evaluation of robustness of eye tracking algorithms the use of real-world data is essential, there are many applications where simulated, synthetic eye images are of advantage. They can generate labelled ground-truth data for appearance based gaze estimation algorithms or enable the development of model based gaze estimation techniques by showing the influence on gaze estimation error of different model factors that can then be simplified or extended. We extend the generation of synthetic eye images by a simulation of refraction and reflection for eyeglasses. On the one hand this allows for the testing of pupil and glint detection algorithms under different illumination and reflection conditions, on the other hand the error of gaze estimation routines can be estimated in conjunction with different eyeglasses. We show how a polynomial function fitting calibration performs equally well with and without eyeglasses, and how a geometrical eye model behaves when exposed to glasses.



### Node Specificity in Convolutional Deep Nets Depends on Receptive Field Position and Size
- **Arxiv ID**: http://arxiv.org/abs/1511.07347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07347v1)
- **Published**: 2015-11-23 18:15:13+00:00
- **Updated**: 2015-11-23 18:15:13+00:00
- **Authors**: Karl Zipser
- **Comment**: None
- **Journal**: None
- **Summary**: In convolutional deep neural networks, receptive field (RF) size increases with hierarchical depth. When RF size approaches full coverage of the input image, different RF positions result in RFs with different specificity, as portions of the RF fall out of the input space. This leads to a departure from the convolutional concept of positional invariance and opens the possibility for complex forms of context specificity.



### Recombinator Networks: Learning Coarse-to-Fine Feature Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1511.07356v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07356v2)
- **Published**: 2015-11-23 18:42:36+00:00
- **Updated**: 2016-04-17 23:29:25+00:00
- **Authors**: Sina Honari, Jason Yosinski, Pascal Vincent, Christopher Pal
- **Comment**: accepted in CVPR 2016
- **Journal**: None
- **Summary**: Deep neural networks with alternating convolutional, max-pooling and decimation layers are widely used in state of the art architectures for computer vision. Max-pooling purposefully discards precise spatial information in order to create features that are more robust, and typically organized as lower resolution spatial feature maps. On some tasks, such as whole-image classification, max-pooling derived features are well suited; however, for tasks requiring precise localization, such as pixel level prediction and segmentation, max-pooling destroys exactly the information required to perform well. Precise localization may be preserved by shallow convnets without pooling but at the expense of robustness. Can we have our max-pooled multi-layered cake and eat it too? Several papers have proposed summation and concatenation based methods for combining upsampled coarse, abstract features with finer features to produce robust pixel level predictions. Here we introduce another model --- dubbed Recombinator Networks --- where coarse features inform finer features early in their formation such that finer features can make use of several layers of computation in deciding how to use coarse features. The model is trained once, end-to-end and performs better than summation-based architectures, reducing the error from the previous state of the art on two facial keypoint datasets, AFW and AFLW, by 30\% and beating the current state-of-the-art on 300W without using extra data. We improve performance even further by adding a denoising prediction model based on a novel convnet formulation.



### CNNdroid: GPU-Accelerated Execution of Trained Deep Convolutional Neural Networks on Android
- **Arxiv ID**: http://arxiv.org/abs/1511.07376v2
- **DOI**: 10.1145/2964284.2973801
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.07376v2)
- **Published**: 2015-11-23 19:32:37+00:00
- **Updated**: 2016-10-15 19:22:46+00:00
- **Authors**: Seyyed Salar Latifi Oskouei, Hossein Golestani, Matin Hashemi, Soheil Ghiasi
- **Comment**: None
- **Journal**: Proceedings of the 2016 ACM Multimedia Conference, Open Source
  Software Track, pages 1201-1205, October 2016
- **Summary**: Many mobile applications running on smartphones and wearable devices would potentially benefit from the accuracy and scalability of deep CNN-based machine learning algorithms. However, performance and energy consumption limitations make the execution of such computationally intensive algorithms on mobile devices prohibitive. We present a GPU-accelerated library, dubbed CNNdroid, for execution of trained deep CNNs on Android-based mobile devices. Empirical evaluations show that CNNdroid achieves up to 60X speedup and 130X energy saving on current mobile devices. The CNNdroid open source library is available for download at https://github.com/ENCP/CNNdroid



### Pushing the Boundaries of Boundary Detection using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1511.07386v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.07386v2)
- **Published**: 2015-11-23 19:54:09+00:00
- **Updated**: 2016-01-22 15:31:32+00:00
- **Authors**: Iasonas Kokkinos
- **Comment**: The previous version reported large improvements w.r.t. the LPO
  region proposal baseline, which turned out to be due to a wrong computation
  for the baseline. The improvements are currently less important, and are
  omitted. We are sorry if the reported results caused any confusion. We have
  also integrated reviewer feedback regarding human performance on the BSD
  benchmark
- **Journal**: None
- **Summary**: In this work we show that adapting Deep Convolutional Neural Network training to the task of boundary detection can result in substantial improvements over the current state-of-the-art in boundary detection.   Our contributions consist firstly in combining a careful design of the loss for boundary detection training, a multi-resolution architecture and training with external data to improve the detection accuracy of the current state of the art. When measured on the standard Berkeley Segmentation Dataset, we improve theoptimal dataset scale F-measure from 0.780 to 0.808 - while human performance is at 0.803. We further improve performance to 0.813 by combining deep learning with grouping, integrating the Normalized Cuts technique within a deep network.   We also examine the potential of our boundary detector in conjunction with the task of semantic segmentation and demonstrate clear improvements over state-of-the-art systems. Our detector is fully integrated in the popular Caffe framework and processes a 320x420 image in less than a second.



### Where To Look: Focus Regions for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1511.07394v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07394v2)
- **Published**: 2015-11-23 20:17:18+00:00
- **Updated**: 2016-01-10 13:26:23+00:00
- **Authors**: Kevin J. Shih, Saurabh Singh, Derek Hoiem
- **Comment**: Submitted to CVPR2016
- **Journal**: None
- **Summary**: We present a method that learns to answer visual questions by selecting image regions relevant to the text-based query. Our method exhibits significant improvements in answering questions such as "what color," where it is necessary to evaluate a specific location, and "what room," where it selectively identifies informative image regions. Our model is tested on the VQA dataset which is the largest human-annotated visual question answering dataset to our knowledge.



### Learning Visual Predictive Models of Physics for Playing Billiards
- **Arxiv ID**: http://arxiv.org/abs/1511.07404v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.07404v3)
- **Published**: 2015-11-23 20:27:48+00:00
- **Updated**: 2016-01-19 20:58:24+00:00
- **Authors**: Katerina Fragkiadaki, Pulkit Agrawal, Sergey Levine, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to plan and execute goal specific actions in varied, unexpected settings is a central requirement of intelligent agents. In this paper, we explore how an agent can be equipped with an internal model of the dynamics of the external world, and how it can use this model to plan novel actions by running multiple internal simulations ("visual imagination"). Our models directly process raw visual input, and use a novel object-centric prediction formulation based on visual glimpses centered on objects (fixations) to enforce translational invariance of the learned physical laws. The agent gathers training data through random interaction with a collection of different environments, and the resulting model can then be used to plan goal-directed actions in novel environments that the agent has not seen before. We demonstrate that our agent can accurately plan actions for playing a simulated billiards game, which requires pushing a ball into a target position or into collision with another ball.



### Top-Down Learning for Structured Labeling with Convolutional Pseudoprior
- **Arxiv ID**: http://arxiv.org/abs/1511.07409v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.07409v2)
- **Published**: 2015-11-23 20:43:14+00:00
- **Updated**: 2016-07-26 05:25:29+00:00
- **Authors**: Saining Xie, Xun Huang, Zhuowen Tu
- **Comment**: To appear in ECCV 2016, 16 pages, 6 figures
- **Journal**: None
- **Summary**: Current practice in convolutional neural networks (CNN) remains largely bottom-up and the role of top-down process in CNN for pattern analysis and visual inference is not very clear. In this paper, we propose a new method for structured labeling by developing convolutional pseudo-prior (ConvPP) on the ground-truth labels. Our method has several interesting properties: (1) compared with classical machine learning algorithms like CRFs and Structural SVM, ConvPP automatically learns rich convolutional kernels to capture both short- and long- range contexts; (2) compared with cascade classifiers like Auto-Context, ConvPP avoids the iterative steps of learning a series of discriminative classifiers and automatically learns contextual configurations; (3) compared with recent efforts combing CNN models with CRFs and RNNs, ConvPP learns convolution in the labeling space with much improved modeling capability and less manual specification; (4) compared with Bayesian models like MRFs, ConvPP capitalizes on the rich representation power of convolution by automatically learning priors built on convolutional filters. We accomplish our task using pseudo-likelihood approximation to the prior under a novel fixed-point network structure that facilitates an end-to-end learning process. We show state-of-the-art results on sequential labeling and image labeling benchmarks.



### Constrained Structured Regression with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.07497v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.07497v1)
- **Published**: 2015-11-23 22:43:37+00:00
- **Updated**: 2015-11-23 22:43:37+00:00
- **Authors**: Deepak Pathak, Philipp Krähenbühl, Stella X. Yu, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have recently emerged as the dominant model in computer vision. If provided with enough training data, they predict almost any visual quantity. In a discrete setting, such as classification, CNNs are not only able to predict a label but often predict a confidence in the form of a probability distribution over the output space. In continuous regression tasks, such a probability estimate is often lacking. We present a regression framework which models the output distribution of neural networks. This output distribution allows us to infer the most likely labeling following a set of physical or modeling constraints. These constraints capture the intricate interplay between different input and output variables, and complement the output of a CNN. However, they may not hold everywhere. Our setup further allows to learn a confidence with which a constraint holds, in the form of a distribution of the constrain satisfaction. We evaluate our approach on the problem of intrinsic image decomposition, and show that constrained structured regression significantly increases the state-of-the-art.



