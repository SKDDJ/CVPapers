# Arxiv Papers in cs.CV on 2015-11-09
### Explicit Knowledge-based Reasoning for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1511.02570v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1511.02570v2)
- **Published**: 2015-11-09 05:25:57+00:00
- **Updated**: 2015-11-12 01:10:38+00:00
- **Authors**: Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, Anthony Dick
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: We describe a method for visual question answering which is capable of reasoning about contents of an image on the basis of information extracted from a large-scale knowledge base. The method not only answers natural language questions using concepts not contained in the image, but can provide an explanation of the reasoning by which it developed its answer. The method is capable of answering far more complex questions than the predominant long short-term memory-based approach, and outperforms it significantly in the testing. We also provide a dataset and a protocol by which to evaluate such methods, thus addressing one of the key issues in general visual ques- tion answering.



### A Century of Portraits: A Visual Historical Record of American High School Yearbooks
- **Arxiv ID**: http://arxiv.org/abs/1511.02575v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.02575v2)
- **Published**: 2015-11-09 05:44:39+00:00
- **Updated**: 2019-06-12 22:50:13+00:00
- **Authors**: Shiry Ginosar, Kate Rakelly, Sarah Sachs, Brian Yin, Crystal Lee, Philipp Krahenbuhl, Alexei A. Efros
- **Comment**: IEEE Transactions on Computational Imaging, September 2017
- **Journal**: None
- **Summary**: Imagery offers a rich description of our world and communicates a volume and type of information that cannot be captured by text alone. Since the invention of the camera, an ever-increasing number of photographs document our "visual culture" complementing historical texts. But currently, this treasure trove of knowledge can only be analyzed manually by historians, and only at small scale. In this paper we perform automated analysis on a large-scale historical image dataset. Our main contributions are: 1) A publicly-available dataset of 168,055 (37,921 frontal-facing) American high school yearbook portraits. 2) Weakly-supervised data-driven techniques to discover historical visual trends in fashion and identify date-specific visual patterns. 3) A classifier to predict when a portrait was taken, with median error of 4 years for women and 6 for men. 4) A new method for discovering and displaying the visual elements used by the CNN-based date-prediction model to date portraits, finding that they correspond to the tell-tale fashions of each era. Project page can be found at: http://people.eecs.berkeley.edu/~shiry/projects/yearbooks/yearbooks.html .



### Batch-normalized Maxout Network in Network
- **Arxiv ID**: http://arxiv.org/abs/1511.02583v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.02583v1)
- **Published**: 2015-11-09 07:09:57+00:00
- **Updated**: 2015-11-09 07:09:57+00:00
- **Authors**: Jia-Ren Chang, Yong-Sheng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper reports a novel deep architecture referred to as Maxout network In Network (MIN), which can enhance model discriminability and facilitate the process of information abstraction within the receptive field. The proposed network adopts the framework of the recently developed Network In Network structure, which slides a universal approximator, multilayer perceptron (MLP) with rectifier units, to exact features. Instead of MLP, we employ maxout MLP to learn a variety of piecewise linear activation functions and to mediate the problem of vanishing gradients that can occur when using rectifier units. Moreover, batch normalization is applied to reduce the saturation of maxout units by pre-conditioning the model and dropout is applied to prevent overfitting. Finally, average pooling is used in all pooling layers to regularize maxout MLP in order to facilitate information abstraction in every receptive field while tolerating the change of object position. Because average pooling preserves all features in the local patch, the proposed MIN model can enforce the suppression of irrelevant information during training. Our experiments demonstrated the state-of-the-art classification performance when the MIN model was applied to MNIST, CIFAR-10, and CIFAR-100 datasets and comparable performance for SVHN dataset.



### Parkinson's disease patient rehabilitation using gaming platforms: lessons learnt
- **Arxiv ID**: http://arxiv.org/abs/1511.02589v1
- **DOI**: 10.5121/ijbes.2015.2401
- **Categories**: **cs.CY**, cs.CV, I.6.3; I.6.8
- **Links**: [PDF](http://arxiv.org/pdf/1511.02589v1)
- **Published**: 2015-11-09 07:36:22+00:00
- **Updated**: 2015-11-09 07:36:22+00:00
- **Authors**: Ioannis Pachoulakis, Nikolaos Papadopoulos, Cleanthe Spanaki
- **Comment**: 12 pages, 3 figures
- **Journal**: None
- **Summary**: Parkinson's disease (PD) is a progressive neurodegenerative movement disorder where motor dysfunction gradually increases as the disease progress. In addition to administering dopaminergic PD-specific drugs, attending neurologists strongly recommend regular exercise combined with physiotherapy. However, because of the long-term nature of the disease, patients following traditional rehabilitation programs may get bored, lose interest and eventually drop out as a direct result of the repeatability and predictability of the prescribed exercises. Technology supported opportunities to liven up a daily exercise schedule have appeared in the form of character-based, virtual reality games which promote physical training in a non-linear and looser fashion and provide an experience that varies from one game loop the next. Such "exergames", a word that results from the amalgamation of the words "exercise" and "game" challenge patients into performing movements of varying complexity in a playful and immersive virtual environment. Today's game consoles such as Nintendo's Wii, Sony PlayStation Eye and Microsoft's Kinect sensor present new opportunities to infuse motivation and variety to an otherwise mundane physiotherapy routine. In this paper we present some of these approaches, discuss their suitability for these PD patients, mainly on the basis of demands made on balance, agility and gesture precision, and present design principles that exergame platforms must comply with in order to be suitable for PD patients.



### An Efficient Multilinear Optimization Framework for Hypergraph Matching
- **Arxiv ID**: http://arxiv.org/abs/1511.02667v2
- **DOI**: 10.1109/TPAMI.2016.2574706
- **Categories**: **cs.CV**, cs.DS, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1511.02667v2)
- **Published**: 2015-11-09 13:15:10+00:00
- **Updated**: 2016-05-24 19:06:19+00:00
- **Authors**: Quynh Nguyen, Francesco Tudisco, Antoine Gautier, Matthias Hein
- **Comment**: accepted at IEEE Transactions on Pattern Analysis and Machine
  Intelligence (PAMI) 2016
- **Journal**: None
- **Summary**: Hypergraph matching has recently become a popular approach for solving correspondence problems in computer vision as it allows to integrate higher-order geometric information. Hypergraph matching can be formulated as a third-order optimization problem subject to the assignment constraints which turns out to be NP-hard. In recent work, we have proposed an algorithm for hypergraph matching which first lifts the third-order problem to a fourth-order problem and then solves the fourth-order problem via optimization of the corresponding multilinear form. This leads to a tensor block coordinate ascent scheme which has the guarantee of providing monotonic ascent in the original matching score function and leads to state-of-the-art performance both in terms of achieved matching score and accuracy. In this paper we show that the lifting step to a fourth-order problem can be avoided yielding a third-order scheme with the same guarantees and performance but being two times faster. Moreover, we introduce a homotopy type method which further improves the performance.



### Semantic Segmentation with Boundary Neural Fields
- **Arxiv ID**: http://arxiv.org/abs/1511.02674v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.02674v2)
- **Published**: 2015-11-09 13:27:30+00:00
- **Updated**: 2016-05-24 23:32:10+00:00
- **Authors**: Gedas Bertasius, Jianbo Shi, Lorenzo Torresani
- **Comment**: None
- **Journal**: None
- **Summary**: The state-of-the-art in semantic segmentation is currently represented by fully convolutional networks (FCNs). However, FCNs use large receptive fields and many pooling layers, both of which cause blurring and low spatial resolution in the deep layers. As a result FCNs tend to produce segmentations that are poorly localized around object boundaries. Prior work has attempted to address this issue in post-processing steps, for example using a color-based CRF on top of the FCN predictions. However, these approaches require additional parameters and low-level features that are difficult to tune and integrate into the original network architecture. Additionally, most CRFs use color-based pixel affinities, which are not well suited for semantic segmentation and lead to spatially disjoint predictions.   To overcome these problems, we introduce a Boundary Neural Field (BNF), which is a global energy model integrating FCN predictions with boundary cues. The boundary information is used to enhance semantic segment coherence and to improve object localization. Specifically, we first show that the convolutional filters of semantic FCNs provide good features for boundary detection. We then employ the predicted boundaries to define pairwise potentials in our energy. Finally, we show that our energy decomposes semantic segmentation into multiple binary problems, which can be relaxed for efficient global optimization. We report extensive experiments demonstrating that minimization of our global boundary-based energy yields results superior to prior globalization methods, both quantitatively as well as qualitatively.



### Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1511.02680v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.02680v2)
- **Published**: 2015-11-09 14:00:21+00:00
- **Updated**: 2016-10-10 22:04:21+00:00
- **Authors**: Alex Kendall, Vijay Badrinarayanan, Roberto Cipolla
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep learning framework for probabilistic pixel-wise semantic segmentation, which we term Bayesian SegNet. Semantic segmentation is an important tool for visual scene understanding and a meaningful measure of uncertainty is essential for decision making. Our contribution is a practical system which is able to predict pixel-wise class labels with a measure of model uncertainty. We achieve this by Monte Carlo sampling with dropout at test time to generate a posterior distribution of pixel class labels. In addition, we show that modelling uncertainty improves segmentation performance by 2-3% across a number of state of the art architectures such as SegNet, FCN and Dilation Network, with no additional parametrisation. We also observe a significant improvement in performance for smaller datasets where modelling uncertainty is more effective. We benchmark Bayesian SegNet on the indoor SUN Scene Understanding and outdoor CamVid driving scenes datasets.



### Exploiting Egocentric Object Prior for 3D Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1511.02682v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.02682v1)
- **Published**: 2015-11-09 14:01:50+00:00
- **Updated**: 2015-11-09 14:01:50+00:00
- **Authors**: Gedas Bertasius, Hyun Soo Park, Jianbo Shi
- **Comment**: None
- **Journal**: None
- **Summary**: On a minute-to-minute basis people undergo numerous fluid interactions with objects that barely register on a conscious level. Recent neuroscientific research demonstrates that humans have a fixed size prior for salient objects. This suggests that a salient object in 3D undergoes a consistent transformation such that people's visual system perceives it with an approximately fixed size. This finding indicates that there exists a consistent egocentric object prior that can be characterized by shape, size, depth, and location in the first person view.   In this paper, we develop an EgoObject Representation, which encodes these characteristics by incorporating shape, location, size and depth features from an egocentric RGBD image. We empirically show that this representation can accurately characterize the egocentric object prior by testing it on an egocentric RGBD dataset for three tasks: the 3D saliency detection, future saliency prediction, and interaction classification. This representation is evaluated on our new Egocentric RGBD Saliency dataset that includes various activities such as cooking, dining, and shopping. By using our EgoObject representation, we outperform previously proposed models for saliency detection (relative 30% improvement for 3D saliency detection task) on our dataset. Additionally, we demonstrate that this representation allows us to predict future salient objects based on the gaze cue and classify people's interactions with objects.



### A Light CNN for Deep Face Representation with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1511.02683v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.02683v4)
- **Published**: 2015-11-09 14:02:03+00:00
- **Updated**: 2018-08-12 02:35:58+00:00
- **Authors**: Xiang Wu, Ran He, Zhenan Sun, Tieniu Tan
- **Comment**: arXiv admin note: text overlap with arXiv:1507.04844. The models are
  released on https://github.com/AlfredXiangWu/LightCNN, IEEE Transactions on
  Information Forensics and Security, 2018
- **Journal**: None
- **Summary**: The volume of convolutional neural network (CNN) models proposed for face recognition has been continuously growing larger to better fit large amount of training data. When training data are obtained from internet, the labels are likely to be ambiguous and inaccurate. This paper presents a Light CNN framework to learn a compact embedding on the large-scale face data with massive noisy labels. First, we introduce a variation of maxout activation, called Max-Feature-Map (MFM), into each convolutional layer of CNN. Different from maxout activation that uses many feature maps to linearly approximate an arbitrary convex activation function, MFM does so via a competitive relationship. MFM can not only separate noisy and informative signals but also play the role of feature selection between two feature maps. Second, three networks are carefully designed to obtain better performance meanwhile reducing the number of parameters and computational costs. Lastly, a semantic bootstrapping method is proposed to make the prediction of the networks more consistent with noisy labels. Experimental results show that the proposed framework can utilize large-scale noisy data to learn a Light model that is efficient in computational costs and storage spaces. The learned single network with a 256-D representation achieves state-of-the-art results on various face benchmarks without fine-tuning. The code is released on https://github.com/AlfredXiangWu/LightCNN.



### Biologically Inspired Dynamic Textures for Probing Motion Perception
- **Arxiv ID**: http://arxiv.org/abs/1511.02705v1
- **DOI**: None
- **Categories**: **cs.CV**, math.ST, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1511.02705v1)
- **Published**: 2015-11-09 14:50:25+00:00
- **Updated**: 2015-11-09 14:50:25+00:00
- **Authors**: Jonathan Vacher, Andrew Meso, Laurent U Perrinet, Gabriel Peyré
- **Comment**: Twenty-ninth Annual Conference on Neural Information Processing
  Systems (NIPS), Dec 2015, Montreal, Canada
- **Journal**: None
- **Summary**: Perception is often described as a predictive process based on an optimal inference with respect to a generative model. We study here the principled construction of a generative model specifically crafted to probe motion perception. In that context, we first provide an axiomatic, biologically-driven derivation of the model. This model synthesizes random dynamic textures which are defined by stationary Gaussian distributions obtained by the random aggregation of warped patterns. Importantly, we show that this model can equivalently be described as a stochastic partial differential equation. Using this characterization of motion in images, it allows us to recast motion-energy models into a principled Bayesian inference framework. Finally, we apply these textures in order to psychophysically probe speed perception in humans. In this framework, while the likelihood is derived from the generative model, the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion.



### Generating Images from Captions with Attention
- **Arxiv ID**: http://arxiv.org/abs/1511.02793v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.02793v2)
- **Published**: 2015-11-09 18:18:53+00:00
- **Updated**: 2016-02-29 17:56:29+00:00
- **Authors**: Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, Ruslan Salakhutdinov
- **Comment**: Published as a conference paper at ICLR 2016
- **Journal**: None
- **Summary**: Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.



### Neural Module Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.02799v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.02799v4)
- **Published**: 2015-11-09 18:48:39+00:00
- **Updated**: 2017-07-24 17:15:06+00:00
- **Authors**: Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein
- **Comment**: Corrects an error in the evaluation of the NMN-only ablation
  experiment
- **Journal**: None
- **Summary**: Visual question answering is fundamentally compositional in nature---a question like "where is the dog?" shares substructure with questions like "what color is the dog?" and "where is the cat?" This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning *neural module networks*, which compose collections of jointly-trained neural "modules" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.



### Partial Membership Latent Dirichlet Allocation
- **Arxiv ID**: http://arxiv.org/abs/1511.02821v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.02821v2)
- **Published**: 2015-11-09 20:04:56+00:00
- **Updated**: 2016-04-05 03:59:15+00:00
- **Authors**: Chao Chen, Alina Zare, J. Tory Cobb
- **Comment**: cut to 6 pages, add sunset results
- **Journal**: None
- **Summary**: Topic models (e.g., pLSA, LDA, SLDA) have been widely used for segmenting imagery. These models are confined to crisp segmentation. Yet, there are many images in which some regions cannot be assigned a crisp label (e.g., transition regions between a foggy sky and the ground or between sand and water at a beach). In these cases, a visual word is best represented with partial memberships across multiple topics. To address this, we present a partial membership latent Dirichlet allocation (PM-LDA) model and associated parameter estimation algorithms. Experimental results on two natural image datasets and one SONAR image dataset show that PM-LDA can produce both crisp and soft semantic image segmentations; a capability existing methods do not have.



### Multiple Instance Dictionary Learning using Functions of Multiple Instances
- **Arxiv ID**: http://arxiv.org/abs/1511.02825v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1511.02825v3)
- **Published**: 2015-11-09 20:12:19+00:00
- **Updated**: 2016-08-03 21:03:51+00:00
- **Authors**: Changzhe Jiao, Alina Zare
- **Comment**: Final submission to ICPR
- **Journal**: None
- **Summary**: A multiple instance dictionary learning method using functions of multiple instances (DL-FUMI) is proposed to address target detection and two-class classification problems with inaccurate training labels. Given inaccurate training labels, DL-FUMI learns a set of target dictionary atoms that describe the most distinctive and representative features of the true positive class as well as a set of nontarget dictionary atoms that account for the shared information found in both the positive and negative instances. Experimental results show that the estimated target dictionary atoms found by DL-FUMI are more representative prototypes and identify better discriminative features of the true positive class than existing methods in the literature. DL-FUMI is shown to have significantly better performance on several target detection and classification problems as compared to other multiple instance learning (MIL) dictionary learning algorithms on a variety of MIL problems.



### Symmetries and control in generative neural nets
- **Arxiv ID**: http://arxiv.org/abs/1511.02841v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.02841v3)
- **Published**: 2015-11-09 20:49:03+00:00
- **Updated**: 2016-04-08 21:38:31+00:00
- **Authors**: Galin Georgiev
- **Comment**: None
- **Journal**: None
- **Summary**: We study generative nets which can control and modify observations, after being trained on real-life datasets. In order to zoom-in on an object, some spatial, color and other attributes are learned by classifiers in specialized attention nets. In field-theoretical terms, these learned symmetry statistics form the gauge group of the data set. Plugging them in the generative layers of auto-classifiers-encoders (ACE) appears to be the most direct way to simultaneously: i) generate new observations with arbitrary attributes, from a given class, ii) describe the low-dimensional manifold encoding the "essence" of the data, after superfluous attributes are factored out, and iii) organically control, i.e., move or modify objects within given observations. We demonstrate the sharp improvement of the generative qualities of shallow ACE, with added spatial and color symmetry statistics, on the distorted MNIST and CIFAR10 datasets.



### Weakly Supervised Deep Detection Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.02853v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.02853v4)
- **Published**: 2015-11-09 20:58:30+00:00
- **Updated**: 2016-12-19 09:44:29+00:00
- **Authors**: Hakan Bilen, Andrea Vedaldi
- **Comment**: 9 pages, 3 figures, 6 tables, Conference on Computer Vision and
  Pattern Recognition (CVPR) 2016. This version fixes the equation in Section
  3.4, thanks to Vadim Kantorov and Sini\v{s}a \v{S}egvi\'c
- **Journal**: None
- **Summary**: Weakly supervised learning of object detection is an important problem in image understanding that still does not have a satisfactory solution. In this paper, we address this problem by exploiting the power of deep convolutional neural networks pre-trained on large-scale image-level classification tasks. We propose a weakly supervised deep detection architecture that modifies one such network to operate at the level of image regions, performing simultaneously region selection and classification. Trained as an image classifier, the architecture implicitly learns object detectors that are better than alternative weakly supervised detection systems on the PASCAL VOC data. The model, which is a simple and elegant end-to-end architecture, outperforms standard data augmentation and fine-tuning techniques for the task of image-level classification as well.



### Visual Language Modeling on CNN Image Representations
- **Arxiv ID**: http://arxiv.org/abs/1511.02872v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.02872v1)
- **Published**: 2015-11-09 21:00:08+00:00
- **Updated**: 2015-11-09 21:00:08+00:00
- **Authors**: Hiroharu Kato, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: Measuring the naturalness of images is important to generate realistic images or to detect unnatural regions in images. Additionally, a method to measure naturalness can be complementary to Convolutional Neural Network (CNN) based features, which are known to be insensitive to the naturalness of images. However, most probabilistic image models have insufficient capability of modeling the complex and abstract naturalness that we feel because they are built directly on raw image pixels. In this work, we assume that naturalness can be measured by the predictability on high-level features during eye movement. Based on this assumption, we propose a novel method to evaluate the naturalness by building a variant of Recurrent Neural Network Language Models on pre-trained CNN representations. Our method is applied to two tasks, demonstrating that 1) using our method as a regularizer enables us to generate more understandable images from image features than existing approaches, and 2) unnaturalness maps produced by our method achieve state-of-the-art eye fixation prediction performance on two well-studied datasets.



### Spatially Coherent Random Forests
- **Arxiv ID**: http://arxiv.org/abs/1511.02911v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.02911v2)
- **Published**: 2015-11-09 22:14:00+00:00
- **Updated**: 2015-12-05 10:13:06+00:00
- **Authors**: Tal Remez, Shai Avidan
- **Comment**: None
- **Journal**: None
- **Summary**: Spatially Coherent Random Forest (SCRF) extends Random Forest to create spatially coherent labeling. Each split function in SCRF is evaluated based on a traditional information gain measure that is regularized by a spatial coherency term. This way, SCRF is encouraged to choose split functions that cluster pixels both in appearance space and in image space. In particular, we use SCRF to detect contours in images, where contours are taken to be the boundaries between different regions. Each tree in the forest produces a segmentation of the image plane and the boundaries of the segmentations of all trees are aggregated to produce a final hierarchical contour map. We show that this modification improves the performance of regular Random Forest by about 10% on the standard Berkeley Segmentation Datasets. We believe that SCRF can be used in other settings as well.



### Spectral-Spatial Classification of Hyperspectral Image Using Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1511.02916v1
- **DOI**: 10.1109/ICICS.2013.6782778
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.02916v1)
- **Published**: 2015-11-09 22:29:13+00:00
- **Updated**: 2015-11-09 22:29:13+00:00
- **Authors**: Zhouhan Lin, Yushi Chen, Xing Zhao, Gang Wang
- **Comment**: Accepted as a conference paper at ICICS 2013, an updated version.
  Codes published. 9 pages, 6 figures
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) classification is a hot topic in the remote sensing community. This paper proposes a new framework of spectral-spatial feature extraction for HSI classification, in which for the first time the concept of deep learning is introduced. Specifically, the model of autoencoder is exploited in our framework to extract various kinds of features. First we verify the eligibility of autoencoder by following classical spectral information based classification and use autoencoders with different depth to classify hyperspectral image. Further in the proposed framework, we combine PCA on spectral dimension and autoencoder on the other two spatial dimensions to extract spectral-spatial information for classification. The experimental results show that this framework achieves the highest classification accuracy among all methods, and outperforms classical classifiers such as SVM and PCA-based SVM.



### Detecting events and key actors in multi-person videos
- **Arxiv ID**: http://arxiv.org/abs/1511.02917v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1511.02917v2)
- **Published**: 2015-11-09 22:30:19+00:00
- **Updated**: 2016-03-17 00:02:03+00:00
- **Authors**: Vignesh Ramanathan, Jonathan Huang, Sami Abu-El-Haija, Alexander Gorban, Kevin Murphy, Li Fei-Fei
- **Comment**: Accepted for publication in CVPR'16
- **Journal**: None
- **Summary**: Multi-person event recognition is a challenging task, often with many people active in the scene but only a small subset contributing to an actual event. In this paper, we propose a model which learns to detect events in such videos while automatically "attending" to the people responsible for the event. Our model does not use explicit annotations regarding who or where those people are during training and testing. In particular, we track people in videos and use a recurrent neural network (RNN) to represent the track features. We learn time-varying attention weights to combine these features at each time-instant. The attended features are then processed using another RNN for event detection/classification. Since most video datasets with multiple people are restricted to a small number of videos, we also collected a new basketball dataset comprising 257 basketball games with 14K event annotations corresponding to 11 event classes. Our model outperforms state-of-the-art methods for both event classification and detection on this new dataset. Additionally, we show that the attention mechanism is able to consistently localize the relevant players.



### Massive Online Crowdsourced Study of Subjective and Objective Picture Quality
- **Arxiv ID**: http://arxiv.org/abs/1511.02919v1
- **DOI**: 10.1109/TIP.2015.2500021
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.02919v1)
- **Published**: 2015-11-09 22:39:58+00:00
- **Updated**: 2015-11-09 22:39:58+00:00
- **Authors**: Deepti Ghadiyaram, Alan C. Bovik
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Most publicly available image quality databases have been created under highly controlled conditions by introducing graded simulated distortions onto high-quality photographs. However, images captured using typical real-world mobile camera devices are usually afflicted by complex mixtures of multiple distortions, which are not necessarily well-modeled by the synthetic distortions found in existing databases. The originators of existing legacy databases usually conducted human psychometric studies to obtain statistically meaningful sets of human opinion scores on images in a stringently controlled visual environment, resulting in small data collections relative to other kinds of image analysis databases. Towards overcoming these limitations, we designed and created a new database that we call the LIVE In the Wild Image Quality Challenge Database, which contains widely diverse authentic image distortions on a large number of images captured using a representative variety of modern mobile devices. We also designed and implemented a new online crowdsourcing system, which we have used to conduct a very large-scale, multi-month image quality assessment subjective study. Our database consists of over 350000 opinion scores on 1162 images evaluated by over 7000 unique human observers. Despite the lack of control over the experimental environments of the numerous study participants, we demonstrate excellent internal consistency of the subjective dataset. We also evaluate several top-performing blind Image Quality Assessment algorithms on it and present insights on how mixtures of distortions challenge both end users as well as automatic perceptual quality prediction models.



### Hyperspectral Image Recovery via Hybrid Regularization
- **Arxiv ID**: http://arxiv.org/abs/1511.02928v2
- **DOI**: 10.1109/TIP.2016.2614131
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.02928v2)
- **Published**: 2015-11-09 23:31:31+00:00
- **Updated**: 2016-08-26 02:27:31+00:00
- **Authors**: Reza Arablouei, Frank de Hoog
- **Comment**: None
- **Journal**: None
- **Summary**: Natural images tend to mostly consist of smooth regions with individual pixels having highly correlated spectra. This information can be exploited to recover hyperspectral images of natural scenes from their incomplete and noisy measurements. To perform the recovery while taking full advantage of the prior knowledge, we formulate a composite cost function containing a square-error data-fitting term and two distinct regularization terms pertaining to spatial and spectral domains. The regularization for the spatial domain is the sum of total-variation of the image frames corresponding to all spectral bands. The regularization for the spectral domain is the l1-norm of the coefficient matrix obtained by applying a suitable sparsifying transform to the spectra of the pixels. We use an accelerated proximal-subgradient method to minimize the formulated cost function. We analyze the performance of the proposed algorithm and prove its convergence. Numerical simulations using real hyperspectral images exhibit that the proposed algorithm offers an excellent recovery performance with a number of measurements that is only a small fraction of the hyperspectral image data size. Simulation results also show that the proposed algorithm significantly outperforms an accelerated proximal-gradient algorithm that solves the classical basis-pursuit denoising problem to recover the hyperspectral image.



