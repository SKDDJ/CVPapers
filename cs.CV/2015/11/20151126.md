# Arxiv Papers in cs.CV on 2015-11-26
### A Computational Model for Amodal Completion
- **Arxiv ID**: http://arxiv.org/abs/1511.08418v2
- **DOI**: 10.1007/s10851-016-0652-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.08418v2)
- **Published**: 2015-11-26 15:25:46+00:00
- **Updated**: 2016-03-29 14:49:41+00:00
- **Authors**: Maria Oliver, Gloria Haro, Mariella Dimiccoli, Baptiste Mazin, Coloma Ballester
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a computational model to recover the most likely interpretation of the 3D scene structure from a planar image, where some objects may occlude others. The estimated scene interpretation is obtained by integrating some global and local cues and provides both the complete disoccluded objects that form the scene and their ordering according to depth. Our method first computes several distal scenes which are compatible with the proximal planar image. To compute these different hypothesized scenes, we propose a perceptually inspired object disocclusion method, which works by minimizing the Euler's elastica as well as by incorporating the relatability of partially occluded contours and the convexity of the disoccluded objects. Then, to estimate the preferred scene we rely on a Bayesian model and define probabilities taking into account the global complexity of the objects in the hypothesized scenes as well as the effort of bringing these objects in their relative position in the planar image, which is also measured by an Euler's elastica-based quantity. The model is illustrated with numerical experiments on, both, synthetic and real images showing the ability of our model to reconstruct the occluded objects and the preferred perceptual order among them. We also present results on images of the Berkeley dataset with provided figure-ground ground-truth labeling.



### Towards Automatic Image Editing: Learning to See another You
- **Arxiv ID**: http://arxiv.org/abs/1511.08446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.08446v1)
- **Published**: 2015-11-26 16:33:10+00:00
- **Updated**: 2015-11-26 16:33:10+00:00
- **Authors**: Amir Ghodrati, Xu Jia, Marco Pedersoli, Tinne Tuytelaars
- **Comment**: None
- **Journal**: None
- **Summary**: Learning the distribution of images in order to generate new samples is a challenging task due to the high dimensionality of the data and the highly non-linear relations that are involved. Nevertheless, some promising results have been reported in the literature recently,building on deep network architectures. In this work, we zoom in on a specific type of image generation: given an image and knowing the category of objects it belongs to (e.g. faces), our goal is to generate a similar and plausible image, but with some altered attributes. This is particularly challenging, as the model needs to learn to disentangle the effect of each attribute and to apply a desired attribute change to a given input image, while keeping the other attributes and overall object appearance intact. To this end, we learn a convolutional network, where the desired attribute information is encoded then merged with the encoded image at feature map level. We show promising results, both qualitatively as well as quantitatively, in the context of a retrieval experiment, on two face datasets (MultiPie and CAS-PEAL-R1).



### An Introduction to Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.08458v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.08458v2)
- **Published**: 2015-11-26 17:45:01+00:00
- **Updated**: 2015-12-02 18:06:03+00:00
- **Authors**: Keiron O'Shea, Ryan Nash
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.   This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.



### An analysis of the factors affecting keypoint stability in scale-space
- **Arxiv ID**: http://arxiv.org/abs/1511.08478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.08478v1)
- **Published**: 2015-11-26 19:09:11+00:00
- **Updated**: 2015-11-26 19:09:11+00:00
- **Authors**: Ives Rey-Otero, Jean-Michel Morel, Mauricio Delbracio
- **Comment**: None
- **Journal**: None
- **Summary**: The most popular image matching algorithm SIFT, introduced by D. Lowe a decade ago, has proven to be sufficiently scale invariant to be used in numerous applications. In practice, however, scale invariance may be weakened by various sources of error inherent to the SIFT implementation affecting the stability and accuracy of keypoint detection. The density of the sampling of the Gaussian scale-space and the level of blur in the input image are two of these sources. This article presents a numerical analysis of their impact on the extracted keypoints stability. Such an analysis has both methodological and practical implications, on how to compare feature detectors and on how to improve SIFT. We show that even with a significantly oversampled scale-space numerical errors prevent from achieving perfect stability. Usual strategies to filter out unstable detections are shown to be inefficient. We also prove that the effect of the error in the assumption on the initial blur is asymmetric and that the method is strongly degraded in presence of aliasing or without a correct assumption on the camera blur.



### Iterative Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1511.08498v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.08498v3)
- **Published**: 2015-11-26 20:51:17+00:00
- **Updated**: 2016-06-10 18:48:10+00:00
- **Authors**: Ke Li, Bharath Hariharan, Jitendra Malik
- **Comment**: 13 pages, 10 figures; IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2016
- **Journal**: None
- **Summary**: Existing methods for pixel-wise labelling tasks generally disregard the underlying structure of labellings, often leading to predictions that are visually implausible. While incorporating structure into the model should improve prediction quality, doing so is challenging - manually specifying the form of structural constraints may be impractical and inference often becomes intractable even if structural constraints are given. We sidestep this problem by reducing structured prediction to a sequence of unconstrained prediction problems and demonstrate that this approach is capable of automatically discovering priors on shape, contiguity of region predictions and smoothness of region contours from data without any a priori specification. On the instance segmentation task, this method outperforms the state-of-the-art, achieving a mean $\mathrm{AP}^{r}$ of 63.6% at 50% overlap and 43.3% at 70% overlap.



### TennisVid2Text: Fine-grained Descriptions for Domain Specific Videos
- **Arxiv ID**: http://arxiv.org/abs/1511.08522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.08522v1)
- **Published**: 2015-11-26 22:21:44+00:00
- **Updated**: 2015-11-26 22:21:44+00:00
- **Authors**: Mohak Sukhwani, C. V. Jawahar
- **Comment**: BMVC 2015
- **Journal**: None
- **Summary**: Automatically describing videos has ever been fascinating. In this work, we attempt to describe videos from a specific domain - broadcast videos of lawn tennis matches. Given a video shot from a tennis match, we intend to generate a textual commentary similar to what a human expert would write on a sports website. Unlike many recent works that focus on generating short captions, we are interested in generating semantically richer descriptions. This demands a detailed low-level analysis of the video content, specially the actions and interactions among subjects. We address this by limiting our domain to the game of lawn tennis. Rich descriptions are generated by leveraging a large corpus of human created descriptions harvested from Internet. We evaluate our method on a newly created tennis video data set. Extensive analysis demonstrate that our approach addresses both semantic correctness as well as readability aspects involved in the task.



