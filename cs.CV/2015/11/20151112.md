# Arxiv Papers in cs.CV on 2015-11-12
### Grounding of Textual Phrases in Images by Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1511.03745v4
- **DOI**: 10.1007/978-3-319-46448-0_49
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.03745v4)
- **Published**: 2015-11-12 01:13:47+00:00
- **Updated**: 2017-02-17 21:02:05+00:00
- **Authors**: Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, Bernt Schiele
- **Comment**: published at ECCV 2016 (oral); updated to final version
- **Journal**: None
- **Summary**: Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual content is a challenging problem with many applications for human-computer interaction and image-text reference resolution. Few datasets provide the ground truth spatial localization of phrases, thus it is desirable to learn from data with no or little grounding supervision. We propose a novel approach which learns grounding by reconstructing a given phrase using an attention mechanism, which can be either latent or optimized directly. During training our approach encodes the phrase using a recurrent network language model and then learns to attend to the relevant image region in order to reconstruct the input phrase. At test time, the correct attention, i.e., the grounding, is evaluated. If grounding supervision is available it can be directly applied via a loss over the attention mechanism. We demonstrate the effectiveness of our approach on the Flickr 30k Entities and ReferItGame datasets with different levels of supervision, ranging from no supervision over partial supervision to full supervision. Our supervised variant improves by a large margin over the state-of-the-art on both datasets.



### Automatic Content-Aware Color and Tone Stylization
- **Arxiv ID**: http://arxiv.org/abs/1511.03748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03748v1)
- **Published**: 2015-11-12 01:31:35+00:00
- **Updated**: 2015-11-12 01:31:35+00:00
- **Authors**: Joon-Young Lee, Kalyan Sunkavalli, Zhe Lin, Xiaohui Shen, In So Kweon
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: We introduce a new technique that automatically generates diverse, visually compelling stylizations for a photograph in an unsupervised manner. We achieve this by learning style ranking for a given input using a large photo collection and selecting a diverse subset of matching styles for final style transfer. We also propose a novel technique that transfers the global color and tone of the chosen exemplars to the input photograph while avoiding the common visual artifacts produced by the existing style transfer methods. Together, our style selection and transfer techniques produce compelling, artifact-free results on a wide range of input photographs, and a user study shows that our results are preferred over other techniques.



### Shearlet-Based Detection of Flame Fronts
- **Arxiv ID**: http://arxiv.org/abs/1511.03753v2
- **DOI**: 10.1007/s00348-016-2128-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03753v2)
- **Published**: 2015-11-12 01:52:12+00:00
- **Updated**: 2016-02-03 18:23:33+00:00
- **Authors**: Rafael Reisenhofer, Johannes Kiefer, Emily J. King
- **Comment**: None
- **Journal**: Experiments in Fluids, vol. 57(3), 41:1-41:14, 2016
- **Summary**: Identifying and characterizing flame fronts is the most common task in the computer-assisted analysis of data obtained from imaging techniques such as planar laser-induced fluorescence (PLIF), laser Rayleigh scattering (LRS), or particle imaging velocimetry (PIV). We present a novel edge and ridge (line) detection algorithm based on complex-valued wavelet-like analyzing functions -- so-called complex shearlets -- displaying several traits useful for the extraction of flame fronts. In addition to providing a unified approach to the detection of edges and ridges, our method inherently yields estimates of local tangent orientations and local curvatures. To examine the applicability for high-frequency recordings of combustion processes, the algorithm is applied to mock images distorted with varying degrees of noise and real-world PLIF images of both OH and CH radicals. Furthermore, we compare the performance of the newly proposed complex shearlet-based measure to well-established edge and ridge detection techniques such as the Canny edge detector, another shearlet-based edge detector, and the phase congruency measure.



### ProNet: Learning to Propose Object-specific Boxes for Cascaded Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.03776v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03776v3)
- **Published**: 2015-11-12 05:06:16+00:00
- **Updated**: 2016-04-13 02:56:43+00:00
- **Authors**: Chen Sun, Manohar Paluri, Ronan Collobert, Ram Nevatia, Lubomir Bourdev
- **Comment**: CVPR 2016 (fixed reference issue)
- **Journal**: None
- **Summary**: This paper aims to classify and locate objects accurately and efficiently, without using bounding box annotations. It is challenging as objects in the wild could appear at arbitrary locations and in different scales. In this paper, we propose a novel classification architecture ProNet based on convolutional neural networks. It uses computationally efficient neural networks to propose image regions that are likely to contain objects, and applies more powerful but slower networks on the proposed regions. The basic building block is a multi-scale fully-convolutional network which assigns object confidence scores to boxes at different locations and scales. We show that such networks can be trained effectively using image-level annotations, and can be connected into cascades or trees for efficient object classification. ProNet outperforms previous state-of-the-art significantly on PASCAL VOC 2012 and MS COCO datasets for object classification and point-based localization.



### Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control
- **Arxiv ID**: http://arxiv.org/abs/1511.03791v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1511.03791v2)
- **Published**: 2015-11-12 06:19:59+00:00
- **Updated**: 2015-11-13 05:41:08+00:00
- **Authors**: Fangyi Zhang, Jürgen Leitner, Michael Milford, Ben Upcroft, Peter Corke
- **Comment**: 8 pages, to appear in the proceedings of Australasian Conference on
  Robotics and Automation (ACRA) 2015
- **Journal**: None
- **Summary**: This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation. A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.



### Hand-Object Interaction and Precise Localization in Transitive Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1511.03814v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03814v2)
- **Published**: 2015-11-12 08:41:47+00:00
- **Updated**: 2016-02-24 09:29:18+00:00
- **Authors**: Amir Rosenfeld, Shimon Ullman
- **Comment**: Minor changes: title and abstract
- **Journal**: None
- **Summary**: Action recognition in still images has seen major improvement in recent years due to advances in human pose estimation, object recognition and stronger feature representations produced by deep neural networks. However, there are still many cases in which performance remains far from that of humans. A major difficulty arises in distinguishing between transitive actions in which the overall actor pose is similar, and recognition therefore depends on details of the grasp and the object, which may be largely occluded. In this paper we demonstrate how recognition is improved by obtaining precise localization of the action-object and consequently extracting details of the object shape together with the actor-object interaction. To obtain exact localization of the action object and its interaction with the actor, we employ a coarse-to-fine approach which combines semantic segmentation and contextual features, in successive stages. We focus on (but are not limited) to face-related actions, a set of actions that includes several currently challenging categories. We present an average relative improvement of 35% over state-of-the art and validate through experimentation the effectiveness of our approach.



### When Naïve Bayes Nearest Neighbours Meet Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.03853v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03853v2)
- **Published**: 2015-11-12 10:54:21+00:00
- **Updated**: 2015-11-17 11:45:14+00:00
- **Authors**: Ilja Kuzborskij, Fabio Maria Carlucci, Barbara Caputo
- **Comment**: None
- **Journal**: None
- **Summary**: Since Convolutional Neural Networks (CNNs) have become the leading learning paradigm in visual recognition, Naive Bayes Nearest Neighbour (NBNN)-based classifiers have lost momentum in the community. This is because (1) such algorithms cannot use CNN activations as input features; (2) they cannot be used as final layer of CNN architectures for end-to-end training , and (3) they are generally not scalable and hence cannot handle big data. This paper proposes a framework that addresses all these issues, thus bringing back NBNNs on the map. We solve the first by extracting CNN activations from local patches at multiple scale levels, similarly to [1]. We address simultaneously the second and third by proposing a scalable version of Naive Bayes Non-linear Learning (NBNL, [2]). Results obtained using pre-trained CNNs on standard scene and domain adaptation databases show the strength of our approach, opening a new season for NBNNs.



### Feature Learning based Deep Supervised Hashing with Pairwise Labels
- **Arxiv ID**: http://arxiv.org/abs/1511.03855v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, H.3.1
- **Links**: [PDF](http://arxiv.org/pdf/1511.03855v2)
- **Published**: 2015-11-12 11:11:42+00:00
- **Updated**: 2016-04-21 09:27:38+00:00
- **Authors**: Wu-Jun Li, Sheng Wang, Wang-Cheng Kang
- **Comment**: IJCAI 2016
- **Journal**: None
- **Summary**: Recent years have witnessed wide application of hashing for large-scale image retrieval. However, most existing hashing methods are based on hand-crafted features which might not be optimally compatible with the hashing procedure. Recently, deep hashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown better performance than traditional hashing methods with hand-crafted features. Most of these deep hashing methods are supervised whose supervised information is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this paper, we propose a novel deep hashing method, called deep pairwise-supervised hashing(DPSH), to perform simultaneous feature learning and hash-code learning for applications with pairwise labels. Experiments on real datasets show that our DPSH method can outperform other methods to achieve the state-of-the-art performance in image retrieval applications.



### Learning Human Identity from Motion Patterns
- **Arxiv ID**: http://arxiv.org/abs/1511.03908v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.03908v4)
- **Published**: 2015-11-12 14:48:53+00:00
- **Updated**: 2016-04-21 16:04:00+00:00
- **Authors**: Natalia Neverova, Christian Wolf, Griffin Lacey, Lex Fridman, Deepak Chandra, Brandon Barbello, Graham Taylor
- **Comment**: 10 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: We present a large-scale study exploring the capability of temporal deep neural networks to interpret natural human kinematics and introduce the first method for active biometric authentication with mobile inertial sensors. At Google, we have created a first-of-its-kind dataset of human movements, passively collected by 1500 volunteers using their smartphones daily over several months. We (1) compare several neural architectures for efficient learning of temporal multi-modal data representations, (2) propose an optimized shift-invariant dense convolutional mechanism (DCWRNN), and (3) incorporate the discriminatively-trained dynamic features in a probabilistic generative framework taking into account temporal characteristics. Our results demonstrate that human kinematics convey important information about user identity and can serve as a valuable component of multi-modal authentication systems.



### Representational Distance Learning for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.03979v6
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.03979v6)
- **Published**: 2015-11-12 17:35:03+00:00
- **Updated**: 2016-11-07 18:37:05+00:00
- **Authors**: Patrick McClure, Nikolaus Kriegeskorte
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) provide useful models of visual representational transformations. We present a method that enables a DNN (student) to learn from the internal representational spaces of a reference model (teacher), which could be another DNN or, in the future, a biological brain. Representational spaces of the student and the teacher are characterized by representational distance matrices (RDMs). We propose representational distance learning (RDL), a stochastic gradient descent method that drives the RDMs of the student to approximate the RDMs of the teacher. We demonstrate that RDL is competitive with other transfer learning techniques for two publicly available benchmark computer vision datasets (MNIST and CIFAR-100), while allowing for architectural differences between student and teacher. By pulling the student's RDMs towards those of the teacher, RDL significantly improved visual classification performance when compared to baseline networks that did not use transfer learning. In the future, RDL may enable combined supervised training of deep neural networks using task constraints (e.g. images and category labels) and constraints from brain-activity measurements, so as to build models that replicate the internal representational spaces of biological brains.



### LLNet: A Deep Autoencoder Approach to Natural Low-light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1511.03995v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.03995v3)
- **Published**: 2015-11-12 18:31:42+00:00
- **Updated**: 2016-04-15 00:54:24+00:00
- **Authors**: Kin Gwn Lore, Adedotun Akintayo, Soumik Sarkar
- **Comment**: None
- **Journal**: None
- **Summary**: In surveillance, monitoring and tactical reconnaissance, gathering the right visual information from a dynamic environment and accurately processing such data are essential ingredients to making informed decisions which determines the success of an operation. Camera sensors are often cost-limited in ability to clearly capture objects without defects from images or videos taken in a poorly-lit environment. The goal in many applications is to enhance the brightness, contrast and reduce noise content of such images in an on-board real-time manner. We propose a deep autoencoder-based approach to identify signal features from low-light images handcrafting and adaptively brighten images without over-amplifying the lighter parts in images (i.e., without saturation of image pixels) in high dynamic range. We show that a variant of the recently proposed stacked-sparse denoising autoencoder can learn to adaptively enhance and denoise from synthetically darkened and noisy training examples. The network can then be successfully applied to naturally low-light environment and/or hardware degraded images. Results show significant credibility of deep learning based approaches both visually and by quantitative comparison with various popular enhancing, state-of-the-art denoising and hybrid enhancing-denoising techniques.



### Human Curation and Convnets: Powering Item-to-Item Recommendations on Pinterest
- **Arxiv ID**: http://arxiv.org/abs/1511.04003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04003v1)
- **Published**: 2015-11-12 18:55:20+00:00
- **Updated**: 2015-11-12 18:55:20+00:00
- **Authors**: Dmitry Kislyuk, Yuchen Liu, David Liu, Eric Tzeng, Yushi Jing
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents Pinterest Related Pins, an item-to-item recommendation system that combines collaborative filtering with content-based ranking. We demonstrate that signals derived from user curation, the activity of users organizing content, are highly effective when used in conjunction with content-based ranking. This paper also demonstrates the effectiveness of visual features, such as image or object representations learned from convnets, in improving the user engagement rate of our item-to-item recommendation system.



### Multimodal Skip-gram Using Convolutional Pseudowords
- **Arxiv ID**: http://arxiv.org/abs/1511.04024v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.04024v2)
- **Published**: 2015-11-12 19:32:08+00:00
- **Updated**: 2015-11-29 19:09:38+00:00
- **Authors**: Zachary Seymour, Yingming Li, Zhongfei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This work studies the representational mapping across multimodal data such that given a piece of the raw data in one modality the corresponding semantic description in terms of the raw data in another modality is immediately obtained. Such a representational mapping can be found in a wide spectrum of real-world applications including image/video retrieval, object recognition, action/behavior recognition, and event understanding and prediction. To that end, we introduce a simplified training objective for learning multimodal embeddings using the skip-gram architecture by introducing convolutional "pseudowords:" embeddings composed of the additive combination of distributed word representations and image features from convolutional neural networks projected into the multimodal space. We present extensive results of the representational properties of these embeddings on various word similarity benchmarks to show the promise of this approach.



### Facial Landmark Detection with Tweaked Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.04031v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04031v2)
- **Published**: 2015-11-12 19:50:53+00:00
- **Updated**: 2016-03-21 20:11:53+00:00
- **Authors**: Yue Wu, Tal Hassner, KangGeon Kim, Gerard Medioni, Prem Natarajan
- **Comment**: First two authors had joint first authorship / equal contribution
- **Journal**: None
- **Summary**: We present a novel convolutional neural network (CNN) design for facial landmark coordinate regression. We examine the intermediate features of a standard CNN trained for landmark detection and show that features extracted from later, more specialized layers capture rough landmark locations. This provides a natural means of applying differential treatment midway through the network, tweaking processing based on facial alignment. The resulting Tweaked CNN model (TCNN) harnesses the robustness of CNNs for landmark detection, in an appearance-sensitive manner without training multi-part or multi-scale models. Our results on standard face landmark detection and face verification benchmarks show TCNN to surpasses previously published performances by wide margins.



### Newtonian Image Understanding: Unfolding the Dynamics of Objects in Static Images
- **Arxiv ID**: http://arxiv.org/abs/1511.04048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04048v1)
- **Published**: 2015-11-12 20:21:11+00:00
- **Updated**: 2015-11-12 20:21:11+00:00
- **Authors**: Roozbeh Mottaghi, Hessam Bagherinezhad, Mohammad Rastegari, Ali Farhadi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the challenging problem of predicting the dynamics of objects in static images. Given a query object in an image, our goal is to provide a physical understanding of the object in terms of the forces acting upon it and its long term motion as response to those forces. Direct and explicit estimation of the forces and the motion of objects from a single image is extremely challenging. We define intermediate physical abstractions called Newtonian scenarios and introduce Newtonian Neural Network ($N^3$) that learns to map a single image to a state in a Newtonian scenario. Our experimental evaluations show that our method can reliably predict dynamics of a query object from a single image. In addition, our approach can provide physical reasoning that supports the predicted dynamics in terms of velocity and force vectors. To spur research in this direction we compiled Visual Newtonian Dynamics (VIND) dataset that includes 6806 videos aligned with Newtonian scenarios represented using game engines, and 4516 still images with their ground truth dynamics.



### Efficient non-greedy optimization of decision trees
- **Arxiv ID**: http://arxiv.org/abs/1511.04056v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.04056v1)
- **Published**: 2015-11-12 20:32:28+00:00
- **Updated**: 2015-11-12 20:32:28+00:00
- **Authors**: Mohammad Norouzi, Maxwell D. Collins, Matthew Johnson, David J. Fleet, Pushmeet Kohli
- **Comment**: in NIPS 2015
- **Journal**: None
- **Summary**: Decision trees and randomized forests are widely used in computer vision and machine learning. Standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal trees. In this paper, we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective. We show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables, and we formulate a convex-concave upper bound on the tree's empirical loss. The run-time of computing the gradient of the proposed surrogate objective with respect to each training exemplar is quadratic in the the tree depth, and thus training deep trees is feasible. The use of stochastic gradient descent for optimization enables effective training with large datasets. Experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.



### Deep Gaussian Conditional Random Field Network: A Model-based Deep Network for Discriminative Denoising
- **Arxiv ID**: http://arxiv.org/abs/1511.04067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04067v1)
- **Published**: 2015-11-12 20:49:20+00:00
- **Updated**: 2015-11-12 20:49:20+00:00
- **Authors**: Raviteja Vemulapalli, Oncel Tuzel, Ming-Yu Liu
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: We propose a novel deep network architecture for image\\ denoising based on a Gaussian Conditional Random Field (GCRF) model. In contrast to the existing discriminative denoising methods that train a separate model for each noise level, the proposed deep network explicitly models the input noise variance and hence is capable of handling a range of noise levels. Our deep network, which we refer to as deep GCRF network, consists of two sub-networks: (i) a parameter generation network that generates the pairwise potential parameters based on the noisy input image, and (ii) an inference network whose layers perform the computations involved in an iterative GCRF inference procedure.\ We train the entire deep GCRF network (both parameter generation and inference networks) discriminatively in an end-to-end fashion by maximizing the peak signal-to-noise ratio measure. Experiments on Berkeley segmentation and PASCALVOC datasets show that the proposed deep GCRF network outperforms state-of-the-art image denoising approaches for several noise levels.



### Basic Level Categorization Facilitates Visual Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1511.04103v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.04103v3)
- **Published**: 2015-11-12 21:41:35+00:00
- **Updated**: 2016-01-07 08:26:54+00:00
- **Authors**: Panqu Wang, Garrison W. Cottrell
- **Comment**: ICLR 2016 submission R1
- **Journal**: None
- **Summary**: Recent advances in deep learning have led to significant progress in the computer vision field, especially for visual object recognition tasks. The features useful for object classification are learned by feed-forward deep convolutional neural networks (CNNs) automatically, and they are shown to be able to predict and decode neural representations in the ventral visual pathway of humans and monkeys. However, despite the huge amount of work on optimizing CNNs, there has not been much research focused on linking CNNs with guiding principles from the human visual cortex. In this work, we propose a network optimization strategy inspired by both of the developmental trajectory of children's visual object recognition capabilities, and Bar (2003), who hypothesized that basic level information is carried in the fast magnocellular pathway through the prefrontal cortex (PFC) and then projected back to inferior temporal cortex (IT), where subordinate level categorization is achieved. We instantiate this idea by training a deep CNN to perform basic level object categorization first, and then train it on subordinate level categorization. We apply this idea to training AlexNet (Krizhevsky et al., 2012) on the ILSVRC 2012 dataset and show that the top-5 accuracy increases from 80.13% to 82.14%, demonstrating the effectiveness of the method. We also show that subsequent transfer learning on smaller datasets gives superior results.



### Going Deeper in Facial Expression Recognition using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1511.04110v1
- **DOI**: 10.1109/WACV.2016.7477450
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.04110v1)
- **Published**: 2015-11-12 22:10:46+00:00
- **Updated**: 2015-11-12 22:10:46+00:00
- **Authors**: Ali Mollahosseini, David Chan, Mohammad H. Mahoor
- **Comment**: To be appear in IEEE Winter Conference on Applications of Computer
  Vision (WACV), 2016 {Accepted in first round submission}
- **Journal**: IEEE Winter Conference on Applications of Computer Vision (WACV),
  2016
- **Summary**: Automated Facial Expression Recognition (FER) has remained a challenging and interesting problem. Despite efforts made in developing various methods for FER, existing approaches traditionally lack generalizability when applied to unseen images or those that are captured in wild setting. Most of the existing approaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where the classifier's hyperparameters are tuned to give best recognition accuracies across a single database, or a small collection of similar databases. Nevertheless, the results are not significant when they are applied to novel data. This paper proposes a deep neural network architecture to address the FER problem across multiple well-known standard face datasets. Specifically, our network consists of two convolutional layers each followed by max pooling and then four Inception layers. The network is a single component architecture that takes registered facial images as the input and classifies them into either of the six basic or the neutral expressions. We conducted comprehensive experiments on seven publically available facial expression databases, viz. MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposed architecture are comparable to or better than the state-of-the-art methods and better than traditional convolutional neural networks and in both accuracy and training time.



### Action Recognition using Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/1511.04119v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1511.04119v3)
- **Published**: 2015-11-12 23:06:42+00:00
- **Updated**: 2016-02-14 17:20:19+00:00
- **Authors**: Shikhar Sharma, Ryan Kiros, Ruslan Salakhutdinov
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a soft attention based model for the task of action recognition in videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally. Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses. The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed.



