# Arxiv Papers in cs.CV on 2015-11-20
### DOC: Deep OCclusion Estimation From a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1511.06457v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06457v4)
- **Published**: 2015-11-20 00:04:06+00:00
- **Updated**: 2016-07-24 07:16:54+00:00
- **Authors**: Peng Wang, Alan Yuille
- **Comment**: Accepted to ECCV 2016
- **Journal**: None
- **Summary**: Recovering the occlusion relationships between objects is a fundamental human visual ability which yields important information about the 3D world. In this paper we propose a deep network architecture, called DOC, which acts on a single image, detects object boundaries and estimates the border ownership (i.e. which side of the boundary is foreground and which is background). We represent occlusion relations by a binary edge map, to indicate the object boundary, and an occlusion orientation variable which is tangential to the boundary and whose direction specifies border ownership by a left-hand rule. We train two related deep convolutional neural networks, called DOC, which exploit local and non-local image cues to estimate this representation and hence recover occlusion relations. In order to train and test DOC we construct a large-scale instance occlusion boundary dataset using PASCAL VOC images, which we call the PASCAL instance occlusion dataset (PIOD). This contains 10,000 images and hence is two orders of magnitude larger than existing occlusion datasets for outdoor images. We test two variants of DOC on PIOD and on the BSDS occlusion dataset and show they outperform state-of-the-art methods. Finally, we perform numerous experiments investigating multiple settings of DOC and transfer between BSDS and PIOD, which provides more insights for further study of occlusion estimation.



### A Simple Hierarchical Pooling Data Structure for Loop Closure
- **Arxiv ID**: http://arxiv.org/abs/1511.06489v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1511.06489v2)
- **Published**: 2015-11-20 04:56:47+00:00
- **Updated**: 2018-10-23 22:04:42+00:00
- **Authors**: Xiaohan Fei, Konstantine Tsotsos, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a data structure obtained by hierarchically averaging bag-of-word descriptors during a sequence of views that achieves average speedups in large-scale loop closure applications ranging from 4 to 20 times on benchmark datasets. Although simple, the method works as well as sophisticated agglomerative schemes at a fraction of the cost with minimal loss of performance.



### Bidirectional Warping of Active Appearance Model
- **Arxiv ID**: http://arxiv.org/abs/1511.06494v1
- **DOI**: 10.1109/CVPRW.2013.129
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06494v1)
- **Published**: 2015-11-20 05:23:12+00:00
- **Updated**: 2015-11-20 05:23:12+00:00
- **Authors**: Ali Mollahosseini, Mohammad H. Mahoor
- **Comment**: None
- **Journal**: 2013 IEEE Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW)
- **Summary**: Active Appearance Model (AAM) is a commonly used method for facial image analysis with applications in face identification and facial expression recognition. This paper proposes a new approach based on image alignment for AAM fitting called bidirectional warping. Previous approaches warp either the input image or the appearance template. We propose to warp both the input image, using incremental update by an affine transformation, and the appearance template, using an inverse compositional approach. Our experimental results on Multi-PIE face database show that the bidirectional approach outperforms state-of-the-art inverse compositional fitting approaches in extracting landmark points of faces with shape and pose variations.



### Integrating Deep Features for Material Recognition
- **Arxiv ID**: http://arxiv.org/abs/1511.06522v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06522v6)
- **Published**: 2015-11-20 08:31:00+00:00
- **Updated**: 2016-04-21 10:19:56+00:00
- **Authors**: Yan Zhang, Mete Ozay, Xing Liu, Takayuki Okatani
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: We propose a method for integration of features extracted using deep representations of Convolutional Neural Networks (CNNs) each of which is learned using a different image dataset of objects and materials for material recognition. Given a set of representations of multiple pre-trained CNNs, we first compute activations of features using the representations on the images to select a set of samples which are best represented by the features. Then, we measure the uncertainty of the features by computing the entropy of class distributions for each sample set. Finally, we compute the contribution of each feature to representation of classes for feature selection and integration. We examine the proposed method on three benchmark datasets for material recognition. Experimental results show that the proposed method achieves state-of-the-art performance by integrating deep features. Additionally, we introduce a new material dataset called EFMD by extending Flickr Material Database (FMD). By the employment of the EFMD with transfer learning for updating the learned CNN models, we achieve 84.0%+/-1.8% accuracy on the FMD dataset which is close to human performance that is 84.9%.



### WIDER FACE: A Face Detection Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1511.06523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06523v1)
- **Published**: 2015-11-20 08:33:57+00:00
- **Updated**: 2015-11-20 08:33:57+00:00
- **Authors**: Shuo Yang, Ping Luo, Chen Change Loy, Xiaoou Tang
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset, which is 10 times larger than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to large variations in scale, pose and occlusion, as shown in Fig. 1. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further investigated. Dataset can be downloaded at: mmlab.ie.cuhk.edu.hk/projects/WIDERFace



### Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications
- **Arxiv ID**: http://arxiv.org/abs/1511.06530v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06530v2)
- **Published**: 2015-11-20 09:20:08+00:00
- **Updated**: 2016-02-24 11:52:12+00:00
- **Authors**: Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, Dongjun Shin
- **Comment**: None
- **Journal**: None
- **Summary**: Although the latest high-end smartphone has powerful CPU and GPU, running deeper convolutional neural networks (CNNs) for complex tasks such as ImageNet classification on mobile devices is challenging. To deploy deep CNNs on mobile devices, we present a simple and effective scheme to compress the entire CNN, which we call one-shot whole network compression. The proposed scheme consists of three steps: (1) rank selection with variational Bayesian matrix factorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning to recover accumulated loss of accuracy, and each step can be easily implemented using publicly available tools. We demonstrate the effectiveness of the proposed scheme by testing the performance of various compressed CNNs (AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significant reductions in model size, runtime, and energy consumption are obtained, at the cost of small loss in accuracy. In addition, we address the important implementation level issue on 1?1 convolution, which is a key operation of inception module of GoogLeNet as well as CNNs compressed by our proposed scheme.



### A dense subgraph based algorithm for compact salient image region detection
- **Arxiv ID**: http://arxiv.org/abs/1511.06545v2
- **DOI**: 10.1016/j.cviu.2015.12.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06545v2)
- **Published**: 2015-11-20 10:09:13+00:00
- **Updated**: 2015-12-19 12:31:16+00:00
- **Authors**: Souradeep Chakraborty, Pabitra Mitra
- **Comment**: 33 pages, 18 figures, Single column manuscript pre-print, Accepted at
  Computer Vision and Image Understanding, Elsevier
- **Journal**: None
- **Summary**: We present an algorithm for graph based saliency computation that utilizes the underlying dense subgraphs in finding visually salient regions in an image. To compute the salient regions, the model first obtains a saliency map using random walks on a Markov chain. Next, k-dense subgraphs are detected to further enhance the salient regions in the image. Dense subgraphs convey more information about local graph structure than simple centrality measures. To generate the Markov chain, intensity and color features of an image in addition to region compactness is used. For evaluating the proposed model, we do extensive experiments on benchmark image data sets. The proposed method performs comparable to well-known algorithms in salient region detection.



### Acceleration of the PDHGM on strongly convex subspaces
- **Arxiv ID**: http://arxiv.org/abs/1511.06566v2
- **DOI**: 10.1007/s10851-016-0692-2
- **Categories**: **math.OC**, cs.CV, 90C25, 49M29, 94A08
- **Links**: [PDF](http://arxiv.org/pdf/1511.06566v2)
- **Published**: 2015-11-20 11:59:11+00:00
- **Updated**: 2016-02-10 23:24:35+00:00
- **Authors**: Tuomo Valkonen, Thomas Pock
- **Comment**: None
- **Journal**: None
- **Summary**: We propose several variants of the primal-dual method due to Chambolle and Pock. Without requiring full strong convexity of the objective functions, our methods are accelerated on subspaces with strong convexity. This yields mixed rates, $O(1/N^2)$ with respect to initialisation and $O(1/N)$ with respect to the dual sequence, and the residual part of the primal sequence. We demonstrate the efficacy of the proposed methods on image processing problems lacking strong convexity, such as total generalised variation denoising and total variation deblurring.



### ElSe: Ellipse Selection for Robust Pupil Detection in Real-World Environments
- **Arxiv ID**: http://arxiv.org/abs/1511.06575v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.3; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1511.06575v2)
- **Published**: 2015-11-20 12:49:20+00:00
- **Updated**: 2015-11-23 06:56:10+00:00
- **Authors**: Wolfgang Fuhl, Thiago C. Santini, Thomas Kuebler, Enkelejda Kasneci
- **Comment**: None
- **Journal**: None
- **Summary**: Fast and robust pupil detection is an essential prerequisite for video-based eye-tracking in real-world settings. Several algorithms for image-based pupil detection have been proposed, their applicability is mostly limited to laboratory conditions. In realworld scenarios, automated pupil detection has to face various challenges, such as illumination changes, reflections (on glasses), make-up, non-centered eye recording, and physiological eye characteristics. We propose ElSe, a novel algorithm based on ellipse evaluation of a filtered edge image. We aim at a robust, resource-saving approach that can be integrated in embedded architectures e.g. driving. The proposed algorithm was evaluated against four state-of-the-art methods on over 93,000 hand-labeled images from which 55,000 are new images contributed by this work. On average, the proposed method achieved a 14.53% improvement on the detection rate relative to the best state-of-the-art performer. download:ftp://emmapupildata@messor.informatik.unituebingen. de (password:eyedata).



### Crowd Behavior Analysis: A Review where Physics meets Biology
- **Arxiv ID**: http://arxiv.org/abs/1511.06586v1
- **DOI**: 10.1016/j.neucom.2015.11.021
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.06586v1)
- **Published**: 2015-11-20 13:19:44+00:00
- **Updated**: 2015-11-20 13:19:44+00:00
- **Authors**: Ven Jyn Kok, Mei Kuan Lim, Chee Seng Chan
- **Comment**: Accepted in Neurocomputing, 31 pages, 180 references
- **Journal**: Neurocomputing 177 (2016) 342-362
- **Summary**: Although the traits emerged in a mass gathering are often non-deliberative, the act of mass impulse may lead to irre- vocable crowd disasters. The two-fold increase of carnage in crowd since the past two decades has spurred significant advances in the field of computer vision, towards effective and proactive crowd surveillance. Computer vision stud- ies related to crowd are observed to resonate with the understanding of the emergent behavior in physics (complex systems) and biology (animal swarm). These studies, which are inspired by biology and physics, share surprisingly common insights, and interesting contradictions. However, this aspect of discussion has not been fully explored. Therefore, this survey provides the readers with a review of the state-of-the-art methods in crowd behavior analysis from the physics and biologically inspired perspectives. We provide insights and comprehensive discussions for a broader understanding of the underlying prospect of blending physics and biology studies in computer vision.



### TEMPO: Feature-Endowed Teichmüller Extremal Mappings of Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1511.06624v2
- **DOI**: 10.1137/15M1049117
- **Categories**: **cs.CG**, cs.CV, cs.GR, math.DG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06624v2)
- **Published**: 2015-11-20 14:57:05+00:00
- **Updated**: 2016-04-26 12:37:02+00:00
- **Authors**: Ting Wei Meng, Gary Pui-Tung Choi, Lok Ming Lui
- **Comment**: None
- **Journal**: SIAM Journal on Imaging Sciences 9, 1922-1962 (2016)
- **Summary**: In recent decades, the use of 3D point clouds has been widespread in computer industry. The development of techniques in analyzing point clouds is increasingly important. In particular, mapping of point clouds has been a challenging problem. In this paper, we develop a discrete analogue of the Teichm\"{u}ller extremal mappings, which guarantee uniform conformality distortions, on point cloud surfaces. Based on the discrete analogue, we propose a novel method called TEMPO for computing Teichm\"{u}ller extremal mappings between feature-endowed point clouds. Using our proposed method, the Teichm\"{u}ller metric is introduced for evaluating the dissimilarity of point clouds. Consequently, our algorithm enables accurate recognition and classification of point clouds. Experimental results demonstrate the effectiveness of our proposed method.



### Towards Arbitrary-View Face Alignment by Recommendation Trees
- **Arxiv ID**: http://arxiv.org/abs/1511.06627v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06627v1)
- **Published**: 2015-11-20 15:01:21+00:00
- **Updated**: 2015-11-20 15:01:21+00:00
- **Authors**: Shizhan Zhu, Cheng Li, Chen Change Loy, Xiaoou Tang
- **Comment**: This is our original submission to ICCV 2015
- **Journal**: None
- **Summary**: Learning to simultaneously handle face alignment of arbitrary views, e.g. frontal and profile views, appears to be more challenging than we thought. The difficulties lay in i) accommodating the complex appearance-shape relations exhibited in different views, and ii) encompassing the varying landmark point sets due to self-occlusion and different landmark protocols. Most existing studies approach this problem via training multiple viewpoint-specific models, and conduct head pose estimation for model selection. This solution is intuitive but the performance is highly susceptible to inaccurate head pose estimation. In this study, we address this shortcoming through learning an Ensemble of Model Recommendation Trees (EMRT), which is capable of selecting optimal model configuration without prior head pose estimation. The unified framework seamlessly handles different viewpoints and landmark protocols, and it is trained by optimising directly on landmark locations, thus yielding superior results on arbitrary-view face alignment. This is the first study that performs face alignment on the full AFLWdataset with faces of different views including profile view. State-of-the-art performances are also reported on MultiPIE and AFW datasets containing both frontaland profile-view faces.



### Multi-Contrast MRI Reconstruction with Structure-Guided Total Variation
- **Arxiv ID**: http://arxiv.org/abs/1511.06631v1
- **DOI**: 10.1137/15M1047325
- **Categories**: **math.NA**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1511.06631v1)
- **Published**: 2015-11-20 15:08:17+00:00
- **Updated**: 2015-11-20 15:08:17+00:00
- **Authors**: Matthias J. Ehrhardt, Marta M. Betcke
- **Comment**: 18 pages, 16 figures
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) is a versatile imaging technique that allows different contrasts depending on the acquisition parameters. Many clinical imaging studies acquire MRI data for more than one of these contrasts---such as for instance T1 and T2 weighted images---which makes the overall scanning procedure very time consuming. As all of these images show the same underlying anatomy one can try to omit unnecessary measurements by taking the similarity into account during reconstruction. We will discuss two modifications of total variation---based on i) location and ii) direction---that take structural a priori knowledge into account and reduce to total variation in the degenerate case when no structural knowledge is available. We solve the resulting convex minimization problem with the alternating direction method of multipliers that separates the forward operator from the prior. For both priors the corresponding proximal operator can be implemented as an extension of the fast gradient projection method on the dual problem for total variation. We tested the priors on six data sets that are based on phantoms and real MRI images. In all test cases exploiting the structural information from the other contrast yields better results than separate reconstruction with total variation in terms of standard metrics like peak signal-to-noise ratio and structural similarity index. Furthermore, we found that exploiting the two dimensional directional information results in images with well defined edges, superior to those reconstructed solely using a priori information about the edge location.



### DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1511.06645v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06645v2)
- **Published**: 2015-11-20 15:37:55+00:00
- **Updated**: 2016-04-26 04:26:29+00:00
- **Authors**: Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele
- **Comment**: Accepted at IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2016)
- **Journal**: None
- **Summary**: This paper considers the task of articulated human pose estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection and pose estimation: it infers the number of persons in a scene, identifies occluded body parts, and disambiguates body parts between people in close proximity of each other. This joint formulation is in contrast to previous strategies, that address the problem by first detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors. Our formulation, an instance of an integer linear program, implicitly performs non-maximum suppression on the set of part candidates and groups them to form configurations of body parts respecting geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation. Models and code available at http://pose.mpi-inf.mpg.de.



### Recurrent Semi-supervised Classification and Constrained Adversarial Generation with Motion Capture Data
- **Arxiv ID**: http://arxiv.org/abs/1511.06653v8
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06653v8)
- **Published**: 2015-11-20 15:47:55+00:00
- **Updated**: 2018-07-11 12:25:55+00:00
- **Authors**: Félix G. Harvey, Julien Roy, David Kanaa, Christopher Pal
- **Comment**: IVC Journal Submission
- **Journal**: None
- **Summary**: We explore recurrent encoder multi-decoder neural network architectures for semi-supervised sequence classification and reconstruction. We find that the use of multiple reconstruction modules helps models generalize in a classification task when only a small amount of labeled data is available, which is often the case in practice. Such models provide useful high-level representations of motions allowing clustering, searching and faster labeling of new sequences. We also propose a new, realistic partitioning of a well-known, high quality motion-capture dataset for better evaluations. We further explore a novel formulation for future-predicting decoders based on conditional recurrent generative adversarial networks, for which we propose both soft and hard constraints for transition generation derived from desired physical properties of synthesized future movements and desired animation goals. We find that using such constraints allow to stabilize the training of recurrent adversarial architectures for animation generation.



### Tracklet Association by Online Target-Specific Metric Learning and Coherent Dynamics Estimation
- **Arxiv ID**: http://arxiv.org/abs/1511.06654v2
- **DOI**: 10.1109/TPAMI.2016.2551245
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06654v2)
- **Published**: 2015-11-20 15:48:21+00:00
- **Updated**: 2016-04-22 03:53:35+00:00
- **Authors**: Bing Wang, Gang Wang, Kap Luk Chan, Li Wang
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence, in
  press, 2016
- **Journal**: None
- **Summary**: In this paper, we present a novel method based on online target-specific metric learning and coherent dynamics estimation for tracklet (track fragment) association by network flow optimization in long-term multi-person tracking. Our proposed framework aims to exploit appearance and motion cues to prevent identity switches during tracking and to recover missed detections. Furthermore, target-specific metrics (appearance cue) and motion dynamics (motion cue) are proposed to be learned and estimated online, i.e. during the tracking process. Our approach is effective even when such cues fail to identify or follow the target due to occlusions or object-to-object interactions. We also propose to learn the weights of these two tracking cues to handle the difficult situations, such as severe occlusions and object-to-object interactions effectively. Our method has been validated on several public datasets and the experimental results show that it outperforms several state-of-the-art tracking methods.



### Stories in the Eye: Contextual Visual Interactions for Efficient Video to Language Translation
- **Arxiv ID**: http://arxiv.org/abs/1511.06674v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1511.06674v1)
- **Published**: 2015-11-20 16:33:13+00:00
- **Updated**: 2015-11-20 16:33:13+00:00
- **Authors**: Anirudh Goyal, Marius Leordeanu
- **Comment**: None
- **Journal**: None
- **Summary**: Integrating higher level visual and linguistic interpretations is at the heart of human intelligence. As automatic visual category recognition in images is approaching human performance, the high level understanding in the dynamic spatiotemporal domain of videos and its translation into natural language is still far from being solved. While most works on vision-to-text translations use pre-learned or pre-established computational linguistic models, in this paper we present an approach that uses vision alone to efficiently learn how to translate into language the video content. We discover, in simple form, the story played by main actors, while using only visual cues for representing objects and their interactions. Our method learns in a hierarchical manner higher level representations for recognizing subjects, actions and objects involved, their relevant contextual background and their interaction to one another over time. We have a three stage approach: first we take in consideration features of the individual entities at the local level of appearance, then we consider the relationship between these objects and actions and their video background, and third, we consider their spatiotemporal relations as inputs to classifiers at the highest level of interpretation. Thus, our approach finds a coherent linguistic description of videos in the form of a subject, verb and object based on their role played in the overall visual story learned directly from training data, without using a known language model. We test the efficiency of our approach on a large scale dataset containing YouTube clips taken in the wild and demonstrate state-of-the-art performance, often superior to current approaches that use more complex, pre-learned linguistic knowledge.



### Personalizing Human Video Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1511.06676v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06676v2)
- **Published**: 2015-11-20 16:34:42+00:00
- **Updated**: 2016-06-15 11:05:05+00:00
- **Authors**: James Charles, Tomas Pfister, Derek Magee, David Hogg, Andrew Zisserman
- **Comment**: CVPR 2016
- **Journal**: None
- **Summary**: We propose a personalized ConvNet pose estimator that automatically adapts itself to the uniqueness of a person's appearance to improve pose estimation in long videos. We make the following contributions: (i) we show that given a few high-precision pose annotations, e.g. from a generic ConvNet pose estimator, additional annotations can be generated throughout the video using a combination of image-based matching for temporally distant frames, and dense optical flow for temporally local frames; (ii) we develop an occlusion aware self-evaluation model that is able to automatically select the high-quality and reject the erroneous additional annotations; and (iii) we demonstrate that these high-quality annotations can be used to fine-tune a ConvNet pose estimator and thereby personalize it to lock on to key discriminative features of the person's appearance. The outcome is a substantial improvement in the pose estimates for the target video using the personalized ConvNet compared to the original generic ConvNet. Our method outperforms the state of the art (including top ConvNet methods) by a large margin on two standard benchmarks, as well as on a new challenging YouTube video dataset. Furthermore, we show that training from the automatically generated annotations can be used to improve the performance of a generic ConvNet on other benchmarks.



### Deep End2End Voxel2Voxel Prediction
- **Arxiv ID**: http://arxiv.org/abs/1511.06681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06681v1)
- **Published**: 2015-11-20 16:42:37+00:00
- **Updated**: 2015-11-20 16:42:37+00:00
- **Authors**: Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last few years deep learning methods have emerged as one of the most prominent approaches for video analysis. However, so far their most successful applications have been in the area of video classification and detection, i.e., problems involving the prediction of a single class label or a handful of output variables per video. Furthermore, while deep networks are commonly recognized as the best models to use in these domains, there is a widespread perception that in order to yield successful results they often require time-consuming architecture search, manual tweaking of parameters and computationally intensive pre-processing or post-processing methods.   In this paper we challenge these views by presenting a deep 3D convolutional architecture trained end to end to perform voxel-level prediction, i.e., to output a variable at every voxel of the video. Most importantly, we show that the same exact architecture can be used to achieve competitive results on three widely different voxel-prediction tasks: video semantic segmentation, optical flow estimation, and video coloring. The three networks learned on these problems are trained from raw video without any form of preprocessing and their outputs do not require post-processing to achieve outstanding performance. Thus, they offer an efficient alternative to traditional and much more computationally expensive methods in these video domains.



### Top-k Multiclass SVM
- **Arxiv ID**: http://arxiv.org/abs/1511.06683v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06683v1)
- **Published**: 2015-11-20 16:49:33+00:00
- **Updated**: 2015-11-20 16:49:33+00:00
- **Authors**: Maksim Lapin, Matthias Hein, Bernt Schiele
- **Comment**: NIPS 2015
- **Journal**: None
- **Summary**: Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.



### Direct Prediction of 3D Body Poses from Motion Compensated Sequences
- **Arxiv ID**: http://arxiv.org/abs/1511.06692v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06692v4)
- **Published**: 2015-11-20 17:08:18+00:00
- **Updated**: 2016-09-02 09:38:08+00:00
- **Authors**: Bugra Tekin, Artem Rozantsev, Vincent Lepetit, Pascal Fua
- **Comment**: Published in CVPR 2016. supersedes arXiv:1504.08200
- **Journal**: None
- **Summary**: We propose an efficient approach to exploiting motion information from consecutive frames of a video sequence to recover the 3D pose of people. Previous approaches typically compute candidate poses in individual frames and then link them in a post-processing step to resolve ambiguities. By contrast, we directly regress from a spatio-temporal volume of bounding boxes to a 3D pose in the central frame.   We further show that, for this approach to achieve its full potential, it is essential to compensate for the motion in consecutive frames so that the subject remains centered. This then allows us to effectively overcome ambiguities and improve upon the state-of-the-art by a large margin on the Human3.6m, HumanEva, and KTH Multiview Football 3D human pose estimation benchmarks.



### Multi-view 3D Models from Single Images with a Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1511.06702v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06702v2)
- **Published**: 2015-11-20 17:34:17+00:00
- **Updated**: 2016-08-02 10:14:07+00:00
- **Authors**: Maxim Tatarchenko, Alexey Dosovitskiy, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: We present a convolutional network capable of inferring a 3D representation of a previously unseen object given a single image of this object. Concretely, the network can predict an RGB image and a depth map of the object as seen from an arbitrary view. Several of these depth maps fused together give a full point cloud of the object. The point cloud can in turn be transformed into a surface mesh. The network is trained on renderings of synthetic 3D models of cars and chairs. It successfully deals with objects on cluttered background and generates reasonable predictions for real images of cars.



### Semantic Diversity versus Visual Diversity in Visual Dictionaries
- **Arxiv ID**: http://arxiv.org/abs/1511.06704v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06704v1)
- **Published**: 2015-11-20 17:38:15+00:00
- **Updated**: 2015-11-20 17:38:15+00:00
- **Authors**: Otávio A. B. Penatti, Sandra Avila, Eduardo Valle, Ricardo da S. Torres
- **Comment**: None
- **Journal**: None
- **Summary**: Visual dictionaries are a critical component for image classification/retrieval systems based on the bag-of-visual-words (BoVW) model. Dictionaries are usually learned without supervision from a training set of images sampled from the collection of interest. However, for large, general-purpose, dynamic image collections (e.g., the Web), obtaining a representative sample in terms of semantic concepts is not straightforward. In this paper, we evaluate the impact of semantics in the dictionary quality, aiming at verifying the importance of semantic diversity in relation visual diversity for visual dictionaries. In the experiments, we vary the amount of classes used for creating the dictionary and then compute different BoVW descriptors, using multiple codebook sizes and different coding and pooling methods (standard BoVW and Fisher Vectors). Results for image classification show that as visual dictionaries are based on low-level visual appearances, visual diversity is more important than semantic diversity. Our conclusions open the opportunity to alleviate the burden in generating visual dictionaries as we need only a visually diverse set of images instead of the whole collection to create a good dictionary.



### Hand Pose Estimation through Semi-Supervised and Weakly-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1511.06728v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06728v4)
- **Published**: 2015-11-20 19:19:00+00:00
- **Updated**: 2017-09-15 09:24:57+00:00
- **Authors**: Natalia Neverova, Christian Wolf, Florian Nebout, Graham Taylor
- **Comment**: 13 pages, 10 figures, 4 tables
- **Journal**: None
- **Summary**: We propose a method for hand pose estimation based on a deep regressor trained on two different kinds of input. Raw depth data is fused with an intermediate representation in the form of a segmentation of the hand into parts. This intermediate representation contains important topological information and provides useful cues for reasoning about joint locations. The mapping from raw depth to segmentation maps is learned in a semi/weakly-supervised way from two different datasets: (i) a synthetic dataset created through a rendering pipeline including densely labeled ground truth (pixelwise segmentations); and (ii) a dataset with real images for which ground truth joint positions are available, but not dense segmentations. Loss for training on real images is generated from a patch-wise restoration process, which aligns tentative segmentation maps with a large dictionary of synthetic poses. The underlying premise is that the domain shift between synthetic and real data is smaller in the intermediate representation, where labels carry geometric and topological meaning, than in the raw input domain. Experiments on the NYU dataset show that the proposed training method decreases error on joints over direct regression of joints from depth data by 15.7%.



### Superpixel Convolutional Networks using Bilateral Inceptions
- **Arxiv ID**: http://arxiv.org/abs/1511.06739v5
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1511.06739v5)
- **Published**: 2015-11-20 19:58:38+00:00
- **Updated**: 2016-08-08 15:31:14+00:00
- **Authors**: Raghudeep Gadde, Varun Jampani, Martin Kiefel, Daniel Kappler, Peter V. Gehler
- **Comment**: European Conference on Computer Vision (ECCV), 2016
- **Journal**: None
- **Summary**: In this paper we propose a CNN architecture for semantic image segmentation. We introduce a new 'bilateral inception' module that can be inserted in existing CNN architectures and performs bilateral filtering, at multiple feature-scales, between superpixels in an image. The feature spaces for bilateral filtering and other parameters of the module are learned end-to-end using standard backpropagation techniques. The bilateral inception module addresses two issues that arise with general CNN segmentation architectures. First, this module propagates information between (super) pixels while respecting image edges, thus using the structured information of the problem for improved results. Second, the layer recovers a full resolution segmentation result from the lower resolution solution of a CNN. In the experiments, we modify several existing CNN architectures by inserting our inception module between the last CNN (1x1 convolution) layers. Empirical results on three different datasets show reliable improvements not only in comparison to the baseline networks, but also in comparison to several dense-pixel prediction techniques such as CRFs, while being competitive in time.



### Training CNNs with Low-Rank Filters for Efficient Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1511.06744v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1511.06744v3)
- **Published**: 2015-11-20 20:14:28+00:00
- **Updated**: 2016-02-07 21:23:19+00:00
- **Authors**: Yani Ioannou, Duncan Robertson, Jamie Shotton, Roberto Cipolla, Antonio Criminisi
- **Comment**: Published as a conference paper at ICLR 2016. v3: updated ICLR
  status. v2: Incorporated reviewer's feedback including: Amend Fig. 2 and 5
  descriptions to explain that there are no ReLUs within the figures. Fix
  headings of Table 5 - Fix typo in the sentence at bottom of page 6. Add ref.
  to Predicting Parameters in Deep Learning. Fix Table 6, GMP-LR and GMP-LR-2x
  had incorrect numbers of filters
- **Journal**: International Conference on Learning Representations (ICLR), San
  Juan, Puerto Rico, 2-4 May 2016
- **Summary**: We propose a new method for creating computationally efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our method to a near state-of-the-art network for CIFAR, we achieved comparable accuracy with 46% less compute and 55% fewer parameters.



### Images Don't Lie: Transferring Deep Visual Semantic Features to Large-Scale Multimodal Learning to Rank
- **Arxiv ID**: http://arxiv.org/abs/1511.06746v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1511.06746v1)
- **Published**: 2015-11-20 20:26:26+00:00
- **Updated**: 2015-11-20 20:26:26+00:00
- **Authors**: Corey Lynch, Kamelia Aryafar, Josh Attenberg
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Search is at the heart of modern e-commerce. As a result, the task of ranking search results automatically (learning to rank) is a multibillion dollar machine learning problem. Traditional models optimize over a few hand-constructed features based on the item's text. In this paper, we introduce a multimodal learning to rank model that combines these traditional features with visual semantic features transferred from a deep convolutional neural network. In a large scale experiment using data from the online marketplace Etsy, we verify that moving to a multimodal representation significantly improves ranking quality. We show how image features can capture fine-grained style information not available in a text-only representation. In addition, we show concrete examples of how image information can successfully disentangle pairs of highly different items that are ranked similarly by a text-only model.



### Recognizing Activities of Daily Living with a Wrist-mounted Camera
- **Arxiv ID**: http://arxiv.org/abs/1511.06783v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06783v2)
- **Published**: 2015-11-20 22:02:09+00:00
- **Updated**: 2016-04-28 04:39:03+00:00
- **Authors**: Katsunori Ohnishi, Atsushi Kanehira, Asako Kanezaki, Tatsuya Harada
- **Comment**: CVPR2016 spotlight presentation
- **Journal**: None
- **Summary**: We present a novel dataset and a novel algorithm for recognizing activities of daily living (ADL) from a first-person wearable camera. Handled objects are crucially important for egocentric ADL recognition. For specific examination of objects related to users' actions separately from other objects in an environment, many previous works have addressed the detection of handled objects in images captured from head-mounted and chest-mounted cameras. Nevertheless, detecting handled objects is not always easy because they tend to appear small in images. They can be occluded by a user's body. As described herein, we mount a camera on a user's wrist. A wrist-mounted camera can capture handled objects at a large scale, and thus it enables us to skip object detection process. To compare a wrist-mounted camera and a head-mounted camera, we also develop a novel and publicly available dataset that includes videos and annotations of daily activities captured simultaneously by both cameras. Additionally, we propose a discriminative video representation that retains spatial and temporal information after encoding frame descriptors extracted by Convolutional Neural Networks (CNN).



### The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition
- **Arxiv ID**: http://arxiv.org/abs/1511.06789v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1511.06789v3)
- **Published**: 2015-11-20 22:40:30+00:00
- **Updated**: 2016-10-18 18:35:31+00:00
- **Authors**: Jonathan Krause, Benjamin Sapp, Andrew Howard, Howard Zhou, Alexander Toshev, Tom Duerig, James Philbin, Li Fei-Fei
- **Comment**: ECCV 2016, data is released
- **Journal**: None
- **Summary**: Current approaches for fine-grained recognition do the following: First, recruit experts to annotate a dataset of images, optionally also collecting more structured data in the form of part annotations and bounding boxes. Second, train a model utilizing this data. Toward the goal of solving fine-grained recognition, we introduce an alternative approach, leveraging free, noisy data from the web and simple, generic methods of recognition. This approach has benefits in both performance and scalability. We demonstrate its efficacy on four fine-grained datasets, greatly exceeding existing state of the art without the manual collection of even a single label, and furthermore show first results at scaling to more than 10,000 fine-grained categories. Quantitatively, we achieve top-1 accuracies of 92.3% on CUB-200-2011, 85.4% on Birdsnap, 93.4% on FGVC-Aircraft, and 80.8% on Stanford Dogs without using their annotated training sets. We compare our approach to an active learning approach for expanding fine-grained datasets.



