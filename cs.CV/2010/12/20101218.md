# Arxiv Papers in cs.CV on 2010-12-18
### lp-Recovery of the Most Significant Subspace among Multiple Subspaces with Outliers
- **Arxiv ID**: http://arxiv.org/abs/1012.4116v4
- **DOI**: 10.1007/s00365-014-9242-6
- **Categories**: **stat.ML**, cs.CV, math.FA
- **Links**: [PDF](http://arxiv.org/pdf/1012.4116v4)
- **Published**: 2010-12-18 20:11:29+00:00
- **Updated**: 2014-01-13 14:05:36+00:00
- **Authors**: Gilad Lerman, Teng Zhang
- **Comment**: This is a revised version of the part of 1002.1994 that deals with
  single subspace recovery. V3: Improved estimates (in particular for Lemma 3.1
  and for estimates relying on it), asymptotic dependence of probabilities and
  constants on D and d and further clarifications; for simplicity it assumes
  uniform distributions on spheres. V4: minor revision for the published
  version
- **Journal**: Constructive Approximation, December 2014, Volume 40, Issue 3, pp
  329-385
- **Summary**: We assume data sampled from a mixture of d-dimensional linear subspaces with spherically symmetric distributions within each subspace and an additional outlier component with spherically symmetric distribution within the ambient space (for simplicity we may assume that all distributions are uniform on their corresponding unit spheres). We also assume mixture weights for the different components. We say that one of the underlying subspaces of the model is most significant if its mixture weight is higher than the sum of the mixture weights of all other subspaces. We study the recovery of the most significant subspace by minimizing the lp-averaged distances of data points from d-dimensional subspaces, where p>0. Unlike other lp minimization problems, this minimization is non-convex for all p>0 and thus requires different methods for its analysis. We show that if 0<p<=1, then for any fraction of outliers the most significant subspace can be recovered by lp minimization with overwhelming probability (which depends on the generating distribution and its parameters). We show that when adding small noise around the underlying subspaces the most significant subspace can be nearly recovered by lp minimization for any 0<p<=1 with an error proportional to the noise level. On the other hand, if p>1 and there is more than one underlying subspace, then with overwhelming probability the most significant subspace cannot be recovered or nearly recovered. This last result does not require spherically symmetric outliers.



### Self-Organising Stochastic Encoders
- **Arxiv ID**: http://arxiv.org/abs/1012.4126v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, I.2.6; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1012.4126v1)
- **Published**: 2010-12-18 22:34:21+00:00
- **Updated**: 2010-12-18 22:34:21+00:00
- **Authors**: Stephen Luttrell
- **Comment**: 23 pages, 23 figures
- **Journal**: None
- **Summary**: The processing of mega-dimensional data, such as images, scales linearly with image size only if fixed size processing windows are used. It would be very useful to be able to automate the process of sizing and interconnecting the processing windows. A stochastic encoder that is an extension of the standard Linde-Buzo-Gray vector quantiser, called a stochastic vector quantiser (SVQ), includes this required behaviour amongst its emergent properties, because it automatically splits the input space into statistically independent subspaces, which it then separately encodes. Various optimal SVQs have been obtained, both analytically and numerically. Analytic solutions which demonstrate how the input space is split into independent subspaces may be obtained when an SVQ is used to encode data that lives on a 2-torus (e.g. the superposition of a pair of uncorrelated sinusoids). Many numerical solutions have also been obtained, using both SVQs and chains of linked SVQs: (1) images of multiple independent targets (encoders for single targets emerge), (2) images of multiple correlated targets (various types of encoder for single and multiple targets emerge), (3) superpositions of various waveforms (encoders for the separate waveforms emerge - this is a type of independent component analysis (ICA)), (4) maternal and foetal ECGs (another example of ICA), (5) images of textures (orientation maps and dominance stripes emerge). Overall, SVQs exhibit a rich variety of self-organising behaviour, which effectively discovers the internal structure of the training data. This should have an immediate impact on "intelligent" computation, because it reduces the need for expert human intervention in the design of data processing algorithms.



