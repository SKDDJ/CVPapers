# Arxiv Papers in cs.CV on 2010-01-07
### An Unsupervised Algorithm For Learning Lie Group Transformations
- **Arxiv ID**: http://arxiv.org/abs/1001.1027v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1001.1027v5)
- **Published**: 2010-01-07 06:22:56+00:00
- **Updated**: 2017-06-07 17:05:16+00:00
- **Authors**: Jascha Sohl-Dickstein, Ching Ming Wang, Bruno A. Olshausen
- **Comment**: None
- **Journal**: None
- **Summary**: We present several theoretical contributions which allow Lie groups to be fit to high dimensional datasets. Transformation operators are represented in their eigen-basis, reducing the computational complexity of parameter estimation to that of training a linear transformation model. A transformation specific "blurring" operator is introduced that allows inference to escape local minima via a smoothing of the transformation space. A penalty on traversed manifold distance is added which encourages the discovery of sparse, minimal distance, transformations between states. Both learning and inference are demonstrated using these methods for the full set of affine transformations on natural image patches. Transformation operators are then trained on natural video sequences. It is shown that the learned video transformations provide a better description of inter-frame differences than the standard motion model based on rigid translation.



### An Empirical Evaluation of Four Algorithms for Multi-Class Classification: Mart, ABC-Mart, Robust LogitBoost, and ABC-LogitBoost
- **Arxiv ID**: http://arxiv.org/abs/1001.1020v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1001.1020v1)
- **Published**: 2010-01-07 06:34:21+00:00
- **Updated**: 2010-01-07 06:34:21+00:00
- **Authors**: Ping Li
- **Comment**: None
- **Journal**: None
- **Summary**: This empirical study is mainly devoted to comparing four tree-based boosting algorithms: mart, abc-mart, robust logitboost, and abc-logitboost, for multi-class classification on a variety of publicly available datasets. Some of those datasets have been thoroughly tested in prior studies using a broad range of classification algorithms including SVM, neural nets, and deep learning.   In terms of the empirical classification errors, our experiment results demonstrate:   1. Abc-mart considerably improves mart. 2. Abc-logitboost considerably improves (robust) logitboost. 3. Robust) logitboost} considerably improves mart on most datasets. 4. Abc-logitboost considerably improves abc-mart on most datasets. 5. These four boosting algorithms (especially abc-logitboost) outperform SVM on many datasets. 6. Compared to the best deep learning methods, these four boosting algorithms (especially abc-logitboost) are competitive.



