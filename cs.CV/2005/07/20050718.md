# Arxiv Papers in cs.CV on 2005-07-18
### Distributed Regression in Sensor Networks: Training Distributively with Alternating Projections
- **Arxiv ID**: http://arxiv.org/abs/cs/0507039v1
- **DOI**: 10.1117/12.620194
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.DC, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/cs/0507039v1)
- **Published**: 2005-07-18 00:45:12+00:00
- **Updated**: 2005-07-18 00:45:12+00:00
- **Authors**: Joel B. Predd, Sanjeev R. Kulkarni, H. Vincent Poor
- **Comment**: To appear in the Proceedings of the SPIE Conference on Advanced
  Signal Processing Algorithms, Architectures and Implementations XV, San
  Diego, CA, July 31 - August 4, 2005
- **Journal**: None
- **Summary**: Wireless sensor networks (WSNs) have attracted considerable attention in recent years and motivate a host of new challenges for distributed signal processing. The problem of distributed or decentralized estimation has often been considered in the context of parametric models. However, the success of parametric methods is limited by the appropriateness of the strong statistical assumptions made by the models. In this paper, a more flexible nonparametric model for distributed regression is considered that is applicable in a variety of WSN applications including field estimation. Here, starting with the standard regularized kernel least-squares estimator, a message-passing algorithm for distributed estimation in WSNs is derived. The algorithm can be viewed as an instantiation of the successive orthogonal projection (SOP) algorithm. Various practical aspects of the algorithm are discussed and several numerical simulations validate the potential of the approach.



### Pattern Recognition for Conditionally Independent Data
- **Arxiv ID**: http://arxiv.org/abs/cs/0507040v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/cs/0507040v1)
- **Published**: 2005-07-18 08:10:10+00:00
- **Updated**: 2005-07-18 08:10:10+00:00
- **Authors**: Daniil Ryabko
- **Comment**: parts of results published at ALT'04 and ICML'04
- **Journal**: Journal of Machine Learning Research 7(Apr):645-664, 2006
- **Summary**: In this work we consider the task of relaxing the i.i.d assumption in pattern recognition (or classification), aiming to make existing learning algorithms applicable to a wider range of tasks. Pattern recognition is guessing a discrete label of some object based on a set of given examples (pairs of objects and labels). We consider the case of deterministically defined labels. Traditionally, this task is studied under the assumption that examples are independent and identically distributed. However, it turns out that many results of pattern recognition theory carry over a weaker assumption. Namely, under the assumption of conditional independence and identical distribution of objects, while the only assumption on the distribution of labels is that the rate of occurrence of each label should be above some positive threshold.   We find a broad class of learning algorithms for which estimations of the probability of a classification error achieved under the classical i.i.d. assumption can be generalised to the similar estimates for the case of conditionally i.i.d. examples.



