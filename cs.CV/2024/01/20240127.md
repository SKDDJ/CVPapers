# Arxiv Papers in cs.CV on 2024-01-27
### Vanishing-Point-Guided Video Semantic Segmentation of Driving Scenes
- **Arxiv ID**: http://arxiv.org/abs/2401.15261v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.15261v1)
- **Published**: 2024-01-27 01:01:58+00:00
- **Updated**: 2024-01-27 01:01:58+00:00
- **Authors**: Diandian Guo, Deng-Ping Fan, Tongyu Lu, Christos Sakaridis, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: The estimation of implicit cross-frame correspondences and the high computational cost have long been major challenges in video semantic segmentation (VSS) for driving scenes. Prior works utilize keyframes, feature propagation, or cross-frame attention to address these issues. By contrast, we are the first to harness vanishing point (VP) priors for more effective segmentation. Intuitively, objects near VPs (i.e., away from the vehicle) are less discernible. Moreover, they tend to move radially away from the VP over time in the usual case of a forward-facing camera, a straight road, and linear forward motion of the vehicle. Our novel, efficient network for VSS, named VPSeg, incorporates two modules that utilize exactly this pair of static and dynamic VP priors: sparse-to-dense feature mining (DenseVP) and VP-guided motion fusion (MotionVP). MotionVP employs VP-guided motion estimation to establish explicit correspondences across frames and help attend to the most relevant features from neighboring frames, while DenseVP enhances weak dynamic features in distant regions around VPs. These modules operate within a context-detail framework, which separates contextual features from high-resolution local features at different input resolutions to reduce computational costs. Contextual and local features are integrated through contextualized motion attention (CMA) for the final prediction. Extensive experiments on two popular driving segmentation benchmarks, Cityscapes and ACDC, demonstrate that VPSeg outperforms previous SOTA methods, with only modest computational overhead.



### SAM-based instance segmentation models for the automation of structural damage detection
- **Arxiv ID**: http://arxiv.org/abs/2401.15266v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.15266v2)
- **Published**: 2024-01-27 02:00:07+00:00
- **Updated**: 2024-01-30 14:11:07+00:00
- **Authors**: Zehao Ye, Lucy Lovell, Asaad Faramarzi, Jelena Ninic
- **Comment**: None
- **Journal**: None
- **Summary**: Automating visual inspection for capturing defects based on civil structures appearance is crucial due to its currently labour-intensive and time-consuming nature. An important aspect of automated inspection is image acquisition, which is rapid and cost-effective considering the pervasive developments in both software and hardware computing in recent years. Previous studies largely focused on concrete and asphalt, with less attention to masonry cracks. The latter also lacks publicly available datasets. In this paper, we first present a corresponding data set for instance segmentation with 1,300 annotated images (640 pixels x 640 pixels), named as MCrack1300, covering bricks, broken bricks, and cracks. We then test several leading algorithms for benchmarking, including the latest large-scale model, the prompt-based Segment Anything Model (SAM). We fine-tune the encoder using Low-Rank Adaptation (LoRA) and proposed two novel methods for automation of SAM execution. The first method involves abandoning the prompt encoder and connecting the SAM encoder to other decoders, while the second method introduces a learnable self-generating prompter. In order to ensure the seamless integration of the two proposed methods with SAM encoder section, we redesign the feature extractor. Both proposed methods exceed state-of-the-art performance, surpassing the best benchmark by approximately 3% for all classes and around 6% for cracks specifically. Based on successful detection, we propose a method based on a monocular camera and the Hough Line Transform to automatically transform images into orthographic projection maps. By incorporating known real sizes of brick units, we accurately estimate crack dimensions, with the results differing by less than 10% from those obtained by laser scanning. Overall, we address important research gaps in automated masonry crack detection and size estimation.



### Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks
- **Arxiv ID**: http://arxiv.org/abs/2401.15275v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.15275v1)
- **Published**: 2024-01-27 03:03:30+00:00
- **Updated**: 2024-01-27 03:03:30+00:00
- **Authors**: Yuliang Cai, Mohammad Rostami
- **Comment**: None
- **Journal**: None
- **Summary**: Transformer neural networks are increasingly replacing prior architectures in a wide range of applications in different data modalities. The increasing size and computational demands of fine-tuning large pre-trained transformer neural networks pose significant challenges for the widespread adoption of these models for applications that demand on-edge computing. To tackle this challenge, continual learning (CL) emerges as a solution by facilitating the transfer of knowledge across tasks that arrive sequentially for an autonomously learning agent. However, current CL methods mainly focus on learning tasks that are exclusively vision-based or language-based. We propose a transformer-based CL framework focusing on learning tasks that involve both vision and language, known as Vision-and-Language (VaL) tasks. Due to the success of transformers in other modalities, our architecture has the potential to be used in multimodal learning settings. In our framework, we benefit from introducing extra parameters to a base transformer to specialize the network for each task. As a result, we enable dynamic model expansion to learn several tasks in a sequence. We also use knowledge distillation to benefit from relevant past experiences to learn the current task more efficiently. Our proposed method, Task Attentive Multimodal Continual Learning (TAM-CL), allows for the exchange of information between tasks while mitigating the problem of catastrophic forgetting. Notably, our approach is scalable, incurring minimal memory and time overhead. TAM-CL achieves state-of-the-art (SOTA) performance on challenging multimodal tasks



### GEM: Boost Simple Network for Glass Surface Segmentation via Segment Anything Model and Data Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2401.15282v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.15282v1)
- **Published**: 2024-01-27 03:36:47+00:00
- **Updated**: 2024-01-27 03:36:47+00:00
- **Authors**: Jing Hao, Moyun Liu, Kuo Feng Hung
- **Comment**: 14 pages, 9 figures, 7 tables
- **Journal**: None
- **Summary**: Detecting glass regions is a challenging task due to the ambiguity of their transparency and reflection properties. These transparent glasses share the visual appearance of both transmitted arbitrary background scenes and reflected objects, thus having no fixed patterns.Recent visual foundation models, which are trained on vast amounts of data, have manifested stunning performance in terms of image perception and image generation. To segment glass surfaces with higher accuracy, we make full use of two visual foundation models: Segment Anything (SAM) and Stable Diffusion.Specifically, we devise a simple glass surface segmentor named GEM, which only consists of a SAM backbone, a simple feature pyramid, a discerning query selection module, and a mask decoder. The discerning query selection can adaptively identify glass surface features, assigning them as initialized queries in the mask decoder. We also propose a Synthetic but photorealistic large-scale Glass Surface Detection dataset dubbed S-GSD via diffusion model with four different scales, which contain 1x, 5x, 10x, and 20x of the original real data size. This dataset is a feasible source for transfer learning. The scale of synthetic data has positive impacts on transfer learning, while the improvement will gradually saturate as the amount of data increases. Extensive experiments demonstrate that GEM achieves a new state-of-the-art on the GSD-S validation set (IoU +2.1%). Codes and datasets are available at: https://github.com/isbrycee/GEM-Glass-Segmentor.



### Applications of Tao General Difference in Discrete Domain
- **Arxiv ID**: http://arxiv.org/abs/2401.15287v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DM, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2401.15287v1)
- **Published**: 2024-01-27 03:57:32+00:00
- **Updated**: 2024-01-27 03:57:32+00:00
- **Authors**: Linmi Tao, Ruiyang Liu, Donglai Tao, Wu Xia, Feilong Ma, Yu Cheng, Jingmao Cui
- **Comment**: This paper is the application part of the paper "Tao General
  Differential and Difference: Theory and Application". The theory part of the
  paper is renamed as "A Theory of General Difference in Continuous and
  Discrete Domain", which is Arxived in arXiv:2305.08098v2
- **Journal**: None
- **Summary**: Numerical difference computation is one of the cores and indispensable in the modern digital era. Tao general difference (TGD) is a novel theory and approach to difference computation for discrete sequences and arrays in multidimensional space. Built on the solid theoretical foundation of the general difference in a finite interval, the TGD operators demonstrate exceptional signal processing capabilities in real-world applications. A novel smoothness property of a sequence is defined on the first- and second TGD. This property is used to denoise one-dimensional signals, where the noise is the non-smooth points in the sequence. Meanwhile, the center of the gradient in a finite interval can be accurately location via TGD calculation. This solves a traditional challenge in computer vision, which is the precise localization of image edges with noise robustness. Furthermore, the power of TGD operators extends to spatio-temporal edge detection in three-dimensional arrays, enabling the identification of kinetic edges in video data. These diverse applications highlight the properties of TGD in discrete domain and the significant promise of TGD for the computation across signal processing, image analysis, and video analytic.



### STAC: Leveraging Spatio-Temporal Data Associations For Efficient Cross-Camera Streaming and Analytics
- **Arxiv ID**: http://arxiv.org/abs/2401.15288v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.NI, I.4.2; I.4.0; C.2.2; C.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2401.15288v1)
- **Published**: 2024-01-27 04:02:52+00:00
- **Updated**: 2024-01-27 04:02:52+00:00
- **Authors**: Volodymyr Vakhniuk, Ayush Sarkar, Ragini Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an efficient cross-cameras surveillance system called,STAC, that leverages spatio-temporal associations between multiple cameras to provide real-time analytics and inference under constrained network environments. STAC is built using the proposed omni-scale feature learning people reidentification (reid) algorithm that allows accurate detection, tracking and re-identification of people across cameras using the spatio-temporal characteristics of video frames. We integrate STAC with frame filtering and state-of-the-art compression for streaming technique (that is, ffmpeg libx264 codec) to remove redundant information from cross-camera frames. This helps in optimizing the cost of video transmission as well as compute/processing, while maintaining high accuracy for real-time query inference. The introduction of AICity Challenge 2023 Data [1] by NVIDIA has allowed exploration of systems utilizing multi-camera people tracking algorithms. We evaluate the performance of STAC using this dataset to measure the accuracy metrics and inference rate for reid. Additionally, we quantify the reduction in video streams achieved through frame filtering and compression using FFmpeg compared to the raw camera streams. For completeness, we make available our repository to reproduce the results, available at https://github.com/VolodymyrVakhniuk/CS444_Final_Project.



### SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection
- **Arxiv ID**: http://arxiv.org/abs/2401.15293v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2401.15293v1)
- **Published**: 2024-01-27 04:24:49+00:00
- **Updated**: 2024-01-27 04:24:49+00:00
- **Authors**: Foozhan Ataiefard, Walid Ahmed, Habib Hajimolahoseini, Saina Asani, Farnoosh Javadi, Mohammad Hassanpour, Omar Mohamed Awad, Austin Wen, Kangling Liu, Yang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Vision transformers are known to be more computationally and data-intensive than CNN models. These transformer models such as ViT, require all the input image tokens to learn the relationship among them. However, many of these tokens are not informative and may contain irrelevant information such as unrelated background or unimportant scenery. These tokens are overlooked by the multi-head self-attention (MHSA), resulting in many redundant and unnecessary computations in MHSA and the feed-forward network (FFN). In this work, we propose a method to optimize the amount of unnecessary interactions between unimportant tokens by separating and sending them through a different low-cost computational path. Our method does not add any parameters to the ViT model and aims to find the best trade-off between training throughput and achieving a 0% loss in the Top-1 accuracy of the final model. Our experimental results on training ViT-small from scratch show that SkipViT is capable of effectively dropping 55% of the tokens while gaining more than 13% training throughput and maintaining classification accuracy at the level of the baseline model on Huawei Ascend910A.



### A Survey on 3D Skeleton Based Person Re-Identification: Approaches, Designs, Challenges, and Future Directions
- **Arxiv ID**: http://arxiv.org/abs/2401.15296v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2401.15296v1)
- **Published**: 2024-01-27 04:52:24+00:00
- **Updated**: 2024-01-27 04:52:24+00:00
- **Authors**: Haocong Rao, Chunyan Miao
- **Comment**: A up-to-date resource (papers, codes, data, etc.) of this survey is
  provided at https://github.com/Kali-Hac/3D-skeleton-based-person-re-ID-survey
- **Journal**: None
- **Summary**: Person re-identification via 3D skeletons is an important emerging research area that triggers great interest in the pattern recognition community. With distinctive advantages for many application scenarios, a great diversity of 3D skeleton based person re-identification (SRID) methods have been proposed in recent years, effectively addressing prominent problems in skeleton modeling and feature learning. Despite recent advances, to the best of our knowledge, little effort has been made to comprehensively summarize these studies and their challenges. In this paper, we attempt to fill this gap by providing a systematic survey on current SRID approaches, model designs, challenges, and future directions. Specifically, we first formulate the SRID problem, and propose a taxonomy of SRID research with a summary of benchmark datasets, commonly-used model architectures, and an analytical review of different methods' characteristics. Then, we elaborate on the design principles of SRID models from multiple aspects to offer key insights for model improvement. Finally, we identify critical challenges confronting current studies and discuss several promising directions for future research of SRID.



### ParaTransCNN: Parallelized TransCNN Encoder for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2401.15307v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2401.15307v1)
- **Published**: 2024-01-27 05:58:36+00:00
- **Updated**: 2024-01-27 05:58:36+00:00
- **Authors**: Hongkun Sun, Jing Xu, Yuping Duan
- **Comment**: None
- **Journal**: None
- **Summary**: The convolutional neural network-based methods have become more and more popular for medical image segmentation due to their outstanding performance. However, they struggle with capturing long-range dependencies, which are essential for accurately modeling global contextual correlations. Thanks to the ability to model long-range dependencies by expanding the receptive field, the transformer-based methods have gained prominence. Inspired by this, we propose an advanced 2D feature extraction method by combining the convolutional neural network and Transformer architectures. More specifically, we introduce a parallelized encoder structure, where one branch uses ResNet to extract local information from images, while the other branch uses Transformer to extract global information. Furthermore, we integrate pyramid structures into the Transformer to extract global information at varying resolutions, especially in intensive prediction tasks. To efficiently utilize the different information in the parallelized encoder at the decoder stage, we use a channel attention module to merge the features of the encoder and propagate them through skip connections and bottlenecks. Intensive numerical experiments are performed on both aortic vessel tree, cardiac, and multi-organ datasets. By comparing with state-of-the-art medical image segmentation methods, our method is shown with better segmentation accuracy, especially on small organs. The code is publicly available on https://github.com/HongkunSun/ParaTransCNN.



### Multi-Robot Relative Pose Estimation in SE(2) with Observability Analysis: A Comparison of Extended Kalman Filtering and Robust Pose Graph Optimization
- **Arxiv ID**: http://arxiv.org/abs/2401.15313v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.SY, eess.SY, math.OC, 93C85, 93E11, 93E24, 90C26, 93E10, 62M20
- **Links**: [PDF](http://arxiv.org/pdf/2401.15313v2)
- **Published**: 2024-01-27 06:09:56+00:00
- **Updated**: 2024-01-30 01:17:39+00:00
- **Authors**: Kihoon Shin, Hyunjae Sim, Seungwon Nam, Yonghee Kim, Jae Hu, Kwang-Ki K. Kim
- **Comment**: 20 pages, 21 figures
- **Journal**: None
- **Summary**: In this paper, we consider multi-robot localization problems with focus on cooperative localization and observability analysis of relative pose estimation. For cooperative localization, there is extra information available to each robot via communication network and message passing. If odometry data of a target robot can be transmitted to the ego-robot then the observability of their relative pose estimation can be achieved by range-only or bearing-only measurements provided both of their linear velocities are non-zero. If odometry data of a target robot is not directly transmitted but estimated by the ego-robot then there must be both range and bearing measurements to guarantee the observability of relative pose estimation. For ROS/Gazebo simulations, we consider four different sensing and communication structures in which extended Kalman filtering (EKF) and pose graph optimization (PGO) estimation with different robust loss functions (filtering and smoothing with different batch sizes of sliding window) are compared in terms of estimation accuracy. For hardware experiments, two Turtlebot3 equipped with UWB modules are used for real-world inter-robot relative pose estimation, in which both EKF and PGO are applied and compared.



### Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting
- **Arxiv ID**: http://arxiv.org/abs/2401.15318v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2401.15318v1)
- **Published**: 2024-01-27 06:45:22+00:00
- **Updated**: 2024-01-27 06:45:22+00:00
- **Authors**: Yutao Feng, Xiang Feng, Yintong Shang, Ying Jiang, Chang Yu, Zeshun Zong, Tianjia Shao, Hongzhi Wu, Kun Zhou, Chenfanfu Jiang, Yin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian splatting and position-based dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, please visit our project page at \url{https://amysteriouscat.github.io/GaussianSplashing/}.



### You Only Look Bottom-Up for Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2401.15319v1
- **DOI**: 10.1109/LRA.2023.3313053
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.15319v1)
- **Published**: 2024-01-27 06:45:35+00:00
- **Updated**: 2024-01-27 06:45:35+00:00
- **Authors**: Kaixin Xiong, Dingyuan Zhang, Dingkang Liang, Zhe Liu, Hongcheng Yang, Wondimu Dikubab, Jianwei Cheng, Xiang Bai
- **Comment**: Accepted by IEEE Robotics and Automation Letters (RA-L)
- **Journal**: None
- **Summary**: Monocular 3D Object Detection is an essential task for autonomous driving. Meanwhile, accurate 3D object detection from pure images is very challenging due to the loss of depth information. Most existing image-based methods infer objects' location in 3D space based on their 2D sizes on the image plane, which usually ignores the intrinsic position clues from images, leading to unsatisfactory performances. Motivated by the fact that humans could leverage the bottom-up positional clues to locate objects in 3D space from a single image, in this paper, we explore the position modeling from the image feature column and propose a new method named You Only Look Bottum-Up (YOLOBU). Specifically, our YOLOBU leverages Column-based Cross Attention to determine how much a pixel contributes to pixels above it. Next, the Row-based Reverse Cumulative Sum (RRCS) is introduced to build the connections of pixels in the bottom-up direction. Our YOLOBU fully explores the position clues for monocular 3D detection via building the relationship of pixels from the bottom-up way. Extensive experiments on the KITTI dataset demonstrate the effectiveness and superiority of our method.



### L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2401.15335v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2401.15335v1)
- **Published**: 2024-01-27 07:57:20+00:00
- **Updated**: 2024-01-27 07:57:20+00:00
- **Authors**: Ping Guo, Fei Liu, Xi Lin, Qingchuan Zhao, Qingfu Zhang
- **Comment**: Under Review of IJCNN 2024
- **Journal**: None
- **Summary**: In the rapidly evolving field of machine learning, adversarial attacks present a significant challenge to model robustness and security. Decision-based attacks, which only require feedback on the decision of a model rather than detailed probabilities or scores, are particularly insidious and difficult to defend against. This work introduces L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), a novel approach leveraging the generative capabilities of Large Language Models (LLMs) to automate the design of these attacks. By iteratively interacting with LLMs in an evolutionary framework, L-AutoDA automatically designs competitive attack algorithms efficiently without much human effort. We demonstrate the efficacy of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline methods in both success rate and computational efficiency. Our findings underscore the potential of language models as tools for adversarial attack generation and highlight new avenues for the development of robust AI systems.



### AniDress: Animatable Loose-Dressed Avatar from Sparse Views Using Garment Rigging Model
- **Arxiv ID**: http://arxiv.org/abs/2401.15348v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2401.15348v1)
- **Published**: 2024-01-27 08:48:18+00:00
- **Updated**: 2024-01-27 08:48:18+00:00
- **Authors**: Beijia Chen, Yuefan Shen, Qing Shuai, Xiaowei Zhou, Kun Zhou, Youyi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Recent communities have seen significant progress in building photo-realistic animatable avatars from sparse multi-view videos. However, current workflows struggle to render realistic garment dynamics for loose-fitting characters as they predominantly rely on naked body models for human modeling while leaving the garment part un-modeled. This is mainly due to that the deformations yielded by loose garments are highly non-rigid, and capturing such deformations often requires dense views as supervision. In this paper, we introduce AniDress, a novel method for generating animatable human avatars in loose clothes using very sparse multi-view videos (4-8 in our setting). To allow the capturing and appearance learning of loose garments in such a situation, we employ a virtual bone-based garment rigging model obtained from physics-based simulation data. Such a model allows us to capture and render complex garment dynamics through a set of low-dimensional bone transformations. Technically, we develop a novel method for estimating temporal coherent garment dynamics from a sparse multi-view video. To build a realistic rendering for unseen garment status using coarse estimations, a pose-driven deformable neural radiance field conditioned on both body and garment motions is introduced, providing explicit control of both parts. At test time, the new garment poses can be captured from unseen situations, derived from a physics-based or neural network-based simulator to drive unseen garment dynamics. To evaluate our approach, we create a multi-view dataset that captures loose-dressed performers with diverse motions. Experiments show that our method is able to render natural garment dynamics that deviate highly from the body and generalize well to both unseen views and poses, surpassing the performance of existing methods. The code and data will be publicly available.



### DeepGI: An Automated Approach for Gastrointestinal Tract Segmentation in MRI Scans
- **Arxiv ID**: http://arxiv.org/abs/2401.15354v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2401.15354v1)
- **Published**: 2024-01-27 09:05:41+00:00
- **Updated**: 2024-01-27 09:05:41+00:00
- **Authors**: Ye Zhang, Yulu Gong, Dongji Cui, Xinrui Li, Xinyu Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Gastrointestinal (GI) tract cancers pose a global health challenge, demanding precise radiotherapy planning for optimal treatment outcomes. This paper introduces a cutting-edge approach to automate the segmentation of GI tract regions in magnetic resonance imaging (MRI) scans. Leveraging advanced deep learning architectures, the proposed model integrates Inception-V4 for initial classification, UNet++ with a VGG19 encoder for 2.5D data, and Edge UNet for grayscale data segmentation. Meticulous data preprocessing, including innovative 2.5D processing, is employed to enhance adaptability, robustness, and accuracy.   This work addresses the manual and time-consuming segmentation process in current radiotherapy planning, presenting a unified model that captures intricate anatomical details. The integration of diverse architectures, each specializing in unique aspects of the segmentation task, signifies a novel and comprehensive solution. This model emerges as an efficient and accurate tool for clinicians, marking a significant advancement in the field of GI tract image segmentation for radiotherapy planning.



### Transformer-based Clipped Contrastive Quantization Learning for Unsupervised Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2401.15362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.15362v1)
- **Published**: 2024-01-27 09:39:11+00:00
- **Updated**: 2024-01-27 09:39:11+00:00
- **Authors**: Ayush Dubey, Shiv Ram Dubey, Satish Kumar Singh, Wei-Ta Chu
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised image retrieval aims to learn the important visual characteristics without any given level to retrieve the similar images for a given query image. The Convolutional Neural Network (CNN)-based approaches have been extensively exploited with self-supervised contrastive learning for image hashing. However, the existing approaches suffer due to lack of effective utilization of global features by CNNs and biased-ness created by false negative pairs in the contrastive learning. In this paper, we propose a TransClippedCLR model by encoding the global context of an image using Transformer having local context through patch based processing, by generating the hash codes through product quantization and by avoiding the potential false negative pairs through clipped contrastive learning. The proposed model is tested with superior performance for unsupervised image retrieval on benchmark datasets, including CIFAR10, NUS-Wide and Flickr25K, as compared to the recent state-of-the-art deep models. The results using the proposed clipped contrastive learning are greatly improved on all datasets as compared to same backbone network with vanilla contrastive learning.



### An open dataset for oracle bone script recognition and decipherment
- **Arxiv ID**: http://arxiv.org/abs/2401.15365v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.15365v1)
- **Published**: 2024-01-27 09:54:16+00:00
- **Updated**: 2024-01-27 09:54:16+00:00
- **Authors**: Pengjie Wang, Kaile Zhang, Yuliang Liu, Jinpeng Wan, Haisu Guan, Zhebin Kuang, Xinyu Wang, Lianwen Jin, Xiang Bai
- **Comment**: None
- **Journal**: None
- **Summary**: Oracle Bone Script (OBS), one of the earliest known forms of ancient Chinese writing, holds invaluable insights into the humanities and geography of the Shang Dynasty, dating back 3,000 years. The immense historical and cultural significance of these writings cannot be overstated. However, the passage of time has obscured much of their meaning, presenting a significant challenge in deciphering these ancient texts. With the advent of Artificial Intelligence (AI), employing AI to assist in interpreting OBS has become a feasible option. Yet, progress in this area has been hindered by a lack of high-quality datasets. To address this issue, this paper details the creation of the HUST-OBS dataset. This dataset encompasses 77,064 images of 1,588 individual deciphered scripts and 62,989 images of 9,411 undeciphered characters, with a total of 140,053 images, compiled from diverse sources. Additionally, all images and labels have been reviewed and corrected by experts in oracle bone studies. The hope is that this dataset could inspire and assist future research in deciphering those unknown OBS.



### Face to Cartoon Incremental Super-Resolution using Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2401.15366v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2401.15366v1)
- **Published**: 2024-01-27 10:06:52+00:00
- **Updated**: 2024-01-27 10:06:52+00:00
- **Authors**: Trinetra Devkatte, Shiv Ram Dubey, Satish Kumar Singh, Abdenour Hadid
- **Comment**: None
- **Journal**: None
- **Summary**: Facial super-resolution/hallucination is an important area of research that seeks to enhance low-resolution facial images for a variety of applications. While Generative Adversarial Networks (GANs) have shown promise in this area, their ability to adapt to new, unseen data remains a challenge. This paper addresses this problem by proposing an incremental super-resolution using GANs with knowledge distillation (ISR-KD) for face to cartoon. Previous research in this area has not investigated incremental learning, which is critical for real-world applications where new data is continually being generated. The proposed ISR-KD aims to develop a novel unified framework for facial super-resolution that can handle different settings, including different types of faces such as cartoon face and various levels of detail. To achieve this, a GAN-based super-resolution network was pre-trained on the CelebA dataset and then incrementally trained on the iCartoonFace dataset, using knowledge distillation to retain performance on the CelebA test set while improving the performance on iCartoonFace test set. Our experiments demonstrate the effectiveness of knowledge distillation in incrementally adding capability to the model for cartoon face super-resolution while retaining the learned knowledge for facial hallucination tasks in GANs.



### Open-RadVLAD: Fast and Robust Radar Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2401.15380v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2401.15380v1)
- **Published**: 2024-01-27 10:55:55+00:00
- **Updated**: 2024-01-27 10:55:55+00:00
- **Authors**: Matthew Gadd, Paul Newman
- **Comment**: accepted at 2024 IEEE Radar Conference
- **Journal**: None
- **Summary**: Radar place recognition often involves encoding a live scan as a vector and matching this vector to a database in order to recognise that the vehicle is in a location that it has visited before. Radar is inherently robust to lighting or weather conditions, but place recognition with this sensor is still affected by: (1) viewpoint variation, i.e. translation and rotation, (2) sensor artefacts or "noises". For 360-degree scanning radar, rotation is readily dealt with by in some way aggregating across azimuths. Also, we argue in this work that it is more critical to deal with the richness of representation and sensor noises than it is to deal with translational invariance - particularly in urban driving where vehicles predominantly follow the same lane when repeating a route. In our method, for computational efficiency, we use only the polar representation. For partial translation invariance and robustness to signal noise, we use only a one-dimensional Fourier Transform along radial returns. We also achieve rotational invariance and a very discriminative descriptor space by building a vector of locally aggregated descriptors. Our method is more comprehensively tested than all prior radar place recognition work - over an exhaustive combination of all 870 pairs of trajectories from 30 Oxford Radar RobotCar Dataset sequences (each approximately 10 km). Code and detailed results are provided at github.com/mttgdd/open-radvlad, as an open implementation and benchmark for future work in this area. We achieve a median of 91.52% in Recall@1, outstripping the 69.55% for the only other open implementation, RaPlace, and at a fraction of its computational cost (relying on fewer integral transforms e.g. Radon, Fourier, and inverse Fourier).



### An Implicit Physical Face Model Driven by Expression and Style
- **Arxiv ID**: http://arxiv.org/abs/2401.15414v1
- **DOI**: 10.1145/3610548.3618156
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2401.15414v1)
- **Published**: 2024-01-27 14:07:48+00:00
- **Updated**: 2024-01-27 14:07:48+00:00
- **Authors**: Lingchen Yang, Gaspard Zoss, Prashanth Chandran, Paulo Gotardo, Markus Gross, Barbara Solenthaler, Eftychios Sifakis, Derek Bradley
- **Comment**: Accepted to SIGGRAPH ASIA 2023. Project page:
  https://studios.disneyresearch.com/2023/11/29/an-implicit-physical-face-model-driven-by-expression-and-style/
  Video: https://www.youtube.com/watch?v=-qM_XUv-JhA&t
- **Journal**: None
- **Summary**: 3D facial animation is often produced by manipulating facial deformation models (or rigs), that are traditionally parameterized by expression controls. A key component that is usually overlooked is expression 'style', as in, how a particular expression is performed. Although it is common to define a semantic basis of expressions that characters can perform, most characters perform each expression in their own style. To date, style is usually entangled with the expression, and it is not possible to transfer the style of one character to another when considering facial animation. We present a new face model, based on a data-driven implicit neural physics model, that can be driven by both expression and style separately. At the core, we present a framework for learning implicit physics-based actuations for multiple subjects simultaneously, trained on a few arbitrary performance capture sequences from a small set of identities. Once trained, our method allows generalized physics-based facial animation for any of the trained identities, extending to unseen performances. Furthermore, it grants control over the animation style, enabling style transfer from one character to another or blending styles of different characters. Lastly, as a physics-based model, it is capable of synthesizing physical effects, such as collision handling, setting our method apart from conventional approaches.



### Decentralized Gossip Mutual Learning (GML) for brain tumor segmentation on multi-parametric MRI
- **Arxiv ID**: http://arxiv.org/abs/2401.15434v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2401.15434v1)
- **Published**: 2024-01-27 15:05:25+00:00
- **Updated**: 2024-01-27 15:05:25+00:00
- **Authors**: Jingyun Chen, Yading Yuan
- **Comment**: 3 pages, 1 figure, accepted to IEEE EMBS 2023. arXiv admin note: text
  overlap with arXiv:2401.06180
- **Journal**: None
- **Summary**: Federated Learning (FL) enables collaborative model training among medical centers without sharing private data. However, traditional FL risks on server failures and suboptimal performance on local data due to the nature of centralized model aggregation. To address these issues, we present Gossip Mutual Learning (GML), a decentralized framework that uses Gossip Protocol for direct peer-to-peer communication. In addition, GML encourages each site to optimize its local model through mutual learning to account for data variations among different sites. For the task of tumor segmentation using 146 cases from four clinical sites in BraTS 2021 dataset, we demonstrated GML outperformed local models and achieved similar performance as FedAvg with only 25% communication overhead.



### A Systematic Review of Available Datasets in Additive Manufacturing
- **Arxiv ID**: http://arxiv.org/abs/2401.15448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.15448v1)
- **Published**: 2024-01-27 16:13:32+00:00
- **Updated**: 2024-01-27 16:13:32+00:00
- **Authors**: Xiao Liu, Alessandra Mileo, Alan F. Smeaton
- **Comment**: 24 pages
- **Journal**: None
- **Summary**: In-situ monitoring incorporating data from visual and other sensor technologies, allows the collection of extensive datasets during the Additive Manufacturing (AM) process. These datasets have potential for determining the quality of the manufactured output and the detection of defects through the use of Machine Learning during the manufacturing process. Open and annotated datasets derived from AM processes are necessary for the machine learning community to address this opportunity, which creates difficulties in the application of computer vision-related machine learning in AM. This systematic review investigates the availability of open image-based datasets originating from AM processes that align with a number of pre-defined selection criteria. The review identifies existing gaps among the current image-based datasets in the domain of AM, and points to the need for greater availability of open datasets in order to allow quality assessment and defect detection during additive manufacturing, to develop.



### New Foggy Object Detecting Model
- **Arxiv ID**: http://arxiv.org/abs/2401.15455v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2401.15455v1)
- **Published**: 2024-01-27 16:29:53+00:00
- **Updated**: 2024-01-27 16:29:53+00:00
- **Authors**: Rahul Banavathu, Modem Veda Sree, Bollina Kavya Sri, Suddhasil De
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection in reduced visibility has become a prominent research area. The existing techniques are not accurate enough in recognizing objects under such circumstances. This paper introduces a new foggy object detection method through a two-staged architecture of region identification from input images and detecting objects in such regions. The paper confirms notable improvements of the proposed method's accuracy and detection time over existing techniques.



### A New Method for Vehicle Logo Recognition Based on Swin Transformer
- **Arxiv ID**: http://arxiv.org/abs/2401.15458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.15458v1)
- **Published**: 2024-01-27 16:46:17+00:00
- **Updated**: 2024-01-27 16:46:17+00:00
- **Authors**: Yang Li, Doudou Zhang, Jianli Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Intelligent Transportation Systems (ITS) utilize sensors, cameras, and big data analysis to monitor real-time traffic conditions, aiming to improve traffic efficiency and safety. Accurate vehicle recognition is crucial in this process, and Vehicle Logo Recognition (VLR) stands as a key method. VLR enables effective management and monitoring by distinguishing vehicles on the road. Convolutional Neural Networks (CNNs) have made impressive strides in VLR research. However, achieving higher performance demands significant time and computational resources for training. Recently, the rise of Transformer models has brought new opportunities to VLR. Swin Transformer, with its efficient computation and global feature modeling capabilities, outperforms CNNs under challenging conditions. In this paper, we implement real-time VLR using Swin Transformer and fine-tune it for optimal performance. Extensive experiments conducted on three public vehicle logo datasets (HFUT-VL1, XMU, CTGU-VLD) demonstrate impressive top accuracy results of 99.28%, 100%, and 99.17%, respectively. Additionally, the use of a transfer learning strategy enables our method to be on par with state-of-the-art VLR methods. These findings affirm the superiority of our approach over existing methods. Future research can explore and optimize the application of the Swin Transformer in other vehicle vision recognition tasks to drive advancements in ITS.



### Temporal evolution in synthetic handwriting
- **Arxiv ID**: http://arxiv.org/abs/2401.15472v1
- **DOI**: 10.1016/j.patcog.2017.03.019
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.15472v1)
- **Published**: 2024-01-27 17:56:03+00:00
- **Updated**: 2024-01-27 17:56:03+00:00
- **Authors**: Cristina Carmona-Duarte, Miguel A. Ferrer, Antonio Parziale, Angelo Marcelli
- **Comment**: Published in Pattern Recognition
- **Journal**: Carmona-Duarte, C., Ferrer, M.A., Parziale, A., Marcelli, A.;
  Temporal evolution in synthetic handwriting; Pattern Recognition 68, p.p 233
  - 244 (2017)
- **Summary**: New methods for generating synthetic handwriting images for biometric applications have recently been developed. The temporal evolution of handwriting from childhood to adulthood is usually left unexplored in these works. This paper proposes a novel methodology for including temporal evolution in a handwriting synthesizer by means of simplifying the text trajectory plan and handwriting dynamics. This is achieved through a tailored version of the kinematic theory of rapid human movements and the neuromotor inspired handwriting synthesizer. The realism of the proposed method has been evaluated by comparing the temporal evolution of real and synthetic samples both quantitatively and subjectively. The quantitative test is based on a visual perception algorithm that compares the letter variability and the number of strokes in the real and synthetic handwriting produced at different ages. In the subjective test, 30 people are asked to evaluate the perceived realism of the evolution of the synthetic handwriting.



### iDeLog: Iterative Dual Spatial and Kinematic Extraction of Sigma-Lognormal Parameters
- **Arxiv ID**: http://arxiv.org/abs/2401.15473v1
- **DOI**: 10.1109/TPAMI.2018.2879312
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.15473v1)
- **Published**: 2024-01-27 17:58:42+00:00
- **Updated**: 2024-01-27 17:58:42+00:00
- **Authors**: Miguel A. Ferrer, Moises Diaz, Cristina Carmona-Duarte, Rejean Plamondon
- **Comment**: Accepted Version published by Transactions on Pattern Analysis and
  Machine Intelligence
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  42(1); p.p. 114-125, 2020
- **Summary**: The Kinematic Theory of rapid movements and its associated Sigma-Lognormal model have been extensively used in a large variety of applications. While the physical and biological meaning of the model have been widely tested and validated for rapid movements, some shortcomings have been detected when it is used with continuous long and complex movements. To alleviate such drawbacks, and inspired by the motor equivalence theory and a conceivable visual feedback, this paper proposes a novel framework to extract the Sigma-Lognormal parameters, namely iDeLog. Specifically, iDeLog consists of two steps. The first one, influenced by the motor equivalence model, separately derives an initial action plan defined by a set of virtual points and angles from the trajectory and a sequence of lognormals from the velocity. In the second step, based on a hypothetical visual feedback compatible with an open-loop motor control, the virtual target points of the action plan are iteratively moved to improve the matching between the observed and reconstructed trajectory and velocity. During experiments conducted with handwritten signatures, iDeLog obtained promising results as compared to the previous development of the Sigma-Lognormal.



### Distilling Privileged Multimodal Information for Expression Recognition using Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/2401.15489v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2401.15489v1)
- **Published**: 2024-01-27 19:44:15+00:00
- **Updated**: 2024-01-27 19:44:15+00:00
- **Authors**: Muhammad Haseeb Aslam, Muhammad Osama Zeeshan, Soufiane Belharbi, Marco Pedersoli, Alessandro Koerich, Simon Bacon, Eric Granger
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal affect recognition models have reached remarkable performance in the lab environment due to their ability to model complementary and redundant semantic information. However, these models struggle in the wild, mainly because of the unavailability or quality of modalities used for training. In practice, only a subset of the training-time modalities may be available at test time. Learning with privileged information (PI) enables deep learning models (DL) to exploit data from additional modalities only available during training. State-of-the-art knowledge distillation (KD) methods have been proposed to distill multiple teacher models (each trained on a modality) to a common student model. These privileged KD methods typically utilize point-to-point matching and have no explicit mechanism to capture the structural information in the teacher representation space formed by introducing the privileged modality. We argue that encoding this same structure in the student space may lead to enhanced student performance. This paper introduces a new structural KD mechanism based on optimal transport (OT), where entropy-regularized OT distills the structural dark knowledge. Privileged KD with OT (PKDOT) method captures the local structures in the multimodal teacher representation by calculating a cosine similarity matrix and selects the top-k anchors to allow for sparse OT solutions, resulting in a more stable distillation process. Experiments were performed on two different problems: pain estimation on the Biovid dataset (ordinal classification) and arousal-valance prediction on the Affwild2 dataset (regression). Results show that the proposed method can outperform state-of-the-art privileged KD methods on these problems. The diversity of different modalities and fusion architectures indicates that the proposed PKDOT method is modality and model-agnostic.



### FloodLense: A Framework for ChatGPT-based Real-time Flood Detection
- **Arxiv ID**: http://arxiv.org/abs/2401.15501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.15501v1)
- **Published**: 2024-01-27 20:52:33+00:00
- **Updated**: 2024-01-27 20:52:33+00:00
- **Authors**: Pranath Reddy Kumbam, Kshitij Maruti Vejre
- **Comment**: None
- **Journal**: None
- **Summary**: This study addresses the vital issue of real-time flood detection and management. It innovatively combines advanced deep learning models with Large language models (LLM), enhancing flood monitoring and response capabilities. This approach addresses the limitations of current methods by offering a more accurate, versatile, user-friendly and accessible solution. The integration of UNet, RDN, and ViT models with natural language processing significantly improves flood area detection in diverse environments, including using aerial and satellite imagery. The experimental evaluation demonstrates the models' efficacy in accurately identifying and mapping flood zones, showcasing the project's potential in transforming environmental monitoring and disaster management fields.



### MiTU-Net: A fine-tuned U-Net with SegFormer backbone for segmenting pubic symphysis-fetal head
- **Arxiv ID**: http://arxiv.org/abs/2401.15513v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2401.15513v1)
- **Published**: 2024-01-27 21:53:05+00:00
- **Updated**: 2024-01-27 21:53:05+00:00
- **Authors**: Fangyijie Wang, Guenole Silvestre, Kathleen Curran
- **Comment**: The 5th place in the Pubic Symphysis-Fetal Head Segmentation
  Challenge in MICCAI 2023
- **Journal**: None
- **Summary**: Ultrasound measurements have been examined as potential tools for predicting the likelihood of successful vaginal delivery. The angle of progression (AoP) is a measurable parameter that can be obtained during the initial stage of labor. The AoP is defined as the angle between a straight line along the longitudinal axis of the pubic symphysis (PS) and a line from the inferior edge of the PS to the leading edge of the fetal head (FH). However, the process of measuring AoP on ultrasound images is time consuming and prone to errors. To address this challenge, we propose the Mix Transformer U-Net (MiTU-Net) network, for automatic fetal head-pubic symphysis segmentation and AoP measurement. The MiTU-Net model is based on an encoder-decoder framework, utilizing a pre-trained efficient transformer to enhance feature representation. Within the efficient transformer encoder, the model significantly reduces the trainable parameters of the encoder-decoder model. The effectiveness of the proposed method is demonstrated through experiments conducted on a recent transperineal ultrasound dataset. Our model achieves competitive performance, ranking 5th compared to existing approaches. The MiTU-Net presents an efficient method for automatic segmentation and AoP measurement, reducing errors and assisting sonographers in clinical practice. Reproducibility: Framework implementation and models available on https://github.com/13204942/MiTU-Net.



### Exploring the Transferability of a Foundation Model for Fundus Images: Application to Hypertensive Retinopathy
- **Arxiv ID**: http://arxiv.org/abs/2401.15526v1
- **DOI**: 10.1007/978-3-031-50075-6_33
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.15526v1)
- **Published**: 2024-01-27 23:40:24+00:00
- **Updated**: 2024-01-27 23:40:24+00:00
- **Authors**: Julio Silva-Rodriguez, Jihed Chelbi, Waziha Kabir, Hadi Chakor, Jose Dolz, Ismail Ben Ayed, Riadh Kobbi
- **Comment**: CGI 2023
- **Journal**: None
- **Summary**: Using deep learning models pre-trained on Imagenet is the traditional solution for medical image classification to deal with data scarcity. Nevertheless, relevant literature supports that this strategy may offer limited gains due to the high dissimilarity between domains. Currently, the paradigm of adapting domain-specialized foundation models is proving to be a promising alternative. However, how to perform such knowledge transfer, and the benefits and limitations it presents, are under study. The CGI-HRDC challenge for Hypertensive Retinopathy diagnosis on fundus images introduces an appealing opportunity to evaluate the transferability of a recently released vision-language foundation model of the retina, FLAIR. In this work, we explore the potential of using FLAIR features as starting point for fundus image classification, and we compare its performance with regard to Imagenet initialization on two popular transfer learning methods: Linear Probing (LP) and Fine-Tuning (FP). Our empirical observations suggest that, in any case, the use of the traditional strategy provides performance gains. In contrast, direct transferability from FLAIR model allows gains of 2.5%. When fine-tuning the whole network, the performance gap increases up to 4%. In this case, we show that avoiding feature deterioration via LP initialization of the classifier allows the best re-use of the rich pre-trained features. Although direct transferability using LP still offers limited performance, we believe that foundation models such as FLAIR will drive the evolution of deep-learning-based fundus image analysis.



