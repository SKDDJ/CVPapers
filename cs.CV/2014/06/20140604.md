# Arxiv Papers in cs.CV on 2014-06-04
### Improvement Tracking Dynamic Programming using Replication Function for Continuous Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/1406.0909v1
- **DOI**: 10.14445/22315381/IJETT-V7P254
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.0909v1)
- **Published**: 2014-06-04 00:03:51+00:00
- **Updated**: 2014-06-04 00:03:51+00:00
- **Authors**: S. Ildarabadi, M. Ebrahimi, H. R. Pourreza
- **Comment**: 5 pages, 13 figures, Published with "International Journal of
  Engineering Trends and Technology (IJETT)"
- **Journal**: None
- **Summary**: In this paper we used a Replication Function (R. F.)for improvement tracking with dynamic programming. The R. F. transforms values of gray level [0 255] to [0 1]. The resulting images of R. F. are more striking and visible in skin regions. The R. F. improves Dynamic Programming (D. P.) in overlapping hand and face. Results show that Tracking Error Rate 11% and Average Tracked Distance 7% reduced



### Multiscale Fields of Patterns
- **Arxiv ID**: http://arxiv.org/abs/1406.0924v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.0924v3)
- **Published**: 2014-06-04 02:10:58+00:00
- **Updated**: 2014-12-12 18:42:35+00:00
- **Authors**: Pedro F. Felzenszwalb, John G. Oberlin
- **Comment**: In NIPS 2014
- **Journal**: None
- **Summary**: We describe a framework for defining high-order image models that can be used in a variety of applications. The approach involves modeling local patterns in a multiscale representation of an image. Local properties of a coarsened image reflect non-local properties of the original image. In the case of binary images local properties are defined by the binary patterns observed over small neighborhoods around each pixel. With the multiscale representation we capture the frequency of patterns observed at different scales of resolution. This framework leads to expressive priors that depend on a relatively small number of parameters. For inference and learning we use an MCMC method for block sampling with very large blocks. We evaluate the approach with two example applications. One involves contour detection. The other involves binary segmentation.



### Beyond $χ^2$ Difference: Learning Optimal Metric for Boundary Detection
- **Arxiv ID**: http://arxiv.org/abs/1406.0946v1
- **DOI**: 10.1109/LSP.2014.2346232
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.0946v1)
- **Published**: 2014-06-04 05:58:53+00:00
- **Updated**: 2014-06-04 05:58:53+00:00
- **Authors**: Fei He, Shengjin Wang
- **Comment**: Submitted to IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: This letter focuses on solving the challenging problem of detecting natural image boundaries. A boundary usually refers to the border between two regions with different semantic meanings. Therefore, a measurement of dissimilarity between image regions plays a pivotal role in boundary detection of natural images. To improve the performance of boundary detection, a Learning-based Boundary Metric (LBM) is proposed to replace $\chi^2$ difference adopted by the classical algorithm mPb. Compared with $\chi^2$ difference, LBM is composed of a single layer neural network and an RBF kernel, and is fine-tuned by supervised learning rather than human-crafted. It is more effective in describing the dissimilarity between natural image regions while tolerating large variance of image data. After substituting $\chi^2$ difference with LBM, the F-measure metric of mPb on the BSDS500 benchmark is increased from 0.69 to 0.71. Moreover, when image features are computed on a single scale, the proposed LBM algorithm still achieves competitive results compared with \emph{mPb}, which makes use of multi-scale image features.



### Learning to Diversify via Weighted Kernels for Classifier Ensemble
- **Arxiv ID**: http://arxiv.org/abs/1406.1167v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.5
- **Links**: [PDF](http://arxiv.org/pdf/1406.1167v1)
- **Published**: 2014-06-04 09:16:42+00:00
- **Updated**: 2014-06-04 09:16:42+00:00
- **Authors**: Xu-Cheng Yin, Chun Yang, Hong-Wei Hao
- **Comment**: Submitted to IEEE Trans. Pattern Analysis and Machine Intelligence
  (TPAMI)
- **Journal**: None
- **Summary**: Classifier ensemble generally should combine diverse component classifiers. However, it is difficult to give a definitive connection between diversity measure and ensemble accuracy. Given a list of available component classifiers, how to adaptively and diversely ensemble classifiers becomes a big challenge in the literature. In this paper, we argue that diversity, not direct diversity on samples but adaptive diversity with data, is highly correlated to ensemble accuracy, and we propose a novel technology for classifier ensemble, learning to diversify, which learns to adaptively combine classifiers by considering both accuracy and diversity. Specifically, our approach, Learning TO Diversify via Weighted Kernels (L2DWK), performs classifier combination by optimizing a direct but simple criterion: maximizing ensemble accuracy and adaptive diversity simultaneously by minimizing a convex loss function. Given a measure formulation, the diversity is calculated with weighted kernels (i.e., the diversity is measured on the component classifiers' outputs which are kernelled and weighted), and the kernel weights are automatically learned. We minimize this loss function by estimating the kernel weights in conjunction with the classifier weights, and propose a self-training algorithm for conducting this convex optimization procedure iteratively. Extensive experiments on a variety of 32 UCI classification benchmark datasets show that the proposed approach consistently outperforms state-of-the-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen, Regularized Selective Ensemble, and Ensemble Pruning via Semi-Definite Programming.



### Local Decorrelation For Improved Detection
- **Arxiv ID**: http://arxiv.org/abs/1406.1134v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.1134v2)
- **Published**: 2014-06-04 18:20:38+00:00
- **Updated**: 2014-11-04 02:50:18+00:00
- **Authors**: Woonhyun Nam, Piotr Dollár, Joon Hee Han
- **Comment**: To appear in Neural Information Processing Systems (NIPS), 2014
- **Journal**: None
- **Summary**: Even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets. While effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data. Given highly correlated data, decision trees with oblique (multiple feature) splits can be effective. Use of oblique splits, however, comes at considerable computational expense. Inspired by recent work on discriminative decorrelation of HOG features, we instead propose an efficient feature transform that removes correlations in local neighborhoods. The result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees. In fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost. The overall improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we reduce false positives nearly tenfold over the previous state-of-the-art.



