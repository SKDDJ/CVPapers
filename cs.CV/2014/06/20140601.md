# Arxiv Papers in cs.CV on 2014-06-01
### Seeing the Big Picture: Deep Embedding with Contextual Evidences
- **Arxiv ID**: http://arxiv.org/abs/1406.0132v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.0132v1)
- **Published**: 2014-06-01 05:04:28+00:00
- **Updated**: 2014-06-01 05:04:28+00:00
- **Authors**: Liang Zheng, Shengjin Wang, Fei He, Qi Tian
- **Comment**: 10 pages, 13 figures, 7 tables, submitted to ACM Multimedia 2014
- **Journal**: None
- **Summary**: In the Bag-of-Words (BoW) model based image retrieval task, the precision of visual matching plays a critical role in improving retrieval performance. Conventionally, local cues of a keypoint are employed. However, such strategy does not consider the contextual evidences of a keypoint, a problem which would lead to the prevalence of false matches. To address this problem, this paper defines "true match" as a pair of keypoints which are similar on three levels, i.e., local, regional, and global. Then, a principled probabilistic framework is established, which is capable of implicitly integrating discriminative cues from all these feature levels.   Specifically, the Convolutional Neural Network (CNN) is employed to extract features from regional and global patches, leading to the so-called "Deep Embedding" framework. CNN has been shown to produce excellent performance on a dozen computer vision tasks such as image classification and detection, but few works have been done on BoW based image retrieval. In this paper, firstly we show that proper pre-processing techniques are necessary for effective usage of CNN feature. Then, in the attempt to fit it into our model, a novel indexing structure called "Deep Indexing" is introduced, which dramatically reduces memory usage.   Extensive experiments on three benchmark datasets demonstrate that, the proposed Deep Embedding method greatly promotes the retrieval accuracy when CNN feature is integrated. We show that our method is efficient in terms of both memory and time cost, and compares favorably with the state-of-the-art methods.



### $l_1$-regularized Outlier Isolation and Regression
- **Arxiv ID**: http://arxiv.org/abs/1406.0156v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1406.0156v2)
- **Published**: 2014-06-01 11:52:19+00:00
- **Updated**: 2014-11-20 08:58:09+00:00
- **Authors**: Sheng Han, Suzhen Wang, Xinyu Wu
- **Comment**: Outlier Detection, Robust Regression, Robust Rank Factorization,
  $l_1$-regularization
- **Journal**: None
- **Summary**: This paper proposed a new regression model called $l_1$-regularized outlier isolation and regression (LOIRE) and a fast algorithm based on block coordinate descent to solve this model. Besides, assuming outliers are gross errors following a Bernoulli process, this paper also presented a Bernoulli estimate model which, in theory, should be very accurate and robust due to its complete elimination of affections caused by outliers. Though this Bernoulli estimate is hard to solve, it could be approximately achieved through a process which takes LOIRE as an important intermediate step. As a result, the approximate Bernoulli estimate is a good combination of Bernoulli estimate's accuracy and LOIRE regression's efficiency with several simulations conducted to strongly verify this point. Moreover, LOIRE can be further extended to realize robust rank factorization which is powerful in recovering low-rank component from massive corruptions. Extensive experimental results showed that the proposed method outperforms state-of-the-art methods like RPCA and GoDec in the aspect of computation speed with a competitive performance.



