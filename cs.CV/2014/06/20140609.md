# Arxiv Papers in cs.CV on 2014-06-09
### Image Tag Completion by Low-rank Factorization with Dual Reconstruction Structure Preserved
- **Arxiv ID**: http://arxiv.org/abs/1406.2049v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1406.2049v1)
- **Published**: 2014-06-09 01:22:43+00:00
- **Updated**: 2014-06-09 01:22:43+00:00
- **Authors**: Xue Li, Yu-Jin Zhang, Bin Shen, Bao-Di Liu
- **Comment**: None
- **Journal**: None
- **Summary**: A novel tag completion algorithm is proposed in this paper, which is designed with the following features: 1) Low-rank and error s-parsity: the incomplete initial tagging matrix D is decomposed into the complete tagging matrix A and a sparse error matrix E. However, instead of minimizing its nuclear norm, A is further factor-ized into a basis matrix U and a sparse coefficient matrix V, i.e. D=UV+E. This low-rank formulation encapsulating sparse coding enables our algorithm to recover latent structures from noisy initial data and avoid performing too much denoising; 2) Local reconstruction structure consistency: to steer the completion of D, the local linear reconstruction structures in feature space and tag space are obtained and preserved by U and V respectively. Such a scheme could alleviate the negative effect of distances measured by low-level features and incomplete tags. Thus, we can seek a balance between exploiting as much information and not being mislead to suboptimal performance. Experiments conducted on Corel5k dataset and the newly issued Flickr30Concepts dataset demonstrate the effectiveness and efficiency of the proposed method.



### Training Convolutional Networks with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1406.2080v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1406.2080v4)
- **Published**: 2014-06-09 05:45:12+00:00
- **Updated**: 2015-04-10 16:44:00+00:00
- **Authors**: Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, Rob Fergus
- **Comment**: Accepted as a workshop contribution at ICLR 2015
- **Journal**: None
- **Summary**: The availability of large labeled datasets has allowed Convolutional Network models to achieve impressive recognition results. However, in many settings manual annotation of the data is impractical; instead our data has noisy labels, i.e. there is some freely available label for each image which may or may not be accurate. In this paper, we explore the performance of discriminatively-trained Convnets when trained on such noisy data. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark.



### Log-Euclidean Bag of Words for Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1406.2139v3
- **DOI**: 10.1049/iet-cvi.2014.0018
- **Categories**: **cs.CV**, I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1406.2139v3)
- **Published**: 2014-06-09 11:14:03+00:00
- **Updated**: 2016-07-07 09:27:40+00:00
- **Authors**: Masoud Faraki, Maziar Palhang, Conrad Sanderson
- **Comment**: None
- **Journal**: IET Computer Vision, Vol. 9, No. 3, 2015
- **Summary**: Representing videos by densely extracted local space-time features has recently become a popular approach for analysing actions. In this paper, we tackle the problem of categorising human actions by devising Bag of Words (BoW) models based on covariance matrices of spatio-temporal features, with the features formed from histograms of optical flow. Since covariance matrices form a special type of Riemannian manifold, the space of Symmetric Positive Definite (SPD) matrices, non-Euclidean geometry should be taken into account while discriminating between covariance matrices. To this end, we propose to embed SPD manifolds to Euclidean spaces via a diffeomorphism and extend the BoW approach to its Riemannian version. The proposed BoW approach takes into account the manifold geometry of SPD matrices during the generation of the codebook and histograms. Experiments on challenging human action datasets show that the proposed method obtains notable improvements in discrimination accuracy, in comparison to several state-of-the-art methods.



### Two-Stream Convolutional Networks for Action Recognition in Videos
- **Arxiv ID**: http://arxiv.org/abs/1406.2199v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.2199v2)
- **Published**: 2014-06-09 14:44:14+00:00
- **Updated**: 2014-11-12 20:48:33+00:00
- **Authors**: Karen Simonyan, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework.   Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both.   Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.



### Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1406.2227v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.2227v4)
- **Published**: 2014-06-09 15:53:33+00:00
- **Updated**: 2014-12-09 11:22:59+00:00
- **Authors**: Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we present a framework for the recognition of natural scene text. Our framework does not require any human-labelled data, and performs word recognition on the whole image holistically, departing from the character based recognition systems of the past. The deep neural network models at the centre of this framework are trained solely on data produced by a synthetic text generation engine -- synthetic data that is highly realistic and sufficient to replace real data, giving us infinite amounts of training data. This excess of data exposes new possibilities for word recognition models, and here we consider three models, each one "reading" words in a different way: via 90k-way dictionary encoding, character sequence encoding, and bag-of-N-grams encoding. In the scenarios of language based and completely unconstrained text recognition we greatly improve upon state-of-the-art performance on standard datasets, using our fast, simple machinery and requiring zero data-acquisition costs.



### Robust Estimation of 3D Human Poses from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1406.2282v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.2282v1)
- **Published**: 2014-06-09 18:55:31+00:00
- **Updated**: 2014-06-09 18:55:31+00:00
- **Authors**: Chunyu Wang, Yizhou Wang, Zhouchen Lin, Alan L. Yuille, Wen Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Human pose estimation is a key step to action recognition. We propose a method of estimating 3D human poses from a single image, which works in conjunction with an existing 2D pose/joint detector. 3D pose estimation is challenging because multiple 3D poses may correspond to the same 2D pose after projection due to the lack of depth information. Moreover, current 2D pose estimators are usually inaccurate which may cause errors in the 3D estimation. We address the challenges in three ways: (i) We represent a 3D pose as a linear combination of a sparse set of bases learned from 3D human skeletons. (ii) We enforce limb length constraints to eliminate anthropomorphically implausible skeletons. (iii) We estimate a 3D pose by minimizing the $L_1$-norm error between the projection of the 3D pose and the corresponding 2D detection. The $L_1$-norm loss term is robust to inaccurate 2D joint estimations. We use the alternating direction method (ADM) to solve the optimization problem efficiently. Our approach outperforms the state-of-the-arts on three benchmark datasets.



### Depth Map Prediction from a Single Image using a Multi-Scale Deep Network
- **Arxiv ID**: http://arxiv.org/abs/1406.2283v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.2283v1)
- **Published**: 2014-06-09 19:01:18+00:00
- **Updated**: 2014-06-09 19:01:18+00:00
- **Authors**: David Eigen, Christian Puhrsch, Rob Fergus
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.



### Parsing Semantic Parts of Cars Using Graphical Models and Segment Appearance Consistency
- **Arxiv ID**: http://arxiv.org/abs/1406.2375v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.2375v2)
- **Published**: 2014-06-09 22:16:57+00:00
- **Updated**: 2014-06-11 23:39:41+00:00
- **Authors**: Wenhao Lu, Xiaochen Lian, Alan Yuille
- **Comment**: 12 pages, CBMM memo
- **Journal**: None
- **Summary**: This paper addresses the problem of semantic part parsing (segmentation) of cars, i.e.assigning every pixel within the car to one of the parts (e.g.body, window, lights, license plates and wheels). We formulate this as a landmark identification problem, where a set of landmarks specifies the boundaries of the parts. A novel mixture of graphical models is proposed, which dynamically couples the landmarks to a hierarchy of segments. When modeling pairwise relation between landmarks, this coupling enables our model to exploit the local image contents in addition to spatial deformation, an aspect that most existing graphical models ignore. In particular, our model enforces appearance consistency between segments within the same part. Parsing the car, including finding the optimal coupling between landmarks and segments in the hierarchy, is performed by dynamic programming. We evaluate our method on a subset of PASCAL VOC 2010 car images and on the car subset of 3D Object Category dataset (CAR3D). We show good results and, in particular, quantify the effectiveness of using the segment appearance consistency in terms of accuracy of part localization and segmentation.



### Unsupervised Deep Haar Scattering on Graphs
- **Arxiv ID**: http://arxiv.org/abs/1406.2390v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1406.2390v2)
- **Published**: 2014-06-09 23:51:30+00:00
- **Updated**: 2014-11-03 15:25:16+00:00
- **Authors**: Xu Chen, Xiuyuan Cheng, Stéphane Mallat
- **Comment**: None
- **Journal**: None
- **Summary**: The classification of high-dimensional data defined on graphs is particularly difficult when the graph geometry is unknown. We introduce a Haar scattering transform on graphs, which computes invariant signal descriptors. It is implemented with a deep cascade of additions, subtractions and absolute values, which iteratively compute orthogonal Haar wavelet transforms. Multiscale neighborhoods of unknown graphs are estimated by minimizing an average total variation, with a pair matching algorithm of polynomial complexity. Supervised classification with dimension reduction is tested on data bases of scrambled images, and for signals sampled on unknown irregular grids on a sphere.



