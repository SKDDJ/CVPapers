# Arxiv Papers in cs.CV on 2014-06-20
### Web-Scale Training for Face Identification
- **Arxiv ID**: http://arxiv.org/abs/1406.5266v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.5266v2)
- **Published**: 2014-06-20 02:51:31+00:00
- **Updated**: 2015-04-18 09:18:19+00:00
- **Authors**: Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: Scaling machine learning methods to very large datasets has attracted considerable attention in recent years, thanks to easy access to ubiquitous sensing and data from the web. We study face recognition and show that three distinct properties have surprising effects on the transferability of deep convolutional networks (CNN): (1) The bottleneck of the network serves as an important transfer learning regularizer, and (2) in contrast to the common wisdom, performance saturation may exist in CNN's (as the number of training samples grows); we propose a solution for alleviating this by replacing the naive random subsampling of the training set with a bootstrapping process. Moreover, (3) we find a link between the representation norm and the ability to discriminate in a target domain, which sheds lights on how such networks represent faces. Based on these discoveries, we are able to improve face recognition accuracy on the widely used LFW benchmark, both in the verification (1:1) and identification (1:N) protocols, and directly compare, for the first time, with the state of the art Commercially-Off-The-Shelf system and show a sizable leap in performance.



### Early Recognition of Human Activities from First-Person Videos Using Onset Representations
- **Arxiv ID**: http://arxiv.org/abs/1406.5309v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.5309v2)
- **Published**: 2014-06-20 08:22:09+00:00
- **Updated**: 2015-07-06 08:21:49+00:00
- **Authors**: M. S. Ryoo, Thomas J. Fuchs, Lu Xia, J. K. Aggarwal, Larry Matthies
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a methodology for early recognition of human activities from videos taken with a first-person viewpoint. Early recognition, which is also known as activity prediction, is an ability to infer an ongoing activity at its early stage. We present an algorithm to perform recognition of activities targeted at the camera from streaming videos, making the system to predict intended activities of the interacting person and avoid harmful events before they actually happen. We introduce the novel concept of 'onset' that efficiently summarizes pre-activity observations, and design an approach to consider event history in addition to ongoing video observation for early first-person recognition of activities. We propose to represent onset using cascade histograms of time series gradients, and we describe a novel algorithmic setup to take advantage of onset for early recognition of activities. The experimental results clearly illustrate that the proposed concept of onset enables better/earlier recognition of human activities from first-person videos.



### Playing with Duality: An Overview of Recent Primal-Dual Approaches for Solving Large-Scale Optimization Problems
- **Arxiv ID**: http://arxiv.org/abs/1406.5429v2
- **DOI**: None
- **Categories**: **cs.NA**, cs.CV, cs.LG, math.OC, G.1.6; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1406.5429v2)
- **Published**: 2014-06-20 15:33:00+00:00
- **Updated**: 2014-12-03 20:59:42+00:00
- **Authors**: Nikos Komodakis, Jean-Christophe Pesquet
- **Comment**: None
- **Journal**: None
- **Summary**: Optimization methods are at the core of many problems in signal/image processing, computer vision, and machine learning. For a long time, it has been recognized that looking at the dual of an optimization problem may drastically simplify its solution. Deriving efficient strategies which jointly brings into play the primal and the dual problems is however a more recent idea which has generated many important new contributions in the last years. These novel developments are grounded on recent advances in convex analysis, discrete optimization, parallel processing, and non-smooth optimization with emphasis on sparsity issues. In this paper, we aim at presenting the principles of primal-dual approaches, while giving an overview of numerical methods which have been proposed in different contexts. We show the benefits which can be drawn from primal-dual algorithms both for solving large-scale convex optimization problems and discrete ones, and we provide various application examples to illustrate their usefulness.



### Predicting Motivations of Actions by Leveraging Text
- **Arxiv ID**: http://arxiv.org/abs/1406.5472v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.5472v2)
- **Published**: 2014-06-20 18:02:02+00:00
- **Updated**: 2016-11-30 03:58:15+00:00
- **Authors**: Carl Vondrick, Deniz Oktay, Hamed Pirsiavash, Antonio Torralba
- **Comment**: CVPR 2016
- **Journal**: None
- **Summary**: Understanding human actions is a key problem in computer vision. However, recognizing actions is only the first step of understanding what a person is doing. In this paper, we introduce the problem of predicting why a person has performed an action in images. This problem has many applications in human activity understanding, such as anticipating or explaining an action. To study this problem, we introduce a new dataset of people performing actions annotated with likely motivations. However, the information in an image alone may not be sufficient to automatically solve this task. Since humans can rely on their lifetime of experiences to infer motivation, we propose to give computer vision systems access to some of these experiences by using recently developed natural language models to mine knowledge stored in massive amounts of text. While we are still far away from fully understanding motivation, our results suggest that transferring knowledge from language into vision can help machines understand why people in images might be performing an action.



### Fast Edge Detection Using Structured Forests
- **Arxiv ID**: http://arxiv.org/abs/1406.5549v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.5549v2)
- **Published**: 2014-06-20 22:28:29+00:00
- **Updated**: 2014-11-25 02:49:28+00:00
- **Authors**: Piotr Doll√°r, C. Lawrence Zitnick
- **Comment**: update corresponding to acceptance to PAMI
- **Journal**: None
- **Summary**: Edge detection is a critical component of many vision systems, including object detectors and image segmentation algorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take advantage of the structure present in local image patches to learn both an accurate and computationally efficient edge detector. We formulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated. The result is an approach that obtains realtime performance that is orders of magnitude faster than many competing state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation dataset and NYU Depth dataset. Finally, we show the potential of our approach as a general purpose edge detector by showing our learned edge models generalize well across datasets.



### Caffe: Convolutional Architecture for Fast Feature Embedding
- **Arxiv ID**: http://arxiv.org/abs/1408.5093v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1408.5093v1)
- **Published**: 2014-06-20 23:00:32+00:00
- **Updated**: 2014-06-20 23:00:32+00:00
- **Authors**: Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell
- **Comment**: Tech report for the Caffe software at http://github.com/BVLC/Caffe/
- **Journal**: None
- **Summary**: Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ($\approx$ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.



