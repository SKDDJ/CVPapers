# Arxiv Papers in cs.CV on 2014-06-23
### Further heuristics for $k$-means: The merge-and-split heuristic and the $(k,l)$-means
- **Arxiv ID**: http://arxiv.org/abs/1406.6314v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1406.6314v1)
- **Published**: 2014-06-23 02:34:34+00:00
- **Updated**: 2014-06-23 02:34:34+00:00
- **Authors**: Frank Nielsen, Richard Nock
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Finding the optimal $k$-means clustering is NP-hard in general and many heuristics have been designed for minimizing monotonically the $k$-means objective. We first show how to extend Lloyd's batched relocation heuristic and Hartigan's single-point relocation heuristic to take into account empty-cluster and single-point cluster events, respectively. Those events tend to increasingly occur when $k$ or $d$ increases, or when performing several restarts. First, we show that those special events are a blessing because they allow to partially re-seed some cluster centers while further minimizing the $k$-means objective function. Second, we describe a novel heuristic, merge-and-split $k$-means, that consists in merging two clusters and splitting this merged cluster again with two new centers provided it improves the $k$-means objective. This novel heuristic can improve Hartigan's $k$-means when it has converged to a local minimum. We show empirically that this merge-and-split $k$-means improves over the Hartigan's heuristic which is the {\em de facto} method of choice. Finally, we propose the $(k,l)$-means objective that generalizes the $k$-means objective by associating the data points to their $l$ closest cluster centers, and show how to either directly convert or iteratively relax the $(k,l)$-means into a $k$-means in order to reach better local minima.



### A Unified Quantitative Model of Vision and Audition
- **Arxiv ID**: http://arxiv.org/abs/1406.5807v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC, q-bio.QM, I.5.4; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/1406.5807v1)
- **Published**: 2014-06-23 05:00:31+00:00
- **Updated**: 2014-06-23 05:00:31+00:00
- **Authors**: Peilei Liu, Ting Wang
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: We have put forwards a unified quantitative framework of vision and audition, based on existing data and theories. According to this model, the retina is a feedforward network self-adaptive to inputs in a specific period. After fully grown, cells become specialized detectors based on statistics of stimulus history. This model has provided explanations for perception mechanisms of colour, shape, depth and motion. Moreover, based on this ground we have put forwards a bold conjecture that single ear can detect sound direction. This is complementary to existing theories and has provided better explanations for sound localization.



### VideoSET: Video Summary Evaluation through Text
- **Arxiv ID**: http://arxiv.org/abs/1406.5824v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1406.5824v1)
- **Published**: 2014-06-23 07:56:23+00:00
- **Updated**: 2014-06-23 07:56:23+00:00
- **Authors**: Serena Yeung, Alireza Fathi, Li Fei-Fei
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present VideoSET, a method for Video Summary Evaluation through Text that can evaluate how well a video summary is able to retain the semantic information contained in its original video. We observe that semantics is most easily expressed in words, and develop a text-based approach for the evaluation. Given a video summary, a text representation of the video summary is first generated, and an NLP-based metric is then used to measure its semantic distance to ground-truth text summaries written by humans. We show that our technique has higher agreement with human judgment than pixel-based distance metrics. We also release text annotations and ground-truth text summaries for a number of publicly available video datasets, for use by the computer vision community.



### Multi-utility Learning: Structured-output Learning with Multiple Annotation-specific Loss Functions
- **Arxiv ID**: http://arxiv.org/abs/1406.5910v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1406.5910v1)
- **Published**: 2014-06-23 14:06:24+00:00
- **Updated**: 2014-06-23 14:06:24+00:00
- **Authors**: Roman Shapovalov, Dmitry Vetrov, Anton Osokin, Pushmeet Kohli
- **Comment**: None
- **Journal**: None
- **Summary**: Structured-output learning is a challenging problem; particularly so because of the difficulty in obtaining large datasets of fully labelled instances for training. In this paper we try to overcome this difficulty by presenting a multi-utility learning framework for structured prediction that can learn from training instances with different forms of supervision. We propose a unified technique for inferring the loss functions most suitable for quantifying the consistency of solutions with the given weak annotation. We demonstrate the effectiveness of our framework on the challenging semantic image segmentation problem for which a wide variety of annotations can be used. For instance, the popular training datasets for semantic segmentation are composed of images with hard-to-generate full pixel labellings, as well as images with easy-to-obtain weak annotations, such as bounding boxes around objects, or image-level labels that specify which object categories are present in an image. Experimental evaluation shows that the use of annotation-specific loss functions dramatically improves segmentation accuracy compared to the baseline system where only one type of weak annotation is used.



### Committees of deep feedforward networks trained with few data
- **Arxiv ID**: http://arxiv.org/abs/1406.5947v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1406.5947v1)
- **Published**: 2014-06-23 15:34:54+00:00
- **Updated**: 2014-06-23 15:34:54+00:00
- **Authors**: Bogdan Miclut, Thomas Kaester, Thomas Martinetz, Erhardt Barth
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks are known to give good results on image classification tasks. In this paper we present a method to improve the classification result by combining multiple such networks in a committee. We adopt the STL-10 dataset which has very few training examples and show that our method can achieve results that are better than the state of the art. The networks are trained layer-wise and no backpropagation is used. We also explore the effects of dataset augmentation by mirroring, rotation, and scaling.



