# Arxiv Papers in cs.CV on 2014-06-18
### Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1406.4729v4
- **DOI**: 10.1007/978-3-319-10578-9_23
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.4729v4)
- **Published**: 2014-06-18 14:24:17+00:00
- **Updated**: 2015-04-23 07:33:24+00:00
- **Authors**: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
- **Comment**: This manuscript is the accepted version for IEEE Transactions on
  Pattern Analysis and Machine Intelligence (TPAMI) 2015. See Changelog
- **Journal**: None
- **Summary**: Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image. This requirement is "artificial" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning.   The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102x faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007.   In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.



### Mass Classification Method in Mammogram Using Fuzzy K-Nearest Neighbour Equality
- **Arxiv ID**: http://arxiv.org/abs/1406.4770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.4770v1)
- **Published**: 2014-06-18 15:38:46+00:00
- **Updated**: 2014-06-18 15:38:46+00:00
- **Authors**: I. Laurence Aroquiaraj, K. Thangavel
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: Mass classification of objects is an important area of research and application in a variety of fields. In this paper, we present an efficient computer aided mass classification method in digitized mammograms using Fuzzy K-Nearest Neighbor Equality, which performs benign or malignant classification on region of interest that contains mass. One of the major mammographic characteristics for mass classification is texture. Fuzzy K-Nearest Neighbor Equality exploits this important factor to classify the mass into benign or malignant. The statistical textural features used in characterizing the masses are Haralick and Run length features. The main aim of the method is to increase the effectiveness and efficiency of the classification process in an objective manner to reduce the numbers of false positive of malignancies. In this paper proposes a novel Fuzzy K-Nearest Neighbor Equality algorithm for classifying the marked regions into benign and malignant and 94.46 sensitivity,96.81 specificity and 96.52 accuracy is achieved that is very much promising compare to the radiologists' accuracy.



### Deep Learning Face Representation by Joint Identification-Verification
- **Arxiv ID**: http://arxiv.org/abs/1406.4773v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.4773v1)
- **Published**: 2014-06-18 15:42:16+00:00
- **Updated**: 2014-06-18 15:42:16+00:00
- **Authors**: Yi Sun, Xiaogang Wang, Xiaoou Tang
- **Comment**: None
- **Journal**: None
- **Summary**: The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15% face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67%.



