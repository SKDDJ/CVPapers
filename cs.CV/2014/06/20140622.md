# Arxiv Papers in cs.CV on 2014-06-22
### 3D ShapeNets: A Deep Representation for Volumetric Shapes
- **Arxiv ID**: http://arxiv.org/abs/1406.5670v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.5670v3)
- **Published**: 2014-06-22 03:31:52+00:00
- **Updated**: 2015-04-15 16:46:05+00:00
- **Authors**: Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, Jianxiong Xiao
- **Comment**: to be appeared in CVPR 2015
- **Journal**: None
- **Summary**: 3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representations automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet -- a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.



### Deep Fragment Embeddings for Bidirectional Image Sentence Mapping
- **Arxiv ID**: http://arxiv.org/abs/1406.5679v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1406.5679v1)
- **Published**: 2014-06-22 06:22:50+00:00
- **Updated**: 2014-06-22 06:22:50+00:00
- **Authors**: Andrej Karpathy, Armand Joulin, Li Fei-Fei
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. In addition to a ranking objective seen in previous work, this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions since the inferred inter-modal fragment alignment is explicit.



### Natural Color Image Enhancement based on Modified Multiscale Retinex Algorithm and Performance Evaluation usingWavelet Energy
- **Arxiv ID**: http://arxiv.org/abs/1406.5710v1
- **DOI**: 10.1007/978-3-319-01778-5_9
- **Categories**: **cs.CV**, 68U10, I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/1406.5710v1)
- **Published**: 2014-06-22 11:56:21+00:00
- **Updated**: 2014-06-22 11:56:21+00:00
- **Authors**: M. C Hanumantharaju, M. Ravishankar, D. R Rameshbabu
- **Comment**: 10 pages, 3 figures, Recent Advances in Intelligent Informatics
  Advances in Intelligent Systems and Computing Volume 235, 2014, pp 83-92
- **Journal**: None
- **Summary**: This paper presents a new color image enhancement technique based on modified MultiScale Retinex(MSR) algorithm and visual quality of the enhanced images are evaluated using a new metric, namely, wavelet energy. The color image enhancement is achieved by down sampling the value component of HSV color space converted image into three scales (normal, medium and fine) following the contrast stretching operation. These down sampled value components are enhanced using the MSR algorithm. The value component is reconstructed by averaging each pixels of the lower scale image with that of the upper scale image subsequent to up sampling the lower scale image. This process replaces dark pixel by the average pixels of both the lower scale and upper scale, while retaining the bright pixels. The quality of the reconstructed images in the proposed method is found to be good and far better then the other researchers method. The performance of the proposed scheme is evaluated using new wavelet domain based assessment criterion, referred as wavelet energy. This scheme computes the energy of both original and enhanced image in wavelet domain. The number of edge details as well as wavelet energy is less in a poor quality image compared with naturally enhanced image. Experimental results presented confirms that the proposed wavelet energy based color image quality assessment technique efficiently characterizes both the local and global details of enhanced image.



### CNN: Single-label to Multi-label
- **Arxiv ID**: http://arxiv.org/abs/1406.5726v3
- **DOI**: 10.1109/TPAMI.2015.2491929
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.5726v3)
- **Published**: 2014-06-22 14:03:07+00:00
- **Updated**: 2014-07-09 11:26:56+00:00
- **Authors**: Yunchao Wei, Wei Xia, Junshi Huang, Bingbing Ni, Jian Dong, Yao Zhao, Shuicheng Yan
- **Comment**: 13 pages, 10 figures, 3 tables
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) has demonstrated promising performance in single-label image classification tasks. However, how CNN best copes with multi-label images still remains an open problem, mainly due to the complex underlying object layouts and insufficient multi-label training images. In this work, we propose a flexible deep CNN infrastructure, called Hypotheses-CNN-Pooling (HCP), where an arbitrary number of object segment hypotheses are taken as the inputs, then a shared CNN is connected with each hypothesis, and finally the CNN output results from different hypotheses are aggregated with max pooling to produce the ultimate multi-label predictions. Some unique characteristics of this flexible deep CNN infrastructure include: 1) no ground truth bounding box information is required for training; 2) the whole HCP infrastructure is robust to possibly noisy and/or redundant hypotheses; 3) no explicit hypothesis label is required; 4) the shared CNN may be well pre-trained with a large-scale single-label image dataset, e.g. ImageNet; and 5) it may naturally output multi-label prediction results. Experimental results on Pascal VOC2007 and VOC2012 multi-label image datasets well demonstrate the superiority of the proposed HCP infrastructure over other state-of-the-arts. In particular, the mAP reaches 84.2% by HCP only and 90.3% after the fusion with our complementary result in [47] based on hand-crafted features on the VOC2012 dataset, which significantly outperforms the state-of-the-arts with a large margin of more than 7%.



### Recovery of Images with Missing Pixels using a Gradient Compressive Sensing Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1407.3695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1407.3695v1)
- **Published**: 2014-06-22 20:18:32+00:00
- **Updated**: 2014-06-22 20:18:32+00:00
- **Authors**: Isidora StankoviÄ‡
- **Comment**: 7 pages, 12 figures, A part of the Final year project at the
  University of Westminster, London, United Kingdom, supervised by Dragana
  Barjamovic (submitted 28. April 2014)
- **Journal**: None
- **Summary**: This paper investigates the possibility of reconstruction of images considering that they are sparse in the DCT transformation domain. Two approaches are considered. One when the image is pre-processed in the DCT domain, using 8x8 blocks. The image is made sparse by setting the smallest DCT coefficients to zero. In the other case the original image is considered without pre-processing, assuming the sparsity as intrinsic property of the analyzed image. A gradient based algorithm is used to recover a large number of missing pixels in the image. The case of a salt-and-paper noise affecting a large number of pixels is easily reduced to the case of missing pixels and considered within the same framework. The reconstruction of images affected with salt-and-paper impulsive is compared with the images filtered using a median filter. The same algorithm can be used considering transformation of the whole image. Reconstructions of black and white and colour images are considered.



### Factors of Transferability for a Generic ConvNet Representation
- **Arxiv ID**: http://arxiv.org/abs/1406.5774v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1406.5774v3)
- **Published**: 2014-06-22 21:57:46+00:00
- **Updated**: 2015-07-15 10:02:19+00:00
- **Authors**: Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto Maki, Stefan Carlsson
- **Comment**: Extended version of the workshop paper with more experiments and
  updated text and title. Original CVPR15 DeepVision workshop paper title:
  "From Generic to Specific Deep Representations for Visual Recognition"
- **Journal**: None
- **Summary**: Evidence is mounting that Convolutional Networks (ConvNets) are the most effective representation learning method for visual recognition tasks. In the common scenario, a ConvNet is trained on a large labeled dataset (source) and the feed-forward units activation of the trained network, at a certain layer of the network, is used as a generic representation of an input image for a task with relatively smaller training set (target). Recent studies have shown this form of representation transfer to be suitable for a wide range of target visual recognition tasks. This paper introduces and investigates several factors affecting the transferability of such representations. It includes parameters for training of the source ConvNet such as its architecture, distribution of the training data, etc. and also the parameters of feature extraction such as layer of the trained ConvNet, dimensionality reduction, etc. Then, by optimizing these factors, we show that significant improvements can be achieved on various (17) visual recognition tasks. We further show that these visual recognition tasks can be categorically ordered based on their distance from the source task such that a correlation between the performance of tasks and their distance from the source task w.r.t. the proposed factors is observed.



