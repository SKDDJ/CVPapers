# Arxiv Papers in cs.CV on 2014-03-29
### Scalable Robust Matrix Recovery: Frank-Wolfe Meets Proximal Methods
- **Arxiv ID**: http://arxiv.org/abs/1403.7588v2
- **DOI**: 10.1137/15M101628X
- **Categories**: **math.OC**, cs.CV, cs.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1403.7588v2)
- **Published**: 2014-03-29 04:04:43+00:00
- **Updated**: 2017-05-29 21:16:42+00:00
- **Authors**: Cun Mu, Yuqian Zhang, John Wright, Donald Goldfarb
- **Comment**: None
- **Journal**: SIAM Journal on Scientific Computing, 2016, Vol. 38, No. 5 : pp.
  A3291-A3317
- **Summary**: Recovering matrices from compressive and grossly corrupted observations is a fundamental problem in robust statistics, with rich applications in computer vision and machine learning. In theory, under certain conditions, this problem can be solved in polynomial time via a natural convex relaxation, known as Compressive Principal Component Pursuit (CPCP). However, all existing provable algorithms for CPCP suffer from superlinear per-iteration cost, which severely limits their applicability to large scale problems. In this paper, we propose provable, scalable and efficient methods to solve CPCP with (essentially) linear per-iteration cost. Our method combines classical ideas from Frank-Wolfe and proximal methods. In each iteration, we mainly exploit Frank-Wolfe to update the low-rank component with rank-one SVD and exploit the proximal step for the sparse term. Convergence results and implementation details are also discussed. We demonstrate the scalability of the proposed approach with promising numerical experiments on visual data.



### Building A Large Concept Bank for Representing Events in Video
- **Arxiv ID**: http://arxiv.org/abs/1403.7591v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.IR, H.3.1
- **Links**: [PDF](http://arxiv.org/pdf/1403.7591v1)
- **Published**: 2014-03-29 05:17:29+00:00
- **Updated**: 2014-03-29 05:17:29+00:00
- **Authors**: Yin Cui, Dong Liu, Jiawei Chen, Shih-Fu Chang
- **Comment**: 25 pages, 9 figures
- **Journal**: None
- **Summary**: Concept-based video representation has proven to be effective in complex event detection. However, existing methods either manually design concepts or directly adopt concept libraries not specifically designed for events. In this paper, we propose to build Concept Bank, the largest concept library consisting of 4,876 concepts specifically designed to cover 631 real-world events. To construct the Concept Bank, we first gather a comprehensive event collection from WikiHow, a collaborative writing project that aims to build the world's largest manual for any possible How-To event. For each event, we then search Flickr and discover relevant concepts from the tags of the returned images. We train a Multiple Kernel Linear SVM for each discovered concept as a concept detector in Concept Bank. We organize the concepts into a five-layer tree structure, in which the higher-level nodes correspond to the event categories while the leaf nodes are the event-specific concepts discovered for each event. Based on such tree ontology, we develop a semantic matching method to select relevant concepts for each textual event query, and then apply the corresponding concept detectors to generate concept-based video representations. We use TRECVID Multimedia Event Detection 2013 and Columbia Consumer Video open source event definitions and videos as our test sets and show very promising results on two video event detection tasks: event modeling over concept space and zero-shot event retrieval. To the best of our knowledge, this is the largest concept library covering the largest number of real-world events.



