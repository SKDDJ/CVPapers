# Arxiv Papers in cs.CV on 2014-03-23
### SmartAnnotator: An Interactive Tool for Annotating RGBD Indoor Images
- **Arxiv ID**: http://arxiv.org/abs/1403.5718v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1403.5718v1)
- **Published**: 2014-03-23 03:45:26+00:00
- **Updated**: 2014-03-23 03:45:26+00:00
- **Authors**: Yu-Shiang Wong, Hung-Kuo Chu, Niloy J. Mitra
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: RGBD images with high quality annotations in the form of geometric (i.e., segmentation) and structural (i.e., how do the segments are mutually related in 3D) information provide valuable priors to a large number of scene and image manipulation applications. While it is now simple to acquire RGBD images, annotating them, automatically or manually, remains challenging especially in cluttered noisy environments. We present SmartAnnotator, an interactive system to facilitate annotating RGBD images. The system performs the tedious tasks of grouping pixels, creating potential abstracted cuboids, inferring object interactions in 3D, and comes up with various hypotheses. The user simply has to flip through a list of suggestions for segment labels, finalize a selection, and the system updates the remaining hypotheses. As objects are finalized, the process speeds up with fewer ambiguities to resolve. Further, as more scenes are annotated, the system makes better suggestions based on structural and geometric priors learns from the previous annotation sessions. We test our system on a large number of database scenes and report significant improvements over naive low-level annotation tools.



### CNN Features off-the-shelf: an Astounding Baseline for Recognition
- **Arxiv ID**: http://arxiv.org/abs/1403.6382v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1403.6382v3)
- **Published**: 2014-03-23 13:42:03+00:00
- **Updated**: 2014-05-12 08:53:31+00:00
- **Authors**: Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, Stefan Carlsson
- **Comment**: version 3 revisions: 1)Added results using feature processing and
  data augmentation 2)Referring to most recent efforts of using CNN for
  different visual recognition tasks 3) updated text/caption
- **Journal**: None
- **Summary**: Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.



### MCL-3D: a database for stereoscopic image quality assessment using 2D-image-plus-depth source
- **Arxiv ID**: http://arxiv.org/abs/1405.1403v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1405.1403v1)
- **Published**: 2014-03-23 23:31:49+00:00
- **Updated**: 2014-03-23 23:31:49+00:00
- **Authors**: Rui Song, Hyunsuk Ko, C. C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: A new stereoscopic image quality assessment database rendered using the 2D-image-plus-depth source, called MCL-3D, is described and the performance benchmarking of several known 2D and 3D image quality metrics using the MCL-3D database is presented in this work. Nine image-plus-depth sources are first selected, and a depth image-based rendering (DIBR) technique is used to render stereoscopic image pairs. Distortions applied to either the texture image or the depth image before stereoscopic image rendering include: Gaussian blur, additive white noise, down-sampling blur, JPEG and JPEG-2000 (JP2K) compression and transmission error. Furthermore, the distortion caused by imperfect rendering is also examined. The MCL-3D database contains 693 stereoscopic image pairs, where one third of them are of resolution 1024x728 and two thirds are of resolution 1920x1080. The pair-wise comparison was adopted in the subjective test for user friendliness, and the Mean Opinion Score (MOS) can be computed accordingly. Finally, we evaluate the performance of several 2D and 3D image quality metrics applied to MCL-3D. All texture images, depth images, rendered image pairs in MCL-3D and their MOS values obtained in the subjective test are available to the public (http://mcl.usc.edu/mcl-3d-database/) for future research and development.



