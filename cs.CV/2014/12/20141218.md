# Arxiv Papers in cs.CV on 2014-12-18
### Towards Open World Recognition
- **Arxiv ID**: http://arxiv.org/abs/1412.5687v1
- **DOI**: 10.1109/CVPR.2015.7298799
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.5687v1)
- **Published**: 2014-12-18 00:07:45+00:00
- **Updated**: 2014-12-18 00:07:45+00:00
- **Authors**: Abhijit Bendale, Terrance Boult
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  (2015) 1893 - 1902
- **Summary**: With the of advent rich classification models and high computational power visual recognition systems have found many operational applications. Recognition in the real world poses multiple challenges that are not apparent in controlled lab environments. The datasets are dynamic and novel categories must be continuously detected and then added. At prediction time, a trained system has to deal with myriad unseen categories. Operational systems require minimum down time, even to learn. To handle these operational issues, we present the problem of Open World recognition and formally define it. We prove that thresholding sums of monotonically decreasing functions of distances in linearly transformed feature space can balance "open space risk" and empirical risk. Our theory extends existing algorithms for open world recognition. We present a protocol for evaluation of open world recognition systems. We present the Nearest Non-Outlier (NNO) algorithm which evolves model efficiently, adding object categories incrementally while detecting outliers and managing open space risk. We perform experiments on the ImageNet dataset with 1.2M+ images to validate the effectiveness of our method on large scale visual recognition tasks. NNO consistently yields superior results on open world recognition.



### Decomposition-Based Domain Adaptation for Real-World Font Recognition
- **Arxiv ID**: http://arxiv.org/abs/1412.5758v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.5758v4)
- **Published**: 2014-12-18 08:51:15+00:00
- **Updated**: 2015-04-01 22:02:59+00:00
- **Authors**: Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem Agarwala, Jonathan Brandt, Thomas S. Huang
- **Comment**: This paper has been withdrawn by the author due to project concerns
- **Journal**: None
- **Summary**: We present a domain adaption framework to address a domain mismatch between synthetic training and real-world testing data. We demonstrate our method on a challenging fine-grain classification problem: recognizing a font style from an image of text. In this task, it is very easy to generate lots of rendered font examples but very hard to obtain real-world labeled images. This real-to-synthetic domain gap caused poor generalization to new real data in previous font recognition methods (Chen et al. (2014)). In this paper, we introduce a Convolutional Neural Network decomposition approach, leveraging a large training corpus of synthetic data to obtain effective features for classification. This is done using an adaptation technique based on a Stacked Convolutional Auto-Encoder that exploits a large collection of unlabeled real-world text images combined with synthetic data preprocessed in a specific way. The proposed DeepFont method achieves an accuracy of higher than 80% (top-5) on a new large labeled real-world dataset we collected.



### Image Dynamic Range Enhancement in the Context of Logarithmic Models
- **Arxiv ID**: http://arxiv.org/abs/1412.5764v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.5764v1)
- **Published**: 2014-12-18 09:09:17+00:00
- **Updated**: 2014-12-18 09:09:17+00:00
- **Authors**: Vasile Patrascu, Vasile Buzuloiu
- **Comment**: The 11th European Signal Processing Conference, EUSIPCO 2002,
  Toulouse, France, 03-06 september 2002
- **Journal**: None
- **Summary**: Images of a scene observed under a variable illumination or with a variable optical aperture are not identical. Does a privileged representant exist? In which mathematical context? How to obtain it? The authors answer to such questions in the context of logarithmic models for images. After a short presentation of the model, the paper presents two image transforms: one performs an optimal enhancement of the dynamic range, and the other does the same for the mean dynamic range. Experimental results are shown.



### Gray level image enhancement using the Bernstein polynomials
- **Arxiv ID**: http://arxiv.org/abs/1412.5769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.5769v1)
- **Published**: 2014-12-18 09:19:50+00:00
- **Updated**: 2014-12-18 09:19:50+00:00
- **Authors**: Vasile Patrascu
- **Comment**: Scientific Bulletin of the Politechnica, University of
  Timisoara,Transactions on Electronics and Communications, Vol. 47 (61), No:
  2,pp.121-126, June 2002
- **Journal**: None
- **Summary**: This paper presents a method for enhancing the gray level images. This presented method takes part from the category of point operations and it is based on piecewise linear functions. The interpolation nodes of these functions are calculated using the Bernstein polynomials.



### Gray Level Image Enhancement Using Polygonal Functions
- **Arxiv ID**: http://arxiv.org/abs/1412.5787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.5787v1)
- **Published**: 2014-12-18 10:12:49+00:00
- **Updated**: 2014-12-18 10:12:49+00:00
- **Authors**: Vasile Patrascu
- **Comment**: The 13th International Conference on Automation, Quality and Testing,
  Robotics, Vol. Robotics, Image and Signal processing, pp. 129-134, May 23-25
  2002, Cluj-Napoca, Romania
- **Journal**: None
- **Summary**: This paper presents a method for enhancing the gray level images. This method takes part from the category of point transforms and it is based on interpolation functions. The latter have a graphic represented by polygonal lines. The interpolation nodes of these functions are calculated taking into account the statistics of gray levels belonging to the image.



### Image enhancement using the mean dynamic range maximization with logarithmic operations
- **Arxiv ID**: http://arxiv.org/abs/1412.6092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.6092v1)
- **Published**: 2014-12-18 10:19:02+00:00
- **Updated**: 2014-12-18 10:19:02+00:00
- **Authors**: Vasile Patrascu
- **Comment**: Periodica Politechnica, Transactions on Automatic Control and
  Computer Science, Vol.47 (61), 2002, ISSN 1224-600X, pp. 121-126, Timisoara,
  Romania. arXiv admin note: text overlap with arXiv:1412.5764
- **Journal**: None
- **Summary**: In this paper we use a logarithmic model for gray level image enhancement. We begin with a short presentation of the model and then, we propose a new formula for the mean dynamic range. After that we present two image transforms: one performs an optimal enhancement of the mean dynamic range using the logarithmic addition, and the other does the same for positive and negative values using the logarithmic scalar multiplication. We present the comparison of the results obtained by dynamic ranges optimization with the results obtained using classical image enhancement methods like gamma correction and histogram equalization.



### Image Enhancement Using a Generalization of Homographic Function
- **Arxiv ID**: http://arxiv.org/abs/1412.5796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.5796v1)
- **Published**: 2014-12-18 10:32:57+00:00
- **Updated**: 2014-12-18 10:32:57+00:00
- **Authors**: Vasile Patrascu
- **Comment**: The IEEE International Conference COMMUNICATIONS 2002, pp. 429-434,
  December 5-7, 2002, Bucharest, Romania
- **Journal**: None
- **Summary**: This paper presents a new method of gray level image enhancement, based on point transforms. In order to define the transform function, it was used a generalization of the homographic function.



### Contour Detection Using Contrast Formulas in the Framework of Logarithmic Models
- **Arxiv ID**: http://arxiv.org/abs/1412.5802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.5802v1)
- **Published**: 2014-12-18 10:59:09+00:00
- **Updated**: 2014-12-18 10:59:09+00:00
- **Authors**: Vasile Patrascu
- **Comment**: The 8th International Conference, Exhibition on Optimization of
  Electrical and Electronic Equipment, OPTIM 2002, Vol III, pp 751-756, 16 - 17
  May 2002, Brasov, Romania
- **Journal**: None
- **Summary**: In this paper we use a new logarithmic model of image representation, developed in [1,2], for edge detection. In fact, in the framework of the new model we obtain the formulas for computing the "contrast of a pixel" and the "contrast" image is just the "contour" or edge image. In our setting the range of values is preserved and the quality of the contour is good for high as well as for low luminosity regions. We present the comparison of our results with the results using classical edge detection operators.



### Minimizing the Number of Matching Queries for Object Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1412.5808v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.5808v3)
- **Published**: 2014-12-18 11:20:39+00:00
- **Updated**: 2015-08-18 07:08:05+00:00
- **Authors**: Johannes Niedermayer, Peer Kr√∂ger
- **Comment**: None
- **Journal**: None
- **Summary**: To increase the computational efficiency of interest-point based object retrieval, researchers have put remarkable research efforts into improving the efficiency of kNN-based feature matching, pursuing to match thousands of features against a database within fractions of a second. However, due to the high-dimensional nature of image features that reduces the effectivity of index structures (curse of dimensionality), due to the vast amount of features stored in image databases (images are often represented by up to several thousand features), this ultimate goal demanded to trade query runtimes for query precision. In this paper we address an approach complementary to indexing in order to improve the runtimes of retrieval by querying only the most promising keypoint descriptors, as this affects matching runtimes linearly and can therefore lead to increased efficiency. As this reduction of kNN queries reduces the number of tentative correspondences, a loss of query precision is minimized by an additional image-level correspondence generation stage with a computational performance independent of the underlying indexing structure. We evaluate such an adaption of the standard recognition pipeline on a variety of datasets using both SIFT and state-of-the-art binary descriptors. Our results suggest that decreasing the number of queried descriptors does not necessarily imply a reduction in the result quality as long as alternative ways of increasing query recall (by thoroughly selecting k) and MAP (using image-level correspondence generation) are considered.



### Deep Structured Output Learning for Unconstrained Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1412.5903v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.5903v5)
- **Published**: 2014-12-18 15:49:46+00:00
- **Updated**: 2015-04-10 15:36:01+00:00
- **Authors**: Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman
- **Comment**: arXiv admin note: text overlap with arXiv:1406.2227
- **Journal**: None
- **Summary**: We develop a representation suitable for the unconstrained recognition of words in natural images: the general case of no fixed lexicon and unknown length.   To this end we propose a convolutional neural network (CNN) based architecture which incorporates a Conditional Random Field (CRF) graphical model, taking the whole word image as a single input. The unaries of the CRF are provided by a CNN that predicts characters at each position of the output, while higher order terms are provided by another CNN that detects the presence of N-grams. We show that this entire model (CRF, character predictor, N-gram predictor) can be jointly optimised by back-propagating the structured output loss, essentially requiring the system to perform multi-task learning, and training uses purely synthetically generated data. The resulting model is a more accurate system on standard real-world text recognition benchmarks than character prediction alone, setting a benchmark for systems that have not been trained on a particular lexicon. In addition, our model achieves state-of-the-art accuracy in lexicon-constrained scenarios, without being specifically modelled for constrained recognition. To test the generalisation of our model, we also perform experiments with random alpha-numeric strings to evaluate the method when no visual language model is applicable.



### Unsupervised Learning of Spatiotemporally Coherent Metrics
- **Arxiv ID**: http://arxiv.org/abs/1412.6056v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.6056v6)
- **Published**: 2014-12-18 20:31:56+00:00
- **Updated**: 2015-09-08 18:39:03+00:00
- **Authors**: Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, Yann LeCun
- **Comment**: To appear at ICCV2015
- **Journal**: None
- **Summary**: Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric.



### Fractional Max-Pooling
- **Arxiv ID**: http://arxiv.org/abs/1412.6071v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.6071v4)
- **Published**: 2014-12-18 20:45:11+00:00
- **Updated**: 2015-05-12 06:36:11+00:00
- **Authors**: Benjamin Graham
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional networks almost always incorporate some form of spatial pooling, and very often it is alpha times alpha max-pooling with alpha=2. Max-pooling act on the hidden layers of the network, reducing their size by an integer multiplicative factor alpha. The amazing by-product of discarding 75% of your data is that you build into the network a degree of invariance with respect to translations and elastic distortions. However, if you simply alternate convolutional layers with max-pooling layers, performance is limited due to the rapid reduction in spatial size, and the disjoint nature of the pooling regions. We have formulated a fractional version of max-pooling where alpha is allowed to take non-integer values. Our version of max-pooling is stochastic as there are lots of different ways of constructing suitable pooling regions. We find that our form of fractional max-pooling reduces overfitting on a variety of datasets: for instance, we improve on the state-of-the art for CIFAR-100 without even using dropout.



### Compressing Deep Convolutional Networks using Vector Quantization
- **Arxiv ID**: http://arxiv.org/abs/1412.6115v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.6115v1)
- **Published**: 2014-12-18 21:09:01+00:00
- **Updated**: 2014-12-18 21:09:01+00:00
- **Authors**: Yunchao Gong, Liu Liu, Ming Yang, Lubomir Bourdev
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classification and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classification task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1% loss of classification accuracy using the state-of-the-art CNN.



### Semantic Part Segmentation using Compositional Model combining Shape and Appearance
- **Arxiv ID**: http://arxiv.org/abs/1412.6124v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.6124v1)
- **Published**: 2014-12-18 21:27:38+00:00
- **Updated**: 2014-12-18 21:27:38+00:00
- **Authors**: Jianyu Wang, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of semantic part segmentation for animals. This is more challenging than standard object detection, object segmentation and pose estimation tasks because semantic parts of animals often have similar appearance and highly varying shapes. To tackle these challenges, we build a mixture of compositional models to represent the object boundary and the boundaries of semantic parts. And we incorporate edge, appearance, and semantic part cues into the compositional model. Given part-level segmentation annotation, we develop a novel algorithm to learn a mixture of compositional models under various poses and viewpoints for certain animal classes. Furthermore, a linear complexity algorithm is offered for efficient inference of the compositional model using dynamic programming. We evaluate our method for horse and cow using a newly annotated dataset on Pascal VOC 2010 which has pixelwise part labels. Experimental results demonstrate the effectiveness of our method.



### Data Representation using the Weyl Transform
- **Arxiv ID**: http://arxiv.org/abs/1412.6134v5
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1412.6134v5)
- **Published**: 2014-12-18 21:34:40+00:00
- **Updated**: 2015-07-21 14:12:37+00:00
- **Authors**: Qiang Qiu, Andrew Thompson, Robert Calderbank, Guillermo Sapiro
- **Comment**: None
- **Journal**: None
- **Summary**: The Weyl transform is introduced as a rich framework for data representation. Transform coefficients are connected to the Walsh-Hadamard transform of multiscale autocorrelations, and different forms of dyadic periodicity in a signal are shown to appear as different features in its Weyl coefficients. The Weyl transform has a high degree of symmetry with respect to a large group of multiscale transformations, which allows compact yet discriminative representations to be obtained by pooling coefficients. The effectiveness of the Weyl transform is demonstrated through the example of textured image classification.



### Automated Objective Surgical Skill Assessment in the Operating Room Using Unstructured Tool Motion
- **Arxiv ID**: http://arxiv.org/abs/1412.6163v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.6163v1)
- **Published**: 2014-12-18 22:09:22+00:00
- **Updated**: 2014-12-18 22:09:22+00:00
- **Authors**: Piyush Poddar, Narges Ahmidi, S. Swaroop Vedula, Lisa Ishii, Gregory D. Hager, Masaru Ishii
- **Comment**: None
- **Journal**: None
- **Summary**: Previous work on surgical skill assessment using intraoperative tool motion in the operating room (OR) has focused on highly-structured surgical tasks such as cholecystectomy. Further, these methods only considered generic motion metrics such as time and number of movements, which are of limited instructive value. In this paper, we developed and evaluated an automated approach to the surgical skill assessment of nasal septoplasty in the OR. The obstructed field of view and highly unstructured nature of septoplasty precludes trainees from efficiently learning the procedure. We propose a descriptive structure of septoplasty consisting of two types of activity: (1) brushing activity directed away from the septum plane characterizing the consistency of the surgeon's wrist motion and (2) activity along the septal plane characterizing the surgeon's coverage pattern. We derived features related to these two activity types that classify a surgeon's level of training with an average accuracy of about 72%. The features we developed provide surgeons with personalized, actionable feedback regarding their tool motion.



