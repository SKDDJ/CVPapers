# Arxiv Papers in cs.CV on 2014-12-02
### Learning Spatiotemporal Features with 3D Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1412.0767v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.0767v4)
- **Published**: 2014-12-02 03:05:54+00:00
- **Updated**: 2015-10-07 01:29:12+00:00
- **Authors**: Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.



### Feedforward semantic segmentation with zoom-out features
- **Arxiv ID**: http://arxiv.org/abs/1412.0774v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.0774v1)
- **Published**: 2014-12-02 03:31:51+00:00
- **Updated**: 2014-12-02 03:31:51+00:00
- **Authors**: Mohammadreza Mostajabi, Payman Yadollahpour, Gregory Shakhnarovich
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a purely feed-forward architecture for semantic segmentation. We map small image elements (superpixels) to rich feature representations extracted from a sequence of nested regions of increasing extent. These regions are obtained by "zooming out" from the superpixel all the way to scene-level resolution. This approach exploits statistical structure in the image and in the label space without setting up explicit structured prediction mechanisms, and thus avoids complex and expensive inference. Instead superpixels are classified by a feedforward multilayer network. Our architecture achieves new state of the art performance in semantic segmentation, obtaining 64.4% average accuracy on the PASCAL VOC 2012 test set.



### Fast Steerable Principal Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/1412.0781v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.0781v5)
- **Published**: 2014-12-02 04:24:03+00:00
- **Updated**: 2015-12-15 19:26:37+00:00
- **Authors**: Zhizhen Zhao, Yoel Shkolnisky, Amit Singer
- **Comment**: None
- **Journal**: None
- **Summary**: Cryo-electron microscopy nowadays often requires the analysis of hundreds of thousands of 2D images as large as a few hundred pixels in each direction. Here we introduce an algorithm that efficiently and accurately performs principal component analysis (PCA) for a large set of two-dimensional images, and, for each image, the set of its uniform rotations in the plane and their reflections. For a dataset consisting of $n$ images of size $L \times L$ pixels, the computational complexity of our algorithm is $O(nL^3 + L^4)$, while existing algorithms take $O(nL^4)$. The new algorithm computes the expansion coefficients of the images in a Fourier-Bessel basis efficiently using the non-uniform fast Fourier transform. We compare the accuracy and efficiency of the new algorithm with traditional PCA and existing algorithms for steerable PCA.



### Analytical Comparison of Noise Reduction Filters for Image Restoration Using SNR Estimation
- **Arxiv ID**: http://arxiv.org/abs/1412.0801v1
- **DOI**: 10.14445/22312803/IJCTT-V17P123
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.0801v1)
- **Published**: 2014-12-02 07:35:27+00:00
- **Updated**: 2014-12-02 07:35:27+00:00
- **Authors**: Poorna Banerjee Dasgupta
- **Comment**: 4 pages,Published with International Journal of Computer Trends and
  Technology (IJCTT), volume 17 number 3, Nov 2014
- **Journal**: International Journal of Computer Trends and Technology (IJCTT)
  V17(3):121-124, Nov 2014. ISSN:2231-2803. www.ijcttjournal.org
- **Summary**: Noise removal from images is a part of image restoration in which we try to reconstruct or recover an image that has been degraded by using apriori knowledge of the degradation phenomenon. Noises present in images can be of various types with their characteristic Probability Distribution Functions (PDF). Noise removal techniques depend on the kind of noise present in the image rather than on the image itself. This paper explores the effects of applying noise reduction filters having similar properties on noisy images with emphasis on Signal-to-Noise-Ratio (SNR) value estimation for comparing the results.



### Hashing on Nonlinear Manifolds
- **Arxiv ID**: http://arxiv.org/abs/1412.0826v1
- **DOI**: 10.1109/TIP.2015.2405340
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.0826v1)
- **Published**: 2014-12-02 09:36:12+00:00
- **Updated**: 2014-12-02 09:36:12+00:00
- **Authors**: Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, Zhenmin Tang, Heng Tao Shen
- **Comment**: 13 pages. arXiv admin note: text overlap with arXiv:1303.7043
- **Journal**: None
- **Summary**: Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes preserving the Euclidean similarity in the original space. Manifold learning techniques, in contrast, are better able to model the intrinsic structure embedded in the original high-dimensional data. The complexities of these models, and the problems with out-of-sample data, have previously rendered them unsuitable for application to large-scale embedding, however. In this work, how to learn compact binary embeddings on their intrinsic manifolds is considered. In order to address the above-mentioned difficulties, an efficient, inductive solution to the out-of-sample data problem, and a process by which non-parametric manifold learning may be used as the basis of a hashing method is proposed. The proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. It is particularly shown that hashing on the basis of t-SNE outperforms state-of-the-art hashing methods on large-scale benchmark datasets, and is very effective for image classification with very short code lengths. The proposed hashing framework is shown to be easily improved, for example, by minimizing the quantization error with learned orthogonal rotations. In addition, a supervised inductive manifold hashing framework is developed by incorporating the label information, which is shown to greatly advance the semantic retrieval performance.



### Covariance estimation using conjugate gradient for 3D classification in Cryo-EM
- **Arxiv ID**: http://arxiv.org/abs/1412.0985v3
- **DOI**: 10.1109/ISBI.2015.7163849
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.0985v3)
- **Published**: 2014-12-02 17:18:13+00:00
- **Updated**: 2015-02-11 17:51:12+00:00
- **Authors**: Joakim And√©n, Eugene Katsevich, Amit Singer
- **Comment**: None
- **Journal**: None
- **Summary**: Classifying structural variability in noisy projections of biological macromolecules is a central problem in Cryo-EM. In this work, we build on a previous method for estimating the covariance matrix of the three-dimensional structure present in the molecules being imaged. Our proposed method allows for incorporation of contrast transfer function and non-uniform distribution of viewing angles, making it more suitable for real-world data. We evaluate its performance on a synthetic dataset and an experimental dataset obtained by imaging a 70S ribosome complex.



### DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection
- **Arxiv ID**: http://arxiv.org/abs/1412.1123v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.1123v3)
- **Published**: 2014-12-02 22:35:48+00:00
- **Updated**: 2015-04-23 13:58:39+00:00
- **Authors**: Gedas Bertasius, Jianbo Shi, Lorenzo Torresani
- **Comment**: Accepted to CVPR 2015
- **Journal**: None
- **Summary**: Contour detection has been a fundamental component in many image segmentation and object detection systems. Most previous work utilizes low-level features such as texture or saliency to detect contours and then use them as cues for a higher-level task such as object detection. However, we claim that recognizing objects and predicting contours are two mutually related tasks. Contrary to traditional approaches, we show that we can invert the commonly established pipeline: instead of detecting contours with low-level cues for a higher-level recognition task, we exploit object-related features as high-level cues for contour detection.   We achieve this goal by means of a multi-scale deep network that consists of five convolutional layers and a bifurcated fully-connected sub-network. The section from the input layer to the fifth convolutional layer is fixed and directly lifted from a pre-trained network optimized over a large-scale object classification task. This section of the network is applied to four different scales of the image input. These four parallel and identical streams are then attached to a bifurcated sub-network consisting of two independently-trained branches. One branch learns to predict the contour likelihood (with a classification objective) whereas the other branch is trained to learn the fraction of human labelers agreeing about the contour presence at a given point (with a regression criterion).   We show that without any feature engineering our multi-scale deep learning approach achieves state-of-the-art results in contour detection.



### Detector Discovery in the Wild: Joint Multiple Instance and Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1412.1135v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.1135v1)
- **Published**: 2014-12-02 23:32:34+00:00
- **Updated**: 2014-12-02 23:32:34+00:00
- **Authors**: Judy Hoffman, Deepak Pathak, Trevor Darrell, Kate Saenko
- **Comment**: None
- **Journal**: Computer Vision and Pattern Recognition (CVPR) 2015
- **Summary**: We develop methods for detector learning which exploit joint training over both weak and strong labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. Previous methods for weak-label learning often learn detector models independently using latent variable optimization, but fail to share deep representation knowledge across classes and usually require strong initialization. Other previous methods transfer deep representations from domains with strong labels to those with only weak labels, but do not optimize over individual latent boxes, and thus may miss specific salient structures for a particular category. We propose a model that subsumes these previous approaches, and simultaneously trains a representation and detectors for categories with either weak or strong labels present. We provide a novel formulation of a joint multiple instance learning method that includes examples from classification-style data when available, and also performs domain transfer learning to improve the underlying detector representation. Our model outperforms known methods on ImageNet-200 detection with weak labels.



