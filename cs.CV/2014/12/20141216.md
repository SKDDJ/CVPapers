# Arxiv Papers in cs.CV on 2014-12-16
### Boltzmann-Machine Learning of Prior Distributions of Binarized Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1412.7012v4
- **DOI**: 10.7566/JPSJ.85.114803
- **Categories**: **stat.ML**, cond-mat.dis-nn, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1412.7012v4)
- **Published**: 2014-12-16 04:41:09+00:00
- **Updated**: 2016-10-24 03:20:38+00:00
- **Authors**: Tomoyuki Obuchi, Hirokazu Koma, Muneki Yasuda
- **Comment**: 32 pages, 33 figures
- **Journal**: J. Phys. Soc. Jpn. 85 (2016) 114803
- **Summary**: Prior distributions of binarized natural images are learned by using a Boltzmann machine. According the results of this study, there emerges a structure with two sublattices in the interactions, and the nearest-neighbor and next-nearest-neighbor interactions correspondingly take two discriminative values, which reflects the individual characteristics of the three sets of pictures that we process. Meanwhile, in a longer spatial scale, a longer-range, although still rapidly decaying, ferromagnetic interaction commonly appears in all cases. The characteristic length scale of the interactions is universally up to approximately four lattice spacings $\xi \approx 4$. These results are derived by using the mean-field method, which effectively reduces the computational time required in a Boltzmann machine. An improved mean-field method called the Bethe approximation also gives the same results, as well as the Monte Carlo method does for small size images. These reinforce the validity of our analysis and findings. Relations to criticality, frustration, and simple-cell receptive fields are also discussed.



### Discovering beautiful attributes for aesthetic image analysis
- **Arxiv ID**: http://arxiv.org/abs/1412.4940v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.4940v1)
- **Published**: 2014-12-16 10:23:17+00:00
- **Updated**: 2014-12-16 10:23:17+00:00
- **Authors**: Luca Marchesotti, Naila Murray, Florent Perronnin
- **Comment**: IJCV, 2014
- **Journal**: None
- **Summary**: Aesthetic image analysis is the study and assessment of the aesthetic properties of images. Current computational approaches to aesthetic image analysis either provide accurate or interpretable results. To obtain both accuracy and interpretability by humans, we advocate the use of learned and nameable visual attributes as mid-level features. For this purpose, we propose to discover and learn the visual appearance of attributes automatically, using a recently introduced database, called AVA, which contains more than 250,000 images together with their aesthetic scores and textual comments given by photography enthusiasts. We provide a detailed analysis of these annotations as well as the context in which they were given. We then describe how these three key components of AVA - images, scores, and comments - can be effectively leveraged to learn visual attributes. Lastly, we show that these learned attributes can be successfully used in three applications: aesthetic quality prediction, image tagging and retrieval.



### Efficient GPU Implementation for Single Block Orthogonal Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1412.4944v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1412.4944v1)
- **Published**: 2014-12-16 10:39:23+00:00
- **Updated**: 2014-12-16 10:39:23+00:00
- **Authors**: Paul Irofti
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Dictionary training for sparse representations involves dealing with large chunks of data and complex algorithms that determine time consuming implementations. SBO is an iterative dictionary learning algorithm based on constructing unions of orthonormal bases via singular value decomposition, that represents each data item through a single best fit orthobase. In this paper we present a GPGPU approach of implementing SBO in OpenCL. We provide a lock-free solution that ensures full-occupancy of the GPU by following the map-reduce model for the sparse-coding stage and by making use of the Partitioned Global Address Space (PGAS) model for developing parallel dictionary updates. The resulting implementation achieves a favourable trade-off between algorithm complexity and data representation quality compared to PAK-SVD which is the standard overcomplete dictionary learning approach. We present and discuss numerical results showing a significant acceleration of the execution time for the dictionary learning process.



### Random Forests Can Hash
- **Arxiv ID**: http://arxiv.org/abs/1412.5083v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1412.5083v3)
- **Published**: 2014-12-16 17:02:18+00:00
- **Updated**: 2015-04-17 01:00:24+00:00
- **Authors**: Qiang Qiu, Guillermo Sapiro, Alex Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: Hash codes are a very efficient data representation needed to be able to cope with the ever growing amounts of data. We introduce a random forest semantic hashing scheme with information-theoretic code aggregation, showing for the first time how random forest, a technique that together with deep learning have shown spectacular results in classification, can also be extended to large-scale retrieval. Traditional random forest fails to enforce the consistency of hashes generated from each tree for the same class data, i.e., to preserve the underlying similarity, and it also lacks a principled way for code aggregation across trees. We start with a simple hashing scheme, where independently trained random trees in a forest are acting as hashing functions. We the propose a subspace model as the splitting function, and show that it enforces the hash consistency in a tree for data from the same class. We also introduce an information-theoretic approach for aggregating codes of individual trees into a single hash code, producing a near-optimal unique hash for each class. Experiments on large-scale public datasets are presented, showing that the proposed approach significantly outperforms state-of-the-art hashing methods for retrieval tasks.



### Locally Scale-Invariant Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1412.5104v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.5104v1)
- **Published**: 2014-12-16 18:09:34+00:00
- **Updated**: 2014-12-16 18:09:34+00:00
- **Authors**: Angjoo Kanazawa, Abhishek Sharma, David Jacobs
- **Comment**: Deep Learning and Representation Learning Workshop: NIPS 2014
- **Journal**: None
- **Summary**: Convolutional Neural Networks (ConvNets) have shown excellent results on many visual classification tasks. With the exception of ImageNet, these datasets are carefully crafted such that objects are well-aligned at similar scales. Naturally, the feature learning problem gets more challenging as the amount of variation in the data increases, as the models have to learn to be invariant to certain changes in appearance. Recent results on the ImageNet dataset show that given enough data, ConvNets can learn such invariances producing very discriminative features [1]. But could we do more: use less parameters, less data, learn more discriminative features, if certain invariances were built into the learning process? In this paper we present a simple model that allows ConvNets to learn features in a locally scale-invariant manner without increasing the number of model parameters. We show on a modified MNIST dataset that when faced with scale variation, building in scale-invariance allows ConvNets to learn more discriminative features with reduced chances of over-fitting.



### A Robust Regression Approach for Background/Foreground Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1412.5126v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.5126v2)
- **Published**: 2014-12-16 19:02:46+00:00
- **Updated**: 2015-09-01 17:26:02+00:00
- **Authors**: Shervin Minaee, Haoping Yu, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Background/foreground segmentation has a lot of applications in image and video processing. In this paper, a segmentation algorithm is proposed which is mainly designed for text and line extraction in screen content. The proposed method makes use of the fact that the background in each block is usually smoothly varying and can be modeled well by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics create sharp discontinuity. The algorithm separates the background and foreground pixels by trying to fit pixel values in the block into a smooth function using a robust regression method. The inlier pixels that can fit well will be considered as background, while remaining outlier pixels will be considered foreground. This algorithm has been extensively tested on several images from HEVC standard test sequences for screen content coding, and is shown to have superior performance over other methods, such as the k-means clustering based segmentation algorithm in DjVu. This background/foreground segmentation can be used in different applications such as: text extraction, separate coding of background and foreground for compression of screen content and mixed content documents, principle line extraction from palmprint and crease detection in fingerprint images.



