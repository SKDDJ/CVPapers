# Arxiv Papers in cs.CV on 2014-12-08
### An Approach for Reducing Outliers of Non Local Means Image Denoising Filter
- **Arxiv ID**: http://arxiv.org/abs/1412.2444v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.2444v2)
- **Published**: 2014-12-08 04:43:00+00:00
- **Updated**: 2014-12-09 15:23:38+00:00
- **Authors**: Raka Kundu, Amlan Chakrabarti, Prasanna Lenka
- **Comment**: The paper presents an improvement in denoising algorithm for
  ultrasound images using the filter non-local means. The paper is accepted in
  MedImage2014 (IISc Bangalore). The research was supported by Centre of
  Excellence in Systems Biology and Biomedical Engineering (TEQIP PHASE-II),
  University of Calcutta and National Institute for the Orthopaedically
  Handicapped, Kolkata, India
- **Journal**: None
- **Summary**: We propose an adaptive approach for non local means (NLM) image filtering termed as non local adaptive clipped means (NLACM), which reduces the effect of outliers and improves the denoising quality as compared to traditional NLM. Common method to neglect outliers from a data population is computation of mean in a range defined by mean and standard deviation. In NLACM we perform the median within the defined range based on statistical estimation of the neighbourhood region of a pixel to be denoised. As parameters of the range are independent of any additional input and is based on local intensity values, hence the approach is adaptive. Experimental results for NLACM show better estimation of true intensity from noisy neighbourhood observation as compared to NLM at high noise levels. We have verified the technique for speckle noise reduction and we have tested it on ultrasound (US) image of lumbar spine. These ultrasound images act as guidance for injection therapy for treatment of lumbar radiculopathy. We believe that the proposed approach for image denoising is first of its kind and its efficiency can be well justified as it shows better performance in image restoration.



### Web image annotation by diffusion maps manifold learning algorithm
- **Arxiv ID**: http://arxiv.org/abs/1412.3352v1
- **DOI**: 10.5121/ijfcst.2014.4606
- **Categories**: **cs.CV**, cs.IR, cs.LG, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1412.3352v1)
- **Published**: 2014-12-08 10:38:28+00:00
- **Updated**: 2014-12-08 10:38:28+00:00
- **Authors**: Neda Pourali
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Automatic image annotation is one of the most challenging problems in machine vision areas. The goal of this task is to predict number of keywords automatically for images captured in real data. Many methods are based on visual features in order to calculate similarities between image samples. But the computation cost of these approaches is very high. These methods require many training samples to be stored in memory. To lessen this burden, a number of techniques have been developed to reduce the number of features in a dataset. Manifold learning is a popular approach to nonlinear dimensionality reduction. In this paper, we investigate Diffusion maps manifold learning method for web image auto-annotation task. Diffusion maps manifold learning method is used to reduce the dimension of some visual features. Extensive experiments and analysis on NUS-WIDE-LITE web image dataset with different visual features show how this manifold learning dimensionality reduction method can be applied effectively to image annotation.



### Actions and Attributes from Wholes and Parts
- **Arxiv ID**: http://arxiv.org/abs/1412.2604v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.2604v2)
- **Published**: 2014-12-08 15:28:21+00:00
- **Updated**: 2015-05-05 20:41:37+00:00
- **Authors**: Georgia Gkioxari, Ross Girshick, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate the importance of parts for the tasks of action and attribute classification. We develop a part-based approach by leveraging convolutional network features inspired by recent advances in computer vision. Our part detectors are a deep version of poselets and capture parts of the human body under a distinct set of poses. For the tasks of action and attribute classification, we train holistic convolutional neural networks and show that adding parts leads to top-performing results for both tasks. In addition, we demonstrate the effectiveness of our approach when we replace an oracle person detector, as is the default in the current evaluation protocol for both tasks, with a state-of-the-art person detection system.



### When Computer Vision Gazes at Cognition
- **Arxiv ID**: http://arxiv.org/abs/1412.2672v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1412.2672v1)
- **Published**: 2014-12-08 17:25:57+00:00
- **Updated**: 2014-12-08 17:25:57+00:00
- **Authors**: Tao Gao, Daniel Harari, Joshua Tenenbaum, Shimon Ullman
- **Comment**: Tao Gao and Daniel Harari contributed equally to this work
- **Journal**: None
- **Summary**: Joint attention is a core, early-developing form of social interaction. It is based on our ability to discriminate the third party objects that other people are looking at. While it has been shown that people can accurately determine whether another person is looking directly at them versus away, little is known about human ability to discriminate a third person gaze directed towards objects that are further away, especially in unconstraint cases where the looker can move her head and eyes freely. In this paper we address this question by jointly exploring human psychophysics and a cognitively motivated computer vision model, which can detect the 3D direction of gaze from 2D face images. The synthesis of behavioral study and computer vision yields several interesting discoveries. (1) Human accuracy of discriminating targets 8{\deg}-10{\deg} of visual angle apart is around 40% in a free looking gaze task; (2) The ability to interpret gaze of different lookers vary dramatically; (3) This variance can be captured by the computational model; (4) Human outperforms the current model significantly. These results collectively show that the acuity of human joint attention is indeed highly impressive, given the computational challenge of the natural looking task. Moreover, the gap between human and model performance, as well as the variability of gaze interpretation across different lookers, require further understanding of the underlying mechanisms utilized by humans for this challenging task.



### Real-Time System of Hand Detection And Gesture Recognition In Cyber Presence Interactive System For E-Learning
- **Arxiv ID**: http://arxiv.org/abs/1502.07243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1502.07243v1)
- **Published**: 2014-12-08 17:44:30+00:00
- **Updated**: 2014-12-08 17:44:30+00:00
- **Authors**: Bousaaid Mourad, Ayaou Tarik, Afdel Karim, Estraillier Pascal
- **Comment**: 5 pages. arXiv admin note: substantial text overlap with
  arXiv:1502.06641
- **Journal**: Journal of Engineering Research and Applications Vol. 4, Issue 9
  (Version 1), September 2014, pp.1-5
- **Summary**: The development of technologies of multimedia, linked to that of Internet and democratization of high outflow, has made henceforth E-learning possible for learners being in virtual classes and geographically distributed. The quality and quantity of asynchronous and synchronous communications are the key elements for E-learning success. It is important to have a propitious supervision to reduce the feeling of isolation in E-learning. This feeling of isolation is among the main causes of loss and high rates of stalling in E-learning. The researches to be conducted in this domain aim to bring solutions of convergence coming from real time image for the capture and recognition of hand gestures. These gestures will be analyzed by the system and transformed as indicator of participation. This latter is displayed in the table of performance of the tutor as a curve according to the time. In case of isolation of learner, the indicator of participation will become red and the tutor will be informed of learners with difficulties to participate during learning session.



### HyperSpectral classification with adaptively weighted L1-norm regularization and spatial postprocessing
- **Arxiv ID**: http://arxiv.org/abs/1412.2684v3
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1412.2684v3)
- **Published**: 2014-12-08 18:14:04+00:00
- **Updated**: 2018-03-06 03:05:43+00:00
- **Authors**: Victor Stefan Aldea
- **Comment**: v3: 11 pages, 2 Figures, 10 Tables. Updated the results for the
  Indian Pines image; added the results for the Pavia University image
- **Journal**: None
- **Summary**: Sparse regression methods have been proven effective in a wide range of signal processing problems such as image compression, speech coding, channel equalization, linear regression and classification. In this paper a new convex method of hyperspectral image classification is developed based on the sparse unmixing algorithm SUnSAL for which a pixel adaptive L1-norm regularization term is introduced. To further enhance class separability, the algorithm is kernelized using an RBF kernel and the final results are improved by a combination of spatial pre and post-processing operations. It is shown that the proposed method is competitive with state of the art algorithms such as SVM-CK, KSOMP-CK and KSSP-CK.



### Image quality assessment measure based on natural image statistics in the Tetrolet domain
- **Arxiv ID**: http://arxiv.org/abs/1412.2697v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.2697v1)
- **Published**: 2014-12-08 18:48:26+00:00
- **Updated**: 2014-12-08 18:48:26+00:00
- **Authors**: Abdelkaher Ait Abdelouahad, Mohammed El Hassouni, Hocine Cherifi, Driss Aboutajdine
- **Comment**: None
- **Journal**: None
- **Summary**: This paper deals with a reduced reference (RR) image quality measure based on natural image statistics modeling. For this purpose, Tetrolet transform is used since it provides a convenient way to capture local geometric structures. This transform is applied to both reference and distorted images. Then, Gaussian Scale Mixture (GSM) is proposed to model subbands in order to take account statistical dependencies between tetrolet coefficients. In order to quantify the visual degradation, a measure based on Kullback Leibler Divergence (KLD) is provided. The proposed measure was tested on the Cornell VCL A-57 dataset and compared with other measures according to FR-TV1 VQEG framework.



### What is a salient object? A dataset and a baseline model for salient object detection
- **Arxiv ID**: http://arxiv.org/abs/1412.5027v1
- **DOI**: 10.1109/TIP.2014.2383320
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.5027v1)
- **Published**: 2014-12-08 23:51:50+00:00
- **Updated**: 2014-12-08 23:51:50+00:00
- **Authors**: Ali Borji
- **Comment**: IEEE Transactions on Image Processing, 2014
- **Journal**: None
- **Summary**: Salient object detection or salient region detection models, diverging from fixation prediction models, have traditionally been dealing with locating and segmenting the most salient object or region in a scene. While the notion of most salient object is sensible when multiple objects exist in a scene, current datasets for evaluation of saliency detection approaches often have scenes with only one single object. We introduce three main contributions in this paper: First, we take an indepth look at the problem of salient object detection by studying the relationship between where people look in scenes and what they choose as the most salient object when they are explicitly asked. Based on the agreement between fixations and saliency judgments, we then suggest that the most salient object is the one that attracts the highest fraction of fixations. Second, we provide two new less biased benchmark datasets containing scenes with multiple objects that challenge existing saliency models. Indeed, we observed a severe drop in performance of 8 state-of-the-art models on our datasets (40% to 70%). Third, we propose a very simple yet powerful model based on superpixels to be used as a baseline for model evaluation and comparison. While on par with the best models on MSRA-5K dataset, our model wins over other models on our data highlighting a serious drawback of existing models, which is convoluting the processes of locating the most salient object and its segmentation. We also provide a review and statistical analysis of some labeled scene datasets that can be used for evaluating salient object detection models. We believe that our work can greatly help remedy the over-fitting of models to existing biased datasets and opens new venues for future research in this fast-evolving field.



### Joint Segmentation and Deconvolution of Ultrasound Images Using a Hierarchical Bayesian Model based on Generalized Gaussian Priors
- **Arxiv ID**: http://arxiv.org/abs/1412.2813v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.2813v4)
- **Published**: 2014-12-08 23:52:57+00:00
- **Updated**: 2016-05-02 16:21:49+00:00
- **Authors**: Ningning Zhao, Adrian Basarab, Denis Kouame, Jean-Yves Tourneret
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a joint segmentation and deconvolution Bayesian method for medical ultrasound (US) images. Contrary to piecewise homogeneous images, US images exhibit heavy characteristic speckle patterns correlated with the tissue structures. The generalized Gaussian distribution (GGD) has been shown to be one of the most relevant distributions for characterizing the speckle in US images. Thus, we propose a GGD-Potts model defined by a label map coupling US image segmentation and deconvolution. The Bayesian estimators of the unknown model parameters, including the US image, the label map and all the hyperparameters are difficult to be expressed in closed form. Thus, we investigate a Gibbs sampler to generate samples distributed according to the posterior of interest. These generated samples are finally used to compute the Bayesian estimators of the unknown parameters. The performance of the proposed Bayesian model is compared with existing approaches via several experiments conducted on realistic synthetic data and in vivo US images.



