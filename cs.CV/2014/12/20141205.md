# Arxiv Papers in cs.CV on 2014-12-05
### A higher homotopic extension of persistent (co)homology
- **Arxiv ID**: http://arxiv.org/abs/1412.1871v1
- **DOI**: None
- **Categories**: **math.AT**, cs.CG, cs.CV, math.KT, 16E45, 16W70, 18G55, 55U10, 68U05
- **Links**: [PDF](http://arxiv.org/pdf/1412.1871v1)
- **Published**: 2014-12-05 01:09:19+00:00
- **Updated**: 2014-12-05 01:09:19+00:00
- **Authors**: Estanislao Herscovich
- **Comment**: Any comment(s) would be highly appreciated
- **Journal**: None
- **Summary**: Our objective in this article is to show a possibly interesting structure of homotopic nature appearing in persistent (co)homology. Assuming that the filtration of the (say) simplicial set embedded in a finite dimensional vector space induces a multiplicative filtration (which would not be a so harsh hypothesis in our setting) on the dg algebra given by the complex of simplicial cochains, we may use a result by T. Kadeishvili to get a unique (up to noncanonical equivalence) A_infinity-algebra structure on the complete persistent cohomology of the filtered simplicial (or topological) set. We then provide a construction of a (pseudo)metric on the set of all (generalized) barcodes (that is, of all cohomological degrees) enriched with the A_infinity-algebra structure stated before, refining the usual bottleneck metric, and which is also independent of the particular A_infinity-algebra structure chosen (among those equivalent to each other). We think that this distance might deserve some attention for topological data analysis, for it in particular can recognize different linking or foldings patterns, as in the Borromean rings. As an aside, we give a simple proof of a result relating the barcode structure between persistent homology and cohomology. This result was observed in a recent article by V. de Silva, D. Morozov and M. Vejdemo-Johansson under some restricted assumptions, which we do not suppose.



### Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images
- **Arxiv ID**: http://arxiv.org/abs/1412.1897v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.1897v4)
- **Published**: 2014-12-05 05:29:43+00:00
- **Updated**: 2015-04-02 23:12:56+00:00
- **Authors**: Anh Nguyen, Jason Yosinski, Jeff Clune
- **Comment**: To appear at CVPR 2015
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.



### Person Re-identification by Saliency Learning
- **Arxiv ID**: http://arxiv.org/abs/1412.1908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.1908v1)
- **Published**: 2014-12-05 07:33:48+00:00
- **Updated**: 2014-12-05 07:33:48+00:00
- **Authors**: Rui Zhao, Wanli Ouyang, Xiaogang Wang
- **Comment**: This manuscript has 14 pages with 25 figures, and a preliminary
  version was published in ICCV 2013
- **Journal**: None
- **Summary**: Human eyes can recognize person identities based on small salient regions, i.e. human saliency is distinctive and reliable in pedestrian matching across disjoint camera views. However, such valuable information is often hidden when computing similarities of pedestrian images with existing approaches. Inspired by our user study result of human perception on human saliency, we propose a novel perspective for person re-identification based on learning human saliency and matching saliency distribution. The proposed saliency learning and matching framework consists of four steps: (1) To handle misalignment caused by drastic viewpoint change and pose variations, we apply adjacency constrained patch matching to build dense correspondence between image pairs. (2) We propose two alternative methods, i.e. K-Nearest Neighbors and One-class SVM, to estimate a saliency score for each image patch, through which distinctive features stand out without using identity labels in the training procedure. (3) saliency matching is proposed based on patch matching. Matching patches with inconsistent saliency brings penalty, and images of the same identity are recognized by minimizing the saliency matching cost. (4) Furthermore, saliency matching is tightly integrated with patch matching in a unified structural RankSVM learning framework. The effectiveness of our approach is validated on the VIPeR dataset and the CUHK01 dataset. Our approach outperforms the state-of-the-art person re-identification methods on both datasets.



### Background Modelling using Octree Color Quantization
- **Arxiv ID**: http://arxiv.org/abs/1412.1945v2
- **DOI**: None
- **Categories**: **cs.CV**, 65D19
- **Links**: [PDF](http://arxiv.org/pdf/1412.1945v2)
- **Published**: 2014-12-05 10:44:56+00:00
- **Updated**: 2015-03-12 01:50:57+00:00
- **Authors**: Aditya A. V. Sastry
- **Comment**: None
- **Journal**: None
- **Summary**: By assuming that the most frequently occuring color in a video or a region of a video I propose a new algorithm for detecting foreground objects in a video. The process of detecting the foreground objects is complicated because of the fact that there may be swaying trees, objects of the background being moved around or lighting changes in the video. To deal with such complexities many have come up with solutions which heavily rely on expensive floating point operations. In this paper I used a data structure called Octree which is implemented only using binary operations. Traditionally octrees were used for color quantization but here in this paper I used it as a data structure to store the most frequently occuring colors in a video as well. For each of the starting few video frames, I constructed a Octree using all the colors of that frame. Next I pruned all the trees by removing nodes below a certain height and gave the leaf nodes a color which is dependant on the topological path from that node to its parent. Hence any two leaf nodes in two different octrees with the same topological path from themselves to the root will represent the same color. Next I merged all these individual trees into a single tree retaining only those nodes whose topological path to itself from the root is most common among all the trees. The colors represented by the leaf nodes in the resultant tree will be the most frequently occuring colors in the starting video frames of the video. Hence any color of an incomming frame that is not close to any of the colors represented by the leaf node of the merged tree can be regarded as belonging to a foreground object.   As an Octree is constructed using only binary operations, it is very fast compared to other leading algorithms.



### CoMIC: Good features for detection and matching at object boundaries
- **Arxiv ID**: http://arxiv.org/abs/1412.1957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.1957v1)
- **Published**: 2014-12-05 11:22:54+00:00
- **Updated**: 2014-12-05 11:22:54+00:00
- **Authors**: Swarna Kamlam Ravindran, Anurag Mittal
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Feature or interest points typically use information aggregation in 2D patches which does not remain stable at object boundaries when there is object motion against a significantly varying background. Level or iso-intensity curves are much more stable under such conditions, especially the longer ones. In this paper, we identify stable portions on long iso-curves and detect corners on them. Further, the iso-curve associated with a corner is used to discard portions from the background and improve matching. Such CoMIC (Corners on Maximally-stable Iso-intensity Curves) points yield superior results at the object boundary regions compared to state-of-the-art detectors while performing comparably at the interior regions as well. This is illustrated in exhaustive matching experiments for both boundary and non-boundary regions in applications such as stereo and point tracking for structure from motion in video sequences.



### Learning Multi-target Tracking with Quadratic Object Interactions
- **Arxiv ID**: http://arxiv.org/abs/1412.2066v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1412.2066v2)
- **Published**: 2014-12-05 17:04:35+00:00
- **Updated**: 2014-12-09 05:25:44+00:00
- **Authors**: Shaofei Wang, Charless C. Fowlkes
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a model for multi-target tracking based on associating collections of candidate detections across frames of a video. In order to model pairwise interactions between different tracks, such as suppression of overlapping tracks and contextual cues about co-occurence of different objects, we augment a standard min-cost flow objective with quadratic terms between detection variables. We learn the parameters of this model using structured prediction and a loss function which approximates the multi-target tracking accuracy. We evaluate two different approaches to finding an optimal set of tracks under model objective based on an LP relaxation and a novel greedy extension to dynamic programming that handles pairwise interactions. We find the greedy algorithm achieves equivalent performance to the LP relaxation while being 2-7x faster than a commercial solver. The resulting model with learned parameters outperforms existing methods across several categories on the KITTI tracking benchmark.



### Subspace based low rank and joint sparse matrix recovery
- **Arxiv ID**: http://arxiv.org/abs/1412.2700v2
- **DOI**: None
- **Categories**: **cs.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1412.2700v2)
- **Published**: 2014-12-05 18:42:14+00:00
- **Updated**: 2015-06-02 18:36:17+00:00
- **Authors**: Sampurna Biswas, Sunrita Poddar, Soura Dasgupta, Raghuraman Mudumbai, Mathews Jacob
- **Comment**: 5 pages, 5 figures, Asilomar 2014 conference submission
- **Journal**: None
- **Summary**: We consider the recovery of a low rank and jointly sparse matrix from under sampled measurements of its columns. This problem is highly relevant in the recovery of dynamic MRI data with high spatio-temporal resolution, where each column of the matrix corresponds to a frame in the image time series; the matrix is highly low-rank since the frames are highly correlated. Similarly the non-zero locations of the matrix in appropriate transform/frame domains (e.g. wavelet, gradient) are roughly the same in different frame. The superset of the support can be safely assumed to be jointly sparse. Unlike the classical multiple measurement vector (MMV) setup that measures all the snapshots using the same matrix, we consider each snapshot to be measured using a different measurement matrix. We show that this approach reduces the total number of measurements, especially when the rank of the matrix is much smaller than than its sparsity. Our experiments in the context of dynamic imaging shows that this approach is very useful in realizing free breathing cardiac MRI.



