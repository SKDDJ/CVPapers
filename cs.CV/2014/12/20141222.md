# Arxiv Papers in cs.CV on 2014-12-22
### A New Way to Factorize Linear Cameras
- **Arxiv ID**: http://arxiv.org/abs/1412.6847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.6847v1)
- **Published**: 2014-12-22 00:10:11+00:00
- **Updated**: 2014-12-22 00:10:11+00:00
- **Authors**: Feng Lu, Ziqiang Chen
- **Comment**: None
- **Journal**: None
- **Summary**: The implementation details of factorizing the 3x4 projection matrices of linear cameras into their left matrix factors and the 4x4 homogeneous central(also parallel for infinite center cases) projection factors are presented in this work. Any full row rank 3x4 real matrix can be factorized into such basic matrices which will be called LC factors.   A further extension to multiple view midpoint triangulation, for both pinhole and affine camera cases, is also presented based on such camera factorizations.



### Object Detectors Emerge in Deep Scene CNNs
- **Arxiv ID**: http://arxiv.org/abs/1412.6856v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.6856v2)
- **Published**: 2014-12-22 01:14:01+00:00
- **Updated**: 2015-04-15 19:06:41+00:00
- **Authors**: Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba
- **Comment**: 12 pages, ICLR 2015 conference paper
- **Journal**: None
- **Summary**: With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.



### Contour Detection Using Cost-Sensitive Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1412.6857v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.6857v5)
- **Published**: 2014-12-22 01:16:50+00:00
- **Updated**: 2015-05-12 08:42:42+00:00
- **Authors**: Jyh-Jing Hwang, Tyng-Luh Liu
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: We address the problem of contour detection via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. The main challenge lies in adapting a pre-trained per-image CNN model for yielding per-pixel image features. We propose to base on the DenseNet architecture to achieve pixelwise fine-tuning and then consider a cost-sensitive strategy to further improve the learning with a small dataset of edge and non-edge image patches. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and obtain comparable performances to the state-of-the-art on BSDS500.



### Half-CNN: A General Framework for Whole-Image Regression
- **Arxiv ID**: http://arxiv.org/abs/1412.6885v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.6885v1)
- **Published**: 2014-12-22 06:43:58+00:00
- **Updated**: 2014-12-22 06:43:58+00:00
- **Authors**: Jun Yuan, Bingbing Ni, Ashraf A. Kassim
- **Comment**: None
- **Journal**: None
- **Summary**: The Convolutional Neural Network (CNN) has achieved great success in image classification. The classification model can also be utilized at image or patch level for many other applications, such as object detection and segmentation. In this paper, we propose a whole-image CNN regression model, by removing the full connection layer and training the network with continuous feature maps. This is a generic regression framework that fits many applications. We demonstrate this method through two tasks: simultaneous face detection & segmentation, and scene saliency prediction. The result is comparable with other models in the respective fields, using only a small scale network. Since the regression model is trained on corresponding image / feature map pairs, there are no requirements on uniform input size as opposed to the classification model. Our framework avoids classifier design, a process that may introduce too much manual intervention in model development. Yet, it is highly correlated to the classification network and offers some in-deep review of CNN structures.



### Multi-modal Sensor Registration for Vehicle Perception via Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1412.7006v2
- **DOI**: 10.1109/HPEC.2015.7322485
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.7006v2)
- **Published**: 2014-12-22 14:54:53+00:00
- **Updated**: 2015-07-08 01:14:14+00:00
- **Authors**: Michael Giering, Vivek Venugopalan, Kishore Reddy
- **Comment**: 7 pages, double column, IEEE format, accepted at IEEE HPEC 2015
- **Journal**: None
- **Summary**: The ability to simultaneously leverage multiple modes of sensor information is critical for perception of an automated vehicle's physical surroundings. Spatio-temporal alignment of registration of the incoming information is often a prerequisite to analyzing the fused data. The persistence and reliability of multi-modal registration is therefore the key to the stability of decision support systems ingesting the fused information. LiDAR-video systems like on those many driverless cars are a common example of where keeping the LiDAR and video channels registered to common physical features is important. We develop a deep learning method that takes multiple channels of heterogeneous data, to detect the misalignment of the LiDAR-video inputs. A number of variations were tested on the Ford LiDAR-video driving test data set and will be discussed. To the best of our knowledge the use of multi-modal deep convolutional neural networks for dynamic real-time LiDAR-video registration has not been presented.



### Occlusion Edge Detection in RGB-D Frames using Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1412.7007v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.7007v3)
- **Published**: 2014-12-22 14:55:17+00:00
- **Updated**: 2015-07-08 01:07:23+00:00
- **Authors**: Soumik Sarkar, Vivek Venugopalan, Kishore Reddy, Michael Giering, Julian Ryde, Navdeep Jaitly
- **Comment**: 7 pages, double column, IEEE HPEC 2015 Conference
- **Journal**: None
- **Summary**: Occlusion edges in images which correspond to range discontinuity in the scene from the point of view of the observer are an important prerequisite for many vision and mobile robot tasks. Although they can be extracted from range data however extracting them from images and videos would be extremely beneficial. We trained a deep convolutional neural network (CNN) to identify occlusion edges in images and videos with both RGB-D and RGB inputs. The use of CNN avoids hand-crafting of features for automatically isolating occlusion edges and distinguishing them from appearance edges. Other than quantitative occlusion edge detection results, qualitative results are provided to demonstrate the trade-off between high resolution analysis and frame-level computation time which is critical for real-time robotics applications.



### Training deep neural networks with low precision multiplications
- **Arxiv ID**: http://arxiv.org/abs/1412.7024v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.7024v5)
- **Published**: 2014-12-22 15:22:45+00:00
- **Updated**: 2015-09-23 01:00:44+00:00
- **Authors**: Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David
- **Comment**: 10 pages, 5 figures, Accepted as a workshop contribution at ICLR 2015
- **Journal**: None
- **Summary**: Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.



### Attention for Fine-Grained Categorization
- **Arxiv ID**: http://arxiv.org/abs/1412.7054v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.7054v3)
- **Published**: 2014-12-22 17:06:07+00:00
- **Updated**: 2015-04-11 01:45:56+00:00
- **Authors**: Pierre Sermanet, Andrea Frome, Esteban Real
- **Comment**: ICLR 2015 Workshop
- **Journal**: None
- **Summary**: This paper presents experiments extending the work of Ba et al. (2014) on recurrent neural models for attention into less constrained visual environments, specifically fine-grained categorization on the Stanford Dogs data set. In this work we use an RNN of the same structure but substitute a more powerful visual network and perform large-scale pre-training of the visual network outside of the attention RNN. Most work in attention models to date focuses on tasks with toy or more constrained visual environments, whereas we present results for fine-grained categorization better than the state-of-the-art GoogLeNet classification model. We show that our model learns to direct high resolution attention to the most discriminative regions without any spatial supervision such as bounding boxes, and it is able to discriminate fine-grained dog breeds moderately well even when given only an initial low-resolution context image and narrow, inexpensive glimpses at faces and fur patterns. This and similar attention models have the major advantage of being trained end-to-end, as opposed to other current detection and recognition pipelines with hand-engineered components where information is lost. While our model is state-of-the-art, further work is needed to fully leverage the sequential input.



### Clustering multi-way data: a novel algebraic approach
- **Arxiv ID**: http://arxiv.org/abs/1412.7056v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1412.7056v2)
- **Published**: 2014-12-22 17:06:44+00:00
- **Updated**: 2015-02-22 02:19:29+00:00
- **Authors**: Eric Kernfeld, Shuchin Aeron, Misha Kilmer
- **Comment**: 20 pages, 8 figures
- **Journal**: None
- **Summary**: In this paper, we develop a method for unsupervised clustering of two-way (matrix) data by combining two recent innovations from different fields: the Sparse Subspace Clustering (SSC) algorithm [10], which groups points coming from a union of subspaces into their respective subspaces, and the t-product [18], which was introduced to provide a matrix-like multiplication for third order tensors. Our algorithm is analogous to SSC in that an "affinity" between different data points is built using a sparse self-representation of the data. Unlike SSC, we employ the t-product in the self-representation. This allows us more flexibility in modeling; infact, SSC is a special case of our method. When using the t-product, three-way arrays are treated as matrices whose elements (scalars) are n-tuples or tubes. Convolutions take the place of scalar multiplication. This framework allows us to embed the 2-D data into a vector-space-like structure called a free module over a commutative ring. These free modules retain many properties of complex inner-product spaces, and we leverage that to provide theoretical guarantees on our algorithm. We show that compared to vector-space counterparts, SSmC achieves higher accuracy and better able to cluster data with less preprocessing in some image clustering problems. In particular we show the performance of the proposed method on Weizmann face database, the Extended Yale B Face database and the MNIST handwritten digits database.



### Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs
- **Arxiv ID**: http://arxiv.org/abs/1412.7062v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.7062v4)
- **Published**: 2014-12-22 17:18:33+00:00
- **Updated**: 2016-06-07 04:00:08+00:00
- **Authors**: Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille
- **Comment**: 14 pages. Updated related work
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.



### Learning Deep Object Detectors from 3D Models
- **Arxiv ID**: http://arxiv.org/abs/1412.7122v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.7122v4)
- **Published**: 2014-12-22 20:10:31+00:00
- **Updated**: 2015-10-12 01:01:39+00:00
- **Authors**: Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko
- **Comment**: None
- **Journal**: None
- **Summary**: Crowdsourced 3D CAD models are becoming easily accessible online, and can potentially generate an infinite number of training images for almost any object category.We show that augmenting the training data of contemporary Deep Convolutional Neural Net (DCNN) models with such synthetic data can be effective, especially when real training data is limited or not well matched to the target domain. Most freely available CAD models capture 3D shape but are often missing other low level cues, such as realistic object texture, pose, or background. In a detailed analysis, we use synthetic CAD-rendered images to probe the ability of DCNN to learn without these cues, with surprising findings. In particular, we show that when the DCNN is fine-tuned on the target detection task, it exhibits a large degree of invariance to missing low-level cues, but, when pretrained on generic ImageNet classification, it learns better when the low-level cues are simulated. We show that our synthetic DCNN training approach significantly outperforms previous methods on the PASCAL VOC2007 dataset when learning in the few-shot scenario and improves performance in a domain shift scenario on the Office benchmark.



### Fully Convolutional Multi-Class Multiple Instance Learning
- **Arxiv ID**: http://arxiv.org/abs/1412.7144v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.7144v4)
- **Published**: 2014-12-22 20:49:54+00:00
- **Updated**: 2015-04-15 05:31:10+00:00
- **Authors**: Deepak Pathak, Evan Shelhamer, Jonathan Long, Trevor Darrell
- **Comment**: in ICLR 2015
- **Journal**: None
- **Summary**: Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge.



### Learning Compact Convolutional Neural Networks with Nested Dropout
- **Arxiv ID**: http://arxiv.org/abs/1412.7155v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.7155v4)
- **Published**: 2014-12-22 20:59:58+00:00
- **Updated**: 2015-04-10 06:11:22+00:00
- **Authors**: Chelsea Finn, Lisa Anne Hendricks, Trevor Darrell
- **Comment**: 4 pages, 2 figures. Accepted as a workshop contribution at ICLR 2015
- **Journal**: None
- **Summary**: Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost. However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity.



### Convolutional Neural Networks for joint object detection and pose estimation: A comparative study
- **Arxiv ID**: http://arxiv.org/abs/1412.7190v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.7190v4)
- **Published**: 2014-12-22 22:26:26+00:00
- **Updated**: 2015-02-28 05:15:45+00:00
- **Authors**: Francisco Massa, Mathieu Aubry, Renaud Marlet
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we study the application of convolutional neural networks for jointly detecting objects depicted in still images and estimating their 3D pose. We identify different feature representations of oriented objects, and energies that lead a network to learn this representations. The choice of the representation is crucial since the pose of an object has a natural, continuous structure while its category is a discrete variable. We evaluate the different approaches on the joint object detection and pose estimation task of the Pascal3D+ benchmark using Average Viewpoint Precision. We show that a classification approach on discretized viewpoints achieves state-of-the-art performance for joint object detection and pose estimation, and significantly outperforms existing baselines on this benchmark.



### Denoising autoencoder with modulated lateral connections learns invariant representations of natural images
- **Arxiv ID**: http://arxiv.org/abs/1412.7210v4
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1412.7210v4)
- **Published**: 2014-12-22 23:36:15+00:00
- **Updated**: 2015-03-31 15:49:16+00:00
- **Authors**: Antti Rasmus, Tapani Raiko, Harri Valpola
- **Comment**: Presentation at ICLR 2015 workshop
- **Journal**: None
- **Summary**: Suitable lateral connections between encoder and decoder are shown to allow higher layers of a denoising autoencoder (dAE) to focus on invariant representations. In regular autoencoders, detailed information needs to be carried through the highest layers but lateral connections from encoder to decoder relieve this pressure. It is shown that abstract invariant features can be translated to detailed reconstructions when invariant features are allowed to modulate the strength of the lateral connection. Three dAE structures with modulated and additive lateral connections, and without lateral connections were compared in experiments using real-world images. The experiments verify that adding modulated lateral connections to the model 1) improves the accuracy of the probability model for inputs, as measured by denoising performance; 2) results in representations whose degree of invariance grows faster towards the higher layers; and 3) supports the formation of diverse invariant poolings.



