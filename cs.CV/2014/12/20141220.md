# Arxiv Papers in cs.CV on 2014-12-20
### Self-informed neural network structure learning
- **Arxiv ID**: http://arxiv.org/abs/1412.6563v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.6563v2)
- **Published**: 2014-12-20 00:05:57+00:00
- **Updated**: 2015-04-13 21:35:29+00:00
- **Authors**: David Warde-Farley, Andrew Rabinovich, Dragomir Anguelov
- **Comment**: Updated with accepted workshop contribution header
- **Journal**: None
- **Summary**: We study the problem of large scale, multi-label visual recognition with a large number of possible classes. We propose a method for augmenting a trained neural network classifier with auxiliary capacity in a manner designed to significantly improve upon an already well-performing model, while minimally impacting its computational footprint. Using the predictions of the network itself as a descriptor for assessing visual similarity, we define a partitioning of the label space into groups of visually similar entities. We then augment the network with auxilliary hidden layer pathways with connectivity only to these groups of label units. We report a significant improvement in mean average precision on a large-scale object recognition task with the augmented model, while increasing the number of multiply-adds by less than 3%.



### Visual Instance Retrieval with Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1412.6574v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.6574v4)
- **Published**: 2014-12-20 01:32:43+00:00
- **Updated**: 2016-05-09 08:54:31+00:00
- **Authors**: Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson, Atsuto Maki
- **Comment**: None
- **Journal**: None
- **Summary**: This paper provides an extensive study on the availability of image representations based on convolutional networks (ConvNets) for the task of visual instance retrieval. Besides the choice of convolutional layers, we present an efficient pipeline exploiting multi-scale schemes to extract local features, in particular, by taking geometric invariance into explicit account, i.e. positions, scales and spatial consistency. In our experiments using five standard image retrieval datasets, we demonstrate that generic ConvNet image representations can outperform other state-of-the-art methods if they are extracted appropriately.



### Discovering Hidden Factors of Variation in Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1412.6583v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.6583v4)
- **Published**: 2014-12-20 02:52:03+00:00
- **Updated**: 2015-06-17 06:47:48+00:00
- **Authors**: Brian Cheung, Jesse A. Livezey, Arjun K. Bansal, Bruno A. Olshausen
- **Comment**: Presented at International Conference on Learning Representations
  2015 Workshop
- **Journal**: None
- **Summary**: Deep learning has enjoyed a great deal of success because of its ability to learn useful features for tasks such as classification. But there has been less exploration in learning the factors of variation apart from the classification signal. By augmenting autoencoders with simple regularization terms during training, we demonstrate that standard deep architectures can discover and explicitly represent factors of variation beyond those relevant for categorization. We introduce a cross-covariance penalty (XCov) as a method to disentangle factors like handwriting style for digits and subject identity in faces. We demonstrate this on the MNIST handwritten digit database, the Toronto Faces Database (TFD) and the Multi-PIE dataset by generating manipulated instances of the data. Furthermore, we demonstrate these deep networks can extrapolate `hidden' variation in the supervised signal.



### Training Deep Neural Networks on Noisy Labels with Bootstrapping
- **Arxiv ID**: http://arxiv.org/abs/1412.6596v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.6596v3)
- **Published**: 2014-12-20 04:11:33+00:00
- **Updated**: 2015-04-15 19:48:37+00:00
- **Authors**: Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, Andrew Rabinovich
- **Comment**: None
- **Journal**: None
- **Summary**: Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overfitting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the- art results, and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection.



### An Analysis of Unsupervised Pre-training in Light of Recent Advances
- **Arxiv ID**: http://arxiv.org/abs/1412.6597v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.6597v4)
- **Published**: 2014-12-20 04:20:55+00:00
- **Updated**: 2015-04-10 21:26:31+00:00
- **Authors**: Tom Le Paine, Pooya Khorrami, Wei Han, Thomas S. Huang
- **Comment**: Accepted as a workshop contribution to ICLR 2015
- **Journal**: None
- **Summary**: Convolutional neural networks perform well on object recognition because of a number of recent advances: rectified linear units (ReLUs), data augmentation, dropout, and large labelled datasets. Unsupervised data has been proposed as another way to improve performance. Unfortunately, unsupervised pre-training is not used by state-of-the-art methods leading to the following question: Is unsupervised pre-training still useful given recent advances? If so, when? We answer this in three parts: we 1) develop an unsupervised method that incorporates ReLUs and recent unsupervised regularization techniques, 2) analyze the benefits of unsupervised pre-training compared to data augmentation and dropout on CIFAR-10 while varying the ratio of unsupervised to supervised samples, 3) verify our findings on STL-10. We discover unsupervised pre-training, as expected, helps when the ratio of unsupervised to supervised samples is high, and surprisingly, hurts when the ratio is low. We also use unsupervised pre-training with additional color augmentation to achieve near state-of-the-art performance on STL-10.



### Automatic Discovery and Optimization of Parts for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1412.6598v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1412.6598v2)
- **Published**: 2014-12-20 04:25:34+00:00
- **Updated**: 2015-04-11 20:13:40+00:00
- **Authors**: Sobhan Naderi Parizi, Andrea Vedaldi, Andrew Zisserman, Pedro Felzenszwalb
- **Comment**: 19 pages, template changed to camera ready version, 1 reference
  added, 1 reference fixed, Fig. 3, 4 updated (larger text)
- **Journal**: None
- **Summary**: Part-based representations have been shown to be very useful for image classification. Learning part-based models is often viewed as a two-stage problem. First, a collection of informative parts is discovered, using heuristics that promote part distinctiveness and diversity, and then classifiers are trained on the vector of part responses. In this paper we unify the two stages and learn the image classifiers and a set of shared parts jointly. We generate an initial pool of parts by randomly sampling part candidates and selecting a good subset using L1/L2 regularization. All steps are driven "directly" by the same objective namely the classification loss on a training set. This lets us do away with engineered heuristics. We also introduce the notion of "negative parts", intended as parts that are negatively correlated with one or more classes. Negative parts are complementary to the parts discovered by other methods, which look only for positive correlations.



### Video (language) modeling: a baseline for generative models of natural videos
- **Arxiv ID**: http://arxiv.org/abs/1412.6604v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1412.6604v5)
- **Published**: 2014-12-20 05:05:51+00:00
- **Updated**: 2016-05-04 14:01:42+00:00
- **Authors**: MarcAurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan Collobert, Sumit Chopra
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a strong baseline model for unsupervised feature learning using video data. By learning to predict missing frames or extrapolate future frames from an input video sequence, the model discovers both spatial and temporal correlations which are useful to represent complex deformations and motion patterns. The models we propose are largely borrowed from the language modeling literature, and adapted to the vision domain by quantizing the space of image patches into a large dictionary. We demonstrate the approach on both a filling and a generation task. For the first time, we show that, after training on natural videos, such a model can predict non-trivial motions over short video sequences.



### Visual Scene Representations: Contrast, Scaling and Occlusion
- **Arxiv ID**: http://arxiv.org/abs/1412.6607v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.6607v5)
- **Published**: 2014-12-20 05:15:42+00:00
- **Updated**: 2015-04-17 18:59:02+00:00
- **Authors**: Stefano Soatto, Jingming Dong, Nikolaos Karianakis
- **Comment**: UCLA Tech Report CSD140023, Nov. 12, 2014. Updated April 13, 2015
- **Journal**: None
- **Summary**: We study the structure of representations, defined as approximations of minimal sufficient statistics that are maximal invariants to nuisance factors, for visual data subject to scaling and occlusion of line-of-sight. We derive analytical expressions for such representations and show that, under certain restrictive assumptions, they are related to features commonly in use in the computer vision community. This link highlights the condition tacitly assumed by these descriptors, and also suggests ways to improve and generalize them. This new interpretation draws connections to the classical theories of sampling, hypothesis testing and group invariance.



### In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1412.6614v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1412.6614v4)
- **Published**: 2014-12-20 06:52:25+00:00
- **Updated**: 2015-04-16 18:48:31+00:00
- **Authors**: Behnam Neyshabur, Ryota Tomioka, Nathan Srebro
- **Comment**: 9 pages, 2 figures
- **Journal**: None
- **Summary**: We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multilayer feed-forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.



### Permutohedral Lattice CNNs
- **Arxiv ID**: http://arxiv.org/abs/1412.6618v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.6618v3)
- **Published**: 2014-12-20 07:08:54+00:00
- **Updated**: 2015-05-03 11:26:34+00:00
- **Authors**: Martin Kiefel, Varun Jampani, Peter V. Gehler
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a convolutional layer that is able to process sparse input features. As an example, for image recognition problems this allows an efficient filtering of signals that do not lie on a dense grid (like pixel position), but of more general features (such as color values). The presented algorithm makes use of the permutohedral lattice data structure. The permutohedral lattice was introduced to efficiently implement a bilateral filter, a commonly used image processing operation. Its use allows for a generalization of the convolution type found in current (spatial) convolutional network architectures.



### Deep metric learning using Triplet network
- **Arxiv ID**: http://arxiv.org/abs/1412.6622v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1412.6622v4)
- **Published**: 2014-12-20 07:34:50+00:00
- **Updated**: 2018-12-04 15:35:35+00:00
- **Authors**: Elad Hoffer, Nir Ailon
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.



### The local low-dimensionality of natural images
- **Arxiv ID**: http://arxiv.org/abs/1412.6626v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.6626v4)
- **Published**: 2014-12-20 07:52:16+00:00
- **Updated**: 2015-03-24 01:28:48+00:00
- **Authors**: Olivier J. Hénaff, Johannes Ballé, Neil C. Rabinowitz, Eero P. Simoncelli
- **Comment**: Published as conference paper at ICLR 2015
- **Journal**: None
- **Summary**: We develop a new statistical model for photographic images, in which the local responses of a bank of linear filters are described as jointly Gaussian, with zero mean and a covariance that varies slowly over spatial position. We optimize sets of filters so as to minimize the nuclear norms of matrices of their local activations (i.e., the sum of the singular values), thus encouraging a flexible form of sparsity that is not tied to any particular dictionary or coordinate system. Filters optimized according to this objective are oriented and bandpass, and their responses exhibit substantial local correlation. We show that images can be reconstructed nearly perfectly from estimates of the local filter response covariances alone, and with minimal degradation (either visual or MSE) from low-rank approximations of these covariances. As such, this representation holds much promise for use in applications such as denoising, compression, and texture representation, and may form a useful substrate for hierarchical decompositions.



### Visualizing and Comparing Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1412.6631v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.6631v2)
- **Published**: 2014-12-20 08:07:32+00:00
- **Updated**: 2014-12-26 10:43:23+00:00
- **Authors**: Wei Yu, Kuiyuan Yang, Yalong Bai, Hongxun Yao, Yong Rui
- **Comment**: 9 pages and 7 figures, submit to ICLR2015
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have achieved comparable error rates to well-trained human on ILSVRC2014 image classification task. To achieve better performance, the complexity of CNNs is continually increasing with deeper and bigger architectures. Though CNNs achieved promising external classification behavior, understanding of their internal work mechanism is still limited. In this work, we attempt to understand the internal work mechanism of CNNs by probing the internal representations in two comprehensive aspects, i.e., visualizing patches in the representation spaces constructed by different layers, and visualizing visual information kept in each layer. We further compare CNNs with different depths and show the advantages brought by deeper architecture.



### Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)
- **Arxiv ID**: http://arxiv.org/abs/1412.6632v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, I.2.6; I.2.7; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1412.6632v5)
- **Published**: 2014-12-20 08:10:04+00:00
- **Updated**: 2015-06-11 15:26:58+00:00
- **Authors**: Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille
- **Comment**: Add a simple strategy to boost the performance of image captioning
  task significantly. More details are shown in Section 8 of the paper. The
  code and related data are available at https://github.com/mjhucla/mRNN-CR ;.
  arXiv admin note: substantial text overlap with arXiv:1410.1090
- **Journal**: ICLR 2015
- **Summary**: In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/~junhua.mao/m-RNN.html .



