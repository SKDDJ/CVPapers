# Arxiv Papers in cs.CV on 2014-12-29
### Rigid and Non-rigid Shape Evolutions for Shape Alignment and Recovery in Images
- **Arxiv ID**: http://arxiv.org/abs/1412.8287v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.8287v1)
- **Published**: 2014-12-29 09:19:55+00:00
- **Updated**: 2014-12-29 09:19:55+00:00
- **Authors**: Junyan Wang, Kap-Luk Chan
- **Comment**: None
- **Journal**: None
- **Summary**: The same type of objects in different images may vary in their shapes because of rigid and non-rigid shape deformations, occluding foreground as well as cluttered background. The problem concerned in this work is the shape extraction in such challenging situations. We approach the shape extraction through shape alignment and recovery. This paper presents a novel and general method for shape alignment and recovery by using one example shapes based on deterministic energy minimization. Our idea is to use general model of shape deformation in minimizing active contour energies. Given \emph{a priori} form of the shape deformation, we show how the curve evolution equation corresponding to the shape deformation can be derived. The curve evolution is called the prior variation shape evolution (PVSE). We also derive the energy-minimizing PVSE for minimizing active contour energies. For shape recovery, we propose to use the PVSE that deforms the shape while preserving its shape characteristics. For choosing such shape-preserving PVSE, a theory of shape preservability of the PVSE is established. Experimental results validate the theory and the formulations, and they demonstrate the effectiveness of our method.



### Fast, simple and accurate handwritten digit classification by training shallow neural network classifiers with the 'extreme learning machine' algorithm
- **Arxiv ID**: http://arxiv.org/abs/1412.8307v2
- **DOI**: 10.1371/journal.pone.0134254
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1412.8307v2)
- **Published**: 2014-12-29 11:14:59+00:00
- **Updated**: 2015-07-22 08:28:03+00:00
- **Authors**: Mark D. McDonnell, Migel D. Tissera, Tony Vladusich, André van Schaik, Jonathan Tapson
- **Comment**: Accepted for publication; 9 pages of text, 6 figures and 1 table
- **Journal**: None
- **Summary**: Recent advances in training deep (multi-layer) architectures have inspired a renaissance in neural network use. For example, deep convolutional networks are becoming the default option for difficult tasks on large datasets, such as image and speech recognition. However, here we show that error rates below 1% on the MNIST handwritten digit benchmark can be replicated with shallow non-convolutional neural networks. This is achieved by training such networks using the 'Extreme Learning Machine' (ELM) approach, which also enables a very rapid training time (~10 minutes). Adding distortions, as is common practise for MNIST, reduces error rates even further. Our methods are also shown to be capable of achieving less than 5.5% error rates on the NORB image database. To achieve these results, we introduce several enhancements to the standard ELM algorithm, which individually and in combination can significantly improve performance. The main innovation is to ensure each hidden-unit operates only on a randomly sized and positioned patch of each image. This form of random `receptive field' sampling of the input ensures the input weight matrix is sparse, with about 90% of weights equal to zero. Furthermore, combining our methods with a small number of iterations of a single-batch backpropagation method can significantly reduce the number of hidden-units required to achieve a particular performance. Our close to state-of-the-art results for MNIST and NORB suggest that the ease of use and accuracy of the ELM algorithm for designing a single-hidden-layer neural network classifier should cause it to be given greater consideration either as a standalone method for simpler problems, or as the final classification stage in deep neural networks applied to more difficult problems.



### Spectral classification using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1412.8341v1
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.IM, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.8341v1)
- **Published**: 2014-12-29 13:47:06+00:00
- **Updated**: 2014-12-29 13:47:06+00:00
- **Authors**: Pavel Hála
- **Comment**: 71 pages, 50 figures, Master's thesis, Masaryk University
- **Journal**: None
- **Summary**: There is a great need for accurate and autonomous spectral classification methods in astrophysics. This thesis is about training a convolutional neural network (ConvNet) to recognize an object class (quasar, star or galaxy) from one-dimension spectra only. Author developed several scripts and C programs for datasets preparation, preprocessing and postprocessing of the data. EBLearn library (developed by Pierre Sermanet and Yann LeCun) was used to create ConvNets. Application on dataset of more than 60000 spectra yielded success rate of nearly 95%. This thesis conclusively proved great potential of convolutional neural networks and deep learning methods in astrophysics.



### A simple coding for cross-domain matching with dimension reduction via spectral graph embedding
- **Arxiv ID**: http://arxiv.org/abs/1412.8380v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1412.8380v2)
- **Published**: 2014-12-29 16:08:27+00:00
- **Updated**: 2015-03-29 18:38:26+00:00
- **Authors**: Hidetoshi Shimodaira
- **Comment**: None
- **Journal**: None
- **Summary**: Data vectors are obtained from multiple domains. They are feature vectors of images or vector representations of words. Domains may have different numbers of data vectors with different dimensions. These data vectors from multiple domains are projected to a common space by linear transformations in order to search closely related vectors across domains. We would like to find projection matrices to minimize distances between closely related data vectors. This formulation of cross-domain matching is regarded as an extension of the spectral graph embedding to multi-domain setting, and it includes several multivariate analysis methods of statistics such as multiset canonical correlation analysis, correspondence analysis, and principal component analysis. Similar approaches are very popular recently in pattern recognition and vision. In this paper, instead of proposing a novel method, we will introduce an embarrassingly simple idea of coding the data vectors for explaining all the above mentioned approaches. A data vector is concatenated with zero vectors from all other domains to make an augmented vector. The cross-domain matching is solved by applying the single-domain version of spectral graph embedding to these augmented vectors of all the domains. An interesting connection to the classical associative memory model of neural networks is also discussed by noticing a coding for association. A cross-validation method for choosing the dimension of the common space and a regularization parameter will be discussed in an illustrative numerical example.



### Simple Image Description Generator via a Linear Phrase-Based Approach
- **Arxiv ID**: http://arxiv.org/abs/1412.8419v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.8419v3)
- **Published**: 2014-12-29 18:43:10+00:00
- **Updated**: 2015-04-11 03:53:26+00:00
- **Authors**: Remi Lebret, Pedro O. Pinheiro, Ronan Collobert
- **Comment**: Accepted as a workshop paper at ICLR 2015
- **Journal**: None
- **Summary**: Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results on the recently release Microsoft COCO dataset.



### Accurate Localization in Dense Urban Area Using Google Street View Image
- **Arxiv ID**: http://arxiv.org/abs/1412.8496v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.8496v1)
- **Published**: 2014-12-29 22:09:41+00:00
- **Updated**: 2014-12-29 22:09:41+00:00
- **Authors**: Mahdi Salarian
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate information about the location and orientation of a camera in mobile devices is central to the utilization of location-based services (LBS). Most of such mobile devices rely on GPS data but this data is subject to inaccuracy due to imperfections in the quality of the signal provided by satellites. This shortcoming has spurred the research into improving the accuracy of localization. Since mobile devices have camera, a major thrust of this research has been seeks to acquire the local scene and apply image retrieval techniques by querying a GPS-tagged image database to find the best match for the acquired scene.. The techniques are however computationally demanding and unsuitable for real-time applications such as assistive technology for navigation by the blind and visually impaired which motivated out work. To overcome the high complexity of those techniques, we investigated the use of inertial sensors as an aid in image-retrieval-based approach. Armed with information of media other than images, such as data from the GPS module along with orientation sensors such as accelerometer and gyro, we sought to limit the size of the image set to c search for the best match. Specifically, data from the orientation sensors along with Dilution of precision (DOP) from GPS are used to find the angle of view and estimation of position. We present analysis of the reduction in the image set size for the search as well as simulations to demonstrate the effectiveness in a fast implementation with 98% Estimated Position Error.



