# Arxiv Papers in cs.CV on 2014-12-04
### Parsing Occluded People by Flexible Compositions
- **Arxiv ID**: http://arxiv.org/abs/1412.1526v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.1526v2)
- **Published**: 2014-12-04 00:45:14+00:00
- **Updated**: 2015-11-24 07:57:19+00:00
- **Authors**: Xianjie Chen, Alan Yuille
- **Comment**: CVPR 15 Camera Ready
- **Journal**: None
- **Summary**: This paper presents an approach to parsing humans when there is significant occlusion. We model humans using a graphical model which has a tree structure building on recent work [32, 6] and exploit the connectivity prior that, even in presence of occlusion, the visible nodes form a connected subtree of the graphical model. We call each connected subtree a flexible composition of object parts. This involves a novel method for learning occlusion cues. During inference we need to search over a mixture of different flexible models. By exploiting part sharing, we show that this inference can be done extremely efficiently requiring only twice as many computations as searching for the entire object (i.e., not modeling occlusion). We evaluate our model on the standard benchmarked "We Are Family" Stickmen dataset and obtain significant performance improvements over the best alternative algorithms.



### Metric Learning Driven Multi-Task Structured Output Optimization for Robust Keypoint Tracking
- **Arxiv ID**: http://arxiv.org/abs/1412.1574v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1412.1574v1)
- **Published**: 2014-12-04 07:42:21+00:00
- **Updated**: 2014-12-04 07:42:21+00:00
- **Authors**: Liming Zhao, Xi Li, Jun Xiao, Fei Wu, Yueting Zhuang
- **Comment**: Accepted by AAAI-15
- **Journal**: None
- **Summary**: As an important and challenging problem in computer vision and graphics, keypoint-based object tracking is typically formulated in a spatio-temporal statistical learning framework. However, most existing keypoint trackers are incapable of effectively modeling and balancing the following three aspects in a simultaneous manner: temporal model coherence across frames, spatial model consistency within frames, and discriminative feature construction. To address this issue, we propose a robust keypoint tracker based on spatio-temporal multi-task structured output optimization driven by discriminative metric learning. Consequently, temporal model coherence is characterized by multi-task structured keypoint model learning over several adjacent frames, while spatial model consistency is modeled by solving a geometric verification based structured learning problem. Discriminative feature construction is enabled by metric learning to ensure the intra-class compactness and inter-class separability. Finally, the above three modules are simultaneously optimized in a joint learning scheme. Experimental results have demonstrated the effectiveness of our tracker.



### On using the Microsoft Kinect$^{\rm TM}$ sensors in the analysis of human motion
- **Arxiv ID**: http://arxiv.org/abs/1412.2032v4
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1412.2032v4)
- **Published**: 2014-12-04 11:15:08+00:00
- **Updated**: 2015-05-27 05:40:37+00:00
- **Authors**: M. J. Malinowski, E. Matsinos, S. Roth
- **Comment**: 29 pages, 1 table, 4 figures. Versions 1 and 2 of the study have been
  retracted due to the use of a non-representative database; save for one
  sentence at the end of the abstract of the current version, versions 3 and 4
  are identical
- **Journal**: None
- **Summary**: The present paper aims at providing the theoretical background required for investigating the use of the Microsoft Kinect$^{\rm TM}$ (`Kinect', for short) sensors (original and upgraded) in the analysis of human motion. Our methodology is developed in such a way that its application be easily adaptable to comparative studies of other systems used in capturing human-motion data. Our future plans include the application of this methodology to two situations: first, in a comparative study of the performance of the two Kinect sensors; second, in pursuing their validation on the basis of comparisons with a marker-based system (MBS). One important feature in our approach is the transformation of the MBS output into Kinect-output format, thus enabling the analysis of the measurements, obtained from different systems, with the same software application, i.e., the one we use in the analysis of Kinect-captured data; one example of such a transformation, for one popular marker-placement scheme (`Plug-in Gait'), is detailed. We propose that the similarity of the output, obtained from the different systems, be assessed on the basis of the comparison of a number of waveforms, representing the variation within the gait cycle of quantities which are commonly used in the modelling of the human motion. The data acquisition may involve commercially-available treadmills and a number of velocity settings: for instance, walking-motion data may be acquired at $5$ km/h, running-motion data at $8$ and $11$ km/h. We recommend that particular attention be called to systematic effects associated with the subject's knee and lower leg, as well as to the ability of the Kinect sensors in reliably capturing the details in the asymmetry of the motion for the left and right parts of the human body. The previous versions of the study have been withdrawn due to the use of a non-representative database.



### Fisher Kernel for Deep Neural Activations
- **Arxiv ID**: http://arxiv.org/abs/1412.1628v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.1628v2)
- **Published**: 2014-12-04 11:30:57+00:00
- **Updated**: 2014-12-19 07:16:18+00:00
- **Authors**: Donggeun Yoo, Sunggyun Park, Joon-Young Lee, In So Kweon
- **Comment**: None
- **Journal**: None
- **Summary**: Compared to image representation based on low-level local descriptors, deep neural activations of Convolutional Neural Networks (CNNs) are richer in mid-level representation, but poorer in geometric invariance properties. In this paper, we present a straightforward framework for better image representation by combining the two approaches. To take advantages of both representations, we propose an efficient method to extract a fair amount of multi-scale dense local activations from a pre-trained CNN. We then aggregate the activations by Fisher kernel framework, which has been modified with a simple scale-wise normalization essential to make it suitable for CNN activations. Replacing the direct use of a single activation vector with our representation demonstrates significant performance improvements: +17.76 (Acc.) on MIT Indoor 67 and +7.18 (mAP) on PASCAL VOC 2007. The results suggest that our proposal can be used as a primary image representation for better performances in visual recognition tasks.



### Convolutional Neural Networks at Constrained Time Cost
- **Arxiv ID**: http://arxiv.org/abs/1412.1710v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.1710v1)
- **Published**: 2014-12-04 16:00:47+00:00
- **Updated**: 2014-12-04 16:00:47+00:00
- **Authors**: Kaiming He, Jian Sun
- **Comment**: 8-page technical report
- **Journal**: None
- **Summary**: Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8% top-5 error, 10-view test), yet is 20% faster than "AlexNet" (16.0% top-5 error, 10-view test).



### Statistical models and regularization strategies in statistical image reconstruction of low-dose X-ray CT: a survey
- **Arxiv ID**: http://arxiv.org/abs/1412.1732v3
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1412.1732v3)
- **Published**: 2014-12-04 17:07:10+00:00
- **Updated**: 2015-05-14 17:51:10+00:00
- **Authors**: Hao Zhang, Jing Wang, Jianhua Ma, Hongbing Lu, Zhengrong Liang
- **Comment**: This paper has been withdrawn because the authors have differnt
  opinions on some contents of the paper
- **Journal**: None
- **Summary**: Statistical image reconstruction (SIR) methods have shown potential to substantially improve the image quality of low-dose X-ray computed tomography (CT) as compared to the conventional filtered back-projection (FBP) method for various clinical tasks. According to the maximum a posterior (MAP) estimation, the SIR methods can be typically formulated by an objective function consisting of two terms: (1) data-fidelity (or equivalently, data-fitting or data-mismatch) term modeling the statistics of projection measurements, and (2) regularization (or equivalently, prior or penalty) term reflecting prior knowledge or expectation on the characteristics of the image to be reconstructed. Existing SIR methods for low-dose CT can be divided into two groups: (1) those that use calibrated transmitted photon counts (before log-transform) with penalized maximum likelihood (pML) criterion, and (2) those that use calibrated line-integrals (after log-transform) with penalized weighted least-squares (PWLS) criterion. Accurate statistical modeling of the projection measurements is a prerequisite for SIR, while the regularization term in the objective function also plays a critical role for successful image reconstruction. This paper reviews several statistical models on CT projection measurements and various regularization strategies incorporating prior knowledge or expected properties of the image to be reconstructed, which together formulate the objective function of the SIR methods for low-dose X-ray CT.



### Image Data Compression for Covariance and Histogram Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1412.1740v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1412.1740v2)
- **Published**: 2014-12-04 17:22:22+00:00
- **Updated**: 2015-05-23 17:07:59+00:00
- **Authors**: Matt J. Kusner, Nicholas I. Kolkin, Stephen Tyree, Kilian Q. Weinberger
- **Comment**: None
- **Journal**: None
- **Summary**: Covariance and histogram image descriptors provide an effective way to capture information about images. Both excel when used in combination with special purpose distance metrics. For covariance descriptors these metrics measure the distance along the non-Euclidean Riemannian manifold of symmetric positive definite matrices. For histogram descriptors the Earth Mover's distance measures the optimal transport between two histograms. Although more precise, these distance metrics are very expensive to compute, making them impractical in many applications, even for data sets of only a few thousand examples. In this paper we present two methods to compress the size of covariance and histogram datasets with only marginal increases in test error for k-nearest neighbor classification. Specifically, we show that we can reduce data sets to 16% and in some cases as little as 2% of their original size, while approximately matching the test error of kNN classification on the full training set. In fact, because the compressed set is learned in a supervised fashion, it sometimes even outperforms the full data set, while requiring only a fraction of the space and drastically reducing test-time computation.



### Reading Text in the Wild with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1412.1842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.1842v1)
- **Published**: 2014-12-04 21:14:59+00:00
- **Updated**: 2014-12-04 21:14:59+00:00
- **Authors**: Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we present an end-to-end system for text spotting -- localising and recognising text in natural scene images -- and text based image retrieval. This system is based on a region proposal mechanism for detection and deep convolutional neural networks for recognition. Our pipeline uses a novel combination of complementary proposal generation techniques to ensure high recall, and a fast subsequent filtering stage for improving precision. For the recognition and ranking of proposals, we train very large convolutional neural networks to perform word recognition on the whole proposal region at the same time, departing from the character classifier based systems of the past. These networks are trained solely on data produced by a synthetic text generation engine, requiring no human labelled data.   Analysing the stages of our pipeline, we show state-of-the-art performance throughout. We perform rigorous experiments across a number of standard end-to-end text spotting benchmarks and text-based image retrieval datasets, showing a large improvement over all previous methods. Finally, we demonstrate a real-world application of our text spotting system to allow thousands of hours of news footage to be instantly searchable via a text query.



