# Arxiv Papers in cs.CV on 2014-12-19
### Non-parametric PSF estimation from celestial transit solar images using blind deconvolution
- **Arxiv ID**: http://arxiv.org/abs/1412.6279v3
- **DOI**: 10.1051/swsc/2015040
- **Categories**: **cs.CV**, astro-ph.SR
- **Links**: [PDF](http://arxiv.org/pdf/1412.6279v3)
- **Published**: 2014-12-19 10:38:18+00:00
- **Updated**: 2015-09-29 16:34:38+00:00
- **Authors**: Adriana Gonzalez, Véronique Delouille, Laurent Jacques
- **Comment**: 31 pages, 47 figures
- **Journal**: J. Space Weather Space Clim., 6, A1, 2016
- **Summary**: Context: Characterization of instrumental effects in astronomical imaging is important in order to extract accurate physical information from the observations. The measured image in a real optical instrument is usually represented by the convolution of an ideal image with a Point Spread Function (PSF). Additionally, the image acquisition process is also contaminated by other sources of noise (read-out, photon-counting). The problem of estimating both the PSF and a denoised image is called blind deconvolution and is ill-posed.   Aims: We propose a blind deconvolution scheme that relies on image regularization. Contrarily to most methods presented in the literature, our method does not assume a parametric model of the PSF and can thus be applied to any telescope.   Methods: Our scheme uses a wavelet analysis prior model on the image and weak assumptions on the PSF. We use observations from a celestial transit, where the occulting body can be assumed to be a black disk. These constraints allow us to retain meaningful solutions for the filter and the image, eliminating trivial, translated and interchanged solutions. Under an additive Gaussian noise assumption, they also enforce noise canceling and avoid reconstruction artifacts by promoting the whiteness of the residual between the blurred observations and the cleaned data.   Results: Our method is applied to synthetic and experimental data. The PSF is estimated for the SECCHI/EUVI instrument using the 2007 Lunar transit, and for SDO/AIA using the 2012 Venus transit. Results show that the proposed non-parametric blind deconvolution method is able to estimate the core of the PSF with a similar quality to parametric methods proposed in the literature. We also show that, if these parametric estimations are incorporated in the acquisition model, the resulting PSF outperforms both the parametric and non-parametric methods.



### Generative Modeling of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1412.6296v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.6296v2)
- **Published**: 2014-12-19 11:34:37+00:00
- **Updated**: 2015-04-09 15:07:06+00:00
- **Authors**: Jifeng Dai, Yang Lu, Ying-Nian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: The convolutional neural networks (CNNs) have proven to be a powerful tool for discriminative learning. Recently researchers have also started to show interest in the generative aspects of CNNs in order to gain a deeper understanding of what they have learned and how to further improve them. This paper investigates generative modeling of CNNs. The main contributions include: (1) We construct a generative model for the CNN in the form of exponential tilting of a reference distribution. (2) We propose a generative gradient for pre-training CNNs by a non-parametric importance sampling scheme, which is fundamentally different from the commonly used discriminative gradient, and yet has the same computational architecture and cost as the latter. (3) We propose a generative visualization method for the CNNs by sampling from an explicit parametric image distribution. The proposed visualization method can directly draw synthetic samples for any given node in a trained CNN by the Hamiltonian Monte Carlo (HMC) algorithm, without resorting to any extra hold-out images. Experiments on the challenging ImageNet benchmark show that the proposed generative gradient pre-training consistently helps improve the performances of CNNs, and the proposed generative visualization method generates meaningful and varied samples of synthetic images from a large-scale deep CNN.



### Py3DFreeHandUS: a library for voxel-array reconstruction using Ultrasonography and attitude sensors
- **Arxiv ID**: http://arxiv.org/abs/1412.6391v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CE
- **Links**: [PDF](http://arxiv.org/pdf/1412.6391v1)
- **Published**: 2014-12-19 15:47:47+00:00
- **Updated**: 2014-12-19 15:47:47+00:00
- **Authors**: Davide Monari, Francesco Cenni, Erwin Aertbeliën, Kaat Desloovere
- **Comment**: Part of the Proceedings of the 7th European Conference on Python in
  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)
- **Journal**: None
- **Summary**: In medical imaging, there is a growing interest to provide real-time images with good quality for large anatomical structures. To cope with this issue, we developed a library that allows to replace, for some specific clinical applications, more robust systems such as Computer Tomography (CT) and Magnetic Resonance Imaging (MRI). Our python library Py3DFreeHandUS is a package for processing data acquired simultaneously by ultra-sonographic systems (US) and marker-based optoelectronic systems. In particular, US data enables to visualize subcutaneous body structures, whereas the optoelectronic system is able to collect the 3D position in space for reflective objects, that are called markers. By combining these two measurement devices, it is possible to reconstruct the real 3D morphology of body structures such as muscles, for relevant clinical implications. In the present research work, the different steps which allow to obtain a relevant 3D data set as well as the procedures for calibrating the systems and for determining the quality of the reconstruction.



### Simplified firefly algorithm for 2D image key-points search
- **Arxiv ID**: http://arxiv.org/abs/1412.6464v1
- **DOI**: 10.1109/CIHLI.2014.7013395
- **Categories**: **cs.NE**, cs.AI, cs.CV, 68T05, 68T10, 68T45, 68U10, 68W25, 68W99, I.2.6; I.2.10; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1412.6464v1)
- **Published**: 2014-12-19 18:00:11+00:00
- **Updated**: 2014-12-19 18:00:11+00:00
- **Authors**: Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana, Zbigniew Marszałek, Dawid Połap, Marcin Woźniak
- **Comment**: Published version on: 2014 IEEE Symposium on Computational
  Intelligence for Human-like Intelligence
- **Journal**: IEEE Symposium on Computational Intelligence for Human-like
  Intelligence, pp. 118-125, 2014
- **Summary**: In order to identify an object, human eyes firstly search the field of view for points or areas which have particular properties. These properties are used to recognise an image or an object. Then this process could be taken as a model to develop computer algorithms for images identification. This paper proposes the idea of applying the simplified firefly algorithm to search for key-areas in 2D images. For a set of input test images the proposed version of firefly algorithm has been examined. Research results are presented and discussed to show the efficiency of this evolutionary computation method.



### Learning to Segment Moving Objects in Videos
- **Arxiv ID**: http://arxiv.org/abs/1412.6504v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.6504v2)
- **Published**: 2014-12-19 20:01:16+00:00
- **Updated**: 2015-05-08 02:05:52+00:00
- **Authors**: Katerina Fragkiadaki, Pablo Arbelaez, Panna Felsen, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: We segment moving objects in videos by ranking spatio-temporal segment proposals according to "moving objectness": how likely they are to contain a moving object. In each video frame, we compute segment proposals using multiple figure-ground segmentations on per frame motion boundaries. We rank them with a Moving Objectness Detector trained on image and motion fields to detect moving objects and discard over/under segmentations or background parts of the scene. We extend the top ranked segments into spatio-temporal tubes using random walkers on motion affinities of dense point trajectories. Our final tube ranking consistently outperforms previous segmentation methods in the two largest video segmentation benchmarks currently available, for any number of proposals. Further, our per frame moving object proposals increase the detection rate up to 7\% over previous state-of-the-art static proposal methods.



### Pooled Motion Features for First-Person Videos
- **Arxiv ID**: http://arxiv.org/abs/1412.6505v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.6505v2)
- **Published**: 2014-12-19 20:03:00+00:00
- **Updated**: 2015-05-06 19:16:08+00:00
- **Authors**: M. S. Ryoo, Brandon Rothrock, Larry Matthies
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a new feature representation for first-person videos. In first-person video understanding (e.g., activity recognition), it is very important to capture both entire scene dynamics (i.e., egomotion) and salient local motion observed in videos. We describe a representation framework based on time series pooling, which is designed to abstract short-term/long-term changes in feature descriptor elements. The idea is to keep track of how descriptor values are changing over time and summarize them to represent motion in the activity video. The framework is general, handling any types of per-frame feature descriptors including conventional motion descriptors like histogram of optical flows (HOF) as well as appearance descriptors from more recent convolutional neural networks (CNN). We experimentally confirm that our approach clearly outperforms previous feature representations including bag-of-visual-words and improved Fisher vector (IFV) when using identical underlying feature descriptors. We also confirm that our feature representation has superior performance to existing state-of-the-art features like local spatio-temporal features and Improved Trajectory Features (originally developed for 3rd-person videos) when handling first-person videos. Multiple first-person activity datasets were tested under various settings to confirm these findings.



### Fracking Deep Convolutional Image Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1412.6537v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.6537v2)
- **Published**: 2014-12-19 21:30:32+00:00
- **Updated**: 2015-02-25 21:30:16+00:00
- **Authors**: Edgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, Francesc Moreno-Noguer
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a novel framework for learning local image descriptors in a discriminative manner. For this purpose we explore a siamese architecture of Deep Convolutional Neural Networks (CNN), with a Hinge embedding loss on the L2 distance between descriptors. Since a siamese architecture uses pairs rather than single image patches to train, there exist a large number of positive samples and an exponential number of negative samples. We propose to explore this space with a stochastic sampling of the training set, in combination with an aggressive mining strategy over both the positive and negative samples which we denote as "fracking". We perform a thorough evaluation of the architecture hyper-parameters, and demonstrate large performance gains compared to both standard CNN learning strategies, hand-crafted image descriptors like SIFT, and the state-of-the-art on learned descriptors: up to 2.5x vs SIFT and 1.5x vs the state-of-the-art in terms of the area under the curve (AUC) of the Precision-Recall curve.



### Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1412.6553v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1412.6553v3)
- **Published**: 2014-12-19 23:02:43+00:00
- **Updated**: 2015-04-24 11:40:54+00:00
- **Authors**: Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discriminative fine-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is fine-tuned on the training data using standard backpropagation process.   We evaluate this approach on two CNNs and show that it is competitive with previous approaches, leading to higher obtained CPU speedups at the cost of lower accuracy drops for the smaller of the two networks. Thus, for the 36-class character classification CNN, our approach obtains a 8.5x CPU speedup of the whole network with only minor accuracy drop (1% from 91% to 90%). For the standard ImageNet architecture (AlexNet), the approach speeds up the second convolution layer by a factor of 4x at the cost of $1\%$ increase of the overall top-5 classification error.



