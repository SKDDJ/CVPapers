# Arxiv Papers in cs.CV on 2014-12-21
### SENNS: Sparse Extraction Neural NetworkS for Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/1412.6749v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE, math.OC, stat.ML, 90-08
- **Links**: [PDF](http://arxiv.org/pdf/1412.6749v1)
- **Published**: 2014-12-21 09:28:05+00:00
- **Updated**: 2014-12-21 09:28:05+00:00
- **Authors**: Abdulrahman Oladipupo Ibraheem
- **Comment**: Eighteen pages in all, but much of the central ideas are covered in
  the first five and a half pages; most of the remaining pages are devoted to
  straightforward mathematical derivations, and the presentation of three
  algorithms. Manuscript contains no figures at this time
- **Journal**: None
- **Summary**: By drawing on ideas from optimisation theory, artificial neural networks (ANN), graph embeddings and sparse representations, I develop a novel technique, termed SENNS (Sparse Extraction Neural NetworkS), aimed at addressing the feature extraction problem. The proposed method uses (preferably deep) ANNs for projecting input attribute vectors to an output space wherein pairwise distances are maximized for vectors belonging to different classes, but minimized for those belonging to the same class, while simultaneously enforcing sparsity on the ANN outputs. The vectors that result from the projection can then be used as features in any classifier of choice. Mathematically, I formulate the proposed method as the minimisation of an objective function which can be interpreted, in the ANN output space, as a negative factor of the sum of the squares of the pair-wise distances between output vectors belonging to different classes, added to a positive factor of the sum of squares of the pair-wise distances between output vectors belonging to the same classes, plus sparsity and weight decay terms. To derive an algorithm for minimizing the objective function via gradient descent, I use the multi-variate version of the chain rule to obtain the partial derivatives of the function with respect to ANN weights and biases, and find that each of the required partial derivatives can be expressed as a sum of six terms. As it turns out, four of those six terms can be computed using the standard back propagation algorithm; the fifth can be computed via a slight modification of the standard backpropagation algorithm; while the sixth one can be computed via simple arithmetic. Finally, I propose experiments on the ARABASE Arabic corpora of digits and letters, the CMU PIE database of faces, the MNIST digits database, and other standard machine learning databases.



### Bi-directional Shape Correspondences (BSC): A Novel Technique for 2-d Shape Warping in Quadratic Time?
- **Arxiv ID**: http://arxiv.org/abs/1412.6759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.6759v1)
- **Published**: 2014-12-21 10:28:36+00:00
- **Updated**: 2014-12-21 10:28:36+00:00
- **Authors**: Abdulrahman Oladipupo Ibraheem
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Bidirectional Shape Correspondence (BSC) as a possible improvement on the famous shape contexts (SC) framework. Our proposals derive from the observation that the SC framework enforces a one-to-one correspondence between sample points, and that this leads to two possible drawbacks. First, this denies the framework the opportunity to effect advantageous many-to-many matching between points on the two shapes being compared. Second, this calls for the Hungarian algorithm which unfortunately usurps cubic time. While the dynamic-space-warping dynamic programming algorithm has provided a standard solution to the first problem above, it demands quintic time for general multi-contour shapes, and w times quadratic time for the special case of single-contour shapes, even after an heuristic search window of width w has been chosen. Therefore, in this work, we propose a simple method for computing "many-to-many" correspondences for the class of all 2-d shapes in quadratic time. Our approach is to explicitly let each point on the first shape choose a best match on the second shape, and vice versa. Along the way, we also propose the use of data-clustering techniques for dealing with the outliers problem, and, from another viewpoint, it turns out that this clustering can be seen as an autonomous, rather than pre-computed, sampling of shape boundary.



### Mixture of Parts Revisited: Expressive Part Interactions for Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1412.6791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.6791v1)
- **Published**: 2014-12-21 14:48:41+00:00
- **Updated**: 2014-12-21 14:48:41+00:00
- **Authors**: Anoop Katti, Anurag Mittal
- **Comment**: None
- **Journal**: None
- **Summary**: Part-based models with restrictive tree-structured interactions for the Human Pose Estimation problem, leaves many part interactions unhandled. Two of the most common and strong manifestations of such unhandled interactions are self-occlusion among the parts and the confusion in the localization of the non-adjacent symmetric parts. By handling the self-occlusion in a data efficient manner, we improve the performance of the basic Mixture of Parts model by a large margin, especially on uncommon poses. Through addressing the confusion in the symmetric limb localization using a combination of two complementing trees, we improve the performance on all the parts by atmost doubling the running time. Finally, we show that the combination of the two solutions improves the results. We report results that are equivalent to the state-of-the-art on two standard datasets. Because of maintaining the tree-structured interactions and only part-level modeling of the base Mixture of Parts model, this is achieved in time that is much less than the best performing part-based model.



### Striving for Simplicity: The All Convolutional Net
- **Arxiv ID**: http://arxiv.org/abs/1412.6806v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.6806v3)
- **Published**: 2014-12-21 16:16:37+00:00
- **Updated**: 2015-04-13 07:58:17+00:00
- **Authors**: Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller
- **Comment**: accepted to ICLR-2015 workshop track; no changes other than style
- **Journal**: None
- **Summary**: Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.



### Learning the nonlinear geometry of high-dimensional data: Models and algorithms
- **Arxiv ID**: http://arxiv.org/abs/1412.6808v2
- **DOI**: 10.1109/TSP.2015.2469637
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1412.6808v2)
- **Published**: 2014-12-21 16:40:31+00:00
- **Updated**: 2015-08-10 02:12:06+00:00
- **Authors**: Tong Wu, Waheed U. Bajwa
- **Comment**: Extended version of the journal paper accepted for publication in
  IEEE Trans. Signal Processing (20 pages, 7 figures, 4 tables)
- **Journal**: IEEE Trans. Signal Processing, vol. 63, no. 23, pp. 6229-6244,
  Dec. 2015
- **Summary**: Modern information processing relies on the axiom that high-dimensional data lie near low-dimensional geometric structures. This paper revisits the problem of data-driven learning of these geometric structures and puts forth two new nonlinear geometric models for data describing "related" objects/phenomena. The first one of these models straddles the two extremes of the subspace model and the union-of-subspaces model, and is termed the metric-constrained union-of-subspaces (MC-UoS) model. The second one of these models---suited for data drawn from a mixture of nonlinear manifolds---generalizes the kernel subspace model, and is termed the metric-constrained kernel union-of-subspaces (MC-KUoS) model. The main contributions of this paper in this regard include the following. First, it motivates and formalizes the problems of MC-UoS and MC-KUoS learning. Second, it presents algorithms that efficiently learn an MC-UoS or an MC-KUoS underlying data of interest. Third, it extends these algorithms to the case when parts of the data are missing. Last, but not least, it reports the outcomes of a series of numerical experiments involving both synthetic and real data that demonstrate the superiority of the proposed geometric models and learning algorithms over existing approaches in the literature. These experiments also help clarify the connections between this work and the literature on (subspace and kernel k-means) clustering.



### A Stable Multi-Scale Kernel for Topological Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1412.6821v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, math.AT
- **Links**: [PDF](http://arxiv.org/pdf/1412.6821v1)
- **Published**: 2014-12-21 19:17:08+00:00
- **Updated**: 2014-12-21 19:17:08+00:00
- **Authors**: Jan Reininghaus, Stefan Huber, Ulrich Bauer, Roland Kwitt
- **Comment**: None
- **Journal**: None
- **Summary**: Topological data analysis offers a rich source of valuable information to study vision problems. Yet, so far we lack a theoretically sound connection to popular kernel-based learning techniques, such as kernel SVMs or kernel PCA. In this work, we establish such a connection by designing a multi-scale kernel for persistence diagrams, a stable summary representation of topological features in data. We show that this kernel is positive definite and prove its stability with respect to the 1-Wasserstein distance. Experiments on two benchmark datasets for 3D shape classification/retrieval and texture recognition show considerable performance gains of the proposed method compared to an alternative approach that is based on the recently introduced persistence landscapes.



### Learning Activation Functions to Improve Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1412.6830v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1412.6830v3)
- **Published**: 2014-12-21 20:20:21+00:00
- **Updated**: 2015-04-21 08:05:02+00:00
- **Authors**: Forest Agostinelli, Matthew Hoffman, Peter Sadowski, Pierre Baldi
- **Comment**: Accepted as a workshop paper contribution at the International
  Conference on Learning Representations (ICLR) 2015
- **Journal**: None
- **Summary**: Artificial neural networks typically have a fixed, non-linear activation function at each neuron. We have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes.



