# Arxiv Papers in cs.CV on 2014-10-21
### Learning to Rank Binary Codes
- **Arxiv ID**: http://arxiv.org/abs/1410.5524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1410.5524v1)
- **Published**: 2014-10-21 03:09:17+00:00
- **Updated**: 2014-10-21 03:09:17+00:00
- **Authors**: Jie Feng, Wei Liu, Yan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Binary codes have been widely used in vision problems as a compact feature representation to achieve both space and time advantages. Various methods have been proposed to learn data-dependent hash functions which map a feature vector to a binary code. However, considerable data information is inevitably lost during the binarization step which also causes ambiguity in measuring sample similarity using Hamming distance. Besides, the learned hash functions cannot be changed after training, which makes them incapable of adapting to new data outside the training data set. To address both issues, in this paper we propose a flexible bitwise weight learning framework based on the binary codes obtained by state-of-the-art hashing methods, and incorporate the learned weights into the weighted Hamming distance computation. We then formulate the proposed framework as a ranking problem and leverage the Ranking SVM model to offline tackle the weight learning. The framework is further extended to an online mode which updates the weights at each time new data comes, thereby making it scalable to large and dynamic data sets. Extensive experimental results demonstrate significant performance gains of using binary codes with bitwise weighting in image retrieval tasks. It is appealing that the online weight learning leads to comparable accuracy with its offline counterpart, which thus makes our approach practical for realistic applications.



### Mobility Enhancement for Elderly
- **Arxiv ID**: http://arxiv.org/abs/1410.5600v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1410.5600v1)
- **Published**: 2014-10-21 09:59:33+00:00
- **Updated**: 2014-10-21 09:59:33+00:00
- **Authors**: Ramviyas Parasuraman
- **Comment**: Masters thesis, Indian Institute of Technology Delhi
- **Journal**: None
- **Summary**: Loss of Mobility is a common handicap to senior citizens. It denies them the ease of movement they would like to have like outdoor visits, movement in hospitals, social outgoings, but more seriously in the day to day in-house routine functions necessary for living etc. Trying to overcome this handicap by means of servant or domestic help and simple wheel chairs is not only costly in the long run, but forces the senior citizen to be at the mercy of sincerity of domestic helps and also the consequent loss of dignity. In order to give a dignified life, the mobility obtained must be at the complete discretion, will and control of the senior citizen. This can be provided only by a reasonably sophisticated and versatile wheel chair, giving enhanced ability of vision, hearing through man-machine interface, and sensor aided navigation and control. More often than not senior people have poor vision which makes it difficult for them to maker visual judgement and so calls for the use of Artificial Intelligence in visual image analysis and guided navigation systems.   In this project, we deal with two important enhancement features for mobility enhancement, Audio command and Vision aided obstacle detection and navigation. We have implemented speech recognition algorithm using template of stored words for identifying the voice command given by the user. This frees the user of an agile hand to operate joystick or mouse control. Also, we have developed a new appearance based obstacle detection system using stereo-vision cameras which estimates the distance of nearest obstacle to the wheel chair and takes necessary action. This helps user in making better judgement of route and navigate obstacles. The main challenge in this project is how to navigate in an unknown/unfamiliar environment by avoiding obstacles.



### Attentive monitoring of multiple video streams driven by a Bayesian foraging strategy
- **Arxiv ID**: http://arxiv.org/abs/1410.5605v3
- **DOI**: 10.1109/TIP.2015.2431438
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1410.5605v3)
- **Published**: 2014-10-21 10:13:51+00:00
- **Updated**: 2015-04-27 13:02:21+00:00
- **Authors**: Paolo Napoletano, Giuseppe Boccignone, Francesco Tisato
- **Comment**: Accepted to IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: In this paper we shall consider the problem of deploying attention to subsets of the video streams for collating the most relevant data and information of interest related to a given task. We formalize this monitoring problem as a foraging problem. We propose a probabilistic framework to model observer's attentive behavior as the behavior of a forager. The forager, moment to moment, focuses its attention on the most informative stream/camera, detects interesting objects or activities, or switches to a more profitable stream. The approach proposed here is suitable to be exploited for multi-stream video summarization. Meanwhile, it can serve as a preliminary step for more sophisticated video surveillance, e.g. activity and behavior analysis. Experimental results achieved on the UCR Videoweb Activities Dataset, a publicly available dataset, are presented to illustrate the utility of the proposed technique.



### Compositional Structure Learning for Action Understanding
- **Arxiv ID**: http://arxiv.org/abs/1410.5861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1410.5861v1)
- **Published**: 2014-10-21 21:25:45+00:00
- **Updated**: 2014-10-21 21:25:45+00:00
- **Authors**: Ran Xu, Gang Chen, Caiming Xiong, Wei Chen, Jason J. Corso
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: The focus of the action understanding literature has predominately been classification, how- ever, there are many applications demanding richer action understanding such as mobile robotics and video search, with solutions to classification, localization and detection. In this paper, we propose a compositional model that leverages a new mid-level representation called compositional trajectories and a locally articulated spatiotemporal deformable parts model (LALSDPM) for fully action understanding. Our methods is advantageous in capturing the variable structure of dynamic human activity over a long range. First, the compositional trajectories capture long-ranging, frequently co-occurring groups of trajectories in space time and represent them in discriminative hierarchies, where human motion is largely separated from camera motion; second, LASTDPM learns a structured model with multi-layer deformable parts to capture multiple levels of articulated motion. We implement our methods and demonstrate state of the art performance on all three problems: action detection, localization, and recognition.



