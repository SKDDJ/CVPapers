# Arxiv Papers in cs.CV on 2014-10-04
### Learning Invariant Color Features for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1410.1035v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1410.1035v2)
- **Published**: 2014-10-04 10:27:51+00:00
- **Updated**: 2014-10-09 10:32:36+00:00
- **Authors**: Rahul Rama Varior, Gang Wang, Jiwen Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Matching people across multiple camera views known as person re-identification, is a challenging problem due to the change in visual appearance caused by varying lighting conditions. The perceived color of the subject appears to be different with respect to illumination. Previous works use color as it is or address these challenges by designing color spaces focusing on a specific cue. In this paper, we propose a data driven approach for learning color patterns from pixels sampled from images across two camera views. The intuition behind this work is that, even though pixel values of same color would be different across views, they should be encoded with the same values. We model color feature generation as a learning problem by jointly learning a linear transformation and a dictionary to encode pixel values. We also analyze different photometric invariant color spaces. Using color as the only cue, we compare our approach with all the photometric invariant color spaces and show superior performance over all of them. Combining with other learned low-level and high-level features, we obtain promising results in ViPER, Person Re-ID 2011 and CAVIAR4REID datasets.



### Facial Feature Point Detection: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/1410.1037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1410.1037v1)
- **Published**: 2014-10-04 11:04:50+00:00
- **Updated**: 2014-10-04 11:04:50+00:00
- **Authors**: Nannan Wang, Xinbo Gao, Dacheng Tao, Xuelong Li
- **Comment**: 32 pages, 13 figures
- **Journal**: None
- **Summary**: This paper presents a comprehensive survey of facial feature point detection with the assistance of abundant manually labeled images. Facial feature point detection favors many applications such as face recognition, animation, tracking, hallucination, expression analysis and 3D face modeling. Existing methods can be categorized into the following four groups: constrained local model (CLM)-based, active appearance model (AAM)-based, regression-based, and other methods. CLM-based methods consist of a shape model and a number of local experts, each of which is utilized to detect a facial feature point. AAM-based methods fit a shape model to an image by minimizing texture synthesis errors. Regression-based methods directly learn a mapping function from facial image appearance to facial feature points. Besides the above three major categories of methods, there are also minor categories of methods which we classify into other methods: graphical model-based methods, joint face alignment methods, independent facial feature point detectors, and deep learning-based methods. Though significant progress has been made, facial feature point detection is limited in its success by wild and real-world conditions: variations across poses, expressions, illuminations, and occlusions. A comparative illustration and analysis of representative methods provide us a holistic understanding and deep insight into facial feature point detection, which also motivates us to explore promising future directions.



### Explain Images with Multimodal Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1410.1090v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, I.2.6; I.2.7; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1410.1090v1)
- **Published**: 2014-10-04 20:24:34+00:00
- **Updated**: 2014-10-04 20:24:34+00:00
- **Authors**: Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan L. Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.



