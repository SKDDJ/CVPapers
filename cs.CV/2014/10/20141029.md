# Arxiv Papers in cs.CV on 2014-10-29
### Collaborative Multi-sensor Classification via Sparsity-based Representation
- **Arxiv ID**: http://arxiv.org/abs/1410.7876v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1410.7876v2)
- **Published**: 2014-10-29 05:25:44+00:00
- **Updated**: 2016-06-16 16:46:36+00:00
- **Authors**: Minh Dao, Nam H. Nguyen, Nasser M. Nasrabadi, Trac D. Tran
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a general collaborative sparse representation framework for multi-sensor classification, which takes into account the correlations as well as complementary information between heterogeneous sensors simultaneously while considering joint sparsity within each sensor's observations. We also robustify our models to deal with the presence of sparse noise and low-rank interference signals. Specifically, we demonstrate that incorporating the noise or interference signal as a low-rank component in our models is essential in a multi-sensor classification problem when multiple co-located sources/sensors simultaneously record the same physical event. We further extend our frameworks to kernelized models which rely on sparsely representing a test sample in terms of all the training samples in a feature space induced by a kernel function. A fast and efficient algorithm based on alternative direction method is proposed where its convergence to an optimal solution is guaranteed. Extensive experiments are conducted on several real multi-sensor data sets and results are compared with the conventional classifiers to verify the effectiveness of the proposed methods.



### Extended Dynamic Programming and Fast Multidimensional Search Algorithm for Energy Minization in Stereo and Motion
- **Arxiv ID**: http://arxiv.org/abs/1410.7922v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1410.7922v1)
- **Published**: 2014-10-29 10:31:27+00:00
- **Updated**: 2014-10-29 10:31:27+00:00
- **Authors**: Mikhail G. Mozerov
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: This paper presents a novel extended dynamic programming approach for energy minimization (EDP) to solve the correspondence problem for stereo and motion. A significant speedup is achieved using a recursive minimum search strategy (RMS). The mentioned speedup is particularly important if the disparity space is 2D as well as 3D. The proposed RMS can also be applied in the well-known dynamic programming (DP) approach for stereo and motion. In this case, the general 2D problem of the global discrete energy minimization is reduced to several mutually independent sub-problems of the one-dimensional minimization. The EDP method is used when the approximation of the general 2D discrete energy minimization problem is considered. Then the RMS algorithm is an essential part of the EDP method. Using the EDP algorithm we obtain a lower energy bound than the graph cuts (GC) expansion technique on stereo and motion problems. The proposed calculation scheme possesses natural parallelism and can be realized on graphics processing unit (GPU) platforms, and can be potentially restricted further by the number of scanlines in the image plane. Furthermore, the RMS and EDP methods can be used in any optimization problem where the objective function meets specific conditions in the smoothness term.



### Towards a Visual Turing Challenge
- **Arxiv ID**: http://arxiv.org/abs/1410.8027v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1410.8027v3)
- **Published**: 2014-10-29 15:38:29+00:00
- **Updated**: 2015-05-05 18:03:56+00:00
- **Authors**: Mateusz Malinowski, Mario Fritz
- **Comment**: Published in the NIPS 2014 Workshop on Learning Semantics
- **Journal**: None
- **Summary**: As language and visual understanding by machines progresses rapidly, we are observing an increasing interest in holistic architectures that tightly interlink both modalities in a joint learning and inference process. This trend has allowed the community to progress towards more challenging and open tasks and refueled the hope at achieving the old AI dream of building machines that could pass a turing test in open domains. In order to steadily make progress towards this goal, we realize that quantifying performance becomes increasingly difficult. Therefore we ask how we can precisely define such challenges and how we can evaluate different algorithms on this open tasks? In this paper, we summarize and discuss such challenges as well as try to give answers where appropriate options are available in the literature. We exemplify some of the solutions on a recently presented dataset of question-answering task based on real-world indoor images that establishes a visual turing challenge. Finally, we argue despite the success of unique ground-truth annotation, we likely have to step away from carefully curated dataset and rather rely on 'social consensus' as the main driving force to create suitable benchmarks. Providing coverage in this inherently ambiguous output space is an emerging challenge that we face in order to make quantifiable progress in this area.



### A comparison of dense region detectors for image search and fine-grained classification
- **Arxiv ID**: http://arxiv.org/abs/1410.8151v3
- **DOI**: 10.1109/TIP.2015.2423557
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1410.8151v3)
- **Published**: 2014-10-29 20:18:41+00:00
- **Updated**: 2015-04-17 18:36:17+00:00
- **Authors**: Ahmet Iscen, Giorgos Tolias, Philippe-Henri Gosselin, Hervé Jégou
- **Comment**: Accepted to IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: We consider a pipeline for image classification or search based on coding approaches like Bag of Words or Fisher vectors. In this context, the most common approach is to extract the image patches regularly in a dense manner on several scales. This paper proposes and evaluates alternative choices to extract patches densely. Beyond simple strategies derived from regular interest region detectors, we propose approaches based on super-pixels, edges, and a bank of Zernike filters used as detectors. The different approaches are evaluated on recent image retrieval and fine-grain classification benchmarks. Our results show that the regular dense detector is outperformed by other methods in most situations, leading us to improve the state of the art in comparable setups on standard retrieval and fined-grain benchmarks. As a byproduct of our study, we show that existing methods for blob and super-pixel extraction achieve high accuracy if the patches are extracted along the edges and not around the detected regions.



### Power-Law Graph Cuts
- **Arxiv ID**: http://arxiv.org/abs/1411.1971v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1411.1971v2)
- **Published**: 2014-10-29 20:46:20+00:00
- **Updated**: 2014-11-25 21:41:07+00:00
- **Authors**: Xiangyang Zhou, Jiaxin Zhang, Brian Kulis
- **Comment**: None
- **Journal**: None
- **Summary**: Algorithms based on spectral graph cut objectives such as normalized cuts, ratio cuts and ratio association have become popular in recent years because they are widely applicable and simple to implement via standard eigenvector computations. Despite strong performance for a number of clustering tasks, spectral graph cut algorithms still suffer from several limitations: first, they require the number of clusters to be known in advance, but this information is often unknown a priori; second, they tend to produce clusters with uniform sizes. In some cases, the true clusters exhibit a known size distribution; in image segmentation, for instance, human-segmented images tend to yield segment sizes that follow a power-law distribution. In this paper, we propose a general framework of power-law graph cut algorithms that produce clusters whose sizes are power-law distributed, and also does not fix the number of clusters upfront. To achieve our goals, we treat the Pitman-Yor exchangeable partition probability function (EPPF) as a regularizer to graph cut objectives. Because the resulting objectives cannot be solved by relaxing via eigenvectors, we derive a simple iterative algorithm to locally optimize the objectives. Moreover, we show that our proposed algorithm can be viewed as performing MAP inference on a particular Pitman-Yor mixture model. Our experiments on various data sets show the effectiveness of our algorithms.



