# Arxiv Papers in cs.CV on 2014-10-03
### HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1410.0736v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1410.0736v4)
- **Published**: 2014-10-03 01:17:20+00:00
- **Updated**: 2015-05-16 03:36:32+00:00
- **Authors**: Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis DeCoste, Wei Di, Yizhou Yu
- **Comment**: Add new results on ImageNet using VGG-16-layer building block net
- **Journal**: None
- **Summary**: In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HD-CNN training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers and layer parameter compression make HD-CNNs scalable for large-scale visual recognition. We achieve state-of-the-art results on both CIFAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65%, 3.1% and 1.1%, respectively.



### Im2Fit: Fast 3D Model Fitting and Anthropometrics using Single Consumer Depth Camera and Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1410.0745v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1410.0745v2)
- **Published**: 2014-10-03 02:33:08+00:00
- **Updated**: 2014-11-19 20:30:32+00:00
- **Authors**: Qiaosong Wang, Vignesh Jagadeesh, Bryan Ressler, Robinson Piramuthu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in consumer depth sensors have created many opportunities for human body measurement and modeling. Estimation of 3D body shape is particularly useful for fashion e-commerce applications such as virtual try-on or fit personalization. In this paper, we propose a method for capturing accurate human body shape and anthropometrics from a single consumer grade depth sensor. We first generate a large dataset of synthetic 3D human body models using real-world body size distributions. Next, we estimate key body measurements from a single monocular depth image. We combine body measurement estimates with local geometry features around key joint positions to form a robust multi-dimensional feature vector. This allows us to conduct a fast nearest-neighbor search to every sample in the dataset and return the closest one. Compared to existing methods, our approach is able to predict accurate full body parameters from a partial view using measurement parameters learned from the synthetic dataset. Furthermore, our system is capable of generating 3D human mesh models in real-time, which is significantly faster than methods which attempt to model shape and pose deformations. To validate the efficiency and applicability of our system, we collected a dataset that contains frontal and back scans of 83 clothed people with ground truth height and weight. Experiments on real-world dataset show that the proposed method can achieve real-time performance with competing results achieving an average error of 1.9 cm in estimated measurements.



### Feature Learning from Incomplete EEG with Denoising Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1410.0818v1
- **DOI**: 10.1016/j.neucom.2014.08.092
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1410.0818v1)
- **Published**: 2014-10-03 11:12:47+00:00
- **Updated**: 2014-10-03 11:12:47+00:00
- **Authors**: Junhua Li, Zbigniew Struzik, Liqing Zhang, Andrzej Cichocki
- **Comment**: The paper was accepted for publication by Neurocomputing
- **Journal**: Neurocomputing, 2015, 165: 23-31
- **Summary**: An alternative pathway for the human brain to communicate with the outside world is by means of a brain computer interface (BCI). A BCI can decode electroencephalogram (EEG) signals of brain activities, and then send a command or an intent to an external interactive device, such as a wheelchair. The effectiveness of the BCI depends on the performance in decoding the EEG. Usually, the EEG is contaminated by different kinds of artefacts (e.g., electromyogram (EMG), background activity), which leads to a low decoding performance. A number of filtering methods can be utilized to remove or weaken the effects of artefacts, but they generally fail when the EEG contains extreme artefacts. In such cases, the most common approach is to discard the whole data segment containing extreme artefacts. This causes the fatal drawback that the BCI cannot output decoding results during that time. In order to solve this problem, we employ the Lomb-Scargle periodogram to estimate the spectral power from incomplete EEG (after removing only parts contaminated by artefacts), and Denoising Autoencoder (DAE) for learning. The proposed method is evaluated with motor imagery EEG data. The results show that our method can successfully decode incomplete EEG to good effect.



### Group Orbit Optimization: A Unified Approach to Data Normalization
- **Arxiv ID**: http://arxiv.org/abs/1410.0868v1
- **DOI**: None
- **Categories**: **cs.NA**, cs.CV, math.NA, 15-02, G.1.3; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1410.0868v1)
- **Published**: 2014-10-03 14:43:01+00:00
- **Updated**: 2014-10-03 14:43:01+00:00
- **Authors**: Shuchang Zhou, Zhihua Zhang, Xiaobing Feng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose and study an optimization problem over a matrix group orbit that we call \emph{Group Orbit Optimization} (GOO). We prove that GOO can be used to induce matrix decomposition techniques such as singular value decomposition (SVD), LU decomposition, QR decomposition, Schur decomposition and Cholesky decomposition, etc. This gives rise to a unified framework for matrix decomposition and allows us to bridge these matrix decomposition methods. Moreover, we generalize GOO for tensor decomposition. As a concrete application of GOO, we devise a new data decomposition method over a special linear group to normalize point cloud data. Experiment results show that our normalization method is able to obtain recovery well from distortions like shearing, rotation and squeezing.



### A Framework for the Volumetric Integration of Depth Images
- **Arxiv ID**: http://arxiv.org/abs/1410.0925v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1410.0925v3)
- **Published**: 2014-10-03 17:42:14+00:00
- **Updated**: 2014-10-23 20:48:42+00:00
- **Authors**: Victor Adrian Prisacariu, Olaf KÃ¤hler, Ming Ming Cheng, Carl Yuheng Ren, Julien Valentin, Philip H. S. Torr, Ian D. Reid, David W. Murray
- **Comment**: 17 pages, 8 figures
- **Journal**: None
- **Summary**: Volumetric models have become a popular representation for 3D scenes in recent years. One of the breakthroughs leading to their popularity was KinectFusion, where the focus is on 3D reconstruction using RGB-D sensors. However, monocular SLAM has since also been tackled with very similar approaches. Representing the reconstruction volumetrically as a truncated signed distance function leads to most of the simplicity and efficiency that can be achieved with GPU implementations of these systems. However, this representation is also memory-intensive and limits the applicability to small scale reconstructions. Several avenues have been explored for overcoming this limitation. With the aim of summarizing them and providing for a fast and flexible 3D reconstruction pipeline, we propose a new, unifying framework called InfiniTAM. The core idea is that individual steps like camera tracking, scene representation and integration of new data can easily be replaced and adapted to the needs of the user. Along with the framework we also provide a set of components for scalable reconstruction: two implementations of camera trackers, based on RGB data and on depth data, two representations of the 3D volumetric data, a dense volume and one based on hashes of subblocks, and an optional module for swapping subblocks in and out of the typically limited GPU memory.



