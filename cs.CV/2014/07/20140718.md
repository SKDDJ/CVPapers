# Arxiv Papers in cs.CV on 2014-07-18
### Analysis of Gait Pattern to Recognize the Human Activities
- **Arxiv ID**: http://arxiv.org/abs/1407.4867v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1407.4867v2)
- **Published**: 2014-07-18 01:45:17+00:00
- **Updated**: 2014-08-14 12:10:04+00:00
- **Authors**: Jay Prakash Gupta, Pushkar Dixit, Nishant Singh, Vijay Bhaskar Semwal
- **Comment**: This paper has been withdrawn by the author due to a crucial sign
  error in equation 3
- **Journal**: None
- **Summary**: Human activity recognition based on the computer vision is the process of labelling image sequences with action labels. Accurate systems for this problem are applied in areas such as visual surveillance, human computer interaction and video retrieval.



### Affine Subspace Representation for Feature Description
- **Arxiv ID**: http://arxiv.org/abs/1407.4874v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1407.4874v1)
- **Published**: 2014-07-18 02:33:56+00:00
- **Updated**: 2014-07-18 02:33:56+00:00
- **Authors**: Zhenhua Wang, Bin Fan, Fuchao Wu
- **Comment**: To Appear in the 2014 European Conference on Computer Vision
- **Journal**: None
- **Summary**: This paper proposes a novel Affine Subspace Representation (ASR) descriptor to deal with affine distortions induced by viewpoint changes. Unlike the traditional local descriptors such as SIFT, ASR inherently encodes local information of multi-view patches, making it robust to affine distortions while maintaining a high discriminative ability. To this end, PCA is used to represent affine-warped patches as PCA-patch vectors for its compactness and efficiency. Then according to the subspace assumption, which implies that the PCA-patch vectors of various affine-warped patches of the same keypoint can be represented by a low-dimensional linear subspace, the ASR descriptor is obtained by using a simple subspace-to-point mapping. Such a linear subspace representation could accurately capture the underlying information of a keypoint (local structure) under multiple views without sacrificing its distinctiveness. To accelerate the computation of ASR descriptor, a fast approximate algorithm is proposed by moving the most computational part (ie, warp patch under various affine transformations) to an offline training stage. Experimental results show that ASR is not only better than the state-of-the-art descriptors under various image transformations, but also performs well without a dedicated affine invariant detector when dealing with viewpoint changes.



### Hand Pointing Detection Using Live Histogram Template of Forehead Skin
- **Arxiv ID**: http://arxiv.org/abs/1407.4898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1407.4898v1)
- **Published**: 2014-07-18 07:10:03+00:00
- **Updated**: 2014-07-18 07:10:03+00:00
- **Authors**: Ghassem Tofighi, Nasser Ali Afarin, Kamraan Raahemifar, Anastasios N. Venetsanopoulos
- **Comment**: Accepted for oral presentation in DSP2014
- **Journal**: None
- **Summary**: Hand pointing detection has multiple applications in many fields such as virtual reality and control devices in smart homes. In this paper, we proposed a novel approach to detect pointing vector in 2D space of a room. After background subtraction, face and forehead is detected. In the second step, forehead skin H-S plane histograms in HSV space is calculated. By using these histogram templates of users skin, and back projection method, skin areas are detected. The contours of hand are extracted using Freeman chain code algorithm. Next step is finding fingertips. Points in hand contour which are candidates for the fingertip can be found in convex defects of convex hull and contour. We introduced a novel method for finding the fingertip based on the special points on the contour and their relationships. Our approach detects hand-pointing vectors in live video from a common webcam with 94%TP and 85%TN.



### Deep Metric Learning for Practical Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1407.4979v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1407.4979v1)
- **Published**: 2014-07-18 13:07:16+00:00
- **Updated**: 2014-07-18 13:07:16+00:00
- **Authors**: Dong Yi, Zhen Lei, Stan Z. Li
- **Comment**: None
- **Journal**: None
- **Summary**: Various hand-crafted features and metric learning methods prevail in the field of person re-identification. Compared to these methods, this paper proposes a more general way that can learn a similarity metric from image pixels directly. By using a "siamese" deep neural network, the proposed method can jointly learn the color feature, texture feature and metric in a unified framework. The network has a symmetry structure with two sub-networks which are connected by Cosine function. To deal with the big variations of person images, binomial deviance is used to evaluate the cost between similarities and labels, which is proved to be robust to outliers.   Compared to existing researches, a more practical setting is studied in the experiments that is training and test on different datasets (cross dataset person re-identification). Both in "intra dataset" and "cross dataset" settings, the superiorities of the proposed method are illustrated on VIPeR and PRID.



### LSDA: Large Scale Detection Through Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1407.5035v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1407.5035v3)
- **Published**: 2014-07-18 17:08:02+00:00
- **Updated**: 2014-11-01 01:48:26+00:00
- **Authors**: Judy Hoffman, Sergio Guadarrama, Eric Tzeng, Ronghang Hu, Jeff Donahue, Ross Girshick, Trevor Darrell, Kate Saenko
- **Comment**: None
- **Journal**: Neural Information Processing Systems (NIPS) 2014
- **Summary**: A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2M+ labeled classification images. Unfortunately, only a small fraction of those labels are available for the detection task. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. In this paper, we propose Large Scale Detection through Adaptation (LSDA), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors. Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge demonstrates the efficacy of our approach. This algorithm enables us to produce a >7.6K detector by using available classification data from leaf nodes in the ImageNet tree. We additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6K detector). Models and software are available at



### Pixels to Voxels: Modeling Visual Representation in the Human Brain
- **Arxiv ID**: http://arxiv.org/abs/1407.5104v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1407.5104v1)
- **Published**: 2014-07-18 20:10:06+00:00
- **Updated**: 2014-07-18 20:10:06+00:00
- **Authors**: Pulkit Agrawal, Dustin Stansbury, Jitendra Malik, Jack L. Gallant
- **Comment**: None
- **Journal**: None
- **Summary**: The human brain is adept at solving difficult high-level visual processing problems such as image interpretation and object recognition in natural scenes. Over the past few years neuroscientists have made remarkable progress in understanding how the human brain represents categories of objects and actions in natural scenes. However, all current models of high-level human vision operate on hand annotated images in which the objects and actions have been assigned semantic tags by a human operator. No current models can account for high-level visual function directly in terms of low-level visual input (i.e., pixels). To overcome this fundamental limitation we sought to develop a new class of models that can predict human brain activity directly from low-level visual input (i.e., pixels). We explored two classes of models drawn from computer vision and machine learning. The first class of models was based on Fisher Vectors (FV) and the second was based on Convolutional Neural Networks (ConvNets). We find that both classes of models accurately predict brain activity in high-level visual areas, directly from pixels and without the need for any semantic tags or hand annotation of images. This is the first time that such a mapping has been obtained. The fit models provide a new platform for exploring the functional principles of human vision, and they show that modern methods of computer vision and machine learning provide important tools for characterizing brain function.



