# Arxiv Papers in cs.CV on 2014-07-06
### Large-scale Supervised Hierarchical Feature Learning for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1407.1490v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1407.1490v1)
- **Published**: 2014-07-06 12:45:23+00:00
- **Updated**: 2014-07-06 12:45:23+00:00
- **Authors**: Jianguo Li, Yurong Chen
- **Comment**: 8 pages; 3 figures
- **Journal**: None
- **Summary**: This paper proposes a novel face recognition algorithm based on large-scale supervised hierarchical feature learning. The approach consists of two parts: hierarchical feature learning and large-scale model learning. The hierarchical feature learning searches feature in three levels of granularity in a supervised way. First, face images are modeled by receptive field theory, and the representation is an image with many channels of Gaussian receptive maps. We activate a few most distinguish channels by supervised learning. Second, the face image is further represented by patches of picked channels, and we search from the over-complete patch pool to activate only those most discriminant patches. Third, the feature descriptor of each patch is further projected to lower dimension subspace with discriminant subspace analysis.   Learned feature of activated patches are concatenated to get a full face representation.A linear classifier is learned to separate face pairs from same subjects and different subjects. As the number of face pairs are extremely large, we introduce ADMM (alternative direction method of multipliers) to train the linear classifier on a computing cluster. Experiments show that more training samples will bring notable accuracy improvement.   We conduct experiments on FRGC and LFW. Results show that the proposed approach outperforms existing algorithms under the same protocol notably. Besides, the proposed approach is small in memory footprint, and low in computing cost, which makes it suitable for embedded applications.



### The jump set under geometric regularisation. Part 1: Basic technique and first-order denoising
- **Arxiv ID**: http://arxiv.org/abs/1407.1531v2
- **DOI**: 10.1137/140976248
- **Categories**: **math.FA**, cs.CV, 26B30, 49Q20, 65J20
- **Links**: [PDF](http://arxiv.org/pdf/1407.1531v2)
- **Published**: 2014-07-06 19:02:45+00:00
- **Updated**: 2015-04-14 09:52:51+00:00
- **Authors**: Tuomo Valkonen
- **Comment**: None
- **Journal**: None
- **Summary**: Let $u \in \mbox{BV}(\Omega)$ solve the total variation denoising problem with $L^2$-squared fidelity and data $f$. Caselles et al. [Multiscale Model. Simul. 6 (2008), 879--894] have shown the containment $\mathcal{H}^{m-1}(J_u \setminus J_f)=0$ of the jump set $J_u$ of $u$ in that of $f$. Their proof unfortunately depends heavily on the co-area formula, as do many results in this area, and as such is not directly extensible to higher-order, curvature-based, and other advanced geometric regularisers, such as total generalised variation (TGV) and Euler's elastica. These have received increased attention in recent times due to their better practical regularisation properties compared to conventional total variation or wavelets. We prove analogous jump set containment properties for a general class of regularisers. We do this with novel Lipschitz transformation techniques, and do not require the co-area formula. In the present Part 1 we demonstrate the general technique on first-order regularisers, while in Part 2 we will extend it to higher-order regularisers. In particular, we concentrate in this part on TV and, as a novelty, Huber-regularised TV. We also demonstrate that the technique would apply to non-convex TV models as well as the Perona-Malik anisotropic diffusion, if these approaches were well-posed to begin with.



