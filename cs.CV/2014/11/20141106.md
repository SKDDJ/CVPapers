# Arxiv Papers in cs.CV on 2014-11-06
### Convolutional Neural Network-based Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1411.1509v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1411.1509v1)
- **Published**: 2014-11-06 07:03:15+00:00
- **Updated**: 2014-11-06 07:03:15+00:00
- **Authors**: Zetao Chen, Obadiah Lam, Adam Jacobson, Michael Milford
- **Comment**: 8 pages, 11 figures, this paper has been accepted by 2014
  Australasian Conference on Robotics and Automation (ACRA 2014) to be held in
  University of Melbourne, Dec 2~4
- **Journal**: None
- **Summary**: Recently Convolutional Neural Networks (CNNs) have been shown to achieve state-of-the-art performance on various classification tasks. In this paper, we present for the first time a place recognition technique based on CNN models, by combining the powerful features learnt by CNNs with a spatial and sequential filter. Applying the system to a 70 km benchmark place recognition dataset we achieve a 75% increase in recall at 100% precision, significantly outperforming all previous state of the art techniques. We also conduct a comprehensive performance comparison of the utility of features from all 21 layers for place recognition, both for the benchmark dataset and for a second dataset with more significant viewpoint changes.



### Large-Margin Determinantal Point Processes
- **Arxiv ID**: http://arxiv.org/abs/1411.1537v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1411.1537v2)
- **Published**: 2014-11-06 09:14:02+00:00
- **Updated**: 2014-11-07 05:21:03+00:00
- **Authors**: Boqing Gong, Wei-lun Chao, Kristen Grauman, Fei Sha
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Determinantal point processes (DPPs) offer a powerful approach to modeling diversity in many applications where the goal is to select a diverse subset. We study the problem of learning the parameters (the kernel matrix) of a DPP from labeled training data. We make two contributions. First, we show how to reparameterize a DPP's kernel matrix with multiple kernel functions, thus enhancing modeling flexibility. Second, we propose a novel parameter estimation technique based on the principle of large margin separation. In contrast to the state-of-the-art method of maximum likelihood estimation, our large-margin loss function explicitly models errors in selecting the target subsets, and it can be customized to trade off different types of errors (precision vs. recall). Extensive empirical studies validate our contributions, including applications on challenging document and video summarization, where flexibility in modeling the kernel matrix and balancing different errors is indispensable.



### Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets
- **Arxiv ID**: http://arxiv.org/abs/1411.1752v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1411.1752v1)
- **Published**: 2014-11-06 20:07:37+00:00
- **Updated**: 2014-11-06 20:07:37+00:00
- **Authors**: Adarsh Prasad, Stefanie Jegelka, Dhruv Batra
- **Comment**: None
- **Journal**: None
- **Summary**: To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits, tradeoffs, and show that our constructions lead to significantly better proposals.



### Conditional Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1411.1784v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1411.1784v1)
- **Published**: 2014-11-06 22:33:22+00:00
- **Updated**: 2014-11-06 22:33:22+00:00
- **Authors**: Mehdi Mirza, Simon Osindero
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.



