# Arxiv Papers in cs.CV on 2014-11-24
### Vision and Learning for Deliberative Monocular Cluttered Flight
- **Arxiv ID**: http://arxiv.org/abs/1411.6326v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1411.6326v1)
- **Published**: 2014-11-24 02:09:59+00:00
- **Updated**: 2014-11-24 02:09:59+00:00
- **Authors**: Debadeepta Dey, Kumar Shaurya Shankar, Sam Zeng, Rupesh Mehta, M. Talha Agcayazi, Christopher Eriksen, Shreyansh Daftry, Martial Hebert, J. Andrew Bagnell
- **Comment**: None
- **Journal**: None
- **Summary**: Cameras provide a rich source of information while being passive, cheap and lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work we present the first implementation of receding horizon control, which is widely used in ground vehicles, with monocular vision as the only sensing mode for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a number of contributions: novel coupling of perception and control via relevant and diverse, multiple interpretations of the scene around the robot, leveraging recent advances in machine learning to showcase anytime budgeted cost-sensitive feature selection, and fast non-linear regression for monocular depth prediction. We empirically demonstrate the efficacy of our novel pipeline via real world experiments of more than 2 kms through dense trees with a quadrotor built from off-the-shelf parts. Moreover our pipeline is designed to combine information from other modalities like stereo and lidar as well if available.



### Iteratively Reweighted Graph Cut for Multi-label MRFs with Non-convex Priors
- **Arxiv ID**: http://arxiv.org/abs/1411.6340v1
- **DOI**: 10.1109/CVPR.2015.7299150
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.6340v1)
- **Published**: 2014-11-24 03:35:34+00:00
- **Updated**: 2014-11-24 03:35:34+00:00
- **Authors**: Thalaiyasingam Ajanthan, Richard Hartley, Mathieu Salzmann, Hongdong Li
- **Comment**: 9 pages, 5 figures and 6 tables
- **Journal**: CVPR, June 2015
- **Summary**: While widely acknowledged as highly effective in computer vision, multi-label MRFs with non-convex priors are difficult to optimize. To tackle this, we introduce an algorithm that iteratively approximates the original energy with an appropriately weighted surrogate energy that is easier to minimize. Our algorithm guarantees that the original energy decreases at each iteration. In particular, we consider the scenario where the global minimizer of the weighted surrogate energy can be obtained by a multi-label graph cut algorithm, and show that our algorithm then lets us handle of large variety of non-convex priors. We demonstrate the benefits of our method over state-of-the-art MRF energy minimization techniques on stereo and inpainting problems.



### On the mathematic modeling of non-parametric curves based on cubic Bézier curves
- **Arxiv ID**: http://arxiv.org/abs/1411.6365v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.6365v1)
- **Published**: 2014-11-24 07:19:17+00:00
- **Updated**: 2014-11-24 07:19:17+00:00
- **Authors**: Ha Jong Won, Choe Chun Hwa, Li Kum Song
- **Comment**: None
- **Journal**: None
- **Summary**: B\'ezier splines are widely available in various systems with the curves and surface designs. In general, the B\'ezier spline can be specified with the B\'ezier curve segments and a B\'ezier curve segment can be fitted to any number of control points. The number of control points determines the degree of the B\'ezier polynomial. This paper presents a method which determines control points for B\'ezier curves approximating segments of obtained image outline(non-parametric curve) by using the properties of cubic B\'ezier curves. Proposed method is a technique to determine the control points that has generality and reduces the error of the B\'ezier curve approximation. Main advantage of proposed method is that it has higher accuracy and compression rate than previous methods. The cubic B\'ezier spline is obtained from cubic B\'ezier curve segments. To demonstrate the various performances of the proposed algorithm, experimental results are compared.



### Scale-Invariant Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1411.6369v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1411.6369v1)
- **Published**: 2014-11-24 07:28:21+00:00
- **Updated**: 2014-11-24 07:28:21+00:00
- **Authors**: Yichong Xu, Tianjun Xiao, Jiaxing Zhang, Kuiyuan Yang, Zheng Zhang
- **Comment**: This paper is submitted for CVPR2015
- **Journal**: None
- **Summary**: Even though convolutional neural networks (CNN) has achieved near-human performance in various computer vision tasks, its ability to tolerate scale variations is limited. The popular practise is making the model bigger first, and then train it with data augmentation using extensive scale-jittering. In this paper, we propose a scaleinvariant convolutional neural network (SiCNN), a modeldesigned to incorporate multi-scale feature exaction and classification into the network structure. SiCNN uses a multi-column architecture, with each column focusing on a particular scale. Unlike previous multi-column strategies, these columns share the same set of filter parameters by a scale transformation among them. This design deals with scale variation without blowing up the model size. Experimental results show that SiCNN detects features at various scales, and the classification result exhibits strong robustness against object scale variations.



### Mid-level Deep Pattern Mining
- **Arxiv ID**: http://arxiv.org/abs/1411.6382v3
- **DOI**: 10.1109/CVPR.2015.7298699
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.6382v3)
- **Published**: 2014-11-24 08:57:16+00:00
- **Updated**: 2015-04-09 03:40:41+00:00
- **Authors**: Yao Li, Lingqiao Liu, Chunhua Shen, Anton van den Hengel
- **Comment**: Published in Proc. IEEE Conf. Computer Vision and Pattern Recognition
  2015
- **Journal**: None
- **Summary**: Mid-level visual element discovery aims to find clusters of image patches that are both representative and discriminative. In this work, we study this problem from the prospective of pattern mining while relying on the recently popularized Convolutional Neural Networks (CNNs). Specifically, we find that for an image patch, activations extracted from the first fully-connected layer of CNNs have two appealing properties which enable its seamless integration with pattern mining. Patterns are then discovered from a large number of CNN activations of image patches through the well-known association rule mining. When we retrieve and visualize image patches with the same pattern, surprisingly, they are not only visually similar but also semantically consistent. We apply our approach to scene and object classification tasks, and demonstrate that our approach outperforms all previous works on mid-level visual element discovery by a sizeable margin with far fewer elements being used. Our approach also outperforms or matches recent works using CNN for these tasks. Source code of the complete system is available online.



### Deep Convolutional Neural Fields for Depth Estimation from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1411.6387v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.6387v2)
- **Published**: 2014-11-24 09:13:00+00:00
- **Updated**: 2014-12-18 04:11:14+00:00
- **Authors**: Fayao Liu, Chunhua Shen, Guosheng Lin
- **Comment**: fixed some typos. in CVPR15 proceedings
- **Journal**: None
- **Summary**: We consider the problem of depth estimation from a single monocular image in this work. It is a challenging task as no reliable depth cues are available, e.g., stereo correspondences, motions, etc. Previous efforts have been focusing on exploiting geometric priors or additional sources of information, with all using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) are setting new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimations can be naturally formulated into a continuous conditional random field (CRF) learning problem. Therefore, we in this paper present a deep convolutional neural field model for estimating depths from a single image, aiming to jointly explore the capacity of deep CNN and continuous CRF. Specifically, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework.   The proposed method can be used for depth estimations of general scenes with no geometric priors nor any extra information injected. In our case, the integral of the partition function can be analytically calculated, thus we can exactly solve the log-likelihood optimization. Moreover, solving the MAP problem for predicting depths of a new image is highly efficient as closed-form solutions exist. We experimentally demonstrate that the proposed method outperforms state-of-the-art depth estimation methods on both indoor and outdoor scene datasets.



### Multiple object tracking with context awareness
- **Arxiv ID**: http://arxiv.org/abs/1411.7935v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.7935v2)
- **Published**: 2014-11-24 09:24:24+00:00
- **Updated**: 2016-11-30 12:23:11+00:00
- **Authors**: Laura Leal-Taixé
- **Comment**: PhD thesis, Leibniz University Hannover, Germany
- **Journal**: None
- **Summary**: Multiple people tracking is a key problem for many applications such as surveillance, animation or car navigation, and a key input for tasks such as activity recognition. In crowded environments occlusions and false detections are common, and although there have been substantial advances in recent years, tracking is still a challenging task. Tracking is typically divided into two steps: detection, i.e., locating the pedestrians in the image, and data association, i.e., linking detections across frames to form complete trajectories.   For the data association task, approaches typically aim at developing new, more complex formulations, which in turn put the focus on the optimization techniques required to solve them. However, they still utilize very basic information such as distance between detections. In this thesis, I focus on the data association task and argue that there is contextual information that has not been fully exploited yet in the tracking community, mainly social context and spatial context coming from different views.



### Encoding High Dimensional Local Features by Sparse Coding Based Fisher Vectors
- **Arxiv ID**: http://arxiv.org/abs/1411.6406v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.6406v1)
- **Published**: 2014-11-24 10:48:47+00:00
- **Updated**: 2014-11-24 10:48:47+00:00
- **Authors**: Lingqiao Liu, Chunhua Shen, Lei Wang, Anton van den Hengel, Chao Wang
- **Comment**: Appearing in Proc. Advances in Neural Information Processing Systems
  (NIPS) 2014, Montreal, Canada
- **Journal**: None
- **Summary**: Deriving from the gradient vector of a generative model of local features, Fisher vector coding (FVC) has been identified as an effective coding method for image classification. Most, if not all, % FVC implementations employ the Gaussian mixture model (GMM) to characterize the generation process of local features. This choice has shown to be sufficient for traditional low dimensional local features, e.g., SIFT; and typically, good performance can be achieved with only a few hundred Gaussian distributions. However, the same number of Gaussians is insufficient to model the feature space spanned by higher dimensional local features, which have become popular recently. In order to improve the modeling capacity for high dimensional features, it turns out to be inefficient and computationally impractical to simply increase the number of Gaussians. In this paper, we propose a model in which each local feature is drawn from a Gaussian distribution whose mean vector is sampled from a subspace. With certain approximation, this model can be converted to a sparse coding procedure and the learning/inference problems can be readily solved by standard sparse coding methods. By calculating the gradient vector of the proposed model, we derive a new fisher vector encoding strategy, termed Sparse Coding based Fisher Vector Coding (SCFVC). Moreover, we adopt the recently developed Deep Convolutional Neural Network (CNN) descriptor as a high dimensional local feature and implement image classification with the proposed SCFVC. Our experimental evaluations demonstrate that our method not only significantly outperforms the traditional GMM based Fisher vector encoding but also achieves the state-of-the-art performance in generic object recognition, indoor scene, and fine-grained image classification problems.



### The Application of Two-level Attention Models in Deep Convolutional Neural Network for Fine-grained Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1411.6447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.6447v1)
- **Published**: 2014-11-24 13:30:07+00:00
- **Updated**: 2014-11-24 13:30:07+00:00
- **Authors**: Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng, Zheng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained classification is challenging because categories can only be discriminated by subtle and local differences. Variances in the pose, scale or rotation usually make the problem more difficult. Most fine-grained classification systems follow the pipeline of finding foreground object or object parts (where) to extract discriminative features (what).   In this paper, we propose to apply visual attention to fine-grained classification task using deep neural network. Our pipeline integrates three types of attention: the bottom-up attention that propose candidate patches, the object-level top-down attention that selects relevant patches to a certain object, and the part-level top-down attention that localizes discriminative parts. We combine these attentions to train domain-specific deep nets, then use it to improve both the what and where aspects. Importantly, we avoid using expensive annotations like bounding box or part information from end-to-end. The weak supervision constraint makes our work easier to generalize.   We have verified the effectiveness of the method on the subsets of ILSVRC2012 dataset and CUB200_2011 dataset. Our pipeline delivered significant improvements and achieved the best accuracy under the weakest supervision condition. The performance is competitive against other methods that rely on additional annotations.



### Persistent Evidence of Local Image Properties in Generic ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1411.6509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.6509v1)
- **Published**: 2014-11-24 16:17:15+00:00
- **Updated**: 2014-11-24 16:17:15+00:00
- **Authors**: Ali Sharif Razavian, Hossein Azizpour, Atsuto Maki, Josephine Sullivan, Carl Henrik Ek, Stefan Carlsson
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised training of a convolutional network for object classification should make explicit any information related to the class of objects and disregard any auxiliary information associated with the capture of the image or the variation within the object class. Does this happen in practice? Although this seems to pertain to the very final layers in the network, if we look at earlier layers we find that this is not the case. Surprisingly, strong spatial information is implicit. This paper addresses this, in particular, exploiting the image representation at the first fully connected layer, i.e. the global image descriptor which has been recently shown to be most effective in a range of visual recognition tasks. We empirically demonstrate evidences for the finding in the contexts of four different tasks: 2d landmark detection, 2d object keypoints prediction, estimation of the RGB values of input image, and recovery of semantic label of each pixel. We base our investigation on a simple framework with ridge rigression commonly across these tasks, and show results which all support our insight. Such spatial information can be used for computing correspondence of landmarks to a good accuracy, but should potentially be useful for improving the training of the convolutional nets for classification purposes.



### Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1411.6660v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.6660v4)
- **Published**: 2014-11-24 21:40:09+00:00
- **Updated**: 2015-04-19 19:13:42+00:00
- **Authors**: Zhenzhong Lan, Ming Lin, Xuanchong Li, Alexander G. Hauptmann, Bhiksha Raj
- **Comment**: None
- **Journal**: None
- **Summary**: Most state-of-the-art action feature extractors involve differential operators, which act as highpass filters and tend to attenuate low frequency action information. This attenuation introduces bias to the resulting features and generates ill-conditioned feature matrices. The Gaussian Pyramid has been used as a feature enhancing technique that encodes scale-invariant characteristics into the feature space in an attempt to deal with this attenuation. However, at the core of the Gaussian Pyramid is a convolutional smoothing operation, which makes it incapable of generating new features at coarse scales. In order to address this problem, we propose a novel feature enhancing technique called Multi-skIp Feature Stacking (MIFS), which stacks features extracted using a family of differential filters parameterized with multiple time skips and encodes shift-invariance into the frequency space. MIFS compensates for information lost from using differential operators by recapturing information at coarse scales. This recaptured information allows us to match actions at different speeds and ranges of motion. We prove that MIFS enhances the learnability of differential-based features exponentially. The resulting feature matrices from MIFS have much smaller conditional numbers and variances than those from conventional methods. Experimental results show significantly improved performance on challenging action recognition and event detection tasks. Specifically, our method exceeds the state-of-the-arts on Hollywood2, UCF101 and UCF50 datasets and is comparable to state-of-the-arts on HMDB51 and Olympics Sports datasets. MIFS can also be used as a speedup strategy for feature extraction with minimal or no accuracy cost.



