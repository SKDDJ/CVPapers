# Arxiv Papers in cs.CV on 2014-11-01
### Complex Events Recognition under Uncertainty in a Sensor Network
- **Arxiv ID**: http://arxiv.org/abs/1411.0085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.0085v1)
- **Published**: 2014-11-01 09:04:45+00:00
- **Updated**: 2014-11-01 09:04:45+00:00
- **Authors**: Atul Kanaujia, Tae Eun Choe, Hongli Deng
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Automated extraction of semantic information from a network of sensors for cognitive analysis and human-like reasoning is a desired capability in future ground surveillance systems. We tackle the problem of complex decision making under uncertainty in network information environment, where lack of effective visual processing tools, incomplete domain knowledge frequently cause uncertainty in the visual primitives, leading to sub-optimal decisions. While state-of-the-art vision techniques exist in detecting visual entities (humans, vehicles and scene elements) in an image, a missing functionality is the ability to merge the information to reveal meaningful information for high level inference. In this work, we develop a probabilistic first order predicate logic(FOPL) based reasoning system for recognizing complex events in synchronized stream of videos, acquired from sensors with non-overlapping fields of view. We adopt Markov Logic Network(MLN) as a tool to model uncertainty in observations, and fuse information extracted from heterogeneous data in a probabilistically consistent way. MLN overcomes strong dependence on pure empirical learning by incorporating domain knowledge, in the form of user-defined rules and confidences associated with them. This work demonstrates that the MLN based decision control system can be made scalable to model statistical relations between a variety of entities and over long video sequences. Experiments with real-world data, under a variety of settings, illustrate the mathematical soundness and wide-ranging applicability of our approach.



### Detection of texts in natural images
- **Arxiv ID**: http://arxiv.org/abs/1411.0126v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.0126v1)
- **Published**: 2014-11-01 15:06:23+00:00
- **Updated**: 2014-11-01 15:06:23+00:00
- **Authors**: Gowtham Rangarajan Raman
- **Comment**: None
- **Journal**: None
- **Summary**: A framework that makes use of Connected components and supervised Support machine to recognise texts is proposed. The image is preprocessed and and edge graph is calculated using a probabilistic framework to compensate for photometric noise. Connected components over the resultant image is calculated, which is bounded and then pruned using geometric constraints. Finally a Gabor Feature based SVM is used to classify the presence of text in the candidates. The proposed method was tested with ICDAR 10 dataset and few other images available on the internet. It resulted in a recall and precision metric of 0.72 and 0.88 comfortably better than the benchmark Eiphstein's algorithm. The proposed method recorded a 0.70 and 0.74 in natural images which is significantly better than current methods on natural images. The proposed method also scales almost linearly for high resolution, cluttered images.



### A Two-phase Decision Support Framework for the Automatic Screening of Digital Fundus Images
- **Arxiv ID**: http://arxiv.org/abs/1411.0130v1
- **DOI**: 10.1016/j.jocs.2012.01.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.0130v1)
- **Published**: 2014-11-01 15:52:29+00:00
- **Updated**: 2014-11-01 15:52:29+00:00
- **Authors**: Balint Antal, Andras Hajdu, Zsuzsanna Maros-Szabo, Zsolt Torok, Adrienne Csutak, Tunde Peto
- **Comment**: None
- **Journal**: Journal of Computational Science, Elsevier, Volume 3, Issue 5,
  September 2012, Pages 262-268
- **Summary**: In this paper we give a brief review on the present status of automated detection systems describe for the screening of diabetic retinopathy. We further detail an enhanced detection procedure that consists of two steps. First, a pre-screening algorithm is considered to classify the input digital fundus images based on the severity of abnormalities. If an image is found to be seriously abnormal, it will not be analysed further with robust lesion detector algorithms. As a further improvement, we introduce a novel feature extraction approach based on clinical observations. The second step of the proposed method detects regions of interest with possible lesions on the images that previously passed the pre-screening step. These regions will serve as input to the specific lesion detectors for detailed analysis. This procedure can increase the computational performance of a screening system. Experimental results show that both two steps of the proposed approach are capable to efficiently exclude a large amount of data from further processing, thus, to decrease the computational burden of the automatic screening system.



### Entropy of Overcomplete Kernel Dictionaries
- **Arxiv ID**: http://arxiv.org/abs/1411.0161v1
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, cs.LG, cs.NE, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1411.0161v1)
- **Published**: 2014-11-01 19:41:14+00:00
- **Updated**: 2014-11-01 19:41:14+00:00
- **Authors**: Paul Honeine
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In signal analysis and synthesis, linear approximation theory considers a linear decomposition of any given signal in a set of atoms, collected into a so-called dictionary. Relevant sparse representations are obtained by relaxing the orthogonality condition of the atoms, yielding overcomplete dictionaries with an extended number of atoms. More generally than the linear decomposition, overcomplete kernel dictionaries provide an elegant nonlinear extension by defining the atoms through a mapping kernel function (e.g., the gaussian kernel). Models based on such kernel dictionaries are used in neural networks, gaussian processes and online learning with kernels.   The quality of an overcomplete dictionary is evaluated with a diversity measure the distance, the approximation, the coherence and the Babel measures. In this paper, we develop a framework to examine overcomplete kernel dictionaries with the entropy from information theory. Indeed, a higher value of the entropy is associated to a further uniform spread of the atoms over the space. For each of the aforementioned diversity measures, we derive lower bounds on the entropy. Several definitions of the entropy are examined, with an extensive analysis in both the input space and the mapped feature space.



