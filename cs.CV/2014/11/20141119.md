# Arxiv Papers in cs.CV on 2014-11-19
### Attentional Neural Network: Feature Selection Using Cognitive Feedback
- **Arxiv ID**: http://arxiv.org/abs/1411.5140v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1411.5140v1)
- **Published**: 2014-11-19 08:33:28+00:00
- **Updated**: 2014-11-19 08:33:28+00:00
- **Authors**: Qian Wang, Jiaxing Zhang, Sen Song, Zheng Zhang
- **Comment**: Poster in Neural Information Processing Systems (NIPS) 2014
- **Journal**: None
- **Summary**: Attentional Neural Network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture. The top-down influence is especially effective when dealing with high noise or difficult segmentation problems. Our system is modular and extensible. It is also easy to train and cheap to run, and yet can accommodate complex behaviors. We obtain classification accuracy better than or competitive with state of art results on the MNIST variation dataset, and successfully disentangle overlaid digits with high success rates. We view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain.



### A Pooling Approach to Modelling Spatial Relations for Image Retrieval and Annotation
- **Arxiv ID**: http://arxiv.org/abs/1411.5190v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.5190v2)
- **Published**: 2014-11-19 11:44:24+00:00
- **Updated**: 2015-05-05 17:55:23+00:00
- **Authors**: Mateusz Malinowski, Mario Fritz
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last two decades we have witnessed strong progress on modeling visual object classes, scenes and attributes that have significantly contributed to automated image understanding. On the other hand, surprisingly little progress has been made on incorporating a spatial representation and reasoning in the inference process. In this work, we propose a pooling interpretation of spatial relations and show how it improves image retrieval and annotations tasks involving spatial language. Due to the complexity of the spatial language, we argue for a learning-based approach that acquires a representation of spatial relations by learning parameters of the pooling operator. We show improvements on previous work on two datasets and two different tasks as well as provide additional insights on a new dataset with an explicit focus on spatial relations.



### Sparse distributed localized gradient fused features of objects
- **Arxiv ID**: http://arxiv.org/abs/1411.5268v1
- **DOI**: 10.1016/j.patcog.2014.10.002
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1411.5268v1)
- **Published**: 2014-11-19 15:57:02+00:00
- **Updated**: 2014-11-19 15:57:02+00:00
- **Authors**: Swathikiran Sudhakarana, Alex Pappachen James
- **Comment**: Pages 13
- **Journal**: Pattern Recognition, Available online 31 October 2014
- **Summary**: The sparse, hierarchical, and modular processing of natural signals is related to the ability of humans to recognize objects with high accuracy. In this study, we report a sparse feature processing and encoding method, which improved the recognition performance of an automated object recognition system. Randomly distributed localized gradient enhanced features were selected before employing aggregate functions for representation, where we used a modular and hierarchical approach to detect the object features. These object features were combined with a minimum distance classifier, thereby obtaining object recognition system accuracies of 93% using the Amsterdam library of object images (ALOI) database, 92% using the Columbia object image library (COIL)-100 database, and 69% using the PASCAL visual object challenge 2007 database. The object recognition performance was shown to be robust to variations in noise, object scaling, and object shifts. Finally, a comparison with eight existing object recognition methods indicated that our new method improved the recognition accuracy by 10% with ALOI, 8% with the COIL-100 database, and 10% with the PASCAL visual object challenge 2007 database.



### Efficient Media Retrieval from Non-Cooperative Queries
- **Arxiv ID**: http://arxiv.org/abs/1411.5307v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1411.5307v1)
- **Published**: 2014-11-19 18:34:28+00:00
- **Updated**: 2014-11-19 18:34:28+00:00
- **Authors**: Kevin Shih, Wei Di, Vignesh Jagadeesh, Robinson Piramuthu
- **Comment**: 8 pages, 9 figures, 1 table
- **Journal**: None
- **Summary**: Text is ubiquitous in the artificial world and easily attainable when it comes to book title and author names. Using the images from the book cover set from the Stanford Mobile Visual Search dataset and additional book covers and metadata from openlibrary.org, we construct a large scale book cover retrieval dataset, complete with 100K distractor covers and title and author strings for each. Because our query images are poorly conditioned for clean text extraction, we propose a method for extracting a matching noisy and erroneous OCR readings and matching it against clean author and book title strings in a standard document look-up problem setup. Finally, we demonstrate how to use this text-matching as a feature in conjunction with popular retrieval features such as VLAD using a simple learning setup to achieve significant improvements in retrieval accuracy over that of either VLAD or the text alone.



### End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression
- **Arxiv ID**: http://arxiv.org/abs/1411.5309v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.5309v1)
- **Published**: 2014-11-19 18:36:09+00:00
- **Updated**: 2014-11-19 18:36:09+00:00
- **Authors**: Li Wan, David Eigen, Rob Fergus
- **Comment**: None
- **Journal**: None
- **Summary**: Deformable Parts Models and Convolutional Networks each have achieved notable performance in object detection. Yet these two approaches find their strengths in complementary areas: DPMs are well-versed in object composition, modeling fine-grained spatial relationships between parts; likewise, ConvNets are adept at producing powerful image features, having been discriminatively trained directly on the pixels. In this paper, we propose a new model that combines these two approaches, obtaining the advantages of each. We train this model using a new structured loss function that considers all bounding boxes within an image, rather than isolated object instances. This enables the non-maximal suppression (NMS) operation, previously treated as a separate post-processing stage, to be integrated into the model. This allows for discriminative training of our combined Convnet + DPM + NMS model in end-to-end fashion. We evaluate our system on PASCAL VOC 2007 and 2011 datasets, achieving competitive results on both benchmarks.



### Fashion Apparel Detection: The Role of Deep Convolutional Neural Network and Pose-dependent Priors
- **Arxiv ID**: http://arxiv.org/abs/1411.5319v2
- **DOI**: 10.1109/WACV.2016.7477611
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.5319v2)
- **Published**: 2014-11-19 19:09:00+00:00
- **Updated**: 2016-01-24 19:45:37+00:00
- **Authors**: Kota Hara, Vignesh Jagadeesh, Robinson Piramuthu
- **Comment**: Accepted for publication at IEEE Winter Conference on Applications of
  Computer Vision (WACV) 2016
- **Journal**: None
- **Summary**: In this work, we propose and address a new computer vision task, which we call fashion item detection, where the aim is to detect various fashion items a person in the image is wearing or carrying. The types of fashion items we consider in this work include hat, glasses, bag, pants, shoes and so on. The detection of fashion items can be an important first step of various e-commerce applications for fashion industry. Our method is based on state-of-the-art object detection method pipeline which combines object proposal methods with a Deep Convolutional Neural Network. Since the locations of fashion items are in strong correlation with the locations of body joints positions, we incorporate contextual information from body poses in order to improve the detection performance. Through the experiments, we demonstrate the effectiveness of the proposed method.



### ConceptLearner: Discovering Visual Concepts from Weakly Labeled Image Collections
- **Arxiv ID**: http://arxiv.org/abs/1411.5328v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1411.5328v1)
- **Published**: 2014-11-19 19:35:39+00:00
- **Updated**: 2014-11-19 19:35:39+00:00
- **Authors**: Bolei Zhou, Vignesh Jagadeesh, Robinson Piramuthu
- **Comment**: 9 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: Discovering visual knowledge from weakly labeled data is crucial to scale up computer vision recognition system, since it is expensive to obtain fully labeled data for a large number of concept categories. In this paper, we propose ConceptLearner, which is a scalable approach to discover visual concepts from weakly labeled image collections. Thousands of visual concept detectors are learned automatically, without human in the loop for additional annotation. We show that these learned detectors could be applied to recognize concepts at image-level and to detect concepts at image region-level accurately. Under domain-specific supervision, we further evaluate the learned concepts for scene recognition on SUN database and for object detection on Pascal VOC 2007. ConceptLearner shows promising performance compared to fully supervised and weakly supervised methods.



### Visual Noise from Natural Scene Statistics Reveals Human Scene Category Representations
- **Arxiv ID**: http://arxiv.org/abs/1411.5331v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1411.5331v1)
- **Published**: 2014-11-19 19:38:50+00:00
- **Updated**: 2014-11-19 19:38:50+00:00
- **Authors**: Michelle R. Greene, Abraham P. Botros, Diane M. Beck, Li Fei-Fei
- **Comment**: None
- **Journal**: None
- **Summary**: Our perceptions are guided both by the bottom-up information entering our eyes, as well as our top-down expectations of what we will see. Although bottom-up visual processing has been extensively studied, comparatively little is known about top-down signals. Here, we describe REVEAL (Representations Envisioned Via Evolutionary ALgorithm), a method for visualizing an observer's internal representation of a complex, real-world scene, allowing us to, for the first time, visualize the top-down information in an observer's mind. REVEAL rests on two innovations for solving this high dimensional problem: visual noise that samples from natural image statistics, and a computer algorithm that collaborates with human observers to efficiently obtain a solution. In this work, we visualize observers' internal representations of a visual scene category (street) using an experiment in which the observer views the naturalistic visual noise and collaborates with the algorithm to externalize his internal representation. As no scene information was presented, observers had to use their internal knowledge of the target, matching it with the visual features in the noise. We matched reconstructed images with images of real-world street scenes to enhance visualization. Critically, we show that the visualized mental images can be used to predict rapid scene detection performance, as each observer had faster and more accurate responses to detecting real-world images that were the most similar to his reconstructed street templates. These results show that it is possible to visualize previously unobservable mental representations of real world stimuli. More broadly, REVEAL provides a general method for objectively examining the content of previously private, subjective mental experiences.



### Affordances Provide a Fundamental Categorization Principle for Visual Scenes
- **Arxiv ID**: http://arxiv.org/abs/1411.5340v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1411.5340v1)
- **Published**: 2014-11-19 19:58:59+00:00
- **Updated**: 2014-11-19 19:58:59+00:00
- **Authors**: Michelle R. Greene, Christopher Baldassano, Andre Esteva, Diane M. Beck, Li Fei-Fei
- **Comment**: None
- **Journal**: None
- **Summary**: How do we know that a kitchen is a kitchen by looking? Relatively little is known about how we conceptualize and categorize different visual environments. Traditional models of visual perception posit that scene categorization is achieved through the recognition of a scene's objects, yet these models cannot account for the mounting evidence that human observers are relatively insensitive to the local details in an image. Psychologists have long theorized that the affordances, or actionable possibilities of a stimulus are pivotal to its perception. To what extent are scene categories created from similar affordances? Using a large-scale experiment using hundreds of scene categories, we show that the activities afforded by a visual scene provide a fundamental categorization principle. Affordance-based similarity explained the majority of the structure in the human scene categorization patterns, outperforming alternative similarities based on objects or visual features. We all models were combined, affordances provided the majority of the predictive power in the combined model, and nearly half of the total explained variance is captured only by affordances. These results challenge many existing models of high-level visual perception, and provide immediately testable hypotheses for the functional organization of the human perceptual system.



