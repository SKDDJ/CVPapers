# Arxiv Papers in cs.CV on 2014-11-18
### Structured Hough Voting for Vision-based Highway Border Detection
- **Arxiv ID**: http://arxiv.org/abs/1411.4701v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1411.4701v1)
- **Published**: 2014-11-18 00:37:35+00:00
- **Updated**: 2014-11-18 00:37:35+00:00
- **Authors**: Zhiding Yu, Wende Zhang, B. V. K. Vijaya Kumar, Dan Levi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a vision-based highway border detection algorithm using structured Hough voting. Our approach takes advantage of the geometric relationship between highway road borders and highway lane markings. It uses a strategy where a number of trained road border and lane marking detectors are triggered, followed by Hough voting to generate corresponding detection of the border and lane marking. Since the initially triggered detectors usually result in large number of positives, conventional frame-wise Hough voting is not able to always generate robust border and lane marking results. Therefore, we formulate this problem as a joint detection-and-tracking problem under the structured Hough voting model, where tracking refers to exploiting inter-frame structural information to stabilize the detection results. Both qualitative and quantitative evaluations show the superiority of the proposed structured Hough voting model over a number of baseline methods.



### Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture
- **Arxiv ID**: http://arxiv.org/abs/1411.4734v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4734v4)
- **Published**: 2014-11-18 04:49:08+00:00
- **Updated**: 2015-12-17 03:19:36+00:00
- **Authors**: David Eigen, Rob Fergus
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.



### Unsupervised Neural Architecture for Saliency Detection: Extended Version
- **Arxiv ID**: http://arxiv.org/abs/1412.3717v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1412.3717v2)
- **Published**: 2014-11-18 11:09:01+00:00
- **Updated**: 2015-04-10 12:30:14+00:00
- **Authors**: Natalia Efremova, Sergey Tarasenko
- **Comment**: 10 pages, 26 figures
- **Journal**: None
- **Summary**: We propose a novel neural network architecture for visual saliency detections, which utilizes neurophysiologically plausible mechanisms for extraction of salient regions. The model has been significantly inspired by recent findings from neurophysiology and aimed to simulate the bottom-up processes of human selective attention. Two types of features were analyzed: color and direction of maximum variance. The mechanism we employ for processing those features is PCA, implemented by means of normalized Hebbian learning and the waves of spikes. To evaluate performance of our model we have conducted psychological experiment. Comparison of simulation results with those of experiment indicates good performance of our model.



### Towards Scene Understanding with Detailed 3D Object Representations
- **Arxiv ID**: http://arxiv.org/abs/1411.5935v1
- **DOI**: 10.1007/s11263-014-0780-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.5935v1)
- **Published**: 2014-11-18 15:07:19+00:00
- **Updated**: 2014-11-18 15:07:19+00:00
- **Authors**: M. Zeeshan Zia, Michael Stark, Konrad Schindler
- **Comment**: International Journal of Computer Vision (appeared online on 4
  November 2014). Online version:
  http://link.springer.com/article/10.1007/s11263-014-0780-y
- **Journal**: None
- **Summary**: Current approaches to semantic image and scene understanding typically employ rather simple object representations such as 2D or 3D bounding boxes. While such coarse models are robust and allow for reliable object detection, they discard much of the information about objects' 3D shape and pose, and thus do not lend themselves well to higher-level reasoning. Here, we propose to base scene understanding on a high-resolution object representation. An object class - in our case cars - is modeled as a deformable 3D wireframe, which enables fine-grained modeling at the level of individual vertices and faces. We augment that model to explicitly include vertex-level occlusion, and embed all instances in a common coordinate frame, in order to infer and exploit object-object interactions. Specifically, from a single view we jointly estimate the shapes and poses of multiple objects in a common 3D frame. A ground plane in that frame is estimated by consensus among different objects, which significantly stabilizes monocular 3D pose estimation. The fine-grained model, in conjunction with the explicit 3D scene model, further allows one to infer part-level occlusions between the modeled objects, as well as occlusions by other, unmodeled scene elements. To demonstrate the benefits of such detailed object class models in the context of scene understanding we systematically evaluate our approach on the challenging KITTI street scene dataset. The experiments show that the model's ability to utilize image evidence at the level of individual parts improves monocular 3D pose estimation w.r.t. both location and (continuous) viewpoint.



### Low-level Vision by Consensus in a Spatial Hierarchy of Regions
- **Arxiv ID**: http://arxiv.org/abs/1411.4894v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4894v2)
- **Published**: 2014-11-18 16:23:06+00:00
- **Updated**: 2015-04-14 15:57:52+00:00
- **Authors**: Ayan Chakrabarti, Ying Xiong, Steven J. Gortler, Todd Zickler
- **Comment**: Accepted to CVPR 2015. Project page:
  http://www.ttic.edu/chakrabarti/consensus/
- **Journal**: None
- **Summary**: We introduce a multi-scale framework for low-level vision, where the goal is estimating physical scene values from image data---such as depth from stereo image pairs. The framework uses a dense, overlapping set of image regions at multiple scales and a "local model," such as a slanted-plane model for stereo disparity, that is expected to be valid piecewise across the visual field. Estimation is cast as optimization over a dichotomous mixture of variables, simultaneously determining which regions are inliers with respect to the local model (binary variables) and the correct co-ordinates in the local model space for each inlying region (continuous variables). When the regions are organized into a multi-scale hierarchy, optimization can occur in an efficient and parallel architecture, where distributed computational units iteratively perform calculations and share information through sparse connections between parents and children. The framework performs well on a standard benchmark for binocular stereo, and it produces a distributional scene representation that is appropriate for combining with higher-level reasoning and other low-level cues.



### A Unified Semantic Embedding: Relating Taxonomies and Attributes
- **Arxiv ID**: http://arxiv.org/abs/1411.5879v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.5879v2)
- **Published**: 2014-11-18 17:03:20+00:00
- **Updated**: 2014-12-09 18:00:13+00:00
- **Authors**: Sung Ju Hwang, Leonid Sigal
- **Comment**: To Appear in NIPS 2014 Learning Semantics Workshop
- **Journal**: None
- **Summary**: We propose a method that learns a discriminative yet semantic space for object categorization, where we also embed auxiliary semantic entities such as supercategories and attributes. Contrary to prior work which only utilized them as side information, we explicitly embed the semantic entities into the same space where we embed categories, which enables us to represent a category as their linear combination. By exploiting such a unified model for semantics, we enforce each category to be represented by a supercategory + sparse combination of attributes, with an additional exclusive regularization to learn discriminative composition.



### From Captions to Visual Concepts and Back
- **Arxiv ID**: http://arxiv.org/abs/1411.4952v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1411.4952v3)
- **Published**: 2014-11-18 18:23:45+00:00
- **Updated**: 2015-04-14 18:05:07+00:00
- **Authors**: Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Doll√°r, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, Geoffrey Zweig
- **Comment**: version corresponding to CVPR15 paper
- **Journal**: None
- **Summary**: This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34% of the time.



### Designing Deep Networks for Surface Normal Estimation
- **Arxiv ID**: http://arxiv.org/abs/1411.4958v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4958v1)
- **Published**: 2014-11-18 18:39:48+00:00
- **Updated**: 2014-11-18 18:39:48+00:00
- **Authors**: Xiaolong Wang, David F. Fouhey, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: In the past few years, convolutional neural nets (CNN) have shown incredible promise for learning visual representations. In this paper, we use CNNs for the task of predicting surface normals from a single image. But what is the right architecture we should use? We propose to build upon the decades of hard work in 3D scene understanding, to design new CNN architecture for the task of surface normal estimation. We show by incorporating several constraints (man-made, manhattan world) and meaningful intermediate representations (room layout, edge labels) in the architecture leads to state of the art performance on surface normal estimation. We also show that our network is quite robust and show state of the art results on other datasets as well without any fine-tuning.



### Salient Object Detection: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1411.5878v6
- **DOI**: 10.1007/s41095-019-0149-9
- **Categories**: **cs.CV**, cs.AI, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1411.5878v6)
- **Published**: 2014-11-18 22:41:24+00:00
- **Updated**: 2019-07-01 11:13:07+00:00
- **Authors**: Ali Borji, Ming-Ming Cheng, Qibin Hou, Huaizu Jiang, Jia Li
- **Comment**: None
- **Journal**: Computational Visual Media, 5(2):117-150, 2019
- **Summary**: Detecting and segmenting salient objects from natural scenes, often referred to as salient object detection, has attracted great interest in computer vision. While many models have been proposed and several applications have emerged, a deep understanding of achievements and issues remains lacking. We aim to provide a comprehensive review of recent progress in salient object detection and situate this field among other closely related areas such as generic scene segmentation, object proposal generation, and saliency for fixation prediction. Covering 228 publications, we survey i) roots, key concepts, and tasks, ii) core techniques and main modeling trends, and iii) datasets and evaluation metrics for salient object detection. We also discuss open problems such as evaluation metrics and dataset bias in model performance, and suggest future research directions.



### Fast Iteratively Reweighted Least Squares Algorithms for Analysis-Based Sparsity Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1411.5057v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.5057v3)
- **Published**: 2014-11-18 22:53:09+00:00
- **Updated**: 2015-04-28 19:43:12+00:00
- **Authors**: Chen Chen, Junzhou Huang, Lei He, Hongsheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel algorithm for analysis-based sparsity reconstruction. It can solve the generalized problem by structured sparsity regularization with an orthogonal basis and total variation regularization. The proposed algorithm is based on the iterative reweighted least squares (IRLS) model, which is further accelerated by the preconditioned conjugate gradient method. The convergence rate of the proposed algorithm is almost the same as that of the traditional IRLS algorithms, that is, exponentially fast. Moreover, with the specifically devised preconditioner, the computational cost for each iteration is significantly less than that of traditional IRLS algorithms, which enables our approach to handle large scale problems. In addition to the fast convergence, it is straightforward to apply our method to standard sparsity, group sparsity, overlapping group sparsity and TV based problems. Experiments are conducted on a practical application: compressive sensing magnetic resonance imaging. Extensive results demonstrate that the proposed algorithm achieves superior performance over 14 state-of-the-art algorithms in terms of both accuracy and computational cost.



### SIRF: Simultaneous Image Registration and Fusion in A Unified Framework
- **Arxiv ID**: http://arxiv.org/abs/1411.5065v2
- **DOI**: 10.1109/TIP.2015.2456415
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.5065v2)
- **Published**: 2014-11-18 23:26:37+00:00
- **Updated**: 2015-01-01 22:00:10+00:00
- **Authors**: Chen Chen, Yeqing Li, Wei Liu, Junzhou Huang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel method for image fusion with a high-resolution panchromatic image and a low-resolution multispectral image at the same geographical location. The fusion is formulated as a convex optimization problem which minimizes a linear combination of a least-squares fitting term and a dynamic gradient sparsity regularizer. The former is to preserve accurate spectral information of the multispectral image, while the latter is to keep sharp edges of the high-resolution panchromatic image. We further propose to simultaneously register the two images during the fusing process, which is naturally achieved by virtue of the dynamic gradient sparsity property. An efficient algorithm is then devised to solve the optimization problem, accomplishing a linear computational complexity in the size of the output image in each iteration. We compare our method against seven state-of-the-art image fusion methods on multispectral image datasets from four satellites. Extensive experimental results demonstrate that the proposed method substantially outperforms the others in terms of both spatial and spectral qualities. We also show that our method can provide high-quality products from coarsely registered real-world datasets. Finally, a MATLAB implementation is provided to facilitate future research.



