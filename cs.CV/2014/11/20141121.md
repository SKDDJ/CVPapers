# Arxiv Papers in cs.CV on 2014-11-21
### Visual Sentiment Prediction with Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1411.5731v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1411.5731v1)
- **Published**: 2014-11-21 00:39:43+00:00
- **Updated**: 2014-11-21 00:39:43+00:00
- **Authors**: Can Xu, Suleyman Cetintas, Kuang-Chih Lee, Li-Jia Li
- **Comment**: None
- **Journal**: None
- **Summary**: Images have become one of the most popular types of media through which users convey their emotions within online social networks. Although vast amount of research is devoted to sentiment analysis of textual data, there has been very limited work that focuses on analyzing sentiment of image data. In this work, we propose a novel visual sentiment prediction framework that performs image understanding with Deep Convolutional Neural Networks (CNN). Specifically, the proposed sentiment prediction framework performs transfer learning from a CNN with millions of parameters, which is pre-trained on large-scale data for object recognition. Experiments conducted on two real-world datasets from Twitter and Tumblr demonstrate the effectiveness of the proposed visual sentiment analysis framework.



### Hypercolumns for Object Segmentation and Fine-grained Localization
- **Arxiv ID**: http://arxiv.org/abs/1411.5752v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.5752v2)
- **Published**: 2014-11-21 03:12:33+00:00
- **Updated**: 2015-04-25 23:08:59+00:00
- **Authors**: Bharath Hariharan, Pablo Arbeláez, Ross Girshick, Jitendra Malik
- **Comment**: CVPR Camera ready
- **Journal**: None
- **Summary**: Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as feature representation. However, the information in this layer may be too coarse to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation[22], where we improve state-of-the-art from 49.7[22] mean AP^r to 60.0, keypoint localization, where we get a 3.3 point boost over[20] and part labeling, where we show a 6.6 point gain over a strong baseline.



### Assessment of algorithms for mitosis detection in breast cancer histopathology images
- **Arxiv ID**: http://arxiv.org/abs/1411.5825v1
- **DOI**: 10.1016/j.media.2014.11.010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.5825v1)
- **Published**: 2014-11-21 11:00:38+00:00
- **Updated**: 2014-11-21 11:00:38+00:00
- **Authors**: Mitko Veta, Paul J. van Diest, Stefan M. Willems, Haibo Wang, Anant Madabhushi, Angel Cruz-Roa, Fabio Gonzalez, Anders B. L. Larsen, Jacob S. Vestergaard, Anders B. Dahl, Dan C. Cireşan, Jürgen Schmidhuber, Alessandro Giusti, Luca M. Gambardella, F. Boray Tek, Thomas Walter, Ching-Wei Wang, Satoshi Kondo, Bogdan J. Matuszewski, Frederic Precioso, Violet Snell, Josef Kittler, Teofilo E. de Campos, Adnan M. Khan, Nasir M. Rajpoot, Evdokia Arkoumani, Miangela M. Lacle, Max A. Viergever, Josien P. W. Pluim
- **Comment**: 23 pages, 5 figures, accepted for publication in the journal Medical
  Image Analysis
- **Journal**: None
- **Summary**: The proliferative activity of breast tumors, which is routinely estimated by counting of mitotic figures in hematoxylin and eosin stained histology sections, is considered to be one of the most important prognostic markers. However, mitosis counting is laborious, subjective and may suffer from low inter-observer agreement. With the wider acceptance of whole slide images in pathology labs, automatic image analysis has been proposed as a potential solution for these issues. In this paper, the results from the Assessment of Mitosis Detection Algorithms 2013 (AMIDA13) challenge are described. The challenge was based on a data set consisting of 12 training and 11 testing subjects, with more than one thousand annotated mitotic figures by multiple observers. Short descriptions and results from the evaluation of eleven methods are presented. The top performing method has an error rate that is comparable to the inter-observer agreement among pathologists.



### Understanding image representations by measuring their equivariance and equivalence
- **Arxiv ID**: http://arxiv.org/abs/1411.5908v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1411.5908v2)
- **Published**: 2014-11-21 15:14:42+00:00
- **Updated**: 2015-06-20 18:35:37+00:00
- **Authors**: Karel Lenc, Andrea Vedaldi
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the importance of image representations such as histograms of oriented gradients and deep Convolutional Neural Networks (CNN), our theoretical understanding of them remains limited. Aiming at filling this gap, we investigate three key mathematical properties of representations: equivariance, invariance, and equivalence. Equivariance studies how transformations of the input image are encoded by the representation, invariance being a special case where a transformation has no effect. Equivalence studies whether two representations, for example two different parametrisations of a CNN, capture the same visual information or not. A number of methods to establish these properties empirically are proposed, including introducing transformation and stitching layers in CNNs. These methods are then applied to popular representations to reveal insightful aspects of their structure, including clarifying at which layers in a CNN certain geometric invariances are achieved. While the focus of the paper is theoretical, direct applications to structured-output regression are demonstrated too.



### Learning to Generate Chairs, Tables and Cars with Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1411.5928v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1411.5928v4)
- **Published**: 2014-11-21 16:01:04+00:00
- **Updated**: 2017-08-02 20:53:43+00:00
- **Authors**: Alexey Dosovitskiy, Jost Tobias Springenberg, Maxim Tatarchenko, Thomas Brox
- **Comment**: v4: final PAMI version. New architecture figure
- **Journal**: None
- **Summary**: We train generative 'up-convolutional' neural networks which are able to generate images of objects given object style, viewpoint, and color. We train the networks on rendered 3D models of chairs, tables, and cars. Our experiments show that the networks do not merely learn all images by heart, but rather find a meaningful representation of 3D models allowing them to assess the similarity of different models, interpolate between given views to generate the missing ones, extrapolate views, and invent new objects not present in the training set by recombining training instances, or even two different object classes. Moreover, we show that such generative networks can be used to find correspondences between different objects from the dataset, outperforming existing approaches on this task.



### Finding Action Tubes
- **Arxiv ID**: http://arxiv.org/abs/1411.6031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.6031v1)
- **Published**: 2014-11-21 21:38:15+00:00
- **Updated**: 2014-11-21 21:38:15+00:00
- **Authors**: Georgia Gkioxari, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of action detection in videos. Driven by the latest progress in object detection from 2D images, we build action models using rich feature hierarchies derived from shape and kinematic cues. We incorporate appearance and motion in two ways. First, starting from image region proposals we select those that are motion salient and thus are more likely to contain the action. This leads to a significant reduction in the number of regions being processed and allows for faster computations. Second, we extract spatio-temporal feature representations to build strong classifiers using Convolutional Neural Networks. We link our predictions to produce detections consistent in time, which we call action tubes. We show that our approach outperforms other techniques in the task of action detection.



