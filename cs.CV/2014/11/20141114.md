# Arxiv Papers in cs.CV on 2014-11-14
### Predictive Encoding of Contextual Relationships for Perceptual Inference, Interpolation and Prediction
- **Arxiv ID**: http://arxiv.org/abs/1411.3815v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1411.3815v6)
- **Published**: 2014-11-14 07:38:45+00:00
- **Updated**: 2015-04-16 15:57:36+00:00
- **Authors**: Mingmin Zhao, Chengxu Zhuang, Yizhou Wang, Tai Sing Lee
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new neurally-inspired model that can learn to encode the global relationship context of visual events across time and space and to use the contextual information to modulate the analysis by synthesis process in a predictive coding framework. The model learns latent contextual representations by maximizing the predictability of visual events based on local and global contextual information through both top-down and bottom-up processes. In contrast to standard predictive coding models, the prediction error in this model is used to update the contextual representation but does not alter the feedforward input for the next layer, and is thus more consistent with neurophysiological observations. We establish the computational feasibility of this model by demonstrating its ability in several aspects. We show that our model can outperform state-of-art performances of gated Boltzmann machines (GBM) in estimation of contextual information. Our model can also interpolate missing events or predict future events in image sequences while simultaneously estimating contextual information. We show it achieves state-of-art performances in terms of prediction accuracy in a variety of tasks and possesses the ability to interpolate missing frames, a function that is lacking in GBM.



### Deep Belief Network Training Improvement Using Elite Samples Minimizing Free Energy
- **Arxiv ID**: http://arxiv.org/abs/1411.4046v1
- **DOI**: 10.1142/S0218001415510064
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1411.4046v1)
- **Published**: 2014-11-14 16:57:48+00:00
- **Updated**: 2014-11-14 16:57:48+00:00
- **Authors**: Mohammad Ali Keyvanrad, Mohammad Mehdi Homayounpour
- **Comment**: 18 pages. arXiv admin note: substantial text overlap with
  arXiv:1408.3264
- **Journal**: Int. J. Patt. Recogn. Artif. Intell. 29, 1551006 (2015)
- **Summary**: Nowadays this is very popular to use deep architectures in machine learning. Deep Belief Networks (DBNs) are deep architectures that use stack of Restricted Boltzmann Machines (RBM) to create a powerful generative model using training data. In this paper we present an improvement in a common method that is usually used in training of RBMs. The new method uses free energy as a criterion to obtain elite samples from generative model. We argue that these samples can more accurately compute gradient of log probability of training data. According to the results, an error rate of 0.99% was achieved on MNIST test set. This result shows that the proposed method outperforms the method presented in the first paper introducing DBN (1.25% error rate) and general classification methods such as SVM (1.4% error rate) and KNN (with 1.6% error rate). In another test using ISOLET dataset, letter classification error dropped to 3.59% compared to 5.59% error rate achieved in those papers using this dataset. The implemented method is available online at "http://ceit.aut.ac.ir/~keyvanrad/DeeBNet Toolbox.html".



### A convex formulation for hyperspectral image superresolution via subspace-based regularization
- **Arxiv ID**: http://arxiv.org/abs/1411.4005v1
- **DOI**: 10.1109/TGRS.2014.2375320
- **Categories**: **cs.CV**, physics.data-an, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1411.4005v1)
- **Published**: 2014-11-14 18:36:31+00:00
- **Updated**: 2014-11-14 18:36:31+00:00
- **Authors**: Miguel Simões, José Bioucas-Dias, Luis B. Almeida, Jocelyn Chanussot
- **Comment**: IEEE Trans. Geosci. Remote Sens., to be published
- **Journal**: None
- **Summary**: Hyperspectral remote sensing images (HSIs) usually have high spectral resolution and low spatial resolution. Conversely, multispectral images (MSIs) usually have low spectral and high spatial resolutions. The problem of inferring images which combine the high spectral and high spatial resolutions of HSIs and MSIs, respectively, is a data fusion problem that has been the focus of recent active research due to the increasing availability of HSIs and MSIs retrieved from the same geographical area.   We formulate this problem as the minimization of a convex objective function containing two quadratic data-fitting terms and an edge-preserving regularizer. The data-fitting terms account for blur, different resolutions, and additive noise. The regularizer, a form of vector Total Variation, promotes piecewise-smooth solutions with discontinuities aligned across the hyperspectral bands.   The downsampling operator accounting for the different spatial resolutions, the non-quadratic and non-smooth nature of the regularizer, and the very large size of the HSI to be estimated lead to a hard optimization problem. We deal with these difficulties by exploiting the fact that HSIs generally "live" in a low-dimensional subspace and by tailoring the Split Augmented Lagrangian Shrinkage Algorithm (SALSA), which is an instance of the Alternating Direction Method of Multipliers (ADMM), to this optimization problem, by means of a convenient variable splitting. The spatial blur and the spectral linear operators linked, respectively, with the HSI and MSI acquisition processes are also estimated, and we obtain an effective algorithm that outperforms the state-of-the-art, as illustrated in a series of experiments with simulated and real-life data.



### A Discriminative CNN Video Representation for Event Detection
- **Arxiv ID**: http://arxiv.org/abs/1411.4006v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4006v1)
- **Published**: 2014-11-14 18:37:31+00:00
- **Updated**: 2014-11-14 18:37:31+00:00
- **Authors**: Zhongwen Xu, Yi Yang, Alexander G. Hauptmann
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a discriminative video representation for event detection over a large scale video dataset when only limited hardware resources are available. The focus of this paper is to effectively leverage deep Convolutional Neural Networks (CNNs) to advance event detection, where only frame level static descriptors can be extracted by the existing CNN toolkit. This paper makes two contributions to the inference of CNN video representation. First, while average pooling and max pooling have long been the standard approaches to aggregating frame level static features, we show that performance can be significantly improved by taking advantage of an appropriate encoding method. Second, we propose using a set of latent concept descriptors as the frame descriptor, which enriches visual information while keeping it computationally affordable. The integration of the two contributions results in a new state-of-the-art performance in event detection over the largest video datasets. Compared to improved Dense Trajectories, which has been recognized as the best video representation for event detection, our new representation improves the Mean Average Precision (mAP) from 27.6% to 36.8% for the TRECVID MEDTest 14 dataset and from 34.0% to 44.6% for the TRECVID MEDTest 13 dataset. This work is the core part of the winning solution of our CMU-Informedia team in TRECVID MED 2014 competition.



### Sparse And Low Rank Decomposition Based Batch Image Alignment for Speckle Reduction of retinal OCT Images
- **Arxiv ID**: http://arxiv.org/abs/1411.4033v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4033v3)
- **Published**: 2014-11-14 20:24:31+00:00
- **Updated**: 2015-02-09 01:58:07+00:00
- **Authors**: Ahmadreza Baghaie, Roshan M. D'souza, Zeyun Yu
- **Comment**: Accepted for presentation at ISBI'15
- **Journal**: None
- **Summary**: Optical Coherence Tomography (OCT) is an emerging technique in the field of biomedical imaging, with applications in ophthalmology, dermatology, coronary imaging etc. Due to the underlying physics, OCT images usually suffer from a granular pattern, called speckle noise, which restricts the process of interpretation. Here, a sparse and low rank decomposition based method is used for speckle reduction in retinal OCT images. This technique works on input data that consists of several B-scans of the same location. The next step is the batch alignment of the images using a sparse and low-rank decomposition based technique. Finally the denoised image is created by median filtering of the low-rank component of the processed data. Simultaneous decomposition and alignment of the images result in better performance in comparison to simple registration-based methods that are used in the literature for noise reduction of OCT images.



### Fully Convolutional Networks for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1411.4038v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4038v2)
- **Published**: 2014-11-14 20:46:03+00:00
- **Updated**: 2015-03-08 22:16:40+00:00
- **Authors**: Jonathan Long, Evan Shelhamer, Trevor Darrell
- **Comment**: to appear in CVPR (2015)
- **Journal**: None
- **Summary**: Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.



### A Faster Method for Tracking and Scoring Videos Corresponding to Sentences
- **Arxiv ID**: http://arxiv.org/abs/1411.4064v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4064v1)
- **Published**: 2014-11-14 21:40:37+00:00
- **Updated**: 2014-11-14 21:40:37+00:00
- **Authors**: Haonan Yu, Daniel P. Barrett, Jeffrey Mark Siskind
- **Comment**: None
- **Journal**: None
- **Summary**: Prior work presented the sentence tracker, a method for scoring how well a sentence describes a video clip or alternatively how well a video clip depicts a sentence. We present an improved method for optimizing the same cost function employed by this prior work, reducing the space complexity from exponential in the sentence length to polynomial, as well as producing a qualitatively identical result in time polynomial in the sentence length instead of exponential. Since this new method is plug-compatible with the prior method, it can be used for the same applications: video retrieval with sentential queries, generating sentential descriptions of video clips, and focusing the attention of a tracker with a sentence, while allowing these applications to scale with significantly larger numbers of object detections, word meanings modeled with HMMs with significantly larger numbers of states, and significantly longer sentences, with no appreciable degradation in quality of results.



### 6 Seconds of Sound and Vision: Creativity in Micro-Videos
- **Arxiv ID**: http://arxiv.org/abs/1411.4080v1
- **DOI**: 10.1109/CVPR.2014.544
- **Categories**: **cs.MM**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1411.4080v1)
- **Published**: 2014-11-14 23:29:18+00:00
- **Updated**: 2014-11-14 23:29:18+00:00
- **Authors**: Miriam Redi, Neil O Hare, Rossano Schifanella, Michele Trevisiol, Alejandro Jaimes
- **Comment**: 8 pages, 1 figures, conference IEEE CVPR 2014
- **Journal**: None
- **Summary**: The notion of creativity, as opposed to related concepts such as beauty or interestingness, has not been studied from the perspective of automatic analysis of multimedia content. Meanwhile, short online videos shared on social media platforms, or micro-videos, have arisen as a new medium for creative expression. In this paper we study creative micro-videos in an effort to understand the features that make a video creative, and to address the problem of automatic detection of creative content. Defining creative videos as those that are novel and have aesthetic value, we conduct a crowdsourcing experiment to create a dataset of over 3,800 micro-videos labelled as creative and non-creative. We propose a set of computational features that we map to the components of our definition of creativity, and conduct an analysis to determine which of these features correlate most with creative video. Finally, we evaluate a supervised approach to automatically detect creative video, with promising results, showing that it is necessary to model both aesthetic value and novelty to achieve optimal classification accuracy.



