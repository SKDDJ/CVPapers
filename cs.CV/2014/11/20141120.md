# Arxiv Papers in cs.CV on 2014-11-20
### Maximum Likelihood Directed Enumeration Method in Piecewise-Regular Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1411.5555v1
- **DOI**: 10.1016/j.patcog.2016.08.015
- **Categories**: **cs.CV**, 68T10, I.5.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1411.5555v1)
- **Published**: 2014-11-20 14:22:15+00:00
- **Updated**: 2014-11-20 14:22:15+00:00
- **Authors**: Andrey Savchenko
- **Comment**: 13 pages, 6 figures, 20 references
- **Journal**: None
- **Summary**: We explore the problems of classification of composite object (images, speech signals) with low number of models per class. We study the question of improving recognition performance for medium-sized database (thousands of classes). The key issue of fast approximate nearest-neighbor methods widely applied in this task is their heuristic nature. It is possible to strongly prove their efficiency by using the theory of algorithms only for simple similarity measures and artificially generated tasks. On the contrary, in this paper we propose an alternative, statistically optimal greedy algorithm. At each step of this algorithm joint density (likelihood) of distances to previously checked models is estimated for each class. The next model to check is selected from the class with the maximal likelihood. The latter is estimated based on the asymptotic properties of the Kullback-Leibler information discrimination and mathematical model of piecewise-regular object with distribution of each regular segment of exponential type. Experimental results in face recognition for FERET dataset prove that the proposed method is much more effective than not only brute force and the baseline (directed enumeration method) but also approximate nearest neighbor methods from FLANN and NonMetricSpaceLib libraries (randomized kd-tree, composite index, perm-sort).



### Learning a Recurrent Visual Representation for Image Caption Generation
- **Arxiv ID**: http://arxiv.org/abs/1411.5654v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1411.5654v1)
- **Published**: 2014-11-20 19:50:27+00:00
- **Updated**: 2014-11-20 19:50:27+00:00
- **Authors**: Xinlei Chen, C. Lawrence Zitnick
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we explore the bi-directional mapping between images and their sentence-based descriptions. We propose learning this mapping using a recurrent neural network. Unlike previous approaches that map both sentences and images to a common embedding, we enable the generation of novel sentences given an image. Using the same model, we can also reconstruct the visual features associated with an image given its visual description. We use a novel recurrent visual memory that automatically learns to remember long-term visual concepts to aid in both sentence generation and visual feature reconstruction. We evaluate our approach on several tasks. These include sentence generation, sentence retrieval and image retrieval. State-of-the-art results are shown for the task of generating novel image descriptions. When compared to human generated captions, our automatically generated captions are preferred by humans over $19.8\%$ of the time. Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features.



### An algorithm for improving Non-Local Means operators via low-rank approximation
- **Arxiv ID**: http://arxiv.org/abs/1412.2067v1
- **DOI**: None
- **Categories**: **cs.CV**, math.GM
- **Links**: [PDF](http://arxiv.org/pdf/1412.2067v1)
- **Published**: 2014-11-20 20:13:00+00:00
- **Updated**: 2014-11-20 20:13:00+00:00
- **Authors**: Victor May, Yosi Keller, Nir Sharon, Yoel Shkolnisky
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for improving a Non Local Means operator by computing its low-rank approximation. The low-rank operator is constructed by applying a filter to the spectrum of the original Non Local Means operator. This results in an operator which is less sensitive to noise while preserving important properties of the original operator. The method is efficiently implemented based on Chebyshev polynomials and is demonstrated on the application of natural images denoising. For this application, we provide a comprehensive comparison of our method with leading denoising methods.



### CIDEr: Consensus-based Image Description Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1411.5726v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1411.5726v2)
- **Published**: 2014-11-20 23:54:35+00:00
- **Updated**: 2015-06-03 01:42:20+00:00
- **Authors**: Ramakrishna Vedantam, C. Lawrence Zitnick, Devi Parikh
- **Comment**: To appear in CVPR 2015
- **Journal**: None
- **Summary**: Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric (CIDEr) that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking.



