# Arxiv Papers in cs.CV on 2014-11-26
### Real time Detection of Lane Markers in Urban Streets
- **Arxiv ID**: http://arxiv.org/abs/1411.7113v1
- **DOI**: 10.1109/IVS.2008.4621152
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1411.7113v1)
- **Published**: 2014-11-26 05:50:02+00:00
- **Updated**: 2014-11-26 05:50:02+00:00
- **Authors**: Mohamed Aly
- **Comment**: 6 pages
- **Journal**: IEEE Intelligent Vehicles Symposium, Eindhoven, The Netherlands,
  June 2008
- **Summary**: We present a robust and real time approach to lane marker detection in urban streets. It is based on generating a top view of the road, filtering using selective oriented Gaussian filters, using RANSAC line fitting to give initial guesses to a new and fast RANSAC algorithm for fitting Bezier Splines, which is then followed by a post-processing step. Our algorithm can detect all lanes in still images of the street in various conditions, while operating at a rate of 50 Hz and achieving comparable results to previous techniques.



### Open-source code for manifold-based 3D rotation recovery of X-ray scattering patterns
- **Arxiv ID**: http://arxiv.org/abs/1411.7889v1
- **DOI**: None
- **Categories**: **physics.optics**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1411.7889v1)
- **Published**: 2014-11-26 18:16:54+00:00
- **Updated**: 2014-11-26 18:16:54+00:00
- **Authors**: Aliakbar Jafarpour
- **Comment**: None
- **Journal**: None
- **Summary**: Single particle 3D imaging with ultrashort X-ray laser pulses is based on collecting and combining the information content of 2D scattering patterns of an object at different orientations. Typical sample-delivery schemes leave little or no room for controlling the orientations. As such, the orientation associated with a given snapshot should be estimated after the experiment. Here we present an open-source code for the most rigorous technique having been reported in this context. Some practical issues along with proposed solutions are also discussed.



### Understanding Deep Image Representations by Inverting Them
- **Arxiv ID**: http://arxiv.org/abs/1412.0035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.0035v1)
- **Published**: 2014-11-26 18:51:52+00:00
- **Updated**: 2014-11-26 18:51:52+00:00
- **Authors**: Aravindh Mahendran, Andrea Vedaldi
- **Comment**: None
- **Journal**: None
- **Summary**: Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.



### Edge direction matrixes-based local binar patterns descriptor for shape pattern recognition
- **Arxiv ID**: http://arxiv.org/abs/1411.7336v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1411.7336v1)
- **Published**: 2014-11-26 19:12:33+00:00
- **Updated**: 2014-11-26 19:12:33+00:00
- **Authors**: Mohammed A. Talab, Siti Norul Huda Sheikh Abdullah, Bilal Bataineh
- **Comment**: None
- **Journal**: None
- **Summary**: Shapes and texture image recognition usage is an essential branch of pattern recognition. It is made up of techniques that aim at extracting information from images via human knowledge and works. Local Binary Pattern (LBP) ensures encoding global and local information and scaling invariance by introducing a look-up table to reflect the uniformity structure of an object. However, edge direction matrixes (EDMS) only apply global invariant descriptor which employs first and secondary order relationships. The main idea behind this methodology is the need of improved recognition capabilities, a goal achieved by the combinative use of these descriptors. This collaboration aims to make use of the major advantages each one presents, by simultaneously complementing each other, in order to elevate their weak points. By using multiple classifier approaches such as random forest and multi-layer perceptron neural network, the proposed combinative descriptor are compared with the state of the art combinative methods based on Gray-Level Co-occurrence matrix (GLCM with EDMS), LBP and moment invariant on four benchmark dataset MPEG-7 CE-Shape-1, KTH-TIPS image, Enghlishfnt and Arabic calligraphy . The experiments have shown the superiority of the introduced descriptor over the GLCM with EDMS, LBP and moment invariants and other well-known descriptor such as Scale Invariant Feature Transform from the literature.



### Fisher Vectors Derived from Hybrid Gaussian-Laplacian Mixture Models for Image Annotation
- **Arxiv ID**: http://arxiv.org/abs/1411.7399v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.7399v2)
- **Published**: 2014-11-26 21:21:51+00:00
- **Updated**: 2015-01-24 20:03:50+00:00
- **Authors**: Benjamin Klein, Guy Lev, Gil Sadeh, Lior Wolf
- **Comment**: new version includes text synthesis by an RNN and experiments with
  the COCO benchmark
- **Journal**: None
- **Summary**: In the traditional object recognition pipeline, descriptors are densely sampled over an image, pooled into a high dimensional non-linear representation and then passed to a classifier. In recent years, Fisher Vectors have proven empirically to be the leading representation for a large variety of applications. The Fisher Vector is typically taken as the gradients of the log-likelihood of descriptors, with respect to the parameters of a Gaussian Mixture Model (GMM). Motivated by the assumption that different distributions should be applied for different datasets, we present two other Mixture Models and derive their Expectation-Maximization and Fisher Vector expressions. The first is a Laplacian Mixture Model (LMM), which is based on the Laplacian distribution. The second Mixture Model presented is a Hybrid Gaussian-Laplacian Mixture Model (HGLMM) which is based on a weighted geometric mean of the Gaussian and Laplacian distribution. An interesting property of the Expectation-Maximization algorithm for the latter is that in the maximization step, each dimension in each component is chosen to be either a Gaussian or a Laplacian. Finally, by using the new Fisher Vectors derived from HGLMMs, we achieve state-of-the-art results for both the image annotation and the image search by a sentence tasks.



### 3D-Assisted Image Feature Synthesis for Novel Views of an Object
- **Arxiv ID**: http://arxiv.org/abs/1412.0003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.0003v1)
- **Published**: 2014-11-26 22:00:30+00:00
- **Updated**: 2014-11-26 22:00:30+00:00
- **Authors**: Hao Su, Fan Wang, Li Yi, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: Comparing two images in a view-invariant way has been a challenging problem in computer vision for a long time, as visual features are not stable under large view point changes. In this paper, given a single input image of an object, we synthesize new features for other views of the same object. To accomplish this, we introduce an aligned set of 3D models in the same class as the input object image. Each 3D model is represented by a set of views, and we study the correlation of image patches between different views, seeking what we call surrogates --- patches in one view whose feature content predicts well the features of a patch in another view. In particular, for each patch in the novel desired view, we seek surrogates from the observed view of the given image. For a given surrogate, we predict that surrogate using linear combination of the corresponding patches of the 3D model views, learn the coefficients, and then transfer these coefficients on a per patch basis to synthesize the features of the patch in the novel view. In this way we can create feature sets for all views of the latent object, providing us a multi-view representation of the object. View-invariant object comparisons are achieved simply by computing the $L^2$ distances between the features of corresponding views. We provide theoretical and empirical analysis of the feature synthesis process, and evaluate the proposed view-agnostic distance (VAD) in fine-grained image retrieval (100 object classes) and classification tasks. Experimental results show that our synthesized features do enable view-independent comparison between images and perform significantly better than traditional image features in this respect.



