# Arxiv Papers in cs.CV on 2014-11-28
### Deep Learning Face Attributes in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1411.7766v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.7766v3)
- **Published**: 2014-11-28 07:13:54+00:00
- **Updated**: 2015-09-24 13:52:26+00:00
- **Authors**: Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang
- **Comment**: To appear in International Conference on Computer Vision (ICCV) 2015
- **Journal**: None
- **Summary**: Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation.   (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies.   (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works.   (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.



### Cross-Modal Learning via Pairwise Constraints
- **Arxiv ID**: http://arxiv.org/abs/1411.7798v1
- **DOI**: 10.1109/TIP.2015.2466106
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.7798v1)
- **Published**: 2014-11-28 10:11:03+00:00
- **Updated**: 2014-11-28 10:11:03+00:00
- **Authors**: Ran He, Man Zhang, Liang Wang, Ye Ji, Qiyue Yin
- **Comment**: 12 pages, 5 figures, 70 references
- **Journal**: None
- **Summary**: In multimedia applications, the text and image components in a web document form a pairwise constraint that potentially indicates the same semantic concept. This paper studies cross-modal learning via the pairwise constraint, and aims to find the common structure hidden in different modalities. We first propose a compound regularization framework to deal with the pairwise constraint, which can be used as a general platform for developing cross-modal algorithms. For unsupervised learning, we propose a cross-modal subspace clustering method to learn a common structure for different modalities. For supervised learning, to reduce the semantic gap and the outliers in pairwise constraints, we propose a cross-modal matching method based on compound ?21 regularization along with an iteratively reweighted algorithm to find the global optimum. Extensive experiments demonstrate the benefits of joint text and image modeling with semantically induced pairwise constraints, and show that the proposed cross-modal methods can further reduce the semantic gap between different modalities and improve the clustering/retrieval accuracy.



### V-variable image compression
- **Arxiv ID**: http://arxiv.org/abs/1411.7855v1
- **DOI**: 10.1142/S0218348X15500073
- **Categories**: **cs.CV**, 28A80, 68U10, 94A08
- **Links**: [PDF](http://arxiv.org/pdf/1411.7855v1)
- **Published**: 2014-11-28 13:18:22+00:00
- **Updated**: 2014-11-28 13:18:22+00:00
- **Authors**: Franklin Mendivil, Ã–rjan Stenflo
- **Comment**: 15 pages, 22 figures
- **Journal**: Fractals, 23, no 02, (2015)
- **Summary**: V-variable fractals, where $V$ is a positive integer, are intuitively fractals with at most $V$ different "forms" or "shapes" at all levels of magnification. In this paper we describe how V-variable fractals can be used for the purpose of image compression.



### Articulated motion discovery using pairs of trajectories
- **Arxiv ID**: http://arxiv.org/abs/1411.7883v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.7883v3)
- **Published**: 2014-11-28 14:43:03+00:00
- **Updated**: 2015-04-24 15:29:06+00:00
- **Authors**: Luca Del Pero, Susanna Ricco, Rahul Sukthankar, Vittorio Ferrari
- **Comment**: 10 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: We propose an unsupervised approach for discovering characteristic motion patterns in videos of highly articulated objects performing natural, unscripted behaviors, such as tigers in the wild. We discover consistent patterns in a bottom-up manner by analyzing the relative displacements of large numbers of ordered trajectory pairs through time, such that each trajectory is attached to a different moving part on the object. The pairs of trajectories descriptor relies entirely on motion and is more discriminative than state-of-the-art features that employ single trajectories. Our method generates temporal video intervals, each automatically trimmed to one instance of the discovered behavior, and clusters them by type (e.g., running, turning head, drinking water). We present experiments on two datasets: dogs from YouTube-Objects and a new dataset of National Geographic tiger videos. Results confirm that our proposed descriptor outperforms existing appearance- and trajectory-based descriptors (e.g., HOG and DTFs) on both datasets and enables us to segment unconstrained animal video into intervals containing single behaviors.



### On Rendering Synthetic Images for Training an Object Detector
- **Arxiv ID**: http://arxiv.org/abs/1411.7911v1
- **DOI**: 10.1016/j.cviu.2014.12.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.7911v1)
- **Published**: 2014-11-28 15:41:11+00:00
- **Updated**: 2014-11-28 15:41:11+00:00
- **Authors**: Artem Rozantsev, Vincent Lepetit, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach to synthesizing images that are effective for training object detectors. Starting from a small set of real images, our algorithm estimates the rendering parameters required to synthesize similar images given a coarse 3D model of the target object. These parameters can then be reused to generate an unlimited number of training images of the object of interest in arbitrary 3D poses, which can then be used to increase classification performances.   A key insight of our approach is that the synthetically generated images should be similar to real images, not in terms of image quality, but rather in terms of features used during the detector training. We show in the context of drone, plane, and car detection that using such synthetically generated images yields significantly better performances than simply perturbing real images or even synthesizing images in such way that they look very realistic, as is often done when only limited amounts of training data are available.



### Learning Face Representation from Scratch
- **Arxiv ID**: http://arxiv.org/abs/1411.7923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.7923v1)
- **Published**: 2014-11-28 16:05:18+00:00
- **Updated**: 2014-11-28 16:05:18+00:00
- **Authors**: Dong Yi, Zhen Lei, Shengcai Liao, Stan Z. Li
- **Comment**: None
- **Journal**: None
- **Summary**: Pushing by big data and deep convolutional neural network (CNN), the performance of face recognition is becoming comparable to human. Using private large scale training datasets, several groups achieve very high performance on LFW, i.e., 97% to 99%. While there are many open source implementations of CNN, none of large scale face dataset is publicly available. The current situation in the field of face recognition is that data is more important than algorithm. To solve this problem, this paper proposes a semi-automatical way to collect face images from Internet and builds a large scale dataset containing about 10,000 subjects and 500,000 images, called CASIAWebFace. Based on the database, we use a 11-layer CNN to learn discriminative representation and obtain state-of-theart accuracy on LFW and YTF. The publication of CASIAWebFace will attract more research groups entering this field and accelerate the development of face recognition in the wild.



### Effective Face Frontalization in Unconstrained Images
- **Arxiv ID**: http://arxiv.org/abs/1411.7964v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.7964v1)
- **Published**: 2014-11-28 18:22:56+00:00
- **Updated**: 2014-11-28 18:22:56+00:00
- **Authors**: Tal Hassner, Shai Harel, Eran Paz, Roee Enbar
- **Comment**: None
- **Journal**: None
- **Summary**: "Frontalization" is the process of synthesizing frontal facing views of faces appearing in single unconstrained photos. Recent reports have suggested that this process may substantially boost the performance of face recognition systems. This, by transforming the challenging problem of recognizing faces viewed from unconstrained viewpoints to the easier problem of recognizing faces in constrained, forward facing poses. Previous frontalization methods did this by attempting to approximate 3D facial shapes for each query image. We observe that 3D face shape estimation from unconstrained photos may be a harder problem than frontalization and can potentially introduce facial misalignments. Instead, we explore the simpler approach of using a single, unmodified, 3D surface as an approximation to the shape of all input faces. We show that this leads to a straightforward, efficient and easy to implement method for frontalization. More importantly, it produces aesthetic new frontal views and is surprisingly effective when used for face recognition and gender estimation.



