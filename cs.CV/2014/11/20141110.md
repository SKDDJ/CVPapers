# Arxiv Papers in cs.CV on 2014-11-10
### Zero-Aliasing Correlation Filters for Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1411.2316v2
- **DOI**: 10.1109/TPAMI.2014.2375215
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1411.2316v2)
- **Published**: 2014-11-10 03:48:21+00:00
- **Updated**: 2014-11-19 15:10:22+00:00
- **Authors**: Joseph A. Fernandez, Vishnu Naresh Boddeti, Andres Rodriguez, B. V. K. Vijaya Kumar
- **Comment**: 14 pages, to appear in IEEE Transactions on Pattern Analysis and
  Machine Intelligence (PAMI)
- **Journal**: None
- **Summary**: Correlation filters (CFs) are a class of classifiers that are attractive for object localization and tracking applications. Traditionally, CFs have been designed in the frequency domain using the discrete Fourier transform (DFT), where correlation is efficiently implemented. However, existing CF designs do not account for the fact that the multiplication of two DFTs in the frequency domain corresponds to a circular correlation in the time/spatial domain. Because this was previously unaccounted for, prior CF designs are not truly optimal, as their optimization criteria do not accurately quantify their optimization intention. In this paper, we introduce new zero-aliasing constraints that completely eliminate this aliasing problem by ensuring that the optimization criterion for a given CF corresponds to a linear correlation rather than a circular correlation. This means that previous CF designs can be significantly improved by this reformulation. We demonstrate the benefits of this new CF design approach with several important CFs. We present experimental results on diverse data sets and present solutions to the computational challenges associated with computing these CFs. Code for the CFs described in this paper and their respective zero-aliasing versions is available at http://vishnu.boddeti.net/projects/correlation-filters.html



### An Improved Tracking using IMU and Vision Fusion for Mobile Augmented Reality Applications
- **Arxiv ID**: http://arxiv.org/abs/1411.2335v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.2335v1)
- **Published**: 2014-11-10 06:15:42+00:00
- **Updated**: 2014-11-10 06:15:42+00:00
- **Authors**: Kriti Kumar, Ashley Varghese, Pavan K Reddy, N Narendra, Prashanth Swamy, M Girish Chandra, P Balamuralidhar
- **Comment**: None
- **Journal**: International Journal of Multimedia and its Applications, ISSN:
  0975-5578 (online); 0975-5934 (print), October 2014, Volume 6, Number 5
- **Summary**: Mobile Augmented Reality (MAR) is becoming an important cyber-physical system application given the ubiquitous availability of mobile phones. With the need to operate in unprepared environments, accurate and robust registration and tracking has become an important research problem to solve. In fact, when MAR is used for tele-interactive applications involving large distances, say from an accident site to insurance office, tracking at both the ends is desirable and further it is essential to appropriately fuse inertial and vision sensors data. In this paper, we present results and discuss some insights gained in marker-less tracking during the development of a prototype pertaining to an example use case related to breakdown or damage assessment of a vehicle. The novelty of this paper is in bringing together different components and modules with appropriate enhancements towards a complete working system.



### Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models
- **Arxiv ID**: http://arxiv.org/abs/1411.2539v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1411.2539v1)
- **Published**: 2014-11-10 19:09:41+00:00
- **Updated**: 2014-11-10 19:09:41+00:00
- **Authors**: Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel
- **Comment**: 13 pages. NIPS 2014 deep learning workshop
- **Journal**: None
- **Summary**: Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - "blue" + "red" is near images of red cars. Sample captions generated for 800 images are made available for comparison.



