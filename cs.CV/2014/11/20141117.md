# Arxiv Papers in cs.CV on 2014-11-17
### Long-term Recurrent Convolutional Networks for Visual Recognition and Description
- **Arxiv ID**: http://arxiv.org/abs/1411.4389v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4389v4)
- **Published**: 2014-11-17 08:25:17+00:00
- **Updated**: 2016-05-31 22:57:33+00:00
- **Authors**: Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Sergio Guadarrama, Kate Saenko, Trevor Darrell
- **Comment**: Originally presented at CVPR 2015 (oral). Updated version (accepted
  as a TPAMI journal article) includes additional results
- **Journal**: None
- **Summary**: Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or "temporally deep", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are "doubly deep"' in that they can be compositional in spatial and temporal "layers". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.



### Automatic Subspace Learning via Principal Coefficients Embedding
- **Arxiv ID**: http://arxiv.org/abs/1411.4419v5
- **DOI**: 10.1109/TCYB.2016.2572306.
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4419v5)
- **Published**: 2014-11-17 10:23:26+00:00
- **Updated**: 2016-10-17 03:17:24+00:00
- **Authors**: Xi Peng, Jiwen Lu, Zhang Yi, Rui Yan
- **Comment**: IEEE Trans. on Cybernetics, 2016
- **Journal**: IEEE Trans. on Cybernetics, 2016
- **Summary**: In this paper, we address two challenging problems in unsupervised subspace learning: 1) how to automatically identify the feature dimension of the learned subspace (i.e., automatic subspace learning), and 2) how to learn the underlying subspace in the presence of Gaussian noise (i.e., robust subspace learning). We show that these two problems can be simultaneously solved by proposing a new method (called principal coefficients embedding, PCE). For a given data set $\mathbf{D}\in \mathds{R}^{m\times n}$, PCE recovers a clean data set $\mathbf{D}_{0}\in \mathds{R}^{m\times n}$ from $\mathbf{D}$ and simultaneously learns a global reconstruction relation $\mathbf{C}\in \mathbf{R}^{n\times n}$ of $\mathbf{D}_{0}$. By preserving $\mathbf{C}$ into an $m^{\prime}$-dimensional space, the proposed method obtains a projection matrix that can capture the latent manifold structure of $\mathbf{D}_{0}$, where $m^{\prime}\ll m$ is automatically determined by the rank of $\mathbf{C}$ with theoretical guarantees. PCE has three advantages: 1) it can automatically determine the feature dimension even though data are sampled from a union of multiple linear subspaces in presence of the Gaussian noise, 2) Although the objective function of PCE only considers the Gaussian noise, experimental results show that it is robust to the non-Gaussian noise (\textit{e.g.}, random pixel corruption) and real disguises, 3) Our method has a closed-form solution and can be calculated very fast. Extensive experimental results show the superiority of PCE on a range of databases with respect to the classification accuracy, robustness and efficiency.



### A Nonparametric Bayesian Approach Toward Stacked Convolutional Independent Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/1411.4423v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4423v5)
- **Published**: 2014-11-17 10:35:09+00:00
- **Updated**: 2015-09-23 20:22:53+00:00
- **Authors**: Sotirios P. Chatzis
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised feature learning algorithms based on convolutional formulations of independent components analysis (ICA) have been demonstrated to yield state-of-the-art results in several action recognition benchmarks. However, existing approaches do not allow for the number of latent components (features) to be automatically inferred from the data in an unsupervised manner. This is a significant disadvantage of the state-of-the-art, as it results in considerable burden imposed on researchers and practitioners, who must resort to tedious cross-validation procedures to obtain the optimal number of latent features. To resolve these issues, in this paper we introduce a convolutional nonparametric Bayesian sparse ICA architecture for overcomplete feature learning from high-dimensional data. Our method utilizes an Indian buffet process prior to facilitate inference of the appropriate number of latent features under a hybrid variational inference algorithm, scalable to massive datasets. As we show, our model can be naturally used to obtain deep unsupervised hierarchical feature extractors, by greedily stacking successive model layers, similar to existing approaches. In addition, inference for this model is completely heuristics-free; thus, it obviates the need of tedious parameter tuning, which is a major challenge most deep learning approaches are faced with. We evaluate our method on several action recognition benchmarks, and exhibit its advantages over the state-of-the-art.



### Fully Convolutional Neural Networks for Crowd Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1411.4464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4464v1)
- **Published**: 2014-11-17 13:09:09+00:00
- **Updated**: 2014-11-17 13:09:09+00:00
- **Authors**: Kai Kang, Xiaogang Wang
- **Comment**: 9 pages,7 figures
- **Journal**: None
- **Summary**: In this paper, we propose a fast fully convolutional neural network (FCNN) for crowd segmentation. By replacing the fully connected layers in CNN with 1 by 1 convolution kernels, FCNN takes whole images as inputs and directly outputs segmentation maps by one pass of forward propagation. It has the property of translation invariance like patch-by-patch scanning but with much lower computation cost. Once FCNN is learned, it can process input images of any sizes without warping them to a standard size. These attractive properties make it extendable to other general image segmentation problems. Based on FCNN, a multi-stage deep learning is proposed to integrate appearance and motion cues for crowd segmentation. Both appearance filters and motion filers are pretrained stage-by-stage and then jointly optimized. Different combination methods are investigated. The effectiveness of our approach and component-wise analysis are evaluated on two crowd segmentation datasets created by us, which include image frames from 235 and 11 scenes, respectively. They are currently the largest crowd segmentation datasets and will be released to the public.



### Joint cross-domain classification and subspace learning for unsupervised adaptation
- **Arxiv ID**: http://arxiv.org/abs/1411.4491v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1411.4491v3)
- **Published**: 2014-11-17 14:29:35+00:00
- **Updated**: 2015-04-29 02:51:00+00:00
- **Authors**: Basura Fernando, Tatiana Tommasi, Tinne Tuytelaars
- **Comment**: Paper is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: Domain adaptation aims at adapting the knowledge acquired on a source domain to a new different but related target domain. Several approaches have beenproposed for classification tasks in the unsupervised scenario, where no labeled target data are available. Most of the attention has been dedicated to searching a new domain-invariant representation, leaving the definition of the prediction function to a second stage. Here we propose to learn both jointly. Specifically we learn the source subspace that best matches the target subspace while at the same time minimizing a regularized misclassification loss. We provide an alternating optimization technique based on stochastic sub-gradient descent to solve the learning problem and we demonstrate its performance on several domain adaptation tasks.



### Show and Tell: A Neural Image Caption Generator
- **Arxiv ID**: http://arxiv.org/abs/1411.4555v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4555v2)
- **Published**: 2014-11-17 17:15:41+00:00
- **Updated**: 2015-04-20 22:26:11+00:00
- **Authors**: Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.



### TILDE: A Temporally Invariant Learned DEtector
- **Arxiv ID**: http://arxiv.org/abs/1411.4568v3
- **DOI**: 10.1109/CVPR.2015.7299165
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4568v3)
- **Published**: 2014-11-17 17:44:21+00:00
- **Updated**: 2015-03-12 20:07:01+00:00
- **Authors**: Yannick Verdie, Kwang Moo Yi, Pascal Fua, Vincent Lepetit
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a learning-based approach to detect repeatable keypoints under drastic imaging changes of weather and lighting conditions to which state-of-the-art keypoint detectors are surprisingly sensitive. We first identify good keypoint candidates in multiple training images taken from the same viewpoint. We then train a regressor to predict a score map whose maxima are those points so that they can be found by simple non-maximum suppression. As there are no standard datasets to test the influence of these kinds of changes, we created our own, which we will make publicly available. We will show that our method significantly outperforms the state-of-the-art methods in such challenging conditions, while still achieving state-of-the-art performance on the untrained standard Oxford dataset.



### AlexU-Word: A New Dataset for Isolated-Word Closed-Vocabulary Offline Arabic Handwriting Recognition
- **Arxiv ID**: http://arxiv.org/abs/1411.4670v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.2; I.7.5
- **Links**: [PDF](http://arxiv.org/pdf/1411.4670v1)
- **Published**: 2014-11-17 21:23:26+00:00
- **Updated**: 2014-11-17 21:23:26+00:00
- **Authors**: Mohamed E. Hussein, Marwan Torki, Ahmed Elsallamy, Mahmoud Fayyaz
- **Comment**: 6 pages, 8 figure, and 6 tables
- **Journal**: None
- **Summary**: In this paper, we introduce the first phase of a new dataset for offline Arabic handwriting recognition. The aim is to collect a very large dataset of isolated Arabic words that covers all letters of the alphabet in all possible shapes using a small number of simple words. The end goal is to collect a very large dataset of segmented letter images, which can be used to build and evaluate Arabic handwriting recognition systems that are based on segmented letter recognition. The current version of the dataset contains $25114$ samples of $109$ unique Arabic words that cover all possible shapes of all alphabet letters. The samples were collected from $907$ writers. In its current form, the dataset can be used for the problem of closed-vocabulary word recognition. We evaluated a number of window-based descriptors and classifiers on this task and obtained an accuracy of $92.16\%$ using a SIFT-based descriptor and ANN.



