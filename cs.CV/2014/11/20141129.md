# Arxiv Papers in cs.CV on 2014-11-29
### Egocentric Pose Recognition in Four Lines of Code
- **Arxiv ID**: http://arxiv.org/abs/1412.0060v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.0060v1)
- **Published**: 2014-11-29 02:26:33+00:00
- **Updated**: 2014-11-29 02:26:33+00:00
- **Authors**: Gregory Rogez, James S. Supancic III, Deva Ramanan
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: We tackle the problem of estimating the 3D pose of an individual's upper limbs (arms+hands) from a chest mounted depth-camera. Importantly, we consider pose estimation during everyday interactions with objects. Past work shows that strong pose+viewpoint priors and depth-based features are crucial for robust performance. In egocentric views, hands and arms are observable within a well defined volume in front of the camera. We call this volume an egocentric workspace. A notable property is that hand appearance correlates with workspace location. To exploit this correlation, we classify arm+hand configurations in a global egocentric coordinate frame, rather than a local scanning window. This greatly simplify the architecture and improves performance. We propose an efficient pipeline which 1) generates synthetic workspace exemplars for training using a virtual chest-mounted camera whose intrinsic parameters match our physical camera, 2) computes perspective-aware depth features on this entire volume and 3) recognizes discrete arm+hand pose classes through a sparse multi-class SVM. Our method provides state-of-the-art hand pose recognition performance from egocentric RGB-D images in real-time.



### A Bayesian Framework for Sparse Representation-Based 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1412.0062v1
- **DOI**: 10.1109/LSP.2014.2301726
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.0062v1)
- **Published**: 2014-11-29 02:53:47+00:00
- **Updated**: 2014-11-29 02:53:47+00:00
- **Authors**: Behnam Babagholami-Mohamadabadi, Amin Jourabloo, Ali Zarghami, Shohreh Kasaei
- **Comment**: Accepted in IEEE Signal Processing Letters (SPL), 2014
- **Journal**: None
- **Summary**: A Bayesian framework for 3D human pose estimation from monocular images based on sparse representation (SR) is introduced. Our probabilistic approach aims at simultaneously learning two overcomplete dictionaries (one for the visual input space and the other for the pose space) with a shared sparse representation. Existing SR-based pose estimation approaches only offer a point estimation of the dictionary and the sparse codes. Therefore, they might be unreliable when the number of training examples is small. Our Bayesian framework estimates a posterior distribution for the sparse codes and the dictionaries from labeled training data. Hence, it is robust to overfitting on small-size training data. Experimental results on various human activities show that the proposed method is superior to the state of-the-art pose estimation algorithms.



### 3D Hand Pose Detection in Egocentric RGB-D Images
- **Arxiv ID**: http://arxiv.org/abs/1412.0065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.0065v1)
- **Published**: 2014-11-29 03:19:56+00:00
- **Updated**: 2014-11-29 03:19:56+00:00
- **Authors**: Gregory Rogez, James S. Supancic III, Maryam Khademi, Jose Maria Martinez Montiel, Deva Ramanan
- **Comment**: 14 pages, 15 figures, extended version of the corresponding ECCV
  workshop paper, submitted to International Journal of Computer Vision
- **Journal**: None
- **Summary**: We focus on the task of everyday hand pose estimation from egocentric viewpoints. For this task, we show that depth sensors are particularly informative for extracting near-field interactions of the camera wearer with his/her environment. Despite the recent advances in full-body pose estimation using Kinect-like sensors, reliable monocular hand pose estimation in RGB-D images is still an unsolved problem. The problem is considerably exacerbated when analyzing hands performing daily activities from a first-person viewpoint, due to severe occlusions arising from object manipulations and a limited field-of-view. Our system addresses these difficulties by exploiting strong priors over viewpoint and pose in a discriminative tracking-by-detection framework. Our priors are operationalized through a photorealistic synthetic model of egocentric scenes, which is used to generate training data for learning depth-based pose classifiers. We evaluate our approach on an annotated dataset of real egocentric object manipulation scenes and compare to both commercial and academic approaches. Our method provides state-of-the-art performance for both hand detection and pose estimation in egocentric RGB-D images.



### Pedestrian Detection aided by Deep Learning Semantic Tasks
- **Arxiv ID**: http://arxiv.org/abs/1412.0069v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.0069v1)
- **Published**: 2014-11-29 04:34:23+00:00
- **Updated**: 2014-11-29 04:34:23+00:00
- **Authors**: Yonglong Tian, Ping Luo, Xiaogang Wang, Xiaoou Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods have achieved great success in pedestrian detection, owing to its ability to learn features from raw pixels. However, they mainly capture middle-level representations, such as pose of pedestrian, but confuse positive with hard negative samples, which have large ambiguity, e.g. the shape and appearance of `tree trunk' or `wire pole' are similar to pedestrian in certain viewpoint. This ambiguity can be distinguished by high-level representation. To this end, this work jointly optimizes pedestrian detection with semantic tasks, including pedestrian attributes (e.g. `carrying backpack') and scene attributes (e.g. `road', `tree', and `horizontal'). Rather than expensively annotating scene attributes, we transfer attributes information from existing scene segmentation datasets to the pedestrian dataset, by proposing a novel deep model to learn high-level features from multiple tasks and multiple data sources. Since distinct tasks have distinct convergence rates and data from different datasets have different distributions, a multi-task objective function is carefully designed to coordinate tasks and reduce discrepancies among datasets. The importance coefficients of tasks and network parameters in this objective function can be iteratively estimated. Extensive evaluations show that the proposed approach outperforms the state-of-the-art on the challenging Caltech and ETH datasets, where it reduces the miss rates of previous deep models by 17 and 5.5 percent, respectively.



### Multiple Instance Reinforcement Learning for Efficient Weakly-Supervised Detection in Images
- **Arxiv ID**: http://arxiv.org/abs/1412.0100v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1412.0100v1)
- **Published**: 2014-11-29 12:18:14+00:00
- **Updated**: 2014-11-29 12:18:14+00:00
- **Authors**: Stefan Mathe, Cristian Sminchisescu
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art visual recognition and detection systems increasingly rely on large amounts of training data and complex classifiers. Therefore it becomes increasingly expensive both to manually annotate datasets and to keep running times at levels acceptable for practical applications. In this paper, we propose two solutions to address these issues. First, we introduce a weakly supervised, segmentation-based approach to learn accurate detectors and image classifiers from weak supervisory signals that provide only approximate constraints on target localization. We illustrate our system on the problem of action detection in static images (Pascal VOC Actions 2012), using human visual search patterns as our training signal. Second, inspired from the saccade-and-fixate operating principle of the human visual system, we use reinforcement learning techniques to train efficient search models for detection. Our sequential method is weakly supervised and general (it does not require eye movements), finds optimal search strategies for any given detection confidence function and achieves performance similar to exhaustive sliding window search at a fraction of its computational cost.



### Color image quality assessment measure using multivariate generalized Gaussian distribution
- **Arxiv ID**: http://arxiv.org/abs/1412.0111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.0111v1)
- **Published**: 2014-11-29 14:45:14+00:00
- **Updated**: 2014-11-29 14:45:14+00:00
- **Authors**: Mounir Omari, Abdelkaher Ait Abdelouahad, Mohammed El Hassouni, Hocine Cherifi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper deals with color image quality assessment in the reduced-reference framework based on natural scenes statistics. In this context, we propose to model the statistics of the steerable pyramid coefficients by a Multivariate Generalized Gaussian distribution (MGGD). This model allows taking into account the high correlation between the components of the RGB color space. For each selected scale and orientation, we extract a parameter matrix from the three color components subbands. In order to quantify the visual degradation, we use a closed-form of Kullback-Leibler Divergence (KLD) between two MGGDs. Using "TID 2008" benchmark, the proposed measure has been compared with the most influential methods according to the FRTV1 VQEG framework. Results demonstrates its effectiveness for a great variety of distortion type. Among other benefits this measure uses only very little information about the original image.



### Robust Camera Location Estimation by Convex Programming
- **Arxiv ID**: http://arxiv.org/abs/1412.0165v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1412.0165v2)
- **Published**: 2014-11-29 23:25:41+00:00
- **Updated**: 2015-06-04 03:23:21+00:00
- **Authors**: Onur Ozyesil, Amit Singer
- **Comment**: 10 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: $3$D structure recovery from a collection of $2$D images requires the estimation of the camera locations and orientations, i.e. the camera motion. For large, irregular collections of images, existing methods for the location estimation part, which can be formulated as the inverse problem of estimating $n$ locations $\mathbf{t}_1, \mathbf{t}_2, \ldots, \mathbf{t}_n$ in $\mathbb{R}^3$ from noisy measurements of a subset of the pairwise directions $\frac{\mathbf{t}_i - \mathbf{t}_j}{\|\mathbf{t}_i - \mathbf{t}_j\|}$, are sensitive to outliers in direction measurements. In this paper, we firstly provide a complete characterization of well-posed instances of the location estimation problem, by presenting its relation to the existing theory of parallel rigidity. For robust estimation of camera locations, we introduce a two-step approach, comprised of a pairwise direction estimation method robust to outliers in point correspondences between image pairs, and a convex program to maintain robustness to outlier directions. In the presence of partially corrupted measurements, we empirically demonstrate that our convex formulation can even recover the locations exactly. Lastly, we demonstrate the utility of our formulations through experiments on Internet photo collections.



