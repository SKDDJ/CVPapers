# Arxiv Papers in cs.CV on 2014-11-16
### Revisiting Kernelized Locality-Sensitive Hashing for Improved Large-Scale Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1411.4199v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1411.4199v1)
- **Published**: 2014-11-16 00:08:24+00:00
- **Updated**: 2014-11-16 00:08:24+00:00
- **Authors**: Ke Jiang, Qichao Que, Brian Kulis
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: We present a simple but powerful reinterpretation of kernelized locality-sensitive hashing (KLSH), a general and popular method developed in the vision community for performing approximate nearest-neighbor searches in an arbitrary reproducing kernel Hilbert space (RKHS). Our new perspective is based on viewing the steps of the KLSH algorithm in an appropriately projected space, and has several key theoretical and practical benefits. First, it eliminates the problematic conceptual difficulties that are present in the existing motivation of KLSH. Second, it yields the first formal retrieval performance bounds for KLSH. Third, our analysis reveals two techniques for boosting the empirical performance of KLSH. We evaluate these extensions on several large-scale benchmark image retrieval data sets, and show that our analysis leads to improved recall performance of at least 12%, and sometimes much higher, over the standard KLSH method.



### Efficient and Accurate Approximations of Nonlinear Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1411.4229v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4229v1)
- **Published**: 2014-11-16 08:37:25+00:00
- **Updated**: 2014-11-16 08:37:25+00:00
- **Authors**: Xiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims to accelerate the test-time computation of deep convolutional neural networks (CNNs). Unlike existing methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We minimize the reconstruction error of the nonlinear responses, subject to a low-rank constraint which helps to reduce the complexity of filters. We develop an effective solution to this constrained nonlinear optimization problem. An algorithm is also presented for reducing the accumulated error when multiple layers are approximated. A whole-model speedup ratio of 4x is demonstrated on a large network trained for ImageNet, while the top-5 error rate is only increased by 0.9%. Our accelerated model has a comparably fast speed as the "AlexNet", but is 4.7% more accurate.



### Efficient Object Localization Using Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1411.4280v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4280v3)
- **Published**: 2014-11-16 17:23:02+00:00
- **Updated**: 2015-06-09 12:29:21+00:00
- **Authors**: Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, Christopher Bregler
- **Comment**: 8 pages with 1 page of citations
- **Journal**: None
- **Summary**: Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC dataset and outperforms all existing approaches on the MPII-human-pose dataset.



### Combining contextual and local edges for line segment extraction in cluttered images
- **Arxiv ID**: http://arxiv.org/abs/1411.4296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4296v1)
- **Published**: 2014-11-16 19:40:32+00:00
- **Updated**: 2014-11-16 19:40:32+00:00
- **Authors**: Rui F. C. Guerreiro
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic extraction methods typically assume that line segments are pronounced, thin, few and far between, do not cross each other, and are noise and clutter-free. Since these assumptions often fail in realistic scenarios, many line segments are not detected or are fragmented. In more severe cases, i.e., many who use the Hough Transform, extraction can fail entirely. In this paper, we propose a method that tackles these issues. Its key aspect is the combination of thresholded image derivatives obtained with filters of large and small footprints, which we denote as contextual and local edges, respectively. Contextual edges are robust to noise and we use them to select valid local edges, i.e., local edges that are of the same type as contextual ones: dark-to-bright transition of vice-versa. If the distance between valid local edges does not exceed a maximum distance threshold, we enforce connectivity by marking them and the pixels in between as edge points. This originates connected edge maps that are robust and well localized. We use a powerful two-sample statistical test to compute contextual edges, which we introduce briefly, as they are unfamiliar to the image processing community. Finally, we present experiments that illustrate, with synthetic and real images, how our method is efficient in extracting complete segments of all lengths and widths in several situations where current methods fail.



### Ten Years of Pedestrian Detection, What Have We Learned?
- **Arxiv ID**: http://arxiv.org/abs/1411.4304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4304v1)
- **Published**: 2014-11-16 21:25:53+00:00
- **Updated**: 2014-11-16 21:25:53+00:00
- **Authors**: Rodrigo Benenson, Mohamed Omran, Jan Hosang, Bernt Schiele
- **Comment**: To appear in ECCV 2014 CVRSUAD workshop proceedings
- **Journal**: None
- **Summary**: Paper-by-paper results make it easy to miss the forest for the trees.We analyse the remarkable progress of the last decade by discussing the main ideas explored in the 40+ detectors currently present in the Caltech pedestrian detection benchmark. We observe that there exist three families of approaches, all currently reaching similar detection quality. Based on our analysis, we study the complementarity of the most promising ideas by combining multiple published strategies. This new decision forest detector achieves the current best known performance on the challenging Caltech-USA dataset.



### A Latent Clothing Attribute Approach for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1411.4331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.4331v1)
- **Published**: 2014-11-16 23:47:59+00:00
- **Updated**: 2014-11-16 23:47:59+00:00
- **Authors**: Weipeng Zhang, Jie Shen, Guangcan Liu, Yong Yu
- **Comment**: accepted to ACCV 2014, preceding work http://arxiv.org/abs/1404.4923
- **Journal**: None
- **Summary**: As a fundamental technique that concerns several vision tasks such as image parsing, action recognition and clothing retrieval, human pose estimation (HPE) has been extensively investigated in recent years. To achieve accurate and reliable estimation of the human pose, it is well-recognized that the clothing attributes are useful and should be utilized properly. Most previous approaches, however, require to manually annotate the clothing attributes and are therefore very costly. In this paper, we shall propose and explore a \emph{latent} clothing attribute approach for HPE. Unlike previous approaches, our approach models the clothing attributes as latent variables and thus requires no explicit labeling for the clothing attributes. The inference of the latent variables are accomplished by utilizing the framework of latent structured support vector machines (LSSVM). We employ the strategy of \emph{alternating direction} to train the LSSVM model: In each iteration, one kind of variables (e.g., human pose or clothing attribute) are fixed and the others are optimized. Our extensive experiments on two real-world benchmarks show the state-of-the-art performance of our proposed approach.



