# Arxiv Papers in cs.CV on 2014-11-12
### Collecting Image Description Datasets using Crowdsourcing
- **Arxiv ID**: http://arxiv.org/abs/1411.3041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.3041v1)
- **Published**: 2014-11-12 01:34:46+00:00
- **Updated**: 2014-11-12 01:34:46+00:00
- **Authors**: Ramakrishna Vedantam, C. Lawrence Zitnick, Devi Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: We describe our two new datasets with images described by humans. Both the datasets were collected using Amazon Mechanical Turk, a crowdsourcing platform. The two datasets contain significantly more descriptions per image than other existing datasets. One is based on a popular image description dataset called the UIUC Pascal Sentence Dataset, whereas the other is based on the Abstract Scenes dataset con- taining images made from clipart objects. In this paper we describe our interfaces, analyze some properties of and show example descriptions from our two datasets.



### Part Detector Discovery in Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1411.3159v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1411.3159v2)
- **Published**: 2014-11-12 12:42:54+00:00
- **Updated**: 2014-11-14 11:57:27+00:00
- **Authors**: Marcel Simon, Erik Rodner, Joachim Denzler
- **Comment**: Accepted for publication on Asian Conference on Computer Vision
  (ACCV) 2014
- **Journal**: None
- **Summary**: Current fine-grained classification approaches often rely on a robust localization of object parts to extract localized feature representations suitable for discrimination. However, part localization is a challenging task due to the large variation of appearance and pose. In this paper, we show how pre-trained convolutional neural networks can be used for robust and efficient object part discovery and localization without the necessity to actually train the network on the current dataset. Our approach called "part detector discovery" (PDD) is based on analyzing the gradient maps of the network outputs and finding activation centers spatially related to annotated semantic parts or bounding boxes.   This allows us not just to obtain excellent performance on the CUB200-2011 dataset, but in contrast to previous approaches also to perform detection and bird classification jointly without requiring a given bounding box annotation during testing and ground-truth parts during training. The code is available at http://www.inf-cv.uni-jena.de/part_discovery and https://github.com/cvjena/PartDetectorDisovery.



### On Coarse Graining of Information and Its Application to Pattern Recognition
- **Arxiv ID**: http://arxiv.org/abs/1411.3169v1
- **DOI**: 10.1063/1.4906011
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1411.3169v1)
- **Published**: 2014-11-12 13:21:48+00:00
- **Updated**: 2014-11-12 13:21:48+00:00
- **Authors**: Ali Ghaderi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method based on finite mixture models for classifying a set of observations into number of different categories. In order to demonstrate the method, we show how the component densities for the mixture model can be derived by using the maximum entropy method in conjunction with conservation of Pythagorean means. Several examples of distributions belonging to the Pythagorean family are derived. A discussion on estimation of model parameters and the number of categories is also given.



### Multi-modal Image Registration for Correlative Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1411.3229v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.3229v2)
- **Published**: 2014-11-12 16:32:17+00:00
- **Updated**: 2015-01-13 15:44:08+00:00
- **Authors**: Tian Cao, Christopher Zach, Shannon Modla, Debbie Powell, Kirk Czymmek, Marc Niethammer
- **Comment**: 24 pages
- **Journal**: None
- **Summary**: Correlative microscopy is a methodology combining the functionality of light microscopy with the high resolution of electron microscopy and other microscopy technologies. Image registration for correlative microscopy is quite challenging because it is a multi-modal, multi-scale and multi-dimensional registration problem. In this report, I introduce two methods of image registration for correlative microscopy. The first method is based on fiducials (beads). I generate landmarks from the fiducials and compute the similarity transformation matrix based on three pairs of nearest corresponding landmarks. A least-squares matching process is applied afterwards to further refine the registration. The second method is inspired by the image analogies approach. I introduce the sparse representation model into image analogies. I first train representative image patches (dictionaries) for pre-registered datasets from two different modalities, and then I use the sparse coding technique to transfer a given image to a predicted image from one modality to another based on the learned dictionaries. The final image registration is between the predicted image and the original image corresponding to the given image in the different modality. The method transforms a multi-modal registration problem to a mono-modal one. I test my approaches on Transmission Electron Microscopy (TEM) and confocal microscopy images. Experimental results of the methods are also shown in this report.



### Sparse Modeling for Image and Vision Processing
- **Arxiv ID**: http://arxiv.org/abs/1411.3230v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1411.3230v2)
- **Published**: 2014-11-12 16:33:37+00:00
- **Updated**: 2014-12-06 14:39:07+00:00
- **Authors**: Julien Mairal, Francis Bach, Jean Ponce
- **Comment**: 205 pages, to appear in Foundations and Trends in Computer Graphics
  and Vision
- **Journal**: None
- **Summary**: In recent years, a large amount of multi-disciplinary research has been conducted on sparse models and their applications. In statistics and machine learning, the sparsity principle is used to perform model selection---that is, automatically selecting a simple model among a large collection of them. In signal processing, sparse coding consists of representing data with linear combinations of a few dictionary elements. Subsequently, the corresponding tools have been widely adopted by several scientific communities such as neuroscience, bioinformatics, or computer vision. The goal of this monograph is to offer a self-contained view of sparse modeling for visual recognition and image processing. More specifically, we focus on applications where the dictionary is learned and adapted to data, yielding a compact representation that has been successful in various contexts.



### Amoeba Techniques for Shape and Texture Analysis
- **Arxiv ID**: http://arxiv.org/abs/1411.3285v2
- **DOI**: 10.1007/978-3-319-24726-7_4
- **Categories**: **cs.CV**, I.4.3; I.4.6; I.2.10; G.1.8; G.2.2
- **Links**: [PDF](http://arxiv.org/pdf/1411.3285v2)
- **Published**: 2014-11-12 19:14:30+00:00
- **Updated**: 2015-06-10 15:28:31+00:00
- **Authors**: Martin Welk
- **Comment**: 38 pages, 19 figures v2: minor corrections and rephrasing, Section 5
  (pre-smoothing) extended
- **Journal**: None
- **Summary**: Morphological amoebas are image-adaptive structuring elements for morphological and other local image filters introduced by Lerallut et al. Their construction is based on combining spatial distance with contrast information into an image-dependent metric. Amoeba filters show interesting parallels to image filtering methods based on partial differential equations (PDEs), which can be confirmed by asymptotic equivalence results. In computing amoebas, graph structures are generated that hold information about local image texture. This paper reviews and summarises the work of the author and his coauthors on morphological amoebas, particularly their relations to PDE filters and texture analysis. It presents some extensions and points out directions for future investigation on the subject.



