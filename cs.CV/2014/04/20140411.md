# Arxiv Papers in cs.CV on 2014-04-11
### A Reverse Hierarchy Model for Predicting Eye Fixations
- **Arxiv ID**: http://arxiv.org/abs/1404.2999v1
- **DOI**: 10.1109/CVPR.2014.361
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1404.2999v1)
- **Published**: 2014-04-11 04:39:21+00:00
- **Updated**: 2014-04-11 04:39:21+00:00
- **Authors**: Tianlin Shi, Liang Ming, Xiaolin Hu
- **Comment**: CVPR 2014, 27th IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR). CVPR 2014
- **Journal**: None
- **Summary**: A number of psychological and physiological evidences suggest that early visual attention works in a coarse-to-fine way, which lays a basis for the reverse hierarchy theory (RHT). This theory states that attention propagates from the top level of the visual hierarchy that processes gist and abstract information of input, to the bottom level that processes local details. Inspired by the theory, we develop a computational model for saliency detection in images. First, the original image is downsampled to different scales to constitute a pyramid. Then, saliency on each layer is obtained by image super-resolution reconstruction from the layer above, which is defined as unpredictability from this coarse-to-fine reconstruction. Finally, saliency on each layer of the pyramid is fused into stochastic fixations through a probabilistic model, where attention initiates from the top layer and propagates downward through the pyramid. Extensive experiments on two standard eye-tracking datasets show that the proposed method can achieve competitive results with state-of-the-art models.



### Bayesian image segmentations by Potts prior and loopy belief propagation
- **Arxiv ID**: http://arxiv.org/abs/1404.3012v5
- **DOI**: 10.7566/JPSJ.83.124002
- **Categories**: **cs.CV**, cond-mat.dis-nn, cond-mat.stat-mech, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1404.3012v5)
- **Published**: 2014-04-11 06:31:03+00:00
- **Updated**: 2014-08-18 04:45:26+00:00
- **Authors**: Kazuyuki Tanaka, Shun Kataoka, Muneki Yasuda, Yuji Waizumi, Chiou-Ting Hsu
- **Comment**: 24 pages, 9 figures
- **Journal**: Journal of the Physical Society of Japan 83 (2014) 124002
- **Summary**: This paper presents a Bayesian image segmentation model based on Potts prior and loopy belief propagation. The proposed Bayesian model involves several terms, including the pairwise interactions of Potts models, and the average vectors and covariant matrices of Gauss distributions in color image modeling. These terms are often referred to as hyperparameters in statistical machine learning theory. In order to determine these hyperparameters, we propose a new scheme for hyperparameter estimation based on conditional maximization of entropy in the Potts prior. The algorithm is given based on loopy belief propagation. In addition, we compare our conditional maximum entropy framework with the conventional maximum likelihood framework, and also clarify how the first order phase transitions in LBP's for Potts models influence our hyperparameter estimation procedures.



### Decreasing Weighted Sorted $\ell_1$ Regularization
- **Arxiv ID**: http://arxiv.org/abs/1404.3184v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1404.3184v1)
- **Published**: 2014-04-11 18:50:34+00:00
- **Updated**: 2014-04-11 18:50:34+00:00
- **Authors**: Xiangrong Zeng, MÃ¡rio A. T. Figueiredo
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: We consider a new family of regularizers, termed {\it weighted sorted $\ell_1$ norms} (WSL1), which generalizes the recently introduced {\it octagonal shrinkage and clustering algorithm for regression} (OSCAR) and also contains the $\ell_1$ and $\ell_{\infty}$ norms as particular instances. We focus on a special case of the WSL1, the {\sl decreasing WSL1} (DWSL1), where the elements of the argument vector are sorted in non-increasing order and the weights are also non-increasing. In this paper, after showing that the DWSL1 is indeed a norm, we derive two key tools for its use as a regularizer: the dual norm and the Moreau proximity operator.



