# Arxiv Papers in cs.CV on 2014-09-15
### Transfer Learning for Video Recognition with Scarce Training Data for Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1409.4127v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1409.4127v2)
- **Published**: 2014-09-15 01:26:55+00:00
- **Updated**: 2015-06-15 14:47:54+00:00
- **Authors**: Yu-Chuan Su, Tzu-Hsuan Chiu, Chun-Yen Yeh, Hsin-Fu Huang, Winston H. Hsu
- **Comment**: None
- **Journal**: None
- **Summary**: Unconstrained video recognition and Deep Convolution Network (DCN) are two active topics in computer vision recently. In this work, we apply DCNs as frame-based recognizers for video recognition. Our preliminary studies, however, show that video corpora with complete ground truth are usually not large and diverse enough to learn a robust model. The networks trained directly on the video data set suffer from significant overfitting and have poor recognition rate on the test set. The same lack-of-training-sample problem limits the usage of deep models on a wide range of computer vision problems where obtaining training data are difficult. To overcome the problem, we perform transfer learning from images to videos to utilize the knowledge in the weakly labeled image corpus for video recognition. The image corpus help to learn important visual patterns for natural images, while these patterns are ignored by models trained only on the video corpus. Therefore, the resultant networks have better generalizability and better recognition rate. We show that by means of transfer learning from image to video, we can learn a frame-based recognizer with only 4k videos. Because the image corpus is weakly labeled, the entire learning process requires only 4k annotated instances, which is far less than the million scale image data sets required by previous works. The same approach may be applied to other visual recognition tasks where only scarce training data is available, and it improves the applicability of DCNs in various computer vision problems. Our experiments also reveal the correlation between meta-parameters and the performance of DCNs, given the properties of the target problem and data. These results lead to a heuristic for meta-parameter selection for future researches, which does not rely on the time consuming meta-parameter search.



### A feasible roadmap for developing volumetric probability atlas of localized prostate cancer
- **Arxiv ID**: http://arxiv.org/abs/1409.4139v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1409.4139v1)
- **Published**: 2014-09-15 02:03:52+00:00
- **Updated**: 2014-09-15 02:03:52+00:00
- **Authors**: Liang Zhao, Jianhua Xuan, Yue Wang
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: A statistical volumetric model, showing the probability map of localized prostate cancer within the host anatomical structure, has been developed from 90 optically-imaged surgical specimens. This master model permits an accurate characterization of prostate cancer distribution patterns and an atlas-informed biopsy sampling strategy. The model is constructed by mapping individual prostate models onto a site model, together with localized tumors. An accurate multi-object non-rigid warping scheme is developed based on a mixture of principal-axis registrations. We report our evaluation and pilot studies on the effectiveness of the method and its application to optimizing needle biopsy strategies.



### Speeding-up Graphical Model Optimization via a Coarse-to-fine Cascade of Pruning Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1409.4205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1409.4205v1)
- **Published**: 2014-09-15 10:53:04+00:00
- **Updated**: 2014-09-15 10:53:04+00:00
- **Authors**: B. Conejo, N. Komodakis, S. Leprince, J. P. Avouac
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a general and versatile framework that significantly speeds-up graphical model optimization while maintaining an excellent solution accuracy. The proposed approach relies on a multi-scale pruning scheme that is able to progressively reduce the solution space by use of a novel strategy based on a coarse-to-fine cascade of learnt classifiers. We thoroughly experiment with classic computer vision related MRF problems, where our framework constantly yields a significant time speed-up (with respect to the most efficient inference methods) and obtains a more accurate solution than directly optimizing the MRF.



### The Ordered Weighted $\ell_1$ Norm: Atomic Formulation, Projections, and Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1409.4271v5
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1409.4271v5)
- **Published**: 2014-09-15 14:22:34+00:00
- **Updated**: 2015-04-10 13:21:46+00:00
- **Authors**: Xiangrong Zeng, Mário A. T. Figueiredo
- **Comment**: 13 pages, 17 figures. The latest version of this paper was submitted
  to a journal
- **Journal**: None
- **Summary**: The ordered weighted $\ell_1$ norm (OWL) was recently proposed, with two different motivations: its good statistical properties as a sparsity promoting regularizer; the fact that it generalizes the so-called {\it octagonal shrinkage and clustering algorithm for regression} (OSCAR), which has the ability to cluster/group regression variables that are highly correlated. This paper contains several contributions to the study and application of OWL regularization: the derivation of the atomic formulation of the OWL norm; the derivation of the dual of the OWL norm, based on its atomic formulation; a new and simpler derivation of the proximity operator of the OWL norm; an efficient scheme to compute the Euclidean projection onto an OWL ball; the instantiation of the conditional gradient (CG, also known as Frank-Wolfe) algorithm for linear regression problems under OWL regularization; the instantiation of accelerated projected gradient algorithms for the same class of problems. Finally, a set of experiments give evidence that accelerated projected gradient algorithms are considerably faster than CG, for the class of problems considered.



### Computing the Stereo Matching Cost with a Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1409.4326v2
- **DOI**: 10.1109/CVPR.2015.7298767
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1409.4326v2)
- **Published**: 2014-09-15 16:54:42+00:00
- **Updated**: 2015-10-20 15:08:48+00:00
- **Authors**: Jure Žbontar, Yann LeCun
- **Comment**: Conference on Computer Vision and Pattern Recognition (CVPR), June
  2015
- **Journal**: None
- **Summary**: We present a method for extracting depth information from a rectified image pair. We train a convolutional neural network to predict how well two image patches match and use it to compute the stereo matching cost. The cost is refined by cross-based cost aggregation and semiglobal matching, followed by a left-right consistency check to eliminate errors in the occluded regions. Our stereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and is currently (August 2014) the top performing method on this dataset.



### Zero Shot Recognition with Unreliable Attributes
- **Arxiv ID**: http://arxiv.org/abs/1409.4327v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1409.4327v2)
- **Published**: 2014-09-15 16:56:07+00:00
- **Updated**: 2016-03-29 19:33:17+00:00
- **Authors**: Dinesh Jayaraman, Kristen Grauman
- **Comment**: NIPS 2014
- **Journal**: None
- **Summary**: In principle, zero-shot learning makes it possible to train a recognition model simply by specifying the category's attributes. For example, with classifiers for generic attributes like \emph{striped} and \emph{four-legged}, one can construct a classifier for the zebra category by enumerating which properties it possesses---even without providing zebra training images. In practice, however, the standard zero-shot paradigm suffers because attribute predictions in novel images are hard to get right. We propose a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions. By leveraging statistics about each attribute's error tendencies, our method obtains more robust discriminative models for the unseen classes. We further devise extensions to handle the few-shot scenario and unreliable attribute descriptions. On three datasets, we demonstrate the benefit for visual category learning with zero or few training examples, a critical domain for rare categories or categories defined on the fly.



### On the optimality of shape and data representation in the spectral domain
- **Arxiv ID**: http://arxiv.org/abs/1409.4349v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1409.4349v1)
- **Published**: 2014-09-15 17:50:26+00:00
- **Updated**: 2014-09-15 17:50:26+00:00
- **Authors**: Yonathan Aflalo, Haim Brezis, Ron Kimmel
- **Comment**: None
- **Journal**: None
- **Summary**: A proof of the optimality of the eigenfunctions of the Laplace-Beltrami operator (LBO) in representing smooth functions on surfaces is provided and adapted to the field of applied shape and data analysis. It is based on the Courant-Fischer min-max principle adapted to our case. % The theorem we present supports the new trend in geometry processing of treating geometric structures by using their projection onto the leading eigenfunctions of the decomposition of the LBO. Utilisation of this result can be used for constructing numerically efficient algorithms to process shapes in their spectrum. We review a couple of applications as possible practical usage cases of the proposed optimality criteria. % We refer to a scale invariant metric, which is also invariant to bending of the manifold. This novel pseudo-metric allows constructing an LBO by which a scale invariant eigenspace on the surface is defined. We demonstrate the efficiency of an intermediate metric, defined as an interpolation between the scale invariant and the regular one, in representing geometric structures while capturing both coarse and fine details. Next, we review a numerical acceleration technique for classical scaling, a member of a family of flattening methods known as multidimensional scaling (MDS). There, the optimality is exploited to efficiently approximate all geodesic distances between pairs of points on a given surface, and thereby match and compare between almost isometric surfaces. Finally, we revisit the classical principal component analysis (PCA) definition by coupling its variational form with a Dirichlet energy on the data manifold. By pairing the PCA with the LBO we can handle cases that go beyond the scope defined by the observation set that is handled by regular PCA.



### Convolutional Networks for Image Processing by Coupled Oscillator Arrays
- **Arxiv ID**: http://arxiv.org/abs/1409.4469v1
- **DOI**: None
- **Categories**: **nlin.PS**, cond-mat.dis-nn, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1409.4469v1)
- **Published**: 2014-09-15 23:34:15+00:00
- **Updated**: 2014-09-15 23:34:15+00:00
- **Authors**: Dmitri E. Nikonov, Ian A. Young, George I. Bourianoff
- **Comment**: 23 pages, 12 figures
- **Journal**: None
- **Summary**: A coupled oscillator array is shown to approximate convolutions with Gabor filters for image processing tasks. Pixelated image fragments and filter functions are converted to voltages, differenced, and input into a corresponding array of weakly coupled Voltage Controlled Oscillators (VCOs). This is referred to as Frequency Shift Keying (FSK). Upon synchronization of the array, the common node amplitude provides a metric for the degree of match between the image fragment and the filter function. The optimal oscillator parameters for synchronization are determined and favor a moderate value of the Q-factor.



