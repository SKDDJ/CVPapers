# Arxiv Papers in cs.CV on 2014-09-13
### Structure Preserving Large Imagery Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1409.3906v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1409.3906v1)
- **Published**: 2014-09-13 03:29:27+00:00
- **Updated**: 2014-09-13 03:29:27+00:00
- **Authors**: Ju Shen, Jianjun Yang, Sami Taha-abusneineh, Bryson Payne, Markus Hitz
- **Comment**: None
- **Journal**: None
- **Summary**: With the explosive growth of web-based cameras and mobile devices, billions of photographs are uploaded to the internet. We can trivially collect a huge number of photo streams for various goals, such as image clustering, 3D scene reconstruction, and other big data applications. However, such tasks are not easy due to the fact the retrieved photos can have large variations in their view perspectives, resolutions, lighting, noises, and distortions. Fur-thermore, with the occlusion of unexpected objects like people, vehicles, it is even more challenging to find feature correspondences and reconstruct re-alistic scenes. In this paper, we propose a structure-based image completion algorithm for object removal that produces visually plausible content with consistent structure and scene texture. We use an edge matching technique to infer the potential structure of the unknown region. Driven by the estimated structure, texture synthesis is performed automatically along the estimated curves. We evaluate the proposed method on different types of images: from highly structured indoor environment to natural scenes. Our experimental results demonstrate satisfactory performance that can be potentially used for subsequent big data processing, such as image localization, object retrieval, and scene reconstruction. Our experiments show that this approach achieves favorable results that outperform existing state-of-the-art techniques.



### Concurrent Tracking of Inliers and Outliers
- **Arxiv ID**: http://arxiv.org/abs/1409.3913v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1409.3913v1)
- **Published**: 2014-09-13 05:52:36+00:00
- **Updated**: 2014-09-13 05:52:36+00:00
- **Authors**: Jae-Yeong Lee, Wonpil Yu
- **Comment**: draft
- **Journal**: None
- **Summary**: In object tracking, outlier is one of primary factors which degrade performance of image-based tracking algorithms. In this respect, therefore, most of the existing methods simply discard detected outliers and pay little or no attention to employing them as an important source of information for motion estimation. We consider outliers as important as inliers for object tracking and propose a motion estimation algorithm based on concurrent tracking of inliers and outliers. Our tracker makes use of pyramidal implementation of the Lucas-Kanade tracker to estimate motion flows of inliers and outliers and final target motion is estimated robustly based on both of these information. Experimental results from challenging benchmark video sequences confirm enhanced tracking performance, showing highly stable target tracking under severe occlusion compared with state-of-the-art algorithms. The proposed algorithm runs at more than 100 frames per second even without using a hardware accelerator, which makes the proposed method more practical and portable.



### Self-taught Object Localization with Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1409.3964v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1409.3964v7)
- **Published**: 2014-09-13 16:12:43+00:00
- **Updated**: 2016-02-02 20:55:59+00:00
- **Authors**: Loris Bazzani, Alessandro Bergamo, Dragomir Anguelov, Lorenzo Torresani
- **Comment**: WACV 2016
- **Journal**: None
- **Summary**: This paper introduces self-taught object localization, a novel approach that leverages deep convolutional networks trained for whole-image recognition to localize objects in images without additional human supervision, i.e., without using any ground-truth bounding boxes for training. The key idea is to analyze the change in the recognition scores when artificially masking out different regions of the image. The masking out of a region that includes the object typically causes a significant drop in recognition score. This idea is embedded into an agglomerative clustering technique that generates self-taught localization hypotheses. Our object localization scheme outperforms existing proposal methods in both precision and recall for small number of subwindow proposals (e.g., on ILSVRC-2012 it produces a relative gain of 23.4% over the state-of-the-art for top-1 hypothesis). Furthermore, our experiments show that the annotations automatically-generated by our method can be used to train object detectors yielding recognition results remarkably close to those obtained by training on manually-annotated bounding boxes.



### A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data
- **Arxiv ID**: http://arxiv.org/abs/1409.3970v3
- **DOI**: 10.1109/TPAMI.2015.2476802
- **Categories**: **cs.CV**, cs.IR, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1409.3970v3)
- **Published**: 2014-09-13 17:17:05+00:00
- **Updated**: 2015-12-31 16:12:31+00:00
- **Authors**: Yin Zheng, Yu-Jin Zhang, Hugo Larochelle
- **Comment**: 24 pages, 10 figures. A version has been accepted by TPAMI on Aug
  4th, 2015. Add footnote about how to train the model in practice in Section
  5.1. arXiv admin note: substantial text overlap with arXiv:1305.5306
- **Journal**: None
- **Summary**: Topic modeling based on latent Dirichlet allocation (LDA) has been a framework of choice to deal with multimodal data, such as in image annotation tasks. Another popular approach to model the multimodal data is through deep neural networks, such as the deep Boltzmann machine (DBM). Recently, a new type of topic model called the Document Neural Autoregressive Distribution Estimator (DocNADE) was proposed and demonstrated state-of-the-art performance for text document modeling. In this work, we show how to successfully apply and extend this model to multimodal data, such as simultaneous image classification and annotation. First, we propose SupDocNADE, a supervised extension of DocNADE, that increases the discriminative power of the learned hidden topic features and show how to employ it to learn a joint representation from image visual words, annotation words and class label information. We test our model on the LabelMe and UIUC-Sports data sets and show that it compares favorably to other topic models. Second, we propose a deep extension of our model and provide an efficient way of training the deep model. Experimental results show that our deep model outperforms its shallow version and reaches state-of-the-art performance on the Multimedia Information Retrieval (MIR) Flickr data set.



