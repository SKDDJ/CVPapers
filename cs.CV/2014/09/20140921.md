# Arxiv Papers in cs.CV on 2014-09-21
### A Global Approach for Solving Edge-Matching Puzzles
- **Arxiv ID**: http://arxiv.org/abs/1409.5957v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1409.5957v2)
- **Published**: 2014-09-21 09:10:24+00:00
- **Updated**: 2015-02-10 08:24:48+00:00
- **Authors**: Shahar Z. Kovalsky, Daniel Glasner, Ronen Basri
- **Comment**: None
- **Journal**: SIAM J. Imaging Sciences, Vol. 8, Issue 2, 916--938, 2015
- **Summary**: We consider apictorial edge-matching puzzles, in which the goal is to arrange a collection of puzzle pieces with colored edges so that the colors match along the edges of adjacent pieces. We devise an algebraic representation for this problem and provide conditions under which it exactly characterizes a puzzle. Using the new representation, we recast the combinatorial, discrete problem of solving puzzles as a global, polynomial system of equations with continuous variables. We further propose new algorithms for generating approximate solutions to the continuous problem by solving a sequence of convex relaxations.



### Domain Adaptive Neural Networks for Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1409.6041v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1409.6041v1)
- **Published**: 2014-09-21 20:42:00+00:00
- **Updated**: 2014-09-21 20:42:00+00:00
- **Authors**: Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a simple neural network model to deal with the domain adaptation problem in object recognition. Our model incorporates the Maximum Mean Discrepancy (MMD) measure as a regularization in the supervised learning to reduce the distribution mismatch between the source and target domains in the latent space. From experiments, we demonstrate that the MMD regularization is an effective tool to provide good domain adaptation models on both SURF features and raw image pixels of a particular image data set. We also show that our proposed model, preceded by the denoising auto-encoder pretraining, achieves better performance than recent benchmark models on the same data sets. This work represents the first study of MMD measure in the context of neural networks.



### Analyzing sparse dictionaries for online learning with kernels
- **Arxiv ID**: http://arxiv.org/abs/1409.6045v1
- **DOI**: 10.1109/TSP.2015.2457396
- **Categories**: **stat.ML**, cs.CV, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1409.6045v1)
- **Published**: 2014-09-21 21:46:19+00:00
- **Updated**: 2014-09-21 21:46:19+00:00
- **Authors**: Paul Honeine
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Many signal processing and machine learning methods share essentially the same linear-in-the-parameter model, with as many parameters as available samples as in kernel-based machines. Sparse approximation is essential in many disciplines, with new challenges emerging in online learning with kernels. To this end, several sparsity measures have been proposed in the literature to quantify sparse dictionaries and constructing relevant ones, the most prolific ones being the distance, the approximation, the coherence and the Babel measures. In this paper, we analyze sparse dictionaries based on these measures. By conducting an eigenvalue analysis, we show that these sparsity measures share many properties, including the linear independence condition and inducing a well-posed optimization problem. Furthermore, we prove that there exists a quasi-isometry between the parameter (i.e., dual) space and the dictionary's induced feature space.



### Approximation errors of online sparsification criteria
- **Arxiv ID**: http://arxiv.org/abs/1409.6046v1
- **DOI**: 10.1109/TSP.2015.2442960
- **Categories**: **stat.ML**, cs.CV, cs.IT, cs.LG, cs.NE, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1409.6046v1)
- **Published**: 2014-09-21 21:53:08+00:00
- **Updated**: 2014-09-21 21:53:08+00:00
- **Authors**: Paul Honeine
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Many machine learning frameworks, such as resource-allocating networks, kernel-based methods, Gaussian processes, and radial-basis-function networks, require a sparsification scheme in order to address the online learning paradigm. For this purpose, several online sparsification criteria have been proposed to restrict the model definition on a subset of samples. The most known criterion is the (linear) approximation criterion, which discards any sample that can be well represented by the already contributing samples, an operation with excessive computational complexity. Several computationally efficient sparsification criteria have been introduced in the literature, such as the distance, the coherence and the Babel criteria. In this paper, we provide a framework that connects these sparsification criteria to the issue of approximating samples, by deriving theoretical bounds on the approximation errors. Moreover, we investigate the error of approximating any feature, by proposing upper-bounds on the approximation error for each of the aforementioned sparsification criteria. Two classes of features are described in detail, the empirical mean and the principal axes in the kernel principal component analysis.



