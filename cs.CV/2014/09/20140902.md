# Arxiv Papers in cs.CV on 2014-09-02
### Transferring Landmark Annotations for Cross-Dataset Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1409.0602v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1409.0602v1)
- **Published**: 2014-09-02 03:36:55+00:00
- **Updated**: 2014-09-02 03:36:55+00:00
- **Authors**: Shizhan Zhu, Cheng Li, Chen Change Loy, Xiaoou Tang
- **Comment**: Shizhan Zhu and Cheng Li share equal contributions
- **Journal**: None
- **Summary**: Dataset bias is a well known problem in object recognition domain. This issue, nonetheless, is rarely explored in face alignment research. In this study, we show that dataset plays an integral part of face alignment performance. Specifically, owing to face alignment dataset bias, training on one database and testing on another or unseen domain would lead to poor performance. Creating an unbiased dataset through combining various existing databases, however, is non-trivial as one has to exhaustively re-label the landmarks for standardisation. In this work, we propose a simple and yet effective method to bridge the disparate annotation spaces between databases, making datasets fusion possible. We show extensive results on combining various popular databases (LFW, AFLW, LFPW, HELEN) for improved cross-dataset and unseen data alignment.



### Effective Spectral Unmixing via Robust Representation and Learning-based Sparsity
- **Arxiv ID**: http://arxiv.org/abs/1409.0685v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1409.0685v4)
- **Published**: 2014-09-02 12:36:53+00:00
- **Updated**: 2017-08-26 17:39:00+00:00
- **Authors**: Feiyun Zhu, Ying Wang, Bin Fan, Gaofeng Meng, Chunhong Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral unmixing (HU) plays a fundamental role in a wide range of hyperspectral applications. It is still challenging due to the common presence of outlier channels and the large solution space. To address the above two issues, we propose a novel model by emphasizing both robust representation and learning-based sparsity. Specifically, we apply the $\ell_{2,1}$-norm to measure the representation error, preventing outlier channels from dominating our objective. In this way, the side effects of outlier channels are greatly relieved. Besides, we observe that the mixed level of each pixel varies over image grids. Based on this observation, we exploit a learning-based sparsity method to simultaneously learn the HU results and a sparse guidance map. Via this guidance map, the sparsity constraint in the $\ell_{p}\!\left(\!0\!<\! p\!\leq\!1\right)$-norm is adaptively imposed according to the learnt mixed level of each pixel. Compared with state-of-the-art methods, our model is better suited to the real situation, thus expected to achieve better HU results. The resulted objective is highly non-convex and non-smooth, and so it is hard to optimize. As a profound theoretical contribution, we propose an efficient algorithm to solve it. Meanwhile, the convergence proof and the computational complexity analysis are systematically provided. Extensive evaluations verify that our method is highly promising for the HU task---it achieves very accurate guidance maps and much better HU results compared with state-of-the-art methods.



### Image Retrieval And Classification Using Local Feature Vectors
- **Arxiv ID**: http://arxiv.org/abs/1409.0749v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1409.0749v1)
- **Published**: 2014-09-02 15:17:33+00:00
- **Updated**: 2014-09-02 15:17:33+00:00
- **Authors**: Vikas Verma
- **Comment**: None
- **Journal**: None
- **Summary**: Content Based Image Retrieval(CBIR) is one of the important subfield in the field of Information Retrieval. The goal of a CBIR algorithm is to retrieve semantically similar images in response to a query image submitted by the end user. CBIR is a hard problem because of the phenomenon known as $\textit {semantic gap}$.   In this thesis, we aim at analyzing the performance of a CBIR system build using local feature vectors and Intermediate Matching Kernel. We also propose a Two-Step Matching process for reducing the response time of the CBIR systems. Further, we develop a Meta-Learning framework for improving the retrieval performance of these systems. Our results show that the Two-Step Matching process significantly reduces response time and the Meta-Learning Framework improves the retrieval performance by more than two fold. We also analyze the performance of various image classification systems that use different image representations constructed from the local feature vectors.



### CoMOGrad and PHOG: From Computer Vision to Fast and Accurate Protein Tertiary Structure Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1409.0814v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CE, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1409.0814v1)
- **Published**: 2014-09-02 18:26:50+00:00
- **Updated**: 2014-09-02 18:26:50+00:00
- **Authors**: Rezaul Karim, Mohd. Momin Al Aziz, Swakkhar Shatabda, M. Sohel Rahman, Md. Abul Kashem Mia, Farhana Zaman, Salman Rakin
- **Comment**: draft
- **Journal**: None
- **Summary**: Due to the advancements in technology number of entries in the structural database of proteins are increasing day by day. Methods for retrieving protein tertiary structures from this large database is the key to comparative analysis of structures which plays an important role to understand proteins and their function. In this paper, we present fast and accurate methods for the retrieval of proteins from a large database with tertiary structures similar to a query protein. Our proposed methods borrow ideas from the field of computer vision. The speed and accuracy of our methods comes from the two newly introduced features, the co-occurrence matrix of the oriented gradient and pyramid histogram of oriented gradient and from the use of Euclidean distance as the distance measure. Experimental results clearly indicate the superiority of our approach in both running time and accuracy. Our method is readily available for use from this website: http://research.buet.ac.bd:8080/Comograd/.



### Action Recognition in the Frequency Domain
- **Arxiv ID**: http://arxiv.org/abs/1409.0908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1409.0908v1)
- **Published**: 2014-09-02 22:34:29+00:00
- **Updated**: 2014-09-02 22:34:29+00:00
- **Authors**: Anh Tran, Jinyan Guan, Thanima Pilantanakitti, Paul Cohen
- **Comment**: Keywords: Artificial Intelligence, Computer Vision, Action
  Recognition
- **Journal**: None
- **Summary**: In this paper, we describe a simple strategy for mitigating variability in temporal data series by shifting focus onto long-term, frequency domain features that are less susceptible to variability. We apply this method to the human action recognition task and demonstrate how working in the frequency domain can yield good recognition features for commonly used optical flow and articulated pose features, which are highly sensitive to small differences in motion, viewpoint, dynamic backgrounds, occlusion and other sources of variability. We show how these frequency-based features can be used in combination with a simple forest classifier to achieve good and robust results on the popular KTH Actions dataset.



### Visual Passwords Using Automatic Lip Reading
- **Arxiv ID**: http://arxiv.org/abs/1409.0924v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1409.0924v1)
- **Published**: 2014-09-02 23:57:04+00:00
- **Updated**: 2014-09-02 23:57:04+00:00
- **Authors**: Ahmad Basheer Hassanat
- **Comment**: None
- **Journal**: International Journal of Sciences: Basic and Applied Research
  (IJSBAR) (2014) Volume 13, No 1, pp 218-231
- **Summary**: This paper presents a visual passwords system to increase security. The system depends mainly on recognizing the speaker using the visual speech signal alone. The proposed scheme works in two stages: setting the visual password stage and the verification stage. At the setting stage the visual passwords system request the user to utter a selected password, a video recording of the user face is captured, and processed by a special words-based VSR system which extracts a sequence of feature vectors. In the verification stage, the same procedure is executed, the features will be sent to be compared with the stored visual password. The proposed scheme has been evaluated using a video database of 20 different speakers (10 females and 10 males), and 15 more males in another video database with different experiment sets. The evaluation has proved the system feasibility, with average error rate in the range of 7.63% to 20.51% at the worst tested scenario, and therefore, has potential to be a practical approach with the support of other conventional authentication methods such as the use of usernames and passwords.



