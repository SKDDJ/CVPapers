# Arxiv Papers in cs.CV on 2014-09-23
### A non-linear learning & classification algorithm that achieves full training accuracy with stellar classification accuracy
- **Arxiv ID**: http://arxiv.org/abs/1409.6440v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1409.6440v2)
- **Published**: 2014-09-23 08:17:07+00:00
- **Updated**: 2014-10-31 17:46:07+00:00
- **Authors**: Rashid Khogali
- **Comment**: 43 Pages
- **Journal**: None
- **Summary**: A fast Non-linear and non-iterative learning and classification algorithm is synthesized and validated. This algorithm named the "Reverse Ripple Effect(R.R.E)", achieves 100% learning accuracy but is computationally expensive upon classification. The R.R.E is a (deterministic) algorithm that super imposes Gaussian weighted functions on training points. In this work, the R.R.E algorithm is compared against known learning and classification techniques/algorithms such as: the Perceptron Criterion algorithm, Linear Support Vector machines, the Linear Fisher Discriminant and a simple Neural Network. The classification accuracy of the R.R.E algorithm is evaluated using simulations conducted in MATLAB. The R.R.E algorithm's behaviour is analyzed under linearly and non-linearly separable data sets. For the comparison with the Neural Network, the classical XOR problem is considered.



### HSR: L1/2 Regularized Sparse Representation for Fast Face Recognition using Hierarchical Feature Selection
- **Arxiv ID**: http://arxiv.org/abs/1409.6448v1
- **DOI**: 10.1007/s00521-015-1907-y
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1409.6448v1)
- **Published**: 2014-09-23 08:36:05+00:00
- **Updated**: 2014-09-23 08:36:05+00:00
- **Authors**: Bo Han, Bo He, Tingting Sun, Mengmeng Ma, Amaury Lendasse
- **Comment**: Submitted to IEEE Computational Intelligence Magazine in 09/2014
- **Journal**: None
- **Summary**: In this paper, we propose a novel method for fast face recognition called L1/2 Regularized Sparse Representation using Hierarchical Feature Selection (HSR). By employing hierarchical feature selection, we can compress the scale and dimension of global dictionary, which directly contributes to the decrease of computational cost in sparse representation that our approach is strongly rooted in. It consists of Gabor wavelets and Extreme Learning Machine Auto-Encoder (ELM-AE) hierarchically. For Gabor wavelets part, local features can be extracted at multiple scales and orientations to form Gabor-feature based image, which in turn improves the recognition rate. Besides, in the presence of occluded face image, the scale of Gabor-feature based global dictionary can be compressed accordingly because redundancies exist in Gabor-feature based occlusion dictionary. For ELM-AE part, the dimension of Gabor-feature based global dictionary can be compressed because high-dimensional face images can be rapidly represented by low-dimensional feature. By introducing L1/2 regularization, our approach can produce sparser and more robust representation compared to regularized Sparse Representation based Classification (SRC), which also contributes to the decrease of the computational cost in sparse representation. In comparison with related work such as SRC and Gabor-feature based SRC (GSRC), experimental results on a variety of face databases demonstrate the great advantage of our method for computational cost. Moreover, we also achieve approximate or even better recognition rate.



### Unified Heat Kernel Regression for Diffusion, Kernel Smoothing and Wavelets on Manifolds and Its Application to Mandible Growth Modeling in CT Images
- **Arxiv ID**: http://arxiv.org/abs/1409.6498v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/1409.6498v2)
- **Published**: 2014-09-23 11:45:19+00:00
- **Updated**: 2015-02-17 11:21:50+00:00
- **Authors**: Moo K. Chung, Anqi Qiu, Seongho Seo, Houri K. Vorperian
- **Comment**: Accepted in Medical Image Analysis
- **Journal**: Medical Image Analysis 2015 22:63-76
- **Summary**: We present a novel kernel regression framework for smoothing scalar surface data using the Laplace-Beltrami eigenfunctions. Starting with the heat kernel constructed from the eigenfunctions, we formulate a new bivariate kernel regression framework as a weighted eigenfunction expansion with the heat kernel as the weights. The new kernel regression is mathematically equivalent to isotropic heat diffusion, kernel smoothing and recently popular diffusion wavelets. Unlike many previous partial differential equation based approaches involving diffusion, our approach represents the solution of diffusion analytically, reducing numerical inaccuracy and slow convergence. The numerical implementation is validated on a unit sphere using spherical harmonics. As an illustration, we have applied the method in characterizing the localized growth pattern of mandible surfaces obtained in CT images from subjects between ages 0 and 20 years by regressing the length of displacement vectors with respect to the template surface.



### A Concept Learning Approach to Multisensory Object Perception
- **Arxiv ID**: http://arxiv.org/abs/1409.6745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1409.6745v1)
- **Published**: 2014-09-23 20:25:46+00:00
- **Updated**: 2014-09-23 20:25:46+00:00
- **Authors**: Ifeoma Nwogu, Goker Erdogan, Ilker Yildirim, Robert Jacobs
- **Comment**: 6 pages and 6 figures
- **Journal**: None
- **Summary**: This paper presents a computational model of concept learning using Bayesian inference for a grammatically structured hypothesis space, and test the model on multisensory (visual and haptics) recognition of 3D objects. The study is performed on a set of artificially generated 3D objects known as fribbles, which are complex, multipart objects with categorical structures. The goal of this work is to develop a working multisensory representational model that integrates major themes on concepts and concepts learning from the cognitive science literature. The model combines the representational power of a probabilistic generative grammar with the inferential power of Bayesian induction.



