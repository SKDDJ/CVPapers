# Arxiv Papers in cs.CV on 2014-05-01
### Relative Facial Action Unit Detection
- **Arxiv ID**: http://arxiv.org/abs/1405.0085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1405.0085v1)
- **Published**: 2014-05-01 03:53:36+00:00
- **Updated**: 2014-05-01 03:53:36+00:00
- **Authors**: Mahmoud Khademi, Louis-Philippe Morency
- **Comment**: Accepted at IEEE Winter Conference on Applications of Computer
  Vision, Steamboat Springs Colorado, USA, 2014
- **Journal**: None
- **Summary**: This paper presents a subject-independent facial action unit (AU) detection method by introducing the concept of relative AU detection, for scenarios where the neutral face is not provided. We propose a new classification objective function which analyzes the temporal neighborhood of the current frame to decide if the expression recently increased, decreased or showed no change. This approach is a significant change from the conventional absolute method which decides about AU classification using the current frame, without an explicit comparison with its neighboring frames. Our proposed method improves robustness to individual differences such as face scale and shape, age-related wrinkles, and transitions among expressions (e.g., lower intensity of expressions). Our experiments on three publicly available datasets (Extended Cohn-Kanade (CK+), Bosphorus, and DISFA databases) show significant improvement of our approach over conventional absolute techniques. Keywords: facial action coding system (FACS); relative facial action unit detection; temporal information;



### VSCAN: An Enhanced Video Summarization using Density-based Spatial Clustering
- **Arxiv ID**: http://arxiv.org/abs/1405.0174v1
- **DOI**: 10.1007/978-3-642-41181-6_74
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1405.0174v1)
- **Published**: 2014-05-01 14:36:35+00:00
- **Updated**: 2014-05-01 14:36:35+00:00
- **Authors**: Karim M. Mohamed, Mohamed A. Ismail, Nagia M. Ghanem
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1401.3590 by
  other authors without attribution
- **Journal**: None
- **Summary**: In this paper, we present VSCAN, a novel approach for generating static video summaries. This approach is based on a modified DBSCAN clustering algorithm to summarize the video content utilizing both color and texture features of the video frames. The paper also introduces an enhanced evaluation method that depends on color and texture features. Video Summaries generated by VSCAN are compared with summaries generated by other approaches found in the literature and those created by users. Experimental results indicate that the video summaries generated by VSCAN have a higher quality than those generated by other approaches.



### Retrieval in Long Surveillance Videos using User Described Motion and Object Attributes
- **Arxiv ID**: http://arxiv.org/abs/1405.0234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1405.0234v1)
- **Published**: 2014-05-01 17:40:51+00:00
- **Updated**: 2014-05-01 17:40:51+00:00
- **Authors**: Greg Castanon, Mohamed Elgharib, Venkatesh Saligrama, Pierre-Marc Jodoin
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: We present a content-based retrieval method for long surveillance videos both for wide-area (Airborne) as well as near-field imagery (CCTV). Our goal is to retrieve video segments, with a focus on detecting objects moving on routes, that match user-defined events of interest. The sheer size and remote locations where surveillance videos are acquired, necessitates highly compressed representations that are also meaningful for supporting user-defined queries. To address these challenges we archive long-surveillance video through lightweight processing based on low-level local spatio-temporal extraction of motion and object features. These are then hashed into an inverted index using locality-sensitive hashing (LSH). This local approach allows for query flexibility as well as leads to significant gains in compression. Our second task is to extract partial matches to the user-created query and assembles them into full matches using Dynamic Programming (DP). DP exploits causality to assemble the indexed low level features into a video segment which matches the query route. We examine CCTV and Airborne footage, whose low contrast makes motion extraction more difficult. We generate robust motion estimates for Airborne data using a tracklets generation algorithm while we use Horn and Schunck approach to generate motion estimates for CCTV. Our approach handles long routes, low contrasts and occlusion. We derive bounds on the rate of false positives and demonstrate the effectiveness of the approach for counting, motion pattern recognition and abandoned object applications.



### Microsoft COCO: Common Objects in Context
- **Arxiv ID**: http://arxiv.org/abs/1405.0312v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1405.0312v3)
- **Published**: 2014-05-01 21:43:32+00:00
- **Updated**: 2015-02-21 01:48:49+00:00
- **Authors**: Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll√°r
- **Comment**: 1) updated annotation pipeline description and figures; 2) added new
  section describing datasets splits; 3) updated author list
- **Journal**: None
- **Summary**: We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.



