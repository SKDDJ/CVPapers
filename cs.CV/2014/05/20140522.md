# Arxiv Papers in cs.CV on 2014-05-22
### Self-tuned Visual Subclass Learning with Shared Samples An Incremental Approach
- **Arxiv ID**: http://arxiv.org/abs/1405.5732v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1405.5732v2)
- **Published**: 2014-05-22 12:47:15+00:00
- **Updated**: 2014-05-26 18:47:09+00:00
- **Authors**: Hossein Azizpour, Stefan Carlsson
- **Comment**: Updated ICCV 2013 submission
- **Journal**: None
- **Summary**: Computer vision tasks are traditionally defined and evaluated using semantic categories. However, it is known to the field that semantic classes do not necessarily correspond to a unique visual class (e.g. inside and outside of a car). Furthermore, many of the feasible learning techniques at hand cannot model a visual class which appears consistent to the human eye. These problems have motivated the use of 1) Unsupervised or supervised clustering as a preprocessing step to identify the visual subclasses to be used in a mixture-of-experts learning regime. 2) Felzenszwalb et al. part model and other works model mixture assignment with latent variables which is optimized during learning 3) Highly non-linear classifiers which are inherently capable of modelling multi-modal input space but are inefficient at the test time. In this work, we promote an incremental view over the recognition of semantic classes with varied appearances. We propose an optimization technique which incrementally finds maximal visual subclasses in a regularized risk minimization framework. Our proposed approach unifies the clustering and classification steps in a single algorithm. The importance of this approach is its compliance with the classification via the fact that it does not need to know about the number of clusters, the representation and similarity measures used in pre-processing clustering methods a priori. Following this approach we show both qualitatively and quantitatively significant results. We show that the visual subclasses demonstrate a long tail distribution. Finally, we show that state of the art object detection methods (e.g. DPM) are unable to use the tails of this distribution comprising 50\% of the training samples. In fact we show that DPM performance slightly increases on average by the removal of this half of the data.



### Semi-supervised Spectral Clustering for Classification
- **Arxiv ID**: http://arxiv.org/abs/1405.5737v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1405.5737v2)
- **Published**: 2014-05-22 13:05:27+00:00
- **Updated**: 2014-09-26 04:01:41+00:00
- **Authors**: Arif Mahmood, Ajmal S. Mian
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a Classification Via Clustering (CVC) algorithm which enables existing clustering methods to be efficiently employed in classification problems. In CVC, training and test data are co-clustered and class-cluster distributions are used to find the label of the test data. To determine an efficient number of clusters, a Semi-supervised Hierarchical Clustering (SHC) algorithm is proposed. Clusters are obtained by hierarchically applying two-way NCut by using signs of the Fiedler vector of the normalized graph Laplacian. To this end, a Direct Fiedler Vector Computation algorithm is proposed. The graph cut is based on the data structure and does not consider labels. Labels are used only to define the stopping criterion for graph cut. We propose clustering to be performed on the Grassmannian manifolds facilitating the formation of spectral ensembles. The proposed algorithm outperformed state-of-the-art image-set classification algorithms on five standard datasets.



### Descriptor Matching with Convolutional Neural Networks: a Comparison to SIFT
- **Arxiv ID**: http://arxiv.org/abs/1405.5769v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.6; I.4.7; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1405.5769v2)
- **Published**: 2014-05-22 14:35:52+00:00
- **Updated**: 2015-06-24 09:16:28+00:00
- **Authors**: Philipp Fischer, Alexey Dosovitskiy, Thomas Brox
- **Comment**: This paper has been merged with arXiv:1406.6909
- **Journal**: None
- **Summary**: Latest results indicate that features learned via convolutional neural networks outperform previous descriptors on classification tasks by a large margin. It has been shown that these networks still work well when they are applied to datasets or recognition tasks different from those they were trained on. However, descriptors like SIFT are not only used in recognition but also for many correspondence problems that rely on descriptor matching. In this paper we compare features from various layers of convolutional neural nets to standard SIFT descriptors. We consider a network that was trained on ImageNet and another one that was trained without supervision. Surprisingly, convolutional neural networks clearly outperform SIFT on descriptor matching. This paper has been merged with arXiv:1406.6909



