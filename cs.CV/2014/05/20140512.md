# Arxiv Papers in cs.CV on 2014-05-12
### Multi Modal Face Recognition Using Block Based Curvelet Features
- **Arxiv ID**: http://arxiv.org/abs/1405.2641v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1405.2641v2)
- **Published**: 2014-05-12 06:51:08+00:00
- **Updated**: 2014-05-21 11:30:51+00:00
- **Authors**: Jyothi K, Prabhakar C. J
- **Comment**: 17 pages, 5 Figures
- **Journal**: None
- **Summary**: In this paper, we present multimodal 2D +3D face recognition method using block based curvelet features. The 3D surface of face (Depth Map) is computed from the stereo face images using stereo vision technique. The statistical measures such as mean, standard deviation, variance and entropy are extracted from each block of curvelet subband for both depth and intensity images independently.In order to compute the decision score, the KNN classifier is employed independently for both intensity and depth map. Further, computed decision scoresof intensity and depth map are combined at decision level to improve the face recognition rate. The combination of intensity and depth map is verified experimentally using benchmark face database. The experimental results show that the proposed multimodal method is better than individual modality.



### Resource-Aware Programming for Robotic Vision
- **Arxiv ID**: http://arxiv.org/abs/1405.2908v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1405.2908v1)
- **Published**: 2014-05-12 16:40:04+00:00
- **Updated**: 2014-05-12 16:40:04+00:00
- **Authors**: Johny Paul, Walter Stechele, Manfred Kr√∂hnert, Tamim Asfour
- **Comment**: Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)
- **Journal**: None
- **Summary**: Humanoid robots are designed to operate in human centered environments. They face changing, dynamic environments in which they need to fulfill a multitude of challenging tasks. Such tasks differ in complexity, resource requirements, and execution time. Latest computer architectures of humanoid robots consist of several industrial PCs containing single- or dual-core processors. According to the SIA roadmap for semiconductors, many-core chips with hundreds to thousands of cores are expected to be available in the next decade. Utilizing the full power of a chip with huge amounts of resources requires new computing paradigms and methodologies.   In this paper, we analyze a resource-aware computing methodology named Invasive Computing, to address these challenges. The benefits and limitations of the new programming model is analyzed using two widely used computer vision algorithms, the Harris Corner detector and SIFT (Scale Invariant Feature Transform) feature matching. The result indicate that the new programming model together with the extensions within the application layer, makes them highly adaptable; leading to better quality in the results obtained.



### Cross-view Action Modeling, Learning and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1405.2941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1405.2941v1)
- **Published**: 2014-05-12 20:21:53+00:00
- **Updated**: 2014-05-12 20:21:53+00:00
- **Authors**: Jiang wang, Xiaohan Nie, Yin Xia, Ying Wu, Song-Chun Zhu
- **Comment**: CVPR 2014
- **Journal**: None
- **Summary**: Existing methods on video-based action recognition are generally view-dependent, i.e., performing recognition from the same views seen in the training data. We present a novel multiview spatio-temporal AND-OR graph (MST-AOG) representation for cross-view action recognition, i.e., the recognition is performed on the video from an unknown and unseen view. As a compositional model, MST-AOG compactly represents the hierarchical combinatorial structures of cross-view actions by explicitly modeling the geometry, appearance and motion variations. This paper proposes effective methods to learn the structure and parameters of MST-AOG. The inference based on MST-AOG enables action recognition from novel views. The training of MST-AOG takes advantage of the 3D human skeleton data obtained from Kinect cameras to avoid annotating enormous multi-view video frames, which is error-prone and time-consuming, but the recognition does not need 3D information and is based on 2D video input. A new Multiview Action3D dataset has been created and will be released. Extensive experiments have demonstrated that this new action representation significantly improves the accuracy and robustness for cross-view action recognition on 2D videos.



