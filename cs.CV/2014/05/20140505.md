# Arxiv Papers in cs.CV on 2014-05-05
### A Continuous Max-Flow Approach to Multi-Labeling Problems under Arbitrary Region Regularization
- **Arxiv ID**: http://arxiv.org/abs/1405.0892v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1405.0892v2)
- **Published**: 2014-05-05 13:19:00+00:00
- **Updated**: 2014-06-05 21:09:48+00:00
- **Authors**: John S. H. Baxter, Martin Rajchl, Jing Yuan, Terry M. Peters
- **Comment**: 10 pages, 2 figures, 3 algorithms - v2: Fixed typos / grammatical
  errors and mathematical errors in the primal/dual formulation. Extended
  methods for weighted DAGs rather than DAGs with edge multiplicity
- **Journal**: None
- **Summary**: The incorporation of region regularization into max-flow segmentation has traditionally focused on ordering and part-whole relationships. A side effect of the development of such models is that it constrained regularization only to those cases, rather than allowing for arbitrary region regularization. Directed Acyclic Graphical Max-Flow (DAGMF) segmentation overcomes these limitations by allowing for the algorithm designer to specify an arbitrary directed acyclic graph to structure a max-flow segmentation. This allows for individual 'parts' to be a member of multiple distinct 'wholes.'



### Comparing apples to apples in the evaluation of binary coding methods
- **Arxiv ID**: http://arxiv.org/abs/1405.1005v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1405.1005v2)
- **Published**: 2014-05-05 19:26:58+00:00
- **Updated**: 2014-09-27 18:35:35+00:00
- **Authors**: Mohammad Rastegari, Shobeir Fakhraei, Jonghyun Choi, David Jacobs, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: We discuss methodological issues related to the evaluation of unsupervised binary code construction methods for nearest neighbor search. These issues have been widely ignored in literature. These coding methods attempt to preserve either Euclidean distance or angular (cosine) distance in the binary embedding space. We explain why when comparing a method whose goal is preserving cosine similarity to one designed for preserving Euclidean distance, the original features should be normalized by mapping them to the unit hypersphere before learning the binary mapping functions. To compare a method whose goal is to preserves Euclidean distance to one that preserves cosine similarity, the original feature data must be mapped to a higher dimension by including a bias term in binary mapping functions. These conditions ensure the fair comparison between different binary code methods for the task of nearest neighbor search. Our experiments show under these conditions the very simple methods (e.g. LSH and ITQ) often outperform recent state-of-the-art methods (e.g. MDSH and OK-means).



