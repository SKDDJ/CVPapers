# Arxiv Papers in cs.CV on 2014-02-13
### Hand-Eye and Robot-World Calibration by Global Polynomial Optimization
- **Arxiv ID**: http://arxiv.org/abs/1402.3261v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1402.3261v1)
- **Published**: 2014-02-13 19:17:01+00:00
- **Updated**: 2014-02-13 19:17:01+00:00
- **Authors**: Jan Heller, Didier Henrion, Tomas Pajdla
- **Comment**: None
- **Journal**: None
- **Summary**: The need to relate measurements made by a camera to a different known coordinate system arises in many engineering applications. Historically, it appeared for the first time in the connection with cameras mounted on robotic systems. This problem is commonly known as hand-eye calibration. In this paper, we present several formulations of hand-eye calibration that lead to multivariate polynomial optimization problems. We show that the method of convex linear matrix inequality (LMI) relaxations can be used to effectively solve these problems and to obtain globally optimal solutions. Further, we show that the same approach can be used for the simultaneous hand-eye and robot-world calibration. Finally, we validate the proposed solutions using both synthetic and real datasets.



### Zero-bias autoencoders and the benefits of co-adapting features
- **Arxiv ID**: http://arxiv.org/abs/1402.3337v5
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1402.3337v5)
- **Published**: 2014-02-13 23:37:39+00:00
- **Updated**: 2015-04-08 14:51:11+00:00
- **Authors**: Kishore Konda, Roland Memisevic, David Krueger
- **Comment**: None
- **Journal**: None
- **Summary**: Regularized training of an autoencoder typically results in hidden unit biases that take on large negative values. We show that negative biases are a natural result of using a hidden layer whose responsibility is to both represent the input data and act as a selection mechanism that ensures sparsity of the representation. We then show that negative biases impede the learning of data distributions whose intrinsic dimensionality is high. We also propose a new activation function that decouples the two roles of the hidden layer and that allows us to learn representations on data with very high intrinsic dimensionality, where standard autoencoders typically fail. Since the decoupled activation function acts like an implicit regularizer, the model can be trained by minimizing the reconstruction error of training data, without requiring any additional regularization.



