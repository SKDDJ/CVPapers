# Arxiv Papers in cs.CV on 2014-08-11
### Physical Computing With No Clock to Implement the Gaussian Pyramid of SIFT Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1408.2289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1408.2289v1)
- **Published**: 2014-08-11 01:08:53+00:00
- **Updated**: 2014-08-11 01:08:53+00:00
- **Authors**: Yi Li, Qi Wei, Fei Qiao, Huazhong Yang
- **Comment**: 6
- **Journal**: None
- **Summary**: Physical computing is a technology utilizing the nature of electronic devices and circuit topology to cope with computing tasks. In this paper, we propose an active circuit network to implement multi-scale Gaussian filter, which is also called Gaussian Pyramid in image preprocessing. Various kinds of methods have been tried to accelerate the key stage in image feature extracting algorithm these years. Compared with existing technologies, GPU parallel computing and FPGA accelerating technology, physical computing has great advantage on processing speed as well as power consumption. We have verified that processing time to implement the Gaussian pyramid of the SIFT algorithm stands on nanosecond level through the physical computing technology, while other existing methods all need at least hundreds of millisecond. With an estimate on the stray capacitance of the circuit, the power consumption is around 670pJ to filter a 256x256 image. To the best of our knowledge, this is the most fast processing technology to accelerate the SIFT algorithm, and it is also a rather energy-efficient method, thanks to the proposed physical computing technology.



### Bags of Affine Subspaces for Robust Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1408.2313v3
- **DOI**: 10.1109/DICTA.2015.7371239
- **Categories**: **cs.CV**, cs.MM, cs.RO, 14M15, 54B05, I.2.10; I.4.8; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1408.2313v3)
- **Published**: 2014-08-11 05:13:15+00:00
- **Updated**: 2016-02-05 07:35:54+00:00
- **Authors**: Sareh Shirazi, Conrad Sanderson, Chris McCool, Mehrtash T. Harandi
- **Comment**: in International Conference on Digital Image Computing: Techniques
  and Applications, 2015
- **Journal**: None
- **Summary**: We propose an adaptive tracking algorithm where the object is modelled as a continuously updated bag of affine subspaces, with each subspace constructed from the object's appearance over several consecutive frames. In contrast to linear subspaces, affine subspaces explicitly model the origin of subspaces. Furthermore, instead of using a brittle point-to-subspace distance during the search for the object in a new frame, we propose to use a subspace-to-subspace distance by representing candidate image areas also as affine subspaces. Distances between subspaces are then obtained by exploiting the non-Euclidean geometry of Grassmann manifolds. Experiments on challenging videos (containing object occlusions, deformations, as well as variations in pose and illumination) indicate that the proposed method achieves higher tracking accuracy than several recent discriminative trackers.



### Video Face Editing Using Temporal-Spatial-Smooth Warping
- **Arxiv ID**: http://arxiv.org/abs/1408.2380v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1408.2380v1)
- **Published**: 2014-08-11 11:48:46+00:00
- **Updated**: 2014-08-11 11:48:46+00:00
- **Authors**: Xiaoyan Li, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Editing faces in videos is a popular yet challenging aspect of computer vision and graphics, which encompasses several applications including facial attractiveness enhancement, makeup transfer, face replacement, and expression manipulation. Simply applying image-based warping algorithms to video-based face editing produces temporal incoherence in the synthesized videos because it is impossible to consistently localize facial features in two frames representing two different faces in two different videos (or even two consecutive frames representing the same face in one video). Therefore, high performance face editing usually requires significant manual manipulation. In this paper we propose a novel temporal-spatial-smooth warping (TSSW) algorithm to effectively exploit the temporal information in two consecutive frames, as well as the spatial smoothness within each frame. TSSW precisely estimates two control lattices in the horizontal and vertical directions respectively from the corresponding control lattices in the previous frame, by minimizing a novel energy function that unifies a data-driven term, a smoothness term, and feature point constraints. Corresponding warping surfaces then precisely map source frames to the target frames. Experimental testing on facial attractiveness enhancement, makeup transfer, face replacement, and expression manipulation demonstrates that the proposed approaches can effectively preserve spatial smoothness and temporal coherence in editing facial geometry, skin detail, identity, and expression, which outperform the existing face editing methods. In particular, TSSW is robust to subtly inaccurate localization of feature points and is a vast improvement over image-based warping methods.



### Hierarchical Saliency Detection on Extended CSSD
- **Arxiv ID**: http://arxiv.org/abs/1408.5418v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1408.5418v2)
- **Published**: 2014-08-11 15:18:47+00:00
- **Updated**: 2015-08-04 07:49:43+00:00
- **Authors**: Jianping Shi, Qiong Yan, Li Xu, Jiaya Jia
- **Comment**: 14 pages, 15 figures
- **Journal**: None
- **Summary**: Complex structures commonly exist in natural images. When an image contains small-scale high-contrast patterns either in the background or foreground, saliency detection could be adversely affected, resulting erroneous and non-uniform saliency assignment. The issue forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. Different from varying patch sizes or downsizing images, we measure region-based scales. The final saliency values are inferred optimally combining all the saliency cues in different scales using hierarchical inference. Through our inference model, single-scale information is selected to obtain a saliency map. Our method improves detection quality on many images that cannot be handled well traditionally. We also construct an extended Complex Scene Saliency Dataset (ECSSD) to include complex but general natural images.



### Learning to see like children: proof of concept
- **Arxiv ID**: http://arxiv.org/abs/1408.2478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1408.2478v1)
- **Published**: 2014-08-11 17:38:19+00:00
- **Updated**: 2014-08-11 17:38:19+00:00
- **Authors**: Marco Gori, Marco Lippi, Marco Maggini, Stefano Melacci
- **Comment**: None
- **Journal**: None
- **Summary**: In the last few years we have seen a growing interest in machine learning approaches to computer vision and, especially, to semantic labeling. Nowadays state of the art systems use deep learning on millions of labeled images with very successful results on benchmarks, though it is unlikely to expect similar results in unrestricted visual environments. Most learning schemes essentially ignore the inherent sequential structure of videos: this might be a critical issue, since any visual recognition process is remarkably more complex when shuffling video frames. Based on this remark, we propose a re-foundation of the communication protocol between visual agents and the environment, which is referred to as learning to see like children. Like for human interaction, visual concepts are acquired by the agents solely by processing their own visual stream along with human supervisions on selected pixels. We give a proof of concept that remarkable semantic labeling can emerge within this protocol by using only a few supervised examples. This is made possible by exploiting a constraint of motion coherent labeling that virtually offers tons of supervisions. Additional visual constraints, including those associated with object supervisions, are used within the context of learning from constraints. The framework is extended in the direction of lifelong learning, so as our visual agents live in their own visual environment without distinguishing learning and test set. Learning takes place in deep architectures under a progressive developmental scheme. In order to evaluate our Developmental Visual Agents (DVAs), in addition to classic benchmarks, we open the doors of our lab, allowing people to evaluate DVAs by crowd-sourcing. Such assessment mechanism might result in a paradigm shift in methodologies and algorithms for computer vision, encouraging truly novel solutions within the proposed framework.



### Homotopy equivalence of finite digital images
- **Arxiv ID**: http://arxiv.org/abs/1408.2584v2
- **DOI**: 10.1007/s10851-015-0578-8
- **Categories**: **math.GN**, cs.CG, cs.CV, 55P10, 68R10, I.4.m
- **Links**: [PDF](http://arxiv.org/pdf/1408.2584v2)
- **Published**: 2014-08-11 23:57:39+00:00
- **Updated**: 2014-10-13 17:14:00+00:00
- **Authors**: Jason Haarmann, Meg P. Murphy, Casey S. Peters, P. Christopher Staecker
- **Comment**: major fixes and removal of errors, terminology changes
- **Journal**: Journal of Mathematical Imaging and Vision 53, Issue 3, p 288-302,
  2015
- **Summary**: For digital images, there is an established homotopy equivalence relation which parallels that of classical topology. Many classical homotopy equivalence invariants, such as the Euler characteristic and the homology groups, do not remain invariants in the digital setting. This paper develops a numerical digital homotopy invariant and begins to catalog all possible connected digital images on a small number of points, up to homotopy equivalence.



