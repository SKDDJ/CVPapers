# Arxiv Papers in cs.CV on 2014-08-30
### Sparse Coding on Symmetric Positive Definite Manifolds using Bregman Divergences
- **Arxiv ID**: http://arxiv.org/abs/1409.0083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1409.0083v1)
- **Published**: 2014-08-30 04:46:43+00:00
- **Updated**: 2014-08-30 04:46:43+00:00
- **Authors**: Mehrtash Harandi, Richard Hartley, Brian Lovell, Conrad Sanderson
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces sparse coding and dictionary learning for Symmetric Positive Definite (SPD) matrices, which are often used in machine learning, computer vision and related areas. Unlike traditional sparse coding schemes that work in vector spaces, in this paper we discuss how SPD matrices can be described by sparse combination of dictionary atoms, where the atoms are also SPD matrices. We propose to seek sparse coding by embedding the space of SPD matrices into Hilbert spaces through two types of Bregman matrix divergences. This not only leads to an efficient way of performing sparse coding, but also an online and iterative scheme for dictionary learning. We apply the proposed methods to several computer vision tasks where images are represented by region covariance matrices. Our proposed algorithms outperform state-of-the-art methods on a wide range of classification tasks, including face recognition, action recognition, material classification and texture categorization.



### Kernel Coding: General Formulation and Special Cases
- **Arxiv ID**: http://arxiv.org/abs/1409.0084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1409.0084v1)
- **Published**: 2014-08-30 05:01:15+00:00
- **Updated**: 2014-08-30 05:01:15+00:00
- **Authors**: Mehrtash Harandi, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: Representing images by compact codes has proven beneficial for many visual recognition tasks. Most existing techniques, however, perform this coding step directly in image feature space, where the distributions of the different classes are typically entangled. In contrast, here, we study the problem of performing coding in a high-dimensional Hilbert space, where the classes are expected to be more easily separable. To this end, we introduce a general coding formulation that englobes the most popular techniques, such as bag of words, sparse coding and locality-based coding, and show how this formulation and its special cases can be kernelized. Importantly, we address several aspects of learning in our general formulation, such as kernel learning, dictionary learning and supervised kernel coding. Our experimental evaluation on several visual recognition tasks demonstrates the benefits of performing coding in Hilbert space, and in particular of jointly learning the kernel, the dictionary and the classifier.



