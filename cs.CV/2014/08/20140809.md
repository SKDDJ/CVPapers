# Arxiv Papers in cs.CV on 2014-08-09
### Automatic Removal of Marginal Annotations in Printed Text Document
- **Arxiv ID**: http://arxiv.org/abs/1408.2015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1408.2015v1)
- **Published**: 2014-08-09 03:56:16+00:00
- **Updated**: 2014-08-09 03:56:16+00:00
- **Authors**: Abdessamad Elboushaki, Rachida Hannane, P. Nagabhushan, Mohammed Javed
- **Comment**: Original Article Published by Elsevier at ERCICA-2014, Pages 123-131,
  August 2014
- **Journal**: Proceedings of Second International Conference on Emerging
  Research in Computing, Information,Communication and Applications
  (ERCICA-14), pages 123-131, August 2014, Bangalore
- **Summary**: Recovering the original printed texts from a document with added handwritten annotations in the marginal area is one of the challenging problems, especially when the original document is not available. Therefore, this paper aims at salvaging automatically the original document from the annotated document by detecting and removing any handwritten annotations that appear in the marginal area of the document without any loss of information. Here a two stage algorithm is proposed, where in the first stage due to approximate marginal boundary detection with horizontal and vertical projection profiles, all of the marginal annotations along with some part of the original printed text that may appear very close to the marginal boundary are removed. Therefore as a second stage, using the connected components, a strategy is applied to bring back the printed text components cropped during the first stage. The proposed method is validated using a dataset of 50 documents having complex handwritten annotations, which gives an overall accuracy of 89.01% in removing the marginal annotations and 97.74% in case of retrieving the original printed text document.



### Video In Sentences Out
- **Arxiv ID**: http://arxiv.org/abs/1408.6418v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1408.6418v1)
- **Published**: 2014-08-09 05:43:12+00:00
- **Updated**: 2014-08-09 05:43:12+00:00
- **Authors**: Andrei Barbu, Alexander Bridge, Zachary Burchill, Dan Coroian, Sven Dickinson, Sanja Fidler, Aaron Michaux, Sam Mussman, Siddharth Narayanaswamy, Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark Siskind, Jarrell Waggoner, Song Wang, Jinlian Wei, Yifan Yin, Zhiqi Zhang
- **Comment**: Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)
- **Journal**: None
- **Summary**: We present a system that produces sentential descriptions of video: who did what to whom, and where and how they did it. Action class is rendered as a verb, participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases, spatial relations between those participants as prepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. Extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks, the trackto-role assignments, and changing body posture.



