# Arxiv Papers in cs.CV on 2014-01-04
### Context-Aware Hypergraph Construction for Robust Spectral Clustering
- **Arxiv ID**: http://arxiv.org/abs/1401.0764v1
- **DOI**: 10.1109/TKDE.2013.126
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1401.0764v1)
- **Published**: 2014-01-04 02:05:35+00:00
- **Updated**: 2014-01-04 02:05:35+00:00
- **Authors**: Xi Li, Weiming Hu, Chunhua Shen, Anthony Dick, Zhongfei Zhang
- **Comment**: 10 pages. Appearing in IEEE TRANSACTIONS ON KNOWLEDGE AND DATA
  ENGINEERING: http://doi.ieeecomputersociety.org/10.1109/TKDE.2013.126
- **Journal**: None
- **Summary**: Spectral clustering is a powerful tool for unsupervised data analysis. In this paper, we propose a context-aware hypergraph similarity measure (CAHSM), which leads to robust spectral clustering in the case of noisy data. We construct three types of hypergraph---the pairwise hypergraph, the k-nearest-neighbor (kNN) hypergraph, and the high-order over-clustering hypergraph. The pairwise hypergraph captures the pairwise similarity of data points; the kNN hypergraph captures the neighborhood of each point; and the clustering hypergraph encodes high-order contexts within the dataset. By combining the affinity information from these three hypergraphs, the CAHSM algorithm is able to explore the intrinsic topological information of the dataset. Therefore, data clustering using CAHSM tends to be more robust. Considering the intra-cluster compactness and the inter-cluster separability of vertices, we further design a discriminative hypergraph partitioning criterion (DHPC). Using both CAHSM and DHPC, a robust spectral clustering algorithm is developed. Theoretical analysis and experimental evaluation demonstrate the effectiveness and robustness of the proposed algorithm.



### From Kernel Machines to Ensemble Learning
- **Arxiv ID**: http://arxiv.org/abs/1401.0767v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1401.0767v1)
- **Published**: 2014-01-04 02:28:48+00:00
- **Updated**: 2014-01-04 02:28:48+00:00
- **Authors**: Chunhua Shen, Fayao Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Ensemble methods such as boosting combine multiple learners to obtain better prediction than could be obtained from any individual learner. Here we propose a principled framework for directly constructing ensemble learning methods from kernel methods. Unlike previous studies showing the equivalence between boosting and support vector machines (SVMs), which needs a translation procedure, we show that it is possible to design boosting-like procedure to solve the SVM optimization problems.   In other words, it is possible to design ensemble methods directly from SVM without any middle procedure.   This finding not only enables us to design new ensemble learning methods directly from kernel methods, but also makes it possible to take advantage of those highly-optimized fast linear SVM solvers for ensemble learning.   We exemplify this framework for designing binary ensemble learning as well as a new multi-class ensemble learning methods.   Experimental results demonstrate the flexibility and usefulness of the proposed framework.



