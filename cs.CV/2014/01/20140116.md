# Arxiv Papers in cs.CV on 2014-01-16
### Structured Priors for Sparse-Representation-Based Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1401.3818v1
- **DOI**: 10.1109/LGRS.2013.2290531
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1401.3818v1)
- **Published**: 2014-01-16 03:21:26+00:00
- **Updated**: 2014-01-16 03:21:26+00:00
- **Authors**: Xiaoxia Sun, Qing Qu, Nasser M. Nasrabadi, Trac D. Tran
- **Comment**: IEEE Geoscience and Remote Sensing Letter
- **Journal**: None
- **Summary**: Pixel-wise classification, where each pixel is assigned to a predefined class, is one of the most important procedures in hyperspectral image (HSI) analysis. By representing a test pixel as a linear combination of a small subset of labeled pixels, a sparse representation classifier (SRC) gives rather plausible results compared with that of traditional classifiers such as the support vector machine (SVM). Recently, by incorporating additional structured sparsity priors, the second generation SRCs have appeared in the literature and are reported to further improve the performance of HSI. These priors are based on exploiting the spatial dependencies between the neighboring pixels, the inherent structure of the dictionary, or both. In this paper, we review and compare several structured priors for sparse-representation-based HSI classification. We also propose a new structured prior called the low rank group prior, which can be considered as a modification of the low rank prior. Furthermore, we will investigate how different structured priors improve the result for the HSI classification.



### An Empirical Evaluation of Similarity Measures for Time Series Classification
- **Arxiv ID**: http://arxiv.org/abs/1401.3973v1
- **DOI**: 10.1016/j.knosys.2014.04.035
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1401.3973v1)
- **Published**: 2014-01-16 10:21:44+00:00
- **Updated**: 2014-01-16 10:21:44+00:00
- **Authors**: Joan Serrà, Josep Lluis Arcos
- **Comment**: 28 pages, 5 figures, 3 tables
- **Journal**: Knowledge-Based Systems 67: 305-314, 2014
- **Summary**: Time series are ubiquitous, and a measure to assess their similarity is a core part of many computational systems. In particular, the similarity measure is the most essential ingredient of time series clustering and classification systems. Because of this importance, countless approaches to estimate time series similarity have been proposed. However, there is a lack of comparative studies using empirical, rigorous, quantitative, and large-scale assessment strategies. In this article, we provide an extensive evaluation of similarity measures for time series classification following the aforementioned principles. We consider 7 different measures coming from alternative measure `families', and 45 publicly-available time series data sets coming from a wide variety of scientific domains. We focus on out-of-sample classification accuracy, but in-sample accuracies and parameter choices are also discussed. Our work is based on rigorous evaluation methodologies and includes the use of powerful statistical significance tests to derive meaningful conclusions. The obtained results show the equivalence, in terms of accuracy, of a number of measures, but with one single candidate outperforming the rest. Such findings, together with the followed methodology, invite researchers on the field to adopt a more consistent evaluation criteria and a more informed decision regarding the baseline measures to which new developments should be compared.



### Learning $\ell_1$-based analysis and synthesis sparsity priors using bi-level optimization
- **Arxiv ID**: http://arxiv.org/abs/1401.4105v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1401.4105v1)
- **Published**: 2014-01-16 17:49:45+00:00
- **Updated**: 2014-01-16 17:49:45+00:00
- **Authors**: Yunjin Chen, Thomas Pock, Horst Bischof
- **Comment**: 5 pages, 1 figure, appear at the Workshop on Analysis Operator
  Learning vs. Dictionary Learning, NIPS 2012
- **Journal**: None
- **Summary**: We consider the analysis operator and synthesis dictionary learning problems based on the the $\ell_1$ regularized sparse representation model. We reveal the internal relations between the $\ell_1$-based analysis model and synthesis model. We then introduce an approach to learn both analysis operator and synthesis dictionary simultaneously by using a unified framework of bi-level optimization. Our aim is to learn a meaningful operator (dictionary) such that the minimum energy solution of the analysis (synthesis)-prior based model is as close as possible to the ground-truth. We solve the bi-level optimization problem using the implicit differentiation technique. Moreover, we demonstrate the effectiveness of our leaning approach by applying the learned analysis operator (dictionary) to the image denoising task and comparing its performance with state-of-the-art methods. Under this unified framework, we can compare the performance of the two types of priors.



### Revisiting loss-specific training of filter-based MRFs for image restoration
- **Arxiv ID**: http://arxiv.org/abs/1401.4107v1
- **DOI**: 10.1007/978-3-642-40602-7_30
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1401.4107v1)
- **Published**: 2014-01-16 17:58:21+00:00
- **Updated**: 2014-01-16 17:58:21+00:00
- **Authors**: Yunjin Chen, Thomas Pock, René Ranftl, Horst Bischof
- **Comment**: 10 pages, 2 figures, appear at 35th German Conference, GCPR 2013,
  Saarbr\"ucken, Germany, September 3-6, 2013. Proceedings
- **Journal**: None
- **Summary**: It is now well known that Markov random fields (MRFs) are particularly effective for modeling image priors in low-level vision. Recent years have seen the emergence of two main approaches for learning the parameters in MRFs: (1) probabilistic learning using sampling-based algorithms and (2) loss-specific training based on MAP estimate. After investigating existing training approaches, it turns out that the performance of the loss-specific training has been significantly underestimated in existing work. In this paper, we revisit this approach and use techniques from bi-level optimization to solve it. We show that we can get a substantial gain in the final performance by solving the lower-level problem in the bi-level framework with high accuracy using our newly proposed algorithm. As a result, our trained model is on par with highly specialized image denoising algorithms and clearly outperforms probabilistically trained MRF models. Our findings suggest that for the loss-specific training scheme, solving the lower-level problem with higher accuracy is beneficial. Our trained model comes along with the additional advantage, that inference is extremely efficient. Our GPU-based implementation takes less than 1s to produce state-of-the-art performance.



### A bi-level view of inpainting - based image compression
- **Arxiv ID**: http://arxiv.org/abs/1401.4112v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1401.4112v2)
- **Published**: 2014-01-16 18:10:56+00:00
- **Updated**: 2014-05-09 16:24:26+00:00
- **Authors**: Yunjin Chen, René Ranftl, Thomas Pock
- **Comment**: 8 pages, 4 figures, best paper award of CVWW 2014, Computer Vision
  Winter Workshop, K\v{r}tiny, Czech Republic, 3-5th February 2014
- **Journal**: None
- **Summary**: Inpainting based image compression approaches, especially linear and non-linear diffusion models, are an active research topic for lossy image compression. The major challenge in these compression models is to find a small set of descriptive supporting points, which allow for an accurate reconstruction of the original image. It turns out in practice that this is a challenging problem even for the simplest Laplacian interpolation model. In this paper, we revisit the Laplacian interpolation compression model and introduce two fast algorithms, namely successive preconditioning primal dual algorithm and the recently proposed iPiano algorithm, to solve this problem efficiently. Furthermore, we extend the Laplacian interpolation based compression model to a more general form, which is based on principles from bi-level optimization. We investigate two different variants of the Laplacian model, namely biharmonic interpolation and smoothed Total Variation regularization. Our numerical results show that significant improvements can be obtained from the biharmonic interpolation model, and it can recover an image with very high quality from only 5% pixels.



