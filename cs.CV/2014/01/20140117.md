# Arxiv Papers in cs.CV on 2014-01-17
### Distortion-driven Turbulence Effect Removal using Variational Model
- **Arxiv ID**: http://arxiv.org/abs/1401.4221v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1401.4221v1)
- **Published**: 2014-01-17 01:40:52+00:00
- **Updated**: 2014-01-17 01:40:52+00:00
- **Authors**: Yuan Xie, Wensheng Zhang, Dacheng Tao, Wenrui Hu, Yanyun Qu, Hanzi Wang
- **Comment**: 28 pages, 15 figures
- **Journal**: None
- **Summary**: It remains a challenge to simultaneously remove geometric distortion and space-time-varying blur in frames captured through a turbulent atmospheric medium. To solve, or at least reduce these effects, we propose a new scheme to recover a latent image from observed frames by integrating a new variational model and distortion-driven spatial-temporal kernel regression. The proposed scheme first constructs a high-quality reference image from the observed frames using low-rank decomposition. Then, to generate an improved registered sequence, the reference image is iteratively optimized using a variational model containing a new spatial-temporal regularization. The proposed fast algorithm efficiently solves this model without the use of partial differential equations (PDEs). Next, to reduce blur variation, distortion-driven spatial-temporal kernel regression is carried out to fuse the registered sequence into one image by introducing the concept of the near-stationary patch. Applying a blind deconvolution algorithm to the fused image produces the final output. Extensive experimental testing shows, both qualitatively and quantitatively, that the proposed method can effectively alleviate distortion and blur and recover details of the original scene compared to state-of-the-art methods.



### An Analysis of Random Projections in Cancelable Biometrics
- **Arxiv ID**: http://arxiv.org/abs/1401.4489v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1401.4489v3)
- **Published**: 2014-01-17 23:21:56+00:00
- **Updated**: 2014-11-14 02:38:09+00:00
- **Authors**: Devansh Arpit, Ifeoma Nwogu, Gaurav Srivastava, Venu Govindaraju
- **Comment**: None
- **Journal**: None
- **Summary**: With increasing concerns about security, the need for highly secure physical biometrics-based authentication systems utilizing \emph{cancelable biometric} technologies is on the rise. Because the problem of cancelable template generation deals with the trade-off between template security and matching performance, many state-of-the-art algorithms successful in generating high quality cancelable biometrics all have random projection as one of their early processing steps. This paper therefore presents a formal analysis of why random projections is an essential step in cancelable biometrics. By formally defining the notion of an \textit{Independent Subspace Structure} for datasets, it can be shown that random projection preserves the subspace structure of data vectors generated from a union of independent linear subspaces. The bound on the minimum number of random vectors required for this to hold is also derived and is shown to depend logarithmically on the number of data samples, not only in independent subspaces but in disjoint subspace settings as well. The theoretical analysis presented is supported in detail with empirical results on real-world face recognition datasets.



