# Arxiv Papers in cs.CV on 2021-10-28
### Characterizing and Taming Resolution in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.14819v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.14819v1)
- **Published**: 2021-10-28 00:08:23+00:00
- **Updated**: 2021-10-28 00:08:23+00:00
- **Authors**: Eddie Yan, Liang Luo, Luis Ceze
- **Comment**: None
- **Journal**: None
- **Summary**: Image resolution has a significant effect on the accuracy and computational, storage, and bandwidth costs of computer vision model inference. These costs are exacerbated when scaling out models to large inference serving systems and make image resolution an attractive target for optimization. However, the choice of resolution inherently introduces additional tightly coupled choices, such as image crop size, image detail, and compute kernel implementation that impact computational, storage, and bandwidth costs. Further complicating this setting, the optimal choices from the perspective of these metrics are highly dependent on the dataset and problem scenario. We characterize this tradeoff space, quantitatively studying the accuracy and efficiency tradeoff via systematic and automated tuning of image resolution, image quality and convolutional neural network operators. With the insights from this study, we propose a dynamic resolution mechanism that removes the need to statically choose a resolution ahead of time.



### ODMTCNet: An Interpretable Multi-view Deep Neural Network Architecture for Image Feature Representation
- **Arxiv ID**: http://arxiv.org/abs/2110.14830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14830v1)
- **Published**: 2021-10-28 00:35:32+00:00
- **Updated**: 2021-10-28 00:35:32+00:00
- **Authors**: Lei Gao, Zheng Guo, Ling Guan
- **Comment**: Submitted to IEEE TPAMI
- **Journal**: None
- **Summary**: This work proposes an interpretable multi-view deep neural network architecture, namely optimal discriminant multi-view tensor convolutional network (ODMTCNet), by integrating statistical machine learning (SML) principles with the deep neural network (DNN) architecture.



### Audio-visual Representation Learning for Anomaly Events Detection in Crowds
- **Arxiv ID**: http://arxiv.org/abs/2110.14862v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14862v1)
- **Published**: 2021-10-28 02:42:48+00:00
- **Updated**: 2021-10-28 02:42:48+00:00
- **Authors**: Junyu Gao, Maoguo Gong, Xuelong Li
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: In recent years, anomaly events detection in crowd scenes attracts many researchers' attention, because of its importance to public safety. Existing methods usually exploit visual information to analyze whether any abnormal events have occurred due to only visual sensors are generally equipped in public places. However, when an abnormal event in crowds occurs, sound information may be discriminative to assist the crowd analysis system to determine whether there is an abnormality. Compare with vision information that is easily occluded, audio signals have a certain degree of penetration. Thus, this paper attempt to exploit multi-modal learning for modeling the audio and visual signals simultaneously. To be specific, we design a two-branch network to model different types of information. The first is a typical 3D CNN model to extract temporal appearance features from video clips. The second is an audio CNN for encoding Log Mel-Spectrogram of audio signals. Finally, by fusing the above features, a more accurate prediction will be produced. We conduct the experiments on SHADE dataset, a synthetic audio-visual dataset in surveillance scenes, and find introducing audio signals effectively improves the performance of anomaly events detection and outperforms other state-of-the-art methods. Furthermore, we will release the code and the pre-trained models as soon as possible.



### Generalized Depthwise-Separable Convolutions for Adversarially Robust and Efficient Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.14871v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14871v2)
- **Published**: 2021-10-28 03:51:24+00:00
- **Updated**: 2021-11-06 17:24:14+00:00
- **Authors**: Hassan Dbouk, Naresh R. Shanbhag
- **Comment**: NeurIPS 2021 (Spotlight)
- **Journal**: None
- **Summary**: Despite their tremendous successes, convolutional neural networks (CNNs) incur high computational/storage costs and are vulnerable to adversarial perturbations. Recent works on robust model compression address these challenges by combining model compression techniques with adversarial training. But these methods are unable to improve throughput (frames-per-second) on real-life hardware while simultaneously preserving robustness to adversarial perturbations. To overcome this problem, we propose the method of Generalized Depthwise-Separable (GDWS) convolution -- an efficient, universal, post-training approximation of a standard 2D convolution. GDWS dramatically improves the throughput of a standard pre-trained network on real-life hardware while preserving its robustness. Lastly, GDWS is scalable to large problem sizes since it operates on pre-trained models and doesn't require any additional training. We establish the optimality of GDWS as a 2D convolution approximator and present exact algorithms for constructing optimal GDWS convolutions under complexity and error constraints. We demonstrate the effectiveness of GDWS via extensive experiments on CIFAR-10, SVHN, and ImageNet datasets. Our code can be found at https://github.com/hsndbk4/GDWS.



### Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training
- **Arxiv ID**: http://arxiv.org/abs/2110.14883v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2110.14883v2)
- **Published**: 2021-10-28 04:45:55+00:00
- **Updated**: 2022-09-20 12:54:20+00:00
- **Authors**: Shenggui Li, Jiarui Fang, Zhengda Bian, Hongxin Liu, Yuliang Liu, Haichen Huang, Boxiang Wang, Yang You
- **Comment**: None
- **Journal**: None
- **Summary**: The success of Transformer models has pushed the deep learning model scale to billions of parameters. Due to the limited memory resource of a single GPU, However, the best practice for choosing the optimal parallel strategy is still lacking, since it requires domain expertise in both deep learning and parallel computing.   The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism, as well as heterogeneous training methods integrated with zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.



### Degraded Reference Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2110.14899v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14899v1)
- **Published**: 2021-10-28 05:50:59+00:00
- **Updated**: 2021-10-28 05:50:59+00:00
- **Authors**: Shahrukh Athar, Zhou Wang
- **Comment**: 15 pages, 11 figures, 9 tables
- **Journal**: None
- **Summary**: In practical media distribution systems, visual content usually undergoes multiple stages of quality degradation along the delivery chain, but the pristine source content is rarely available at most quality monitoring points along the chain to serve as a reference for quality assessment. As a result, full-reference (FR) and reduced-reference (RR) image quality assessment (IQA) methods are generally infeasible. Although no-reference (NR) methods are readily applicable, their performance is often not reliable. On the other hand, intermediate references of degraded quality are often available, e.g., at the input of video transcoders, but how to make the best use of them in proper ways has not been deeply investigated. Here we make one of the first attempts to establish a new paradigm named degraded-reference IQA (DR IQA). Specifically, we lay out the architectures of DR IQA and introduce a 6-bit code to denote the choices of configurations. We construct the first large-scale databases dedicated to DR IQA and will make them publicly available. We make novel observations on distortion behavior in multi-stage distortion pipelines by comprehensively analyzing five multiple distortion combinations. Based on these observations, we develop novel DR IQA models and make extensive comparisons with a series of baseline models derived from top-performing FR and NR models. The results suggest that DR IQA may offer significant performance improvement in multiple distortion environments, thereby establishing DR IQA as a valid IQA paradigm that is worth further exploration.



### E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches
- **Arxiv ID**: http://arxiv.org/abs/2110.14908v2
- **DOI**: 10.1109/TVCG.2021.3114789
- **Categories**: **cs.HC**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.14908v2)
- **Published**: 2021-10-28 06:14:27+00:00
- **Updated**: 2021-10-29 04:03:41+00:00
- **Authors**: Kevin Maher, Zeyuan Huang, Jiancheng Song, Xiaoming Deng, Yu-Kun Lai, Cuixia Ma, Hao Wang, Yong-Jin Liu, Hongan Wang
- **Comment**: IEEE Transactions of Visualization and Computer Graphics (TVCG, Proc.
  VIS 2021), to appear
- **Journal**: None
- **Summary**: What makes speeches effective has long been a subject for debate, and until today there is broad controversy among public speaking experts about what factors make a speech effective as well as the roles of these factors in speeches. Moreover, there is a lack of quantitative analysis methods to help understand effective speaking strategies. In this paper, we propose E-ffective, a visual analytic system allowing speaking experts and novices to analyze both the role of speech factors and their contribution in effective speeches. From interviews with domain experts and investigating existing literature, we identified important factors to consider in inspirational speeches. We obtained the generated factors from multi-modal data that were then related to effectiveness data. Our system supports rapid understanding of critical factors in inspirational speeches, including the influence of emotions by means of novel visualization methods and interaction. Two novel visualizations include E-spiral (that shows the emotional shifts in speeches in a visually compact way) and E-script (that connects speech content with key speech delivery information). In our evaluation we studied the influence of our system on experts' domain knowledge about speech factors. We further studied the usability of the system by speaking novices and experts on assisting analysis of inspirational speech effectiveness.



### 3D Object Tracking with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2110.14921v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14921v1)
- **Published**: 2021-10-28 07:03:19+00:00
- **Updated**: 2021-10-28 07:03:19+00:00
- **Authors**: Yubo Cui, Zheng Fang, Jiayao Shan, Zuoxu Gu, Sifan Zhou
- **Comment**: BMVC2021
- **Journal**: None
- **Summary**: Feature fusion and similarity computation are two core problems in 3D object tracking, especially for object tracking using sparse and disordered point clouds. Feature fusion could make similarity computing more efficient by including target object information. However, most existing LiDAR-based approaches directly use the extracted point cloud feature to compute similarity while ignoring the attention changes of object regions during tracking. In this paper, we propose a feature fusion network based on transformer architecture. Benefiting from the self-attention mechanism, the transformer encoder captures the inter- and intra- relations among different regions of the point cloud. By using cross-attention, the transformer decoder fuses features and includes more target cues into the current point cloud feature to compute the region attentions, which makes the similarity computing more efficient. Based on this feature fusion network, we propose an end-to-end point cloud object tracking framework, a simple yet effective method for 3D object tracking using point clouds. Comprehensive experimental results on the KITTI dataset show that our method achieves new state-of-the-art performance. Code is available at: https://github.com/3bobo/lttr.



### Counterfactual Explanation of Brain Activity Classifiers using Image-to-Image Transfer by Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2110.14927v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.14927v1)
- **Published**: 2021-10-28 07:21:12+00:00
- **Updated**: 2021-10-28 07:21:12+00:00
- **Authors**: Teppei Matsui, Masato Taki, Trung Quang Pham, Junichi Chikazoe, Koji Jimura
- **Comment**: 28 pages, 6 figures, 3 tables, 2 supplementary figures, 1
  supplementary table
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) can accurately decode task-related information from brain activations. However, because of the nonlinearity of the DNN, the decisions made by DNNs are hardly interpretable. One of the promising approaches for explaining such a black-box system is counterfactual explanation. In this framework, the behavior of a black-box system is explained by comparing real data and realistic synthetic data that are specifically generated such that the black-box system outputs an unreal outcome. Here we introduce a novel generative DNN (counterfactual activation generator, CAG) that can provide counterfactual explanations for DNN-based classifiers of brain activations. Importantly, CAG can simultaneously handle image transformation among multiple classes associated with different behavioral tasks. Using CAG, we demonstrated counterfactual explanation of DNN-based classifiers that learned to discriminate brain activations of seven behavioral tasks. Furthermore, by iterative applications of CAG, we were able to enhance and extract subtle spatial brain activity patterns that affected the classifier's decisions. Together, these results demonstrate that the counterfactual explanation based on image-to-image transformation would be a promising approach to understand and extend the current application of DNNs in fMRI analyses.



### A recursive robust filtering approach for 3D registration
- **Arxiv ID**: http://arxiv.org/abs/2110.14932v1
- **DOI**: 10.1007/s11760-015-0823-z
- **Categories**: **cs.CV**, cs.RO, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2110.14932v1)
- **Published**: 2021-10-28 07:50:02+00:00
- **Updated**: 2021-10-28 07:50:02+00:00
- **Authors**: Abdenour Amamra, Nabil Aouf, Dowling Stuart, Mark Richardson
- **Comment**: Accepted in the journal of Signal Image and Video Processing
- **Journal**: Signal, Image and Video Processing, 10(5), pp.835-842 (2016)
- **Summary**: This work presents a new recursive robust filtering approach for feature-based 3D registration. Unlike the common state-of-the-art alignment algorithms, the proposed method has four advantages that have not yet occurred altogether in any previous solution. For instance, it is able to deal with inherent noise contaminating sensory data; it is robust to uncertainties caused by noisy feature localisation; it also combines the advantages of both (Formula presented.) and (Formula presented.) norms for a higher performance and a more prospective prevention of local minima. The result is an accurate and stable rigid body transformation. The latter enables a thorough control over the convergence regarding the alignment as well as a correct assessment of the quality of registration. The mathematical rationale behind the proposed approach is explained, and the results are validated on physical and synthetic data.



### GPU based GMM segmentation of kinect data
- **Arxiv ID**: http://arxiv.org/abs/2110.14934v1
- **DOI**: 10.1109/ELMAR.2014.6923325
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.14934v1)
- **Published**: 2021-10-28 08:05:04+00:00
- **Updated**: 2021-10-28 08:05:04+00:00
- **Authors**: Abdenour Amamra, Tarek Mouats, Nabil Aouf
- **Comment**: Accepted in ELMAR-2014 conference
- **Journal**: In Proceedings ELMAR-2014 (pp. 1-4). IEEE (2014)
- **Summary**: This paper presents a novel approach for background/foreground segmentation of RGBD data with the Gaussian Mixture Models (GMM). We first start by the background subtraction from the colour and depth images separately. The foregrounds resulting from both streams are then fused for a more accurate detection. Our segmentation solution is implemented on the GPU. Thus, it works at the full frame rate of the sensor (30fps). Test results show its robustness against illumination change, shadows and reflections.



### FocusFace: Multi-task Contrastive Learning for Masked Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.14940v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.14940v2)
- **Published**: 2021-10-28 08:17:12+00:00
- **Updated**: 2021-11-01 18:45:48+00:00
- **Authors**: Pedro C. Neto, Fadi Boutros, João Ribeiro Pinto, Naser Damer, Ana F. Sequeira, Jaime S. Cardoso
- **Comment**: Accepted at the 16th IEEE International Conference on Automatic Face
  and Gesture Recognition, FG 2021
- **Journal**: None
- **Summary**: SARS-CoV-2 has presented direct and indirect challenges to the scientific community. One of the most prominent indirect challenges advents from the mandatory use of face masks in a large number of countries. Face recognition methods struggle to perform identity verification with similar accuracy on masked and unmasked individuals. It has been shown that the performance of these methods drops considerably in the presence of face masks, especially if the reference image is unmasked. We propose FocusFace, a multi-task architecture that uses contrastive learning to be able to accurately perform masked face recognition. The proposed architecture is designed to be trained from scratch or to work on top of state-of-the-art face recognition methods without sacrificing the capabilities of a existing models in conventional face recognition tasks. We also explore different approaches to design the contrastive learning module. Results are presented in terms of masked-masked (M-M) and unmasked-masked (U-M) face verification performance. For both settings, the results are on par with published methods, but for M-M specifically, the proposed method was able to outperform all the solutions that it was compared to. We further show that when using our method on top of already existing methods the training computational costs decrease significantly while retaining similar performances. The implementation and the trained models are available at GitHub.



### Dispensed Transformer Network for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2110.14944v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.14944v1)
- **Published**: 2021-10-28 08:27:44+00:00
- **Updated**: 2021-10-28 08:27:44+00:00
- **Authors**: Yunxiang Li, Jingxiong Li, Ruilong Dan, Shuai Wang, Kai Jin, Guodong Zeng, Jun Wang, Xiangji Pan, Qianni Zhang, Huiyu Zhou, Qun Jin, Li Wang, Yaqi Wang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Accurate segmentation is a crucial step in medical image analysis and applying supervised machine learning to segment the organs or lesions has been substantiated effective. However, it is costly to perform data annotation that provides ground truth labels for training the supervised algorithms, and the high variance of data that comes from different domains tends to severely degrade system performance over cross-site or cross-modality datasets. To mitigate this problem, a novel unsupervised domain adaptation (UDA) method named dispensed Transformer network (DTNet) is introduced in this paper. Our novel DTNet contains three modules. First, a dispensed residual transformer block is designed, which realizes global attention by dispensed interleaving operation and deals with the excessive computational cost and GPU memory usage of the Transformer. Second, a multi-scale consistency regularization is proposed to alleviate the loss of details in the low-resolution output for better feature alignment. Finally, a feature ranking discriminator is introduced to automatically assign different weights to domain-gap features to lessen the feature distribution distance, reducing the performance shift of two domains. The proposed method is evaluated on large fluorescein angiography (FA) retinal nonperfusion (RNP) cross-site dataset with 676 images and a wide used cross-modality dataset from the MM-WHS challenge. Extensive results demonstrate that our proposed network achieves the best performance in comparison with several state-of-the-art techniques.



### Towards Large-Scale Rendering of Simulated Crops for Synthetic Ground Truth Generation on Modular Supercomputers
- **Arxiv ID**: http://arxiv.org/abs/2110.14946v1
- **DOI**: None
- **Categories**: **cs.CV**, I.3; I.4; I.6
- **Links**: [PDF](http://arxiv.org/pdf/2110.14946v1)
- **Published**: 2021-10-28 08:32:38+00:00
- **Updated**: 2021-10-28 08:32:38+00:00
- **Authors**: Dirk Norbert Helmrich, Jens Henrik Göbbert, Mona Giraud, Hanno Scharr, Andrea Schnepf, Morris Riedel
- **Comment**: Accepted Poster for the 11th IEEE Symposium on Large Data Analysis
  and Visualization
- **Journal**: None
- **Summary**: Computer Vision problems deal with the semantic extraction of information from camera images. Especially for field crop images, the underlying problems are hard to label and even harder to learn, and the availability of high-quality training data is low. Deep neural networks do a good job of extracting the necessary models from training examples. However, they rely on an abundance of training data that is not feasible to generate or label by expert annotation. To address this challenge, we make use of the Unreal Engine to render large and complex virtual scenes. We rely on the performance of individual nodes by distributing plant simulations across nodes and both generate scenes as well as train neural networks on GPUs, restricting node communication to parallel learning.



### Multi-Attribute Balanced Sampling for Disentangled GAN Controls
- **Arxiv ID**: http://arxiv.org/abs/2111.00909v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.00909v2)
- **Published**: 2021-10-28 08:44:13+00:00
- **Updated**: 2022-01-27 07:42:51+00:00
- **Authors**: Perla Doubinsky, Nicolas Audebert, Michel Crucianu, Hervé Le Borgne
- **Comment**: None
- **Journal**: None
- **Summary**: Various controls over the generated data can be extracted from the latent space of a pre-trained GAN, as it implicitly encodes the semantics of the training data. The discovered controls allow to vary semantic attributes in the generated images but usually lead to entangled edits that affect multiple attributes at the same time. Supervised approaches typically sample and annotate a collection of latent codes, then train classifiers in the latent space to identify the controls. Since the data generated by GANs reflects the biases of the original dataset, so do the resulting semantic controls. We propose to address disentanglement by subsampling the generated data to remove over-represented co-occuring attributes thus balancing the semantics of the dataset before training the classifiers. We demonstrate the effectiveness of this approach by extracting disentangled linear directions for face manipulation on two popular GAN architectures, PGGAN and StyleGAN, and two datasets, CelebAHQ and FFHQ. We show that this approach outperforms state-of-the-art classifier-based methods while avoiding the need for disentanglement-enforcing post-processing.



### DocScanner: Robust Document Image Rectification with Progressive Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.14968v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14968v3)
- **Published**: 2021-10-28 09:15:02+00:00
- **Updated**: 2022-12-24 01:39:48+00:00
- **Authors**: Hao Feng, Wengang Zhou, Jiajun Deng, Qi Tian, Houqiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Compared with flatbed scanners, portable smartphones provide more convenience for physical document digitization. However, such digitized documents are often distorted due to uncontrolled physical deformations, camera positions, and illumination variations. To this end, we present DocScanner, a novel framework for document image rectification. Different from existing solutions, DocScanner addresses this issue by introducing a progressive learning mechanism. Specifically, DocScanner maintains a single estimate of the rectified image, which is progressively corrected with a recurrent architecture. The iterative refinements make DocScanner converge to a robust and superior rectification performance, while the lightweight recurrent architecture ensures the running efficiency. To further improve the rectification quality, based on the geometric priori between the distorted and the rectified images, a geometric regularization is introduced during training to further improve the performance. Extensive experiments are conducted on the Doc3D dataset and the DocUNet Benchmark dataset, and the quantitative and qualitative evaluation results verify the effectiveness of DocScanner, which outperforms previous methods on OCR accuracy, image similarity, and our proposed distortion metric by a considerable margin. Furthermore, our DocScanner shows superior efficiency in runtime latency and model size.



### Real-time multiview data fusion for object tracking with RGBD sensors
- **Arxiv ID**: http://arxiv.org/abs/2110.15815v1
- **DOI**: 10.1017/S026357471400263X
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.15815v1)
- **Published**: 2021-10-28 09:24:49+00:00
- **Updated**: 2021-10-28 09:24:49+00:00
- **Authors**: Abdenour Amamra, Nabil Aouf
- **Comment**: Accepted in Robotica
- **Journal**: Robotica, 34(8), pp.1855-1879 (2016)
- **Summary**: This paper presents a new approach to accurately track a moving vehicle with a multiview setup of red-green-blue depth (RGBD) cameras. We first propose a correction method to eliminate a shift, which occurs in depth sensors when they become worn. This issue could not be otherwise corrected with the ordinary calibration procedure. Next, we present a sensor-wise filtering system to correct for an unknown vehicle motion. A data fusion algorithm is then used to optimally merge the sensor-wise estimated trajectories. We implement most parts of our solution in the graphic processor. Hence, the whole system is able to operate at up to 25 frames per second with a configuration of five cameras. Test results show the accuracy we achieved and the robustness of our solution to overcome uncertainties in the measurements and the modelling.



### A GIS Data Realistic Road Generation Approach for Traffic Simulation
- **Arxiv ID**: http://arxiv.org/abs/2110.15814v1
- **DOI**: 10.15439/2019F223
- **Categories**: **cs.CV**, cs.CG, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.15814v1)
- **Published**: 2021-10-28 09:50:10+00:00
- **Updated**: 2021-10-28 09:50:10+00:00
- **Authors**: Yacine Amara, Abdenour Amamra, Yasmine Daheur, Lamia Saichi
- **Comment**: Accepted in Federated Conference on Computer Science and Information
  Systems (FedCSIS)
- **Journal**: In 2019 Federated Conference on Computer Science and Information
  Systems (FedCSIS) (pp. 385-390). IEEE (2019)
- **Summary**: Road networks exist in the form of polylines with attributes within the GIS databases. Such a representation renders the geographic data impracticable for 3D road traffic simulation. In this work, we propose a method to transform raw GIS data into a realistic, operational model for real-time road traffic simulation. For instance, the proposed raw to simulation ready data transformation is achieved through several curvature estimation, interpolation/approximation, and clustering schemes. The obtained results show the performance of our approach and prove its adequacy to real traffic simulation scenario as can be seen in this video 1 .



### Skeleton-Based Mutually Assisted Interacted Object Localization and Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.14994v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14994v2)
- **Published**: 2021-10-28 10:09:34+00:00
- **Updated**: 2022-05-10 07:34:14+00:00
- **Authors**: Liang Xu, Cuiling Lan, Wenjun Zeng, Cewu Lu
- **Comment**: Accepted to the IEEE Transactions on Multimedia 2022
- **Journal**: None
- **Summary**: Skeleton data carries valuable motion information and is widely explored in human action recognition. However, not only the motion information but also the interaction with the environment provides discriminative cues to recognize the action of persons. In this paper, we propose a joint learning framework for mutually assisted "interacted object localization" and "human action recognition" based on skeleton data. The two tasks are serialized together and collaborate to promote each other, where preliminary action type derived from skeleton alone helps improve interacted object localization, which in turn provides valuable cues for the final human action recognition. Besides, we explore the temporal consistency of interacted object as constraint to better localize the interacted object with the absence of ground-truth labels. Extensive experiments on the datasets of SYSU-3D, NTU60 RGB+D, Northwestern-UCLA and UAV-Human show that our method achieves the best or competitive performance with the state-of-the-art methods for human action recognition. Visualization results show that our method can also provide reasonable interacted object localization results.



### Smart Fashion: A Review of AI Applications in the Fashion & Apparel Industry
- **Arxiv ID**: http://arxiv.org/abs/2111.00905v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, A.1; I.2
- **Links**: [PDF](http://arxiv.org/pdf/2111.00905v2)
- **Published**: 2021-10-28 10:51:34+00:00
- **Updated**: 2021-11-02 10:09:23+00:00
- **Authors**: Seyed Omid Mohammadi, Ahmad Kalhor
- **Comment**: 99 Pages, 79 Figures, 24 Tables, Full length manuscript
- **Journal**: None
- **Summary**: The fashion industry is on the verge of an unprecedented change. The implementation of machine learning, computer vision, and artificial intelligence (AI) in fashion applications is opening lots of new opportunities for this industry. This paper provides a comprehensive survey on this matter, categorizing more than 580 related articles into 22 well-defined fashion-related tasks. Such structured task-based multi-label classification of fashion research articles provides researchers with explicit research directions and facilitates their access to the related studies, improving the visibility of studies simultaneously. For each task, a time chart is provided to analyze the progress through the years. Furthermore, we provide a list of 86 public fashion datasets accompanied by a list of suggested applications and additional information for each.



### Sliding Sequential CVAE with Time Variant Socially-aware Rethinking for Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2110.15016v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.15016v1)
- **Published**: 2021-10-28 10:56:21+00:00
- **Updated**: 2021-10-28 10:56:21+00:00
- **Authors**: Hao Zhou, Dongchun Ren, Xu Yang, Mingyu Fan, Hai Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrian trajectory prediction is a key technology in many applications such as video surveillance, social robot navigation, and autonomous driving, and significant progress has been made in this research topic. However, there remain two limitations of previous studies. First, with the continuation of time, the prediction error at each time step increases significantly, causing the final displacement error to be impossible to ignore. Second, the prediction results of multiple pedestrians might be impractical in the prediction horizon, i.e., the predicted trajectories might collide with each other. To overcome these limitations, this work proposes a novel trajectory prediction method called CSR, which consists of a cascaded conditional variational autoencoder (CVAE) module and a socially-aware regression module. The cascaded CVAE module first estimates the future trajectories in a sequential pattern. Specifically, each CVAE concatenates the past trajectories and the predicted points so far as the input and predicts the location at the following time step. Then, the socially-aware regression module generates offsets from the estimated future trajectories to produce the socially compliant final predictions, which are more reasonable and accurate results than the estimated trajectories. Moreover, considering the large model parameters of the cascaded CVAE module, a slide CVAE module is further exploited to improve the model efficiency using one shared CVAE, in a slidable manner. Experiments results demonstrate that the proposed method exhibits improvements over state-of-the-art method on the Stanford Drone Dataset (SDD) and ETH/UCY of approximately 38.0% and 22.2%, respectively.



### Bridging Non Co-occurrence with Unlabeled In-the-wild Data for Incremental Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.15017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15017v1)
- **Published**: 2021-10-28 10:57:25+00:00
- **Updated**: 2021-10-28 10:57:25+00:00
- **Authors**: Na Dong, Yongqiang Zhang, Mingli Ding, Gim Hee Lee
- **Comment**: Accepted paper at NeurIPS 2021
- **Journal**: None
- **Summary**: Deep networks have shown remarkable results in the task of object detection. However, their performance suffers critical drops when they are subsequently trained on novel classes without any sample from the base classes originally used to train the model. This phenomenon is known as catastrophic forgetting. Recently, several incremental learning methods are proposed to mitigate catastrophic forgetting for object detection. Despite the effectiveness, these methods require co-occurrence of the unlabeled base classes in the training data of the novel classes. This requirement is impractical in many real-world settings since the base classes do not necessarily co-occur with the novel classes. In view of this limitation, we consider a more practical setting of complete absence of co-occurrence of the base and novel classes for the object detection task. We propose the use of unlabeled in-the-wild data to bridge the non co-occurrence caused by the missing base classes during the training of additional novel classes. To this end, we introduce a blind sampling strategy based on the responses of the base-class model and pre-trained novel-class model to select a smaller relevant dataset from the large in-the-wild dataset for incremental learning. We then design a dual-teacher distillation framework to transfer the knowledge distilled from the base- and novel-class teacher models to the student model using the sampled in-the-wild data. Experimental results on the PASCAL VOC and MS COCO datasets show that our proposed method significantly outperforms other state-of-the-art class-incremental object detection methods when there is no co-occurrence between the base and novel classes during training.



### Deformable Registration of Brain MR Images via a Hybrid Loss
- **Arxiv ID**: http://arxiv.org/abs/2110.15027v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15027v2)
- **Published**: 2021-10-28 11:22:39+00:00
- **Updated**: 2021-12-19 13:39:09+00:00
- **Authors**: Luyi Han, Haoran Dou, Yunzhi Huang, Pew-Thian Yap
- **Comment**: Ranked fifth on the brain T1w deformable registration task organized
  by the MICCAI 2021 Learn2Reg challenge
- **Journal**: None
- **Summary**: Unsupervised learning strategy is widely adopted by the deformable registration models due to the lack of ground truth of deformation fields. These models typically depend on the intensity-based similarity loss to obtain the learning convergence. Despite the success, such dependence is insufficient. For the deformable registration of mono-modality image, well-aligned two images not only have indistinguishable intensity differences, but also are close in the statistical distribution and the boundary areas. Considering that well-designed loss functions can facilitate a learning model into a desirable convergence, we learn a deformable registration model for T1-weighted MR images by integrating multiple image characteristics via a hybrid loss. Our method registers the OASIS dataset with high accuracy while preserving deformation smoothness.



### Facial Emotion Recognition: A multi-task approach using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2110.15028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15028v1)
- **Published**: 2021-10-28 11:23:00+00:00
- **Updated**: 2021-10-28 11:23:00+00:00
- **Authors**: Aakash Saroop, Pathik Ghugare, Sashank Mathamsetty, Vaibhav Vasani
- **Comment**: None
- **Journal**: None
- **Summary**: Facial Emotion Recognition is an inherently difficult problem, due to vast differences in facial structures of individuals and ambiguity in the emotion displayed by a person. Recently, a lot of work is being done in the field of Facial Emotion Recognition, and the performance of the CNNs for this task has been inferior compared to the results achieved by CNNs in other fields like Object detection, Facial recognition etc. In this paper, we propose a multi-task learning algorithm, in which a single CNN detects gender, age and race of the subject along with their emotion. We validate this proposed methodology using two datasets containing real-world images. The results show that this approach is significantly better than the current State of the art algorithms for this task.



### Explicitly Modeling the Discriminability for Instance-Aware Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2110.15030v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15030v1)
- **Published**: 2021-10-28 11:24:01+00:00
- **Updated**: 2021-10-28 11:24:01+00:00
- **Authors**: Mengmeng Wang, Xiaoqian Yang, Yong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Visual object tracking performance has been dramatically improved in recent years, but some severe challenges remain open, like distractors and occlusions. We suspect the reason is that the feature representations of the tracking targets are only expressively learned but not fully discriminatively modeled. In this paper, we propose a novel Instance-Aware Tracker (IAT) to explicitly excavate the discriminability of feature representations, which improves the classical visual tracking pipeline with an instance-level classifier. First, we introduce a contrastive learning mechanism to formulate the classification task, ensuring that every training sample could be uniquely modeled and be highly distinguishable from plenty of other samples. Besides, we design an effective negative sample selection scheme to contain various intra and inter classes in the instance classification branch. Furthermore, we implement two variants of the proposed IAT, including a video-level one and an object-level one. They realize the concept of \textbf{instance} in different granularity as videos and target bounding boxes, respectively. The former enhances the ability to recognize the target from the background while the latter boosts the discriminative power for mitigating the target-distractor dilemma. Extensive experimental evaluations on 8 benchmark datasets show that both two versions of the proposed IAT achieve leading results against state-of-the-art methods while running at 30FPS. Code will be available when it is published.



### Learning Deep Representation with Energy-Based Self-Expressiveness for Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/2110.15037v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15037v2)
- **Published**: 2021-10-28 11:51:08+00:00
- **Updated**: 2022-05-24 13:43:05+00:00
- **Authors**: Yanming Li, Changsheng Li, Shiye Wang, Ye Yuan, Guoren Wang
- **Comment**: There are some errors in Table 1
- **Journal**: None
- **Summary**: Deep subspace clustering has attracted increasing attention in recent years. Almost all the existing works are required to load the whole training data into one batch for learning the self-expressive coefficients in the framework of deep learning. Although these methods achieve promising results, such a learning fashion severely prevents from the usage of deeper neural network architectures (e.g., ResNet), leading to the limited representation abilities of the models. In this paper, we propose a new deep subspace clustering framework, motivated by the energy-based models. In contrast to previous approaches taking the weights of a fully connected layer as the self-expressive coefficients, we propose to learn an energy-based network to obtain the self-expressive coefficients by mini-batch training. By this means, it is no longer necessary to load all data into one batch for learning, and it thus becomes a reality that we can utilize deeper neural network models for subspace clustering. Considering the powerful representation ability of the recently popular self-supervised learning, we attempt to leverage self-supervised representation learning to learn the dictionary. Finally, we propose a joint framework to learn both the self-expressive coefficients and dictionary simultaneously, and train the model in an end-to-end manner. The experiments are performed on three publicly available datasets, and extensive experimental results demonstrate our method can significantly outperform the other related approaches. For instance, on the three datasets, our method can averagely achieve $13.8\%$, $15.4\%$, $20.8\%$ improvements in terms of Accuracy, NMI, and ARI over SENet which is proposed very recently and obtains the second best results in the experiments.



### LF-YOLO: A Lighter and Faster YOLO for Weld Defect Detection of X-ray Image
- **Arxiv ID**: http://arxiv.org/abs/2110.15045v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15045v2)
- **Published**: 2021-10-28 12:19:32+00:00
- **Updated**: 2021-11-18 03:04:50+00:00
- **Authors**: Moyun Liu, Youping Chen, Lei He, Yang Zhang, Jingming Xie
- **Comment**: None
- **Journal**: None
- **Summary**: X-ray image plays an important role in manufacturing industry for quality assurance, because it can reflect the internal condition of weld region. However, the shape and scale of different defect types vary greatly, which makes it challenging for model to detect weld defects. In this paper, we propose a weld defect detection method based on convolution neural network, namely Lighter and Faster YOLO (LF-YOLO). In particularly, a reinforced multiscale feature (RMF) module is designed to implement both parameter-based and parameter-free multi-scale information extracting operation. RMF enables the extracted feature map capable to represent more plentiful information, which is achieved by superior hierarchical fusion structure. To improve the performance of detection network, we propose an efficient feature extraction (EFE) module. EFE processes input data with extremely low consumption, and improves the practicability of whole network in actual industry. Experimental results show that our weld defect detection network achieves satisfactory balance between performance and consumption, and reaches 92.9 mean average precision mAP50 with 61.5 frames per second (FPS). To further prove the ability of our method, we test it on public dataset MS COCO, and the results show that our LF-YOLO has a outstanding versatility detection performance. The code is available at https://github.com/lmomoy/LF-YOLO.



### Meta Guided Metric Learner for Overcoming Class Confusion in Few-Shot Road Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.15074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15074v1)
- **Published**: 2021-10-28 12:51:08+00:00
- **Updated**: 2021-10-28 12:51:08+00:00
- **Authors**: Anay Majee, Anbumani Subramanian, Kshitij Agrawal
- **Comment**: Accepted to NeurIPS 2021 Workshop on Machine Learning For Autonomous
  Driving, 12 pages, 6 figures
- **Journal**: None
- **Summary**: Localization and recognition of less-occurring road objects have been a challenge in autonomous driving applications due to the scarcity of data samples. Few-Shot Object Detection techniques extend the knowledge from existing base object classes to learn novel road objects given few training examples. Popular techniques in FSOD adopt either meta or metric learning techniques which are prone to class confusion and base class forgetting. In this work, we introduce a novel Meta Guided Metric Learner (MGML) to overcome class confusion in FSOD. We re-weight the features of the novel classes higher than the base classes through a novel Squeeze and Excite module and encourage the learning of truly discriminative class-specific features by applying an Orthogonality Constraint to the meta learner. Our method outperforms State-of-the-Art (SoTA) approaches in FSOD on the India Driving Dataset (IDD) by upto 11 mAP points while suffering from the least class confusion of 20% given only 10 examples of each novel road object. We further show similar improvements on the few-shot splits of PASCAL VOC dataset where we outperform SoTA approaches by upto 5.8 mAP accross all splits.



### SpineOne: A One-Stage Detection Framework for Degenerative Discs and Vertebrae
- **Arxiv ID**: http://arxiv.org/abs/2110.15082v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15082v2)
- **Published**: 2021-10-28 12:59:06+00:00
- **Updated**: 2021-11-10 09:04:12+00:00
- **Authors**: Jiabo He, Wei Liu, Yu Wang, Xingjun Ma, Xian-Sheng Hua
- **Comment**: None
- **Journal**: None
- **Summary**: Spinal degeneration plagues many elders, office workers, and even the younger generations. Effective pharmic or surgical interventions can help relieve degenerative spine conditions. However, the traditional diagnosis procedure is often too laborious. Clinical experts need to detect discs and vertebrae from spinal magnetic resonance imaging (MRI) or computed tomography (CT) images as a preliminary step to perform pathological diagnosis or preoperative evaluation. Machine learning systems have been developed to aid this procedure generally following a two-stage methodology: first perform anatomical localization, then pathological classification. Towards more efficient and accurate diagnosis, we propose a one-stage detection framework termed SpineOne to simultaneously localize and classify degenerative discs and vertebrae from MRI slices. SpineOne is built upon the following three key techniques: 1) a new design of the keypoint heatmap to facilitate simultaneous keypoint localization and classification; 2) the use of attention modules to better differentiate the representations between discs and vertebrae; and 3) a novel gradient-guided objective association mechanism to associate multiple learning objectives at the later training stage. Empirical results on the Spinal Disease Intelligent Diagnosis Tianchi Competition (SDID-TC) dataset of 550 exams demonstrate that our approach surpasses existing methods by a large margin.



### Contrast and Mix: Temporal Contrastive Video Domain Adaptation with Background Mixing
- **Arxiv ID**: http://arxiv.org/abs/2110.15128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15128v1)
- **Published**: 2021-10-28 14:03:29+00:00
- **Updated**: 2021-10-28 14:03:29+00:00
- **Authors**: Aadarsh Sahoo, Rutav Shah, Rameswar Panda, Kate Saenko, Abir Das
- **Comment**: Accepted to NeurIPS 2021. Project page:
  https://cvir.github.io/projects/comix
- **Journal**: None
- **Summary**: Unsupervised domain adaptation which aims to adapt models trained on a labeled source domain to a completely unlabeled target domain has attracted much attention in recent years. While many domain adaptation techniques have been proposed for images, the problem of unsupervised domain adaptation in videos remains largely underexplored. In this paper, we introduce Contrast and Mix (CoMix), a new contrastive learning framework that aims to learn discriminative invariant feature representations for unsupervised video domain adaptation. First, unlike existing methods that rely on adversarial learning for feature alignment, we utilize temporal contrastive learning to bridge the domain gap by maximizing the similarity between encoded representations of an unlabeled video at two different speeds as well as minimizing the similarity between different videos played at different speeds. Second, we propose a novel extension to the temporal contrastive loss by using background mixing that allows additional positives per anchor, thus adapting contrastive learning to leverage action semantics shared across both domains. Moreover, we also integrate a supervised contrastive learning objective using target pseudo-labels to enhance discriminability of the latent space for video domain adaptation. Extensive experiments on several benchmark datasets demonstrate the superiority of our proposed approach over state-of-the-art methods. Project page: https://cvir.github.io/projects/comix



### Blending Anti-Aliasing into Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2110.15156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15156v1)
- **Published**: 2021-10-28 14:30:02+00:00
- **Updated**: 2021-10-28 14:30:02+00:00
- **Authors**: Shengju Qian, Hao Shao, Yi Zhu, Mu Li, Jiaya Jia
- **Comment**: Accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: The transformer architectures, based on self-attention mechanism and convolution-free design, recently found superior performance and booming applications in computer vision. However, the discontinuous patch-wise tokenization process implicitly introduces jagged artifacts into attention maps, arising the traditional problem of aliasing for vision transformers. Aliasing effect occurs when discrete patterns are used to produce high frequency or continuous information, resulting in the indistinguishable distortions. Recent researches have found that modern convolution networks still suffer from this phenomenon. In this work, we analyze the uncharted problem of aliasing in vision transformer and explore to incorporate anti-aliasing properties. Specifically, we propose a plug-and-play Aliasing-Reduction Module(ARM) to alleviate the aforementioned issue. We investigate the effectiveness and generalization of the proposed method across multiple tasks and various vision transformer families. This lightweight design consistently attains a clear boost over several famous structures. Furthermore, our module also improves data efficiency and robustness of vision transformers.



### Authentication Attacks on Projection-based Cancelable Biometric Schemes
- **Arxiv ID**: http://arxiv.org/abs/2110.15163v6
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15163v6)
- **Published**: 2021-10-28 14:39:35+00:00
- **Updated**: 2022-07-18 12:23:51+00:00
- **Authors**: Axel Durbet, Pascal Lafourcade, Denis Migdal, Kevin Thiry-Atighehchi, Paul-Marie Grollemund
- **Comment**: arXiv admin note: text overlap with arXiv:1910.01389 by other authors
- **Journal**: None
- **Summary**: Cancelable biometric schemes aim at generating secure biometric templates by combining user specific tokens, such as password, stored secret or salt, along with biometric data. This type of transformation is constructed as a composition of a biometric transformation with a feature extraction algorithm. The security requirements of cancelable biometric schemes concern the irreversibility, unlinkability and revocability of templates, without losing in accuracy of comparison. While several schemes were recently attacked regarding these requirements, full reversibility of such a composition in order to produce colliding biometric characteristics, and specifically presentation attacks, were never demonstrated to the best of our knowledge. In this paper, we formalize these attacks for a traditional cancelable scheme with the help of integer linear programming (ILP) and quadratically constrained quadratic programming (QCQP). Solving these optimization problems allows an adversary to slightly alter its fingerprint image in order to impersonate any individual. Moreover, in an even more severe scenario, it is possible to simultaneously impersonate several individuals.



### Privacy Aware Person Detection in Surveillance Data
- **Arxiv ID**: http://arxiv.org/abs/2110.15171v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.15171v1)
- **Published**: 2021-10-28 14:49:21+00:00
- **Updated**: 2021-10-28 14:49:21+00:00
- **Authors**: Sander De Coninck, Sam Leroux, Pieter Simoens
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd management relies on inspection of surveillance video either by operators or by object detection models. These models are large, making it difficult to deploy them on resource constrained edge hardware. Instead, the computations are often offloaded to a (third party) cloud platform. While crowd management may be a legitimate application, transferring video from the camera to remote infrastructure may open the door for extracting additional information that are infringements of privacy, like person tracking or face recognition. In this paper, we use adversarial training to obtain a lightweight obfuscator that transforms video frames to only retain the necessary information for person detection. Importantly, the obfuscated data can be processed by publicly available object detectors without retraining and without significant loss of accuracy.



### A Comparative Study of Coarse to Dense 3D Indoor Scene Registration Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2110.15179v1
- **DOI**: 10.1109/ICAEE47123.2019.9014836
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.15179v1)
- **Published**: 2021-10-28 14:56:27+00:00
- **Updated**: 2021-10-28 14:56:27+00:00
- **Authors**: Abdenour Amamra, Khalid Boumaza
- **Comment**: Accepted in International Conference on Advanced Electrical
  Engineering (ICAEE)
- **Journal**: In 2019 International Conference on Advanced Electrical
  Engineering (ICAEE) (pp. 1-6). IEEE (2019)
- **Summary**: 3D alignment has become a very important part of 3D scanning technology. For instance, we can divide the alignment process into four steps: key point detection, key point description, initial pose estimation, and alignment refinement. Researchers have contributed several approaches to the literature for each step, which suggests a natural need for a comparative study for an educated more appropriate choice. In this work, we propose a description and an evaluation of the different methods used for 3D registration with special focus on RGB-D data to find the best combinations that permit a complete and more accurate 3D reconstruction of indoor scenes with cheap depth cameras.



### The magnitude vector of images
- **Arxiv ID**: http://arxiv.org/abs/2110.15188v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.AT
- **Links**: [PDF](http://arxiv.org/pdf/2110.15188v2)
- **Published**: 2021-10-28 15:05:08+00:00
- **Updated**: 2022-10-07 16:07:33+00:00
- **Authors**: Michael F. Adamer, Edward De Brouwer, Leslie O'Bray, Bastian Rieck
- **Comment**: None
- **Journal**: None
- **Summary**: The magnitude of a finite metric space has recently emerged as a novel invariant quantity, allowing to measure the effective size of a metric space. Despite encouraging first results demonstrating the descriptive abilities of the magnitude, such as being able to detect the boundary of a metric space, the potential use cases of magnitude remain under-explored. In this work, we investigate the properties of the magnitude on images, an important data modality in many machine learning applications. By endowing each individual images with its own metric space, we are able to define the concept of magnitude on images and analyse the individual contribution of each pixel with the magnitude vector. In particular, we theoretically show that the previously known properties of boundary detection translate to edge detection abilities in images. Furthermore, we demonstrate practical use cases of magnitude for machine learning applications and propose a novel magnitude model that consists of a computationally efficient magnitude computation and a learnable metric. By doing so, we address the computational hurdle that used to make magnitude impractical for many applications and open the way for the adoption of magnitude in machine learning research.



### Exploring Covariate and Concept Shift for Detection and Calibration of Out-of-Distribution Data
- **Arxiv ID**: http://arxiv.org/abs/2110.15231v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15231v2)
- **Published**: 2021-10-28 15:42:55+00:00
- **Updated**: 2021-11-21 20:35:07+00:00
- **Authors**: Junjiao Tian, Yen-Change Hsu, Yilin Shen, Hongxia Jin, Zsolt Kira
- **Comment**: A short version of the paper is accepted to NeurIPS DistShift
  Workshop 2021
- **Journal**: None
- **Summary**: Moving beyond testing on in-distribution data works on Out-of-Distribution (OOD) detection have recently increased in popularity. A recent attempt to categorize OOD data introduces the concept of near and far OOD detection. Specifically, prior works define characteristics of OOD data in terms of detection difficulty. We propose to characterize the spectrum of OOD data using two types of distribution shifts: covariate shift and concept shift, where covariate shift corresponds to change in style, e.g., noise, and concept shift indicates a change in semantics. This characterization reveals that sensitivity to each type of shift is important to the detection and confidence calibration of OOD data. Consequently, we investigate score functions that capture sensitivity to each type of dataset shift and methods that improve them. To this end, we theoretically derive two score functions for OOD detection, the covariate shift score and concept shift score, based on the decomposition of KL-divergence for both scores, and propose a geometrically-inspired method (Geometric ODIN) to improve OOD detection under both shifts with only in-distribution data. Additionally, the proposed method naturally leads to an expressive post-hoc calibration function which yields state-of-the-art calibration performance on both in-distribution and out-of-distribution data. We are the first to propose a method that works well across both OOD detection and calibration and under different types of shifts. View project page at https://sites.google.com/view/geometric-decomposition.



### Guided Evolution for Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2110.15232v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.15232v1)
- **Published**: 2021-10-28 15:43:20+00:00
- **Updated**: 2021-10-28 15:43:20+00:00
- **Authors**: Vasco Lopes, Miguel Santos, Bruno Degardin, Luís A. Alexandre
- **Comment**: Paper accepted at 35th Conference on Neural Information Processing
  Systems (NeurIPS) - New In ML. 9 pages, 2 figures, 1 table
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) methods have been successfully applied to image tasks with excellent results. However, NAS methods are often complex and tend to converge to local minima as soon as generated architectures seem to yield good results. In this paper, we propose G-EA, a novel approach for guided evolutionary NAS. The rationale behind G-EA, is to explore the search space by generating and evaluating several architectures in each generation at initialization stage using a zero-proxy estimator, where only the highest-scoring network is trained and kept for the next generation. This evaluation at initialization stage allows continuous extraction of knowledge from the search space without increasing computation, thus allowing the search to be efficiently guided. Moreover, G-EA forces exploitation of the most performant networks by descendant generation while at the same time forcing exploration by parent mutation and by favouring younger architectures to the detriment of older ones. Experimental results demonstrate the effectiveness of the proposed method, showing that G-EA achieves state-of-the-art results in NAS-Bench-201 search space in CIFAR-10, CIFAR-100 and ImageNet16-120, with mean accuracies of 93.98%, 72.12% and 45.94% respectively.



### Subpixel object segmentation using wavelets and multi resolution analysis
- **Arxiv ID**: http://arxiv.org/abs/2110.15233v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15233v1)
- **Published**: 2021-10-28 15:43:21+00:00
- **Updated**: 2021-10-28 15:43:21+00:00
- **Authors**: Ray Sheombarsing, Nikita Moriakov, Jan-Jakob Sonke, Jonas Teuwen
- **Comment**: 19 pages, 10 figures, 1 table
- **Journal**: None
- **Summary**: We propose a novel deep learning framework for fast prediction of boundaries of two-dimensional simply connected domains using wavelets and Multi Resolution Analysis (MRA). The boundaries are modelled as (piecewise) smooth closed curves using wavelets and the so-called Pyramid Algorithm. Our network architecture is a hybrid analog of the U-Net, where the down-sampling path is a two-dimensional encoder with learnable filters, and the upsampling path is a one-dimensional decoder, which builds curves up from low to high resolution levels. Any wavelet basis induced by a MRA can be used. This flexibility allows for incorporation of priors on the smoothness of curves. The effectiveness of the proposed method is demonstrated by delineating boundaries of simply connected domains (organs) in medical images using Debauches wavelets and comparing performance with a U-Net baseline. Our model demonstrates up to 5x faster inference speed compared to the U-Net, while maintaining similar performance in terms of Dice score and Hausdorff distance.



### End-to-end Learning the Partial Permutation Matrix for Robust 3D Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2110.15250v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15250v2)
- **Published**: 2021-10-28 16:07:02+00:00
- **Updated**: 2022-03-01 05:36:32+00:00
- **Authors**: Zhiyuan Zhang, Jiadai Sun, Yuchao Dai, Dingfu Zhou, Xibin Song, Mingyi He
- **Comment**: Accepted by AAAI 2022
- **Journal**: None
- **Summary**: Even though considerable progress has been made in deep learning-based 3D point cloud processing, how to obtain accurate correspondences for robust registration remains a major challenge because existing hard assignment methods cannot deal with outliers naturally. Alternatively, the soft matching-based methods have been proposed to learn the matching probability rather than hard assignment. However, in this paper, we prove that these methods have an inherent ambiguity causing many deceptive correspondences. To address the above challenges, we propose to learn a partial permutation matching matrix, which does not assign corresponding points to outliers, and implements hard assignment to prevent ambiguity. However, this proposal poses two new problems, i.e., existing hard assignment algorithms can only solve a full rank permutation matrix rather than a partial permutation matrix, and this desired matrix is defined in the discrete space, which is non-differentiable. In response, we design a dedicated soft-to-hard (S2H) matching procedure within the registration pipeline consisting of two steps: solving the soft matching matrix (S-step) and projecting this soft matrix to the partial permutation matrix (H-step). Specifically, we augment the profit matrix before the hard assignment to solve an augmented permutation matrix, which is cropped to achieve the final partial permutation matrix. Moreover, to guarantee end-to-end learning, we supervise the learned partial permutation matrix but propagate the gradient to the soft matrix instead. Our S2H matching procedure can be easily integrated with existing registration frameworks, which has been verified in representative frameworks including DCP, RPMNet, and DGR. Extensive experiments have validated our method, which creates a new state-of-the-art performance for robust 3D point cloud registration. The code will be made public.



### Self-Supervised Learning Disentangled Group Representation as Feature
- **Arxiv ID**: http://arxiv.org/abs/2110.15255v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.15255v2)
- **Published**: 2021-10-28 16:12:33+00:00
- **Updated**: 2021-10-29 11:14:37+00:00
- **Authors**: Tan Wang, Zhongqi Yue, Jianqiang Huang, Qianru Sun, Hanwang Zhang
- **Comment**: Accepted by NeurIPS 2021 (Spotlight)
- **Journal**: None
- **Summary**: A good visual representation is an inference map from observations (images) to features (vectors) that faithfully reflects the hidden modularized generative factors (semantics). In this paper, we formulate the notion of "good" representation from a group-theoretic view using Higgins' definition of disentangled representation, and show that existing Self-Supervised Learning (SSL) only disentangles simple augmentation features such as rotation and colorization, thus unable to modularize the remaining semantics. To break the limitation, we propose an iterative SSL algorithm: Iterative Partition-based Invariant Risk Minimization (IP-IRM), which successfully grounds the abstract semantics and the group acting on them into concrete contrastive learning. At each iteration, IP-IRM first partitions the training samples into two subsets that correspond to an entangled group element. Then, it minimizes a subset-invariant contrastive loss, where the invariance guarantees to disentangle the group element. We prove that IP-IRM converges to a fully disentangled representation and show its effectiveness on various benchmarks. Codes are available at https://github.com/Wangt-CN/IP-IRM.



### UltraPose: Synthesizing Dense Pose with 1 Billion Points by Human-body Decoupling 3D Model
- **Arxiv ID**: http://arxiv.org/abs/2110.15267v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15267v1)
- **Published**: 2021-10-28 16:24:55+00:00
- **Updated**: 2021-10-28 16:24:55+00:00
- **Authors**: Haonan Yan, Jiaqi Chen, Xujie Zhang, Shengkai Zhang, Nianhong Jiao, Xiaodan Liang, Tianxiang Zheng
- **Comment**: Accepted to ICCV 2021
- **Journal**: ICCV 2021
- **Summary**: Recovering dense human poses from images plays a critical role in establishing an image-to-surface correspondence between RGB images and the 3D surface of the human body, serving the foundation of rich real-world applications, such as virtual humans, monocular-to-3d reconstruction. However, the popular DensePose-COCO dataset relies on a sophisticated manual annotation system, leading to severe limitations in acquiring the denser and more accurate annotated pose resources. In this work, we introduce a new 3D human-body model with a series of decoupled parameters that could freely control the generation of the body. Furthermore, we build a data generation system based on this decoupling 3D model, and construct an ultra dense synthetic benchmark UltraPose, containing around 1.3 billion corresponding points. Compared to the existing manually annotated DensePose-COCO dataset, the synthetic UltraPose has ultra dense image-to-surface correspondences without annotation cost and error. Our proposed UltraPose provides the largest benchmark and data resources for lifting the model capability in predicting more accurate dense poses. To promote future researches in this field, we also propose a transformer-based method to model the dense correspondence between 2D and 3D worlds. The proposed model trained on synthetic UltraPose can be applied to real-world scenarios, indicating the effectiveness of our benchmark and model.



### Equivariant Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.00899v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.00899v2)
- **Published**: 2021-10-28 17:21:33+00:00
- **Updated**: 2022-03-15 01:42:01+00:00
- **Authors**: Rumen Dangovski, Li Jing, Charlotte Loh, Seungwook Han, Akash Srivastava, Brian Cheung, Pulkit Agrawal, Marin Soljačić
- **Comment**: Camera Ready Revision. ICLR 2022. Discussion:
  https://openreview.net/forum?id=gKLAAfiytI Code:
  https://github.com/rdangovs/essl
- **Journal**: None
- **Summary**: In state-of-the-art self-supervised learning (SSL) pre-training produces semantically good representations by encouraging them to be invariant under meaningful transformations prescribed from human knowledge. In fact, the property of invariance is a trivial instance of a broader class called equivariance, which can be intuitively understood as the property that representations transform according to the way the inputs transform. Here, we show that rather than using only invariance, pre-training that encourages non-trivial equivariance to some transformations, while maintaining invariance to other transformations, can be used to improve the semantic quality of representations. Specifically, we extend popular SSL methods to a more general framework which we name Equivariant Self-Supervised Learning (E-SSL). In E-SSL, a simple additional pre-training objective encourages equivariance by predicting the transformations applied to the input. We demonstrate E-SSL's effectiveness empirically on several popular computer vision benchmarks, e.g. improving SimCLR to 72.5% linear probe accuracy on ImageNet. Furthermore, we demonstrate usefulness of E-SSL for applications beyond computer vision; in particular, we show its utility on regression problems in photonics science. Our code, datasets and pre-trained models are available at https://github.com/rdangovs/essl to aid further research in E-SSL.



### MEGAN: Memory Enhanced Graph Attention Network for Space-Time Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2110.15327v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15327v2)
- **Published**: 2021-10-28 17:37:07+00:00
- **Updated**: 2021-11-30 02:34:41+00:00
- **Authors**: Chenyu You, Lianyi Han, Aosong Feng, Ruihan Zhao, Hui Tang, Wei Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Space-time video super-resolution (STVSR) aims to construct a high space-time resolution video sequence from the corresponding low-frame-rate, low-resolution video sequence. Inspired by the recent success to consider spatial-temporal information for space-time super-resolution, our main goal in this work is to take full considerations of spatial and temporal correlations within the video sequences of fast dynamic events. To this end, we propose a novel one-stage memory enhanced graph attention network (MEGAN) for space-time video super-resolution. Specifically, we build a novel long-range memory graph aggregation (LMGA) module to dynamically capture correlations along the channel dimensions of the feature maps and adaptively aggregate channel features to enhance the feature representations. We introduce a non-local residual block, which enables each channel-wise feature to attend global spatial hierarchical features. In addition, we adopt a progressive fusion module to further enhance the representation ability by extensively exploiting spatial-temporal correlations from multiple frames. Experiment results demonstrate that our method achieves better results compared with the state-of-the-art methods quantitatively and visually.



### Residual Relaxation for Multi-view Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.15348v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15348v1)
- **Published**: 2021-10-28 17:57:17+00:00
- **Updated**: 2021-10-28 17:57:17+00:00
- **Authors**: Yifei Wang, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang, Zhouchen Lin
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Multi-view methods learn representations by aligning multiple views of the same image and their performance largely depends on the choice of data augmentation. In this paper, we notice that some other useful augmentations, such as image rotation, are harmful for multi-view methods because they cause a semantic shift that is too large to be aligned well. This observation motivates us to relax the exact alignment objective to better cultivate stronger augmentations. Taking image rotation as a case study, we develop a generic approach, Pretext-aware Residual Relaxation (Prelax), that relaxes the exact alignment by allowing an adaptive residual vector between different views and encoding the semantic shift through pretext-aware learning. Extensive experiments on different backbones show that our method can not only improve multi-view methods with existing augmentations, but also benefit from stronger image augmentations like rotation.



### XDEEP-MSI: Explainable Bias-Rejecting Microsatellite Instability Deep Learning System In Colorectal Cancer
- **Arxiv ID**: http://arxiv.org/abs/2110.15350v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.15350v1)
- **Published**: 2021-10-28 17:58:01+00:00
- **Updated**: 2021-10-28 17:58:01+00:00
- **Authors**: Aurelia Bustos, Artemio Payá, Andres Torrubia, Rodrigo Jover, Xavier Llor, Xavier Bessa, Antoni Castells, Cristina Alenda
- **Comment**: 16 pages, 8 figures
- **Journal**: None
- **Summary**: We present a system for the prediction of microsatellite instability (MSI) from H&E images of colorectal cancer using deep learning (DL) techniques customized for tissue microarrays (TMAs). The system incorporates an end-to-end image preprocessing module that produces tiles at multiple magnifications in the regions of interest as guided by a tissue classifier module, and a multiple-bias rejecting module. The training and validation TMA samples were obtained from the EPICOLON project and further enriched with samples from a single institution. A systematic study of biases at tile level identified three protected (bias) variables associated with the learned representations of a baseline model: the project of origin of samples, the patient spot and the TMA glass where each spot was placed. A multiple bias rejecting technique based on adversarial training is implemented at the DL architecture so to directly avoid learning the batch effects of those variables. The learned features from the bias-ablated model have maximum discriminative power with respect to the task and minimal statistical mean dependence with the biases. The impact of different magnifications, types of tissues and the model performance at tile vs patient level is analyzed. The AUC at tile level, and including all three selected tissues (tumor epithelium, mucine and lymphocytic regions) and 4 magnifications, was 0.87 +/- 0.03 and increased to 0.9 +/- 0.03 at patient level. To the best of our knowledge, this is the first work that incorporates a multiple bias ablation technique at the DL architecture in digital pathology, and the first using TMAs for the MSI prediction task.



### MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.15352v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15352v1)
- **Published**: 2021-10-28 17:58:45+00:00
- **Updated**: 2021-10-28 17:58:45+00:00
- **Authors**: Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, Song Han
- **Comment**: None
- **Journal**: None
- **Summary**: Tiny deep learning on microcontroller units (MCUs) is challenging due to the limited memory size. We find that the memory bottleneck is due to the imbalanced memory distribution in convolutional neural network (CNN) designs: the first several blocks have an order of magnitude larger memory usage than the rest of the network. To alleviate this issue, we propose a generic patch-by-patch inference scheduling, which operates only on a small spatial region of the feature map and significantly cuts down the peak memory. However, naive implementation brings overlapping patches and computation overhead. We further propose network redistribution to shift the receptive field and FLOPs to the later stage and reduce the computation overhead. Manually redistributing the receptive field is difficult. We automate the process with neural architecture search to jointly optimize the neural architecture and inference scheduling, leading to MCUNetV2. Patch-based inference effectively reduces the peak memory usage of existing networks by 4-8x. Co-designed with neural networks, MCUNetV2 sets a record ImageNet accuracy on MCU (71.8%), and achieves >90% accuracy on the visual wake words dataset under only 32kB SRAM. MCUNetV2 also unblocks object detection on tiny devices, achieving 16.9% higher mAP on Pascal VOC compared to the state-of-the-art result. Our study largely addressed the memory bottleneck in tinyML and paved the way for various vision applications beyond image classification.



### Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language
- **Arxiv ID**: http://arxiv.org/abs/2110.15358v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.15358v1)
- **Published**: 2021-10-28 17:59:13+00:00
- **Updated**: 2021-10-28 17:59:13+00:00
- **Authors**: Mingyu Ding, Zhenfang Chen, Tao Du, Ping Luo, Joshua B. Tenenbaum, Chuang Gan
- **Comment**: NeurIPS 2021. Project page: http://vrdp.csail.mit.edu/
- **Journal**: None
- **Summary**: In this work, we propose a unified framework, called Visual Reasoning with Differ-entiable Physics (VRDP), that can jointly learn visual concepts and infer physics models of objects and their interactions from videos and language. This is achieved by seamlessly integrating three components: a visual perception module, a concept learner, and a differentiable physics engine. The visual perception module parses each video frame into object-centric trajectories and represents them as latent scene representations. The concept learner grounds visual concepts (e.g., color, shape, and material) from these object-centric representations based on the language, thus providing prior knowledge for the physics engine. The differentiable physics model, implemented as an impulse-based differentiable rigid-body simulator, performs differentiable physical simulation based on the grounded concepts to infer physical properties, such as mass, restitution, and velocity, by fitting the simulated trajectories into the video observations. Consequently, these learned concepts and physical models can explain what we have seen and imagine what is about to happen in future and counterfactual scenarios. Integrating differentiable physics into the dynamic reasoning framework offers several appealing benefits. More accurate dynamics prediction in learned physics models enables state-of-the-art performance on both synthetic and real-world benchmarks while still maintaining high transparency and interpretability; most notably, VRDP improves the accuracy of predictive and counterfactual questions by 4.5% and 11.5% compared to its best counterpart. VRDP is also highly data-efficient: physical parameters can be optimized from very few videos, and even a single video can be sufficient. Finally, with all physical parameters inferred, VRDP can quickly learn new concepts from a few examples.



### Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives
- **Arxiv ID**: http://arxiv.org/abs/2110.15360v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.15360v1)
- **Published**: 2021-10-28 17:59:30+00:00
- **Updated**: 2021-10-28 17:59:30+00:00
- **Authors**: Murtaza Dalal, Deepak Pathak, Ruslan Salakhutdinov
- **Comment**: Published at NeurIPS 2021. Website at
  https://mihdalal.github.io/raps/
- **Journal**: None
- **Summary**: Despite the potential of reinforcement learning (RL) for building general-purpose robotic systems, training RL agents to solve robotics tasks still remains challenging due to the difficulty of exploration in purely continuous action spaces. Addressing this problem is an active area of research with the majority of focus on improving RL methods via better optimization or more efficient exploration. An alternate but important component to consider improving is the interface of the RL algorithm with the robot. In this work, we manually specify a library of robot action primitives (RAPS), parameterized with arguments that are learned by an RL policy. These parameterized primitives are expressive, simple to implement, enable efficient exploration and can be transferred across robots, tasks and environments. We perform a thorough empirical study across challenging tasks in three distinct domains with image input and a sparse terminal reward. We find that our simple change to the action interface substantially improves both the learning efficiency and task performance irrespective of the underlying RL algorithm, significantly outperforming prior methods which learn skills from offline expert data. Code and videos at https://mihdalal.github.io/raps/



### Neural Disparity Refinement for Arbitrary Resolution Stereo
- **Arxiv ID**: http://arxiv.org/abs/2110.15367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15367v1)
- **Published**: 2021-10-28 18:00:00+00:00
- **Updated**: 2021-10-28 18:00:00+00:00
- **Authors**: Filippo Aleotti, Fabio Tosi, Pierluigi Zama Ramirez, Matteo Poggi, Samuele Salti, Stefano Mattoccia, Luigi Di Stefano
- **Comment**: 3DV 2021 Oral paper. Project page:
  https://cvlab-unibo.github.io/neural-disparity-refinement-web
- **Journal**: None
- **Summary**: We introduce a novel architecture for neural disparity refinement aimed at facilitating deployment of 3D computer vision on cheap and widespread consumer devices, such as mobile phones. Our approach relies on a continuous formulation that enables to estimate a refined disparity map at any arbitrary output resolution. Thereby, it can handle effectively the unbalanced camera setup typical of nowadays mobile phones, which feature both high and low resolution RGB sensors within the same device. Moreover, our neural network can process seamlessly the output of a variety of stereo methods and, by refining the disparity maps computed by a traditional matching algorithm like SGM, it can achieve unpaired zero-shot generalization performance compared to state-of-the-art end-to-end stereo models.



### New SAR target recognition based on YOLO and very deep multi-canonical correlation analysis
- **Arxiv ID**: http://arxiv.org/abs/2110.15383v1
- **DOI**: 10.1080/01431161.2021.1953719
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.15383v1)
- **Published**: 2021-10-28 18:10:26+00:00
- **Updated**: 2021-10-28 18:10:26+00:00
- **Authors**: Moussa Amrani, Abdelatif Bey, Abdenour Amamra
- **Comment**: 20 pages. International Journal of Remote Sensing (2021)
- **Journal**: None
- **Summary**: Synthetic Aperture Radar (SAR) images are prone to be contaminated by noise, which makes it very difficult to perform target recognition in SAR images. Inspired by great success of very deep convolutional neural networks (CNNs), this paper proposes a robust feature extraction method for SAR image target classification by adaptively fusing effective features from different CNN layers. First, YOLOv4 network is fine-tuned to detect the targets from the respective MF SAR target images. Second, a very deep CNN is trained from scratch on the moving and stationary target acquisition and recognition (MSTAR) database by using small filters throughout the whole net to reduce the speckle noise. Besides, using small-size convolution filters decreases the number of parameters in each layer and, therefore, reduces computation cost as the CNN goes deeper. The resulting CNN model is capable of extracting very deep features from the target images without performing any noise filtering or pre-processing techniques. Third, our approach proposes to use the multi-canonical correlation analysis (MCCA) to adaptively learn CNN features from different layers such that the resulting representations are highly linearly correlated and therefore can achieve better classification accuracy even if a simple linear support vector machine is used. Experimental results on the MSTAR dataset demonstrate that the proposed method outperforms the state-of-the-art methods.



### Automated Translation of Rebar Information from GPR Data into As-Built BIM: A Deep Learning-based Approach
- **Arxiv ID**: http://arxiv.org/abs/2110.15448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.15448v1)
- **Published**: 2021-10-28 21:54:48+00:00
- **Updated**: 2021-10-28 21:54:48+00:00
- **Authors**: Zhongming Xiang, Ge Ou, Abbas Rashidi
- **Comment**: 8 pages, the 2021 ASCE International Conference on Computing in Civil
  Engineering
- **Journal**: None
- **Summary**: Building Information Modeling (BIM) is increasingly used in the construction industry, but existing studies often ignore embedded rebars. Ground Penetrating Radar (GPR) provides a potential solution to develop as-built BIM with surface elements and rebars. However, automatically translating rebars from GPR into BIM is challenging since GPR cannot provide any information about the scanned element. Thus, we propose an approach to link GPR data and BIM according to Faster R-CNN. A label is attached to each element scanned by GPR for capturing the labeled images, which are used with other images to build a 3D model. Meanwhile, Faster R-CNN is introduced to identify the labels, and the projection relationship between images and the model is used to localize the scanned elements in the 3D model. Two concrete buildings is selected to evaluate the proposed approach, and the results reveal that our method could accurately translate the rebars from GPR data into corresponding elements in BIM with correct distributions.



