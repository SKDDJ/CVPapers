# Arxiv Papers in cs.CV on 2021-10-22
### Pseudo Supervised Monocular Depth Estimation with Teacher-Student Network
- **Arxiv ID**: http://arxiv.org/abs/2110.11545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11545v1)
- **Published**: 2021-10-22 01:08:36+00:00
- **Updated**: 2021-10-22 01:08:36+00:00
- **Authors**: Huan Liu, Junsong Yuan, Chen Wang, Jun Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Despite recent improvement of supervised monocular depth estimation, the lack of high quality pixel-wise ground truth annotations has become a major hurdle for further progress. In this work, we propose a new unsupervised depth estimation method based on pseudo supervision mechanism by training a teacher-student network with knowledge distillation. It strategically integrates the advantages of supervised and unsupervised monocular depth estimation, as well as unsupervised binocular depth estimation. Specifically, the teacher network takes advantage of the effectiveness of binocular depth estimation to produce accurate disparity maps, which are then used as the pseudo ground truth to train the student network for monocular depth estimation. This effectively converts the problem of unsupervised learning to supervised learning. Our extensive experimental results demonstrate that the proposed method outperforms the state-of-the-art on the KITTI benchmark.



### Signature-Graph Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.11551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11551v1)
- **Published**: 2021-10-22 01:44:14+00:00
- **Updated**: 2021-10-22 01:44:14+00:00
- **Authors**: Ali Hamdi, Flora Salim, Du Yong Kim, Xiaojun Chang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach for visual representation learning called Signature-Graph Neural Networks (SGN). SGN learns latent global structures that augment the feature representation of Convolutional Neural Networks (CNN). SGN constructs unique undirected graphs for each image based on the CNN feature maps. The feature maps are partitioned into a set of equal and non-overlapping patches. The graph nodes are located on high-contrast sharp convolution features with the local maxima or minima in these patches. The node embeddings are aggregated through novel Signature-Graphs based on horizontal and vertical edge connections. The representation vectors are then computed based on the spectral Laplacian eigenvalues of the graphs. SGN outperforms existing methods of recent graph convolutional networks, generative adversarial networks, and auto-encoders with image classification accuracy of 99.65% on ASIRRA, 99.91% on MNIST, 98.55% on Fashion-MNIST, 96.18% on CIFAR-10, 84.71% on CIFAR-100, 94.36% on STL10, and 95.86% on SVHN datasets. We also introduce a novel implementation of the state-of-the-art multi-head attention (MHA) on top of the proposed SGN. Adding SGN to MHA improved the image classification accuracy from 86.92% to 94.36% on the STL10 dataset



### Prototypical Classifier for Robust Class-Imbalanced Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.11553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11553v1)
- **Published**: 2021-10-22 01:55:01+00:00
- **Updated**: 2021-10-22 01:55:01+00:00
- **Authors**: Tong Wei, Jiang-Xin Shi, Yu-Feng Li, Min-Ling Zhang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Deep neural networks have been shown to be very powerful methods for many supervised learning tasks. However, they can also easily overfit to training set biases, i.e., label noise and class imbalance. While both learning with noisy labels and class-imbalanced learning have received tremendous attention, existing works mainly focus on one of these two training set biases. To fill the gap, we propose \textit{Prototypical Classifier}, which does not require fitting additional parameters given the embedding network. Unlike conventional classifiers that are biased towards head classes, Prototypical Classifier produces balanced and comparable predictions for all classes even though the training set is class-imbalanced. By leveraging this appealing property, we can easily detect noisy labels by thresholding the confidence scores predicted by Prototypical Classifier, where the threshold is dynamically adjusted through the iteration. A sample reweghting strategy is then applied to mitigate the influence of noisy labels. We test our method on CIFAR-10-LT, CIFAR-100-LT and Webvision datasets, observing that Prototypical Classifier obtains substaintial improvements compared with state of the arts.



### MHAttnSurv: Multi-Head Attention for Survival Prediction Using Whole-Slide Pathology Images
- **Arxiv ID**: http://arxiv.org/abs/2110.11558v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2110.11558v1)
- **Published**: 2021-10-22 02:18:27+00:00
- **Updated**: 2021-10-22 02:18:27+00:00
- **Authors**: Shuai Jiang, Arief A. Suriawinata, Saeed Hassanpour
- **Comment**: None
- **Journal**: None
- **Summary**: In pathology, whole-slide images (WSI) based survival prediction has attracted increasing interest. However, given the large size of WSIs and the lack of pathologist annotations, extracting the prognostic information from WSIs remains a challenging task. Previous studies have used multiple instance learning approaches to combine the information from multiple randomly sampled patches, but different visual patterns may contribute differently to prognosis prediction. In this study, we developed a multi-head attention approach to focus on various parts of a tumor slide, for more comprehensive information extraction from WSIs. We evaluated our approach on four cancer types from The Cancer Genome Atlas database. Our model achieved an average c-index of 0.640, outperforming two existing state-of-the-art approaches for WSI-based survival prediction, which have an average c-index of 0.603 and 0.619 on these datasets. Visualization of our attention maps reveals each attention head focuses synergistically on different morphological patterns.



### EvoGAN: An Evolutionary Computation Assisted GAN
- **Arxiv ID**: http://arxiv.org/abs/2110.11583v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2110.11583v1)
- **Published**: 2021-10-22 04:23:19+00:00
- **Updated**: 2021-10-22 04:23:19+00:00
- **Authors**: Feng Liu, HanYang Wang, Jiahao Zhang, Ziwang Fu, Aimin Zhou, Jiayin Qi, Zhibin Li
- **Comment**: 20 pages, 9 figures, 1 table
- **Journal**: None
- **Summary**: The image synthesis technique is relatively well established which can generate facial images that are indistinguishable even by human beings. However, all of these approaches uses gradients to condition the output, resulting in the outputting the same image with the same input. Also, they can only generate images with basic expression or mimic an expression instead of generating compound expression. In real life, however, human expressions are of great diversity and complexity. In this paper, we propose an evolutionary algorithm (EA) assisted GAN, named EvoGAN, to generate various compound expressions with any accurate target compound expression. EvoGAN uses an EA to search target results in the data distribution learned by GAN. Specifically, we use the Facial Action Coding System (FACS) as the encoding of an EA and use a pre-trained GAN to generate human facial images, and then use a pre-trained classifier to recognize the expression composition of the synthesized images as the fitness function to guide the search of the EA. Combined random searching algorithm, various images with the target expression can be easily sythesized. Quantitative and Qualitative results are presented on several compound expressions, and the experimental results demonstrate the feasibility and the potential of EvoGAN.



### Wide and Narrow: Video Prediction from Context and Motion
- **Arxiv ID**: http://arxiv.org/abs/2110.11586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11586v1)
- **Published**: 2021-10-22 04:35:58+00:00
- **Updated**: 2021-10-22 04:35:58+00:00
- **Authors**: Jaehoon Cho, Jiyoung Lee, Changjae Oh, Wonil Song, Kwanghoon Sohn
- **Comment**: British Machine Vision Conference 2021
- **Journal**: None
- **Summary**: Video prediction, forecasting the future frames from a sequence of input frames, is a challenging task since the view changes are influenced by various factors, such as the global context surrounding the scene and local motion dynamics. In this paper, we propose a new framework to integrate these complementary attributes to predict complex pixel dynamics through deep networks. We present global context propagation networks that iteratively aggregate the non-local neighboring representations to preserve the contextual information over the past frames. To capture the local motion pattern of objects, we also devise local filter memory networks that generate adaptive filter kernels by storing the prototypical motion of moving objects in the memory. The proposed framework, utilizing the outputs from both networks, can address blurry predictions and color distortion. We conduct experiments on Caltech pedestrian and UCF101 datasets, and demonstrate state-of-the-art results. Especially for multi-step prediction, we obtain an outstanding performance in quantitative and qualitative evaluation.



### DIML/CVL RGB-D Dataset: 2M RGB-D Images of Natural Indoor and Outdoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/2110.11590v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11590v1)
- **Published**: 2021-10-22 05:12:42+00:00
- **Updated**: 2021-10-22 05:12:42+00:00
- **Authors**: Jaehoon Cho, Dongbo Min, Youngjung Kim, Kwanghoon Sohn
- **Comment**: Technical report
- **Journal**: None
- **Summary**: This manual is intended to provide a detailed description of the DIML/CVL RGB-D dataset. This dataset is comprised of 2M color images and their corresponding depth maps from a great variety of natural indoor and outdoor scenes. The indoor dataset was constructed using the Microsoft Kinect v2, while the outdoor dataset was built using the stereo cameras (ZED stereo camera and built-in stereo camera). Table I summarizes the details of our dataset, including acquisition, processing, format, and toolbox. Refer to Section II and III for more details.



### Model Inspired Autoencoder for Unsupervised Hyperspectral Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2110.11591v1
- **DOI**: 10.1109/TGRS.2022.3143156
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11591v1)
- **Published**: 2021-10-22 05:15:16+00:00
- **Updated**: 2021-10-22 05:15:16+00:00
- **Authors**: Jianjun Liu, Zebin Wu, Liang Xiao, Xiao-Jun Wu
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, 2022, vol 60
- **Summary**: This paper focuses on hyperspectral image (HSI) super-resolution that aims to fuse a low-spatial-resolution HSI and a high-spatial-resolution multispectral image to form a high-spatial-resolution HSI (HR-HSI). Existing deep learning-based approaches are mostly supervised that rely on a large number of labeled training samples, which is unrealistic. The commonly used model-based approaches are unsupervised and flexible but rely on hand-craft priors. Inspired by the specific properties of model, we make the first attempt to design a model inspired deep network for HSI super-resolution in an unsupervised manner. This approach consists of an implicit autoencoder network built on the target HR-HSI that treats each pixel as an individual sample. The nonnegative matrix factorization (NMF) of the target HR-HSI is integrated into the autoencoder network, where the two NMF parts, spectral and spatial matrices, are treated as decoder parameters and hidden outputs respectively. In the encoding stage, we present a pixel-wise fusion model to estimate hidden outputs directly, and then reformulate and unfold the model's algorithm to form the encoder network. With the specific architecture, the proposed network is similar to a manifold prior-based model, and can be trained patch by patch rather than the entire image. Moreover, we propose an additional unsupervised network to estimate the point spread function and spectral response function. Experimental results conducted on both synthetic and real datasets demonstrate the effectiveness of the proposed approach.



### Learning Text-Image Joint Embedding for Efficient Cross-Modal Retrieval with Deep Feature Engineering
- **Arxiv ID**: http://arxiv.org/abs/2110.11592v1
- **DOI**: 10.1145/3490519
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2110.11592v1)
- **Published**: 2021-10-22 05:18:28+00:00
- **Updated**: 2021-10-22 05:18:28+00:00
- **Authors**: Zhongwei Xie, Ling Liu, Yanzhao Wu, Luo Zhong, Lin Li
- **Comment**: accepted by ACM Transactions on Information Systems(TOIS). arXiv
  admin note: text overlap with arXiv:2108.00705, arXiv:2108.03788
- **Journal**: None
- **Summary**: This paper introduces a two-phase deep feature engineering framework for efficient learning of semantics enhanced joint embedding, which clearly separates the deep feature engineering in data preprocessing from training the text-image joint embedding model. We use the Recipe1M dataset for the technical description and empirical validation. In preprocessing, we perform deep feature engineering by combining deep feature engineering with semantic context features derived from raw text-image input data. We leverage LSTM to identify key terms, deep NLP models from the BERT family, TextRank, or TF-IDF to produce ranking scores for key terms before generating the vector representation for each key term by using word2vec. We leverage wideResNet50 and word2vec to extract and encode the image category semantics of food images to help semantic alignment of the learned recipe and image embeddings in the joint latent space. In joint embedding learning, we perform deep feature engineering by optimizing the batch-hard triplet loss function with soft-margin and double negative sampling, taking into account also the category-based alignment loss and discriminator-based alignment loss. Extensive experiments demonstrate that our SEJE approach with deep feature engineering significantly outperforms the state-of-the-art approaches.



### Automatic Detection of Injection and Press Mold Parts on 2D Drawing Using Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2110.11593v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, F.2.2; I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2110.11593v1)
- **Published**: 2021-10-22 05:20:13+00:00
- **Updated**: 2021-10-22 05:20:13+00:00
- **Authors**: Junseok Lee, Jongwon Kim, Jumi Park, Seunghyeok Back, Seongho Bak, Kyoobin Lee
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: This paper proposes a method to automatically detect the key feature parts in a CAD of commercial TV and monitor using a deep neural network. We developed a deep learning pipeline that can detect the injection parts such as hook, boss, undercut and press parts such as DPS, Embo-Screwless, Embo-Burring, and EMBO in the 2D CAD drawing images. We first cropped the drawing to a specific size for the training efficiency of a deep neural network. Then, we use Cascade R-CNN to find the position of injection and press parts and use Resnet-50 to predict the orientation of the parts. Finally, we convert the position of the parts found through the cropped image to the position of the original image. As a result, we obtained detection accuracy of injection and press parts with 84.1% in AP (Average Precision), 91.2% in AR(Average Recall), 72.0% in AP, 87.0% in AR, and orientation accuracy of injection and press parts with 94.4% and 92.0%, which can facilitate the faster design in industrial product design.



### ProtoShotXAI: Using Prototypical Few-Shot Architecture for Explainable AI
- **Arxiv ID**: http://arxiv.org/abs/2110.11597v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11597v2)
- **Published**: 2021-10-22 05:24:52+00:00
- **Updated**: 2022-09-26 06:36:34+00:00
- **Authors**: Samuel Hess, Gregory Ditzler
- **Comment**: 38 pages, 13 figures, 5 tables
- **Journal**: None
- **Summary**: Unexplainable black-box models create scenarios where anomalies cause deleterious responses, thus creating unacceptable risks. These risks have motivated the field of eXplainable Artificial Intelligence (XAI) to improve trust by evaluating local interpretability in black-box neural networks. Unfortunately, the ground truth is unavailable for the model's decision, so evaluation is limited to qualitative assessment. Further, interpretability may lead to inaccurate conclusions about the model or a false sense of trust. We propose to improve XAI from the vantage point of the user's trust by exploring a black-box model's latent feature space. We present an approach, ProtoShotXAI, that uses a Prototypical few-shot network to explore the contrastive manifold between nonlinear features of different classes. A user explores the manifold by perturbing the input features of a query sample and recording the response for a subset of exemplars from any class. Our approach is the first locally interpretable XAI model that can be extended to, and demonstrated on, few-shot networks. We compare ProtoShotXAI to the state-of-the-art XAI approaches on MNIST, Omniglot, and ImageNet to demonstrate, both quantitatively and qualitatively, that ProtoShotXAI provides more flexibility for model exploration. Finally, ProtoShotXAI also demonstrates novel explainabilty and detectabilty on adversarial samples.



### High Fidelity 3D Reconstructions with Limited Physical Views
- **Arxiv ID**: http://arxiv.org/abs/2110.11599v1
- **DOI**: 10.1109/3DV53792.2021.00137
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.11599v1)
- **Published**: 2021-10-22 05:27:24+00:00
- **Updated**: 2021-10-22 05:27:24+00:00
- **Authors**: Mosam Dabhi, Chaoyang Wang, Kunal Saluja, Laszlo Jeni, Ian Fasel, Simon Lucey
- **Comment**: Accepted to 3DV 2021 (project page & code:
  https://sites.google.com/view/high-fidelity-3d-neural-prior)
- **Journal**: None
- **Summary**: Multi-view triangulation is the gold standard for 3D reconstruction from 2D correspondences given known calibration and sufficient views. However in practice, expensive multi-view setups -- involving tens sometimes hundreds of cameras -- are required in order to obtain the high fidelity 3D reconstructions necessary for many modern applications. In this paper we present a novel approach that leverages recent advances in 2D-3D lifting using neural shape priors while also enforcing multi-view equivariance. We show how our method can achieve comparable fidelity to expensive calibrated multi-view rigs using a limited (2-3) number of uncalibrated camera views.



### Multimodal Semi-Supervised Learning for 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/2110.11601v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11601v2)
- **Published**: 2021-10-22 05:33:16+00:00
- **Updated**: 2021-10-25 02:35:34+00:00
- **Authors**: Zhimin Chen, Longlong Jing, Yang Liang, YingLi Tian, Bing Li
- **Comment**: BMVC 2021 poster
- **Journal**: None
- **Summary**: In recent years, semi-supervised learning has been widely explored and shows excellent data efficiency for 2D data. There is an emerging need to improve data efficiency for 3D tasks due to the scarcity of labeled 3D data. This paper explores how the coherence of different modelities of 3D data (e.g. point cloud, image, and mesh) can be used to improve data efficiency for both 3D classification and retrieval tasks. We propose a novel multimodal semi-supervised learning framework by introducing instance-level consistency constraint and a novel multimodal contrastive prototype (M2CP) loss. The instance-level consistency enforces the network to generate consistent representations for multimodal data of the same object regardless of its modality. The M2CP maintains a multimodal prototype for each class and learns features with small intra-class variations by minimizing the feature distance of each object to its prototype while maximizing the distance to the others. Our proposed framework significantly outperforms all the state-of-the-art counterparts for both classification and retrieval tasks by a large margin on the modelNet10 and ModelNet40 datasets.



### Multi-Stream Attention Learning for Monocular Vehicle Velocity and Inter-Vehicle Distance Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.11608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11608v1)
- **Published**: 2021-10-22 06:14:12+00:00
- **Updated**: 2021-10-22 06:14:12+00:00
- **Authors**: Kuan-Chih Huang, Yu-Kai Huang, Winston H. Hsu
- **Comment**: Accepted to BMVC 2021
- **Journal**: None
- **Summary**: Vehicle velocity and inter-vehicle distance estimation are essential for ADAS (Advanced driver-assistance systems) and autonomous vehicles. To save the cost of expensive ranging sensors, recent studies focus on using a low-cost monocular camera to perceive the environment around the vehicle in a data-driven fashion. Existing approaches treat each vehicle independently for perception and cause inconsistent estimation. Furthermore, important information like context and spatial relation in 2D object detection is often neglected in the velocity estimation pipeline. In this paper, we explore the relationship between vehicles of the same frame with a global-relative-constraint (GLC) loss to encourage consistent estimation. A novel multi-stream attention network (MSANet) is proposed to extract different aspects of features, e.g., spatial and contextual features, for joint vehicle velocity and inter-vehicle distance estimation. Experiments show the effectiveness and robustness of our proposed approach. MSANet outperforms state-of-the-art algorithms on both the KITTI dataset and TuSimple velocity dataset.



### SciCap: Generating Captions for Scientific Figures
- **Arxiv ID**: http://arxiv.org/abs/2110.11624v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11624v2)
- **Published**: 2021-10-22 07:10:41+00:00
- **Updated**: 2021-10-25 04:37:30+00:00
- **Authors**: Ting-Yao Hsu, C. Lee Giles, Ting-Hao 'Kenneth' Huang
- **Comment**: To Appear in EMNLP 2021 Findings. The dataset is available at:
  https://github.com/tingyaohsu/SciCap
- **Journal**: None
- **Summary**: Researchers use figures to communicate rich, complex information in scientific papers. The captions of these figures are critical to conveying effective messages. However, low-quality figure captions commonly occur in scientific articles and may decrease understanding. In this paper, we propose an end-to-end neural framework to automatically generate informative, high-quality captions for scientific figures. To this end, we introduce SCICAP, a large-scale figure-caption dataset based on computer science arXiv papers published between 2010 and 2020. After pre-processing - including figure-type classification, sub-figure identification, text normalization, and caption text selection - SCICAP contained more than two million figures extracted from over 290,000 papers. We then established baseline models that caption graph plots, the dominant (19.2%) figure type. The experimental results showed both opportunities and steep challenges of generating captions for scientific figures.



### Rethinking Generalization Performance of Surgical Phase Recognition with Expert-Generated Annotations
- **Arxiv ID**: http://arxiv.org/abs/2110.11626v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11626v1)
- **Published**: 2021-10-22 07:21:06+00:00
- **Updated**: 2021-10-22 07:21:06+00:00
- **Authors**: Seungbum Hong, Jiwon Lee, Bokyung Park, Ahmed A. Alwusaibie, Anwar H. Alfadhel, SungHyun Park, Woo Jin Hyung, Min-Kook Choi
- **Comment**: Bridging the Gap: From Machine Learning Research to Clinical Practice
  @ NeurIPS 2021 (Spotlight)
- **Journal**: None
- **Summary**: As the area of application of deep neural networks expands to areas requiring expertise, e.g., in medicine and law, more exquisite annotation processes for expert knowledge training are required. In particular, it is difficult to guarantee generalization performance in the clinical field in the case of expert knowledge training where opinions may differ even among experts on annotations. To raise the issue of the annotation generation process for expertise training of CNNs, we verified the annotations for surgical phase recognition of laparoscopic cholecystectomy and subtotal gastrectomy for gastric cancer. We produce calibrated annotations for the seven phases of cholecystectomy by analyzing the discrepancies of previously annotated labels and by discussing the criteria of surgical phases. For gastrectomy for gastric cancer has more complex twenty-one surgical phases, we generate consensus annotation by the revision process with five specialists. By training the CNN-based surgical phase recognition networks with revised annotations, we achieved improved generalization performance over models trained with original annotation under the same cross-validation settings. We showed that the expertise data annotation pipeline for deep neural networks should be more rigorous based on the type of problem to apply clinical field.



### Improving Face Recognition with Large Age Gaps by Learning to Distinguish Children
- **Arxiv ID**: http://arxiv.org/abs/2110.11630v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11630v1)
- **Published**: 2021-10-22 07:31:14+00:00
- **Updated**: 2021-10-22 07:31:14+00:00
- **Authors**: Jungsoo Lee, Jooyeol Yun, Sunghyun Park, Yonggyu Kim, Jaegul Choo
- **Comment**: Accepted to BMVC 2021
- **Journal**: None
- **Summary**: Despite the unprecedented improvement of face recognition, existing face recognition models still show considerably low performances in determining whether a pair of child and adult images belong to the same identity. Previous approaches mainly focused on increasing the similarity between child and adult images of a given identity to overcome the discrepancy of facial appearances due to aging. However, we observe that reducing the similarity between child images of different identities is crucial for learning distinct features among children and thus improving face recognition performance in child-adult pairs. Based on this intuition, we propose a novel loss function called the Inter-Prototype loss which minimizes the similarity between child images. Unlike the previous studies, the Inter-Prototype loss does not require additional child images or training additional learnable parameters. Our extensive experiments and in-depth analyses show that our approach outperforms existing baselines in face recognition with child-adult pairs. Our code and newly-constructed test sets of child-adult pairs are available at https://github.com/leebebeto/Inter-Prototype.



### Occlusion-Robust Object Pose Estimation with Holistic Representation
- **Arxiv ID**: http://arxiv.org/abs/2110.11636v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11636v1)
- **Published**: 2021-10-22 08:00:26+00:00
- **Updated**: 2021-10-22 08:00:26+00:00
- **Authors**: Bo Chen, Tat-Jun Chin, Marius Klimavicius
- **Comment**: WACV 2022
- **Journal**: None
- **Summary**: Practical object pose estimation demands robustness against occlusions to the target object. State-of-the-art (SOTA) object pose estimators take a two-stage approach, where the first stage predicts 2D landmarks using a deep network and the second stage solves for 6DOF pose from 2D-3D correspondences. Albeit widely adopted, such two-stage approaches could suffer from novel occlusions when generalising and weak landmark coherence due to disrupted features. To address these issues, we develop a novel occlude-and-blackout batch augmentation technique to learn occlusion-robust deep features, and a multi-precision supervision architecture to encourage holistic pose representation learning for accurate and coherent landmark predictions. We perform careful ablation tests to verify the impact of our innovations and compare our method to SOTA pose estimators. Without the need of any post-processing or refinement, our method exhibits superior performance on the LINEMOD dataset. On the YCB-Video dataset our method outperforms all non-refinement methods in terms of the ADD(-S) metric. We also demonstrate the high data-efficiency of our method. Our code is available at http://github.com/BoChenYS/ROPE



### Cross-domain Trajectory Prediction with CTP-Net
- **Arxiv ID**: http://arxiv.org/abs/2110.11645v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11645v2)
- **Published**: 2021-10-22 08:18:31+00:00
- **Updated**: 2022-08-16 15:15:19+00:00
- **Authors**: Pingxuan Huang, Zhenhua Cui, Jing Li, Shenghua Gao, bo Hu, Yanyan Fang
- **Comment**: Work is accepted by CICAI(CAAI International Conference on Artificial
  Intelligence), 12 pages
- **Journal**: None
- **Summary**: Most pedestrian trajectory prediction methods rely on a huge amount of trajectories annotation, which is time-consuming and expensive. Moreover, a well-trained model may not effectively generalize to a new scenario captured by another camera. Therefore, it is desirable to adapt the model trained on an annotated source domain to the target domain. To achieve domain adaptation for trajectory prediction, we propose a Cross-domain Trajectory Prediction Network (CTP-Net). In this framework, encoders are used in both domains to encode the observed trajectories, then their features are aligned by a cross-domain feature discriminator. Further, considering the consistency between the observed and the predicted trajectories, a target domain offset discriminator is utilized to adversarially regularize the future trajectory predictions to be in line with the observed trajectories. Extensive experiments demonstrate the effectiveness of our method on domain adaptation for pedestrian trajectory prediction.



### Pixel-by-Pixel Cross-Domain Alignment for Few-Shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.11650v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11650v1)
- **Published**: 2021-10-22 08:27:17+00:00
- **Updated**: 2021-10-22 08:27:17+00:00
- **Authors**: Antonio Tavera, Fabio Cermelli, Carlo Masone, Barbara Caputo
- **Comment**: Accepted at WACV 2022
- **Journal**: None
- **Summary**: In this paper we consider the task of semantic segmentation in autonomous driving applications. Specifically, we consider the cross-domain few-shot setting where training can use only few real-world annotated images and many annotated synthetic images. In this context, aligning the domains is made more challenging by the pixel-wise class imbalance that is intrinsic in the segmentation and that leads to ignoring the underrepresented classes and overfitting the well represented ones. We address this problem with a novel framework called Pixel-By-Pixel Cross-Domain Alignment (PixDA). We propose a novel pixel-by-pixel domain adversarial loss following three criteria: (i) align the source and the target domain for each pixel, (ii) avoid negative transfer on the correctly represented pixels, and (iii) regularize the training of infrequent classes to avoid overfitting. The pixel-wise adversarial training is assisted by a novel sample selection procedure, that handles the imbalance between source and target data, and a knowledge distillation strategy, that avoids overfitting towards the few target images. We demonstrate on standard synthetic-to-real benchmarks that PixDA outperforms previous state-of-the-art methods in (1-5)-shot settings.



### Projective Manifold Gradient Layer for Deep Rotation Regression
- **Arxiv ID**: http://arxiv.org/abs/2110.11657v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11657v3)
- **Published**: 2021-10-22 08:34:15+00:00
- **Updated**: 2022-03-30 03:06:06+00:00
- **Authors**: Jiayi Chen, Yingda Yin, Tolga Birdal, Baoquan Chen, Leonidas Guibas, He Wang
- **Comment**: CVPR2022
- **Journal**: None
- **Summary**: Regressing rotations on SO(3) manifold using deep neural networks is an important yet unsolved problem. The gap between the Euclidean network output space and the non-Euclidean SO(3) manifold imposes a severe challenge for neural network learning in both forward and backward passes. While several works have proposed different regression-friendly rotation representations, very few works have been devoted to improving the gradient backpropagating in the backward pass. In this paper, we propose a manifold-aware gradient that directly backpropagates into deep network weights. Leveraging Riemannian optimization to construct a novel projective gradient, our proposed regularized projective manifold gradient (RPMG) method helps networks achieve new state-of-the-art performance in a variety of rotation estimation tasks. Our proposed gradient layer can also be applied to other smooth manifolds such as the unit sphere. Our project page is at https://jychen18.github.io/RPMG.



### UVO Challenge on Video-based Open-World Segmentation 2021: 1st Place Solution
- **Arxiv ID**: http://arxiv.org/abs/2110.11661v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11661v2)
- **Published**: 2021-10-22 08:39:02+00:00
- **Updated**: 2021-11-01 20:29:42+00:00
- **Authors**: Yuming Du, Wen Guo, Yang Xiao, Vincent Lepetit
- **Comment**: Code:https://github.com/dulucas/UVO_Challenge. arXiv admin note:
  substantial text overlap with arXiv:2110.10239
- **Journal**: None
- **Summary**: In this report, we introduce our (pretty straightforard) two-step "detect-then-match" video instance segmentation method. The first step performs instance segmentation for each frame to get a large number of instance mask proposals. The second step is to do inter-frame instance mask matching with the help of optical flow. We demonstrate that with high quality mask proposals, a simple matching mechanism is good enough for tracking. Our approach achieves the first place in the UVO 2021 Video-based Open-World Segmentation Challenge.



### Reimagine BiSeNet for Real-Time Domain Adaptation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.11662v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11662v1)
- **Published**: 2021-10-22 08:39:28+00:00
- **Updated**: 2021-10-22 08:39:28+00:00
- **Authors**: Antonio Tavera, Carlo Masone, Barbara Caputo
- **Comment**: Accepted at I-RIM 3D 2021
- **Journal**: None
- **Summary**: Semantic segmentation models have reached remarkable performance across various tasks. However, this performance is achieved with extremely large models, using powerful computational resources and without considering training and inference time. Real-world applications, on the other hand, necessitate models with minimal memory demands, efficient inference speed, and executable with low-resources embedded devices, such as self-driving vehicles. In this paper, we look at the challenge of real-time semantic segmentation across domains, and we train a model to act appropriately on real-world data even though it was trained on a synthetic realm. We employ a new lightweight and shallow discriminator that was specifically created for this purpose. To the best of our knowledge, we are the first to present a real-time adversarial approach for assessing the domain adaption problem in semantic segmentation. We tested our framework in the two standard protocol: GTA5 to Cityscapes and SYNTHIA to Cityscapes. Code is available at: https://github.com/taveraantonio/RTDA.



### GCCN: Global Context Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2110.11664v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11664v1)
- **Published**: 2021-10-22 08:46:54+00:00
- **Updated**: 2021-10-22 08:46:54+00:00
- **Authors**: Ali Hamdi, Flora Salim, Du Yong Kim
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose Global Context Convolutional Network (GCCN) for visual recognition. GCCN computes global features representing contextual information across image patches. These global contextual features are defined as local maxima pixels with high visual sharpness in each patch. These features are then concatenated and utilised to augment the convolutional features. The learnt feature vector is normalised using the global context features using Frobenius norm. This straightforward approach achieves high accuracy in compassion to the state-of-the-art methods with 94.6% and 95.41% on CIFAR-10 and STL-10 datasets, respectively. To explore potential impact of GCCN on other visual representation tasks, we implemented GCCN as a based model to few-shot image classification. We learn metric distances between the augmented feature vectors and their prototypes representations, similar to Prototypical and Matching Networks. GCCN outperforms state-of-the-art few-shot learning methods achieving 99.9%, 84.8% and 80.74% on Omniglot, MiniImageNet and CUB-200, respectively. GCCN has significantly improved on the accuracy of state-of-the-art prototypical and matching networks by up to 30% in different few-shot learning scenarios.



### Explainable, automated urban interventions to improve pedestrian and vehicle safety
- **Arxiv ID**: http://arxiv.org/abs/2110.11672v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11672v2)
- **Published**: 2021-10-22 09:17:39+00:00
- **Updated**: 2021-11-08 21:43:17+00:00
- **Authors**: Cristina Bustos, Daniel Rhoads, Albert Sole-Ribalta, David Masip, Alex Arenas, Agata Lapedriza, Javier Borge-Holthoefer
- **Comment**: None
- **Journal**: None
- **Summary**: At the moment, urban mobility research and governmental initiatives are mostly focused on motor-related issues, e.g. the problems of congestion and pollution. And yet, we can not disregard the most vulnerable elements in the urban landscape: pedestrians, exposed to higher risks than other road users. Indeed, safe, accessible, and sustainable transport systems in cities are a core target of the UN's 2030 Agenda. Thus, there is an opportunity to apply advanced computational tools to the problem of traffic safety, in regards especially to pedestrians, who have been often overlooked in the past. This paper combines public data sources, large-scale street imagery and computer vision techniques to approach pedestrian and vehicle safety with an automated, relatively simple, and universally-applicable data-processing scheme. The steps involved in this pipeline include the adaptation and training of a Residual Convolutional Neural Network to determine a hazard index for each given urban scene, as well as an interpretability analysis based on image segmentation and class activation mapping on those same images. Combined, the outcome of this computational approach is a fine-grained map of hazard levels across a city, and an heuristic to identify interventions that might simultaneously improve pedestrian and vehicle safety. The proposed framework should be taken as a complement to the work of urban planners and public authorities.



### Depth-only Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2110.11679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11679v1)
- **Published**: 2021-10-22 09:59:31+00:00
- **Updated**: 2021-10-22 09:59:31+00:00
- **Authors**: Song Yan, Jinyu Yang, Ales Leonardis, Joni-Kristian Kamarainen
- **Comment**: Accepted to BMVC2021
- **Journal**: None
- **Summary**: Depth (D) indicates occlusion and is less sensitive to illumination changes, which make depth attractive modality for Visual Object Tracking (VOT). Depth is used in RGBD object tracking where the best trackers are deep RGB trackers with additional heuristic using depth maps. There are two potential reasons for the heuristics: 1) the lack of large RGBD tracking datasets to train deep RGBD trackers and 2) the long-term evaluation protocol of VOT RGBD that benefits from heuristics such as depth-based occlusion detection. In this work, we study how far D-only tracking can go if trained with large amounts of depth data. To compensate the lack of depth data, we generate depth maps for tracking. We train a "Depth-DiMP" from the scratch with the generated data and fine-tune it with the available small RGBD tracking datasets. The depth-only DiMP achieves good accuracy in depth-only tracking and combined with the original RGB DiMP the end-to-end trained RGBD-DiMP outperforms the recent VOT 2020 RGBD winners.



### Deep Two-Stream Video Inference for Human Body Pose and Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.11680v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.11680v1)
- **Published**: 2021-10-22 10:01:13+00:00
- **Updated**: 2021-10-22 10:01:13+00:00
- **Authors**: Ziwen Li, Bo Xu, Han Huang, Cheng Lu, Yandong Guo
- **Comment**: Accepted by WACV2022
- **Journal**: None
- **Summary**: Several video-based 3D pose and shape estimation algorithms have been proposed to resolve the temporal inconsistency of single-image-based methods. However it still remains challenging to have stable and accurate reconstruction. In this paper, we propose a new framework Deep Two-Stream Video Inference for Human Body Pose and Shape Estimation (DTS-VIBE), to generate 3D human pose and mesh from RGB videos. We reformulate the task as a multi-modality problem that fuses RGB and optical flow for more reliable estimation. In order to fully utilize both sensory modalities (RGB or optical flow), we train a two-stream temporal network based on transformer to predict SMPL parameters. The supplementary modality, optical flow, helps to maintain temporal consistency by leveraging motion knowledge between two consecutive frames. The proposed algorithm is extensively evaluated on the Human3.6 and 3DPW datasets. The experimental results show that it outperforms other state-of-the-art methods by a significant margin.



### Conditional Variational Autoencoder for Learned Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2110.11681v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2110.11681v2)
- **Published**: 2021-10-22 10:02:48+00:00
- **Updated**: 2021-10-25 01:10:52+00:00
- **Authors**: Chen Zhang, Riccardo Barbano, Bangti Jin
- **Comment**: 22 pages, preliminary version appeared as 1908.01010
- **Journal**: None
- **Summary**: Learned image reconstruction techniques using deep neural networks have recently gained popularity, and have delivered promising empirical results. However, most approaches focus on one single recovery for each observation, and thus neglect the uncertainty information. In this work, we develop a novel computational framework that approximates the posterior distribution of the unknown image at each query observation. The proposed framework is very flexible: It handles implicit noise models and priors, it incorporates the data formation process (i.e., the forward operator), and the learned reconstructive properties are transferable between different datasets. Once the network is trained using the conditional variational autoencoder loss, it provides a computationally efficient sampler for the approximate posterior distribution via feed-forward propagation, and the summarizing statistics of the generated samples are used for both point-estimation and uncertainty quantification. We illustrate the proposed framework with extensive numerical experiments on positron emission tomography (with both moderate and low count levels) showing that the framework generates high-quality samples when compared with state-of-the-art methods.



### Spoofing Detection on Hand Images Using Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2110.12923v1
- **DOI**: 10.1007/s11042-021-10976-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12923v1)
- **Published**: 2021-10-22 10:06:53+00:00
- **Updated**: 2021-10-22 10:06:53+00:00
- **Authors**: Asish Bera, Ratnadeep Dey, Debotosh Bhattacharjee, Mita Nasipuri, Hubert P. H. Shum
- **Comment**: None
- **Journal**: Multimedia Tools and Applications, Springer. 2021
- **Summary**: Recent research on biometrics focuses on achieving a high success rate of authentication and addressing the concern of various spoofing attacks. Although hand geometry recognition provides adequate security over unauthorized access, it is susceptible to presentation attack. This paper presents an anti-spoofing method toward hand biometrics. A presentation attack detection approach is addressed by assessing the visual quality of genuine and fake hand images. A threshold-based gradient magnitude similarity quality metric is proposed to discriminate between the real and spoofed hand samples. The visual hand images of 255 subjects from the Bogazici University hand database are considered as original samples. Correspondingly, from each genuine sample, we acquire a forged image using a Canon EOS 700D camera. Such fake hand images with natural degradation are considered for electronic screen display based spoofing attack detection. Furthermore, we create another fake hand dataset with artificial degradation by introducing additional Gaussian blur, salt and pepper, and speckle noises to original images. Ten quality metrics are measured from each sample for classification between original and fake hand image. The classification experiments are performed using the k-nearest neighbors, random forest, and support vector machine classifiers, as well as deep convolutional neural networks. The proposed gradient similarity-based quality metric achieves 1.5% average classification er ror using the k-nearest neighbors and random forest classifiers. An average classification error of 2.5% is obtained using the baseline evaluation with the MobileNetV2 deep network for discriminating original and different types of fake hand samples.



### Multimodal-Boost: Multimodal Medical Image Super-Resolution using Multi-Attention Network with Wavelet Transform
- **Arxiv ID**: http://arxiv.org/abs/2110.11684v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11684v2)
- **Published**: 2021-10-22 10:13:46+00:00
- **Updated**: 2022-03-12 13:33:55+00:00
- **Authors**: Fayaz Ali Dharejo, Muhammad Zawish, Farah Deeba Yuanchun Zhou, Kapal Dev, Sunder Ali Khowaja, Nawab Muhammad Faseeh Qureshi
- **Comment**: 14 pages, 13 Figures, and 3 Tables. Submitted to IEEE/ACM TCBB
- **Journal**: None
- **Summary**: Deep learning based single image super resolution (SISR) algorithms has revolutionized the overall diagnosis framework by continually improving the architectural components and training strategies associated with convolutional neural networks (CNN) on low-resolution images. However, existing work lacks in two ways: i) the SR output produced exhibits poor texture details, and often produce blurred edges, ii) most of the models have been developed for a single modality, hence, require modification to adapt to a new one. This work addresses (i) by proposing generative adversarial network (GAN) with deep multi-attention modules to learn high-frequency information from low-frequency data. Existing approaches based on the GAN have yielded good SR results; however, the texture details of their SR output have been experimentally confirmed to be deficient for medical images particularly. The integration of wavelet transform (WT) and GANs in our proposed SR model addresses the aforementioned limitation concerning textons. While the WT divides the LR image into multiple frequency bands, the transferred GAN uses multi-attention and upsample blocks to predict high-frequency components. Additionally, we present a learning method for training domain-specific classifiers as perceptual loss functions. Using a combination of multi-attention GAN loss and a perceptual loss function results in an efficient and reliable performance. Applying the same model for medical images from diverse modalities is challenging, our work addresses (ii) by training and performing on several modalities via transfer learning. Using two medical datasets, we validate our proposed SR network against existing state-of-the-art approaches and achieve promising results in terms of SSIM and PSNR.



### Adaptive Fusion Affinity Graph with Noise-free Online Low-rank Representation for Natural Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.11685v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.11685v1)
- **Published**: 2021-10-22 10:15:27+00:00
- **Updated**: 2021-10-22 10:15:27+00:00
- **Authors**: Yang Zhang, Moyun Liu, Huiming Zhang, Guodong Sun, Jingwu He
- **Comment**: 12 pages, 12 figures
- **Journal**: None
- **Summary**: Affinity graph-based segmentation methods have become a major trend in computer vision. The performance of these methods relies on the constructed affinity graph, with particular emphasis on the neighborhood topology and pairwise affinities among superpixels. Due to the advantages of assimilating different graphs, a multi-scale fusion graph has a better performance than a single graph with single-scale. However, these methods ignore the noise from images which influences the accuracy of pairwise similarities. Multi-scale combinatorial grouping and graph fusion also generate a higher computational complexity. In this paper, we propose an adaptive fusion affinity graph (AFA-graph) with noise-free low-rank representation in an online manner for natural image segmentation. An input image is first over-segmented into superpixels at different scales and then filtered by the proposed improved kernel density estimation method. Moreover, we select global nodes of these superpixels on the basis of their subspace-preserving presentation, which reveals the feature distribution of superpixels exactly. To reduce time complexity while improving performance, a sparse representation of global nodes based on noise-free online low-rank representation is used to obtain a global graph at each scale. The global graph is finally used to update a local graph which is built upon all superpixels at each scale. Experimental results on the BSD300, BSD500, MSRC, SBD, and PASCAL VOC show the effectiveness of AFA-graph in comparison with state-of-the-art approaches.



### BlendGAN: Implicitly GAN Blending for Arbitrary Stylized Face Generation
- **Arxiv ID**: http://arxiv.org/abs/2110.11728v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11728v1)
- **Published**: 2021-10-22 12:00:27+00:00
- **Updated**: 2021-10-22 12:00:27+00:00
- **Authors**: Mingcong Liu, Qiang Li, Zekui Qin, Guoxin Zhang, Pengfei Wan, Wen Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have made a dramatic leap in high-fidelity image synthesis and stylized face generation. Recently, a layer-swapping mechanism has been developed to improve the stylization performance. However, this method is incapable of fitting arbitrary styles in a single model and requires hundreds of style-consistent training images for each style. To address the above issues, we propose BlendGAN for arbitrary stylized face generation by leveraging a flexible blending strategy and a generic artistic dataset. Specifically, we first train a self-supervised style encoder on the generic artistic dataset to extract the representations of arbitrary styles. In addition, a weighted blending module (WBM) is proposed to blend face and style representations implicitly and control the arbitrary stylization effect. By doing so, BlendGAN can gracefully fit arbitrary styles in a unified model while avoiding case-by-case preparation of style-consistent training images. To this end, we also present a novel large-scale artistic face dataset AAHQ. Extensive experiments demonstrate that BlendGAN outperforms state-of-the-art methods in terms of visual quality and style diversity for both latent-guided and reference-guided stylized face synthesis.



### UBR$^2$S: Uncertainty-Based Resampling and Reweighting Strategy for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2110.11739v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11739v1)
- **Published**: 2021-10-22 12:18:40+00:00
- **Updated**: 2021-10-22 12:18:40+00:00
- **Authors**: Tobias Ringwald, Rainer Stiefelhagen
- **Comment**: Accepted at the 32nd British Machine Vision Conference (BMVC 2021)
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) deals with the adaptation process of a model to an unlabeled target domain while annotated data is only available for a given source domain. This poses a challenging task, as the domain shift between source and target instances deteriorates a model's performance when not addressed. In this paper, we propose UBR$^2$S - the Uncertainty-Based Resampling and Reweighting Strategy - to tackle this problem. UBR$^2$S employs a Monte Carlo dropout-based uncertainty estimate to obtain per-class probability distributions, which are then used for dynamic resampling of pseudo-labels and reweighting based on their sample likelihood and the accompanying decision error. Our proposed method achieves state-of-the-art results on multiple UDA datasets with single and multi-source adaptation tasks and can be applied to any off-the-shelf network architecture. Code for our method is available at https://gitlab.com/tringwald/UBR2S.



### Semantic Detection of Potential Wind-borne Debris in Construction Jobsites: Digital Twining for Hurricane Preparedness and Jobsite Safety
- **Arxiv ID**: http://arxiv.org/abs/2110.12968v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.12968v1)
- **Published**: 2021-10-22 12:26:25+00:00
- **Updated**: 2021-10-22 12:26:25+00:00
- **Authors**: Mirsalar Kamari, Youngjib Ham
- **Comment**: This paper has been accepted in i3CE(2021) conference
- **Journal**: International Conference on Computing in Civil Engineering
  (i3CE2021)
- **Summary**: In the United States, hurricanes are the most devastating natural disasters causing billions of dollars worth of damage every year. More importantly, construction jobsites are classified among the most vulnerable environments to severe wind events. During hurricanes, unsecured and incomplete elements of construction sites, such as scaffoldings, plywoods, and metal rods, will become the potential wind-borne debris, causing cascading damages to the construction projects and the neighboring communities. Thus, it is no wonder that construction firms implement jobsite emergency plans to enforce preparedness responses before extreme weather events. However, relying on checklist-based emergency action plans to carry out a thorough hurricane preparedness is challenging in large-scale and complex site environments. For enabling systematic responses for hurricane preparedness, we have proposed a vision-based technique to identify and analyze the potential wind-borne debris in construction jobsites. Building on this, this paper demonstrates the fidelity of a new machine vision-based method to support construction site hurricane preparedness and further discuss its implications. The outcomes indicate that the convenience of visual data collection and the advantages of the machine vision-based frameworks enable rapid scene understanding and thus, provide critical heads up for practitioners to recognize and localize the potential wind-borne derbies in construction jobsites and effectively implement hurricane preparedness.



### Few-shot Semantic Segmentation with Self-supervision from Pseudo-classes
- **Arxiv ID**: http://arxiv.org/abs/2110.11742v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.11742v1)
- **Published**: 2021-10-22 12:32:36+00:00
- **Updated**: 2021-10-22 12:32:36+00:00
- **Authors**: Yiwen Li, Gratianus Wesley Putra Data, Yunguan Fu, Yipeng Hu, Victor Adrian Prisacariu
- **Comment**: To appear in the proceedings of the British Machine Vision Conference
  (BMVC) 2021
- **Journal**: None
- **Summary**: Despite the success of deep learning methods for semantic segmentation, few-shot semantic segmentation remains a challenging task due to the limited training data and the generalisation requirement for unseen classes. While recent progress has been particularly encouraging, we discover that existing methods tend to have poor performance in terms of meanIoU when query images contain other semantic classes besides the target class. To address this issue, we propose a novel self-supervised task that generates random pseudo-classes in the background of the query images, providing extra training data that would otherwise be unavailable when predicting individual target classes. To that end, we adopted superpixel segmentation for generating the pseudo-classes. With this extra supervision, we improved the meanIoU performance of the state-of-the-art method by 2.5% and 5.1% on the one-shot tasks, as well as 6.7% and 4.4% on the five-shot tasks, on the PASCAL-5i and COCO benchmarks, respectively.



### Creating and Reenacting Controllable 3D Humans with Differentiable Rendering
- **Arxiv ID**: http://arxiv.org/abs/2110.11746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11746v1)
- **Published**: 2021-10-22 12:40:09+00:00
- **Updated**: 2021-10-22 12:40:09+00:00
- **Authors**: Thiago L. Gomes, Thiago M. Coutinho, Rafael Azevedo, Renato Martins, Erickson R. Nascimento
- **Comment**: 10 pages, 6 figures, to appear in Proceedings of the IEEE Winter
  Conference on Applications of Computer Vision (WACV) 2022
- **Journal**: None
- **Summary**: This paper proposes a new end-to-end neural rendering architecture to transfer appearance and reenact human actors. Our method leverages a carefully designed graph convolutional network (GCN) to model the human body manifold structure, jointly with differentiable rendering, to synthesize new videos of people in different contexts from where they were initially recorded. Unlike recent appearance transferring methods, our approach can reconstruct a fully controllable 3D texture-mapped model of a person, while taking into account the manifold structure from body shape and texture appearance in the view synthesis. Specifically, our approach models mesh deformations with a three-stage GCN trained in a self-supervised manner on rendered silhouettes of the human body. It also infers texture appearance with a convolutional network in the texture domain, which is trained in an adversarial regime to reconstruct human texture from rendered images of actors in different poses. Experiments on different videos show that our method successfully infers specific body deformations and avoid creating texture artifacts while achieving the best values for appearance in terms of Structural Similarity (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), Mean Squared Error (MSE), and Fr\'echet Video Distance (FVD). By taking advantages of both differentiable rendering and the 3D parametric model, our method is fully controllable, which allows controlling the human synthesis from both pose and rendering parameters. The source code is available at https://www.verlab.dcc.ufmg.br/retargeting-motion/wacv2022.



### Exploiting Cross-Modal Prediction and Relation Consistency for Semi-Supervised Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2110.11767v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.11767v2)
- **Published**: 2021-10-22 13:14:32+00:00
- **Updated**: 2021-10-28 14:16:16+00:00
- **Authors**: Yang Yang, Hongchen Wei, Hengshu Zhu, Dianhai Yu, Hui Xiong, Jian Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The task of image captioning aims to generate captions directly from images via the automatically learned cross-modal generator. To build a well-performing generator, existing approaches usually need a large number of described images, which requires a huge effects on manual labeling. However, in real-world applications, a more general scenario is that we only have limited amount of described images and a large number of undescribed images. Therefore, a resulting challenge is how to effectively combine the undescribed images into the learning of cross-modal generator. To solve this problem, we propose a novel image captioning method by exploiting the Cross-modal Prediction and Relation Consistency (CPRC), which aims to utilize the raw image input to constrain the generated sentence in the commonly semantic space. In detail, considering that the heterogeneous gap between modalities always leads to the supervision difficulty of using the global embedding directly, CPRC turns to transform both the raw image and corresponding generated sentence into the shared semantic space, and measure the generated sentence from two aspects: 1) Prediction consistency. CPRC utilizes the prediction of raw image as soft label to distill useful supervision for the generated sentence, rather than employing the traditional pseudo labeling; 2) Relation consistency. CPRC develops a novel relation consistency between augmented images and corresponding generated sentences to retain the important relational knowledge. In result, CPRC supervises the generated sentence from both the informativeness and representativeness perspectives, and can reasonably use the undescribed images to learn a more effective generator under the semi-supervised scenario.



### Domain Adaptation and Active Learning for Fine-Grained Recognition in the Field of Biodiversity
- **Arxiv ID**: http://arxiv.org/abs/2110.11778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11778v1)
- **Published**: 2021-10-22 13:34:13+00:00
- **Updated**: 2021-10-22 13:34:13+00:00
- **Authors**: Bernd Gruner, Matthias Körschens, Björn Barz, Joachim Denzler
- **Comment**: https://sites.google.com/view/clvision2021/call-for-papers/accepted-papers
- **Journal**: None
- **Summary**: Deep-learning methods offer unsurpassed recognition performance in a wide range of domains, including fine-grained recognition tasks. However, in most problem areas there are insufficient annotated training samples. Therefore, the topic of transfer learning respectively domain adaptation is particularly important. In this work, we investigate to what extent unsupervised domain adaptation can be used for fine-grained recognition in a biodiversity context to learn a real-world classifier based on idealized training data, e.g. preserved butterflies and plants. Moreover, we investigate the influence of different normalization layers, such as Group Normalization in combination with Weight Standardization, on the classifier. We discovered that domain adaptation works very well for fine-grained recognition and that the normalization methods have a great influence on the results. Using domain adaptation and Transferable Normalization, the accuracy of the classifier could be increased by up to 12.35 % compared to the baseline. Furthermore, the domain adaptation system is combined with an active learning component to improve the results. We compare different active learning strategies with each other. Surprisingly, we found that more sophisticated strategies provide better results than the random selection baseline for only one of the two datasets. In this case, the distance and diversity strategy performed best. Finally, we present a problem analysis of the datasets.



### SwiftLane: Towards Fast and Efficient Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.11779v1
- **DOI**: 10.1109/ICMLA52953.2021.00142
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11779v1)
- **Published**: 2021-10-22 13:35:05+00:00
- **Updated**: 2021-10-22 13:35:05+00:00
- **Authors**: Oshada Jayasinghe, Damith Anhettigama, Sahan Hemachandra, Shenali Kariyawasam, Ranga Rodrigo, Peshala Jayasekara
- **Comment**: Accepted to 20th IEEE International Conference on Machine Learning
  and Applications (ICMLA) 2021
- **Journal**: None
- **Summary**: Recent work done on lane detection has been able to detect lanes accurately in complex scenarios, yet many fail to deliver real-time performance specifically with limited computational resources. In this work, we propose SwiftLane: a simple and light-weight, end-to-end deep learning based framework, coupled with the row-wise classification formulation for fast and efficient lane detection. This framework is supplemented with a false positive suppression algorithm and a curve fitting technique to further increase the accuracy. Our method achieves an inference speed of 411 frames per second, surpassing state-of-the-art in terms of speed while achieving comparable results in terms of accuracy on the popular CULane benchmark dataset. In addition, our proposed framework together with TensorRT optimization facilitates real-time lane detection on a Nvidia Jetson AGX Xavier as an embedded system while achieving a high inference speed of 56 frames per second.



### Federated Unlearning via Class-Discriminative Pruning
- **Arxiv ID**: http://arxiv.org/abs/2110.11794v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.11794v3)
- **Published**: 2021-10-22 14:01:42+00:00
- **Updated**: 2022-01-29 03:06:56+00:00
- **Authors**: Junxiao Wang, Song Guo, Xin Xie, Heng Qi
- **Comment**: WWW2022
- **Journal**: None
- **Summary**: We explore the problem of selectively forgetting categories from trained CNN classification models in the federated learning (FL). Given that the data used for training cannot be accessed globally in FL, our insights probe deep into the internal influence of each channel. Through the visualization of feature maps activated by different channels, we observe that different channels have a varying contribution to different categories in image classification. Inspired by this, we propose a method for scrubbing the model clean of information about particular categories. The method does not require retraining from scratch, nor global access to the data used for training. Instead, we introduce the concept of Term Frequency Inverse Document Frequency (TF-IDF) to quantize the class discrimination of channels. Channels with high TF-IDF scores have more discrimination on the target categories and thus need to be pruned to unlearn. The channel pruning is followed by a fine-tuning process to recover the performance of the pruned model. Evaluated on CIFAR10 dataset, our method accelerates the speed of unlearning by 8.9x for the ResNet model, and 7.9x for the VGG model under no degradation in accuracy, compared to retraining from scratch. For CIFAR100 dataset, the speedups are 9.9x and 8.4x, respectively. We envision this work as a complementary block for FL towards compliance with legal and ethical criteria.



### HDRVideo-GAN: Deep Generative HDR Video Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2110.11795v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11795v2)
- **Published**: 2021-10-22 14:02:03+00:00
- **Updated**: 2021-11-03 07:35:48+00:00
- **Authors**: Mrinal Anand, Nidhin Harilal, Chandan Kumar, Shanmuganathan Raman
- **Comment**: In Proceedings of 12th Indian Conference on Computer Vision, Graphics
  and Image Processing (ICVGIP-21)
- **Journal**: None
- **Summary**: High dynamic range (HDR) videos provide a more visually realistic experience than the standard low dynamic range (LDR) videos. Despite having significant progress in HDR imaging, it is still a challenging task to capture high-quality HDR video with a conventional off-the-shelf camera. Existing approaches rely entirely on using dense optical flow between the neighboring LDR sequences to reconstruct an HDR frame. However, they lead to inconsistencies in color and exposure over time when applied to alternating exposures with noisy frames. In this paper, we propose an end-to-end GAN-based framework for HDR video reconstruction from LDR sequences with alternating exposures. We first extract clean LDR frames from noisy LDR video with alternating exposures with a denoising network trained in a self-supervised setting. Using optical flow, we then align the neighboring alternating-exposure frames to a reference frame and then reconstruct high-quality HDR frames in a complete adversarial setting. To further improve the robustness and quality of generated frames, we incorporate temporal stability-based regularization term along with content and style-based losses in the cost function during the training procedure. Experimental results demonstrate that our framework achieves state-of-the-art performance and generates superior quality HDR frames of a video over the existing methods.



### PropMix: Hard Sample Filtering and Proportional MixUp for Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2110.11809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11809v1)
- **Published**: 2021-10-22 14:27:37+00:00
- **Updated**: 2021-10-22 14:27:37+00:00
- **Authors**: Filipe R. Cordeiro, Vasileios Belagiannis, Ian Reid, Gustavo Carneiro
- **Comment**: Paper accepted at BMVC'21: The 32nd British Machine Vision Conference
- **Journal**: None
- **Summary**: The most competitive noisy label learning methods rely on an unsupervised classification of clean and noisy samples, where samples classified as noisy are re-labelled and "MixMatched" with the clean samples. These methods have two issues in large noise rate problems: 1) the noisy set is more likely to contain hard samples that are in-correctly re-labelled, and 2) the number of samples produced by MixMatch tends to be reduced because it is constrained by the small clean set size. In this paper, we introduce the learning algorithm PropMix to handle the issues above. PropMix filters out hard noisy samples, with the goal of increasing the likelihood of correctly re-labelling the easy noisy samples. Also, PropMix places clean and re-labelled easy noisy samples in a training set that is augmented with MixUp, removing the clean set size constraint and including a large proportion of correctly re-labelled easy noisy samples. We also include self-supervised pre-training to improve robustness to high noisy label scenarios. Our experiments show that PropMix has state-of-the-art (SOTA) results on CIFAR-10/-100(with symmetric, asymmetric and semantic label noise), Red Mini-ImageNet (from the Controlled Noisy Web Labels), Clothing1M and WebVision. In severe label noise bench-marks, our results are substantially better than other methods. The code is available athttps://github.com/filipe-research/PropMix.



### IVS3D: An Open Source Framework for Intelligent Video Sampling and Preprocessing to Facilitate 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2110.11810v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11810v1)
- **Published**: 2021-10-22 14:31:04+00:00
- **Updated**: 2021-10-22 14:31:04+00:00
- **Authors**: Max Hermann, Thomas Pollok, Daniel Brommer, Dominic Zahn
- **Comment**: Accepted for the 16th International Symposium on Visual Computing
  (ISVC 2021)
- **Journal**: None
- **Summary**: The creation of detailed 3D models is relevant for a wide range of applications such as navigation in three-dimensional space, construction planning or disaster assessment. However, the complex processing and long execution time for detailed 3D reconstructions require the original database to be reduced in order to obtain a result in reasonable time. In this paper we therefore present our framework iVS3D for intelligent pre-processing of image sequences. Our software is able to down sample entire videos to a specific frame rate, as well as to resize and crop the individual images. Furthermore, thanks to our modular architecture, it is easy to develop and integrate plugins with additional algorithms. We provide three plugins as baseline methods that enable an intelligent selection of suitable images and can enrich them with additional information. To filter out images affected by motion blur, we developed a plugin that detects these frames and also searches the spatial neighbourhood for suitable images as replacements. The second plugin uses optical flow to detect redundant images caused by a temporarily stationary camera. In our experiments, we show how this approach leads to a more balanced image sampling if the camera speed varies, and that excluding such redundant images leads to a time saving of 8.1\percent for our sequences. A third plugin makes it possible to exclude challenging image regions from the 3D reconstruction by performing semantic segmentation. As we think that the community can greatly benefit from such an approach, we will publish our framework and the developed plugins open source using the MIT licence to allow co-development and easy extension.



### CNN-based Omnidirectional Object Detection for HermesBot Autonomous Delivery Robot with Preliminary Frame Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.11829v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11829v1)
- **Published**: 2021-10-22 15:05:37+00:00
- **Updated**: 2021-10-22 15:05:37+00:00
- **Authors**: Saian Protasov, Pavel Karpyshev, Ivan Kalinov, Pavel Kopanev, Nikita Mikhailovskiy, Alexander Sedunin, Dzmitry Tsetserukou
- **Comment**: Accepted to IEEE 20th International Conference on Advanced Robotics
  (ICAR) 2021, 6 pages, 7 figures
- **Journal**: None
- **Summary**: Mobile autonomous robots include numerous sensors for environment perception. Cameras are an essential tool for robot's localization, navigation, and obstacle avoidance. To process a large flow of data from the sensors, it is necessary to optimize algorithms, or to utilize substantial computational power. In our work, we propose an algorithm for optimizing a neural network for object detection using preliminary binary frame classification. An autonomous outdoor mobile robot with 6 rolling-shutter cameras on the perimeter providing a 360-degree field of view was used as the experimental setup. The obtained experimental results revealed that the proposed optimization accelerates the inference time of the neural network in the cases with up to 5 out of 6 cameras containing target objects.



### Multi-attribute Pizza Generator: Cross-domain Attribute Control with Conditional StyleGAN
- **Arxiv ID**: http://arxiv.org/abs/2110.11830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11830v1)
- **Published**: 2021-10-22 15:07:06+00:00
- **Updated**: 2021-10-22 15:07:06+00:00
- **Authors**: Fangda Han, Guoyao Hao, Ricardo Guerrero, Vladimir Pavlovic
- **Comment**: To appear in British Machine Vision Conference (BMVC) 2021. arXiv
  admin note: text overlap with arXiv:2012.02821
- **Journal**: None
- **Summary**: Multi-attribute conditional image generation is a challenging problem in computervision. We propose Multi-attribute Pizza Generator (MPG), a conditional Generative Neural Network (GAN) framework for synthesizing images from a trichotomy of attributes: content, view-geometry, and implicit visual style. We design MPG by extending the state-of-the-art StyleGAN2, using a new conditioning technique that guides the intermediate feature maps to learn multi-scale multi-attribute entangled representationsof controlling attributes. Because of the complex nature of the multi-attribute image generation problem, we regularize the image generation by predicting the explicit conditioning attributes (ingredients and view). To synthesize a pizza image with view attributesoutside the range of natural training images, we design a CGI pizza dataset PizzaView using 3D pizza models and employ it to train a view attribute regressor to regularize the generation process, bridging the real and CGI training datasets. To verify the efficacy of MPG, we test it on Pizza10, a carefully annotated multi-ingredient pizza image dataset. MPG can successfully generate photo-realistic pizza images with desired ingredients and view attributes, beyond the range of those observed in real-world training data.



### Multi-view Contrastive Graph Clustering
- **Arxiv ID**: http://arxiv.org/abs/2110.11842v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2110.11842v1)
- **Published**: 2021-10-22 15:22:42+00:00
- **Updated**: 2021-10-22 15:22:42+00:00
- **Authors**: Erlin Pan, Zhao Kang
- **Comment**: Accepted by NeurIPS 2021
- **Journal**: None
- **Summary**: With the explosive growth of information technology, multi-view graph data have become increasingly prevalent and valuable. Most existing multi-view clustering techniques either focus on the scenario of multiple graphs or multi-view attributes. In this paper, we propose a generic framework to cluster multi-view attributed graph data. Specifically, inspired by the success of contrastive learning, we propose multi-view contrastive graph clustering (MCGC) method to learn a consensus graph since the original graph could be noisy or incomplete and is not directly applicable. Our method composes of two key steps: we first filter out the undesirable high-frequency noise while preserving the graph geometric features via graph filtering and obtain a smooth representation of nodes; we then learn a consensus graph regularized by graph contrastive loss. Results on several benchmark datasets show the superiority of our method with respect to state-of-the-art approaches. In particular, our simple approach outperforms existing deep learning-based methods.



### Bayesian Optimization and Deep Learning forsteering wheel angle prediction
- **Arxiv ID**: http://arxiv.org/abs/2110.13629v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2110.13629v1)
- **Published**: 2021-10-22 15:25:14+00:00
- **Updated**: 2021-10-22 15:25:14+00:00
- **Authors**: Alessandro Riboni, Nicolò Ghioldi, Antonio Candelieri, Matteo Borrotti
- **Comment**: None
- **Journal**: None
- **Summary**: Automated driving systems (ADS) have undergone a significant improvement in the last years. ADS and more precisely self-driving cars technologies will change the way we perceive and know the world of transportation systems in terms of user experience, mode choices and business models. The emerging field of Deep Learning (DL) has been successfully applied for the development of innovative ADS solutions. However, the attempt to single out the best deep neural network architecture and tuning its hyperparameters are all expensive processes, both in terms of time and computational resources. In this work, Bayesian Optimization (BO) is used to optimize the hyperparameters of a Spatiotemporal-Long Short Term Memory (ST-LSTM) network with the aim to obtain an accurate model for the prediction of the steering angle in a ADS. BO was able to identify, within a limited number of trials, a model -- namely BOST-LSTM -- which resulted, on a public dataset, the most accurate when compared to classical end-to-end driving models.



### Recurrence along Depth: Deep Convolutional Neural Networks with Recurrent Layer Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2110.11852v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11852v1)
- **Published**: 2021-10-22 15:36:33+00:00
- **Updated**: 2021-10-22 15:36:33+00:00
- **Authors**: Jingyu Zhao, Yanwen Fang, Guodong Li
- **Comment**: Accepted by NeurIPS 2021
- **Journal**: None
- **Summary**: This paper introduces a concept of layer aggregation to describe how information from previous layers can be reused to better extract features at the current layer. While DenseNet is a typical example of the layer aggregation mechanism, its redundancy has been commonly criticized in the literature. This motivates us to propose a very light-weighted module, called recurrent layer aggregation (RLA), by making use of the sequential structure of layers in a deep CNN. Our RLA module is compatible with many mainstream deep CNNs, including ResNets, Xception and MobileNetV2, and its effectiveness is verified by our extensive experiments on image classification, object detection and instance segmentation tasks. Specifically, improvements can be uniformly observed on CIFAR, ImageNet and MS COCO datasets, and the corresponding RLA-Nets can surprisingly boost the performances by 2-3% on the object detection task. This evidences the power of our RLA module in helping main CNNs better learn structural information in images.



### AIR-Nets: An Attention-Based Framework for Locally Conditioned Implicit Representations
- **Arxiv ID**: http://arxiv.org/abs/2110.11860v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.11860v1)
- **Published**: 2021-10-22 15:48:31+00:00
- **Updated**: 2021-10-22 15:48:31+00:00
- **Authors**: Simon Giebenhain, Bastian Goldlücke
- **Comment**: Project code: https://github.com/SimonGiebenhain/AIR-Nets
- **Journal**: None
- **Summary**: This paper introduces Attentive Implicit Representation Networks (AIR-Nets), a simple, but highly effective architecture for 3D reconstruction from point clouds. Since representing 3D shapes in a local and modular fashion increases generalization and reconstruction quality, AIR-Nets encode an input point cloud into a set of local latent vectors anchored in 3D space, which locally describe the object's geometry, as well as a global latent description, enforcing global consistency. Our model is the first grid-free, encoder-based approach that locally describes an implicit function. The vector attention mechanism from [Zhao et al. 2020] serves as main point cloud processing module, and allows for permutation invariance and translation equivariance. When queried with a 3D coordinate, our decoder gathers information from the global and nearby local latent vectors in order to predict an occupancy value. Experiments on the ShapeNet dataset show that AIR-Nets significantly outperform previous state-of-the-art encoder-based, implicit shape learning methods and especially dominate in the sparse setting. Furthermore, our model generalizes well to the FAUST dataset in a zero-shot setting. Finally, since AIR-Nets use a sparse latent representation and follow a simple operating scheme, the model offers several exiting avenues for future work. Our code is available at https://github.com/SimonGiebenhain/AIR-Nets.



### CeyMo: See More on Roads -- A Novel Benchmark Dataset for Road Marking Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.11867v3
- **DOI**: 10.1109/WACV51458.2022.00344
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11867v3)
- **Published**: 2021-10-22 15:56:17+00:00
- **Updated**: 2022-05-03 05:27:37+00:00
- **Authors**: Oshada Jayasinghe, Sahan Hemachandra, Damith Anhettigama, Shenali Kariyawasam, Ranga Rodrigo, Peshala Jayasekara
- **Comment**: Accepted to 2022 IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV 2022)
- **Journal**: None
- **Summary**: In this paper, we introduce a novel road marking benchmark dataset for road marking detection, addressing the limitations in the existing publicly available datasets such as lack of challenging scenarios, prominence given to lane markings, unavailability of an evaluation script, lack of annotation formats and lower resolutions. Our dataset consists of 2887 total images with 4706 road marking instances belonging to 11 classes. The images have a high resolution of 1920 x 1080 and capture a wide range of traffic, lighting and weather conditions. We provide road marking annotations in polygons, bounding boxes and pixel-level segmentation masks to facilitate a diverse range of road marking detection algorithms. The evaluation metrics and the evaluation script we provide, will further promote direct comparison of novel approaches for road marking detection with existing methods. Furthermore, we evaluate the effectiveness of using both instance segmentation and object detection based approaches for the road marking detection task. Speed and accuracy scores for two instance segmentation models and two object detector models are provided as a performance baseline for our benchmark dataset. The dataset and the evaluation script is publicly available at https://github.com/oshadajay/CeyMo.



### Simple Dialogue System with AUDITED
- **Arxiv ID**: http://arxiv.org/abs/2110.11881v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2110.11881v1)
- **Published**: 2021-10-22 16:07:16+00:00
- **Updated**: 2021-10-22 16:07:16+00:00
- **Authors**: Yusuf Tas, Piotr Koniusz
- **Comment**: Accepted by the BMVC 2021
- **Journal**: None
- **Summary**: We devise a multimodal conversation system for dialogue utterances composed of text, image or both modalities. We leverage Auxiliary UnsuperviseD vIsual and TExtual Data (AUDITED). To improve the performance of text-based task, we utilize translations of target sentences from English to French to form the assisted supervision. For the image-based task, we employ the DeepFashion dataset in which we seek nearest neighbor images of positive and negative target images of the MMD data. These nearest neighbors form the nearest neighbor embedding providing an external context for target images. We form two methods to create neighbor embedding vectors, namely Neighbor Embedding by Hard Assignment (NEHA) and Neighbor Embedding by Soft Assignment (NESA) which generate context subspaces per target image. Subsequently, these subspaces are learnt by our pipeline as a context for the target data. We also propose a discriminator which switches between the image- and text-based tasks. We show improvements over baselines on the large-scale Multimodal Dialogue Dataset (MMD) and SIMMC.



### C$^{4}$Net: Contextual Compression and Complementary Combination Network for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.11887v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.11887v1)
- **Published**: 2021-10-22 16:14:10+00:00
- **Updated**: 2021-10-22 16:14:10+00:00
- **Authors**: Hazarapet Tunanyan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning solutions of the salient object detection problem have achieved great results in recent years. The majority of these models are based on encoders and decoders, with a different multi-feature combination. In this paper, we show that feature concatenation works better than other combination methods like multiplication or addition. Also, joint feature learning gives better results, because of the information sharing during their processing. We designed a Complementary Extraction Module (CEM) to extract necessary features with edge preservation. Our proposed Excessiveness Loss (EL) function helps to reduce false-positive predictions and purifies the edges with other weighted loss functions. Our designed Pyramid-Semantic Module (PSM) with Global guiding flow (G) makes the prediction more accurate by providing high-level complementary information to shallower layers. Experimental results show that the proposed model outperforms the state-of-the-art methods on all benchmark datasets under three evaluation metrics.



### Challenges in Procedural Multimodal Machine Comprehension:A Novel Way To Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2110.11899v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2110.11899v1)
- **Published**: 2021-10-22 16:33:57+00:00
- **Updated**: 2021-10-22 16:33:57+00:00
- **Authors**: Pritish Sahu, Karan Sikka, Ajay Divakaran
- **Comment**: None
- **Journal**: None
- **Summary**: We focus on Multimodal Machine Reading Comprehension (M3C) where a model is expected to answer questions based on given passage (or context), and the context and the questions can be in different modalities. Previous works such as RecipeQA have proposed datasets and cloze-style tasks for evaluation. However, we identify three critical biases stemming from the question-answer generation process and memorization capabilities of large deep models. These biases makes it easier for a model to overfit by relying on spurious correlations or naive data patterns. We propose a systematic framework to address these biases through three Control-Knobs that enable us to generate a test bed of datasets of progressive difficulty levels. We believe that our benchmark (referred to as Meta-RecipeQA) will provide, for the first time, a fine grained estimate of a model's generalization capabilities. We also propose a general M3C model that is used to realize several prior SOTA models and motivate a novel hierarchical transformer based reasoning network (HTRN). We perform a detailed evaluation of these models with different language and visual features on our benchmark. We observe a consistent improvement with HTRN over SOTA (~18% in Visual Cloze task and ~13% in average over all the tasks). We also observe a drop in performance across all the models when testing on RecipeQA and proposed Meta-RecipeQA (e.g. 83.6% versus 67.1% for HTRN), which shows that the proposed dataset is relatively less biased. We conclude by highlighting the impact of the control knobs with some quantitative results.



### MIGS: Meta Image Generation from Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/2110.11918v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.11918v1)
- **Published**: 2021-10-22 17:02:44+00:00
- **Updated**: 2021-10-22 17:02:44+00:00
- **Authors**: Azade Farshad, Sabrina Musatian, Helisa Dhamo, Nassir Navab
- **Comment**: Accepted at BMVC 2021
- **Journal**: None
- **Summary**: Generation of images from scene graphs is a promising direction towards explicit scene generation and manipulation. However, the images generated from the scene graphs lack quality, which in part comes due to high difficulty and diversity in the data. We propose MIGS (Meta Image Generation from Scene Graphs), a meta-learning based approach for few-shot image generation from graphs that enables adapting the model to different scenes and increases the image quality by training on diverse sets of tasks. By sampling the data in a task-driven fashion, we train the generator using meta-learning on different sets of tasks that are categorized based on the scene attributes. Our results show that using this meta-learning approach for the generation of images from scene graphs achieves state-of-the-art performance in terms of image quality and capturing the semantic relationships in the scene. Project Website: https://migs2021.github.io/



### Logical Activation Functions: Logit-space equivalents of Probabilistic Boolean Operators
- **Arxiv ID**: http://arxiv.org/abs/2110.11940v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11940v2)
- **Published**: 2021-10-22 17:49:42+00:00
- **Updated**: 2022-11-29 09:30:48+00:00
- **Authors**: Scott C. Lowe, Robert Earle, Jason d'Eon, Thomas Trappenberg, Sageev Oore
- **Comment**: None
- **Journal**: Neural Information Processing Systems (2022)
- **Summary**: The choice of activation functions and their motivation is a long-standing issue within the neural network community. Neuronal representations within artificial neural networks are commonly understood as logits, representing the log-odds score of presence of features within the stimulus. We derive logit-space operators equivalent to probabilistic Boolean logic-gates AND, OR, and XNOR for independent probabilities. Such theories are important to formalize more complex dendritic operations in real neurons, and these operations can be used as activation functions within a neural network, introducing probabilistic Boolean-logic as the core operation of the neural network. Since these functions involve taking multiple exponents and logarithms, they are computationally expensive and not well suited to be directly used within neural networks. Consequently, we construct efficient approximations named $\text{AND}_\text{AIL}$ (the AND operator Approximate for Independent Logits), $\text{OR}_\text{AIL}$, and $\text{XNOR}_\text{AIL}$, which utilize only comparison and addition operations, have well-behaved gradients, and can be deployed as activation functions in neural networks. Like MaxOut, $\text{AND}_\text{AIL}$ and $\text{OR}_\text{AIL}$ are generalizations of ReLU to two-dimensions. While our primary aim is to formalize dendritic computations within a logit-space probabilistic-Boolean framework, we deploy these new activation functions, both in isolation and in conjunction to demonstrate their effectiveness on a variety of tasks including image classification, transfer learning, abstract reasoning, and compositional zero-shot learning.



### SOFT: Softmax-free Transformer with Linear Complexity
- **Arxiv ID**: http://arxiv.org/abs/2110.11945v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.11945v3)
- **Published**: 2021-10-22 17:57:29+00:00
- **Updated**: 2022-04-30 11:34:05+00:00
- **Authors**: Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, Li Zhang
- **Comment**: NeurIPS 2021 Spotlight. Project page at
  https://fudan-zvg.github.io/SOFT/
- **Journal**: None
- **Summary**: Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.



### Learning Proposals for Practical Energy-Based Regression
- **Arxiv ID**: http://arxiv.org/abs/2110.11948v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.11948v1)
- **Published**: 2021-10-22 17:58:05+00:00
- **Updated**: 2021-10-22 17:58:05+00:00
- **Authors**: Fredrik K. Gustafsson, Martin Danelljan, Thomas B. Schön
- **Comment**: Code is available at https://github.com/fregu856/ebms_proposals
- **Journal**: None
- **Summary**: Energy-based models (EBMs) have experienced a resurgence within machine learning in recent years, including as a promising alternative for probabilistic regression. However, energy-based regression requires a proposal distribution to be manually designed for training, and an initial estimate has to be provided at test-time. We address both of these issues by introducing a conceptually simple method to automatically learn an effective proposal distribution, which is parameterized by a separate network head. To this end, we derive a surprising result, leading to a unified training objective that jointly minimizes the KL divergence from the proposal to the EBM, and the negative log-likelihood of the EBM. At test-time, we can then employ importance sampling with the trained proposal to efficiently evaluate the learned EBM and produce stand-alone predictions. Furthermore, we utilize our derived training objective to learn mixture density networks (MDNs) with a jointly trained energy-based teacher, consistently outperforming conventional MDN training on four real-world regression tasks within computer vision. Code is available at https://github.com/fregu856/ebms_proposals.



### PhotoWCT$^2$: Compact Autoencoder for Photorealistic Style Transfer Resulting from Blockwise Training and Skip Connections of High-Frequency Residuals
- **Arxiv ID**: http://arxiv.org/abs/2110.11995v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11995v1)
- **Published**: 2021-10-22 18:20:41+00:00
- **Updated**: 2021-10-22 18:20:41+00:00
- **Authors**: Tai-Yin Chiu, Danna Gurari
- **Comment**: None
- **Journal**: None
- **Summary**: Photorealistic style transfer is an image editing task with the goal to modify an image to match the style of another image while ensuring the result looks like a real photograph. A limitation of existing models is that they have many parameters, which in turn prevents their use for larger image resolutions and leads to slower run-times. We introduce two mechanisms that enable our design of a more compact model that we call PhotoWCT$^2$, which preserves state-of-art stylization strength and photorealism. First, we introduce blockwise training to perform coarse-to-fine feature transformations that enable state-of-art stylization strength in a single autoencoder in place of the inefficient cascade of four autoencoders used in PhotoWCT. Second, we introduce skip connections of high-frequency residuals in order to preserve image quality when applying the sequential coarse-to-fine feature transformations. Our PhotoWCT$^2$ model requires fewer parameters (e.g., 30.3\% fewer) while supporting higher resolution images (e.g., 4K) and achieving faster stylization than existing models.



### Semi-Supervised Semantic Segmentation of Vessel Images using Leaking Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2110.11998v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11998v1)
- **Published**: 2021-10-22 18:25:08+00:00
- **Updated**: 2021-10-22 18:25:08+00:00
- **Authors**: Jinyong Hou, Xuejie Ding, Jeremiah D. Deng
- **Comment**: To appear in WACV'22
- **Journal**: None
- **Summary**: Semantic segmentation based on deep learning methods can attain appealing accuracy provided large amounts of annotated samples. However, it remains a challenging task when only limited labelled data are available, which is especially common in medical imaging. In this paper, we propose to use Leaking GAN, a GAN-based semi-supervised architecture for retina vessel semantic segmentation. Our key idea is to pollute the discriminator by leaking information from the generator. This leads to more moderate generations that benefit the training of GAN. As a result, the unlabelled examples can be better utilized to boost the learning of the discriminator, which eventually leads to stronger classification performance. In addition, to overcome the variations in medical images, the mean-teacher mechanism is utilized as an auxiliary regularization of the discriminator. Further, we modify the focal loss to fit it as the consistency objective for mean-teacher regularizer. Extensive experiments demonstrate that the Leaking GAN framework achieves competitive performance compared to the state-of-the-art methods when evaluated on benchmark datasets including DRIVE, STARE and CHASE\_DB1, using as few as 8 labelled images in the semi-supervised setting. It also outperforms existing algorithms on cross-domain segmentation tasks.



### When to Prune? A Policy towards Early Structural Pruning
- **Arxiv ID**: http://arxiv.org/abs/2110.12007v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12007v1)
- **Published**: 2021-10-22 18:39:22+00:00
- **Updated**: 2021-10-22 18:39:22+00:00
- **Authors**: Maying Shen, Pavlo Molchanov, Hongxu Yin, Jose M. Alvarez
- **Comment**: None
- **Journal**: None
- **Summary**: Pruning enables appealing reductions in network memory footprint and time complexity. Conventional post-training pruning techniques lean towards efficient inference while overlooking the heavy computation for training. Recent exploration of pre-training pruning at initialization hints on training cost reduction via pruning, but suffers noticeable performance degradation. We attempt to combine the benefits of both directions and propose a policy that prunes as early as possible during training without hurting performance. Instead of pruning at initialization, our method exploits initial dense training for few epochs to quickly guide the architecture, while constantly evaluating dominant sub-networks via neuron importance ranking. This unveils dominant sub-networks whose structures turn stable, allowing conventional pruning to be pushed earlier into the training. To do this early, we further introduce an Early Pruning Indicator (EPI) that relies on sub-network architectural similarity and quickly triggers pruning when the sub-network's architecture stabilizes. Through extensive experiments on ImageNet, we show that EPI empowers a quick tracking of early training epochs suitable for pruning, offering same efficacy as an otherwise ``oracle'' grid-search that scans through epochs and requires orders of magnitude more compute. Our method yields $1.4\%$ top-1 accuracy boost over state-of-the-art pruning counterparts, cuts down training cost on GPU by $2.4\times$, hence offers a new efficiency-accuracy boundary for network pruning during training.



### Local-Global Associative Frame Assemble in Video Re-ID
- **Arxiv ID**: http://arxiv.org/abs/2110.12018v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12018v1)
- **Published**: 2021-10-22 19:07:39+00:00
- **Updated**: 2021-10-22 19:07:39+00:00
- **Authors**: Qilei Li, Jiabo Huang, Shaogang Gong
- **Comment**: British Machine Vision Conference (BMVC) 2021. Project at
  http://liqilei.github.io/projects/li2021loga
- **Journal**: None
- **Summary**: Noisy and unrepresentative frames in automatically generated object bounding boxes from video sequences cause significant challenges in learning discriminative representations in video re-identification (Re-ID). Most existing methods tackle this problem by assessing the importance of video frames according to either their local part alignments or global appearance correlations separately. However, given the diverse and unknown sources of noise which usually co-exist in captured video data, existing methods have not been effective satisfactorily. In this work, we explore jointly both local alignments and global correlations with further consideration of their mutual promotion/reinforcement so to better assemble complementary discriminative Re-ID information within all the relevant frames in video tracklets. Specifically, we concurrently optimise a local aligned quality (LAQ) module that distinguishes the quality of each frame based on local alignments, and a global correlated quality (GCQ) module that estimates global appearance correlations. With the help of a local-assembled global appearance prototype, we associate LAQ and GCQ to exploit their mutual complement. Extensive experiments demonstrate the superiority of the proposed model against state-of-the-art methods on five Re-ID benchmarks, including MARS, Duke-Video, Duke-SI, iLIDS-VID, and PRID2011.



### A Prototype-Oriented Framework for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2110.12024v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.12024v1)
- **Published**: 2021-10-22 19:23:22+00:00
- **Updated**: 2021-10-22 19:23:22+00:00
- **Authors**: Korawat Tanwisuth, Xinjie Fan, Huangjie Zheng, Shujian Zhang, Hao Zhang, Bo Chen, Mingyuan Zhou
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Existing methods for unsupervised domain adaptation often rely on minimizing some statistical distance between the source and target samples in the latent space. To avoid the sampling variability, class imbalance, and data-privacy concerns that often plague these methods, we instead provide a memory and computation-efficient probabilistic framework to extract class prototypes and align the target features with them. We demonstrate the general applicability of our method on a wide range of scenarios, including single-source, multi-source, class-imbalance, and source-private domain adaptation. Requiring no additional model parameters and having a moderate increase in computation over the source model alone, the proposed method achieves competitive performance with state-of-the-art methods.



### A Simple Baseline for Low-Budget Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.12033v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12033v2)
- **Published**: 2021-10-22 19:36:56+00:00
- **Updated**: 2022-04-01 17:57:19+00:00
- **Authors**: Kossar Pourahmadi, Parsa Nooralinejad, Hamed Pirsiavash
- **Comment**: 20 pages, 16 tables; additional experiments
- **Journal**: None
- **Summary**: Active learning focuses on choosing a subset of unlabeled data to be labeled. However, most such methods assume that a large subset of the data can be annotated. We are interested in low-budget active learning where only a small subset (e.g., 0.2% of ImageNet) can be annotated. Instead of proposing a new query strategy to iteratively sample batches of unlabeled data given an initial pool, we learn rich features by an off-the-shelf self-supervised learning method only once, and then study the effectiveness of different sampling strategies given a low labeling budget on a variety of datasets including ImageNet. We show that although the state-of-the-art active learning methods work well given a large labeling budget, a simple K-means clustering algorithm can outperform them on low budgets. We believe this method can be used as a simple baseline for low-budget active learning on image classification. Code is available at: https://github.com/UCDvision/low-budget-al



### Generative Adversarial Networks for Non-Raytraced Global Illumination on Older GPU Hardware
- **Arxiv ID**: http://arxiv.org/abs/2110.12039v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12039v1)
- **Published**: 2021-10-22 19:58:31+00:00
- **Updated**: 2021-10-22 19:58:31+00:00
- **Authors**: Jared Harris-Dewey, Richard Klein
- **Comment**: 5 pages,7 figure
- **Journal**: None
- **Summary**: We give an overview of the different rendering methods and we demonstrate that the use of a Generative Adversarial Networks (GAN) for Global Illumination (GI) gives a superior quality rendered image to that of a rasterisations image. We utilise the Pix2Pix architecture and specify the hyper-parameters and methodology used to mimic ray-traced images from a set of input features. We also demonstrate that the GANs quality is comparable to the quality of the ray-traced images, but is able to produce the image, at a fraction of the time.



### CD&S Dataset: Handheld Imagery Dataset Acquired Under Field Conditions for Corn Disease Identification and Severity Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.12084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12084v1)
- **Published**: 2021-10-22 22:33:51+00:00
- **Updated**: 2021-10-22 22:33:51+00:00
- **Authors**: Aanis Ahmad, Dharmendra Saraswat, Aly El Gamal, Gurmukh Johal
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate disease identification and its severity estimation is an important consideration for disease management. Deep learning-based solutions for disease management using imagery datasets are being increasingly explored by the research community. However, most reported studies have relied on imagery datasets that were acquired under controlled lab conditions. As a result, such models lacked the ability to identify diseases in the field. Therefore, to train a robust deep learning model for field use, an imagery dataset was created using raw images acquired under field conditions using a handheld sensor and augmented images with varying backgrounds. The Corn Disease and Severity (CD&S) dataset consisted of 511, 524, and 562, field acquired raw images, corresponding to three common foliar corn diseases, namely Northern Leaf Blight (NLB), Gray Leaf Spot (GLS), and Northern Leaf Spot (NLS), respectively. For training disease identification models, half of the imagery data for each disease was annotated using bounding boxes and also used to generate 2343 additional images through augmentation using three different backgrounds. For severity estimation, an additional 515 raw images for NLS were acquired and categorized into severity classes ranging from 1 (resistant) to 5 (susceptible). Overall, the CD&S dataset consisted of 4455 total images comprising of 2112 field images and 2343 augmented images.



### Circle Representation for Medical Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.12093v1
- **DOI**: 10.1109/TMI.2021.3122835
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12093v1)
- **Published**: 2021-10-22 23:16:42+00:00
- **Updated**: 2021-10-22 23:16:42+00:00
- **Authors**: Ethan H. Nguyen, Haichun Yang, Ruining Deng, Yuzhe Lu, Zheyu Zhu, Joseph T. Roland, Le Lu, Bennett A. Landman, Agnes B. Fogo, Yuankai Huo
- **Comment**: 10 pages, 8 figures, to be published in IEEE Transactions on Medical
  Imaging
- **Journal**: in IEEE Transactions on Medical Imaging, vol. 41, no. 3, pp.
  746-754, March 2022
- **Summary**: Box representation has been extensively used for object detection in computer vision. Such representation is efficacious but not necessarily optimized for biomedical objects (e.g., glomeruli), which play an essential role in renal pathology. In this paper, we propose a simple circle representation for medical object detection and introduce CircleNet, an anchor-free detection framework. Compared with the conventional bounding box representation, the proposed bounding circle representation innovates in three-fold: (1) it is optimized for ball-shaped biomedical objects; (2) The circle representation reduced the degree of freedom compared with box representation; (3) It is naturally more rotation invariant. When detecting glomeruli and nuclei on pathological images, the proposed circle representation achieved superior detection performance and be more rotation-invariant, compared with the bounding box. The code has been made publicly available: https://github.com/hrlblab/CircleNet



