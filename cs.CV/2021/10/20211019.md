# Arxiv Papers in cs.CV on 2021-10-19
### Osteoporosis Prescreening using Panoramic Radiographs through a Deep Convolutional Neural Network with Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2110.09662v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09662v1)
- **Published**: 2021-10-19 00:03:57+00:00
- **Updated**: 2021-10-19 00:03:57+00:00
- **Authors**: Heng Fan, Jiaxiang Ren, Jie Yang, Yi-Xian Qin, Haibin Ling
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Objectives. The aim of this study was to investigate whether a deep convolutional neural network (CNN) with an attention module can detect osteoporosis on panoramic radiographs.   Study Design. A dataset of 70 panoramic radiographs (PRs) from 70 different subjects of age between 49 to 60 was used, including 49 subjects with osteoporosis and 21 normal subjects. We utilized the leave-one-out cross-validation approach to generate 70 training and test splits. Specifically, for each split, one image was used for testing and the remaining 69 images were used for training. A deep convolutional neural network (CNN) using the Siamese architecture was implemented through a fine-tuning process to classify an PR image using patches extracted from eight representative trabecula bone areas (Figure 1). In order to automatically learn the importance of different PR patches, an attention module was integrated into the deep CNN. Three metrics, including osteoporosis accuracy (OPA), non-osteoporosis accuracy (NOPA) and overall accuracy (OA), were utilized for performance evaluation.   Results. The proposed baseline CNN approach achieved the OPA, NOPA and OA scores of 0.667, 0.878 and 0.814, respectively. With the help of the attention module, the OPA, NOPA and OA scores were further improved to 0.714, 0.939 and 0.871, respectively.   Conclusions. The proposed method obtained promising results using deep CNN with an attention module, which might be applied to osteoporosis prescreening.



### Adaptive Distillation: Aggregating Knowledge from Multiple Paths for Efficient Distillation
- **Arxiv ID**: http://arxiv.org/abs/2110.09674v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.09674v2)
- **Published**: 2021-10-19 00:57:40+00:00
- **Updated**: 2021-10-24 00:12:27+00:00
- **Authors**: Sumanth Chennupati, Mohammad Mahdi Kamani, Zhongwei Cheng, Lin Chen
- **Comment**: Accepted to BMVC 2021 for publication. V2. Added more results for
  ImageNet-1K
- **Journal**: None
- **Summary**: Knowledge Distillation is becoming one of the primary trends among neural network compression algorithms to improve the generalization performance of a smaller student model with guidance from a larger teacher model. This momentous rise in applications of knowledge distillation is accompanied by the introduction of numerous algorithms for distilling the knowledge such as soft targets and hint layers. Despite this advancement in different techniques for distilling the knowledge, the aggregation of different paths for distillation has not been studied comprehensively. This is of particular significance, not only because different paths have different importance, but also due to the fact that some paths might have negative effects on the generalization performance of the student model. Hence, we need to adaptively adjust the importance of each path to maximize the impact of distillation on the student model. In this paper, we explore different approaches for aggregating these different paths and introduce our proposed adaptive approach based on multitask learning methods. We empirically demonstrate the effectiveness of the proposed approach over other baselines on the applications of knowledge distillation in classification, semantic segmentation, and object detection tasks.



### Cross-Vendor CT Image Data Harmonization Using CVH-CT
- **Arxiv ID**: http://arxiv.org/abs/2110.09693v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09693v1)
- **Published**: 2021-10-19 02:15:26+00:00
- **Updated**: 2021-10-19 02:15:26+00:00
- **Authors**: Md Selim, Jie Zhang, Baowei Fei, Guo-Qiang Zhang, Gary Yeeming Ge, Jin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: While remarkable advances have been made in Computed Tomography (CT), most of the existing efforts focus on imaging enhancement while reducing radiation dose. How to harmonize CT image data captured using different scanners is vital in cross-center large-scale radiomics studies but remains the boundary to explore. Furthermore, the lack of paired training image problem makes it computationally challenging to adopt existing deep learning models. %developed for CT image standardization. %this problem more challenging. We propose a novel deep learning approach called CVH-CT for harmonizing CT images captured using scanners from different vendors. The generator of CVH-CT uses a self-attention mechanism to learn the scanner-related information. We also propose a VGG feature-based domain loss to effectively extract texture properties from unpaired image data to learn the scanner-based texture distributions. The experimental results show that CVH-CT is clearly better than the baselines because of the use of the proposed domain loss, and CVH-CT can effectively reduce the scanner-related variability in terms of radiomic features.



### Image Quality Assessment in the Modern Age
- **Arxiv ID**: http://arxiv.org/abs/2110.09699v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09699v1)
- **Published**: 2021-10-19 02:38:46+00:00
- **Updated**: 2021-10-19 02:38:46+00:00
- **Authors**: Kede Ma, Yuming Fang
- **Comment**: ACM Multimedia 2021 Tutorial
- **Journal**: None
- **Summary**: This tutorial provides the audience with the basic theories, methodologies, and current progresses of image quality assessment (IQA). From an actionable perspective, we will first revisit several subjective quality assessment methodologies, with emphasis on how to properly select visual stimuli. We will then present in detail the design principles of objective quality assessment models, supplemented by an in-depth analysis of their advantages and disadvantages. Both hand-engineered and (deep) learning-based methods will be covered. Moreover, the limitations with the conventional model comparison methodology for objective quality models will be pointed out, and novel comparison methodologies such as those based on the theory of "analysis by synthesis" will be introduced. We will last discuss the real-world multimedia applications of IQA, and give a list of open challenging problems, in the hope of encouraging more and more talented researchers and engineers devoting to this exciting and rewarding research field.



### DetectorNet: Transformer-enhanced Spatial Temporal Graph Neural Network for Traffic Prediction
- **Arxiv ID**: http://arxiv.org/abs/2111.00869v1
- **DOI**: 10.1145/3474717.3483920
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.00869v1)
- **Published**: 2021-10-19 03:47:38+00:00
- **Updated**: 2021-10-19 03:47:38+00:00
- **Authors**: He Li, Shiyu Zhang, Xuejiao Li, Liangcai Su, Hongjie Huang, Duo Jin, Linghao Chen, Jianbing Huang, Jaesoo Yoo
- **Comment**: The 29th ACM SIGSPATIAL International Conference on Advances in
  Geographic Information Systems (ACM SIGSPATIAL 2021)
- **Journal**: None
- **Summary**: Detectors with high coverage have direct and far-reaching benefits for road users in route planning and avoiding traffic congestion, but utilizing these data presents unique challenges including: the dynamic temporal correlation, and the dynamic spatial correlation caused by changes in road conditions. Although the existing work considers the significance of modeling with spatial-temporal correlation, what it has learned is still a static road network structure, which cannot reflect the dynamic changes of roads, and eventually loses much valuable potential information. To address these challenges, we propose DetectorNet enhanced by Transformer. Differs from previous studies, our model contains a Multi-view Temporal Attention module and a Dynamic Attention module, which focus on the long-distance and short-distance temporal correlation, and dynamic spatial correlation by dynamically updating the learned knowledge respectively, so as to make accurate prediction. In addition, the experimental results on two public datasets and the comparison results of four ablation experiments proves that the performance of DetectorNet is better than the eleven advanced baselines.



### Mask-aware IoU for Anchor Assignment in Real-time Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.09734v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.09734v1)
- **Published**: 2021-10-19 05:06:53+00:00
- **Updated**: 2021-10-19 05:06:53+00:00
- **Authors**: Kemal Oksuz, Baris Can Cam, Fehmi Kahraman, Zeynep Sonat Baltaci, Sinan Kalkan, Emre Akbas
- **Comment**: BMVC 2021, camera ready version
- **Journal**: None
- **Summary**: This paper presents Mask-aware Intersection-over-Union (maIoU) for assigning anchor boxes as positives and negatives during training of instance segmentation methods. Unlike conventional IoU or its variants, which only considers the proximity of two boxes; maIoU consistently measures the proximity of an anchor box with not only a ground truth box but also its associated ground truth mask. Thus, additionally considering the mask, which, in fact, represents the shape of the object, maIoU enables a more accurate supervision during training. We present the effectiveness of maIoU on a state-of-the-art (SOTA) assigner, ATSS, by replacing IoU operation by our maIoU and training YOLACT, a SOTA real-time instance segmentation method. Using ATSS with maIoU consistently outperforms (i) ATSS with IoU by $\sim 1$ mask AP, (ii) baseline YOLACT with fixed IoU threshold assigner by $\sim 2$ mask AP over different image sizes and (iii) decreases the inference time by $25 \%$ owing to using less anchors. Then, exploiting this efficiency, we devise maYOLACT, a faster and $+6$ AP more accurate detector than YOLACT. Our best model achieves $37.7$ mask AP at $25$ fps on COCO test-dev establishing a new state-of-the-art for real-time instance segmentation. Code is available at https://github.com/kemaloksuz/Mask-aware-IoU



### Learning Not to Reconstruct Anomalies
- **Arxiv ID**: http://arxiv.org/abs/2110.09742v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.09742v2)
- **Published**: 2021-10-19 05:22:38+00:00
- **Updated**: 2021-10-24 06:43:21+00:00
- **Authors**: Marcella Astrid, Muhammad Zaigham Zaheer, Jae-Yeong Lee, Seung-Ik Lee
- **Comment**: Accepted in BMVC 2021
- **Journal**: None
- **Summary**: Video anomaly detection is often seen as one-class classification (OCC) problem due to the limited availability of anomaly examples. Typically, to tackle this problem, an autoencoder (AE) is trained to reconstruct the input with training set consisting only of normal data. At test time, the AE is then expected to well reconstruct the normal data while poorly reconstructing the anomalous data. However, several studies have shown that, even with only normal data training, AEs can often start reconstructing anomalies as well which depletes the anomaly detection performance. To mitigate this problem, we propose a novel methodology to train AEs with the objective of reconstructing only normal data, regardless of the input (i.e., normal or abnormal). Since no real anomalies are available in the OCC settings, the training is assisted by pseudo anomalies that are generated by manipulating normal data to simulate the out-of-normal-data distribution. We additionally propose two ways to generate pseudo anomalies: patch and skip frame based. Extensive experiments on three challenging video anomaly datasets demonstrate the effectiveness of our method in improving conventional AEs, achieving state-of-the-art performance.



### Spectral Variability Augmented Sparse Unmixing of Hyperspectral Images
- **Arxiv ID**: http://arxiv.org/abs/2110.09744v2
- **DOI**: 10.1109/TGRS.2022.3169228
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2110.09744v2)
- **Published**: 2021-10-19 05:25:30+00:00
- **Updated**: 2021-10-21 14:40:14+00:00
- **Authors**: Ge Zhang, Shaohui Mei, Mingyang Ma, Yan Feng, Qian Du
- **Comment**: None
- **Journal**: None
- **Summary**: Spectral unmixing (SU) expresses the mixed pixels existed in hyperspectral images as the product of endmember and abundance, which has been widely used in hyperspectral imagery analysis. However, the influence of light, acquisition conditions and the inherent properties of materials, results in that the identified endmembers can vary spectrally within a given image (construed as spectral variability). To address this issue, recent methods usually use a priori obtained spectral library to represent multiple characteristic spectra of the same object, but few of them extracted the spectral variability explicitly. In this paper, a spectral variability augmented sparse unmixing model (SVASU) is proposed, in which the spectral variability is extracted for the first time. The variable spectra are divided into two parts of intrinsic spectrum and spectral variability for spectral reconstruction, and modeled synchronously in the SU model adding the regular terms restricting the sparsity of abundance and the generalization of the variability coefficient. It is noted that the spectral variability library and the intrinsic spectral library are all constructed from the In-situ observed image. Experimental results over both synthetic and real-world data sets demonstrate that the augmented decomposition by spectral variability significantly improves the unmixing performance than the decomposition only by spectral library, as well as compared to state-of-the-art algorithms.



### Unifying Multimodal Transformer for Bi-directional Image and Text Generation
- **Arxiv ID**: http://arxiv.org/abs/2110.09753v1
- **DOI**: 10.1145/3474085.3481540
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.09753v1)
- **Published**: 2021-10-19 06:01:24+00:00
- **Updated**: 2021-10-19 06:01:24+00:00
- **Authors**: Yupan Huang, Hongwei Xue, Bei Liu, Yutong Lu
- **Comment**: ACM MM 2021 (Industrial Track). Code:
  https://github.com/researchmm/generate-it
- **Journal**: None
- **Summary**: We study the joint learning of image-to-text and text-to-image generations, which are naturally bi-directional tasks. Typical existing works design two separate task-specific models for each task, which impose expensive design efforts. In this work, we propose a unified image-and-text generative framework based on a single multimodal model to jointly study the bi-directional tasks. We adopt Transformer as our unified architecture for its strong performance and task-agnostic design. Specifically, we formulate both tasks as sequence generation tasks, where we represent images and text as unified sequences of tokens, and the Transformer learns multimodal interactions to generate sequences. We further propose two-level granularity feature representations and sequence-level training to improve the Transformer-based unified framework. Experiments show that our approach significantly improves previous Transformer-based model X-LXMERT's FID from 37.0 to 29.9 (lower is better) for text-to-image generation, and improves CIDEr-D score from 100.9% to 122.6% for fine-tuned image-to-text generation on the MS-COCO dataset. Our code is available online.



### A Picture is Worth a Thousand Words: A Unified System for Diverse Captions and Rich Images Generation
- **Arxiv ID**: http://arxiv.org/abs/2110.09756v1
- **DOI**: 10.1145/3474085.3478561
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.09756v1)
- **Published**: 2021-10-19 06:10:42+00:00
- **Updated**: 2021-10-19 06:10:42+00:00
- **Authors**: Yupan Huang, Bei Liu, Jianlong Fu, Yutong Lu
- **Comment**: ACM MM 2021 (Video and Demo Track). Code:
  https://github.com/researchmm/generate-it
- **Journal**: None
- **Summary**: A creative image-and-text generative AI system mimics humans' extraordinary abilities to provide users with diverse and comprehensive caption suggestions, as well as rich image creations. In this work, we demonstrate such an AI creation system to produce both diverse captions and rich images. When users imagine an image and associate it with multiple captions, our system paints a rich image to reflect all captions faithfully. Likewise, when users upload an image, our system depicts it with multiple diverse captions. We propose a unified multi-modal framework to achieve this goal. Specifically, our framework jointly models image-and-text representations with a Transformer network, which supports rich image creation by accepting multiple captions as input. We consider the relations among input captions to encourage diversity in training and adopt a non-autoregressive decoding strategy to enable real-time inference. Based on these, our system supports both diverse captions and rich images generations. Our code is available online.



### A Regularization Method to Improve Adversarial Robustness of Neural Networks for ECG Signal Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.09759v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2110.09759v2)
- **Published**: 2021-10-19 06:22:02+00:00
- **Updated**: 2022-03-15 05:27:45+00:00
- **Authors**: Linhai Ma, Liang Liang
- **Comment**: This paper has been published by Computers in Biology and Medicine
- **Journal**: None
- **Summary**: Electrocardiogram (ECG) is the most widely used diagnostic tool to monitor the condition of the human heart. By using deep neural networks (DNNs), interpretation of ECG signals can be fully automated for the identification of potential abnormalities in a patient's heart in a fraction of a second. Studies have shown that given a sufficiently large amount of training data, DNN accuracy for ECG classification could reach human-expert cardiologist level. However, despite of the excellent performance in classification accuracy, DNNs are highly vulnerable to adversarial noises that are subtle changes in the input of a DNN and may lead to a wrong class-label prediction. It is challenging and essential to improve robustness of DNNs against adversarial noises, which are a threat to life-critical applications. In this work, we proposed a regularization method to improve DNN robustness from the perspective of noise-to-signal ratio (NSR) for the application of ECG signal classification. We evaluated our method on PhysioNet MIT-BIH dataset and CPSC2018 ECG dataset, and the results show that our method can substantially enhance DNN robustness against adversarial noises generated from adversarial attacks, with a minimal change in accuracy on clean data.



### Detecting Blurred Ground-based Sky/Cloud Images
- **Arxiv ID**: http://arxiv.org/abs/2110.09764v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.09764v1)
- **Published**: 2021-10-19 06:51:31+00:00
- **Updated**: 2021-10-19 06:51:31+00:00
- **Authors**: Mayank Jain, Navya Jain, Yee Hui Lee, Stefan Winkler, Soumyabrata Dev
- **Comment**: Accepted in Proc. IEEE AP-S Symposium on Antennas and Propagation and
  USNC-URSI Radio Science Meeting, 2021
- **Journal**: None
- **Summary**: Ground-based whole sky imagers (WSIs) are being used by researchers in various fields to study the atmospheric events. These ground-based sky cameras capture visible-light images of the sky at regular intervals of time. Owing to the atmospheric interference and camera sensor noise, the captured images often exhibit noise and blur. This may pose a problem in subsequent image processing stages. Therefore, it is important to accurately identify the blurred images. This is a difficult task, as clouds have varying shapes, textures, and soft edges whereas the sky acts as a homogeneous and uniform background. In this paper, we propose an efficient framework that can identify the blurred sky/cloud images. Using a static external marker, our proposed methodology has a detection accuracy of 94\%. To the best of our knowledge, our approach is the first of its kind in the automatic identification of blurred images for ground-based sky/cloud images.



### Memory-Augmented Deep Unfolding Network for Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/2110.09766v2
- **DOI**: 10.1145/3474085.3475562
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09766v2)
- **Published**: 2021-10-19 07:03:12+00:00
- **Updated**: 2021-10-25 01:33:55+00:00
- **Authors**: Jiechong Song, Bin Chen, Jian Zhang
- **Comment**: 10 pages, 7 figures, ACM MM 2021
- **Journal**: None
- **Summary**: Mapping a truncated optimization method into a deep neural network, deep unfolding network (DUN) has attracted growing attention in compressive sensing (CS) due to its good interpretability and high performance. Each stage in DUNs corresponds to one iteration in optimization. By understanding DUNs from the perspective of the human brain's memory processing, we find there exists two issues in existing DUNs. One is the information between every two adjacent stages, which can be regarded as short-term memory, is usually lost seriously. The other is no explicit mechanism to ensure that the previous stages affect the current stage, which means memory is easily forgotten. To solve these issues, in this paper, a novel DUN with persistent memory for CS is proposed, dubbed Memory-Augmented Deep Unfolding Network (MADUN). We design a memory-augmented proximal mapping module (MAPMM) by combining two types of memory augmentation mechanisms, namely High-throughput Short-term Memory (HSM) and Cross-stage Long-term Memory (CLM). HSM is exploited to allow DUNs to transmit multi-channel short-term memory, which greatly reduces information loss between adjacent stages. CLM is utilized to develop the dependency of deep information across cascading stages, which greatly enhances network representation capability. Extensive CS experiments on natural and MR images show that with the strong ability to maintain and balance information our MADUN outperforms existing state-of-the-art methods by a large margin. The source code is available at https://github.com/jianzhangcs/MADUN/.



### Synthetic Temporal Anomaly Guided End-to-End Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.09768v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.09768v1)
- **Published**: 2021-10-19 07:08:44+00:00
- **Updated**: 2021-10-19 07:08:44+00:00
- **Authors**: Marcella Astrid, Muhammad Zaigham Zaheer, Seung-Ik Lee
- **Comment**: Published at ICCV Workshops 2021.
  https://openaccess.thecvf.com/content/ICCV2021W/RSLCV/html/Astrid_Synthetic_Temporal_Anomaly_Guided_End-to-End_Video_Anomaly_Detection_ICCVW_2021_paper.html
- **Journal**: None
- **Summary**: Due to the limited availability of anomaly examples, video anomaly detection is often seen as one-class classification (OCC) problem. A popular way to tackle this problem is by utilizing an autoencoder (AE) trained only on normal data. At test time, the AE is then expected to reconstruct the normal input well while reconstructing the anomalies poorly. However, several studies show that, even with normal data only training, AEs can often start reconstructing anomalies as well which depletes their anomaly detection performance. To mitigate this, we propose a temporal pseudo anomaly synthesizer that generates fake-anomalies using only normal data. An AE is then trained to maximize the reconstruction loss on pseudo anomalies while minimizing this loss on normal data. This way, the AE is encouraged to produce distinguishable reconstructions for normal and anomalous frames. Extensive experiments and analysis on three challenging video anomaly datasets demonstrate the effectiveness of our approach to improve the basic AEs in achieving superiority against several existing state-of-the-art models.



### Synergy between 3DMM and 3D Landmarks for Accurate 3D Facial Geometry
- **Arxiv ID**: http://arxiv.org/abs/2110.09772v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.09772v2)
- **Published**: 2021-10-19 07:29:14+00:00
- **Updated**: 2021-10-20 02:09:07+00:00
- **Authors**: Cho-Ying Wu, Qiangeng Xu, Ulrich Neumann
- **Comment**: Accepted at 3DV 2021. This conference version supersedes
  arXiv:2104.08403
- **Journal**: None
- **Summary**: This work studies learning from a synergy process of 3D Morphable Models (3DMM) and 3D facial landmarks to predict complete 3D facial geometry, including 3D alignment, face orientation, and 3D face modeling. Our synergy process leverages a representation cycle for 3DMM parameters and 3D landmarks. 3D landmarks can be extracted and refined from face meshes built by 3DMM parameters. We next reverse the representation direction and show that predicting 3DMM parameters from sparse 3D landmarks improves the information flow. Together we create a synergy process that utilizes the relation between 3D landmarks and 3DMM parameters, and they collaboratively contribute to better performance. We extensively validate our contribution on full tasks of facial geometry prediction and show our superior and robust performance on these tasks for various scenarios. Particularly, we adopt only simple and widely-used network operations to attain fast and accurate facial geometry prediction. Codes and data: https://choyingw.github.io/works/SynergyNet/



### Aesthetic Photo Collage with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.09775v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.09775v1)
- **Published**: 2021-10-19 07:34:48+00:00
- **Updated**: 2021-10-19 07:34:48+00:00
- **Authors**: Mingrui Zhang, Mading Li, Li Chen, Jiahao Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Photo collage aims to automatically arrange multiple photos on a given canvas with high aesthetic quality. Existing methods are based mainly on handcrafted feature optimization, which cannot adequately capture high-level human aesthetic senses. Deep learning provides a promising way, but owing to the complexity of collage and lack of training data, a solution has yet to be found. In this paper, we propose a novel pipeline for automatic generation of aspect ratio specified collage and the reinforcement learning technique is introduced in collage for the first time. Inspired by manual collages, we model the collage generation as sequential decision process to adjust spatial positions, orientation angles, placement order and the global layout. To instruct the agent to improve both the overall layout and local details, the reward function is specially designed for collage, considering subjective and objective factors. To overcome the lack of training data, we pretrain our deep aesthetic network on a large scale image aesthetic dataset (CPC) for general aesthetic feature extraction and propose an attention fusion module for structural collage feature representation. We test our model against competing methods on two movie datasets and our results outperform others in aesthetic quality evaluation. Further user study is also conducted to demonstrate the effectiveness.



### Towards Toxic and Narcotic Medication Detection with Rotated Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2110.09777v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.09777v1)
- **Published**: 2021-10-19 07:46:02+00:00
- **Updated**: 2021-10-19 07:46:02+00:00
- **Authors**: Jiao Peng, Feifan Wang, Zhongqiang Fu, Yiying Hu, Zichen Chen, Xinghan Zhou, Lijun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have witnessed the advancement of deep learning vision technologies and applications in the medical industry. Intelligent devices for special medication management are in great need of, which requires more precise detection algorithms to identify the specifications and locations. In this work, YOLO (You only look once) based object detectors are tailored for toxic and narcotic medications detection tasks. Specifically, a more flexible annotation with rotated degree ranging from $0^\circ$ to $90^\circ$ and a mask-mapping-based non-maximum suppression method are proposed to achieve a feasible and efficient medication detector aiming at arbitrarily oriented bounding boxes. Extensive experiments demonstrate that the rotated YOLO detectors are more suitable for identifying densely arranged drugs. The best shot mean average precision of the proposed network reaches 0.811 while the inference time is less than 300ms.



### Spatial-Temporal Transformer for 3D Point Cloud Sequences
- **Arxiv ID**: http://arxiv.org/abs/2110.09783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.09783v1)
- **Published**: 2021-10-19 07:55:47+00:00
- **Updated**: 2021-10-19 07:55:47+00:00
- **Authors**: Yimin Wei, Hao Liu, Tingting Xie, Qiuhong Ke, Yulan Guo
- **Comment**: None
- **Journal**: WACV2022
- **Summary**: Effective learning of spatial-temporal information within a point cloud sequence is highly important for many down-stream tasks such as 4D semantic segmentation and 3D action recognition. In this paper, we propose a novel framework named Point Spatial-Temporal Transformer (PST2) to learn spatial-temporal representations from dynamic 3D point cloud sequences. Our PST2 consists of two major modules: a Spatio-Temporal Self-Attention (STSA) module and a Resolution Embedding (RE) module. Our STSA module is introduced to capture the spatial-temporal context information across adjacent frames, while the RE module is proposed to aggregate features across neighbors to enhance the resolution of feature maps. We test the effectiveness our PST2 with two different tasks on point cloud sequences, i.e., 4D semantic segmentation and 3D action recognition. Extensive experiments on three benchmarks show that our PST2 outperforms existing methods on all datasets. The effectiveness of our STSA and RE modules have also been justified with ablation experiments.



### CIPS-3D: A 3D-Aware Generator of GANs Based on Conditionally-Independent Pixel Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2110.09788v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09788v1)
- **Published**: 2021-10-19 08:02:16+00:00
- **Updated**: 2021-10-19 08:02:16+00:00
- **Authors**: Peng Zhou, Lingxi Xie, Bingbing Ni, Qi Tian
- **Comment**: 3D-aware GANs based on NeRF, https://github.com/PeterouZh/CIPS-3D
- **Journal**: None
- **Summary**: The style-based GAN (StyleGAN) architecture achieved state-of-the-art results for generating high-quality images, but it lacks explicit and precise control over camera poses. The recently proposed NeRF-based GANs made great progress towards 3D-aware generators, but they are unable to generate high-quality images yet. This paper presents CIPS-3D, a style-based, 3D-aware generator that is composed of a shallow NeRF network and a deep implicit neural representation (INR) network. The generator synthesizes each pixel value independently without any spatial convolution or upsampling operation. In addition, we diagnose the problem of mirror symmetry that implies a suboptimal solution and solve it by introducing an auxiliary discriminator. Trained on raw, single-view images, CIPS-3D sets new records for 3D-aware image synthesis with an impressive FID of 6.97 for images at the $256\times256$ resolution on FFHQ. We also demonstrate several interesting directions for CIPS-3D such as transfer learning and 3D-aware face stylization. The synthesis results are best viewed as videos, so we recommend the readers to check our github project at https://github.com/PeterouZh/CIPS-3D



### Geo-DefakeHop: High-Performance Geographic Fake Image Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.09795v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09795v1)
- **Published**: 2021-10-19 08:19:40+00:00
- **Updated**: 2021-10-19 08:19:40+00:00
- **Authors**: Hong-Shuo Chen, Kaitai Zhang, Shuowen Hu, Suya You, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: A robust fake satellite image detection method, called Geo-DefakeHop, is proposed in this work. Geo-DefakeHop is developed based on the parallel subspace learning (PSL) methodology. PSL maps the input image space into several feature subspaces using multiple filter banks. By exploring response differences of different channels between real and fake images for a filter bank, Geo-DefakeHop learns the most discriminant channels and uses their soft decision scores as features. Then, Geo-DefakeHop selects a few discriminant features from each filter bank and ensemble them to make a final binary decision. Geo-DefakeHop offers a light-weight high-performance solution to fake satellite images detection. Its model size is analyzed, which ranges from 0.8 to 62K parameters. Furthermore, it is shown by experimental results that it achieves an F1-score higher than 95\% under various common image manipulations such as resizing, compression and noise corruption.



### Latent reweighting, an almost free improvement for GANs
- **Arxiv ID**: http://arxiv.org/abs/2110.09803v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09803v1)
- **Published**: 2021-10-19 08:33:57+00:00
- **Updated**: 2021-10-19 08:33:57+00:00
- **Authors**: Thibaut Issenhuth, Ugo Tanielian, David Picard, Jeremie Mary
- **Comment**: None
- **Journal**: None
- **Summary**: Standard formulations of GANs, where a continuous function deforms a connected latent space, have been shown to be misspecified when fitting different classes of images. In particular, the generator will necessarily sample some low-quality images in between the classes. Rather than modifying the architecture, a line of works aims at improving the sampling quality from pre-trained generators at the expense of increased computational cost. Building on this, we introduce an additional network to predict latent importance weights and two associated sampling methods to avoid the poorest samples. This idea has several advantages: 1) it provides a way to inject disconnectedness into any GAN architecture, 2) since the rejection happens in the latent space, it avoids going through both the generator and the discriminator, saving computation time, 3) this importance weights formulation provides a principled way to reduce the Wasserstein's distance to the target distribution. We demonstrate the effectiveness of our method on several datasets, both synthetic and high-dimensional.



### Microstructure reconstruction via artificial neural networks: A combination of causal and non-causal approach
- **Arxiv ID**: http://arxiv.org/abs/2110.09815v1
- **DOI**: None
- **Categories**: **cond-mat.mtrl-sci**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09815v1)
- **Published**: 2021-10-19 09:04:19+00:00
- **Updated**: 2021-10-19 09:04:19+00:00
- **Authors**: Kryštof Latka, Martin Doškář, Jan Zeman
- **Comment**: 6 pages, 4 figures, and 7 tables
- **Journal**: None
- **Summary**: We investigate the applicability of artificial neural networks (ANNs) in reconstructing a sample image of a sponge-like microstructure. We propose to reconstruct the image by predicting the phase of the current pixel based on its causal neighbourhood, and subsequently, use a non-causal ANN model to smooth out the reconstructed image as a form of post-processing. We also consider the impacts of different configurations of the ANN model (e.g. number of densely connected layers, number of neurons in each layer, the size of both the causal and non-causal neighbourhood) on the models' predictive abilities quantified by the discrepancy between the spatial statistics of the reference and the reconstructed sample.



### LSTC: Boosting Atomic Action Detection with Long-Short-Term Context
- **Arxiv ID**: http://arxiv.org/abs/2110.09819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.09819v1)
- **Published**: 2021-10-19 10:09:09+00:00
- **Updated**: 2021-10-19 10:09:09+00:00
- **Authors**: Yuxi Li, Boshen Zhang, Jian Li, Yabiao Wang, Weiyao Lin, Chengjie Wang, Jilin Li, Feiyue Huang
- **Comment**: ACM Multimedia 2021
- **Journal**: None
- **Summary**: In this paper, we place the atomic action detection problem into a Long-Short Term Context (LSTC) to analyze how the temporal reliance among video signals affect the action detection results. To do this, we decompose the action recognition pipeline into short-term and long-term reliance, in terms of the hypothesis that the two kinds of context are conditionally independent given the objective action instance. Within our design, a local aggregation branch is utilized to gather dense and informative short-term cues, while a high order long-term inference branch is designed to reason the objective action class from high-order interaction between actor and other person or person pairs. Both branches independently predict the context-specific actions and the results are merged in the end. We demonstrate that both temporal grains are beneficial to atomic action recognition. On the mainstream benchmarks of atomic action detection, our design can bring significant performance gain from the existing state-of-the-art pipeline. The code of this project can be found at [this url](https://github.com/TencentYoutuResearch/ActionDetection-LSTC)



### Measuring Hidden Bias within Face Recognition via Racial Phenotypes
- **Arxiv ID**: http://arxiv.org/abs/2110.09839v1
- **DOI**: 10.1109/WACV51458.2022.00326
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.09839v1)
- **Published**: 2021-10-19 10:46:59+00:00
- **Updated**: 2021-10-19 10:46:59+00:00
- **Authors**: Seyma Yucer, Furkan Tektas, Noura Al Moubayed, Toby P. Breckon
- **Comment**: published in IEEE Winter Conference on Applications of Computer
  Vision, WACV, 2022
- **Journal**: None
- **Summary**: Recent work reports disparate performance for intersectional racial groups across face recognition tasks: face verification and identification. However, the definition of those racial groups has a significant impact on the underlying findings of such racial bias analysis. Previous studies define these groups based on either demographic information (e.g. African, Asian etc.) or skin tone (e.g. lighter or darker skins). The use of such sensitive or broad group definitions has disadvantages for bias investigation and subsequent counter-bias solutions design. By contrast, this study introduces an alternative racial bias analysis methodology via facial phenotype attributes for face recognition. We use the set of observable characteristics of an individual face where a race-related facial phenotype is hence specific to the human face and correlated to the racial profile of the subject. We propose categorical test cases to investigate the individual influence of those attributes on bias within face recognition tasks. We compare our phenotype-based grouping methodology with previous grouping strategies and show that phenotype-based groupings uncover hidden bias without reliance upon any potentially protected attributes or ill-defined grouping strategies. Furthermore, we contribute corresponding phenotype attribute category labels for two face recognition tasks: RFW for face verification and VGGFace2 (test set) for face identification.



### Cutting Voxel Projector a New Approach to Construct 3D Cone Beam CT Operator
- **Arxiv ID**: http://arxiv.org/abs/2110.09841v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2110.09841v1)
- **Published**: 2021-10-19 10:54:01+00:00
- **Updated**: 2021-10-19 10:54:01+00:00
- **Authors**: Vojtěch Kulvait, Georg Rose
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: In this paper, we introduce a new class of projectors for 3D cone beam tomographic reconstruction. We find analytical formulas for the relationship between the voxel volume projected onto a given detector pixel and its contribution to the extinction value detected on that pixel. Using this approach, we construct a near-exact projector and backprojector that can be used especially for algebraic reconstruction techniques. We have implemented this cutting voxel projector and a less accurate, speed-optimized version of it together with two established projectors, a ray tracing projector based on Siddon's algorithm and a TT footprint projector. We show that the cutting voxel projector achieves, especially for large cone beam angles, noticeably higher accuracy than the TT projector. Moreover, our implementation of the relaxed version of the cutting voxel projector is significantly faster than current footprint projector implementations. We further show that Siddon's algorithm with comparable accuracy would be much slower than the cutting voxel projector. All algorithms are implemented within an open source framework for algebraic reconstruction in OpenCL 1.2 and C++ and are optimized for GPU computation. They are published as open-source software under the GNU GPL 3 license, see https://github.com/kulvait/KCT_cbct.



### Self-Supervised Object Detection via Generative Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2110.09848v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.09848v1)
- **Published**: 2021-10-19 11:04:05+00:00
- **Updated**: 2021-10-19 11:04:05+00:00
- **Authors**: Siva Karthik Mustikovela, Shalini De Mello, Aayush Prakash, Umar Iqbal, Sifei Liu, Thu Nguyen-Phuoc, Carsten Rother, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: We present SSOD, the first end-to-end analysis-by synthesis framework with controllable GANs for the task of self-supervised object detection. We use collections of real world images without bounding box annotations to learn to synthesize and detect objects. We leverage controllable GANs to synthesize images with pre-defined object properties and use them to train object detectors. We propose a tight end-to-end coupling of the synthesis and detection networks to optimally train our system. Finally, we also propose a method to optimally adapt SSOD to an intended target data without requiring labels for it. For the task of car detection, on the challenging KITTI and Cityscapes datasets, we show that SSOD outperforms the prior state-of-the-art purely image-based self-supervised object detection method Wetectron. Even without requiring any 3D CAD assets, it also surpasses the state-of-the-art rendering based method Meta-Sim2. Our work advances the field of self-supervised object detection by introducing a successful new paradigm of using controllable GAN-based image synthesis for it and by significantly improving the baseline accuracy of the task. We open-source our code at https://github.com/NVlabs/SSOD.



### Bilateral-ViT for Robust Fovea Localization
- **Arxiv ID**: http://arxiv.org/abs/2110.09860v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09860v2)
- **Published**: 2021-10-19 11:26:04+00:00
- **Updated**: 2022-03-04 03:44:25+00:00
- **Authors**: Sifan Song, Kang Dang, Qinji Yu, Zilong Wang, Frans Coenen, Jionglong Su, Xiaowei Ding
- **Comment**: This work has been accepted for oral presentation by ISBI2022
- **Journal**: None
- **Summary**: The fovea is an important anatomical landmark of the retina. Detecting the location of the fovea is essential for the analysis of many retinal diseases. However, robust fovea localization remains a challenging problem, as the fovea region often appears fuzzy, and retina diseases may further obscure its appearance. This paper proposes a novel Vision Transformer (ViT) approach that integrates information both inside and outside the fovea region to achieve robust fovea localization. Our proposed network, named Bilateral-Vision-Transformer (Bilateral-ViT), consists of two network branches: a transformer-based main network branch for integrating global context across the entire fundus image and a vessel branch for explicitly incorporating the structure of blood vessels. The encoded features from both network branches are subsequently merged with a customized Multi-scale Feature Fusion (MFF) module. Our comprehensive experiments demonstrate that the proposed approach is significantly more robust for diseased images and establishes the new state of the arts using the Messidor and PALM datasets.



### Learning a self-supervised tone mapping operator via feature contrast masking loss
- **Arxiv ID**: http://arxiv.org/abs/2110.09866v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09866v1)
- **Published**: 2021-10-19 11:42:19+00:00
- **Updated**: 2021-10-19 11:42:19+00:00
- **Authors**: Chao Wang, Bin Chen, Hans-Peter Seidel, Karol Myszkowski, Ana Serrano
- **Comment**: None
- **Journal**: None
- **Summary**: High Dynamic Range (HDR) content is becoming ubiquitous due to the rapid development of capture technologies. Nevertheless, the dynamic range of common display devices is still limited, therefore tone mapping (TM) remains a key challenge for image visualization. Recent work has demonstrated that neural networks can achieve remarkable performance in this task when compared to traditional methods, however, the quality of the results of these learning-based methods is limited by the training data. Most existing works use as training set a curated selection of best-performing results from existing traditional tone mapping operators (often guided by a quality metric), therefore, the quality of newly generated results is fundamentally limited by the performance of such operators. This quality might be even further limited by the pool of HDR content that is used for training. In this work we propose a learning-based self-supervised tone mapping operator that is trained at test time specifically for each HDR image and does not need any data labeling. The key novelty of our approach is a carefully designed loss function built upon fundamental knowledge on contrast perception that allows for directly comparing the content in the HDR and tone mapped images. We achieve this goal by reformulating classic VGG feature maps into feature contrast maps that normalize local feature differences by their average magnitude in a local neighborhood, allowing our loss to account for contrast masking effects. We perform extensive ablation studies and exploration of parameters and demonstrate that our solution outperforms existing approaches with a single set of fixed parameters, as confirmed by both objective and subjective metrics.



### HM-Net: A Regression Network for Object Center Detection and Tracking on Wide Area Motion Imagery
- **Arxiv ID**: http://arxiv.org/abs/2110.09881v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.09881v2)
- **Published**: 2021-10-19 11:56:30+00:00
- **Updated**: 2021-12-27 21:12:11+00:00
- **Authors**: Hakki Motorcu, Hasan F. Ates, H. Fatih Ugurdag, Bahadir Gunturk
- **Comment**: 14 pages, 13 figures
- **Journal**: None
- **Summary**: Wide Area Motion Imagery (WAMI) yields high-resolution images with a large number of extremely small objects. Target objects have large spatial displacements throughout consecutive frames. This nature of WAMI images makes object tracking and detection challenging. In this paper, we present our deep neural network-based combined object detection and tracking model, namely, Heat Map Network (HM-Net). HM-Net is significantly faster than state-of-the-art frame differencing and background subtraction-based methods, without compromising detection and tracking performances. HM-Net follows the object center-based joint detection and tracking paradigm. Simple heat map-based predictions support an unlimited number of simultaneous detections. The proposed method uses two consecutive frames and the object detection heat map obtained from the previous frame as input, which helps HM-Net monitor spatio-temporal changes between frames and keeps track of previously predicted objects. Although reuse of prior object detection heat map acts as a vital feedback-based memory element, it can lead to an unintended surge of false-positive detections. To increase the robustness of the method against false positives and to eliminate low confidence detections, HM-Net employs novel feedback filters and advanced data augmentations. HM-Net outperforms state-of-the-art WAMI moving object detection and tracking methods on the WPAFB dataset with its 96.2% F1 and 94.4% mAP detection scores while achieving a 61.8% mAP tracking score on the same dataset. This performance corresponds to an improvement of 2.1% for F1, 6.1% for mAP scores on detection, and 9.5% for mAP score on tracking over the state-of-the-art.



### NeuralDiff: Segmenting 3D objects that move in egocentric videos
- **Arxiv ID**: http://arxiv.org/abs/2110.09936v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.09936v1)
- **Published**: 2021-10-19 12:51:35+00:00
- **Updated**: 2021-10-19 12:51:35+00:00
- **Authors**: Vadim Tschernezki, Diane Larlus, Andrea Vedaldi
- **Comment**: 3DV2021. Project page: https://www.robots.ox.ac.uk/~vadim/neuraldiff/
- **Journal**: None
- **Summary**: Given a raw video sequence taken from a freely-moving camera, we study the problem of decomposing the observed 3D scene into a static background and a dynamic foreground containing the objects that move in the video sequence. This task is reminiscent of the classic background subtraction problem, but is significantly harder because all parts of the scene, static and dynamic, generate a large apparent motion due to the camera large viewpoint change. In particular, we consider egocentric videos and further separate the dynamic component into objects and the actor that observes and moves them. We achieve this factorization by reconstructing the video via a triple-stream neural rendering network that explains the different motions based on corresponding inductive biases. We demonstrate that our method can successfully separate the different types of motion, outperforming recent neural rendering baselines at this task, and can accurately segment moving objects. We do so by assessing the method empirically on challenging videos from the EPIC-KITCHENS dataset which we augment with appropriate annotations to create a new benchmark for the task of dynamic object segmentation on unconstrained video sequences, for complex 3D environments.



### Talking Head Generation with Audio and Speech Related Facial Action Units
- **Arxiv ID**: http://arxiv.org/abs/2110.09951v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.09951v1)
- **Published**: 2021-10-19 13:14:27+00:00
- **Updated**: 2021-10-19 13:14:27+00:00
- **Authors**: Sen Chen, Zhilei Liu, Jiaxing Liu, Zhengxiang Yan, Longbiao Wang
- **Comment**: Accepted by BMVC 2021
- **Journal**: None
- **Summary**: The task of talking head generation is to synthesize a lip synchronized talking head video by inputting an arbitrary face image and audio clips. Most existing methods ignore the local driving information of the mouth muscles. In this paper, we propose a novel recurrent generative network that uses both audio and speech-related facial action units (AUs) as the driving information. AU information related to the mouth can guide the movement of the mouth more accurately. Since speech is highly correlated with speech-related AUs, we propose an Audio-to-AU module in our system to predict the speech-related AU information from speech. In addition, we use AU classifier to ensure that the generated images contain correct AU information. Frame discriminator is also constructed for adversarial training to improve the realism of the generated face. We verify the effectiveness of our model on the GRID dataset and TCD-TIMIT dataset. We also conduct an ablation study to verify the contribution of each component in our model. Quantitative and qualitative experiments demonstrate that our method outperforms existing methods in both image quality and lip-sync accuracy.



### Fully Three-dimensional Radial Visualization
- **Arxiv ID**: http://arxiv.org/abs/2110.09971v1
- **DOI**: 10.1080/10618600.2021.2020129
- **Categories**: **stat.ME**, cs.CV, cs.GR, stat.AP, stat.ML, 62H25, 62H99, 62P10, 62P25, G.3; I.3.3; I.3.7; J.3; J.4
- **Links**: [PDF](http://arxiv.org/pdf/2110.09971v1)
- **Published**: 2021-10-19 13:35:42+00:00
- **Updated**: 2021-10-19 13:35:42+00:00
- **Authors**: Yifan Zhu, Fan Dai, Ranjan Maitra
- **Comment**: 10 pages, 7 figures, 1 table
- **Journal**: Journal of Computational and Graphical Statistics, 31(3), 935-944,
  2022
- **Summary**: We develop methodology for three-dimensional (3D) radial visualization (RadViz) of multidimensional datasets. The classical two-dimensional (2D) RadViz visualizes multivariate data in the 2D plane by mapping every observation to a point inside the unit circle. Our tool, RadViz3D, distributes anchor points uniformly on the 3D unit sphere. We show that this uniform distribution provides the best visualization with minimal artificial visual correlation for data with uncorrelated variables. However, anchor points can be placed exactly equi-distant from each other only for the five Platonic solids, so we provide equi-distant anchor points for these five settings, and approximately equi-distant anchor points via a Fibonacci grid for the other cases. Our methodology, implemented in the R package $radviz3d$, makes fully 3D RadViz possible and is shown to improve the ability of this nonlinear technique in more faithfully displaying simulated data as well as the crabs, olive oils and wine datasets. Additionally, because radial visualization is naturally suited for compositional data, we use RadViz3D to illustrate (i) the chemical composition of Longquan celadon ceramics and their Jingdezhen imitation over centuries, and (ii) US regional SARS-Cov-2 variants' prevalence in the Covid-19 pandemic during the summer 2021 surge of the Delta variant.



### SOSP: Efficiently Capturing Global Correlations by Second-Order Structured Pruning
- **Arxiv ID**: http://arxiv.org/abs/2110.11395v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.11395v2)
- **Published**: 2021-10-19 13:53:28+00:00
- **Updated**: 2022-06-30 08:17:28+00:00
- **Authors**: Manuel Nonnenmacher, Thomas Pfeil, Ingo Steinwart, David Reeb
- **Comment**: None
- **Journal**: None
- **Summary**: Pruning neural networks reduces inference time and memory costs. On standard hardware, these benefits will be especially prominent if coarse-grained structures, like feature maps, are pruned. We devise two novel saliency-based methods for second-order structured pruning (SOSP) which include correlations among all structures and layers. Our main method SOSP-H employs an innovative second-order approximation, which enables saliency evaluations by fast Hessian-vector products. SOSP-H thereby scales like a first-order method despite taking into account the full Hessian. We validate SOSP-H by comparing it to our second method SOSP-I that uses a well-established Hessian approximation, and to numerous state-of-the-art methods. While SOSP-H performs on par or better in terms of accuracy, it has clear advantages in terms of scalability and efficiency. This allowed us to scale SOSP-H to large-scale vision tasks, even though it captures correlations across all layers of the network. To underscore the global nature of our pruning methods, we evaluate their performance not only by removing structures from a pretrained network, but also by detecting architectural bottlenecks. We show that our algorithms allow to systematically reveal architectural bottlenecks, which we then remove to further increase the accuracy of the networks.



### Towards Optimal Correlational Object Search
- **Arxiv ID**: http://arxiv.org/abs/2110.09991v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09991v3)
- **Published**: 2021-10-19 14:03:43+00:00
- **Updated**: 2022-04-01 14:23:38+00:00
- **Authors**: Kaiyu Zheng, Rohan Chitnis, Yoonchang Sung, George Konidaris, Stefanie Tellex
- **Comment**: 10 pages, 5 figures, 4 tables. IEEE Conference on Robotics and
  Automation (ICRA) 2022; minor fix in appendix & references
- **Journal**: None
- **Summary**: In realistic applications of object search, robots will need to locate target objects in complex environments while coping with unreliable sensors, especially for small or hard-to-detect objects. In such settings, correlational information can be valuable for planning efficiently. Previous approaches that consider correlational information typically resort to ad-hoc, greedy search strategies. We introduce the Correlational Object Search POMDP (COS-POMDP), which models correlations while preserving optimal solutions with a reduced state space. We propose a hierarchical planning algorithm to scale up COS-POMDPs for practical domains. Our evaluation, conducted with the AI2-THOR household simulator and the YOLOv5 object detector, shows that our method finds objects more successfully and efficiently compared to baselines,particularly for hard-to-detect objects such as srub brush and remote control.



### ERQA: Edge-Restoration Quality Assessment for Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2110.09992v2
- **DOI**: 10.5220/0010780900003124
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09992v2)
- **Published**: 2021-10-19 14:04:16+00:00
- **Updated**: 2022-01-24 12:11:21+00:00
- **Authors**: Anastasia Kirillova, Eugene Lyapustin, Anastasia Antsiferova, Dmitry Vatolin
- **Comment**: Accepted for presentation at the International Conference on Computer
  Vision Theory and Applications (VISAPP) 2022
- **Journal**: None
- **Summary**: Despite the growing popularity of video super-resolution (VSR), there is still no good way to assess the quality of the restored details in upscaled frames. Some SR methods may produce the wrong digit or an entirely different face. Whether a method's results are trustworthy depends on how well it restores truthful details. Image super-resolution can use natural distributions to produce a high-resolution image that is only somewhat similar to the real one. VSR enables exploration of additional information in neighboring frames to restore details from the original scene. The ERQA metric, which we propose in this paper, aims to estimate a model's ability to restore real details using VSR. On the assumption that edges are significant for detail and character recognition, we chose edge fidelity as the foundation for this metric. Experimental validation of our work is based on the MSU Video Super-Resolution Benchmark, which includes the most difficult patterns for detail restoration and verifies the fidelity of details from the original frame. Code for the proposed metric is publicly available at https://github.com/msu-video-group/ERQA.



### DPFM: Deep Partial Functional Maps
- **Arxiv ID**: http://arxiv.org/abs/2110.09994v1
- **DOI**: 10.1109/3DV53792.2021.00040
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.09994v1)
- **Published**: 2021-10-19 14:05:37+00:00
- **Updated**: 2021-10-19 14:05:37+00:00
- **Authors**: Souhaib Attaiki, Gautam Pai, Maks Ovsjanikov
- **Comment**: None
- **Journal**: 2021 International Conference on 3D Vision (3DV)
- **Summary**: We consider the problem of computing dense correspondences between non-rigid shapes with potentially significant partiality. Existing formulations tackle this problem through heavy manifold optimization in the spectral domain, given hand-crafted shape descriptors. In this paper, we propose the first learning method aimed directly at partial non-rigid shape correspondence. Our approach uses the functional map framework, can be trained in a supervised or unsupervised manner, and learns descriptors directly from the data, thus both improving robustness and accuracy in challenging cases. Furthermore, unlike existing techniques, our method is also applicable to partial-to-partial non-rigid matching, in which the common regions on both shapes are unknown a priori. We demonstrate that the resulting method is data-efficient, and achieves state-of-the-art results on several benchmark datasets. Our code and data can be found online: https://github.com/pvnieo/DPFM



### Data-driven and Automatic Surface Texture Analysis Using Persistent Homology
- **Arxiv ID**: http://arxiv.org/abs/2110.10005v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CG, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10005v1)
- **Published**: 2021-10-19 14:19:58+00:00
- **Updated**: 2021-10-19 14:19:58+00:00
- **Authors**: Melih C. Yesilli, Firas A. Khasawneh
- **Comment**: None
- **Journal**: None
- **Summary**: Surface roughness plays an important role in analyzing engineering surfaces. It quantifies the surface topography and can be used to determine whether the resulting surface finish is acceptable or not. Nevertheless, while several existing tools and standards are available for computing surface roughness, these methods rely heavily on user input thus slowing down the analysis and increasing manufacturing costs. Therefore, fast and automatic determination of the roughness level is essential to avoid costs resulting from surfaces with unacceptable finish, and user-intensive analysis. In this study, we propose a Topological Data Analysis (TDA) based approach to classify the roughness level of synthetic surfaces using both their areal images and profiles. We utilize persistent homology from TDA to generate persistence diagrams that encapsulate information on the shape of the surface. We then obtain feature matrices for each surface or profile using Carlsson coordinates, persistence images, and template functions. We compare our results to two widely used methods in the literature: Fast Fourier Transform (FFT) and Gaussian filtering. The results show that our approach yields mean accuracies as high as 97%. We also show that, in contrast to existing surface analysis tools, our TDA-based approach is fully automatable and provides adaptive feature extraction.



### A Data-Driven Reconstruction Technique based on Newton's Method for Emission Tomography
- **Arxiv ID**: http://arxiv.org/abs/2110.11396v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2110.11396v1)
- **Published**: 2021-10-19 14:54:34+00:00
- **Updated**: 2021-10-19 14:54:34+00:00
- **Authors**: Loizos Koutsantonis, Tiago Carneiro, Emmanuel Kieffer, Frederic Pinel, Pascal Bouvry
- **Comment**: 7 pages, 4 figures, Proceedings
- **Journal**: IEEE Nuclear Science Symposium and Medical Imaging Conference 2021
- **Summary**: In this work, we present the Deep Newton Reconstruction Network (DNR-Net), a hybrid data-driven reconstruction technique for emission tomography inspired by Newton's method, a well-known iterative optimization algorithm. The DNR-Net employs prior information about the tomographic problem provided by the projection operator while utilizing deep learning approaches to a) imitate Newton's method by approximating the Newton descent direction and b) provide data-driven regularisation. We demonstrate that DNR-Net is capable of providing high-quality image reconstructions using data from SPECT phantom simulations by applying it to reconstruct images from noisy sinograms, each one containing 24 projections. The Structural Similarity Index (SSIM) and the Contrast-to-Noise ratio (CNR) were used to quantify the image quality. We also compare our results to those obtained by the OSEM method. According to the quantitative results, the DNR-Net produces reconstructions comparable to the ones produced by OSEM while featuring higher contrast and less noise.



### Online Continual Learning on Class Incremental Blurry Task Configuration with Anytime Inference
- **Arxiv ID**: http://arxiv.org/abs/2110.10031v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10031v2)
- **Published**: 2021-10-19 14:59:48+00:00
- **Updated**: 2022-03-21 08:03:20+00:00
- **Authors**: Hyunseo Koh, Dahyun Kim, Jung-Woo Ha, Jonghyun Choi
- **Comment**: to appear in ICLR2022
- **Journal**: None
- **Summary**: Despite rapid advances in continual learning, a large body of research is devoted to improving performance in the existing setups. While a handful of work do propose new continual learning setups, they still lack practicality in certain aspects. For better practicality, we first propose a novel continual learning setup that is online, task-free, class-incremental, of blurry task boundaries and subject to inference queries at any moment. We additionally propose a new metric to better measure the performance of the continual learning methods subject to inference queries at any moment. To address the challenging setup and evaluation protocol, we propose an effective method that employs a new memory management scheme and novel learning techniques. Our empirical validation demonstrates that the proposed method outperforms prior arts by large margins. Code and data splits are available at https://github.com/naver-ai/i-Blurry.



### Improving Tail-Class Representation with Centroid Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.10048v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10048v2)
- **Published**: 2021-10-19 15:24:48+00:00
- **Updated**: 2023-05-04 13:48:07+00:00
- **Authors**: Anthony Meng Huat Tiong, Junnan Li, Guosheng Lin, Boyang Li, Caiming Xiong, Steven C. H. Hoi
- **Comment**: Add in acknowledgment
- **Journal**: None
- **Summary**: In vision domain, large-scale natural datasets typically exhibit long-tailed distribution which has large class imbalance between head and tail classes. This distribution poses difficulty in learning good representations for tail classes. Recent developments have shown good long-tailed model can be learnt by decoupling the training into representation learning and classifier balancing. However, these works pay insufficient consideration on the long-tailed effect on representation learning. In this work, we propose interpolative centroid contrastive learning (ICCL) to improve long-tailed representation learning. ICCL interpolates two images from a class-agnostic sampler and a class-aware sampler, and trains the model such that the representation of the interpolative image can be used to retrieve the centroids for both source classes. We demonstrate the effectiveness of our approach on multiple long-tailed image classification benchmarks. Our result shows a significant accuracy gain of 2.8% on the iNaturalist 2018 dataset with a real-world long-tailed distribution.



### Cross-Sim-NGF: FFT-Based Global Rigid Multimodal Alignment of Image Volumes using Normalized Gradient Fields
- **Arxiv ID**: http://arxiv.org/abs/2110.10156v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10156v1)
- **Published**: 2021-10-19 15:28:37+00:00
- **Updated**: 2021-10-19 15:28:37+00:00
- **Authors**: Johan Öfverstedt, Joakim Lindblad, Nataša Sladoje
- **Comment**: 5 pages, 3 figures, 3 tables. This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible
- **Journal**: None
- **Summary**: Multimodal image alignment involves finding spatial correspondences between volumes varying in appearance and structure. Automated alignment methods are often based on local optimization that can be highly sensitive to their initialization. We propose a global optimization method for rigid multimodal 3D image alignment, based on a novel efficient algorithm for computing similarity of normalized gradient fields (NGF) in the frequency domain. We validate the method experimentally on a dataset comprised of 20 brain volumes acquired in four modalities (T1w, Flair, CT, [18F] FDG PET), synthetically displaced with known transformations. The proposed method exhibits excellent performance on all six possible modality combinations, and outperforms all four reference methods by a large margin. The method is fast; a 3.4Mvoxel global rigid alignment requires approximately 40 seconds of computation, and the proposed algorithm outperforms a direct algorithm for the same task by more than three orders of magnitude. Open-source implementation is provided.



### Stochastic Primal-Dual Deep Unrolling
- **Arxiv ID**: http://arxiv.org/abs/2110.10093v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2110.10093v4)
- **Published**: 2021-10-19 16:46:03+00:00
- **Updated**: 2022-02-15 11:10:56+00:00
- **Authors**: Junqi Tang, Subhadip Mukherjee, Carola-Bibiane Schönlieb
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new type of efficient deep-unrolling networks for solving imaging inverse problems. Conventional deep-unrolling methods require full forward operator and its adjoint across each layer, and hence can be significantly more expensive computationally as compared with other end-to-end methods that are based on post-processing of model-based reconstructions, especially for 3D image reconstruction tasks. We develop a stochastic (ordered-subsets) variant of the classical learned primal-dual (LPD), which is a state-of-the-art unrolling network for tomographic image reconstruction. The proposed learned stochastic primal-dual (LSPD) network only uses subsets of the forward and adjoint operators and offers considerable computational efficiency. We provide theoretical analysis of a special case of our LSPD framework, suggesting that it has the potential to achieve image reconstruction quality competitive with the full-batch LPD while requiring only a fraction of the computation. The numerical results for two different X-ray computed tomography (CT) imaging tasks (namely, low-dose and sparse-view CT) corroborate this theoretical finding, demonstrating the promise of LSPD networks for large-scale imaging problems.



### Domain Generalization through Audio-Visual Relative Norm Alignment in First Person Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.10101v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10101v1)
- **Published**: 2021-10-19 16:52:39+00:00
- **Updated**: 2021-10-19 16:52:39+00:00
- **Authors**: Mirco Planamente, Chiara Plizzari, Emanuele Alberti, Barbara Caputo
- **Comment**: Accepted at WACV 2022. arXiv admin note: substantial text overlap
  with arXiv:2106.01689
- **Journal**: None
- **Summary**: First person action recognition is becoming an increasingly researched area thanks to the rising popularity of wearable cameras. This is bringing to light cross-domain issues that are yet to be addressed in this context. Indeed, the information extracted from learned representations suffers from an intrinsic "environmental bias". This strongly affects the ability to generalize to unseen scenarios, limiting the application of current methods to real settings where labeled data are not available during training. In this work, we introduce the first domain generalization approach for egocentric activity recognition, by proposing a new audio-visual loss, called Relative Norm Alignment loss. It re-balances the contributions from the two modalities during training, over different domains, by aligning their feature norm representations. Our approach leads to strong results in domain generalization on both EPIC-Kitchens-55 and EPIC-Kitchens-100, as demonstrated by extensive experiments, and can be extended to work also on domain adaptation settings with competitive results.



### Hand-Object Contact Prediction via Motion-Based Pseudo-Labeling and Guided Progressive Label Correction
- **Arxiv ID**: http://arxiv.org/abs/2110.10174v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10174v1)
- **Published**: 2021-10-19 18:00:02+00:00
- **Updated**: 2021-10-19 18:00:02+00:00
- **Authors**: Takuma Yagi, Md Tasnimul Hasan, Yoichi Sato
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Every hand-object interaction begins with contact. Despite predicting the contact state between hands and objects is useful in understanding hand-object interactions, prior methods on hand-object analysis have assumed that the interacting hands and objects are known, and were not studied in detail. In this study, we introduce a video-based method for predicting contact between a hand and an object. Specifically, given a video and a pair of hand and object tracks, we predict a binary contact state (contact or no-contact) for each frame. However, annotating a large number of hand-object tracks and contact labels is costly. To overcome the difficulty, we propose a semi-supervised framework consisting of (i) automatic collection of training data with motion-based pseudo-labels and (ii) guided progressive label correction (gPLC), which corrects noisy pseudo-labels with a small amount of trusted data. We validated our framework's effectiveness on a newly built benchmark dataset for hand-object contact prediction and showed superior performance against existing baseline methods. Code and data are available at https://github.com/takumayagi/hand_object_contact_prediction.



### Cascaded Cross MLP-Mixer GANs for Cross-View Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2110.10183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10183v1)
- **Published**: 2021-10-19 18:03:30+00:00
- **Updated**: 2021-10-19 18:03:30+00:00
- **Authors**: Bin Ren, Hao Tang, Nicu Sebe
- **Comment**: 16 pages, 5 figures
- **Journal**: None
- **Summary**: It is hard to generate an image at target view well for previous cross-view image translation methods that directly adopt a simple encoder-decoder or U-Net structure, especially for drastically different views and severe deformation cases. To ease this problem, we propose a novel two-stage framework with a new Cascaded Cross MLP-Mixer (CrossMLP) sub-network in the first stage and one refined pixel-level loss in the second stage. In the first stage, the CrossMLP sub-network learns the latent transformation cues between image code and semantic map code via our novel CrossMLP blocks. Then the coarse results are generated progressively under the guidance of those cues. Moreover, in the second stage, we design a refined pixel-level loss that eases the noisy semantic label problem with more reasonable regularization in a more compact fashion for better optimization. Extensive experimental results on Dayton~\cite{vo2016localizing} and CVUSA~\cite{workman2015wide} datasets show that our method can generate significantly better results than state-of-the-art methods. The source code and trained models are available at https://github.com/Amazingren/CrossMLP.



### StructFormer: Learning Spatial Structure for Language-Guided Semantic Rearrangement of Novel Objects
- **Arxiv ID**: http://arxiv.org/abs/2110.10189v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10189v1)
- **Published**: 2021-10-19 18:13:01+00:00
- **Updated**: 2021-10-19 18:13:01+00:00
- **Authors**: Weiyu Liu, Chris Paxton, Tucker Hermans, Dieter Fox
- **Comment**: None
- **Journal**: None
- **Summary**: Geometric organization of objects into semantically meaningful arrangements pervades the built world. As such, assistive robots operating in warehouses, offices, and homes would greatly benefit from the ability to recognize and rearrange objects into these semantically meaningful structures. To be useful, these robots must contend with previously unseen objects and receive instructions without significant programming. While previous works have examined recognizing pairwise semantic relations and sequential manipulation to change these simple relations none have shown the ability to arrange objects into complex structures such as circles or table settings. To address this problem we propose a novel transformer-based neural network, StructFormer, which takes as input a partial-view point cloud of the current object arrangement and a structured language command encoding the desired object configuration. We show through rigorous experiments that StructFormer enables a physical robot to rearrange novel objects into semantically meaningful structures with multi-object relational constraints inferred from the language command.



### CoFi: Coarse-to-Fine ICP for LiDAR Localization in an Efficient Long-lasting Point Cloud Map
- **Arxiv ID**: http://arxiv.org/abs/2110.10194v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2110.10194v1)
- **Published**: 2021-10-19 18:23:08+00:00
- **Updated**: 2021-10-19 18:23:08+00:00
- **Authors**: Yecheng Lyu, Xinming Huang, Ziming Zhang
- **Comment**: 8 pages, submitted to ICRA 2022
- **Journal**: None
- **Summary**: LiDAR odometry and localization has attracted increasing research interest in recent years. In the existing works, iterative closest point (ICP) is widely used since it is precise and efficient. Due to its non-convexity and its local iterative strategy, however, ICP-based method easily falls into local optima, which in turn calls for a precise initialization. In this paper, we propose CoFi, a Coarse-to-Fine ICP algorithm for LiDAR localization. Specifically, the proposed algorithm down-samples the input point sets under multiple voxel resolution, and gradually refines the transformation from the coarse point sets to the fine-grained point sets. In addition, we propose a map based LiDAR localization algorithm that extracts semantic feature points from the LiDAR frames and apply CoFi to estimate the pose on an efficient point cloud map. With the help of the Cylinder3D algorithm for LiDAR scan semantic segmentation, the proposed CoFi localization algorithm demonstrates the state-of-the-art performance on the KITTI odometry benchmark, with significant improvement over the literature.



### Evaluating and Improving Interactions with Hazy Oracles
- **Arxiv ID**: http://arxiv.org/abs/2110.10206v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10206v3)
- **Published**: 2021-10-19 19:01:30+00:00
- **Updated**: 2022-11-30 19:28:56+00:00
- **Authors**: Stephan J. Lemmer, Jason J. Corso
- **Comment**: To be published in the Proceedings of the 2023 AAAI Conference on
  Artificial Intelligence (AAAI) 14 pages, 5 tables, 9 figures
- **Journal**: None
- **Summary**: Many AI systems integrate sensor inputs, world knowledge, and human-provided information to perform inference. While such systems often treat the human input as flawless, humans are better thought of as hazy oracles whose input may be ambiguous or outside of the AI system's understanding. In such situations it makes sense for the AI system to defer its inference while it disambiguates the human-provided information by, for example, asking the human to rephrase the query. Though this approach has been considered in the past, current work is typically limited to application-specific methods and non-standardized human experiments. We instead introduce and formalize a general notion of deferred inference. Using this formulation, we then propose a novel evaluation centered around the Deferred Error Volume (DEV) metric, which explicitly considers the tradeoff between error reduction and the additional human effort required to achieve it. We demonstrate this new formalization and an innovative deferred inference method on the disparate tasks of Single-Target Video Object Tracking and Referring Expression Comprehension, ultimately reducing error by up to 48% without any change to the underlying model or its parameters.



### Learning Partial Equivariances from Data
- **Arxiv ID**: http://arxiv.org/abs/2110.10211v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10211v3)
- **Published**: 2021-10-19 19:17:32+00:00
- **Updated**: 2023-01-14 16:24:14+00:00
- **Authors**: David W. Romero, Suhas Lohit
- **Comment**: Published at NeurIPS 2022
- **Journal**: None
- **Summary**: Group Convolutional Neural Networks (G-CNNs) constrain learned features to respect the symmetries in the selected group, and lead to better generalization when these symmetries appear in the data. If this is not the case, however, equivariance leads to overly constrained models and worse performance. Frequently, transformations occurring in data can be better represented by a subset of a group than by a group as a whole, e.g., rotations in $[-90^{\circ}, 90^{\circ}]$. In such cases, a model that respects equivariance $\textit{partially}$ is better suited to represent the data. In addition, relevant transformations may differ for low and high-level features. For instance, full rotation equivariance is useful to describe edge orientations in a face, but partial rotation equivariance is better suited to describe face poses relative to the camera. In other words, the optimal level of equivariance may differ per layer. In this work, we introduce $\textit{Partial G-CNNs}$: G-CNNs able to learn layer-wise levels of partial and full equivariance to discrete, continuous groups and combinations thereof as part of training. Partial G-CNNs retain full equivariance when beneficial, e.g., for rotated MNIST, but adjust it whenever it becomes harmful, e.g., for classification of 6 / 9 digits or natural images. We empirically show that partial G-CNNs pair G-CNNs when full equivariance is advantageous, and outperform them otherwise.



### An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.10217v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10217v1)
- **Published**: 2021-10-19 19:31:52+00:00
- **Updated**: 2021-10-19 19:31:52+00:00
- **Authors**: Peyton Chandarana, Junlin Ou, Ramtin Zand
- **Comment**: None
- **Journal**: None
- **Summary**: Current state-of-the-art methods of image classification using convolutional neural networks are often constrained by both latency and power consumption. This places a limit on the devices, particularly low-power edge devices, that can employ these methods. Spiking neural networks (SNNs) are considered to be the third generation of artificial neural networks which aim to address these latency and power constraints by taking inspiration from biological neuronal communication processes. Before data such as images can be input into an SNN, however, they must be first encoded into spike trains. Herein, we propose a method for encoding static images into temporal spike trains using edge detection and an adaptive signal sampling method for use in SNNs. The edge detection process consists of first performing Canny edge detection on the 2D static images and then converting the edge detected images into two X and Y signals using an image-to-signal conversion method. The adaptive signaling approach consists of sampling the signals such that the signals maintain enough detail and are sensitive to abrupt changes in the signal. Temporal encoding mechanisms such as threshold-based representation (TBR) and step-forward (SF) are then able to be used to convert the sampled signals into spike trains. We use various error and indicator metrics to optimize and evaluate the efficiency and precision of the proposed image encoding approach. Comparison results between the original and reconstructed signals from spike trains generated using edge-detection and adaptive temporal encoding mechanism exhibit 18x and 7x reduction in average root mean square error (RMSE) compared to the conventional SF and TBR encoding, respectively, while used for encoding MNIST dataset.



### Test time Adaptation through Perturbation Robustness
- **Arxiv ID**: http://arxiv.org/abs/2110.10232v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10232v1)
- **Published**: 2021-10-19 20:00:58+00:00
- **Updated**: 2021-10-19 20:00:58+00:00
- **Authors**: Prabhu Teja Sivaprasad, François Fleuret
- **Comment**: Under review
- **Journal**: None
- **Summary**: Data samples generated by several real world processes are dynamic in nature \textit{i.e.}, their characteristics vary with time. Thus it is not possible to train and tackle all possible distributional shifts between training and inference, using the host of transfer learning methods in literature. In this paper, we tackle this problem of adapting to domain shift at inference time \textit{i.e.}, we do not change the training process, but quickly adapt the model at test-time to handle any domain shift. For this, we propose to enforce consistency of predictions of data sampled in the vicinity of test sample on the image manifold. On a host of test scenarios like dealing with corruptions (CIFAR-10-C and CIFAR-100-C), and domain adaptation (VisDA-C), our method is at par or significantly outperforms previous methods.



### 1st Place Solution for the UVO Challenge on Image-based Open-World Segmentation 2021
- **Arxiv ID**: http://arxiv.org/abs/2110.10239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10239v1)
- **Published**: 2021-10-19 20:19:07+00:00
- **Updated**: 2021-10-19 20:19:07+00:00
- **Authors**: Yuming Du, Wen Guo, Yang Xiao, Vincent Lepetit
- **Comment**: Code:https://github.com/dulucas/UVO_Challenge
- **Journal**: None
- **Summary**: We describe our two-stage instance segmentation framework we use to compete in the challenge. The first stage of our framework consists of an object detector, which generates object proposals in the format of bounding boxes. Then, the images and the detected bounding boxes are fed to the second stage, where a segmentation network is applied to segment the objects in the bounding boxes. We train all our networks in a class-agnostic way. Our approach achieves the first place in the UVO 2021 Image-based Open-World Segmentation Challenge.



### A New Automatic Change Detection Frame-work Based on Region Growing and Weighted Local Mutual Information: Analysis of Breast Tumor Response to Chemotherapy in Serial MR Images
- **Arxiv ID**: http://arxiv.org/abs/2110.10242v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10242v1)
- **Published**: 2021-10-19 20:28:45+00:00
- **Updated**: 2021-10-19 20:28:45+00:00
- **Authors**: Narges Norouzi, Reza Azmi, Nooshin Noshiri, Robab Anbiaee
- **Comment**: 18 pages, 16 figures, 14 tables
- **Journal**: None
- **Summary**: The automatic analysis of subtle changes between longitudinal MR images is an important task as it is still a challenging issue in scope of the breast medical image processing. In this paper we propose an effective automatic change detection framework composed of two phases since previously used methods have features with low distinctive power. First, in the preprocessing phase an intensity normalization method is suggested based on Hierarchical Histogram Matching (HHM) that is more robust to noise than previous methods. To eliminate undesirable changes and extract the regions containing significant changes the proposed Extraction Region of Changes (EROC) method is applied based on intensity distribution and Hill-Climbing algorithm. Second, in the detection phase a region growing-based approach is suggested to differentiate significant changes from unreal ones. Due to using proposed Weighted Local Mutual Information (WLMI) method to extract high level features and also utilizing the principle of the local consistency of changes, the proposed approach enjoys reasonable performance. The experimental results on both simulated and real longitudinal Breast MR Images confirm the effectiveness of the proposed framework. Also, this framework outperforms the human expert in some cases which can detect many lesion evolutions that are missed by expert.



### Early- and in-season crop type mapping without current-year ground truth: generating labels from historical information via a topology-based approach
- **Arxiv ID**: http://arxiv.org/abs/2110.10275v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10275v1)
- **Published**: 2021-10-19 21:44:02+00:00
- **Updated**: 2021-10-19 21:44:02+00:00
- **Authors**: Chenxi Lin, Liheng Zhong, Xiao-Peng Song, Jinwei Dong, David B. Lobell, Zhenong Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Land cover classification in remote sensing is often faced with the challenge of limited ground truth. Incorporating historical information has the potential to significantly lower the expensive cost associated with collecting ground truth and, more importantly, enable early- and in-season mapping that is helpful to many pre-harvest decisions. In this study, we propose a new approach that can effectively transfer knowledge about the topology (i.e. relative position) of different crop types in the spectral feature space (e.g. the histogram of SWIR1 vs RDEG1 bands) to generate labels, thereby support crop classification in a different year. Importantly, our approach does not attempt to transfer classification decision boundaries that are susceptible to inter-annual variations of weather and management, but relies on the more robust and shift-invariant topology information. We tested this approach for mapping corn/soybeans in the US Midwest and paddy rice/corn/soybeans in Northeast China using Landsat-8 and Sentinel-2 data. Results show that our approach automatically generates high-quality labels for crops in the target year immediately after each image becomes available. Based on these generated labels from our approach, the subsequent crop type mapping using a random forest classifier reach the F1 score as high as 0.887 for corn as early as the silking stage and 0.851 for soybean as early as the flowering stage and the overall accuracy of 0.873 in Iowa. In Northeast China, F1 scores of paddy rice, corn and soybeans and the overall accuracy can exceed 0.85 two and half months ahead of harvest. Overall, these results highlight unique advantages of our approach in transferring historical knowledge and maximizing the timeliness of crop maps. Our approach supports a general paradigm shift towards learning transferrable and generalizable knowledge to facilitate land cover classification.



### Fine-Grained Control of Artistic Styles in Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2110.10278v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10278v2)
- **Published**: 2021-10-19 21:51:52+00:00
- **Updated**: 2021-10-25 00:16:18+00:00
- **Authors**: Xin Miao, Huayan Wang, Jun Fu, Jiayi Liu, Shen Wang, Zhenyu Liao
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in generative models and adversarial training have enabled artificially generating artworks in various artistic styles. It is highly desirable to gain more control over the generated style in practice. However, artistic styles are unlike object categories -- there are a continuous spectrum of styles distinguished by subtle differences. Few works have been explored to capture the continuous spectrum of styles and apply it to a style generation task. In this paper, we propose to achieve this by embedding original artwork examples into a continuous style space. The style vectors are fed to the generator and discriminator to achieve fine-grained control. Our method can be used with common generative adversarial networks (such as StyleGAN). Experiments show that our method not only precisely controls the fine-grained artistic style but also improves image quality over vanilla StyleGAN as measured by FID.



### On Coordinate Decoding for Keypoint Estimation Tasks
- **Arxiv ID**: http://arxiv.org/abs/2110.10289v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10289v1)
- **Published**: 2021-10-19 22:14:48+00:00
- **Updated**: 2021-10-19 22:14:48+00:00
- **Authors**: Anargyros Chatzitofis, Nikolaos Zioulis, Georgios Nikolaos Albanis, Dimitrios Zarpalas, Petros Daras
- **Comment**: None
- **Journal**: None
- **Summary**: A series of 2D (and 3D) keypoint estimation tasks are built upon heatmap coordinate representation, i.e. a probability map that allows for learnable and spatially aware encoding and decoding of keypoint coordinates on grids, even allowing for sub-pixel coordinate accuracy. In this report, we aim to reproduce the findings of DARK that investigated the 2D heatmap representation by highlighting the importance of the encoding of the ground truth heatmap and the decoding of the predicted heatmap to keypoint coordinates. The authors claim that a) a more principled distribution-aware coordinate decoding method overcomes the limitations of the standard techniques widely used in the literature, and b), that the reconstruction of heatmaps from ground-truth coordinates by generating accurate and continuous heatmap distributions lead to unbiased model training, contrary to the standard coordinate encoding process that quantizes the keypoint coordinates on the resolution of the input image grid.



### Learning Rich Nearest Neighbor Representations from Self-supervised Ensembles
- **Arxiv ID**: http://arxiv.org/abs/2110.10293v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10293v1)
- **Published**: 2021-10-19 22:24:57+00:00
- **Updated**: 2021-10-19 22:24:57+00:00
- **Authors**: Bram Wallace, Devansh Arpit, Huan Wang, Caiming Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Pretraining convolutional neural networks via self-supervision, and applying them in transfer learning, is an incredibly fast-growing field that is rapidly and iteratively improving performance across practically all image domains. Meanwhile, model ensembling is one of the most universally applicable techniques in supervised learning literature and practice, offering a simple solution to reliably improve performance. But how to optimally combine self-supervised models to maximize representation quality has largely remained unaddressed. In this work, we provide a framework to perform self-supervised model ensembling via a novel method of learning representations directly through gradient descent at inference time. This technique improves representation quality, as measured by k-nearest neighbors, both on the in-domain dataset and in the transfer setting, with models transferable from the former setting to the latter. Additionally, this direct learning of feature through backpropagation improves representations from even a single model, echoing the improvements found in self-distillation.



### Momentum Contrastive Autoencoder: Using Contrastive Learning for Latent Space Distribution Matching in WAE
- **Arxiv ID**: http://arxiv.org/abs/2110.10303v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10303v2)
- **Published**: 2021-10-19 22:55:47+00:00
- **Updated**: 2023-02-15 23:20:36+00:00
- **Authors**: Devansh Arpit, Aadyot Bhatnagar, Huan Wang, Caiming Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Wasserstein autoencoder (WAE) shows that matching two distributions is equivalent to minimizing a simple autoencoder (AE) loss under the constraint that the latent space of this AE matches a pre-specified prior distribution. This latent space distribution matching is a core component of WAE, and a challenging task. In this paper, we propose to use the contrastive learning framework that has been shown to be effective for self-supervised representation learning, as a means to resolve this problem. We do so by exploiting the fact that contrastive learning objectives optimize the latent space distribution to be uniform over the unit hyper-sphere, which can be easily sampled from. We show that using the contrastive learning framework to optimize the WAE loss achieves faster convergence and more stable optimization compared with existing popular algorithms for WAE. This is also reflected in the FID scores on CelebA and CIFAR-10 datasets, and the realistic generated image quality on the CelebA-HQ dataset.



### Constrained Mean Shift for Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.10309v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10309v1)
- **Published**: 2021-10-19 23:14:23+00:00
- **Updated**: 2021-10-19 23:14:23+00:00
- **Authors**: Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Hamed Pirsiavash
- **Comment**: None
- **Journal**: None
- **Summary**: We are interested in representation learning from labeled or unlabeled data. Inspired by recent success of self-supervised learning (SSL), we develop a non-contrastive representation learning method that can exploit additional knowledge. This additional knowledge may come from annotated labels in the supervised setting or an SSL model from another modality in the SSL setting. Our main idea is to generalize the mean-shift algorithm by constraining the search space of nearest neighbors, resulting in semantically purer representations. Our method simply pulls the embedding of an instance closer to its nearest neighbors in a search space that is constrained using the additional knowledge. By leveraging this non-contrastive loss, we show that the supervised ImageNet-1k pretraining with our method results in better transfer performance as compared to the baselines. Further, we demonstrate that our method is relatively robust to label noise. Finally, we show that it is possible to use the noisy constraint across modalities to train self-supervised video models.



