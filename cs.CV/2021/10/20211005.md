# Arxiv Papers in cs.CV on 2021-10-05
### An Integrated System for Mobile Image-Based Dietary Assessment
- **Arxiv ID**: http://arxiv.org/abs/2110.01754v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2110.01754v1)
- **Published**: 2021-10-05 00:04:19+00:00
- **Updated**: 2021-10-05 00:04:19+00:00
- **Authors**: Zeman Shao, Yue Han, Jiangpeng He, Runyu Mao, Janine Wright, Deborah Kerr, Carol Boushey, Fengqing Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate assessment of dietary intake requires improved tools to overcome limitations of current methods including user burden and measurement error. Emerging technologies such as image-based approaches using advanced machine learning techniques coupled with widely available mobile devices present new opportunities to improve the accuracy of dietary assessment that is cost-effective, convenient and timely. However, the quality and quantity of datasets are essential for achieving good performance for automated image analysis. Building a large image dataset with high quality groundtruth annotation is a challenging problem, especially for food images as the associated nutrition information needs to be provided or verified by trained dietitians with domain knowledge. In this paper, we present the design and development of a mobile, image-based dietary assessment system to capture and analyze dietary intake, which has been deployed in both controlled-feeding and community-dwelling dietary studies. Our system is capable of collecting high quality food images in naturalistic settings and provides groundtruth annotations for developing new computational approaches.



### Bottom-up Hierarchical Classification Using Confusion-based Logit Compression
- **Arxiv ID**: http://arxiv.org/abs/2110.01756v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2110.01756v1)
- **Published**: 2021-10-05 00:18:58+00:00
- **Updated**: 2021-10-05 00:18:58+00:00
- **Authors**: Tong Liang, Jim Davis, Roman Ilin
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a method to efficiently compute label posteriors of a base flat classifier in the presence of few validation examples within a bottom-up hierarchical inference framework. A stand-alone validation set (not used to train the base classifier) is preferred for posterior estimation to avoid overfitting the base classifier, however a small validation set limits the number of features one can effectively use. We propose a simple, yet robust, logit vector compression approach based on generalized logits and label confusions for the task of label posterior estimation within the context of hierarchical classification. Extensive comparative experiments with other compression techniques are provided across multiple sized validation sets, and a comparison with related hierarchical classification approaches is also conducted. The proposed approach mitigates the problem of not having enough validation examples for reliable posterior estimation while maintaining strong hierarchical classification performance.



### Quantified Facial Expressiveness for Affective Behavior Analytics
- **Arxiv ID**: http://arxiv.org/abs/2110.01758v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01758v2)
- **Published**: 2021-10-05 00:21:33+00:00
- **Updated**: 2021-10-07 14:55:24+00:00
- **Authors**: Md Taufeeq Uddin, Shaun Canavan
- **Comment**: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV).
  2022
- **Journal**: None
- **Summary**: The quantified measurement of facial expressiveness is crucial to analyze human affective behavior at scale. Unfortunately, methods for expressiveness quantification at the video frame-level are largely unexplored, unlike the study of discrete expression. In this work, we propose an algorithm that quantifies facial expressiveness using a bounded, continuous expressiveness score using multimodal facial features, such as action units (AUs), landmarks, head pose, and gaze. The proposed algorithm more heavily weights AUs with high intensities and large temporal changes. The proposed algorithm can compute the expressiveness in terms of discrete expression, and can be used to perform tasks including facial behavior tracking and subjectivity quantification in context. Our results on benchmark datasets show the proposed algorithm is effective in terms of capturing temporal changes and expressiveness, measuring subjective differences in context, and extracting useful insight.



### Proxy-bridged Image Reconstruction Network for Anomaly Detection in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2110.01761v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.01761v1)
- **Published**: 2021-10-05 00:40:43+00:00
- **Updated**: 2021-10-05 00:40:43+00:00
- **Authors**: Kang Zhou, Jing Li, Weixin Luo, Zhengxin Li, Jianlong Yang, Huazhu Fu, Jun Cheng, Jiang Liu, Shenghua Gao
- **Comment**: This paper is accepted to IEEE TMI
- **Journal**: None
- **Summary**: Anomaly detection in medical images refers to the identification of abnormal images with only normal images in the training set. Most existing methods solve this problem with a self-reconstruction framework, which tends to learn an identity mapping and reduces the sensitivity to anomalies. To mitigate this problem, in this paper, we propose a novel Proxy-bridged Image Reconstruction Network (ProxyAno) for anomaly detection in medical images. Specifically, we use an intermediate proxy to bridge the input image and the reconstructed image. We study different proxy types, and we find that the superpixel-image (SI) is the best one. We set all pixels' intensities within each superpixel as their average intensity, and denote this image as SI. The proposed ProxyAno consists of two modules, a Proxy Extraction Module and an Image Reconstruction Module. In the Proxy Extraction Module, a memory is introduced to memorize the feature correspondence for normal image to its corresponding SI, while the memorized correspondence does not apply to the abnormal images, which leads to the information loss for abnormal image and facilitates the anomaly detection. In the Image Reconstruction Module, we map an SI to its reconstructed image. Further, we crop a patch from the image and paste it on the normal SI to mimic the anomalies, and enforce the network to reconstruct the normal image even with the pseudo abnormal SI. In this way, our network enlarges the reconstruction error for anomalies. Extensive experiments on brain MR images, retinal OCT images and retinal fundus images verify the effectiveness of our method for both image-level and pixel-level anomaly detection.



### Procedure Planning in Instructional Videos via Contextual Modeling and Model-based Policy Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.01770v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.01770v2)
- **Published**: 2021-10-05 01:06:53+00:00
- **Updated**: 2021-10-08 18:27:02+00:00
- **Authors**: Jing Bi, Jiebo Luo, Chenliang Xu
- **Comment**: ICCV 2021 Oral
- **Journal**: None
- **Summary**: Learning new skills by observing humans' behaviors is an essential capability of AI. In this work, we leverage instructional videos to study humans' decision-making processes, focusing on learning a model to plan goal-directed actions in real-life videos. In contrast to conventional action recognition, goal-directed actions are based on expectations of their outcomes requiring causal knowledge of potential consequences of actions. Thus, integrating the environment structure with goals is critical for solving this task. Previous works learn a single world model will fail to distinguish various tasks, resulting in an ambiguous latent space; planning through it will gradually neglect the desired outcomes since the global information of the future goal degrades quickly as the procedure evolves. We address these limitations with a new formulation of procedure planning and propose novel algorithms to model human behaviors through Bayesian Inference and model-based Imitation Learning. Experiments conducted on real-world instructional videos show that our method can achieve state-of-the-art performance in reaching the indicated goals. Furthermore, the learned contextual information presents interesting features for planning in a latent space.



### HighlightMe: Detecting Highlights from Human-Centric Videos
- **Arxiv ID**: http://arxiv.org/abs/2110.01774v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01774v1)
- **Published**: 2021-10-05 01:18:15+00:00
- **Updated**: 2021-10-05 01:18:15+00:00
- **Authors**: Uttaran Bhattacharya, Gang Wu, Stefano Petrangeli, Viswanathan Swaminathan, Dinesh Manocha
- **Comment**: 10 pages, 5 figures, 5 tables. In Proceedings of the IEEE/CVF
  International Conference on Computer Vision (ICCV), 2021
- **Journal**: None
- **Summary**: We present a domain- and user-preference-agnostic approach to detect highlightable excerpts from human-centric videos. Our method works on the graph-based representation of multiple observable human-centric modalities in the videos, such as poses and faces. We use an autoencoder network equipped with spatial-temporal graph convolutions to detect human activities and interactions based on these modalities. We train our network to map the activity- and interaction-based latent structural representations of the different modalities to per-frame highlight scores based on the representativeness of the frames. We use these scores to compute which frames to highlight and stitch contiguous frames to produce the excerpts. We train our network on the large-scale AVA-Kinetics action dataset and evaluate it on four benchmark video highlight datasets: DSH, TVSum, PHD2, and SumMe. We observe a 4-12% improvement in the mean average precision of matching the human-annotated highlights over state-of-the-art methods in these datasets, without requiring any user-provided preferences or dataset-specific fine-tuning.



### Deep Instance Segmentation with Automotive Radar Detection Points
- **Arxiv ID**: http://arxiv.org/abs/2110.01775v7
- **DOI**: 10.1109/TIV.2022.3168899
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2110.01775v7)
- **Published**: 2021-10-05 01:18:27+00:00
- **Updated**: 2023-02-05 11:28:48+00:00
- **Authors**: Jianan Liu, Weiyi Xiong, Liping Bai, Yuxuan Xia, Tao Huang, Wanli Ouyang, Bing Zhu
- **Comment**: 11 pages, 9 figures, 3 tables, accepted by IEEE Transactions on
  Intelligent Vehicles
- **Journal**: None
- **Summary**: Automotive radar provides reliable environmental perception in all-weather conditions with affordable cost, but it hardly supplies semantic and geometry information due to the sparsity of radar detection points. With the development of automotive radar technologies in recent years, instance segmentation becomes possible by using automotive radar. Its data contain contexts such as radar cross section and micro-Doppler effects, and sometimes can provide detection when the field of view is obscured. The outcome from instance segmentation could be potentially used as the input of trackers for tracking targets. The existing methods often utilize a clustering-based classification framework, which fits the need of real-time processing but has limited performance due to minimum information provided by sparse radar detection points. In this paper, we propose an efficient method based on clustering of estimated semantic information to achieve instance segmentation for the sparse radar detection points. In addition, we show that the performance of the proposed approach can be further enhanced by incorporating the visual multi-layer perceptron. The effectiveness of the proposed method is verified by experimental results on the popular RadarScenes dataset, achieving 89.53% mean coverage and 86.97% mean average precision with the IoU threshold of 0.5, which is superior to other approaches in the literature. More significantly, the consumed memory is around 1MB, and the inference time is less than 40ms, indicating that our proposed algorithm is storage and time efficient. These two criteria ensure the practicality of the proposed method in real-world systems.



### MetaPix: Domain Transfer for Semantic Segmentation by Meta Pixel Weighting
- **Arxiv ID**: http://arxiv.org/abs/2110.01777v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.01777v1)
- **Published**: 2021-10-05 01:31:00+00:00
- **Updated**: 2021-10-05 01:31:00+00:00
- **Authors**: Yiren Jian, Chongyang Gao
- **Comment**: None
- **Journal**: Vision and Image Computing, 2021
- **Summary**: Training a deep neural model for semantic segmentation requires collecting a large amount of pixel-level labeled data. To alleviate the data scarcity problem presented in the real world, one could utilize synthetic data whose label is easy to obtain. Previous work has shown that the performance of a semantic segmentation model can be improved by training jointly with real and synthetic examples with a proper weighting on the synthetic data. Such weighting was learned by a heuristic to maximize the similarity between synthetic and real examples. In our work, we instead learn a pixel-level weighting of the synthetic data by meta-learning, i.e., the learning of weighting should only be minimizing the loss on the target task. We achieve this by gradient-on-gradient technique to propagate the target loss back into the parameters of the weighting model. The experiments show that our method with only one single meta module can outperform a complicated combination of an adversarial feature alignment, a reconstruction loss, plus a hierarchical heuristic weighting at pixel, region and image levels.



### Deep Subspace analysing for Semi-Supervised multi-label classification of Diabetic Foot Ulcer
- **Arxiv ID**: http://arxiv.org/abs/2110.01795v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.01795v1)
- **Published**: 2021-10-05 03:00:42+00:00
- **Updated**: 2021-10-05 03:00:42+00:00
- **Authors**: Azadeh Alavi
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Diabetes is a global raising pandemic. Diabetes patients are at risk of developing foot ulcer that usually leads to limb amputation. In order to develop a self monitoring mobile application, in this work, we propose a novel deep subspace analysis pipeline for semi-supervised diabetic foot ulcer mulit-label classification. To avoid any chance of over-fitting, unlike recent state of the art deep semi-supervised methods, the proposed pipeline dose not include any data augmentation. Whereas, after extracting deep features, in order to make the representation shift invariant, we employ variety of data augmentation methods on each image and generate an image-sets, which is then mapped into a linear subspace. Moreover, the proposed pipeline reduces the cost of retraining when more new unlabelled data become available. Thus, the first stage of the pipeline employs the concept of transfer learning for feature extraction purpose through modifying and retraining a deep convolutional network architect known as Xception. Then, the output of a mid-layer is extracted to generate an image set representer of any given image with help of data augmentation methods. At this stage, each image is transferred to a linear subspace which is a point on a Grassmann Manifold topological space. Hence, to perform analyse them, the geometry of such manifold must be considered. As such, each labelled image is represented as a vector of distances to number of unlabelled images using geodesic distance on Grassmann manifold. Finally, Random Forest is trained for multi-label classification of diabetic foot ulcer images. The method is then evaluated on the blind test set provided by DFU2021 competition, and the result considerable improvement compared to using classical transfer learning with data augmentation.



### Self-Supervised Learning of Perceptually Optimized Block Motion Estimates for Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2110.01805v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2110.01805v4)
- **Published**: 2021-10-05 03:38:43+00:00
- **Updated**: 2022-12-04 04:55:09+00:00
- **Authors**: Somdyuti Paul, Andrey Norkin, Alan C. Bovik
- **Comment**: None
- **Journal**: None
- **Summary**: Block based motion estimation is integral to inter prediction processes performed in hybrid video codecs. Prevalent block matching based methods that are used to compute block motion vectors (MVs) rely on computationally intensive search procedures. They also suffer from the aperture problem, which can worsen as the block size is reduced. Moreover, the block matching criteria used in typical codecs do not account for the resulting levels of perceptual quality of the motion compensated pictures that are created upon decoding. Towards achieving the elusive goal of perceptually optimized motion estimation, we propose a search-free block motion estimation framework using a multi-stage convolutional neural network, which is able to conduct motion estimation on multiple block sizes simultaneously, using a triplet of frames as input. This composite block translation network (CBT-Net) is trained in a self-supervised manner on a large database that we created from publicly available uncompressed video content. We deploy the multi-scale structural similarity (MS-SSIM) loss function to optimize the perceptual quality of the motion compensated predicted frames. Our experimental results highlight the computational efficiency of our proposed model relative to conventional block matching based motion estimation algorithms, for comparable prediction errors. Further, when used to perform inter prediction in AV1, the MV predictions of the perceptually optimized model result in average Bjontegaard-delta rate (BD-rate) improvements of -1.70% and -1.52% with respect to the MS-SSIM and Video Multi-Method Assessment Fusion (VMAF) quality metrics, respectively as compared to the block matching based motion estimation system employed in the SVT-AV1 encoder.



### DA-DRN: Degradation-Aware Deep Retinex Network for Low-Light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2110.01809v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2110.01809v1)
- **Published**: 2021-10-05 03:53:52+00:00
- **Updated**: 2021-10-05 03:53:52+00:00
- **Authors**: Xinxu Wei, Xianshi Zhang, Shisen Wang, Cheng Cheng, Yanlin Huang, Kaifu Yang, Yongjie Li
- **Comment**: 16 pages, 17 figures
- **Journal**: None
- **Summary**: Images obtained in real-world low-light conditions are not only low in brightness, but they also suffer from many other types of degradation, such as color distortion, unknown noise, detail loss and halo artifacts. In this paper, we propose a Degradation-Aware Deep Retinex Network (denoted as DA-DRN) for low-light image enhancement and tackle the above degradation. Based on Retinex Theory, the decomposition net in our model can decompose low-light images into reflectance and illumination maps and deal with the degradation in the reflectance during the decomposition phase directly. We propose a Degradation-Aware Module (DA Module) which can guide the training process of the decomposer and enable the decomposer to be a restorer during the training phase without additional computational cost in the test phase. DA Module can achieve the purpose of noise removal while preserving detail information into the illumination map as well as tackle color distortion and halo artifacts. We introduce Perceptual Loss to train the enhancement network to generate the brightness-improved illumination maps which are more consistent with human visual perception. We train and evaluate the performance of our proposed model over the LOL real-world and LOL synthetic datasets, and we also test our model over several other frequently used datasets without Ground-Truth (LIME, DICM, MEF and NPE datasets). We conduct extensive experiments to demonstrate that our approach achieves a promising effect with good rubustness and generalization and outperforms many other state-of-the-art methods qualitatively and quantitatively. Our method only takes 7 ms to process an image with 600x400 resolution on a TITAN Xp GPU.



### UHP-SOT: An Unsupervised High-Performance Single Object Tracker
- **Arxiv ID**: http://arxiv.org/abs/2110.01812v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01812v1)
- **Published**: 2021-10-05 04:04:09+00:00
- **Updated**: 2021-10-05 04:04:09+00:00
- **Authors**: Zhiruo Zhou, Hongyu Fu, Suya You, Christoph C. Borel-Donohue, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: An unsupervised online object tracking method that exploits both foreground and background correlations is proposed and named UHP-SOT (Unsupervised High-Performance Single Object Tracker) in this work. UHP-SOT consists of three modules: 1) appearance model update, 2) background motion modeling, and 3) trajectory-based box prediction. A state-of-the-art discriminative correlation filters (DCF) based tracker is adopted by UHP-SOT as the first module. We point out shortcomings of using the first module alone such as failure in recovering from tracking loss and inflexibility in object box adaptation and then propose the second and third modules to overcome them. Both are novel in single object tracking (SOT). We test UHP-SOT on two popular object tracking benchmarks, TB-50 and TB-100, and show that it outperforms all previous unsupervised SOT methods, achieves a performance comparable with the best supervised deep-learning-based SOT methods, and operates at a fast speed (i.e. 22.7-32.0 FPS on a CPU).



### Adversarial Attacks on Black Box Video Classifiers: Leveraging the Power of Geometric Transformations
- **Arxiv ID**: http://arxiv.org/abs/2110.01823v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01823v2)
- **Published**: 2021-10-05 05:05:59+00:00
- **Updated**: 2021-10-26 20:26:32+00:00
- **Authors**: Shasha Li, Abhishek Aich, Shitong Zhu, M. Salman Asif, Chengyu Song, Amit K. Roy-Chowdhury, Srikanth V. Krishnamurthy
- **Comment**: Accepted at NeurIPS 2021; First two authors contributed equally;
  Includes Supplementary Material
- **Journal**: None
- **Summary**: When compared to the image classification models, black-box adversarial attacks against video classification models have been largely understudied. This could be possible because, with video, the temporal dimension poses significant additional challenges in gradient estimation. Query-efficient black-box attacks rely on effectively estimated gradients towards maximizing the probability of misclassifying the target video. In this work, we demonstrate that such effective gradients can be searched for by parameterizing the temporal structure of the search space with geometric transformations. Specifically, we design a novel iterative algorithm Geometric TRAnsformed Perturbations (GEO-TRAP), for attacking video classification models. GEO-TRAP employs standard geometric transformation operations to reduce the search space for effective gradients into searching for a small group of parameters that define these operations. This group of parameters describes the geometric progression of gradients, resulting in a reduced and structured search space. Our algorithm inherently leads to successful perturbations with surprisingly few queries. For example, adversarial examples generated from GEO-TRAP have better attack success rates with ~73.55% fewer queries compared to the state-of-the-art method for video adversarial attacks on the widely used Jester dataset. Overall, our algorithm exposes vulnerabilities of diverse video classification models and achieves new state-of-the-art results under black-box settings on two large datasets. Code is available here: https://github.com/sli057/Geo-TRAP



### Hybrid Classical-Quantum method for Diabetic Foot Ulcer Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.02222v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02222v1)
- **Published**: 2021-10-05 06:28:36+00:00
- **Updated**: 2021-10-05 06:28:36+00:00
- **Authors**: Azadeh Alavi, Hossein Akhoundi
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2110.01795
- **Journal**: None
- **Summary**: Diabetes is a raising problem that affects many people globally. Diabetic patients are at risk of developing foot ulcer that usually leads to limb amputation, causing significant morbidity, and psychological distress. In order to develop a self monitoring mobile application, it is necessary to be able to classify such ulcers into either of the following classes: Infection, Ischaemia, None, or Both. In this work, we compare the performance of a classical transfer-learning-based method, with the performance of a hybrid classical-quantum Classifier on diabetic foot ulcer classification task. As such, we merge the pre-trained Xception network with a multi-class variational classifier. Thus, after modifying and re-training the Xception network, we extract the output of a mid-layer and employ it as deep-features presenters of the given images. Finally, we use those deep-features to train multi-class variational classifier, where each classifier is implemented on an individual variational circuit. The method is then evaluated on the blind test set DFUC2021. The results proves that our proposed hybrid classical-quantum Classifier leads to considerable improvement compared to solely relying on transfer learning concept through training the modified version of Xception network.



### Deep reinforcement learning for guidewire navigation in coronary artery phantom
- **Arxiv ID**: http://arxiv.org/abs/2110.01840v1
- **DOI**: 10.1109/ACCESS.2021.3135277
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01840v1)
- **Published**: 2021-10-05 06:30:03+00:00
- **Updated**: 2021-10-05 06:30:03+00:00
- **Authors**: Jihoon Kweon, Kyunghwan Kim, Chaehyuk Lee, Hwi Kwon, Jinwoo Park, Kyoseok Song, Young In Kim, Jeeone Park, Inwook Back, Jae-Hyung Roh, Youngjin Moon, Jaesoon Choi, Young-Hak Kim
- **Comment**: 15 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: In percutaneous intervention for treatment of coronary plaques, guidewire navigation is a primary procedure for stent delivery. Steering a flexible guidewire within coronary arteries requires considerable training, and the non-linearity between the control operation and the movement of the guidewire makes precise manipulation difficult. Here, we introduce a deep reinforcement learning(RL) framework for autonomous guidewire navigation in a robot-assisted coronary intervention. Using Rainbow, a segment-wise learning approach is applied to determine how best to accelerate training using human demonstrations with deep Q-learning from demonstrations (DQfD), transfer learning, and weight initialization. `State' for RL is customized as a focus window near the guidewire tip, and subgoals are placed to mitigate a sparse reward problem. The RL agent improves performance, eventually enabling the guidewire to reach all valid targets in `stable' phase. Our framework opens anew direction in the automation of robot-assisted intervention, providing guidance on RL in physical spaces involving mechanical fatigue.



### Dataset Structural Index: Leveraging a machine's perspective towards visual data
- **Arxiv ID**: http://arxiv.org/abs/2110.04070v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.04070v3)
- **Published**: 2021-10-05 06:40:16+00:00
- **Updated**: 2023-01-23 05:33:48+00:00
- **Authors**: Dishant Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: With advances in vision and perception architectures, we have realized that working with data is equally crucial, if not more, than the algorithms. Till today, we have trained machines based on our knowledge and perspective of the world. The entire concept of Dataset Structural Index(DSI) revolves around understanding a machine`s perspective of the dataset. With DSI, I show two meta values with which we can get more information over a visual dataset and use it to optimize data, create better architectures, and have an ability to guess which model would work best. These two values are the Variety contribution ratio and Similarity matrix. In the paper, I show many applications of DSI, one of which is how the same level of accuracy can be achieved with the same model architectures trained over less amount of data.



### Hypernetworks for Continual Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.01856v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.01856v1)
- **Published**: 2021-10-05 07:42:38+00:00
- **Updated**: 2021-10-05 07:42:38+00:00
- **Authors**: Dhanajit Brahma, Vinay Kumar Verma, Piyush Rai
- **Comment**: Accepted to CSSL workshop at IJCAI 2021 (Best Student Paper Award)
- **Journal**: None
- **Summary**: Learning from data sequentially arriving, possibly in a non i.i.d. way, with changing task distribution over time is called continual learning. Much of the work thus far in continual learning focuses on supervised learning and some recent works on unsupervised learning. In many domains, each task contains a mix of labelled (typically very few) and unlabelled (typically plenty) training examples, which necessitates a semi-supervised learning approach. To address this in a continual learning setting, we propose a framework for semi-supervised continual learning called Meta-Consolidation for Continual Semi-Supervised Learning (MCSSL). Our framework has a hypernetwork that learns the meta-distribution that generates the weights of a semi-supervised auxiliary classifier generative adversarial network $(\textit{Semi-ACGAN})$ as the base network. We consolidate the knowledge of sequential tasks in the hypernetwork, and the base network learns the semi-supervised learning task. Further, we present $\textit{Semi-Split CIFAR-10}$, a new benchmark for continual semi-supervised learning, obtained by modifying the $\textit{Split CIFAR-10}$ dataset, in which the tasks with labelled and unlabelled data arrive sequentially. Our proposed model yields significant improvements in the continual semi-supervised learning setting. We compare the performance of several existing continual learning approaches on the proposed continual semi-supervised learning benchmark of the Semi-Split CIFAR-10 dataset.



### Extensions of Karger's Algorithm: Why They Fail in Theory and How They Are Useful in Practice
- **Arxiv ID**: http://arxiv.org/abs/2110.02750v2
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.02750v2)
- **Published**: 2021-10-05 07:46:28+00:00
- **Updated**: 2021-12-16 15:55:22+00:00
- **Authors**: Erik Jenner, Enrique Fita Sanmartín, Fred A. Hamprecht
- **Comment**: Oral at ICCV 2021; added acknowledgements
- **Journal**: None
- **Summary**: The minimum graph cut and minimum $s$-$t$-cut problems are important primitives in the modeling of combinatorial problems in computer science, including in computer vision and machine learning. Some of the most efficient algorithms for finding global minimum cuts are randomized algorithms based on Karger's groundbreaking contraction algorithm. Here, we study whether Karger's algorithm can be successfully generalized to other cut problems. We first prove that a wide class of natural generalizations of Karger's algorithm cannot efficiently solve the $s$-$t$-mincut or the normalized cut problem to optimality. However, we then present a simple new algorithm for seeded segmentation / graph-based semi-supervised learning that is closely based on Karger's original algorithm, showing that for these problems, extensions of Karger's algorithm can be useful. The new algorithm has linear asymptotic runtime and yields a potential that can be interpreted as the posterior probability of a sample belonging to a given seed / class. We clarify its relation to the random walker algorithm / harmonic energy minimization in terms of distributions over spanning forests. On classical problems from seeded image segmentation and graph-based semi-supervised learning on image data, the method performs at least as well as the random walker / harmonic energy minimization / Gaussian processes.



### Frequency Aware Face Hallucination Generative Adversarial Network with Semantic Structural Constraint
- **Arxiv ID**: http://arxiv.org/abs/2110.01880v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.4.3; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2110.01880v1)
- **Published**: 2021-10-05 08:51:29+00:00
- **Updated**: 2021-10-05 08:51:29+00:00
- **Authors**: Shailza Sharma, Abhinav Dhall, Vinay Kumar
- **Comment**: 12 pages, 12 figures, submitted to IEEE Transactions on Computational
  Imaging
- **Journal**: None
- **Summary**: In this paper, we address the issue of face hallucination. Most current face hallucination methods rely on two-dimensional facial priors to generate high resolution face images from low resolution face images. These methods are only capable of assimilating global information into the generated image. Still there exist some inherent problems in these methods; such as, local features, subtle structural details and missing depth information in final output image. Present work proposes a Generative Adversarial Network (GAN) based novel progressive Face Hallucination (FH) network to address these issues present among current methods. The generator of the proposed model comprises of FH network and two sub-networks, assisting FH network to generate high resolution images. The first sub-network leverages on explicitly adding high frequency components into the model. To explicitly encode the high frequency components, an auto encoder is proposed to generate high resolution coefficients of Discrete Cosine Transform (DCT). To add three dimensional parametric information into the network, second sub-network is proposed. This network uses a shape model of 3D Morphable Models (3DMM) to add structural constraint to the FH network. Extensive experimentation results in the paper shows that the proposed model outperforms the state-of-the-art methods.



### De-rendering Stylized Texts
- **Arxiv ID**: http://arxiv.org/abs/2110.01890v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01890v1)
- **Published**: 2021-10-05 09:23:39+00:00
- **Updated**: 2021-10-05 09:23:39+00:00
- **Authors**: Wataru Shimoda, Daichi Haraguchi, Seiichi Uchida, Kota Yamaguchi
- **Comment**: Accepted to ICCV 2021. Codes:
  https://github.com/CyberAgentAILab/derendering-text
- **Journal**: None
- **Summary**: Editing raster text is a promising but challenging task. We propose to apply text vectorization for the task of raster text editing in display media, such as posters, web pages, or advertisements. In our approach, instead of applying image transformation or generation in the raster domain, we learn a text vectorization model to parse all the rendering parameters including text, location, size, font, style, effects, and hidden background, then utilize those parameters for reconstruction and any editing task. Our text vectorization takes advantage of differentiable text rendering to accurately reproduce the input raster text in a resolution-free parametric format. We show in the experiments that our approach can successfully parse text, styling, and background information in the unified model, and produces artifact-free text editing compared to a raster baseline.



### RapidAI4EO: A Corpus for Higher Spatial and Temporal Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2110.01919v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2110.01919v1)
- **Published**: 2021-10-05 10:12:20+00:00
- **Updated**: 2021-10-05 10:12:20+00:00
- **Authors**: Giovanni Marchisio, Patrick Helber, Benjamin Bischke, Timothy Davis, Caglar Senaras, Daniele Zanaga, Ruben Van De Kerchove, Annett Wania
- **Comment**: Published in the IEEE 2021 International Geoscience & Remote Sensing
  Symposium (IGARSS 2021)
- **Journal**: None
- **Summary**: Under the sponsorship of the European Union Horizon 2020 program, RapidAI4EO will establish the foundations for the next generation of Copernicus Land Monitoring Service (CLMS) products. The project aims to provide intensified monitoring of Land Use (LU), Land Cover (LC), and LU change at a much higher level of detail and temporal cadence than it is possible today. Focus is on disentangling phenology from structural change and in providing critical training data to drive advancement in the Copernicus community and ecosystem well beyond the lifetime of this project. To this end we are creating the densest spatiotemporal training sets ever by fusing open satellite data with Planet imagery at as many as 500,000 patch locations over Europe and delivering high resolution daily time series at all locations. We plan to open source these datasets for the benefit of the entire remote sensing community.



### CNN-based Human Detection for UAVs in Search and Rescue
- **Arxiv ID**: http://arxiv.org/abs/2110.01930v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01930v1)
- **Published**: 2021-10-05 10:43:10+00:00
- **Updated**: 2021-10-05 10:43:10+00:00
- **Authors**: Nikite Mesvan
- **Comment**: 9 pages, 12 figures
- **Journal**: None
- **Summary**: The use of Unmanned Aerial Vehicles (UAVs) as a substitute for ordinary vehicles in applications of search and rescue is being studied all over the world due to its flexible mobility and less obstruction, including two main tasks: search and rescue. This paper proposes an approach for the first task of searching and detecting victims using a type of convolutional neural network technique, the Single Shot Detector (SSD) model, with the Quadcopter hardware platform, a type of UAVs. The model used in the research is a pre-trained model and is applied to test on a Raspberry Pi model B, which is attached on a Quadcopter, while a single camera is equipped at the bottom of the Quadcopter to look from above for search and detection. The Quadcopter in this research is a DIY hardware model that uses accelerometer and gyroscope sensors and ultrasonic sensor as the essential components for balancing control, however, these sensors are susceptible to noise caused by the driving forces on the model, such as the vibration of the motors, therefore, the issues about the PID controller, noise processing for the sensors are also mentioned in the paper. Experimental results proved that the Quadcopter is able to stably flight and the SSD model works well on the Raspberry Pi model B with a processing speed of 3 fps and produces the best detection results at the distance of 1 to 20 meters to objects.



### Anchor-free Oriented Proposal Generator for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.01931v2
- **DOI**: 10.1109/TGRS.2022.3183022
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01931v2)
- **Published**: 2021-10-05 10:45:51+00:00
- **Updated**: 2022-04-26 14:40:13+00:00
- **Authors**: Gong Cheng, Jiabao Wang, Ke Li, Xingxing Xie, Chunbo Lang, Yanqing Yao, Junwei Han
- **Comment**: None
- **Journal**: None
- **Summary**: Oriented object detection is a practical and challenging task in remote sensing image interpretation. Nowadays, oriented detectors mostly use horizontal boxes as intermedium to derive oriented boxes from them. However, the horizontal boxes are inclined to get small Intersection-over-Unions (IoUs) with ground truths, which may have some undesirable effects, such as introducing redundant noise, mismatching with ground truths, detracting from the robustness of detectors, etc. In this paper, we propose a novel Anchor-free Oriented Proposal Generator (AOPG) that abandons horizontal box-related operations from the network architecture. AOPG first produces coarse oriented boxes by a Coarse Location Module (CLM) in an anchor-free manner and then refines them into high-quality oriented proposals. After AOPG, we apply a Fast R-CNN head to produce the final detection results. Furthermore, the shortage of large-scale datasets is also a hindrance to the development of oriented object detection. To alleviate the data insufficiency, we release a new dataset on the basis of our DIOR dataset and name it DIOR-R. Massive experiments demonstrate the effectiveness of AOPG. Particularly, without bells and whistles, we achieve the accuracy of 64.41%, 75.24% and 96.22% mAP on the DIOR-R, DOTA and HRSC2016 datasets respectively. Code and models are available at https://github.com/jbwang1997/AOPG.



### Double Encoder-Decoder Networks for Gastrointestinal Polyp Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.01939v1
- **DOI**: 10.1007/978-3-030-68763-2_22
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.01939v1)
- **Published**: 2021-10-05 11:07:42+00:00
- **Updated**: 2021-10-05 11:07:42+00:00
- **Authors**: Adrian Galdran, Gustavo Carneiro, Miguel A. González Ballester
- **Comment**: Accepted at International Conference on Pattern Recognition, AIHA
  workshop 2021
- **Journal**: None
- **Summary**: Polyps represent an early sign of the development of Colorectal Cancer. The standard procedure for their detection consists of colonoscopic examination of the gastrointestinal tract. However, the wide range of polyp shapes and visual appearances, as well as the reduced quality of this image modality, turn their automatic identification and segmentation with computational tools into a challenging computer vision task. In this work, we present a new strategy for the delineation of gastrointestinal polyps from endoscopic images based on a direct extension of common encoder-decoder networks for semantic segmentation. In our approach, two pretrained encoder-decoder networks are sequentially stacked: the second network takes as input the concatenation of the original frame and the initial prediction generated by the first network, which acts as an attention mechanism enabling the second network to focus on interesting areas within the image, thereby improving the quality of its predictions. Quantitative evaluation carried out on several polyp segmentation databases shows that double encoder-decoder networks clearly outperform their single encoder-decoder counterparts in all cases. In addition, our best double encoder-decoder combination attains excellent segmentation accuracy and reaches state-of-the-art performance results in all the considered datasets, with a remarkable boost of accuracy on images extracted from datasets not used for training.



### Distribution Mismatch Correction for Improved Robustness in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.01955v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.0; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2110.01955v1)
- **Published**: 2021-10-05 11:36:25+00:00
- **Updated**: 2021-10-05 11:36:25+00:00
- **Authors**: Alexander Fuchs, Christian Knoll, Franz Pernkopf
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks rely heavily on normalization methods to improve their performance and learning behavior. Although normalization methods spurred the development of increasingly deep and efficient architectures, they also increase the vulnerability with respect to noise and input corruptions. In most applications, however, noise is ubiquitous and diverse; this can often lead to complete failure of machine learning systems as they fail to cope with mismatches between the input distribution during training- and test-time. The most common normalization method, batch normalization, reduces the distribution shift during training but is agnostic to changes in the input distribution during test time. This makes batch normalization prone to performance degradation whenever noise is present during test-time. Sample-based normalization methods can correct linear transformations of the activation distribution but cannot mitigate changes in the distribution shape; this makes the network vulnerable to distribution changes that cannot be reflected in the normalization parameters. We propose an unsupervised non-parametric distribution correction method that adapts the activation distribution of each layer. This reduces the mismatch between the training and test-time distribution by minimizing the 1-D Wasserstein distance. In our experiments, we empirically show that the proposed method effectively reduces the impact of intense image corruptions and thus improves the classification performance without the need for retraining or fine-tuning the model.



### Structured Bird's-Eye-View Traffic Scene Understanding from Onboard Images
- **Arxiv ID**: http://arxiv.org/abs/2110.01997v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01997v1)
- **Published**: 2021-10-05 12:40:33+00:00
- **Updated**: 2021-10-05 12:40:33+00:00
- **Authors**: Yigit Baran Can, Alexander Liniger, Danda Pani Paudel, Luc Van Gool
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Autonomous navigation requires structured representation of the road network and instance-wise identification of the other traffic agents. Since the traffic scene is defined on the ground plane, this corresponds to scene understanding in the bird's-eye-view (BEV). However, the onboard cameras of autonomous cars are customarily mounted horizontally for a better view of the surrounding, making this task very challenging. In this work, we study the problem of extracting a directed graph representing the local road network in BEV coordinates, from a single onboard camera image. Moreover, we show that the method can be extended to detect dynamic objects on the BEV plane. The semantics, locations, and orientations of the detected objects together with the road graph facilitates a comprehensive understanding of the scene. Such understanding becomes fundamental for the downstream tasks, such as path planning and navigation. We validate our approach against powerful baselines and show that our network achieves superior performance. We also demonstrate the effects of various design choices through ablation studies. Code: https://github.com/ybarancan/STSU



### FooDI-ML: a large multi-language dataset of food, drinks and groceries images and descriptions
- **Arxiv ID**: http://arxiv.org/abs/2110.02035v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02035v2)
- **Published**: 2021-10-05 13:33:08+00:00
- **Updated**: 2022-08-26 11:23:29+00:00
- **Authors**: David Amat Olóndriz, Ponç Palau Puigdevall, Adrià Salvador Palau
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce the FooDI-ML dataset. This dataset contains over 1.5M unique images and over 9.5M store names, product names descriptions, and collection sections gathered from the Glovo application. The data made available corresponds to food, drinks and groceries products from 37 countries in Europe, the Middle East, Africa and Latin America. The dataset comprehends 33 languages, including 870K samples of languages of countries from Eastern Europe and Western Asia such as Ukrainian and Kazakh, which have been so far underrepresented in publicly available visio-linguistic datasets. The dataset also includes widely spoken languages such as Spanish and English. To assist further research, we include benchmarks over two tasks: text-image retrieval and conditional image generation.



### Multi-Object Tracking with Deep Learning Ensemble for Unmanned Aerial System Applications
- **Arxiv ID**: http://arxiv.org/abs/2110.02044v1
- **DOI**: 10.1117/12.2600209
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.02044v1)
- **Published**: 2021-10-05 13:50:38+00:00
- **Updated**: 2021-10-05 13:50:38+00:00
- **Authors**: Wanlin Xie, Jaime Ide, Daniel Izadi, Sean Banger, Thayne Walker, Ryan Ceresani, Dylan Spagnuolo, Christopher Guagliano, Henry Diaz, Jason Twedt
- **Comment**: None
- **Journal**: Proc. SPIE 11870, Artificial Intelligence and Machine Learning in
  Defense Applications III, 118700I (12 September 2021)
- **Summary**: Multi-object tracking (MOT) is a crucial component of situational awareness in military defense applications. With the growing use of unmanned aerial systems (UASs), MOT methods for aerial surveillance is in high demand. Application of MOT in UAS presents specific challenges such as moving sensor, changing zoom levels, dynamic background, illumination changes, obscurations and small objects. In this work, we present a robust object tracking architecture aimed to accommodate for the noise in real-time situations. We propose a kinematic prediction model, called Deep Extended Kalman Filter (DeepEKF), in which a sequence-to-sequence architecture is used to predict entity trajectories in latent space. DeepEKF utilizes a learned image embedding along with an attention mechanism trained to weight the importance of areas in an image to predict future states. For the visual scoring, we experiment with different similarity measures to calculate distance based on entity appearances, including a convolutional neural network (CNN) encoder, pre-trained using Siamese networks. In initial evaluation experiments, we show that our method, combining scoring structure of the kinematic and visual models within a MHT framework, has improved performance especially in edge cases where entity motion is unpredictable, or the data presents frames with significant gaps.



### Spatial Context Awareness for Unsupervised Change Detection in Optical Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2110.02068v1
- **DOI**: 10.1109/TGRS.2021.3130842
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02068v1)
- **Published**: 2021-10-05 14:13:48+00:00
- **Updated**: 2021-10-05 14:13:48+00:00
- **Authors**: Lukas Kondmann, Aysim Toker, Sudipan Saha, Bernhard Schölkopf, Laura Leal-Taixé, Xiao Xiang Zhu
- **Comment**: Submitted to IEEE Transactions on Geoscience and Remote Sensing (IEEE
  TGRS)
- **Journal**: None
- **Summary**: Detecting changes on the ground in multitemporal Earth observation data is one of the key problems in remote sensing. In this paper, we introduce Sibling Regression for Optical Change detection (SiROC), an unsupervised method for change detection in optical satellite images with medium and high resolution. SiROC is a spatial context-based method that models a pixel as a linear combination of its distant neighbors. It uses this model to analyze differences in the pixel and its spatial context-based predictions in subsequent time periods for change detection. We combine this spatial context-based change detection with ensembling over mutually exclusive neighborhoods and transitioning from pixel to object-level changes with morphological operations. SiROC achieves competitive performance for change detection with medium-resolution Sentinel-2 and high-resolution Planetscope imagery on four datasets. Besides accurate predictions without the need for training, SiROC also provides a well-calibrated uncertainty of its predictions. This makes the method especially useful in conjunction with deep-learning based methods for applications such as pseudo-labeling.



### A Methodology to Identify Cognition Gaps in Visual Recognition Applications Based on Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.02080v1
- **DOI**: 10.1109/CASE49439.2021.9551605
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02080v1)
- **Published**: 2021-10-05 14:26:17+00:00
- **Updated**: 2021-10-05 14:26:17+00:00
- **Authors**: Hannes Vietz, Tristan Rauch, Andreas Löcklin, Nasser Jazdi, Michael Weyrich
- **Comment**: to be published in 16th CASE 2021: Lyon
- **Journal**: None
- **Summary**: Developing consistently well performing visual recognition applications based on convolutional neural networks, e.g. for autonomous driving, is very challenging. One of the obstacles during the development is the opaqueness of their cognitive behaviour. A considerable amount of literature has been published which describes irrational behaviour of trained CNNs showcasing gaps in their cognition. In this paper, a methodology is presented that creates worstcase images using image augmentation techniques. If the CNN's cognitive performance on such images is weak while the augmentation techniques are supposedly harmless, a potential gap in the cognition has been found. The presented worst-case image generator is using adversarial search approaches to efficiently identify the most challenging image. This is evaluated with the well-known AlexNet CNN using images depicting a typical driving scenario.



### Numerisation D'un Siecle de Paysage Ferroviaire Français : recul du rail, conséquences territoriales et coût environnemental
- **Arxiv ID**: http://arxiv.org/abs/2111.03433v1
- **DOI**: None
- **Categories**: **physics.soc-ph**, cs.AI, cs.CV, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/2111.03433v1)
- **Published**: 2021-10-05 14:36:11+00:00
- **Updated**: 2021-10-05 14:36:11+00:00
- **Authors**: Robert Jeansoulin
- **Comment**: in French. Territoire(s) et Num\'erique : innovations, mutations et
  d\'ecision, ASRDLF (Association de Sciences R\'egionales de Langue
  Fran\c{c}aise), Sep 2021, Avignon, France
- **Journal**: None
- **Summary**: The reconstruction of geographical data over a century, allows to figuring out the evolution of the French railway landscape, and how it has been impacted by major events (eg.: WWII), or longer time span processes : industry outsourcing, metropolization, public transport policies or absence of them. This work is resulting from the fusion of several public geographical data (SNCF, IGN), enriched with the computer-assisted addition of multiple data gathered on the Internet (Wikipedia, volunteer geographic information). The dataset compounds almost every rail stations (even simple stops) and railway branch nodes, whose link to their respective rail lines allows to build the underlying consistent graph of the network. Every rail line has a "valid to" date (or approx) so that time evolution can be displayed. The present progress of that reconstruction sums up to roughly 90% of what is expected (exact total unknown). This allows to consider temporal demographic analysis (how many cities and towns served by the railway since 1925 up on today), and environmental simulations as well (CO2 cost by given destination ).



### Exploring the Limits of Large Scale Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2110.02095v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.02095v1)
- **Published**: 2021-10-05 14:49:00+00:00
- **Updated**: 2021-10-05 14:49:00+00:00
- **Authors**: Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, Hanie Sedghi
- **Comment**: None
- **Journal**: None
- **Summary**: Recent developments in large-scale machine learning suggest that by scaling up data, model size and training time properly, one might observe that improvements in pre-training would transfer favorably to most downstream tasks. In this work, we systematically study this phenomena and establish that, as we increase the upstream accuracy, the performance of downstream tasks saturates. In particular, we investigate more than 4800 experiments on Vision Transformers, MLP-Mixers and ResNets with number of parameters ranging from ten million to ten billion, trained on the largest scale of available image data (JFT, ImageNet21K) and evaluated on more than 20 downstream image recognition tasks. We propose a model for downstream performance that reflects the saturation phenomena and captures the nonlinear relationship in performance of upstream and downstream tasks. Delving deeper to understand the reasons that give rise to these phenomena, we show that the saturation behavior we observe is closely related to the way that representations evolve through the layers of the models. We showcase an even more extreme scenario where performance on upstream and downstream are at odds with each other. That is, to have a better downstream performance, we need to hurt upstream accuracy.



### Self-Supervised Generative Style Transfer for One-Shot Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.02117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02117v1)
- **Published**: 2021-10-05 15:28:42+00:00
- **Updated**: 2021-10-05 15:28:42+00:00
- **Authors**: Devavrat Tomar, Behzad Bozorgtabar, Manana Lortkipanidze, Guillaume Vray, Mohammad Saeed Rad, Jean-Philippe Thiran
- **Comment**: Accepted in WACV 2022
- **Journal**: None
- **Summary**: In medical image segmentation, supervised deep networks' success comes at the cost of requiring abundant labeled data. While asking domain experts to annotate only one or a few of the cohort's images is feasible, annotating all available images is impractical. This issue is further exacerbated when pre-trained deep networks are exposed to a new image dataset from an unfamiliar distribution. Using available open-source data for ad-hoc transfer learning or hand-tuned techniques for data augmentation only provides suboptimal solutions. Motivated by atlas-based segmentation, we propose a novel volumetric self-supervised learning for data augmentation capable of synthesizing volumetric image-segmentation pairs via learning transformations from a single labeled atlas to the unlabeled data. Our work's central tenet benefits from a combined view of one-shot generative learning and the proposed self-supervised training strategy that cluster unlabeled volumetric images with similar styles together. Unlike previous methods, our method does not require input volumes at inference time to synthesize new images. Instead, it can generate diversified volumetric image-segmentation pairs from a prior distribution given a single or multi-site dataset. Augmented data generated by our method used to train the segmentation network provide significant improvements over state-of-the-art deep one-shot learning methods on the task of brain MRI segmentation. Ablation studies further exemplified that the proposed appearance model and joint training are crucial to synthesize realistic examples compared to existing medical registration methods. The code, data, and models are available at https://github.com/devavratTomar/SST.



### Efficient Modelling Across Time of Human Actions and Interactions
- **Arxiv ID**: http://arxiv.org/abs/2110.02120v1
- **DOI**: 10.33540/789
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02120v1)
- **Published**: 2021-10-05 15:39:11+00:00
- **Updated**: 2021-10-05 15:39:11+00:00
- **Authors**: Alexandros Stergiou
- **Comment**: PhD thesis
- **Journal**: None
- **Summary**: This thesis focuses on video understanding for human action and interaction recognition. We start by identifying the main challenges related to action recognition from videos and review how they have been addressed by current methods.   Based on these challenges, and by focusing on the temporal aspect of actions, we argue that current fixed-sized spatio-temporal kernels in 3D convolutional neural networks (CNNs) can be improved to better deal with temporal variations in the input. Our contributions are based on the enlargement of the convolutional receptive fields through the introduction of spatio-temporal size-varying segments of videos, as well as the discovery of the local feature relevance over the entire video sequence. The resulting extracted features encapsulate information that includes the importance of local features across multiple temporal durations, as well as the entire video sequence.   Subsequently, we study how we can better handle variations between classes of actions, by enhancing their feature differences over different layers of the architecture. The hierarchical extraction of features models variations of relatively similar classes the same as very dissimilar classes. Therefore, distinctions between similar classes are less likely to be modelled. The proposed approach regularises feature maps by amplifying features that correspond to the class of the video that is processed. We move away from class-agnostic networks and make early predictions based on feature amplification mechanism.   The proposed approaches are evaluated on several benchmark action recognition datasets and show competitive results. In terms of performance, we compete with the state-of-the-art while being more efficient in terms of GFLOPs.   Finally, we present a human-understandable approach aimed at providing visual explanations for features learned over spatio-temporal networks.



### A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.04079v5
- **DOI**: 10.1111/mice.12829
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.04079v5)
- **Published**: 2021-10-05 15:47:45+00:00
- **Updated**: 2022-07-20 01:54:05+00:00
- **Authors**: Yongqi Dong, Sandeep Patil, Bart van Arem, Haneen Farah
- **Comment**: 18 pages, 5 figures. Published by Computer-Aided Civil and
  Infrastructure Engineering (CACIE). Open access from
  https://doi.org/10.1111/mice.12829
- **Journal**: None
- **Summary**: Accurate and reliable lane detection is vital for the safe performance of lane-keeping assistance and lane departure warning systems. However, under certain challenging circumstances, it is difficult to get satisfactory performance in accurately detecting the lanes from one single image as mostly done in current literature. Since lane markings are continuous lines, the lanes that are difficult to be accurately detected in the current single image can potentially be better deduced if information from previous frames is incorporated. This study proposes a novel hybrid spatial-temporal (ST) sequence-to-one deep learning architecture. This architecture makes full use of the ST information in multiple continuous image frames to detect the lane markings in the very last frame. Specifically, the hybrid model integrates the following aspects: (a) the single image feature extraction module equipped with the spatial convolutional neural network; (b) the ST feature integration module constructed by ST recurrent neural network; (c) the encoder-decoder structure, which makes this image segmentation problem work in an end-to-end supervised learning format. Extensive experiments reveal that the proposed model architecture can effectively handle challenging driving scenes and outperforms available state-of-the-art methods.



### FacialFilmroll: High-resolution multi-shot video editing
- **Arxiv ID**: http://arxiv.org/abs/2110.02124v2
- **DOI**: 10.1145/3485441.3485646
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02124v2)
- **Published**: 2021-10-05 15:51:08+00:00
- **Updated**: 2021-11-17 15:06:08+00:00
- **Authors**: Bharath Bhushan Damodaran, Emmanuel Jolly, Gilles Puy, Philippe Henri Gosselin, Cédric Thébault, Junghyun Ahn, Tim Christensen, Paul Ghezzo, Pierre Hellier
- **Comment**: European Conference on Visual Media Production (CVMP '21)
- **Journal**: European Conference on Visual Media Production (CVMP '21), 2021
- **Summary**: We present FacialFilmroll, a solution for spatially and temporally consistent editing of faces in one or multiple shots. We build upon unwrap mosaic [Rav-Acha et al. 2008] by specializing it to faces. We leverage recent techniques to fit a 3D face model on monocular videos to (i) improve the quality of the mosaic for edition and (ii) permit the automatic transfer of edits from one shot to other shots of the same actor. We explain how FacialFilmroll is integrated in post-production facility. Finally, we present video editing results using FacialFilmroll on high resolution videos.



### Machine learning attack on copy detection patterns: are 1x1 patterns cloneable?
- **Arxiv ID**: http://arxiv.org/abs/2110.02176v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02176v2)
- **Published**: 2021-10-05 17:06:28+00:00
- **Updated**: 2021-10-06 11:22:32+00:00
- **Authors**: Roman Chaban, Olga Taran, Joakim Tutt, Taras Holotyak, Slavi Bonev, Slava Voloshynovskiy
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, the modern economy critically requires reliable yet cheap protection solutions against product counterfeiting for the mass market. Copy detection patterns (CDP) are considered as such solution in several applications. It is assumed that being printed at the maximum achievable limit of a printing resolution of an industrial printer with the smallest symbol size 1x1 elements, the CDP cannot be copied with sufficient accuracy and thus are unclonable. In this paper, we challenge this hypothesis and consider a copy attack against the CDP based on machine learning. The experimental based on samples produced on two industrial printers demonstrate that simple detection metrics used in the CDP authentication cannot reliably distinguish the original CDP from their fakes. Thus, the paper calls for a need of careful reconsideration of CDP cloneability and search for new authentication techniques and CDP optimization because of the current attack.



### MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2110.02178v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02178v2)
- **Published**: 2021-10-05 17:07:53+00:00
- **Updated**: 2022-03-04 17:17:31+00:00
- **Authors**: Sachin Mehta, Mohammad Rastegari
- **Comment**: Accepted at ICLR'22
- **Journal**: None
- **Summary**: Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters.   Our source code is open-source and available at: https://github.com/apple/ml-cvnets



### Transfer Learning U-Net Deep Learning for Lung Ultrasound Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.02196v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02196v1)
- **Published**: 2021-10-05 17:43:28+00:00
- **Updated**: 2021-10-05 17:43:28+00:00
- **Authors**: Dorothy Cheng, Edmund Y. Lam
- **Comment**: 14 pages, 8 figures
- **Journal**: None
- **Summary**: Transfer learning (TL) for medical image segmentation helps deep learning models achieve more accurate performances when there are scarce medical images. This study focuses on completing segmentation of the ribs from lung ultrasound images and finding the best TL technique with U-Net, a convolutional neural network for precise and fast image segmentation. Two approaches of TL were used, using a pre-trained VGG16 model to build the U-Net (V-Unet) and pre-training U-Net network with grayscale natural salient object dataset (X-Unet). Visual results and dice coefficients (DICE) of the models were compared. X-Unet showed more accurate and artifact-free visual performances on the actual mask prediction, despite its lower DICE than V-Unet. A partial-frozen network fine-tuning (FT) technique was also applied to X-Unet to compare results between different FT strategies, which FT all layers slightly outperformed freezing part of the network. The effect of dataset sizes was also evaluated, showing the importance of the combination between TL and data augmentation.



### $Δ$-UQ: Accurate Uncertainty Quantification via Anchor Marginalization
- **Arxiv ID**: http://arxiv.org/abs/2110.02197v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.02197v1)
- **Published**: 2021-10-05 17:44:31+00:00
- **Updated**: 2021-10-05 17:44:31+00:00
- **Authors**: Rushil Anirudh, Jayaraman J. Thiagarajan
- **Comment**: None
- **Journal**: None
- **Summary**: We present $\Delta$-UQ -- a novel, general-purpose uncertainty estimator using the concept of anchoring in predictive models. Anchoring works by first transforming the input into a tuple consisting of an anchor point drawn from a prior distribution, and a combination of the input sample with the anchor using a pretext encoding scheme. This encoding is such that the original input can be perfectly recovered from the tuple -- regardless of the choice of the anchor. Therefore, any predictive model should be able to predict the target response from the tuple alone (since it implicitly represents the input). Moreover, by varying the anchors for a fixed sample, we can estimate uncertainty in the prediction even using only a single predictive model. We find this uncertainty is deeply connected to improper sampling of the input data, and inherent noise, enabling us to estimate the total uncertainty in any system. With extensive empirical studies on a variety of use-cases, we demonstrate that $\Delta$-UQ outperforms several competitive baselines. Specifically, we study model fitting, sequential model optimization, model based inversion in the regression setting and out of distribution detection, & calibration under distribution shifts for classification.



### Waypoint Models for Instruction-guided Navigation in Continuous Environments
- **Arxiv ID**: http://arxiv.org/abs/2110.02207v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.02207v1)
- **Published**: 2021-10-05 17:55:49+00:00
- **Updated**: 2021-10-05 17:55:49+00:00
- **Authors**: Jacob Krantz, Aaron Gokaslan, Dhruv Batra, Stefan Lee, Oleksandr Maksymets
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Little inquiry has explicitly addressed the role of action spaces in language-guided visual navigation -- either in terms of its effect on navigation success or the efficiency with which a robotic agent could execute the resulting trajectory. Building on the recently released VLN-CE setting for instruction following in continuous environments, we develop a class of language-conditioned waypoint prediction networks to examine this question. We vary the expressivity of these models to explore a spectrum between low-level actions and continuous waypoint prediction. We measure task performance and estimated execution time on a profiled LoCoBot robot. We find more expressive models result in simpler, faster to execute trajectories, but lower-level actions can achieve better navigation metrics by approximating shortest paths better. Further, our models outperform prior work in VLN-CE and set a new state-of-the-art on the public leaderboard -- increasing success rate by 4% with our best model on this challenging task.



### Mix3D: Out-of-Context Data Augmentation for 3D Scenes
- **Arxiv ID**: http://arxiv.org/abs/2110.02210v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02210v2)
- **Published**: 2021-10-05 17:57:45+00:00
- **Updated**: 2021-11-29 12:01:46+00:00
- **Authors**: Alexey Nekrasov, Jonas Schult, Or Litany, Bastian Leibe, Francis Engelmann
- **Comment**: Accepted for publication at 3DV 2021. Camera-ready submission. Link
  to code: https://github.com/kumuji/mix3d - Project page:
  https://nekrasov.dev/mix3d/
- **Journal**: None
- **Summary**: We present Mix3D, a data augmentation technique for segmenting large-scale 3D scenes. Since scene context helps reasoning about object semantics, current works focus on models with large capacity and receptive fields that can fully capture the global context of an input 3D scene. However, strong contextual priors can have detrimental implications like mistaking a pedestrian crossing the street for a car. In this work, we focus on the importance of balancing global scene context and local geometry, with the goal of generalizing beyond the contextual priors in the training set. In particular, we propose a "mixing" technique which creates new training samples by combining two augmented scenes. By doing so, object instances are implicitly placed into novel out-of-context environments and therefore making it harder for models to rely on scene context alone, and instead infer semantics from local structure as well. We perform detailed analysis to understand the importance of global context, local structures and the effect of mixing scenes. In experiments, we show that models trained with Mix3D profit from a significant performance boost on indoor (ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially used with any existing method, e.g., trained with Mix3D, MinkowskiNet outperforms all prior state-of-the-art methods by a significant margin on the ScanNet test benchmark 78.1 mIoU. Code is available at: https://nekrasov.dev/mix3d/



### Transformer Assisted Convolutional Network for Cell Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.02270v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.02270v1)
- **Published**: 2021-10-05 18:18:31+00:00
- **Updated**: 2021-10-05 18:18:31+00:00
- **Authors**: Deepanshu Pandey, Pradyumna Gupta, Sumit Bhattacharya, Aman Sinha, Rohit Agarwal
- **Comment**: None
- **Journal**: None
- **Summary**: Region proposal based methods like R-CNN and Faster R-CNN models have proven to be extremely successful in object detection and segmentation tasks. Recently, Transformers have also gained popularity in the domain of Computer Vision, and are being utilised to improve the performance of conventional models. In this paper, we present a relatively new transformer based approach to enhance the performance of the conventional convolutional feature extractor in the existing region proposal based methods. Our approach merges the convolutional feature maps with transformer-based token embeddings by applying a projection operation similar to self-attention in transformers. The results of our experiments show that transformer assisted feature extractor achieves a significant improvement in mIoU (mean Intersection over Union) scores compared to vanilla convolutional backbone.



### Bilevel Imaging Learning Problems as Mathematical Programs with Complementarity Constraints: Reformulation and Theory
- **Arxiv ID**: http://arxiv.org/abs/2110.02273v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG, 49K99, 90C33, 68U10, 68T99, 65K10
- **Links**: [PDF](http://arxiv.org/pdf/2110.02273v2)
- **Published**: 2021-10-05 18:26:22+00:00
- **Updated**: 2023-03-20 16:18:27+00:00
- **Authors**: Juan Carlos De los Reyes
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate a family of bilevel imaging learning problems where the lower-level instance corresponds to a convex variational model involving first- and second-order nonsmooth sparsity-based regularizers. By using geometric properties of the primal-dual reformulation of the lower-level problem and introducing suitable auxiliar variables, we are able to reformulate the original bilevel problems as Mathematical Programs with Complementarity Constraints (MPCC). For the latter, we prove tight constraint qualification conditions (MPCC-RCPLD and partial MPCC-LICQ) and derive Mordukhovich (M-) and Strong (S-) stationarity conditions. The stationarity systems for the MPCC turn also into stationarity conditions for the original formulation. Second-order sufficient optimality conditions are derived as well, together with a local uniqueness result for stationary points. The proposed reformulation may be extended to problems in function spaces, leading to MPCC's with constraints on the gradient of the state. The MPCC reformulation also leads to the efficient use of available large-scale nonlinear programming solvers, as shown in a companion paper, where different imaging applications are studied.



### Scaling up instance annotation via label propagation
- **Arxiv ID**: http://arxiv.org/abs/2110.02277v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02277v1)
- **Published**: 2021-10-05 18:29:34+00:00
- **Updated**: 2021-10-05 18:29:34+00:00
- **Authors**: Dim P. Papadopoulos, Ethan Weber, Antonio Torralba
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Manually annotating object segmentation masks is very time-consuming. While interactive segmentation methods offer a more efficient alternative, they become unaffordable at a large scale because the cost grows linearly with the number of annotated masks. In this paper, we propose a highly efficient annotation scheme for building large datasets with object segmentation masks. At a large scale, images contain many object instances with similar appearance. We exploit these similarities by using hierarchical clustering on mask predictions made by a segmentation model. We propose a scheme that efficiently searches through the hierarchy of clusters and selects which clusters to annotate. Humans manually verify only a few masks per cluster, and the labels are propagated to the whole cluster. Through a large-scale experiment to populate 1M unlabeled images with object segmentation masks for 80 object classes, we show that (1) we obtain 1M object segmentation masks with an total annotation time of only 290 hours; (2) we reduce annotation time by 76x compared to manual annotation; (3) the segmentation quality of our masks is on par with those from manually annotated datasets. Code, data, and models are available online.



### Turing approximations, toric isometric embeddings & manifold convolutions
- **Arxiv ID**: http://arxiv.org/abs/2110.02279v1
- **DOI**: None
- **Categories**: **math.DG**, cs.AI, cs.CG, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02279v1)
- **Published**: 2021-10-05 18:36:16+00:00
- **Updated**: 2021-10-05 18:36:16+00:00
- **Authors**: P. Suárez-Serrato
- **Comment**: 31 pages, 5 figures
- **Journal**: None
- **Summary**: Convolutions are fundamental elements in deep learning architectures. Here, we present a theoretical framework for combining extrinsic and intrinsic approaches to manifold convolution through isometric embeddings into tori. In this way, we define a convolution operator for a manifold of arbitrary topology and dimension. We also explain geometric and topological conditions that make some local definitions of convolutions which rely on translating filters along geodesic paths on a manifold, computationally intractable. A result of Alan Turing from 1938 underscores the need for such a toric isometric embedding approach to achieve a global definition of convolution on computable, finite metric space approximations to a smooth manifold.



### BI-RADS-Net: An Explainable Multitask Learning Approach for Cancer Diagnosis in Breast Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2110.04069v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.04069v1)
- **Published**: 2021-10-05 19:14:46+00:00
- **Updated**: 2021-10-05 19:14:46+00:00
- **Authors**: Boyu Zhang, Aleksandar Vakanski, Min Xian
- **Comment**: None
- **Journal**: None
- **Summary**: In healthcare, it is essential to explain the decision-making process of machine learning models to establish the trustworthiness of clinicians. This paper introduces BI-RADS-Net, a novel explainable deep learning approach for cancer detection in breast ultrasound images. The proposed approach incorporates tasks for explaining and classifying breast tumors, by learning feature representations relevant to clinical diagnosis. Explanations of the predictions (benign or malignant) are provided in terms of morphological features that are used by clinicians for diagnosis and reporting in medical practice. The employed features include the BI-RADS descriptors of shape, orientation, margin, echo pattern, and posterior features. Additionally, our approach predicts the likelihood of malignancy of the findings, which relates to the BI-RADS assessment category reported by clinicians. Experimental validation on a dataset consisting of 1,192 images indicates improved model accuracy, supported by explanations in clinical terms using the BI-RADS lexicon.



### Enhancement of Anime Imaging Enlargement using Modified Super-Resolution CNN
- **Arxiv ID**: http://arxiv.org/abs/2110.02321v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.02321v1)
- **Published**: 2021-10-05 19:38:50+00:00
- **Updated**: 2021-10-05 19:38:50+00:00
- **Authors**: Tanakit Intaniyom, Warinthorn Thananporn, Kuntpong Woraratpanya
- **Comment**: 6 pages, 11 figures, to be published in The 11th Joint Symposium on
  Computational Intelligence (JSCI11)
- **Journal**: None
- **Summary**: Anime is a storytelling medium similar to movies and books. Anime images are a kind of artworks, which are almost entirely drawn by hand. Hence, reproducing existing Anime with larger sizes and higher quality images is expensive. Therefore, we proposed a model based on convolutional neural networks to extract outstanding features of images, enlarge those images, and enhance the quality of Anime images. We trained the model with a training set of 160 images and a validation set of 20 images. We tested the trained model with a testing set of 20 images. The experimental results indicated that our model successfully enhanced the image quality with a larger image-size when compared with the common existing image enlargement and the original SRCNN method.



### Shape-aware Multi-Person Pose Estimation from Multi-View Images
- **Arxiv ID**: http://arxiv.org/abs/2110.02330v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02330v1)
- **Published**: 2021-10-05 20:04:21+00:00
- **Updated**: 2021-10-05 20:04:21+00:00
- **Authors**: Zijian Dong, Jie Song, Xu Chen, Chen Guo, Otmar Hilliges
- **Comment**: ICCV'2021; Video: https://www.youtube.com/watch?v=KE5Jpnyqmh4 |
  Project page: https://ait.ethz.ch/projects/2021/multi-human-pose/
- **Journal**: None
- **Summary**: In this paper we contribute a simple yet effective approach for estimating 3D poses of multiple people from multi-view images. Our proposed coarse-to-fine pipeline first aggregates noisy 2D observations from multiple camera views into 3D space and then associates them into individual instances based on a confidence-aware majority voting technique. The final pose estimates are attained from a novel optimization scheme which links high-confidence multi-view 2D observations and 3D joint candidates. Moreover, a statistical parametric body model such as SMPL is leveraged as a regularizing prior for these 3D joint candidates. Specifically, both 3D poses and SMPL parameters are optimized jointly in an alternating fashion. Here the parametric models help in correcting implausible 3D pose estimates and filling in missing joint detections while updated 3D poses in turn guide obtaining better SMPL estimations. By linking 2D and 3D observations, our method is both accurate and generalizes to different data sources because it better decouples the final 3D pose from the inter-person constellation and is more robust to noisy 2D detections. We systematically evaluate our method on public datasets and achieve state-of-the-art performance. The code and video will be available on the project page: https://ait.ethz.ch/projects/2021/multi-human-pose/.



### Geometric Algebra Attention Networks for Small Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2110.02393v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.02393v2)
- **Published**: 2021-10-05 22:52:12+00:00
- **Updated**: 2022-10-03 21:41:04+00:00
- **Authors**: Matthew Spellings
- **Comment**: None
- **Journal**: None
- **Summary**: Much of the success of deep learning is drawn from building architectures that properly respect underlying symmetry and structure in the data on which they operate - a set of considerations that have been united under the banner of geometric deep learning. Often problems in the physical sciences deal with relatively small sets of points in two- or three-dimensional space wherein translation, rotation, and permutation equivariance are important or even vital for models to be useful in practice. In this work, we present rotation- and permutation-equivariant architectures for deep learning on these small point clouds, composed of a set of products of terms from the geometric algebra and reductions over those products using an attention mechanism. The geometric algebra provides valuable mathematical structure by which to combine vector, scalar, and other types of geometric inputs in a systematic way to account for rotation invariance or covariance, while attention yields a powerful way to impose permutation equivariance. We demonstrate the usefulness of these architectures by training models to solve sample problems relevant to physics, chemistry, and biology.



### Task Affinity with Maximum Bipartite Matching in Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.02399v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.02399v2)
- **Published**: 2021-10-05 23:15:55+00:00
- **Updated**: 2022-01-21 22:06:53+00:00
- **Authors**: Cat P. Le, Juncheng Dong, Mohammadreza Soltani, Vahid Tarokh
- **Comment**: Accepted as a conference paper at ICLR 2022
- **Journal**: None
- **Summary**: We propose an asymmetric affinity score for representing the complexity of utilizing the knowledge of one task for learning another one. Our method is based on the maximum bipartite matching algorithm and utilizes the Fisher Information matrix. We provide theoretical analyses demonstrating that the proposed score is mathematically well-defined, and subsequently use the affinity score to propose a novel algorithm for the few-shot learning problem. In particular, using this score, we find relevant training data labels to the test data and leverage the discovered relevant data for episodically fine-tuning a few-shot model. Results on various few-shot benchmark datasets demonstrate the efficacy of the proposed approach by improving the classification accuracy over the state-of-the-art methods even when using smaller models.



### 3D-MOV: Audio-Visual LSTM Autoencoder for 3D Reconstruction of Multiple Objects from Video
- **Arxiv ID**: http://arxiv.org/abs/2110.02404v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2110.02404v1)
- **Published**: 2021-10-05 23:23:19+00:00
- **Updated**: 2021-10-05 23:23:19+00:00
- **Authors**: Justin Wilson, Ming C. Lin
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object reconstructions of transparent and concave structured objects, with inferred material properties, remains an open research problem for robot navigation in unstructured environments. In this paper, we propose a multimodal single- and multi-frame neural network for 3D reconstructions using audio-visual inputs. Our trained reconstruction LSTM autoencoder 3D-MOV accepts multiple inputs to account for a variety of surface types and views. Our neural network produces high-quality 3D reconstructions using voxel representation. Based on Intersection-over-Union (IoU), we evaluate against other baseline methods using synthetic audio-visual datasets ShapeNet and Sound20K with impact sounds and bounding box annotations. To the best of our knowledge, our single- and multi-frame model is the first audio-visual reconstruction neural network for 3D geometry and material representation.



### Echo-Reconstruction: Audio-Augmented 3D Scene Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2110.02405v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2110.02405v1)
- **Published**: 2021-10-05 23:23:51+00:00
- **Updated**: 2021-10-05 23:23:51+00:00
- **Authors**: Justin Wilson, Nicholas Rewkowski, Ming C. Lin, Henry Fuchs
- **Comment**: None
- **Journal**: None
- **Summary**: Reflective and textureless surfaces such as windows, mirrors, and walls can be a challenge for object and scene reconstruction. These surfaces are often poorly reconstructed and filled with depth discontinuities and holes, making it difficult to cohesively reconstruct scenes that contain these planar discontinuities. We propose Echoreconstruction, an audio-visual method that uses the reflections of sound to aid in geometry and audio reconstruction for virtual conferencing, teleimmersion, and other AR/VR experience. The mobile phone prototype emits pulsed audio, while recording video for RGB-based 3D reconstruction and audio-visual classification. Reflected sound and images from the video are input into our audio (EchoCNN-A) and audio-visual (EchoCNN-AV) convolutional neural networks for surface and sound source detection, depth estimation, and material classification. The inferences from these classifications enhance scene 3D reconstructions containing open spaces and reflective surfaces by depth filtering, inpainting, and placement of unmixed sound sources in the scene. Our prototype, VR demo, and experimental results from real-world and virtual scenes with challenging surfaces and sound indicate high success rates on classification of material, depth estimation, and closed/open surfaces, leading to considerable visual and audio improvement in 3D scenes (see Figure 1).



### A Multi-Scale A Contrario method for Unsupervised Image Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.02407v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.02407v1)
- **Published**: 2021-10-05 23:29:58+00:00
- **Updated**: 2021-10-05 23:29:58+00:00
- **Authors**: Matias Tailanian, Pablo Musé, Álvaro Pardo
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Anomalies can be defined as any non-random structure which deviates from normality. Anomaly detection methods reported in the literature are numerous and diverse, as what is considered anomalous usually varies depending on particular scenarios and applications. In this work we propose an a contrario framework to detect anomalies in images applying statistical analysis to feature maps obtained via convolutions. We evaluate filters learned from the image under analysis via patch PCA, Gabor filters and the feature maps obtained from a pre-trained deep neural network (Resnet). The proposed method is multi-scale and fully unsupervised and is able to detect anomalies in a wide variety of scenarios. While the end goal of this work is the detection of subtle defects in leather samples for the automotive industry, we show that the same algorithm achieves state of the art results in public anomalies datasets.



### CADA: Multi-scale Collaborative Adversarial Domain Adaptation for Unsupervised Optic Disc and Cup Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.02417v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02417v1)
- **Published**: 2021-10-05 23:44:26+00:00
- **Updated**: 2021-10-05 23:44:26+00:00
- **Authors**: Peng Liu, Charlie T. Tran, Bin Kong, Ruogu Fang
- **Comment**: arXiv admin note: text overlap with arXiv:1910.07638
- **Journal**: None
- **Summary**: The diversity of retinal imaging devices poses a significant challenge: domain shift, which leads to performance degradation when applying the deep learning models trained on one domain to new testing domains. In this paper, we propose a multi-scale input along with multiple domain adaptors applied hierarchically in both feature and output spaces. The proposed training strategy and novel unsupervised domain adaptation framework, called Collaborative Adversarial Domain Adaptation (CADA), can effectively overcome the challenge. Multi-scale inputs can reduce the information loss due to the pooling layers used in the network for feature extraction, while our proposed CADA is an interactive paradigm that presents an exquisite collaborative adaptation through both adversarial learning and ensembling weights at different network layers. In particular, to produce a better prediction for the unlabeled target domain data, we simultaneously achieve domain invariance and model generalizability via adversarial learning at multi-scale outputs from different levels of network layers and maintaining an exponential moving average (EMA) of the historical weights during training. Without annotating any sample from the target domain, multiple adversarial losses in encoder and decoder layers guide the extraction of domain-invariant features to confuse the domain classifier. Meanwhile, the ensembling of weights via EMA reduces the uncertainty of adapting multiple discriminator learning. Comprehensive experimental results demonstrate that our CADA model incorporating multi-scale input training can overcome performance degradation and outperform state-of-the-art domain adaptation methods in segmenting retinal optic disc and cup from fundus images stemming from the REFUGE, Drishti-GS, and Rim-One-r3 datasets.



