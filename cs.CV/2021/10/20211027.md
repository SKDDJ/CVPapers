# Arxiv Papers in cs.CV on 2021-10-27
### Towards Robust Bisimulation Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.14096v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, I.2.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2110.14096v1)
- **Published**: 2021-10-27 00:32:07+00:00
- **Updated**: 2021-10-27 00:32:07+00:00
- **Authors**: Mete Kemertas, Tristan Aumentado-Armstrong
- **Comment**: Accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: Learned representations in deep reinforcement learning (DRL) have to extract task-relevant information from complex observations, balancing between robustness to distraction and informativeness to the policy. Such stable and rich representations, often learned via modern function approximation techniques, can enable practical application of the policy improvement theorem, even in high-dimensional continuous state-action spaces. Bisimulation metrics offer one solution to this representation learning problem, by collapsing functionally similar states together in representation space, which promotes invariance to noise and distractors. In this work, we generalize value function approximation bounds for on-policy bisimulation metrics to non-optimal policies and approximate environment dynamics. Our theoretical results help us identify embedding pathologies that may occur in practical use. In particular, we find that these issues stem from an underconstrained dynamics model and an unstable dependence of the embedding norm on the reward signal in environments with sparse rewards. Further, we propose a set of practical remedies: (i) a norm constraint on the representation space, and (ii) an extension of prior approaches with intrinsic rewards and latent space regularization. Finally, we provide evidence that the resulting method is not only more robust to sparse reward functions, but also able to solve challenging continuous control tasks with observational distractions, where prior methods fail.



### ScaleCert: Scalable Certified Defense against Adversarial Patches with Sparse Superficial Layers
- **Arxiv ID**: http://arxiv.org/abs/2110.14120v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.14120v2)
- **Published**: 2021-10-27 02:05:00+00:00
- **Updated**: 2021-11-04 07:53:13+00:00
- **Authors**: Husheng Han, Kaidi Xu, Xing Hu, Xiaobing Chen, Ling Liang, Zidong Du, Qi Guo, Yanzhi Wang, Yunji Chen
- **Comment**: Accepted at NeurIPS 2021
- **Journal**: None
- **Summary**: Adversarial patch attacks that craft the pixels in a confined region of the input images show their powerful attack effectiveness in physical environments even with noises or deformations. Existing certified defenses towards adversarial patch attacks work well on small images like MNIST and CIFAR-10 datasets, but achieve very poor certified accuracy on higher-resolution images like ImageNet. It is urgent to design both robust and effective defenses against such a practical and harmful attack in industry-level larger images. In this work, we propose the certified defense methodology that achieves high provable robustness for high-resolution images and largely improves the practicality for real adoption of the certified defense. The basic insight of our work is that the adversarial patch intends to leverage localized superficial important neurons (SIN) to manipulate the prediction results. Hence, we leverage the SIN-based DNN compression techniques to significantly improve the certified accuracy, by reducing the adversarial region searching overhead and filtering the prediction noises. Our experimental results show that the certified accuracy is increased from 36.3% (the state-of-the-art certified detection) to 60.4% on the ImageNet dataset, largely pushing the certified defenses for practical use.



### SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2110.14143v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14143v1)
- **Published**: 2021-10-27 03:29:34+00:00
- **Updated**: 2021-10-27 03:29:34+00:00
- **Authors**: Abhinav Moudgil, Arjun Majumdar, Harsh Agrawal, Stefan Lee, Dhruv Batra
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Natural language instructions for visual navigation often use scene descriptions (e.g., "bedroom") and object references (e.g., "green chairs") to provide a breadcrumb trail to a goal location. This work presents a transformer-based vision-and-language navigation (VLN) agent that uses two different visual encoders -- a scene classification network and an object detector -- which produce features that match these two distinct types of visual cues. In our method, scene features contribute high-level contextual information that supports object-level processing. With this design, our model is able to use vision-and-language pretraining (i.e., learning the alignment between images and text from large-scale web data) to substantially improve performance on the Room-to-Room (R2R) and Room-Across-Room (RxR) benchmarks. Specifically, our approach leads to improvements of 1.8% absolute in SPL on R2R and 3.7% absolute in SR on RxR. Our analysis reveals even larger gains for navigation instructions that contain six or more object references, which further suggests that our approach is better able to use object features and align them to references in the instructions.



### Physically Explainable CNN for SAR Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.14144v2
- **DOI**: 10.1016/j.isprsjprs.2022.05.008
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14144v2)
- **Published**: 2021-10-27 03:30:18+00:00
- **Updated**: 2022-06-02 08:41:13+00:00
- **Authors**: Zhongling Huang, Xiwen Yao, Ying Liu, Corneliu Octavian Dumitru, Mihai Datcu, Junwei Han
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing Volume 190,
  August 2022, Pages 25-37
- **Summary**: Integrating the special electromagnetic characteristics of Synthetic Aperture Radar (SAR) in deep neural networks is essential in order to enhance the explainability and physics awareness of deep learning. In this paper, we first propose a novel physically explainable convolutional neural network for SAR image classification, namely physics guided and injected learning (PGIL). It comprises three parts: (1) explainable models (XM) to provide prior physics knowledge, (2) physics guided network (PGN) to encode the knowledge into physics-aware features, and (3) physics injected network (PIN) to adaptively introduce the physics-aware features into classification pipeline for label prediction. A hybrid Image-Physics SAR dataset format is proposed for evaluation, with both Sentinel-1 and Gaofen-3 SAR data being experimented. The results show that the proposed PGIL substantially improve the classification performance in case of limited labeled data compared with the counterpart data-driven CNN and other pre-training methods. Additionally, the physics explanations are discussed to indicate the interpretability and the physical consistency preserved in the predictions. We deem the proposed method would promote the development of physically explainable deep learning in SAR image interpretation field.



### Image Comes Dancing with Collaborative Parsing-Flow Video Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2110.14147v2
- **DOI**: 10.1109/TIP.2021.3123549
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14147v2)
- **Published**: 2021-10-27 03:42:41+00:00
- **Updated**: 2021-10-28 03:08:58+00:00
- **Authors**: Bowen Wu, Zhenyu Xie, Xiaodan Liang, Yubei Xiao, Haoye Dong, Liang Lin
- **Comment**: TIP 2021
- **Journal**: None
- **Summary**: Transferring human motion from a source to a target person poses great potential in computer vision and graphics applications. A crucial step is to manipulate sequential future motion while retaining the appearance characteristic.Previous work has either relied on crafted 3D human models or trained a separate model specifically for each target person, which is not scalable in practice.This work studies a more general setting, in which we aim to learn a single model to parsimoniously transfer motion from a source video to any target person given only one image of the person, named as Collaborative Parsing-Flow Network (CPF-Net). The paucity of information regarding the target person makes the task particularly challenging to faithfully preserve the appearance in varying designated poses. To address this issue, CPF-Net integrates the structured human parsing and appearance flow to guide the realistic foreground synthesis which is merged into the background by a spatio-temporal fusion module. In particular, CPF-Net decouples the problem into stages of human parsing sequence generation, foreground sequence generation and final video generation. The human parsing generation stage captures both the pose and the body structure of the target. The appearance flow is beneficial to keep details in synthesized frames. The integration of human parsing and appearance flow effectively guides the generation of video frames with realistic appearance. Finally, the dedicated designed fusion network ensure the temporal coherence. We further collect a large set of human dancing videos to push forward this research field. Both quantitative and qualitative results show our method substantially improves over previous approaches and is able to generate appealing and photo-realistic target videos given any input person image. All source code and dataset will be released at https://github.com/xiezhy6/CPF-Net.



### Training Wasserstein GANs without gradient penalties
- **Arxiv ID**: http://arxiv.org/abs/2110.14150v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2110.14150v1)
- **Published**: 2021-10-27 03:46:13+00:00
- **Updated**: 2021-10-27 03:46:13+00:00
- **Authors**: Dohyun Kwon, Yeoneung Kim, Guido Montúfar, Insoon Yang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a stable method to train Wasserstein generative adversarial networks. In order to enhance stability, we consider two objective functions using the $c$-transform based on Kantorovich duality which arises in the theory of optimal transport. We experimentally show that this algorithm can effectively enforce the Lipschitz constraint on the discriminator while other standard methods fail to do so. As a consequence, our method yields an accurate estimation for the optimal discriminator and also for the Wasserstein distance between the true distribution and the generated one. Our method requires no gradient penalties nor corresponding hyperparameter tuning and is computationally more efficient than other methods. At the same time, it yields competitive generators of synthetic images based on the MNIST, F-MNIST, and CIFAR-10 datasets.



### Identifying the key components in ResNet-50 for diabetic retinopathy grading from fundus images: a systematic investigation
- **Arxiv ID**: http://arxiv.org/abs/2110.14160v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14160v2)
- **Published**: 2021-10-27 04:37:52+00:00
- **Updated**: 2022-10-17 23:46:43+00:00
- **Authors**: Yijin Huang, Li Lin, Pujin Cheng, Junyan Lyu, Roger Tam, Xiaoying Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep learning based diabetic retinopathy (DR) classification methods typically benefit from well-designed architectures of convolutional neural networks, the training setting also has a non-negligible impact on the prediction performance. The training setting includes various interdependent components, such as objective function, data sampling strategy and data augmentation approach. To identify the key components in a standard deep learning framework (ResNet-50) for DR grading, we systematically analyze the impact of several major components. Extensive experiments are conducted on a publicly-available dataset EyePACS. We demonstrate that (1) the DR grading framework is sensitive to input resolution, objective function, and composition of data augmentation, (2) using mean square error as the loss function can effectively improve the performance with respect to a task-specific evaluation metric, namely the quadratically-weighted Kappa, (3) utilizing eye pairs boosts the performance of DR grading and (4) using data resampling to address the problem of imbalanced data distribution in EyePACS hurts the performance. Based on these observations and an optimal combination of the investigated components, our framework, without any specialized network design, achieves the state-of-the-art result (0.8631 for Kappa) on the EyePACS test set (a total of 42670 fundus images) with only image-level labels. We also examine the proposed training practices on other fundus datasets and other network architectures to evaluate their generalizability. Our codes and pre-trained model are available at https://github.com/YijinHuang/pytorch-classification.



### QU-net++: Image Quality Detection Framework for Segmentation of Medical 3D Image Stacks
- **Arxiv ID**: http://arxiv.org/abs/2110.14181v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.14181v4)
- **Published**: 2021-10-27 05:28:02+00:00
- **Updated**: 2022-04-12 22:42:12+00:00
- **Authors**: Sohini Roychowdhury
- **Comment**: 4 pages, 7 figures, 1 Table
- **Journal**: IEEE EMBC, 2022
- **Summary**: Automated segmentation of pathological regions of interest aids medical image diagnostics and follow-up care. However, accurate pathological segmentations require high quality of annotated data that can be both cost and time intensive to generate. In this work, we propose an automated two-step method that detects a minimal image subset required to train segmentation models by evaluating the quality of medical images from 3D image stacks using a U-net++ model. These images that represent a lack of quality training can then be annotated and used to fully train a U-net-based segmentation model. The proposed QU-net++ model detects this lack of quality training based on the disagreement in segmentations produced from the final two output layers. The proposed model isolates around 10% of the slices per 3D image stack and can scale across imaging modalities to segment cysts in OCT images and ground glass opacity (GGO) in lung CT images with Dice scores in the range 0.56-0.72. Thus, the proposed method can be applied for cost effective multi-modal pathology segmentation tasks.



### Evidential Softmax for Sparse Multimodal Distributions in Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2110.14182v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.14182v1)
- **Published**: 2021-10-27 05:32:25+00:00
- **Updated**: 2021-10-27 05:32:25+00:00
- **Authors**: Phil Chen, Masha Itkina, Ransalu Senanayake, Mykel J. Kochenderfer
- **Comment**: Accepted to NeurIPS 2021. Code is available at
  https://github.com/sisl/EvSoftmax
- **Journal**: None
- **Summary**: Many applications of generative models rely on the marginalization of their high-dimensional output probability distributions. Normalization functions that yield sparse probability distributions can make exact marginalization more computationally tractable. However, sparse normalization functions usually require alternative loss functions for training since the log-likelihood is undefined for sparse probability distributions. Furthermore, many sparse normalization functions often collapse the multimodality of distributions. In this work, we present $\textit{ev-softmax}$, a sparse normalization function that preserves the multimodality of probability distributions. We derive its properties, including its gradient in closed-form, and introduce a continuous family of approximations to $\textit{ev-softmax}$ that have full support and can be trained with probabilistic loss functions such as negative log-likelihood and Kullback-Leibler divergence. We evaluate our method on a variety of generative models, including variational autoencoders and auto-regressive architectures. Our method outperforms existing dense and sparse normalization techniques in distributional accuracy. We demonstrate that $\textit{ev-softmax}$ successfully reduces the dimensionality of probability distributions while maintaining multimodality.



### Robust Contrastive Learning Using Negative Samples with Diminished Semantics
- **Arxiv ID**: http://arxiv.org/abs/2110.14189v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.14189v2)
- **Published**: 2021-10-27 05:38:00+00:00
- **Updated**: 2022-01-03 18:53:23+00:00
- **Authors**: Songwei Ge, Shlok Mishra, Haohan Wang, Chun-Liang Li, David Jacobs
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Unsupervised learning has recently made exceptional progress because of the development of more effective contrastive learning methods. However, CNNs are prone to depend on low-level features that humans deem non-semantic. This dependency has been conjectured to induce a lack of robustness to image perturbations or domain shift. In this paper, we show that by generating carefully designed negative samples, contrastive learning can learn more robust representations with less dependence on such features. Contrastive learning utilizes positive pairs that preserve semantic information while perturbing superficial features in the training images. Similarly, we propose to generate negative samples in a reversed way, where only the superfluous instead of the semantic features are preserved. We develop two methods, texture-based and patch-based augmentations, to generate negative samples. These samples achieve better generalization, especially under out-of-domain settings. We also analyze our method and the generated texture-based samples, showing that texture features are indispensable in classifying particular ImageNet classes and especially finer classes. We also show that model bias favors texture and shape features differently under different test settings. Our code, trained models, and ImageNet-Texture dataset can be found at https://github.com/SongweiGe/Contrastive-Learning-with-Non-Semantic-Negatives.



### Mixed Supervised Object Detection by Transferring Mask Prior and Semantic Similarity
- **Arxiv ID**: http://arxiv.org/abs/2110.14191v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14191v1)
- **Published**: 2021-10-27 05:43:09+00:00
- **Updated**: 2021-10-27 05:43:09+00:00
- **Authors**: Yan Liu, Zhijie Zhang, Li Niu, Junjie Chen, Liqing Zhang
- **Comment**: accepted by NeurIPS2021
- **Journal**: None
- **Summary**: Object detection has achieved promising success, but requires large-scale fully-annotated data, which is time-consuming and labor-extensive. Therefore, we consider object detection with mixed supervision, which learns novel object categories using weak annotations with the help of full annotations of existing base object categories. Previous works using mixed supervision mainly learn the class-agnostic objectness from fully-annotated categories, which can be transferred to upgrade the weak annotations to pseudo full annotations for novel categories. In this paper, we further transfer mask prior and semantic similarity to bridge the gap between novel categories and base categories. Specifically, the ability of using mask prior to help detect objects is learned from base categories and transferred to novel categories. Moreover, the semantic similarity between objects learned from base categories is transferred to denoise the pseudo full annotations for novel categories. Experimental results on three benchmark datasets demonstrate the effectiveness of our method over existing methods. Codes are available at https://github.com/bcmi/TraMaS-Weak-Shot-Object-Detection.



### Smooth head tracking for virtual reality applications
- **Arxiv ID**: http://arxiv.org/abs/2110.14193v1
- **DOI**: 10.1007/s11760-016-0984-4
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.14193v1)
- **Published**: 2021-10-27 05:47:21+00:00
- **Updated**: 2021-10-27 05:47:21+00:00
- **Authors**: Abdenour Amamra
- **Comment**: 8 pages, 1 figure
- **Journal**: SIViP 11, 479-486 (2017)
- **Summary**: In this work, we propose a new head-tracking solution for human-machine real-time interaction with virtual 3D environments. This solution leverages RGBD data to compute virtual camera pose according to the movements of the user's head. The process starts with the extraction of a set of facial features from the images delivered by the sensor. Such features are matched against their respective counterparts in a reference image for the computation of the current head pose. Afterwards, a prediction approach is used to guess the most likely next head move (final pose). Pythagorean Hodograph interpolation is then adapted to determine the path and local frames taken between the two poses. The result is a smooth head trajectory that serves as an input to set the camera in virtual scenes according to the user's gaze. The resulting motion model has the advantage of being: continuous in time, it adapts to any frame rate of rendering; it is ergonomic, as it frees the user from wearing tracking markers; it is smooth and free from rendering jerks; and it is also torsion and curvature minimizing as it produces a path with minimum bending energy.



### From Image to Imuge: Immunized Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2110.14196v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.14196v1)
- **Published**: 2021-10-27 05:56:15+00:00
- **Updated**: 2021-10-27 05:56:15+00:00
- **Authors**: Qichao Ying, Zhenxing Qian, Hang Zhou, Haisheng Xu, Xinpeng Zhang, Siyi Li
- **Comment**: Accepted as Poster at ACMMM 2021. Authors are from Fudan University,
  Simon Fraser University and NVIDIA, China
- **Journal**: None
- **Summary**: We introduce Imuge, an image tamper resilient generative scheme for image self-recovery. The traditional manner of concealing image content within the image are inflexible and fragile to diverse digital attack, i.e. image cropping and JPEG compression. To address this issue, we jointly train a U-Net backboned encoder, a tamper localization network and a decoder for image recovery. Given an original image, the encoder produces a visually indistinguishable immunized image. At the recipient's side, the verifying network localizes the malicious modifications, and the original content can be approximately recovered by the decoder, despite the presence of the attacks. Several strategies are proposed to boost the training efficiency. We demonstrate that our method can recover the details of the tampered regions with a high quality despite the presence of various kinds of attacks. Comprehensive ablation studies are conducted to validate our network designs.



### Denoised Non-Local Neural Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.14200v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14200v1)
- **Published**: 2021-10-27 06:16:31+00:00
- **Updated**: 2021-10-27 06:16:31+00:00
- **Authors**: Qi Song, Jie Li, Hao Guo, Rui Huang
- **Comment**: submitted to IEEE Transactions on Neural Networks and Learning
  Systems (TNNLS)
- **Journal**: None
- **Summary**: The non-local network has become a widely used technique for semantic segmentation, which computes an attention map to measure the relationships of each pixel pair. However, most of the current popular non-local models tend to ignore the phenomenon that the calculated attention map appears to be very noisy, containing inter-class and intra-class inconsistencies, which lowers the accuracy and reliability of the non-local methods. In this paper, we figuratively denote these inconsistencies as attention noises and explore the solutions to denoise them. Specifically, we inventively propose a Denoised Non-Local Network (Denoised NL), which consists of two primary modules, i.e., the Global Rectifying (GR) block and the Local Retention (LR) block, to eliminate the inter-class and intra-class noises respectively. First, GR adopts the class-level predictions to capture a binary map to distinguish whether the selected two pixels belong to the same category. Second, LR captures the ignored local dependencies and further uses them to rectify the unwanted hollows in the attention map. The experimental results on two challenging semantic segmentation datasets demonstrate the superior performance of our model. Without any external training data, our proposed Denoised NL can achieve the state-of-the-art performance of 83.5\% and 46.69\% mIoU on Cityscapes and ADE20K, respectively.



### Revisit Multimodal Meta-Learning through the Lens of Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.14202v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14202v1)
- **Published**: 2021-10-27 06:23:45+00:00
- **Updated**: 2021-10-27 06:23:45+00:00
- **Authors**: Milad Abdollahzadeh, Touba Malekzadeh, Ngai-Man Cheung
- **Comment**: Accepted in 35th Conference on Neural Information Processing Systems
  (NeurIPS 2021); 27 pages
- **Journal**: None
- **Summary**: Multimodal meta-learning is a recent problem that extends conventional few-shot meta-learning by generalizing its setup to diverse multimodal task distributions. This setup makes a step towards mimicking how humans make use of a diverse set of prior skills to learn new skills. Previous work has achieved encouraging performance. In particular, in spite of the diversity of the multimodal tasks, previous work claims that a single meta-learner trained on a multimodal distribution can sometimes outperform multiple specialized meta-learners trained on individual unimodal distributions. The improvement is attributed to knowledge transfer between different modes of task distributions. However, there is no deep investigation to verify and understand the knowledge transfer between multimodal tasks. Our work makes two contributions to multimodal meta-learning. First, we propose a method to quantify knowledge transfer between tasks of different modes at a micro-level. Our quantitative, task-level analysis is inspired by the recent transference idea from multi-task learning. Second, inspired by hard parameter sharing in multi-task learning and a new interpretation of related work, we propose a new multimodal meta-learner that outperforms existing work by considerable margins. While the major focus is on multimodal meta-learning, our work also attempts to shed light on task interaction in conventional meta-learning. The code for this project is available at https://miladabd.github.io/KML.



### Neural View Synthesis and Matching for Semi-Supervised Few-Shot Learning of 3D Pose
- **Arxiv ID**: http://arxiv.org/abs/2110.14213v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14213v1)
- **Published**: 2021-10-27 06:53:53+00:00
- **Updated**: 2021-10-27 06:53:53+00:00
- **Authors**: Angtian Wang, Shenxiao Mei, Alan Yuille, Adam Kortylewski
- **Comment**: NeurIPS 2021; Code is available under
  https://github.com/Angtian/NeuralVS
- **Journal**: None
- **Summary**: We study the problem of learning to estimate the 3D object pose from a few labelled examples and a collection of unlabelled data. Our main contribution is a learning framework, neural view synthesis and matching, that can transfer the 3D pose annotation from the labelled to unlabelled images reliably, despite unseen 3D views and nuisance variations such as the object shape, texture, illumination or scene context. In our approach, objects are represented as 3D cuboid meshes composed of feature vectors at each mesh vertex. The model is initialized from a few labelled images and is subsequently used to synthesize feature representations of unseen 3D views. The synthesized views are matched with the feature representations of unlabelled images to generate pseudo-labels of the 3D pose. The pseudo-labelled data is, in turn, used to train the feature extractor such that the features at each mesh vertex are more invariant across varying 3D views of the object. Our model is trained in an EM-type manner alternating between increasing the 3D pose invariance of the feature extractor and annotating unlabelled data through neural view synthesis and matching. We demonstrate the effectiveness of the proposed semi-supervised learning framework for 3D pose estimation on the PASCAL3D+ and KITTI datasets. We find that our approach outperforms all baselines by a wide margin, particularly in an extreme few-shot setting where only 7 annotated images are given. Remarkably, we observe that our model also achieves an exceptional robustness in out-of-distribution scenarios that involve partial occlusion.



### Beyond Classification: Knowledge Distillation using Multi-Object Impressions
- **Arxiv ID**: http://arxiv.org/abs/2110.14215v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.14215v1)
- **Published**: 2021-10-27 06:59:27+00:00
- **Updated**: 2021-10-27 06:59:27+00:00
- **Authors**: Gaurav Kumar Nayak, Monish Keswani, Sharan Seshadri, Anirban Chakraborty
- **Comment**: Accepted in BMVC 2021
- **Journal**: None
- **Summary**: Knowledge Distillation (KD) utilizes training data as a transfer set to transfer knowledge from a complex network (Teacher) to a smaller network (Student). Several works have recently identified many scenarios where the training data may not be available due to data privacy or sensitivity concerns and have proposed solutions under this restrictive constraint for the classification task. Unlike existing works, we, for the first time, solve a much more challenging problem, i.e., "KD for object detection with zero knowledge about the training data and its statistics". Our proposed approach prepares pseudo-targets and synthesizes corresponding samples (termed as "Multi-Object Impressions"), using only the pretrained Faster RCNN Teacher network. We use this pseudo-dataset as a transfer set to conduct zero-shot KD for object detection. We demonstrate the efficacy of our proposed method through several ablations and extensive experiments on benchmark datasets like KITTI, Pascal and COCO. Our approach with no training samples, achieves a respectable mAP of 64.2% and 55.5% on the student with same and half capacity while performing distillation from a Resnet-18 Teacher of 73.3% mAP on KITTI.



### Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects
- **Arxiv ID**: http://arxiv.org/abs/2110.14217v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, 68T40 (Primary), 68T45 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/2110.14217v1)
- **Published**: 2021-10-27 07:02:53+00:00
- **Updated**: 2021-10-27 07:02:53+00:00
- **Authors**: Jeffrey Ichnowski, Yahav Avigal, Justin Kerr, Ken Goldberg
- **Comment**: 11 pages, 9 figures, to be published in the Conference on Robot
  Learning (CoRL) 2021
- **Journal**: None
- **Summary**: The ability to grasp and manipulate transparent objects is a major challenge for robots. Existing depth cameras have difficulty detecting, localizing, and inferring the geometry of such objects. We propose using neural radiance fields (NeRF) to detect, localize, and infer the geometry of transparent objects with sufficient accuracy to find and grasp them securely. We leverage NeRF's view-independent learned density, place lights to increase specular reflections, and perform a transparency-aware depth-rendering that we feed into the Dex-Net grasp planner. We show how additional lights create specular reflections that improve the quality of the depth map, and test a setup for a robot workcell equipped with an array of cameras to perform transparent object manipulation. We also create synthetic and real datasets of transparent objects in real-world settings, including singulated objects, cluttered tables, and the top rack of a dishwasher. In each setting we show that NeRF and Dex-Net are able to reliably compute robust grasps on transparent objects, achieving 90% and 100% grasp success rates in physical experiments on an ABB YuMi, on objects where baseline methods fail.



### RRNet: Relational Reasoning Network with Parallel Multi-scale Attention for Salient Object Detection in Optical Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2110.14223v2
- **DOI**: 10.1109/TGRS.2021.3123984
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14223v2)
- **Published**: 2021-10-27 07:18:32+00:00
- **Updated**: 2022-02-21 00:52:00+00:00
- **Authors**: Runmin Cong, Yumo Zhang, Leyuan Fang, Jun Li, Yao Zhao, Sam Kwong
- **Comment**: 11 pages, 9 figures, Accepted by IEEE Transactions on Geoscience and
  Remote Sensing 2021, project: https://rmcong.github.io/proj_RRNet.html
- **Journal**: None
- **Summary**: Salient object detection (SOD) for optical remote sensing images (RSIs) aims at locating and extracting visually distinctive objects/regions from the optical RSIs. Despite some saliency models were proposed to solve the intrinsic problem of optical RSIs (such as complex background and scale-variant objects), the accuracy and completeness are still unsatisfactory. To this end, we propose a relational reasoning network with parallel multi-scale attention for SOD in optical RSIs in this paper. The relational reasoning module that integrates the spatial and the channel dimensions is designed to infer the semantic relationship by utilizing high-level encoder features, thereby promoting the generation of more complete detection results. The parallel multi-scale attention module is proposed to effectively restore the detail information and address the scale variation of salient objects by using the low-level features refined by multi-scale attention. Extensive experiments on two datasets demonstrate that our proposed RRNet outperforms the existing state-of-the-art SOD competitors both qualitatively and quantitatively.



### 2nd Place Solution for VisDA 2021 Challenge -- Universally Domain Adaptive Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.14240v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14240v1)
- **Published**: 2021-10-27 07:48:29+00:00
- **Updated**: 2021-10-27 07:48:29+00:00
- **Authors**: Haojin Liao, Xiaolin Song, Sicheng Zhao, Shanghang Zhang, Xiangyu Yue, Xingxu Yao, Yueming Zhang, Tengfei Xing, Pengfei Xu, Qiang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The Visual Domain Adaptation (VisDA) 2021 Challenge calls for unsupervised domain adaptation (UDA) methods that can deal with both input distribution shift and label set variance between the source and target domains. In this report, we introduce a universal domain adaptation (UniDA) method by aggregating several popular feature extraction and domain adaptation schemes. First, we utilize VOLO, a Transformer-based architecture with state-of-the-art performance in several visual tasks, as the backbone to extract effective feature representations. Second, we modify the open-set classifier of OVANet to recognize the unknown class with competitive accuracy and robustness. As shown in the leaderboard, our proposed UniDA method ranks the 2nd place with 48.56% ACC and 70.72% AUROC in the VisDA 2021 Challenge.



### Multilayer Lookahead: a Nested Version of Lookahead
- **Arxiv ID**: http://arxiv.org/abs/2110.14254v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14254v1)
- **Published**: 2021-10-27 08:13:39+00:00
- **Updated**: 2021-10-27 08:13:39+00:00
- **Authors**: Denys Pushkin, Luis Barba
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, SGD and its variants have become the standard tool to train Deep Neural Networks. In this paper, we focus on the recently proposed variant Lookahead, which improves upon SGD in a wide range of applications. Following this success, we study an extension of this algorithm, the \emph{Multilayer Lookahead} optimizer, which recursively wraps Lookahead around itself. We prove the convergence of Multilayer Lookahead with two layers to a stationary point of smooth non-convex functions with $O(\frac{1}{\sqrt{T}})$ rate. We also justify the improved generalization of both Lookahead over SGD, and of Multilayer Lookahead over Lookahead, by showing how they amplify the implicit regularization effect of SGD. We empirically verify our results and show that Multilayer Lookahead outperforms Lookahead on CIFAR-10 and CIFAR-100 classification tasks, and on GANs training on the MNIST dataset.



### How Important is Importance Sampling for Deep Budgeted Training?
- **Arxiv ID**: http://arxiv.org/abs/2110.14283v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14283v1)
- **Published**: 2021-10-27 09:03:57+00:00
- **Updated**: 2021-10-27 09:03:57+00:00
- **Authors**: Eric Arazo, Diego Ortego, Paul Albert, Noel E. O'Connor, Kevin McGuinness
- **Comment**: British Machine Vision Conference (BMVC) 2021, oral presentation
- **Journal**: None
- **Summary**: Long iterative training processes for Deep Neural Networks (DNNs) are commonly required to achieve state-of-the-art performance in many computer vision tasks. Importance sampling approaches might play a key role in budgeted training regimes, i.e. when limiting the number of training iterations. These approaches aim at dynamically estimating the importance of each sample to focus on the most relevant and speed up convergence. This work explores this paradigm and how a budget constraint interacts with importance sampling approaches and data augmentation techniques. We show that under budget restrictions, importance sampling approaches do not provide a consistent improvement over uniform sampling. We suggest that, given a specific budget, the best course of action is to disregard the importance and introduce adequate data augmentation; e.g. when reducing the budget to a 30% in CIFAR-10/100, RICAP data augmentation maintains accuracy, while importance sampling does not. We conclude from our work that DNNs under budget restrictions benefit greatly from variety in the training set and that finding the right samples to train on is not the most effective strategy when balancing high performance with low computational requirements. Source code available at https://git.io/JKHa3 .



### Improving Super-Resolution Performance using Meta-Attention Layers
- **Arxiv ID**: http://arxiv.org/abs/2110.14638v1
- **DOI**: 10.1109/LSP.2021.3116518
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.14638v1)
- **Published**: 2021-10-27 09:20:21+00:00
- **Updated**: 2021-10-27 09:20:21+00:00
- **Authors**: Matthew Aquilina, Christian Galea, John Abela, Kenneth P. Camilleri, Reuben A. Farrugia
- **Comment**: Accepted for publication in the IEEE Signal Processing Letters. This
  is the accepted version of the paper, for the final formatted version and
  supplementary information, please visit the IEEE's publication at the linked
  DOI
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have achieved impressive results across many super-resolution (SR) and image restoration tasks. While many such networks can upscale low-resolution (LR) images using just the raw pixel-level information, the ill-posed nature of SR can make it difficult to accurately super-resolve an image which has undergone multiple different degradations. Additional information (metadata) describing the degradation process (such as the blur kernel applied, compression level, etc.) can guide networks to super-resolve LR images with higher fidelity to the original source. Previous attempts at informing SR networks with degradation parameters have indeed been able to improve performance in a number of scenarios. However, due to the fully-convolutional nature of many SR networks, most of these metadata fusion methods either require a complete architectural change, or necessitate the addition of significant extra complexity. Thus, these approaches are difficult to introduce into arbitrary SR networks without considerable design alterations. In this paper, we introduce meta-attention, a simple mechanism which allows any SR CNN to exploit the information available in relevant degradation parameters. The mechanism functions by translating the metadata into a channel attention vector, which in turn selectively modulates the network's feature maps. Incorporating meta-attention into SR networks is straightforward, as it requires no specific type of architecture to function correctly. Extensive testing has shown that meta-attention can consistently improve the pixel-level accuracy of state-of-the-art (SOTA) networks when provided with relevant degradation metadata. For PSNR, the gain on blurred/downsampled (X4) images is of 0.2969 dB (on average) and 0.3320 dB for SOTA general and face SR models, respectively.



### Revisiting Sanity Checks for Saliency Maps
- **Arxiv ID**: http://arxiv.org/abs/2110.14297v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2110.14297v1)
- **Published**: 2021-10-27 09:24:26+00:00
- **Updated**: 2021-10-27 09:24:26+00:00
- **Authors**: Gal Yona, Daniel Greenfeld
- **Comment**: None
- **Journal**: None
- **Summary**: Saliency methods are a popular approach for model debugging and explainability. However, in the absence of ground-truth data for what the correct maps should be, evaluating and comparing different approaches remains a long-standing challenge. The sanity checks methodology of Adebayo et al [Neurips 2018] has sought to address this challenge. They argue that some popular saliency methods should not be used for explainability purposes since the maps they produce are not sensitive to the underlying model that is to be explained. Through a causal re-framing of their objective, we argue that their empirical evaluation does not fully establish these conclusions, due to a form of confounding introduced by the tasks they evaluate on. Through various experiments on simple custom tasks we demonstrate that some of their conclusions may indeed be artifacts of the tasks more than a criticism of the saliency methods themselves. More broadly, our work challenges the utility of the sanity check methodology, and further highlights that saliency map evaluation beyond ad-hoc visual examination remains a fundamental challenge.



### Inferring the Class Conditional Response Map for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.14309v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14309v1)
- **Published**: 2021-10-27 09:43:40+00:00
- **Updated**: 2021-10-27 09:43:40+00:00
- **Authors**: Weixuan Sun, Jing Zhang, Nick Barnes
- **Comment**: None
- **Journal**: WACV 2022
- **Summary**: Image-level weakly supervised semantic segmentation (WSSS) relies on class activation maps (CAMs) for pseudo labels generation. As CAMs only highlight the most discriminative regions of objects, the generated pseudo labels are usually unsatisfactory to serve directly as supervision. To solve this, most existing approaches follow a multi-training pipeline to refine CAMs for better pseudo-labels, which includes: 1) re-training the classification model to generate CAMs; 2) post-processing CAMs to obtain pseudo labels; and 3) training a semantic segmentation model with the obtained pseudo labels. However, this multi-training pipeline requires complicated adjustment and additional time. To address this, we propose a class-conditional inference strategy and an activation aware mask refinement loss function to generate better pseudo labels without re-training the classifier. The class conditional inference-time approach is presented to separately and iteratively reveal the classification network's hidden object activation to generate more complete response maps. Further, our activation aware mask refinement loss function introduces a novel way to exploit saliency maps during segmentation training and refine the foreground object masks without suppressing background objects. Our method achieves superior WSSS results without requiring re-training of the classifier.



### MEmoBERT: Pre-training Model with Prompt-based Learning for Multimodal Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.00865v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.00865v1)
- **Published**: 2021-10-27 09:57:00+00:00
- **Updated**: 2021-10-27 09:57:00+00:00
- **Authors**: Jinming Zhao, Ruichen Li, Qin Jin, Xinchao Wang, Haizhou Li
- **Comment**: 4 papges, 2 figures
- **Journal**: None
- **Summary**: Multimodal emotion recognition study is hindered by the lack of labelled corpora in terms of scale and diversity, due to the high annotation cost and label ambiguity. In this paper, we propose a pre-training model \textbf{MEmoBERT} for multimodal emotion recognition, which learns multimodal joint representations through self-supervised learning from large-scale unlabeled video data that come in sheer volume. Furthermore, unlike the conventional "pre-train, finetune" paradigm, we propose a prompt-based method that reformulates the downstream emotion classification task as a masked text prediction one, bringing the downstream task closer to the pre-training. Extensive experiments on two benchmark datasets, IEMOCAP and MSP-IMPROV, show that our proposed MEmoBERT significantly enhances emotion recognition performance.



### Multi-frequency image completion via a biologically-inspired sub-Riemannian model with frequency and phase
- **Arxiv ID**: http://arxiv.org/abs/2110.14330v1
- **DOI**: 10.3390/jimaging7120271
- **Categories**: **cs.CV**, math.DG
- **Links**: [PDF](http://arxiv.org/pdf/2110.14330v1)
- **Published**: 2021-10-27 10:19:07+00:00
- **Updated**: 2021-10-27 10:19:07+00:00
- **Authors**: Emre Baspinar
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel cortically-inspired image completion algorithm. It uses a five dimensional sub-Riemannian cortical geometry modelling the orientation, spatial frequency and phase selective behavior of the cells in the visual cortex. The algorithm extracts the orientation, frequency and phase information existing in a given two dimensional corrupted input image via a Gabor transform and represent those values in terms of cortical cell output responses in the model geometry. Then it performs completion via a diffusion concentrated in a neighbourhood along the neural connections within the model geometry. The diffusion models the activity propagation integrating orientation, frequency and phase features along the neural connections. Finally, the algorithm transforms back the diffused and completed output responses back to the two dimensional image plane.



### Feature and Label Embedding Spaces Matter in Addressing Image Classifier Bias
- **Arxiv ID**: http://arxiv.org/abs/2110.14336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14336v1)
- **Published**: 2021-10-27 10:37:35+00:00
- **Updated**: 2021-10-27 10:37:35+00:00
- **Authors**: William Thong, Cees G. M. Snoek
- **Comment**: Accepted at British Machine Vision Conference (BMVC) 2021
- **Journal**: None
- **Summary**: This paper strives to address image classifier bias, with a focus on both feature and label embedding spaces. Previous works have shown that spurious correlations from protected attributes, such as age, gender, or skin tone, can cause adverse decisions. To balance potential harms, there is a growing need to identify and mitigate image classifier bias. First, we identify in the feature space a bias direction. We compute class prototypes of each protected attribute value for every class, and reveal an existing subspace that captures the maximum variance of the bias. Second, we mitigate biases by mapping image inputs to label embedding spaces. Each value of the protected attribute has its projection head where classes are embedded through a latent vector representation rather than a common one-hot encoding. Once trained, we further reduce in the feature space the bias effect by removing its direction. Evaluation on biased image datasets, for multi-class, multi-label and binary classifications, shows the effectiveness of tackling both feature and label embedding spaces in improving the fairness of the classifier predictions, while preserving classification performance.



### CamLessMonoDepth: Monocular Depth Estimation with Unknown Camera Parameters
- **Arxiv ID**: http://arxiv.org/abs/2110.14347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14347v1)
- **Published**: 2021-10-27 10:54:15+00:00
- **Updated**: 2021-10-27 10:54:15+00:00
- **Authors**: Sai Shyam Chanduri, Zeeshan Khan Suri, Igor Vozniak, Christian Müller
- **Comment**: Accepted to BMVC 2021
- **Journal**: None
- **Summary**: Perceiving 3D information is of paramount importance in many applications of computer vision. Recent advances in monocular depth estimation have shown that gaining such knowledge from a single camera input is possible by training deep neural networks to predict inverse depth and pose, without the necessity of ground truth data. The majority of such approaches, however, require camera parameters to be fed explicitly during training. As a result, image sequences from wild cannot be used during training. While there exist methods which also predict camera intrinsics, their performance is not on par with novel methods taking camera parameters as input. In this work, we propose a method for implicit estimation of pinhole camera intrinsics along with depth and pose, by learning from monocular image sequences alone. In addition, by utilizing efficient sub-pixel convolutions, we show that high fidelity depth estimates can be obtained. We also embed pixel-wise uncertainty estimation into the framework, to emphasize the possible applicability of this work in practical domain. Finally, we demonstrate the possibility of accurate prediction of depth information without prior knowledge of camera intrinsics, while outperforming the existing state-of-the-art approaches on KITTI benchmark.



### ConAM: Confidence Attention Module for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.14369v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14369v3)
- **Published**: 2021-10-27 12:06:31+00:00
- **Updated**: 2022-03-19 06:57:50+00:00
- **Authors**: Yu Xue, Ziming Yuan, Ferrante Neri
- **Comment**: We need to withdraw this article due to a conflict of interest
- **Journal**: None
- **Summary**: The so-called "attention" is an efficient mechanism to improve the performance of convolutional neural networks. It uses contextual information to recalibrate the input to strengthen the propagation of informative features. However, the majority of the attention mechanisms only consider either local or global contextual information, which is singular to extract features. Moreover, many existing mechanisms directly use the contextual information to recalibrate the input, which unilaterally enhances the propagation of the informative features, but does not suppress the useless ones. This paper proposes a new attention mechanism module based on the correlation between local and global contextual information and we name this correlation as confidence. The novel attention mechanism extracts the local and global contextual information simultaneously, and calculates the confidence between them, then uses this confidence to recalibrate the input pixels. The extraction of local and global contextual information increases the diversity of features. The recalibration with confidence suppresses useless information while enhancing the informative one with fewer parameters. We use CIFAR-10 and CIFAR-100 in our experiments and explore the performance of our method's components by sufficient ablation studies. Finally, we compare our method with a various state-of-the-art convolutional neural networks and the results show that our method completely surpasses these models. We implement ConAM with the Python library, Pytorch, and the code and models will be publicly available.



### Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2110.14373v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.14373v1)
- **Published**: 2021-10-27 12:17:47+00:00
- **Updated**: 2021-10-27 12:17:47+00:00
- **Authors**: Mark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan T. Barron, Hendrik P. A. Lensch
- **Comment**: Project page: https://markboss.me/publication/2021-neural-pil/ Video:
  https://youtu.be/AsdAR5u3vQ8 - Accepted at NeurIPS 2021
- **Journal**: None
- **Summary**: Decomposing a scene into its shape, reflectance and illumination is a fundamental problem in computer vision and graphics. Neural approaches such as NeRF have achieved remarkable success in view synthesis, but do not explicitly perform decomposition and instead operate exclusively on radiance (the product of reflectance and illumination). Extensions to NeRF, such as NeRD, can perform decomposition but struggle to accurately recover detailed illumination, thereby significantly limiting realism. We propose a novel reflectance decomposition network that can estimate shape, BRDF, and per-image illumination given a set of object images captured under varying illumination. Our key technique is a novel illumination integration network called Neural-PIL that replaces a costly illumination integral operation in the rendering with a simple network query. In addition, we also learn deep low-dimensional priors on BRDF and illumination representations using novel smooth manifold auto-encoders. Our decompositions can result in considerably better BRDF and light estimates enabling more accurate novel view-synthesis and relighting compared to prior art. Project page: https://markboss.me/publication/2021-neural-pil/



### Perceptual Score: What Data Modalities Does Your Model Perceive?
- **Arxiv ID**: http://arxiv.org/abs/2110.14375v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.14375v1)
- **Published**: 2021-10-27 12:19:56+00:00
- **Updated**: 2021-10-27 12:19:56+00:00
- **Authors**: Itai Gat, Idan Schwartz, Alexander Schwing
- **Comment**: Accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: Machine learning advances in the last decade have relied significantly on large-scale datasets that continue to grow in size. Increasingly, those datasets also contain different data modalities. However, large multi-modal datasets are hard to annotate, and annotations may contain biases that we are often unaware of. Deep-net-based classifiers, in turn, are prone to exploit those biases and to find shortcuts. To study and quantify this concern, we introduce the perceptual score, a metric that assesses the degree to which a model relies on the different subsets of the input features, i.e., modalities. Using the perceptual score, we find a surprisingly consistent trend across four popular datasets: recent, more accurate state-of-the-art multi-modal models for visual question-answering or visual dialog tend to perceive the visual data less than their predecessors. This trend is concerning as answers are hence increasingly inferred from textual cues only. Using the perceptual score also helps to analyze model biases by decomposing the score into data subset contributions. We hope to spur a discussion on the perceptiveness of multi-modal models and also hope to encourage the community working on multi-modal classifiers to start quantifying perceptiveness via the proposed perceptual score.



### Temporal-attentive Covariance Pooling Networks for Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.14381v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.14381v3)
- **Published**: 2021-10-27 12:31:29+00:00
- **Updated**: 2021-11-06 08:54:47+00:00
- **Authors**: Zilin Gao, Qilong Wang, Bingbing Zhang, Qinghua Hu, Peihua Li
- **Comment**: Accepted to NeurIPS 2021; Project page:
  https://github.com/ZilinGao/Temporal-attentive-Covariance-Pooling-Networks-for-Video-Recognition
- **Journal**: None
- **Summary**: For video recognition task, a global representation summarizing the whole contents of the video snippets plays an important role for the final performance. However, existing video architectures usually generate it by using a simple, global average pooling (GAP) method, which has limited ability to capture complex dynamics of videos. For image recognition task, there exist evidences showing that covariance pooling has stronger representation ability than GAP. Unfortunately, such plain covariance pooling used in image recognition is an orderless representative, which cannot model spatio-temporal structure inherent in videos. Therefore, this paper proposes a Temporal-attentive Covariance Pooling(TCP), inserted at the end of deep architectures, to produce powerful video representations. Specifically, our TCP first develops a temporal attention module to adaptively calibrate spatio-temporal features for the succeeding covariance pooling, approximatively producing attentive covariance representations. Then, a temporal covariance pooling performs temporal pooling of the attentive covariance representations to characterize both intra-frame correlations and inter-frame cross-correlations of the calibrated features. As such, the proposed TCP can capture complex temporal dynamics. Finally, a fast matrix power normalization is introduced to exploit geometry of covariance representations. Note that our TCP is model-agnostic and can be flexibly integrated into any video architectures, resulting in TCPNet for effective video recognition. The extensive experiments on six benchmarks (e.g., Kinetics, Something-Something V1 and Charades) using various video architectures show our TCPNet is clearly superior to its counterparts, while having strong generalization ability. The source code is publicly available.



### Traffic Forecasting on Traffic Moving Snippets
- **Arxiv ID**: http://arxiv.org/abs/2110.14383v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14383v1)
- **Published**: 2021-10-27 12:36:58+00:00
- **Updated**: 2021-10-27 12:36:58+00:00
- **Authors**: Nina Wiedemann, Martin Raubal
- **Comment**: None
- **Journal**: None
- **Summary**: Advances in traffic forecasting technology can greatly impact urban mobility. In the traffic4cast competition, the task of short-term traffic prediction is tackled in unprecedented detail, with traffic volume and speed information available at 5 minute intervals and high spatial resolution. To improve generalization to unknown cities, as required in the 2021 extended challenge, we propose to predict small quadratic city sections, rather than processing a full-city-raster at once. At test time, breaking down the test data into spatially-cropped overlapping snippets improves stability and robustness of the final predictions, since multiple patches covering one cell can be processed independently. With the performance on the traffic4cast test data and further experiments on a validation set it is shown that patch-wise prediction indeed improves accuracy. Further advantages can be gained with a Unet++ architecture and with an increasing number of patches per sample processed at test time. We conclude that our snippet-based method, combined with other successful network architectures proposed in the competition, can leverage performance, in particular on unseen cities. All source code is available at https://github.com/NinaWie/NeurIPS2021-traffic4cast.



### TaylorSwiftNet: Taylor Driven Temporal Modeling for Swift Future Frame Prediction
- **Arxiv ID**: http://arxiv.org/abs/2110.14392v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14392v2)
- **Published**: 2021-10-27 12:46:17+00:00
- **Updated**: 2022-10-12 17:59:00+00:00
- **Authors**: Saber Pourheydari, Emad Bahrami, Mohsen Fayyaz, Gianpiero Francesca, Mehdi Noroozi, Juergen Gall
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: While recurrent neural networks (RNNs) demonstrate outstanding capabilities for future video frame prediction, they model dynamics in a discrete time space, i.e., they predict the frames sequentially with a fixed temporal step. RNNs are therefore prone to accumulate the error as the number of future frames increases. In contrast, partial differential equations (PDEs) model physical phenomena like dynamics in a continuous time space. However, the estimated PDE for frame forecasting needs to be numerically solved, which is done by discretization of the PDE and diminishes most of the advantages compared to discrete models. In this work, we, therefore, propose to approximate the motion in a video by a continuous function using the Taylor series. To this end, we introduce TaylorSwiftNet, a novel convolutional neural network that learns to estimate the higher order terms of the Taylor series for a given input video. TaylorSwiftNet can swiftly predict future frames in parallel and it allows to change the temporal resolution of the forecast frames on-the-fly. The experimental results on various datasets demonstrate the superiority of our model.



### Separating Content and Style for Unsupervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2110.14404v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14404v1)
- **Published**: 2021-10-27 12:56:50+00:00
- **Updated**: 2021-10-27 12:56:50+00:00
- **Authors**: Yunfei Liu, Haofei Wang, Yang Yue, Feng Lu
- **Comment**: Accepted by BMVC2021
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation aims to learn the mapping between two visual domains with unpaired samples. Existing works focus on disentangling domain-invariant content code and domain-specific style code individually for multimodal purposes. However, less attention has been paid to interpreting and manipulating the translated image. In this paper, we propose to separate the content code and style code simultaneously in a unified framework. Based on the correlation between the latent features and the high-level domain-invariant tasks, the proposed framework demonstrates superior performance in multimodal translation, interpretability and manipulation of the translated image. Experimental results show that the proposed approach outperforms the existing unsupervised image translation methods in terms of visual quality and diversity.



### Mosaicking to Distill: Knowledge Distillation from Out-of-Domain Data
- **Arxiv ID**: http://arxiv.org/abs/2110.15094v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.15094v1)
- **Published**: 2021-10-27 13:01:10+00:00
- **Updated**: 2021-10-27 13:01:10+00:00
- **Authors**: Gongfan Fang, Yifan Bao, Jie Song, Xinchao Wang, Donglin Xie, Chengchao Shen, Mingli Song
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge distillation~(KD) aims to craft a compact student model that imitates the behavior of a pre-trained teacher in a target domain. Prior KD approaches, despite their gratifying results, have largely relied on the premise that \emph{in-domain} data is available to carry out the knowledge transfer. Such an assumption, unfortunately, in many cases violates the practical setting, since the original training data or even the data domain is often unreachable due to privacy or copyright reasons. In this paper, we attempt to tackle an ambitious task, termed as \emph{out-of-domain} knowledge distillation~(OOD-KD), which allows us to conduct KD using only OOD data that can be readily obtained at a very low cost. Admittedly, OOD-KD is by nature a highly challenging task due to the agnostic domain gap. To this end, we introduce a handy yet surprisingly efficacious approach, dubbed as~\textit{MosaicKD}. The key insight behind MosaicKD lies in that, samples from various domains share common local patterns, even though their global semantic may vary significantly; these shared local patterns, in turn, can be re-assembled analogous to mosaic tiling, to approximate the in-domain data and to further alleviating the domain discrepancy. In MosaicKD, this is achieved through a four-player min-max game, in which a generator, a discriminator, a student network, are collectively trained in an adversarial manner, partially under the guidance of a pre-trained teacher. We validate MosaicKD over {classification and semantic segmentation tasks} across various benchmarks, and demonstrate that it yields results much superior to the state-of-the-art counterparts on OOD data. Our code is available at \url{https://github.com/zju-vipa/MosaicKD}.



### Localized Super Resolution for Foreground Images using U-Net and MR-CNN
- **Arxiv ID**: http://arxiv.org/abs/2110.14413v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14413v1)
- **Published**: 2021-10-27 13:18:10+00:00
- **Updated**: 2021-10-27 13:18:10+00:00
- **Authors**: Umashankar Kumaravelan, Nivedita M
- **Comment**: Submitted for peer review and acceptance at ICRTAC 2021
- **Journal**: None
- **Summary**: Images play a vital role in understanding data through visual representation. It gives a clear representation of the object in context. But if this image is not clear it might not be of much use. Thus, the topic of Image Super Resolution arose and many researchers have been working towards applying Computer Vision and Deep Learning Techniques to increase the quality of images. One of the applications of Super Resolution is to increase the quality of Portrait Images. Portrait Images are images which mainly focus on capturing the essence of the main object in the frame, where the object in context is highlighted whereas the background is occluded. When performing Super Resolution the model tries to increase the overall resolution of the image. But in portrait images the foreground resolution is more important than that of the background. In this paper, the performance of a Convolutional Neural Network (CNN) architecture known as U-Net for Super Resolution combined with Mask Region Based CNN (MR-CNN) for foreground super resolution is analysed. This analysis is carried out based on Localized Super Resolution i.e. We pass the LR Images to a pre-trained Image Segmentation model (MR-CNN) and perform super resolution inference on the foreground or Segmented Images and compute the Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR) metrics for comparisons.



### Adversarial Neuron Pruning Purifies Backdoored Deep Models
- **Arxiv ID**: http://arxiv.org/abs/2110.14430v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14430v1)
- **Published**: 2021-10-27 13:41:53+00:00
- **Updated**: 2021-10-27 13:41:53+00:00
- **Authors**: Dongxian Wu, Yisen Wang
- **Comment**: To appear in NeurIPS 2021
- **Journal**: None
- **Summary**: As deep neural networks (DNNs) are growing larger, their requirements for computational resources become huge, which makes outsourcing training more popular. Training in a third-party platform, however, may introduce potential risks that a malicious trainer will return backdoored DNNs, which behave normally on clean samples but output targeted misclassifications whenever a trigger appears at the test time. Without any knowledge of the trigger, it is difficult to distinguish or recover benign DNNs from backdoored ones. In this paper, we first identify an unexpected sensitivity of backdoored DNNs, that is, they are much easier to collapse and tend to predict the target label on clean samples when their neurons are adversarially perturbed. Based on these observations, we propose a novel model repairing method, termed Adversarial Neuron Pruning (ANP), which prunes some sensitive neurons to purify the injected backdoor. Experiments show, even with only an extremely small amount of clean data (e.g., 1%), ANP effectively removes the injected backdoor without causing obvious performance degradation.



### Iterative Teaching by Label Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2110.14432v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14432v5)
- **Published**: 2021-10-27 13:45:29+00:00
- **Updated**: 2023-01-26 16:18:30+00:00
- **Authors**: Weiyang Liu, Zhen Liu, Hanchen Wang, Liam Paull, Bernhard Schölkopf, Adrian Weller
- **Comment**: NeurIPS 2021 Spotlight (v5: 28 pages, 20 figures, fixed typos in v4)
- **Journal**: None
- **Summary**: In this paper, we consider the problem of iterative machine teaching, where a teacher provides examples sequentially based on the current iterative learner. In contrast to previous methods that have to scan over the entire pool and select teaching examples from it in each iteration, we propose a label synthesis teaching framework where the teacher randomly selects input teaching examples (e.g., images) and then synthesizes suitable outputs (e.g., labels) for them. We show that this framework can avoid costly example selection while still provably achieving exponential teachability. We propose multiple novel teaching algorithms in this framework. Finally, we empirically demonstrate the value of our framework.



### Revisiting Discriminator in GAN Compression: A Generator-discriminator Cooperative Compression Scheme
- **Arxiv ID**: http://arxiv.org/abs/2110.14439v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14439v2)
- **Published**: 2021-10-27 13:54:55+00:00
- **Updated**: 2021-11-09 07:34:21+00:00
- **Authors**: Shaojie Li, Jie Wu, Xuefeng Xiao, Fei Chao, Xudong Mao, Rongrong Ji
- **Comment**: Accepted by NeurIPS2021 (The 35th Conference on Neural Information
  Processing Systems)
- **Journal**: None
- **Summary**: Recently, a series of algorithms have been explored for GAN compression, which aims to reduce tremendous computational overhead and memory usages when deploying GANs on resource-constrained edge devices. However, most of the existing GAN compression work only focuses on how to compress the generator, while fails to take the discriminator into account. In this work, we revisit the role of discriminator in GAN compression and design a novel generator-discriminator cooperative compression scheme for GAN compression, termed GCC. Within GCC, a selective activation discriminator automatically selects and activates convolutional channels according to a local capacity constraint and a global coordination constraint, which help maintain the Nash equilibrium with the lightweight generator during the adversarial training and avoid mode collapse. The original generator and discriminator are also optimized from scratch, to play as a teacher model to progressively refine the pruned generator and the selective activation discriminator. A novel online collaborative distillation scheme is designed to take full advantage of the intermediate feature of the teacher generator and discriminator to further boost the performance of the lightweight generator. Extensive experiments on various GAN-based generation tasks demonstrate the effectiveness and generalization of GCC. Among them, GCC contributes to reducing 80% computational costs while maintains comparable performance in image translation tasks. Our code and models are available at https://github.com/SJLeo/GCC.



### CBIR using Pre-Trained Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.14455v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.14455v1)
- **Published**: 2021-10-27 14:19:48+00:00
- **Updated**: 2021-10-27 14:19:48+00:00
- **Authors**: Agnel Lazar Alappat, Prajwal Nakhate, Sagar Suman, Ambarish Chandurkar, Varad Pimpalkhute, Tapan Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Much of the recent research work in image retrieval, has been focused around using Neural Networks as the core component. Many of the papers in other domain have shown that training multiple models, and then combining their outcomes, provide good results. This is since, a single Neural Network model, may not extract sufficient information from the input. In this paper, we aim to follow a different approach. Instead of the using a single model, we use a pretrained Inception V3 model, and extract activation of its last fully connected layer, which forms a low dimensional representation of the image. This feature matrix, is then divided into branches and separate feature extraction is done for each branch, to obtain multiple features flattened into a vector. Such individual vectors are then combined, to get a single combined feature. We make use of CUB200-2011 Dataset, which comprises of 200 birds classes to train the model on. We achieved a training accuracy of 99.46% and validation accuracy of 84.56% for the same. On further use of 3 branched global descriptors, we improve the validation accuracy to 88.89%. For this, we made use of MS-RMAC feature extraction method.



### Hand gesture detection in tests performed by older adults
- **Arxiv ID**: http://arxiv.org/abs/2110.14461v2
- **DOI**: 10.1007/s00521-022-08090-8
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.14461v2)
- **Published**: 2021-10-27 14:29:01+00:00
- **Updated**: 2021-10-29 00:48:51+00:00
- **Authors**: Guan Huang, Son N. Tran, Quan Bai, Jane Alty
- **Comment**: None
- **Journal**: Neural Comput & Applic (2022)
- **Summary**: Our team are developing a new online test that analyses hand movement features associated with ageing that can be completed remotely from the research centre. To obtain hand movement features, participants will be asked to perform a variety of hand gestures using their own computer cameras. However, it is challenging to collect high quality hand movement video data, especially for older participants, many of whom have no IT background. During the data collection process, one of the key steps is to detect whether the participants are following the test instructions correctly and also to detect similar gestures from different devices. Furthermore, we need this process to be automated and accurate as we expect many thousands of participants to complete the test. We have implemented a hand gesture detector to detect the gestures in the hand movement tests and our detection mAP is 0.782 which is better than the state-of-the-art. In this research, we have processed 20,000 images collected from hand movement tests and labelled 6,450 images to detect different hand gestures in the hand movement tests. This paper has the following three contributions. Firstly, we compared and analysed the performance of different network structures for hand gesture detection. Secondly, we have made many attempts to improve the accuracy of the model and have succeeded in improving the classification accuracy for similar gestures by implementing attention layers. Thirdly, we have created two datasets and included 20 percent of blurred images in the dataset to investigate how different network structures were impacted by noisy data, our experiments have also shown our network has better performance on the noisy dataset.



### An Arbitrary Scale Super-Resolution Approach for 3D MR Images via Implicit Neural Representation
- **Arxiv ID**: http://arxiv.org/abs/2110.14476v3
- **DOI**: 10.1109/JBHI.2022.3223106
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14476v3)
- **Published**: 2021-10-27 14:48:54+00:00
- **Updated**: 2022-11-30 03:52:44+00:00
- **Authors**: Qing Wu, Yuwei Li, Yawen Sun, Yan Zhou, Hongjiang Wei, Jingyi Yu, Yuyao Zhang
- **Comment**: 12 pagee, acceted by IEEE J-BHI
- **Journal**: IEEE Journal of Biomedical and Health Informatics (2022)
- **Summary**: High Resolution (HR) medical images provide rich anatomical structure details to facilitate early and accurate diagnosis. In MRI, restricted by hardware capacity, scan time, and patient cooperation ability, isotropic 3D HR image acquisition typically requests long scan time and, results in small spatial coverage and low SNR. Recent studies showed that, with deep convolutional neural networks, isotropic HR MR images could be recovered from low-resolution (LR) input via single image super-resolution (SISR) algorithms. However, most existing SISR methods tend to approach a scale-specific projection between LR and HR images, thus these methods can only deal with a fixed up-sampling rate. For achieving different up-sampling rates, multiple SR networks have to be built up respectively, which is very time-consuming and resource-intensive. In this paper, we propose ArSSR, an Arbitrary Scale Super-Resolution approach for recovering 3D HR MR images. In the ArSSR model, the reconstruction of HR images with different up-scaling rates is defined as learning a continuous implicit voxel function from the observed LR images. Then the SR task is converted to represent the implicit voxel function via deep neural networks from a set of paired HR-LR training examples. The ArSSR model consists of an encoder network and a decoder network. Specifically, the convolutional encoder network is to extract feature maps from the LR input images and the fully-connected decoder network is to approximate the implicit voxel function. Due to the continuity of the learned function, a single ArSSR model can achieve arbitrary up-sampling rate reconstruction of HR images from any input LR image after training. Experimental results on three datasets show that the ArSSR model can achieve state-of-the-art SR performance for 3D HR MR image reconstruction while using a single trained model to achieve arbitrary up-sampling scales.



### PL-Net: Progressive Learning Network for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.14484v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.14484v2)
- **Published**: 2021-10-27 14:57:05+00:00
- **Updated**: 2022-08-29 13:20:06+00:00
- **Authors**: Junlong Cheng, Chengrui Gao, Hongchun Lu, Zhangqiang Ming, Yong Yang, Min Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, segmentation methods based on deep convolutional neural networks (CNNs) have made state-of-the-art achievements for many medical analysis tasks. However, most of these approaches improve performance by optimizing the structure or adding new functional modules of the U-Net, which ignoring the complementation and fusion of the coarse-grained and fine-grained semantic information. To solve the above problems, we propose a medical image segmentation framework called progressive learning network (PL-Net), which includes internal progressive learning (IPL) and external progressive learning (EPL). PL-Net has the following advantages: (1) IPL divides feature extraction into two "steps", which can mix different size receptive fields and capture semantic information from coarse to fine granularity without introducing additional parameters; (2) EPL divides the training process into two "stages" to optimize parameters, and realizes the fusion of coarse-grained information in the previous stage and fine-grained information in the latter stage. We evaluate our method in different medical image analysis tasks, and the results show that the segmentation performance of PL-Net is better than the state-of-the-art methods of U-Net and its variants.



### Training Lightweight CNNs for Human-Nanodrone Proximity Interaction from Small Datasets using Background Randomization
- **Arxiv ID**: http://arxiv.org/abs/2110.14491v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.14491v1)
- **Published**: 2021-10-27 15:07:31+00:00
- **Updated**: 2021-10-27 15:07:31+00:00
- **Authors**: Marco Ferri, Dario Mantegazza, Elia Cereda, Nicky Zimmerman, Luca M. Gambardella, Daniele Palossi, Jérôme Guzzi, Alessandro Giusti
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the task of visually estimating the pose of a human from images acquired by a nearby nano-drone; in this context, we propose a data augmentation approach based on synthetic background substitution to learn a lightweight CNN model from a small real-world training set. Experimental results on data from two different labs proves that the approach improves generalization to unseen environments.



### GenURL: A General Framework for Unsupervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.14553v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14553v3)
- **Published**: 2021-10-27 16:24:39+00:00
- **Updated**: 2022-10-17 17:23:31+00:00
- **Authors**: Siyuan Li, Zicheng Liu, Zelin Zang, Di Wu, Zhiyuan Chen, Stan Z. Li
- **Comment**: Tech report (revision) with 12 pages and 14 figures
- **Journal**: None
- **Summary**: Unsupervised representation learning (URL) that learns compact embeddings of high-dimensional data without supervision has achieved remarkable progress recently. Although the ultimate goal of URLs is similar across various scenarios, the related algorithms differ widely in different tasks because they were separately designed according to a specific URL task or data. For example, dimension reduction methods, t-SNE, and UMAP, optimize pair-wise data relationships by preserving the global geometric structure, while self-supervised learning, SimCLR, and BYOL, focus on mining the local statistics of instances under specific augmentations. From a general perspective, we summarize and propose a unified similarity-based URL framework, GenURL, which can adapt to various URL tasks smoothly and efficiently. Based on the manifold assumption, we regard URL tasks as different implicit constraints on the data geometric structure or content that help to seek an optimal low-dimensional representation for the high-dimensional data. Therefore, our method has two key steps to learning task-agnostic representation in URL: (1) data structural modeling and (2) low-dimensional transformation. Specifically, (1) provides a simple yet effective graph-based submodule to model data structures adaptively with predefined or constructed graphs; and based on data-specific pretext tasks, (2) learns compact low-dimensional embeddings. Moreover, (1) and (2) are successfully connected and benefit mutually through our novel objective function. Our comprehensive experiments demonstrate that GenURL achieves consistent state-of-the-art performance in self-supervised visual representation learning, unsupervised knowledge distillation, graph embeddings, and dimension reduction.



### A Geometric Perspective towards Neural Calibration via Sensitivity Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2110.14577v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.14577v3)
- **Published**: 2021-10-27 16:46:41+00:00
- **Updated**: 2021-11-21 20:15:38+00:00
- **Authors**: Junjiao Tian, Dylan Yung, Yen-Chang Hsu, Zsolt Kira
- **Comment**: Accepted at NeurIPS 2021 as a spotlight paper
- **Journal**: None
- **Summary**: It is well known that vision classification models suffer from poor calibration in the face of data distribution shifts. In this paper, we take a geometric approach to this problem. We propose Geometric Sensitivity Decomposition (GSD) which decomposes the norm of a sample feature embedding and the angular similarity to a target classifier into an instance-dependent and an instance-independent component. The instance-dependent component captures the sensitive information about changes in the input while the instance-independent component represents the insensitive information serving solely to minimize the loss on the training dataset. Inspired by the decomposition, we analytically derive a simple extension to current softmax-linear models, which learns to disentangle the two components during training. On several common vision models, the disentangled model outperforms other calibration methods on standard calibration metrics in the face of out-of-distribution (OOD) data and corruption with significantly less complexity. Specifically, we surpass the current state of the art by 30.8% relative improvement on corrupted CIFAR100 in Expected Calibration Error. Code available at https://github.com/GT-RIPL/Geometric-Sensitivity-Decomposition.git.



### Boundary Guided Context Aggregation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.14587v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14587v1)
- **Published**: 2021-10-27 17:04:38+00:00
- **Updated**: 2021-10-27 17:04:38+00:00
- **Authors**: Haoxiang Ma, Hongyu Yang, Di Huang
- **Comment**: Accepted at BMVC'2021
- **Journal**: None
- **Summary**: The recent studies on semantic segmentation are starting to notice the significance of the boundary information, where most approaches see boundaries as the supplement of semantic details. However, simply combing boundaries and the mainstream features cannot ensure a holistic improvement of semantics modeling. In contrast to the previous studies, we exploit boundary as a significant guidance for context aggregation to promote the overall semantic understanding of an image. To this end, we propose a Boundary guided Context Aggregation Network (BCANet), where a Multi-Scale Boundary extractor (MSB) borrowing the backbone features at multiple scales is specifically designed for accurate boundary detection. Based on which, a Boundary guided Context Aggregation module (BCA) improved from Non-local network is further proposed to capture long-range dependencies between the pixels in the boundary regions and the ones inside the objects. By aggregating the context information along the boundaries, the inner pixels of the same category achieve mutual gains and therefore the intra-class consistency is enhanced. We conduct extensive experiments on the Cityscapes and ADE20K databases, and comparable results are achieved with the state-of-the-art methods, clearly demonstrating the effectiveness of the proposed one.



### TMBuD: A dataset for urban scene building detection
- **Arxiv ID**: http://arxiv.org/abs/2110.14590v1
- **DOI**: 10.1007/978-3-030-88304-1_20
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.14590v1)
- **Published**: 2021-10-27 17:08:11+00:00
- **Updated**: 2021-10-27 17:08:11+00:00
- **Authors**: Orhei Ciprian, Vert Silviu, Mocofan Muguras, Vasiu Radu
- **Comment**: None
- **Journal**: None
- **Summary**: Building recognition and 3D reconstruction of human made structures in urban scenarios has become an interesting and actual topic in the image processing domain. For this research topic the Computer Vision and Augmented Reality areas intersect for creating a better understanding of the urban scenario for various topics. In this paper we aim to introduce a dataset solution, the TMBuD, that is better fitted for image processing on human made structures for urban scene scenarios. The proposed dataset will allow proper evaluation of salient edges and semantic segmentation of images focusing on the street view perspective of buildings. The images that form our dataset offer various street view perspectives of buildings from urban scenarios, which allows for evaluating complex algorithms. The dataset features 160 images of buildings from Timisoara, Romania, with a resolution of 768 x 1024 pixels each.



### TA-Net: Topology-Aware Network for Gland Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.14593v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14593v1)
- **Published**: 2021-10-27 17:10:58+00:00
- **Updated**: 2021-10-27 17:10:58+00:00
- **Authors**: Haotian Wang, Min Xian, Aleksandar Vakanski
- **Comment**: None
- **Journal**: None
- **Summary**: Gland segmentation is a critical step to quantitatively assess the morphology of glands in histopathology image analysis. However, it is challenging to separate densely clustered glands accurately. Existing deep learning-based approaches attempted to use contour-based techniques to alleviate this issue but only achieved limited success. To address this challenge, we propose a novel topology-aware network (TA-Net) to accurately separate densely clustered and severely deformed glands. The proposed TA-Net has a multitask learning architecture and enhances the generalization of gland segmentation by learning shared representation from two tasks: instance segmentation and gland topology estimation. The proposed topology loss computes gland topology using gland skeletons and markers. It drives the network to generate segmentation results that comply with the true gland topology. We validate the proposed approach on the GlaS and CRAG datasets using three quantitative metrics, F1-score, object-level Dice coefficient, and object-level Hausdorff distance. Extensive experiments demonstrate that TA-Net achieves state-of-the-art performance on the two datasets. TA-Net outperforms other approaches in the presence of densely clustered glands.



### International Workshop on Continual Semi-Supervised Learning: Introduction, Benchmarks and Baselines
- **Arxiv ID**: http://arxiv.org/abs/2110.14613v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.14613v1)
- **Published**: 2021-10-27 17:34:40+00:00
- **Updated**: 2021-10-27 17:34:40+00:00
- **Authors**: Ajmal Shahbaz, Salman Khan, Mohammad Asiful Hossain, Vincenzo Lomonaco, Kevin Cannons, Zhan Xu, Fabio Cuzzolin
- **Comment**: None
- **Journal**: None
- **Summary**: The aim of this paper is to formalize a new continual semi-supervised learning (CSSL) paradigm, proposed to the attention of the machine learning community via the IJCAI 2021 International Workshop on Continual Semi-Supervised Learning (CSSL-IJCAI), with the aim of raising field awareness about this problem and mobilizing its effort in this direction. After a formal definition of continual semi-supervised learning and the appropriate training and testing protocols, the paper introduces two new benchmarks specifically designed to assess CSSL on two important computer vision tasks: activity recognition and crowd counting. We describe the Continual Activity Recognition (CAR) and Continual Crowd Counting (CCC) challenges built upon those benchmarks, the baseline models proposed for the challenges, and describe a simple CSSL baseline which consists in applying batch self-training in temporal sessions, for a limited number of rounds. The results show that learning from unlabelled data streams is extremely challenging, and stimulate the search for methods that can encode the dynamics of the data stream.



### Sensing Anomalies as Potential Hazards: Datasets and Benchmarks
- **Arxiv ID**: http://arxiv.org/abs/2110.14706v2
- **DOI**: 10.1007/978-3-031-15908-4_17
- **Categories**: **cs.RO**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.14706v2)
- **Published**: 2021-10-27 18:47:06+00:00
- **Updated**: 2022-09-20 15:21:21+00:00
- **Authors**: Dario Mantegazza, Carlos Redondo, Fran Espada, Luca M. Gambardella, Alessandro Giusti, Jérôme Guzzi
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of detecting, in the visual sensing data stream of an autonomous mobile robot, semantic patterns that are unusual (i.e., anomalous) with respect to the robot's previous experience in similar environments. These anomalies might indicate unforeseen hazards and, in scenarios where failure is costly, can be used to trigger an avoidance behavior. We contribute three novel image-based datasets acquired in robot exploration scenarios, comprising a total of more than 200k labeled frames, spanning various types of anomalies. On these datasets, we study the performance of an anomaly detection approach based on autoencoders operating at different scales.



### Sharp-GAN: Sharpness Loss Regularized GAN for Histopathology Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2110.14709v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14709v1)
- **Published**: 2021-10-27 18:54:25+00:00
- **Updated**: 2021-10-27 18:54:25+00:00
- **Authors**: Sujata Butte, Haotian Wang, Min Xian, Aleksandar Vakanski
- **Comment**: None
- **Journal**: None
- **Summary**: Existing deep learning-based approaches for histopathology image analysis require large annotated training sets to achieve good performance; but annotating histopathology images is slow and resource-intensive. Conditional generative adversarial networks have been applied to generate synthetic histopathology images to alleviate this issue, but current approaches fail to generate clear contours for overlapped and touching nuclei. In this study, We propose a sharpness loss regularized generative adversarial network to synthesize realistic histopathology images. The proposed network uses normalized nucleus distance map rather than the binary mask to encode nuclei contour information. The proposed sharpness loss enhances the contrast of nuclei contour pixels. The proposed method is evaluated using four image quality metrics and segmentation results on two public datasets. Both quantitative and qualitative results demonstrate that the proposed approach can generate realistic histopathology images with clear nuclei contours.



### A Survey of Self-Supervised and Few-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.14711v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.14711v3)
- **Published**: 2021-10-27 18:55:47+00:00
- **Updated**: 2022-08-23 08:38:52+00:00
- **Authors**: Gabriel Huang, Issam Laradji, David Vazquez, Simon Lacoste-Julien, Pau Rodriguez
- **Comment**: To appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence. Awesome Few-Shot Object Detection (Leaderboard) at
  https://github.com/gabrielhuang/awesome-few-shot-object-detection
- **Journal**: None
- **Summary**: Labeling data is often expensive and time-consuming, especially for tasks such as object detection and instance segmentation, which require dense labeling of the image. While few-shot object detection is about training a model on novel (unseen) object classes with little data, it still requires prior training on many labeled examples of base (seen) classes. On the other hand, self-supervised methods aim at learning representations from unlabeled data which transfer well to downstream tasks such as object detection. Combining few-shot and self-supervised object detection is a promising research direction. In this survey, we review and characterize the most recent approaches on few-shot and self-supervised object detection. Then, we give our main takeaways and discuss future research directions. Project page at https://gabrielhuang.github.io/fsod-survey/



### Lung Cancer Lesion Detection in Histopathology Images Using Graph-Based Sparse PCA Network
- **Arxiv ID**: http://arxiv.org/abs/2110.14728v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14728v2)
- **Published**: 2021-10-27 19:28:36+00:00
- **Updated**: 2022-02-16 01:58:38+00:00
- **Authors**: Sundaresh Ram, Wenfei Tang, Alexander J. Bell, Cara Spencer, Alexander Buschhaus, Charles R. Hatt, Marina Pasca diMagliano, Jeffrey J. Rodriguez, Stefanie Galban, Craig J. Galban
- **Comment**: 10 pages, 9 figures, 3 tables
- **Journal**: None
- **Summary**: Early detection of lung cancer is critical for improvement of patient survival. To address the clinical need for efficacious treatments, genetically engineered mouse models (GEMM) have become integral in identifying and evaluating the molecular underpinnings of this complex disease that may be exploited as therapeutic targets. Assessment of GEMM tumor burden on histopathological sections performed by manual inspection is both time consuming and prone to subjective bias. Therefore, an interplay of needs and challenges exists for computer-aided diagnostic tools, for accurate and efficient analysis of these histopathology images. In this paper, we propose a simple machine learning approach called the graph-based sparse principal component analysis (GS-PCA) network, for automated detection of cancerous lesions on histological lung slides stained by hematoxylin and eosin (H&E). Our method comprises four steps: 1) cascaded graph-based sparse PCA, 2) PCA binary hashing, 3) block-wise histograms, and 4) support vector machine (SVM) classification. In our proposed architecture, graph-based sparse PCA is employed to learn the filter banks of the multiple stages of a convolutional network. This is followed by PCA hashing and block histograms for indexing and pooling. The meaningful features extracted from this GS-PCA are then fed to an SVM classifier. We evaluate the performance of the proposed algorithm on H&E slides obtained from an inducible K-rasG12D lung cancer mouse model using precision/recall rates, F-score, Tanimoto coefficient, and area under the curve (AUC) of the receiver operator characteristic (ROC) and show that our algorithm is efficient and provides improved detection accuracy compared to existing algorithms.



### Vision Transformer for Classification of Breast Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2110.14731v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14731v2)
- **Published**: 2021-10-27 19:33:23+00:00
- **Updated**: 2022-03-16 22:57:35+00:00
- **Authors**: Behnaz Gheflati, Hassan Rivaz
- **Comment**: 5 pages, 2 figures, Under review in EMBC
- **Journal**: None
- **Summary**: Medical ultrasound (US) imaging has become a prominent modality for breast cancer imaging due to its ease-of-use, low-cost and safety. In the past decade, convolutional neural networks (CNNs) have emerged as the method of choice in vision applications and have shown excellent potential in automatic classification of US images. Despite their success, their restricted local receptive field limits their ability to learn global context information. Recently, Vision Transformer (ViT) designs that are based on self-attention between image patches have shown great potential to be an alternative to CNNs. In this study, for the first time, we utilize ViT to classify breast US images using different augmentation strategies. The results are provided as classification accuracy and Area Under the Curve (AUC) metrics, and the performance is compared with the state-of-the-art CNNs. The results indicate that the ViT models have comparable efficiency with or even better than the CNNs in classification of US breast images.



### Algorithmic encoding of protected characteristics in image-based models for disease detection
- **Arxiv ID**: http://arxiv.org/abs/2110.14755v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14755v4)
- **Published**: 2021-10-27 20:30:57+00:00
- **Updated**: 2022-07-21 15:33:21+00:00
- **Authors**: Ben Glocker, Charles Jones, Melanie Bernhardt, Stefan Winzeck
- **Comment**: Code available on https://github.com/biomedia-mira/chexploration
- **Journal**: None
- **Summary**: It has been rightfully emphasized that the use of AI for clinical decision making could amplify health disparities. An algorithm may encode protected characteristics, and then use this information for making predictions due to undesirable correlations in the (historical) training data. It remains unclear how we can establish whether such information is actually used. Besides the scarcity of data from underserved populations, very little is known about how dataset biases manifest in predictive models and how this may result in disparate performance. This article aims to shed some light on these issues by exploring new methodology for subgroup analysis in image-based disease detection models. We utilize two publicly available chest X-ray datasets, CheXpert and MIMIC-CXR, to study performance disparities across race and biological sex in deep learning models. We explore test set resampling, transfer learning, multitask learning, and model inspection to assess the relationship between the encoding of protected characteristics and disease detection performance across subgroups. We confirm subgroup disparities in terms of shifted true and false positive rates which are partially removed after correcting for population and prevalence shifts in the test sets. We further find a previously used transfer learning method to be insufficient for establishing whether specific patient information is used for making predictions. The proposed combination of test-set resampling, multitask learning, and model inspection reveals valuable new insights about the way protected characteristics are encoded in the feature representations of deep neural networks.



### Regularized Frank-Wolfe for Dense CRFs: Generalizing Mean Field and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2110.14759v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.14759v2)
- **Published**: 2021-10-27 20:44:47+00:00
- **Updated**: 2022-09-07 22:51:35+00:00
- **Authors**: Đ. Khuê Lê-Huu, Karteek Alahari
- **Comment**: NeurIPS 2021. This version fixed some minor typos (constant factor 2
  removed from bottom-right cell of Theorem 1's table, and from last row of
  Table 5)
- **Journal**: None
- **Summary**: We introduce regularized Frank-Wolfe, a general and effective algorithm for inference and learning of dense conditional random fields (CRFs). The algorithm optimizes a nonconvex continuous relaxation of the CRF inference problem using vanilla Frank-Wolfe with approximate updates, which are equivalent to minimizing a regularized energy function. Our proposed method is a generalization of existing algorithms such as mean field or concave-convex procedure. This perspective not only offers a unified analysis of these algorithms, but also allows an easy way of exploring different variants that potentially yield better performance. We illustrate this in our empirical results on standard semantic segmentation datasets, where several instantiations of our regularized Frank-Wolfe outperform mean field inference, both as a standalone component and as an end-to-end trainable layer in a neural network. We also show that dense CRFs, coupled with our new algorithms, produce significant improvements over strong CNN baselines.



### Detecting Dementia from Speech and Transcripts using Transformers
- **Arxiv ID**: http://arxiv.org/abs/2110.14769v3
- **DOI**: 10.1016/j.csl.2023.101485
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14769v3)
- **Published**: 2021-10-27 21:00:01+00:00
- **Updated**: 2023-01-17 10:57:43+00:00
- **Authors**: Loukas Ilias, Dimitris Askounis, John Psarras
- **Comment**: Computer Speech & Language (Accepted)
- **Journal**: Computer Speech & Language (Volume: 79, April 2023)
- **Summary**: Alzheimer's disease (AD) constitutes a neurodegenerative disease with serious consequences to peoples' everyday lives, if it is not diagnosed early since there is no available cure. Alzheimer's is the most common cause of dementia, which constitutes a general term for loss of memory. Due to the fact that dementia affects speech, existing research initiatives focus on detecting dementia from spontaneous speech. However, little work has been done regarding the conversion of speech data to Log-Mel spectrograms and Mel-frequency cepstral coefficients (MFCCs) and the usage of pretrained models. Concurrently, little work has been done in terms of both the usage of transformer networks and the way the two modalities, i.e., speech and transcripts, are combined in a single neural network. To address these limitations, first we represent speech signal as an image and employ several pretrained models, with Vision Transformer (ViT) achieving the highest evaluation results. Secondly, we propose multimodal models. More specifically, our introduced models include Gated Multimodal Unit in order to control the influence of each modality towards the final classification and crossmodal attention so as to capture in an effective way the relationships between the two modalities. Extensive experiments conducted on the ADReSS Challenge dataset demonstrate the effectiveness of the proposed models and their superiority over state-of-the-art approaches.



### SiamPolar: Semi-supervised Realtime Video Object Segmentation with Polar Representation
- **Arxiv ID**: http://arxiv.org/abs/2110.14773v1
- **DOI**: 10.1016/j.neucom.2021.09.063
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.14773v1)
- **Published**: 2021-10-27 21:10:18+00:00
- **Updated**: 2021-10-27 21:10:18+00:00
- **Authors**: Yaochen Li, Yuhui Hong, Yonghong Song, Chao Zhu, Ying Zhang, Ruihao Wang
- **Comment**: 11 pages, 11 figures, journal
- **Journal**: Neurocomputing, Volume 467, 7 January 2022, Pages 491-503
- **Summary**: Video object segmentation (VOS) is an essential part of autonomous vehicle navigation. The real-time speed is very important for the autonomous vehicle algorithms along with the accuracy metric. In this paper, we propose a semi-supervised real-time method based on the Siamese network using a new polar representation. The input of bounding boxes is initialized rather than the object masks, which are applied to the video object detection tasks. The polar representation could reduce the parameters for encoding masks with subtle accuracy loss so that the algorithm speed can be improved significantly. An asymmetric siamese network is also developed to extract the features from different spatial scales. Moreover, the peeling convolution is proposed to reduce the antagonism among the branches of the polar head. The repeated cross-correlation and semi-FPN are designed based on this idea. The experimental results on the DAVIS-2016 dataset and other public datasets demonstrate the effectiveness of the proposed method.



### BI-GCN: Boundary-Aware Input-Dependent Graph Convolution Network for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.14775v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.14775v2)
- **Published**: 2021-10-27 21:12:27+00:00
- **Updated**: 2021-10-31 15:23:47+00:00
- **Authors**: Yanda Meng, Hongrun Zhang, Dongxu Gao, Yitian Zhao, Xiaoyun Yang, Xuesheng Qian, Xiaowei Huang, Yalin Zheng
- **Comment**: Accepted in BMVC2021 as Oral
- **Journal**: None
- **Summary**: Segmentation is an essential operation of image processing. The convolution operation suffers from a limited receptive field, while global modelling is fundamental to segmentation tasks. In this paper, we apply graph convolution into the segmentation task and propose an improved \textit{Laplacian}. Different from existing methods, our \textit{Laplacian} is data-dependent, and we introduce two attention diagonal matrices to learn a better vertex relationship. In addition, it takes advantage of both region and boundary information when performing graph-based information propagation. Specifically, we model and reason about the boundary-aware region-wise correlations of different classes through learning graph representations, which is capable of manipulating long range semantic reasoning across various regions with the spatial enhancement along the object's boundary. Our model is well-suited to obtain global semantic region information while also accommodates local spatial boundary characteristics simultaneously. Experiments on two types of challenging datasets demonstrate that our method outperforms the state-of-the-art approaches on the segmentation of polyps in colonoscopy images and of the optic disc and optic cup in colour fundus images.



### SCALP -- Supervised Contrastive Learning for Cardiopulmonary Disease Classification and Localization in Chest X-rays using Patient Metadata
- **Arxiv ID**: http://arxiv.org/abs/2110.14787v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14787v1)
- **Published**: 2021-10-27 21:38:12+00:00
- **Updated**: 2021-10-27 21:38:12+00:00
- **Authors**: Ajay Jaiswal, Tianhao Li, Cyprian Zander, Yan Han, Justin F. Rousseau, Yifan Peng, Ying Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Computer-aided diagnosis plays a salient role in more accessible and accurate cardiopulmonary diseases classification and localization on chest radiography. Millions of people get affected and die due to these diseases without an accurate and timely diagnosis. Recently proposed contrastive learning heavily relies on data augmentation, especially positive data augmentation. However, generating clinically-accurate data augmentations for medical images is extremely difficult because the common data augmentation methods in computer vision, such as sharp, blur, and crop operations, can severely alter the clinical settings of medical images. In this paper, we proposed a novel and simple data augmentation method based on patient metadata and supervised knowledge to create clinically accurate positive and negative augmentations for chest X-rays. We introduce an end-to-end framework, SCALP, which extends the self-supervised contrastive approach to a supervised setting. Specifically, SCALP pulls together chest X-rays from the same patient (positive keys) and pushes apart chest X-rays from different patients (negative keys). In addition, it uses ResNet-50 along with the triplet-attention mechanism to identify cardiopulmonary diseases, and Grad-CAM++ to highlight the abnormal regions. Our extensive experiments demonstrate that SCALP outperforms existing baselines with significant margins in both classification and localization tasks. Specifically, the average classification AUCs improve from 82.8% (SOTA using DenseNet-121) to 83.9% (SCALP using ResNet-50), while the localization results improve on average by 3.7% over different IoU thresholds.



### MedMNIST v2 -- A large-scale lightweight benchmark for 2D and 3D biomedical image classification
- **Arxiv ID**: http://arxiv.org/abs/2110.14795v2
- **DOI**: 10.1038/s41597-022-01721-8
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.14795v2)
- **Published**: 2021-10-27 22:02:04+00:00
- **Updated**: 2022-09-25 06:07:53+00:00
- **Authors**: Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, Bingbing Ni
- **Comment**: The data and code are publicly available at https://medmnist.com/.
  arXiv admin note: text overlap with arXiv:2010.14925
- **Journal**: Scientific Data 2023
- **Summary**: We introduce MedMNIST v2, a large-scale MNIST-like dataset collection of standardized biomedical images, including 12 datasets for 2D and 6 datasets for 3D. All images are pre-processed into a small size of 28x28 (2D) or 28x28x28 (3D) with the corresponding classification labels so that no background knowledge is required for users. Covering primary data modalities in biomedical images, MedMNIST v2 is designed to perform classification on lightweight 2D and 3D images with various dataset scales (from 100 to 100,000) and diverse tasks (binary/multi-class, ordinal regression, and multi-label). The resulting dataset, consisting of 708,069 2D images and 10,214 3D images in total, could support numerous research / educational purposes in biomedical image analysis, computer vision, and machine learning. We benchmark several baseline methods on MedMNIST v2, including 2D / 3D neural networks and open-source / commercial AutoML tools. The data and code are publicly available at https://medmnist.com/.



### Intermediate Layers Matter in Momentum Contrastive Self Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.14805v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.14805v1)
- **Published**: 2021-10-27 22:40:41+00:00
- **Updated**: 2021-10-27 22:40:41+00:00
- **Authors**: Aakash Kaku, Sahana Upadhya, Narges Razavian
- **Comment**: Accepted at NeurIPS 2021 (main conference)
- **Journal**: None
- **Summary**: We show that bringing intermediate layers' representations of two augmented versions of an image closer together in self-supervised learning helps to improve the momentum contrastive (MoCo) method. To this end, in addition to the contrastive loss, we minimize the mean squared error between the intermediate layer representations or make their cross-correlation matrix closer to an identity matrix. Both loss objectives either outperform standard MoCo, or achieve similar performances on three diverse medical imaging datasets: NIH-Chest Xrays, Breast Cancer Histopathology, and Diabetic Retinopathy. The gains of the improved MoCo are especially large in a low-labeled data regime (e.g. 1% labeled data) with an average gain of 5% across three datasets. We analyze the models trained using our novel approach via feature similarity analysis and layer-wise probing. Our analysis reveals that models trained via our approach have higher feature reuse compared to a standard MoCo and learn informative features earlier in the network. Finally, by comparing the output probability distribution of models fine-tuned on small versus large labeled data, we conclude that our proposed method of pre-training leads to lower Kolmogorov-Smirnov distance, as compared to a standard MoCo. This provides additional evidence that our proposed method learns more informative features in the pre-training phase which could be leveraged in a low-labeled data regime.



