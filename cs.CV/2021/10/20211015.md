# Arxiv Papers in cs.CV on 2021-10-15
### NeuroView: Explainable Deep Network Decision Making
- **Arxiv ID**: http://arxiv.org/abs/2110.07778v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.07778v1)
- **Published**: 2021-10-15 00:00:59+00:00
- **Updated**: 2021-10-15 00:00:59+00:00
- **Authors**: CJ Barberan, Randall Balestriero, Richard G. Baraniuk
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Deep neural networks (DNs) provide superhuman performance in numerous computer vision tasks, yet it remains unclear exactly which of a DN's units contribute to a particular decision. NeuroView is a new family of DN architectures that are interpretable/explainable by design. Each member of the family is derived from a standard DN architecture by vector quantizing the unit output values and feeding them into a global linear classifier. The resulting architecture establishes a direct, causal link between the state of each unit and the classification decision. We validate NeuroView on standard datasets and classification tasks to show that how its unit/class mapping aids in understanding the decision-making process.



### Active Learning for Improved Semi-Supervised Semantic Segmentation in Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2110.07782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07782v1)
- **Published**: 2021-10-15 00:29:31+00:00
- **Updated**: 2021-10-15 00:29:31+00:00
- **Authors**: Shasvat Desai, Debasmita Ghose
- **Comment**: Accepted to Winter Conference on Applications of Computer Vision 2022
  (WACV 2022)
- **Journal**: None
- **Summary**: Remote sensing data is crucial for applications ranging from monitoring forest fires and deforestation to tracking urbanization. Most of these tasks require dense pixel-level annotations for the model to parse visual information from limited labeled data available for these satellite images. Due to the dearth of high-quality labeled training data in this domain, there is a need to focus on semi-supervised techniques. These techniques generate pseudo-labels from a small set of labeled examples which are used to augment the labeled training set. This makes it necessary to have a highly representative and diverse labeled training set. Therefore, we propose to use an active learning-based sampling strategy to select a highly representative set of labeled training data. We demonstrate our proposed method's effectiveness on two existing semantic segmentation datasets containing satellite images: UC Merced Land Use Classification Dataset and DeepGlobe Land Cover Classification Dataset. We report a 27% improvement in mIoU with as little as 2% labeled data using active learning sampling strategies over randomly sampling the small set of labeled training data.



### DG-Labeler and DGL-MOTS Dataset: Boost the Autonomous Driving Perception
- **Arxiv ID**: http://arxiv.org/abs/2110.07790v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07790v1)
- **Published**: 2021-10-15 01:04:31+00:00
- **Updated**: 2021-10-15 01:04:31+00:00
- **Authors**: Yiming Cui, Zhiwen Cao, Yixin Xie, Xingyu Jiang, Feng Tao, Yingjie Chen, Lin Li, Dongfang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-object tracking and segmentation (MOTS) is a critical task for autonomous driving applications. The existing MOTS studies face two critical challenges: 1) the published datasets inadequately capture the real-world complexity for network training to address various driving settings; 2) the working pipeline annotation tool is under-studied in the literature to improve the quality of MOTS learning examples. In this work, we introduce the DG-Labeler and DGL-MOTS dataset to facilitate the training data annotation for the MOTS task and accordingly improve network training accuracy and efficiency. DG-Labeler uses the novel Depth-Granularity Module to depict the instance spatial relations and produce fine-grained instance masks. Annotated by DG-Labeler, our DGL-MOTS dataset exceeds the prior effort (i.e., KITTI MOTS and BDD100K) in data diversity, annotation quality, and temporal representations. Results on extensive cross-dataset evaluations indicate significant performance improvements for several state-of-the-art methods trained on our DGL-MOTS dataset. We believe our DGL-MOTS Dataset and DG-Labeler hold the valuable potential to boost the visual perception of future transportation.



### Occupancy Estimation from Thermal Images
- **Arxiv ID**: http://arxiv.org/abs/2110.07796v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.07796v1)
- **Published**: 2021-10-15 01:21:31+00:00
- **Updated**: 2021-10-15 01:21:31+00:00
- **Authors**: Zishan Qin, Dipankar Chaki, Abdallah Lakhdari, Amani Abusafia, Athman Bouguettaya
- **Comment**: 4 pages, 2 figures. This is an accepted demo paper and to be
  published in the proceedings of 19th International Conference on Service
  Oriented Computing (ICSOC 2021)
- **Journal**: None
- **Summary**: We propose a non-intrusive, and privacy-preserving occupancy estimation system for smart environments. The proposed scheme uses thermal images to detect the number of people in a given area. The occupancy estimation model is designed using the concepts of intensity-based and motion-based human segmentation. The notion of difference catcher, connected component labeling, noise filter, and memory propagation are utilized to estimate the occupancy number. We use a real dataset to demonstrate the effectiveness of the proposed system.



### EFENet: Reference-based Video Super-Resolution with Enhanced Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.07797v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07797v2)
- **Published**: 2021-10-15 01:36:30+00:00
- **Updated**: 2021-10-28 07:59:52+00:00
- **Authors**: Yaping Zhao, Mengqi Ji, Ruqi Huang, Bin Wang, Shengjin Wang
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: In this paper, we consider the problem of reference-based video super-resolution(RefVSR), i.e., how to utilize a high-resolution (HR) reference frame to super-resolve a low-resolution (LR) video sequence. The existing approaches to RefVSR essentially attempt to align the reference and the input sequence, in the presence of resolution gap and long temporal range. However, they either ignore temporal structure within the input sequence, or suffer accumulative alignment errors. To address these issues, we propose EFENet to exploit simultaneously the visual cues contained in the HR reference and the temporal information contained in the LR sequence. EFENet first globally estimates cross-scale flow between the reference and each LR frame. Then our novel flow refinement module of EFENet refines the flow regarding the furthest frame using all the estimated flows, which leverages the global temporal information within the sequence and therefore effectively reduces the alignment errors. We provide comprehensive evaluations to validate the strengths of our approach, and to demonstrate that the proposed framework outperforms the state-of-the-art methods. Code is available at https://github.com/IndigoPurple/EFENet.



### Adversarial Purification through Representation Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2110.07801v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.07801v1)
- **Published**: 2021-10-15 01:45:31+00:00
- **Updated**: 2021-10-15 01:45:31+00:00
- **Authors**: Tao Bai, Jun Zhao, Lanqing Guo, Bihan Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models are vulnerable to adversarial examples and make incomprehensible mistakes, which puts a threat on their real-world deployment. Combined with the idea of adversarial training, preprocessing-based defenses are popular and convenient to use because of their task independence and good generalizability. Current defense methods, especially purification, tend to remove ``noise" by learning and recovering the natural images. However, different from random noise, the adversarial patterns are much easier to be overfitted during model training due to their strong correlation to the images. In this work, we propose a novel adversarial purification scheme by presenting disentanglement of natural images and adversarial perturbations as a preprocessing defense. With extensive experiments, our defense is shown to be generalizable and make significant protection against unseen strong adversarial attacks. It reduces the success rates of state-of-the-art \textbf{ensemble} attacks from \textbf{61.7\%} to \textbf{14.9\%} on average, superior to a number of existing methods. Notably, our defense restores the perturbed images perfectly and does not hurt the clean accuracy of backbone models, which is highly desirable in practice.



### Gait-based Human Identification through Minimum Gait-phases and Sensors
- **Arxiv ID**: http://arxiv.org/abs/2110.09286v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP, I.2
- **Links**: [PDF](http://arxiv.org/pdf/2110.09286v1)
- **Published**: 2021-10-15 02:09:45+00:00
- **Updated**: 2021-10-15 02:09:45+00:00
- **Authors**: Muhammad Zeeshan Arshad, Dawoon Jung, Mina Park, Kyung-Ryoul Mun, Jinwook Kim
- **Comment**: Accepted in 43rd Annual International Conference of the IEEE
  Engineering in Medicine and Biology Society (EMBC 2021)
- **Journal**: None
- **Summary**: Human identification is one of the most common and critical tasks for condition monitoring, human-machine interaction, and providing assistive services in smart environments. Recently, human gait has gained new attention as a biometric for identification to achieve contactless identification from a distance robust to physical appearances. However, an important aspect of gait identification through wearables and image-based systems alike is accurate identification when limited information is available, for example, when only a fraction of the whole gait cycle or only a part of the subject body is visible. In this paper, we present a gait identification technique based on temporal and descriptive statistic parameters of different gait phases as the features and we investigate the performance of using only single gait phases for the identification task using a minimum number of sensors. It was shown that it is possible to achieve high accuracy of over 95.5 percent by monitoring a single phase of the whole gait cycle through only a single sensor. It was also shown that the proposed methodology could be used to achieve 100 percent identification accuracy when the whole gait cycle was monitored through pelvis and foot sensors combined. The ANN was found to be more robust to fewer data features compared to SVM and was concluded as the best machine algorithm for the purpose.



### PTQ-SL: Exploring the Sub-layerwise Post-training Quantization
- **Arxiv ID**: http://arxiv.org/abs/2110.07809v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.07809v2)
- **Published**: 2021-10-15 02:18:54+00:00
- **Updated**: 2021-10-18 00:42:16+00:00
- **Authors**: Zhihang Yuan, Yiqi Chen, Chenhao Xue, Chenguang Zhang, Qiankun Wang, Guangyu Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Network quantization is a powerful technique to compress convolutional neural networks. The quantization granularity determines how to share the scaling factors in weights, which affects the performance of network quantization. Most existing approaches share the scaling factors layerwisely or channelwisely for quantization of convolutional layers. Channelwise quantization and layerwise quantization have been widely used in various applications. However, other quantization granularities are rarely explored. In this paper, we will explore the sub-layerwise granularity that shares the scaling factor across multiple input and output channels. We propose an efficient post-training quantization method in sub-layerwise granularity (PTQ-SL). Then we systematically experiment on various granularities and observe that the prediction accuracy of the quantized neural network has a strong correlation with the granularity. Moreover, we find that adjusting the position of the channels can improve the performance of sub-layerwise quantization. Therefore, we propose a method to reorder the channels for sub-layerwise quantization. The experiments demonstrate that the sub-layerwise quantization with appropriate channel reordering can outperform the channelwise quantization.



### Gait-based Frailty Assessment using Image Representation of IMU Signals and Deep CNN
- **Arxiv ID**: http://arxiv.org/abs/2110.07821v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP, I.2
- **Links**: [PDF](http://arxiv.org/pdf/2110.07821v1)
- **Published**: 2021-10-15 02:36:22+00:00
- **Updated**: 2021-10-15 02:36:22+00:00
- **Authors**: Muhammad Zeeshan Arshad, Dawoon Jung, Mina Park, Hyungeun Shin, Jinwook Kim, Kyung-Ryoul Mun
- **Comment**: Accepted in 43rd Annual International Conference of the IEEE
  Engineering in Medicine and Biology Society (EMBC 2021)
- **Journal**: None
- **Summary**: Frailty is a common and critical condition in elderly adults, which may lead to further deterioration of health. However, difficulties and complexities exist in traditional frailty assessments based on activity-related questionnaires. These can be overcome by monitoring the effects of frailty on the gait. In this paper, it is shown that by encoding gait signals as images, deep learning-based models can be utilized for the classification of gait type. Two deep learning models (a) SS-CNN, based on single stride input images, and (b) MS-CNN, based on 3 consecutive strides were proposed. It was shown that MS-CNN performs best with an accuracy of 85.1\%, while SS-CNN achieved an accuracy of 77.3\%. This is because MS-CNN can observe more features corresponding to stride-to-stride variations which is one of the key symptoms of frailty. Gait signals were encoded as images using STFT, CWT, and GAF. While the MS-CNN model using GAF images achieved the best overall accuracy and precision, CWT has a slightly better recall. This study demonstrates how image encoded gait data can be used to exploit the full potential of deep learning CNN models for the assessment of frailty.



### FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling
- **Arxiv ID**: http://arxiv.org/abs/2110.08263v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08263v3)
- **Published**: 2021-10-15 02:53:38+00:00
- **Updated**: 2022-01-29 02:39:54+00:00
- **Authors**: Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, Takahiro Shinozaki
- **Comment**: NeurIPS 2021; camera-ready version; 16 pages with appendix; code:
  https://github.com/TorchSSL/TorchSSL
- **Journal**: None
- **Summary**: The recently proposed FixMatch achieved state-of-the-art results on most semi-supervised learning (SSL) benchmarks. However, like other modern SSL algorithms, FixMatch uses a pre-defined constant threshold for all classes to select unlabeled data that contribute to the training, thus failing to consider different learning status and learning difficulties of different classes. To address this issue, we propose Curriculum Pseudo Labeling (CPL), a curriculum learning approach to leverage unlabeled data according to the model's learning status. The core of CPL is to flexibly adjust thresholds for different classes at each time step to let pass informative unlabeled data and their pseudo labels. CPL does not introduce additional parameters or computations (forward or backward propagation). We apply CPL to FixMatch and call our improved algorithm FlexMatch. FlexMatch achieves state-of-the-art performance on a variety of SSL benchmarks, with especially strong performances when the labeled data are extremely limited or when the task is challenging. For example, FlexMatch achieves 13.96% and 18.96% error rate reduction over FixMatch on CIFAR-100 and STL-10 datasets respectively, when there are only 4 labels per class. CPL also significantly boosts the convergence speed, e.g., FlexMatch can use only 1/5 training time of FixMatch to achieve even better performance. Furthermore, we show that CPL can be easily adapted to other SSL algorithms and remarkably improve their performances. We open-source our code at https://github.com/TorchSSL/TorchSSL.



### Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.07858v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.07858v2)
- **Published**: 2021-10-15 04:53:18+00:00
- **Updated**: 2023-02-22 06:45:58+00:00
- **Authors**: Yao Qin, Chiyuan Zhang, Ting Chen, Balaji Lakshminarayanan, Alex Beutel, Xuezhi Wang
- **Comment**: Accepted to NeurIPS-2022
- **Journal**: None
- **Summary**: We investigate the robustness of vision transformers (ViTs) through the lens of their special patch-based architectural structure, i.e., they process an image as a sequence of image patches. We find that ViTs are surprisingly insensitive to patch-based transformations, even when the transformation largely destroys the original semantics and makes the image unrecognizable by humans. This indicates that ViTs heavily use features that survived such transformations but are generally not indicative of the semantic class to humans. Further investigations show that these features are useful but non-robust, as ViTs trained on them can achieve high in-distribution accuracy, but break down under distribution shifts. From this understanding, we ask: can training the model to rely less on these features improve ViT robustness and out-of-distribution performance? We use the images transformed with our patch-based operations as negatively augmented views and offer losses to regularize the training away from using non-robust features. This is a complementary view to existing research that mostly focuses on augmenting inputs with semantic-preserving transformations to enforce models' invariance. We show that patch-based negative augmentation consistently improves robustness of ViTs across a wide set of ImageNet based robustness benchmarks. Furthermore, we find our patch-based negative augmentation are complementary to traditional (positive) data augmentation, and together boost the performance further.



### Receptive Field Broadening and Boosting for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.07859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07859v1)
- **Published**: 2021-10-15 05:10:33+00:00
- **Updated**: 2021-10-15 05:10:33+00:00
- **Authors**: Mingcan Ma, Changqun Xia, Chenxi Xie, Xiaowu Chen, Jia Li
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Salient object detection requires a comprehensive and scalable receptive field to locate the visually significant objects in the image. Recently, the emergence of visual transformers and multi-branch modules has significantly enhanced the ability of neural networks to perceive objects at different scales. However, compared to the traditional backbone, the calculation process of transformers is time-consuming. Moreover, different branches of the multi-branch modules could cause the same error back propagation in each training iteration, which is not conducive to extracting discriminative features. To solve these problems, we propose a bilateral network based on transformer and CNN to efficiently broaden local details and global semantic information simultaneously. Besides, a Multi-Head Boosting (MHB) strategy is proposed to enhance the specificity of different network branches. By calculating the errors of different prediction heads, each branch can separately pay more attention to the pixels that other branches predict incorrectly. Moreover, Unlike multi-path parallel training, MHB randomly selects one branch each time for gradient back propagation in a boosting way. Additionally, an Attention Feature Fusion Module (AF) is proposed to fuse two types of features according to respective characteristics. Comprehensive experiments on five benchmark datasets demonstrate that the proposed method can achieve a significant performance improvement compared with the state-of-the-art methods.



### Single volume lung biomechanics from chest computed tomography using a mode preserving generative adversarial network
- **Arxiv ID**: http://arxiv.org/abs/2110.07878v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.07878v1)
- **Published**: 2021-10-15 06:17:52+00:00
- **Updated**: 2021-10-15 06:17:52+00:00
- **Authors**: Muhammad F. A. Chaudhary, Sarah E. Gerard, Di Wang, Gary E. Christensen, Christopher B. Cooper, Joyce D. Schroeder, Eric A. Hoffman, Joseph M. Reinhardt
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: Local tissue expansion of the lungs is typically derived by registering computed tomography (CT) scans acquired at multiple lung volumes. However, acquiring multiple scans incurs increased radiation dose, time, and cost, and may not be possible in many cases, thus restricting the applicability of registration-based biomechanics. We propose a generative adversarial learning approach for estimating local tissue expansion directly from a single CT scan. The proposed framework was trained and evaluated on 2500 subjects from the SPIROMICS cohort. Once trained, the framework can be used as a registration-free method for predicting local tissue expansion. We evaluated model performance across varying degrees of disease severity and compared its performance with two image-to-image translation frameworks - UNet and Pix2Pix. Our model achieved an overall PSNR of 18.95 decibels, SSIM of 0.840, and Spearman's correlation of 0.61 at a high spatial resolution of 1 mm3.



### Advances and Challenges in Deep Lip Reading
- **Arxiv ID**: http://arxiv.org/abs/2110.07879v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07879v1)
- **Published**: 2021-10-15 06:18:26+00:00
- **Updated**: 2021-10-15 06:18:26+00:00
- **Authors**: Marzieh Oghbaie, Arian Sabaghi, Kooshan Hashemifard, Mohammad Akbari
- **Comment**: None
- **Journal**: None
- **Summary**: Driven by deep learning techniques and large-scale datasets, recent years have witnessed a paradigm shift in automatic lip reading. While the main thrust of Visual Speech Recognition (VSR) was improving accuracy of Audio Speech Recognition systems, other potential applications, such as biometric identification, and the promised gains of VSR systems, have motivated extensive efforts on developing the lip reading technology. This paper provides a comprehensive survey of the state-of-the-art deep learning based VSR research with a focus on data challenges, task-specific complications, and the corresponding solutions. Advancements in these directions will expedite the transformation of silent speech interface from theory to practice. We also discuss the main modules of a VSR pipeline and the influential datasets. Finally, we introduce some typical VSR application concerns and impediments to real-world scenarios as well as future research directions.



### PolyNet: Polynomial Neural Network for 3D Shape Recognition with PolyShape Representation
- **Arxiv ID**: http://arxiv.org/abs/2110.07882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07882v1)
- **Published**: 2021-10-15 06:45:59+00:00
- **Updated**: 2021-10-15 06:45:59+00:00
- **Authors**: Mohsen Yavartanoo, Shih-Hsuan Hung, Reyhaneh Neshatavar, Yue Zhang, Kyoung Mu Lee
- **Comment**: None
- **Journal**: 2021 International Conference on 3D Vision (3DV)
- **Summary**: 3D shape representation and its processing have substantial effects on 3D shape recognition. The polygon mesh as a 3D shape representation has many advantages in computer graphics and geometry processing. However, there are still some challenges for the existing deep neural network (DNN)-based methods on polygon mesh representation, such as handling the variations in the degree and permutations of the vertices and their pairwise distances. To overcome these challenges, we propose a DNN-based method (PolyNet) and a specific polygon mesh representation (PolyShape) with a multi-resolution structure. PolyNet contains two operations; (1) a polynomial convolution (PolyConv) operation with learnable coefficients, which learns continuous distributions as the convolutional filters to share the weights across different vertices, and (2) a polygonal pooling (PolyPool) procedure by utilizing the multi-resolution structure of PolyShape to aggregate the features in a much lower dimension. Our experiments demonstrate the strength and the advantages of PolyNet on both 3D shape classification and retrieval tasks compared to existing polygon mesh-based methods and its superiority in classifying graph representations of images. The code is publicly available from https://myavartanoo.github.io/polynet/.



### Improving Unsupervised Domain Adaptive Re-Identification via Source-Guided Selection of Pseudo-Labeling Hyperparameters
- **Arxiv ID**: http://arxiv.org/abs/2110.07897v2
- **DOI**: 10.1109/ACCESS.2021.3124678
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.07897v2)
- **Published**: 2021-10-15 07:26:19+00:00
- **Updated**: 2021-11-04 11:11:13+00:00
- **Authors**: Fabian Dubourvieux, Angélique Loesch, Romaric Audigier, Samia Ainouz, Stéphane Canu
- **Comment**: Preprint version. Accepted in IEEE Access (see IEEE Access for final
  version)
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) for re-identification (re-ID) is a challenging task: to avoid a costly annotation of additional data, it aims at transferring knowledge from a domain with annotated data to a domain of interest with only unlabeled data. Pseudo-labeling approaches have proven to be effective for UDA re-ID. However, the effectiveness of these approaches heavily depends on the choice of some hyperparameters (HP) that affect the generation of pseudo-labels by clustering. The lack of annotation in the domain of interest makes this choice non-trivial. Current approaches simply reuse the same empirical value for all adaptation tasks and regardless of the target data representation that changes through pseudo-labeling training phases. As this simplistic choice may limit their performance, we aim at addressing this issue. We propose new theoretical grounds on HP selection for clustering UDA re-ID as well as method of automatic and cyclic HP tuning for pseudo-labeling UDA clustering: HyPASS. HyPASS consists in incorporating two modules in pseudo-labeling methods: (i) HP selection based on a labeled source validation set and (ii) conditional domain alignment of feature discriminativeness to improve HP selection based on source samples. Experiments on commonly used person re-ID and vehicle re-ID datasets show that our proposed HyPASS consistently improves the best state-of-the-art methods in re-ID compared to the commonly used empirical HP setting.



### Learning to Infer Kinematic Hierarchies for Novel Object Instances
- **Arxiv ID**: http://arxiv.org/abs/2110.07911v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.07911v1)
- **Published**: 2021-10-15 07:50:48+00:00
- **Updated**: 2021-10-15 07:50:48+00:00
- **Authors**: Hameed Abdul-Rashid, Miles Freeman, Ben Abbatematteo, George Konidaris, Daniel Ritchie
- **Comment**: None
- **Journal**: None
- **Summary**: Manipulating an articulated object requires perceiving itskinematic hierarchy: its parts, how each can move, and howthose motions are coupled. Previous work has explored per-ception for kinematics, but none infers a complete kinematichierarchy on never-before-seen object instances, without relyingon a schema or template. We present a novel perception systemthat achieves this goal. Our system infers the moving parts ofan object and the kinematic couplings that relate them. Toinfer parts, it uses a point cloud instance segmentation neuralnetwork and to infer kinematic hierarchies, it uses a graphneural network to predict the existence, direction, and typeof edges (i.e. joints) that relate the inferred parts. We trainthese networks using simulated scans of synthetic 3D models.We evaluate our system on simulated scans of 3D objects, andwe demonstrate a proof-of-concept use of our system to drivereal-world robotic manipulation.



### Combining CNNs With Transformer for Multimodal 3D MRI Brain Tumor Segmentation With Self-Supervised Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2110.07919v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.07919v1)
- **Published**: 2021-10-15 08:03:27+00:00
- **Updated**: 2021-10-15 08:03:27+00:00
- **Authors**: Mariia Dobko, Danylo-Ivan Kolinko, Ostap Viniavskyi, Yurii Yelisieiev
- **Comment**: None
- **Journal**: None
- **Summary**: We apply an ensemble of modified TransBTS, nnU-Net, and a combination of both for the segmentation task of the BraTS 2021 challenge. In fact, we change the original architecture of the TransBTS model by adding Squeeze-and-Excitation blocks, an increasing number of CNN layers, replacing positional encoding in Transformer block with a learnable Multilayer Perceptron (MLP) embeddings, which makes Transformer adjustable to any input size during inference. With these modifications, we are able to largely improve TransBTS performance. Inspired by a nnU-Net framework we decided to combine it with our modified TransBTS by changing the architecture inside nnU-Net to our custom model. On the Validation set of BraTS 2021, the ensemble of these approaches achieves 0.8496, 0.8698, 0.9256 Dice score and 15.72, 11.057, 3.374 HD95 for enhancing tumor, tumor core, and whole tumor, correspondingly. Our code is publicly available.



### Content Preserving Image Translation with Texture Co-occurrence and Spatial Self-Similarity for Texture Debiasing and Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2110.07920v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07920v5)
- **Published**: 2021-10-15 08:04:59+00:00
- **Updated**: 2023-01-03 13:43:21+00:00
- **Authors**: Myeongkyun Kang, Dongkyu Won, Miguel Luna, Philip Chikontwe, Kyung Soo Hong, June Hong Ahn, Sang Hyun Park
- **Comment**: None
- **Journal**: None
- **Summary**: Models trained on datasets with texture bias usually perform poorly on out-of-distribution samples since biased representations are embedded into the model. Recently, various image translation and debiasing methods have attempted to disentangle texture biased representations for downstream tasks, but accurately discarding biased features without altering other relevant information is still challenging. In this paper, we propose a novel framework that leverages image translation to generate additional training images using the content of a source image and the texture of a target image with a different bias property to explicitly mitigate texture bias when training a model on a target task. Our model ensures texture similarity between the target and generated images via a texture co-occurrence loss while preserving content details from source images with a spatial self-similarity loss. Both the generated and original training images are combined to train improved classification or segmentation models robust to inconsistent texture bias. Evaluation on five classification- and two segmentation-datasets with known texture biases demonstrates the utility of our method, and reports significant improvements over recent state-of-the-art methods in all cases.



### Relation Preserving Triplet Mining for Stabilising the Triplet Loss in Re-identification Systems
- **Arxiv ID**: http://arxiv.org/abs/2110.07933v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07933v2)
- **Published**: 2021-10-15 08:23:40+00:00
- **Updated**: 2022-11-14 23:29:28+00:00
- **Authors**: Adhiraj Ghosh, Kuruparan Shanmugalingam, Wen-Yan Lin
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: Object appearances change dramatically with pose variations. This creates a challenge for embedding schemes that seek to map instances with the same object ID to locations that are as close as possible. This issue becomes significantly heightened in complex computer vision tasks such as re-identification(reID). In this paper, we suggest that these dramatic appearance changes are indications that an object ID is composed of multiple natural groups, and it is counterproductive to forcefully map instances from different groups to a common location. This leads us to introduce Relation Preserving Triplet Mining (RPTM), a feature-matching guided triplet mining scheme, that ensures that triplets will respect the natural subgroupings within an object ID. We use this triplet mining mechanism to establish a pose-aware, well-conditioned triplet loss by implicitly enforcing view consistency. This allows a single network to be trained with fixed parameters across datasets while providing state-of-the-art results. Code is available at https://github.com/adhirajghosh/RPTM_reid.



### Streaming Machine Learning and Online Active Learning for Automated Visual Inspection
- **Arxiv ID**: http://arxiv.org/abs/2110.09396v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09396v2)
- **Published**: 2021-10-15 09:39:04+00:00
- **Updated**: 2021-12-09 15:17:44+00:00
- **Authors**: Jože M. Rožanec, Elena Trajkova, Paulien Dam, Blaž Fortuna, Dunja Mladenić
- **Comment**: None
- **Journal**: None
- **Summary**: Quality control is a key activity performed by manufacturing companies to verify product conformance to the requirements and specifications. Standardized quality control ensures that all the products are evaluated under the same criteria. The decreased cost of sensors and connectivity enabled an increasing digitalization of manufacturing and provided greater data availability. Such data availability has spurred the development of artificial intelligence models, which allow higher degrees of automation and reduced bias when inspecting the products. Furthermore, the increased speed of inspection reduces overall costs and time required for defect inspection. In this research, we compare five streaming machine learning algorithms applied to visual defect inspection with real-world data provided by Philips Consumer Lifestyle BV. Furthermore, we compare them in a streaming active learning context, which reduces the data labeling effort in a real-world context. Our results show that active learning reduces the data labeling effort by almost 15% on average for the worst case, while keeping an acceptable classification performance. The use of machine learning models for automated visual inspection are expected to speed up the quality inspection up to 40%.



### On Generating Identifiable Virtual Faces
- **Arxiv ID**: http://arxiv.org/abs/2110.07986v2
- **DOI**: 10.1145/3503161.3548110
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07986v2)
- **Published**: 2021-10-15 10:19:48+00:00
- **Updated**: 2022-07-25 02:50:13+00:00
- **Authors**: Zhuowen Yuan, Zhengxin You, Sheng Li, Xinpeng Zhang, Zhenxin Qian, Alex Kot
- **Comment**: None
- **Journal**: None
- **Summary**: Face anonymization with generative models have become increasingly prevalent since they sanitize private information by generating virtual face images, ensuring both privacy and image utility. Such virtual face images are usually not identifiable after the removal or protection of the original identity. In this paper, we formalize and tackle the problem of generating identifiable virtual face images. Our virtual face images are visually different from the original ones for privacy protection. In addition, they are bound with new virtual identities, which can be directly used for face recognition. We propose an Identifiable Virtual Face Generator (IVFG) to generate the virtual face images. The IVFG projects the latent vectors of the original face images into virtual ones according to a user specific key, based on which the virtual face images are generated. To make the virtual face images identifiable, we propose a multi-task learning objective as well as a triplet styled training strategy to learn the IVFG. We evaluate the performance of our virtual face images using different face recognizers on diffident face image datasets, all of which demonstrate the effectiveness of the IVFG for generate identifiable virtual face images.



### Pose-guided Generative Adversarial Net for Novel View Action Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2110.07993v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07993v2)
- **Published**: 2021-10-15 10:33:09+00:00
- **Updated**: 2021-12-09 02:53:15+00:00
- **Authors**: Xianhang Li, Junhao Zhang, Kunchang Li, Shruti Vyas, Yogesh S Rawat
- **Comment**: Accepted by WACV2022
- **Journal**: None
- **Summary**: We focus on the problem of novel-view human action synthesis. Given an action video, the goal is to generate the same action from an unseen viewpoint. Naturally, novel view video synthesis is more challenging than image synthesis. It requires the synthesis of a sequence of realistic frames with temporal coherency. Besides, transferring the different actions to a novel target view requires awareness of action category and viewpoint change simultaneously. To address these challenges, we propose a novel framework named Pose-guided Action Separable Generative Adversarial Net (PAS-GAN), which utilizes pose to alleviate the difficulty of this task. First, we propose a recurrent pose-transformation module which transforms actions from the source view to the target view and generates novel view pose sequence in 2D coordinate space. Second, a well-transformed pose sequence enables us to separatethe action and background in the target view. We employ a novel local-global spatial transformation module to effectively generate sequential video features in the target view using these action and background features. Finally, the generated video features are used to synthesize human action with the help of a 3D decoder. Moreover, to focus on dynamic action in the video, we propose a novel multi-scale action-separable loss which further improves the video quality. We conduct extensive experiments on two large-scale multi-view human action datasets, NTU-RGBD and PKU-MMD, demonstrating the effectiveness of PAS-GAN which outperforms existing approaches.



### Pyramid Correlation based Deep Hough Voting for Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2110.07994v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07994v1)
- **Published**: 2021-10-15 10:37:00+00:00
- **Updated**: 2021-10-15 10:37:00+00:00
- **Authors**: Ying Wang, Tingfa Xu, Jianan Li, Shenwang Jiang, Junjie Chen
- **Comment**: Accepted by ACML 2021 Conference Track (Short Oral)
- **Journal**: None
- **Summary**: Most of the existing Siamese-based trackers treat tracking problem as a parallel task of classification and regression. However, some studies show that the sibling head structure could lead to suboptimal solutions during the network training. Through experiments we find that, without regression, the performance could be equally promising as long as we delicately design the network to suit the training objective. We introduce a novel voting-based classification-only tracking algorithm named Pyramid Correlation based Deep Hough Voting (short for PCDHV), to jointly locate the top-left and bottom-right corners of the target. Specifically we innovatively construct a Pyramid Correlation module to equip the embedded feature with fine-grained local structures and global spatial contexts; The elaborately designed Deep Hough Voting module further take over, integrating long-range dependencies of pixels to perceive corners; In addition, the prevalent discretization gap is simply yet effectively alleviated by increasing the spatial resolution of the feature maps while exploiting channel-space relationships. The algorithm is general, robust and simple. We demonstrate the effectiveness of the module through a series of ablation experiments. Without bells and whistles, our tracker achieves better or comparable performance to the SOTA algorithms on three challenging benchmarks (TrackingNet, GOT-10k and LaSOT) while running at a real-time speed of 80 FPS. Codes and models will be released.



### MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining
- **Arxiv ID**: http://arxiv.org/abs/2110.08009v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08009v3)
- **Published**: 2021-10-15 11:12:56+00:00
- **Updated**: 2022-01-21 03:10:39+00:00
- **Authors**: Ahmed Imtiaz Humayun, Randall Balestriero, Richard Baraniuk
- **Comment**: ICLR Accepted version, 28 pages, 23 figures
- **Journal**: None
- **Summary**: Deep Generative Networks (DGNs) are extensively employed in Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and their variants to approximate the data manifold and distribution. However, training samples are often distributed in a non-uniform fashion on the manifold, due to costs or convenience of collection. For example, the CelebA dataset contains a large fraction of smiling faces. These inconsistencies will be reproduced when sampling from the trained DGN, which is not always preferred, e.g., for fairness or data augmentation. In response, we develop MaGNET, a novel and theoretically motivated latent space sampler for any pre-trained DGN, that produces samples uniformly distributed on the learned manifold. We perform a range of experiments on various datasets and DGNs, e.g., for the state-of-the-art StyleGAN2 trained on FFHQ dataset, uniform sampling via MaGNET increases distribution precision and recall by 4.1\% \& 3.0\% and decreases gender bias by 41.2\%, without requiring labels or retraining. As uniform distribution does not imply uniform semantic distribution, we also explore separately how semantic attributes of generated samples vary under MaGNET sampling.



### Joint Channel and Weight Pruning for Model Acceleration on Moblie Devices
- **Arxiv ID**: http://arxiv.org/abs/2110.08013v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08013v2)
- **Published**: 2021-10-15 11:18:42+00:00
- **Updated**: 2021-11-09 05:36:41+00:00
- **Authors**: Tianli Zhao, Xi Sheryl Zhang, Wentao Zhu, Jiaxing Wang, Sen Yang, Ji Liu, Jian Cheng
- **Comment**: 23 pages, 6 figures
- **Journal**: None
- **Summary**: For practical deep neural network design on mobile devices, it is essential to consider the constraints incurred by the computational resources and the inference latency in various applications. Among deep network acceleration related approaches, pruning is a widely adopted practice to balance the computational resource consumption and the accuracy, where unimportant connections can be removed either channel-wisely or randomly with a minimal impact on model accuracy. The channel pruning instantly results in a significant latency reduction, while the random weight pruning is more flexible to balance the latency and accuracy. In this paper, we present a unified framework with Joint Channel pruning and Weight pruning (JCW), and achieves a better Pareto-frontier between the latency and accuracy than previous model compression approaches. To fully optimize the trade-off between the latency and accuracy, we develop a tailored multi-objective evolutionary algorithm in the JCW framework, which enables one single search to obtain the optimal candidate architectures for various deployment requirements. Extensive experiments demonstrate that the JCW achieves a better trade-off between the latency and accuracy against various state-of-the-art pruning methods on the ImageNet classification dataset. Our codes are available at https://github.com/jcw-anonymous/JCW.



### Adversarial Attacks on ML Defense Models Competition
- **Arxiv ID**: http://arxiv.org/abs/2110.08042v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.08042v1)
- **Published**: 2021-10-15 12:12:41+00:00
- **Updated**: 2021-10-15 12:12:41+00:00
- **Authors**: Yinpeng Dong, Qi-An Fu, Xiao Yang, Wenzhao Xiang, Tianyu Pang, Hang Su, Jun Zhu, Jiayu Tang, Yuefeng Chen, XiaoFeng Mao, Yuan He, Hui Xue, Chao Li, Ye Liu, Qilong Zhang, Lianli Gao, Yunrui Yu, Xitong Gao, Zhe Zhao, Daquan Lin, Jiadong Lin, Chuanbiao Song, Zihao Wang, Zhennan Wu, Yang Guo, Jiequan Cui, Xiaogang Xu, Pengguang Chen
- **Comment**: Competition Report
- **Journal**: None
- **Summary**: Due to the vulnerability of deep neural networks (DNNs) to adversarial examples, a large number of defense techniques have been proposed to alleviate this problem in recent years. However, the progress of building more robust models is usually hampered by the incomplete or incorrect robustness evaluation. To accelerate the research on reliable evaluation of adversarial robustness of the current defense models in image classification, the TSAIL group at Tsinghua University and the Alibaba Security group organized this competition along with a CVPR 2021 workshop on adversarial machine learning (https://aisecure-workshop.github.io/amlcvpr2021/). The purpose of this competition is to motivate novel attack algorithms to evaluate adversarial robustness more effectively and reliably. The participants were encouraged to develop stronger white-box attack algorithms to find the worst-case robustness of different defenses. This competition was conducted on an adversarial robustness evaluation platform -- ARES (https://github.com/thu-ml/ares), and is held on the TianChi platform (https://tianchi.aliyun.com/competition/entrance/531847/introduction) as one of the series of AI Security Challengers Program. After the competition, we summarized the results and established a new adversarial robustness benchmark at https://ml.cs.tsinghua.edu.cn/ares-bench/, which allows users to upload adversarial attack algorithms and defense models for evaluation.



### FlexConv: Continuous Kernel Convolutions with Differentiable Kernel Sizes
- **Arxiv ID**: http://arxiv.org/abs/2110.08059v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.08059v3)
- **Published**: 2021-10-15 12:35:49+00:00
- **Updated**: 2022-03-17 13:20:29+00:00
- **Authors**: David W. Romero, Robert-Jan Bruintjes, Jakub M. Tomczak, Erik J. Bekkers, Mark Hoogendoorn, Jan C. van Gemert
- **Comment**: First two authors contributed equally to this work
- **Journal**: None
- **Summary**: When designing Convolutional Neural Networks (CNNs), one must select the size\break of the convolutional kernels before training. Recent works show CNNs benefit from different kernel sizes at different layers, but exploring all possible combinations is unfeasible in practice. A more efficient approach is to learn the kernel size during training. However, existing works that learn the kernel size have a limited bandwidth. These approaches scale kernels by dilation, and thus the detail they can describe is limited. In this work, we propose FlexConv, a novel convolutional operation with which high bandwidth convolutional kernels of learnable kernel size can be learned at a fixed parameter cost. FlexNets model long-term dependencies without the use of pooling, achieve state-of-the-art performance on several sequential datasets, outperform recent works with learned kernel sizes, and are competitive with much deeper ResNets on image benchmark datasets. Additionally, FlexNets can be deployed at higher resolutions than those seen during training. To avoid aliasing, we propose a novel kernel parameterization with which the frequency of the kernels can be analytically controlled. Our novel kernel parameterization shows higher descriptive power and faster convergence speed than existing parameterizations. This leads to important improvements in classification accuracy.



### Automated Quality Control of Vacuum Insulated Glazing by Convolutional Neural Network Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.08079v1
- **DOI**: 10.1016/j.autcon.2022.104144
- **Categories**: **cs.CV**, cs.AI, I.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2110.08079v1)
- **Published**: 2021-10-15 13:10:54+00:00
- **Updated**: 2021-10-15 13:10:54+00:00
- **Authors**: Henrik Riedel, Sleheddine Mokdad, Isabell Schulz, Cenk Kocer, Philipp Rosendahl, Jens Schneider, Michael A. Kraus, Michael Drass
- **Comment**: 10 pages, 11 figures, 1 table
- **Journal**: Automation in Construction 135 (2022) 104144
- **Summary**: Vacuum Insulated Glazing (VIG) is a highly thermally insulating window technology, which boasts an extremely thin profile and lower weight as compared to gas-filled insulated glazing units of equivalent performance. The VIG is a double-pane configuration with a submillimeter vacuum gap between the panes and therefore under constant atmospheric pressure over their service life. Small pillars are positioned between the panes to maintain the gap, which can damage the glass reducing the lifetime of the VIG unit. To efficiently assess any surface damage on the glass, an automated damage detection system is highly desirable. For the purpose of classifying the damage, we have developed, trained, and tested a deep learning computer vision system using convolutional neural networks. The classification model flawlessly classified the test dataset with an area under the curve (AUC) for the receiver operating characteristic (ROC) of 100%. We have automatically cropped the images down to their relevant information by using Faster-RCNN to locate the position of the pillars. We employ the state-of-the-art methods Grad-CAM and Score-CAM of explainable Artificial Intelligence (XAI) to provide an understanding of the internal mechanisms and were able to show that our classifier outperforms ResNet50V2 for identification of crack locations and geometry. The proposed methods can therefore be used to detect systematic defects even without large amounts of training data. Further analyses of our model's predictive capabilities demonstrates its superiority over state-of-the-art models (ResNet50V2, ResNet101V2 and ResNet152V2) in terms of convergence speed, accuracy, precision at 100% recall and AUC for ROC.



### Deep multi-modal aggregation network for MR image reconstruction with auxiliary modality
- **Arxiv ID**: http://arxiv.org/abs/2110.08080v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08080v3)
- **Published**: 2021-10-15 13:16:59+00:00
- **Updated**: 2022-02-21 13:15:23+00:00
- **Authors**: Chun-Mei Feng, Huazhu Fu, Tianfei Zhou, Yong Xu, Ling Shao, David Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic resonance (MR) imaging produces detailed images of organs and tissues with better contrast, but it suffers from a long acquisition time, which makes the image quality vulnerable to say motion artifacts. Recently, many approaches have been developed to reconstruct full-sampled images from partially observed measurements to accelerate MR imaging. However, most approaches focused on reconstruction over a single modality, neglecting the discovery of correlation knowledge between the different modalities. Here we propose a Multi-modal Aggregation network for mR Image recOnstruction with auxiliary modality (MARIO), which is capable of discovering complementary representations from a fully sampled auxiliary modality, with which to hierarchically guide the reconstruction of a given target modality. This implies that our method can selectively aggregate multi-modal representations for better reconstruction, yielding comprehensive, multi-scale, multi-modal feature fusion. Extensive experiments on IXI and fastMRI datasets demonstrate the superiority of the proposed approach over state-of-the-art MR image reconstruction methods in removing artifacts.



### Prediction of Lung CT Scores of Systemic Sclerosis by Cascaded Regression Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.08085v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08085v1)
- **Published**: 2021-10-15 13:28:12+00:00
- **Updated**: 2021-10-15 13:28:12+00:00
- **Authors**: Jingnan Jia, Marius Staring, Irene Hernández-Girón, Lucia J. M. Kroft, Anne A. Schouffoer, Berend C. Stoel
- **Comment**: SPIE 2022 accepted
- **Journal**: None
- **Summary**: Visually scoring lung involvement in systemic sclerosis from CT scans plays an important role in monitoring progression, but its labor intensiveness hinders practical application. We proposed, therefore, an automatic scoring framework that consists of two cascaded deep regression neural networks. The first (3D) network aims to predict the craniocaudal position of five anatomically defined scoring levels on the 3D CT scans. The second (2D) network receives the resulting 2D axial slices and predicts the scores. We used 227 3D CT scans to train and validate the first network, and the resulting 1135 axial slices were used in the second network. Two experts scored independently a subset of data to obtain intra- and interobserver variabilities and the ground truth for all data was obtained in consensus. To alleviate the unbalance in training labels in the second network, we introduced a sampling technique and to increase the diversity of the training samples synthetic data was generated, mimicking ground glass and reticulation patterns. The 4-fold cross validation showed that our proposed network achieved an average MAE of 5.90, 4.66 and 4.49, weighted kappa of 0.66, 0.58 and 0.65 for total score (TOT), ground glass (GG) and reticular pattern (RET), respectively. Our network performed slightly worse than the best experts on TOT and GG prediction but it has competitive performance on RET prediction and has the potential to be an objective alternative for the visual scoring of SSc in CT thorax studies.



### Automatic Detection of COVID-19 and Pneumonia from Chest X-Ray using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.09384v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09384v1)
- **Published**: 2021-10-15 14:57:14+00:00
- **Updated**: 2021-10-15 14:57:14+00:00
- **Authors**: Sarath Pathari
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, a dataset of X-ray images from patients with common viral pneumonia, bacterial pneumonia, confirmed Covid-19 disease was utilized for the automatic detection of the Coronavirus disease. The point of the investigation is to assess the exhibition of cutting edge convolutional neural system structures proposed over the ongoing years for clinical picture order. In particular, the system called Transfer Learning was received. With transfer learning, the location of different variations from the norm in little clinical picture datasets is a reachable objective, regularly yielding amazing outcomes. The datasets used in this trial. Firstly, a collection of 24000 X-ray images includes 6000 images for confirmed Covid-19 disease,6000 confirmed common bacterial pneumonia and 6000 images of normal conditions. The information was gathered and expanded from the accessible X-Ray pictures on open clinical stores. The outcomes recommend that Deep Learning with X-Ray imaging may separate noteworthy biological markers identified with the Covid-19 sickness, while the best precision, affectability, and particularity acquired is 97.83%, 96.81%, and 98.56% individually.



### Trade-offs of Local SGD at Scale: An Empirical Study
- **Arxiv ID**: http://arxiv.org/abs/2110.08133v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08133v1)
- **Published**: 2021-10-15 15:00:42+00:00
- **Updated**: 2021-10-15 15:00:42+00:00
- **Authors**: Jose Javier Gonzalez Ortiz, Jonathan Frankle, Mike Rabbat, Ari Morcos, Nicolas Ballas
- **Comment**: None
- **Journal**: None
- **Summary**: As datasets and models become increasingly large, distributed training has become a necessary component to allow deep neural networks to train in reasonable amounts of time. However, distributed training can have substantial communication overhead that hinders its scalability. One strategy for reducing this overhead is to perform multiple unsynchronized SGD steps independently on each worker between synchronization steps, a technique known as local SGD. We conduct a comprehensive empirical study of local SGD and related methods on a large-scale image classification task. We find that performing local SGD comes at a price: lower communication costs (and thereby faster training) are accompanied by lower accuracy. This finding is in contrast from the smaller-scale experiments in prior work, suggesting that local SGD encounters challenges at scale. We further show that incorporating the slow momentum framework of Wang et al. (2020) consistently improves accuracy without requiring additional communication, hinting at future directions for potentially escaping this trade-off.



### Multi-Tailed, Multi-Headed, Spatial Dynamic Memory refined Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2110.08143v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08143v1)
- **Published**: 2021-10-15 15:16:58+00:00
- **Updated**: 2021-10-15 15:16:58+00:00
- **Authors**: Amrit Diggavi Seshadri, Balaraman Ravindran
- **Comment**: None
- **Journal**: None
- **Summary**: Synthesizing high-quality, realistic images from text-descriptions is a challenging task, and current methods synthesize images from text in a multi-stage manner, typically by first generating a rough initial image and then refining image details at subsequent stages. However, existing methods that follow this paradigm suffer from three important limitations. Firstly, they synthesize initial images without attempting to separate image attributes at a word-level. As a result, object attributes of initial images (that provide a basis for subsequent refinement) are inherently entangled and ambiguous in nature. Secondly, by using common text-representations for all regions, current methods prevent us from interpreting text in fundamentally different ways at different parts of an image. Different image regions are therefore only allowed to assimilate the same type of information from text at each refinement stage. Finally, current methods generate refinement features only once at each refinement stage and attempt to address all image aspects in a single shot. This single-shot refinement limits the precision with which each refinement stage can learn to improve the prior image. Our proposed method introduces three novel components to address these shortcomings: (1) An initial generation stage that explicitly generates separate sets of image features for each word n-gram. (2) A spatial dynamic memory module for refinement of images. (3) An iterative multi-headed mechanism to make it easier to improve upon multiple image aspects. Experimental results demonstrate that our Multi-Headed Spatial Dynamic Memory image refinement with our Multi-Tailed Word-level Initial Generation (MSMT-GAN) performs favourably against the previous state of the art on the CUB and COCO datasets.



### Accurate Fine-grained Layout Analysis for the Historical Tibetan Document Based on the Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.08164v3
- **DOI**: 10.1109/ACCESS.2021.3128536
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08164v3)
- **Published**: 2021-10-15 15:49:44+00:00
- **Updated**: 2021-11-15 01:33:11+00:00
- **Authors**: Penghai Zhao, Weilan Wang, Zhengqi Cai, Guowei Zhang, Yuqi Lu
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Accurate layout analysis without subsequent text-line segmentation remains an ongoing challenge, especially when facing the Kangyur, a kind of historical Tibetan document featuring considerable touching components and mottled background. Aiming at identifying different regions in document images, layout analysis is indispensable for subsequent procedures such as character recognition. However, there was only a little research being carried out to perform line-level layout analysis which failed to deal with the Kangyur. To obtain the optimal results, a fine-grained sub-line level layout analysis approach is presented. Firstly, we introduced an accelerated method to build the dataset which is dynamic and reliable. Secondly, enhancement had been made to the SOLOv2 according to the characteristics of the Kangyur. Then, we fed the enhanced SOLOv2 with the prepared annotation file during the training phase. Once the network is trained, instances of the text line, sentence, and titles can be segmented and identified during the inference stage. The experimental results show that the proposed method delivers a decent 72.7% average precision on our dataset. In general, this preliminary research provides insights into the fine-grained sub-line level layout analysis and testifies the SOLOv2-based approaches. We also believe that the proposed methods can be adopted on other language documents with various layouts.



### Training Deep Neural Networks with Joint Quantization and Pruning of Weights and Activations
- **Arxiv ID**: http://arxiv.org/abs/2110.08271v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08271v2)
- **Published**: 2021-10-15 16:14:36+00:00
- **Updated**: 2021-11-01 09:06:55+00:00
- **Authors**: Xinyu Zhang, Ian Colbert, Ken Kreutz-Delgado, Srinjoy Das
- **Comment**: None
- **Journal**: None
- **Summary**: Quantization and pruning are core techniques used to reduce the inference costs of deep neural networks. State-of-the-art quantization techniques are currently applied to both the weights and activations; however, pruning is most often applied to only the weights of the network. In this work, we jointly apply novel uniform quantization and unstructured pruning methods to both the weights and activations of deep neural networks during training. Using our methods, we empirically evaluate the currently accepted prune-then-quantize paradigm across a wide range of computer vision tasks and observe a non-commutative nature when applied to both the weights and activations of deep neural networks. Informed by these observations, we articulate the non-commutativity hypothesis: for a given deep neural network being trained for a specific task, there exists an exact training schedule in which quantization and pruning can be introduced to optimize network performance. We identify that this optimal ordering not only exists, but also varies across discriminative and generative tasks. Using the optimal training schedule within our training framework, we demonstrate increased performance per memory footprint over existing solutions.



### The World of an Octopus: How Reporting Bias Influences a Language Model's Perception of Color
- **Arxiv ID**: http://arxiv.org/abs/2110.08182v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08182v1)
- **Published**: 2021-10-15 16:28:17+00:00
- **Updated**: 2021-10-15 16:28:17+00:00
- **Authors**: Cory Paik, Stéphane Aroca-Ouellette, Alessandro Roncone, Katharina Kann
- **Comment**: Accepted to EMNLP 2021, 9 Pages
- **Journal**: None
- **Summary**: Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that reporting bias, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset (CoDa), a dataset of human-perceived color distributions for 521 common objects; 2) use CoDa to analyze and compare the color distribution found in text, the distribution captured by language models, and a human's perception of color; and 3) investigate the performance differences between text-only and multimodal models on CoDa. Our results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that reporting bias negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research.



### Crop Rotation Modeling for Deep Learning-Based Parcel Classification from Satellite Time Series
- **Arxiv ID**: http://arxiv.org/abs/2110.08187v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2110.08187v2)
- **Published**: 2021-10-15 16:38:41+00:00
- **Updated**: 2021-11-16 12:36:42+00:00
- **Authors**: Félix Quinton, Loic Landrieu
- **Comment**: Published in Remote Sensing
- **Journal**: None
- **Summary**: While annual crop rotations play a crucial role for agricultural optimization, they have been largely ignored for automated crop type mapping. In this paper, we take advantage of the increasing quantity of annotated satellite data to propose the first deep learning approach modeling simultaneously the inter- and intra-annual agricultural dynamics of parcel classification. Along with simple training adjustments, our model provides an improvement of over 6.3 mIoU points over the current state-of-the-art of crop classification. Furthermore, we release the first large-scale multi-year agricultural dataset with over 300,000 annotated parcels.



### Guided Point Contrastive Learning for Semi-supervised Point Cloud Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.08188v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08188v1)
- **Published**: 2021-10-15 16:38:54+00:00
- **Updated**: 2021-10-15 16:38:54+00:00
- **Authors**: Li Jiang, Shaoshuai Shi, Zhuotao Tian, Xin Lai, Shu Liu, Chi-Wing Fu, Jiaya Jia
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Rapid progress in 3D semantic segmentation is inseparable from the advances of deep network models, which highly rely on large-scale annotated data for training. To address the high cost and challenges of 3D point-level labeling, we present a method for semi-supervised point cloud semantic segmentation to adopt unlabeled point clouds in training to boost the model performance. Inspired by the recent contrastive loss in self-supervised tasks, we propose the guided point contrastive loss to enhance the feature representation and model generalization ability in semi-supervised setting. Semantic predictions on unlabeled point clouds serve as pseudo-label guidance in our loss to avoid negative pairs in the same category. Also, we design the confidence guidance to ensure high-quality feature learning. Besides, a category-balanced sampling strategy is proposed to collect positive and negative samples to mitigate the class imbalance problem. Extensive experiments on three datasets (ScanNet V2, S3DIS, and SemanticKITTI) show the effectiveness of our semi-supervised method to improve the prediction quality with unlabeled data.



### Attention meets Geometry: Geometry Guided Spatial-Temporal Attention for Consistent Self-Supervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.08192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08192v1)
- **Published**: 2021-10-15 16:43:31+00:00
- **Updated**: 2021-10-15 16:43:31+00:00
- **Authors**: Patrick Ruhkamp, Daoyi Gao, Hanzhi Chen, Nassir Navab, Benjamin Busam
- **Comment**: Accepted at 3DV 2021
- **Journal**: None
- **Summary**: Inferring geometrically consistent dense 3D scenes across a tuple of temporally consecutive images remains challenging for self-supervised monocular depth prediction pipelines. This paper explores how the increasingly popular transformer architecture, together with novel regularized loss formulations, can improve depth consistency while preserving accuracy. We propose a spatial attention module that correlates coarse depth predictions to aggregate local geometric information. A novel temporal attention mechanism further processes the local geometric information in a global context across consecutive images. Additionally, we introduce geometric constraints between frames regularized by photometric cycle consistency. By combining our proposed regularization and the novel spatial-temporal-attention module we fully leverage both the geometric and appearance-based consistency across monocular frames. This yields geometrically meaningful attention and improves temporal depth stability and accuracy compared to previous methods.



### Shared Visual Representations of Drawing for Communication: How do different biases affect human interpretability and intent?
- **Arxiv ID**: http://arxiv.org/abs/2110.08203v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08203v2)
- **Published**: 2021-10-15 17:02:34+00:00
- **Updated**: 2022-01-20 13:08:02+00:00
- **Authors**: Daniela Mihai, Jonathon Hare
- **Comment**: None
- **Journal**: 3rd Workshop on Shared Visual Representations in Human and Machine
  Intelligence (SVRHM 2021) of the Neural Information Processing Systems
  (NeurIPS) conference
- **Summary**: We present an investigation into how representational losses can affect the drawings produced by artificial agents playing a communication game. Building upon recent advances, we show that a combination of powerful pretrained encoder networks, with appropriate inductive biases, can lead to agents that draw recognisable sketches, whilst still communicating well. Further, we start to develop an approach to help automatically analyse the semantic content being conveyed by a sketch and demonstrate that current approaches to inducing perceptual biases lead to a notion of objectness being a key feature despite the agent training being self-supervised.



### Combining Diverse Feature Priors
- **Arxiv ID**: http://arxiv.org/abs/2110.08220v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08220v2)
- **Published**: 2021-10-15 17:31:10+00:00
- **Updated**: 2022-07-14 22:43:44+00:00
- **Authors**: Saachi Jain, Dimitris Tsipras, Aleksander Madry
- **Comment**: None
- **Journal**: None
- **Summary**: To improve model generalization, model designers often restrict the features that their models use, either implicitly or explicitly. In this work, we explore the design space of leveraging such feature priors by viewing them as distinct perspectives on the data. Specifically, we find that models trained with diverse sets of feature priors have less overlapping failure modes, and can thus be combined more effectively. Moreover, we demonstrate that jointly training such models on additional (unlabeled) data allows them to correct each other's mistakes, which, in turn, leads to better generalization and resilience to spurious correlations. Code available at https://github.com/MadryLab/copriors



### Guiding Visual Question Generation
- **Arxiv ID**: http://arxiv.org/abs/2110.08226v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08226v3)
- **Published**: 2021-10-15 17:38:08+00:00
- **Updated**: 2022-07-26 14:22:13+00:00
- **Authors**: Nihir Vedd, Zixu Wang, Marek Rei, Yishu Miao, Lucia Specia
- **Comment**: 14 pages including references and Appendix. 3 figures and 4 tables
- **Journal**: None
- **Summary**: In traditional Visual Question Generation (VQG), most images have multiple concepts (e.g. objects and categories) for which a question could be generated, but models are trained to mimic an arbitrary choice of concept as given in their training data. This makes training difficult and also poses issues for evaluation -- multiple valid questions exist for most images but only one or a few are captured by the human references. We present Guiding Visual Question Generation - a variant of VQG which conditions the question generator on categorical information based on expectations on the type of question and the objects it should explore. We propose two variants: (i) an explicitly guided model that enables an actor (human or automated) to select which objects and categories to generate a question for; and (ii) an implicitly guided model that learns which objects and categories to condition on, based on discrete latent variables. The proposed models are evaluated on an answer-category augmented VQA dataset and our quantitative results show a substantial improvement over the current state of the art (over 9 BLEU-4 increase). Human evaluation validates that guidance helps the generation of questions that are grammatically coherent and relevant to the given image and objects.



### Fire Together Wire Together: A Dynamic Pruning Approach with Self-Supervised Mask Prediction
- **Arxiv ID**: http://arxiv.org/abs/2110.08232v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.08232v4)
- **Published**: 2021-10-15 17:39:53+00:00
- **Updated**: 2022-06-29 01:40:15+00:00
- **Authors**: Sara Elkerdawy, Mostafa Elhoushi, Hong Zhang, Nilanjan Ray
- **Comment**: None
- **Journal**: None
- **Summary**: Dynamic model pruning is a recent direction that allows for the inference of a different sub-network for each input sample during deployment. However, current dynamic methods rely on learning a continuous channel gating through regularization by inducing sparsity loss. This formulation introduces complexity in balancing different losses (e.g task loss, regularization loss). In addition, regularization based methods lack transparent tradeoff hyperparameter selection to realize a computational budget. Our contribution is two-fold: 1) decoupled task and pruning losses. 2) Simple hyperparameter selection that enables FLOPs reduction estimation before training. Inspired by the Hebbian theory in Neuroscience: "neurons that fire together wire together", we propose to predict a mask to process k filters in a layer based on the activation of its previous layer. We pose the problem as a self-supervised binary classification problem. Each mask predictor module is trained to predict if the log-likelihood for each filter in the current layer belongs to the top-k activated filters. The value k is dynamically estimated for each input based on a novel criterion using the mass of heatmaps. We show experiments on several neural architectures, such as VGG, ResNet and MobileNet on CIFAR and ImageNet datasets. On CIFAR, we reach similar accuracy to SOTA methods with 15% and 24% higher FLOPs reduction. Similarly in ImageNet, we achieve lower drop in accuracy with up to 13% improvement in FLOPs reduction.



### Neural Dubber: Dubbing for Videos According to Scripts
- **Arxiv ID**: http://arxiv.org/abs/2110.08243v3
- **DOI**: None
- **Categories**: **eess.AS**, cs.CL, cs.CV, cs.LG, cs.SD, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08243v3)
- **Published**: 2021-10-15 17:56:07+00:00
- **Updated**: 2022-03-15 14:37:46+00:00
- **Authors**: Chenxu Hu, Qiao Tian, Tingle Li, Yuping Wang, Yuxuan Wang, Hang Zhao
- **Comment**: Accepted by NeurIPS 2021; Project page at
  https://tsinghua-mars-lab.github.io/NeuralDubber/
- **Journal**: None
- **Summary**: Dubbing is a post-production process of re-recording actors' dialogues, which is extensively used in filmmaking and video production. It is usually performed manually by professional voice actors who read lines with proper prosody, and in synchronization with the pre-recorded videos. In this work, we propose Neural Dubber, the first neural network model to solve a novel automatic video dubbing (AVD) task: synthesizing human speech synchronized with the given video from the text. Neural Dubber is a multi-modal text-to-speech (TTS) model that utilizes the lip movement in the video to control the prosody of the generated speech. Furthermore, an image-based speaker embedding (ISE) module is developed for the multi-speaker setting, which enables Neural Dubber to generate speech with a reasonable timbre according to the speaker's face. Experiments on the chemistry lecture single-speaker dataset and LRS2 multi-speaker dataset show that Neural Dubber can generate speech audios on par with state-of-the-art TTS models in terms of speech quality. Most importantly, both qualitative and quantitative evaluations show that Neural Dubber can control the prosody of synthesized speech by the video, and generate high-fidelity speech temporally synchronized with the video. Our project page is at https://tsinghua-mars-lab.github.io/NeuralDubber/ .



### Performance, Successes and Limitations of Deep Learning Semantic Segmentation of Multiple Defects in Transmission Electron Micrographs
- **Arxiv ID**: http://arxiv.org/abs/2110.08244v1
- **DOI**: None
- **Categories**: **cs.CV**, cond-mat.mtrl-sci
- **Links**: [PDF](http://arxiv.org/pdf/2110.08244v1)
- **Published**: 2021-10-15 17:57:59+00:00
- **Updated**: 2021-10-15 17:57:59+00:00
- **Authors**: Ryan Jacobs, Mingren Shen, Yuhan Liu, Wei Hao, Xiaoshan Li, Ruoyu He, Jacob RC Greaves, Donglin Wang, Zeming Xie, Zitong Huang, Chao Wang, Kevin G. Field, Dane Morgan
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we perform semantic segmentation of multiple defect types in electron microscopy images of irradiated FeCrAl alloys using a deep learning Mask Regional Convolutional Neural Network (Mask R-CNN) model. We conduct an in-depth analysis of key model performance statistics, with a focus on quantities such as predicted distributions of defect shapes, defect sizes, and defect areal densities relevant to informing modeling and understanding of irradiated Fe-based materials properties. To better understand the performance and present limitations of the model, we provide examples of useful evaluation tests which include a suite of random splits, and dataset size-dependent and domain-targeted cross validation tests. Overall, we find that the current model is a fast, effective tool for automatically characterizing and quantifying multiple defect types in microscopy images, with a level of accuracy on par with human domain expert labelers. More specifically, the model can achieve average defect identification F1 scores as high as 0.8, and, based on random cross validation, have low overall average (+/- standard deviation) defect size and density percentage errors of 7.3 (+/- 3.8)% and 12.7 (+/- 5.3)%, respectively. Further, our model predicts the expected material hardening to within 10-20 MPa (about 10% of total hardening), which is about the same error level as experiments. Our targeted evaluation tests also suggest the best path toward improving future models is not expanding existing databases with more labeled images but instead data additions that target weak points of the model domain, such as images from different microscopes, imaging conditions, irradiation environments, and alloy types. Finally, we discuss the first phase of an effort to provide an easy-to-use, open-source object detection tool to the broader community for identifying defects in new images.



### 3D Human Pose Estimation for Free-form Activity Using WiFi Signals
- **Arxiv ID**: http://arxiv.org/abs/2110.08314v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2110.08314v1)
- **Published**: 2021-10-15 18:47:16+00:00
- **Updated**: 2021-10-15 18:47:16+00:00
- **Authors**: Yili Ren, Jie Yang
- **Comment**: None
- **Journal**: None
- **Summary**: WiFi human sensing has become increasingly attractive in enabling emerging human-computer interaction applications. The corresponding technique has gradually evolved from the classification of multiple activity types to more fine-grained tracking of 3D human poses. However, existing WiFi-based 3D human pose tracking is limited to a set of predefined activities. In this work, we present Winect, a 3D human pose tracking system for free-form activity using commodity WiFi devices. Our system tracks free-form activity by estimating a 3D skeleton pose that consists of a set of joints of the human body. In particular, we combine signal separation and joint movement modeling to achieve free-form activity tracking. Our system first identifies the moving limbs by leveraging the two-dimensional angle of arrival of the signals reflected off the human body and separates the entangled signals for each limb. Then, it tracks each limb and constructs a 3D skeleton of the body by modeling the inherent relationship between the movements of the limb and the corresponding joints. Our evaluation results show that Winect is environment-independent and achieves centimeter-level accuracy for free-form activity tracking under various challenging environments including the none-line-of-sight (NLoS) scenarios.



### Solving Image PDEs with a Shallow Network
- **Arxiv ID**: http://arxiv.org/abs/2110.08327v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, math.DS
- **Links**: [PDF](http://arxiv.org/pdf/2110.08327v1)
- **Published**: 2021-10-15 19:25:30+00:00
- **Updated**: 2021-10-15 19:25:30+00:00
- **Authors**: Pascal Tom Getreuer, Peyman Milanfar, Xiyang Luo
- **Comment**: 21 pages, 22 figures, references arXiv:1802.06130, arXiv:1711.10700,
  arXiv:1606.01299
- **Journal**: None
- **Summary**: Partial differential equations (PDEs) are typically used as models of physical processes but are also of great interest in PDE-based image processing. However, when it comes to their use in imaging, conventional numerical methods for solving PDEs tend to require very fine grid resolution for stability, and as a result have impractically high computational cost. This work applies BLADE (Best Linear Adaptive Enhancement), a shallow learnable filtering framework, to PDE solving, and shows that the resulting approach is efficient and accurate, operating more reliably at coarse grid resolutions than classical methods. As such, the model can be flexibly used for a wide variety of problems in imaging.



### Trigger Hunting with a Topological Prior for Trojan Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.08335v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2110.08335v2)
- **Published**: 2021-10-15 19:47:00+00:00
- **Updated**: 2022-04-02 04:36:03+00:00
- **Authors**: Xiaoling Hu, Xiao Lin, Michael Cogswell, Yi Yao, Susmit Jha, Chao Chen
- **Comment**: 17 pages, 10 figures
- **Journal**: None
- **Summary**: Despite their success and popularity, deep neural networks (DNNs) are vulnerable when facing backdoor attacks. This impedes their wider adoption, especially in mission critical applications. This paper tackles the problem of Trojan detection, namely, identifying Trojaned models -- models trained with poisoned data. One popular approach is reverse engineering, i.e., recovering the triggers on a clean image by manipulating the model's prediction. One major challenge of reverse engineering approach is the enormous search space of triggers. To this end, we propose innovative priors such as diversity and topological simplicity to not only increase the chances of finding the appropriate triggers but also improve the quality of the found triggers. Moreover, by encouraging a diverse set of trigger candidates, our method can perform effectively in cases with unknown target labels. We demonstrate that these priors can significantly improve the quality of the recovered triggers, resulting in substantially improved Trojan detection accuracy as validated on both synthetic and publicly available TrojAI benchmarks.



### Video-Data Pipelines for Machine Learning Applications
- **Arxiv ID**: http://arxiv.org/abs/2110.11407v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2110.11407v1)
- **Published**: 2021-10-15 20:28:56+00:00
- **Updated**: 2021-10-15 20:28:56+00:00
- **Authors**: Sohini Roychowdhury, James Y. Sato
- **Comment**: 10 pages, 6 Figures, 5 Tables, conference
- **Journal**: None
- **Summary**: Data pipelines are an essential component for end-to-end solutions that take machine learning algorithms to production. Engineering data pipelines for video-sequences poses several challenges including isolation of key-frames from video sequences that are high quality and represent significant variations in the scene. Manual isolation of such quality key-frames can take hours of sifting through hours worth of video data. In this work, we present a data pipeline framework that can automate this process of manual frame sifting in video sequences by controlling the fraction of frames that can be removed based on image quality and content type. Additionally, the frames that are retained can be automatically tagged per sequence, thereby simplifying the process of automated data retrieval for future ML model deployments. We analyze the performance of the proposed video-data pipeline for versioned deployment and monitoring for object detection algorithms that are trained on outdoor autonomous driving video sequences. The proposed video-data pipeline can retain anywhere between 0.1-20% of the all input frames that are representative of high image quality and high variations in content. This frame selection, automated scene tagging followed by model verification can be completed in under 30 seconds for 22 video-sequences under analysis in this work. Thus, the proposed framework can be scaled to additional video-sequence data sets for automating ML versioned deployments.



### Counting Objects by Diffused Index: geometry-free and training-free approach
- **Arxiv ID**: http://arxiv.org/abs/2110.08365v1
- **DOI**: 10.1016/j.jvcir.2022.103527
- **Categories**: **cs.CV**, cs.NA, math.NA, 65Z05(Primary), 65S05(Secondary), I.4.9; I.4.6; I.5.5
- **Links**: [PDF](http://arxiv.org/pdf/2110.08365v1)
- **Published**: 2021-10-15 20:53:37+00:00
- **Updated**: 2021-10-15 20:53:37+00:00
- **Authors**: Mengyi Tang, Maryam Yashtini, Sung Ha Kang
- **Comment**: None
- **Journal**: None
- **Summary**: Counting objects is a fundamental but challenging problem. In this paper, we propose diffusion-based, geometry-free, and learning-free methodologies to count the number of objects in images. The main idea is to represent each object by a unique index value regardless of its intensity or size, and to simply count the number of index values. First, we place different vectors, refer to as seed vectors, uniformly throughout the mask image. The mask image has boundary information of the objects to be counted. Secondly, the seeds are diffused using an edge-weighted harmonic variational optimization model within each object. We propose an efficient algorithm based on an operator splitting approach and alternating direction minimization method, and theoretical analysis of this algorithm is given. An optimal solution of the model is obtained when the distributed seeds are completely diffused such that there is a unique intensity within each object, which we refer to as an index. For computational efficiency, we stop the diffusion process before a full convergence, and propose to cluster these diffused index values. We refer to this approach as Counting Objects by Diffused Index (CODI). We explore scalar and multi-dimensional seed vectors. For Scalar seeds, we use Gaussian fitting in histogram to count, while for vector seeds, we exploit a high-dimensional clustering method for the final step of counting via clustering. The proposed method is flexible even if the boundary of the object is not clear nor fully enclosed. We present counting results in various applications such as biological cells, agriculture, concert crowd, and transportation. Some comparisons with existing methods are presented.



### Starkit: RoboCup Humanoid KidSize 2021 Worldwide Champion Team Paper
- **Arxiv ID**: http://arxiv.org/abs/2110.08377v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, 68T40, D.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2110.08377v1)
- **Published**: 2021-10-15 21:34:03+00:00
- **Updated**: 2021-10-15 21:34:03+00:00
- **Authors**: Egor Davydenko, Ivan Khokhlov, Vladimir Litvinenko, Ilya Ryakin, Ilya Osokin, Azer Babaev
- **Comment**: 15 pages, 10 figures
- **Journal**: None
- **Summary**: This article is devoted to the features that were under development between RoboCup 2019 Sydney and RoboCup 2021 Worldwide. These features include vision-related matters, such as detection and localization, mechanical and algorithmic novelties. Since the competition was held virtually, the simulation-specific features are also considered in the article. We give an overview of the approaches that were tried out along with the analysis of their preconditions, perspectives and the evaluation of their performance.



### Comparing Human and Machine Bias in Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.08396v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.08396v2)
- **Published**: 2021-10-15 22:26:20+00:00
- **Updated**: 2021-10-25 15:30:11+00:00
- **Authors**: Samuel Dooley, Ryan Downing, George Wei, Nathan Shankar, Bradon Thymes, Gudrun Thorkelsdottir, Tiye Kurtz-Miott, Rachel Mattson, Olufemi Obiwumi, Valeriia Cherepanova, Micah Goldblum, John P Dickerson, Tom Goldstein
- **Comment**: None
- **Journal**: None
- **Summary**: Much recent research has uncovered and discussed serious concerns of bias in facial analysis technologies, finding performance disparities between groups of people based on perceived gender, skin type, lighting condition, etc. These audits are immensely important and successful at measuring algorithmic bias but have two major challenges: the audits (1) use facial recognition datasets which lack quality metadata, like LFW and CelebA, and (2) do not compare their observed algorithmic bias to the biases of their human alternatives. In this paper, we release improvements to the LFW and CelebA datasets which will enable future researchers to obtain measurements of algorithmic bias that are not tainted by major flaws in the dataset (e.g. identical images appearing in both the gallery and test set). We also use these new data to develop a series of challenging facial identification and verification questions that we administered to various algorithms and a large, balanced sample of human reviewers. We find that both computer models and human survey participants perform significantly better at the verification task, generally obtain lower accuracy rates on dark-skinned or female subjects for both tasks, and obtain higher accuracy rates when their demographics match that of the question. Computer models are observed to achieve a higher level of accuracy than the survey participants on both tasks and exhibit bias to similar degrees as the human survey participants.



### Mind the Gap: Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.08398v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08398v2)
- **Published**: 2021-10-15 22:32:12+00:00
- **Updated**: 2021-11-28 23:47:36+00:00
- **Authors**: Peihao Zhu, Rameen Abdal, John Femiani, Peter Wonka
- **Comment**: Video: https://youtu.be/RLBJ-mem9gM
- **Journal**: None
- **Summary**: We present a new method for one shot domain adaptation. The input to our method is trained GAN that can produce images in domain A and a single reference image I_B from domain B. The proposed algorithm can translate any output of the trained GAN from domain A to domain B. There are two main advantages of our method compared to the current state of the art: First, our solution achieves higher visual quality, e.g. by noticeably reducing overfitting. Second, our solution allows for more degrees of freedom to control the domain gap, i.e. what aspects of image I_B are used to define the domain B. Technically, we realize the new method by building on a pre-trained StyleGAN generator as GAN and a pre-trained CLIP model for representing the domain gap. We propose several new regularizers for controlling the domain gap to optimize the weights of the pre-trained StyleGAN generator to output images in domain B instead of domain A. The regularizers prevent the optimization from taking on too many attributes of the single reference image. Our results show significant visual improvements over the state of the art as well as multiple applications that highlight improved control.



### Bridging the gap between paired and unpaired medical image translation
- **Arxiv ID**: http://arxiv.org/abs/2110.08407v1
- **DOI**: 10.1007/978-3-030-88210-5_4
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08407v1)
- **Published**: 2021-10-15 23:15:12+00:00
- **Updated**: 2021-10-15 23:15:12+00:00
- **Authors**: Pauliina Paavilainen, Saad Ullah Akram, Juho Kannala
- **Comment**: Deep Generative Models for MICCAI (DGM4MICCAI) workshop 2021
- **Journal**: None
- **Summary**: Medical image translation has the potential to reduce the imaging workload, by removing the need to capture some sequences, and to reduce the annotation burden for developing machine learning methods. GANs have been used successfully to translate images from one domain to another, such as MR to CT. At present, paired data (registered MR and CT images) or extra supervision (e.g. segmentation masks) is needed to learn good translation models. Registering multiple modalities or annotating structures within each of them is a tedious and laborious task. Thus, there is a need to develop improved translation methods for unpaired data. Here, we introduce modified pix2pix models for tasks CT$\rightarrow$MR and MR$\rightarrow$CT, trained with unpaired CT and MR data, and MRCAT pairs generated from the MR scans. The proposed modifications utilize the paired MR and MRCAT images to ensure good alignment between input and translated images, and unpaired CT images ensure the MR$\rightarrow$CT model produces realistic-looking CT and CT$\rightarrow$MR model works well with real CT as input. The proposed pix2pix variants outperform baseline pix2pix, pix2pixHD and CycleGAN in terms of FID and KID, and generate more realistic looking CT and MR translations.



