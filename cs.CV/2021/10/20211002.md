# Arxiv Papers in cs.CV on 2021-10-02
### Light Field Saliency Detection with Dual Local Graph Learning andReciprocative Guidance
- **Arxiv ID**: http://arxiv.org/abs/2110.00698v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.00698v1)
- **Published**: 2021-10-02 00:54:39+00:00
- **Updated**: 2021-10-02 00:54:39+00:00
- **Authors**: Nian Liu, Wangbo Zhao, Dingwen Zhang, Junwei Han, Ling Shao
- **Comment**: Accepted by International Conference on Computer Vision (ICCV'21)
- **Journal**: None
- **Summary**: The application of light field data in salient object de-tection is becoming increasingly popular recently. The diffi-culty lies in how to effectively fuse the features within the fo-cal stack and how to cooperate them with the feature of theall-focus image. Previous methods usually fuse focal stackfeatures via convolution or ConvLSTM, which are both lesseffective and ill-posed. In this paper, we model the infor-mation fusion within focal stack via graph networks. Theyintroduce powerful context propagation from neighbouringnodes and also avoid ill-posed implementations. On the onehand, we construct local graph connections thus avoidingprohibitive computational costs of traditional graph net-works. On the other hand, instead of processing the twokinds of data separately, we build a novel dual graph modelto guide the focal stack fusion process using all-focus pat-terns. To handle the second difficulty, previous methods usu-ally implement one-shot fusion for focal stack and all-focusfeatures, hence lacking a thorough exploration of their sup-plements. We introduce a reciprocative guidance schemeand enable mutual guidance between these two kinds of in-formation at multiple steps. As such, both kinds of featurescan be enhanced iteratively, finally benefiting the saliencyprediction. Extensive experimental results show that theproposed models are all beneficial and we achieve signif-icantly better results than state-of-the-art methods.



### Universal Adversarial Spoofing Attacks against Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.00708v1
- **DOI**: 10.1109/IJCB52358.2021.9484380
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.00708v1)
- **Published**: 2021-10-02 02:11:22+00:00
- **Updated**: 2021-10-02 02:11:22+00:00
- **Authors**: Takuma Amada, Seng Pei Liew, Kazuya Kakizaki, Toshinori Araki
- **Comment**: Accepted to International Joint Conference on Biometrics (IJCB 2021)
- **Journal**: None
- **Summary**: We assess the vulnerabilities of deep face recognition systems for images that falsify/spoof multiple identities simultaneously. We demonstrate that, by manipulating the deep feature representation extracted from a face image via imperceptibly small perturbations added at the pixel level using our proposed Universal Adversarial Spoofing Examples (UAXs), one can fool a face verification system into recognizing that the face image belongs to multiple different identities with a high success rate. One characteristic of the UAXs crafted with our method is that they are universal (identity-agnostic); they are successful even against identities not known in advance. For a certain deep neural network, we show that we are able to spoof almost all tested identities (99\%), including those not known beforehand (not included in training). Our results indicate that a multiple-identity attack is a real threat and should be taken into account when deploying face recognition systems.



### Asking questions on handwritten document collections
- **Arxiv ID**: http://arxiv.org/abs/2110.00711v1
- **DOI**: 10.1007/s10032-021-00383-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.00711v1)
- **Published**: 2021-10-02 02:40:40+00:00
- **Updated**: 2021-10-02 02:40:40+00:00
- **Authors**: Minesh Mathew, Lluis Gomez, Dimosthenis Karatzas, CV Jawahar
- **Comment**: pre-print version
- **Journal**: journal = {Int. J. Document Anal. Recognit.}, volume = {24},
  number = {3}, pages = {235--249}, year = {2021}
- **Summary**: This work addresses the problem of Question Answering (QA) on handwritten document collections. Unlike typical QA and Visual Question Answering (VQA) formulations where the answer is a short text, we aim to locate a document snippet where the answer lies. The proposed approach works without recognizing the text in the documents. We argue that the recognition-free approach is suitable for handwritten documents and historical collections where robust text recognition is often difficult. At the same time, for human users, document image snippets containing answers act as a valid alternative to textual answers. The proposed approach uses an off-the-shelf deep embedding network which can project both textual words and word images into a common sub-space. This embedding bridges the textual and visual domains and helps us retrieve document snippets that potentially answer a question. We evaluate results of the proposed approach on two new datasets: (i) HW-SQuAD: a synthetic, handwritten document image counterpart of SQuAD1.0 dataset and (ii) BenthamQA: a smaller set of QA pairs defined on documents from the popular Bentham manuscripts collection. We also present a thorough analysis of the proposed recognition-free approach compared to a recognition-based approach which uses text recognized from the images using an OCR. Datasets presented in this work are available to download at docvqa.org



### An Optimization-Based Meta-Learning Model for MRI Reconstruction with Diverse Dataset
- **Arxiv ID**: http://arxiv.org/abs/2110.00715v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2110.00715v1)
- **Published**: 2021-10-02 03:21:52+00:00
- **Updated**: 2021-10-02 03:21:52+00:00
- **Authors**: Wanyu Bian, Yunmei Chen, Xiaojing Ye, Qingchao Zhang
- **Comment**: 27 pages
- **Journal**: None
- **Summary**: Purpose: This work aims at developing a generalizable MRI reconstruction model in the meta-learning framework. The standard benchmarks in meta-learning are challenged by learning on diverse task distributions. The proposed network learns the regularization function in a variational model and reconstructs MR images with various under-sampling ratios or patterns that may or may not be seen in the training data by leveraging a heterogeneous dataset. Methods: We propose an unrolling network induced by learnable optimization algorithms (LOA) for solving our nonconvex nonsmooth variational model for MRI reconstruction. In this model, the learnable regularization function contains a task-invariant common feature encoder and task-specific learner represented by a shallow network. To train the network we split the training data into two parts: training and validation, and introduce a bilevel optimization algorithm. The lower-level optimization trains task-invariant parameters for the feature encoder with fixed parameters of the task-specific learner on the training dataset, and the upper-level optimizes the parameters of the task-specific learner on the validation dataset. Results: The average PSNR increases significantly compared to the network trained through conventional supervised learning on the seen CS ratios. We test the result of quick adaption on the unseen tasks after meta-training and in the meanwhile saving half of the training time; Conclusion: We proposed a meta-learning framework consisting of the base network architecture, design of regularization, and bi-level optimization-based training. The network inherits the convergence property of the LOA and interpretation of the variational model. The generalization ability is improved by the designated regularization and bilevel optimization-based training algorithm.



### Domain-Specific Bias Filtering for Single Labeled Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2110.00726v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.00726v3)
- **Published**: 2021-10-02 05:08:01+00:00
- **Updated**: 2022-11-22 14:02:02+00:00
- **Authors**: Junkun Yuan, Xu Ma, Defang Chen, Kun Kuang, Fei Wu, Lanfen Lin
- **Comment**: Accepted by International Journal of Computer Vision (IJCV)
- **Journal**: None
- **Summary**: Conventional Domain Generalization (CDG) utilizes multiple labeled source datasets to train a generalizable model for unseen target domains. However, due to expensive annotation costs, the requirements of labeling all the source data are hard to be met in real-world applications. In this paper, we investigate a Single Labeled Domain Generalization (SLDG) task with only one source domain being labeled, which is more practical and challenging than the CDG task. A major obstacle in the SLDG task is the discriminability-generalization bias: the discriminative information in the labeled source dataset may contain domain-specific bias, constraining the generalization of the trained model. To tackle this challenging task, we propose a novel framework called Domain-Specific Bias Filtering (DSBF), which initializes a discriminative model with the labeled source data and then filters out its domain-specific bias with the unlabeled source data for generalization improvement. We divide the filtering process into (1) feature extractor debiasing via k-means clustering-based semantic feature re-extraction and (2) classifier rectification through attention-guided semantic feature projection. DSBF unifies the exploration of the labeled and the unlabeled source data to enhance the discriminability and generalization of the trained model, resulting in a highly generalizable model. We further provide theoretical analysis to verify the proposed domain-specific bias filtering process. Extensive experiments on multiple datasets show the superior performance of DSBF in tackling both the challenging SLDG task and the CDG task.



### FICGAN: Facial Identity Controllable GAN for De-identification
- **Arxiv ID**: http://arxiv.org/abs/2110.00740v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.00740v1)
- **Published**: 2021-10-02 07:09:27+00:00
- **Updated**: 2021-10-02 07:09:27+00:00
- **Authors**: Yonghyun Jeong, Jooyoung Choi, Sungwon Kim, Youngmin Ro, Tae-Hyun Oh, Doyeon Kim, Heonseok Ha, Sungroh Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present Facial Identity Controllable GAN (FICGAN) for not only generating high-quality de-identified face images with ensured privacy protection, but also detailed controllability on attribute preservation for enhanced data utility. We tackle the less-explored yet desired functionality in face de-identification based on the two factors. First, we focus on the challenging issue to obtain a high level of privacy protection in the de-identification task while uncompromising the image quality. Second, we analyze the facial attributes related to identity and non-identity and explore the trade-off between the degree of face de-identification and preservation of the source attributes for enhanced data utility. Based on the analysis, we develop Facial Identity Controllable GAN (FICGAN), an autoencoder-based conditional generative model that learns to disentangle the identity attributes from non-identity attributes on a face image. By applying the manifold k-same algorithm to satisfy k-anonymity for strengthened security, our method achieves enhanced privacy protection in de-identified face images. Numerous experiments demonstrate that our model outperforms others in various scenarios of face de-identification.



### Using Out-of-the-Box Frameworks for Contrastive Unpaired Image Translation for Vestibular Schwannoma and Cochlea Segmentation: An approach for the crossMoDA Challenge
- **Arxiv ID**: http://arxiv.org/abs/2110.01607v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01607v3)
- **Published**: 2021-10-02 08:04:46+00:00
- **Updated**: 2021-12-08 03:54:00+00:00
- **Authors**: Jae Won Choi
- **Comment**: 9 pages, 3 figures, MICCAI 2021 Cross-Modality Domain Adaptation for
  Medical Image Segmentation Challenge; extended manuscript and modified
  results
- **Journal**: None
- **Summary**: The purpose of this study is to apply and evaluate out-of-the-box deep learning frameworks for the crossMoDA challenge. We use the CUT model, a model for unpaired image-to-image translation based on patchwise contrastive learning and adversarial learning, for domain adaptation from contrast-enhanced T1 MR to high-resolution T2 MR. As data augmentation, we generate additional images with vestibular schwannomas with lower signal intensity. For the segmentation task, we use the nnU-Net framework. Our final submission achieved mean Dice scores of 0.8299 in the validation phase and 0.8253 in the test phase. Our method ranked 3rd in the crossMoDA challenge.



### Explainable Event Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.00755v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.00755v2)
- **Published**: 2021-10-02 08:40:33+00:00
- **Updated**: 2021-10-10 12:27:27+00:00
- **Authors**: Imran Khan, Kashif Ahmad, Namra Gul, Talhat Khan, Nasir Ahmad, Ala Al-Fuqaha
- **Comment**: 16 pages, 10 figures, 6 tables
- **Journal**: None
- **Summary**: The literature shows outstanding capabilities for CNNs in event recognition in images. However, fewer attempts are made to analyze the potential causes behind the decisions of the models and exploring whether the predictions are based on event-salient objects or regions? To explore this important aspect of event recognition, in this work, we propose an explainable event recognition framework relying on Grad-CAM and an Xception architecture-based CNN model. Experiments are conducted on three large-scale datasets covering a diversified set of natural disasters, social, and sports events. Overall, the model showed outstanding generalization capabilities obtaining overall F1-scores of 0.91, 0.94, and 0.97 on natural disasters, social, and sports events, respectively. Moreover, for subjective analysis of activation maps generated through Grad-CAM for the predicted samples of the model, a crowdsourcing study is conducted to analyze whether the model's predictions are based on event-related objects/regions or not? The results of the study indicate that 78%, 84%, and 78% of the model decisions on natural disasters, sports, and social events datasets, respectively, are based onevent-related objects or regions.



### Automated Seed Quality Testing System using GAN & Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.00777v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/2110.00777v1)
- **Published**: 2021-10-02 10:28:25+00:00
- **Updated**: 2021-10-02 10:28:25+00:00
- **Authors**: Sandeep Nagar, Prateek Pani, Raj Nair, Girish Varma
- **Comment**: None
- **Journal**: 9th International Conference on Pattern Recognition and Machine
  Intelligence 2021
- **Summary**: Quality assessment of agricultural produce is a crucial step in minimizing food stock wastage. However, this is currently done manually and often requires expert supervision, especially in smaller seeds like corn. We propose a novel computer vision-based system for automating this process. We build a novel seed image acquisition setup, which captures both the top and bottom views. Dataset collection for this problem has challenges of data annotation costs/time and class imbalance. We address these challenges by i.) using a Conditional Generative Adversarial Network (CGAN) to generate real-looking images for the classes with lesser images and ii.) annotate a large dataset with minimal expert human intervention by using a Batch Active Learning (BAL) based annotation tool. We benchmark different image classification models on the dataset obtained. We are able to get accuracies of up to 91.6% for testing the physical purity of seed samples.



### Seeking Visual Discomfort: Curiosity-driven Representations for Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.00784v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, I.2.6; I.2.8; I.2.9; I.2.10; I.5.0
- **Links**: [PDF](http://arxiv.org/pdf/2110.00784v1)
- **Published**: 2021-10-02 11:15:04+00:00
- **Updated**: 2021-10-02 11:15:04+00:00
- **Authors**: Elie Aljalbout, Maximilian Ulmer, Rudolph Triebel
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2109.13588
- **Journal**: None
- **Summary**: Vision-based reinforcement learning (RL) is a promising approach to solve control tasks involving images as the main observation. State-of-the-art RL algorithms still struggle in terms of sample efficiency, especially when using image observations. This has led to increased attention on integrating state representation learning (SRL) techniques into the RL pipeline. Work in this field demonstrates a substantial improvement in sample efficiency among other benefits. However, to take full advantage of this paradigm, the quality of samples used for training plays a crucial role. More importantly, the diversity of these samples could affect the sample efficiency of vision-based RL, but also its generalization capability. In this work, we present an approach to improve sample diversity for state representation learning. Our method enhances the exploration capability of RL algorithms, by taking advantage of the SRL setup. Our experiments show that our proposed approach boosts the visitation of problematic states, improves the learned state representation, and outperforms the baselines for all tested environments. These results are most apparent for environments where the baseline methods struggle. Even in simple environments, our method stabilizes the training, reduces the reward variance, and promotes sample efficiency.



### Inference-InfoGAN: Inference Independence via Embedding Orthogonal Basis Expansion
- **Arxiv ID**: http://arxiv.org/abs/2110.00788v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.00788v1)
- **Published**: 2021-10-02 11:54:23+00:00
- **Updated**: 2021-10-02 11:54:23+00:00
- **Authors**: Hongxiang Jiang, Jihao Yin, Xiaoyan Luo, Fuxiang Wang
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Disentanglement learning aims to construct independent and interpretable latent variables in which generative models are a popular strategy. InfoGAN is a classic method via maximizing Mutual Information (MI) to obtain interpretable latent variables mapped to the target space. However, it did not emphasize independent characteristic. To explicitly infer latent variables with inter-independence, we propose a novel GAN-based disentanglement framework via embedding Orthogonal Basis Expansion (OBE) into InfoGAN network (Inference-InfoGAN) in an unsupervised way. Under the OBE module, one set of orthogonal basis can be adaptively found to expand arbitrary data with independence property. To ensure the target-wise interpretable representation, we add a consistence constraint between the expansion coefficients and latent variables on the base of MI maximization. Additionally, we design an alternating optimization step on the consistence constraint and orthogonal requirement updating, so that the training of Inference-InfoGAN can be more convenient. Finally, experiments validate that our proposed OBE module obtains adaptive orthogonal basis, which can express better independent characteristics than fixed basis expression of Discrete Cosine Transform (DCT). To depict the performance in downstream tasks, we compared with the state-of-the-art GAN-based and even VAE-based approaches on different datasets. Our Inference-InfoGAN achieves higher disentanglement score in terms of FactorVAE, Separated Attribute Predictability (SAP), Mutual Information Gap (MIG) and Variation Predictability (VP) metrics without model fine-tuning. All the experimental results illustrate that our method has inter-independence inference ability because of the OBE module, and provides a good trade-off between it and target-wise interpretability of latent variables via jointing the alternating optimization.



### Optimizing Neural Network for Computer Vision task in Edge Device
- **Arxiv ID**: http://arxiv.org/abs/2110.00791v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.00791v1)
- **Published**: 2021-10-02 12:25:18+00:00
- **Updated**: 2021-10-02 12:25:18+00:00
- **Authors**: Ranjith M S, S Parameshwara, Pavan Yadav A, Shriganesh Hegde
- **Comment**: None
- **Journal**: None
- **Summary**: The field of computer vision has grown very rapidly in the past few years due to networks like convolution neural networks and their variants. The memory required to store the model and computational expense are very high for such a network limiting it to deploy on the edge device. Many times, applications rely on the cloud but that makes it hard for working in real-time due to round-trip delays. We overcome these problems by deploying the neural network on the edge device itself. The computational expense for edge devices is reduced by reducing the floating-point precision of the parameters in the model. After this the memory required for the model decreases and the speed of the computation increases where the performance of the model is least affected. This makes an edge device to predict from the neural network all by itself.



### Welsch Based Multiview Disparity Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.00803v1
- **DOI**: 10.1109/ICIP42928.2021.9506766
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.00803v1)
- **Published**: 2021-10-02 13:44:49+00:00
- **Updated**: 2021-10-02 13:44:49+00:00
- **Authors**: James L. Gray, Aous T. Naman, David S. Taubman
- **Comment**: Published in 2021 IEEE International Conference on Image Processing
  (ICIP), 5 pages
- **Journal**: 2021 IEEE International Conference on Image Processing (ICIP),
  2021, pp. 3223-3227,
- **Summary**: In this work, we explore disparity estimation from a high number of views. We experimentally identify occlusions as a key challenge for disparity estimation for applications with high numbers of views. In particular, occlusions can actually result in a degradation in accuracy as more views are added to a dataset. We propose the use of a Welsch loss function for the data term in a global variational framework for disparity estimation. We also propose a disciplined warping strategy and a progressive inclusion of views strategy that can reduce the need for coarse to fine strategies that discard high spatial frequency components from the early iterations. Experimental results demonstrate that the proposed approach produces superior and/or more robust estimates than other conventional variational approaches.



### ProTo: Program-Guided Transformer for Program-Guided Tasks
- **Arxiv ID**: http://arxiv.org/abs/2110.00804v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.00804v2)
- **Published**: 2021-10-02 13:46:32+00:00
- **Updated**: 2021-10-16 02:14:06+00:00
- **Authors**: Zelin Zhao, Karan Samel, Binghong Chen, Le Song
- **Comment**: Accepted in NeurIPS 2021
- **Journal**: None
- **Summary**: Programs, consisting of semantic and structural information, play an important role in the communication between humans and agents. Towards learning general program executors to unify perception, reasoning, and decision making, we formulate program-guided tasks which require learning to execute a given program on the observed task specification. Furthermore, we propose the Program-guided Transformer (ProTo), which integrates both semantic and structural guidance of a program by leveraging cross-attention and masked self-attention to pass messages between the specification and routines in the program. ProTo executes a program in a learned latent space and enjoys stronger representation ability than previous neural-symbolic approaches. We demonstrate that ProTo significantly outperforms the previous state-of-the-art methods on GQA visual reasoning and 2D Minecraft policy learning datasets. Additionally, ProTo demonstrates better generalization to unseen, complex, and human-written programs.



### Implicit and Explicit Attention for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.00860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.00860v1)
- **Published**: 2021-10-02 18:06:21+00:00
- **Updated**: 2021-10-02 18:06:21+00:00
- **Authors**: Faisal Alamri, Anjan Dutta
- **Comment**: GCPR 2021
- **Journal**: None
- **Summary**: Most of the existing Zero-Shot Learning (ZSL) methods focus on learning a compatibility function between the image representation and class attributes. Few others concentrate on learning image representation combining local and global features. However, the existing approaches still fail to address the bias issue towards the seen classes. In this paper, we propose implicit and explicit attention mechanisms to address the existing bias problem in ZSL models. We formulate the implicit attention mechanism with a self-supervised image angle rotation task, which focuses on specific image features aiding to solve the task. The explicit attention mechanism is composed with the consideration of a multi-headed self-attention mechanism via Vision Transformer model, which learns to map image features to semantic space during the training stage. We conduct comprehensive experiments on three popular benchmarks: AWA2, CUB and SUN. The performance of our proposed attention mechanisms has proved its effectiveness, and has achieved the state-of-the-art harmonic mean on all the three datasets.



### BdSL36: A Dataset for Bangladeshi Sign Letters Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.00869v1
- **DOI**: 10.1007/978-3-030-69756-3_6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.00869v1)
- **Published**: 2021-10-02 19:52:48+00:00
- **Updated**: 2021-10-02 19:52:48+00:00
- **Authors**: Oishee Bintey Hoque, Mohammad Imrul Jubair, Al-Farabi Akash, Saiful Islam
- **Comment**: Proceedings of the Asian Conference on Computer Vision (ACCV)
  Workshops, 2020
- **Journal**: None
- **Summary**: Bangladeshi Sign Language (BdSL) is a commonly used medium of communication for the hearing-impaired people in Bangladesh. A real-time BdSL interpreter with no controlled lab environment has a broad social impact and an interesting avenue of research as well. Also, it is a challenging task due to the variation in different subjects (age, gender, color, etc.), complex features, and similarities of signs and clustered backgrounds. However, the existing dataset for BdSL classification task is mainly built in a lab friendly setup which limits the application of powerful deep learning technology. In this paper, we introduce a dataset named BdSL36 which incorporates background augmentation to make the dataset versatile and contains over four million images belonging to 36 categories. Besides, we annotate about 40,000 images with bounding boxes to utilize the potentiality of object detection algorithms. Furthermore, several intensive experiments are performed to establish the baseline performance of our BdSL36. Moreover, we employ beta testing of our classifiers at the user level to justify the possibilities of real-world application with this dataset. We believe our BdSL36 will expedite future research on practical sign letter classification. We make the datasets and all the pre-trained models available for further researcher.



### Weakly Supervised Attention-based Models Using Activation Maps for Citrus Mite and Insect Pest Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.00881v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.00881v1)
- **Published**: 2021-10-02 21:42:22+00:00
- **Updated**: 2021-10-02 21:42:22+00:00
- **Authors**: Edson Bollis, Helena Maia, Helio Pedrini, Sandra Avila
- **Comment**: 18 pages, 9 figures, 5 tables. Paper under review
- **Journal**: None
- **Summary**: Citrus juices and fruits are commodities with great economic potential in the international market, but productivity losses caused by mites and other pests are still far from being a good mark. Despite the integrated pest mechanical aspect, only a few works on automatic classification have handled images with orange mite characteristics, which means tiny and noisy regions of interest. On the computational side, attention-based models have gained prominence in deep learning research, and, along with weakly supervised learning algorithms, they have improved tasks performed with some label restrictions. In agronomic research of pests and diseases, these techniques can improve classification performance while pointing out the location of mites and insects without specific labels, reducing deep learning development costs related to generating bounding boxes. In this context, this work proposes an attention-based activation map approach developed to improve the classification of tiny regions called Two-Weighted Activation Mapping, which also produces locations using feature map scores learned from class labels. We apply our method in a two-stage network process called Attention-based Multiple Instance Learning Guided by Saliency Maps. We analyze the proposed approach in two challenging datasets, the Citrus Pest Benchmark, which was captured directly in the field using magnifying glasses, and the Insect Pest, a large pest image benchmark. In addition, we evaluate and compare our models with weakly supervised methods, such as Attention-based Deep MIL and WILDCAT. The results show that our classifier is superior to literature methods that use tiny regions in their classification tasks, surpassing them in all scenarios by at least 16 percentage points. Moreover, our approach infers bounding box locations for salient insects, even training without any location labels.



