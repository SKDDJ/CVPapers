# Arxiv Papers in cs.CV on 2021-10-20
### R$^3$Net:Relation-embedded Representation Reconstruction Network for Change Captioning
- **Arxiv ID**: http://arxiv.org/abs/2110.10328v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10328v1)
- **Published**: 2021-10-20 00:57:39+00:00
- **Updated**: 2021-10-20 00:57:39+00:00
- **Authors**: Yunbin Tu, Liang Li, Chenggang Yan, Shengxiang Gao, Zhengtao Yu
- **Comment**: Accepted by EMNLP 2021
- **Journal**: None
- **Summary**: Change captioning is to use a natural language sentence to describe the fine-grained disagreement between two similar images. Viewpoint change is the most typical distractor in this task, because it changes the scale and location of the objects and overwhelms the representation of real change. In this paper, we propose a Relation-embedded Representation Reconstruction Network (R$^3$Net) to explicitly distinguish the real change from the large amount of clutter and irrelevant changes. Specifically, a relation-embedded module is first devised to explore potential changed objects in the large amount of clutter. Then, based on the semantic similarities of corresponding locations in the two images, a representation reconstruction module (RRM) is designed to learn the reconstruction representation and further model the difference representation. Besides, we introduce a syntactic skeleton predictor (SSP) to enhance the semantic interaction between change localization and caption generation. Extensive experiments show that the proposed method achieves the state-of-the-art results on two public datasets.



### AI-Based Detection, Classification and Prediction/Prognosis in Medical Imaging: Towards Radiophenomics
- **Arxiv ID**: http://arxiv.org/abs/2110.10332v4
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10332v4)
- **Published**: 2021-10-20 01:05:47+00:00
- **Updated**: 2022-01-13 20:52:08+00:00
- **Authors**: Fereshteh Yousefirizi, Pierre Decazes, Amine Amyar, Su Ruan, Babak Saboury, Arman Rahmim
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial intelligence (AI) techniques have significant potential to enable effective, robust and automated image phenotyping including identification of subtle patterns. AI-based detection searches the image space to find the regions of interest based on patterns and features. There is a spectrum of tumor histologies from benign to malignant that can be identified by AI-based classification approaches using image features. The extraction of minable information from images gives way to the field of radiomics and can be explored via explicit (handcrafted/engineered) and deep radiomics frameworks. Radiomics analysis has the potential to be utilized as a noninvasive technique for the accurate characterization of tumors to improve diagnosis and treatment monitoring. This work reviews AI-based techniques, with a special focus on oncological PET and PET/CT imaging, for different detection, classification, and prediction/prognosis tasks. We also discuss needed efforts to enable the translation of AI techniques to routine clinical workflows, and potential improvements and complementary techniques such as the use of natural language processing on electronic health records and neuro-symbolic AI techniques.



### Toward Accurate and Reliable Iris Segmentation Using Uncertainty Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.10334v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10334v2)
- **Published**: 2021-10-20 01:37:19+00:00
- **Updated**: 2021-11-18 01:23:35+00:00
- **Authors**: Jianze Wei, Huaibo Huang, Muyi Sun, Yunlong Wang, Min Ren, Ran He, Zhenan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Iris segmentation is a deterministic part of the iris recognition system. Unreliable segmentation of iris regions especially the limbic area is still the bottleneck problem, which impedes more accurate recognition. To make further efforts on accurate and reliable iris segmentation, we propose a bilateral self-attention module and design Bilateral Transformer (BiTrans) with hierarchical architecture by exploring spatial and visual relationships. The bilateral self-attention module adopts a spatial branch to capture spatial contextual information without resolution reduction and a visual branch with a large receptive field to extract the visual contextual features. BiTrans actively applies convolutional projections and cross-attention to improve spatial perception and hierarchical feature fusion. Besides, Iris Segmentation Uncertainty Learning is developed to learn the uncertainty map according to prediction discrepancy. With the estimated uncertainty, a weighting scheme and a regularization term are designed to reduce predictive uncertainty. More importantly, the uncertainty estimate reflects the reliability of the segmentation predictions. Experimental results on three publicly available databases demonstrate that the proposed approach achieves better segmentation performance using 20% FLOPs of the SOTA IrisParseNet.



### Simpler Does It: Generating Semantic Labels with Objectness Guidance
- **Arxiv ID**: http://arxiv.org/abs/2110.10335v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10335v1)
- **Published**: 2021-10-20 01:52:05+00:00
- **Updated**: 2021-10-20 01:52:05+00:00
- **Authors**: Md Amirul Islam, Matthew Kowal, Sen Jia, Konstantinos G. Derpanis, Neil D. B. Bruce
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Existing weakly or semi-supervised semantic segmentation methods utilize image or box-level supervision to generate pseudo-labels for weakly labeled images. However, due to the lack of strong supervision, the generated pseudo-labels are often noisy near the object boundaries, which severely impacts the network's ability to learn strong representations. To address this problem, we present a novel framework that generates pseudo-labels for training images, which are then used to train a segmentation model. To generate pseudo-labels, we combine information from: (i) a class agnostic objectness network that learns to recognize object-like regions, and (ii) either image-level or bounding box annotations. We show the efficacy of our approach by demonstrating how the objectness network can naturally be leveraged to generate object-like regions for unseen categories. We then propose an end-to-end multi-task learning strategy, that jointly learns to segment semantics and objectness using the generated pseudo-labels. Extensive experiments demonstrate the high quality of our generated pseudo-labels and effectiveness of the proposed framework in a variety of domains. Our approach achieves better or competitive performance compared to existing weakly-supervised and semi-supervised methods.



### EBJR: Energy-Based Joint Reasoning for Adaptive Inference
- **Arxiv ID**: http://arxiv.org/abs/2110.10343v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10343v1)
- **Published**: 2021-10-20 02:33:31+00:00
- **Updated**: 2021-10-20 02:33:31+00:00
- **Authors**: Mohammad Akbari, Amin Banitalebi-Dehkordi, Yong Zhang
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: State-of-the-art deep learning models have achieved significant performance levels on various benchmarks. However, the excellent performance comes at a cost of inefficient computational cost. Light-weight architectures, on the other hand, achieve moderate accuracies, but at a much more desirable latency. This paper presents a new method of jointly using the large accurate models together with the small fast ones. To this end, we propose an Energy-Based Joint Reasoning (EBJR) framework that adaptively distributes the samples between shallow and deep models to achieve an accuracy close to the deep model, but latency close to the shallow one. Our method is applicable to out-of-the-box pre-trained models as it does not require an architecture change nor re-training. Moreover, it is easy to use and deploy, especially for cloud services. Through a comprehensive set of experiments on different down-stream tasks, we show that our method outperforms strong state-of-the-art approaches with a considerable margin. In addition, we propose specialized EBJR, an extension of our method where we create a smaller specialized side model that performs the target task only partially, but yields an even higher accuracy and faster inference. We verify the strengths of our methods with both theoretical and experimental evaluations.



### GTM: Gray Temporal Model for Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.10348v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10348v1)
- **Published**: 2021-10-20 02:45:48+00:00
- **Updated**: 2021-10-20 02:45:48+00:00
- **Authors**: Yanping Zhang, Yongxin Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Data input modality plays an important role in video action recognition. Normally, there are three types of input: RGB, flow stream and compressed data. In this paper, we proposed a new input modality: gray stream. Specifically, taken the stacked consecutive 3 gray images as input, which is the same size of RGB, can not only skip the conversion process from video decoding data to RGB, but also improve the spatio-temporal modeling ability at zero computation and zero parameters. Meanwhile, we proposed a 1D Identity Channel-wise Spatio-temporal Convolution(1D-ICSC) which captures the temporal relationship at channel-feature level within a controllable computation budget(by parameters G & R). Finally, we confirm its effectiveness and efficiency on several action recognition benchmarks, such as Kinetics, Something-Something, HMDB-51 and UCF-101, and achieve impressive results.



### Contextual Gradient Scaling for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.10353v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10353v1)
- **Published**: 2021-10-20 03:05:58+00:00
- **Updated**: 2021-10-20 03:05:58+00:00
- **Authors**: Sanghyuk Lee, Seunghyun Lee, Byung Cheol Song
- **Comment**: Accepted to WACV2022
- **Journal**: None
- **Summary**: Model-agnostic meta-learning (MAML) is a well-known optimization-based meta-learning algorithm that works well in various computer vision tasks, e.g., few-shot classification. MAML is to learn an initialization so that a model can adapt to a new task in a few steps. However, since the gradient norm of a classifier (head) is much bigger than those of backbone layers, the model focuses on learning the decision boundary of the classifier with similar representations. Furthermore, gradient norms of high-level layers are small than those of the other layers. So, the backbone of MAML usually learns task-generic features, which results in deteriorated adaptation performance in the inner-loop. To resolve or mitigate this problem, we propose contextual gradient scaling (CxGrad), which scales gradient norms of the backbone to facilitate learning task-specific knowledge in the inner-loop. Since the scaling factors are generated from task-conditioned parameters, gradient norms of the backbone can be scaled in a task-wise fashion. Experimental results show that CxGrad effectively encourages the backbone to learn task-specific knowledge in the inner-loop and improves the performance of MAML up to a significant margin in both same- and cross-domain few-shot classification.



### Detecting Backdoor Attacks Against Point Cloud Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2110.10354v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10354v1)
- **Published**: 2021-10-20 03:12:06+00:00
- **Updated**: 2021-10-20 03:12:06+00:00
- **Authors**: Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, George Kesidis
- **Comment**: None
- **Journal**: None
- **Summary**: Backdoor attacks (BA) are an emerging threat to deep neural network classifiers. A classifier being attacked will predict to the attacker's target class when a test sample from a source class is embedded with the backdoor pattern (BP). Recently, the first BA against point cloud (PC) classifiers was proposed, creating new threats to many important applications including autonomous driving. Such PC BAs are not detectable by existing BA defenses due to their special BP embedding mechanism. In this paper, we propose a reverse-engineering defense that infers whether a PC classifier is backdoor attacked, without access to its training set or to any clean classifiers for reference. The effectiveness of our defense is demonstrated on the benchmark ModeNet40 dataset for PCs.



### Dynamic Multi-Person Mesh Recovery From Uncalibrated Multi-View Cameras
- **Arxiv ID**: http://arxiv.org/abs/2110.10355v1
- **DOI**: 10.1109/3DV53792.2021.00080
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10355v1)
- **Published**: 2021-10-20 03:19:20+00:00
- **Updated**: 2021-10-20 03:19:20+00:00
- **Authors**: Buzhen Huang, Yuan Shu, Tianshu Zhang, Yangang Wang
- **Comment**: 3DV 2021
- **Journal**: None
- **Summary**: Dynamic multi-person mesh recovery has been a hot topic in 3D vision recently. However, few works focus on the multi-person motion capture from uncalibrated cameras, which mainly faces two challenges: the one is that inter-person interactions and occlusions introduce inherent ambiguities for both camera calibration and motion capture; The other is that a lack of dense correspondences can be used to constrain sparse camera geometries in a dynamic multi-person scene. Our key idea is incorporating motion prior knowledge into simultaneous optimization of extrinsic camera parameters and human meshes from noisy human semantics. First, we introduce a physics-geometry consistency to reduce the low and high frequency noises of the detected human semantics. Then a novel latent motion prior is proposed to simultaneously optimize extrinsic camera parameters and coherent human motions from slightly noisy inputs. Experimental results show that accurate camera parameters and human motions can be obtained through one-stage optimization. The codes will be publicly available at~\url{https://www.yangangwang.com}.



### NOD: Taking a Closer Look at Detection under Extreme Low-Light Conditions with Night Object Detection Dataset
- **Arxiv ID**: http://arxiv.org/abs/2110.10364v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10364v1)
- **Published**: 2021-10-20 03:44:04+00:00
- **Updated**: 2021-10-20 03:44:04+00:00
- **Authors**: Igor Morawski, Yu-An Chen, Yu-Sheng Lin, Winston H. Hsu
- **Comment**: 13 pages, 6 figures, to be published in BMVC 2021
- **Journal**: None
- **Summary**: Recent work indicates that, besides being a challenge in producing perceptually pleasing images, low light proves more difficult for machine cognition than previously thought. In our work, we take a closer look at object detection in low light. First, to support the development and evaluation of new methods in this domain, we present a high-quality large-scale Night Object Detection (NOD) dataset showing dynamic scenes captured on the streets at night. Next, we directly link the lighting conditions to perceptual difficulty and identify what makes low light problematic for machine cognition. Accordingly, we provide instance-level annotation for a subset of the dataset for an in-depth evaluation of future methods. We also present an analysis of the baseline model performance to highlight opportunities for future research and show that low light is a non-trivial problem that requires special attention from the researchers. Further, to address the issues caused by low light, we propose to incorporate an image enhancement module into the object detection framework and two novel data augmentation techniques. Our image enhancement module is trained under the guidance of the object detector to learn image representation optimal for machine cognition rather than for the human visual system. Finally, experimental results confirm that the proposed method shows consistent improvement of the performance on low-light datasets.



### Repaint: Improving the Generalization of Down-Stream Visual Tasks by Generating Multiple Instances of Training Examples
- **Arxiv ID**: http://arxiv.org/abs/2110.10366v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10366v1)
- **Published**: 2021-10-20 04:00:47+00:00
- **Updated**: 2021-10-20 04:00:47+00:00
- **Authors**: Amin Banitalebi-Dehkordi, Yong Zhang
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) for visual tasks are believed to learn both the low-level textures and high-level object attributes, throughout the network depth. This paper further investigates the `texture bias' in CNNs. To this end, we regenerate multiple instances of training examples from each original image, through a process we call `repainting'. The repainted examples preserve the shape and structure of the regions and objects within the scenes, but diversify their texture and color. Our method can regenerate a same image at different daylight, season, or weather conditions, can have colorization or de-colorization effects, or even bring back some texture information from blacked-out areas. The in-place repaint allows us to further use these repainted examples for improving the generalization of CNNs. Through an extensive set of experiments, we demonstrate the usefulness of the repainted examples in training, for the tasks of image classification (ImageNet) and object detection (COCO), over several state-of-the-art network architectures at different capacities, and across different data availability regimes.



### ABC: Auxiliary Balanced Classifier for Class-imbalanced Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.10368v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10368v1)
- **Published**: 2021-10-20 04:07:48+00:00
- **Updated**: 2021-10-20 04:07:48+00:00
- **Authors**: Hyuck Lee, Seungjae Shin, Heeyoung Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Existing semi-supervised learning (SSL) algorithms typically assume class-balanced datasets, although the class distributions of many real-world datasets are imbalanced. In general, classifiers trained on a class-imbalanced dataset are biased toward the majority classes. This issue becomes more problematic for SSL algorithms because they utilize the biased prediction of unlabeled data for training. However, traditional class-imbalanced learning techniques, which are designed for labeled data, cannot be readily combined with SSL algorithms. We propose a scalable class-imbalanced SSL algorithm that can effectively use unlabeled data, while mitigating class imbalance by introducing an auxiliary balanced classifier (ABC) of a single layer, which is attached to a representation layer of an existing SSL algorithm. The ABC is trained with a class-balanced loss of a minibatch, while using high-quality representations learned from all data points in the minibatch using the backbone SSL algorithm to avoid overfitting and information loss.Moreover, we use consistency regularization, a recent SSL technique for utilizing unlabeled data in a modified way, to train the ABC to be balanced among the classes by selecting unlabeled data with the same probability for each class. The proposed algorithm achieves state-of-the-art performance in various class-imbalanced SSL experiments using four benchmark datasets.



### Model Composition: Can Multiple Neural Networks Be Combined into a Single Network Using Only Unlabeled Data?
- **Arxiv ID**: http://arxiv.org/abs/2110.10369v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10369v1)
- **Published**: 2021-10-20 04:17:25+00:00
- **Updated**: 2021-10-20 04:17:25+00:00
- **Authors**: Amin Banitalebi-Dehkordi, Xinyu Kang, Yong Zhang
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: The diversity of deep learning applications, datasets, and neural network architectures necessitates a careful selection of the architecture and data that match best to a target application. As an attempt to mitigate this dilemma, this paper investigates the idea of combining multiple trained neural networks using unlabeled data. In addition, combining multiple models into one can speed up the inference, result in stronger, more capable models, and allows us to select efficient device-friendly target network architectures. To this end, the proposed method makes use of generation, filtering, and aggregation of reliable pseudo-labels collected from unlabeled data. Our method supports using an arbitrary number of input models with arbitrary architectures and categories. Extensive performance evaluations demonstrated that our method is very effective. For example, for the task of object detection and without using any ground-truth labels, an EfficientDet-D0 trained on Pascal-VOC and an EfficientDet-D1 trained on COCO, can be combined to a RetinaNet-ResNet50 model, with a similar mAP as the supervised training. If fine-tuned in a semi-supervised setting, the combined model achieves +18.6%, +12.6%, and +8.1% mAP improvements over supervised training with 1%, 5%, and 10% of labels.



### Medical Knowledge-Guided Deep Curriculum Learning for Elbow Fracture Diagnosis from X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2110.10381v1
- **DOI**: 10.1117/12.2582184
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10381v1)
- **Published**: 2021-10-20 05:24:35+00:00
- **Updated**: 2021-10-20 05:24:35+00:00
- **Authors**: Jun Luo, Gene Kitamura, Emine Doganay, Dooman Arefan, Shandong Wu
- **Comment**: SPIE Medical Imaging 2021. DOI: https://doi.org/10.1117/12.2582184.
  URL:
  https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11597/1159712/Medical-knowledge-guided-deep-curriculum-learning-for-elbow-fracture-diagnosis/10.1117/12.2582184.short?SSO=1
- **Journal**: SPIE Medical Imaging 2021: Computer-Aided Diagnosis
- **Summary**: Elbow fractures are one of the most common fracture types. Diagnoses on elbow fractures often need the help of radiographic imaging to be read and analyzed by a specialized radiologist with years of training. Thanks to the recent advances of deep learning, a model that can classify and detect different types of bone fractures needs only hours of training and has shown promising results. However, most existing deep learning models are purely data-driven, lacking incorporation of known domain knowledge from human experts. In this work, we propose a novel deep learning method to diagnose elbow fracture from elbow X-ray images by integrating domain-specific medical knowledge into a curriculum learning framework. In our method, the training data are permutated by sampling without replacement at the beginning of each training epoch. The sampling probability of each training sample is guided by a scoring criterion constructed based on clinically known knowledge from human experts, where the scoring indicates the diagnosis difficultness of different elbow fracture subtypes. We also propose an algorithm that updates the sampling probabilities at each epoch, which is applicable to other sampling-based curriculum learning frameworks. We design an experiment with 1865 elbow X-ray images for a fracture/normal binary classification task and compare our proposed method to a baseline method and a previous method using multiple metrics. Our results show that the proposed method achieves the highest classification performance. Also, our proposed probability update algorithm boosts the performance of the previous method.



### Knowledge-Guided Multiview Deep Curriculum Learning for Elbow Fracture Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.10383v1
- **DOI**: 10.1007/978-3-030-87589-3_57
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10383v1)
- **Published**: 2021-10-20 05:42:20+00:00
- **Updated**: 2021-10-20 05:42:20+00:00
- **Authors**: Jun Luo, Gene Kitamura, Dooman Arefan, Emine Doganay, Ashok Panigrahy, Shandong Wu
- **Comment**: MICCAI 2021 workshop. DOI:
  https://doi.org/10.1007/978-3-030-87589-3_57. URL:
  https://link.springer.com/chapter/10.1007/978-3-030-87589-3_57
- **Journal**: In International Workshop on Machine Learning in Medical Imaging
  (pp. 555-564) at MICCAI 2021. Springer, Cham
- **Summary**: Elbow fracture diagnosis often requires patients to take both frontal and lateral views of elbow X-ray radiographs. In this paper, we propose a multiview deep learning method for an elbow fracture subtype classification task. Our strategy leverages transfer learning by first training two single-view models, one for frontal view and the other for lateral view, and then transferring the weights to the corresponding layers in the proposed multiview network architecture. Meanwhile, quantitative medical knowledge was integrated into the training process through a curriculum learning framework, which enables the model to first learn from "easier" samples and then transition to "harder" samples to reach better performance. In addition, our multiview network can work both in a dual-view setting and with a single view as input. We evaluate our method through extensive experiments on a classification task of elbow fracture with a dataset of 1,964 images. Results show that our method outperforms two related methods on bone fracture study in multiple settings, and our technique is able to boost the performance of the compared methods. The code is available at https://github.com/ljaiverson/multiview-curriculum.



### Does Data Repair Lead to Fair Models? Curating Contextually Fair Data To Reduce Model Bias
- **Arxiv ID**: http://arxiv.org/abs/2110.10389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10389v1)
- **Published**: 2021-10-20 06:00:03+00:00
- **Updated**: 2021-10-20 06:00:03+00:00
- **Authors**: Sharat Agarwal, Sumanyu Muku, Saket Anand, Chetan Arora
- **Comment**: A variant of this report is accepted in WACV 2022
- **Journal**: None
- **Summary**: Contextual information is a valuable cue for Deep Neural Networks (DNNs) to learn better representations and improve accuracy. However, co-occurrence bias in the training dataset may hamper a DNN model's generalizability to unseen scenarios in the real world. For example, in COCO, many object categories have a much higher co-occurrence with men compared to women, which can bias a DNN's prediction in favor of men. Recent works have focused on task-specific training strategies to handle bias in such scenarios, but fixing the available data is often ignored. In this paper, we propose a novel and more generic solution to address the contextual bias in the datasets by selecting a subset of the samples, which is fair in terms of the co-occurrence with various classes for a protected attribute. We introduce a data repair algorithm using the coefficient of variation, which can curate fair and contextually balanced data for a protected class(es). This helps in training a fair model irrespective of the task, architecture or training methodology. Our proposed solution is simple, effective, and can even be used in an active learning setting where the data labels are not present or being generated incrementally. We demonstrate the effectiveness of our algorithm for the task of object detection and multi-label image classification across different datasets. Through a series of experiments, we validate that curating contextually fair data helps make model predictions fair by balancing the true positive rate for the protected class across groups without compromising on the model's overall performance.



### Deep Learning for HDR Imaging: State-of-the-Art and Future Trends
- **Arxiv ID**: http://arxiv.org/abs/2110.10394v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10394v3)
- **Published**: 2021-10-20 06:24:30+00:00
- **Updated**: 2021-11-07 15:17:58+00:00
- **Authors**: Lin Wang, Kuk-Jin Yoon
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI), main and suppl. material
- **Journal**: None
- **Summary**: High dynamic range (HDR) imaging is a technique that allows an extensive dynamic range of exposures, which is important in image processing, computer graphics, and computer vision. In recent years, there has been a significant advancement in HDR imaging using deep learning (DL). This study conducts a comprehensive and insightful survey and analysis of recent developments in deep HDR imaging methodologies. We hierarchically and structurally group existing deep HDR imaging methods into five categories based on (1) number/domain of input exposures, (2) number of learning tasks, (3) novel sensor data, (4) novel learning strategies, and (5) applications. Importantly, we provide a constructive discussion on each category regarding its potential and challenges. Moreover, we review some crucial aspects of deep HDR imaging, such as datasets and evaluation metrics. Finally, we highlight some open problems and point out future research directions.



### 3DFaceFill: An Analysis-By-Synthesis Approach to Face Completion
- **Arxiv ID**: http://arxiv.org/abs/2110.10395v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10395v1)
- **Published**: 2021-10-20 06:31:47+00:00
- **Updated**: 2021-10-20 06:31:47+00:00
- **Authors**: Rahul Dey, Vishnu Boddeti
- **Comment**: Winter Conference on Applications of Computer Vision, WACV 2022
- **Journal**: None
- **Summary**: Existing face completion solutions are primarily driven by end-to-end models that directly generate 2D completions of 2D masked faces. By having to implicitly account for geometric and photometric variations in facial shape and appearance, such approaches result in unrealistic completions, especially under large variations in pose, shape, illumination and mask sizes. To alleviate these limitations, we introduce 3DFaceFill, an analysis-by-synthesis approach for face completion that explicitly considers the image formation process. It comprises three components, (1) an encoder that disentangles the face into its constituent 3D mesh, 3D pose, illumination and albedo factors, (2) an autoencoder that inpaints the UV representation of facial albedo, and (3) a renderer that resynthesizes the completed face. By operating on the UV representation, 3DFaceFill affords the power of correspondence and allows us to naturally enforce geometrical priors (e.g. facial symmetry) more effectively. Quantitatively, 3DFaceFill improves the state-of-the-art by up to 4dB higher PSNR and 25% better LPIPS for large masks. And, qualitatively, it leads to demonstrably more photorealistic face completions over a range of masks and occlusions while preserving consistency in global and component-wise shape, pose, illumination and eye-gaze.



### AFTer-UNet: Axial Fusion Transformer UNet for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.10403v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10403v1)
- **Published**: 2021-10-20 06:47:28+00:00
- **Updated**: 2021-10-20 06:47:28+00:00
- **Authors**: Xiangyi Yan, Hao Tang, Shanlin Sun, Haoyu Ma, Deying Kong, Xiaohui Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in transformer-based models have drawn attention to exploring these techniques in medical image segmentation, especially in conjunction with the U-Net model (or its variants), which has shown great success in medical image segmentation, under both 2D and 3D settings. Current 2D based methods either directly replace convolutional layers with pure transformers or consider a transformer as an additional intermediate encoder between the encoder and decoder of U-Net. However, these approaches only consider the attention encoding within one single slice and do not utilize the axial-axis information naturally provided by a 3D volume. In the 3D setting, convolution on volumetric data and transformers both consume large GPU memory. One has to either downsample the image or use cropped local patches to reduce GPU memory usage, which limits its performance. In this paper, we propose Axial Fusion Transformer UNet (AFTer-UNet), which takes both advantages of convolutional layers' capability of extracting detailed features and transformers' strength on long sequence modeling. It considers both intra-slice and inter-slice long-range cues to guide the segmentation. Meanwhile, it has fewer parameters and takes less GPU memory to train than the previous transformer-based models. Extensive experiments on three multi-organ segmentation datasets demonstrate that our method outperforms current state-of-the-art methods.



### ARTS: Eliminating Inconsistency between Text Detection and Recognition with Auto-Rectification Text Spotter
- **Arxiv ID**: http://arxiv.org/abs/2110.10405v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10405v1)
- **Published**: 2021-10-20 06:53:44+00:00
- **Updated**: 2021-10-20 06:53:44+00:00
- **Authors**: Humen Zhong, Jun Tang, Wenhai Wang, Zhibo Yang, Cong Yao, Tong Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent approaches for end-to-end text spotting have achieved promising results. However, most of the current spotters were plagued by the inconsistency problem between text detection and recognition. In this work, we introduce and prove the existence of the inconsistency problem and analyze it from two aspects: (1) inconsistency of text recognition features between training and testing, and (2) inconsistency of optimization targets between text detection and recognition. To solve the aforementioned issues, we propose a differentiable Auto-Rectification Module (ARM) together with a new training strategy to enable propagating recognition loss back into detection branch, so that our detection branch can be jointly optimized by detection and recognition targets, which largely alleviates the inconsistency problem between text detection and recognition. Based on these designs, we present a simple yet robust end-to-end text spotting framework, termed Auto-Rectification Text Spotter (ARTS), to detect and recognize arbitrarily-shaped text in natural scenes. Extensive experiments demonstrate the superiority of our method. In particular, our ARTS-S achieves 77.1% end-to-end text spotting F-measure on Total-Text at a competitive speed of 10.5 FPS, which significantly outperforms previous methods in both accuracy and inference speed.



### Depth360: Self-supervised Learning for Monocular Depth Estimation using Learnable Camera Distortion Model
- **Arxiv ID**: http://arxiv.org/abs/2110.10415v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.10415v2)
- **Published**: 2021-10-20 07:21:04+00:00
- **Updated**: 2022-02-18 08:19:58+00:00
- **Authors**: Noriaki Hirose, Kosuke Tahara
- **Comment**: 8 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: Self-supervised monocular depth estimation has been widely investigated to estimate depth images and relative poses from RGB images. This framework is attractive for researchers because the depth and pose networks can be trained from just time sequence images without the need for the ground truth depth and poses.   In this work, we estimate the depth around a robot (360 degree view) using time sequence spherical camera images, from a camera whose parameters are unknown. We propose a learnable axisymmetric camera model which accepts distorted spherical camera images with two fisheye camera images. In addition, we trained our models with a photo-realistic simulator to generate ground truth depth images to provide supervision. Moreover, we introduced loss functions to provide floor constraints to reduce artifacts that can result from reflective floor surfaces. We demonstrate the efficacy of our method using the spherical camera images from the GO Stanford dataset and pinhole camera images from the KITTI dataset to compare our method's performance with that of baseline method in learning the camera parameters.



### Semantic Segmentation for Urban-Scene Images
- **Arxiv ID**: http://arxiv.org/abs/2110.13813v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.13813v1)
- **Published**: 2021-10-20 08:31:26+00:00
- **Updated**: 2021-10-20 08:31:26+00:00
- **Authors**: Shorya Sharma
- **Comment**: arXiv admin note: text overlap with arXiv:2003.05128,
  arXiv:1912.03183 by other authors
- **Journal**: None
- **Summary**: Urban-scene Image segmentation is an important and trending topic in computer vision with wide use cases like autonomous driving [1]. Starting with the breakthrough work of Long et al. [2] that introduces Fully Convolutional Networks (FCNs), the development of novel architectures and practical uses of neural networks in semantic segmentation has been expedited in the recent 5 years. Aside from seeking solutions in general model design for information shrinkage due to pooling, urban-scene image itself has intrinsic features like positional patterns [3]. Our project seeks an advanced and integrated solution that specifically targets urban-scene image semantic segmentation among the most novel approaches in the current field. We re-implement the cutting edge model DeepLabv3+ [4] with ResNet-101 [5] backbone as our strong baseline model. Based upon DeepLabv3+, we incorporate HANet [3] to account for the vertical spatial priors in urban-scene image tasks. To boost up model efficiency and performance, we further explore the Atrous Spatial Pooling (ASP) layer in DeepLabv3+ and infuse a computational efficient variation called "Waterfall" Atrous Spatial Pooling (WASP) [6] architecture in our model. We find that our two-step integrated model improves the mean Intersection-Over-Union (mIoU) score gradually from the baseline model. In particular, HANet successfully identifies height-driven patterns and improves per-class IoU of common class labels in urban scenario like fence and bus. We also demonstrate the improvement of model efficiency with help of WASP in terms of computational times during training and parameter reduction from the original ASPP module.



### VLDeformer: Vision-Language Decomposed Transformer for Fast Cross-Modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2110.11338v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2110.11338v3)
- **Published**: 2021-10-20 09:00:51+00:00
- **Updated**: 2021-11-25 03:53:16+00:00
- **Authors**: Lisai Zhang, Hongfa Wu, Qingcai Chen, Yimeng Deng, Zhonghua Li, Dejiang Kong, Zhao Cao, Joanna Siebert, Yunpeng Han
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-model retrieval has emerged as one of the most important upgrades for text-only search engines (SE). Recently, with powerful representation for pairwise text-image inputs via early interaction, the accuracy of vision-language (VL) transformers has outperformed existing methods for text-image retrieval. However, when the same paradigm is used for inference, the efficiency of the VL transformers is still too low to be applied in a real cross-modal SE. Inspired by the mechanism of human learning and using cross-modal knowledge, this paper presents a novel Vision-Language Decomposed Transformer (VLDeformer), which greatly increases the efficiency of VL transformers while maintaining their outstanding accuracy. By the proposed method, the cross-model retrieval is separated into two stages: the VL transformer learning stage, and the VL decomposition stage. The latter stage plays the role of single modal indexing, which is to some extent like the term indexing of a text SE. The model learns cross-modal knowledge from early-interaction pre-training and is then decomposed into an individual encoder. The decomposition requires only small target datasets for supervision and achieves both $1000+$ times acceleration and less than $0.6$\% average recall drop. VLDeformer also outperforms state-of-the-art visual-semantic embedding methods on COCO and Flickr30k.



### A unifying framework for $n$-dimensional quasi-conformal mappings
- **Arxiv ID**: http://arxiv.org/abs/2110.10437v2
- **DOI**: 10.1137/21M1457497
- **Categories**: **cs.CG**, cs.CV, cs.GR, cs.NA, math.DG, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2110.10437v2)
- **Published**: 2021-10-20 09:04:41+00:00
- **Updated**: 2022-03-26 02:51:51+00:00
- **Authors**: Daoping Zhang, Gary P. T. Choi, Jianping Zhang, Lok Ming Lui
- **Comment**: None
- **Journal**: SIAM Journal on Imaging Sciences, 15(2), 960-988 (2022)
- **Summary**: With the advancement of computer technology, there is a surge of interest in effective mapping methods for objects in higher-dimensional spaces. To establish a one-to-one correspondence between objects, higher-dimensional quasi-conformal theory can be utilized for ensuring the bijectivity of the mappings. In addition, it is often desirable for the mappings to satisfy certain prescribed geometric constraints and possess low distortion in conformality or volume. In this work, we develop a unifying framework for computing $n$-dimensional quasi-conformal mappings. More specifically, we propose a variational model that integrates quasi-conformal distortion, volumetric distortion, landmark correspondence, intensity mismatch and volume prior information to handle a large variety of deformation problems. We further prove the existence of a minimizer for the proposed model and devise efficient numerical methods to solve the optimization problem. We demonstrate the effectiveness of the proposed framework using various experiments in two- and three-dimensions, with applications to medical image registration, adaptive remeshing and shape modeling.



### Moiré Attack (MA): A New Potential Risk of Screen Photos
- **Arxiv ID**: http://arxiv.org/abs/2110.10444v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10444v1)
- **Published**: 2021-10-20 09:19:04+00:00
- **Updated**: 2021-10-20 09:19:04+00:00
- **Authors**: Dantong Niu, Ruohao Guo, Yisen Wang
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Images, captured by a camera, play a critical role in training Deep Neural Networks (DNNs). Usually, we assume the images acquired by cameras are consistent with the ones perceived by human eyes. However, due to the different physical mechanisms between human-vision and computer-vision systems, the final perceived images could be very different in some cases, for example shooting on digital monitors. In this paper, we find a special phenomenon in digital image processing, the moir\'e effect, that could cause unnoticed security threats to DNNs. Based on it, we propose a Moir\'e Attack (MA) that generates the physical-world moir\'e pattern adding to the images by mimicking the shooting process of digital devices. Extensive experiments demonstrate that our proposed digital Moir\'e Attack (MA) is a perfect camouflage for attackers to tamper with DNNs with a high success rate ($100.0\%$ for untargeted and $97.0\%$ for targeted attack with the noise budget $\epsilon=4$), high transferability rate across different models, and high robustness under various defenses. Furthermore, MA owns great stealthiness because the moir\'e effect is unavoidable due to the camera's inner physical structure, which therefore hardly attracts the awareness of humans. Our code is available at https://github.com/Dantong88/Moire_Attack.



### Noisy Annotation Refinement for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.10456v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10456v2)
- **Published**: 2021-10-20 09:39:50+00:00
- **Updated**: 2021-12-07 11:42:39+00:00
- **Authors**: Jiafeng Mao, Qing Yu, Yoko Yamakata, Kiyoharu Aizawa
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised training of object detectors requires well-annotated large-scale datasets, whose production is costly. Therefore, some efforts have been made to obtain annotations in economical ways, such as cloud sourcing. However, datasets obtained by these methods tend to contain noisy annotations such as inaccurate bounding boxes and incorrect class labels. In this study, we propose a new problem setting of training object detectors on datasets with entangled noises of annotations of class labels and bounding boxes. Our proposed method efficiently decouples the entangled noises, corrects the noisy annotations, and subsequently trains the detector using the corrected annotations. We verified the effectiveness of our proposed method and compared it with the baseline on noisy datasets with different noise levels. The experimental results show that our proposed method significantly outperforms the baseline.



### Unified Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2110.10481v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2110.10481v1)
- **Published**: 2021-10-20 10:45:38+00:00
- **Updated**: 2021-10-20 10:45:38+00:00
- **Authors**: Guanjie Huang, Hongjian He, Xiang Li, Xingchen Li, Ziang Liu
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Currently, it is hard to compare and evaluate different style transfer algorithms due to chaotic definitions of style and the absence of agreed objective validation methods in the study of style transfer. In this paper, a novel approach, the Unified Style Transfer (UST) model, is proposed. With the introduction of a generative model for internal style representation, UST can transfer images in two approaches, i.e., Domain-based and Image-based, simultaneously. At the same time, a new philosophy based on the human sense of art and style distributions for evaluating the transfer model is presented and demonstrated, called Statistical Style Analysis. It provides a new path to validate style transfer models' feasibility by validating the general consistency between internal style representation and art facts. Besides, the translation-invariance of AdaIN features is also discussed.



### Evaluation of augmentation methods in classifying autism spectrum disorders from fMRI data with 3D convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2110.10489v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2110.10489v1)
- **Published**: 2021-10-20 11:03:17+00:00
- **Updated**: 2021-10-20 11:03:17+00:00
- **Authors**: Johan Jönemo, David Abramian, Anders Eklund
- **Comment**: None
- **Journal**: None
- **Summary**: Classifying subjects as healthy or diseased using neuroimaging data has gained a lot of attention during the last 10 years. Here we apply deep learning to derivatives from resting state fMRI data, and investigate how different 3D augmentation techniques affect the test accuracy. Specifically, we use resting state derivatives from 1,112 subjects in ABIDE preprocessed to train a 3D convolutional neural network (CNN) to perform the classification. Our results show that augmentation only provide minor improvements to the test accuracy.



### Deep Point Cloud Normal Estimation via Triplet Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.10494v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.10494v1)
- **Published**: 2021-10-20 11:16:00+00:00
- **Updated**: 2021-10-20 11:16:00+00:00
- **Authors**: Weijia Wang, Xuequan Lu, Dasith de Silva Edirimuni, Xiao Liu, Antonio Robles-Kelly
- **Comment**: None
- **Journal**: None
- **Summary**: Normal estimation on 3D point clouds is a fundamental problem in 3D vision and graphics. Current methods often show limited accuracy in predicting normals at sharp features (e.g., edges and corners) and less robustness to noise. In this paper, we propose a novel normal estimation method for point clouds. It consists of two phases: (a) feature encoding which learns representations of local patches, and (b) normal estimation that takes the learned representation as input and regresses the normal vector. We are motivated that local patches on isotropic and anisotropic surfaces have similar or distinct normals, and that separable features or representations can be learned to facilitate normal estimation. To realise this, we first construct triplets of local patches on 3D point cloud data, and design a triplet network with a triplet loss for feature encoding. We then design a simple network with several MLPs and a loss function to regress the normal vector. Despite having a smaller network size compared to most other methods, experimental results show that our method preserves sharp features and achieves better normal estimation results on CAD-like shapes.



### STALP: Style Transfer with Auxiliary Limited Pairing
- **Arxiv ID**: http://arxiv.org/abs/2110.10501v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.10501v1)
- **Published**: 2021-10-20 11:38:41+00:00
- **Updated**: 2021-10-20 11:38:41+00:00
- **Authors**: David Futschik, Michal Kučera, Michal Lukáč, Zhaowen Wang, Eli Shechtman, Daniel Sýkora
- **Comment**: Eurographics 2021
- **Journal**: None
- **Summary**: We present an approach to example-based stylization of images that uses a single pair of a source image and its stylized counterpart. We demonstrate how to train an image translation network that can perform real-time semantically meaningful style transfer to a set of target images with similar content as the source image. A key added value of our approach is that it considers also consistency of target images during training. Although those have no stylized counterparts, we constrain the translation to keep the statistics of neural responses compatible with those extracted from the stylized source. In contrast to concurrent techniques that use a similar input, our approach better preserves important visual characteristics of the source style and can deliver temporally stable results without the need to explicitly handle temporal consistency. We demonstrate its practical utility on various applications including video stylization, style transfer to panoramas, faces, and 3D models.



### Event Guided Depth Sensing
- **Arxiv ID**: http://arxiv.org/abs/2110.10505v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10505v2)
- **Published**: 2021-10-20 11:41:11+00:00
- **Updated**: 2021-12-20 11:26:52+00:00
- **Authors**: Manasi Muglikar, Diederik Paul Moeys, Davide Scaramuzza
- **Comment**: None
- **Journal**: International Conference on 3D Vision (3DV), 2021
- **Summary**: Active depth sensors like structured light, lidar, and time-of-flight systems sample the depth of the entire scene uniformly at a fixed scan rate. This leads to limited spatio-temporal resolution where redundant static information is over-sampled and precious motion information might be under-sampled. In this paper, we present an efficient bio-inspired event-camera-driven depth estimation algorithm. In our approach, we dynamically illuminate areas of interest densely, depending on the scene activity detected by the event camera, and sparsely illuminate areas in the field of view with no motion. The depth estimation is achieved by an event-based structured light system consisting of a laser point projector coupled with a second event-based sensor tuned to detect the reflection of the laser from the scene. We show the feasibility of our approach in a simulated autonomous driving scenario and real indoor sequences using our prototype. We show that, in natural scenes like autonomous driving and indoor environments, moving edges correspond to less than 10% of the scene on average. Thus our setup requires the sensor to scan only 10% of the scene, which could lead to almost 90% less power consumption by the illumination source. While we present the evaluation and proof-of-concept for an event-based structured-light system, the ideas presented here are applicable for a wide range of depth-sensing modalities like LIDAR, time-of-flight, and standard stereo. Video is available at \url{https://youtu.be/Rvv9IQLYjCQ}.



### Development and accuracy evaluation of Coded Phase-shift 3D scanner
- **Arxiv ID**: http://arxiv.org/abs/2110.10520v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10520v1)
- **Published**: 2021-10-20 12:16:13+00:00
- **Updated**: 2021-10-20 12:16:13+00:00
- **Authors**: Pranav Kant Gaur, D. M. Sarode, S. K. Bose
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we provide an overview of development of a structured light 3D-scanner based on combination of binary-coded patterns and sinusoidal phase-shifted fringe patterns called Coded Phase-shift technique. Further, we describe the experiments performed to evaluate measurement accuracy and precision of the developed system. A study of this kind is expected to be helpful in understanding the basic working of current structured-light 3D scanners and the approaches followed for their performance assessment.



### Detecting and Identifying Optical Signal Attacks on Autonomous Driving Systems
- **Arxiv ID**: http://arxiv.org/abs/2110.10523v1
- **DOI**: 10.1109/JIOT.2020.3011690
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10523v1)
- **Published**: 2021-10-20 12:21:04+00:00
- **Updated**: 2021-10-20 12:21:04+00:00
- **Authors**: Jindi Zhang, Yifan Zhang, Kejie Lu, Jianping Wang, Kui Wu, Xiaohua Jia, Bin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: For autonomous driving, an essential task is to detect surrounding objects accurately. To this end, most existing systems use optical devices, including cameras and light detection and ranging (LiDAR) sensors, to collect environment data in real time. In recent years, many researchers have developed advanced machine learning models to detect surrounding objects. Nevertheless, the aforementioned optical devices are vulnerable to optical signal attacks, which could compromise the accuracy of object detection. To address this critical issue, we propose a framework to detect and identify sensors that are under attack. Specifically, we first develop a new technique to detect attacks on a system that consists of three sensors. Our main idea is to: 1) use data from three sensors to obtain two versions of depth maps (i.e., disparity) and 2) detect attacks by analyzing the distribution of disparity errors. In our study, we use real data sets and the state-of-the-art machine learning model to evaluate our attack detection scheme and the results confirm the effectiveness of our detection method. Based on the detection scheme, we further develop an identification model that is capable of identifying up to n-2 attacked sensors in a system with one LiDAR and n cameras. We prove the correctness of our identification scheme and conduct experiments to show the accuracy of our identification method. Finally, we investigate the overall sensitivity of our framework.



### AniFormer: Data-driven 3D Animation with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2110.10533v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10533v1)
- **Published**: 2021-10-20 12:36:55+00:00
- **Updated**: 2021-10-20 12:36:55+00:00
- **Authors**: Haoyu Chen, Hao Tang, Nicu Sebe, Guoying Zhao
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: We present a novel task, i.e., animating a target 3D object through the motion of a raw driving sequence. In previous works, extra auxiliary correlations between source and target meshes or intermedia factors are inevitable to capture the motions in the driving sequences. Instead, we introduce AniFormer, a novel Transformer-based architecture, that generates animated 3D sequences by directly taking the raw driving sequences and arbitrary same-type target meshes as inputs. Specifically, we customize the Transformer architecture for 3D animation that generates mesh sequences by integrating styles from target meshes and motions from the driving meshes. Besides, instead of the conventional single regression head in the vanilla Transformer, AniFormer generates multiple frames as outputs to preserve the sequential consistency of the generated meshes. To achieve this, we carefully design a pair of regression constraints, i.e., motion and appearance constraints, that can provide strong regularization on the generated mesh sequences. Our AniFormer achieves high-fidelity, realistic, temporally coherent animated results and outperforms compared start-of-the-art methods on benchmarks of diverse categories. Code is available: https://github.com/mikecheninoulu/AniFormer.



### Improving Model Generalization by Agreement of Learned Representations from Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.10536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10536v1)
- **Published**: 2021-10-20 12:44:52+00:00
- **Updated**: 2021-10-20 12:44:52+00:00
- **Authors**: Rowel Atienza
- **Comment**: Accepted at WACV2022
- **Journal**: None
- **Summary**: Data augmentation reduces the generalization error by forcing a model to learn invariant representations given different transformations of the input image. In computer vision, on top of the standard image processing functions, data augmentation techniques based on regional dropout such as CutOut, MixUp, and CutMix and policy-based selection such as AutoAugment demonstrated state-of-the-art (SOTA) results. With an increasing number of data augmentation algorithms being proposed, the focus is always on optimizing the input-output mapping while not realizing that there might be an untapped value in the transformed images with the same label. We hypothesize that by forcing the representations of two transformations to agree, we can further reduce the model generalization error. We call our proposed method Agreement Maximization or simply AgMax. With this simple constraint applied during training, empirical results show that data augmentation algorithms can further improve the classification accuracy of ResNet50 on ImageNet by up to 1.5%, WideResNet40-2 on CIFAR10 by up to 0.7%, WideResNet40-2 on CIFAR100 by up to 1.6%, and LeNet5 on Speech Commands Dataset by up to 1.4%. Experimental results further show that unlike other regularization terms such as label smoothing, AgMax can take advantage of the data augmentation to consistently improve model generalization by a significant margin. On downstream tasks such as object detection and segmentation on PascalVOC and COCO, AgMax pre-trained models outperforms other data augmentation methods by as much as 1.0mAP (box) and 0.5mAP (mask). Code is available at https://github.com/roatienza/agmax.



### Classification of PS and ABS Black Plastics for WEEE Recycling Applications
- **Arxiv ID**: http://arxiv.org/abs/2110.12896v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12896v1)
- **Published**: 2021-10-20 12:47:18+00:00
- **Updated**: 2021-10-20 12:47:18+00:00
- **Authors**: Anton Persson, Niklas Dymne, Fernando Alonso-Fernandez
- **Comment**: Published at 8th Intl. Conference on Soft Computing & Machine
  Intelligence, ISCMI, Cairo, Egypt 26-27 November 2021
- **Journal**: None
- **Summary**: Pollution and climate change are some of the biggest challenges that humanity is facing. In such a context, efficient recycling is a crucial tool for a sustainable future. This work is aimed at creating a system that can classify different types of plastics by using picture analysis, in particular, black plastics of the type Polystyrene (PS) and Acrylonitrile Butadiene Styrene (ABS). They are two common plastics from Waste from Electrical and Electronic Equipment (WEEE). For this purpose, a Convolutional Neural Network has been tested and retrained, obtaining a validation accuracy of 95%. Using a separate test set, average accuracy goes down to 86.6%, but a further look at the results shows that the ABS type is correctly classified 100% of the time, so it is the PS type that accumulates all the errors. Overall, this demonstrates the feasibility of classifying black plastics using CNN machine learning techniques. It is believed that if a more diverse and extensive image dataset becomes available, a system with higher reliability that generalizes well could be developed using the proposed methodology.



### ASSANet: An Anisotropic Separable Set Abstraction for Efficient Point Cloud Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.10538v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10538v2)
- **Published**: 2021-10-20 12:47:42+00:00
- **Updated**: 2021-10-24 11:11:23+00:00
- **Authors**: Guocheng Qian, Hasan Abed Al Kader Hammoud, Guohao Li, Ali Thabet, Bernard Ghanem
- **Comment**: ASSANet gets accepted to NeurIPS'21 as a Spotlight paper. code
  available at https://github.com/guochengqian/ASSANet
- **Journal**: None
- **Summary**: Access to 3D point cloud representations has been widely facilitated by LiDAR sensors embedded in various mobile devices. This has led to an emerging need for fast and accurate point cloud processing techniques. In this paper, we revisit and dive deeper into PointNet++, one of the most influential yet under-explored networks, and develop faster and more accurate variants of the model. We first present a novel Separable Set Abstraction (SA) module that disentangles the vanilla SA module used in PointNet++ into two separate learning stages: (1) learning channel correlation and (2) learning spatial correlation. The Separable SA module is significantly faster than the vanilla version, yet it achieves comparable performance. We then introduce a new Anisotropic Reduction function into our Separable SA module and propose an Anisotropic Separable SA (ASSA) module that substantially increases the network's accuracy. We later replace the vanilla SA modules in PointNet++ with the proposed ASSA module, and denote the modified network as ASSANet. Extensive experiments on point cloud classification, semantic segmentation, and part segmentation show that ASSANet outperforms PointNet++ and other methods, achieving much higher accuracy and faster speeds. In particular, ASSANet outperforms PointNet++ by $7.4$ mIoU on S3DIS Area 5, while maintaining $1.6 \times $ faster inference speed on a single NVIDIA 2080Ti GPU. Our scaled ASSANet variant achieves $66.8$ mIoU and outperforms KPConv, while being more than $54 \times$ faster.



### Trash or Treasure? An Interactive Dual-Stream Strategy for Single Image Reflection Separation
- **Arxiv ID**: http://arxiv.org/abs/2110.10546v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10546v1)
- **Published**: 2021-10-20 13:05:16+00:00
- **Updated**: 2021-10-20 13:05:16+00:00
- **Authors**: Qiming Hu, Xiaojie Guo
- **Comment**: Accepted to NeurIPS2021
- **Journal**: None
- **Summary**: Single image reflection separation (SIRS), as a representative blind source separation task, aims to recover two layers, $\textit{i.e.}$, transmission and reflection, from one mixed observation, which is challenging due to the highly ill-posed nature. Existing deep learning based solutions typically restore the target layers individually, or with some concerns at the end of the output, barely taking into account the interaction across the two streams/branches. In order to utilize information more efficiently, this work presents a general yet simple interactive strategy, namely $\textit{your trash is my treasure}$ (YTMT), for constructing dual-stream decomposition networks. To be specific, we explicitly enforce the two streams to communicate with each other block-wisely. Inspired by the additive property between the two components, the interactive path can be easily built via transferring, instead of discarding, deactivated information by the ReLU rectifier from one stream to the other. Both ablation studies and experimental results on widely-used SIRS datasets are conducted to demonstrate the efficacy of YTMT, and reveal its superiority over other state-of-the-art alternatives. The implementation is quite simple and our code is publicly available at $\href{https://github.com/mingcv/YTMT-Strategy}{\textit{https://github.com/mingcv/YTMT-Strategy}}$.



### Few-Shot Temporal Action Localization with Query Adaptive Transformer
- **Arxiv ID**: http://arxiv.org/abs/2110.10552v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.10552v1)
- **Published**: 2021-10-20 13:18:01+00:00
- **Updated**: 2021-10-20 13:18:01+00:00
- **Authors**: Sauradip Nag, Xiatian Zhu, Tao Xiang
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Existing temporal action localization (TAL) works rely on a large number of training videos with exhaustive segment-level annotation, preventing them from scaling to new classes. As a solution to this problem, few-shot TAL (FS-TAL) aims to adapt a model to a new class represented by as few as a single video. Exiting FS-TAL methods assume trimmed training videos for new classes. However, this setting is not only unnatural actions are typically captured in untrimmed videos, but also ignores background video segments containing vital contextual cues for foreground action segmentation. In this work, we first propose a new FS-TAL setting by proposing to use untrimmed training videos. Further, a novel FS-TAL model is proposed which maximizes the knowledge transfer from training classes whilst enabling the model to be dynamically adapted to both the new class and each video of that class simultaneously. This is achieved by introducing a query adaptive Transformer in the model. Extensive experiments on two action localization benchmarks demonstrate that our method can outperform all the state of the art alternatives significantly in both single-domain and cross-domain scenarios. The source code can be found in https://github.com/sauradip/fewshotQAT



### ESOD:Edge-based Task Scheduling for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.11342v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.11342v1)
- **Published**: 2021-10-20 13:43:51+00:00
- **Updated**: 2021-10-20 13:43:51+00:00
- **Authors**: Yihao Wang, Ling Gao, Jie Ren, Rui Cao, Hai Wang, Jie Zheng, Quanli Gao
- **Comment**: Accepted by The Ninth International Conference on Advanced Cloud and
  Big Data
- **Journal**: None
- **Summary**: Object Detection on the mobile system is a challenge in terms of everything. Nowadays, many object detection models have been designed, and most of them concentrate on precision. However, the computation burden of those models on mobile systems is unacceptable. Researchers have designed some lightweight networks for mobiles by sacrificing precision. We present a novel edge-based task scheduling framework for object detection (termed as ESOD). In detail, we train a DNN model (termed as pre-model) to predict which object detection model to use for the coming task and offloads to which edge servers by physical characteristics of the image task (e.g., brightness, saturation). The results show that ESOD can reduce latency and energy consumption by an average of 22.13% and 29.60% and improve the mAP to 45.8(with 0.9 mAP better), respectively, compared with the SOTA DETR model.



### Robust Monocular Localization in Sparse HD Maps Leveraging Multi-Task Uncertainty Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.10563v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10563v1)
- **Published**: 2021-10-20 13:46:15+00:00
- **Updated**: 2021-10-20 13:46:15+00:00
- **Authors**: Kürsat Petek, Kshitij Sirohi, Daniel Büscher, Wolfram Burgard
- **Comment**: None
- **Journal**: None
- **Summary**: Robust localization in dense urban scenarios using a low-cost sensor setup and sparse HD maps is highly relevant for the current advances in autonomous driving, but remains a challenging topic in research. We present a novel monocular localization approach based on a sliding-window pose graph that leverages predicted uncertainties for increased precision and robustness against challenging scenarios and per frame failures. To this end, we propose an efficient multi-task uncertainty-aware perception module, which covers semantic segmentation, as well as bounding box detection, to enable the localization of vehicles in sparse maps, containing only lane borders and traffic lights. Further, we design differentiable cost maps that are directly generated from the estimated uncertainties. This opens up the possibility to minimize the reprojection loss of amorphous map elements in an association free and uncertainty-aware manner. Extensive evaluation on the Lyft 5 dataset shows that, despite the sparsity of the map, our approach enables robust and accurate 6D localization in challenging urban scenarios



### Fingerprint recognition with embedded presentation attacks detection: are we ready?
- **Arxiv ID**: http://arxiv.org/abs/2110.10567v1
- **DOI**: 10.1109/TIFS.2021.3121201
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10567v1)
- **Published**: 2021-10-20 13:53:16+00:00
- **Updated**: 2021-10-20 13:53:16+00:00
- **Authors**: Marco Micheletto, Gian Luca Marcialis, Giulia Orrù, Fabio Roli
- **Comment**: None
- **Journal**: IEEE Transactions on Information Forensics and Security (2021)
- **Summary**: The diffusion of fingerprint verification systems for security applications makes it urgent to investigate the embedding of software-based presentation attack detection algorithms (PAD) into such systems. Companies and institutions need to know whether such integration would make the system more "secure" and whether the technology available is ready, and, if so, at what operational working conditions. Despite significant improvements, especially by adopting deep learning approaches to fingerprint PAD, current research did not state much about their effectiveness when embedded in fingerprint verification systems. We believe that the lack of works is explained by the lack of instruments to investigate the problem, that is, modeling the cause-effect relationships when two non-zero error-free systems work together. Accordingly, this paper explores the fusion of PAD into verification systems by proposing a novel investigation instrument: a performance simulator based on the probabilistic modeling of the relationships among the Receiver Operating Characteristics (ROC) of the two individual systems when PAD and verification stages are implemented sequentially. As a matter of fact, this is the most straightforward, flexible, and widespread approach. We carry out simulations on the PAD algorithms' ROCs submitted to the most recent editions of LivDet (2017-2019), the state-of-the-art NIST Bozorth3, and the top-level Veryfinger 12 matchers. Reported experiments explore significant scenarios to get the conditions under which fingerprint matching with embedded PAD can improve, rather than degrade, the overall personal verification performance.



### Inference Graphs for CNN Interpretation
- **Arxiv ID**: http://arxiv.org/abs/2110.10568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10568v1)
- **Published**: 2021-10-20 13:56:09+00:00
- **Updated**: 2021-10-20 13:56:09+00:00
- **Authors**: Yael Konforti, Alon Shpigler, Boaz Lernerand Aharon Bar-Hillel
- **Comment**: None
- **Journal**: European Conference on Computer Vision (ECCV), pp. 69-84.
  Springer, Cham, 2020
- **Summary**: Convolutional neural networks (CNNs) have achieved superior accuracy in many visual related tasks. However, the inference process through intermediate layers is opaque, making it difficult to interpret such networks or develop trust in their operation. We propose to model the network hidden layers activity using probabilistic models. The activity patterns in layers of interest are modeled as Gaussian mixture models, and transition probabilities between clusters in consecutive modeled layers are estimated. Based on maximum-likelihood considerations, nodes and paths relevant for network prediction are chosen, connected, and visualized as an inference graph. We show that such graphs are useful for understanding the general inference process of a class, as well as explaining decisions the network makes regarding specific images.



### A Deep Learning Framework for Diffeomorphic Mapping Problems via Quasi-conformal Geometry applied to Imaging
- **Arxiv ID**: http://arxiv.org/abs/2110.10580v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10580v2)
- **Published**: 2021-10-20 14:23:24+00:00
- **Updated**: 2022-08-13 08:27:12+00:00
- **Authors**: Qiguang Chen, Zhiwen Li, Lok Ming Lui
- **Comment**: None
- **Journal**: None
- **Summary**: Many imaging problems can be formulated as mapping problems. A general mapping problem aims to obtain an optimal mapping that minimizes an energy functional subject to the given constraints. Existing methods to solve the mapping problems are often inefficient and can sometimes get trapped in local minima. An extra challenge arises when the optimal mapping is required to be diffeomorphic. In this work, we address the problem by proposing a deep-learning framework based on the Quasiconformal (QC) Teichmuller theories. The main strategy is to learn the Beltrami coefficient (BC) that represents a mapping as the latent feature vector in the deep neural network. The BC measures the local geometric distortion under the mapping, with which the interpretability of the deep neural network can be enhanced. Under this framework, the diffeomorphic property of the mapping can be controlled via a simple activation function within the network. The optimal mapping can also be easily regularized by integrating the BC into the loss function. A crucial advantage of the proposed framework is that once the network is successfully trained, the optimized mapping corresponding to each input data information can be obtained in real time. To examine the efficacy of the proposed framework, we apply the method to the diffeomorphic image registration problem. Experimental results outperform other state-of-the-art registration algorithms in both efficiency and accuracy, which demonstrate the effectiveness of our proposed framework to solve the mapping problem.



### Look at What I'm Doing: Self-Supervised Spatial Grounding of Narrations in Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/2110.10596v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10596v2)
- **Published**: 2021-10-20 14:45:13+00:00
- **Updated**: 2021-12-02 16:55:56+00:00
- **Authors**: Reuben Tan, Bryan A. Plummer, Kate Saenko, Hailin Jin, Bryan Russell
- **Comment**: Accepted at NeurIPS 2021 (Spotlight)
- **Journal**: None
- **Summary**: We introduce the task of spatially localizing narrated interactions in videos. Key to our approach is the ability to learn to spatially localize interactions with self-supervision on a large corpus of videos with accompanying transcribed narrations. To achieve this goal, we propose a multilayer cross-modal attention network that enables effective optimization of a contrastive loss during training. We introduce a divided strategy that alternates between computing inter- and intra-modal attention across the visual and natural language modalities, which allows effective training via directly contrasting the two modalities' representations. We demonstrate the effectiveness of our approach by self-training on the HowTo100M instructional video dataset and evaluating on a newly collected dataset of localized described interactions in the YouCook2 dataset. We show that our approach outperforms alternative baselines, including shallow co-attention and full cross-modal attention. We also apply our approach to grounding phrases in images with weak supervision on Flickr30K and show that stacking multiple attention layers is effective and, when combined with a word-to-region loss, achieves state of the art on recall-at-one and pointing hand accuracies.



### Video Instance Segmentation by Instance Flow Assembly
- **Arxiv ID**: http://arxiv.org/abs/2110.10599v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10599v1)
- **Published**: 2021-10-20 14:49:28+00:00
- **Updated**: 2021-10-20 14:49:28+00:00
- **Authors**: Xiang Li, Jinglu Wang, Xiao Li, Yan Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Instance segmentation is a challenging task aiming at classifying and segmenting all object instances of specific classes. While two-stage box-based methods achieve top performances in the image domain, they cannot easily extend their superiority into the video domain. This is because they usually deal with features or images cropped from the detected bounding boxes without alignment, failing to capture pixel-level temporal consistency. We embrace the observation that bottom-up methods dealing with box-free features could offer accurate spacial correlations across frames, which can be fully utilized for object and pixel level tracking. We first propose our bottom-up framework equipped with a temporal context fusion module to better encode inter-frame correlations. Intra-frame cues for semantic segmentation and object localization are simultaneously extracted and reconstructed by corresponding decoders after a shared backbone. For efficient and robust tracking among instances, we introduce an instance-level correspondence across adjacent frames, which is represented by a center-to-center flow, termed as instance flow, to assemble messy dense temporal correspondences. Experiments demonstrate that the proposed method outperforms the state-of-the-art online methods (taking image-level input) on the challenging Youtube-VIS dataset.



### Development of Semantic Web-based Imaging Database for Biological Morphome
- **Arxiv ID**: http://arxiv.org/abs/2110.12058v1
- **DOI**: 10.1007/978-3-319-70682-5_19
- **Categories**: **q-bio.QM**, cs.AI, cs.CV, cs.DB, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12058v1)
- **Published**: 2021-10-20 15:59:35+00:00
- **Updated**: 2021-10-20 15:59:35+00:00
- **Authors**: Satoshi Kume, Hiroshi Masuya, Mitsuyo Maeda, Mitsuo Suga, Yosky Kataoka, Norio Kobayashi
- **Comment**: None
- **Journal**: JIST 2017: Semantic Technology
- **Summary**: We introduce the RIKEN Microstructural Imaging Metadatabase, a semantic web-based imaging database in which image metadata are described using the Resource Description Framework (RDF) and detailed biological properties observed in the images can be represented as Linked Open Data. The metadata are used to develop a large-scale imaging viewer that provides a straightforward graphical user interface to visualise a large microstructural tiling image at the gigabyte level. We applied the database to accumulate comprehensive microstructural imaging data produced by automated scanning electron microscopy. As a result, we have successfully managed vast numbers of images and their metadata, including the interpretation of morphological phenotypes occurring in sub-cellular components and biosamples captured in the images. We also discuss advanced utilisation of morphological imaging data that can be promoted by this database.



### Semi-supervised Domain Adaptation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.10639v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10639v1)
- **Published**: 2021-10-20 16:13:00+00:00
- **Updated**: 2021-10-20 16:13:00+00:00
- **Authors**: Ying Chen, Xu Ouyang, Kaiyue Zhu, Gady Agam
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning approaches for semantic segmentation rely primarily on supervised learning approaches and require substantial efforts in producing pixel-level annotations. Further, such approaches may perform poorly when applied to unseen image domains. To cope with these limitations, both unsupervised domain adaptation (UDA) with full source supervision but without target supervision and semi-supervised learning (SSL) with partial supervision have been proposed. While such methods are effective at aligning different feature distributions, there is still a need to efficiently exploit unlabeled data to address the performance gap with respect to fully-supervised methods. In this paper we address semi-supervised domain adaptation (SSDA) for semantic segmentation, where a large amount of labeled source data as well as a small amount of labeled target data are available. We propose a novel and effective two-step semi-supervised dual-domain adaptation (SSDDA) approach to address both cross- and intra-domain gaps in semantic segmentation. The proposed framework is comprised of two mixing modules. First, we conduct a cross-domain adaptation via an image-level mixing strategy, which learns to align the distribution shift of features between the source data and target data. Second, intra-domain adaptation is achieved using a separate student-teacher network which is built to generate category-level data augmentation by mixing unlabeled target data in a way that respects predicted object boundaries. We demonstrate that the proposed approach outperforms state-of-the-art methods on two common synthetic-to-real semantic segmentation benchmarks. An extensive ablation study is provided to further validate the effectiveness of our approach.



### OSS-Net: Memory Efficient High Resolution Semantic Segmentation of 3D Medical Data
- **Arxiv ID**: http://arxiv.org/abs/2110.10640v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10640v1)
- **Published**: 2021-10-20 16:14:26+00:00
- **Updated**: 2021-10-20 16:14:26+00:00
- **Authors**: Christoph Reich, Tim Prangemeier, Özdemir Cetin, Heinz Koeppl
- **Comment**: BMVC 2021 (accepted), https://github.com/ChristophReich1996/OSS-Net
  (code)
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are the current state-of-the-art meta-algorithm for volumetric segmentation of medical data, for example, to localize COVID-19 infected tissue on computer tomography scans or the detection of tumour volumes in magnetic resonance imaging. A key limitation of 3D CNNs on voxelised data is that the memory consumption grows cubically with the training data resolution. Occupancy networks (O-Nets) are an alternative for which the data is represented continuously in a function space and 3D shapes are learned as a continuous decision boundary. While O-Nets are significantly more memory efficient than 3D CNNs, they are limited to simple shapes, are relatively slow at inference, and have not yet been adapted for 3D semantic segmentation of medical data. Here, we propose Occupancy Networks for Semantic Segmentation (OSS-Nets) to accurately and memory-efficiently segment 3D medical data. We build upon the original O-Net with modifications for increased expressiveness leading to improved segmentation performance comparable to 3D CNNs, as well as modifications for faster inference. We leverage local observations to represent complex shapes and prior encoder predictions to expedite inference. We showcase OSS-Net's performance on 3D brain tumour and liver segmentation against a function space baseline (O-Net), a performance baseline (3D residual U-Net), and an efficiency baseline (2D residual U-Net). OSS-Net yields segmentation results similar to the performance baseline and superior to the function space and efficiency baselines. In terms of memory efficiency, OSS-Net consumes comparable amounts of memory as the function space baseline, somewhat more memory than the efficiency baseline and significantly less than the performance baseline. As such, OSS-Net enables memory-efficient and accurate 3D semantic segmentation that can scale to high resolutions.



### Combining Different V1 Brain Model Variants to Improve Robustness to Image Corruptions in CNNs
- **Arxiv ID**: http://arxiv.org/abs/2110.10645v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2110.10645v2)
- **Published**: 2021-10-20 16:35:09+00:00
- **Updated**: 2021-12-07 23:51:00+00:00
- **Authors**: Avinash Baidya, Joel Dapello, James J. DiCarlo, Tiago Marques
- **Comment**: 15 pages with supplementary material, 3 main figures, 2 supplementary
  figures, 4 supplementary tables
- **Journal**: Workshop on Shared Visual Representations in Human and Machine
  Intelligence 2021
- **Summary**: While some convolutional neural networks (CNNs) have surpassed human visual abilities in object classification, they often struggle to recognize objects in images corrupted with different types of common noise patterns, highlighting a major limitation of this family of models. Recently, it has been shown that simulating a primary visual cortex (V1) at the front of CNNs leads to small improvements in robustness to these image perturbations. In this study, we start with the observation that different variants of the V1 model show gains for specific corruption types. We then build a new model using an ensembling technique, which combines multiple individual models with different V1 front-end variants. The model ensemble leverages the strengths of each individual model, leading to significant improvements in robustness across all corruption categories and outperforming the base model by 38% on average. Finally, we show that using distillation, it is possible to partially compress the knowledge in the ensemble model into a single model with a V1 front-end. While the ensembling and distillation techniques used here are hardly biologically-plausible, the results presented here demonstrate that by combining the specific strengths of different neuronal circuits in V1 it is possible to improve the robustness of CNNs for a wide range of perturbations.



### Self-Supervision and Spatial-Sequential Attention Based Loss for Multi-Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.10734v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10734v1)
- **Published**: 2021-10-20 19:13:17+00:00
- **Updated**: 2021-10-20 19:13:17+00:00
- **Authors**: Haiyang Liu, Dingli Luo, Songlin Du, Takeshi Ikenaga
- **Comment**: None
- **Journal**: None
- **Summary**: Bottom-up based multi-person pose estimation approaches use heatmaps with auxiliary predictions to estimate joint positions and belonging at one time. Recently, various combinations between auxiliary predictions and heatmaps have been proposed for higher performance, these predictions are supervised by the corresponding L2 loss function directly. However, the lack of more explicit supervision results in low features utilization and contradictions between predictions in one model. To solve these problems, this paper proposes (i) a new loss organization method which uses self-supervised heatmaps to reduce prediction contradictions and spatial-sequential attention to enhance networks' features extraction; (ii) a new combination of predictions composed by heatmaps, Part Affinity Fields (PAFs) and our block-inside offsets to fix pixel-level joints positions and further demonstrates the effectiveness of proposed loss function. Experiments are conducted on the MS COCO keypoint dataset and adopting OpenPose as the baseline model. Our method outperforms the baseline overall. On the COCO verification dataset, the mAP of OpenPose trained with our proposals outperforms the OpenPose baseline by over 5.5%.



### Class Incremental Online Streaming Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.10741v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10741v1)
- **Published**: 2021-10-20 19:24:31+00:00
- **Updated**: 2021-10-20 19:24:31+00:00
- **Authors**: Soumya Banerjee, Vinay Kumar Verma, Toufiq Parag, Maneesh Singh, Vinay P. Namboodiri
- **Comment**: None
- **Journal**: None
- **Summary**: A wide variety of methods have been developed to enable lifelong learning in conventional deep neural networks. However, to succeed, these methods require a `batch' of samples to be available and visited multiple times during training. While this works well in a static setting, these methods continue to suffer in a more realistic situation where data arrives in \emph{online streaming manner}. We empirically demonstrate that the performance of current approaches degrades if the input is obtained as a stream of data with the following restrictions: $(i)$ each instance comes one at a time and can be seen only once, and $(ii)$ the input data violates the i.i.d assumption, i.e., there can be a class-based correlation. We propose a novel approach (CIOSL) for the class-incremental learning in an \emph{online streaming setting} to address these challenges. The proposed approach leverages implicit and explicit dual weight regularization and experience replay. The implicit regularization is leveraged via the knowledge distillation, while the explicit regularization incorporates a novel approach for parameter regularization by learning the joint distribution of the buffer replay and the current sample. Also, we propose an efficient online memory replay and replacement buffer strategy that significantly boosts the model's performance. Extensive experiments and ablation on challenging datasets show the efficacy of the proposed method.



### Toward Real-world Image Super-resolution via Hardware-based Adaptive Degradation Models
- **Arxiv ID**: http://arxiv.org/abs/2110.10755v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10755v1)
- **Published**: 2021-10-20 19:53:48+00:00
- **Updated**: 2021-10-20 19:53:48+00:00
- **Authors**: Rui Ma, Johnathan Czernik, Xian Du
- **Comment**: None
- **Journal**: None
- **Summary**: Most single image super-resolution (SR) methods are developed on synthetic low-resolution (LR) and high-resolution (HR) image pairs, which are simulated by a predetermined degradation operation, e.g., bicubic downsampling. However, these methods only learn the inverse process of the predetermined operation, so they fail to super resolve the real-world LR images; the true formulation deviates from the predetermined operation. To address this problem, we propose a novel supervised method to simulate an unknown degradation process with the inclusion of the prior hardware knowledge of the imaging system. We design an adaptive blurring layer (ABL) in the supervised learning framework to estimate the target LR images. The hyperparameters of the ABL can be adjusted for different imaging hardware. The experiments on the real-world datasets validate that our degradation model can estimate LR images more accurately than the predetermined degradation operation, as well as facilitate existing SR methods to perform reconstructions on real-world LR images more accurately than the conventional approaches.



### Closed-loop Feedback Registration for Consecutive Images of Moving Flexible Targets
- **Arxiv ID**: http://arxiv.org/abs/2110.10772v1
- **DOI**: 10.1007/s10489-022-04068-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10772v1)
- **Published**: 2021-10-20 20:31:43+00:00
- **Updated**: 2021-10-20 20:31:43+00:00
- **Authors**: Rui Ma, Xian Du
- **Comment**: None
- **Journal**: None
- **Summary**: Advancement of imaging techniques enables consecutive image sequences to be acquired for quality monitoring of manufacturing production lines. Registration for these image sequences is essential for in-line pattern inspection and metrology, e.g., in the printing process of flexible electronics. However, conventional image registration algorithms cannot produce accurate results when the images contain many similar and deformable patterns in the manufacturing process. Such a failure originates from a fact that the conventional algorithms only use the spatial and pixel intensity information for registration. Considering the nature of temporal continuity and consecution of the product images, in this paper, we propose a closed-loop feedback registration algorithm for matching and stitching the deformable printed patterns on a moving flexible substrate. The algorithm leverages the temporal and spatial relationships of the consecutive images and the continuity of the image sequence for fast, accurate, and robust point matching. Our experimental results show that our algorithm can find more matching point pairs with a lower root mean squared error (RMSE) compared to other state-of-the-art algorithms while offering significant improvements to running time.



### Style Agnostic 3D Reconstruction via Adversarial Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2110.10784v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10784v1)
- **Published**: 2021-10-20 21:24:44+00:00
- **Updated**: 2021-10-20 21:24:44+00:00
- **Authors**: Felix Petersen, Bastian Goldluecke, Oliver Deussen, Hilde Kuehne
- **Comment**: To be published at WACV 2022, Code @
  https://github.com/Felix-Petersen/style-agnostic-3d-reconstruction
- **Journal**: None
- **Summary**: Reconstructing the 3D geometry of an object from an image is a major challenge in computer vision. Recently introduced differentiable renderers can be leveraged to learn the 3D geometry of objects from 2D images, but those approaches require additional supervision to enable the renderer to produce an output that can be compared to the input image. This can be scene information or constraints such as object silhouettes, uniform backgrounds, material, texture, and lighting. In this paper, we propose an approach that enables a differentiable rendering-based learning of 3D objects from images with backgrounds without the need for silhouette supervision. Instead of trying to render an image close to the input, we propose an adversarial style-transfer and domain adaptation pipeline that allows to translate the input image domain to the rendered image domain. This allows us to directly compare between a translated image and the differentiable rendering of a 3D object reconstruction in order to train the 3D object reconstruction network. We show that the approach learns 3D geometry from images with backgrounds and provides a better performance than constrained methods for single-view 3D object reconstruction on this task.



### DVIO: Depth aided visual inertial odometry for RGBD sensors
- **Arxiv ID**: http://arxiv.org/abs/2110.10805v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.10805v1)
- **Published**: 2021-10-20 22:12:01+00:00
- **Updated**: 2021-10-20 22:12:01+00:00
- **Authors**: Abhishek Tyagi, Yangwen Liang, Shuangquan Wang, Dongwoon Bai
- **Comment**: None
- **Journal**: None
- **Summary**: In past few years we have observed an increase in the usage of RGBD sensors in mobile devices. These sensors provide a good estimate of the depth map for the camera frame, which can be used in numerous augmented reality applications. This paper presents a new visual inertial odometry (VIO) system, which uses measurements from a RGBD sensor and an inertial measurement unit (IMU) sensor for estimating the motion state of the mobile device. The resulting system is called the depth-aided VIO (DVIO) system. In this system we add the depth measurement as part of the nonlinear optimization process. Specifically, we propose methods to use the depth measurement using one-dimensional (1D) feature parameterization as well as three-dimensional (3D) feature parameterization. In addition, we propose to utilize the depth measurement for estimating time offset between the unsynchronized IMU and the RGBD sensors. Last but not least, we propose a novel block-based marginalization approach to speed up the marginalization processes and maintain the real-time performance of the overall system. Experimental results validate that the proposed DVIO system outperforms the other state-of-the-art VIO systems in terms of trajectory accuracy as well as processing time.



### Text-Based Person Search with Limited Data
- **Arxiv ID**: http://arxiv.org/abs/2110.10807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.10807v1)
- **Published**: 2021-10-20 22:20:47+00:00
- **Updated**: 2021-10-20 22:20:47+00:00
- **Authors**: Xiao Han, Sen He, Li Zhang, Tao Xiang
- **Comment**: 20 pages, 7 figures, 6 tables, to appear in BMVC2021
- **Journal**: None
- **Summary**: Text-based person search (TBPS) aims at retrieving a target person from an image gallery with a descriptive text query. Solving such a fine-grained cross-modal retrieval task is challenging, which is further hampered by the lack of large-scale datasets. In this paper, we present a framework with two novel components to handle the problems brought by limited data. Firstly, to fully utilize the existing small-scale benchmarking datasets for more discriminative feature learning, we introduce a cross-modal momentum contrastive learning framework to enrich the training data for a given mini-batch. Secondly, we propose to transfer knowledge learned from existing coarse-grained large-scale datasets containing image-text pairs from drastically different problem domains to compensate for the lack of TBPS training data. A transfer learning method is designed so that useful information can be transferred despite the large domain gap. Armed with these components, our method achieves new state of the art on the CUHK-PEDES dataset with significant improvements over the prior art in terms of Rank-1 and mAP. Our code is available at https://github.com/BrandonHanx/TextReID.



### HALP: Hardware-Aware Latency Pruning
- **Arxiv ID**: http://arxiv.org/abs/2110.10811v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10811v1)
- **Published**: 2021-10-20 22:34:51+00:00
- **Updated**: 2021-10-20 22:34:51+00:00
- **Authors**: Maying Shen, Hongxu Yin, Pavlo Molchanov, Lei Mao, Jianna Liu, Jose M. Alvarez
- **Comment**: None
- **Journal**: None
- **Summary**: Structural pruning can simplify network architecture and improve inference speed. We propose Hardware-Aware Latency Pruning (HALP) that formulates structural pruning as a global resource allocation optimization problem, aiming at maximizing the accuracy while constraining latency under a predefined budget. For filter importance ranking, HALP leverages latency lookup table to track latency reduction potential and global saliency score to gauge accuracy drop. Both metrics can be evaluated very efficiently during pruning, allowing us to reformulate global structural pruning under a reward maximization problem given target constraint. This makes the problem solvable via our augmented knapsack solver, enabling HALP to surpass prior work in pruning efficacy and accuracy-efficiency trade-off. We examine HALP on both classification and detection tasks, over varying networks, on ImageNet and VOC datasets. In particular, for ResNet-50/-101 pruning on ImageNet, HALP improves network throughput by $1.60\times$/$1.90\times$ with $+0.3\%$/$-0.2\%$ top-1 accuracy changes, respectively. For SSD pruning on VOC, HALP improves throughput by $1.94\times$ with only a $0.56$ mAP drop. HALP consistently outperforms prior art, sometimes by large margins.



### CXR-Net: An Encoder-Decoder-Encoder Multitask Deep Neural Network for Explainable and Accurate Diagnosis of COVID-19 pneumonia with Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2110.10813v1
- **DOI**: 10.1109/JBHI.2022.3220813
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.10813v1)
- **Published**: 2021-10-20 22:50:35+00:00
- **Updated**: 2021-10-20 22:50:35+00:00
- **Authors**: Xin Zhang, Liangxiu Han, Tam Sobeih, Lianghao Han, Nina Dempsey, Symeon Lechareas, Ascanio Tridente, Haoming Chen, Stephen White
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and rapid detection of COVID-19 pneumonia is crucial for optimal patient treatment. Chest X-Ray (CXR) is the first line imaging test for COVID-19 pneumonia diagnosis as it is fast, cheap and easily accessible. Inspired by the success of deep learning (DL) in computer vision, many DL-models have been proposed to detect COVID-19 pneumonia using CXR images. Unfortunately, these deep classifiers lack the transparency in interpreting findings, which may limit their applications in clinical practice. The existing commonly used visual explanation methods are either too noisy or imprecise, with low resolution, and hence are unsuitable for diagnostic purposes. In this work, we propose a novel explainable deep learning framework (CXRNet) for accurate COVID-19 pneumonia detection with an enhanced pixel-level visual explanation from CXR images. The proposed framework is based on a new Encoder-Decoder-Encoder multitask architecture, allowing for both disease classification and visual explanation. The method has been evaluated on real world CXR datasets from both public and private data sources, including: healthy, bacterial pneumonia, viral pneumonia and COVID-19 pneumonia cases The experimental results demonstrate that the proposed method can achieve a satisfactory level of accuracy and provide fine-resolution classification activation maps for visual explanation in lung disease detection. The Average Accuracy, the Precision, Recall and F1-score of COVID-19 pneumonia reached 0.879, 0.985, 0.992 and 0.989, respectively. We have also found that using lung segmented (CXR) images can help improve the performance of the model. The proposed method can provide more detailed high resolution visual explanation for the classification decision, compared to current state-of-the-art visual explanation methods and has a great potential to be used in clinical practice for COVID-19 pneumonia diagnosis.



