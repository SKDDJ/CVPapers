# Arxiv Papers in cs.CV on 2021-10-04
### Enhance Images as You Like with Unpaired Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.01161v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01161v1)
- **Published**: 2021-10-04 03:00:44+00:00
- **Updated**: 2021-10-04 03:00:44+00:00
- **Authors**: Xiaopeng Sun, Muxingzi Li, Tianyu He, Lubin Fan
- **Comment**: 7 pages; IJCAI 2021
- **Journal**: None
- **Summary**: Low-light image enhancement exhibits an ill-posed nature, as a given image may have many enhanced versions, yet recent studies focus on building a deterministic mapping from input to an enhanced version. In contrast, we propose a lightweight one-path conditional generative adversarial network (cGAN) to learn a one-to-many relation from low-light to normal-light image space, given only sets of low- and normal-light training images without any correspondence. By formulating this ill-posed problem as a modulation code learning task, our network learns to generate a collection of enhanced images from a given input conditioned on various reference images. Therefore our inference model easily adapts to various user preferences, provided with a few favorable photos from each user. Our model achieves competitive visual and quantitative results on par with fully supervised methods on both noisy and clean datasets, while being 6 to 10 times lighter than state-of-the-art generative adversarial networks (GANs) approaches.



### Deep Kernel Representation for Image Reconstruction in PET
- **Arxiv ID**: http://arxiv.org/abs/2110.01174v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01174v4)
- **Published**: 2021-10-04 03:53:33+00:00
- **Updated**: 2022-05-25 16:27:47+00:00
- **Authors**: Siqi Li, Guobao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Image reconstruction for positron emission tomography (PET) is challenging because of the ill-conditioned tomographic problem and low counting statistics. Kernel methods address this challenge by using kernel representation to incorporate image prior information in the forward model of iterative PET image reconstruction. Existing kernel methods construct the kernels commonly using an empirical process, which may lead to unsatisfactory performance. In this paper, we describe the equivalence between the kernel representation and a trainable neural network model. A deep kernel method is then proposed by exploiting a deep neural network to enable automated learning of an improved kernel model and is directly applicable to single subjects in dynamic PET. The training process utilizes available image prior data to form a set of robust kernels in an optimized way rather than empirically. The results from computer simulations and a real patient dataset demonstrate that the proposed deep kernel method can outperform the existing kernel method and neural network method for dynamic PET image reconstruction.



### BPFNet: A Unified Framework for Bimodal Palmprint Alignment and Fusion
- **Arxiv ID**: http://arxiv.org/abs/2110.01179v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01179v2)
- **Published**: 2021-10-04 04:30:36+00:00
- **Updated**: 2021-12-14 06:38:55+00:00
- **Authors**: Zhaoqun Li, Xu Liang, Dandan Fan, Jinxing Li, David Zhang
- **Comment**: Extended version of ICONIP 2021 paper
- **Journal**: None
- **Summary**: Bimodal palmprint recognition leverages palmprint and palm vein images simultaneously,which achieves high accuracy by multi-model information fusion and has strong anti-falsification property. In the recognition pipeline, the detection of palm and the alignment of region-of-interest (ROI) are two crucial steps for accurate matching. Most existing methods localize palm ROI by keypoint detection algorithms, however the intrinsic difficulties of keypoint detection tasks make the results unsatisfactory. Besides, the ROI alignment and fusion algorithms at image-level are not fully investigaged.To bridge the gap, in this paper, we propose Bimodal Palmprint Fusion Network (BPFNet) which focuses on ROI localization, alignment and bimodal image fusion.BPFNet is an end-to-end framework containing two subnets: The detection network directly regresses the palmprint ROIs based on bounding box prediction and conducts alignment by translation estimation.In the downstream,the bimodal fusion network implements bimodal ROI image fusion leveraging a novel proposed cross-modal selection scheme. To show the effectiveness of BPFNet,we carry out experiments on the large-scale touchless palmprint datasets CUHKSZ-v1 and TongJi and the proposed method achieves state-of-the-art performances.



### Improving Axial-Attention Network Classification via Cross-Channel Weight Sharing
- **Arxiv ID**: http://arxiv.org/abs/2110.01185v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.01185v2)
- **Published**: 2021-10-04 04:47:41+00:00
- **Updated**: 2023-01-12 18:20:13+00:00
- **Authors**: Nazmul Shahadat, Anthony S. Maida
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, hypercomplex-inspired neural networks (HCNNs) have been used to improve deep learning architectures due to their ability to enable channel-based weight sharing, treat colors as a single entity, and improve representational coherence within the layers. The work described herein studies the effect of replacing existing layers in an Axial Attention network with their representationally coherent variants to assess the effect on image classification. We experiment with the stem of the network, the bottleneck layers, and the fully connected backend, by replacing them with representationally coherent variants. These various modifications lead to novel architectures which all yield improved accuracy performance on the ImageNet300k classification dataset. Our baseline networks for comparison were the original real-valued ResNet, the original quaternion-valued ResNet, and the Axial Attention ResNet. Since improvement was observed regardless of which part of the network was modified, there is a promise that this technique may be generally useful in improving classification accuracy for a large class of networks.



### Learning Structural Representations for Recipe Generation and Food Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2110.01209v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01209v2)
- **Published**: 2021-10-04 06:36:31+00:00
- **Updated**: 2022-08-01 06:42:58+00:00
- **Authors**: Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao
- **Comment**: Accepted at IEEE Transactions on Pattern Analysis and Machine
  Intelligence. arXiv admin note: substantial text overlap with
  arXiv:2009.00944
- **Journal**: None
- **Summary**: Food is significant to human daily life. In this paper, we are interested in learning structural representations for lengthy recipes, that can benefit the recipe generation and food cross-modal retrieval tasks. Different from the common vision-language data, here the food images contain mixed ingredients and target recipes are lengthy paragraphs, where we do not have annotations on structure information. To address the above limitations, we propose a novel method to unsupervisedly learn the sentence-level tree structures for the cooking recipes. Our approach brings together several novel ideas in a systematic framework: (1) exploiting an unsupervised learning approach to obtain the sentence-level tree structure labels before training; (2) generating trees of target recipes from images with the supervision of tree structure labels learned from (1); and (3) integrating the learned tree structures into the recipe generation and food cross-modal retrieval procedure. Our proposed model can produce good-quality sentence-level tree structures and coherent recipes. We achieve the state-of-the-art recipe generation and food cross-modal retrieval performance on the benchmark Recipe1M dataset.



### Max and Coincidence Neurons in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.01218v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01218v1)
- **Published**: 2021-10-04 07:13:50+00:00
- **Updated**: 2021-10-04 07:13:50+00:00
- **Authors**: Albert Lee, Kang L. Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Network design has been a central topic in machine learning. Large amounts of effort have been devoted towards creating efficient architectures through manual exploration as well as automated neural architecture search. However, todays architectures have yet to consider the diversity of neurons and the existence of neurons with specific processing functions. In this work, we optimize networks containing models of the max and coincidence neurons using neural architecture search, and analyze the structure, operations, and neurons of optimized networks to develop a signal-processing ResNet. The developed network achieves an average of 2% improvement in accuracy and a 25% improvement in network size across a variety of datasets, demonstrating the importance of neuronal functions in creating compact, efficient networks.



### A free lunch from ViT:Adaptive Attention Multi-scale Fusion Transformer for Fine-grained Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.01240v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.01240v2)
- **Published**: 2021-10-04 08:11:21+00:00
- **Updated**: 2021-10-11 08:26:19+00:00
- **Authors**: Yuan Zhang, Jian Cao, Ling Zhang, Xiangcheng Liu, Zhiyi Wang, Feng Ling, Weiqian Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Learning subtle representation about object parts plays a vital role in fine-grained visual recognition (FGVR) field. The vision transformer (ViT) achieves promising results on computer vision due to its attention mechanism. Nonetheless, with the fixed size of patches in ViT, the class token in deep layer focuses on the global receptive field and cannot generate multi-granularity features for FGVR. To capture region attention without box annotations and compensate for ViT shortcomings in FGVR, we propose a novel method named Adaptive attention multi-scale Fusion Transformer (AFTrans). The Selective Attention Collection Module (SACM) in our approach leverages attention weights in ViT and filters them adaptively to correspond with the relative importance of input patches. The multiple scales (global and local) pipeline is supervised by our weights sharing encoder and can be easily trained end-to-end. Comprehensive experiments demonstrate that AFTrans can achieve SOTA performance on three published fine-grained benchmarks: CUB-200-2011, Stanford Dogs and iNat2017.



### Consistency Regularization Can Improve Robustness to Label Noise
- **Arxiv ID**: http://arxiv.org/abs/2110.01242v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.01242v1)
- **Published**: 2021-10-04 08:15:08+00:00
- **Updated**: 2021-10-04 08:15:08+00:00
- **Authors**: Erik Englesson, Hossein Azizpour
- **Comment**: Presented at the ICML 2021 Workshop on Uncertainty and Robustness in
  Deep Learning. arXiv admin note: text overlap with arXiv:2105.04522
- **Journal**: None
- **Summary**: Consistency regularization is a commonly-used technique for semi-supervised and self-supervised learning. It is an auxiliary objective function that encourages the prediction of the network to be similar in the vicinity of the observed training samples. Hendrycks et al. (2020) have recently shown such regularization naturally brings test-time robustness to corrupted data and helps with calibration. This paper empirically studies the relevance of consistency regularization for training-time robustness to noisy labels. First, we make two interesting and useful observations regarding the consistency of networks trained with the standard cross entropy loss on noisy datasets which are: (i) networks trained on noisy data have lower consistency than those trained on clean data, and(ii) the consistency reduces more significantly around noisy-labelled training data points than correctly-labelled ones. Then, we show that a simple loss function that encourages consistency improves the robustness of the models to label noise on both synthetic (CIFAR-10, CIFAR-100) and real-world (WebVision) noise as well as different noise rates and types and achieves state-of-the-art results.



### Spatial Ensemble: a Novel Model Smoothing Mechanism for Student-Teacher Framework
- **Arxiv ID**: http://arxiv.org/abs/2110.01253v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01253v1)
- **Published**: 2021-10-04 08:45:18+00:00
- **Updated**: 2021-10-04 08:45:18+00:00
- **Authors**: Tengteng Huang, Yifan Sun, Xun Wang, Haotian Yao, Chi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Model smoothing is of central importance for obtaining a reliable teacher model in the student-teacher framework, where the teacher generates surrogate supervision signals to train the student. A popular model smoothing method is the Temporal Moving Average (TMA), which continuously averages the teacher parameters with the up-to-date student parameters. In this paper, we propose "Spatial Ensemble", a novel model smoothing mechanism in parallel with TMA. Spatial Ensemble randomly picks up a small fragment of the student model to directly replace the corresponding fragment of the teacher model. Consequentially, it stitches different fragments of historical student models into a unity, yielding the "Spatial Ensemble" effect. Spatial Ensemble obtains comparable student-teacher learning performance by itself and demonstrates valuable complementarity with temporal moving average. Their integration, named Spatial-Temporal Smoothing, brings general (sometimes significant) improvement to the student-teacher learning framework on a variety of state-of-the-art methods. For example, based on the self-supervised method BYOL, it yields +0.9% top-1 accuracy improvement on ImageNet, while based on the semi-supervised approach FixMatch, it increases the top-1 accuracy by around +6% on CIFAR-10 when only few training labels are available. Codes and models are available at: https://github.com/tengteng95/Spatial_Ensemble.



### GenCo: Generative Co-training for Generative Adversarial Networks with Limited Data
- **Arxiv ID**: http://arxiv.org/abs/2110.01254v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01254v2)
- **Published**: 2021-10-04 08:45:53+00:00
- **Updated**: 2021-12-06 13:54:28+00:00
- **Authors**: Kaiwen Cui, Jiaxing Huang, Zhipeng Luo, Gongjie Zhang, Fangneng Zhan, Shijian Lu
- **Comment**: Accepted to AAAI2022
- **Journal**: None
- **Summary**: Training effective Generative Adversarial Networks (GANs) requires large amounts of training data, without which the trained models are usually sub-optimal with discriminator over-fitting. Several prior studies address this issue by expanding the distribution of the limited training data via massive and hand-crafted data augmentation. We handle data-limited image generation from a very different perspective. Specifically, we design GenCo, a Generative Co-training network that mitigates the discriminator over-fitting issue by introducing multiple complementary discriminators that provide diverse supervision from multiple distinctive views in training. We instantiate the idea of GenCo in two ways. The first way is Weight-Discrepancy Co-training (WeCo) which co-trains multiple distinctive discriminators by diversifying their parameters. The second way is Data-Discrepancy Co-training (DaCo) which achieves co-training by feeding discriminators with different views of the input images (e.g., different frequency components of the input images). Extensive experiments over multiple benchmarks show that GenCo achieves superior generation with limited training data. In addition, GenCo also complements the augmentation approach with consistent and clear performance gains when combined.



### PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2110.01269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01269v1)
- **Published**: 2021-10-04 09:23:27+00:00
- **Updated**: 2021-10-04 09:23:27+00:00
- **Authors**: Anh-Quan Cao, Gilles Puy, Alexandre Boulch, Renaud Marlet
- **Comment**: ICCV21
- **Journal**: None
- **Summary**: Rigid registration of point clouds with partial overlaps is a longstanding problem usually solved in two steps: (a) finding correspondences between the point clouds; (b) filtering these correspondences to keep only the most reliable ones to estimate the transformation. Recently, several deep nets have been proposed to solve these steps jointly. We built upon these works and propose PCAM: a neural network whose key element is a pointwise product of cross-attention matrices that permits to mix both low-level geometric and high-level contextual information to find point correspondences. These cross-attention matrices also permits the exchange of context information between the point clouds, at each layer, allowing the network construct better matching features within the overlapping regions. The experiments show that PCAM achieves state-of-the-art results among methods which, like us, solve steps (a) and (b) jointly via deepnets. Our code and trained models are available at https://github.com/valeoai/PCAM.



### Light-weight Deformable Registration using Adversarial Learning with Distilling Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2110.01293v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01293v1)
- **Published**: 2021-10-04 09:59:01+00:00
- **Updated**: 2021-10-04 09:59:01+00:00
- **Authors**: Minh Q. Tran, Tuong Do, Huy Tran, Erman Tjiputra, Quang D. Tran, Anh Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Deformable registration is a crucial step in many medical procedures such as image-guided surgery and radiation therapy. Most recent learning-based methods focus on improving the accuracy by optimizing the non-linear spatial correspondence between the input images. Therefore, these methods are computationally expensive and require modern graphic cards for real-time deployment. In this paper, we introduce a new Light-weight Deformable Registration network that significantly reduces the computational cost while achieving competitive accuracy. In particular, we propose a new adversarial learning with distilling knowledge algorithm that successfully leverages meaningful information from the effective but expensive teacher network to the student network. We design the student network such as it is light-weight and well suitable for deployment on a typical CPU. The extensively experimental results on different public datasets show that our proposed method achieves state-of-the-art accuracy while significantly faster than recent methods. We further show that the use of our adversarial learning algorithm is essential for a time-efficiency deformable registration method. Finally, our source code and trained models are available at: https://github.com/aioz-ai/LDR_ALDK.



### Incremental Class Learning using Variational Autoencoders with Similarity Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.01303v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01303v3)
- **Published**: 2021-10-04 10:19:53+00:00
- **Updated**: 2023-03-14 11:40:35+00:00
- **Authors**: Jiahao Huo, Terence L. van Zyl
- **Comment**: None
- **Journal**: None
- **Summary**: Catastrophic forgetting in neural networks during incremental learning remains a challenging problem. Previous research investigated catastrophic forgetting in fully connected networks, with some earlier work exploring activation functions and learning algorithms. Applications of neural networks have been extended to include similarity learning. Understanding how similarity learning loss functions would be affected by catastrophic forgetting is of significant interest. Our research investigates catastrophic forgetting for four well-known similarity-based loss functions during incremental class learning. The loss functions are Angular, Contrastive, Center, and Triplet loss. Our results show that the catastrophic forgetting rate differs across loss functions on multiple datasets. The Angular loss was least affected, followed by Contrastive, Triplet loss, and Center loss with good mining techniques. We implemented three existing incremental learning techniques, iCaRL, EWC, and EBLL. We further proposed a novel technique using Variational Autoencoders (VAEs) to generate representation as exemplars passed through the network's intermediate layers. Our method outperformed three existing state-of-the-art techniques. We show that one does not require stored images (exemplars) for incremental learning with similarity learning. The generated representations from VAEs help preserve regions of the embedding space used by prior knowledge so that new knowledge does not ``overwrite'' it.



### Synthetic Velocity Mapping Cardiac MRI Coupled with Automated Left Ventricle Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.01304v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01304v2)
- **Published**: 2021-10-04 10:20:27+00:00
- **Updated**: 2021-11-01 13:06:05+00:00
- **Authors**: Xiaodan Xing, Yinzhe Wu, David Firmin, Peter Gatehouse, Guang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal patterns of cardiac motion provide important information for cardiac disease diagnosis. This pattern could be obtained by three-directional CINE multi-slice left ventricular myocardial velocity mapping (3Dir MVM), which is a cardiac MR technique providing magnitude and phase information of the myocardial motion simultaneously. However, long acquisition time limits the usage of this technique by causing breathing artifacts, while shortening the time causes low temporal resolution and may provide an inaccurate assessment of cardiac motion. In this study, we proposed a frame synthesis algorithm to increase the temporal resolution of 3Dir MVM data. Our algorithm is featured by 1) three attention-based encoders which accept magnitude images, phase images, and myocardium segmentation masks respectively as inputs; 2) three decoders that output the interpolated frames and corresponding myocardium segmentation results; and 3) loss functions highlighting myocardium pixels. Our algorithm can not only increase the temporal resolution 3Dir MVMs, but can also generates the myocardium segmentation results at the same time.



### Automated Aerial Animal Detection When Spatial Resolution Conditions Are Varied
- **Arxiv ID**: http://arxiv.org/abs/2110.01329v1
- **DOI**: 10.1016/j.compag.2022.106689
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01329v1)
- **Published**: 2021-10-04 11:20:14+00:00
- **Updated**: 2021-10-04 11:20:14+00:00
- **Authors**: Jasper Brown, Yongliang Qiao, Cameron Clark, Sabrina Lomax, Khalid Rafique, Salah Sukkarieh
- **Comment**: 20 pages, 9 figures, 4 tables in appendix
- **Journal**: Computers and Electronics in Agriculture, Volume 193, 2022
- **Summary**: Knowing where livestock are located enables optimized management and mustering. However, Australian farms are large meaning that many of Australia's livestock are unmonitored which impacts farm profit, animal welfare and the environment. Effective animal localisation and counting by analysing satellite imagery overcomes this management hurdle however, high resolution satellite imagery is expensive. Thus, to minimise cost the lowest spatial resolution data that enables accurate livestock detection should be selected. In our work, we determine the association between object detector performance and spatial degradation for cattle, sheep and dogs. Accurate ground truth was established using high resolution drone images which were then downsampled to various ground sample distances (GSDs). Both circular and cassegrain aperture optics were simulated to generate point spread functions (PSFs) corresponding to various optical qualities. By simulating the PSF, rather than approximating it as a Gaussian, the images were accurately degraded to match the spatial resolution and blurring structure of satellite imagery.   Two existing datasets were combined and used to train and test a YoloV5 object detection network. Detector performance was found to drop steeply around a GSD of 0.5m/px and was associated with PSF matrix structure within this GSD region. Detector mAP performance fell by 52 percent when a cassegrain, rather than circular, aperture was used at a 0.5m/px GSD. Overall blurring magnitude also had a small impact when matched to GSD, as did the internal network resolution. Our results here inform the selection of remote sensing data requirements for animal detection tasks, allowing farmers and ecologists to use more accessible medium resolution imagery with confidence.



### InfiniteForm: A synthetic, minimal bias dataset for fitness applications
- **Arxiv ID**: http://arxiv.org/abs/2110.01330v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01330v2)
- **Published**: 2021-10-04 11:26:30+00:00
- **Updated**: 2021-12-15 23:54:15+00:00
- **Authors**: Andrew Weitz, Lina Colucci, Sidney Primas, Brinnae Bent
- **Comment**: Presented at the NeurIPS Data Centric AI Workshop 2021
- **Journal**: None
- **Summary**: The growing popularity of remote fitness has increased the demand for highly accurate computer vision models that track human poses. However, the best methods still fail in many real-world fitness scenarios, suggesting that there is a domain gap between current datasets and real-world fitness data. To enable the field to address fitness-specific vision problems, we created InfiniteForm, an open-source synthetic dataset of 60k images with diverse fitness poses (15 categories), both single- and multi-person scenes, and realistic variation in lighting, camera angles, and occlusions. As a synthetic dataset, InfiniteForm offers minimal bias in body shape and skin tone, and provides pixel-perfect labels for standard annotations like 2D keypoints, as well as those that are difficult or impossible for humans to produce like depth and occlusion. In addition, we introduce a novel generative procedure for creating diverse synthetic poses from predefined exercise categories. This generative process can be extended to any application where pose diversity is needed to train robust computer vision models.



### Blindness (Diabetic Retinopathy) Severity Scale Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.01333v1
- **DOI**: 10.1109/SDS51136.2021.00009
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.01333v1)
- **Published**: 2021-10-04 11:31:15+00:00
- **Updated**: 2021-10-04 11:31:15+00:00
- **Authors**: Ramya Bygari, Rachita Naik, Uday Kumar P
- **Comment**: 7 pages, 7 figures, 2021 8th Swiss Conference on Data Science (SDS)
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) is a severe complication of diabetes that can cause permanent blindness. Timely diagnosis and treatment of DR are critical to avoid total loss of vision. Manual diagnosis is time consuming and error-prone. In this paper, we propose a novel deep learning based method for automatic screening of retinal fundus images to detect and classify DR based on the severity. The method uses a dual-path configuration of deep neural networks to achieve the objective. In the first step, a modified UNet++ based retinal vessel segmentation is used to create a fundus image that emphasises elements like haemorrhages, cotton wool spots, and exudates that are vital to identify the DR stages. Subsequently, two convolutional neural networks (CNN) classifiers take the original image and the newly created fundus image respectively as inputs and identify the severity of DR on a scale of 0 to 4. These two scores are then passed through a shallow neural network classifier (ANN) to predict the final DR stage. The public datasets STARE, DRIVE, CHASE DB1, and APTOS are used for training and evaluation. Our method achieves an accuracy of 94.80% and Quadratic Weighted Kappa (QWK) score of 0.9254, and outperform many state-of-the-art methods.



### Context-Aware Unsupervised Clustering for Person Search
- **Arxiv ID**: http://arxiv.org/abs/2110.01341v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.01341v1)
- **Published**: 2021-10-04 11:39:18+00:00
- **Updated**: 2021-10-04 11:39:18+00:00
- **Authors**: Byeong-Ju Han, Kuhyeun Ko, Jae-Young Sim
- **Comment**: None
- **Journal**: None
- **Summary**: The existing person search methods use the annotated labels of person identities to train deep networks in a supervised manner that requires a huge amount of time and effort for human labeling. In this paper, we first introduce a novel framework of person search that is able to train the network in the absence of the person identity labels, and propose efficient unsupervised clustering methods to substitute the supervision process using annotated person identity labels. Specifically, we propose a hard negative mining scheme based on the uniqueness property that only a single person has the same identity to a given query person in each image. We also propose a hard positive mining scheme by using the contextual information of co-appearance that neighboring persons in one image tend to appear simultaneously in other images. The experimental results show that the proposed method achieves comparable performance to that of the state-of-the-art supervised person search methods, and furthermore outperforms the extended unsupervised person re-identification methods on the benchmark person search datasets.



### 3d sequential image mosaicing for underwater navigation and mapping
- **Arxiv ID**: http://arxiv.org/abs/2110.01382v1
- **DOI**: 10.5194/isprs-archives-XLIII-B2-2020-991-2020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01382v1)
- **Published**: 2021-10-04 12:32:51+00:00
- **Updated**: 2021-10-04 12:32:51+00:00
- **Authors**: E. Nocerino, F. Menna, B. Chemisky, P. Drap
- **Comment**: None
- **Journal**: International Archives of the Photogrammetry, Remote Sensing and
  Spatial Information Sciences, Copernicus GmbH (Copernicus Publications),
  2020, XLIII-B2-2020, pp.991-998
- **Summary**: Although fully autonomous mapping methods are becoming more and more common and reliable, still the human operator is regularly employed in many 3D surveying missions. In a number of underwater applications, divers or pilots of remotely operated vehicles (ROVs) are still considered irreplaceable, and tools for real-time visualization of the mapped scene are essential to support and maximize the navigation and surveying efforts. For underwater exploration, image mosaicing has proved to be a valid and effective approach to visualize large mapped areas, often employed in conjunction with autonomous underwater vehicles (AUVs) and ROVs. In this work, we propose the use of a modified image mosaicing algorithm that coupled with image-based real-time navigation and mapping algorithms provides two visual navigation aids. The first is a classic image mosaic, where the recorded and processed images are incrementally added, named 2D sequential image mosaicing (2DSIM). The second one geometrically transform the images so that they are projected as planar point clouds in the 3D space providing an incremental point cloud mosaicing, named 3D sequential image plane projection (3DSIP). In the paper, the implemented procedure is detailed, and experiments in different underwater scenarios presented and discussed. Technical considerations about computational efforts, frame rate capabilities and scalability to different and more compact architectures (i.e. embedded systems) is also provided.



### Seeking Similarities over Differences: Similarity-based Domain Alignment for Adaptive Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.01428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01428v1)
- **Published**: 2021-10-04 13:09:56+00:00
- **Updated**: 2021-10-04 13:09:56+00:00
- **Authors**: Farzaneh Rezaeianaran, Rakshith Shetty, Rahaf Aljundi, Daniel Olmeda Reino, Shanshan Zhang, Bernt Schiele
- **Comment**: Accepted in ICCV 2021
- **Journal**: None
- **Summary**: In order to robustly deploy object detectors across a wide range of scenarios, they should be adaptable to shifts in the input distribution without the need to constantly annotate new data. This has motivated research in Unsupervised Domain Adaptation (UDA) algorithms for detection. UDA methods learn to adapt from labeled source domains to unlabeled target domains, by inducing alignment between detector features from source and target domains. Yet, there is no consensus on what features to align and how to do the alignment. In our work, we propose a framework that generalizes the different components commonly used by UDA methods laying the ground for an in-depth analysis of the UDA design space. Specifically, we propose a novel UDA algorithm, ViSGA, a direct implementation of our framework, that leverages the best design choices and introduces a simple but effective method to aggregate features at instance-level based on visual similarity before inducing group alignment via adversarial training. We show that both similarity-based grouping and adversarial training allows our model to focus on coarsely aligning feature groups, without being forced to match all instances across loosely aligned domains. Finally, we examine the applicability of ViSGA to the setting where labeled data are gathered from different sources. Experiments show that not only our method outperforms previous single-source approaches on Sim2Real and Adverse Weather, but also generalizes well to the multi-source setting.



### Instrumental Variable-Driven Domain Generalization with Unobserved Confounders
- **Arxiv ID**: http://arxiv.org/abs/2110.01438v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01438v2)
- **Published**: 2021-10-04 13:32:57+00:00
- **Updated**: 2023-05-25 08:21:33+00:00
- **Authors**: Junkun Yuan, Xu Ma, Ruoxuan Xiong, Mingming Gong, Xiangyu Liu, Fei Wu, Lanfen Lin, Kun Kuang
- **Comment**: Accepted by ACM Transactions on Knowledge Discovery from Data (TKDD)
- **Journal**: None
- **Summary**: Domain generalization (DG) aims to learn from multiple source domains a model that can generalize well on unseen target domains. Existing DG methods mainly learn the representations with invariant marginal distribution of the input features, however, the invariance of the conditional distribution of the labels given the input features is more essential for unknown domain prediction. Meanwhile, the existing of unobserved confounders which affect the input features and labels simultaneously cause spurious correlation and hinder the learning of the invariant relationship contained in the conditional distribution. Interestingly, with a causal view on the data generating process, we find that the input features of one domain are valid instrumental variables for other domains. Inspired by this finding, we propose an instrumental variable-driven DG method (IV-DG) by removing the bias of the unobserved confounders with two-stage learning. In the first stage, it learns the conditional distribution of the input features of one domain given input features of another domain. In the second stage, it estimates the relationship by predicting labels with the learned conditional distribution. Theoretical analyses and simulation experiments show that it accurately captures the invariant relationship. Extensive experiments on real-world datasets demonstrate that IV-DG method yields state-of-the-art results.



### A new weakly supervised approach for ALS point cloud semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.01462v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01462v2)
- **Published**: 2021-10-04 14:00:23+00:00
- **Updated**: 2021-10-06 11:37:36+00:00
- **Authors**: Puzuo Wang, Wei Yao
- **Comment**: None
- **Journal**: None
- **Summary**: While there are novel point cloud semantic segmentation schemes that continuously surpass state-of-the-art results, the success of learning an effective model usually rely on the availability of abundant labeled data. However, data annotation is a time-consuming and labor-intensive task, particularly for large-scale airborne laser scanning (ALS) point clouds involving multiple classes in urban areas. Thus, how to attain promising results while largely reducing labeling works become an essential issue. In this study, we propose a deep-learning based weakly supervised framework for semantic segmentation of ALS point clouds, exploiting potential information from unlabeled data subject to incomplete and sparse labels. Entropy regularization is introduced to penalize the class overlap in predictive probability. Additionally, a consistency constraint by minimizing the discrepancy distance between instant and ensemble predictions is designed to improve the robustness of predictions. Finally, we propose an online soft pseudo-labeling strategy to create extra supervisory sources in an efficient and nonpaprametric way. Extensive experimental analysis using three benchmark datasets demonstrates that in case of sparse point annotations, our proposed method significantly boosts the classification performance without compromising the computational efficiency. It outperforms current weakly supervised methods and achieves a comparable result against full supervision competitors. For the ISPRS 3D Labeling Vaihingen data, by using only 0.1% of labels, our method achieves an overall accuracy of 83.0% and an average F1 score of 70.0%, which have increased by 6.9% and 12.8% respectively, compared to model trained by sparse label information only.



### Fine-Grained Neural Network Explanation by Identifying Input Features with Predictive Information
- **Arxiv ID**: http://arxiv.org/abs/2110.01471v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01471v2)
- **Published**: 2021-10-04 14:13:42+00:00
- **Updated**: 2021-12-07 22:26:38+00:00
- **Authors**: Yang Zhang, Ashkan Khakzar, Yawei Li, Azade Farshad, Seong Tae Kim, Nassir Navab
- **Comment**: NeurIPS 2021 (Neural Information Processing Systems)
- **Journal**: None
- **Summary**: One principal approach for illuminating a black-box neural network is feature attribution, i.e. identifying the importance of input features for the network's prediction. The predictive information of features is recently proposed as a proxy for the measure of their importance. So far, the predictive information is only identified for latent features by placing an information bottleneck within the network. We propose a method to identify features with predictive information in the input domain. The method results in fine-grained identification of input features' information and is agnostic to network architecture. The core idea of our method is leveraging a bottleneck on the input that only lets input features associated with predictive latent features pass through. We compare our method with several feature attribution methods using mainstream feature attribution evaluation experiments. The code is publicly available.



### Distributed Learning Approaches for Automated Chest X-Ray Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2110.01474v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01474v1)
- **Published**: 2021-10-04 14:22:29+00:00
- **Updated**: 2021-10-04 14:22:29+00:00
- **Authors**: Edoardo Giacomello, Michele Cataldo, Daniele Loiacono, Pier Luca Lanzi
- **Comment**: 8 pages, 1 figure
- **Journal**: None
- **Summary**: Deep Learning has established in the latest years as a successful approach to address a great variety of tasks. Healthcare is one of the most promising field of application for Deep Learning approaches since it would allow to help clinicians to analyze patient data and perform diagnoses. However, despite the vast amount of data collected every year in hospitals and other clinical institutes, privacy regulations on sensitive data - such as those related to health - pose a serious challenge to the application of these methods. In this work, we focus on strategies to cope with privacy issues when a consortium of healthcare institutions needs to train machine learning models for identifying a particular disease, comparing the performances of two recent distributed learning approaches - Federated Learning and Split Learning - on the task of Automated Chest X-Ray Diagnosis. In particular, in our analysis we investigated the impact of different data distributions in client data and the possible policies on the frequency of data exchange between the institutions.



### Learning Online Visual Invariances for Novel Objects via Supervised and Self-Supervised Training
- **Arxiv ID**: http://arxiv.org/abs/2110.01476v3
- **DOI**: 10.1016/j.neunet.2022.02.017
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.01476v3)
- **Published**: 2021-10-04 14:29:43+00:00
- **Updated**: 2022-01-14 17:03:01+00:00
- **Authors**: Valerio Biscione, Jeffrey S. Bowers
- **Comment**: None
- **Journal**: Neural Networks, Volume 150, 2022, Pages 222-236, ISSN 0893-6080,
- **Summary**: Humans can identify objects following various spatial transformations such as scale and viewpoint. This extends to novel objects, after a single presentation at a single pose, sometimes referred to as online invariance. CNNs have been proposed as a compelling model of human vision, but their ability to identify objects across transformations is typically tested on held-out samples of trained categories after extensive data augmentation. This paper assesses whether standard CNNs can support human-like online invariance by training models to recognize images of synthetic 3D objects that undergo several transformations: rotation, scaling, translation, brightness, contrast, and viewpoint. Through the analysis of models' internal representations, we show that standard supervised CNNs trained on transformed objects can acquire strong invariances on novel classes even when trained with as few as 50 objects taken from 10 classes. This extended to a different dataset of photographs of real objects. We also show that these invariances can be acquired in a self-supervised way, through solving the same/different task. We suggest that this latter approach may be similar to how humans acquire invariances.



### Skill Induction and Planning with Latent Language
- **Arxiv ID**: http://arxiv.org/abs/2110.01517v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.01517v2)
- **Published**: 2021-10-04 15:36:32+00:00
- **Updated**: 2022-05-02 12:04:59+00:00
- **Authors**: Pratyusha Sharma, Antonio Torralba, Jacob Andreas
- **Comment**: 14 pages, 7 figures
- **Journal**: None
- **Summary**: We present a framework for learning hierarchical policies from demonstrations, using sparse natural language annotations to guide the discovery of reusable skills for autonomous decision-making. We formulate a generative model of action sequences in which goals generate sequences of high-level subtask descriptions, and these descriptions generate sequences of low-level actions. We describe how to train this model using primarily unannotated demonstrations by parsing demonstrations into sequences of named high-level subtasks, using only a small number of seed annotations to ground language in action. In trained models, natural language commands index a combinatorial library of skills; agents can use these skills to plan by generating high-level instruction sequences tailored to novel goals. We evaluate this approach in the ALFRED household simulation environment, providing natural language annotations for only 10% of demonstrations. It achieves task completion rates comparable to state-of-the-art models (outperforming several recent methods with access to ground-truth plans during training and evaluation) while providing structured and human-readable high-level plans.



### Weak-shot Semantic Segmentation by Transferring Semantic Affinity and Boundary
- **Arxiv ID**: http://arxiv.org/abs/2110.01519v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01519v2)
- **Published**: 2021-10-04 15:37:25+00:00
- **Updated**: 2022-10-16 06:58:46+00:00
- **Authors**: Siyuan Zhou, Li Niu, Jianlou Si, Chen Qian, Liqing Zhang
- **Comment**: 29 pages, 8 figures
- **Journal**: None
- **Summary**: Weakly-supervised semantic segmentation (WSSS) with image-level labels has been widely studied to relieve the annotation burden of the traditional segmentation task. In this paper, we show that existing fully-annotated base categories can help segment objects of novel categories with only image-level labels, even if base categories and novel categories have no overlap. We refer to this task as weak-shot semantic segmentation, which could also be treated as WSSS with auxiliary fully-annotated categories. Recent advanced WSSS methods usually obtain class activation maps (CAMs) and refine them by affinity propagation. Based on the observation that semantic affinity and boundary are class-agnostic, we propose a method under the WSSS framework to transfer semantic affinity and boundary from base to novel categories. As a result, we find that pixel-level annotation of base categories can facilitate affinity learning and propagation, leading to higher-quality CAMs of novel categories. Extensive experiments on PASCAL VOC 2012 dataset prove that our method significantly outperforms WSSS baselines on novel categories.



### Balanced Masked and Standard Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.01521v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.01521v1)
- **Published**: 2021-10-04 15:41:05+00:00
- **Updated**: 2021-10-04 15:41:05+00:00
- **Authors**: Delong Qi, Kangli Hu, Weijun Tan, Qi Yao, Jingfeng Liu
- **Comment**: None
- **Journal**: 2021 ICCV Workshops
- **Summary**: We present the improved network architecture, data augmentation, and training strategies for the Webface track and Insightface/Glint360K track of the masked face recognition challenge of ICCV2021. One of the key goals is to have a balanced performance of masked and standard face recognition. In order to prevent the overfitting for the masked face recognition, we control the total number of masked faces by not more than 10\% of the total face recognition in the training dataset. We propose a few key changes to the face recognition network including a new stem unit, drop block, face detection and alignment using YOLO5Face, feature concatenation, a cycle cosine learning rate, etc. With this strategy, we achieve good and balanced performance for both masked and standard face recognition.



### Assessing glaucoma in retinal fundus photographs using Deep Feature Consistent Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2110.01534v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.01534v1)
- **Published**: 2021-10-04 16:06:49+00:00
- **Updated**: 2021-10-04 16:06:49+00:00
- **Authors**: Sayan Mandal, Alessandro A. Jammal, Felipe A. Medeiros
- **Comment**: Corresponding author: Felipe A. Medeiros
- **Journal**: None
- **Summary**: One of the leading causes of blindness is glaucoma, which is challenging to detect since it remains asymptomatic until the symptoms are severe. Thus, diagnosis is usually possible until the markers are easy to identify, i.e., the damage has already occurred. Early identification of glaucoma is generally made based on functional, structural, and clinical assessments. However, due to the nature of the disease, researchers still debate which markers qualify as a consistent glaucoma metric. Deep learning methods have partially solved this dilemma by bypassing the marker identification stage and analyzing high-level information directly to classify the data. Although favorable, these methods make expert analysis difficult as they provide no insight into the model discrimination process. In this paper, we overcome this using deep generative networks, a deep learning model that learns complicated, high-dimensional probability distributions. We train a Deep Feature consistent Variational Autoencoder (DFC-VAE) to reconstruct optic disc images. We show that a small-sized latent space obtained from the DFC-VAE can learn the high-dimensional glaucoma data distribution and provide discriminatory evidence between normal and glaucoma eyes. Latent representations of size as low as 128 from our model got a 0.885 area under the receiver operating characteristic curve when trained with Support Vector Classifier.



### Traffic Flow Forecasting with Maintenance Downtime via Multi-Channel Attention-Based Spatio-Temporal Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.01535v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01535v1)
- **Published**: 2021-10-04 16:07:37+00:00
- **Updated**: 2021-10-04 16:07:37+00:00
- **Authors**: Yuanjie Lu, Parastoo Kamranfar, David Lattanzi, Amarda Shehu
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Forecasting traffic flows is a central task in intelligent transportation system management. Graph structures have shown promise as a modeling framework, with recent advances in spatio-temporal modeling via graph convolution neural networks, improving the performance or extending the prediction horizon on traffic flows. However, a key shortcoming of state-of-the-art methods is their inability to take into account information of various modalities, for instance the impact of maintenance downtime on traffic flows. This is the issue we address in this paper. Specifically, we propose a novel model to predict traffic speed under the impact of construction work. The model is based on the powerful attention-based spatio-temporal graph convolution architecture but utilizes various channels to integrate different sources of information, explicitly builds spatio-temporal dependencies among traffic states, captures the relationships between heterogeneous roadway networks, and then predicts changes in traffic flow resulting from maintenance downtime events. The model is evaluated on two benchmark datasets and a novel dataset we have collected over the bustling Tyson's corner region in Northern Virginia. Extensive comparative experiments and ablation studies show that the proposed model can capture complex and nonlinear spatio-temporal relationships across a transportation corridor, outperforming baseline models.



### Causal Representation Learning for Context-Aware Face Transfer
- **Arxiv ID**: http://arxiv.org/abs/2110.01571v4
- **DOI**: None
- **Categories**: **cs.CV**, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2110.01571v4)
- **Published**: 2021-10-04 17:10:30+00:00
- **Updated**: 2021-11-17 07:44:15+00:00
- **Authors**: Gege Gao, Huaibo Huang, Chaoyou Fu, Ran He
- **Comment**: None
- **Journal**: None
- **Summary**: Human face synthesis involves transferring knowledge about the identity and identity-dependent face shape (IDFS) of a human face to target face images where the context (e.g., facial expressions, head poses, and other background factors) may change dramatically. Human faces are non-rigid, so facial expression leads to deformation of face shape, and head pose also affects the face observed in 2D images. A key challenge in face transfer is to match the face with unobserved new contexts, adapting the face appearance to different poses and expressions accordingly. In this work, we find a way to provide prior knowledge for generative models to reason about the appropriate appearance of a human face in response to various expressions and poses. We propose a novel context-aware face transfer method, called CarTrans, that incorporates causal effects of contextual factors into face representation, and thus is able to be aware of the uncertainty of new contexts. We estimate the effect of facial expression and head pose in terms of counterfactual inference by designing a controlled intervention trial, thus avoiding the requirement of a large number of observations to cover the pose-expression space well. Moreover, we propose a kernel regression-based encoder that eliminates the identity specificity of target faces when encoding contextual information from target images. The resulting method shows impressive performance, allowing fine-grained control over face shape and appearance under various contextual conditions.



### Fast Scalable Image Restoration using Total Variation Priors and Expectation Propagation
- **Arxiv ID**: http://arxiv.org/abs/2110.01585v1
- **DOI**: 10.1109/TIP.2022.3202092
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01585v1)
- **Published**: 2021-10-04 17:28:41+00:00
- **Updated**: 2021-10-04 17:28:41+00:00
- **Authors**: Dan Yao, Stephen McLaughlin, Yoann Altmann
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: This paper presents a scalable approximate Bayesian method for image restoration using total variation (TV) priors. In contrast to most optimization methods based on maximum a posteriori estimation, we use the expectation propagation (EP) framework to approximate minimum mean squared error (MMSE) estimators and marginal (pixel-wise) variances, without resorting to Monte Carlo sampling. For the classical anisotropic TV-based prior, we also propose an iterative scheme to automatically adjust the regularization parameter via expectation-maximization (EM). Using Gaussian approximating densities with diagonal covariance matrices, the resulting method allows highly parallelizable steps and can scale to large images for denoising, deconvolution and compressive sensing (CS) problems. The simulation results illustrate that such EP methods can provide a posteriori estimates on par with those obtained via sampling methods but at a fraction of the computational cost. Moreover, EP does not exhibit strong underestimation of posteriori variances, in contrast to variational Bayes alternatives.



### Effectiveness of Optimization Algorithms in Deep Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.01598v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2110.01598v1)
- **Published**: 2021-10-04 17:50:51+00:00
- **Updated**: 2021-10-04 17:50:51+00:00
- **Authors**: Zhaoyang Zhu, Haozhe Sun, Chi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Adam is applied widely to train neural networks. Different kinds of Adam methods with different features pop out. Recently two new adam optimizers, AdaBelief and Padam are introduced among the community. We analyze these two adam optimizers and compare them with other conventional optimizers (Adam, SGD + Momentum) in the scenario of image classification. We evaluate the performance of these optimization algorithms on AlexNet and simplified versions of VGGNet, ResNet using the EMNIST dataset. (Benchmark algorithm is available at \hyperref[https://github.com/chuiyunjun/projectCSC413]{https://github.com/chuiyunjun/projectCSC413}).



### CertainNet: Sampling-free Uncertainty Estimation for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.01604v2
- **DOI**: 10.1109/LRA.2021.3130976
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.01604v2)
- **Published**: 2021-10-04 17:59:31+00:00
- **Updated**: 2021-12-28 11:50:16+00:00
- **Authors**: Stefano Gasperini, Jan Haug, Mohammad-Ali Nikouei Mahani, Alvaro Marcos-Ramiro, Nassir Navab, Benjamin Busam, Federico Tombari
- **Comment**: Published at IEEE Robotics and Automation Letters (RA-L)
- **Journal**: None
- **Summary**: Estimating the uncertainty of a neural network plays a fundamental role in safety-critical settings. In perception for autonomous driving, measuring the uncertainty means providing additional calibrated information to downstream tasks, such as path planning, that can use it towards safe navigation. In this work, we propose a novel sampling-free uncertainty estimation method for object detection. We call it CertainNet, and it is the first to provide separate uncertainties for each output signal: objectness, class, location and size. To achieve this, we propose an uncertainty-aware heatmap, and exploit the neighboring bounding boxes provided by the detector at inference time. We evaluate the detection performance and the quality of the different uncertainty estimates separately, also with challenging out-of-domain samples: BDD100K and nuImages with models trained on KITTI. Additionally, we propose a new metric to evaluate location and size uncertainties. When transferring to unseen datasets, CertainNet generalizes substantially better than previous methods and an ensemble, while being real-time and providing high quality and comprehensive uncertainty estimates.



### An Experimental Evaluation on Deepfake Detection using Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.01640v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.01640v1)
- **Published**: 2021-10-04 18:02:56+00:00
- **Updated**: 2021-10-04 18:02:56+00:00
- **Authors**: Sreeraj Ramachandran, Aakash Varma Nadimpalli, Ajita Rattani
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: Significant advances in deep learning have obtained hallmark accuracy rates for various computer vision applications. However, advances in deep generative models have also led to the generation of very realistic fake content, also known as deepfakes, causing a threat to privacy, democracy, and national security. Most of the current deepfake detection methods are deemed as a binary classification problem in distinguishing authentic images or videos from fake ones using two-class convolutional neural networks (CNNs). These methods are based on detecting visual artifacts, temporal or color inconsistencies produced by deep generative models. However, these methods require a large amount of real and fake data for model training and their performance drops significantly in cross dataset evaluation with samples generated using advanced deepfake generation techniques. In this paper, we thoroughly evaluate the efficacy of deep face recognition in identifying deepfakes, using different loss functions and deepfake generation techniques. Experimental investigations on challenging Celeb-DF and FaceForensics++ deepfake datasets suggest the efficacy of deep face recognition in identifying deepfakes over two-class CNNs and the ocular modality. Reported results suggest a maximum Area Under Curve (AUC) of 0.98 and an Equal Error Rate (EER) of 7.1% in detecting deepfakes using face recognition on the Celeb-DF dataset. This EER is lower by 16.6% compared to the EER obtained for the two-class CNN and the ocular modality on the Celeb-DF dataset. Further on the FaceForensics++ dataset, an AUC of 0.99 and EER of 2.04% were obtained. The use of biometric facial recognition technology has the advantage of bypassing the need for a large amount of fake data for model training and obtaining better generalizability to evolving deepfake creation techniques.



### Investigating Fairness of Ocular Biometrics Among Young, Middle-Aged, and Older Adults
- **Arxiv ID**: http://arxiv.org/abs/2110.01641v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.01641v1)
- **Published**: 2021-10-04 18:03:18+00:00
- **Updated**: 2021-10-04 18:03:18+00:00
- **Authors**: Anoop Krishnan, Ali Almadan, Ajita Rattani
- **Comment**: 6 Pages, Fairness of Ocular Biometrics
- **Journal**: None
- **Summary**: A number of studies suggest bias of the face biometrics, i.e., face recognition and soft-biometric estimation methods, across gender, race, and age groups. There is a recent urge to investigate the bias of different biometric modalities toward the deployment of fair and trustworthy biometric solutions. Ocular biometrics has obtained increased attention from academia and industry due to its high accuracy, security, privacy, and ease of use in mobile devices. A recent study in $2020$ also suggested the fairness of ocular-based user recognition across males and females. This paper aims to evaluate the fairness of ocular biometrics in the visible spectrum among age groups; young, middle, and older adults. Thanks to the availability of the latest large-scale 2020 UFPR ocular biometric dataset, with subjects acquired in the age range 18 - 79 years, to facilitate this study. Experimental results suggest the overall equivalent performance of ocular biometrics across gender and age groups in user verification and gender classification. Performance difference for older adults at lower false match rate and young adults was noted at user verification and age classification, respectively. This could be attributed to inherent characteristics of the biometric data from these age groups impacting specific applications, which suggest a need for advancement in sensor technology and software solutions.



### Pixel-Level Bijective Matching for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.01644v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01644v3)
- **Published**: 2021-10-04 18:15:45+00:00
- **Updated**: 2021-11-12 07:24:15+00:00
- **Authors**: Suhwan Cho, Heansung Lee, Minjung Kim, Sungjun Jang, Sangyoun Lee
- **Comment**: WACV 2022
- **Journal**: None
- **Summary**: Semi-supervised video object segmentation (VOS) aims to track the designated objects present in the initial frame of a video at the pixel level. To fully exploit the appearance information of an object, pixel-level feature matching is widely used in VOS. Conventional feature matching runs in a surjective manner, i.e., only the best matches from the query frame to the reference frame are considered. Each location in the query frame refers to the optimal location in the reference frame regardless of how often each reference frame location is referenced. This works well in most cases and is robust against rapid appearance variations, but may cause critical errors when the query frame contains background distractors that look similar to the target object. To mitigate this concern, we introduce a bijective matching mechanism to find the best matches from the query frame to the reference frame and vice versa. Before finding the best matches for the query frame pixels, the optimal matches for the reference frame pixels are first considered to prevent each reference frame pixel from being overly referenced. As this mechanism operates in a strict manner, i.e., pixels are connected if and only if they are the sure matches for each other, it can effectively eliminate background distractors. In addition, we propose a mask embedding module to improve the existing mask propagation method. By embedding multiple historic masks with coordinate information, it can effectively capture the position information of a target object.



### VTAMIQ: Transformers for Attention Modulated Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2110.01655v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2110.01655v1)
- **Published**: 2021-10-04 18:35:29+00:00
- **Updated**: 2021-10-04 18:35:29+00:00
- **Authors**: Andrei Chubarau, James Clark
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: Following the major successes of self-attention and Transformers for image analysis, we investigate the use of such attention mechanisms in the context of Image Quality Assessment (IQA) and propose a novel full-reference IQA method, Vision Transformer for Attention Modulated Image Quality (VTAMIQ). Our method achieves competitive or state-of-the-art performance on the existing IQA datasets and significantly outperforms previous metrics in cross-database evaluations. Most patch-wise IQA methods treat each patch independently; this partially discards global information and limits the ability to model long-distance interactions. We avoid this problem altogether by employing a transformer to encode a sequence of patches as a single global representation, which by design considers interdependencies between patches. We rely on various attention mechanisms -- first with self-attention within the Transformer, and second with channel attention within our difference modulation network -- specifically to reveal and enhance the more salient features throughout our architecture. With large-scale pre-training for both classification and IQA tasks, VTAMIQ generalizes well to unseen sets of images and distortions, further demonstrating the strength of transformer-based networks for vision modelling.



### HDR-cGAN: Single LDR to HDR Image Translation using Conditional GAN
- **Arxiv ID**: http://arxiv.org/abs/2110.01660v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01660v2)
- **Published**: 2021-10-04 18:50:35+00:00
- **Updated**: 2021-10-15 06:16:33+00:00
- **Authors**: Prarabdh Raipurkar, Rohil Pal, Shanmuganathan Raman
- **Comment**: Accepted in ICVGIP 2021
- **Journal**: None
- **Summary**: The prime goal of digital imaging techniques is to reproduce the realistic appearance of a scene. Low Dynamic Range (LDR) cameras are incapable of representing the wide dynamic range of the real-world scene. The captured images turn out to be either too dark (underexposed) or too bright (overexposed). Specifically, saturation in overexposed regions makes the task of reconstructing a High Dynamic Range (HDR) image from single LDR image challenging. In this paper, we propose a deep learning based approach to recover details in the saturated areas while reconstructing the HDR image. We formulate this problem as an image-to-image (I2I) translation task. To this end, we present a novel conditional GAN (cGAN) based framework trained in an end-to-end fashion over the HDR-REAL and HDR-SYNTH datasets. Our framework uses an overexposed mask obtained from a pre-trained segmentation model to facilitate the hallucination task of adding details in the saturated regions. We demonstrate the effectiveness of the proposed method by performing an extensive quantitative and qualitative comparison with several state-of-the-art single-image HDR reconstruction techniques.



### Deep Learning Approach Protecting Privacy in Camera-Based Critical Applications
- **Arxiv ID**: http://arxiv.org/abs/2110.01676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01676v1)
- **Published**: 2021-10-04 19:16:27+00:00
- **Updated**: 2021-10-04 19:16:27+00:00
- **Authors**: Gautham Ramajayam, Tao Sun, Chiu C. Tan, Lannan Luo, Haibin Ling
- **Comment**: None
- **Journal**: None
- **Summary**: Many critical applications rely on cameras to capture video footage for analytical purposes. This has led to concerns about these cameras accidentally capturing more information than is necessary. In this paper, we propose a deep learning approach towards protecting privacy in camera-based systems. Instead of specifying specific objects (e.g. faces) are privacy sensitive, our technique distinguishes between salient (visually prominent) and non-salient objects based on the intuition that the latter is unlikely to be needed by the application.



### How You Move Your Head Tells What You Do: Self-supervised Video Representation Learning with Egocentric Cameras and IMU Sensors
- **Arxiv ID**: http://arxiv.org/abs/2110.01680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01680v1)
- **Published**: 2021-10-04 19:25:15+00:00
- **Updated**: 2021-10-04 19:25:15+00:00
- **Authors**: Satoshi Tsutsui, Ruta Desai, Karl Ridgeway
- **Comment**: Accepted to 2021 ICCV Workshop on Egocentric Perception, Interaction
  and Computing (EPIC)
- **Journal**: None
- **Summary**: Understanding users' activities from head-mounted cameras is a fundamental task for Augmented and Virtual Reality (AR/VR) applications. A typical approach is to train a classifier in a supervised manner using data labeled by humans. This approach has limitations due to the expensive annotation cost and the closed coverage of activity labels. A potential way to address these limitations is to use self-supervised learning (SSL). Instead of relying on human annotations, SSL leverages intrinsic properties of data to learn representations. We are particularly interested in learning egocentric video representations benefiting from the head-motion generated by users' daily activities, which can be easily obtained from IMU sensors embedded in AR/VR devices. Towards this goal, we propose a simple but effective approach to learn video representation by learning to tell the corresponding pairs of video clip and head-motion. We demonstrate the effectiveness of our learned representation for recognizing egocentric activities of people and dogs.



### Let there be a clock on the beach: Reducing Object Hallucination in Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2110.01705v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.01705v2)
- **Published**: 2021-10-04 20:25:22+00:00
- **Updated**: 2021-11-02 15:25:41+00:00
- **Authors**: Ali Furkan Biten, Lluis Gomez, Dimosthenis Karatzas
- **Comment**: Accepted to WACV 2022
- **Journal**: None
- **Summary**: Explaining an image with missing or non-existent objects is known as object bias (hallucination) in image captioning. This behaviour is quite common in the state-of-the-art captioning models which is not desirable by humans. To decrease the object hallucination in captioning, we propose three simple yet efficient training augmentation method for sentences which requires no new training data or increase in the model size. By extensive analysis, we show that the proposed methods can significantly diminish our models' object bias on hallucination metrics. Moreover, we experimentally demonstrate that our methods decrease the dependency on the visual features. All of our code, configuration files and model weights will be made public.



### AdjointBackMapV2: Precise Reconstruction of Arbitrary CNN Unit's Activation via Adjoint Operators
- **Arxiv ID**: http://arxiv.org/abs/2110.01736v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01736v1)
- **Published**: 2021-10-04 22:56:11+00:00
- **Updated**: 2021-10-04 22:56:11+00:00
- **Authors**: Qing Wan, Yoonsuck Choe
- **Comment**: 19 pages, 8 figures
- **Journal**: None
- **Summary**: Adjoint operators have been found to be effective in the exploration of CNN's inner workings [1]. However, the previous no-bias assumption restricted its generalization. We overcome the restriction via embedding input images into an extended normed space that includes bias in all CNN layers as part of the extended input space and propose an adjoint-operator-based algorithm that maps high-level weights back to the extended input space for reconstructing an effective hypersurface. Such hypersurface can be computed for an arbitrary unit in the CNN, and we prove that this reconstructed hypersurface, when multiplied by the original input (through an inner product), will precisely replicate the output value of each unit. We show experimental results based on the CIFAR-10 dataset that the proposed approach achieves near $0$ reconstruction error.



