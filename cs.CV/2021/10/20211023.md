# Arxiv Papers in cs.CV on 2021-10-23
### MTGLS: Multi-Task Gaze Estimation with Limited Supervision
- **Arxiv ID**: http://arxiv.org/abs/2110.12100v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12100v2)
- **Published**: 2021-10-23 00:20:23+00:00
- **Updated**: 2021-12-13 13:07:14+00:00
- **Authors**: Shreya Ghosh, Munawar Hayat, Abhinav Dhall, Jarrod Knibbe
- **Comment**: None
- **Journal**: None
- **Summary**: Robust gaze estimation is a challenging task, even for deep CNNs, due to the non-availability of large-scale labeled data. Moreover, gaze annotation is a time-consuming process and requires specialized hardware setups. We propose MTGLS: a Multi-Task Gaze estimation framework with Limited Supervision, which leverages abundantly available non-annotated facial image data. MTGLS distills knowledge from off-the-shelf facial image analysis models, and learns strong feature representations of human eyes, guided by three complementary auxiliary signals: (a) the line of sight of the pupil (i.e. pseudo-gaze) defined by the localized facial landmarks, (b) the head-pose given by Euler angles, and (c) the orientation of the eye patch (left/right eye). To overcome inherent noise in the supervisory signals, MTGLS further incorporates a noise distribution modelling approach. Our experimental results show that MTGLS learns highly generalized representations which consistently perform well on a range of datasets. Our proposed framework outperforms the unsupervised state-of-the-art on CAVE (by 6.43%) and even supervised state-of-the-art methods on Gaze360 (by 6.59%) datasets.



### ConformalLayers: A non-linear sequential neural network with associative layers
- **Arxiv ID**: http://arxiv.org/abs/2110.12108v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12108v2)
- **Published**: 2021-10-23 01:33:13+00:00
- **Updated**: 2021-11-09 13:46:27+00:00
- **Authors**: Eduardo Vera Sousa, Leandro A. F. Fernandes, Cristina Nader Vasconcelos
- **Comment**: Best Paper on Pattern Recognition and Related Field at SIBGRAPI 2021
  -- 34th Conference on Graphics, Patterns and Images
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have been widely applied. But as the CNNs grow, the number of arithmetic operations and memory footprint also increase. Furthermore, typical non-linear activation functions do not allow associativity of the operations encoded by consecutive layers, preventing the simplification of intermediate steps by combining them. We present a new activation function that allows associativity between sequential layers of CNNs. Even though our activation function is non-linear, it can be represented by a sequence of linear operations in the conformal model for Euclidean geometry. In this domain, operations like, but not limited to, convolution, average pooling, and dropout remain linear. We take advantage of associativity to combine all the "conformal layers" and make the cost of inference constant regardless of the depth of the network.



### Dense Dual-Attention Network for Light Field Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2110.12114v1
- **DOI**: 10.1109/TCSVT.2021.3121679
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12114v1)
- **Published**: 2021-10-23 02:10:47+00:00
- **Updated**: 2021-10-23 02:10:47+00:00
- **Authors**: Yu Mo, Yingqian Wang, Chao Xiao, Jungang Yang, Wei An
- **Comment**: Accept by IEEE Transactions on Circuits and Systems for Video
  Technology
- **Journal**: None
- **Summary**: Light field (LF) images can be used to improve the performance of image super-resolution (SR) because both angular and spatial information is available. It is challenging to incorporate distinctive information from different views for LF image SR. Moreover, the long-term information from the previous layers can be weakened as the depth of network increases. In this paper, we propose a dense dual-attention network for LF image SR. Specifically, we design a view attention module to adaptively capture discriminative features across different views and a channel attention module to selectively focus on informative information across all channels. These two modules are fed to two branches and stacked separately in a chain structure for adaptive fusion of hierarchical features and distillation of valid information. Meanwhile, a dense connection is used to fully exploit multi-level information. Extensive experiments demonstrate that our dense dual-attention mechanism can capture informative information across views and channels to improve SR performance. Comparative results show the advantage of our method over state-of-the-art methods on public datasets.



### RCNet: Reverse Feature Pyramid and Cross-scale Shift Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.12130v1
- **DOI**: 10.1145/3474085.3475708
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12130v1)
- **Published**: 2021-10-23 04:00:25+00:00
- **Updated**: 2021-10-23 04:00:25+00:00
- **Authors**: Zhuofan Zong, Qianggang Cao, Biao Leng
- **Comment**: Accepted by ACM MM2021
- **Journal**: None
- **Summary**: Feature pyramid networks (FPN) are widely exploited for multi-scale feature fusion in existing advanced object detection frameworks. Numerous previous works have developed various structures for bidirectional feature fusion, all of which are shown to improve the detection performance effectively. We observe that these complicated network structures require feature pyramids to be stacked in a fixed order, which introduces longer pipelines and reduces the inference speed. Moreover, semantics from non-adjacent levels are diluted in the feature pyramid since only features at adjacent pyramid levels are merged by the local fusion operation in a sequence manner. To address these issues, we propose a novel architecture named RCNet, which consists of Reverse Feature Pyramid (RevFP) and Cross-scale Shift Network (CSN). RevFP utilizes local bidirectional feature fusion to simplify the bidirectional pyramid inference pipeline. CSN directly propagates representations to both adjacent and non-adjacent levels to enable multi-scale features more correlative. Extensive experiments on the MS COCO dataset demonstrate RCNet can consistently bring significant improvements over both one-stage and two-stage detectors with subtle extra computational overhead. In particular, RetinaNet is boosted to 40.2 AP, which is 3.7 points higher than baseline, by replacing FPN with our proposed model. On COCO test-dev, RCNet can achieve very competitive performance with a single-model single-scale 50.5 AP. Codes will be made available.



### A Study of Multimodal Person Verification Using Audio-Visual-Thermal Data
- **Arxiv ID**: http://arxiv.org/abs/2110.12136v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12136v2)
- **Published**: 2021-10-23 04:41:03+00:00
- **Updated**: 2022-03-04 05:46:01+00:00
- **Authors**: Madina Abdrakhmanova, Saniya Abushakimova, Yerbolat Khassanov, Huseyin Atakan Varol
- **Comment**: 7 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: In this paper, we study an approach to multimodal person verification using audio, visual, and thermal modalities. The combination of audio and visual modalities has already been shown to be effective for robust person verification. From this perspective, we investigate the impact of further increasing the number of modalities by adding thermal images. In particular, we implemented unimodal, bimodal, and trimodal verification systems using state-of-the-art deep learning architectures and compared their performance under clean and noisy conditions. We also compared two popular fusion approaches based on simple score averaging and the soft attention mechanism. The experiment conducted on the SpeakingFaces dataset demonstrates the superior performance of the trimodal verification system. Specifically, on the easy test set, the trimodal system outperforms the best unimodal and bimodal systems by over 50% and 18% relative equal error rates, respectively, under both the clean and noisy conditions. On the hard test set, the trimodal system outperforms the best unimodal and bimodal systems by over 40% and 13% relative equal error rates, respectively, under both the clean and noisy conditions. To enable reproducibility of the experiment and facilitate research into multimodal person verification, we made our code, pretrained models, and preprocessed dataset freely available in our GitHub repository.



### Spatio-Temporal Graph Complementary Scattering Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.12150v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2110.12150v1)
- **Published**: 2021-10-23 06:02:43+00:00
- **Updated**: 2021-10-23 06:02:43+00:00
- **Authors**: Zida Cheng, Siheng Chen, Ya Zhang
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Spatio-temporal graph signal analysis has a significant impact on a wide range of applications, including hand/body pose action recognition. To achieve effective analysis, spatio-temporal graph convolutional networks (ST-GCN) leverage the powerful learning ability to achieve great empirical successes; however, those methods need a huge amount of high-quality training data and lack theoretical interpretation. To address this issue, the spatio-temporal graph scattering transform (ST-GST) was proposed to put forth a theoretically interpretable framework; however, the empirical performance of this approach is constrainted by the fully mathematical design. To benefit from both sides, this work proposes a novel complementary mechanism to organically combine the spatio-temporal graph scattering transform and neural networks, resulting in the proposed spatio-temporal graph complementary scattering networks (ST-GCSN). The essence is to leverage the mathematically designed graph wavelets with pruning techniques to cover major information and use trainable networks to capture complementary information. The empirical experiments on hand pose action recognition show that the proposed ST-GCSN outperforms both ST-GCN and ST-GST.



### Spectrum-to-Kernel Translation for Accurate Blind Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2110.12151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12151v1)
- **Published**: 2021-10-23 06:03:22+00:00
- **Updated**: 2021-10-23 06:03:22+00:00
- **Authors**: Guangpin Tao, Xiaozhong Ji, Wenzhuo Wang, Shuo Chen, Chuming Lin, Yun Cao, Tong Lu, Donghao Luo, Ying Tai
- **Comment**: Accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: Deep-learning based Super-Resolution (SR) methods have exhibited promising performance under non-blind setting where blur kernel is known. However, blur kernels of Low-Resolution (LR) images in different practical applications are usually unknown. It may lead to significant performance drop when degradation process of training images deviates from that of real images. In this paper, we propose a novel blind SR framework to super-resolve LR images degraded by arbitrary blur kernel with accurate kernel estimation in frequency domain. To our best knowledge, this is the first deep learning method which conducts blur kernel estimation in frequency domain. Specifically, we first demonstrate that feature representation in frequency domain is more conducive for blur kernel reconstruction than in spatial domain. Next, we present a Spectrum-to-Kernel (S$2$K) network to estimate general blur kernels in diverse forms. We use a Conditional GAN (CGAN) combined with SR-oriented optimization target to learn the end-to-end translation from degraded images' spectra to unknown kernels. Extensive experiments on both synthetic and real-world images demonstrate that our proposed method sufficiently reduces blur kernel estimation error, thus enables the off-the-shelf non-blind SR methods to work under blind setting effectively, and achieves superior performance over state-of-the-art blind SR methods, averagely by 1.39dB, 0.48dB on commom blind SR setting (with Gaussian kernels) for scales $2\times$ and $4\times$, respectively.



### Vertebrae localization, segmentation and identification using a graph optimization and an anatomic consistency cycle
- **Arxiv ID**: http://arxiv.org/abs/2110.12177v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12177v3)
- **Published**: 2021-10-23 09:21:35+00:00
- **Updated**: 2022-06-24 15:02:14+00:00
- **Authors**: Di Meng, Edmond Boyer, Sergi Pujades
- **Comment**: None
- **Journal**: None
- **Summary**: Vertebrae localization, segmentation and identification in CT images is key to numerous clinical applications. While deep learning strategies have brought to this field significant improvements over recent years, transitional and pathological vertebrae are still plaguing most existing approaches as a consequence of their poor representation in training datasets. Alternatively, proposed non-learning based methods take benefit of prior knowledge to handle such particular cases. In this work we propose to combine both strategies. To this purpose we introduce an iterative cycle in which individual vertebrae are recursively localized, segmented and identified using deep-networks, while anatomic consistency is enforced using statistical priors. In this strategy, the transitional vertebrae identification is handled by encoding their configurations in a graphical model that aggregates local deep-network predictions into an anatomically consistent final result. Our approach achieves state-of-the-art results on the VerSe20 challenge benchmark, and outperforms all methods on transitional vertebrae as well as the generalization to the VerSe19 challenge benchmark. Furthermore, our method can detect and report inconsistent spine regions that do not satisfy the anatomic consistency priors. Our code and model are openly available for research purposes.



### An attention-driven hierarchical multi-scale representation for visual recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.12178v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12178v1)
- **Published**: 2021-10-23 09:22:22+00:00
- **Updated**: 2021-10-23 09:22:22+00:00
- **Authors**: Zachary Wharton, Ardhendu Behera, Asish Bera
- **Comment**: Accepted in the 32nd British Machine Vision Conference (BMVC) 2021
- **Journal**: BMVC 2021
- **Summary**: Convolutional Neural Networks (CNNs) have revolutionized the understanding of visual content. This is mainly due to their ability to break down an image into smaller pieces, extract multi-scale localized features and compose them to construct highly expressive representations for decision making. However, the convolution operation is unable to capture long-range dependencies such as arbitrary relations between pixels since it operates on a fixed-size window. Therefore, it may not be suitable for discriminating subtle changes (e.g. fine-grained visual recognition). To this end, our proposed method captures the high-level long-range dependencies by exploring Graph Convolutional Networks (GCNs), which aggregate information by establishing relationships among multi-scale hierarchical regions. These regions consist of smaller (closer look) to larger (far look), and the dependency between regions is modeled by an innovative attention-driven message propagation, guided by the graph structure to emphasize the neighborhoods of a given region. Our approach is simple yet extremely effective in solving both the fine-grained and generic visual classification problems. It outperforms the state-of-the-arts with a significant margin on three and is very competitive on other two datasets.



### MisMatch: Calibrated Segmentation via Consistency on Differential Morphological Feature Perturbations with Limited Labels
- **Arxiv ID**: http://arxiv.org/abs/2110.12179v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12179v3)
- **Published**: 2021-10-23 09:22:41+00:00
- **Updated**: 2023-05-02 11:46:47+00:00
- **Authors**: Mou-Cheng Xu, Yukun Zhou, Chen Jin, Marius De Groot, Neil P. Oxtoby, Daniel C. Alexander, Joseph Jacob
- **Comment**: To appear in IEEE Transactions on Medical Imaging 2023
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) is a promising machine learning paradigm to address the issue of label scarcity in medical imaging. SSL methods were originally developed in image classification. The state-of-the-art SSL methods in image classification utilise consistency regularisation to learn unlabelled predictions which are invariant to input level perturbations. However, image level perturbations violate the cluster assumption in the setting of segmentation. Moreover, existing image level perturbations are hand-crafted which could be sub-optimal. Therefore, it is a not trivial to straightforwardly adapt existing SSL image classification methods in segmentation. In this paper, we propose MisMatch, a semi-supervised segmentation framework based on the consistency between paired predictions which are derived from two differently learnt morphological feature perturbations. MisMatch consists of an encoder and two decoders. One decoder learns positive attention for foreground on unlabelled data thereby generating dilated features of foreground. The other decoder learns negative attention for foreground on the same unlabelled data thereby generating eroded features of foreground. We first develop a 2D U-net based MisMatch framework and perform extensive cross-validation on a CT-based pulmonary vessel segmentation task and show that MisMatch statistically outperforms state-of-the-art semi-supervised methods when only 6.25\% of the total labels are used. In a second experiment, we show that U-net based MisMatch outperforms state-of-the-art methods on an MRI-based brain tumour segmentation task. In a third experiment, we show that a 3D MisMatch outperforms a previous method using input level augmentations, on a left atrium segmentation task. Lastly, we find that the performance improvement of MisMatch over the baseline might originate from its better calibration.



### Attend and Guide (AG-Net): A Keypoints-driven Attention-based Deep Network for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.12183v1
- **DOI**: 10.1109/TIP.2021.3064256
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12183v1)
- **Published**: 2021-10-23 09:43:36+00:00
- **Updated**: 2021-10-23 09:43:36+00:00
- **Authors**: Asish Bera, Zachary Wharton, Yonghuai Liu, Nik Bessis, Ardhendu Behera
- **Comment**: Published in IEEE Transaction on Image Processing 2021, Vol. 30, pp.
  3691 - 3704
- **Journal**: IEEE Transactions on Image Processing 2021
- **Summary**: This paper presents a novel keypoints-based attention mechanism for visual recognition in still images. Deep Convolutional Neural Networks (CNNs) for recognizing images with distinctive classes have shown great success, but their performance in discriminating fine-grained changes is not at the same level. We address this by proposing an end-to-end CNN model, which learns meaningful features linking fine-grained changes using our novel attention mechanism. It captures the spatial structures in images by identifying semantic regions (SRs) and their spatial distributions, and is proved to be the key to modelling subtle changes in images. We automatically identify these SRs by grouping the detected keypoints in a given image. The ``usefulness'' of these SRs for image recognition is measured using our innovative attentional mechanism focusing on parts of the image that are most relevant to a given task. This framework applies to traditional and fine-grained image recognition tasks and does not require manually annotated regions (e.g. bounding-box of body parts, objects, etc.) for learning and prediction. Moreover, the proposed keypoints-driven attention mechanism can be easily integrated into the existing CNN models. The framework is evaluated on six diverse benchmark datasets. The model outperforms the state-of-the-art approaches by a considerable margin using Distracted Driver V1 (Acc: 3.39%), Distracted Driver V2 (Acc: 6.58%), Stanford-40 Actions (mAP: 2.15%), People Playing Musical Instruments (mAP: 16.05%), Food-101 (Acc: 6.30%) and Caltech-256 (Acc: 2.59%) datasets.



### Group-disentangled Representation Learning with Weakly-Supervised Regularization
- **Arxiv ID**: http://arxiv.org/abs/2110.12185v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12185v1)
- **Published**: 2021-10-23 10:01:05+00:00
- **Updated**: 2021-10-23 10:01:05+00:00
- **Authors**: Linh Tran, Amir Hosein Khasahmadi, Aditya Sanghi, Saeid Asgari
- **Comment**: None
- **Journal**: None
- **Summary**: Learning interpretable and human-controllable representations that uncover factors of variation in data remains an ongoing key challenge in representation learning. We investigate learning group-disentangled representations for groups of factors with weak supervision. Existing techniques to address this challenge merely constrain the approximate posterior by averaging over observations of a shared group. As a result, observations with a common set of variations are encoded to distinct latent representations, reducing their capacity to disentangle and generalize to downstream tasks. In contrast to previous works, we propose GroupVAE, a simple yet effective Kullback-Leibler (KL) divergence-based regularization across shared latent representations to enforce consistent and disentangled representations. We conduct a thorough evaluation and demonstrate that our GroupVAE significantly improves group disentanglement. Further, we demonstrate that learning group-disentangled representations improve upon downstream tasks, including fair classification and 3D shape-related tasks such as reconstruction, classification, and transfer learning, and is competitive to supervised methods.



### Dual Shape Guided Segmentation Network for Organs-at-Risk in Head and Neck CT Images
- **Arxiv ID**: http://arxiv.org/abs/2110.12192v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12192v1)
- **Published**: 2021-10-23 10:53:37+00:00
- **Updated**: 2021-10-23 10:53:37+00:00
- **Authors**: Shuai Wang, Theodore Yanagihara, Bhishamjit Chera, Colette Shen, Pew-Thian Yap, Jun Lian
- **Comment**: None
- **Journal**: None
- **Summary**: The accurate segmentation of organs-at-risk (OARs) in head and neck CT images is a critical step for radiation therapy of head and neck cancer patients. However, manual delineation for numerous OARs is time-consuming and laborious, even for expert oncologists. Moreover, manual delineation results are susceptible to high intra- and inter-variability. To this end, we propose a novel dual shape guided network (DSGnet) to automatically delineate nine important OARs in head and neck CT images. To deal with the large shape variation and unclear boundary of OARs in CT images, we represent the organ shape using an organ-specific unilateral inverse-distance map (UIDM) and guide the segmentation task from two different perspectives: direct shape guidance by following the segmentation prediction and across shape guidance by sharing the segmentation feature. In the direct shape guidance, the segmentation prediction is not only supervised by the true label mask, but also by the true UIDM, which is implemented through a simple yet effective encoder-decoder mapping from the label space to the distance space. In the across shape guidance, UIDM is used to facilitate the segmentation by optimizing the shared feature maps. For the experiments, we build a large head and neck CT dataset with a total of 699 images from different volunteers, and conduct comprehensive experiments and comparisons with other state-of-the-art methods to justify the effectiveness and efficiency of our proposed method. The overall Dice Similarity Coefficient (DSC) value of 0.842 across the nine important OARs demonstrates great potential applications in improving the delineation quality and reducing the time cost.



### RPT++: Customized Feature Representation for Siamese Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2110.12194v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12194v2)
- **Published**: 2021-10-23 10:58:57+00:00
- **Updated**: 2022-04-26 12:40:18+00:00
- **Authors**: Ziang Ma, Haitao Zhang, Linyuan Wang, Jun Yin
- **Comment**: the authors hold different opinions on whether or not to public the
  paper
- **Journal**: None
- **Summary**: While recent years have witnessed remarkable progress in the feature representation of visual tracking, the problem of feature misalignment between the classification and regression tasks is largely overlooked. The approaches of feature extraction make no difference for these two tasks in most of advanced trackers. We argue that the performance gain of visual tracking is limited since features extracted from the salient area provide more recognizable visual patterns for classification, while these around the boundaries contribute to accurately estimating the target state.   We address this problem by proposing two customized feature extractors, named polar pooling and extreme pooling to capture task-specific visual patterns. Polar pooling plays the role of enriching information collected from the semantic keypoints for stronger classification, while extreme pooling facilitates explicit visual patterns of the object boundary for accurate target state estimation. We demonstrate the effectiveness of the task-specific feature representation by integrating it into the recent and advanced tracker RPT. Extensive experiments on several benchmarks show that our Customized Features based RPT (RPT++) achieves new state-of-the-art performances on OTB-100, VOT2018, VOT2019, GOT-10k, TrackingNet and LaSOT.



### Towards a Robust Differentiable Architecture Search under Label Noise
- **Arxiv ID**: http://arxiv.org/abs/2110.12197v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12197v1)
- **Published**: 2021-10-23 11:31:06+00:00
- **Updated**: 2021-10-23 11:31:06+00:00
- **Authors**: Christian Simon, Piotr Koniusz, Lars Petersson, Yan Han, Mehrtash Harandi
- **Comment**: Accepted to WACV 2022
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) is the game changer in designing robust neural architectures. Architectures designed by NAS outperform or compete with the best manual network designs in terms of accuracy, size, memory footprint and FLOPs. That said, previous studies focus on developing NAS algorithms for clean high quality data, a restrictive and somewhat unrealistic assumption. In this paper, focusing on the differentiable NAS algorithms, we show that vanilla NAS algorithms suffer from a performance loss if class labels are noisy. To combat this issue, we make use of the principle of information bottleneck as a regularizer. This leads us to develop a noise injecting operation that is included during the learning process, preventing the network from learning from noisy samples. Our empirical evaluations show that the noise injecting operation does not degrade the performance of the NAS algorithm if the data is indeed clean. In contrast, if the data is noisy, the architecture learned by our algorithm comfortably outperforms algorithms specifically equipped with sophisticated mechanisms to learn in the presence of label noise. In contrast to many algorithms designed to work in the presence of noisy labels, prior knowledge about the properties of the noise and its characteristics are not required for our algorithm.



### Cascading Feature Extraction for Fast Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2110.12204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12204v1)
- **Published**: 2021-10-23 12:17:00+00:00
- **Updated**: 2021-10-23 12:17:00+00:00
- **Authors**: Yoichiro Hisadome, Yusuke Matsui
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: We propose a method for speeding up a 3D point cloud registration through a cascading feature extraction. The current approach with the highest accuracy is realized by iteratively executing feature extraction and registration using deep features. However, iterative feature extraction takes time. Our proposed method significantly reduces the computational cost using cascading shallow layers. Our idea is to omit redundant computations that do not always contribute to the final accuracy. The proposed approach is approximately three times faster than the existing methods without a loss of accuracy.



### Multi-Domain Incremental Learning for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.12205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12205v1)
- **Published**: 2021-10-23 12:21:42+00:00
- **Updated**: 2021-10-23 12:21:42+00:00
- **Authors**: Prachi Garg, Rohit Saluja, Vineeth N Balasubramanian, Chetan Arora, Anbumani Subramanian, C. V. Jawahar
- **Comment**: 11 pages, 5 figures, Accepted in WACV 2022
- **Journal**: None
- **Summary**: Recent efforts in multi-domain learning for semantic segmentation attempt to learn multiple geographical datasets in a universal, joint model. A simple fine-tuning experiment performed sequentially on three popular road scene segmentation datasets demonstrates that existing segmentation frameworks fail at incrementally learning on a series of visually disparate geographical domains. When learning a new domain, the model catastrophically forgets previously learned knowledge. In this work, we pose the problem of multi-domain incremental learning for semantic segmentation. Given a model trained on a particular geographical domain, the goal is to (i) incrementally learn a new geographical domain, (ii) while retaining performance on the old domain, (iii) given that the previous domain's dataset is not accessible. We propose a dynamic architecture that assigns universally shared, domain-invariant parameters to capture homogeneous semantic features present in all domains, while dedicated domain-specific parameters learn the statistics of each domain. Our novel optimization strategy helps achieve a good balance between retention of old knowledge (stability) and acquiring new knowledge (plasticity). We demonstrate the effectiveness of our proposed solution on domain incremental settings pertaining to real-world driving scenes from roads of Germany (Cityscapes), the United States (BDD100k), and India (IDD).



### MaskSplit: Self-supervised Meta-learning for Few-shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.12207v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12207v2)
- **Published**: 2021-10-23 12:30:05+00:00
- **Updated**: 2021-11-03 17:08:50+00:00
- **Authors**: Mustafa Sercan Amac, Ahmet Sencan, Orhun Bugra Baran, Nazli Ikizler-Cinbis, Ramazan Gokberk Cinbis
- **Comment**: To appear at WACV 2022, 11 pages, 5 figures
- **Journal**: None
- **Summary**: Just like other few-shot learning problems, few-shot segmentation aims to minimize the need for manual annotation, which is particularly costly in segmentation tasks. Even though the few-shot setting reduces this cost for novel test classes, there is still a need to annotate the training data. To alleviate this need, we propose a self-supervised training approach for learning few-shot segmentation models. We first use unsupervised saliency estimation to obtain pseudo-masks on images. We then train a simple prototype based model over different splits of pseudo masks and augmentations of images. Our extensive experiments show that the proposed approach achieves promising results, highlighting the potential of self-supervised training. To the best of our knowledge this is the first work that addresses unsupervised few-shot segmentation problem on natural images.



### ES-ImageNet: A Million Event-Stream Classification Dataset for Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.12211v1
- **DOI**: 10.3389/fnins.2021.726582
- **Categories**: **cs.CV**, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/2110.12211v1)
- **Published**: 2021-10-23 12:56:23+00:00
- **Updated**: 2021-10-23 12:56:23+00:00
- **Authors**: Yihan Lin, Wei Ding, Shaohua Qiang, Lei Deng, Guoqi Li
- **Comment**: None
- **Journal**: None
- **Summary**: With event-driven algorithms, especially the spiking neural networks (SNNs), achieving continuous improvement in neuromorphic vision processing, a more challenging event-stream-dataset is urgently needed. However, it is well known that creating an ES-dataset is a time-consuming and costly task with neuromorphic cameras like dynamic vision sensors (DVS). In this work, we propose a fast and effective algorithm termed Omnidirectional Discrete Gradient (ODG) to convert the popular computer vision dataset ILSVRC2012 into its event-stream (ES) version, generating about 1,300,000 frame-based images into ES-samples in 1000 categories. In this way, we propose an ES-dataset called ES-ImageNet, which is dozens of times larger than other neuromorphic classification datasets at present and completely generated by the software. The ODG algorithm implements an image motion to generate local value changes with discrete gradient information in different directions, providing a low-cost and high-speed way for converting frame-based images into event streams, along with Edge-Integral to reconstruct the high-quality images from event streams. Furthermore, we analyze the statistics of the ES-ImageNet in multiple ways, and a performance benchmark of the dataset is also provided using both famous deep neural network algorithms and spiking neural network algorithms. We believe that this work shall provide a new large-scale benchmark dataset for SNNs and neuromorphic vision.



### Domain Adaptation for Rare Classes Augmented with Synthetic Samples
- **Arxiv ID**: http://arxiv.org/abs/2110.12216v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12216v1)
- **Published**: 2021-10-23 13:29:54+00:00
- **Updated**: 2021-10-23 13:29:54+00:00
- **Authors**: Tuhin Das, Robert-Jan Bruintjes, Attila Lengyel, Jan van Gemert, Sara Beery
- **Comment**: 14 pages, 6 figures, to be published
- **Journal**: None
- **Summary**: To alleviate lower classification performance on rare classes in imbalanced datasets, a possible solution is to augment the underrepresented classes with synthetic samples. Domain adaptation can be incorporated in a classifier to decrease the domain discrepancy between real and synthetic samples. While domain adaptation is generally applied on completely synthetic source domains and real target domains, we explore how domain adaptation can be applied when only a single rare class is augmented with simulated samples. As a testbed, we use a camera trap animal dataset with a rare deer class, which is augmented with synthetic deer samples. We adapt existing domain adaptation methods to two new methods for the single rare class setting: DeerDANN, based on the Domain-Adversarial Neural Network (DANN), and DeerCORAL, based on deep correlation alignment (Deep CORAL) architectures. Experiments show that DeerDANN has the highest improvement in deer classification accuracy of 24.0% versus 22.4% improvement of DeerCORAL when compared to the baseline. Further, both methods require fewer than 10k synthetic samples, as used by the baseline, to achieve these higher accuracies. DeerCORAL requires the least number of synthetic samples (2k deer), followed by DeerDANN (8k deer).



### Parametric Variational Linear Units (PVLUs) in Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.12246v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12246v4)
- **Published**: 2021-10-23 15:37:25+00:00
- **Updated**: 2021-12-16 21:34:17+00:00
- **Authors**: Aarush Gupta, Shikhar Ahuja
- **Comment**: Both authors contributed equally to this research
- **Journal**: None
- **Summary**: The Rectified Linear Unit is currently a state-of-the-art activation function in deep convolutional neural networks. To combat ReLU's dying neuron problem, we propose the Parametric Variational Linear Unit (PVLU), which adds a sinusoidal function with trainable coefficients to ReLU. Along with introducing nonlinearity and non-zero gradients across the entire real domain, PVLU acts as a mechanism of fine-tuning when implemented in the context of transfer learning. On a simple, non-transfer sequential CNN, PVLU substitution allowed for relative error decreases of 16.3% and 11.3% (without and with data augmentation) on CIFAR-100. PVLU is also tested on transfer learning models. The VGG-16 and VGG-19 models experience relative error reductions of 9.5% and 10.7% on CIFAR-10, respectively, after the substitution of ReLU with PVLU. When training on Gaussian-filtered CIFAR-10 images, similar improvements are noted for the VGG models. Most notably, fine-tuning using PVLU allows for relative error reductions up to and exceeding 10% for near state-of-the-art residual neural network architectures on the CIFAR datasets.



### Confidence-Aware Active Feedback for Interactive Instance Search
- **Arxiv ID**: http://arxiv.org/abs/2110.12255v3
- **DOI**: 10.1109/TMM.2022.3217965
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.12255v3)
- **Published**: 2021-10-23 16:14:03+00:00
- **Updated**: 2022-10-27 06:27:49+00:00
- **Authors**: Yue Zhang, Chao Liang, Longxiang Jiang
- **Comment**: Accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Online relevance feedback (RF) is widely utilized in instance search (INS) tasks to further refine imperfect ranking results, but it often has low interaction efficiency. The active learning (AL) technique addresses this problem by selecting valuable feedback candidates. However, mainstream AL methods require an initial labeled set for a cold start and are often computationally complex to solve. Therefore, they cannot fully satisfy the requirements for online RF in interactive INS tasks. To address this issue, we propose a confidence-aware active feedback method (CAAF) that is specifically designed for online RF in interactive INS tasks. Inspired by the explicit difficulty modeling scheme in self-paced learning, CAAF utilizes a pairwise manifold ranking loss to evaluate the ranking confidence of each unlabeled sample. The ranking confidence improves not only the interaction efficiency by indicating valuable feedback candidates but also the ranking quality by modulating the diffusion weights in manifold ranking. In addition, we design two acceleration strategies, an approximate optimization scheme and a top-K search scheme, to reduce the computational complexity of CAAF. Extensive experiments on both image INS tasks and video INS tasks searching for buildings, landscapes, persons, and human behaviors demonstrate the effectiveness of the proposed method. Notably, in the real-world, large-scale video INS task of NIST TRECVID 2021, CAAF uses 25% fewer feedback samples to achieve a performance that is nearly equivalent to the champion solution. Moreover, with the same number of feedback samples, CAAF's mAP is 51.9%, significantly surpassing the champion solution by 5.9%. Code is available at https://github.com/nercms-mmap/caaf.



### espiownage: Tracking Transients in Steelpan Drum Strikes Using Surveillance Technology
- **Arxiv ID**: http://arxiv.org/abs/2110.12261v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.4.6; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2110.12261v1)
- **Published**: 2021-10-23 17:10:54+00:00
- **Updated**: 2021-10-23 17:10:54+00:00
- **Authors**: Scott H. Hawley, Andrew C. Morrison, Grant S. Morgan
- **Comment**: 6 pages, 5 figures, submitted to NeurIPS 2021 Workshop on Machine
  Learning and the Physical Sciences
- **Journal**: None
- **Summary**: We present an improvement in the ability to meaningfully track features in high speed videos of Caribbean steelpan drums illuminated by Electronic Speckle Pattern Interferometry (ESPI). This is achieved through the use of up-to-date computer vision libraries for object detection and image segmentation as well as a significant effort toward cleaning the dataset previously used to train systems for this application. Besides improvements on previous metric scores by 10% or more, noteworthy in this project are the introduction of a segmentation-regression map for the entire drum surface yielding interference fringe counts comparable to those obtained via object detection, as well as the accelerated workflow for coordinating the data-cleaning-and-model-training feedback loop for rapid iteration allowing this project to be conducted on a timescale of only 18 days.



### Benchmarking of Lightweight Deep Learning Architectures for Skin Cancer Classification using ISIC 2017 Dataset
- **Arxiv ID**: http://arxiv.org/abs/2110.12270v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12270v1)
- **Published**: 2021-10-23 18:18:17+00:00
- **Updated**: 2021-10-23 18:18:17+00:00
- **Authors**: Abdurrahim Yilmaz, Mucahit Kalebasi, Yegor Samoylenko, Mehmet Erhan Guvenilir, Huseyin Uvet
- **Comment**: 4 pages, supplementary with 9 figures
- **Journal**: None
- **Summary**: Skin cancer is one of the deadly types of cancer and is common in the world. Recently, there has been a huge jump in the rate of people getting skin cancer. For this reason, the number of studies on skin cancer classification with deep learning are increasing day by day. For the growth of work in this area, the International Skin Imaging Collaboration (ISIC) organization was established and they created an open dataset archive. In this study, images were taken from ISIC 2017 Challenge. The skin cancer images taken were preprocessed and data augmented. Later, these images were trained with transfer learning and fine-tuning approach and deep learning models were created in this way. 3 different mobile deep learning models and 3 different batch size values were determined for each, and a total of 9 models were created. Among these models, the NASNetMobile model with 16 batch size got the best result. The accuracy value of this model is 82.00%, the precision value is 81.77% and the F1 score value is 0.8038. Our method is to benchmark mobile deep learning models which have few parameters and compare the results of the models.



### Self-Validation: Early Stopping for Single-Instance Deep Generative Priors
- **Arxiv ID**: http://arxiv.org/abs/2110.12271v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2110.12271v1)
- **Published**: 2021-10-23 18:27:03+00:00
- **Updated**: 2021-10-23 18:27:03+00:00
- **Authors**: Taihui Li, Zhong Zhuang, Hengyue Liang, Le Peng, Hengkang Wang, Ju Sun
- **Comment**: To appear in British Machine Vision Conference (BMVC) 2021
- **Journal**: None
- **Summary**: Recent works have shown the surprising effectiveness of deep generative models in solving numerous image reconstruction (IR) tasks, even without training data. We call these models, such as deep image prior and deep decoder, collectively as single-instance deep generative priors (SIDGPs). The successes, however, often hinge on appropriate early stopping (ES), which by far has largely been handled in an ad-hoc manner. In this paper, we propose the first principled method for ES when applying SIDGPs to IR, taking advantage of the typical bell trend of the reconstruction quality. In particular, our method is based on collaborative training and self-validation: the primal reconstruction process is monitored by a deep autoencoder, which is trained online with the historic reconstructed images and used to validate the reconstruction quality constantly. Experimentally, on several IR problems and different SIDGPs, our self-validation method is able to reliably detect near-peak performance and signal good ES points. Our code is available at https://sun-umn.github.io/Self-Validation/.



### "One-Shot" Reduction of Additive Artifacts in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2110.12274v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12274v1)
- **Published**: 2021-10-23 18:35:00+00:00
- **Updated**: 2021-10-23 18:35:00+00:00
- **Authors**: Yu-Jen Chen, Yen-Jung Chang, Shao-Cheng Wen, Yiyu Shi, Xiaowei Xu, Tsung-Yi Ho, Meiping Huang, Haiyun Yuan, Jian Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: Medical images may contain various types of artifacts with different patterns and mixtures, which depend on many factors such as scan setting, machine condition, patients' characteristics, surrounding environment, etc. However, existing deep-learning-based artifact reduction methods are restricted by their training set with specific predetermined artifact types and patterns. As such, they have limited clinical adoption. In this paper, we introduce One-Shot medical image Artifact Reduction (OSAR), which exploits the power of deep learning but without using pre-trained general networks. Specifically, we train a light-weight image-specific artifact reduction network using data synthesized from the input image at test-time. Without requiring any prior large training data set, OSAR can work with almost any medical images that contain varying additive artifacts which are not in any existing data sets. In addition, Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are used as vehicles and show that the proposed method can reduce artifacts better than state-of-the-art both qualitatively and quantitatively using shorter test time.



### Signal to Noise Ratio Loss Function
- **Arxiv ID**: http://arxiv.org/abs/2110.12275v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12275v1)
- **Published**: 2021-10-23 18:44:32+00:00
- **Updated**: 2021-10-23 18:44:32+00:00
- **Authors**: Ali Ghobadzadeh, Amir Lashkari
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposes a new loss function targeting classification problems, utilizing a source of information overlooked by cross entropy loss. First, we derive a series of the tightest upper and lower bounds for the probability of a random variable in a given interval. Second, a lower bound is proposed for the probability of a true positive for a parametric classification problem, where the form of probability density function (pdf) of data is given. A closed form for finding the optimal function of unknowns is derived to maximize the probability of true positives. Finally, for the case that the pdf of data is unknown, we apply the proposed boundaries to find the lower bound of the probability of true positives and upper bound of the probability of false positives and optimize them using a loss function which is given by combining the boundaries. We demonstrate that the resultant loss function is a function of the signal to noise ratio both within and across logits. We empirically evaluate our proposals to show their benefit for classification problems.



### Perineural Invasion Detection in Multiple Organ Cancer Based on Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2110.12283v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12283v1)
- **Published**: 2021-10-23 19:39:08+00:00
- **Updated**: 2021-10-23 19:39:08+00:00
- **Authors**: Ramin Nateghi, Fattaneh Pourakpour
- **Comment**: None
- **Journal**: None
- **Summary**: Perineural invasion (PNI) by malignant tumor cells has been reported as an independent indicator of poor prognosis in various cancers. Assessment of PNI in small nerves on glass slides is a labor-intensive task. In this study, we propose an algorithm to detect the perineural invasions in colon, prostate, and pancreas cancers based on a convolutional neural network (CNN).



### Face sketch to photo translation using generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/2110.12290v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12290v1)
- **Published**: 2021-10-23 20:01:20+00:00
- **Updated**: 2021-10-23 20:01:20+00:00
- **Authors**: Nastaran Moradzadeh Farid, Maryam Saeedi Fard, Ahmad Nickabadi
- **Comment**: None
- **Journal**: None
- **Summary**: Translating face sketches to photo-realistic faces is an interesting and essential task in many applications like law enforcement and the digital entertainment industry. One of the most important challenges of this task is the inherent differences between the sketch and the real image such as the lack of color and details of the skin tissue in the sketch. With the advent of adversarial generative models, an increasing number of methods have been proposed for sketch-to-image synthesis. However, these models still suffer from limitations such as the large number of paired data required for training, the low resolution of the produced images, or the unrealistic appearance of the generated images. In this paper, we propose a method for converting an input facial sketch to a colorful photo without the need for any paired dataset. To do so, we use a pre-trained face photo generating model to synthesize high-quality natural face photos and employ an optimization procedure to keep high-fidelity to the input sketch. We train a network to map the facial features extracted from the input sketch to a vector in the latent space of the face generating model. Also, we study different optimization criteria and compare the results of the proposed model with those of the state-of-the-art models quantitatively and qualitatively. The proposed model achieved 0.655 in the SSIM index and 97.59% rank-1 face recognition rate with higher quality of the produced images.



### A Layer-wise Adversarial-aware Quantization Optimization for Improving Robustness
- **Arxiv ID**: http://arxiv.org/abs/2110.12308v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12308v1)
- **Published**: 2021-10-23 22:11:30+00:00
- **Updated**: 2021-10-23 22:11:30+00:00
- **Authors**: Chang Song, Riya Ranjan, Hai Li
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2012.14965
- **Journal**: None
- **Summary**: Neural networks are getting better accuracy with higher energy and computational cost. After quantization, the cost can be greatly saved, and the quantized models are more hardware friendly with acceptable accuracy loss. On the other hand, recent research has found that neural networks are vulnerable to adversarial attacks, and the robustness of a neural network model can only be improved with defense methods, such as adversarial training. In this work, we find that adversarially-trained neural networks are more vulnerable to quantization loss than plain models. To minimize both the adversarial and the quantization losses simultaneously and to make the quantized model robust, we propose a layer-wise adversarial-aware quantization method, using the Lipschitz constant to choose the best quantization parameter settings for a neural network. We theoretically derive the losses and prove the consistency of our metric selection. The experiment results show that our method can effectively and efficiently improve the robustness of quantized adversarially-trained neural networks.



