# Arxiv Papers in cs.CV on 2021-10-12
### Defocus Map Estimation and Deblurring from a Single Dual-Pixel Image
- **Arxiv ID**: http://arxiv.org/abs/2110.05655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.05655v1)
- **Published**: 2021-10-12 00:09:07+00:00
- **Updated**: 2021-10-12 00:09:07+00:00
- **Authors**: Shumian Xin, Neal Wadhwa, Tianfan Xue, Jonathan T. Barron, Pratul P. Srinivasan, Jiawen Chen, Ioannis Gkioulekas, Rahul Garg
- **Comment**: ICCV 2021 (Oral)
- **Journal**: None
- **Summary**: We present a method that takes as input a single dual-pixel image, and simultaneously estimates the image's defocus map -- the amount of defocus blur at each pixel -- and recovers an all-in-focus image. Our method is inspired from recent works that leverage the dual-pixel sensors available in many consumer cameras to assist with autofocus, and use them for recovery of defocus maps or all-in-focus images. These prior works have solved the two recovery problems independently of each other, and often require large labeled datasets for supervised training. By contrast, we show that it is beneficial to treat these two closely-connected problems simultaneously. To this end, we set up an optimization problem that, by carefully modeling the optics of dual-pixel images, jointly solves both problems. We use data captured with a consumer smartphone camera to demonstrate that, after a one-time calibration step, our approach improves upon prior works for both defocus map estimation and blur removal, despite being entirely unsupervised.



### Accurate and Generalizable Quantitative Scoring of Liver Steatosis from Ultrasound Images via Scalable Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.05664v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2110.05664v1)
- **Published**: 2021-10-12 00:52:04+00:00
- **Updated**: 2021-10-12 00:52:04+00:00
- **Authors**: Bowen Li, Dar-In Tai, Ke Yan, Yi-Cheng Chen, Shiu-Feng Huang, Tse-Hwa Hsu, Wan-Ting Yu, Jing Xiao, Le Lu, Adam P. Harrison
- **Comment**: Journal paper submission, 45 pages (main body: 28 pages,
  supplementary material: 17 pages)
- **Journal**: None
- **Summary**: Background & Aims: Hepatic steatosis is a major cause of chronic liver disease. 2D ultrasound is the most widely used non-invasive tool for screening and monitoring, but associated diagnoses are highly subjective. We developed a scalable deep learning (DL) algorithm for quantitative scoring of liver steatosis from 2D ultrasound images.   Approach & Results: Using retrospectively collected multi-view ultrasound data from 3,310 patients, 19,513 studies, and 228,075 images, we trained a DL algorithm to diagnose steatosis stages (healthy, mild, moderate, or severe) from ultrasound diagnoses. Performance was validated on two multi-scanner unblinded and blinded (initially to DL developer) histology-proven cohorts (147 and 112 patients) with histopathology fatty cell percentage diagnoses, and a subset with FibroScan diagnoses. We also quantified reliability across scanners and viewpoints. Results were evaluated using Bland-Altman and receiver operating characteristic (ROC) analysis. The DL algorithm demonstrates repeatable measurements with a moderate number of images (3 for each viewpoint) and high agreement across 3 premium ultrasound scanners. High diagnostic performance was observed across all viewpoints: area under the curves of the ROC to classify >=mild, >=moderate, =severe steatosis grades were 0.85, 0.90, and 0.93, respectively. The DL algorithm outperformed or performed at least comparably to FibroScan with statistically significant improvements for all levels on the unblinded histology-proven cohort, and for =severe steatosis on the blinded histology-proven cohort.   Conclusions: The DL algorithm provides a reliable quantitative steatosis assessment across view and scanners on two multi-scanner cohorts. Diagnostic performance was high with comparable or better performance than FibroScan.



### NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks
- **Arxiv ID**: http://arxiv.org/abs/2110.05668v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.05668v6)
- **Published**: 2021-10-12 01:13:18+00:00
- **Updated**: 2023-01-19 23:17:16+00:00
- **Authors**: Renbo Tu, Nicholas Roberts, Mikhail Khodak, Junhong Shen, Frederic Sala, Ameet Talwalkar
- **Comment**: NeurIPS 2022 Datasets and Benchmarks Track
- **Journal**: None
- **Summary**: Most existing neural architecture search (NAS) benchmarks and algorithms prioritize well-studied tasks, e.g. image classification on CIFAR or ImageNet. This makes the performance of NAS approaches in more diverse areas poorly understood. In this paper, we present NAS-Bench-360, a benchmark suite to evaluate methods on domains beyond those traditionally studied in architecture search, and use it to address the following question: do state-of-the-art NAS methods perform well on diverse tasks? To construct the benchmark, we curate ten tasks spanning a diverse array of application domains, dataset sizes, problem dimensionalities, and learning objectives. Each task is carefully chosen to interoperate with modern CNN-based search methods while possibly being far-afield from its original development domain. To speed up and reduce the cost of NAS research, for two of the tasks we release the precomputed performance of 15,625 architectures comprising a standard CNN search space. Experimentally, we show the need for more robust NAS evaluation of the kind NAS-Bench-360 enables by showing that several modern NAS procedures perform inconsistently across the ten tasks, with many catastrophically poor results. We also demonstrate how NAS-Bench-360 and its associated precomputed results will enable future scientific discoveries by testing whether several recent hypotheses promoted in the NAS literature hold on diverse tasks. NAS-Bench-360 is hosted at https://nb360.ml.cmu.edu.



### Improved Heatmap-based Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.05676v1
- **DOI**: 10.1007/978-3-030-88210-5_11
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.05676v1)
- **Published**: 2021-10-12 01:41:01+00:00
- **Updated**: 2021-10-12 01:41:01+00:00
- **Authors**: Huifeng Yao, Ziyu Guo, Yatao Zhang, Xiaomeng Li
- **Comment**: Accepted by MICCAI2021 workshop
- **Journal**: None
- **Summary**: Mitral valve repair is a very difficult operation, often requiring experienced surgeons. The doctor will insert a prosthetic ring to aid in the restoration of heart function. The location of the prosthesis' sutures is critical. Obtaining and studying them during the procedure is a valuable learning experience for new surgeons. This paper proposes a landmark detection network for detecting sutures in endoscopic pictures, which solves the problem of a variable number of suture points in the images. Because there are two datasets, one from the simulated domain and the other from real intraoperative data, this work uses cycleGAN to interconvert the images from the two domains to obtain a larger dataset and a better score on real intraoperative data. This paper performed the tests using a simulated dataset of 2708 photos and a real dataset of 2376 images. The mean sensitivity on the simulated dataset is about 75.64% and the precision is about 73.62%. The mean sensitivity on the real dataset is about 50.23% and the precision is about 62.76%. The data is from the AdaptOR MICCAI Challenge 2021, which can be found at https://zenodo.org/record/4646979\#.YO1zLUxCQ2x.



### RWN: Robust Watermarking Network for Image Cropping Localization
- **Arxiv ID**: http://arxiv.org/abs/2110.05687v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.05687v2)
- **Published**: 2021-10-12 02:19:42+00:00
- **Updated**: 2022-05-31 14:57:39+00:00
- **Authors**: Qichao Ying, Xiaoxiao Hu, Xiangyu Zhang, Zhenxing Qian, Xinpeng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Image cropping can be maliciously used to manipulate the layout of an image and alter the underlying meaning. Previous image crop detection schemes only predicts whether an image has been cropped, ignoring which part of the image is cropped. This paper presents a novel robust watermarking network (RWN) for image crop localization. We train an anti-crop processor (ACP) that embeds a watermark into a target image. The visually indistinguishable protected image is then posted on the social network instead of the original image. At the recipient's side, ACP extracts the watermark from the attacked image, and we conduct feature matching on the original and extracted watermark to locate the position of the crop in the original image plane. We further extend our scheme to detect tampering attack on the attacked image. Besides, we explore a simple yet efficient method (JPEG-Mixup) to improve the generalization of JPEG robustness. According to our comprehensive experiments, RWN is the first to provide high-accuracy and robust image crop localization. Besides, the accuracy of tamper detection is comparable with many state-of-the-art passive-based methods.



### Inclusive Design: Accessibility Settings for People with Cognitive Disabilities
- **Arxiv ID**: http://arxiv.org/abs/2110.05688v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.05688v1)
- **Published**: 2021-10-12 02:19:52+00:00
- **Updated**: 2021-10-12 02:19:52+00:00
- **Authors**: Trae Waggoner, Julia Ann Jose, Ashwin Nair, Sudarsan Manikandan
- **Comment**: None
- **Journal**: None
- **Summary**: The advancement of technology has progressed faster than any other field in the world and with the development of these new technologies, it is important to make sure that these tools can be used by everyone, including people with disabilities. Accessibility options in computing devices help ensure that everyone has the same access to advanced technologies. Unfortunately, for those who require more unique and sometimes challenging accommodations, such as people with Amyotrophic lateral sclerosis ( ALS), the most commonly used accessibility features are simply not enough. While assistive technology for those with ALS does exist, it requires multiple peripheral devices that can become quite expensive collectively. The purpose of this paper is to suggest a more affordable and readily available option for ALS assistive technology that can be implemented on a smartphone or tablet.



### Hiding Images into Images with Real-world Robustness
- **Arxiv ID**: http://arxiv.org/abs/2110.05689v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.05689v1)
- **Published**: 2021-10-12 02:20:34+00:00
- **Updated**: 2021-10-12 02:20:34+00:00
- **Authors**: Qichao Ying, Hang Zhou, Xianhan Zeng, Haisheng Xu, Zhenxing Qian, Xinpeng Zhang
- **Comment**: A modified version is submitted to IEEE ICASSP 2021. Authors are from
  Fudan University, Simon Fraser University and NVIDIA
- **Journal**: None
- **Summary**: The existing image embedding networks are basically vulnerable to malicious attacks such as JPEG compression and noise adding, not applicable for real-world copyright protection tasks. To solve this problem, we introduce a generative deep network based method for hiding images into images while assuring high-quality extraction from the destructive synthesized images. An embedding network is sequentially concatenated with an attack layer, a decoupling network and an image extraction network. The addition of decoupling network learns to extract the embedded watermark from the attacked image. We also pinpoint the weaknesses of the adversarial training for robustness in previous works and build our improved real-world attack simulator. Experimental results demonstrate the superiority of the proposed method against typical digital attacks by a large margin, as well as the performance boost of the recovered images with the aid of progressive recovery strategy. Besides, we are the first to robustly hide three secret images.



### Hierarchical Modeling for Task Recognition and Action Segmentation in Weakly-Labeled Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/2110.05697v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.05697v1)
- **Published**: 2021-10-12 02:32:15+00:00
- **Updated**: 2021-10-12 02:32:15+00:00
- **Authors**: Reza Ghoddoosian, Saif Sayed, Vassilis Athitsos
- **Comment**: Accepted in WACV 2022
- **Journal**: None
- **Summary**: This paper focuses on task recognition and action segmentation in weakly-labeled instructional videos, where only the ordered sequence of video-level actions is available during training. We propose a two-stream framework, which exploits semantic and temporal hierarchies to recognize top-level tasks in instructional videos. Further, we present a novel top-down weakly-supervised action segmentation approach, where the predicted task is used to constrain the inference of fine-grained action sequences. Experimental results on the popular Breakfast and Cooking 2 datasets show that our two-stream hierarchical task modeling significantly outperforms existing methods in top-level task recognition for all datasets and metrics. Additionally, using our task recognition framework in the proposed top-down action segmentation approach consistently improves the state of the art, while also reducing segmentation inference time by 80-90 percent.



### On Exploring and Improving Robustness of Scene Text Detection Models
- **Arxiv ID**: http://arxiv.org/abs/2110.05700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.05700v1)
- **Published**: 2021-10-12 02:36:48+00:00
- **Updated**: 2021-10-12 02:36:48+00:00
- **Authors**: Shilian Wu, Wei Zhai, Yongrui Li, Kewei Wang, Zengfu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: It is crucial to understand the robustness of text detection models with regard to extensive corruptions, since scene text detection techniques have many practical applications. For systematically exploring this problem, we propose two datasets from which to evaluate scene text detection models: ICDAR2015-C (IC15-C) and CTW1500-C (CTW-C). Our study extends the investigation of the performance and robustness of the proposed region proposal, regression and segmentation-based scene text detection frameworks. Furthermore, we perform a robustness analysis of six key components: pre-training data, backbone, feature fusion module, multi-scale predictions, representation of text instances and loss function. Finally, we present a simple yet effective data-based method to destroy the smoothness of text regions by merging background and foreground, which can significantly increase the robustness of different text detection networks. We hope that this study will provide valid data points as well as experience for future research. Benchmark, code and data will be made available at \url{https://github.com/wushilian/robust-scene-text-detection-benchmark}.



### Deep Fusion Prior for Plenoptic Super-Resolution All-in-Focus Imaging
- **Arxiv ID**: http://arxiv.org/abs/2110.05706v5
- **DOI**: 10.1117/1.OE.61.12.123103
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.05706v5)
- **Published**: 2021-10-12 02:44:07+00:00
- **Updated**: 2022-10-15 11:04:18+00:00
- **Authors**: Yuanjie Gu, Yinghan Guan, Zhibo Xiao, Haoran Dai, Cheng Liu, Shouyu Wang
- **Comment**: 24 pages
- **Journal**: None
- **Summary**: Multi-focus image fusion (MFIF) and super-resolution (SR) are the inverse problem of imaging model, purposes of MFIF and SR are obtaining all-in-focus and high-resolution 2D mapping of targets. Though various MFIF and SR methods have been designed; almost all the them deal with MFIF and SR separately. This paper unifies MFIF and SR problems in the physical perspective as the multi-focus image super resolution fusion (MFISRF), and we propose a novel unified dataset-free unsupervised framework named deep fusion prior (DFP) based-on deep image prior (DIP) to address such MFISRF with single model. Experiments have proved that our proposed DFP approaches or even outperforms those state-of-art MFIF and SR method combinations. To our best knowledge, our proposed work is a dataset-free unsupervised method to simultaneously implement the multi-focus fusion and super-resolution task for the first time. Additionally, DFP is a general framework, thus its networks and focus measurement tactics can be continuously updated to further improve the MFISRF performance. DFP codes are open source available at http://github.com/GuYuanjie/DeepFusionPrior.



### Relation-aware Video Reading Comprehension for Temporal Language Grounding
- **Arxiv ID**: http://arxiv.org/abs/2110.05717v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2110.05717v3)
- **Published**: 2021-10-12 03:10:21+00:00
- **Updated**: 2021-12-01 04:45:03+00:00
- **Authors**: Jialin Gao, Xin Sun, Mengmeng Xu, Xi Zhou, Bernard Ghanem
- **Comment**: Accepted by EMNLP-21
- **Journal**: None
- **Summary**: Temporal language grounding in videos aims to localize the temporal span relevant to the given query sentence. Previous methods treat it either as a boundary regression task or a span extraction task. This paper will formulate temporal language grounding into video reading comprehension and propose a Relation-aware Network (RaNet) to address it. This framework aims to select a video moment choice from the predefined answer set with the aid of coarse-and-fine choice-query interaction and choice-choice relation construction. A choice-query interactor is proposed to match the visual and textual information simultaneously in sentence-moment and token-moment levels, leading to a coarse-and-fine cross-modal interaction. Moreover, a novel multi-choice relation constructor is introduced by leveraging graph convolution to capture the dependencies among video moment choices for the best choice selection. Extensive experiments on ActivityNet-Captions, TACoS, and Charades-STA demonstrate the effectiveness of our solution. Codes have been available.



### A Prior Guided Adversarial Representation Learning and Hypergraph Perceptual Network for Predicting Abnormal Connections of Alzheimer's Disease
- **Arxiv ID**: http://arxiv.org/abs/2110.09302v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09302v1)
- **Published**: 2021-10-12 03:10:37+00:00
- **Updated**: 2021-10-12 03:10:37+00:00
- **Authors**: Qiankun Zuo, Baiying Lei, Shuqiang Wang, Yong Liu, Bingchuan Wang, Yanyan Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Alzheimer's disease is characterized by alterations of the brain's structural and functional connectivity during its progressive degenerative processes. Existing auxiliary diagnostic methods have accomplished the classification task, but few of them can accurately evaluate the changing characteristics of brain connectivity. In this work, a prior guided adversarial representation learning and hypergraph perceptual network (PGARL-HPN) is proposed to predict abnormal brain connections using triple-modality medical images. Concretely, a prior distribution from the anatomical knowledge is estimated to guide multimodal representation learning using an adversarial strategy. Also, the pairwise collaborative discriminator structure is further utilized to narrow the difference of representation distribution. Moreover, the hypergraph perceptual network is developed to effectively fuse the learned representations while establishing high-order relations within and between multimodal images. Experimental results demonstrate that the proposed model outperforms other related methods in analyzing and predicting Alzheimer's disease progression. More importantly, the identified abnormal connections are partly consistent with the previous neuroscience discoveries. The proposed model can evaluate characteristics of abnormal brain connections at different stages of Alzheimer's disease, which is helpful for cognitive disease study and early treatment.



### Rethinking the Spatial Route Prior in Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2110.05728v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.05728v1)
- **Published**: 2021-10-12 03:55:43+00:00
- **Updated**: 2021-10-12 03:55:43+00:00
- **Authors**: Xinzhe Zhou, Wei Liu, Yadong Mu
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Vision-and-language navigation (VLN) is a trending topic which aims to navigate an intelligent agent to an expected position through natural language instructions. This work addresses the task of VLN from a previously-ignored aspect, namely the spatial route prior of the navigation scenes. A critically enabling innovation of this work is explicitly considering the spatial route prior under several different VLN settings. In a most information-rich case of knowing environment maps and admitting shortest-path prior, we observe that given an origin-destination node pair, the internal route can be uniquely determined. Thus, VLN can be effectively formulated as an ordinary classification problem over all possible destination nodes in the scenes. Furthermore, we relax it to other more general VLN settings, proposing a sequential-decision variant (by abandoning the shortest-path route prior) and an explore-and-exploit scheme (for addressing the case of not knowing the environment maps) that curates a compact and informative sub-graph to exploit. As reported by [34], the performance of VLN methods has been stuck at a plateau in past two years. Even with increased model complexity, the state-of-the-art success rate on R2R validation-unseen set has stayed around 62% for single-run and 73% for beam-search with model-ensemble. We have conducted comprehensive evaluations on both R2R and R4R, and surprisingly found that utilizing the spatial route priors may be the key of breaking above-mentioned performance ceiling. For example, on R2R validation-unseen set, when the number of discrete nodes explored is about 40, our single-model success rate reaches 73%, and increases to 78% if a Speaker model is ensembled, which significantly outstrips previous state-of-the-art VLN-BERT with 3 models ensembled.



### Topic Scene Graph Generation by Attention Distillation from Caption
- **Arxiv ID**: http://arxiv.org/abs/2110.05731v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.05731v1)
- **Published**: 2021-10-12 04:26:12+00:00
- **Updated**: 2021-10-12 04:26:12+00:00
- **Authors**: W. Wang, R. Wang, X. Chen
- **Comment**: published in ICCV 2021
- **Journal**: None
- **Summary**: If an image tells a story, the image caption is the briefest narrator. Generally, a scene graph prefers to be an omniscient generalist, while the image caption is more willing to be a specialist, which outlines the gist. Lots of previous studies have found that a scene graph is not as practical as expected unless it can reduce the trivial contents and noises. In this respect, the image caption is a good tutor. To this end, we let the scene graph borrow the ability from the image caption so that it can be a specialist on the basis of remaining all-around, resulting in the so-called Topic Scene Graph. What an image caption pays attention to is distilled and passed to the scene graph for estimating the importance of partial objects, relationships, and events. Specifically, during the caption generation, the attention about individual objects in each time step is collected, pooled, and assembled to obtain the attention about relationships, which serves as weak supervision for regularizing the estimated importance scores of relationships. In addition, as this attention distillation process provides an opportunity for combining the generation of image caption and scene graph together, we further transform the scene graph into linguistic form with rich and free-form expressions by sharing a single generation model with image caption. Experiments show that attention distillation brings significant improvements in mining important relationships without strong supervision, and the topic scene graph shows great potential in subsequent applications.



### Learning Efficient Multi-Agent Cooperative Visual Exploration
- **Arxiv ID**: http://arxiv.org/abs/2110.05734v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2110.05734v3)
- **Published**: 2021-10-12 04:48:10+00:00
- **Updated**: 2022-11-22 14:26:46+00:00
- **Authors**: Chao Yu, Xinyi Yang, Jiaxuan Gao, Huazhong Yang, Yu Wang, Yi Wu
- **Comment**: First three authors share equal contribution. This paper has been
  accepted by ECCV
  (https://link.springer.com/chapter/10.1007/978-3-031-19842-7_29)
- **Journal**: None
- **Summary**: We tackle the problem of cooperative visual exploration where multiple agents need to jointly explore unseen regions as fast as possible based on visual signals. Classical planning-based methods often suffer from expensive computation overhead at each step and a limited expressiveness of complex cooperation strategy. By contrast, reinforcement learning (RL) has recently become a popular paradigm for tackling this challenge due to its modeling capability of arbitrarily complex strategies and minimal inference overhead. In this paper, we extend the state-of-the-art single-agent visual navigation method, Active Neural SLAM (ANS), to the multi-agent setting by introducing a novel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages a transformer-based architecture, Spatial-TeamFormer, which effectively captures spatial relations and intra-agent interactions via hierarchical spatial self-attentions. In addition, we also implement a few multi-agent enhancements to process local information from each agent for an aligned spatial representation and more precise planning. Finally, we perform policy distillation to extract a meta policy to significantly improve the generalization capability of final policy. We call this overall solution, Multi-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms classical planning-based baselines for the first time in a photo-realistic 3D simulator, Habitat. Code and videos can be found at https://sites.google.com/view/maans.



### Online Refinement of Low-level Feature Based Activation Map for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2110.05741v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.05741v1)
- **Published**: 2021-10-12 05:09:21+00:00
- **Updated**: 2021-10-12 05:09:21+00:00
- **Authors**: Jinheng Xie, Cheng Luo, Xiangping Zhu, Ziqi Jin, Weizeng Lu, Linlin Shen
- **Comment**: Accepted to ICCV 2021.(corrected some minor mistakes)
- **Journal**: None
- **Summary**: We present a two-stage learning framework for weakly supervised object localization (WSOL). While most previous efforts rely on high-level feature based CAMs (Class Activation Maps), this paper proposes to localize objects using the low-level feature based activation maps. In the first stage, an activation map generator produces activation maps based on the low-level feature maps in the classifier, such that rich contextual object information is included in an online manner. In the second stage, we employ an evaluator to evaluate the activation maps predicted by the activation map generator. Based on this, we further propose a weighted entropy loss, an attentive erasing, and an area loss to drive the activation map generator to substantially reduce the uncertainty of activations between object and background, and explore less discriminative regions. Based on the low-level object information preserved in the first stage, the second stage model gradually generates a well-separated, complete, and compact activation map of object in the image, which can be easily thresholded for accurate localization. Extensive experiments on CUB-200-2011 and ImageNet-1K datasets show that our framework surpasses previous methods by a large margin, which sets a new state-of-the-art for WSOL.



### Seamless Copy Move Manipulation in Digital Images
- **Arxiv ID**: http://arxiv.org/abs/2110.05747v1
- **DOI**: 10.3390/jimaging8030069
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.05747v1)
- **Published**: 2021-10-12 05:35:26+00:00
- **Updated**: 2021-10-12 05:35:26+00:00
- **Authors**: Tanzila Qazi, Mushtaq Ali, Khizar Hayat
- **Comment**: 9 pages and 9 figures (most having subfigures)
- **Journal**: J. Imaging 2022, 8(3), 69
- **Summary**: The importance and relevance of digital image forensics has attracted researchers to establish different techniques for creating as well as detecting forgeries. The core category in passive image forgery is copy-move image forgery that affects the originality of image by applying a different transformation. In this paper frequency domain image manipulation method is being presented.The method exploits the localized nature of discrete wavelet transform (DWT) to get hold of the region of the host image to be manipulated. Both the patch and host image are subjected to DWT at the same level $l$ to get $3l + 1$ sub-bands and each sub-band of the patch is pasted to the identified region in the corresponding sub-band of the host image. The resultant manipulated host sub-bands are then subjected to inverse DWT to get the final manipulated host image. The proposed method shows good resistance against detection by two frequency domain forgery detection methods from the literature. The purpose of this research work is to create the forgery and highlight the need to produce forgery detection methods that are robust against the malicious copy-move forgery.



### Detecting Damage Building Using Real-time Crowdsourced Images and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.05762v2
- **DOI**: 10.1038/s41598-022-12965-0
- **Categories**: **cs.CV**, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/2110.05762v2)
- **Published**: 2021-10-12 06:31:54+00:00
- **Updated**: 2021-11-15 18:31:11+00:00
- **Authors**: Gaurav Chachra, Qingkai Kong, Jim Huang, Srujay Korlakunta, Jennifer Grannen, Alexander Robson, Richard Allen
- **Comment**: None
- **Journal**: Sci Rep 12, 8968 (2022)
- **Summary**: After significant earthquakes, we can see images posted on social media platforms by individuals and media agencies owing to the mass usage of smartphones these days. These images can be utilized to provide information about the shaking damage in the earthquake region both to the public and research community, and potentially to guide rescue work. This paper presents an automated way to extract the damaged building images after earthquakes from social media platforms such as Twitter and thus identify the particular user posts containing such images. Using transfer learning and ~6500 manually labelled images, we trained a deep learning model to recognize images with damaged buildings in the scene. The trained model achieved good performance when tested on newly acquired images of earthquakes at different locations and ran in near real-time on Twitter feed after the 2020 M7.0 earthquake in Turkey. Furthermore, to better understand how the model makes decisions, we also implemented the Grad-CAM method to visualize the important locations on the images that facilitate the decision.



### Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents
- **Arxiv ID**: http://arxiv.org/abs/2110.05769v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2110.05769v1)
- **Published**: 2021-10-12 06:56:11+00:00
- **Updated**: 2021-10-12 06:56:11+00:00
- **Authors**: Shivansh Patel, Saim Wani, Unnat Jain, Alexander Schwing, Svetlana Lazebnik, Manolis Savva, Angel X. Chang
- **Comment**: Project page: https://shivanshpatel35.github.io/comon/ ; the first
  three authors contributed equally
- **Journal**: None
- **Summary**: Communication between embodied AI agents has received increasing attention in recent years. Despite its use, it is still unclear whether the learned communication is interpretable and grounded in perception. To study the grounding of emergent forms of communication, we first introduce the collaborative multi-object navigation task CoMON. In this task, an oracle agent has detailed environment information in the form of a map. It communicates with a navigator agent that perceives the environment visually and is tasked to find a sequence of goals. To succeed at the task, effective communication is essential. CoMON hence serves as a basis to study different communication mechanisms between heterogeneous agents, that is, agents with different capabilities and roles. We study two common communication mechanisms and analyze their communication patterns through an egocentric and spatial lens. We show that the emergent communication can be grounded to the agent observations and the spatial structure of the 3D environment. Video summary: https://youtu.be/kLv2rxO9t0g



### HyperCube: Implicit Field Representations of Voxelized 3D Models
- **Arxiv ID**: http://arxiv.org/abs/2110.05770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.05770v1)
- **Published**: 2021-10-12 06:56:48+00:00
- **Updated**: 2021-10-12 06:56:48+00:00
- **Authors**: Magdalena Proszewska, Marcin Mazur, Tomasz Trzciński, Przemysław Spurek
- **Comment**: None
- **Journal**: None
- **Summary**: Recently introduced implicit field representations offer an effective way of generating 3D object shapes. They leverage implicit decoder trained to take a 3D point coordinate concatenated with a shape encoding and to output a value which indicates whether the point is outside the shape or not. Although this approach enables efficient rendering of visually plausible objects, it has two significant limitations. First, it is based on a single neural network dedicated for all objects from a training set which results in a cumbersome training procedure and its application in real life. More importantly, the implicit decoder takes only points sampled within voxels (and not the entire voxels) which yields problems at the classification boundaries and results in empty spaces within the rendered mesh.   To solve the above limitations, we introduce a new HyperCube architecture based on interval arithmetic network, that enables direct processing of 3D voxels, trained using a hypernetwork paradigm to enforce model convergence. Instead of processing individual 3D samples from within a voxel, our approach allows to input the entire voxel (3D cube) represented with its convex hull coordinates, while the target network constructed by a hypernet assigns it to an inside or outside category. As a result our HyperCube model outperforms the competing approaches both in terms of training and inference efficiency, as well as the final mesh quality.



### SDWNet: A Straight Dilated Network with Wavelet Transformation for Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2110.05803v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.05803v1)
- **Published**: 2021-10-12 07:58:10+00:00
- **Updated**: 2021-10-12 07:58:10+00:00
- **Authors**: Wenbin Zou, Mingchao Jiang, Yunchen Zhang, Liang Chen, Zhiyong Lu, Yi Wu
- **Comment**: 10 pages, 7 figures, ICCVW 2021
- **Journal**: None
- **Summary**: Image deblurring is a classical computer vision problem that aims to recover a sharp image from a blurred image. To solve this problem, existing methods apply the Encode-Decode architecture to design the complex networks to make a good performance. However, most of these methods use repeated up-sampling and down-sampling structures to expand the receptive field, which results in texture information loss during the sampling process and some of them design the multiple stages that lead to difficulties with convergence. Therefore, our model uses dilated convolution to enable the obtainment of the large receptive field with high spatial resolution. Through making full use of the different receptive fields, our method can achieve better performance. On this basis, we reduce the number of up-sampling and down-sampling and design a simple network structure. Besides, we propose a novel module using the wavelet transform, which effectively helps the network to recover clear high-frequency texture details. Qualitative and quantitative evaluations of real and synthetic datasets show that our deblurring method is comparable to existing algorithms in terms of performance with much lower training requirements. The source code and pre-trained models are available at https://github.com/FlyEgle/SDWNet.



### Satellite Image Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.05812v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2110.05812v1)
- **Published**: 2021-10-12 08:14:52+00:00
- **Updated**: 2021-10-12 08:14:52+00:00
- **Authors**: Eric Guérin, Killian Oechslin, Christian Wolf, Benoît Martinez
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: In this paper, we propose a method for the automatic semantic segmentation of satellite images into six classes (sparse forest, dense forest, moor, herbaceous formation, building, and road). We rely on Swin Transformer architecture and build the dataset from IGN open data. We report quantitative and qualitative segmentation results on this dataset and discuss strengths and limitations. The dataset and the trained model are made publicly available.



### Event-Based high-speed low-latency fiducial marker tracking
- **Arxiv ID**: http://arxiv.org/abs/2110.05819v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.05819v1)
- **Published**: 2021-10-12 08:34:31+00:00
- **Updated**: 2021-10-12 08:34:31+00:00
- **Authors**: Adam Loch, Germain Haessig, Markus Vincze
- **Comment**: None
- **Journal**: None
- **Summary**: Motion and dynamic environments, especially under challenging lighting conditions, are still an open issue for robust robotic applications. In this paper, we propose an end-to-end pipeline for real-time, low latency, 6 degrees-of-freedom pose estimation of fiducial markers. Instead of achieving a pose estimation through a conventional frame-based approach, we employ the high-speed abilities of event-based sensors to directly refine the spatial transformation, using consecutive events. Furthermore, we introduce a novel two-way verification process for detecting tracking errors by backtracking the estimated pose, allowing us to evaluate the quality of our tracking. This approach allows us to achieve pose estimation at a rate up to 156~kHz, while only relying on CPU resources. The average end-to-end latency of our method is 3~ms. Experimental results demonstrate outstanding potential for robotic tasks, such as visual servoing in fast action-perception loops.



### AVoE: A Synthetic 3D Dataset on Understanding Violation of Expectation for Artificial Cognition
- **Arxiv ID**: http://arxiv.org/abs/2110.05836v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2110.05836v2)
- **Published**: 2021-10-12 08:59:19+00:00
- **Updated**: 2021-11-16 22:24:45+00:00
- **Authors**: Arijit Dasgupta, Jiafei Duan, Marcelo H. Ang Jr, Cheston Tan
- **Comment**: Accepted at the NeurIPS Workshop for Physical Reasoning and Inductive
  Biases for the Real World
- **Journal**: None
- **Summary**: Recent work in cognitive reasoning and computer vision has engendered an increasing popularity for the Violation-of-Expectation (VoE) paradigm in synthetic datasets. Inspired by work in infant psychology, researchers have started evaluating a model's ability to discriminate between expected and surprising scenes as a sign of its reasoning ability. Existing VoE-based 3D datasets in physical reasoning only provide vision data. However, current cognitive models of physical reasoning by psychologists reveal infants create high-level abstract representations of objects and interactions. Capitalizing on this knowledge, we propose AVoE: a synthetic 3D VoE-based dataset that presents stimuli from multiple novel sub-categories for five event categories of physical reasoning. Compared to existing work, AVoE is armed with ground-truth labels of abstract features and rules augmented to vision data, paving the way for high-level symbolic predictions in physical reasoning tasks.



### PLNet: Plane and Line Priors for Unsupervised Indoor Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.05839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.05839v1)
- **Published**: 2021-10-12 09:02:24+00:00
- **Updated**: 2021-10-12 09:02:24+00:00
- **Authors**: Hualie Jiang, Laiyan Ding, Junjie Hu, Rui Huang
- **Comment**: Accepted by 3DV 2021
- **Journal**: None
- **Summary**: Unsupervised learning of depth from indoor monocular videos is challenging as the artificial environment contains many textureless regions. Fortunately, the indoor scenes are full of specific structures, such as planes and lines, which should help guide unsupervised depth learning. This paper proposes PLNet that leverages the plane and line priors to enhance the depth estimation. We first represent the scene geometry using local planar coefficients and impose the smoothness constraint on the representation. Moreover, we enforce the planar and linear consistency by randomly selecting some sets of points that are probably coplanar or collinear to construct simple and effective consistency losses. To verify the proposed method's effectiveness, we further propose to evaluate the flatness and straightness of the predicted point cloud on the reliable planar and linear regions. The regularity of these regions indicates quality indoor reconstruction. Experiments on NYU Depth V2 and ScanNet show that PLNet outperforms existing methods. The code is available at \url{https://github.com/HalleyJiang/PLNet}.



### Fine-Grained Adversarial Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.05848v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.05848v1)
- **Published**: 2021-10-12 09:24:22+00:00
- **Updated**: 2021-10-12 09:24:22+00:00
- **Authors**: Daniele Mugnai, Federico Pernici, Francesco Turchini, Alberto Del Bimbo
- **Comment**: None
- **Journal**: ACM Transactions on Multimedia Computing, Communications, and
  Applications 2021
- **Summary**: In this paper we exploit Semi-Supervised Learning (SSL) to increase the amount of training data to improve the performance of Fine-Grained Visual Categorization (FGVC). This problem has not been investigated in the past in spite of prohibitive annotation costs that FGVC requires. Our approach leverages unlabeled data with an adversarial optimization strategy in which the internal features representation is obtained with a second-order pooling model. This combination allows to back-propagate the information of the parts, represented by second-order pooling, onto unlabeled data in an adversarial training setting. We demonstrate the effectiveness of the combined use by conducting experiments on six state-of-the-art fine-grained datasets, which include Aircrafts, Stanford Cars, CUB-200-2011, Oxford Flowers, Stanford Dogs, and the recent Semi-Supervised iNaturalist-Aves. Experimental results clearly show that our proposed method has better performance than the only previous approach that examined this problem; it also obtained higher classification accuracy with respect to the supervised learning methods with which we compared.



### Improving Binary Neural Networks through Fully Utilizing Latent Weights
- **Arxiv ID**: http://arxiv.org/abs/2110.05850v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.05850v1)
- **Published**: 2021-10-12 09:32:38+00:00
- **Updated**: 2021-10-12 09:32:38+00:00
- **Authors**: Weixiang Xu, Qiang Chen, Xiangyu He, Peisong Wang, Jian Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Binary Neural Networks (BNNs) rely on a real-valued auxiliary variable W to help binary training. However, pioneering binary works only use W to accumulate gradient updates during backward propagation, which can not fully exploit its power and may hinder novel advances in BNNs. In this work, we explore the role of W in training besides acting as a latent variable. Notably, we propose to add W into the computation graph, making it perform as a real-valued feature extractor to aid the binary training. We make different attempts on how to utilize the real-valued weights and propose a specialized supervision. Visualization experiments qualitatively verify the effectiveness of our approach in making it easier to distinguish between different categories. Quantitative experiments show that our approach outperforms current state-of-the-arts, further closing the performance gap between floating-point networks and BNNs. Evaluation on ImageNet with ResNet-18 (Top-1 63.4%), ResNet-34 (Top-1 67.0%) achieves new state-of-the-art.



### Joint Learning On The Hierarchy Representation for Fine-Grained Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.05853v1
- **DOI**: 10.1109/ICIP42928.2021.9506157
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.05853v1)
- **Published**: 2021-10-12 09:37:51+00:00
- **Updated**: 2021-10-12 09:37:51+00:00
- **Authors**: Mei Chee Leong, Hui Li Tan, Haosong Zhang, Liyuan Li, Feng Lin, Joo Hwee Lim
- **Comment**: Camera ready for IEEE ICIP 2021
- **Journal**: 2021 IEEE International Conference on Image Processing (ICIP)
- **Summary**: Fine-grained human action recognition is a core research topic in computer vision. Inspired by the recently proposed hierarchy representation of fine-grained actions in FineGym and SlowFast network for action recognition, we propose a novel multi-task network which exploits the FineGym hierarchy representation to achieve effective joint learning and prediction for fine-grained human action recognition. The multi-task network consists of three pathways of SlowOnly networks with gradually increased frame rates for events, sets and elements of fine-grained actions, followed by our proposed integration layers for joint learning and prediction. It is a two-stage approach, where it first learns deep feature representation at each hierarchical level, and is followed by feature encoding and fusion for multi-task learning. Our empirical results on the FineGym dataset achieve a new state-of-the-art performance, with 91.80% Top-1 accuracy and 88.46% mean accuracy for element actions, which are 3.40% and 7.26% higher than the previous best results.



### Convolutional Neural Networks Are Not Invariant to Translation, but They Can Learn to Be
- **Arxiv ID**: http://arxiv.org/abs/2110.05861v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.05861v1)
- **Published**: 2021-10-12 09:51:07+00:00
- **Updated**: 2021-10-12 09:51:07+00:00
- **Authors**: Valerio Biscione, Jeffrey S. Bowers
- **Comment**: None
- **Journal**: Journal of Machine Learning Research 2021 22(229) 1-28
- **Summary**: When seeing a new object, humans can immediately recognize it across different retinal locations: the internal object representation is invariant to translation. It is commonly believed that Convolutional Neural Networks (CNNs) are architecturally invariant to translation thanks to the convolution and/or pooling operations they are endowed with. In fact, several studies have found that these networks systematically fail to recognise new objects on untrained locations. In this work, we test a wide variety of CNNs architectures showing how, apart from DenseNet-121, none of the models tested was architecturally invariant to translation. Nevertheless, all of them could learn to be invariant to translation. We show how this can be achieved by pretraining on ImageNet, and it is sometimes possible with much simpler data sets when all the items are fully translated across the input canvas. At the same time, this invariance can be disrupted by further training due to catastrophic forgetting/interference. These experiments show how pretraining a network on an environment with the right `latent' characteristics (a more naturalistic environment) can result in the network learning deep perceptual rules which would dramatically improve subsequent generalization.



### OpenHands: Making Sign Language Recognition Accessible with Pose-based Pretrained Models across Languages
- **Arxiv ID**: http://arxiv.org/abs/2110.05877v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG, I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2110.05877v1)
- **Published**: 2021-10-12 10:33:02+00:00
- **Updated**: 2021-10-12 10:33:02+00:00
- **Authors**: Prem Selvaraj, Gokul NC, Pratyush Kumar, Mitesh Khapra
- **Comment**: Submitted to AAAI22, 13 pages, 9 figures, 6 tables
- **Journal**: None
- **Summary**: AI technologies for Natural Languages have made tremendous progress recently. However, commensurate progress has not been made on Sign Languages, in particular, in recognizing signs as individual words or as complete sentences. We introduce OpenHands, a library where we take four key ideas from the NLP community for low-resource languages and apply them to sign languages for word-level recognition. First, we propose using pose extracted through pretrained models as the standard modality of data to reduce training time and enable efficient inference, and we release standardized pose datasets for 6 different sign languages - American, Argentinian, Chinese, Greek, Indian, and Turkish. Second, we train and release checkpoints of 4 pose-based isolated sign language recognition models across all 6 languages, providing baselines and ready checkpoints for deployment. Third, to address the lack of labelled data, we propose self-supervised pretraining on unlabelled data. We curate and release the largest pose-based pretraining dataset on Indian Sign Language (Indian-SL). Fourth, we compare different pretraining strategies and for the first time establish that pretraining is effective for sign language recognition by demonstrating (a) improved fine-tuning performance especially in low-resource settings, and (b) high crosslingual transfer from Indian-SL to few other sign languages. We open-source all models and datasets in OpenHands with a hope that it makes research in sign languages more accessible, available here at https://github.com/AI4Bharat/OpenHands .



### Fourier-based Video Prediction through Relational Object Motion
- **Arxiv ID**: http://arxiv.org/abs/2110.05881v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.05881v1)
- **Published**: 2021-10-12 10:43:05+00:00
- **Updated**: 2021-10-12 10:43:05+00:00
- **Authors**: Malte Mosbach, Sven Behnke
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to predict future outcomes conditioned on observed video frames is crucial for intelligent decision-making in autonomous systems. Recently, deep recurrent architectures have been applied to the task of video prediction. However, this often results in blurry predictions and requires tedious training on large datasets. Here, we explore a different approach by (1) using frequency-domain approaches for video prediction and (2) explicitly inferring object-motion relationships in the observed scene. The resulting predictions are consistent with the observed dynamics in a scene and do not suffer from blur.



### Monocular Depth Estimation with Sharp Boundary
- **Arxiv ID**: http://arxiv.org/abs/2110.05885v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.05885v1)
- **Published**: 2021-10-12 10:55:12+00:00
- **Updated**: 2021-10-12 10:55:12+00:00
- **Authors**: Xin Yang, Qingling Chang, Xinlin Liu, Yan Cui
- **Comment**: 20 pages,9 figures
- **Journal**: None
- **Summary**: Monocular depth estimation is the base task in computer vision. It has a tremendous development in the decade with the development of deep learning. But the boundary blur of the depth map is still a serious problem. Research finds the boundary blur problem is mainly caused by two factors, first, the low-level features containing boundary and structure information may loss in deeper networks during the convolution process., second, the model ignores the errors introduced by the boundary area due to the few portions of the boundary in the whole areas during the backpropagation. In order to mitigate the boundary blur problem, we focus on the above two impact factors. Firstly, we design a scene understanding module to learn the global information with low- and high-level features, and then to transform the global information to different scales with our proposed scale transform module according to the different phases in the decoder. Secondly, we propose a boundary-aware depth loss function to pay attention to the effects of the boundary's depth value. The extensive experiments show that our method can predict the depth maps with clearer boundaries, and the performance of the depth accuracy base on NYU-depth v2 and SUN RGB-D is competitive.



### MGH: Metadata Guided Hypergraph Modeling for Unsupervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2110.05886v1
- **DOI**: 10.1145/3474085.3475296
- **Categories**: **cs.CV**, cs.AI, cs.MM, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2110.05886v1)
- **Published**: 2021-10-12 10:55:13+00:00
- **Updated**: 2021-10-12 10:55:13+00:00
- **Authors**: Yiming Wu, Xintian Wu, Xi Li, Jian Tian
- **Comment**: 10 pages, 3 figures
- **Journal**: None
- **Summary**: As a challenging task, unsupervised person ReID aims to match the same identity with query images which does not require any labeled information. In general, most existing approaches focus on the visual cues only, leaving potentially valuable auxiliary metadata information (e.g., spatio-temporal context) unexplored. In the real world, such metadata is normally available alongside captured images, and thus plays an important role in separating several hard ReID matches. With this motivation in mind, we propose~\textbf{MGH}, a novel unsupervised person ReID approach that uses meta information to construct a hypergraph for feature learning and label refinement. In principle, the hypergraph is composed of camera-topology-aware hyperedges, which can model the heterogeneous data correlations across cameras. Taking advantage of label propagation on the hypergraph, the proposed approach is able to effectively refine the ReID results, such as correcting the wrong labels or smoothing the noisy labels. Given the refined results, We further present a memory-based listwise loss to directly optimize the average precision in an approximate manner. Extensive experiments on three benchmarks demonstrate the effectiveness of the proposed approach against the state-of-the-art.



### Video Is Graph: Structured Graph Module for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.05904v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.05904v3)
- **Published**: 2021-10-12 11:27:29+00:00
- **Updated**: 2022-01-29 08:42:57+00:00
- **Authors**: Rongchang Li, Xiao-Jun Wu, Tianyang Xu
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: In the field of action recognition, video clips are always treated as ordered frames for subsequent processing. To achieve spatio-temporal perception, existing approaches propose to embed adjacent temporal interaction in the convolutional layer. The global semantic information can therefore be obtained by stacking multiple local layers hierarchically. However, such global temporal accumulation can only reflect the high-level semantics in deep layers, neglecting the potential low-level holistic clues in shallow layers. In this paper, we first propose to transform a video sequence into a graph to obtain direct long-term dependencies among temporal frames. To preserve sequential information during transformation, we devise a structured graph module (SGM), achieving fine-grained temporal interactions throughout the entire network. In particular, SGM divides the neighbors of each node into several temporal regions so as to extract global structural information with diverse sequential flows. Extensive experiments are performed on standard benchmark datasets, i.e., Something-Something V1 & V2, Diving48, Kinetics-400, UCF101, and HMDB51. The reported performance and analysis demonstrate that SGM can achieve outstanding precision with less computational complexity.



### Rescoring Sequence-to-Sequence Models for Text Line Recognition with CTC-Prefixes
- **Arxiv ID**: http://arxiv.org/abs/2110.05909v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.05909v3)
- **Published**: 2021-10-12 11:40:05+00:00
- **Updated**: 2022-03-29 15:47:24+00:00
- **Authors**: Christoph Wick, Jochen Zöllner, Tobias Grüning
- **Comment**: 15 pages, 6 tables, 3 figures
- **Journal**: None
- **Summary**: In contrast to Connectionist Temporal Classification (CTC) approaches, Sequence-To-Sequence (S2S) models for Handwritten Text Recognition (HTR) suffer from errors such as skipped or repeated words which often occur at the end of a sequence. In this paper, to combine the best of both approaches, we propose to use the CTC-Prefix-Score during S2S decoding. Hereby, during beam search, paths that are invalid according to the CTC confidence matrix are penalised. Our network architecture is composed of a Convolutional Neural Network (CNN) as visual backbone, bidirectional Long-Short-Term-Memory-Cells (LSTMs) as encoder, and a decoder which is a Transformer with inserted mutual attention layers. The CTC confidences are computed on the encoder while the Transformer is only used for character-wise S2S decoding. We evaluate this setup on three HTR data sets: IAM, Rimes, and StAZH. On IAM, we achieve a competitive Character Error Rate (CER) of 2.95% when pretraining our model on synthetic data and including a character-based language model for contemporary English. Compared to other state-of-the-art approaches, our model requires about 10-20 times less parameters. Access our shared implementations via this link to GitHub: https://github.com/Planet-AI-GmbH/tfaip-hybrid-ctc-s2s.



### Reliable Shot Identification for Complex Event Detection via Visual-Semantic Embedding
- **Arxiv ID**: http://arxiv.org/abs/2110.08063v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08063v1)
- **Published**: 2021-10-12 11:46:56+00:00
- **Updated**: 2021-10-12 11:46:56+00:00
- **Authors**: Minnan Luo, Xiaojun Chang, Chen Gong
- **Comment**: 11 pages, accepted by CVIU
- **Journal**: None
- **Summary**: Multimedia event detection is the task of detecting a specific event of interest in an user-generated video on websites. The most fundamental challenge facing this task lies in the enormously varying quality of the video as well as the high-level semantic abstraction of event inherently. In this paper, we decompose the video into several segments and intuitively model the task of complex event detection as a multiple instance learning problem by representing each video as a "bag" of segments in which each segment is referred to as an instance. Instead of treating the instances equally, we associate each instance with a reliability variable to indicate its importance and then select reliable instances for training. To measure the reliability of the varying instances precisely, we propose a visual-semantic guided loss by exploiting low-level feature from visual information together with instance-event similarity based high-level semantic feature. Motivated by curriculum learning, we introduce a negative elastic-net regularization term to start training the classifier with instances of high reliability and gradually taking the instances with relatively low reliability into consideration. An alternative optimization algorithm is developed to solve the proposed challenging non-convex non-smooth problem. Experimental results on standard datasets, i.e., TRECVID MEDTest 2013 and TRECVID MEDTest 2014, demonstrate the effectiveness and superiority of the proposed method to the baseline algorithms.



### Trivial or impossible -- dichotomous data difficulty masks model differences (on ImageNet and beyond)
- **Arxiv ID**: http://arxiv.org/abs/2110.05922v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2110.05922v3)
- **Published**: 2021-10-12 12:09:59+00:00
- **Updated**: 2022-04-27 18:25:19+00:00
- **Authors**: Kristof Meding, Luca M. Schulze Buschoff, Robert Geirhos, Felix A. Wichmann
- **Comment**: Published as a conference paper at ICLR 2022
- **Journal**: None
- **Summary**: "The power of a generalization system follows directly from its biases" (Mitchell 1980). Today, CNNs are incredibly powerful generalisation systems -- but to what degree have we understood how their inductive bias influences model decisions? We here attempt to disentangle the various aspects that determine how a model decides. In particular, we ask: what makes one model decide differently from another? In a meticulously controlled setting, we find that (1.) irrespective of the network architecture or objective (e.g. self-supervised, semi-supervised, vision transformers, recurrent models) all models end up with a similar decision boundary. (2.) To understand these findings, we analysed model decisions on the ImageNet validation set from epoch to epoch and image by image. We find that the ImageNet validation set, among others, suffers from dichotomous data difficulty (DDD): For the range of investigated models and their accuracies, it is dominated by 46.0% "trivial" and 11.5% "impossible" images (beyond label errors). Only 42.5% of the images could possibly be responsible for the differences between two models' decision boundaries. (3.) Only removing the "impossible" and "trivial" images allows us to see pronounced differences between models. (4.) Humans are highly accurate at predicting which images are "trivial" and "impossible" for CNNs (81.4%). This implies that in future comparisons of brains, machines and behaviour, much may be gained from investigating the decisive role of images and the distribution of their difficulties.



### Weakly-Supervised Semantic Segmentation by Learning Label Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2110.05926v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.05926v1)
- **Published**: 2021-10-12 12:19:22+00:00
- **Updated**: 2021-10-12 12:19:22+00:00
- **Authors**: Robby Neven, Davy Neven, Bert De Brabandere, Marc Proesmans, Toon Goedemé
- **Comment**: None
- **Journal**: None
- **Summary**: Since the rise of deep learning, many computer vision tasks have seen significant advancements. However, the downside of deep learning is that it is very data-hungry. Especially for segmentation problems, training a deep neural net requires dense supervision in the form of pixel-perfect image labels, which are very costly. In this paper, we present a new loss function to train a segmentation network with only a small subset of pixel-perfect labels, but take the advantage of weakly-annotated training samples in the form of cheap bounding-box labels. Unlike recent works which make use of box-to-mask proposal generators, our loss trains the network to learn a label uncertainty within the bounding-box, which can be leveraged to perform online bootstrapping (i.e. transforming the boxes to segmentation masks), while training the network. We evaluated our method on binary segmentation tasks, as well as a multi-class segmentation task (CityScapes vehicles and persons). We trained each task on a dataset comprised of only 18% pixel-perfect and 82% bounding-box labels, and compared the results to a baseline model trained on a completely pixel-perfect dataset. For the binary segmentation tasks, our method achieves an IoU score which is ~98.33% as good as our baseline model, while for the multi-class task, our method is 97.12% as good as our baseline model (77.5 vs. 79.8 mIoU).



### Can machines learn to see without visual databases?
- **Arxiv ID**: http://arxiv.org/abs/2110.05973v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.05973v2)
- **Published**: 2021-10-12 13:03:54+00:00
- **Updated**: 2021-11-22 08:40:08+00:00
- **Authors**: Alessandro Betti, Marco Gori, Stefano Melacci, Marcello Pelillo, Fabio Roli
- **Comment**: None
- **Journal**: None
- **Summary**: This paper sustains the position that the time has come for thinking of learning machines that conquer visual skills in a truly human-like context, where a few human-like object supervisions are given by vocal interactions and pointing aids only. This likely requires new foundations on computational processes of vision with the final purpose of involving machines in tasks of visual description by living in their own visual environment under simple man-machine linguistic interactions. The challenge consists of developing machines that learn to see without needing to handle visual databases. This might open the doors to a truly orthogonal competitive track concerning deep learning technologies for vision which does not rely on the accumulation of huge visual databases.



### Early Melanoma Diagnosis with Sequential Dermoscopic Images
- **Arxiv ID**: http://arxiv.org/abs/2110.05976v1
- **DOI**: 10.1109/TMI.2021.3120091
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.05976v1)
- **Published**: 2021-10-12 13:05:41+00:00
- **Updated**: 2021-10-12 13:05:41+00:00
- **Authors**: Zhen Yu, Jennifer Nguyen, Toan D Nguyen, John Kelly, Catriona Mclean, Paul Bonnington, Lei Zhang, Victoria Mar, Zongyuan Ge
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging, 2021
- **Summary**: Dermatologists often diagnose or rule out early melanoma by evaluating the follow-up dermoscopic images of skin lesions. However, existing algorithms for early melanoma diagnosis are developed using single time-point images of lesions. Ignoring the temporal, morphological changes of lesions can lead to misdiagnosis in borderline cases. In this study, we propose a framework for automated early melanoma diagnosis using sequential dermoscopic images. To this end, we construct our method in three steps. First, we align sequential dermoscopic images of skin lesions using estimated Euclidean transformations, extract the lesion growth region by computing image differences among the consecutive images, and then propose a spatio-temporal network to capture the dermoscopic changes from aligned lesion images and the corresponding difference images. Finally, we develop an early diagnosis module to compute probability scores of malignancy for lesion images over time. We collected 179 serial dermoscopic imaging data from 122 patients to verify our method. Extensive experiments show that the proposed model outperforms other commonly used sequence models. We also compared the diagnostic results of our model with those of seven experienced dermatologists and five registrars. Our model achieved higher diagnostic accuracy than clinicians (63.69% vs. 54.33%, respectively) and provided an earlier diagnosis of melanoma (60.7% vs. 32.7% of melanoma correctly diagnosed on the first follow-up images). These results demonstrate that our model can be used to identify melanocytic lesions that are at high-risk of malignant transformation earlier in the disease process and thereby redefine what is possible in the early detection of melanoma.



### Rethinking Supervised Pre-training for Better Downstream Transferring
- **Arxiv ID**: http://arxiv.org/abs/2110.06014v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.06014v2)
- **Published**: 2021-10-12 13:57:38+00:00
- **Updated**: 2022-03-12 07:24:37+00:00
- **Authors**: Yutong Feng, Jianwen Jiang, Mingqian Tang, Rong Jin, Yue Gao
- **Comment**: None
- **Journal**: None
- **Summary**: The pretrain-finetune paradigm has shown outstanding performance on many applications of deep learning, where a model is pre-trained on a upstream large dataset (e.g. ImageNet), and is then fine-tuned to different downstream tasks. Though for most cases, the pre-training stage is conducted based on supervised methods, recent works on self-supervised pre-training have shown powerful transferability and even outperform supervised pre-training on multiple downstream tasks. It thus remains an open question how to better generalize supervised pre-training model to downstream tasks. In this paper, we argue that the worse transferability of existing supervised pre-training methods arise from the negligence of valuable intra-class semantic difference. This is because these methods tend to push images from the same class close to each other despite of the large diversity in their visual contents, a problem to which referred as "overfit of upstream tasks". To alleviate this problem, we propose a new supervised pre-training method based on Leave-One-Out K-Nearest-Neighbor, or LOOK for short. It relieves the problem of overfitting upstream tasks by only requiring each image to share its class label with most of its k nearest neighbors, thus allowing each class to exhibit a multi-mode distribution and consequentially preserving part of intra-class difference for better transferring to downstream tasks. We developed efficient implementation of the proposed method that scales well to large datasets. Experimental studies on multiple downstream tasks show that LOOK outperforms other state-of-the-art methods for supervised and self-supervised pre-training.



### On the Security Risks of AutoML
- **Arxiv ID**: http://arxiv.org/abs/2110.06018v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.06018v1)
- **Published**: 2021-10-12 14:04:15+00:00
- **Updated**: 2021-10-12 14:04:15+00:00
- **Authors**: Ren Pang, Zhaohan Xi, Shouling Ji, Xiapu Luo, Ting Wang
- **Comment**: Accepted as a full paper at USENIX Security '22
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) represents an emerging machine learning (ML) paradigm that automatically searches for models tailored to given tasks, which greatly simplifies the development of ML systems and propels the trend of ML democratization. Yet, little is known about the potential security risks incurred by NAS, which is concerning given the increasing use of NAS-generated models in critical domains.   This work represents a solid initial step towards bridging the gap. Through an extensive empirical study of 10 popular NAS methods, we show that compared with their manually designed counterparts, NAS-generated models tend to suffer greater vulnerability to various malicious attacks (e.g., adversarial evasion, model poisoning, and functionality stealing). Further, with both empirical and analytical evidence, we provide possible explanations for such phenomena: given the prohibitive search space and training cost, most NAS methods favor models that converge fast at early training stages; this preference results in architectural properties associated with attack vulnerability (e.g., high loss smoothness and low gradient variance). Our findings not only reveal the relationships between model characteristics and attack vulnerability but also suggest the inherent connections underlying different attacks. Finally, we discuss potential remedies to mitigate such drawbacks, including increasing cell depth and suppressing skip connects, which lead to several promising research directions.



### SoftNeuro: Fast Deep Inference using Multi-platform Optimization
- **Arxiv ID**: http://arxiv.org/abs/2110.06037v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.06037v1)
- **Published**: 2021-10-12 14:31:53+00:00
- **Updated**: 2021-10-12 14:31:53+00:00
- **Authors**: Masaki Hilaga, Yasuhiro Kuroda, Hitoshi Matsuo, Tatsuya Kawaguchi, Gabriel Ogawa, Hiroshi Miyake, Yusuke Kozawa
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Faster inference of deep learning models is highly demanded on edge devices and even servers, for both financial and environmental reasons. To address this issue, we propose SoftNeuro, a novel, high-performance inference framework with efficient performance tuning. The key idea is to separate algorithmic routines from network layers. Our framework maximizes the inference performance by profiling various routines for each layer and selecting the fastest path. To efficiently find the best path, we propose a routine-selection algorithm based on dynamic programming. Experiments show that the proposed framework achieves both fast inference and efficient tuning.



### SlideGraph+: Whole Slide Image Level Graphs to Predict HER2Status in Breast Cancer
- **Arxiv ID**: http://arxiv.org/abs/2110.06042v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.06042v1)
- **Published**: 2021-10-12 14:40:15+00:00
- **Updated**: 2021-10-12 14:40:15+00:00
- **Authors**: Wenqi Lu, Michael Toss, Emad Rakha, Nasir Rajpoot, Fayyaz Minhas
- **Comment**: 20 pages, 11 figures, 3 tables
- **Journal**: None
- **Summary**: Human epidermal growth factor receptor 2 (HER2) is an important prognostic and predictive factor which is overexpressed in 15-20% of breast cancer (BCa). The determination of its status is a key clinical decision making step for selection of treatment regimen and prognostication. HER2 status is evaluated using transcroptomics or immunohistochemistry (IHC) through situ hybridisation (ISH) which require additional costs and tissue burden in addition to analytical variabilities in terms of manual observational biases in scoring. In this study, we propose a novel graph neural network (GNN) based model (termed SlideGraph+) to predict HER2 status directly from whole-slide images of routine Haematoxylin and Eosin (H&E) slides. The network was trained and tested on slides from The Cancer Genome Atlas (TCGA) in addition to two independent test datasets. We demonstrate that the proposed model outperforms the state-of-the-art methods with area under the ROC curve (AUC) values > 0.75 on TCGA and 0.8 on independent test sets. Our experiments show that the proposed approach can be utilised for case triaging as well as pre-ordering diagnostic tests in a diagnostic setting. It can also be used for other weakly supervised prediction problems in computational pathology. The SlideGraph+ code is available at https://github.com/wenqi006/SlideGraph.



### Improved Pillar with Fine-grained Feature for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.06049v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.06049v1)
- **Published**: 2021-10-12 14:53:14+00:00
- **Updated**: 2021-10-12 14:53:14+00:00
- **Authors**: Jiahui Fu, Guanghui Ren, Yunpeng Chen, Si Liu
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: 3D object detection with LiDAR point clouds plays an important role in autonomous driving perception module that requires high speed, stability and accuracy. However, the existing point-based methods are challenging to reach the speed requirements because of too many raw points, and the voxel-based methods are unable to ensure stable speed because of the 3D sparse convolution. In contrast, the 2D grid-based methods, such as PointPillar, can easily achieve a stable and efficient speed based on simple 2D convolution, but it is hard to get the competitive accuracy limited by the coarse-grained point clouds representation. So we propose an improved pillar with fine-grained feature based on PointPillar that can significantly improve detection accuracy. It consists of two modules, including height-aware sub-pillar and sparsity-based tiny-pillar, which get fine-grained representation respectively in the vertical and horizontal direction of 3D space. For height-aware sub-pillar, we introduce a height position encoding to keep height information of each sub-pillar during projecting to a 2D pseudo image. For sparsity-based tiny-pillar, we introduce sparsity-based CNN backbone stacked by dense feature and sparse attention module to extract feature with larger receptive field efficiently. Experimental results show that our proposed method significantly outperforms previous state-of-the-art 3D detection methods on the Waymo Open Dataset. The related code will be released to facilitate the academic and industrial study.



### Multi-Modal Interaction Graph Convolutional Network for Temporal Language Localization in Videos
- **Arxiv ID**: http://arxiv.org/abs/2110.06058v1
- **DOI**: 10.1109/TIP.2021.3113791
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.06058v1)
- **Published**: 2021-10-12 14:59:25+00:00
- **Updated**: 2021-10-12 14:59:25+00:00
- **Authors**: Zongmeng Zhang, Xianjing Han, Xuemeng Song, Yan Yan, Liqiang Nie
- **Comment**: Accepted by IEEE Transactions on Image Processing
- **Journal**: in IEEE Transactions on Image Processing, vol. 30, pp. 8265-8277,
  2021
- **Summary**: This paper focuses on tackling the problem of temporal language localization in videos, which aims to identify the start and end points of a moment described by a natural language sentence in an untrimmed video. However, it is non-trivial since it requires not only the comprehensive understanding of the video and sentence query, but also the accurate semantic correspondence capture between them. Existing efforts are mainly centered on exploring the sequential relation among video clips and query words to reason the video and sentence query, neglecting the other intra-modal relations (e.g., semantic similarity among video clips and syntactic dependency among the query words). Towards this end, in this work, we propose a Multi-modal Interaction Graph Convolutional Network (MIGCN), which jointly explores the complex intra-modal relations and inter-modal interactions residing in the video and sentence query to facilitate the understanding and semantic correspondence capture of the video and sentence query. In addition, we devise an adaptive context-aware localization method, where the context information is taken into the candidate moments and the multi-scale fully connected layers are designed to rank and adjust the boundary of the generated coarse candidate moments with different lengths. Extensive experiments on Charades-STA and ActivityNet datasets demonstrate the promising performance and superior efficiency of our model.



### MEDUSA: Multi-scale Encoder-Decoder Self-Attention Deep Neural Network Architecture for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2110.06063v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.06063v1)
- **Published**: 2021-10-12 15:05:15+00:00
- **Updated**: 2021-10-12 15:05:15+00:00
- **Authors**: Hossein Aboutalebi, Maya Pavlova, Hayden Gunraj, Mohammad Javad Shafiee, Ali Sabri, Amer Alaref, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image analysis continues to hold interesting challenges given the subtle characteristics of certain diseases and the significant overlap in appearance between diseases. In this work, we explore the concept of self-attention for tackling such subtleties in and between diseases. To this end, we introduce MEDUSA, a multi-scale encoder-decoder self-attention mechanism tailored for medical image analysis. While self-attention deep convolutional neural network architectures in existing literature center around the notion of multiple isolated lightweight attention mechanisms with limited individual capacities being incorporated at different points in the network architecture, MEDUSA takes a significant departure from this notion by possessing a single, unified self-attention mechanism with significantly higher capacity with multiple attention heads feeding into different scales in the network architecture. To the best of the authors' knowledge, this is the first "single body, multi-scale heads" realization of self-attention and enables explicit global context amongst selective attention at different levels of representational abstractions while still enabling differing local attention context at individual levels of abstractions. With MEDUSA, we obtain state-of-the-art performance on multiple challenging medical image analysis benchmarks including COVIDx, RSNA RICORD, and RSNA Pneumonia Challenge when compared to previous work. Our MEDUSA model is publicly available.



### Spectral analysis of re-parameterized light fields
- **Arxiv ID**: http://arxiv.org/abs/2110.06064v2
- **DOI**: 10.1016/j.image.2022.116751
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2110.06064v2)
- **Published**: 2021-10-12 15:05:36+00:00
- **Updated**: 2022-09-07 17:10:15+00:00
- **Authors**: Martin Alain, Aljosa Smolic
- **Comment**: None
- **Journal**: Signal Processing: Image Communication 2022
- **Summary**: In this paper, we study the spectral properties of re-parameterized light field. Following previous studies of the light field spectrum, which notably provided sampling guidelines, we focus on the two plane parameterization of the light field. However, we introduce additional flexibility by allowing the image plane to be tilted and not only parallel. A formal theoretical analysis is first presented, which shows that more flexible sampling guidelines (i.e. wider camera baselines) can be used to sample the light field when adapting the image plane orientation to the scene geometry. We then present our simulations and results to support these theoretical findings. While the work introduced in this paper is mostly theoretical, we believe these new findings open exciting avenues for more practical application of light fields, such as view synthesis or compact representation.



### On Expressivity and Trainability of Quadratic Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.06081v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2110.06081v2)
- **Published**: 2021-10-12 15:33:32+00:00
- **Updated**: 2022-04-28 01:01:24+00:00
- **Authors**: Feng-Lei Fan, Mengzhou Li, Fei Wang, Rongjie Lai, Ge Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by the diversity of biological neurons, quadratic artificial neurons can play an important role in deep learning models. The type of quadratic neurons of our interest replaces the inner-product operation in the conventional neuron with a quadratic function. Despite promising results so far achieved by networks of quadratic neurons, there are important issues not well addressed. Theoretically, the superior expressivity of a quadratic network over either a conventional network or a conventional network via quadratic activation is not fully elucidated, which makes the use of quadratic networks not well grounded. Practically, although a quadratic network can be trained via generic backpropagation, it can be subject to a higher risk of collapse than the conventional counterpart. To address these issues, we first apply the spline theory and a measure from algebraic geometry to give two theorems that demonstrate better model expressivity of a quadratic network than the conventional counterpart with or without quadratic activation. Then, we propose an effective and efficient training strategy referred to as ReLinear to stabilize the training process of a quadratic network, thereby unleashing the full potential in its associated machine learning tasks. Comprehensive experiments on popular datasets are performed to support our findings and evaluate the performance of quadratic deep learning.



### Continuous Conditional Random Field Convolution for Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.06085v1
- **DOI**: 10.1016/j.patcog.2021.108357
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.06085v1)
- **Published**: 2021-10-12 15:35:38+00:00
- **Updated**: 2021-10-12 15:35:38+00:00
- **Authors**: Fei Yang, Franck Davoine, Huan Wang, Zhong Jin
- **Comment**: 20 pages + 8 pages (supplemental material)
- **Journal**: Pattern Recognition, Volume 122, February 2022
- **Summary**: Point cloud segmentation is the foundation of 3D environmental perception for modern intelligent systems. To solve this problem and image segmentation, conditional random fields (CRFs) are usually formulated as discrete models in label space to encourage label consistency, which is actually a kind of postprocessing. In this paper, we reconsider the CRF in feature space for point cloud segmentation because it can capture the structure of features well to improve the representation ability of features rather than simply smoothing. Therefore, we first model the point cloud features with a continuous quadratic energy model and formulate its solution process as a message-passing graph convolution, by which it can be easily integrated into a deep network. We theoretically demonstrate that the message passing in the graph convolution is equivalent to the mean-field approximation of a continuous CRF model. Furthermore, we build an encoder-decoder network based on the proposed continuous CRF graph convolution (CRFConv), in which the CRFConv embedded in the decoding layers can restore the details of high-level features that were lost in the encoding stage to enhance the location ability of the network, thereby benefiting segmentation. Analogous to the CRFConv, we show that the classical discrete CRF can also work collaboratively with the proposed network via another graph convolution to further improve the segmentation results. Experiments on various point cloud benchmarks demonstrate the effectiveness and robustness of the proposed method. Compared with the state-of-the-art methods, the proposed method can also achieve competitive segmentation performance.



### Deep Human-guided Conditional Variational Generative Modeling for Automated Urban Planning
- **Arxiv ID**: http://arxiv.org/abs/2110.07717v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.07717v1)
- **Published**: 2021-10-12 15:45:38+00:00
- **Updated**: 2021-10-12 15:45:38+00:00
- **Authors**: Dongjie Wang, Kunpeng Liu, Pauline Johnson, Leilei Sun, Bowen Du, Yanjie Fu
- **Comment**: ICDM2021
- **Journal**: None
- **Summary**: Urban planning designs land-use configurations and can benefit building livable, sustainable, safe communities. Inspired by image generation, deep urban planning aims to leverage deep learning to generate land-use configurations. However, urban planning is a complex process. Existing studies usually ignore the need of personalized human guidance in planning, and spatial hierarchical structure in planning generation. Moreover, the lack of large-scale land-use configuration samples poses a data sparsity challenge. This paper studies a novel deep human guided urban planning method to jointly solve the above challenges. Specifically, we formulate the problem into a deep conditional variational autoencoder based framework. In this framework, we exploit the deep encoder-decoder design to generate land-use configurations. To capture the spatial hierarchy structure of land uses, we enforce the decoder to generate both the coarse-grained layer of functional zones, and the fine-grained layer of POI distributions. To integrate human guidance, we allow humans to describe what they need as texts and use these texts as a model condition input. To mitigate training data sparsity and improve model robustness, we introduce a variational Gaussian embedding mechanism. It not just allows us to better approximate the embedding space distribution of training data and sample a larger population to overcome sparsity, but also adds more probabilistic randomness into the urban planning generation to improve embedding diversity so as to improve robustness. Finally, we present extensive experiments to validate the enhanced performances of our method.



### Sign Language Recognition via Skeleton-Aware Multi-Model Ensemble
- **Arxiv ID**: http://arxiv.org/abs/2110.06161v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.06161v1)
- **Published**: 2021-10-12 16:57:18+00:00
- **Updated**: 2021-10-12 16:57:18+00:00
- **Authors**: Songyao Jiang, Bin Sun, Lichen Wang, Yue Bai, Kunpeng Li, Yun Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Sign language is commonly used by deaf or mute people to communicate but requires extensive effort to master. It is usually performed with the fast yet delicate movement of hand gestures, body posture, and even facial expressions. Current Sign Language Recognition (SLR) methods usually extract features via deep neural networks and suffer overfitting due to limited and noisy data. Recently, skeleton-based action recognition has attracted increasing attention due to its subject-invariant and background-invariant nature, whereas skeleton-based SLR is still under exploration due to the lack of hand annotations. Some researchers have tried to use off-line hand pose trackers to obtain hand keypoints and aid in recognizing sign language via recurrent neural networks. Nevertheless, none of them outperforms RGB-based approaches yet. To this end, we propose a novel Skeleton Aware Multi-modal Framework with a Global Ensemble Model (GEM) for isolated SLR (SAM-SLR-v2) to learn and fuse multi-modal feature representations towards a higher recognition rate. Specifically, we propose a Sign Language Graph Convolution Network (SL-GCN) to model the embedded dynamics of skeleton keypoints and a Separable Spatial-Temporal Convolution Network (SSTCN) to exploit skeleton features. The skeleton-based predictions are fused with other RGB and depth based modalities by the proposed late-fusion GEM to provide global information and make a faithful SLR prediction. Experiments on three isolated SLR datasets demonstrate that our proposed SAM-SLR-v2 framework is exceedingly effective and achieves state-of-the-art performance with significant margins. Our code will be available at https://github.com/jackyjsy/SAM-SLR-v2



### M2GAN: A Multi-Stage Self-Attention Network for Image Rain Removal on Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2110.06164v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.06164v1)
- **Published**: 2021-10-12 16:58:33+00:00
- **Updated**: 2021-10-12 16:58:33+00:00
- **Authors**: Duc Manh Nguyen, Sang-Woong Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Image deraining is a new challenging problem in applications of autonomous vehicles. In a bad weather condition of heavy rainfall, raindrops, mainly hitting the vehicle's windshield, can significantly reduce observation ability even though the windshield wipers might be able to remove part of it. Moreover, rain flows spreading over the windshield can yield the physical effect of refraction, which seriously impede the sightline or undermine the machine learning system equipped in the vehicle. In this paper, we propose a new multi-stage multi-task recurrent generative adversarial network (M2GAN) to deal with challenging problems of raindrops hitting the car's windshield. This method is also applicable for removing raindrops appearing on a glass window or lens. M2GAN is a multi-stage multi-task generative adversarial network that can utilize prior high-level information, such as semantic segmentation, to boost deraining performance. To demonstrate M2GAN, we introduce the first real-world dataset for rain removal on autonomous vehicles. The experimental results show that our proposed method is superior to other state-of-the-art approaches of deraining raindrops in respect of quantitative metrics and visual quality. M2GAN is considered the first method to deal with challenging problems of real-world rains under unconstrained environments such as autonomous vehicles.



### TAda! Temporally-Adaptive Convolutions for Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/2110.06178v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.06178v4)
- **Published**: 2021-10-12 17:25:07+00:00
- **Updated**: 2022-03-17 15:14:37+00:00
- **Authors**: Ziyuan Huang, Shiwei Zhang, Liang Pan, Zhiwu Qing, Mingqian Tang, Ziwei Liu, Marcelo H. Ang Jr
- **Comment**: Accepted to ICLR 2022. Project page:
  https://tadaconv-iclr2022.github.io
- **Journal**: None
- **Summary**: Spatial convolutions are widely used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modelling complex temporal dynamics in videos. Specifically, TAdaConv empowers the spatial convolutions with temporal modelling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to previous temporal modelling operations, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, the kernel calibration brings an increased model capacity. We construct TAda2D and TAdaConvNeXt networks by replacing the 2D convolutions in ResNet and ConvNeXt with TAdaConv, which leads to at least on par or better performance compared to state-of-the-art approaches on multiple video action recognition and localization benchmarks. We also demonstrate that as a readily plug-in operation with negligible computation overhead, TAdaConv can effectively improve many existing video models with a convincing margin.



### ABO: Dataset and Benchmarks for Real-World 3D Object Understanding
- **Arxiv ID**: http://arxiv.org/abs/2110.06199v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.06199v2)
- **Published**: 2021-10-12 17:52:42+00:00
- **Updated**: 2022-06-24 16:21:09+00:00
- **Authors**: Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F. Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Amazon Berkeley Objects (ABO), a new large-scale dataset designed to help bridge the gap between real and virtual 3D worlds. ABO contains product catalog images, metadata, and artist-created 3D models with complex geometries and physically-based materials that correspond to real, household objects. We derive challenging benchmarks that exploit the unique properties of ABO and measure the current limits of the state-of-the-art on three open problems for real-world 3D object understanding: single-view 3D reconstruction, material estimation, and cross-domain multi-view object retrieval.



### Open-Set Recognition: a Good Closed-Set Classifier is All You Need?
- **Arxiv ID**: http://arxiv.org/abs/2110.06207v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.06207v2)
- **Published**: 2021-10-12 17:58:59+00:00
- **Updated**: 2022-04-13 17:59:07+00:00
- **Authors**: Sagar Vaze, Kai Han, Andrea Vedaldi, Andrew Zisserman
- **Comment**: ICLR 22 Oral. Changes from pre-print highlighted on Github page
- **Journal**: None
- **Summary**: The ability to identify whether or not a test sample belongs to one of the semantic classes in a classifier's training set is critical to practical deployment of the model. This task is termed open-set recognition (OSR) and has received significant attention in recent years. In this paper, we first demonstrate that the ability of a classifier to make the 'none-of-above' decision is highly correlated with its accuracy on the closed-set classes. We find that this relationship holds across loss objectives and architectures, and further demonstrate the trend both on the standard OSR benchmarks as well as on a large-scale ImageNet evaluation. Second, we use this correlation to boost the performance of a maximum logit score OSR 'baseline' by improving its closed-set accuracy, and with this strong baseline achieve state-of-the-art on a number of OSR benchmarks. Similarly, we boost the performance of the existing state-of-the-art method by improving its closed-set accuracy, but the resulting discrepancy with the strong baseline is marginal. Our third contribution is to present the 'Semantic Shift Benchmark' (SSB), which better respects the task of detecting semantic novelty, in contrast to other forms of distribution shift also considered in related sub-fields, such as out-of-distribution detection. On this new evaluation, we again demonstrate that there is negligible difference between the strong baseline and the existing state-of-the-art. Project Page: https://www.robots.ox.ac.uk/~vgg/research/osr/



### Real Image Inversion via Segments
- **Arxiv ID**: http://arxiv.org/abs/2110.06269v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.06269v1)
- **Published**: 2021-10-12 18:37:39+00:00
- **Updated**: 2021-10-12 18:37:39+00:00
- **Authors**: David Futschik, Michal Lukáč, Eli Shechtman, Daniel Sýkora
- **Comment**: 7 pages, 10 figures
- **Journal**: None
- **Summary**: In this short report, we present a simple, yet effective approach to editing real images via generative adversarial networks (GAN). Unlike previous techniques, that treat all editing tasks as an operation that affects pixel values in the entire image in our approach we cut up the image into a set of smaller segments. For those segments corresponding latent codes of a generative network can be estimated with greater accuracy due to the lower number of constraints. When codes are altered by the user the content in the image is manipulated locally while the rest of it remains unaffected. Thanks to this property the final edited image better retains the original structures and thus helps to preserve natural look.



### Real-Time Learning from An Expert in Deep Recommendation Systems with Marginal Distance Probability Distribution
- **Arxiv ID**: http://arxiv.org/abs/2110.06287v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.06287v2)
- **Published**: 2021-10-12 19:20:18+00:00
- **Updated**: 2022-04-04 14:13:11+00:00
- **Authors**: Arash Mahyari, Peter Pirolli, Jacqueline A. LeBlanc
- **Comment**: None
- **Journal**: IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, 2022
- **Summary**: Recommendation systems play an important role in today's digital world. They have found applications in various applications such as music platforms, e.g., Spotify, and movie streaming services, e.g., Netflix. Less research effort has been devoted to physical exercise recommendation systems. Sedentary lifestyles have become the major driver of several diseases as well as healthcare costs. In this paper, we develop a recommendation system for daily exercise activities to users based on their history, profile and similar users. The developed recommendation system uses a deep recurrent neural network with user-profile attention and temporal attention mechanisms.   Moreover, exercise recommendation systems are significantly different from streaming recommendation systems in that we are not able to collect click feedback from the participants in exercise recommendation systems. Thus, we propose a real-time, expert-in-the-loop active learning procedure. The active learners calculate the uncertainty of the recommender at each time step for each user and ask an expert for a recommendation when the certainty is low. In this paper, we derive the probability distribution function of marginal distance, and use it to determine when to ask experts for feedback. Our experimental results on a mHealth dataset show improved accuracy after incorporating the real-time active learner with the recommendation system.



### Persistent Homology with Improved Locality Information for more Effective Delineation
- **Arxiv ID**: http://arxiv.org/abs/2110.06295v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.06295v3)
- **Published**: 2021-10-12 19:28:39+00:00
- **Updated**: 2022-12-24 12:00:22+00:00
- **Authors**: Doruk Oner, Adélie Garin, Mateusz Koziński, Kathryn Hess, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Persistent Homology (PH) has been successfully used to train networks to detect curvilinear structures and to improve the topological quality of their results. However, existing methods are very global and ignore the location of topological features. In this paper, we remedy this by introducing a new filtration function that fuses two earlier approaches: thresholding-based filtration, previously used to train deep networks to segment medical images, and filtration with height functions, typically used to compare 2D and 3D shapes. We experimentally demonstrate that deep networks trained using our PH-based loss function yield reconstructions of road networks and neuronal processes that reflect ground-truth connectivity better than networks trained with existing loss functions based on PH. Code is available at https://github.com/doruk-oner/PH-TopoLoss.



### Application of Homomorphic Encryption in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2110.07768v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.07768v1)
- **Published**: 2021-10-12 19:57:12+00:00
- **Updated**: 2021-10-12 19:57:12+00:00
- **Authors**: Francis Dutil, Alexandre See, Lisa Di Jorio, Florent Chandelier
- **Comment**: None
- **Journal**: None
- **Summary**: In this technical report, we explore the use of homomorphic encryption (HE) in the context of training and predicting with deep learning (DL) models to deliver strict \textit{Privacy by Design} services, and to enforce a zero-trust model of data governance. First, we show how HE can be used to make predictions over medical images while preventing unauthorized secondary use of data, and detail our results on a disease classification task with OCT images. Then, we demonstrate that HE can be used to secure the training of DL models through federated learning, and report some experiments using 3D chest CT-Scans for a nodule detection task.



### Exploring Content Based Image Retrieval for Highly Imbalanced Melanoma Data using Style Transfer, Semantic Image Segmentation and Ensemble Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.06331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.06331v1)
- **Published**: 2021-10-12 20:34:51+00:00
- **Updated**: 2021-10-12 20:34:51+00:00
- **Authors**: Priyam Mehta
- **Comment**: None
- **Journal**: None
- **Summary**: Lesion images are frequently taken in open-set settings. Because of this, the image data generated is extremely varied in nature.It is difficult for a convolutional neural network to find proper features and generalise well, as a result content based image retrieval (CBIR) system for lesion images are difficult to build. This paper explores this domain and proposes multiple similarity measures which uses Style Loss and Dice Coefficient via a novel similarity measure called I1-Score. Out of the CBIR similarity measures proposed, pure style loss approach achieves a remarkable accuracy increase over traditional approaches like Euclidean Distance and Cosine Similarity. The I1-Scores using style loss performed better than traditional approaches by a small margin, whereas, I1-Scores with dice-coefficient faired very poorly. The model used is trained using ensemble learning for better generalization.



### Voice-assisted Image Labelling for Endoscopic Ultrasound Classification using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.06367v1
- **DOI**: 10.1109/TMI.2021.3139023
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.06367v1)
- **Published**: 2021-10-12 21:22:24+00:00
- **Updated**: 2021-10-12 21:22:24+00:00
- **Authors**: Ester Bonmati, Yipeng Hu, Alexander Grimwood, Gavin J. Johnson, George Goodchild, Margaret G. Keane, Kurinchi Gurusamy, Brian Davidson, Matthew J. Clarkson, Stephen P. Pereira, Dean C. Barratt
- **Comment**: Submitted to IEEE TMI
- **Journal**: None
- **Summary**: Ultrasound imaging is a commonly used technology for visualising patient anatomy in real-time during diagnostic and therapeutic procedures. High operator dependency and low reproducibility make ultrasound imaging and interpretation challenging with a steep learning curve. Automatic image classification using deep learning has the potential to overcome some of these challenges by supporting ultrasound training in novices, as well as aiding ultrasound image interpretation in patient with complex pathology for more experienced practitioners. However, the use of deep learning methods requires a large amount of data in order to provide accurate results. Labelling large ultrasound datasets is a challenging task because labels are retrospectively assigned to 2D images without the 3D spatial context available in vivo or that would be inferred while visually tracking structures between frames during the procedure. In this work, we propose a multi-modal convolutional neural network (CNN) architecture that labels endoscopic ultrasound (EUS) images from raw verbal comments provided by a clinician during the procedure. We use a CNN composed of two branches, one for voice data and another for image data, which are joined to predict image labels from the spoken names of anatomical landmarks. The network was trained using recorded verbal comments from expert operators. Our results show a prediction accuracy of 76% at image level on a dataset with 5 different labels. We conclude that the addition of spoken commentaries can increase the performance of ultrasound image classification, and eliminate the burden of manually labelling large EUS datasets necessary for deep learning applications.



### A Survey of Open Source User Activity Traces with Applications to User Mobility Characterization and Modeling
- **Arxiv ID**: http://arxiv.org/abs/2110.06382v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2110.06382v2)
- **Published**: 2021-10-12 22:05:11+00:00
- **Updated**: 2021-10-15 18:42:56+00:00
- **Authors**: Sinjoni Mukhopadhyay King, Faisal Nawab, Katia Obraczka
- **Comment**: 23 pages, 6 pages references
- **Journal**: None
- **Summary**: The current state-of-the-art in user mobility research has extensively relied on open-source mobility traces captured from pedestrian and vehicular activity through a variety of communication technologies as users engage in a wide-range of applications, including connected healthcare, localization, social media, e-commerce, etc. Most of these traces are feature-rich and diverse, not only in the information they provide, but also in how they can be used and leveraged. This diversity poses two main challenges for researchers and practitioners who wish to make use of available mobility datasets. First, it is quite difficult to get a bird's eye view of the available traces without spending considerable time looking them up. Second, once they have found the traces, they still need to figure out whether the traces are adequate to their needs.   The purpose of this survey is three-fold. It proposes a taxonomy to classify open-source mobility traces including their mobility mode, data source and collection technology. It then uses the proposed taxonomy to classify existing open-source mobility traces and finally, highlights three case studies using popular publicly available datasets to showcase how our taxonomy can tease out feature sets in traces to help determine their applicability to specific use-cases.



### CovXR: Automated Detection of COVID-19 Pneumonia in Chest X-Rays through Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.06398v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.06398v1)
- **Published**: 2021-10-12 23:21:13+00:00
- **Updated**: 2021-10-12 23:21:13+00:00
- **Authors**: Vishal Shenoy, Sachin B. Malik
- **Comment**: 6 pages, 7 figures
- **Journal**: None
- **Summary**: Coronavirus disease 2019 (COVID-19) is the highly contagious illness caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The standard diagnostic testing procedure for COVID-19 is testing a nasopharyngeal swab for SARS-CoV-2 nucleic acid using a real-time polymerase chain reaction (PCR), which can take multiple days to provide a diagnosis. Another widespread form of testing is rapid antigen testing, which has a low sensitivity compared to PCR, but is favored for its quick diagnosis time of usually 15-30 minutes. Patients who test positive for COVID-19 demonstrate diffuse alveolar damage in 87% of cases. Machine learning has proven to have advantages in image classification problems with radiology. In this work, we introduce CovXR as a machine learning model designed to detect COVID-19 pneumonia in chest X-rays (CXR). CovXR is a convolutional neural network (CNN) trained on over 4,300 chest X-rays. The performance of the model is measured through accuracy, F1 score, sensitivity, and specificity. The model achieves an accuracy of 95.5% and an F1 score of 0.954. The sensitivity is 93.5% and specificity is 97.5%. With accuracy above 95% and F1 score above 0.95, CovXR is highly accurate in predicting COVID-19 pneumonia on CXRs. The model achieves better accuracy than prior work and uses a unique approach to identify COVID-19 pneumonia. CovXR is highly accurate in identifying COVID-19 on CXRs of patients with a PCR confirmed positive diagnosis and provides much faster results than PCR tests.



### Dynamic Inference with Neural Interpreters
- **Arxiv ID**: http://arxiv.org/abs/2110.06399v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.06399v1)
- **Published**: 2021-10-12 23:22:45+00:00
- **Updated**: 2021-10-12 23:22:45+00:00
- **Authors**: Nasim Rahaman, Muhammad Waleed Gondal, Shruti Joshi, Peter Gehler, Yoshua Bengio, Francesco Locatello, Bernhard Schölkopf
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Modern neural network architectures can leverage large amounts of data to generalize well within the training distribution. However, they are less capable of systematic generalization to data drawn from unseen but related distributions, a feat that is hypothesized to require compositional reasoning and reuse of knowledge. In this work, we present Neural Interpreters, an architecture that factorizes inference in a self-attention network as a system of modules, which we call \emph{functions}. Inputs to the model are routed through a sequence of functions in a way that is end-to-end learned. The proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. To demonstrate the versatility of Neural Interpreters, we evaluate it in two distinct settings: image classification and visual abstract reasoning on Raven Progressive Matrices. In the former, we show that Neural Interpreters perform on par with the vision transformer using fewer parameters, while being transferrable to a new task in a sample efficient manner. In the latter, we find that Neural Interpreters are competitive with respect to the state-of-the-art in terms of systematic generalization



### CyTran: A Cycle-Consistent Transformer with Multi-Level Consistency for Non-Contrast to Contrast CT Translation
- **Arxiv ID**: http://arxiv.org/abs/2110.06400v3
- **DOI**: 10.1016/j.neucom.2023.03.072
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.06400v3)
- **Published**: 2021-10-12 23:25:03+00:00
- **Updated**: 2023-04-05 09:36:33+00:00
- **Authors**: Nicolae-Catalin Ristea, Andreea-Iuliana Miron, Olivian Savencu, Mariana-Iuliana Georgescu, Nicolae Verga, Fahad Shahbaz Khan, Radu Tudor Ionescu
- **Comment**: Accepted for publication in Neurocomputing
- **Journal**: None
- **Summary**: We propose a novel approach to translate unpaired contrast computed tomography (CT) scans to non-contrast CT scans and the other way around. Solving this task has two important applications: (i) to automatically generate contrast CT scans for patients for whom injecting contrast substance is not an option, and (ii) to enhance the alignment between contrast and non-contrast CT by reducing the differences induced by the contrast substance before registration. Our approach is based on cycle-consistent generative adversarial convolutional transformers, for short, CyTran. Our neural model can be trained on unpaired images, due to the integration of a multi-level cycle-consistency loss. Aside from the standard cycle-consistency loss applied at the image level, we propose to apply additional cycle-consistency losses between intermediate feature representations, which enforces the model to be cycle-consistent at multiple representations levels, leading to superior results. To deal with high-resolution images, we design a hybrid architecture based on convolutional and multi-head attention layers. In addition, we introduce a novel data set, Coltea-Lung-CT-100W, containing 100 3D triphasic lung CT scans (with a total of 37,290 images) collected from 100 female patients (there is one examination per patient). Each scan contains three phases (non-contrast, early portal venous, and late arterial), allowing us to perform experiments to compare our novel approach with state-of-the-art methods for image style transfer. Our empirical results show that CyTran outperforms all competing methods. Moreover, we show that CyTran can be employed as a preliminary step to improve a state-of-the-art medical image alignment method. We release our novel model and data set as open source at https://github.com/ristea/cycle-transformer.



