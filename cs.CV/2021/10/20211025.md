# Arxiv Papers in cs.CV on 2021-10-25
### MUSE: Feature Self-Distillation with Mutual Information and Self-Information
- **Arxiv ID**: http://arxiv.org/abs/2110.12606v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12606v1)
- **Published**: 2021-10-25 02:36:25+00:00
- **Updated**: 2021-10-25 02:36:25+00:00
- **Authors**: Yu Gong, Ye Yu, Gaurav Mittal, Greg Mori, Mei Chen
- **Comment**: The 32nd British Machine Vision Conference (BMVC 2021)
- **Journal**: None
- **Summary**: We present a novel information-theoretic approach to introduce dependency among features of a deep convolutional neural network (CNN). The core idea of our proposed method, called MUSE, is to combine MUtual information and SElf-information to jointly improve the expressivity of all features extracted from different layers in a CNN. We present two variants of the realization of MUSE -- Additive Information and Multiplicative Information. Importantly, we argue and empirically demonstrate that MUSE, compared to other feature discrepancy functions, is a more functional proxy to introduce dependency and effectively improve the expressivity of all features in the knowledge distillation framework. MUSE achieves superior performance over a variety of popular architectures and feature discrepancy functions for self-distillation and online distillation, and performs competitively with the state-of-the-art methods for offline distillation. MUSE is also demonstrably versatile that enables it to be easily extended to CNN-based models on tasks other than image classification such as object detection.



### Federated Test-Time Adaptive Face Presentation Attack Detection with Dual-Phase Privacy Preservation
- **Arxiv ID**: http://arxiv.org/abs/2110.12613v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12613v1)
- **Published**: 2021-10-25 02:51:05+00:00
- **Updated**: 2021-10-25 02:51:05+00:00
- **Authors**: Rui Shao, Bochao Zhang, Pong C. Yuen, Vishal M. Patel
- **Comment**: Accepted by FG 2021. arXiv admin note: substantial text overlap with
  arXiv:2104.06595, arXiv:2005.14638
- **Journal**: None
- **Summary**: Face presentation attack detection (fPAD) plays a critical role in the modern face recognition pipeline. The generalization ability of face presentation attack detection models to unseen attacks has become a key issue for real-world deployment, which can be improved when models are trained with face images from different input distributions and different types of spoof attacks. In reality, due to legal and privacy issues, training data (both real face images and spoof images) are not allowed to be directly shared between different data sources. In this paper, to circumvent this challenge, we propose a Federated Test-Time Adaptive Face Presentation Attack Detection with Dual-Phase Privacy Preservation framework, with the aim of enhancing the generalization ability of fPAD models in both training and testing phase while preserving data privacy. In the training phase, the proposed framework exploits the federated learning technique, which simultaneously takes advantage of rich fPAD information available at different data sources by aggregating model updates from them without accessing their private data. To further boost the generalization ability, in the testing phase, we explore test-time adaptation by minimizing the entropy of fPAD model prediction on the testing data, which alleviates the domain gap between training and testing data and thus reduces the generalization error of a fPAD model. We introduce the experimental setting to evaluate the proposed framework and carry out extensive experiments to provide various insights about the proposed method for fPAD.



### Accelerate 3D Object Processing via Spectral Layout
- **Arxiv ID**: http://arxiv.org/abs/2110.12621v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12621v2)
- **Published**: 2021-10-25 03:18:37+00:00
- **Updated**: 2021-10-28 00:10:09+00:00
- **Authors**: Yongyu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: 3D image processing is an important problem in computer vision and pattern recognition fields. Compared with 2D image processing, its computation difficulty and cost are much higher due to the extra dimension. To fundamentally address this problem, we propose to embed the essential information in a 3D object into 2D space via spectral layout. Specifically, we construct a 3D adjacency graph to capture spatial structure of the 3D voxel grid. Then we calculate the eigenvectors corresponding to the second and third smallest eigenvalues of its graph Laplacian and perform spectral layout to map each voxel into a pixel in 2D Cartesian coordinate plane. The proposed method can achieve high quality 2D representations for 3D objects, which enables to use 2D-based methods to process 3D objects. The experimental results demonstrate the effectiveness and efficiency of our method.



### Ultra Light OCR Competition Technical Report
- **Arxiv ID**: http://arxiv.org/abs/2110.12623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12623v1)
- **Published**: 2021-10-25 03:21:49+00:00
- **Updated**: 2021-10-25 03:21:49+00:00
- **Authors**: Shuhan Zhang, Yuxin Zou, Tianhe Wang, Yichao Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Ultra Light OCR Competition is a Chinese scene text recognition competition jointly organized by CSIG (China Society of Image and Graphics) and Baidu, Inc. In addition to focusing on common problems in Chinese scene text recognition, such as long text length and massive characters, we need to balance the trade-off of model scale and accuracy since the model size limitation in the competition is 10M.   From experiments in aspects of data, model, training, etc, we proposed a general and effective method for Chinese scene text recognition, which got us second place among over 100 teams with accuracy 0.817 in TestB dataset. The code is available at https://aistudio.baidu.com/aistudio/projectdetail/2159102.



### Learning Continuous Face Representation with Explicit Functions
- **Arxiv ID**: http://arxiv.org/abs/2110.15268v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.15268v1)
- **Published**: 2021-10-25 03:49:20+00:00
- **Updated**: 2021-10-25 03:49:20+00:00
- **Authors**: Liping Zhang, Weijun Li, Linjun Sun, Lina Yu, Xin Ning, Xiaoli Dong, Jian Xu, Hong Qin
- **Comment**: None
- **Journal**: None
- **Summary**: How to represent a face pattern? While it is presented in a continuous way in our visual system, computers often store and process the face image in a discrete manner with 2D arrays of pixels. In this study, we attempt to learn a continuous representation for face images with explicit functions. First, we propose an explicit model (EmFace) for human face representation in the form of a finite sum of mathematical terms, where each term is an analytic function element. Further, to estimate the unknown parameters of EmFace, a novel neural network, EmNet, is designed with an encoder-decoder structure and trained using the backpropagation algorithm, where the encoder is defined by a deep convolutional neural network and the decoder is an explicit mathematical expression of EmFace. Experimental results show that EmFace has a higher representation performance on faces with various expressions, postures, and other factors, compared to that of other methods. Furthermore, EmFace achieves reasonable performance on several face image processing tasks, including face image restoration, denoising, and transformation.



### Age and Gender Prediction using Deep CNNs and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.12633v1
- **DOI**: 10.1007/978-981-16-1092-9_25
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12633v1)
- **Published**: 2021-10-25 04:23:59+00:00
- **Updated**: 2021-10-25 04:23:59+00:00
- **Authors**: Vikas Sheoran, Shreyansh Joshi, Tanisha R. Bhayani
- **Comment**: 12 pages, 2 figures, 11 tables
- **Journal**: CCIS 1377 (2021) 293-304
- **Summary**: The last decade or two has witnessed a boom of images. With the increasing ubiquity of cameras and with the advent of selfies, the number of facial images available in the world has skyrocketed. Consequently, there has been a growing interest in automatic age and gender prediction of a person using facial images. We in this paper focus on this challenging problem. Specifically, this paper focuses on age estimation, age classification and gender classification from still facial images of an individual. We train different models for each problem and we also draw comparisons between building a custom CNN (Convolutional Neural Network) architecture and using various CNN architectures as feature extractors, namely VGG16 pre-trained on VGGFace, Res-Net50 and SE-ResNet50 pre-trained on VGGFace2 dataset and training over those extracted features. We also provide baseline performance of various machine learning algorithms on the feature extraction which gave us the best results. It was observed that even simple linear regression trained on such extracted features outperformed training CNN, ResNet50 and ResNeXt50 from scratch for age estimation.



### Progressively Select and Reject Pseudo-labelled Samples for Open-Set Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2110.12635v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12635v1)
- **Published**: 2021-10-25 04:28:55+00:00
- **Updated**: 2021-10-25 04:28:55+00:00
- **Authors**: Qian Wang, Fanlin Meng, Toby P. Breckon
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Domain adaptation solves image classification problems in the target domain by taking advantage of the labelled source data and unlabelled target data. Usually, the source and target domains share the same set of classes. As a special case, Open-Set Domain Adaptation (OSDA) assumes there exist additional classes in the target domain but not present in the source domain. To solve such a domain adaptation problem, our proposed method learns discriminative common subspaces for the source and target domains using a novel Open-Set Locality Preserving Projection (OSLPP) algorithm. The source and target domain data are aligned in the learned common spaces class-wisely. To handle the open-set classification problem, our method progressively selects target samples to be pseudo-labelled as known classes and rejects the outliers if they are detected as from unknown classes. The common subspace learning algorithm OSLPP simultaneously aligns the labelled source data and pseudo-labelled target data from known classes and pushes the rejected target data away from the known classes. The common subspace learning and the pseudo-labelled sample selection/rejection facilitate each other in an iterative learning framework and achieves state-of-the-art performance on benchmark datasets Office-31 and Office-Home with the average HOS of 87.4% and 67.0% respectively.



### Deep Learning for UAV-based Object Detection and Tracking: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2110.12638v1
- **DOI**: 10.1109/MGRS.2021.3115137
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12638v1)
- **Published**: 2021-10-25 04:43:24+00:00
- **Updated**: 2021-10-25 04:43:24+00:00
- **Authors**: Xin Wu, Wei Li, Danfeng Hong, Ran Tao, Qian Du
- **Comment**: None
- **Journal**: IEEE Geoscience and Remote Sensing Magazine,2021
- **Summary**: Owing to effective and flexible data acquisition, unmanned aerial vehicle (UAV) has recently become a hotspot across the fields of computer vision (CV) and remote sensing (RS). Inspired by recent success of deep learning (DL), many advanced object detection and tracking approaches have been widely applied to various UAV-related tasks, such as environmental monitoring, precision agriculture, traffic management. This paper provides a comprehensive survey on the research progress and prospects of DL-based UAV object detection and tracking methods. More specifically, we first outline the challenges, statistics of existing methods, and provide solutions from the perspectives of DL-based models in three research topics: object detection from the image, object detection from the video, and object tracking from the video. Open datasets related to UAV-dominated object detection and tracking are exhausted, and four benchmark datasets are employed for performance evaluation using some state-of-the-art methods. Finally, prospects and considerations for the future work are discussed and summarized. It is expected that this survey can facilitate those researchers who come from remote sensing field with an overview of DL-based UAV object detection and tracking methods, along with some thoughts on their further developments.



### Kernel density estimation-based sampling for neural network classification
- **Arxiv ID**: http://arxiv.org/abs/2110.12644v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12644v1)
- **Published**: 2021-10-25 04:59:42+00:00
- **Updated**: 2021-10-25 04:59:42+00:00
- **Authors**: Firuz Kamalov, Ashraf Elnagar
- **Comment**: Accepted ISNCC
- **Journal**: None
- **Summary**: Imbalanced data occurs in a wide range of scenarios. The skewed distribution of the target variable elicits bias in machine learning algorithms. One of the popular methods to combat imbalanced data is to artificially balance the data through resampling. In this paper, we compare the efficacy of a recently proposed kernel density estimation (KDE) sampling technique in the context of artificial neural networks. We benchmark the KDE sampling method against two base sampling techniques and perform comparative experiments using 8 datasets and 3 neural networks architectures. The results show that KDE sampling produces the best performance on 6 out of 8 datasets. However, it must be used with caution on image datasets. We conclude that KDE sampling is capable of significantly improving the performance of neural networks.



### Bone Marrow Cell Recognition: Training Deep Object Detection with A New Loss Function
- **Arxiv ID**: http://arxiv.org/abs/2110.12647v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12647v1)
- **Published**: 2021-10-25 05:17:04+00:00
- **Updated**: 2021-10-25 05:17:04+00:00
- **Authors**: Dehao Huang, Jintao Cheng, Rui Fan, Zhihao Su, Qiongxiong Ma, Jie Li
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: For a long time, bone marrow cell morphology examination has been an essential tool for diagnosing blood diseases. However, it is still mainly dependent on the subjective diagnosis of experienced doctors, and there is no objective quantitative standard. Therefore, it is crucial to study a robust bone marrow cell detection algorithm for a quantitative automatic analysis system. Currently, due to the dense distribution of cells in the bone marrow smear and the diverse cell classes, the detection of bone marrow cells is difficult. The existing bone marrow cell detection algorithms are still insufficient for the automatic analysis system of bone marrow smears. This paper proposes a bone marrow cell detection algorithm based on the YOLOv5 network, trained by minimizing a novel loss function. The classification method of bone marrow cell detection tasks is the basis of the proposed novel loss function. Since bone marrow cells are classified according to series and stages, part of the classes in adjacent stages are similar. The proposed novel loss function considers the similarity between bone marrow cell classes, increases the penalty for prediction errors between dissimilar classes, and reduces the penalty for prediction errors between similar classes. The results show that the proposed loss function effectively improves the algorithm's performance, and the proposed bone marrow cell detection algorithm has achieved better performance than other cell detection algorithms.



### ZerO Initialization: Initializing Neural Networks with only Zeros and Ones
- **Arxiv ID**: http://arxiv.org/abs/2110.12661v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12661v3)
- **Published**: 2021-10-25 06:17:33+00:00
- **Updated**: 2022-11-04 17:17:26+00:00
- **Authors**: Jiawei Zhao, Florian Schäfer, Anima Anandkumar
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are usually initialized with random weights, with adequately selected initial variance to ensure stable signal propagation during training. However, selecting the appropriate variance becomes challenging especially as the number of layers grows. In this work, we replace random weight initialization with a fully deterministic initialization scheme, viz., ZerO, which initializes the weights of networks with only zeros and ones (up to a normalization factor), based on identity and Hadamard transforms. Through both theoretical and empirical studies, we demonstrate that ZerO is able to train networks without damaging their expressivity. Applying ZerO on ResNet achieves state-of-the-art performance on various datasets, including ImageNet, which suggests random weights may be unnecessary for network initialization. In addition, ZerO has many benefits, such as training ultra deep networks (without batch-normalization), exhibiting low-rank learning trajectories that result in low-rank and sparse solutions, and improving training reproducibility.



### Industrial Scene Text Detection with Refined Feature-attentive Network
- **Arxiv ID**: http://arxiv.org/abs/2110.12663v2
- **DOI**: 10.1109/TCSVT.2022.3156390
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12663v2)
- **Published**: 2021-10-25 06:23:44+00:00
- **Updated**: 2022-03-29 10:55:56+00:00
- **Authors**: Tongkun Guan, Chaochen Gu, Changsheng Lu, Jingzheng Tu, Qi Feng, Kaijie Wu, Xinping Guan
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting the marking characters of industrial metal parts remains challenging due to low visual contrast, uneven illumination, corroded character structures, and cluttered background of metal part images. Affected by these factors, bounding boxes generated by most existing methods locate low-contrast text areas inaccurately. In this paper, we propose a refined feature-attentive network (RFN) to solve the inaccurate localization problem. Specifically, we design a parallel feature integration mechanism to construct an adaptive feature representation from multi-resolution features, which enhances the perception of multi-scale texts at each scale-specific level to generate a high-quality attention map. Then, an attentive refinement network is developed by the attention map to rectify the location deviation of candidate boxes. In addition, a re-scoring mechanism is designed to select text boxes with the best rectified location. Moreover, we construct two industrial scene text datasets, including a total of 102156 images and 1948809 text instances with various character structures and metal parts. Extensive experiments on our dataset and four public datasets demonstrate that our proposed method achieves the state-of-the-art performance.



### Automatic Extraction of Road Networks from Satellite Images by using Adaptive Structural Deep Belief Network
- **Arxiv ID**: http://arxiv.org/abs/2110.12684v1
- **DOI**: 10.1109/ICIEVicIVPR52578.2021.9564155
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2110.12684v1)
- **Published**: 2021-10-25 07:06:10+00:00
- **Updated**: 2021-10-25 07:06:10+00:00
- **Authors**: Shin Kamada, Takumi Ichimura
- **Comment**: 7 pages, 9 figures, 2021 Joint 10th International Conference on
  Informatics, Electronics & Vision (ICIEV) and 2021 5th International
  Conference on Imaging, Vision & Pattern Recognition (icIVPR)
- **Journal**: None
- **Summary**: In our research, an adaptive structural learning method of Restricted Boltzmann Machine (RBM) and Deep Belief Network (DBN) has been developed as one of prominent deep learning models. The neuron generation-annihilation in RBM and layer generation algorithms in DBN make an optimal network structure for given input during the learning. In this paper, our model is applied to an automatic recognition method of road network system, called RoadTracer. RoadTracer can generate a road map on the ground surface from aerial photograph data. In the iterative search algorithm, a CNN is trained to find network graph connectivities between roads with high detection capability. However, the system takes a long calculation time for not only the training phase but also the inference phase, then it may not realize high accuracy. In order to improve the accuracy and the calculation time, our Adaptive DBN was implemented on the RoadTracer instead of the CNN. The performance of our developed model was evaluated on a satellite image in the suburban area, Japan. Our Adaptive DBN had an advantage of not only the detection accuracy but also the inference time compared with the conventional CNN in the experiment results.



### Self-Supervised Knowledge Transfer via Loosely Supervised Auxiliary Tasks
- **Arxiv ID**: http://arxiv.org/abs/2110.12696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12696v1)
- **Published**: 2021-10-25 07:18:26+00:00
- **Updated**: 2021-10-25 07:18:26+00:00
- **Authors**: Seungbum Hong, Jihun Yoon, Junmo Kim, Min-Kook Choi
- **Comment**: Accepted at WACV 2022
- **Journal**: None
- **Summary**: Knowledge transfer using convolutional neural networks (CNNs) can help efficiently train a CNN with fewer parameters or maximize the generalization performance under limited supervision. To enable a more efficient transfer of pretrained knowledge under relaxed conditions, we propose a simple yet powerful knowledge transfer methodology without any restrictions regarding the network structure or dataset used, namely self-supervised knowledge transfer (SSKT), via loosely supervised auxiliary tasks. For this, we devise a training methodology that transfers previously learned knowledge to the current training process as an auxiliary task for the target task through self-supervision using a soft label. The SSKT is independent of the network structure and dataset, and is trained differently from existing knowledge transfer methods; hence, it has an advantage in that the prior knowledge acquired from various tasks can be naturally transferred during the training process to the target task. Furthermore, it can improve the generalization performance on most datasets through the proposed knowledge transfer between different problem domains from multiple source networks. SSKT outperforms the other transfer learning methods (KD, DML, and MAXL) through experiments under various knowledge transfer settings. The source code will be made available to the public.



### An Embedded System for Image-based Crack Detection by using Fine-Tuning model of Adaptive Structural Learning of Deep Belief Network
- **Arxiv ID**: http://arxiv.org/abs/2110.13145v1
- **DOI**: 10.1109/TENCON50793.2020.9293705
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.13145v1)
- **Published**: 2021-10-25 07:28:50+00:00
- **Updated**: 2021-10-25 07:28:50+00:00
- **Authors**: Shin Kamada, Takumi Ichimura
- **Comment**: 6 pages, 4 figures, 2020 IEEE Region 10 Conference. arXiv admin note:
  substantial text overlap with arXiv:2110.12700
- **Journal**: None
- **Summary**: Deep learning has been a successful model which can effectively represent several features of input space and remarkably improve image recognition performance on the deep architectures. In our research, an adaptive structural learning method of Restricted Boltzmann Machine (Adaptive RBM) and Deep Belief Network (Adaptive DBN) have been developed as a deep learning model. The models have a self-organize function which can discover an optimal number of hidden neurons for given input data in a RBM by neuron generation-annihilation algorithm, and can obtain an appropriate number of RBM as hidden layers in the trained DBN. The proposed method was applied to a concrete image benchmark data set SDNET 2018 for crack detection. The dataset contains about 56,000 crack images for three types of concrete structures: bridge decks, walls, and paved roads. The fine-tuning method of the Adaptive DBN can show 99.7%, 99.7%, and 99.4% classification accuracy for test dataset of three types of structures. In this paper, our developed Adaptive DBN was embedded to a tiny PC with GPU for real-time inference on a drone. For fast inference, the fine tuning algorithm also removed some inactivated hidden neurons to make a small model and then the model was able to improve not only classification accuracy but also inference speed simultaneously. The inference speed and running time of portable battery charger were evaluated on three kinds of Nvidia embedded systems; Jetson Nano, AGX Xavier, and Xavier NX.



### An Adaptive Structural Learning of Deep Belief Network for Image-based Crack Detection in Concrete Structures Using SDNET2018
- **Arxiv ID**: http://arxiv.org/abs/2110.12700v1
- **DOI**: 10.1109/ICIP48927.2020.9367339
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2110.12700v1)
- **Published**: 2021-10-25 07:29:25+00:00
- **Updated**: 2021-10-25 07:29:25+00:00
- **Authors**: Shin Kamada, Takumi Ichimura, Takashi Iwasaki
- **Comment**: 6 pages, 10 figures, 2020 International Conference on Image
  Processing and Robotics (ICIP)
- **Journal**: None
- **Summary**: We have developed an adaptive structural Deep Belief Network (Adaptive DBN) that finds an optimal network structure in a self-organizing manner during learning. The Adaptive DBN is the hierarchical architecture where each layer employs Adaptive Restricted Boltzmann Machine (Adaptive RBM). The Adaptive RBM can find the appropriate number of hidden neurons during learning. The proposed method was applied to a concrete image benchmark data set SDNET2018 for crack detection. The dataset contains about 56,000 crack images for three types of concrete structures: bridge decks, walls, and paved roads. The fine-tuning method of the Adaptive DBN can show 99.7%, 99.7%, and 99.4% classification accuracy for three types of structures. However, we found the database included some wrong annotated data which cannot be judged from images by human experts. This paper discusses consideration that purses the major factor for the wrong cases and the removal of the adversarial examples from the dataset.



### SRT3D: A Sparse Region-Based 3D Object Tracking Approach for the Real World
- **Arxiv ID**: http://arxiv.org/abs/2110.12715v1
- **DOI**: 10.1007/s11263-022-01579-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12715v1)
- **Published**: 2021-10-25 07:58:18+00:00
- **Updated**: 2021-10-25 07:58:18+00:00
- **Authors**: Manuel Stoiber, Martin Pfanne, Klaus H. Strobl, Rudolph Triebel, Alin Albu-Schäffer
- **Comment**: Submitted to the International Journal of Computer Vision
- **Journal**: None
- **Summary**: Region-based methods have become increasingly popular for model-based, monocular 3D tracking of texture-less objects in cluttered scenes. However, while they achieve state-of-the-art results, most methods are computationally expensive, requiring significant resources to run in real-time. In the following, we build on our previous work and develop SRT3D, a sparse region-based approach to 3D object tracking that bridges this gap in efficiency. Our method considers image information sparsely along so-called correspondence lines that model the probability of the object's contour location. We thereby improve on the current state of the art and introduce smoothed step functions that consider a defined global and local uncertainty. For the resulting probabilistic formulation, a thorough analysis is provided. Finally, we use a pre-rendered sparse viewpoint model to create a joint posterior probability for the object pose. The function is maximized using second-order Newton optimization with Tikhonov regularization. During the pose estimation, we differentiate between global and local optimization, using a novel approximation for the first-order derivative employed in the Newton method. In multiple experiments, we demonstrate that the resulting algorithm improves the current state of the art both in terms of runtime and quality, performing particularly well for noisy and cluttered images encountered in the real world.



### A Distillation Learning Model of Adaptive Structural Deep Belief Network for AffectNet: Facial Expression Image Database
- **Arxiv ID**: http://arxiv.org/abs/2110.12717v1
- **DOI**: 10.1109/IIAI-AAI50415.2020.00096
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2110.12717v1)
- **Published**: 2021-10-25 08:01:36+00:00
- **Updated**: 2021-10-25 08:01:36+00:00
- **Authors**: Takumi Ichimura, Shin Kamada
- **Comment**: 6 pages, 8 figures, 020 9th International Congress on Advanced
  Applied Informatics (IIAI-AAI)
- **Journal**: None
- **Summary**: Deep Learning has a hierarchical network architecture to represent the complicated feature of input patterns. We have developed the adaptive structure learning method of Deep Belief Network (DBN) that can discover an optimal number of hidden neurons for given input data in a Restricted Boltzmann Machine (RBM) by neuron generation-annihilation algorithm, and can obtain the appropriate number of hidden layers in DBN. In this paper, our model is applied to a facial expression image data set, AffectNet. The system has higher classification capability than the traditional CNN. However, our model was not able to classify some test cases correctly because human emotions contain many ambiguous features or patterns leading wrong answer by two or more annotators who have different subjective judgment for a facial image. In order to represent such cases, this paper investigated a distillation learning model of Adaptive DBN. The original trained model can be seen as a parent model and some child models are trained for some mis-classified cases. For the difference between the parent model and the child one, KL divergence is monitored and then some appropriate new neurons at the parent model are generated according to KL divergence to improve classification accuracy. In this paper, the classification accuracy was improved from 78.4% to 91.3% by the proposed method.



### Instance-Conditional Knowledge Distillation for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.12724v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12724v2)
- **Published**: 2021-10-25 08:23:29+00:00
- **Updated**: 2022-01-06 13:21:29+00:00
- **Authors**: Zijian Kang, Peizhen Zhang, Xiangyu Zhang, Jian Sun, Nanning Zheng
- **Comment**: Accepted by NeurIPS 2021
- **Journal**: None
- **Summary**: Knowledge distillation has shown great success in classification, however, it is still challenging for detection. In a typical image for detection, representations from different locations may have different contributions to detection targets, making the distillation hard to balance. In this paper, we propose a conditional distillation framework to distill the desired knowledge, namely knowledge that is beneficial in terms of both classification and localization for every instance. The framework introduces a learnable conditional decoding module, which retrieves information given each target instance as query. Specifically, we encode the condition information as query and use the teacher's representations as key. The attention between query and key is used to measure the contribution of different features, guided by a localization-recognition-sensitive auxiliary task. Extensive experiments demonstrate the efficacy of our method: we observe impressive improvements under various settings. Notably, we boost RetinaNet with ResNet-50 backbone from 37.4 to 40.7 mAP (+3.3) under 1x schedule, that even surpasses the teacher (40.4 mAP) with ResNet-101 backbone under 3x schedule. Code has been released on https://github.com/megvii-research/ICD.



### Fast Gradient Non-sign Methods
- **Arxiv ID**: http://arxiv.org/abs/2110.12734v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12734v3)
- **Published**: 2021-10-25 08:46:00+00:00
- **Updated**: 2022-02-04 04:32:52+00:00
- **Authors**: Yaya Cheng, Jingkuan Song, Xiaosu Zhu, Qilong Zhang, Lianli Gao, Heng Tao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks make their success in DNNs, and among them, gradient-based algorithms become one of the mainstreams. Based on the linearity hypothesis, under $\ell_\infty$ constraint, $sign$ operation applied to the gradients is a good choice for generating perturbations. However, side-effects from such operation exist since it leads to the bias of direction between real gradients and perturbations. In other words, current methods contain a gap between real gradients and actual noises, which leads to biased and inefficient attacks. Therefore in this paper, based on the Taylor expansion, the bias is analyzed theoretically, and the correction of $sign$, i.e., Fast Gradient Non-sign Method (FGNM), is further proposed. Notably, FGNM is a general routine that seamlessly replaces the conventional $sign$ operation in gradient-based attacks with negligible extra computational cost. Extensive experiments demonstrate the effectiveness of our methods. Specifically, for untargeted black-box attacks, ours outperform them by 27.5% at most and 9.5% on average. For targeted attacks against defense models, it is 15.1% and 12.7%. Our anonymous code is publicly available at https://github.com/yaya-cheng/FGNM



### Practical Galaxy Morphology Tools from Deep Supervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.12735v2
- **DOI**: 10.1093/mnras/stac525
- **Categories**: **astro-ph.GA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12735v2)
- **Published**: 2021-10-25 08:46:16+00:00
- **Updated**: 2022-06-08 10:57:31+00:00
- **Authors**: Mike Walmsley, Anna M. M. Scaife, Chris Lintott, Michelle Lochner, Verlon Etsebeth, Tobias Géron, Hugh Dickinson, Lucy Fortson, Sandor Kruk, Karen L. Masters, Kameswara Bharadwaj Mantha, Brooke D. Simmons
- **Comment**: 20 pages plus appendix. Accepted to MNRAS (open-access DOI below).
  Code, documentation, pretrained models: https://github.com/mwalmsley/zoobot
  (PyTorch and TensorFlow)
- **Journal**: MNRAS Volume 513, Issue 2, June 2022, Pages 1581-1599
- **Summary**: Astronomers have typically set out to solve supervised machine learning problems by creating their own representations from scratch. We show that deep learning models trained to answer every Galaxy Zoo DECaLS question learn meaningful semantic representations of galaxies that are useful for new tasks on which the models were never trained. We exploit these representations to outperform several recent approaches at practical tasks crucial for investigating large galaxy samples. The first task is identifying galaxies of similar morphology to a query galaxy. Given a single galaxy assigned a free text tag by humans (e.g. "#diffuse"), we can find galaxies matching that tag for most tags. The second task is identifying the most interesting anomalies to a particular researcher. Our approach is 100% accurate at identifying the most interesting 100 anomalies (as judged by Galaxy Zoo 2 volunteers). The third task is adapting a model to solve a new task using only a small number of newly-labelled galaxies. Models fine-tuned from our representation are better able to identify ring galaxies than models fine-tuned from terrestrial images (ImageNet) or trained from scratch. We solve each task with very few new labels; either one (for the similarity search) or several hundred (for anomaly detection or fine-tuning). This challenges the longstanding view that deep supervised methods require new large labelled datasets for practical use in astronomy. To help the community benefit from our pretrained models, we release our fine-tuning code Zoobot. Zoobot is accessible to researchers with no prior experience in deep learning.



### LAE : Long-tailed Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.12741v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12741v1)
- **Published**: 2021-10-25 09:05:44+00:00
- **Updated**: 2021-10-25 09:05:44+00:00
- **Authors**: Zenghao Bao, Zichang Tan, Yu Zhu, Jun Wan, Xibo Ma, Zhen Lei, Guodong Guo
- **Comment**: The 1st Place in Guess The Age Contest, CAIP2021 (The 19th
  International Conference on Computer Analysis of Images and Patterns)
- **Journal**: None
- **Summary**: Facial age estimation is an important yet very challenging problem in computer vision. To improve the performance of facial age estimation, we first formulate a simple standard baseline and build a much strong one by collecting the tricks in pre-training, data augmentation, model architecture, and so on. Compared with the standard baseline, the proposed one significantly decreases the estimation errors. Moreover, long-tailed recognition has been an important topic in facial age datasets, where the samples often lack on the elderly and children. To train a balanced age estimator, we propose a two-stage training method named Long-tailed Age Estimation (LAE), which decouples the learning procedure into representation learning and classification. The effectiveness of our approach has been demonstrated on the dataset provided by organizers of Guess The Age Contest 2021.



### Highly Efficient Natural Image Matting
- **Arxiv ID**: http://arxiv.org/abs/2110.12748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12748v1)
- **Published**: 2021-10-25 09:23:46+00:00
- **Updated**: 2021-10-25 09:23:46+00:00
- **Authors**: Yijie Zhong, Bo Li, Lv Tang, Hao Tang, Shouhong Ding
- **Comment**: Full version of BMVC2021
- **Journal**: None
- **Summary**: Over the last few years, deep learning based approaches have achieved outstanding improvements in natural image matting. However, there are still two drawbacks that impede the widespread application of image matting: the reliance on user-provided trimaps and the heavy model sizes. In this paper, we propose a trimap-free natural image matting method with a lightweight model. With a lightweight basic convolution block, we build a two-stages framework: Segmentation Network (SN) is designed to capture sufficient semantics and classify the pixels into unknown, foreground and background regions; Matting Refine Network (MRN) aims at capturing detailed texture information and regressing accurate alpha values. With the proposed cross-level fusion Module (CFM), SN can efficiently utilize multi-scale features with less computational cost. Efficient non-local attention module (ENA) in MRN can efficiently model the relevance between different pixels and help regress high-quality alpha values. Utilizing these techniques, we construct an extremely light-weighted model, which achieves comparable performance with ~1\% parameters (344k) of large models on popular natural image matting benchmarks.



### Multi-scale Iterative Residuals for Fast and Scalable Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2110.12769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12769v1)
- **Published**: 2021-10-25 09:54:17+00:00
- **Updated**: 2021-10-25 09:54:17+00:00
- **Authors**: Kumail Raza, René Schuster, Didier Stricker
- **Comment**: Accepted at CSCS21
- **Journal**: None
- **Summary**: Despite the remarkable progress of deep learning in stereo matching, there exists a gap in accuracy between real-time models and slower state-of-the-art models which are suitable for practical applications. This paper presents an iterative multi-scale coarse-to-fine refinement (iCFR) framework to bridge this gap by allowing it to adopt any stereo matching network to make it fast, more efficient and scalable while keeping comparable accuracy. To reduce the computational cost of matching, we use multi-scale warped features to estimate disparity residuals and push the disparity search range in the cost volume to a minimum limit. Finally, we apply a refinement network to recover the loss of precision which is inherent in multi-scale approaches. We test our iCFR framework by adopting the matching networks from state-of-the art GANet and AANet. The result is 49$\times$ faster inference time compared to GANetdeep and 4$\times$ less memory consumption, with comparable error. Our best performing network, which we call FRSNet is scalable even up to an input resolution of 6K on a GTX 1080Ti, with inference time still below one second and comparable accuracy to AANet+. It out-performs all real-time stereo methods and achieves competitive accuracy on the KITTI benchmark.



### Domain Adaptation in Multi-View Embedding for Cross-Modal Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2110.12812v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12812v1)
- **Published**: 2021-10-25 11:22:49+00:00
- **Updated**: 2021-10-25 11:22:49+00:00
- **Authors**: Jonathan Munro, Michael Wray, Diane Larlus, Gabriela Csurka, Dima Damen
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Given a gallery of uncaptioned video sequences, this paper considers the task of retrieving videos based on their relevance to an unseen text query. To compensate for the lack of annotations, we rely instead on a related video gallery composed of video-caption pairs, termed the source gallery, albeit with a domain gap between its videos and those in the target gallery. We thus introduce the problem of Unsupervised Domain Adaptation for Cross-modal Video Retrieval, along with a new benchmark on fine-grained actions. We propose a novel iterative domain alignment method by means of pseudo-labelling target videos and cross-domain (i.e. source-target) ranking. Our approach adapts the embedding space to the target gallery, consistently outperforming source-only as well as marginal and conditional alignment methods.



### Restore from Restored: Single-image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2110.12822v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.12822v1)
- **Published**: 2021-10-25 11:38:51+00:00
- **Updated**: 2021-10-25 11:38:51+00:00
- **Authors**: Eunhye Lee, Jeongmu Kim, Jisu Kim, Tae Hyun Kim
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2102.08078
- **Journal**: None
- **Summary**: Recent image inpainting methods have shown promising results due to the power of deep learning, which can explore external information available from the large training dataset. However, many state-of-the-art inpainting networks are still limited in exploiting internal information available in the given input image at test time. To mitigate this problem, we present a novel and efficient self-supervised fine-tuning algorithm that can adapt the parameters of fully pre-trained inpainting networks without using ground-truth target images. We update the parameters of the pre-trained state-of-the-art inpainting networks by utilizing existing self-similar patches (i.e., self-exemplars) within the given input image without changing the network architecture and improve the inpainting quality by a large margin. Qualitative and quantitative experimental results demonstrate the superiority of the proposed algorithm, and we achieve state-of-the-art inpainting results on publicly available benchmark datasets.



### Raw Bayer Pattern Image Synthesis for Computer Vision-oriented Image Signal Processing Pipeline Design
- **Arxiv ID**: http://arxiv.org/abs/2110.12823v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12823v2)
- **Published**: 2021-10-25 11:40:36+00:00
- **Updated**: 2021-12-15 14:19:17+00:00
- **Authors**: Wei Zhou, Xiangyu Zhang, Hongyu Wang, Shenghua Gao, Xin Lou
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In this paper, we propose a method to add constraints that are un-formulatable in generative adversarial networks (GAN)-based arbitrary size RAW Bayer image generation. It is shown theoretically that by using the transformed data in GAN training, it is able to improve the learning of the original data distribution, owing to the invariant of Jensen-Shannon (JS) divergence between two distributions under invertible and differentiable transformation. Benefiting from the proposed method, RAW Bayer pattern images can be generated by configuring the transformation as demosaicing. It is shown that by adding another transformation, the proposed method is able to synthesize high-quality RAW Bayer images with arbitrary size. Experimental results show that images generated by the proposed method outperform the existing methods in the Fr\'echet inception distance (FID) score, peak signal to noise ratio (PSNR), and mean structural similarity (MSSIM), and the training process is more stable. To the best knowledge of the authors, there is no open-source, large-scale image dataset in the RAW Bayer domain, which is crucial for research works aiming to explore the image signal processing (ISP) pipeline design for computer vision tasks. Converting the existing commonly used color image datasets to their corresponding RAW Bayer versions, the proposed method can be a promising solution to the RAW image dataset problem. We also show in the experiments that, by training object detection frameworks using the synthesized RAW Bayer images, they can be used in an end-to-end manner (from RAW images to vision tasks) with negligible performance degradation.



### TPSNet: Reverse Thinking of Thin Plate Splines for Arbitrary Shape Scene Text Representation
- **Arxiv ID**: http://arxiv.org/abs/2110.12826v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12826v2)
- **Published**: 2021-10-25 11:47:17+00:00
- **Updated**: 2022-09-11 16:40:39+00:00
- **Authors**: Wei Wang, Yu Zhou, Jiahao Lv, Dayan Wu, Guoqing Zhao, Ning Jiang, Weiping Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The research focus of scene text detection and recognition has shifted to arbitrary shape text in recent years, where the text shape representation is a fundamental problem. An ideal representation should be compact, complete, efficient, and reusable for subsequent recognition in our opinion. However, previous representations have flaws in one or more aspects. Thin-Plate-Spline (TPS) transformation has achieved great success in scene text recognition. Inspired by this, we reversely think of its usage and sophisticatedly take TPS as an exquisite representation for arbitrary shape text representation. The TPS representation is compact, complete, and efficient. With the predicted TPS parameters, the detected text region can be directly rectified to a near-horizontal one to assist the subsequent recognition. To further exploit the potential of the TPS representation, the Border Alignment Loss is proposed. Based on these designs, we implement the text detector TPSNet, which can be extended to a text spotter conveniently. Extensive evaluation and ablation of several public benchmarks demonstrate the effectiveness and superiority of the proposed method for text representation and spotting. Particularly, TPSNet achieves the detection F-Measure improvement of 4.4\% (78.4\% vs. 74.0\%) on Art dataset and the end-to-end spotting F-Measure improvement of 5.0\% (78.5\% vs. 73.5\%) on Total-Text, which are large margins with no bells and whistles.



### Novel coronavirus pneumonia lesion segmentation in CT images
- **Arxiv ID**: http://arxiv.org/abs/2110.12827v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12827v3)
- **Published**: 2021-10-25 11:49:20+00:00
- **Updated**: 2021-12-23 03:31:41+00:00
- **Authors**: Yuanyuan Peng, Zixu Zhang, Hongbin Tu, Xiong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Background: The 2019 novel coronavirus disease (COVID-19) has been spread widely in the world, causing a huge threat to people's living environment. Objective: Under computed tomography (CT) imaging, the structure features of COVID-19 lesions are complicated and varied greatly in different cases. To accurately locate COVID-19 lesions and assist doctors to make the best diagnosis and treatment plan, a deep-supervised ensemble learning network is presented for COVID-19 lesion segmentation in CT images. Methods: Considering the fact that a large number of COVID-19 CT images and the corresponding lesion annotations are difficult to obtained, a transfer learning strategy is employed to make up for the shortcoming and alleviate the overfitting problem. Based on the reality that traditional single deep learning framework is difficult to extract COVID-19 lesion features effectively, which may cause some lesions to be undetected. To overcome the problem, a deep-supervised ensemble learning network is presented to combine with local and global features for COVID-19 lesion segmentation. Results: The performance of the proposed method was validated in experiments with a publicly available dataset. Compared with manual annotations, the proposed method acquired a high intersection over union (IoU) of 0.7279. Conclusion: A deep-supervised ensemble learning network was presented for coronavirus pneumonia lesion segmentation in CT images. The effectiveness of the proposed method was verified by visual inspection and quantitative evaluation. Experimental results shown that the proposed mehtod has a perfect performance in COVID-19 lesion segmentation.



### Reconstructing Pruned Filters using Cheap Spatial Transformations
- **Arxiv ID**: http://arxiv.org/abs/2110.12844v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12844v3)
- **Published**: 2021-10-25 12:13:45+00:00
- **Updated**: 2023-08-24 11:11:55+00:00
- **Authors**: Roy Miles, Krystian Mikolajczyk
- **Comment**: ICCV 2023 Workshop on Resource Efficient Deep Learning for Computer
  Vision
- **Journal**: None
- **Summary**: We present an efficient alternative to the convolutional layer using cheap spatial transformations. This construction exploits an inherent spatial redundancy of the learned convolutional filters to enable a much greater parameter efficiency, while maintaining the top-end accuracy of their dense counter-parts. Training these networks is modelled as a generalised pruning problem, whereby the pruned filters are replaced with cheap transformations from the set of non-pruned filters. We provide an efficient implementation of the proposed layer, followed by two natural extensions to avoid excessive feature compression and to improve the expressivity of the transformed features. We show that these networks can achieve comparable or improved performance to state-of-the-art pruning models across both the CIFAR-10 and ImageNet-1K datasets.



### Anatomical and Diagnostic Bayesian Segmentation in Prostate MRI $-$Should Different Clinical Objectives Mandate Different Loss Functions?
- **Arxiv ID**: http://arxiv.org/abs/2110.12889v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12889v1)
- **Published**: 2021-10-25 12:44:52+00:00
- **Updated**: 2021-10-25 12:44:52+00:00
- **Authors**: Anindo Saha, Joeran Bosma, Jasper Linmans, Matin Hosseinzadeh, Henkjan Huisman
- **Comment**: Accepted to Medical Imaging Meets NeurIPS Workshop of the 35th
  Conference on Neural Information Processing Systems (NeurIPS 2021)
- **Journal**: None
- **Summary**: We hypothesize that probabilistic voxel-level classification of anatomy and malignancy in prostate MRI, although typically posed as near-identical segmentation tasks via U-Nets, require different loss functions for optimal performance due to inherent differences in their clinical objectives. We investigate distribution, region and boundary-based loss functions for both tasks across 200 patient exams from the publicly-available ProstateX dataset. For evaluation, we conduct a thorough comparative analysis of model predictions and calibration, measured with respect to multi-class volume segmentation of the prostate anatomy (whole-gland, transitional zone, peripheral zone), as well as, patient-level diagnosis and lesion-level detection of clinically significant prostate cancer. Notably, we find that distribution-based loss functions (in particular, focal loss) are well-suited for diagnostic or panoptic segmentation tasks such as lesion detection, primarily due to their implicit property of inducing better calibration. Meanwhile, (with the exception of focal loss) both distribution and region/boundary-based loss functions perform equally well for anatomical or semantic segmentation tasks, such as quantification of organ shape, size and boundaries.



### The Efficiency Misnomer
- **Arxiv ID**: http://arxiv.org/abs/2110.12894v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.12894v2)
- **Published**: 2021-10-25 12:48:07+00:00
- **Updated**: 2022-03-16 13:27:44+00:00
- **Authors**: Mostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish Vaswani, Yi Tay
- **Comment**: None
- **Journal**: None
- **Summary**: Model efficiency is a critical aspect of developing and deploying machine learning models. Inference time and latency directly affect the user experience, and some applications have hard requirements. In addition to inference costs, model training also have direct financial and environmental impacts. Although there are numerous well-established metrics (cost indicators) for measuring model efficiency, researchers and practitioners often assume that these metrics are correlated with each other and report only few of them. In this paper, we thoroughly discuss common cost indicators, their advantages and disadvantages, and how they can contradict each other. We demonstrate how incomplete reporting of cost indicators can lead to partial conclusions and a blurred or incomplete picture of the practical considerations of different models. We further present suggestions to improve reporting of efficiency metrics.



### SILT: Self-supervised Lighting Transfer Using Implicit Image Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2110.12914v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.12914v2)
- **Published**: 2021-10-25 12:52:53+00:00
- **Updated**: 2022-03-15 12:29:42+00:00
- **Authors**: Nikolina Kubiak, Armin Mustafa, Graeme Phillipson, Stephen Jolly, Simon Hadfield
- **Comment**: Accepted to BMVC 2021. The code and pre-trained models can be found
  at https://github.com/n-kubiak/SILT
- **Journal**: None
- **Summary**: We present SILT, a Self-supervised Implicit Lighting Transfer method. Unlike previous research on scene relighting, we do not seek to apply arbitrary new lighting configurations to a given scene. Instead, we wish to transfer the lighting style from a database of other scenes, to provide a uniform lighting style regardless of the input. The solution operates as a two-branch network that first aims to map input images of any arbitrary lighting style to a unified domain, with extra guidance achieved through implicit image decomposition. We then remap this unified input domain using a discriminator that is presented with the generated outputs and the style reference, i.e. images of the desired illumination conditions. Our method is shown to outperform supervised relighting solutions across two different datasets without requiring lighting supervision.



### Revealing unforeseen diagnostic image features with deep learning by detecting cardiovascular diseases from apical four-chamber ultrasounds
- **Arxiv ID**: http://arxiv.org/abs/2110.12915v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2110.12915v1)
- **Published**: 2021-10-25 12:53:45+00:00
- **Updated**: 2021-10-25 12:53:45+00:00
- **Authors**: Li-Hsin Cheng, Pablo B. J. Bosch, Rutger F. H. Hofman, Timo B. Brakenhoff, Eline F. Bruggemans, Rob J. van der Geest, Eduard R. Holman
- **Comment**: None
- **Journal**: None
- **Summary**: Background. With the rise of highly portable, wireless, and low-cost ultrasound devices and automatic ultrasound acquisition techniques, an automated interpretation method requiring only a limited set of views as input could make preliminary cardiovascular disease diagnoses more accessible. In this study, we developed a deep learning (DL) method for automated detection of impaired left ventricular (LV) function and aortic valve (AV) regurgitation from apical four-chamber (A4C) ultrasound cineloops and investigated which anatomical structures or temporal frames provided the most relevant information for the DL model to enable disease classification.   Methods and Results. A4C ultrasounds were extracted from 3,554 echocardiograms of patients with either impaired LV function (n=928), AV regurgitation (n=738), or no significant abnormalities (n=1,888). Two convolutional neural networks (CNNs) were trained separately to classify the respective disease cases against normal cases. The overall classification accuracy of the impaired LV function detection model was 86%, and that of the AV regurgitation detection model was 83%. Feature importance analyses demonstrated that the LV myocardium and mitral valve were important for detecting impaired LV function, while the tip of the mitral valve anterior leaflet, during opening, was considered important for detecting AV regurgitation.   Conclusion. The proposed method demonstrated the feasibility of a 3D CNN approach in detection of impaired LV function and AV regurgitation using A4C ultrasound cineloops. The current research shows that DL methods can exploit large training data to detect diseases in a different way than conventionally agreed upon methods, and potentially reveal unforeseen diagnostic image features.



### Interactive Segmentation via Deep Learning and B-Spline Explicit Active Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2110.12939v1
- **DOI**: 10.1007/978-3-030-87193-2_30
- **Categories**: **eess.IV**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2110.12939v1)
- **Published**: 2021-10-25 13:17:53+00:00
- **Updated**: 2021-10-25 13:17:53+00:00
- **Authors**: Helena Williams, João Pedrosa, Laura Cattani, Susanne Housmans, Tom Vercauteren, Jan Deprest, Jan D'hooge
- **Comment**: 11 pages, 3 figures, 2 tables
- **Journal**: MICCAI 2021. Lecture Notes in Computer Science, vol 12901.
  Springer, Cham
- **Summary**: Automatic medical image segmentation via convolutional neural networks (CNNs) has shown promising results. However, they may not always be robust enough for clinical use. Sub-optimal segmentation would require clinician's to manually delineate the target object, causing frustration. To address this problem, a novel interactive CNN-based segmentation framework is proposed in this work. The aim is to represent the CNN segmentation contour as B-splines by utilising B-spline explicit active surfaces (BEAS). The interactive element of the framework allows the user to precisely edit the contour in real-time, and by utilising BEAS it ensures the final contour is smooth and anatomically plausible. This framework was applied to the task of 2D segmentation of the levator hiatus from 2D ultrasound (US) images, and compared to the current clinical tools used in pelvic floor disorder clinic (4DView, GE Healthcare; Zipf, Austria). Experimental results show that: 1) the proposed framework is more robust than current state-of-the-art CNNs; 2) the perceived workload calculated via the NASA-TLX index was reduced more than half for the proposed approach in comparison to current clinical tools; and 3) the proposed tool requires at least 13 seconds less user time than the clinical tools, which was significant (p=0.001).



### DocTr: Document Image Transformer for Geometric Unwarping and Illumination Correction
- **Arxiv ID**: http://arxiv.org/abs/2110.12942v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12942v2)
- **Published**: 2021-10-25 13:27:10+00:00
- **Updated**: 2022-10-08 06:29:24+00:00
- **Authors**: Hao Feng, Yuechen Wang, Wengang Zhou, Jiajun Deng, Houqiang Li
- **Comment**: This paper has been accepted by ACM Multimedia 2021
- **Journal**: None
- **Summary**: In this work, we propose a new framework, called Document Image Transformer (DocTr), to address the issue of geometry and illumination distortion of the document images. Specifically, DocTr consists of a geometric unwarping transformer and an illumination correction transformer. By setting a set of learned query embedding, the geometric unwarping transformer captures the global context of the document image by self-attention mechanism and decodes the pixel-wise displacement solution to correct the geometric distortion. After geometric unwarping, our illumination correction transformer further removes the shading artifacts to improve the visual quality and OCR accuracy. Extensive evaluations are conducted on several datasets, and superior results are reported against the state-of-the-art methods. Remarkably, our DocTr achieves 20.02% Character Error Rate (CER), a 15% absolute improvement over the state-of-the-art methods. Moreover, it also shows high efficiency on running time and parameter count. The results will be available at https://github.com/fh2019ustc/DocTr for further comparison.



### Seeing biodiversity: perspectives in machine learning for wildlife conservation
- **Arxiv ID**: http://arxiv.org/abs/2110.12951v1
- **DOI**: 10.1038/s41467-022-27980-y
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12951v1)
- **Published**: 2021-10-25 13:40:36+00:00
- **Updated**: 2021-10-25 13:40:36+00:00
- **Authors**: Devis Tuia, Benjamin Kellenberger, Sara Beery, Blair R. Costelloe, Silvia Zuffi, Benjamin Risse, Alexander Mathis, Mackenzie W. Mathis, Frank van Langevelde, Tilo Burghardt, Roland Kays, Holger Klinck, Martin Wikelski, Iain D. Couzin, Grant van Horn, Margaret C. Crofoot, Charles V. Stewart, Tanya Berger-Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: Data acquisition in animal ecology is rapidly accelerating due to inexpensive and accessible sensors such as smartphones, drones, satellites, audio recorders and bio-logging devices. These new technologies and the data they generate hold great potential for large-scale environmental monitoring and understanding, but are limited by current data processing approaches which are inefficient in how they ingest, digest, and distill data into relevant information. We argue that machine learning, and especially deep learning approaches, can meet this analytic challenge to enhance our understanding, monitoring capacity, and conservation of wildlife species. Incorporating machine learning into ecological workflows could improve inputs for population and behavior models and eventually lead to integrated hybrid modeling tools, with ecological models acting as constraints for machine learning models and the latter providing data-supported insights. In essence, by combining new machine learning approaches with ecological domain knowledge, animal ecologists can capitalize on the abundance of data generated by modern sensor technologies in order to reliably estimate population abundances, study animal behavior and mitigate human/wildlife conflicts. To succeed, this approach will require close collaboration and cross-disciplinary education between the computer science and animal ecology communities in order to ensure the quality of machine learning approaches and train a new generation of data scientists in ecology and conservation.



### Event Data Association via Robust Model Fitting for Event-based Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2110.12962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12962v1)
- **Published**: 2021-10-25 13:56:00+00:00
- **Updated**: 2021-10-25 13:56:00+00:00
- **Authors**: Haosheng Chen, Shuyuan Lin, David Suter, Yan Yan, Hanzi Wang
- **Comment**: 30 pages, 7 figures
- **Journal**: None
- **Summary**: Event-based approaches, which are based on bio-inspired asynchronous event cameras, have achieved promising performance on various computer vision tasks. However, the study of the fundamental event data association problem is still in its infancy. In this paper, we propose a novel Event Data Association approach (called EDA) to explicitly address the data association problem. The proposed EDA seeks for event trajectories that best fit the event data, in order to perform unifying data association. In EDA, we first asynchronously gather the event data, based on its information entropy. Then, we introduce a deterministic model hypothesis generation strategy, which effectively generates model hypotheses from the gathered events, to represent the corresponding event trajectories. After that, we present a two-stage weighting algorithm, which robustly weighs and selects true models from the generated model hypotheses, through multi-structural geometric model fitting. Meanwhile, we also propose an adaptive model selection strategy to automatically determine the number of the true models. Finally, we use the selected true models to associate the event data, without being affected by sensor noise and irrelevant structures. We evaluate the performance of the proposed EDA on the object tracking task. The experimental results show the effectiveness of EDA under challenging scenarios, such as high speed, motion blur, and high dynamic range conditions.



### Shape and Reflectance Reconstruction in Uncontrolled Environments by Differentiable Rendering
- **Arxiv ID**: http://arxiv.org/abs/2110.12975v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12975v2)
- **Published**: 2021-10-25 14:09:10+00:00
- **Updated**: 2022-02-28 09:01:54+00:00
- **Authors**: Rui Li, Guangmin Zang, Miao Qi, Wolfgang Heidrich
- **Comment**: None
- **Journal**: None
- **Summary**: Simultaneous reconstruction of geometry and reflectance properties in uncontrolled environments remains a challenging problem. In this paper, we propose an efficient method to reconstruct the scene's 3D geometry and reflectance from multi-view photography using conventional hand-held cameras. Our method automatically builds a virtual scene in a differentiable rendering system that roughly matches the real world's scene parameters, optimized by minimizing photometric objectives alternatingly and stochastically. With the optimal scene parameters evaluated, photo-realistic novel views for various viewing angles and distances can then be generated by our approach. We present the results of captured scenes with complex geometry and various reflection types. Our method also shows superior performance compared to state-of-the-art alternatives in novel view synthesis visually and quantitatively.



### Stable Neural ODE with Lyapunov-Stable Equilibrium Points for Defending Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2110.12976v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12976v1)
- **Published**: 2021-10-25 14:09:45+00:00
- **Updated**: 2021-10-25 14:09:45+00:00
- **Authors**: Qiyu Kang, Yang Song, Qinxu Ding, Wee Peng Tay
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are well-known to be vulnerable to adversarial attacks, where malicious human-imperceptible perturbations are included in the input to the deep network to fool it into making a wrong classification. Recent studies have demonstrated that neural Ordinary Differential Equations (ODEs) are intrinsically more robust against adversarial attacks compared to vanilla DNNs. In this work, we propose a stable neural ODE with Lyapunov-stable equilibrium points for defending against adversarial attacks (SODEF). By ensuring that the equilibrium points of the ODE solution used as part of SODEF is Lyapunov-stable, the ODE solution for an input with a small perturbation converges to the same solution as the unperturbed input. We provide theoretical results that give insights into the stability of SODEF as well as the choice of regularizers to ensure its stability. Our analysis suggests that our proposed regularizers force the extracted feature points to be within a neighborhood of the Lyapunov-stable equilibrium points of the ODE. SODEF is compatible with many defense methods and can be applied to any neural network's final regressor layer to enhance its stability against adversarial attacks.



### MoDeRNN: Towards Fine-grained Motion Details for Spatiotemporal Predictive Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.12978v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12978v2)
- **Published**: 2021-10-25 14:12:17+00:00
- **Updated**: 2022-02-12 05:55:43+00:00
- **Authors**: Zenghao Chai, Zhengzhuo Xu, Chun Yuan
- **Comment**: Accepted at ICASSP 2022
- **Journal**: None
- **Summary**: Spatiotemporal predictive learning (ST-PL) aims at predicting the subsequent frames via limited observed sequences, and it has broad applications in the real world. However, learning representative spatiotemporal features for prediction is challenging. Moreover, chaotic uncertainty among consecutive frames exacerbates the difficulty in long-term prediction. This paper concentrates on improving prediction quality by enhancing the correspondence between the previous context and the current state. We carefully design Detail Context Block (DCB) to extract fine-grained details and improve the isolated correlation between upper context state and current input state. We integrate DCB with standard ConvLSTM and introduce Motion Details RNN (MoDeRNN) to capture fine-grained spatiotemporal features and improve the expression of latent states of RNNs to achieve significant quality. Experiments on Moving MNIST and Typhoon datasets demonstrate the effectiveness of the proposed method. MoDeRNN outperforms existing state-of-the-art techniques qualitatively and quantitatively with lower computation loads.



### Stochastic Rounding for Image Interpolation and Scan Conversion
- **Arxiv ID**: http://arxiv.org/abs/2110.12983v2
- **DOI**: 10.14569/IJACSA.2022.0130303
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12983v2)
- **Published**: 2021-10-25 14:15:50+00:00
- **Updated**: 2022-03-31 17:04:44+00:00
- **Authors**: Olivier Rukundo, Samuel Emil Schmidt
- **Comment**: 10 pages, 17 figures, 3 tables. International Journal of Advanced
  Computer Science and Applications, 2022
- **Journal**: None
- **Summary**: The stochastic rounding (SR) function is proposed to evaluate and demonstrate the effects of stochastically rounding row and column subscripts in image interpolation and scan conversion. The proposed SR function is based on a pseudorandom number, enabling the pseudorandom rounding up or down any non-integer row and column subscripts. Also, the SR function exceptionally enables rounding up any possible cases of subscript inputs that are inferior to a pseudorandom number. The algorithm of interest is the nearest-neighbor interpolation (NNI) which is traditionally based on the deterministic rounding (DR) function. Experimental simulation results are provided to demonstrate the performance of NNI-SR and NNI-DR algorithms before and after applying smoothing and sharpening filters of interest. Additional results are also provided to demonstrate the performance of NNI-SR and NNI-DR interpolated scan conversion algorithms in cardiac ultrasound videos.



### Generative Residual Attention Network for Disease Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.12984v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12984v1)
- **Published**: 2021-10-25 14:15:57+00:00
- **Updated**: 2021-10-25 14:15:57+00:00
- **Authors**: Euyoung Kim, Soochahn Lee, Kyoung Mu Lee
- **Comment**: The paper is about Pneumonia detection using Generative Modeling. It
  proposes a novel approach to construct pseudo-pair images and a GAN to
  generate radio-realistic Chest Xray images. Then, the paper propose to
  leverage the differences between the input and the generated Xray images as
  an additional attention-map to boost the performance in Pneumonia detection
- **Journal**: None
- **Summary**: Accurate identification and localization of abnormalities from radiology images serve as a critical role in computer-aided diagnosis (CAD) systems. Building a highly generalizable system usually requires a large amount of data with high-quality annotations, including disease-specific global and localization information. However, in medical images, only a limited number of high-quality images and annotations are available due to annotation expenses. In this paper, we explore this problem by presenting a novel approach for disease generation in X-rays using a conditional generative adversarial learning. Specifically, given a chest X-ray image from a source domain, we generate a corresponding radiology image in a target domain while preserving the identity of the patient. We then use the generated X-ray image in the target domain to augment our training to improve the detection performance. We also present a unified framework that simultaneously performs disease generation and localization.We evaluate the proposed approach on the X-ray image dataset provided by the Radiological Society of North America (RSNA), surpassing the state-of-the-art baseline detection algorithms.



### Neural Relightable Participating Media Rendering
- **Arxiv ID**: http://arxiv.org/abs/2110.12993v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12993v1)
- **Published**: 2021-10-25 14:36:15+00:00
- **Updated**: 2021-10-25 14:36:15+00:00
- **Authors**: Quan Zheng, Gurprit Singh, Hans-Peter Seidel
- **Comment**: Accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: Learning neural radiance fields of a scene has recently allowed realistic novel view synthesis of the scene, but they are limited to synthesize images under the original fixed lighting condition. Therefore, they are not flexible for the eagerly desired tasks like relighting, scene editing and scene composition. To tackle this problem, several recent methods propose to disentangle reflectance and illumination from the radiance field. These methods can cope with solid objects with opaque surfaces but participating media are neglected. Also, they take into account only direct illumination or at most one-bounce indirect illumination, thus suffer from energy loss due to ignoring the high-order indirect illumination. We propose to learn neural representations for participating media with a complete simulation of global illumination. We estimate direct illumination via ray tracing and compute indirect illumination with spherical harmonics. Our approach avoids computing the lengthy indirect bounces and does not suffer from energy loss. Our experiments on multiple scenes show that our approach achieves superior visual quality and numerical performance compared to state-of-the-art methods, and it can generalize to deal with solid objects with opaque surfaces as well.



### Logsig-RNN: a novel network for robust and efficient skeleton-based action recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.13008v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.13008v2)
- **Published**: 2021-10-25 14:47:15+00:00
- **Updated**: 2021-11-01 17:54:17+00:00
- **Authors**: Shujian Liao, Terry Lyons, Weixin Yang, Kevin Schlegel, Hao Ni
- **Comment**: This paper is accepted by British Machine Vision Conference 2021
- **Journal**: None
- **Summary**: This paper contributes to the challenge of skeleton-based human action recognition in videos. The key step is to develop a generic network architecture to extract discriminative features for the spatio-temporal skeleton data. In this paper, we propose a novel module, namely Logsig-RNN, which is the combination of the log-signature layer and recurrent type neural networks (RNNs). The former one comes from the mathematically principled technology of signatures and log-signatures as representations for streamed data, which can manage high sample rate streams, non-uniform sampling and time series of variable length. It serves as an enhancement of the recurrent layer, which can be conveniently plugged into neural networks. Besides we propose two path transformation layers to significantly reduce path dimension while retaining the essential information fed into the Logsig-RNN module. Finally, numerical results demonstrate that replacing the RNN module by the Logsig-RNN module in SOTA networks consistently improves the performance on both Chalearn gesture data and NTU RGB+D 120 action data in terms of accuracy and robustness. In particular, we achieve the state-of-the-art accuracy on Chalearn2013 gesture data by combining simple path transformation layers with the Logsig-RNN. Codes are available at https://github.com/steveliao93/GCN_LogsigRNN.



### TAPL: Dynamic Part-based Visual Tracking via Attention-guided Part Localization
- **Arxiv ID**: http://arxiv.org/abs/2110.13027v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.13027v1)
- **Published**: 2021-10-25 15:05:43+00:00
- **Updated**: 2021-10-25 15:05:43+00:00
- **Authors**: Wei han, Hantao Huang, Xiaoxi Yu
- **Comment**: Accepted by BMVC2021
- **Journal**: None
- **Summary**: Holistic object representation-based trackers suffer from performance drop under large appearance change such as deformation and occlusion. In this work, we propose a dynamic part-based tracker and constantly update the target part representation to adapt to object appearance change. Moreover, we design an attention-guided part localization network to directly predict the target part locations, and determine the final bounding box with the distribution of target parts. Our proposed tracker achieves promising results on various benchmarks: VOT2018, OTB100 and GOT-10k



### Dual Skip Connections Minimize the False Positive Rate of Lung Nodule Detection in CT images
- **Arxiv ID**: http://arxiv.org/abs/2110.13036v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.13036v1)
- **Published**: 2021-10-25 15:19:59+00:00
- **Updated**: 2021-10-25 15:19:59+00:00
- **Authors**: Jiahua Xu, Philipp Ernst, Tung Lung Liu, Andreas Nürnberger
- **Comment**: to be published at IEEE EMBC 2021, in IEEE Xplore
- **Journal**: None
- **Summary**: Pulmonary cancer is one of the most commonly diagnosed and fatal cancers and is often diagnosed by incidental findings on computed tomography. Automated pulmonary nodule detection is an essential part of computer-aided diagnosis, which is still facing great challenges and difficulties to quickly and accurately locate the exact nodules' positions. This paper proposes a dual skip connection upsampling strategy based on Dual Path network in a U-Net structure generating multiscale feature maps, which aims to minimize the ratio of false positives and maximize the sensitivity for lesion detection of nodules. The results show that our new upsampling strategy improves the performance by having 85.3% sensitivity at 4 FROC per image compared to 84.2% for the regular upsampling strategy or 81.2% for VGG16-based Faster-R-CNN.



### Detecting speaking persons in video
- **Arxiv ID**: http://arxiv.org/abs/2110.13806v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.13806v1)
- **Published**: 2021-10-25 15:51:35+00:00
- **Updated**: 2021-10-25 15:51:35+00:00
- **Authors**: Hannes Fassold
- **Comment**: Accepted for MMSP 2021
- **Journal**: None
- **Summary**: We present a novel method for detecting speaking persons in video, by extracting facial landmarks with a neural network and analysing these landmarks statistically over time



### Some like it tough: Improving model generalization via progressively increasing the training difficulty
- **Arxiv ID**: http://arxiv.org/abs/2110.13058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.13058v1)
- **Published**: 2021-10-25 15:54:21+00:00
- **Updated**: 2021-10-25 15:54:21+00:00
- **Authors**: Hannes Fassold
- **Comment**: Accepted for ASPAI 2021 conference
- **Journal**: None
- **Summary**: In this work, we propose to progressively increase the training difficulty during learning a neural network model via a novel strategy which we call mini-batch trimming. This strategy makes sure that the optimizer puts its focus in the later training stages on the more difficult samples, which we identify as the ones with the highest loss in the current mini-batch. The strategy is very easy to integrate into an existing training pipeline and does not necessitate a change of the network model. Experiments on several image classification problems show that mini-batch trimming is able to increase the generalization ability (measured via final test error) of the trained model.



### Exploiting Redundancy: Separable Group Convolutional Networks on Lie Groups
- **Arxiv ID**: http://arxiv.org/abs/2110.13059v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.13059v2)
- **Published**: 2021-10-25 15:56:53+00:00
- **Updated**: 2022-04-04 13:32:53+00:00
- **Authors**: David M. Knigge, David W. Romero, Erik J. Bekkers
- **Comment**: None
- **Journal**: None
- **Summary**: Group convolutional neural networks (G-CNNs) have been shown to increase parameter efficiency and model accuracy by incorporating geometric inductive biases. In this work, we investigate the properties of representations learned by regular G-CNNs, and show considerable parameter redundancy in group convolution kernels. This finding motivates further weight-tying by sharing convolution kernels over subgroups. To this end, we introduce convolution kernels that are separable over the subgroup and channel dimensions. In order to obtain equivariance to arbitrary affine Lie groups we provide a continuous parameterisation of separable convolution kernels. We evaluate our approach across several vision datasets, and show that our weight sharing leads to improved performance and computational efficiency. In many settings, separable G-CNNs outperform their non-separable counterpart, while only using a fraction of their training time. In addition, thanks to the increase in computational efficiency, we are able to implement G-CNNs equivariant to the $\mathrm{Sim(2)}$ group; the group of dilations, rotations and translations. $\mathrm{Sim(2)}$-equivariance further improves performance on all tasks considered.



### 2nd Place Solution for SODA10M Challenge 2021 -- Continual Detection Track
- **Arxiv ID**: http://arxiv.org/abs/2110.13064v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.13064v1)
- **Published**: 2021-10-25 15:58:19+00:00
- **Updated**: 2021-10-25 15:58:19+00:00
- **Authors**: Manoj Acharya, Christopher Kanan
- **Comment**: Published in SSLAD workshop at ICCV 2021
- **Journal**: None
- **Summary**: In this technical report, we present our approaches for the continual object detection track of the SODA10M challenge. We adapt ResNet50-FPN as the baseline and try several improvements for the final submission model. We find that task-specific replay scheme, learning rate scheduling, model calibration, and using original image scale helps to improve performance for both large and small objects in images. Our team `hypertune28' secured the second position among 52 participants in the challenge. This work will be presented at the ICCV 2021 Workshop on Self-supervised Learning for Next-Generation Industry-level Autonomous Driving (SSLAD).



### AutoMTL: A Programming Framework for Automating Efficient Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.13076v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.13076v3)
- **Published**: 2021-10-25 16:13:39+00:00
- **Updated**: 2022-09-29 20:11:56+00:00
- **Authors**: Lijun Zhang, Xiao Liu, Hui Guan
- **Comment**: Accepted by NeurIPS 2022
- **Journal**: None
- **Summary**: Multi-task learning (MTL) jointly learns a set of tasks by sharing parameters among tasks. It is a promising approach for reducing storage costs while improving task accuracy for many computer vision tasks. The effective adoption of MTL faces two main challenges. The first challenge is to determine what parameters to share across tasks to optimize for both memory efficiency and task accuracy. The second challenge is to automatically apply MTL algorithms to an arbitrary CNN backbone without requiring time-consuming manual re-implementation and significant domain expertise. This paper addresses the challenges by developing the first programming framework AutoMTL that automates efficient MTL model development for vision tasks. AutoMTL takes as inputs an arbitrary backbone convolutional neural network (CNN) and a set of tasks to learn, and automatically produces a multi-task model that achieves high accuracy and small memory footprint simultaneously. Experiments on three popular MTL benchmarks (CityScapes, NYUv2, Tiny-Taskonomy) demonstrate the effectiveness of AutoMTL over state-of-the-art approaches as well as the generalizability of AutoMTL across CNNs. AutoMTL is open-sourced and available at https://github.com/zhanglijun95/AutoMTL.



### MVT: Multi-view Vision Transformer for 3D Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.13083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.13083v1)
- **Published**: 2021-10-25 16:23:25+00:00
- **Updated**: 2021-10-25 16:23:25+00:00
- **Authors**: Shuo Chen, Tan Yu, Ping Li
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Inspired by the great success achieved by CNN in image recognition, view-based methods applied CNNs to model the projected views for 3D object understanding and achieved excellent performance. Nevertheless, multi-view CNN models cannot model the communications between patches from different views, limiting its effectiveness in 3D object recognition. Inspired by the recent success gained by vision Transformer in image recognition, we propose a Multi-view Vision Transformer (MVT) for 3D object recognition. Since each patch feature in a Transformer block has a global reception field, it naturally achieves communications between patches from different views. Meanwhile, it takes much less inductive bias compared with its CNN counterparts. Considering both effectiveness and efficiency, we develop a global-local structure for our MVT. Our experiments on two public benchmarks, ModelNet40 and ModelNet10, demonstrate the competitive performance of our MVT.



### As if by magic: self-supervised training of deep despeckling networks with MERLIN
- **Arxiv ID**: http://arxiv.org/abs/2110.13148v2
- **DOI**: 10.1109/TGRS.2021.3128621
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.13148v2)
- **Published**: 2021-10-25 16:30:09+00:00
- **Updated**: 2021-11-15 14:49:52+00:00
- **Authors**: Emanuele Dalsasso, Loïc Denis, Florence Tupin
- **Comment**: To appear on IEEE Transactions on Geoscience and Remote Sensing
- **Journal**: None
- **Summary**: Speckle fluctuations seriously limit the interpretability of synthetic aperture radar (SAR) images. Speckle reduction has thus been the subject of numerous works spanning at least four decades. Techniques based on deep neural networks have recently achieved a new level of performance in terms of SAR image restoration quality. Beyond the design of suitable network architectures or the selection of adequate loss functions, the construction of training sets is of uttermost importance. So far, most approaches have considered a supervised training strategy: the networks are trained to produce outputs as close as possible to speckle-free reference images. Speckle-free images are generally not available, which requires resorting to natural or optical images or the selection of stable areas in long time series to circumvent the lack of ground truth. Self-supervision, on the other hand, avoids the use of speckle-free images. We introduce a self-supervised strategy based on the separation of the real and imaginary parts of single-look complex SAR images, called MERLIN (coMplex sElf-supeRvised despeckLINg), and show that it offers a straightforward way to train all kinds of deep despeckling networks. Networks trained with MERLIN take into account the spatial correlations due to the SAR transfer function specific to a given sensor and imaging mode. By requiring only a single image, and possibly exploiting large archives, MERLIN opens the door to hassle-free as well as large-scale training of despeckling networks. The code of the trained models is made freely available at https://gitlab.telecom-paris.fr/RING/MERLIN.



### Rotation Equivariant Deforestation Segmentation and Driver Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.13097v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.13097v2)
- **Published**: 2021-10-25 16:49:46+00:00
- **Updated**: 2021-12-16 11:14:57+00:00
- **Authors**: Joshua Mitton, Roderick Murray-Smith
- **Comment**: Tackling Climate Change with Machine Learning workshop at NeurIPS
  2021
- **Journal**: None
- **Summary**: Deforestation has become a significant contributing factor to climate change and, due to this, both classifying the drivers and predicting segmentation maps of deforestation has attracted significant interest. In this work, we develop a rotation equivariant convolutional neural network model to predict the drivers and generate segmentation maps of deforestation events from Landsat 8 satellite images. This outperforms previous methods in classifying the drivers and predicting the segmentation map of deforestation, offering a 9% improvement in classification accuracy and a 7% improvement in segmentation map accuracy. In addition, this method predicts stable segmentation maps under rotation of the input image, which ensures that predicted regions of deforestation are not dependent upon the rotational orientation of the satellite.



### Parameter Prediction for Unseen Deep Architectures
- **Arxiv ID**: http://arxiv.org/abs/2110.13100v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.13100v1)
- **Published**: 2021-10-25 16:52:33+00:00
- **Updated**: 2021-10-25 16:52:33+00:00
- **Authors**: Boris Knyazev, Michal Drozdzal, Graham W. Taylor, Adriana Romero-Soriano
- **Comment**: NeurIPS 2021 camera ready, the code is available at
  https://github.com/facebookresearch/ppuda
- **Journal**: None
- **Summary**: Deep learning has been successful in automating the design of features in machine learning pipelines. However, the algorithms optimizing neural network parameters remain largely hand-designed and computationally inefficient. We study if we can use deep learning to directly predict these parameters by exploiting the past knowledge of training other networks. We introduce a large-scale dataset of diverse computational graphs of neural architectures - DeepNets-1M - and use it to explore parameter prediction on CIFAR-10 and ImageNet. By leveraging advances in graph neural networks, we propose a hypernetwork that can predict performant parameters in a single forward pass taking a fraction of a second, even on a CPU. The proposed model achieves surprisingly good performance on unseen and diverse networks. For example, it is able to predict all 24 million parameters of a ResNet-50 achieving a 60% accuracy on CIFAR-10. On ImageNet, top-5 accuracy of some of our networks approaches 50%. Our task along with the model and results can potentially lead to a new, more computationally efficient paradigm of training networks. Our model also learns a strong representation of neural architectures enabling their analysis.



### Latent-Insensitive autoencoders for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.13101v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.13101v2)
- **Published**: 2021-10-25 16:53:49+00:00
- **Updated**: 2021-11-14 15:24:35+00:00
- **Authors**: Muhammad S. Battikh, Artem A. Lenskiy
- **Comment**: 19 pages
- **Journal**: None
- **Summary**: Reconstruction-based approaches to anomaly detection tend to fall short when applied to complex datasets with target classes that possess high inter-class variance. Similar to the idea of self-taught learning used in transfer learning, many domains are rich with similar unlabelled datasets that could be leveraged as a proxy for out-of-distribution samples. In this paper we introduce Latent-Insensitive autoencoder (LIS-AE) where unlabeled data from a similar domain is utilized as negative examples to shape the latent layer (bottleneck) of a regular autoencoder such that it is only capable of reconstructing one task. We provide theoretical justification for the proposed training process and loss functions along with an extensive ablation study highlighting important aspects of our model. We test our model in multiple anomaly detection settings presenting quantitative and qualitative analysis showcasing the significant performance improvement of our model for anomaly detection tasks.



### The Nuts and Bolts of Adopting Transformer in GANs
- **Arxiv ID**: http://arxiv.org/abs/2110.13107v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.13107v3)
- **Published**: 2021-10-25 17:01:29+00:00
- **Updated**: 2023-06-13 15:07:15+00:00
- **Authors**: Rui Xu, Xiangyu Xu, Kai Chen, Bolei Zhou, Chen Change Loy
- **Comment**: CVPR2023 Workshop AI4CC. Project Page:
  https://nbei.github.io/stransgan.html
- **Journal**: None
- **Summary**: Transformer becomes prevalent in computer vision, especially for high-level vision tasks. However, adopting Transformer in the generative adversarial network (GAN) framework is still an open yet challenging problem. In this paper, we conduct a comprehensive empirical study to investigate the properties of Transformer in GAN for high-fidelity image synthesis. Our analysis highlights and reaffirms the importance of feature locality in image generation, although the merits of the locality are well known in the classification task. Perhaps more interestingly, we find the residual connections in self-attention layers harmful for learning Transformer-based discriminators and conditional generators. We carefully examine the influence and propose effective ways to mitigate the negative impacts. Our study leads to a new alternative design of Transformers in GAN, a convolutional neural network (CNN)-free generator termed as STrans-G, which achieves competitive results in both unconditional and conditional image generations. The Transformer-based discriminator, STrans-D, also significantly reduces its gap against the CNN-based discriminators.



### Diagnosing Errors in Video Relation Detectors
- **Arxiv ID**: http://arxiv.org/abs/2110.13110v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.13110v1)
- **Published**: 2021-10-25 17:04:08+00:00
- **Updated**: 2021-10-25 17:04:08+00:00
- **Authors**: Shuo Chen, Pascal Mettes, Cees G. M. Snoek
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Video relation detection forms a new and challenging problem in computer vision, where subjects and objects need to be localized spatio-temporally and a predicate label needs to be assigned if and only if there is an interaction between the two. Despite recent progress in video relation detection, overall performance is still marginal and it remains unclear what the key factors are towards solving the problem. Following examples set in the object detection and action localization literature, we perform a deep dive into the error diagnosis of current video relation detection approaches. We introduce a diagnostic tool for analyzing the sources of detection errors. Our tool evaluates and compares current approaches beyond the single scalar metric of mean Average Precision by defining different error types specific to video relation detection, used for false positive analyses. Moreover, we examine different factors of influence on the performance in a false negative analysis, including relation length, number of subject/object/predicate instances, and subject/object size. Finally, we present the effect on video relation performance when considering an oracle fix for each error type. On two video relation benchmarks, we show where current approaches excel and fall short, allowing us to pinpoint the most important future directions in the field. The tool is available at \url{https://github.com/shanshuo/DiagnoseVRD}.



### Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots
- **Arxiv ID**: http://arxiv.org/abs/2111.01674v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.01674v1)
- **Published**: 2021-10-25 17:59:58+00:00
- **Updated**: 2021-10-25 17:59:58+00:00
- **Authors**: Zipeng Fu, Ashish Kumar, Jitendra Malik, Deepak Pathak
- **Comment**: CoRL 2021. Website at https://energy-locomotion.github.io
- **Journal**: None
- **Summary**: Legged locomotion is commonly studied and expressed as a discrete set of gait patterns, like walk, trot, gallop, which are usually treated as given and pre-programmed in legged robots for efficient locomotion at different speeds. However, fixing a set of pre-programmed gaits limits the generality of locomotion. Recent animal motor studies show that these conventional gaits are only prevalent in ideal flat terrain conditions while real-world locomotion is unstructured and more like bouts of intermittent steps. What principles could lead to both structured and unstructured patterns across mammals and how to synthesize them in robots? In this work, we take an analysis-by-synthesis approach and learn to move by minimizing mechanical energy. We demonstrate that learning to minimize energy consumption plays a key role in the emergence of natural locomotion gaits at different speeds in real quadruped robots. The emergent gaits are structured in ideal terrains and look similar to that of horses and sheep. The same approach leads to unstructured gaits in rough terrains which is consistent with the findings in animal motor control. We validate our hypothesis in both simulation and real hardware across natural terrains. Videos at https://energy-locomotion.github.io



### Self-supervised similarity search for large scientific datasets
- **Arxiv ID**: http://arxiv.org/abs/2110.13151v2
- **DOI**: None
- **Categories**: **astro-ph.IM**, astro-ph.GA, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.13151v2)
- **Published**: 2021-10-25 18:00:00+00:00
- **Updated**: 2021-11-30 19:01:18+00:00
- **Authors**: George Stein, Peter Harrington, Jacqueline Blaum, Tomislav Medan, Zarija Lukic
- **Comment**: 5 pages, 2 figures. The similarity search web app can be found at
  https://github.com/georgestein/galaxy_search. Accepted to the Fourth Workshop
  on Machine Learning and the Physical Sciences (NeurIPS 2021). ArXiv admin
  note: text overlap with arXiv:2110.00023
- **Journal**: None
- **Summary**: We present the use of self-supervised learning to explore and exploit large unlabeled datasets. Focusing on 42 million galaxy images from the latest data release of the Dark Energy Spectroscopic Instrument (DESI) Legacy Imaging Surveys, we first train a self-supervised model to distill low-dimensional representations that are robust to symmetries, uncertainties, and noise in each image. We then use the representations to construct and publicly release an interactive semantic similarity search tool. We demonstrate how our tool can be used to rapidly discover rare objects given only a single example, increase the speed of crowd-sourcing campaigns, and construct and improve training sets for supervised applications. While we focus on images from sky surveys, the technique is straightforward to apply to any scientific dataset of any dimensionality. The similarity search web app can be found at https://github.com/georgestein/galaxy_search



### Generalized Multi-Task Learning from Substantially Unlabeled Multi-Source Medical Image Data
- **Arxiv ID**: http://arxiv.org/abs/2110.13185v1
- **DOI**: 10.59275/j.melba.2021-d8a3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.13185v1)
- **Published**: 2021-10-25 18:09:19+00:00
- **Updated**: 2021-10-25 18:09:19+00:00
- **Authors**: Ayaan Haque, Abdullah-Al-Zubaer Imran, Adam Wang, Demetri Terzopoulos
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://www.melba-journal.org/
- **Journal**: None
- **Summary**: Deep learning-based models, when trained in a fully-supervised manner, can be effective in performing complex image analysis tasks, although contingent upon the availability of large labeled datasets. Especially in the medical imaging domain, however, expert image annotation is expensive, time-consuming, and prone to variability. Semi-supervised learning from limited quantities of labeled data has shown promise as an alternative. Maximizing knowledge gains from copious unlabeled data benefits semi-supervised learning models. Moreover, learning multiple tasks within the same model further improves its generalizability. We propose MultiMix, a new multi-task learning model that jointly learns disease classification and anatomical segmentation in a semi-supervised manner, while preserving explainability through a novel saliency bridge between the two tasks. Our experiments with varying quantities of multi-source labeled data in the training sets confirm the effectiveness of MultiMix in the simultaneous classification of pneumonia and segmentation of the lungs in chest X-ray images. Moreover, both in-domain and cross-domain evaluations across these tasks further showcase the potential of our model to adapt to challenging generalization scenarios.



### Simultaneous Perturbation Method for Multi-Task Weight Optimization in One-Shot Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.13188v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.13188v3)
- **Published**: 2021-10-25 18:11:49+00:00
- **Updated**: 2022-10-02 19:39:39+00:00
- **Authors**: Andrei Boiarov, Kostiantyn Khabarlak, Igor Yastrebov
- **Comment**: Accepted at ICONIP 2022
- **Journal**: None
- **Summary**: Meta-learning methods aim to build learning algorithms capable of quickly adapting to new tasks in low-data regime. One of the most difficult benchmarks of such algorithms is a one-shot learning problem. In this setting many algorithms face uncertainties associated with limited amount of training samples, which may result in overfitting. This problem can be resolved by providing additional information to the model. One of the most efficient ways to do this is multi-task learning. In this paper we investigate the modification of a standard meta-learning pipeline. The proposed method simultaneously utilizes information from several meta-training tasks in a common loss function. The impact of these tasks in the loss function is controlled by a per task weight. Proper optimization of the weights can have big influence on training and the final quality of the model. We propose and investigate the use of methods from the family of Simultaneous Perturbation Stochastic Approximation (SPSA) for optimization of meta-train tasks weights. We also demonstrate superiority of stochastic approximation in comparison to gradient-based method. The proposed Multi-Task Modification can be applied to almost all meta-learning methods. We study applications of this modification on Model-Agnostic Meta-Learning and Prototypical Network algorithms on CIFAR-FS, FC100, miniImageNet and tieredImageNet one-shot learning benchmarks. During these experiments Multi-Task Modification has demonstrated improvement over original methods. SPSA-Tracking algorithm first adapted in this paper for multi-task weight optimization shows the largest accuracy boost that is competitive to the state-of-the-art meta-learning methods. Our code is available online.



### Spectral unmixing of Raman microscopic images of single human cells using Independent Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/2110.13189v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.13189v1)
- **Published**: 2021-10-25 18:13:24+00:00
- **Updated**: 2021-10-25 18:13:24+00:00
- **Authors**: M. Hamed Mozaffari, Li-Lin Tay
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Application of independent component analysis (ICA) as an unmixing and image clustering technique for high spatial resolution Raman maps is reported. A hyperspectral map of a fixed human cell was collected by a Raman micro spectrometer in a raster pattern on a 0.5um grid. Unlike previously used unsupervised machine learning techniques such as principal component analysis, ICA is based on non-Gaussianity and statistical independence of data which is the case for mixture Raman spectra. Hence, ICA is a great candidate for assembling pseudo-colour maps from the spectral hypercube of Raman spectra. Our experimental results revealed that ICA is capable of reconstructing false colour maps of Raman hyperspectral data of human cells, showing the nuclear region constituents as well as subcellular organelle in the cytoplasm and distribution of mitochondria in the perinuclear region. Minimum preprocessing requirements and label-free nature of the ICA method make it a great unmixed method for extraction of endmembers in Raman hyperspectral maps of living cells.



### IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2110.13214v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.13214v4)
- **Published**: 2021-10-25 18:52:26+00:00
- **Updated**: 2022-07-25 04:05:29+00:00
- **Authors**: Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, Song-Chun Zhu
- **Comment**: Corrected typos. Accepted to NeurIPS 2021, 27 pages, 18 figures. Data
  and code are available at https://iconqa.github.io
- **Journal**: None
- **Summary**: Current visual question answering (VQA) tasks mainly consider answering human-annotated questions for natural images. However, aside from natural images, abstract diagrams with semantic richness are still understudied in visual understanding and reasoning research. In this work, we introduce a new challenge of Icon Question Answering (IconQA) with the goal of answering a question in an icon image context. We release IconQA, a large-scale dataset that consists of 107,439 questions and three sub-tasks: multi-image-choice, multi-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by real-world diagram word problems that highlight the importance of abstract diagram understanding and comprehensive cognitive reasoning. Thus, IconQA requires not only perception skills like object recognition and text understanding, but also diverse cognitive reasoning skills, such as geometric reasoning, commonsense reasoning, and arithmetic reasoning. To facilitate potential IconQA models to learn semantic representations for icon images, we further release an icon dataset Icon645 which contains 645,687 colored icons on 377 classes. We conduct extensive user studies and blind experiments and reproduce a wide range of advanced VQA methods to benchmark the IconQA task. Also, we develop a strong IconQA baseline Patch-TRM that applies a pyramid cross-modal Transformer with input diagram embeddings pre-trained on the icon dataset. IconQA and Icon645 are available at https://iconqa.github.io.



### RBSRICNN: Raw Burst Super-Resolution through Iterative Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2110.13217v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.13217v2)
- **Published**: 2021-10-25 19:01:28+00:00
- **Updated**: 2021-11-10 12:07:30+00:00
- **Authors**: Rao Muhammad Umer, Christian Micheloni
- **Comment**: Fourth Workshop on Machine Learning and the Physical Sciences
  (NeurIPS 2021)
- **Journal**: None
- **Summary**: Modern digital cameras and smartphones mostly rely on image signal processing (ISP) pipelines to produce realistic colored RGB images. However, compared to DSLR cameras, low-quality images are usually obtained in many portable mobile devices with compact camera sensors due to their physical limitations. The low-quality images have multiple degradations i.e., sub-pixel shift due to camera motion, mosaick patterns due to camera color filter array, low-resolution due to smaller camera sensors, and the rest information are corrupted by the noise. Such degradations limit the performance of current Single Image Super-resolution (SISR) methods in recovering high-resolution (HR) image details from a single low-resolution (LR) image. In this work, we propose a Raw Burst Super-Resolution Iterative Convolutional Neural Network (RBSRICNN) that follows the burst photography pipeline as a whole by a forward (physical) model. The proposed Burst SR scheme solves the problem with classical image regularization, convex optimization, and deep learning techniques, compared to existing black-box data-driven methods. The proposed network produces the final output by an iterative refinement of the intermediate SR estimates. We demonstrate the effectiveness of our proposed approach in quantitative and qualitative experiments that generalize robustly to real LR burst inputs with onl synthetic burst data available for training.



### Identifying and Benchmarking Natural Out-of-Context Prediction Problems
- **Arxiv ID**: http://arxiv.org/abs/2110.13223v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.13223v1)
- **Published**: 2021-10-25 19:15:47+00:00
- **Updated**: 2021-10-25 19:15:47+00:00
- **Authors**: David Madras, Richard Zemel
- **Comment**: Accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: Deep learning systems frequently fail at out-of-context (OOC) prediction, the problem of making reliable predictions on uncommon or unusual inputs or subgroups of the training distribution. To this end, a number of benchmarks for measuring OOC performance have recently been introduced. In this work, we introduce a framework unifying the literature on OOC performance measurement, and demonstrate how rich auxiliary information can be leveraged to identify candidate sets of OOC examples in existing datasets. We present NOOCh: a suite of naturally-occurring "challenge sets", and show how varying notions of context can be used to probe specific OOC failure modes. Experimentally, we explore the tradeoffs between various learning approaches on these challenge sets and demonstrate how the choices made in designing OOC benchmarks can yield varying conclusions.



### Pediatric Otoscopy Video Screening with Shift Contrastive Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.13254v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.13254v1)
- **Published**: 2021-10-25 20:39:28+00:00
- **Updated**: 2021-10-25 20:39:28+00:00
- **Authors**: Weiyao Wang, Aniruddha Tamhane, Christine Santos, John R. Rzasa, James H. Clark, Therese L. Canares, Mathias Unberath
- **Comment**: None
- **Journal**: None
- **Summary**: Ear related concerns and symptoms represents the leading indication for seeking pediatric healthcare attention. Despite the high incidence of such encounters, the diagnostic process of commonly encountered disease of the middle and external presents significant challenge. Much of this challenge stems from the lack of cost effective diagnostic testing, which necessitating the presence or absence of ear pathology to be determined clinically. Research has however demonstrated considerable variation among clinicians in their ability to accurately diagnose and consequently manage ear pathology. With recent advances in computer vision and machine learning, there is an increasing interest in helping clinicians to accurately diagnose middle and external ear pathology with computer-aided systems. It has been shown that AI has the capacity to analyse a single clinical image captured during examination of the ear canal and eardrum from which it can determine the likelihood of a pathognomonic pattern for a specific diagnosis being present. The capture of such an image can however be challenging especially to inexperienced clinicians. To help mitigate this technical challenge we have developed and tested a method using video sequences. We present a two stage method that first, identifies valid frames by detecting and extracting ear drum patches from the video sequence, and second, performs the proposed shift contrastive anomaly detection to flag the otoscopy video sequences as normal or abnormal. Our method achieves an AUROC of 88.0% on the patient-level and also outperforms the average of a group of 25 clinicians in a comparative study, which is the largest of such published to date. We conclude that the presented method achieves a promising first step towards automated analysis of otoscopy video.



### Image Quality Assessment using Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.13266v1
- **DOI**: 10.1109/TIP.2022.3181496
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.13266v1)
- **Published**: 2021-10-25 21:01:00+00:00
- **Updated**: 2021-10-25 21:01:00+00:00
- **Authors**: Pavan C. Madhusudana, Neil Birkbeck, Yilin Wang, Balu Adsumilli, Alan C. Bovik
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing. 31 (2022) 4149 - 4161
- **Summary**: We consider the problem of obtaining image quality representations in a self-supervised manner. We use prediction of distortion type and degree as an auxiliary task to learn features from an unlabeled image dataset containing a mixture of synthetic and realistic distortions. We then train a deep Convolutional Neural Network (CNN) using a contrastive pairwise objective to solve the auxiliary problem. We refer to the proposed training framework and resulting deep IQA model as the CONTRastive Image QUality Evaluator (CONTRIQUE). During evaluation, the CNN weights are frozen and a linear regressor maps the learned representations to quality scores in a No-Reference (NR) setting. We show through extensive experiments that CONTRIQUE achieves competitive performance when compared to state-of-the-art NR image quality models, even without any additional fine-tuning of the CNN backbone. The learned representations are highly robust and generalize well across images afflicted by either synthetic or authentic distortions. Our results suggest that powerful quality representations with perceptual relevance can be obtained without requiring large labeled subjective image quality datasets. The implementations used in this paper are available at \url{https://github.com/pavancm/CONTRIQUE}.



### Facial Recognition in Collaborative Learning Videos
- **Arxiv ID**: http://arxiv.org/abs/2110.13269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.13269v1)
- **Published**: 2021-10-25 21:05:06+00:00
- **Updated**: 2021-10-25 21:05:06+00:00
- **Authors**: Phuong Tran, Marios Pattichis, Sylvia Celedón-Pattichis, Carlos LópezLeiva
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition in collaborative learning videos presents many challenges. In collaborative learning videos, students sit around a typical table at different positions to the recording camera, come and go, move around, get partially or fully occluded. Furthermore, the videos tend to be very long, requiring the development of fast and accurate methods. We develop a dynamic system of recognizing participants in collaborative learning systems. We address occlusion and recognition failures by using past information about the face detection history. We address the need for detecting faces from different poses and the need for speed by associating each participant with a collection of prototype faces computed through sampling or K-means clustering. Our results show that the proposed system is proven to be very fast and accurate. We also compare our system against a baseline system that uses InsightFace [2] and the original training video segments. We achieved an average accuracy of 86.2% compared to 70.8% for the baseline system. On average, our recognition rate was 28.1 times faster than the baseline system.



### Learning Neural Transmittance for Efficient Rendering of Reflectance Fields
- **Arxiv ID**: http://arxiv.org/abs/2110.13272v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.13272v1)
- **Published**: 2021-10-25 21:12:25+00:00
- **Updated**: 2021-10-25 21:12:25+00:00
- **Authors**: Mohammad Shafiei, Sai Bi, Zhengqin Li, Aidas Liaudanskas, Rodrigo Ortiz-Cayon, Ravi Ramamoorthi
- **Comment**: None
- **Journal**: None
- **Summary**: Recently neural volumetric representations such as neural reflectance fields have been widely applied to faithfully reproduce the appearance of real-world objects and scenes under novel viewpoints and lighting conditions. However, it remains challenging and time-consuming to render such representations under complex lighting such as environment maps, which requires individual ray marching towards each single light to calculate the transmittance at every sampled point. In this paper, we propose a novel method based on precomputed Neural Transmittance Functions to accelerate the rendering of neural reflectance fields. Our neural transmittance functions enable us to efficiently query the transmittance at an arbitrary point in space along an arbitrary ray without tedious ray marching, which effectively reduces the time-complexity of the rendering. We propose a novel formulation for the neural transmittance function, and train it jointly with the neural reflectance fields on images captured under collocated camera and light, while enforcing monotonicity. Results on real and synthetic scenes demonstrate almost two order of magnitude speedup for renderings under environment maps with minimal accuracy loss.



### A Variational Graph Autoencoder for Manipulation Action Recognition and Prediction
- **Arxiv ID**: http://arxiv.org/abs/2110.13280v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.13280v1)
- **Published**: 2021-10-25 21:40:42+00:00
- **Updated**: 2021-10-25 21:40:42+00:00
- **Authors**: Gamze Akyol, Sanem Sariel, Eren Erdal Aksoy
- **Comment**: Accepted for publication in the Proceedings of 2021 20th
  International Conference on Advanced Robotics (ICAR)
- **Journal**: None
- **Summary**: Despite decades of research, understanding human manipulation activities is, and has always been, one of the most attractive and challenging research topics in computer vision and robotics. Recognition and prediction of observed human manipulation actions have their roots in the applications related to, for instance, human-robot interaction and robot learning from demonstration. The current research trend heavily relies on advanced convolutional neural networks to process the structured Euclidean data, such as RGB camera images. These networks, however, come with immense computational complexity to be able to process high dimensional raw data.   Different from the related works, we here introduce a deep graph autoencoder to jointly learn recognition and prediction of manipulation tasks from symbolic scene graphs, instead of relying on the structured Euclidean data. Our network has a variational autoencoder structure with two branches: one for identifying the input graph type and one for predicting the future graphs. The input of the proposed network is a set of semantic graphs which store the spatial relations between subjects and objects in the scene. The network output is a label set representing the detected and predicted class types. We benchmark our new model against different state-of-the-art methods on two different datasets, MANIAC and MSRC-9, and show that our proposed model can achieve better performance. We also release our source code https://github.com/gamzeakyol/GNet.



### Generative Flows as a General Purpose Solution for Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/2110.13285v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.13285v3)
- **Published**: 2021-10-25 21:56:44+00:00
- **Updated**: 2022-05-27 03:15:16+00:00
- **Authors**: José A. Chávez
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: Due to the success of generative flows to model data distributions, they have been explored in inverse problems. Given a pre-trained generative flow, previous work proposed to minimize the 2-norm of the latent variables as a regularization term. The intuition behind it was to ensure high likelihood latent variables that produce the closest restoration. However, high-likelihood latent variables may generate unrealistic samples as we show in our experiments. We therefore propose a solver to directly produce high-likelihood reconstructions. We hypothesize that our approach could make generative flows a general purpose solver for inverse problems. Furthermore, we propose 1 x 1 coupling functions to introduce permutations in a generative flow. It has the advantage that its inverse does not require to be calculated in the generation process. Finally, we evaluate our method for denoising, deblurring, inpainting, and colorization. We observe a compelling improvement of our method over prior works.



### Uncertainty quantification in non-rigid image registration via stochastic gradient Markov chain Monte Carlo
- **Arxiv ID**: http://arxiv.org/abs/2110.13289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.13289v1)
- **Published**: 2021-10-25 22:05:20+00:00
- **Updated**: 2021-10-25 22:05:20+00:00
- **Authors**: Daniel Grzech, Mohammad Farid Azampour, Huaqi Qiu, Ben Glocker, Bernhard Kainz, Loïc Le Folgoc
- **Comment**: MELBA Special Issue: Uncertainty for Safe Utilization of Machine
  Learning in Medical Imaging (UNSURE) 2020
- **Journal**: None
- **Summary**: We develop a new Bayesian model for non-rigid registration of three-dimensional medical images, with a focus on uncertainty quantification. Probabilistic registration of large images with calibrated uncertainty estimates is difficult for both computational and modelling reasons. To address the computational issues, we explore connections between the Markov chain Monte Carlo by backpropagation and the variational inference by backpropagation frameworks, in order to efficiently draw samples from the posterior distribution of transformation parameters. To address the modelling issues, we formulate a Bayesian model for image registration that overcomes the existing barriers when using a dense, high-dimensional, and diffeomorphic transformation parametrisation. This results in improved calibration of uncertainty estimates. We compare the model in terms of both image registration accuracy and uncertainty quantification to VoxelMorph, a state-of-the-art image registration model based on deep learning.



### History Aware Multimodal Transformer for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2110.13309v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.13309v2)
- **Published**: 2021-10-25 22:54:41+00:00
- **Updated**: 2023-08-17 22:42:07+00:00
- **Authors**: Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, Ivan Laptev
- **Comment**: Accepted in NeurIPS 2021; project page at
  https://cshizhe.github.io/projects/vln_hamt.html; corrected a typo
- **Journal**: None
- **Summary**: Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy. HAMT achieves new state of the art on a broad range of VLN tasks, including VLN with fine-grained instructions (R2R, RxR), high-level instructions (R2R-Last, REVERIE), dialogs (CVDN) as well as long-horizon VLN (R4R, R2R-Back). We demonstrate HAMT to be particularly effective for navigation tasks with longer trajectories.



