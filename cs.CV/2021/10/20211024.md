# Arxiv Papers in cs.CV on 2021-10-24
### CoVA: Context-aware Visual Attention for Webpage Information Extraction
- **Arxiv ID**: http://arxiv.org/abs/2110.12320v1
- **DOI**: 10.18653/v1/2022.ecnlp-1.11
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.HC, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2110.12320v1)
- **Published**: 2021-10-24 00:21:46+00:00
- **Updated**: 2021-10-24 00:21:46+00:00
- **Authors**: Anurendra Kumar, Keval Morabia, Jingjin Wang, Kevin Chen-Chuan Chang, Alexander Schwing
- **Comment**: 11 Pages, 6 Figures, 3 Tables
- **Journal**: None
- **Summary**: Webpage information extraction (WIE) is an important step to create knowledge bases. For this, classical WIE methods leverage the Document Object Model (DOM) tree of a website. However, use of the DOM tree poses significant challenges as context and appearance are encoded in an abstract manner. To address this challenge we propose to reformulate WIE as a context-aware Webpage Object Detection task. Specifically, we develop a Context-aware Visual Attention-based (CoVA) detection pipeline which combines appearance features with syntactical structure from the DOM tree. To study the approach we collect a new large-scale dataset of e-commerce websites for which we manually annotate every web element with four labels: product price, product title, product image and background. On this dataset we show that the proposed CoVA approach is a new challenging baseline which improves upon prior state-of-the-art methods.



### ADC: Adversarial attacks against object Detection that evade Context consistency checks
- **Arxiv ID**: http://arxiv.org/abs/2110.12321v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12321v1)
- **Published**: 2021-10-24 00:25:09+00:00
- **Updated**: 2021-10-24 00:25:09+00:00
- **Authors**: Mingjun Yin, Shasha Li, Chengyu Song, M. Salman Asif, Amit K. Roy-Chowdhury, Srikanth V. Krishnamurthy
- **Comment**: WCAV'22 Acceptted
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have been shown to be vulnerable to adversarial examples, which are slightly perturbed input images which lead DNNs to make wrong predictions. To protect from such examples, various defense strategies have been proposed. A very recent defense strategy for detecting adversarial examples, that has been shown to be robust to current attacks, is to check for intrinsic context consistencies in the input data, where context refers to various relationships (e.g., object-to-object co-occurrence relationships) in images. In this paper, we show that even context consistency checks can be brittle to properly crafted adversarial examples and to the best of our knowledge, we are the first to do so. Specifically, we propose an adaptive framework to generate examples that subvert such defenses, namely, Adversarial attacks against object Detection that evade Context consistency checks (ADC). In ADC, we formulate a joint optimization problem which has two attack goals, viz., (i) fooling the object detector and (ii) evading the context consistency check system, at the same time. Experiments on both PASCAL VOC and MS COCO datasets show that examples generated with ADC fool the object detector with a success rate of over 85% in most cases, and at the same time evade the recently proposed context consistency checks, with a bypassing rate of over 80% in most cases. Our results suggest that how to robustly model context and check its consistency, is still an open problem.



### A methodology for detection and localization of fruits in apples orchards from aerial images
- **Arxiv ID**: http://arxiv.org/abs/2110.12331v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2110.12331v1)
- **Published**: 2021-10-24 01:57:52+00:00
- **Updated**: 2021-10-24 01:57:52+00:00
- **Authors**: Thiago T. Santos, Luciano Gebler
- **Comment**: Accepted for oral presentation at SBIAgro 2021 (XIII Congresso
  Brasileiro de Agroinform\'atica)
- **Journal**: None
- **Summary**: Computer vision methods based on convolutional neural networks (CNNs) have presented promising results on image-based fruit detection at ground-level for different crops. However, the integration of the detections found in different images, allowing accurate fruit counting and yield prediction, have received less attention. This work presents a methodology for automated fruit counting employing aerial-images. It includes algorithms based on multiple view geometry to perform fruits tracking, not just avoiding double counting but also locating the fruits in the 3-D space. Preliminary assessments show correlations above 0.8 between fruit counting and true yield for apples. The annotated dataset employed on CNN training is publicly available.



### SOLVER: Scene-Object Interrelated Visual Emotion Reasoning Network
- **Arxiv ID**: http://arxiv.org/abs/2110.12334v1
- **DOI**: 10.1109/TIP.2021.3118983
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.12334v1)
- **Published**: 2021-10-24 02:41:41+00:00
- **Updated**: 2021-10-24 02:41:41+00:00
- **Authors**: Jingyuan Yang, Xinbo Gao, Leida Li, Xiumei Wang, Jinshan Ding
- **Comment**: Accepted by TIP
- **Journal**: in IEEE Transactions on Image Processing, vol. 30, pp. 8686-8701,
  2021
- **Summary**: Visual Emotion Analysis (VEA) aims at finding out how people feel emotionally towards different visual stimuli, which has attracted great attention recently with the prevalence of sharing images on social networks. Since human emotion involves a highly complex and abstract cognitive process, it is difficult to infer visual emotions directly from holistic or regional features in affective images. It has been demonstrated in psychology that visual emotions are evoked by the interactions between objects as well as the interactions between objects and scenes within an image. Inspired by this, we propose a novel Scene-Object interreLated Visual Emotion Reasoning network (SOLVER) to predict emotions from images. To mine the emotional relationships between distinct objects, we first build up an Emotion Graph based on semantic concepts and visual features. Then, we conduct reasoning on the Emotion Graph using Graph Convolutional Network (GCN), yielding emotion-enhanced object features. We also design a Scene-Object Fusion Module to integrate scenes and objects, which exploits scene features to guide the fusion process of object features with the proposed scene-based attention mechanism. Extensive experiments and comparisons are conducted on eight public visual emotion datasets, and the results demonstrate that the proposed SOLVER consistently outperforms the state-of-the-art methods by a large margin. Ablation studies verify the effectiveness of our method and visualizations prove its interpretability, which also bring new insight to explore the mysteries in VEA. Notably, we further discuss SOLVER on three other potential datasets with extended experiments, where we validate the robustness of our method and notice some limitations of it.



### Quality Map Fusion for Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.12338v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12338v1)
- **Published**: 2021-10-24 03:01:46+00:00
- **Updated**: 2021-10-24 03:01:46+00:00
- **Authors**: Uche Osahor, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial models that capture salient low-level features which convey visual information in correlation with the human visual system (HVS) still suffer from perceptible image degradations. The inability to convey such highly informative features can be attributed to mode collapse, convergence failure and vanishing gradients. In this paper, we improve image quality adversarially by introducing a novel quality map fusion technique that harnesses image features similar to the HVS and the perceptual properties of a deep convolutional neural network (DCNN). We extend the widely adopted l2 Wasserstein distance metric to other preferable quality norms derived from Banach spaces that capture richer image properties like structure, luminance, contrast and the naturalness of images. We also show that incorporating a perceptual attention mechanism (PAM) that extracts global feature embeddings from the network bottleneck with aggregated perceptual maps derived from standard image quality metrics translate to a better image quality. We also demonstrate impressive performance over other methods.



### A Closer Look at Few-Shot Video Classification: A New Baseline and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2110.12358v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12358v1)
- **Published**: 2021-10-24 06:01:46+00:00
- **Updated**: 2021-10-24 06:01:46+00:00
- **Authors**: Zhenxi Zhu, Limin Wang, Sheng Guo, Gangshan Wu
- **Comment**: BMVC2021 camera-ready version
- **Journal**: None
- **Summary**: The existing few-shot video classification methods often employ a meta-learning paradigm by designing customized temporal alignment module for similarity calculation. While significant progress has been made, these methods fail to focus on learning effective representations, and heavily rely on the ImageNet pre-training, which might be unreasonable for the few-shot recognition setting due to semantics overlap. In this paper, we aim to present an in-depth study on few-shot video classification by making three contributions. First, we perform a consistent comparative study on the existing metric-based methods to figure out their limitations in representation learning. Accordingly, we propose a simple classifier-based baseline without any temporal alignment that surprisingly outperforms the state-of-the-art meta-learning based methods. Second, we discover that there is a high correlation between the novel action class and the ImageNet object class, which is problematic in the few-shot recognition setting. Our results show that the performance of training from scratch drops significantly, which implies that the existing benchmarks cannot provide enough base data. Finally, we present a new benchmark with more base data to facilitate future few-shot video classification without pre-training. The code will be made available at https://github.com/MCG-NJU/FSL-Video.



### CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot MultiBox Detector
- **Arxiv ID**: http://arxiv.org/abs/2110.12364v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12364v1)
- **Published**: 2021-10-24 06:45:33+00:00
- **Updated**: 2021-10-24 06:45:33+00:00
- **Authors**: Weiqiang Jin, Hang Yu, Hang Yu
- **Comment**: 9 pages;5 figures; conference: IEEE ICTAI; Acknowledgment: The
  research reported in this paper was supported in part by the National Natural
  Science Foundation of China under the grant 91746203 and the Outstanding
  Academic Leader Project of Shanghai under the grant No.20XD1401700
- **Journal**: None
- **Summary**: Due to the success of Bidirectional Encoder Representations from Transformers (BERT) in natural language process (NLP), the multi-head attention transformer has been more and more prevalent in computer-vision researches (CV). However, it still remains a challenge for researchers to put forward complex tasks such as vision detection and semantic segmentation. Although multiple Transformer-Based architectures like DETR and ViT-FRCNN have been proposed to complete object detection task, they inevitably decreases discrimination accuracy and brings down computational efficiency caused by the enormous learning parameters and heavy computational complexity incurred by the traditional self-attention operation. In order to alleviate these issues, we present a novel object detection architecture, named Convolutional vision Transformer Based Attentive Single Shot MultiBox Detector (CvT-ASSD), that built on the top of Convolutional vision Transormer (CvT) with the efficient Attentive Single Shot MultiBox Detector (ASSD). We provide comprehensive empirical evidence showing that our model CvT-ASSD can leads to good system efficiency and performance while being pretrained on large-scale detection datasets such as PASCAL VOC and MS COCO. Code has been released on public github repository at https://github.com/albert-jin/CvT-ASSD.



### AuxAdapt: Stable and Efficient Test-Time Adaptation for Temporally Consistent Video Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.12369v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.12369v1)
- **Published**: 2021-10-24 07:07:41+00:00
- **Updated**: 2021-10-24 07:07:41+00:00
- **Authors**: Yizhe Zhang, Shubhankar Borse, Hong Cai, Fatih Porikli
- **Comment**: To appear in WACV 2022; Comments and questions are welcome;
- **Journal**: None
- **Summary**: In video segmentation, generating temporally consistent results across frames is as important as achieving frame-wise accuracy. Existing methods rely either on optical flow regularization or fine-tuning with test data to attain temporal consistency. However, optical flow is not always avail-able and reliable. Besides, it is expensive to compute. Fine-tuning the original model in test time is cost sensitive.   This paper presents an efficient, intuitive, and unsupervised online adaptation method, AuxAdapt, for improving the temporal consistency of most neural network models. It does not require optical flow and only takes one pass of the video. Since inconsistency mainly arises from the model's uncertainty in its output, we propose an adaptation scheme where the model learns from its own segmentation decisions as it streams a video, which allows producing more confident and temporally consistent labeling for similarly-looking pixels across frames. For stability and efficiency, we leverage a small auxiliary segmentation network (AuxNet) to assist with this adaptation. More specifically, AuxNet readjusts the decision of the original segmentation network (Main-Net) by adding its own estimations to that of MainNet. At every frame, only AuxNet is updated via back-propagation while keeping MainNet fixed. We extensively evaluate our test-time adaptation approach on standard video benchmarks, including Cityscapes, CamVid, and KITTI. The results demonstrate that our approach provides label-wise accurate, temporally consistent, and computationally efficient adaptation (5+ folds overhead reduction comparing to state-of-the-art test-time adaptation methods).



### Uncertainty-Guided Lung Nodule Segmentation with Feature-Aware Attention
- **Arxiv ID**: http://arxiv.org/abs/2110.12372v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.12372v4)
- **Published**: 2021-10-24 07:19:37+00:00
- **Updated**: 2022-06-07 08:49:29+00:00
- **Authors**: Han Yang, Lu Shen, Mengke Zhang, Qiuli Wang
- **Comment**: 10 pages, 4 figures, 30 references
- **Journal**: None
- **Summary**: Since radiologists have different training and clinical experiences, they may provide various segmentation annotations for a lung nodule. Conventional studies choose a single annotation as the learning target by default, but they waste valuable information of consensus or disagreements ingrained in the multiple annotations. This paper proposes an Uncertainty-Guided Segmentation Network (UGS-Net), which learns the rich visual features from the regions that may cause segmentation uncertainty and contributes to a better segmentation result. With an Uncertainty-Aware Module, this network can provide a Multi-Confidence Mask (MCM), pointing out regions with different segmentation uncertainty levels. Moreover, this paper introduces a Feature-Aware Attention Module to enhance the learning of the nodule boundary and density differences. Experimental results show that our method can predict the nodule regions with different uncertainty levels and achieve superior performance in LIDC-IDRI dataset.



### Perceptual Consistency in Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.12385v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.12385v1)
- **Published**: 2021-10-24 08:08:49+00:00
- **Updated**: 2021-10-24 08:08:49+00:00
- **Authors**: Yizhe Zhang, Shubhankar Borse, Hong Cai, Ying Wang, Ning Bi, Xiaoyun Jiang, Fatih Porikli
- **Comment**: To appear in WACV 2022. Comments and questions are welcome
- **Journal**: None
- **Summary**: In this paper, we present a novel perceptual consistency perspective on video semantic segmentation, which can capture both temporal consistency and pixel-wise correctness. Given two nearby video frames, perceptual consistency measures how much the segmentation decisions agree with the pixel correspondences obtained via matching general perceptual features. More specifically, for each pixel in one frame, we find the most perceptually correlated pixel in the other frame. Our intuition is that such a pair of pixels are highly likely to belong to the same class. Next, we assess how much the segmentation agrees with such perceptual correspondences, based on which we derive the perceptual consistency of the segmentation maps across these two frames. Utilizing perceptual consistency, we can evaluate the temporal consistency of video segmentation by measuring the perceptual consistency over consecutive pairs of segmentation maps in a video. Furthermore, given a sparsely labeled test video, perceptual consistency can be utilized to aid with predicting the pixel-wise correctness of the segmentation on an unlabeled frame. More specifically, by measuring the perceptual consistency between the predicted segmentation and the available ground truth on a nearby frame and combining it with the segmentation confidence, we can accurately assess the classification correctness on each pixel. Our experiments show that the proposed perceptual consistency can more accurately evaluate the temporal consistency of video segmentation as compared to flow-based measures. Furthermore, it can help more confidently predict segmentation accuracy on unlabeled test frames, as compared to using classification confidence alone. Finally, our proposed measure can be used as a regularizer during the training of segmentation models, which leads to more temporally consistent video segmentation while maintaining accuracy.



### Using Motion History Images with 3D Convolutional Networks in Isolated Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.12396v2
- **DOI**: 10.1109/ACCESS.2022.3151362
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12396v2)
- **Published**: 2021-10-24 09:25:28+00:00
- **Updated**: 2022-02-18 15:38:54+00:00
- **Authors**: Ozge Mercanoglu Sincan, Hacer Yalim Keles
- **Comment**: None
- **Journal**: None
- **Summary**: Sign language recognition using computational models is a challenging problem that requires simultaneous spatio-temporal modeling of the multiple sources, i.e. faces, hands, body, etc. In this paper, we propose an isolated sign language recognition model based on a model trained using Motion History Images (MHI) that are generated from RGB video frames. RGB-MHI images represent spatio-temporal summary of each sign video effectively in a single RGB image. We propose two different approaches using this RGB-MHI model. In the first approach, we use the RGB-MHI model as a motion-based spatial attention module integrated into a 3D-CNN architecture. In the second approach, we use RGB-MHI model features directly with the features of a 3D-CNN model using a late fusion technique. We perform extensive experiments on two recently released large-scale isolated sign language datasets, namely AUTSL and BosphorusSign22k. Our experiments show that our models, which use only RGB data, can compete with the state-of-the-art models in the literature that use multi-modal data.



### BINAS: Bilinear Interpretable Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2110.12399v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, math.OC, stat.ML, 68T09, 68T45, G.1.6; G.3; I.2.8; I.2.10; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2110.12399v3)
- **Published**: 2021-10-24 09:45:00+00:00
- **Updated**: 2022-04-27 12:55:23+00:00
- **Authors**: Niv Nayman, Yonathan Aflalo, Asaf Noy, Rong Jin, Lihi Zelnik-Manor
- **Comment**: The full code is released at https://github.com/Alibaba-MIIL/BINAS
- **Journal**: None
- **Summary**: Practical use of neural networks often involves requirements on latency, energy and memory among others. A popular approach to find networks under such requirements is through constrained Neural Architecture Search (NAS). However, previous methods use complicated predictors for the accuracy of the network. Those predictors are hard to interpret and sensitive to many hyperparameters to be tuned, hence, the resulting accuracy of the generated models is often harmed. In this work we resolve this by introducing Bilinear Interpretable Neural Architecture Search (BINAS), that is based on an accurate and simple bilinear formulation of both an accuracy estimator and the expected resource requirement, together with a scalable search method with theoretical guarantees. The simplicity of our proposed estimator together with the intuitive way it is constructed bring interpretability through many insights about the contribution of different design choices. For example, we find that in the examined search space, adding depth and width is more effective at deeper stages of the network and at the beginning of each resolution stage. Our experiments show that BINAS generates comparable to or better architectures than other state-of-the-art NAS methods within a reduced marginal search cost, while strictly satisfying the resource constraints.



### A Dynamic Keypoints Selection Network for 6DoF Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.12401v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12401v1)
- **Published**: 2021-10-24 09:58:56+00:00
- **Updated**: 2021-10-24 09:58:56+00:00
- **Authors**: Haowen Sun, Taiyong Wang
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: 6 DoF poses estimation problem aims to estimate the rotation and translation parameters between two coordinates, such as object world coordinate and camera world coordinate. Although some advances are made with the help of deep learning, how to full use scene information is still a problem. Prior works tackle the problem by pixel-wise feature fusion but need to randomly selecte numerous points from images, which can not satisfy the demands of fast inference simultaneously and accurate pose estimation. In this work, we present a novel deep neural network based on dynamic keypoints selection designed for 6DoF pose estimation from a single RGBD image. Our network includes three parts, instance semantic segmentation, edge points detection and 6DoF pose estimation. Given an RGBD image, our network is trained to predict pixel category and the translation to edge points and center points. Then, a least-square fitting manner is applied to estimate the 6DoF pose parameters. Specifically, we propose a dynamic keypoints selection algorithm to choose keypoints from the foreground feature map. It allows us to leverage geometric and appearance information. During 6DoF pose estimation, we utilize the instance semantic segmentation result to filter out background points and only use foreground points to finish edge points detection and 6DoF pose estimation. Experiments on two commonly used 6DoF estimation benchmark datasets, YCB-Video and LineMoD, demonstrate that our method outperforms the state-of-the-art methods and achieves significant improvements over other same category methods time efficiency.



### NAS-FCOS: Efficient Search for Object Detection Architectures
- **Arxiv ID**: http://arxiv.org/abs/2110.12423v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12423v1)
- **Published**: 2021-10-24 12:20:04+00:00
- **Updated**: 2021-10-24 12:20:04+00:00
- **Authors**: Ning Wang, Yang Gao, Hao Chen, Peng Wang, Zhi Tian, Chunhua Shen, Yanning Zhang
- **Comment**: 14 pages, 11 figures, journal extension of NAS-FCOS (CVPR 2020),
  accepted by IJCV. arXiv admin note: substantial text overlap with
  arXiv:1906.04423
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) has shown great potential in effectively reducing manual effort in network design by automatically discovering optimal architectures. What is noteworthy is that as of now, object detection is less touched by NAS algorithms despite its significant importance in computer vision. To the best of our knowledge, most of the recent NAS studies on object detection tasks fail to satisfactorily strike a balance between performance and efficiency of the resulting models, let alone the excessive amount of computational resources cost by those algorithms. Here we propose an efficient method to obtain better object detectors by searching for the feature pyramid network (FPN) as well as the prediction head of a simple anchor-free object detector, namely, FCOS [36], using a tailored reinforcement learning paradigm. With carefully designed search space, search algorithms, and strategies for evaluating network quality, we are able to find top-performing detection architectures within 4 days using 8 V100 GPUs. The discovered architectures surpass state-of-the-art object detection models (such as Faster R-CNN, Retina-Net and, FCOS) by 1.0% to 5.4% points in AP on the COCO dataset, with comparable computation complexity and memory footprint, demonstrating the efficacy of the proposed NAS method for object detection. Code is available at https://github.com/Lausannen/NAS-FCOS.



### Image-Based CLIP-Guided Essence Transfer
- **Arxiv ID**: http://arxiv.org/abs/2110.12427v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12427v4)
- **Published**: 2021-10-24 12:46:53+00:00
- **Updated**: 2022-10-11 17:41:02+00:00
- **Authors**: Hila Chefer, Sagie Benaim, Roni Paiss, Lior Wolf
- **Comment**: To appear in ECCV'22
- **Journal**: None
- **Summary**: We make the distinction between (i) style transfer, in which a source image is manipulated to match the textures and colors of a target image, and (ii) essence transfer, in which one edits the source image to include high-level semantic attributes from the target. Crucially, the semantic attributes that constitute the essence of an image may differ from image to image. Our blending operator combines the powerful StyleGAN generator and the semantic encoder of CLIP in a novel way that is simultaneously additive in both latent spaces, resulting in a mechanism that guarantees both identity preservation and high-level feature transfer without relying on a facial recognition network. We present two variants of our method. The first is based on optimization, while the second fine-tunes an existing inversion encoder to perform essence extraction. Through extensive experiments, we demonstrate the superiority of our methods for essence transfer over existing methods for style transfer, domain adaptation, and text-based semantic editing. Our code is available at https://github.com/hila-chefer/TargetCLIP.



### WARPd: A linearly convergent first-order method for inverse problems with approximate sharpness conditions
- **Arxiv ID**: http://arxiv.org/abs/2110.12437v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.IR, cs.NA, math.OC, stat.ML, 65K10, 68U10, 65Y20, 68Q25, 90C25, 94A08, 15A83
- **Links**: [PDF](http://arxiv.org/pdf/2110.12437v1)
- **Published**: 2021-10-24 13:19:41+00:00
- **Updated**: 2021-10-24 13:19:41+00:00
- **Authors**: Matthew J. Colbrook
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstruction of signals from undersampled and noisy measurements is a topic of considerable interest. Sharpness conditions directly control the recovery performance of restart schemes for first-order methods without the need for restrictive assumptions such as strong convexity. However, they are challenging to apply in the presence of noise or approximate model classes (e.g., approximate sparsity). We provide a first-order method: Weighted, Accelerated and Restarted Primal-dual (WARPd), based on primal-dual iterations and a novel restart-reweight scheme. Under a generic approximate sharpness condition, WARPd achieves stable linear convergence to the desired vector. Many problems of interest fit into this framework. For example, we analyze sparse recovery in compressed sensing, low-rank matrix recovery, matrix completion, TV regularization, minimization of $\|Bx\|_{l^1}$ under constraints ($l^1$-analysis problems for general $B$), and mixed regularization problems. We show how several quantities controlling recovery performance also provide explicit approximate sharpness constants. Numerical experiments show that WARPd compares favorably with specialized state-of-the-art methods and is ideally suited for solving large-scale problems. We also present a noise-blind variant based on the Square-Root LASSO decoder. Finally, we show how to unroll WARPd as neural networks. This approximation theory result provides lower bounds for stable and accurate neural networks for inverse problems and sheds light on architecture choices. Code and a gallery of examples are made available online as a MATLAB package.



### Bangla Image Caption Generation through CNN-Transformer based Encoder-Decoder Network
- **Arxiv ID**: http://arxiv.org/abs/2110.12442v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.12442v1)
- **Published**: 2021-10-24 13:33:23+00:00
- **Updated**: 2021-10-24 13:33:23+00:00
- **Authors**: Md Aminul Haque Palash, MD Abdullah Al Nasim, Sourav Saha, Faria Afrin, Raisa Mallik, Sathishkumar Samiappan
- **Comment**: 15 pages, 6 figures, 1 table, 6 equations
- **Journal**: None
- **Summary**: Automatic Image Captioning is the never-ending effort of creating syntactically and validating the accuracy of textual descriptions of an image in natural language with context. The encoder-decoder structure used throughout existing Bengali Image Captioning (BIC) research utilized abstract image feature vectors as the encoder's input. We propose a novel transformer-based architecture with an attention mechanism with a pre-trained ResNet-101 model image encoder for feature extraction from images. Experiments demonstrate that the language decoder in our technique captures fine-grained information in the caption and, then paired with image features, produces accurate and diverse captions on the BanglaLekhaImageCaptions dataset. Our approach outperforms all existing Bengali Image Captioning work and sets a new benchmark by scoring 0.694 on BLEU-1, 0.630 on BLEU-2, 0.582 on BLEU-3, and 0.337 on METEOR.



### Requirement analysis for an artificial intelligence model for the diagnosis of the COVID-19 from chest X-ray data
- **Arxiv ID**: http://arxiv.org/abs/2110.12464v2
- **DOI**: 10.1109/BIBM52615.2021.9669525
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12464v2)
- **Published**: 2021-10-24 15:28:18+00:00
- **Updated**: 2021-11-19 21:43:25+00:00
- **Authors**: Tuomo Kalliokoski
- **Comment**: Accepted to AI&BDvsPandemics BIBM 2021 Workshop
- **Journal**: None
- **Summary**: There are multiple papers published about different AI models for the COVID-19 diagnosis with promising results. Unfortunately according to the reviews many of the papers do not reach the level of sophistication needed for a clinically usable model. In this paper I go through multiple review papers, guidelines, and other relevant material in order to generate more comprehensive requirements for the future papers proposing a AI based diagnosis of the COVID-19 from chest X-ray data (CXR). Main findings are that a clinically usable AI needs to have an extremely good documentation, comprehensive statistical analysis of the possible biases and performance, and an explainability module.



### Robustness via Uncertainty-aware Cycle Consistency
- **Arxiv ID**: http://arxiv.org/abs/2110.12467v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.12467v1)
- **Published**: 2021-10-24 15:33:21+00:00
- **Updated**: 2021-10-24 15:33:21+00:00
- **Authors**: Uddeshya Upadhyay, Yanbei Chen, Zeynep Akata
- **Comment**: Accepted at NeurIPS 2021. Code is at
  https://github.com/ExplainableML/UncertaintyAwareCycleConsistency. arXiv
  admin note: substantial text overlap with arXiv:2102.11747
- **Journal**: None
- **Summary**: Unpaired image-to-image translation refers to learning inter-image-domain mapping without corresponding image pairs. Existing methods learn deterministic mappings without explicitly modelling the robustness to outliers or predictive uncertainty, leading to performance degradation when encountering unseen perturbations at test time. To address this, we propose a novel probabilistic method based on Uncertainty-aware Generalized Adaptive Cycle Consistency (UGAC), which models the per-pixel residual by generalized Gaussian distribution, capable of modelling heavy-tailed distributions. We compare our model with a wide variety of state-of-the-art methods on various challenging tasks including unpaired image translation of natural images, using standard datasets, spanning autonomous driving, maps, facades, and also in medical imaging domain consisting of MRI. Experimental results demonstrate that our method exhibits stronger robustness towards unseen perturbations in test data. Code is released here: https://github.com/ExplainableML/UncertaintyAwareCycleConsistency.



### Exploring Gradient Flow Based Saliency for DNN Model Compression
- **Arxiv ID**: http://arxiv.org/abs/2110.12477v1
- **DOI**: 10.1145/3474085.3475474
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12477v1)
- **Published**: 2021-10-24 16:09:40+00:00
- **Updated**: 2021-10-24 16:09:40+00:00
- **Authors**: Xinyu Liu, Baopu Li, Zhen Chen, Yixuan Yuan
- **Comment**: ACM Multimedia 2021 (ACM MM'21)
- **Journal**: None
- **Summary**: Model pruning aims to reduce the deep neural network (DNN) model size or computational overhead. Traditional model pruning methods such as l-1 pruning that evaluates the channel significance for DNN pay too much attention to the local analysis of each channel and make use of the magnitude of the entire feature while ignoring its relevance to the batch normalization (BN) and ReLU layer after each convolutional operation. To overcome these problems, we propose a new model pruning method from a new perspective of gradient flow in this paper. Specifically, we first theoretically analyze the channel's influence based on Taylor expansion by integrating the effects of BN layer and ReLU activation function. Then, the incorporation of the first-order Talyor polynomial of the scaling parameter and the shifting parameter in the BN layer is suggested to effectively indicate the significance of a channel in a DNN. Comprehensive experiments on both image classification and image denoising tasks demonstrate the superiority of the proposed novel theory and scheme. Code is available at https://github.com/CityU-AIM-Group/GFBS.



### Deep Asymmetric Hashing with Dual Semantic Regression and Class Structure Quantization
- **Arxiv ID**: http://arxiv.org/abs/2110.12478v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2110.12478v2)
- **Published**: 2021-10-24 16:14:36+00:00
- **Updated**: 2021-12-23 14:03:35+00:00
- **Authors**: Jianglin Lu, Hailing Wang, Jie Zhou, Mengfan Yan, Jiajun Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep hashing methods have been widely used in image retrieval task. Most existing deep hashing approaches adopt one-to-one quantization to reduce information loss. However, such class-unrelated quantization cannot give discriminative feedback for network training. In addition, these methods only utilize single label to integrate supervision information of data for hashing function learning, which may result in inferior network generalization performance and relatively low-quality hash codes since the inter-class information of data is totally ignored. In this paper, we propose a dual semantic asymmetric hashing (DSAH) method, which generates discriminative hash codes under three-fold constraints. Firstly, DSAH utilizes class prior to conduct class structure quantization so as to transmit class information during the quantization process. Secondly, a simple yet effective label mechanism is designed to characterize both the intra-class compactness and inter-class separability of data, thereby achieving semantic-sensitive binary code learning. Finally, a meaningful pairwise similarity preserving loss is devised to minimize the distances between class-related network outputs based on an affinity graph. With these three main components, high-quality hash codes can be generated through network. Extensive experiments conducted on various datasets demonstrate the superiority of DSAH in comparison with state-of-the-art deep hashing methods.



### EgoNN: Egocentric Neural Network for Point Cloud Based 6DoF Relocalization at the City Scale
- **Arxiv ID**: http://arxiv.org/abs/2110.12486v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12486v1)
- **Published**: 2021-10-24 16:46:57+00:00
- **Updated**: 2021-10-24 16:46:57+00:00
- **Authors**: Jacek Komorowski, Monika Wysoczanska, Tomasz Trzcinski
- **Comment**: None
- **Journal**: None
- **Summary**: The paper presents a deep neural network-based method for global and local descriptors extraction from a point cloud acquired by a rotating 3D LiDAR. The descriptors can be used for two-stage 6DoF relocalization. First, a course position is retrieved by finding candidates with the closest global descriptor in the database of geo-tagged point clouds. Then, the 6DoF pose between a query point cloud and a database point cloud is estimated by matching local descriptors and using a robust estimator such as RANSAC. Our method has a simple, fully convolutional architecture based on a sparse voxelized representation. It can efficiently extract a global descriptor and a set of keypoints with local descriptors from large point clouds with tens of thousand points. Our code and pretrained models are publicly available on the project website.



### A Deep Learning Approach to Predicting Collateral Flow in Stroke Patients Using Radiomic Features from Perfusion Images
- **Arxiv ID**: http://arxiv.org/abs/2110.12508v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12508v1)
- **Published**: 2021-10-24 18:58:40+00:00
- **Updated**: 2021-10-24 18:58:40+00:00
- **Authors**: Giles Tetteh, Fernando Navarro, Johannes Paetzold, Jan Kirschke, Claus Zimmer, Bjoern H. Menze
- **Comment**: None
- **Journal**: None
- **Summary**: Collateral circulation results from specialized anastomotic channels which are capable of providing oxygenated blood to regions with compromised blood flow caused by ischemic injuries. The quality of collateral circulation has been established as a key factor in determining the likelihood of a favorable clinical outcome and goes a long way to determine the choice of stroke care model - that is the decision to transport or treat eligible patients immediately.   Though there exist several imaging methods and grading criteria for quantifying collateral blood flow, the actual grading is mostly done through manual inspection of the acquired images. This approach is associated with a number of challenges. First, it is time-consuming - the clinician needs to scan through several slices of images to ascertain the region of interest before deciding on what severity grade to assign to a patient. Second, there is a high tendency for bias and inconsistency in the final grade assigned to a patient depending on the experience level of the clinician.   We present a deep learning approach to predicting collateral flow grading in stroke patients based on radiomic features extracted from MR perfusion data. First, we formulate a region of interest detection task as a reinforcement learning problem and train a deep learning network to automatically detect the occluded region within the 3D MR perfusion volumes. Second, we extract radiomic features from the obtained region of interest through local image descriptors and denoising auto-encoders. Finally, we apply a convolutional neural network and other machine learning classifiers to the extracted radiomic features to automatically predict the collateral flow grading of the given patient volume as one of three severity classes - no flow (0), moderate flow (1), and good flow (2)...



### Per-Pixel Lung Thickness and Lung Capacity Estimation on Chest X-Rays using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.12509v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.12509v4)
- **Published**: 2021-10-24 19:09:28+00:00
- **Updated**: 2022-03-29 15:17:40+00:00
- **Authors**: Manuel Schultheiss, Philipp Schmette, Thorsten Sellerer, Rafael Schick, Kirsten Taphorn, Korbinian Mechlem, Lorenz Birnbacher, Bernhard Renger, Marcus R. Makowski, Franz Pfeiffer, Daniela Pfeiffer
- **Comment**: v4: fixed simulation bug, improved text, various other improvements
- **Journal**: None
- **Summary**: Estimating the lung depth on x-ray images could provide both an accurate opportunistic lung volume estimation during clinical routine and improve image contrast in modern structural chest imaging techniques like x-ray dark-field imaging. We present a method based on a convolutional neural network that allows a per-pixel lung thickness estimation and subsequent total lung capacity estimation. The network was trained and validated using 5250 simulated radiographs generated from 525 real CT scans. The network was evaluated on a test set of 131 synthetic radiographs and a retrospective evaluation was performed on another test set of 45 standard clinical radiographs. The standard clinical radiographs were obtained from 45 patients, who got a CT examination between July 1, 2021 and September 1, 2021 and a chest x-ray 6 month before or after the CT. For 45 standard clinical radiographs, the mean-absolute error between the estimated lung volume and groundtruth volume was 0.75 liter with a positive correlation (r = 0.78). When accounting for the patient diameter, the error decreases to 0.69 liter with a positive correlation (r = 0.83). Additionally, we predicted the lung thicknesses on the synthetic test set, where the mean-absolute error between the total volumes was 0.19 liter with a positive correlation (r = 0.99). The results show, that creation of lung thickness maps and estimation of approximate total lung volume is possible from standard clinical radiographs.



### X-Distill: Improving Self-Supervised Monocular Depth via Cross-Task Distillation
- **Arxiv ID**: http://arxiv.org/abs/2110.12516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12516v1)
- **Published**: 2021-10-24 19:47:14+00:00
- **Updated**: 2021-10-24 19:47:14+00:00
- **Authors**: Hong Cai, Janarbek Matai, Shubhankar Borse, Yizhe Zhang, Amin Ansari, Fatih Porikli
- **Comment**: Accepted to BMVC 2021
- **Journal**: None
- **Summary**: In this paper, we propose a novel method, X-Distill, to improve the self-supervised training of monocular depth via cross-task knowledge distillation from semantic segmentation to depth estimation. More specifically, during training, we utilize a pretrained semantic segmentation teacher network and transfer its semantic knowledge to the depth network. In order to enable such knowledge distillation across two different visual tasks, we introduce a small, trainable network that translates the predicted depth map to a semantic segmentation map, which can then be supervised by the teacher network. In this way, this small network enables the backpropagation from the semantic segmentation teacher's supervision to the depth network during training. In addition, since the commonly used object classes in semantic segmentation are not directly transferable to depth, we study the visual and geometric characteristics of the objects and design a new way of grouping them that can be shared by both tasks. It is noteworthy that our approach only modifies the training process and does not incur additional computation during inference. We extensively evaluate the efficacy of our proposed approach on the standard KITTI benchmark and compare it with the latest state of the art. We further test the generalizability of our approach on Make3D. Overall, the results show that our approach significantly improves the depth estimation accuracy and outperforms the state of the art.



### GraspLook: a VR-based Telemanipulation System with R-CNN-driven Augmentation of Virtual Environment
- **Arxiv ID**: http://arxiv.org/abs/2110.12518v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2110.12518v1)
- **Published**: 2021-10-24 19:50:39+00:00
- **Updated**: 2021-10-24 19:50:39+00:00
- **Authors**: Polina Ponomareva, Daria Trinitatova, Aleksey Fedoseev, Ivan Kalinov, Dzmitry Tsetserukou
- **Comment**: Accepted to IEEE 20th International Conference on Advanced Robotics
  (ICAR) 2021, 6 pages, 8 figures
- **Journal**: None
- **Summary**: The teleoperation of robotic systems in medical applications requires stable and convenient visual feedback for the operator. The most accessible approach to delivering visual information from the remote area is using cameras to transmit a video stream from the environment. However, such systems are sensitive to the camera resolution, limited viewpoints, and cluttered environment bringing additional mental demands to the human operator. The paper proposes a novel system of teleoperation based on an augmented virtual environment (VE). The region-based convolutional neural network (R-CNN) is applied to detect the laboratory instrument and estimate its position in the remote environment to display further its digital twin in the VE, which is necessary for dexterous telemanipulation. The experimental results revealed that the developed system allows users to operate the robot smoother, which leads to a decrease in task execution time when manipulating test tubes. In addition, the participants evaluated the developed system as less mentally demanding (by 11%) and requiring less effort (by 16%) to accomplish the task than the camera-based teleoperation approach and highly assessed their performance in the augmented VE. The proposed technology can be potentially applied for conducting laboratory tests in remote areas when operating with infectious and poisonous reagents.



### Reachability Embeddings: Scalable Self-Supervised Representation Learning from Mobility Trajectories for Multimodal Geospatial Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2110.12521v2
- **DOI**: 10.1109/MDM55031.2022.00028
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.12521v2)
- **Published**: 2021-10-24 20:10:22+00:00
- **Updated**: 2022-07-15 21:30:37+00:00
- **Authors**: Swetava Ganguli, C. V. Krishnakumar Iyer, Vipul Pandey
- **Comment**: Extended version of the accepted research track paper at the 23rd
  IEEE International Conference on Mobile Data Management (MDM), 2022, Paphos,
  Cyprus. 12 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: Self-supervised representation learning techniques utilize large datasets without semantic annotations to learn meaningful, universal features that can be conveniently transferred to solve a wide variety of downstream supervised tasks. In this paper, we propose a self-supervised method for learning representations of geographic locations from unlabeled GPS trajectories to solve downstream geospatial computer vision tasks. Tiles resulting from a raster representation of the earth's surface are modeled as nodes on a graph or pixels of an image. GPS trajectories are modeled as allowed Markovian paths on these nodes. A scalable and distributed algorithm is presented to compute image-like tensors, called reachability summaries, of the spatial connectivity patterns between tiles and their neighbors implied by the observed Markovian paths. A convolutional, contractive autoencoder is trained to learn compressed representations, called reachability embeddings, of reachability summaries for every tile. Reachability embeddings serve as task-agnostic, feature representations of geographic locations. Using reachability embeddings as pixel representations for five different downstream geospatial tasks, cast as supervised semantic segmentation problems, we quantitatively demonstrate that reachability embeddings are semantically meaningful representations and result in 4-23% gain in performance, while using upto 67% less trajectory data, as measured using area under the precision-recall curve (AUPRC) metric, when compared to baseline models that use pixel representations that do not account for the spatial connectivity between tiles. Reachability embeddings transform sequential, spatiotemporal mobility data into semantically meaningful image-like tensor representations that can be combined with other sources of imagery and are designed to facilitate multimodal learning in geospatial computer vision.



### Light-Field Microscopy for optical imaging of neuronal activity: when model-based methods meet data-driven approaches
- **Arxiv ID**: http://arxiv.org/abs/2110.13142v1
- **DOI**: 10.1109/MSP.2021.3123557
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2110.13142v1)
- **Published**: 2021-10-24 20:58:51+00:00
- **Updated**: 2021-10-24 20:58:51+00:00
- **Authors**: Pingfan Song, Herman Verinaz Jadan, Carmel L. Howe, Amanda J. Foust, Pier Luigi Dragotti
- **Comment**: 20 pages, 9 figures, article accepted by IEEE Signal Processing
  Magazine
- **Journal**: None
- **Summary**: Understanding how networks of neurons process information is one of the key challenges in modern neuroscience. A necessary step to achieve this goal is to be able to observe the dynamics of large populations of neurons over a large area of the brain. Light-field microscopy (LFM), a type of scanless microscope, is a particularly attractive candidate for high-speed three-dimensional (3D) imaging. It captures volumetric information in a single snapshot, allowing volumetric imaging at video frame-rates. Specific features of imaging neuronal activity using LFM call for the development of novel machine learning approaches that fully exploit priors embedded in physics and optics models. Signal processing theory and wave-optics theory could play a key role in filling this gap, and contribute to novel computational methods with enhanced interpretability and generalization by integrating model-driven and data-driven approaches. This paper is devoted to a comprehensive survey to state-of-the-art of computational methods for LFM, with a focus on model-based and data-driven approaches.



### hSDB-instrument: Instrument Localization Database for Laparoscopic and Robotic Surgeries
- **Arxiv ID**: http://arxiv.org/abs/2110.12555v2
- **DOI**: 10.1007/978-3-030-87202-1_38 10.1007/978-3-030-87202-1_38
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.12555v2)
- **Published**: 2021-10-24 23:35:37+00:00
- **Updated**: 2021-10-26 02:02:04+00:00
- **Authors**: Jihun Yoon, Jiwon Lee, Sunghwan Heo, Hayeong Yu, Jayeon Lim, Chi Hyun Song, SeulGi Hong, Seungbum Hong, Bokyung Park, SungHyun Park, Woo Jin Hyung, Min-Kook Choi
- **Comment**: https://hsdb-instrument.github.io
- **Journal**: MICCAI 2021 pp 393-402
- **Summary**: Automated surgical instrument localization is an important technology to understand the surgical process and in order to analyze them to provide meaningful guidance during surgery or surgical index after surgery to the surgeon. We introduce a new dataset that reflects the kinematic characteristics of surgical instruments for automated surgical instrument localization of surgical videos. The hSDB(hutom Surgery DataBase)-instrument dataset consists of instrument localization information from 24 cases of laparoscopic cholecystecomy and 24 cases of robotic gastrectomy. Localization information for all instruments is provided in the form of a bounding box for object detection. To handle class imbalance problem between instruments, synthesized instruments modeled in Unity for 3D models are included as training data. Besides, for 3D instrument data, a polygon annotation is provided to enable instance segmentation of the tool. To reflect the kinematic characteristics of all instruments, they are annotated with head and body parts for laparoscopic instruments, and with head, wrist, and body parts for robotic instruments separately. Annotation data of assistive tools (specimen bag, needle, etc.) that are frequently used for surgery are also included. Moreover, we provide statistical information on the hSDB-instrument dataset and the baseline localization performances of the object detection networks trained by the MMDetection library and resulting analyses.



