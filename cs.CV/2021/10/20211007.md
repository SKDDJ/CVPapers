# Arxiv Papers in cs.CV on 2021-10-07
### Generic tool for numerical simulation of transformation-diffusion processes in complex volume geometric shapes: application to microbial decomposition of organic matter
- **Arxiv ID**: http://arxiv.org/abs/2110.03130v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.comp-ph, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2110.03130v3)
- **Published**: 2021-10-07 01:01:48+00:00
- **Updated**: 2022-08-28 15:03:08+00:00
- **Authors**: Monga Olivier, Hecht Frédéric, Moto Serge, Klai Mouad, Mbe Bruno, Dias Jorge, Garnier Patricia, Pot Valérie
- **Comment**: This paper represents, in my opinion, a breakthrough and then is
  worthing to be online before the end of the review process
- **Journal**: None
- **Summary**: This paper presents a generic framework for the numerical simulation of transformation-diffusion processes in complex volume geometric shapes. This work follows a previous one devoted to the simulation of microbial degradation of organic matter in porous system at microscopic scale. We generalized and improved the MOSAIC method significantly and thus yielding a much more generic and efficient numerical simulation scheme. In particular, regarding the simulation of diffusion processes from the graph, in this study we proposed a completely explicit and semi-implicit numerical scheme that can significantly reduce the computational complexity. We validated our method by comparing the results to the one provided by classical Lattice Boltzmann Method (LBM) within the context of microbial decomposition simulation. For the same datasets, we obtained similar results in a significantly shorter computing time (i.e., 10-15 minutes) than the prior work (several hours). Besides the classical LBM method takes around 3 weeks computing time.



### Efficient Sharpness-aware Minimization for Improved Training of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.03141v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03141v2)
- **Published**: 2021-10-07 02:20:37+00:00
- **Updated**: 2022-05-28 15:35:32+00:00
- **Authors**: Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, Vincent Y. F. Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Overparametrized Deep Neural Networks (DNNs) often achieve astounding performances, but may potentially result in severe generalization error. Recently, the relation between the sharpness of the loss landscape and the generalization error has been established by Foret et al. (2020), in which the Sharpness Aware Minimizer (SAM) was proposed to mitigate the degradation of the generalization. Unfortunately, SAM s computational cost is roughly double that of base optimizers, such as Stochastic Gradient Descent (SGD). This paper thus proposes Efficient Sharpness Aware Minimizer (ESAM), which boosts SAM s efficiency at no cost to its generalization performance. ESAM includes two novel and efficient training strategies-StochasticWeight Perturbation and Sharpness-Sensitive Data Selection. In the former, the sharpness measure is approximated by perturbing a stochastically chosen set of weights in each iteration; in the latter, the SAM loss is optimized using only a judiciously selected subset of data that is sensitive to the sharpness. We provide theoretical explanations as to why these strategies perform well. We also show, via extensive experiments on the CIFAR and ImageNet datasets, that ESAM enhances the efficiency over SAM from requiring 100% extra computations to 40% vis-a-vis base optimizers, while test accuracies are preserved or even improved.



### Meta-UDA: Unsupervised Domain Adaptive Thermal Object Detection using Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.03143v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03143v1)
- **Published**: 2021-10-07 02:28:18+00:00
- **Updated**: 2021-10-07 02:28:18+00:00
- **Authors**: Vibashan VS, Domenick Poster, Suya You, Shuowen Hu, Vishal M. Patel
- **Comment**: Accepted to WACV 2022
- **Journal**: None
- **Summary**: Object detectors trained on large-scale RGB datasets are being extensively employed in real-world applications. However, these RGB-trained models suffer a performance drop under adverse illumination and lighting conditions. Infrared (IR) cameras are robust under such conditions and can be helpful in real-world applications. Though thermal cameras are widely used for military applications and increasingly for commercial applications, there is a lack of robust algorithms to robustly exploit the thermal imagery due to the limited availability of labeled thermal data. In this work, we aim to enhance the object detection performance in the thermal domain by leveraging the labeled visible domain data in an Unsupervised Domain Adaptation (UDA) setting. We propose an algorithm agnostic meta-learning framework to improve existing UDA methods instead of proposing a new UDA strategy. We achieve this by meta-learning the initial condition of the detector, which facilitates the adaptation process with fine updates without overfitting or getting stuck at local optima. However, meta-learning the initial condition for the detection scenario is computationally heavy due to long and intractable computation graphs. Therefore, we propose an online meta-learning paradigm which performs online updates resulting in a short and tractable computation graph. To this end, we demonstrate the superiority of our method over many baselines in the UDA setting, producing a state-of-the-art thermal detector for the KAIST and DSIAC datasets.



### DoubleStar: Long-Range Attack Towards Depth Estimation based Obstacle Avoidance in Autonomous Systems
- **Arxiv ID**: http://arxiv.org/abs/2110.03154v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03154v1)
- **Published**: 2021-10-07 03:06:26+00:00
- **Updated**: 2021-10-07 03:06:26+00:00
- **Authors**: Ce Zhou, Qiben Yan, Yan Shi, Lichao Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Depth estimation-based obstacle avoidance has been widely adopted by autonomous systems (drones and vehicles) for safety purpose. It normally relies on a stereo camera to automatically detect obstacles and make flying/driving decisions, e.g., stopping several meters ahead of the obstacle in the path or moving away from the detected obstacle. In this paper, we explore new security risks associated with the stereo vision-based depth estimation algorithms used for obstacle avoidance. By exploiting the weaknesses of the stereo matching in depth estimation algorithms and the lens flare effect in optical imaging, we propose DoubleStar, a long-range attack that injects fake obstacle depth by projecting pure light from two complementary light sources.   DoubleStar includes two distinctive attack formats: beams attack and orbs attack, which leverage projected light beams and lens flare orbs respectively to cause false depth perception. We successfully attack two commercial stereo cameras designed for autonomous systems (ZED and Intel RealSense). The visualization of fake depth perceived by the stereo cameras illustrates the false stereo matching induced by DoubleStar. We further use Ardupilot to simulate the attack and demonstrate its impact on drones. To validate the attack on real systems, we perform a real-world attack towards a commercial drone equipped with state-of-the-art obstacle avoidance algorithms. Our attack can continuously bring a flying drone to a sudden stop or drift it away across a long distance under various lighting conditions, even bypassing sensor fusion mechanisms. Specifically, our experimental results show that DoubleStar creates fake depth up to 15 meters in distance at night and up to 8 meters during the daytime. To mitigate this newly discovered threat, we provide discussions on potential countermeasures to defend against DoubleStar.



### TreeGCN-ED: Encoding Point Cloud using a Tree-Structured Graph Network
- **Arxiv ID**: http://arxiv.org/abs/2110.03170v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03170v3)
- **Published**: 2021-10-07 03:52:56+00:00
- **Updated**: 2022-09-28 06:21:31+00:00
- **Authors**: Prajwal Singh, Kaustubh Sadekar, Shanmuganathan Raman
- **Comment**: None
- **Journal**: None
- **Summary**: Point cloud is one of the widely used techniques for representing and storing 3D geometric data. In the past several methods have been proposed for processing point clouds. Methods such as PointNet and FoldingNet have shown promising results for tasks like 3D shape classification and segmentation. This work proposes a tree-structured autoencoder framework to generate robust embeddings of point clouds by utilizing hierarchical information using graph convolution. We perform multiple experiments to assess the quality of embeddings generated by the proposed encoder architecture and visualize the t-SNE map to highlight its ability to distinguish between different object classes. We further demonstrate the applicability of the proposed framework in applications like: 3D point cloud completion and Single image-based 3D reconstruction.



### Tile Embedding: A General Representation for Procedural Level Generation via Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.03181v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03181v1)
- **Published**: 2021-10-07 04:48:48+00:00
- **Updated**: 2021-10-07 04:48:48+00:00
- **Authors**: Mrunal Jadhav, Matthew Guzdial
- **Comment**: 8 pages, 5 figures, AIIDE 2021
- **Journal**: Proceedings of the 17th AAAI Conference on Artificial Intelligence
  and Interactive Digital Entertainment 2021 (AIIDE-21)
- **Summary**: In recent years, Procedural Level Generation via Machine Learning (PLGML) techniques have been applied to generate game levels with machine learning. These approaches rely on human-annotated representations of game levels. Creating annotated datasets for games requires domain knowledge and is time-consuming. Hence, though a large number of video games exist, annotated datasets are curated only for a small handful. Thus current PLGML techniques have been explored in limited domains, with Super Mario Bros. as the most common example. To address this problem, we present tile embeddings, a unified, affordance-rich representation for tile-based 2D games. To learn this embedding, we employ autoencoders trained on the visual and semantic information of tiles from a set of existing, human-annotated games. We evaluate this representation on its ability to predict affordances for unseen tiles, and to serve as a PLGML representation for annotated and unannotated games.



### Curating Subject ID Labels using Keypoint Signatures
- **Arxiv ID**: http://arxiv.org/abs/2110.04055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.04055v1)
- **Published**: 2021-10-07 05:23:35+00:00
- **Updated**: 2021-10-07 05:23:35+00:00
- **Authors**: Laurent Chauvin, Matthew Toews
- **Comment**: None
- **Journal**: None
- **Summary**: Subject ID labels are unique, anonymized codes that can be used to group all images of a subject while maintaining anonymity. ID errors may be inadvertently introduced manually error during enrollment and may lead to systematic error into machine learning evaluation (e.g. due to double-dipping) or potential patient misdiagnosis in clinical contexts. Here we describe a highly efficient system for curating subject ID labels in large generic medical image datasets, based on the 3D image keypoint representation, which recently led to the discovery of previously unknown labeling errors in widely-used public brain MRI datasets



### Joint optimization of system design and reconstruction in MIMO radar imaging
- **Arxiv ID**: http://arxiv.org/abs/2110.03218v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03218v1)
- **Published**: 2021-10-07 07:04:57+00:00
- **Updated**: 2021-10-07 07:04:57+00:00
- **Authors**: Tomer Weiss, Nissim Peretz, Sanketh Vedula, Arie Feuer, Alex Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple-input multiple-output (MIMO) radar is one of the leading depth sensing modalities. However, the usage of multiple receive channels lead to relative high costs and prevent the penetration of MIMOs in many areas such as the automotive industry. Over the last years, few studies concentrated on designing reduced measurement schemes and image reconstruction schemes for MIMO radars, however these problems have been so far addressed separately. On the other hand, recent works in optical computational imaging have demonstrated growing success of simultaneous learning-based design of the acquisition and reconstruction schemes, manifesting significant improvement in the reconstruction quality. Inspired by these successes, in this work, we propose to learn MIMO acquisition parameters in the form of receive (Rx) antenna elements locations jointly with an image neural-network based reconstruction. To this end, we propose an algorithm for training the combined acquisition-reconstruction pipeline end-to-end in a differentiable way. We demonstrate the significance of using our learned acquisition parameters with and without the neural-network reconstruction.



### Gradient Step Denoiser for convergent Plug-and-Play
- **Arxiv ID**: http://arxiv.org/abs/2110.03220v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2110.03220v2)
- **Published**: 2021-10-07 07:11:48+00:00
- **Updated**: 2022-02-14 10:55:19+00:00
- **Authors**: Samuel Hurault, Arthur Leclaire, Nicolas Papadakis
- **Comment**: None
- **Journal**: None
- **Summary**: Plug-and-Play methods constitute a class of iterative algorithms for imaging problems where regularization is performed by an off-the-shelf denoiser. Although Plug-and-Play methods can lead to tremendous visual performance for various image problems, the few existing convergence guarantees are based on unrealistic (or suboptimal) hypotheses on the denoiser, or limited to strongly convex data terms. In this work, we propose a new type of Plug-and-Play methods, based on half-quadratic splitting, for which the denoiser is realized as a gradient descent step on a functional parameterized by a deep neural network. Exploiting convergence results for proximal gradient descent algorithms in the non-convex setting, we show that the proposed Plug-and-Play algorithm is a convergent iterative scheme that targets stationary points of an explicit global functional. Besides, experiments show that it is possible to learn such a deep denoiser while not compromising the performance in comparison to other state-of-the-art deep denoisers used in Plug-and-Play schemes. We apply our proximal gradient algorithm to various ill-posed inverse problems, e.g. deblurring, super-resolution and inpainting. For all these applications, numerical results empirically confirm the convergence results. Experiments also show that this new algorithm reaches state-of-the-art performance, both quantitatively and qualitatively.



### Design of an Intelligent Vision Algorithm for Recognition and Classification of Apples in an Orchard Scene
- **Arxiv ID**: http://arxiv.org/abs/2110.03232v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03232v1)
- **Published**: 2021-10-07 07:31:44+00:00
- **Updated**: 2021-10-07 07:31:44+00:00
- **Authors**: Hamid Majidi Balanji, Alaeedin Rahmani Didar, Mohamadali Hadad Derafshi
- **Comment**: 34 pages, 12 figures
- **Journal**: None
- **Summary**: Apple is one of the remarkable fresh fruit that contains a high degree of nutritious and medicinal value. Hand harvesting of apples by seasonal farmworkers increases physical damages on the surface of these fruits, which causes a great loss in marketing quality. The main objective of this study is focused on designing a robust vision algorithm for robotic apple harvesters. The proposed algorithm is able to recognize and classify 4-classes of objects found in an orchard scene including apples, leaves, trunk and branches, and sky into two apples and non-apples classes. 100 digital images of Red Delicious apples and 100 digital images of Golden Delicious apples were selected among 1000 captured images of apples from 18 apple gardens in West Azerbaijan, Iran. An image processing algorithm is proposed for segmentation and extraction of the image classes based on the color characteristics of mentioned classes. Invariant-Momentums were chosen as the extracted features from the segmented classes, e.g. apples. Multilayer Feedforward Neural Networks, MFNNs, were used as an artificial intelligence tool for the recognition and classification of image classes.



### Self-Supervised Depth Completion for Active Stereo
- **Arxiv ID**: http://arxiv.org/abs/2110.03234v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03234v2)
- **Published**: 2021-10-07 07:33:52+00:00
- **Updated**: 2022-01-20 14:32:18+00:00
- **Authors**: Frederik Warburg, Daniel Hernandez-Juarez, Juan Tarrio, Alexander Vakhitov, Ujwal Bonde, Pablo F. Alcantarilla
- **Comment**: Accepted to RAL-ICRA 21
- **Journal**: None
- **Summary**: Active stereo systems are used in many robotic applications that require 3D information. These depth sensors, however, suffer from stereo artefacts and do not provide dense depth estimates.In this work, we present the first self-supervised depth completion method for active stereo systems that predicts accurate dense depth maps. Our system leverages a feature-based visual inertial SLAM system to produce motion estimates and accurate (but sparse) 3D landmarks. The 3D landmarks are used both as model input and as supervision during training. The motion estimates are used in our novel reconstruction loss that relies on a combination of passive and active stereo frames, resulting in significant improvements in textureless areas that are common in indoor environments. Due to the nonexistence of publicly available active stereo datasets, we release a real dataset together with additional information for a publicly available synthetic dataset (TartanAir [42]) needed for active depth completion and prediction. Through rigorous evaluations we show that our method outperforms state of the art on both datasets. Additionally we show how our method obtains more complete, and therefore safer, 3D maps when used in a robotic platform.



### Multimodal Colored Point Cloud to Image Alignment
- **Arxiv ID**: http://arxiv.org/abs/2110.03249v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03249v2)
- **Published**: 2021-10-07 08:12:56+00:00
- **Updated**: 2022-04-12 13:59:55+00:00
- **Authors**: Noam Rotstein, Amit Bracha, Ron Kimmel
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstruction of geometric structures from images using supervised learning suffers from limited available amount of accurate data. One type of such data is accurate real-world RGB-D images. A major challenge in acquiring such ground truth data is the accurate alignment between RGB images and the point cloud measured by a depth scanner. To overcome this difficulty, we consider a differential optimization method that aligns a colored point cloud with a given color image through iterative geometric and color matching. In the proposed framework, the optimization minimizes the photometric difference between the colors of the point cloud and the corresponding colors of the image pixels. Unlike other methods that try to reduce this photometric error, we analyze the computation of the gradient on the image plane and propose a different direct scheme. We assume that the colors produced by the geometric scanner camera and the color camera sensor are different and therefore characterized by different chromatic acquisition properties. Under these multimodal conditions, we find the transformation between the camera image and the point cloud colors. We alternately optimize for aligning the position of the point cloud and matching the different color spaces. The alignments produced by the proposed method are demonstrated on both synthetic data with quantitative evaluation and real scenes with qualitative results.



### An Uncertainty-aware Loss Function for Training Neural Networks with Calibrated Predictions
- **Arxiv ID**: http://arxiv.org/abs/2110.03260v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03260v2)
- **Published**: 2021-10-07 08:31:23+00:00
- **Updated**: 2023-02-06 00:10:14+00:00
- **Authors**: Afshar Shamsi, Hamzeh Asgharnezhad, AmirReza Tajally, Saeid Nahavandi, Henry Leung
- **Comment**: 11 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: Uncertainty quantification of machine learning and deep learning methods plays an important role in enhancing trust to the obtained result. In recent years, a numerous number of uncertainty quantification methods have been introduced. Monte Carlo dropout (MC-Dropout) is one of the most well-known techniques to quantify uncertainty in deep learning methods. In this study, we propose two new loss functions by combining cross entropy with Expected Calibration Error (ECE) and Predictive Entropy (PE). The obtained results clearly show that the new proposed loss functions lead to having a calibrated MC-Dropout method. Our results confirmed the great impact of the new hybrid loss functions for minimising the overlap between the distributions of uncertainty estimates for correct and incorrect predictions without sacrificing the model's overall performance.



### Propagating State Uncertainty Through Trajectory Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2110.03267v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03267v4)
- **Published**: 2021-10-07 08:51:16+00:00
- **Updated**: 2022-07-12 17:58:07+00:00
- **Authors**: Boris Ivanovic, Yifeng Lin, Shubham Shrivastava, Punarjay Chakravarty, Marco Pavone
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA) 2022
  -- 8 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: Uncertainty pervades through the modern robotic autonomy stack, with nearly every component (e.g., sensors, detection, classification, tracking, behavior prediction) producing continuous or discrete probabilistic distributions. Trajectory forecasting, in particular, is surrounded by uncertainty as its inputs are produced by (noisy) upstream perception and its outputs are predictions that are often probabilistic for use in downstream planning. However, most trajectory forecasting methods do not account for upstream uncertainty, instead taking only the most-likely values. As a result, perceptual uncertainties are not propagated through forecasting and predictions are frequently overconfident. To address this, we present a novel method for incorporating perceptual state uncertainty in trajectory forecasting, a key component of which is a new statistical distance-based loss function which encourages predicting uncertainties that better match upstream perception. We evaluate our approach both in illustrative simulations and on large-scale, real-world data, demonstrating its efficacy in propagating perceptual state uncertainty through prediction and producing more calibrated predictions.



### Injecting Planning-Awareness into Prediction and Detection Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2110.03270v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2110.03270v1)
- **Published**: 2021-10-07 08:52:48+00:00
- **Updated**: 2021-10-07 08:52:48+00:00
- **Authors**: Boris Ivanovic, Marco Pavone
- **Comment**: 8 pages, 9 figures. arXiv admin note: substantial text overlap with
  arXiv:2107.10297
- **Journal**: None
- **Summary**: Detecting other agents and forecasting their behavior is an integral part of the modern robotic autonomy stack, especially in safety-critical scenarios entailing human-robot interaction such as autonomous driving. Due to the importance of these components, there has been a significant amount of interest and research in perception and trajectory forecasting, resulting in a wide variety of approaches. Common to most works, however, is the use of the same few accuracy-based evaluation metrics, e.g., intersection-over-union, displacement error, log-likelihood, etc. While these metrics are informative, they are task-agnostic and outputs that are evaluated as equal can lead to vastly different outcomes in downstream planning and decision making. In this work, we take a step back and critically assess current evaluation metrics, proposing task-aware metrics as a better measure of performance in systems where they are deployed. Experiments on an illustrative simulation as well as real-world autonomous driving data validate that our proposed task-aware metrics are able to account for outcome asymmetry and provide a better estimate of a model's closed-loop performance.



### Virtual Multi-Modality Self-Supervised Foreground Matting for Human-Object Interaction
- **Arxiv ID**: http://arxiv.org/abs/2110.03278v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.03278v2)
- **Published**: 2021-10-07 09:03:01+00:00
- **Updated**: 2021-10-22 08:21:09+00:00
- **Authors**: Bo Xu, Han Huang, Cheng Lu, Ziwen Li, Yandong Guo
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: Most existing human matting algorithms tried to separate pure human-only foreground from the background. In this paper, we propose a Virtual Multi-modality Foreground Matting (VMFM) method to learn human-object interactive foreground (human and objects interacted with him or her) from a raw RGB image. The VMFM method requires no additional inputs, e.g. trimap or known background. We reformulate foreground matting as a self-supervised multi-modality problem: factor each input image into estimated depth map, segmentation mask, and interaction heatmap using three auto-encoders. In order to fully utilize the characteristics of each modality, we first train a dual encoder-to-decoder network to estimate the same alpha matte. Then we introduce a self-supervised method: Complementary Learning(CL) to predict deviation probability map and exchange reliable gradients across modalities without label. We conducted extensive experiments to analyze the effectiveness of each modality and the significance of different components in complementary learning. We demonstrate that our model outperforms the state-of-the-art methods.



### MC-LCR: Multi-modal contrastive classification by locally correlated representations for effective face forgery detection
- **Arxiv ID**: http://arxiv.org/abs/2110.03290v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03290v2)
- **Published**: 2021-10-07 09:24:12+00:00
- **Updated**: 2022-04-01 07:03:46+00:00
- **Authors**: Gaojian Wang, Qian Jiang, Xin Jin, Wei Li, Xiaohui Cui
- **Comment**: 20 pages, 12 figures
- **Journal**: None
- **Summary**: As the remarkable development of facial manipulation technologies is accompanied by severe security concerns, face forgery detection has become a recent research hotspot. Most existing detection methods train a binary classifier under global supervision to judge real or fake. However, advanced manipulations only perform small-scale tampering, posing challenges to comprehensively capture subtle and local forgery artifacts, especially in high compression settings and cross-dataset scenarios. To address such limitations, we propose a novel framework named Multi-modal Contrastive Classification by Locally Correlated Representations(MC-LCR), for effective face forgery detection. Instead of specific appearance features, our MC-LCR aims to amplify implicit local discrepancies between authentic and forged faces from both spatial and frequency domains. Specifically, we design the shallow style representation block that measures the pairwise correlation of shallow feature maps, which encodes local style information to extract more discriminative features in the spatial domain. Moreover, we make a key observation that subtle forgery artifacts can be further exposed in the patch-wise phase and amplitude spectrum and exhibit different clues. According to the complementarity of amplitude and phase information, we develop a patch-wise amplitude and phase dual attention module to capture locally correlated inconsistencies with each other in the frequency domain. Besides the above two modules, we further introduce the collaboration of supervised contrastive loss with cross-entropy loss. It helps the network learn more discriminative and generalized representations. Through extensive experiments and comprehensive studies, we achieve state-of-the-art performance and demonstrate the robustness and generalization of our method.



### End-to-End Supermask Pruning: Learning to Prune Image Captioning Models
- **Arxiv ID**: http://arxiv.org/abs/2110.03298v1
- **DOI**: 10.1016/j.patcog.2021.108366
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03298v1)
- **Published**: 2021-10-07 09:34:00+00:00
- **Updated**: 2021-10-07 09:34:00+00:00
- **Authors**: Jia Huei Tan, Chee Seng Chan, Joon Huang Chuah
- **Comment**: Pattern Recognition; In Press
- **Journal**: None
- **Summary**: With the advancement of deep models, research work on image captioning has led to a remarkable gain in raw performance over the last decade, along with increasing model complexity and computational cost. However, surprisingly works on compression of deep networks for image captioning task has received little to no attention. For the first time in image captioning research, we provide an extensive comparison of various unstructured weight pruning methods on three different popular image captioning architectures, namely Soft-Attention, Up-Down and Object Relation Transformer. Following this, we propose a novel end-to-end weight pruning method that performs gradual sparsification based on weight sensitivity to the training loss. The pruning schemes are then extended with encoder pruning, where we show that conducting both decoder pruning and training simultaneously prior to the encoder pruning provides good overall performance. Empirically, we show that an 80% to 95% sparse network (up to 75% reduction in model size) can either match or outperform its dense counterpart. The code and pre-trained models for Up-Down and Object Relation Transformer that are capable of achieving CIDEr scores >120 on the MS-COCO dataset but with only 8.7 MB and 14.5 MB in model size (size reduction of 96% and 94% respectively against dense versions) are publicly available at https://github.com/jiahuei/sparse-image-captioning.



### MPSN: Motion-aware Pseudo Siamese Network for Indoor Video Head Detection in Buildings
- **Arxiv ID**: http://arxiv.org/abs/2110.03302v5
- **DOI**: 10.1016/j.buildenv.2022.109354
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03302v5)
- **Published**: 2021-10-07 09:40:22+00:00
- **Updated**: 2022-12-12 02:38:41+00:00
- **Authors**: Kailai Sun, Xiaoteng Ma, Peng Liu, Qianchuan Zhao
- **Comment**: Published in Building and Environment.Copyright:Elsevier. Paper
  link:https://www.sciencedirect.com/science/article/pii/S036013232200587X
- **Journal**: Building and Environment,Volume 222,2022
- **Summary**: Head detection in the indoor video is an essential component of building occupancy detection. While deep models have achieved remarkable progress in general object detection, they are not satisfying enough in complex indoor scenes. The indoor surveillance video often includes cluttered background objects, among which heads have small scales and diverse poses. In this paper, we propose Motion-aware Pseudo Siamese Network (MPSN), an end-to-end approach that leverages head motion information to guide the deep model to extract effective head features in indoor scenarios. By taking the pixel-wise difference of adjacent frames as the auxiliary input, MPSN effectively enhances human head motion information and removes the irrelevant objects in the background. Compared with prior methods, it achieves superior performance on the two indoor video datasets. Our experiments show that MPSN successfully suppresses static background objects and highlights the moving instances, especially human heads in indoor videos. We also compare different methods to capture head motion, which demonstrates the simplicity and flexibility of MPSN. To validate the robustness of MPSN, we conduct adversarial experiments with a mathematical solution of small perturbations for robust model selection. Finally, for confirming its potential in building control systems, we apply MPSN to occupancy counting. Code is available at https://github.com/pl-share/MPSN.



### Moment evolution equations and moment matching for stochastic image EPDiff
- **Arxiv ID**: http://arxiv.org/abs/2110.03337v2
- **DOI**: None
- **Categories**: **cs.CV**, math.ST, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/2110.03337v2)
- **Published**: 2021-10-07 11:08:11+00:00
- **Updated**: 2022-12-07 08:22:30+00:00
- **Authors**: Alexander Christgau, Alexis Arnaudon, Stefan Sommer
- **Comment**: To appear in: Journal of Mathematical Imaging and Vision
- **Journal**: None
- **Summary**: Models of stochastic image deformation allow study of time-continuous stochastic effects transforming images by deforming the image domain. Applications include longitudinal medical image analysis with both population trends and random subject specific variation. Focusing on a stochastic extension of the LDDMM models with evolutions governed by a stochastic EPDiff equation, we use moment approximations of the corresponding It\^o diffusion to construct estimators for statistical inference in the full stochastic model. We show that this approach, when efficiently implemented with automatic differentiation tools, can successfully estimate parameters encoding the spatial correlation of the noise fields on the image.



### Uncertainty-aware GAN with Adaptive Loss for Robust MRI Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2110.03343v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03343v1)
- **Published**: 2021-10-07 11:29:03+00:00
- **Updated**: 2021-10-07 11:29:03+00:00
- **Authors**: Uddeshya Upadhyay, Viswanath P. Sudarshan, Suyash P. Awate
- **Comment**: Accepted at IEEE ICCV-2021 workshop on Computer Vision for Automated
  Medical Diagnosis
- **Journal**: None
- **Summary**: Image-to-image translation is an ill-posed problem as unique one-to-one mapping may not exist between the source and target images. Learning-based methods proposed in this context often evaluate the performance on test data that is similar to the training data, which may be impractical. This demands robust methods that can quantify uncertainty in the prediction for making informed decisions, especially for critical areas such as medical imaging. Recent works that employ conditional generative adversarial networks (GANs) have shown improved performance in learning photo-realistic image-to-image mappings between the source and the target images. However, these methods do not focus on (i)~robustness of the models to out-of-distribution (OOD)-noisy data and (ii)~uncertainty quantification. This paper proposes a GAN-based framework that (i)~models an adaptive loss function for robustness to OOD-noisy data that automatically tunes the spatially varying norm for penalizing the residuals and (ii)~estimates the per-voxel uncertainty in the predictions. We demonstrate our method on two key applications in medical imaging: (i)~undersampled magnetic resonance imaging (MRI) reconstruction (ii)~MRI modality propagation. Our experiments with two different real-world datasets show that the proposed method (i)~is robust to OOD-noisy test data and provides improved accuracy and (ii)~quantifies voxel-level uncertainty in the predictions.



### MSHCNet: Multi-Stream Hybridized Convolutional Networks with Mixed Statistics in Euclidean/Non-Euclidean Spaces and Its Application to Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.03346v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.03346v1)
- **Published**: 2021-10-07 11:35:40+00:00
- **Updated**: 2021-10-07 11:35:40+00:00
- **Authors**: Shuang He, Haitong Tang, Xia Lu, Hongjie Yan, Nizhuan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: It is well known that hyperspectral images (HSI) contain rich spatial-spectral contextual information, and how to effectively combine both spectral and spatial information using DNN for HSI classification has become a new research hotspot. Compared with CNN with square kernels, GCN have exhibited exciting potential to model spatial contextual structure and conduct flexible convolution on arbitrarily irregular image regions. However, current GCN only using first-order spectral-spatial signatures can result in boundary blurring and isolated misclassification. To address these, we first designed the graph-based second-order pooling (GSOP) operation to obtain contextual nodes information in non-Euclidean space for GCN. Further, we proposed a novel multi-stream hybridized convolutional network (MSHCNet) with combination of first and second order statistics in Euclidean/non-Euclidean spaces to learn and fuse multi-view complementary information to segment HSIs. Specifically, our MSHCNet adopted four parallel streams, which contained G-stream, utilizing the irregular correlation between adjacent land covers in terms of first-order graph in non-Euclidean space; C-stream, adopting convolution operator to learn regular spatial-spectral features in Euclidean space; N-stream, combining first and second order features to learn representative and discriminative regular spatial-spectral features of Euclidean space; S-stream, using GSOP to capture boundary correlations and obtain graph representations from all nodes in graphs of non-Euclidean space. Besides, these feature representations learned from four different streams were fused to integrate the multi-view complementary information for HSI classification. Finally, we evaluated our proposed MSHCNet on three hyperspectral datasets, and experimental results demonstrated that our method significantly outperformed state-of-the-art eight methods.



### Optimized U-Net for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.03352v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03352v2)
- **Published**: 2021-10-07 11:44:09+00:00
- **Updated**: 2021-12-24 09:27:03+00:00
- **Authors**: Michał Futrega, Alexandre Milesi, Michal Marcinkiewicz, Pablo Ribalta
- **Comment**: 15 pages, 7 figures, MICCAI submission, BraTS21 submission
- **Journal**: None
- **Summary**: We propose an optimized U-Net architecture for a brain tumor segmentation task in the BraTS21 challenge. To find the optimal model architecture and the learning schedule, we have run an extensive ablation study to test: deep supervision loss, Focal loss, decoder attention, drop block, and residual connections. Additionally, we have searched for the optimal depth of the U-Net encoder, number of convolutional channels and post-processing strategy. Our method won the validation phase and took third place in the test phase. We have open-sourced the code to reproduce our BraTS21 submission at the NVIDIA Deep Learning Examples GitHub Repository.



### Sparse MoEs meet Efficient Ensembles
- **Arxiv ID**: http://arxiv.org/abs/2110.03360v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.03360v2)
- **Published**: 2021-10-07 11:58:35+00:00
- **Updated**: 2023-07-09 19:30:59+00:00
- **Authors**: James Urquhart Allingham, Florian Wenzel, Zelda E Mariet, Basil Mustafa, Joan Puigcerver, Neil Houlsby, Ghassen Jerfel, Vincent Fortuin, Balaji Lakshminarayanan, Jasper Snoek, Dustin Tran, Carlos Riquelme Ruiz, Rodolphe Jenatton
- **Comment**: 59 pages, 26 figures, 36 tables. Accepted at TMLR
- **Journal**: None
- **Summary**: Machine learning models based on the aggregated outputs of submodels, either at the activation or prediction levels, often exhibit strong performance compared to individual models. We study the interplay of two popular classes of such models: ensembles of neural networks and sparse mixture of experts (sparse MoEs). First, we show that the two approaches have complementary features whose combination is beneficial. This includes a comprehensive evaluation of sparse MoEs in uncertainty related benchmarks. Then, we present Efficient Ensemble of Experts (E$^3$), a scalable and simple ensemble of sparse MoEs that takes the best of both classes of models, while using up to 45% fewer FLOPs than a deep ensemble. Extensive experiments demonstrate the accuracy, log-likelihood, few-shot learning, robustness, and uncertainty improvements of E$^3$ over several challenging vision Transformer-based baselines. E$^3$ not only preserves its efficiency while scaling to models with up to 2.7B parameters, but also provides better predictive performance and uncertainty estimates for larger models.



### A Baseline Framework for Part-level Action Parsing and Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.03368v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03368v2)
- **Published**: 2021-10-07 12:04:59+00:00
- **Updated**: 2022-09-01 13:47:25+00:00
- **Authors**: Xiaodong Chen, Xinchen Liu, Kun Liu, Wu Liu, Tao Mei
- **Comment**: 4 pages, 1 figures, ICCV 2021 Challenge, 2nd place solution for
  Kinetics-TPS Track in ICCV DeeperAction Workshop 2021
- **Journal**: None
- **Summary**: This technical report introduces our 2nd place solution to Kinetics-TPS Track on Part-level Action Parsing in ICCV DeeperAction Workshop 2021. Our entry is mainly based on YOLOF for instance and part detection, HRNet for human pose estimation, and CSN for video-level action recognition and frame-level part state parsing. We describe technical details for the Kinetics-TPS dataset, together with some experimental results. In the competition, we achieved 61.37% mAP on the test set of Kinetics-TPS.



### Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data
- **Arxiv ID**: http://arxiv.org/abs/2110.03374v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03374v6)
- **Published**: 2021-10-07 12:13:00+00:00
- **Updated**: 2022-06-04 14:58:51+00:00
- **Authors**: Jiaxing Huang, Dayan Guan, Aoran Xiao, Shijian Lu
- **Comment**: Accepted to Advances in Neural Information Processing Systems 34
  (NeurIPS 2021)
- **Journal**: None
- **Summary**: Unsupervised domain adaptation aims to align a labeled source domain and an unlabeled target domain, but it requires to access the source data which often raises concerns in data privacy, data portability and data transmission efficiency. We study unsupervised model adaptation (UMA), or called Unsupervised Domain Adaptation without Source Data, an alternative setting that aims to adapt source-trained models towards target distributions without accessing source data. To this end, we design an innovative historical contrastive learning (HCL) technique that exploits historical source hypothesis to make up for the absence of source data in UMA. HCL addresses the UMA challenge from two perspectives. First, it introduces historical contrastive instance discrimination (HCID) that learns from target samples by contrasting their embeddings which are generated by the currently adapted model and the historical models. With the historical models, HCID encourages UMA to learn instance-discriminative target representations while preserving the source hypothesis. Second, it introduces historical contrastive category discrimination (HCCD) that pseudo-labels target samples to learn category-discriminative target representations. Specifically, HCCD re-weights pseudo labels according to their prediction consistency across the current and historical models. Extensive experiments show that HCL outperforms and state-of-the-art methods consistently across a variety of visual tasks and setups.



### Deep Learning Model Explainability for Inspection Accuracy Improvement in the Automotive Industry
- **Arxiv ID**: http://arxiv.org/abs/2110.03384v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03384v1)
- **Published**: 2021-10-07 12:23:00+00:00
- **Updated**: 2021-10-07 12:23:00+00:00
- **Authors**: Anass El Houd, Charbel El Hachem, Loic Painvin
- **Comment**: None
- **Journal**: None
- **Summary**: The welding seams visual inspection is still manually operated by humans in different companies, so the result of the test is still highly subjective and expensive. At present, the integration of deep learning methods for welds classification is a research focus in engineering applications. This work intends to apprehend and emphasize the contribution of deep learning model explainability to the improvement of welding seams classification accuracy and reliability, two of the various metrics affecting the production lines and cost in the automotive industry. For this purpose, we implement a novel hybrid method that relies on combining the model prediction scores and visual explanation heatmap of the model in order to make a more accurate classification of welding seam defects and improve both its performance and its reliability. The results show that the hybrid model performance is relatively above our target performance and helps to increase the accuracy by at least 18%, which presents new perspectives to the developments of deep Learning explainability and interpretability.



### AnoSeg: Anomaly Segmentation Network Using Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.03396v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03396v1)
- **Published**: 2021-10-07 12:36:36+00:00
- **Updated**: 2021-10-07 12:36:36+00:00
- **Authors**: Jouwon Song, Kyeongbo Kong, Ye-In Park, Seong-Gyun Kim, Suk-Ju Kang
- **Comment**: 10 pages, 17 figures
- **Journal**: None
- **Summary**: Anomaly segmentation, which localizes defective areas, is an important component in large-scale industrial manufacturing. However, most recent researches have focused on anomaly detection. This paper proposes a novel anomaly segmentation network (AnoSeg) that can directly generate an accurate anomaly map using self-supervised learning. For highly accurate anomaly segmentation, the proposed AnoSeg considers three novel techniques: Anomaly data generation based on hard augmentation, self-supervised learning with pixel-wise and adversarial losses, and coordinate channel concatenation. First, to generate synthetic anomaly images and reference masks for normal data, the proposed method uses hard augmentation to change the normal sample distribution. Then, the proposed AnoSeg is trained in a self-supervised learning manner from the synthetic anomaly data and normal data. Finally, the coordinate channel, which represents the pixel location information, is concatenated to an input of AnoSeg to consider the positional relationship of each pixel in the image. The estimated anomaly map can also be utilized to improve the performance of anomaly detection. Our experiments show that the proposed method outperforms the state-of-the-art anomaly detection and anomaly segmentation methods for the MVTec AD dataset. In addition, we compared the proposed method with the existing methods through the intersection over union (IoU) metric commonly used in segmentation tasks and demonstrated the superiority of our method for anomaly segmentation.



### Differential Anomaly Detection for Facial Images
- **Arxiv ID**: http://arxiv.org/abs/2110.03464v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03464v1)
- **Published**: 2021-10-07 13:45:13+00:00
- **Updated**: 2021-10-07 13:45:13+00:00
- **Authors**: Mathias Ibsen, Lázaro J. González-Soler, Christian Rathgeb, Pawel Drozdowski, Marta Gomez-Barrero, Christoph Busch
- **Comment**: Accepted at WIFS'21
- **Journal**: None
- **Summary**: Due to their convenience and high accuracy, face recognition systems are widely employed in governmental and personal security applications to automatically recognise individuals. Despite recent advances, face recognition systems have shown to be particularly vulnerable to identity attacks (i.e., digital manipulations and attack presentations). Identity attacks pose a big security threat as they can be used to gain unauthorised access and spread misinformation. In this context, most algorithms for detecting identity attacks generalise poorly to attack types that are unknown at training time. To tackle this problem, we introduce a differential anomaly detection framework in which deep face embeddings are first extracted from pairs of images (i.e., reference and probe) and then combined for identity attack detection. The experimental evaluation conducted over several databases shows a high generalisation capability of the proposed method for detecting unknown attacks in both the digital and physical domains.



### Unsupervised Image Decomposition with Phase-Correlation Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.03473v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03473v3)
- **Published**: 2021-10-07 13:57:33+00:00
- **Updated**: 2022-01-10 10:23:55+00:00
- **Authors**: Angel Villar-Corrales, Sven Behnke
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to decompose scenes into their object components is a desired property for autonomous agents, allowing them to reason and act in their surroundings. Recently, different methods have been proposed to learn object-centric representations from data in an unsupervised manner. These methods often rely on latent representations learned by deep neural networks, hence requiring high computational costs and large amounts of curated data. Such models are also difficult to interpret. To address these challenges, we propose the Phase-Correlation Decomposition Network (PCDNet), a novel model that decomposes a scene into its object components, which are represented as transformed versions of a set of learned object prototypes. The core building block in PCDNet is the Phase-Correlation Cell (PC Cell), which exploits the frequency-domain representation of the images in order to estimate the transformation between an object prototype and its transformed version in the image. In our experiments, we show how PCDNet outperforms state-of-the-art methods for unsupervised object discovery and segmentation on simple benchmark datasets and on more challenging data, while using a small number of learnable parameters and being fully interpretable.



### InfoSeg: Unsupervised Semantic Image Segmentation with Mutual Information Maximization
- **Arxiv ID**: http://arxiv.org/abs/2110.03477v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03477v1)
- **Published**: 2021-10-07 14:01:42+00:00
- **Updated**: 2021-10-07 14:01:42+00:00
- **Authors**: Robert Harb, Patrick Knöbelreiter
- **Comment**: GCPR 2021 - Best Paper
- **Journal**: None
- **Summary**: We propose a novel method for unsupervised semantic image segmentation based on mutual information maximization between local and global high-level image features. The core idea of our work is to leverage recent progress in self-supervised image representation learning. Representation learning methods compute a single high-level feature capturing an entire image. In contrast, we compute multiple high-level features, each capturing image segments of one particular semantic class. To this end, we propose a novel two-step learning procedure comprising a segmentation and a mutual information maximization step. In the first step, we segment images based on local and global features. In the second step, we maximize the mutual information between local features and high-level features of their respective class. For training, we provide solely unlabeled images and start from random network initialization. For quantitative and qualitative evaluation, we use established benchmarks, and COCO-Persons, whereby we introduce the latter in this paper as a challenging novel benchmark. InfoSeg significantly outperforms the current state-of-the-art, e.g., we achieve a relative increase of 26% in the Pixel Accuracy metric on the COCO-Stuff dataset.



### Camera Calibration through Camera Projection Loss
- **Arxiv ID**: http://arxiv.org/abs/2110.03479v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03479v4)
- **Published**: 2021-10-07 14:03:10+00:00
- **Updated**: 2022-12-19 11:36:47+00:00
- **Authors**: Talha Hanif Butt, Murtaza Taj
- **Comment**: 5 pages, ICASSP 2022
- **Journal**: None
- **Summary**: Camera calibration is a necessity in various tasks including 3D reconstruction, hand-eye coordination for a robotic interaction, autonomous driving, etc. In this work we propose a novel method to predict extrinsic (baseline, pitch, and translation), intrinsic (focal length and principal point offset) parameters using an image pair. Unlike existing methods, instead of designing an end-to-end solution, we proposed a new representation that incorporates camera model equations as a neural network in multi-task learning framework. We estimate the desired parameters via novel camera projection loss (CPL) that uses the camera model neural network to reconstruct the 3D points and uses the reconstruction loss to estimate the camera parameters. To the best of our knowledge, ours is the first method to jointly estimate both the intrinsic and extrinsic parameters via a multi-task learning methodology that combines analytical equations in learning framework for the estimation of camera parameters. We also proposed a novel dataset using CARLA Simulator. Empirically, we demonstrate that our proposed approach achieves better performance with respect to both deep learning-based and traditional methods on 8 out of 10 parameters evaluated using both synthetic and real data. Our code and generated dataset are available at https://github.com/thanif/Camera-Calibration-through-Camera-Projection-Loss.



### Learning to Regress Bodies from Images using Differentiable Semantic Rendering
- **Arxiv ID**: http://arxiv.org/abs/2110.03480v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03480v2)
- **Published**: 2021-10-07 14:03:29+00:00
- **Updated**: 2022-02-23 16:02:59+00:00
- **Authors**: Sai Kumar Dwivedi, Nikos Athanasiou, Muhammed Kocabas, Michael J. Black
- **Comment**: ICCV2021
- **Journal**: None
- **Summary**: Learning to regress 3D human body shape and pose (e.g.~SMPL parameters) from monocular images typically exploits losses on 2D keypoints, silhouettes, and/or part-segmentation when 3D training data is not available. Such losses, however, are limited because 2D keypoints do not supervise body shape and segmentations of people in clothing do not match projected minimally-clothed SMPL shapes. To exploit richer image information about clothed people, we introduce higher-level semantic information about clothing to penalize clothed and non-clothed regions of the image differently. To do so, we train a body regressor using a novel Differentiable Semantic Rendering - DSR loss. For Minimally-Clothed regions, we define the DSR-MC loss, which encourages a tight match between a rendered SMPL body and the minimally-clothed regions of the image. For clothed regions, we define the DSR-C loss to encourage the rendered SMPL body to be inside the clothing mask. To ensure end-to-end differentiable training, we learn a semantic clothing prior for SMPL vertices from thousands of clothed human scans. We perform extensive qualitative and quantitative experiments to evaluate the role of clothing semantics on the accuracy of 3D human pose and shape estimation. We outperform all previous state-of-the-art methods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP. Code and trained models are available for research at https://dsr.is.tue.mpg.de/.



### Cartoon Explanations of Image Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2110.03485v5
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03485v5)
- **Published**: 2021-10-07 14:12:28+00:00
- **Updated**: 2022-10-20 11:18:55+00:00
- **Authors**: Stefan Kolek, Duc Anh Nguyen, Ron Levie, Joan Bruna, Gitta Kutyniok
- **Comment**: ECCV 2022 (oral)
- **Journal**: None
- **Summary**: We present CartoonX (Cartoon Explanation), a novel model-agnostic explanation method tailored towards image classifiers and based on the rate-distortion explanation (RDE) framework. Natural images are roughly piece-wise smooth signals -- also called cartoon-like images -- and tend to be sparse in the wavelet domain. CartoonX is the first explanation method to exploit this by requiring its explanations to be sparse in the wavelet domain, thus extracting the relevant piece-wise smooth part of an image instead of relevant pixel-sparse regions. We demonstrate that CartoonX can reveal novel valuable explanatory information, particularly for misclassifications. Moreover, we show that CartoonX achieves a lower distortion with fewer coefficients than other state-of-the-art methods.



### Scale Invariant Domain Generalization Image Recapture Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.03496v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03496v1)
- **Published**: 2021-10-07 14:32:56+00:00
- **Updated**: 2021-10-07 14:32:56+00:00
- **Authors**: Jinian Luo, Jie Guo, Weidong Qiu, Zheng Huang, Hong Hui
- **Comment**: None
- **Journal**: None
- **Summary**: Recapturing and rebroadcasting of images are common attack methods in insurance frauds and face identification spoofing, and an increasing number of detection techniques were introduced to handle this problem. However, most of them ignored the domain generalization scenario and scale variances, with an inferior performance on domain shift situations, and normally were exacerbated by intra-domain and inter-domain scale variances. In this paper, we propose a scale alignment domain generalization framework (SADG) to address these challenges. First, an adversarial domain discriminator is exploited to minimize the discrepancies of image representation distributions among different domains. Meanwhile, we exploit triplet loss as a local constraint to achieve a clearer decision boundary. Moreover, a scale alignment loss is introduced as a global relationship regularization to force the image representations of the same class across different scales to be undistinguishable. Experimental results on four databases and comparison with state-of-the-art approaches show that better performance can be achieved using our framework.



### A Multi-viewpoint Outdoor Dataset for Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.04119v1
- **DOI**: 10.1109/THMS.2020.2971958
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.04119v1)
- **Published**: 2021-10-07 14:50:43+00:00
- **Updated**: 2021-10-07 14:50:43+00:00
- **Authors**: Asanka G. Perera, Yee Wei Law, Titilayo T. Ogunwa, Javaan Chahl
- **Comment**: 10 pages, 4 figures
- **Journal**: IEEE Transactions on Human-Machine Systems, Volume: 50, Issue: 5,
  Oct. 2020
- **Summary**: Advancements in deep neural networks have contributed to near perfect results for many computer vision problems such as object recognition, face recognition and pose estimation. However, human action recognition is still far from human-level performance. Owing to the articulated nature of the human body, it is challenging to detect an action from multiple viewpoints, particularly from an aerial viewpoint. This is further compounded by a scarcity of datasets that cover multiple viewpoints of actions. To fill this gap and enable research in wider application areas, we present a multi-viewpoint outdoor action recognition dataset collected from YouTube and our own drone. The dataset consists of 20 dynamic human action classes, 2324 video clips and 503086 frames. All videos are cropped and resized to 720x720 without distorting the original aspect ratio of the human subjects in videos. This dataset should be useful to many research areas including action recognition, surveillance and situational awareness. We evaluated the dataset with a two-stream CNN architecture coupled with a recently proposed temporal pooling scheme called kernelized rank pooling that produces nonlinear feature subspace representations. The overall baseline action recognition accuracy is 74.0%.



### RAR: Region-Aware Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2110.03544v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03544v2)
- **Published**: 2021-10-07 15:06:52+00:00
- **Updated**: 2023-07-19 11:46:34+00:00
- **Authors**: Yu Hao, Yi Fang
- **Comment**: arXiv admin note: text overlap with arXiv:2006.06200
- **Journal**: None
- **Summary**: This paper concerns the research problem of point cloud registration to find the rigid transformation to optimally align the source point set with the target one. Learning robust point cloud registration models with deep neural networks has emerged as a powerful paradigm, offering promising performance in predicting the global geometric transformation for a pair of point sets. Existing methods firstly leverage an encoder to regress a latent shape embedding, which is then decoded into a shape-conditioned transformation via concatenation-based conditioning. However, different regions of a 3D shape vary in their geometric structures which makes it more sense that we have a region-conditioned transformation instead of the shape-conditioned one. In this paper we present a \underline{R}egion-\underline{A}ware point cloud \underline{R}egistration, denoted as RAR, to predict transformation for pairwise point sets in the self-supervised learning fashion. More specifically, we develop a novel region-aware decoder (RAD) module that is formed with an implicit neural region representation parameterized by neural networks. The implicit neural region representation is learned with a self-supervised 3D shape reconstruction loss without the need for region labels. Consequently, the region-aware decoder (RAD) module guides the training of the region-aware transformation (RAT) module and region-aware weight (RAW) module, which predict the transforms and weights for different regions respectively. The global geometric transformation from source point set to target one is then formed by the weighted fusion of region-aware transforms. Compared to the state-of-the-art approaches, our experiments show that our RAR achieves superior registration performance over various benchmark datasets (e.g. ModelNet40).



### Weakly Supervised Human-Object Interaction Detection in Video via Contrastive Spatiotemporal Regions
- **Arxiv ID**: http://arxiv.org/abs/2110.03562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03562v1)
- **Published**: 2021-10-07 15:30:18+00:00
- **Updated**: 2021-10-07 15:30:18+00:00
- **Authors**: Shuang Li, Yilun Du, Antonio Torralba, Josef Sivic, Bryan Russell
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the task of weakly supervised learning for detecting human and object interactions in videos. Our task poses unique challenges as a system does not know what types of human-object interactions are present in a video or the actual spatiotemporal location of the human and the object. To address these challenges, we introduce a contrastive weakly supervised training loss that aims to jointly associate spatiotemporal regions in a video with an action and object vocabulary and encourage temporal continuity of the visual appearance of moving objects as a form of self-supervision. To train our model, we introduce a dataset comprising over 6.5k videos with human-object interaction annotations that have been semi-automatically curated from sentence captions associated with the videos. We demonstrate improved performance over weakly supervised baselines adapted to our task on our video dataset.



### A New Simple Vision Algorithm for Detecting the Enzymic Browning Defects in Golden Delicious Apples
- **Arxiv ID**: http://arxiv.org/abs/2110.03574v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03574v1)
- **Published**: 2021-10-07 15:53:41+00:00
- **Updated**: 2021-10-07 15:53:41+00:00
- **Authors**: Hamid Majidi Balanji
- **Comment**: 23 pages, 11 figures, Conference
- **Journal**: None
- **Summary**: In this work, a simple vision algorithm is designed and implemented to extract and identify the surface defects on the Golden Delicious apples caused by the enzymic browning process. 34 Golden Delicious apples were selected for the experiments, of which 17 had enzymic browning defects and the other 17 were sound. The image processing part of the proposed vision algorithm extracted the defective surface area of the apples with high accuracy of 97.15%. The area and mean of the segmented images were selected as the 2x1 feature vectors to feed into a designed artificial neural network. The analysis based on the above features indicated that the images with a mean less than 0.0065 did not belong to the defective apples; rather, they were extracted as part of the calyx and stem of the healthy apples. The classification accuracy of the neural network applied in this study was 99.19%



### Estimating Image Depth in the Comics Domain
- **Arxiv ID**: http://arxiv.org/abs/2110.03575v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03575v2)
- **Published**: 2021-10-07 15:54:27+00:00
- **Updated**: 2022-08-15 14:05:17+00:00
- **Authors**: Deblina Bhattacharjee, Martin Everaert, Mathieu Salzmann, Sabine Süsstrunk
- **Comment**: Accepted to WACV 2022 : Winter Conference on Applications of Computer
  Vision
- **Journal**: None
- **Summary**: Estimating the depth of comics images is challenging as such images a) are monocular; b) lack ground-truth depth annotations; c) differ across different artistic styles; d) are sparse and noisy. We thus, use an off-the-shelf unsupervised image to image translation method to translate the comics images to natural ones and then use an attention-guided monocular depth estimator to predict their depth. This lets us leverage the depth annotations of existing natural images to train the depth estimator. Furthermore, our model learns to distinguish between text and images in the comics panels to reduce text-based artefacts in the depth estimates. Our method consistently outperforms the existing state-ofthe-art approaches across all metrics on both the DCM and eBDtheque images. Finally, we introduce a dataset to evaluate depth prediction on comics. Our project website can be accessed at https://github.com/IVRL/ComicsDepth.



### Towards Accurate Cross-Domain In-Bed Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2110.03578v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03578v1)
- **Published**: 2021-10-07 15:54:46+00:00
- **Updated**: 2021-10-07 15:54:46+00:00
- **Authors**: Mohamed Afham, Udith Haputhanthri, Jathurshan Pradeepkumar, Mithunjha Anandakumar, Ashwin De Silva, Chamira Edussooriya
- **Comment**: Code is available at https://github.com/MohamedAfham/CD_HPE
- **Journal**: None
- **Summary**: Human behavioral monitoring during sleep is essential for various medical applications. Majority of the contactless human pose estimation algorithms are based on RGB modality, causing ineffectiveness in in-bed pose estimation due to occlusions by blankets and varying illumination conditions. Long-wavelength infrared (LWIR) modality based pose estimation algorithms overcome the aforementioned challenges; however, ground truth pose generations by a human annotator under such conditions are not feasible. A feasible solution to address this issue is to transfer the knowledge learned from images with pose labels and no occlusions, and adapt it towards real world conditions (occlusions due to blankets). In this paper, we propose a novel learning strategy comprises of two-fold data augmentation to reduce the cross-domain discrepancy and knowledge distillation to learn the distribution of unlabeled images in real world conditions. Our experiments and analysis show the effectiveness of our approach over multiple standard human pose estimation baselines.



### A transformer-based deep learning approach for classifying brain metastases into primary organ sites using clinical whole brain MRI
- **Arxiv ID**: http://arxiv.org/abs/2110.03588v6
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2110.03588v6)
- **Published**: 2021-10-07 16:10:44+00:00
- **Updated**: 2022-04-20 22:51:26+00:00
- **Authors**: Qing Lyu, Sanjeev V. Namjoshi, Emory McTyre, Umit Topaloglu, Richard Barcus, Michael D. Chan, Christina K. Cramer, Waldemar Debinski, Metin N. Gurcan, Glenn J. Lesser, Hui-Kuan Lin, Reginald F. Munden, Boris C. Pasche, Kiran Kumar Solingapuram Sai, Roy E. Strowd, Stephen B. Tatter, Kounosuke Watabe, Wei Zhang, Ge Wang, Christopher T. Whitlow
- **Comment**: None
- **Journal**: None
- **Summary**: Treatment decisions for brain metastatic disease rely on knowledge of the primary organ site, and currently made with biopsy and histology. Here we develop a novel deep learning approach for accurate non-invasive digital histology with whole-brain MRI data. Our IRB-approved single-site retrospective study was comprised of patients (n=1,399) referred for MRI treatment-planning and gamma knife radiosurgery over 21 years. Contrast-enhanced T1-weighted and T2-weighted Fluid-Attenuated Inversion Recovery brain MRI exams (n=1,582) were preprocessed and input to the proposed deep learning workflow for tumor segmentation, modality transfer, and primary site classification into one of five classes. Ten-fold cross-validation generated overall AUC of 0.878 (95%CI:0.873,0.883), lung class AUC of 0.889 (95%CI:0.883,0.895), breast class AUC of 0.873 (95%CI:0.860,0.886), melanoma class AUC of 0.852 (95%CI:0.842,0.862), renal class AUC of 0.830 (95%CI:0.809,0.851), and other class AUC of 0.822 (95%CI:0.805,0.839). These data establish that whole-brain imaging features are discriminative to allow accurate diagnosis of the primary organ site of malignancy. Our end-to-end deep radiomic approach has great potential for classifying metastatic tumor types from whole-brain MRI images. Further refinement may offer an invaluable clinical tool to expedite primary cancer site identification for precision treatment and improved outcomes.



### TranSalNet: Towards perceptually relevant visual saliency prediction
- **Arxiv ID**: http://arxiv.org/abs/2110.03593v3
- **DOI**: 10.1016/j.neucom.2022.04.080
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03593v3)
- **Published**: 2021-10-07 16:17:38+00:00
- **Updated**: 2022-06-29 16:44:57+00:00
- **Authors**: Jianxun Lou, Hanhe Lin, David Marshall, Dietmar Saupe, Hantao Liu
- **Comment**: Source code: https://github.com/LJOVO/TranSalNet
- **Journal**: Neurocomputing, Volume 494, 14 July 2022, Pages 455-467
- **Summary**: Visual saliency prediction using transformers - Convolutional neural networks (CNNs) have significantly advanced computational modelling for saliency prediction. However, accurately simulating the mechanisms of visual attention in the human cortex remains an academic challenge. It is critical to integrate properties of human vision into the design of CNN architectures, leading to perceptually more relevant saliency prediction. Due to the inherent inductive biases of CNN architectures, there is a lack of sufficient long-range contextual encoding capacity. This hinders CNN-based saliency models from capturing properties that emulate viewing behaviour of humans. Transformers have shown great potential in encoding long-range information by leveraging the self-attention mechanism. In this paper, we propose a novel saliency model that integrates transformer components to CNNs to capture the long-range contextual visual information. Experimental results show that the transformers provide added value to saliency prediction, enhancing its perceptual relevance in the performance. Our proposed saliency model using transformers has achieved superior results on public benchmarks and competitions for saliency prediction models.   The source code of our proposed saliency model TranSalNet is available at: https://github.com/LJOVO/TranSalNet



### Learning Higher-Order Dynamics in Video-Based Cardiac Measurement
- **Arxiv ID**: http://arxiv.org/abs/2110.03690v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03690v2)
- **Published**: 2021-10-07 16:29:55+00:00
- **Updated**: 2022-03-27 22:55:13+00:00
- **Authors**: Brian L. Hill, Xin Liu, Daniel McDuff
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision methods typically optimize for first-order dynamics (e.g., optical flow). However, in many cases the properties of interest are subtle variations in higher-order changes, such as acceleration. This is true in the cardiac pulse, where the second derivative can be used as an indicator of blood pressure and arterial disease. Recent developments in camera-based vital sign measurement have shown that cardiac measurements can be recovered with impressive accuracy from videos; however, most of the research has focused on extracting summary statistics such as heart rate. Less emphasis has been put on the accuracy of waveform morphology that is necessary for many clinically meaningful assessments. In this work, we provide evidence that higher-order dynamics are better estimated by neural models when explicitly optimized for in the loss function. Furthermore, adding second-derivative inputs also improves performance when estimating second-order dynamics. We illustrate this, by showing that incorporating the second derivative of both the input frames and the target vital sign signals into the training procedure, models are better able to estimate left ventricle ejection time (LVET) intervals.



### Robust Feature-Level Adversaries are Interpretability Tools
- **Arxiv ID**: http://arxiv.org/abs/2110.03605v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03605v6)
- **Published**: 2021-10-07 16:33:11+00:00
- **Updated**: 2023-01-07 23:40:24+00:00
- **Authors**: Stephen Casper, Max Nadeau, Dylan Hadfield-Menell, Gabriel Kreiman
- **Comment**: Code available at
  https://github.com/thestephencasper/feature_level_adv
- **Journal**: None
- **Summary**: The literature on adversarial attacks in computer vision typically focuses on pixel-level perturbations. These tend to be very difficult to interpret. Recent work that manipulates the latent representations of image generators to create "feature-level" adversarial perturbations gives us an opportunity to explore perceptible, interpretable adversarial attacks. We make three contributions. First, we observe that feature-level attacks provide useful classes of inputs for studying representations in models. Second, we show that these adversaries are uniquely versatile and highly robust. We demonstrate that they can be used to produce targeted, universal, disguised, physically-realizable, and black-box attacks at the ImageNet scale. Third, we show how these adversarial images can be used as a practical interpretability tool for identifying bugs in networks. We use these adversaries to make predictions about spurious associations between features and classes which we then test by designing "copy/paste" attacks in which one natural image is pasted into another to cause a targeted misclassification. Our results suggest that feature-level attacks are a promising approach for rigorous interpretability research. They support the design of tools to better understand what a model has learned and diagnose brittle feature associations. Code is available at https://github.com/thestephencasper/feature_level_adv



### Boxhead: A Dataset for Learning Hierarchical Representations
- **Arxiv ID**: http://arxiv.org/abs/2110.03628v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.03628v2)
- **Published**: 2021-10-07 17:15:25+00:00
- **Updated**: 2021-12-06 17:34:26+00:00
- **Authors**: Yukun Chen, Andrea Dittadi, Frederik Träuble, Stefan Bauer, Bernhard Schölkopf
- **Comment**: NeurIPS 2021 Workshop on Shared Visual Representations in Human and
  Machine Intelligence (SVRHM 2021)
- **Journal**: None
- **Summary**: Disentanglement is hypothesized to be beneficial towards a number of downstream tasks. However, a common assumption in learning disentangled representations is that the data generative factors are statistically independent. As current methods are almost solely evaluated on toy datasets where this ideal assumption holds, we investigate their performance in hierarchical settings, a relevant feature of real-world data. In this work, we introduce Boxhead, a dataset with hierarchically structured ground-truth generative factors. We use this novel dataset to evaluate the performance of state-of-the-art autoencoder-based disentanglement models and observe that hierarchical models generally outperform single-layer VAEs in terms of disentanglement of hierarchically arranged factors.



### Flow Plugin Network for conditional generation
- **Arxiv ID**: http://arxiv.org/abs/2110.04081v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.04081v1)
- **Published**: 2021-10-07 17:26:57+00:00
- **Updated**: 2021-10-07 17:26:57+00:00
- **Authors**: Patryk Wielopolski, Michał Koperski, Maciej Zięba
- **Comment**: None
- **Journal**: None
- **Summary**: Generative models have gained many researchers' attention in the last years resulting in models such as StyleGAN for human face generation or PointFlow for the 3D point cloud generation. However, by default, we cannot control its sampling process, i.e., we cannot generate a sample with a specific set of attributes. The current approach is model retraining with additional inputs and different architecture, which requires time and computational resources. We propose a novel approach that enables to a generation of objects with a given set of attributes without retraining the base model. For this purpose, we utilize the normalizing flow models - Conditional Masked Autoregressive Flow and Conditional Real NVP, as a Flow Plugin Network (FPN).



### Using Contrastive Learning and Pseudolabels to learn representations for Retail Product Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.03639v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03639v1)
- **Published**: 2021-10-07 17:29:05+00:00
- **Updated**: 2021-10-07 17:29:05+00:00
- **Authors**: Muktabh Mayank Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Retail product Image classification problems are often few shot classification problems, given retail product classes cannot have the type of variations across images like a cat or dog or tree could have. Previous works have shown different methods to finetune Convolutional Neural Networks to achieve better classification accuracy on such datasets. In this work, we try to address the problem statement : Can we pretrain a Convolutional Neural Network backbone which yields good enough representations for retail product images, so that training a simple logistic regression on these representations gives us good classifiers ? We use contrastive learning and pseudolabel based noisy student training to learn representations that get accuracy in order of finetuning the entire Convnet backbone for retail product image classification.



### Using Keypoint Matching and Interactive Self Attention Network to verify Retail POSMs
- **Arxiv ID**: http://arxiv.org/abs/2110.03646v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03646v1)
- **Published**: 2021-10-07 17:37:09+00:00
- **Updated**: 2021-10-07 17:37:09+00:00
- **Authors**: Harshita Seth, Sonaal Kant, Muktabh Mayank Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Point of Sale Materials(POSM) are the merchandising and decoration items that are used by companies to communicate product information and offers in retail stores. POSMs are part of companies' retail marketing strategy and are often applied as stylized window displays around retail shelves. In this work, we apply computer vision techniques to the task of verification of POSMs in supermarkets by telling if all desired components of window display are present in a shelf image. We use Convolutional Neural Network based unsupervised keypoint matching as a baseline to verify POSM components and propose a supervised Neural Network based method to enhance the accuracy of baseline by a large margin. We also show that the supervised pipeline is not restricted to the POSM material it is trained on and can generalize. We train and evaluate our model on a private dataset composed of retail shelf images.



### Pre-training Molecular Graph Representation with 3D Geometry
- **Arxiv ID**: http://arxiv.org/abs/2110.07728v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2110.07728v2)
- **Published**: 2021-10-07 17:48:57+00:00
- **Updated**: 2022-05-29 13:01:37+00:00
- **Authors**: Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, Jian Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Molecular graph representation learning is a fundamental problem in modern drug and material discovery. Molecular graphs are typically modeled by their 2D topological structures, but it has been recently discovered that 3D geometric information plays a more vital role in predicting molecular functionalities. However, the lack of 3D information in real-world scenarios has significantly impeded the learning of geometric graph representation. To cope with this challenge, we propose the Graph Multi-View Pre-training (GraphMVP) framework where self-supervised learning (SSL) is performed by leveraging the correspondence and consistency between 2D topological structures and 3D geometric views. GraphMVP effectively learns a 2D molecular graph encoder that is enhanced by richer and more discriminative 3D geometry. We further provide theoretical insights to justify the effectiveness of GraphMVP. Finally, comprehensive experiments show that GraphMVP can consistently outperform existing graph SSL methods.



### Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design
- **Arxiv ID**: http://arxiv.org/abs/2110.03659v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.03659v3)
- **Published**: 2021-10-07 17:51:05+00:00
- **Updated**: 2022-04-09 16:56:11+00:00
- **Authors**: Ye Yuan, Yuda Song, Zhengyi Luo, Wen Sun, Kris Kitani
- **Comment**: ICLR 2022 (Oral). Project page:
  https://sites.google.com/view/transform2act. Code:
  https://github.com/Khrylx/Transform2Act
- **Journal**: None
- **Summary**: An agent's functionality is largely determined by its design, i.e., skeletal structure and joint attributes (e.g., length, size, strength). However, finding the optimal agent design for a given function is extremely challenging since the problem is inherently combinatorial and the design space is prohibitively large. Additionally, it can be costly to evaluate each candidate design which requires solving for its optimal controller. To tackle these problems, our key idea is to incorporate the design procedure of an agent into its decision-making process. Specifically, we learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent's skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, we use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Using policy gradient methods, our approach enables joint optimization of agent design and control as well as experience sharing across different designs, which improves sample efficiency substantially. Experiments show that our approach, Transform2Act, outperforms prior methods significantly in terms of convergence speed and final performance. Notably, Transform2Act can automatically discover plausible designs similar to giraffes, squids, and spiders. Code and videos are available at https://sites.google.com/view/transform2act.



### Dense Gaussian Processes for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.03674v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03674v2)
- **Published**: 2021-10-07 17:57:54+00:00
- **Updated**: 2022-08-31 08:55:16+00:00
- **Authors**: Joakim Johnander, Johan Edstedt, Michael Felsberg, Fahad Shahbaz Khan, Martin Danelljan
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot segmentation is a challenging dense prediction task, which entails segmenting a novel query image given only a small annotated support set. The key problem is thus to design a method that aggregates detailed information from the support set, while being robust to large variations in appearance and context. To this end, we propose a few-shot segmentation method based on dense Gaussian process (GP) regression. Given the support set, our dense GP learns the mapping from local deep image features to mask values, capable of capturing complex appearance distributions. Furthermore, it provides a principled means of capturing uncertainty, which serves as another powerful cue for the final segmentation, obtained by a CNN decoder. Instead of a one-dimensional mask output, we further exploit the end-to-end learning capabilities of our approach to learn a high-dimensional output space for the GP. Our approach sets a new state-of-the-art on the PASCAL-5$^i$ and COCO-20$^i$ benchmarks, achieving an absolute gain of $+8.4$ mIoU in the COCO-20$^i$ 5-shot setting. Furthermore, the segmentation quality of our approach scales gracefully when increasing the support set size, while achieving robust cross-dataset transfer. Code and trained models are available at \url{https://github.com/joakimjohnander/dgpnet}.



### ATISS: Autoregressive Transformers for Indoor Scene Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2110.03675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03675v1)
- **Published**: 2021-10-07 17:58:05+00:00
- **Updated**: 2021-10-07 17:58:05+00:00
- **Authors**: Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, Sanja Fidler
- **Comment**: To appear in NeurIPS 2021, Project Page:
  https://nv-tlabs.github.io/ATISS/
- **Journal**: None
- **Summary**: The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8 times faster than existing methods.



### Burst Image Restoration and Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2110.03680v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03680v2)
- **Published**: 2021-10-07 17:58:56+00:00
- **Updated**: 2022-04-14 10:41:45+00:00
- **Authors**: Akshay Dudhane, Syed Waqas Zamir, Salman Khan, Fahad Shahbaz Khan, Ming-Hsuan Yang
- **Comment**: Accepted at CVPR 2022 [Oral]
- **Journal**: None
- **Summary**: Modern handheld devices can acquire burst image sequence in a quick succession. However, the individual acquired frames suffer from multiple degradations and are misaligned due to camera shake and object motions. The goal of Burst Image Restoration is to effectively combine complimentary cues across multiple burst frames to generate high-quality outputs. Towards this goal, we develop a novel approach by solely focusing on the effective information exchange between burst frames, such that the degradations get filtered out while the actual scene details are preserved and enhanced. Our central idea is to create a set of pseudo-burst features that combine complementary information from all the input burst frames to seamlessly exchange information. However, the pseudo-burst cannot be successfully created unless the individual burst frames are properly aligned to discount inter-frame movements. Therefore, our approach initially extracts pre-processed features from each burst frame and matches them using an edge-boosting burst alignment module. The pseudo-burst features are then created and enriched using multi-scale contextual information. Our final step is to adaptively aggregate information from the pseudo-burst features to progressively increase resolution in multiple stages while merging the pseudo-burst features. In comparison to existing works that usually follow a late fusion scheme with single-stage upsampling, our approach performs favorably, delivering state-of-the-art performance on burst superresolution, burst low-light image enhancement, and burst denoising tasks. The source code and pre-trained models are available at \url{https://github.com/akshaydudhane16/BIPNet}.



### SVG-Net: An SVG-based Trajectory Prediction Model
- **Arxiv ID**: http://arxiv.org/abs/2110.03706v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T45 68T07, I.2.10; I.5.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2110.03706v2)
- **Published**: 2021-10-07 18:00:08+00:00
- **Updated**: 2021-10-11 18:00:58+00:00
- **Authors**: Mohammadhossein Bahari, Vahid Zehtab, Sadegh Khorasani, Sana Ayromlou, Saeed Saadatnejad, Alexandre Alahi
- **Comment**: None
- **Journal**: None
- **Summary**: Anticipating motions of vehicles in a scene is an essential problem for safe autonomous driving systems. To this end, the comprehension of the scene's infrastructure is often the main clue for predicting future trajectories. Most of the proposed approaches represent the scene with a rasterized format and some of the more recent approaches leverage custom vectorized formats. In contrast, we propose representing the scene's information by employing Scalable Vector Graphics (SVG). SVG is a well-established format that matches the problem of trajectory prediction better than rasterized formats while being more general than arbitrary vectorized formats. SVG has the potential to provide the convenience and generality of raster-based solutions if coupled with a powerful tool such as CNNs, for which we introduce SVG-Net. SVG-Net is a Transformer-based Neural Network that can effectively capture the scene's information from SVG inputs. Thanks to the self-attention mechanism in its Transformers, SVG-Net can also adequately apprehend relations amongst the scene and the agents. We demonstrate SVG-Net's effectiveness by evaluating its performance on the publicly available Argoverse forecasting dataset. Finally, we illustrate how, by using SVG, one can benefit from datasets and advancements in other research fronts that also utilize the same input format. Our code is available at https://vita-epfl.github.io/SVGNet/.



### Adversarial Unlearning of Backdoors via Implicit Hypergradient
- **Arxiv ID**: http://arxiv.org/abs/2110.03735v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03735v4)
- **Published**: 2021-10-07 18:32:54+00:00
- **Updated**: 2022-02-06 21:26:28+00:00
- **Authors**: Yi Zeng, Si Chen, Won Park, Z. Morley Mao, Ming Jin, Ruoxi Jia
- **Comment**: In proceeding of the Tenth International Conference on Learning
  Representations (ICLR 2022)
- **Journal**: None
- **Summary**: We propose a minimax formulation for removing backdoors from a given poisoned model based on a small set of clean data. This formulation encompasses much of prior work on backdoor removal. We propose the Implicit Bacdoor Adversarial Unlearning (I-BAU) algorithm to solve the minimax. Unlike previous work, which breaks down the minimax into separate inner and outer problems, our algorithm utilizes the implicit hypergradient to account for the interdependence between inner and outer optimization. We theoretically analyze its convergence and the generalizability of the robustness gained by solving minimax on clean data to unseen test data. In our evaluation, we compare I-BAU with six state-of-art backdoor defenses on seven backdoor attacks over two datasets and various attack settings, including the common setting where the attacker targets one class as well as important but underexplored settings where multiple classes are targeted. I-BAU's performance is comparable to and most often significantly better than the best baseline. Particularly, its performance is more robust to the variation on triggers, attack settings, poison ratio, and clean data size. Moreover, I-BAU requires less computation to take effect; particularly, it is more than $13\times$ faster than the most efficient baseline in the single-target attack setting. Furthermore, it can remain effective in the extreme case where the defender can only access 100 clean samples -- a setting where all the baselines fail to produce acceptable results.



### Adaptive Early-Learning Correction for Segmentation from Noisy Annotations
- **Arxiv ID**: http://arxiv.org/abs/2110.03740v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03740v2)
- **Published**: 2021-10-07 18:46:23+00:00
- **Updated**: 2022-03-06 06:03:54+00:00
- **Authors**: Sheng Liu, Kangning Liu, Weicheng Zhu, Yiqiu Shen, Carlos Fernandez-Granda
- **Comment**: The first two authors contribute equally, order decided by coin
  flipping. Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Deep learning in the presence of noisy annotations has been studied extensively in classification, but much less in segmentation tasks. In this work, we study the learning dynamics of deep segmentation networks trained on inaccurately-annotated data. We discover a phenomenon that has been previously reported in the context of classification: the networks tend to first fit the clean pixel-level labels during an "early-learning" phase, before eventually memorizing the false annotations. However, in contrast to classification, memorization in segmentation does not arise simultaneously for all semantic categories. Inspired by these findings, we propose a new method for segmentation from noisy annotations with two key elements. First, we detect the beginning of the memorization phase separately for each category during training. This allows us to adaptively correct the noisy annotations in order to exploit early learning. Second, we incorporate a regularization term that enforces consistency across scales to boost robustness against annotation noise. Our method outperforms standard approaches on a medical-imaging segmentation task where noises are synthesized to mimic human annotation errors. It also provides robustness to realistic noisy annotations present in weakly-supervised semantic segmentation, achieving state-of-the-art results on PASCAL VOC 2012. Code is available at https://github.com/Kangningthu/ADELE



### Adversarial Attack by Limited Point Cloud Surface Modifications
- **Arxiv ID**: http://arxiv.org/abs/2110.03745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03745v1)
- **Published**: 2021-10-07 18:58:18+00:00
- **Updated**: 2021-10-07 18:58:18+00:00
- **Authors**: Atrin Arya, Hanieh Naderi, Shohreh Kasaei
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research has revealed that the security of deep neural networks that directly process 3D point clouds to classify objects can be threatened by adversarial samples. Although existing adversarial attack methods achieve high success rates, they do not restrict the point modifications enough to preserve the point cloud appearance. To overcome this shortcoming, two constraints are proposed. These include applying hard boundary constraints on the number of modified points and on the point perturbation norms. Due to the restrictive nature of the problem, the search space contains many local maxima. The proposed method addresses this issue by using a high step-size at the beginning of the algorithm to search the main surface of the point cloud fast and effectively. Then, in order to converge to the desired output, the step-size is gradually decreased. To evaluate the performance of the proposed method, it is run on the ModelNet40 and ScanObjectNN datasets by employing the state-of-the-art point cloud classification models; including PointNet, PointNet++, and DGCNN. The obtained results show that it can perform successful attacks and achieve state-of-the-art results by only a limited number of point modifications while preserving the appearance of the point cloud. Moreover, due to the effective search algorithm, it can perform successful attacks in just a few steps. Additionally, the proposed step-size scheduling algorithm shows an improvement of up to $14.5\%$ when adopted by other methods as well. The proposed method also performs effectively against popular defense methods.



### Machine Learning approaches to do size based reasoning on Retail Shelf objects to classify product variants
- **Arxiv ID**: http://arxiv.org/abs/2110.03783v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03783v1)
- **Published**: 2021-10-07 20:29:07+00:00
- **Updated**: 2021-10-07 20:29:07+00:00
- **Authors**: Muktabh Mayank Srivastava, Pratyush Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: There has been a surge in the number of Machine Learning methods to analyze products kept on retail shelves images. Deep learning based computer vision methods can be used to detect products on retail shelves and then classify them. However, there are different sized variants of products which look exactly the same visually and the method to differentiate them is to look at their relative sizes with other products on shelves. This makes the process of deciphering the sized based variants from each other using computer vision algorithms alone impractical. In this work, we propose methods to ascertain the size variant of the product as a downstream task to an object detector which extracts products from shelf and a classifier which determines product brand. Product variant determination is the task which assigns a product variant to products of a brand based on the size of bounding boxes and brands predicted by classifier. While gradient boosting based methods work well for products whose facings are clear and distinct, a noise accommodating Neural Network method is proposed for cases where the products are stacked irregularly.



### Efficient large-scale image retrieval with deep feature orthogonality and Hybrid-Swin-Transformers
- **Arxiv ID**: http://arxiv.org/abs/2110.03786v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03786v2)
- **Published**: 2021-10-07 20:41:13+00:00
- **Updated**: 2021-10-27 08:30:13+00:00
- **Authors**: Christof Henkel
- **Comment**: None
- **Journal**: None
- **Summary**: We present an efficient end-to-end pipeline for largescale landmark recognition and retrieval. We show how to combine and enhance concepts from recent research in image retrieval and introduce two architectures especially suited for large-scale landmark identification. A model with deep orthogonal fusion of local and global features (DOLG) using an EfficientNet backbone as well as a novel Hybrid-Swin-Transformer is discussed and details how to train both architectures efficiently using a step-wise approach and a sub-center arcface loss with dynamic margins are provided. Furthermore, we elaborate a novel discriminative re-ranking methodology for image retrieval. The superiority of our approach was demonstrated by winning the recognition and retrieval track of the Google Landmark Competition 2021.



### A Probabilistic Graphical Model Approach to the Structure-and-Motion Problem
- **Arxiv ID**: http://arxiv.org/abs/2110.03792v1
- **DOI**: 10.1109/RoboMech.2016.7813137
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.03792v1)
- **Published**: 2021-10-07 21:04:38+00:00
- **Updated**: 2021-10-07 21:04:38+00:00
- **Authors**: Simon Streicher, Willie Brink, Johan du Preez
- **Comment**: None
- **Journal**: PRASA-RobMech International Conference; paper 8; 2016
- **Summary**: We present a means of formulating and solving the well known structure-and-motion problem in computer vision with probabilistic graphical models. We model the unknown camera poses and 3D feature coordinates as well as the observed 2D projections as Gaussian random variables, using sigma point parameterizations to effectively linearize the nonlinear relationships between these variables. Those variables involved in every projection are grouped into a cluster, and we connect the clusters in a cluster graph. Loopy belief propagation is performed over this graph, in an iterative re-initialization and estimation procedure, and we find that our approach shows promise in both simulation and on real-world data. The PGM is easily extendable to include additional parameters or constraints.



### FOCUS: Familiar Objects in Common and Uncommon Settings
- **Arxiv ID**: http://arxiv.org/abs/2110.03804v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03804v2)
- **Published**: 2021-10-07 21:39:33+00:00
- **Updated**: 2022-07-14 16:44:11+00:00
- **Authors**: Priyatham Kattakinda, Soheil Feizi
- **Comment**: 23 pages, 14 figures, 4 tables. Accepted to ICML 2022
- **Journal**: None
- **Summary**: Standard training datasets for deep learning often contain objects in common settings (e.g., "a horse on grass" or "a ship in water") since they are usually collected by randomly scraping the web. Uncommon and rare settings (e.g., "a plane on water", "a car in snowy weather") are thus severely under-represented in the training data. This can lead to an undesirable bias in model predictions towards common settings and create a false sense of accuracy. In this paper, we introduce FOCUS (Familiar Objects in Common and Uncommon Settings), a dataset for stress-testing the generalization power of deep image classifiers. By leveraging the power of modern search engines, we deliberately gather data containing objects in common and uncommon settings in a wide range of locations, weather conditions, and time of day. We present a detailed analysis of the performance of various popular image classifiers on our dataset and demonstrate a clear drop in performance when classifying images in uncommon settings. By analyzing deep features of these models, we show that such errors can be due to the use of spurious features in model predictions. We believe that our dataset will aid researchers in understanding the inability of deep models to generalize well to uncommon settings and drive future work on improving their distributional robustness.



### StyleGAN-induced data-driven regularization for inverse problems
- **Arxiv ID**: http://arxiv.org/abs/2110.03814v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03814v1)
- **Published**: 2021-10-07 22:25:30+00:00
- **Updated**: 2021-10-07 22:25:30+00:00
- **Authors**: Arthur Conmy, Subhadip Mukherjee, Carola-Bibiane Schönlieb
- **Comment**: Submitted to IEEE ICASSP 2022. Under review
- **Journal**: None
- **Summary**: Recent advances in generative adversarial networks (GANs) have opened up the possibility of generating high-resolution photo-realistic images that were impossible to produce previously. The ability of GANs to sample from high-dimensional distributions has naturally motivated researchers to leverage their power for modeling the image prior in inverse problems. We extend this line of research by developing a Bayesian image reconstruction framework that utilizes the full potential of a pre-trained StyleGAN2 generator, which is the currently dominant GAN architecture, for constructing the prior distribution on the underlying image. Our proposed approach, which we refer to as learned Bayesian reconstruction with generative models (L-BRGM), entails joint optimization over the style-code and the input latent code, and enhances the expressive power of a pre-trained StyleGAN2 generator by allowing the style-codes to be different for different generator layers. Considering the inverse problems of image inpainting and super-resolution, we demonstrate that the proposed approach is competitive with, and sometimes superior to, state-of-the-art GAN-based image reconstruction methods.



### Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.03825v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.03825v5)
- **Published**: 2021-10-07 23:13:33+00:00
- **Updated**: 2022-01-23 03:21:33+00:00
- **Authors**: Hanxun Huang, Yisen Wang, Sarah Monazam Erfani, Quanquan Gu, James Bailey, Xingjun Ma
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks. A range of defense methods have been proposed to train adversarially robust DNNs, among which adversarial training has demonstrated promising results. However, despite preliminary understandings developed for adversarial training, it is still not clear, from the architectural perspective, what configurations can lead to more robust DNNs. In this paper, we address this gap via a comprehensive investigation on the impact of network width and depth on the robustness of adversarially trained DNNs. Specifically, we make the following key observations: 1) more parameters (higher model capacity) does not necessarily help adversarial robustness; 2) reducing capacity at the last stage (the last group of blocks) of the network can actually improve adversarial robustness; and 3) under the same parameter budget, there exists an optimal architectural configuration for adversarial robustness. We also provide a theoretical analysis explaning why such network configuration can help robustness. These architectural insights can help design adversarially robust DNNs. Code is available at \url{https://github.com/HanxunH/RobustWRN}.



### SkullEngine: A Multi-stage CNN Framework for Collaborative CBCT Image Segmentation and Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.03828v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03828v2)
- **Published**: 2021-10-07 23:25:33+00:00
- **Updated**: 2021-12-21 16:45:49+00:00
- **Authors**: Qin Liu, Han Deng, Chunfeng Lian, Xiaoyang Chen, Deqiang Xiao, Lei Ma, Xu Chen, Tianshu Kuang, Jaime Gateno, Pew-Thian Yap, James J. Xia
- **Comment**: 10 pages, 5 figures, accepted by MLMI 2021
- **Journal**: None
- **Summary**: We propose a multi-stage coarse-to-fine CNN-based framework, called SkullEngine, for high-resolution segmentation and large-scale landmark detection through a collaborative, integrated, and scalable JSD model and three segmentation and landmark detection refinement models. We evaluated our framework on a clinical dataset consisting of 170 CBCT/CT images for the task of segmenting 2 bones (midface and mandible) and detecting 175 clinically common landmarks on bones, teeth, and soft tissues.



