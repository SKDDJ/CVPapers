# Arxiv Papers in cs.CV on 2021-10-31
### Progressive observation of Covid-19 vaccination effects on skin-cellular structures by use of Intelligent Laser Speckle Classification (ILSC)
- **Arxiv ID**: http://arxiv.org/abs/2111.01682v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.01682v1)
- **Published**: 2021-10-31 00:01:40+00:00
- **Updated**: 2021-10-31 00:01:40+00:00
- **Authors**: Ahmet Orun, Fatih Kurugollu
- **Comment**: None
- **Journal**: None
- **Summary**: We have made a progressive observation of Covid-19 Astra Zeneca Vaccination effect on Skin cellular network and properties by use of well established Intelligent Laser Speckle Classification (ILSC) image based technique and managed to distinguish between three different subjects groups via their laser speckle skin image samplings such as early-vaccinated, late-vaccinated and non-vaccinated individuals. The results have proven that the ILSC technique in association with the optimised Bayesian network is capable of classifying skin changes of vaccinated and non-vaccinated individuals and also of detecting progressive development made on skin cellular properties for a month period.



### Dual Attention Network for Heart Rate and Respiratory Rate Estimation
- **Arxiv ID**: http://arxiv.org/abs/2111.00390v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2111.00390v1)
- **Published**: 2021-10-31 03:00:28+00:00
- **Updated**: 2021-10-31 03:00:28+00:00
- **Authors**: Yuzhuo Ren, Braeden Syrnyk, Niranjan Avadhanam
- **Comment**: None
- **Journal**: None
- **Summary**: Heart rate and respiratory rate measurement is a vital step for diagnosing many diseases. Non-contact camera based physiological measurement is more accessible and convenient in Telehealth nowadays than contact instruments such as fingertip oximeters since non-contact methods reduce risk of infection. However, remote physiological signal measurement is challenging due to environment illumination variations, head motion, facial expression, etc. It's also desirable to have a unified network which could estimate both heart rate and respiratory rate to reduce system complexity and latency. We propose a convolutional neural network which leverages spatial attention and channel attention, which we call it dual attention network (DAN) to jointly estimate heart rate and respiratory rate with camera video as input. Extensive experiments demonstrate that our proposed system significantly improves heart rate and respiratory rate measurement accuracy.



### A robust single-pixel particle image velocimetry based on fully convolutional networks with cross-correlation embedded
- **Arxiv ID**: http://arxiv.org/abs/2111.00395v1
- **DOI**: 10.1063/5.0077146
- **Categories**: **physics.flu-dyn**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.00395v1)
- **Published**: 2021-10-31 03:26:08+00:00
- **Updated**: 2021-10-31 03:26:08+00:00
- **Authors**: Qi Gao, Hongtao Lin, Han Tu, Haoran Zhu, Runjie Wei, Guoping Zhang, Xueming Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Particle image velocimetry (PIV) is essential in experimental fluid dynamics. In the current work, we propose a new velocity field estimation paradigm, which achieves a synergetic combination of the deep learning method and the traditional cross-correlation method. Specifically, the deep learning method is used to optimize and correct a coarse velocity guess to achieve a super-resolution calculation. And the cross-correlation method provides the initial velocity field based on a coarse correlation with a large interrogation window. As a reference, the coarse velocity guess helps with improving the robustness of the proposed algorithm. This fully convolutional network with embedded cross-correlation is named as CC-FCN. CC-FCN has two types of input layers, one is for the particle images, and the other is for the initial velocity field calculated using cross-correlation with a coarse resolution. Firstly, two pyramidal modules extract features of particle images and initial velocity field respectively. Then the fusion module appropriately fuses these features. Finally, CC-FCN achieves the super-resolution calculation through a series of deconvolution layers to obtain the single-pixel velocity field. As the supervised learning strategy is considered, synthetic data sets including ground-truth fluid motions are generated to train the network parameters. Synthetic and real experimental PIV data sets are used to test the trained neural network in terms of accuracy, precision, spatial resolution and robustness. The test results show that these attributes of CC-FCN are further improved compared with those of other tested PIV algorithms. The proposed model could therefore provide competitive and robust estimations for PIV experiments.



### A Simple Approach to Image Tilt Correction with Self-Attention MobileNet for Smartphones
- **Arxiv ID**: http://arxiv.org/abs/2111.00398v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.00398v1)
- **Published**: 2021-10-31 03:41:46+00:00
- **Updated**: 2021-10-31 03:41:46+00:00
- **Authors**: Siddhant Garg, Debi Prasanna Mohanty, Siva Prasad Thota, Sukumar Moharana
- **Comment**: Accepted - British Machine vision Conference 2021
- **Journal**: None
- **Summary**: The main contributions of our work are two-fold. First, we present a Self-Attention MobileNet, called SA-MobileNet Network that can model long-range dependencies between the image features instead of processing the local region as done by standard convolutional kernels. SA-MobileNet contains self-attention modules integrated with the inverted bottleneck blocks of the MobileNetV3 model which results in modeling of both channel-wise attention and spatial attention of the image features and at the same time introduce a novel self-attention architecture for low-resource devices. Secondly, we propose a novel training pipeline for the task of image tilt detection. We treat this problem in a multi-label scenario where we predict multiple angles for a tilted input image in a narrow interval of range 1-2 degrees, depending on the dataset used. This process induces an implicit correlation between labels without any computational overhead of the second or higher-order methods in multi-label learning. With the combination of our novel approach and the architecture, we present state-of-the-art results on detecting the image tilt angle on mobile devices as compared to the MobileNetV3 model. Finally, we establish that SA-MobileNet is more accurate than MobileNetV3 on SUN397, NYU-V1, and ADE20K datasets by 6.42%, 10.51%, and 9.09% points respectively, and faster by at least 4 milliseconds on Snapdragon 750 Octa-core.



### PANet: Perspective-Aware Network with Dynamic Receptive Fields and Self-Distilling Supervision for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2111.00406v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.00406v2)
- **Published**: 2021-10-31 04:43:05+00:00
- **Updated**: 2022-08-18 09:51:20+00:00
- **Authors**: Xiaoshuang Chen, Yiru Zhao, Yu Qin, Fei Jiang, Mingyuan Tao, Xiansheng Hua, Hongtao Lu
- **Comment**: The paper is under consideration at Computer Vision and Image
  Understanding
- **Journal**: None
- **Summary**: Crowd counting aims to learn the crowd density distributions and estimate the number of objects (e.g. persons) in images. The perspective effect, which significantly influences the distribution of data points, plays an important role in crowd counting. In this paper, we propose a novel perspective-aware approach called PANet to address the perspective problem. Based on the observation that the size of the objects varies greatly in one image due to the perspective effect, we propose the dynamic receptive fields (DRF) framework. The framework is able to adjust the receptive field by the dilated convolution parameters according to the input image, which helps the model to extract more discriminative features for each local region. Different from most previous works which use Gaussian kernels to generate the density map as the supervised information, we propose the self-distilling supervision (SDS) training method. The ground-truth density maps are refined from the first training stage and the perspective information is distilled to the model in the second stage. The experimental results on ShanghaiTech Part_A and Part_B, UCF_QNRF, and UCF_CC_50 datasets demonstrate that our proposed PANet outperforms the state-of-the-art methods by a large margin.



### Hierarchical Deep Residual Reasoning for Temporal Moment Localization
- **Arxiv ID**: http://arxiv.org/abs/2111.00417v1
- **DOI**: 10.1145/3469877.3490595
- **Categories**: **cs.MM**, cs.CL, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2111.00417v1)
- **Published**: 2021-10-31 07:13:34+00:00
- **Updated**: 2021-10-31 07:13:34+00:00
- **Authors**: Ziyang Ma, Xianjing Han, Xuemeng Song, Yiran Cui, Liqiang Nie
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal Moment Localization (TML) in untrimmed videos is a challenging task in the field of multimedia, which aims at localizing the start and end points of the activity in the video, described by a sentence query. Existing methods mainly focus on mining the correlation between video and sentence representations or investigating the fusion manner of the two modalities. These works mainly understand the video and sentence coarsely, ignoring the fact that a sentence can be understood from various semantics, and the dominant words affecting the moment localization in the semantics are the action and object reference. Toward this end, we propose a Hierarchical Deep Residual Reasoning (HDRR) model, which decomposes the video and sentence into multi-level representations with different semantics to achieve a finer-grained localization. Furthermore, considering that videos with different resolution and sentences with different length have different difficulty in understanding, we design the simple yet effective Res-BiGRUs for feature fusion, which is able to grasp the useful information in a self-adapting manner. Extensive experiments conducted on Charades-STA and ActivityNet-Captions datasets demonstrate the superiority of our HDRR model compared with other state-of-the-art methods.



### Loop closure detection using local 3D deep descriptors
- **Arxiv ID**: http://arxiv.org/abs/2111.00440v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.00440v2)
- **Published**: 2021-10-31 09:18:38+00:00
- **Updated**: 2022-02-27 14:48:13+00:00
- **Authors**: Youjie Zhou, Yiming Wang, Fabio Poiesi, Qi Qin, Yi Wan
- **Comment**: This work is accepted for publication in IEEE Robotics and Automation
  Letters
- **Journal**: None
- **Summary**: We present a simple yet effective method to address loop closure detection in simultaneous localisation and mapping using local 3D deep descriptors (L3Ds). L3Ds are emerging compact representations of patches extracted from point clouds that are learnt from data using a deep learning algorithm. We propose a novel overlap measure for loop detection by computing the metric error between points that correspond to mutually-nearest-neighbour descriptors after registering the loop candidate point cloud by its estimated relative pose. This novel approach enables us to accurately detect loops and estimate six degrees-of-freedom poses in the case of small overlaps. We compare our L3D-based loop closure approach with recent approaches on LiDAR data and achieve state-of-the-art loop closure detection accuracy. Additionally, we embed our loop closure approach in RESLAM, a recent edge-based SLAM system, and perform the evaluation on real-world RGBD-TUM and synthetic ICL datasets. Our approach enables RESLAM to achieve a better localisation accuracy compared to its original loop closure strategy. Our project page is available at github.com/yiming107/l3d_loop_closure.



### Gaussian Kernel Mixture Network for Single Image Defocus Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2111.00454v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.00454v1)
- **Published**: 2021-10-31 10:16:09+00:00
- **Updated**: 2021-10-31 10:16:09+00:00
- **Authors**: Yuhui Quan, Zicong Wu, Hui Ji
- **Comment**: Accepted by NeurIPS 2021
- **Journal**: None
- **Summary**: Defocus blur is one kind of blur effects often seen in images, which is challenging to remove due to its spatially variant amount. This paper presents an end-to-end deep learning approach for removing defocus blur from a single image, so as to have an all-in-focus image for consequent vision tasks. First, a pixel-wise Gaussian kernel mixture (GKM) model is proposed for representing spatially variant defocus blur kernels in an efficient linear parametric form, with higher accuracy than existing models. Then, a deep neural network called GKMNet is developed by unrolling a fixed-point iteration of the GKM-based deblurring. The GKMNet is built on a lightweight scale-recurrent architecture, with a scale-recurrent attention module for estimating the mixing coefficients in GKM for defocus deblurring. Extensive experiments show that the GKMNet not only noticeably outperforms existing defocus deblurring methods, but also has its advantages in terms of model complexity and computational efficiency.



### Rethinking the Knowledge Distillation From the Perspective of Model Calibration
- **Arxiv ID**: http://arxiv.org/abs/2111.01684v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01684v2)
- **Published**: 2021-10-31 12:18:56+00:00
- **Updated**: 2021-11-03 07:16:02+00:00
- **Authors**: Lehan Yang, Jincen Song
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have witnessed dramatically improvements in the knowledge distillation, which can generate a compact student model for better efficiency while retaining the model effectiveness of the teacher model. Previous studies find that: more accurate teachers do not necessary make for better teachers due to the mismatch of abilities. In this paper, we aim to analysis the phenomenon from the perspective of model calibration. We found that the larger teacher model may be too over-confident, thus the student model cannot effectively imitate. While, after the simple model calibration of the teacher model, the size of the teacher model has a positive correlation with the performance of the student model.



### IGCN: Image-to-graph Convolutional Network for 2D/3D Deformable Registration
- **Arxiv ID**: http://arxiv.org/abs/2111.00484v1
- **DOI**: 10.1109/TMI.2022.3194517
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.00484v1)
- **Published**: 2021-10-31 12:48:37+00:00
- **Updated**: 2021-10-31 12:48:37+00:00
- **Authors**: Megumi Nakao, Mitsuhiro Nakamura, Tetsuya Matsuda
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging, 2022
- **Summary**: Organ shape reconstruction based on a single-projection image during treatment has wide clinical scope, e.g., in image-guided radiotherapy and surgical guidance. We propose an image-to-graph convolutional network that achieves deformable registration of a 3D organ mesh for a single-viewpoint 2D projection image. This framework enables simultaneous training of two types of transformation: from the 2D projection image to a displacement map, and from the sampled per-vertex feature to a 3D displacement that satisfies the geometrical constraint of the mesh structure. Assuming application to radiation therapy, the 2D/3D deformable registration performance is verified for multiple abdominal organs that have not been targeted to date, i.e., the liver, stomach, duodenum, and kidney, and for pancreatic cancer. The experimental results show shape prediction considering relationships among multiple organs can be used to predict respiratory motion and deformation from digitally reconstructed radiographs with clinically acceptable accuracy.



### Learned Image Compression with Separate Hyperprior Decoders
- **Arxiv ID**: http://arxiv.org/abs/2111.00485v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.00485v1)
- **Published**: 2021-10-31 13:01:56+00:00
- **Updated**: 2021-10-31 13:01:56+00:00
- **Authors**: Zhao Zan, Chao Liu, Heming Sun, Xiaoyang Zeng, Yibo Fan
- **Comment**: This paper has been accepted by IEEE Open Journal of Circuits and
  Systems
- **Journal**: None
- **Summary**: Learned image compression techniques have achieved considerable development in recent years. In this paper, we find that the performance bottleneck lies in the use of a single hyperprior decoder, in which case the ternary Gaussian model collapses to a binary one. To solve this, we propose to use three hyperprior decoders to separate the decoding process of the mixed parameters in discrete Gaussian mixture likelihoods, achieving more accurate parameters estimation. Experimental results demonstrate the proposed method optimized by MS-SSIM achieves on average 3.36% BD-rate reduction compared with state-of-the-art approach. The contribution of the proposed method to the coding time and FLOPs is negligible.



### Smart(Sampling)Augment: Optimal and Efficient Data Augmentation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.00487v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.00487v1)
- **Published**: 2021-10-31 13:04:45+00:00
- **Updated**: 2021-10-31 13:04:45+00:00
- **Authors**: Misgana Negassi, Diane Wagner, Alexander Reiterer
- **Comment**: Negassi and Wagner provided an equal contribution
- **Journal**: None
- **Summary**: Data augmentation methods enrich datasets with augmented data to improve the performance of neural networks. Recently, automated data augmentation methods have emerged, which automatically design augmentation strategies. Existing work focuses on image classification and object detection, whereas we provide the first study on semantic image segmentation and introduce two new approaches: \textit{SmartAugment} and \textit{SmartSamplingAugment}. SmartAugment uses Bayesian Optimization to search over a rich space of augmentation strategies and achieves a new state-of-the-art performance in all semantic segmentation tasks we consider. SmartSamplingAugment, a simple parameter-free approach with a fixed augmentation strategy competes in performance with the existing resource-intensive approaches and outperforms cheap state-of-the-art data augmentation methods. Further, we analyze the impact, interaction, and importance of data augmentation hyperparameters and perform ablation studies, which confirm our design choices behind SmartAugment and SmartSamplingAugment. Lastly, we will provide our source code for reproducibility and to facilitate further research.



### DPNET: Dual-Path Network for Efficient Object Detectioj with Lightweight Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/2111.00500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.00500v1)
- **Published**: 2021-10-31 13:38:16+00:00
- **Updated**: 2021-10-31 13:38:16+00:00
- **Authors**: Huimin Shi, Quan Zhou, Yinghao Ni, Xiaofu Wu, Longin Jan Latecki
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection often costs a considerable amount of computation to get satisfied performance, which is unfriendly to be deployed in edge devices. To address the trade-off between computational cost and detection accuracy, this paper presents a dual path network, named DPNet, for efficient object detection with lightweight self-attention. In backbone, a single input/output lightweight self-attention module (LSAM) is designed to encode global interactions between different positions. LSAM is also extended into a multiple-inputs version in feature pyramid network (FPN), which is employed to capture cross-resolution dependencies in two paths. Extensive experiments on the COCO dataset demonstrate that our method achieves state-of-the-art detection results. More specifically, DPNet obtains 29.0% AP on COCO test-dev, with only 1.14 GFLOPs and 2.27M model size for a 320x320 image.



### Fully convolutional Siamese neural networks for buildings damage assessment from satellite images
- **Arxiv ID**: http://arxiv.org/abs/2111.00508v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.00508v1)
- **Published**: 2021-10-31 14:18:59+00:00
- **Updated**: 2021-10-31 14:18:59+00:00
- **Authors**: Eugene Khvedchenya, Tatiana Gabruseva
- **Comment**: None
- **Journal**: None
- **Summary**: Damage assessment after natural disasters is needed to distribute aid and forces to recovery from damage dealt optimally. This process involves acquiring satellite imagery for the region of interest, localization of buildings, and classification of the amount of damage caused by nature or urban factors to buildings. In case of natural disasters, this means processing many square kilometers of the area to judge whether a particular building had suffered from the damaging factors.   In this work, we develop a computational approach for an automated comparison of the same region's satellite images before and after the disaster, and classify different levels of damage in buildings. Our solution is based on Siamese neural networks with encoder-decoder architecture. We include an extensive ablation study and compare different encoders, decoders, loss functions, augmentations, and several methods to combine two images. The solution achieved one of the best results in the Computer Vision for Building Damage Assessment competition.



### DRBANET: A Lightweight Dual-Resolution Network for Semantic Segmentation with Boundary Auxiliary
- **Arxiv ID**: http://arxiv.org/abs/2111.00509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.00509v1)
- **Published**: 2021-10-31 14:20:02+00:00
- **Updated**: 2021-10-31 14:20:02+00:00
- **Authors**: Linjie Wang, Quan Zhou, Chenfeng Jiang, Xiaofu Wu, Longin Jan Latecki
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the powerful ability to encode image details and semantics, many lightweight dual-resolution networks have been proposed in recent years. However, most of them ignore the benefit of boundary information. This paper introduces a lightweight dual-resolution network, called DRBANet, aiming to refine semantic segmentation results with the aid of boundary information. DRBANet adopts dual parallel architecture, including: high resolution branch (HRB) and low resolution branch (LRB). Specifically, HRB mainly consists of a set of Efficient Inverted Bottleneck Modules (EIBMs), which learn feature representations with larger receptive fields. LRB is composed of a series of EIBMs and an Extremely Lightweight Pyramid Pooling Module (ELPPM), where ELPPM is utilized to capture multi-scale context through hierarchical residual connections. Finally, a boundary supervision head is designed to capture object boundaries in HRB. Extensive experiments on Cityscapes and CamVid datasets demonstrate that our method achieves promising trade-off between segmentation accuracy and running efficiency.



### Calibrating the Dice loss to handle neural network overconfidence for biomedical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.00528v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2111.00528v2)
- **Published**: 2021-10-31 16:02:02+00:00
- **Updated**: 2022-11-01 09:26:05+00:00
- **Authors**: Michael Yeung, Leonardo Rundo, Yang Nan, Evis Sala, Carola-Bibiane Schönlieb, Guang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The Dice similarity coefficient (DSC) is both a widely used metric and loss function for biomedical image segmentation due to its robustness to class imbalance. However, it is well known that the DSC loss is poorly calibrated, resulting in overconfident predictions that cannot be usefully interpreted in biomedical and clinical practice. Performance is often the only metric used to evaluate segmentations produced by deep neural networks, and calibration is often neglected. However, calibration is important for translation into biomedical and clinical practice, providing crucial contextual information to model predictions for interpretation by scientists and clinicians. In this study, we provide a simple yet effective extension of the DSC loss, named the DSC++ loss, that selectively modulates the penalty associated with overconfident, incorrect predictions. As a standalone loss function, the DSC++ loss achieves significantly improved calibration over the conventional DSC loss across six well-validated open-source biomedical imaging datasets, including both 2D binary and 3D multi-class segmentation tasks. Similarly, we observe significantly improved calibration when integrating the DSC++ loss into four DSC-based loss functions. Finally, we use softmax thresholding to illustrate that well calibrated outputs enable tailoring of recall-precision bias, which is an important post-processing technique to adapt the model predictions to suit the biomedical or clinical task. The DSC++ loss overcomes the major limitation of the DSC loss, providing a suitable loss function for training deep learning segmentation models for use in biomedical and clinical practice. Source code is available at: https://github.com/mlyg/DicePlusPlus.



### Learning Debiased and Disentangled Representations for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.00531v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.00531v1)
- **Published**: 2021-10-31 16:15:09+00:00
- **Updated**: 2021-10-31 16:15:09+00:00
- **Authors**: Sanghyeok Chu, Dongwan Kim, Bohyung Han
- **Comment**: Accepted by NeurIPS 2021
- **Journal**: None
- **Summary**: Deep neural networks are susceptible to learn biased models with entangled feature representations, which may lead to subpar performances on various downstream tasks. This is particularly true for under-represented classes, where a lack of diversity in the data exacerbates the tendency. This limitation has been addressed mostly in classification tasks, but there is little study on additional challenges that may appear in more complex dense prediction problems including semantic segmentation. To this end, we propose a model-agnostic and stochastic training scheme for semantic segmentation, which facilitates the learning of debiased and disentangled representations. For each class, we first extract class-specific information from the highly entangled feature map. Then, information related to a randomly sampled class is suppressed by a feature selection process in the feature space. By randomly eliminating certain class information in each training iteration, we effectively reduce feature dependencies among classes, and the model is able to learn more debiased and disentangled feature representations. Models trained with our approach demonstrate strong results on multiple semantic segmentation benchmarks, with especially notable performance gains on under-represented classes.



### Incorporating Boundary Uncertainty into loss functions for biomedical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.00533v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2111.00533v1)
- **Published**: 2021-10-31 16:19:57+00:00
- **Updated**: 2021-10-31 16:19:57+00:00
- **Authors**: Michael Yeung, Guang Yang, Evis Sala, Carola-Bibiane Schönlieb, Leonardo Rundo
- **Comment**: None
- **Journal**: None
- **Summary**: Manual segmentation is used as the gold-standard for evaluating neural networks on automated image segmentation tasks. Due to considerable heterogeneity in shapes, colours and textures, demarcating object boundaries is particularly difficult in biomedical images, resulting in significant inter and intra-rater variability. Approaches, such as soft labelling and distance penalty term, apply a global transformation to the ground truth, redefining the loss function with respect to uncertainty. However, global operations are computationally expensive, and neither approach accurately reflects the uncertainty underlying manual annotation. In this paper, we propose the Boundary Uncertainty, which uses morphological operations to restrict soft labelling to object boundaries, providing an appropriate representation of uncertainty in ground truth labels, and may be adapted to enable robust model training where systematic manual segmentation errors are present. We incorporate Boundary Uncertainty with the Dice loss, achieving consistently improved performance across three well-validated biomedical imaging datasets compared to soft labelling and distance-weighted penalty. Boundary Uncertainty not only more accurately reflects the segmentation process, but it is also efficient, robust to segmentation errors and exhibits better generalisation.



### Focal Attention Networks: optimising attention for biomedical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.00534v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2111.00534v1)
- **Published**: 2021-10-31 16:20:22+00:00
- **Updated**: 2021-10-31 16:20:22+00:00
- **Authors**: Michael Yeung, Leonardo Rundo, Evis Sala, Carola-Bibiane Schönlieb, Guang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, there has been increasing interest to incorporate attention into deep learning architectures for biomedical image segmentation. The modular design of attention mechanisms enables flexible integration into convolutional neural network architectures, such as the U-Net. Whether attention is appropriate to use, what type of attention to use, and where in the network to incorporate attention modules, are all important considerations that are currently overlooked. In this paper, we investigate the role of the Focal parameter in modulating attention, revealing a link between attention in loss functions and networks. By incorporating a Focal distance penalty term, we extend the Unified Focal loss framework to include boundary-based losses. Furthermore, we develop a simple and interpretable, dataset and model-specific heuristic to integrate the Focal parameter into the Squeeze-and-Excitation block and Attention Gate, achieving optimal performance with fewer number of attention modules on three well-validated biomedical imaging datasets, suggesting judicious use of attention modules results in better performance and efficiency.



### From Face to Gait: Weakly-Supervised Learning of Gender Information from Walking Patterns
- **Arxiv ID**: http://arxiv.org/abs/2111.00538v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.00538v1)
- **Published**: 2021-10-31 16:34:54+00:00
- **Updated**: 2021-10-31 16:34:54+00:00
- **Authors**: Andy Catruna, Adrian Cosma, Ion Emilian Radoi
- **Comment**: Accepted at Face & Gesture Recognition 2021
- **Journal**: None
- **Summary**: Obtaining demographics information from video is valuable for a range of real-world applications. While approaches that leverage facial features for gender inference are very successful in restrained environments, they do not work in most real-world scenarios when the subject is not facing the camera, has the face obstructed or the face is not clear due to distance from the camera or poor resolution. We propose a weakly-supervised method for learning gender information of people based on their manner of walking. We make use of state-of-the art facial analysis models to automatically annotate front-view walking sequences and generalise to unseen angles by leveraging gait-based label propagation. Our results show on par or higher performance with facial analysis models with an F1 score of 91% and the ability to successfully generalise to scenarios in which facial analysis is unfeasible due to subjects not facing the camera or having the face obstructed.



### Learning to Detect Open Carry and Concealed Object with 77GHz Radar
- **Arxiv ID**: http://arxiv.org/abs/2111.00551v2
- **DOI**: 10.1109/JSTSP.2022.3171168
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.00551v2)
- **Published**: 2021-10-31 17:33:28+00:00
- **Updated**: 2022-04-26 19:22:34+00:00
- **Authors**: Xiangyu Gao, Hui Liu, Sumit Roy, Guanbin Xing, Ali Alansari, Youchen Luo
- **Comment**: 12 pages
- **Journal**: IEEE Journal of Selected Topics in Signal Processing, 2022
- **Summary**: Detecting harmful carried objects plays a key role in intelligent surveillance systems and has widespread applications, for example, in airport security. In this paper, we focus on the relatively unexplored area of using low-cost 77GHz mmWave radar for the carried objects detection problem. The proposed system is capable of real-time detecting three classes of objects - laptop, phone, and knife - under open carry and concealed cases where objects are hidden with clothes or bags. This capability is achieved by the initial signal processing for localization and generating range-azimuth-elevation image cubes, followed by a deep learning-based prediction network and a multi-shot post-processing module for detecting objects. Extensive experiments for validating the system performance on detecting open carry and concealed objects have been presented with a self-built radar-camera testbed and collected dataset. Additionally, the influence of different input formats, factors, and parameters on system performance is analyzed, providing an intuitive understanding of the system. This system would be the very first baseline for other future works aiming to detect carried objects using 77GHz radar.



### TorchXRayVision: A library of chest X-ray datasets and models
- **Arxiv ID**: http://arxiv.org/abs/2111.00595v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.00595v1)
- **Published**: 2021-10-31 21:19:08+00:00
- **Updated**: 2021-10-31 21:19:08+00:00
- **Authors**: Joseph Paul Cohen, Joseph D. Viviano, Paul Bertin, Paul Morrison, Parsa Torabian, Matteo Guarrera, Matthew P Lungren, Akshay Chaudhari, Rupert Brooks, Mohammad Hashir, Hadrien Bertrand
- **Comment**: Library source code: https://github.com/mlmed/torchxrayvision
- **Journal**: None
- **Summary**: TorchXRayVision is an open source software library for working with chest X-ray datasets and deep learning models. It provides a common interface and common pre-processing chain for a wide set of publicly available chest X-ray datasets. In addition, a number of classification and representation learning models with different architectures, trained on different data combinations, are available through the library to serve as baselines or feature extractors.



### The 5th Recognizing Families in the Wild Data Challenge: Predicting Kinship from Faces
- **Arxiv ID**: http://arxiv.org/abs/2111.00598v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.00598v4)
- **Published**: 2021-10-31 21:37:40+00:00
- **Updated**: 2021-11-26 20:27:52+00:00
- **Authors**: Joseph P. Robinson, Can Qin, Ming Shao, Matthew A. Turk, Rama Chellappa, Yun Fu
- **Comment**: 2021 IEEE Conference on Automatic Face and Gesture Recognition
- **Journal**: None
- **Summary**: Recognizing Families In the Wild (RFIW), held as a data challenge in conjunction with the 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG), is a large-scale, multi-track visual kinship recognition evaluation. For the fifth edition of RFIW, we continue to attract scholars, bring together professionals, publish new work, and discuss prospects. In this paper, we summarize submissions for the three tasks of this year's RFIW: specifically, we review the results for kinship verification, tri-subject verification, and family member search and retrieval. We look at the RFIW problem, share current efforts, and make recommendations for promising future directions.



### PIE: Pseudo-Invertible Encoder
- **Arxiv ID**: http://arxiv.org/abs/2111.00619v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.00619v1)
- **Published**: 2021-10-31 23:13:37+00:00
- **Updated**: 2021-10-31 23:13:37+00:00
- **Authors**: Jan Jetze Beitler, Ivan Sosnovik, Arnold Smeulders
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of information compression from high dimensional data. Where many studies consider the problem of compression by non-invertible transformations, we emphasize the importance of invertible compression. We introduce new class of likelihood-based autoencoders with pseudo bijective architecture, which we call Pseudo Invertible Encoders. We provide the theoretical explanation of their principles. We evaluate Gaussian Pseudo Invertible Encoder on MNIST, where our model outperforms WAE and VAE in sharpness of the generated images.



### Distantly Supervised Semantic Text Detection and Recognition for Broadcast Sports Videos Understanding
- **Arxiv ID**: http://arxiv.org/abs/2111.00629v1
- **DOI**: 10.1145/3474085.3481543
- **Categories**: **cs.MM**, cs.CV, cs.IR, cs.LG, H.5.1; I.4.8; I.2.10; I.2.7; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2111.00629v1)
- **Published**: 2021-10-31 23:59:29+00:00
- **Updated**: 2021-10-31 23:59:29+00:00
- **Authors**: Avijit Shah, Topojoy Biswas, Sathish Ramadoss, Deven Santosh Shah
- **Comment**: 9 pages, 7 figures and 6 tables. To be published in the proceedings
  of ACM Multimedia 21, Industrial Track, held from October 20-24 in China
- **Journal**: None
- **Summary**: Comprehensive understanding of key players and actions in multiplayer sports broadcast videos is a challenging problem. Unlike in news or finance videos, sports videos have limited text. While both action recognition for multiplayer sports and detection of players has seen robust research, understanding contextual text in video frames still remains one of the most impactful avenues of sports video understanding. In this work we study extremely accurate semantic text detection and recognition in sports clocks, and challenges therein. We observe unique properties of sports clocks, which makes it hard to utilize general-purpose pre-trained detectors and recognizers, so that text can be accurately understood to the degree of being used to align to external knowledge. We propose a novel distant supervision technique to automatically build sports clock datasets. Along with suitable data augmentations, combined with any state-of-the-art text detection and recognition model architectures, we extract extremely accurate semantic text. Finally, we share our computational architecture pipeline to scale this system in industrial setting and proposed a robust dataset for the same to validate our results.



