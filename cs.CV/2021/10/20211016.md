# Arxiv Papers in cs.CV on 2021-10-16
### Dataset Knowledge Transfer for Class-Incremental Learning without Memory
- **Arxiv ID**: http://arxiv.org/abs/2110.08421v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.08421v1)
- **Published**: 2021-10-16 00:33:33+00:00
- **Updated**: 2021-10-16 00:33:33+00:00
- **Authors**: Habib Slim, Eden Belouadah, Adrian Popescu, Darian Onchis
- **Comment**: Accepted to WACV 2022
- **Journal**: None
- **Summary**: Incremental learning enables artificial agents to learn from sequential data. While important progress was made by exploiting deep neural networks, incremental learning remains very challenging. This is particularly the case when no memory of past data is allowed and catastrophic forgetting has a strong negative effect. We tackle class-incremental learning without memory by adapting prediction bias correction, a method which makes predictions of past and new classes more comparable. It was proposed when a memory is allowed and cannot be directly used without memory, since samples of past classes are required. We introduce a two-step learning process which allows the transfer of bias correction parameters between reference and target datasets. Bias correction is first optimized offline on reference datasets which have an associated validation memory. The obtained correction parameters are then transferred to target datasets, for which no memory is available. The second contribution is to introduce a finer modeling of bias correction by learning its parameters per incremental state instead of the usual past vs. new class modeling. The proposed dataset knowledge transfer is applicable to any incremental method which works without memory. We test its effectiveness by applying it to four existing methods. Evaluation with four target datasets and different configurations shows consistent improvement, with practically no computational and memory overhead.



### Deep learning-based detection of intravenous contrast in computed tomography scans
- **Arxiv ID**: http://arxiv.org/abs/2110.08424v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.08424v2)
- **Published**: 2021-10-16 00:46:45+00:00
- **Updated**: 2021-10-19 23:43:41+00:00
- **Authors**: Zezhong Ye, Jack M. Qian, Ahmed Hosny, Roman Zeleznik, Deborah Plana, Jirapat Likitlersuang, Zhongyi Zhang, Raymond H. Mak, Hugo J. W. L. Aerts, Benjamin H. Kann
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Identifying intravenous (IV) contrast use within CT scans is a key component of data curation for model development and testing. Currently, IV contrast is poorly documented in imaging metadata and necessitates manual correction and annotation by clinician experts, presenting a major barrier to imaging analyses and algorithm deployment. We sought to develop and validate a convolutional neural network (CNN)-based deep learning (DL) platform to identify IV contrast within CT scans. Methods: For model development and evaluation, we used independent datasets of CT scans of head, neck (HN) and lung cancer patients, totaling 133,480 axial 2D scan slices from 1,979 CT scans manually annotated for contrast presence by clinical experts. Five different DL models were adopted and trained in HN training datasets for slice-level contrast detection. Model performances were evaluated on a hold-out set and on an independent validation set from another institution. DL models was then fine-tuned on chest CT data and externally validated on a separate chest CT dataset. Results: Initial DICOM metadata tags for IV contrast were missing or erroneous in 1,496 scans (75.6%). The EfficientNetB4-based model showed the best overall detection performance. For HN scans, AUC was 0.996 in the internal validation set (n = 216) and 1.0 in the external validation set (n = 595). The fine-tuned model on chest CTs yielded an AUC: 1.0 for the internal validation set (n = 53), and AUC: 0.980 for the external validation set (n = 402). Conclusion: The DL model could accurately detect IV contrast in both HN and chest CT scans with near-perfect performance.



### COVID-19 Detection in Chest X-ray Images Using Swin-Transformer and Transformer in Transformer
- **Arxiv ID**: http://arxiv.org/abs/2110.08427v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08427v2)
- **Published**: 2021-10-16 00:53:21+00:00
- **Updated**: 2022-12-31 18:28:20+00:00
- **Authors**: Juntao Jiang, Shuyi Lin
- **Comment**: None
- **Journal**: None
- **Summary**: The Coronavirus Disease 2019 (COVID-19) has spread globally and caused serious damage. Chest X-ray images are widely used for COVID-19 diagnosis and the Artificial Intelligence method can increase efficiency and accuracy. In the Challenge of Chest XR COVID-19 detection in Ethics and Explainability for Responsible Data Science (EE-RDS) conference 2021, we proposed a method that combined Swin Transformer and Transformer in Transformer to classify chest X-ray images as three classes: COVID-19, Pneumonia, and Normal (healthy) and achieved 0.9475 accuracies on the test set.



### TorchEsegeta: Framework for Interpretability and Explainability of Image-based Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2110.08429v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.08429v2)
- **Published**: 2021-10-16 01:00:15+00:00
- **Updated**: 2022-02-07 10:22:32+00:00
- **Authors**: Soumick Chatterjee, Arnab Das, Chirag Mandal, Budhaditya Mukhopadhyay, Manish Vipinraj, Aniruddh Shukla, Rajatha Nagaraja Rao, Chompunuch Sarasaen, Oliver Speck, Andreas NÃ¼rnberger
- **Comment**: None
- **Journal**: None
- **Summary**: Clinicians are often very sceptical about applying automatic image processing approaches, especially deep learning based methods, in practice. One main reason for this is the black-box nature of these approaches and the inherent problem of missing insights of the automatically derived decisions. In order to increase trust in these methods, this paper presents approaches that help to interpret and explain the results of deep learning algorithms by depicting the anatomical areas which influence the decision of the algorithm most. Moreover, this research presents a unified framework, TorchEsegeta, for applying various interpretability and explainability techniques for deep learning models and generate visual interpretations and explanations for clinicians to corroborate their clinical findings. In addition, this will aid in gaining confidence in such methods. The framework builds on existing interpretability and explainability techniques that are currently focusing on classification models, extending them to segmentation tasks. In addition, these methods have been adapted to 3D models for volumetric analysis. The proposed framework provides methods to quantitatively compare visual explanations using infidelity and sensitivity metrics. This framework can be used by data scientists to perform post-hoc interpretations and explanations of their models, develop more explainable tools and present the findings to clinicians to increase their faith in such models. The proposed framework was evaluated based on a use case scenario of vessel segmentation models trained on Time-of-fight (TOF) Magnetic Resonance Angiogram (MRA) images of the human brain. Quantitative and qualitative results of a comparative study of different models and interpretability methods are presented. Furthermore, this paper provides an extensive overview of several existing interpretability and explainability methods.



### Self-Annotated Training for Controllable Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2110.08446v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08446v2)
- **Published**: 2021-10-16 02:10:23+00:00
- **Updated**: 2021-11-20 02:54:03+00:00
- **Authors**: Zhangzi Zhu, Tianlei Wang, Hong Qu
- **Comment**: None
- **Journal**: None
- **Summary**: The Controllable Image Captioning (CIC) task aims to generate captions conditioned on designated control signals. Several structure-related control signals are proposed to control the semantic structure of sentences, such as sentence length and Part-of-Speech tag sequences. However, due to the fact that the accuracy-based reward focuses mainly on contents rather than semantic structures, existing reinforcement training methods are not applicable to structure-related CIC models. The lack of reinforcement training leads to exposure bias and the inconsistency between the optimizing function and evaluation metrics. In this paper, we propose a novel reinforcement training method for structure-related control signals: Self-Annotated Training (SAT), to improve both the accuracy and controllability of CIC models. In SAT, a recursive annotation mechanism (RAM) is designed to force the input control signal to match the actual output sentence. Moreover, we propose an extra alignment reward to finetune the CIC model trained after SAT method, which further enhances the controllability of models. On the MSCOCO benchmark, we conduct extensive experiments on different structure-related control signals and on different baseline models, the results of which demonstrate the effectiveness and generalizability of our methods.



### Joint 3D Human Shape Recovery and Pose Estimation from a Single Image with Bilayer Graph
- **Arxiv ID**: http://arxiv.org/abs/2110.08472v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08472v2)
- **Published**: 2021-10-16 05:04:02+00:00
- **Updated**: 2021-12-05 18:46:36+00:00
- **Authors**: Xin Yu, Jeroen van Baar, Siheng Chen
- **Comment**: 3DV'21
- **Journal**: None
- **Summary**: The ability to estimate the 3D human shape and pose from images can be useful in many contexts. Recent approaches have explored using graph convolutional networks and achieved promising results. The fact that the 3D shape is represented by a mesh, an undirected graph, makes graph convolutional networks a natural fit for this problem. However, graph convolutional networks have limited representation power. Information from nodes in the graph is passed to connected neighbors, and propagation of information requires successive graph convolutions. To overcome this limitation, we propose a dual-scale graph approach. We use a coarse graph, derived from a dense graph, to estimate the human's 3D pose, and the dense graph to estimate the 3D shape. Information in coarse graphs can be propagated over longer distances compared to dense graphs. In addition, information about pose can guide to recover local shape detail and vice versa. We recognize that the connection between coarse and dense is itself a graph, and introduce graph fusion blocks to exchange information between graphs with different scales. We train our model end-to-end and show that we can achieve state-of-the-art results for several evaluation datasets.



### A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2110.08484v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2110.08484v2)
- **Published**: 2021-10-16 06:07:59+00:00
- **Updated**: 2022-03-15 01:52:38+00:00
- **Authors**: Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, Xiang Ren
- **Comment**: Accepted to ACL 2022
- **Journal**: None
- **Summary**: Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning. However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed. To solve this limitation, we study prompt-based low-resource learning of VL tasks with our proposed method, FewVLM, relatively smaller than recent few-shot learners. For FewVLM, we pre-train a sequence-to-sequence transformer model with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM). Furthermore, we analyze the effect of diverse prompts for few-shot tasks. Experimental results on VQA show that FewVLM with prompt-based learning outperforms Frozen which is 31x larger than FewVLM by 18.2% point and achieves comparable results to a 246x larger model, PICa. In our analysis, we observe that (1) prompts significantly affect zero-shot performance but marginally affect few-shot performance, (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts captioning performance. Our code is publicly available at \url{https://github.com/woojeongjin/FewVLM}



### Understanding Multimodal Procedural Knowledge by Sequencing Multimodal Instructional Manuals
- **Arxiv ID**: http://arxiv.org/abs/2110.08486v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08486v3)
- **Published**: 2021-10-16 06:12:15+00:00
- **Updated**: 2022-03-17 03:24:54+00:00
- **Authors**: Te-Lin Wu, Alex Spangher, Pegah Alipoormolabashi, Marjorie Freedman, Ralph Weischedel, Nanyun Peng
- **Comment**: In Proceedings of the Conference of the 60th Annual Meeting of the
  Association for Computational Linguistics (ACL), 2022
- **Journal**: None
- **Summary**: The ability to sequence unordered events is an essential skill to comprehend and reason about real world task procedures, which often requires thorough understanding of temporal common sense and multimodal information, as these procedures are often communicated through a combination of texts and images. Such capability is essential for applications such as sequential task planning and multi-source instruction summarization. While humans are capable of reasoning about and sequencing unordered multimodal procedural instructions, whether current machine learning models have such essential capability is still an open question. In this work, we benchmark models' capability of reasoning over and sequencing unordered multimodal instructions by curating datasets from popular online instructional manuals and collecting comprehensive human annotations. We find models not only perform significantly worse than humans but also seem incapable of efficiently utilizing the multimodal information. To improve machines' performance on multimodal event sequencing, we propose sequentiality-aware pretraining techniques that exploit the sequential alignment properties of both texts and images, resulting in > 5% significant improvements.



### Improvised Aerial Object Detection approach for YOLOv3 Using Weighted Luminance
- **Arxiv ID**: http://arxiv.org/abs/2110.08493v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 68T, I.5.4; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2110.08493v3)
- **Published**: 2021-10-16 06:51:35+00:00
- **Updated**: 2022-12-20 07:11:16+00:00
- **Authors**: Sai Ganesh CS, Aouthithiye Barathwaj SR Y, R. Swethaa S, R. Azhagumurugan
- **Comment**: 17 pages, 4 figures, Journal Expert Systems with Applications
- **Journal**: None
- **Summary**: Aerial imaging plays a crucial role in navigation and data acquisition for unmanned aerial vehicles and satellite imaging systems. In recent days, the employment of drones has been escalated in several applications that are not limited to surveillance, delivery systems, aerial warfare, and agricultural activities. Aerial imaging of ground targets is highly challenging because of various factors that affect light propagation through different mediums. Several convolutional neural network-based object detection algorithms that are developed require more robustness when applied in the field of aerial imaging and remote sensing. In order to handle the adverse effects of light propagation with respect to time and solar radiance, adaptive RGB filters for grayscale imaging based on weighted luminance are introduced that extensively solve the problem of rayleigh scattering effect. Images of objects that are easily diminished by rayleigh scattering are acquired in various timezones. The acquired images are labelled precisely and subjected to training and validation. The results show that the proposed method detects the object more accurately and efficiently than the traditional YOLOv3 approach.



### Hybrid Mutimodal Fusion for Dimensional Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.08495v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08495v1)
- **Published**: 2021-10-16 06:57:18+00:00
- **Updated**: 2021-10-16 06:57:18+00:00
- **Authors**: Ziyu Ma, Fuyan Ma, Bin Sun, Shutao Li
- **Comment**: 8 pages, 2 figures, accepted by ACM MM2021
- **Journal**: None
- **Summary**: In this paper, we extensively present our solutions for the MuSe-Stress sub-challenge and the MuSe-Physio sub-challenge of Multimodal Sentiment Challenge (MuSe) 2021. The goal of MuSe-Stress sub-challenge is to predict the level of emotional arousal and valence in a time-continuous manner from audio-visual recordings and the goal of MuSe-Physio sub-challenge is to predict the level of psycho-physiological arousal from a) human annotations fused with b) galvanic skin response (also known as Electrodermal Activity (EDA)) signals from the stressed people. The Ulm-TSST dataset which is a novel subset of the audio-visual textual Ulm-Trier Social Stress dataset that features German speakers in a Trier Social Stress Test (TSST) induced stress situation is used in both sub-challenges. For the MuSe-Stress sub-challenge, we highlight our solutions in three aspects: 1) the audio-visual features and the bio-signal features are used for emotional state recognition. 2) the Long Short-Term Memory (LSTM) with the self-attention mechanism is utilized to capture complex temporal dependencies within the feature sequences. 3) the late fusion strategy is adopted to further boost the model's recognition performance by exploiting complementary information scattered across multimodal sequences. Our proposed model achieves CCC of 0.6159 and 0.4609 for valence and arousal respectively on the test set, which both rank in the top 3. For the MuSe-Physio sub-challenge, we first extract the audio-visual features and the bio-signal features from multiple modalities. Then, the LSTM module with the self-attention mechanism, and the Gated Convolutional Neural Networks (GCNN) as well as the LSTM network are utilized for modeling the complex temporal dependencies in the sequence. Finally, the late fusion strategy is used. Our proposed method also achieves CCC of 0.5412 on the test set, which ranks in the top 3.



### BAPGAN: GAN-based Bone Age Progression of Femur and Phalange X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2110.08509v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.08509v1)
- **Published**: 2021-10-16 08:27:59+00:00
- **Updated**: 2021-10-16 08:27:59+00:00
- **Authors**: Shinji Nakazawa, Changhee Han, Joe Hasei, Ryuichi Nakahara, Toshifumi Ozaki
- **Comment**: 6 pages, 5 figures, accepted to SPIE Medical Imaging 2022
- **Journal**: None
- **Summary**: Convolutional Neural Networks play a key role in bone age assessment for investigating endocrinology, genetic, and growth disorders under various modalities and body regions. However, no researcher has tackled bone age progression/regression despite its valuable potential applications: bone-related disease diagnosis, clinical knowledge acquisition, and museum education. Therefore, we propose Bone Age Progression Generative Adversarial Network (BAPGAN) to progress/regress both femur/phalange X-ray images while preserving identity and realism. We exhaustively confirm the BAPGAN's clinical potential via Frechet Inception Distance, Visual Turing Test by two expert orthopedists, and t-Distributed Stochastic Neighbor Embedding.



### Multimodal Dialogue Response Generation
- **Arxiv ID**: http://arxiv.org/abs/2110.08515v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.08515v2)
- **Published**: 2021-10-16 08:52:26+00:00
- **Updated**: 2022-03-29 13:39:57+00:00
- **Authors**: Qingfeng Sun, Yujing Wang, Can Xu, Kai Zheng, Yaming Yang, Huang Hu, Fei Xu, Jessica Zhang, Xiubo Geng, Daxin Jiang
- **Comment**: Accepted to ACL 2022 Main Conference
- **Journal**: None
- **Summary**: Responsing with image has been recognized as an important capability for an intelligent conversational agent. Yet existing works only focus on exploring the multimodal dialogue models which depend on retrieval-based methods, but neglecting generation methods. To fill in the gaps, we first present a multimodal dialogue generation model, which takes the dialogue history as input, then generates a textual sequence or an image as response. Learning such a model often requires multimodal dialogues containing both texts and images which are difficult to obtain. Motivated by the challenge in practice, we consider multimodal dialogue generation under a natural assumption that only limited training examples are available. In such a low-resource setting, we devise a novel conversational agent, Divter, in order to isolate parameters that depend on multimodal dialogues from the entire generation model. By this means, the major part of the model can be learned from a large number of text-only dialogues and text-image pairs respectively, then the whole parameters can be well fitted using the limited training examples. Extensive experiments demonstrate our method achieves state-of-the-art results in both automatic and human evaluation, and can generate informative text and high-resolution image responses.



### Locally Adaptive Structure and Texture Similarity for Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2110.08521v1
- **DOI**: 10.1145/3474085.3475419
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08521v1)
- **Published**: 2021-10-16 09:19:56+00:00
- **Updated**: 2021-10-16 09:19:56+00:00
- **Authors**: Keyan Ding, Yi Liu, Xueyi Zou, Shiqi Wang, Kede Ma
- **Comment**: None
- **Journal**: Proceedings of the 29th ACM International Conference on
  Multimedia, 2021
- **Summary**: The latest advances in full-reference image quality assessment (IQA) involve unifying structure and texture similarity based on deep representations. The resulting Deep Image Structure and Texture Similarity (DISTS) metric, however, makes rather global quality measurements, ignoring the fact that natural photographic images are locally structured and textured across space and scale. In this paper, we describe a locally adaptive structure and texture similarity index for full-reference IQA, which we term A-DISTS. Specifically, we rely on a single statistical feature, namely the dispersion index, to localize texture regions at different scales. The estimated probability (of one patch being texture) is in turn used to adaptively pool local structure and texture measurements. The resulting A-DISTS is adapted to local image content, and is free of expensive human perceptual scores for supervised training. We demonstrate the advantages of A-DISTS in terms of correlation with human data on ten IQA databases and optimization of single image super-resolution methods.



### Multi-View Stereo Network with attention thin volume
- **Arxiv ID**: http://arxiv.org/abs/2110.08556v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08556v2)
- **Published**: 2021-10-16 11:51:23+00:00
- **Updated**: 2022-07-16 06:28:37+00:00
- **Authors**: Zihang Wan
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an efficient multi-view stereo (MVS) network for infering depth value from multiple RGB images. Recent studies have shown that mapping the geometric relationship in real space to neural network is an essential topic of the MVS problem. Specifically, these methods focus on how to express the correspondence between different views by constructing a nice cost volume. In this paper, we propose a more complete cost volume construction approach based on absorbing previous experience. First of all, we introduce the self-attention mechanism to fully aggregate the dominant information from input images and accurately model the long-range dependency, so as to selectively aggregate reference features. Secondly, we introduce the group-wise correlation to feature aggregation, which greatly reduces the memory and calculation burden. Meanwhile, this method enhances the information interaction between different feature channels. With this approach, a more lightweight and efficient cost volume is constructed. Finally we follow the coarse to fine strategy and refine the depth sampling range scale by scale with the help of uncertainty estimation. We further combine the previous steps to get the attention thin volume. Quantitative and qualitative experiments are presented to demonstrate the performance of our model.



### Neural Network Pruning Through Constrained Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.08558v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08558v2)
- **Published**: 2021-10-16 11:57:38+00:00
- **Updated**: 2021-10-28 22:38:14+00:00
- **Authors**: Shehryar Malik, Muhammad Umair Haider, Omer Iqbal, Murtaza Taj
- **Comment**: Submitted to ICASSP 2021
- **Journal**: None
- **Summary**: Network pruning reduces the size of neural networks by removing (pruning) neurons such that the performance drop is minimal. Traditional pruning approaches focus on designing metrics to quantify the usefulness of a neuron which is often quite tedious and sub-optimal. More recent approaches have instead focused on training auxiliary networks to automatically learn how useful each neuron is however, they often do not take computational limitations into account. In this work, we propose a general methodology for pruning neural networks. Our proposed methodology can prune neural networks to respect pre-defined computational budgets on arbitrary, possibly non-differentiable, functions. Furthermore, we only assume the ability to be able to evaluate these functions for different inputs, and hence they do not need to be fully specified beforehand. We achieve this by proposing a novel pruning strategy via constrained reinforcement learning algorithms. We prove the effectiveness of our approach via comparison with state-of-the-art methods on standard image classification datasets. Specifically, we reduce 83-92.90 of total parameters on various variants of VGG while achieving comparable or better performance than that of original networks. We also achieved 75.09 reduction in parameters on ResNet18 without incurring any loss in accuracy.



### BNAS v2: Learning Architectures for Binary Networks with Empirical Improvements
- **Arxiv ID**: http://arxiv.org/abs/2110.08562v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.08562v1)
- **Published**: 2021-10-16 12:38:26+00:00
- **Updated**: 2021-10-16 12:38:26+00:00
- **Authors**: Dahyun Kim, Kunal Pratap Singh, Jonghyun Choi
- **Comment**: arXiv admin note: text overlap with arXiv:2002.06963
- **Journal**: None
- **Summary**: Backbone architectures of most binary networks are well-known floating point (FP) architectures such as the ResNet family. Questioning that the architectures designed for FP networks might not be the best for binary networks, we propose to search architectures for binary networks (BNAS) by defining a new search space for binary architectures and a novel search objective. Specifically, based on the cell based search method, we define the new search space of binary layer types, design a new cell template, and rediscover the utility of and propose to use the Zeroise layer instead of using it as a placeholder. The novel search objective diversifies early search to learn better performing binary architectures. We show that our method searches architectures with stable training curves despite the quantization error inherent in binary networks. Quantitative analyses demonstrate that our searched architectures outperform the architectures used in state-of-the-art binary networks and outperform or perform on par with state-of-the-art binary networks that employ various techniques other than architectural changes. In addition, we further propose improvements to the training scheme of our searched architectures. With the new training scheme for our searched architectures, we achieve the state-of-the-art performance by binary networks by outperforming all previous methods by non-trivial margins.



### ASFormer: Transformer for Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.08568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08568v1)
- **Published**: 2021-10-16 13:07:20+00:00
- **Updated**: 2021-10-16 13:07:20+00:00
- **Authors**: Fangqiu Yi, Hongyu Wen, Tingting Jiang
- **Comment**: Accepted by BMVC 2021
- **Journal**: None
- **Summary**: Algorithms for the action segmentation task typically use temporal models to predict what action is occurring at each frame for a minute-long daily activity. Recent studies have shown the potential of Transformer in modeling the relations among elements in sequential data. However, there are several major concerns when directly applying the Transformer to the action segmentation task, such as the lack of inductive biases with small training sets, the deficit in processing long input sequence, and the limitation of the decoder architecture to utilize temporal relations among multiple action segments to refine the initial predictions. To address these concerns, we design an efficient Transformer-based model for action segmentation task, named ASFormer, with three distinctive characteristics: (i) We explicitly bring in the local connectivity inductive priors because of the high locality of features. It constrains the hypothesis space within a reliable scope, and is beneficial for the action segmentation task to learn a proper target function with small training sets. (ii) We apply a pre-defined hierarchical representation pattern that efficiently handles long input sequences. (iii) We carefully design the decoder to refine the initial predictions from the encoder. Extensive experiments on three public datasets demonstrate that effectiveness of our methods. Code is available at \url{https://github.com/ChinaYi/ASFormer}.



### Deep Image Debanding
- **Arxiv ID**: http://arxiv.org/abs/2110.08569v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08569v1)
- **Published**: 2021-10-16 13:11:48+00:00
- **Updated**: 2021-10-16 13:11:48+00:00
- **Authors**: Raymond Zhou, Shahrukh Athar, Zhongling Wang, Zhou Wang
- **Comment**: 5 pages, 4 figures, 5 tables
- **Journal**: None
- **Summary**: Banding or false contour is an annoying visual artifact whose impact is even more pronounced in ultra high definition, high dynamic range, and wide colour gamut visual content, which is becoming increasingly popular. Since users associate a heightened expectation of quality with such content and banding leads to deteriorated visual quality-of-experience, the area of banding removal or debanding has taken paramount importance. Existing debanding approaches are mostly knowledge-driven. Despite the widespread success of deep learning in other areas of image processing and computer vision, data-driven debanding approaches remain surprisingly missing. In this work, we make one of the first attempts to develop a deep learning based banding artifact removal method for images and name it deep debanding network (deepDeband). For its training, we construct a large-scale dataset of 51,490 pairs of corresponding pristine and banded image patches. Performance evaluation shows that deepDeband is successful at greatly reducing banding artifacts in images, outperforming existing methods both quantitatively and visually.



### Explore before Moving: A Feasible Path Estimation and Memory Recalling Framework for Embodied Navigation
- **Arxiv ID**: http://arxiv.org/abs/2110.08571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08571v1)
- **Published**: 2021-10-16 13:30:55+00:00
- **Updated**: 2021-10-16 13:30:55+00:00
- **Authors**: Yang Wu, Shirui Feng, Guanbin Li, Liang Lin
- **Comment**: None
- **Journal**: None
- **Summary**: An embodied task such as embodied question answering (EmbodiedQA), requires an agent to explore the environment and collect clues to answer a given question that related with specific objects in the scene. The solution of such task usually includes two stages, a navigator and a visual Q&A module. In this paper, we focus on the navigation and solve the problem of existing navigation algorithms lacking experience and common sense, which essentially results in a failure finding target when robot is spawn in unknown environments.   Inspired by the human ability to think twice before moving and conceive several feasible paths to seek a goal in unfamiliar scenes, we present a route planning method named Path Estimation and Memory Recalling (PEMR) framework. PEMR includes a "looking ahead" process, \textit{i.e.} a visual feature extractor module that estimates feasible paths for gathering 3D navigational information, which is mimicking the human sense of direction. PEMR contains another process ``looking behind'' process that is a memory recall mechanism aims at fully leveraging past experience collected by the feature extractor. Last but not the least, to encourage the navigator to learn more accurate prior expert experience, we improve the original benchmark dataset and provide a family of evaluation metrics for diagnosing both navigation and question answering modules. We show strong experimental results of PEMR on the EmbodiedQA navigation task.



### Visual-aware Attention Dual-stream Decoder for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/2110.08578v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.08578v1)
- **Published**: 2021-10-16 14:08:20+00:00
- **Updated**: 2021-10-16 14:08:20+00:00
- **Authors**: Zhixin Sun, Xian Zhong, Shuqin Chen, Lin Li, Luo Zhong
- **Comment**: None
- **Journal**: None
- **Summary**: Video captioning is a challenging task that captures different visual parts and describes them in sentences, for it requires visual and linguistic coherence. The attention mechanism in the current video captioning method learns to assign weight to each frame, promoting the decoder dynamically. This may not explicitly model the correlation and the temporal coherence of the visual features extracted in the sequence frames.To generate semantically coherent sentences, we propose a new Visual-aware Attention (VA) model, which concatenates dynamic changes of temporal sequence frames with the words at the previous moment, as the input of attention mechanism to extract sequence features.In addition, the prevalent approaches widely use the teacher-forcing (TF) learning during training, where the next token is generated conditioned on the previous ground-truth tokens. The semantic information in the previously generated tokens is lost. Therefore, we design a self-forcing (SF) stream that takes the semantic information in the probability distribution of the previous token as input to enhance the current token.The Dual-stream Decoder (DD) architecture unifies the TF and SF streams, generating sentences to promote the annotated captioning for both streams.Meanwhile, with the Dual-stream Decoder utilized, the exposure bias problem is alleviated, caused by the discrepancy between the training and testing in the TF learning.The effectiveness of the proposed Visual-aware Attention Dual-stream Decoder (VADD) is demonstrated through the result of experimental studies on Microsoft video description (MSVD) corpus and MSR-Video to text (MSR-VTT) datasets.



### Intelligent Video Editing: Incorporating Modern Talking Face Generation Algorithms in a Video Editor
- **Arxiv ID**: http://arxiv.org/abs/2110.08580v1
- **DOI**: 10.1145/3490035.3490284
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08580v1)
- **Published**: 2021-10-16 14:19:12+00:00
- **Updated**: 2021-10-16 14:19:12+00:00
- **Authors**: Anchit Gupta, Faizan Farooq Khan, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, C. V. Jawahar
- **Comment**: 9 pages, 7 figures, accepted in ICVGIP 2021
- **Journal**: None
- **Summary**: This paper proposes a video editor based on OpenShot with several state-of-the-art facial video editing algorithms as added functionalities. Our editor provides an easy-to-use interface to apply modern lip-syncing algorithms interactively. Apart from lip-syncing, the editor also uses audio and facial re-enactment to generate expressive talking faces. The manual control improves the overall experience of video editing without missing out on the benefits of modern synthetic video generation algorithms. This control enables us to lip-sync complex dubbed movie scenes, interviews, television shows, and other visual content. Furthermore, our editor provides features that automatically translate lectures from spoken content, lip-sync of the professor, and background content like slides. While doing so, we also tackle the critical aspect of synchronizing background content with the translated speech. We qualitatively evaluate the usefulness of the proposed editor by conducting human evaluations. Our evaluations show a clear improvement in the efficiency of using human editors and an improved video generation quality. We attach demo videos with the supplementary material clearly explaining the tool and also showcasing multiple results.



### Pseudo-label refinement using superpixels for semi-supervised brain tumour segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.08589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08589v1)
- **Published**: 2021-10-16 15:17:11+00:00
- **Updated**: 2021-10-16 15:17:11+00:00
- **Authors**: Bethany H. Thompson, Gaetano Di Caterina, Jeremy P. Voisey
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Training neural networks using limited annotations is an important problem in the medical domain. Deep Neural Networks (DNNs) typically require large, annotated datasets to achieve acceptable performance which, in the medical domain, are especially difficult to obtain as they require significant time from expert radiologists. Semi-supervised learning aims to overcome this problem by learning segmentations with very little annotated data, whilst exploiting large amounts of unlabelled data. However, the best-known technique, which utilises inferred pseudo-labels, is vulnerable to inaccurate pseudo-labels degrading the performance. We propose a framework based on superpixels - meaningful clusters of adjacent pixels - to improve the accuracy of the pseudo labels and address this issue. Our framework combines superpixels with semi-supervised learning, refining the pseudo-labels during training using the features and edges of the superpixel maps. This method is evaluated on a multimodal magnetic resonance imaging (MRI) dataset for the task of brain tumour region segmentation. Our method demonstrates improved performance over the standard semi-supervised pseudo-labelling baseline when there is a reduced annotator burden and only 5 annotated patients are available. We report DSC=0.824 and DSC=0.707 for the test set whole tumour and tumour core regions respectively.



### Automated Remote Sensing Forest Inventory Using Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2110.08590v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08590v2)
- **Published**: 2021-10-16 15:24:12+00:00
- **Updated**: 2021-11-07 07:35:00+00:00
- **Authors**: Abduragim Shtanchaev, Artur Bille, Olga Sutyrina, Sara Elelimy
- **Comment**: 15 pages, 11 figures, 71th International Astronautical Congress (IAC)
  - The CyberSpace Edition
- **Journal**: None
- **Summary**: For many countries like Russia, Canada, or the USA, a robust and detailed tree species inventory is essential to manage their forests sustainably. Since one can not apply unmanned aerial vehicle (UAV) imagery-based approaches to large-scale forest inventory applications, the utilization of machine learning algorithms on satellite imagery is a rising topic of research. Although satellite imagery quality is relatively low, additional spectral channels provide a sufficient amount of information for tree crown classification tasks. Assuming that tree crowns are detected already, we use embeddings of tree crowns generated by Autoencoders as a data set to train classical Machine Learning algorithms. We compare our Autoencoder (AE) based approach to traditional convolutional neural networks (CNN) end-to-end classifiers.



### A MIMO Radar-based Few-Shot Learning Approach for Human-ID
- **Arxiv ID**: http://arxiv.org/abs/2110.08595v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08595v2)
- **Published**: 2021-10-16 15:37:57+00:00
- **Updated**: 2022-06-13 14:37:13+00:00
- **Authors**: Pascal Weller, Fady Aziz, Sherif Abdulatif, Urs Schneider, Marco F. Huber
- **Comment**: 5 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: Radar for deep learning-based human identification has become a research area of increasing interest. It has been shown that micro-Doppler ($\mu$-D) can reflect the walking behavior through capturing the periodic limbs' micro-motions. One of the main aspects is maximizing the number of included classes while considering the real-time and training dataset size constraints. In this paper, a multiple-input-multiple-output (MIMO) radar is used to formulate micro-motion spectrograms of the elevation angular velocity ($\mu$-$\omega$). The effectiveness of concatenating this newly-formulated spectrogram with the commonly used $\mu$-D is investigated. To accommodate for non-constrained real walking motion, an adaptive cycle segmentation framework is utilized and a metric learning network is trained on half gait cycles ($\approx$ 0.5 s). Studies on the effects of various numbers of classes (5--20), different dataset sizes, and varying observation time windows 1--2 s are conducted. A non-constrained walking dataset of 22 subjects is collected with different aspect angles with respect to the radar. The proposed few-shot learning (FSL) approach achieves a classification error of 11.3 % with only 2 min of training data per subject.



### Mapping illegal waste dumping sites with neural-network classification of satellite imagery
- **Arxiv ID**: http://arxiv.org/abs/2110.08599v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2110.08599v2)
- **Published**: 2021-10-16 16:05:42+00:00
- **Updated**: 2021-10-28 01:24:43+00:00
- **Authors**: Maria Roberta Devesa, Antonio Vazquez Brust
- **Comment**: 5 pages, 3 figures, KDD Workshop on Data-driven Humanitarian Mapping
  held with the 27th ACM SIGKDD Conference on Knowledge Discovery and Data
  Mining, August 14, 2021
- **Journal**: None
- **Summary**: Public health and habitat quality are crucial goals of urban planning. In recent years, the severe social and environmental impact of illegal waste dumping sites has made them one of the most serious problems faced by cities in the Global South, in a context of scarce information available for decision making. To help identify the location of dumping sites and track their evolution over time we adopt a data-driven model from the machine learning domain, analyzing satellite images. This allows us to take advantage of the increasing availability of geo-spatial open-data, high-resolution satellite imagery, and open source tools to train machine learning algorithms with a small set of known waste dumping sites in Buenos Aires, and then predict the location of other sites over vast areas at high speed and low cost. This case study shows the results of a collaboration between Dymaxion Labs and Fundaci\'on Bunge y Born to harness this technique in order to create a comprehensive map of potential locations of illegal waste dumping sites in the region.



### MAAD: A Model and Dataset for "Attended Awareness" in Driving
- **Arxiv ID**: http://arxiv.org/abs/2110.08610v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.08610v1)
- **Published**: 2021-10-16 16:36:10+00:00
- **Updated**: 2021-10-16 16:36:10+00:00
- **Authors**: Deepak Gopinath, Guy Rosman, Simon Stent, Katsuya Terahata, Luke Fletcher, Brenna Argall, John Leonard
- **Comment**: 25 pages, 13 figures, 14 tables, Accepted at EPIC@ICCV 2021 Workshop.
  Main paper + Supplementary Material
- **Journal**: None
- **Summary**: We propose a computational model to estimate a person's attended awareness of their environment. We define attended awareness to be those parts of a potentially dynamic scene which a person has attended to in recent history and which they are still likely to be physically aware of. Our model takes as input scene information in the form of a video and noisy gaze estimates, and outputs visual saliency, a refined gaze estimate, and an estimate of the person's attended awareness. In order to test our model, we capture a new dataset with a high-precision gaze tracker including 24.5 hours of gaze sequences from 23 subjects attending to videos of driving scenes. The dataset also contains third-party annotations of the subjects' attended awareness based on observations of their scan path. Our results show that our model is able to reasonably estimate attended awareness in a controlled setting, and in the future could potentially be extended to real egocentric driving data to help enable more effective ahead-of-time warnings in safety systems and thereby augment driver performance. We also demonstrate our model's effectiveness on the tasks of saliency, gaze calibration, and denoising, using both our dataset and an existing saliency dataset. We make our model and dataset available at https://github.com/ToyotaResearchInstitute/att-aware/.



### SAGAN: Adversarial Spatial-asymmetric Attention for Noisy Nona-Bayer Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2110.08619v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.08619v2)
- **Published**: 2021-10-16 17:21:57+00:00
- **Updated**: 2021-10-19 03:29:50+00:00
- **Authors**: S M A Sharif, Rizwan Ali Naqvi, Mithun Biswas
- **Comment**: None
- **Journal**: None
- **Summary**: Nona-Bayer colour filter array (CFA) pattern is considered one of the most viable alternatives to traditional Bayer patterns. Despite the substantial advantages, such non-Bayer CFA patterns are susceptible to produce visual artefacts while reconstructing RGB images from noisy sensor data. This study addresses the challenges of learning RGB image reconstruction from noisy Nona-Bayer CFA comprehensively. We propose a novel spatial-asymmetric attention module to jointly learn bi-direction transformation and large-kernel global attention to reduce the visual artefacts. We combine our proposed module with adversarial learning to produce plausible images from Nona-Bayer CFA. The feasibility of the proposed method has been verified and compared with the state-of-the-art image reconstruction method. The experiments reveal that the proposed method can reconstruct RGB images from noisy Nona-Bayer CFA without producing any visually disturbing artefacts. Also, it can outperform the state-of-the-art image reconstruction method in both qualitative and quantitative comparison. Code available: https://github.com/sharif-apu/SAGAN_BMVC21.



### DPC: Unsupervised Deep Point Correspondence via Cross and Self Construction
- **Arxiv ID**: http://arxiv.org/abs/2110.08636v1
- **DOI**: 10.1109/3DV53792.2021.00141
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08636v1)
- **Published**: 2021-10-16 18:41:13+00:00
- **Updated**: 2021-10-16 18:41:13+00:00
- **Authors**: Itai Lang, Dvir Ginzburg, Shai Avidan, Dan Raviv
- **Comment**: 3DV 2021
- **Journal**: None
- **Summary**: We present a new method for real-time non-rigid dense correspondence between point clouds based on structured shape construction. Our method, termed Deep Point Correspondence (DPC), requires a fraction of the training data compared to previous techniques and presents better generalization capabilities. Until now, two main approaches have been suggested for the dense correspondence problem. The first is a spectral-based approach that obtains great results on synthetic datasets but requires mesh connectivity of the shapes and long inference processing time while being unstable in real-world scenarios. The second is a spatial approach that uses an encoder-decoder framework to regress an ordered point cloud for the matching alignment from an irregular input. Unfortunately, the decoder brings considerable disadvantages, as it requires a large amount of training data and struggles to generalize well in cross-dataset evaluations. DPC's novelty lies in its lack of a decoder component. Instead, we use latent similarity and the input coordinates themselves to construct the point cloud and determine correspondence, replacing the coordinate regression done by the decoder. Extensive experiments show that our construction scheme leads to a performance boost in comparison to recent state-of-the-art correspondence methods. Our code is publicly available at https://github.com/dvirginz/DPC.



### Face Verification with Challenging Imposters and Diversified Demographics
- **Arxiv ID**: http://arxiv.org/abs/2110.08667v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08667v1)
- **Published**: 2021-10-16 21:34:59+00:00
- **Updated**: 2021-10-16 21:34:59+00:00
- **Authors**: Adrian Popescu, Liviu-Daniel Åtefan, JÃ©rÃ´me Deshayes-Chossart, Bogdan Ionescu
- **Comment**: None
- **Journal**: None
- **Summary**: Face verification aims to distinguish between genuine and imposter pairs of faces, which include the same or different identities, respectively. The performance reported in recent years gives the impression that the task is practically solved. Here, we revisit the problem and argue that existing evaluation datasets were built using two oversimplifying design choices. First, the usual identity selection to form imposter pairs is not challenging enough because, in practice, verification is needed to detect challenging imposters. Second, the underlying demographics of existing datasets are often insufficient to account for the wide diversity of facial characteristics of people from across the world. To mitigate these limitations, we introduce the $FaVCI2D$ dataset. Imposter pairs are challenging because they include visually similar faces selected from a large pool of demographically diversified identities. The dataset also includes metadata related to gender, country and age to facilitate fine-grained analysis of results. $FaVCI2D$ is generated from freely distributable resources. Experiments with state-of-the-art deep models that provide nearly 100\% performance on existing datasets show a significant performance drop for $FaVCI2D$, confirming our starting hypothesis. Equally important, we analyze legal and ethical challenges which appeared in recent years and hindered the development of face analysis research. We introduce a series of design choices which address these challenges and make the dataset constitution and usage more sustainable and fairer. $FaVCI2D$ is available at~\url{https://github.com/AIMultimediaLab/FaVCI2D-Face-Verification-with-Challenging-Imposters-and-Diversified-Demographics}.



### An Acceleration Method Based on Deep Learning and Multilinear Feature Space
- **Arxiv ID**: http://arxiv.org/abs/2110.08679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.08679v1)
- **Published**: 2021-10-16 23:49:12+00:00
- **Updated**: 2021-10-16 23:49:12+00:00
- **Authors**: Michel Vinagreiro Edson Kitani Armando Lagana Leopoldo Yoshioka
- **Comment**: 20 pages, International Journal of Artificial Intelligence and
  Applications
- **Journal**: None
- **Summary**: Computer vision plays a crucial role in Advanced Assistance Systems. Most computer vision systems are based on Deep Convolutional Neural Networks (deep CNN) architectures. However, the high computational resource to run a CNN algorithm is demanding. Therefore, the methods to speed up computation have become a relevant research issue. Even though several works on architecture reduction found in the literature have not yet been achieved satisfactory results for embedded real-time system applications. This paper presents an alternative approach based on the Multilinear Feature Space (MFS) method resorting to transfer learning from large CNN architectures. The proposed method uses CNNs to generate feature maps, although it does not work as complexity reduction approach. After the training process, the generated features maps are used to create vector feature space. We use this new vector space to make projections of any new sample to classify them. Our method, named AMFC, uses the transfer learning from pre-trained CNN to reduce the classification time of new sample image, with minimal accuracy loss. Our method uses the VGG-16 model as the base CNN architecture for experiments; however, the method works with any similar CNN model. Using the well-known Vehicle Image Database and the German Traffic Sign Recognition Benchmark, we compared the classification time of the original VGG-16 model with the AMFC method, and our method is, on average, 17 times faster. The fast classification time reduces the computational and memory demands in embedded applications requiring a large CNN architecture.



