# Arxiv Papers in cs.CV on 2021-10-14
### A Comprehensive Study on Torchvision Pre-trained Models for Fine-grained Inter-species Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.07097v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.07097v1)
- **Published**: 2021-10-14 00:36:02+00:00
- **Updated**: 2021-10-14 00:36:02+00:00
- **Authors**: Feras Albardi, H M Dipu Kabir, Md Mahbub Islam Bhuiyan, Parham M. Kebria, Abbas Khosravi, Saeid Nahavandi
- **Comment**: Accepted
- **Journal**: 2021 IEEE International Conference on Systems, Man, and
  Cybernetics
- **Summary**: This study aims to explore different pre-trained models offered in the Torchvision package which is available in the PyTorch library. And investigate their effectiveness on fine-grained images classification. Transfer Learning is an effective method of achieving extremely good performance with insufficient training data. In many real-world situations, people cannot collect sufficient data required to train a deep neural network model efficiently. Transfer Learning models are pre-trained on a large data set, and can bring a good performance on smaller datasets with significantly lower training time. Torchvision package offers us many models to apply the Transfer Learning on smaller datasets. Therefore, researchers may need a guideline for the selection of a good model. We investigate Torchvision pre-trained models on four different data sets: 10 Monkey Species, 225 Bird Species, Fruits 360, and Oxford 102 Flowers. These data sets have images of different resolutions, class numbers, and different achievable accuracies. We also apply their usual fully-connected layer and the Spinal fully-connected layer to investigate the effectiveness of SpinalNet. The Spinal fully-connected layer brings better performance in most situations. We apply the same augmentation for different models for the same data set for a fair comparison. This paper may help future Computer Vision researchers in choosing a proper Transfer Learning model.



### Video-based cattle identification and action recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.07103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07103v1)
- **Published**: 2021-10-14 00:55:56+00:00
- **Updated**: 2021-10-14 00:55:56+00:00
- **Authors**: Chuong Nguyen, Dadong Wang, Karl Von Richter, Philip Valencia, Flavio A. P. Alvarenga, Gregory Bishop-Hurley
- **Comment**: 5 pages, 7 figures, DICTA2021
- **Journal**: None
- **Summary**: We demonstrate a working prototype for the monitoring of cow welfare by automatically analysing the animal behaviours. Deep learning models have been developed and tested with videos acquired in a farm, and a precision of 81.2\% has been achieved for cow identification. An accuracy of 84.4\% has been achieved for the detection of drinking events, and 94.4\% for the detection of grazing events. Experimental results show that the proposed deep learning method can be used to identify the behaviours of individual animals to enable automated farm provenance. Our raw and ground-truth dataset will be released as the first public video dataset for cow identification and action recognition. Recommendations for further development are also provided.



### Weakly Supervised Semantic Segmentation by Pixel-to-Prototype Contrast
- **Arxiv ID**: http://arxiv.org/abs/2110.07110v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07110v3)
- **Published**: 2021-10-14 01:44:57+00:00
- **Updated**: 2022-03-14 01:52:36+00:00
- **Authors**: Ye Du, Zehua Fu, Qingjie Liu, Yunhong Wang
- **Comment**: 10 pages, 5 figures. Accepted by CVPR'22
- **Journal**: None
- **Summary**: Though image-level weakly supervised semantic segmentation (WSSS) has achieved great progress with Class Activation Maps (CAMs) as the cornerstone, the large supervision gap between classification and segmentation still hampers the model to generate more complete and precise pseudo masks for segmentation. In this study, we propose weakly-supervised pixel-to-prototype contrast that can provide pixel-level supervisory signals to narrow the gap. Guided by two intuitive priors, our method is executed across different views and within per single view of an image, aiming to impose cross-view feature semantic consistency regularization and facilitate intra(inter)-class compactness(dispersion) of the feature space. Our method can be seamlessly incorporated into existing WSSS models without any changes to the base networks and does not incur any extra inference burden. Extensive experiments manifest that our method consistently improves two strong baselines by large margins, demonstrating the effectiveness. Specifically, built on top of SEAM, we improve the initial seed mIoU on PASCAL VOC 2012 from 55.4% to 61.5%. Moreover, armed with our method, we increase the segmentation mIoU of EPS from 70.8% to 73.6%, achieving new state-of-the-art.



### Nuisance-Label Supervision: Robustness Improvement by Free Labels
- **Arxiv ID**: http://arxiv.org/abs/2110.07118v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07118v1)
- **Published**: 2021-10-14 02:07:00+00:00
- **Updated**: 2021-10-14 02:07:00+00:00
- **Authors**: Xinyue Wei, Weichao Qiu, Yi Zhang, Zihao Xiao, Alan Yuille
- **Comment**: ICCV 2021 Workshop
- **Journal**: None
- **Summary**: In this paper, we present a Nuisance-label Supervision (NLS) module, which can make models more robust to nuisance factor variations. Nuisance factors are those irrelevant to a task, and an ideal model should be invariant to them. For example, an activity recognition model should perform consistently regardless of the change of clothes and background. But our experiments show existing models are far from this capability. So we explicitly supervise a model with nuisance labels to make extracted features less dependent on nuisance factors. Although the values of nuisance factors are rarely annotated, we demonstrate that besides existing annotations, nuisance labels can be acquired freely from data augmentation and synthetic data. Experiments show consistent improvement in robustness towards image corruption and appearance change in action recognition.



### Making Corgis Important for Honeycomb Classification: Adversarial Attacks on Concept-based Explainability Tools
- **Arxiv ID**: http://arxiv.org/abs/2110.07120v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.07120v2)
- **Published**: 2021-10-14 02:12:33+00:00
- **Updated**: 2022-07-26 13:23:56+00:00
- **Authors**: Davis Brown, Henry Kvinge
- **Comment**: AdvML Frontiers 2022 @ ICML 2022 workshop
- **Journal**: None
- **Summary**: Methods for model explainability have become increasingly critical for testing the fairness and soundness of deep learning. Concept-based interpretability techniques, which use a small set of human-interpretable concept exemplars in order to measure the influence of a concept on a model's internal representation of input, are an important thread in this line of research. In this work we show that these explainability methods can suffer the same vulnerability to adversarial attacks as the models they are meant to analyze. We demonstrate this phenomenon on two well-known concept-based interpretability methods: TCAV and faceted feature visualization. We show that by carefully perturbing the examples of the concept that is being investigated, we can radically change the output of the interpretability method. The attacks that we propose can either induce positive interpretations (polka dots are an important concept for a model when classifying zebras) or negative interpretations (stripes are not an important factor in identifying images of a zebra). Our work highlights the fact that in safety-critical applications, there is need for security around not only the machine learning pipeline but also the model interpretation process.



### Region Semantically Aligned Network for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.07130v1
- **DOI**: 10.1145/3459637.3482471
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.07130v1)
- **Published**: 2021-10-14 03:23:40+00:00
- **Updated**: 2021-10-14 03:23:40+00:00
- **Authors**: Ziyang Wang, Yunhao Gou, Jingjing Li, Yu Zhang, Yang Yang
- **Comment**: Accepted to CIKM 2021
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to recognize unseen classes based on the knowledge of seen classes. Previous methods focused on learning direct embeddings from global features to the semantic space in hope of knowledge transfer from seen classes to unseen classes. However, an unseen class shares local visual features with a set of seen classes and leveraging global visual features makes the knowledge transfer ineffective. To tackle this problem, we propose a Region Semantically Aligned Network (RSAN), which maps local features of unseen classes to their semantic attributes. Instead of using global features which are obtained by an average pooling layer after an image encoder, we directly utilize the output of the image encoder which maintains local information of the image. Concretely, we obtain each attribute from a specific region of the output and exploit these attributes for recognition. As a result, the knowledge of seen classes can be successfully transferred to unseen classes in a region-bases manner. In addition, we regularize the image encoder through attribute regression with a semantic knowledge to extract robust and attribute-related visual features. Experiments on several standard ZSL datasets reveal the benefit of the proposed RSAN method, outperforming state-of-the-art methods.



### A CLIP-Enhanced Method for Video-Language Understanding
- **Arxiv ID**: http://arxiv.org/abs/2110.07137v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2110.07137v1)
- **Published**: 2021-10-14 03:50:23+00:00
- **Updated**: 2021-10-14 03:50:23+00:00
- **Authors**: Guohao Li, Feng He, Zhifan Feng
- **Comment**: 3 pages, 1 figure, 2 tables. Technical report
- **Journal**: None
- **Summary**: This technical report summarizes our method for the Video-And-Language Understanding Evaluation (VALUE) challenge (https://value-benchmark.github.io/challenge\_2021.html). We propose a CLIP-Enhanced method to incorporate the image-text pretrained knowledge into downstream video-text tasks. Combined with several other improved designs, our method outperforms the state-of-the-art by $2.4\%$ ($57.58$ to $60.00$) Meta-Ave score on VALUE benchmark.



### Unsupervised Data-Driven Nuclei Segmentation For Histology Images
- **Arxiv ID**: http://arxiv.org/abs/2110.07147v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2110.07147v1)
- **Published**: 2021-10-14 04:26:50+00:00
- **Updated**: 2021-10-14 04:26:50+00:00
- **Authors**: Vasileios Magoulianitis, Peida Han, Yijing Yang, C. -C. Jay Kuo
- **Comment**: 5 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: An unsupervised data-driven nuclei segmentation method for histology images, called CBM, is proposed in this work. CBM consists of three modules applied in a block-wise manner: 1) data-driven color transform for energy compaction and dimension reduction, 2) data-driven binarization, and 3) incorporation of geometric priors with morphological processing. CBM comes from the first letter of the three modules - "Color transform", "Binarization" and "Morphological processing". Experiments on the MoNuSeg dataset validate the effectiveness of the proposed CBM method. CBM outperforms all other unsupervised methods and offers a competitive standing among supervised models based on the Aggregated Jaccard Index (AJI) metric.



### Comparative Analysis of Deep Learning Algorithms for Classification of COVID-19 X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2110.09294v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.09294v1)
- **Published**: 2021-10-14 04:51:32+00:00
- **Updated**: 2021-10-14 04:51:32+00:00
- **Authors**: Unsa Maheen, Khawar Iqbal Malik, Gohar Ali
- **Comment**: None
- **Journal**: None
- **Summary**: The Coronavirus was first emerged in December, in the city of China named Wuhan in 2019 and spread quickly all over the world. It has very harmful effects all over the global economy, education, social, daily living and general health of humans. To restrict the quick expansion of the disease initially, main difficulty is to explore the positive corona patients as quickly as possible. As there are no automatic tool kits accessible the requirement for supplementary diagnostic tools has risen up. Previous studies have findings acquired from radiological techniques proposed that this kind of images have important details related to the coronavirus. The usage of modified Artificial Intelligence (AI) system in combination with radio-graphical images can be fruitful for the precise and exact solution of this virus and can also be helpful to conquer the issue of deficiency of professional physicians in distant villages. In our research, we analyze the different techniques for the detection of COVID-19 using X-Ray radiographic images of the chest, we examined the different pre-trained CNN models AlexNet, VGG-16, MobileNet-V2, SqeezeNet, ResNet-34, ResNet-50 and COVIDX-Net to correct analytics for classification system of COVID-19. Our study shows that the pre trained CNN Model with ResNet-34 technique gives the higher accuracy rate of 98.33, 96.77% precision, and 98.36 F1-score, which is better than other CNN techniques. Our model may be helpful for the researchers to fine train the CNN model for the the quick screening of COVID patients.



### DeepSSM: A Blueprint for Image-to-Shape Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2110.07152v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.07152v2)
- **Published**: 2021-10-14 04:52:37+00:00
- **Updated**: 2022-03-16 15:46:08+00:00
- **Authors**: Riddhish Bhalodia, Shireen Elhabian, Jadie Adams, Wenzheng Tao, Ladislav Kavan, Ross Whitaker
- **Comment**: pre-print
- **Journal**: None
- **Summary**: Statistical shape modeling (SSM) characterizes anatomical variations in a population of shapes generated from medical images. SSM requires consistent shape representation across samples in shape cohort. Establishing this representation entails a processing pipeline that includes anatomy segmentation, re-sampling, registration, and non-linear optimization. These shape representations are then used to extract low-dimensional shape descriptors that facilitate subsequent analyses in different applications. However, the current process of obtaining these shape descriptors from imaging data relies on human and computational resources, requiring domain expertise for segmenting anatomies of interest. Moreover, this same taxing pipeline needs to be repeated to infer shape descriptors for new image data using a pre-trained/existing shape model. Here, we propose DeepSSM, a deep learning-based framework for learning the functional mapping from images to low-dimensional shape descriptors and their associated shape representations, thereby inferring statistical representation of anatomy directly from 3D images. Once trained using an existing shape model, DeepSSM circumvents the heavy and manual pre-processing and segmentation and significantly improves the computational time, making it a viable solution for fully end-to-end SSM applications. In addition, we introduce a model-based data-augmentation strategy to address data scarcity. Finally, this paper presents and analyzes two different architectural variants of DeepSSM with different loss functions using three medical datasets and their downstream clinical application. Experiments showcase that DeepSSM performs comparably or better to the state-of-the-art SSM both quantitatively and on application-driven downstream tasks. Therefore, DeepSSM aims to provide a comprehensive blueprint for deep learning-based image-to-shape models.



### Semantically Distributed Robust Optimization for Vision-and-Language Inference
- **Arxiv ID**: http://arxiv.org/abs/2110.07165v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2110.07165v2)
- **Published**: 2021-10-14 06:02:46+00:00
- **Updated**: 2022-03-14 23:32:07+00:00
- **Authors**: Tejas Gokhale, Abhishek Chaudhary, Pratyay Banerjee, Chitta Baral, Yezhou Yang
- **Comment**: Findings of ACL 2022; code available at
  https://github.com/ASU-APG/VLI_SDRO
- **Journal**: None
- **Summary**: Analysis of vision-and-language models has revealed their brittleness under linguistic phenomena such as paraphrasing, negation, textual entailment, and word substitutions with synonyms or antonyms. While data augmentation techniques have been designed to mitigate against these failure modes, methods that can integrate this knowledge into the training pipeline remain under-explored. In this paper, we present \textbf{SDRO}, a model-agnostic method that utilizes a set linguistic transformations in a distributed robust optimization setting, along with an ensembling technique to leverage these transformations during inference. Experiments on benchmark datasets with images (NLVR$^2$) and video (VIOLIN) demonstrate performance improvements as well as robustness to adversarial attacks. Experiments on binary VQA explore the generalizability of this method to other V\&L tasks.



### SGoLAM: Simultaneous Goal Localization and Mapping for Multi-Object Goal Navigation
- **Arxiv ID**: http://arxiv.org/abs/2110.07171v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.07171v1)
- **Published**: 2021-10-14 06:15:14+00:00
- **Updated**: 2021-10-14 06:15:14+00:00
- **Authors**: Junho Kim, Eun Sun Lee, Mingi Lee, Donsu Zhang, Young Min Kim
- **Comment**: None
- **Journal**: None
- **Summary**: We present SGoLAM, short for simultaneous goal localization and mapping, which is a simple and efficient algorithm for Multi-Object Goal navigation. Given an agent equipped with an RGB-D camera and a GPS/Compass sensor, our objective is to have the agent navigate to a sequence of target objects in realistic 3D environments. Our pipeline fully leverages the strength of classical approaches for visual navigation, by decomposing the problem into two key components: mapping and goal localization. The mapping module converts the depth observations into an occupancy map, and the goal localization module marks the locations of goal objects. The agent's policy is determined using the information provided by the two modules: if a current goal is found, plan towards the goal and otherwise, perform exploration. As our approach does not require any training of neural networks, it could be used in an off-the-shelf manner, and amenable for fast generalization in new, unseen environments. Nonetheless, our approach performs on par with the state-of-the-art learning-based approaches. SGoLAM is ranked 2nd in the CVPR 2021 MultiON (Multi-Object Goal Navigation) challenge. We have made our code publicly available at \emph{https://github.com/eunsunlee/SGoLAM}.



### Adversarial examples by perturbing high-level features in intermediate decoder layers
- **Arxiv ID**: http://arxiv.org/abs/2110.07182v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.07182v1)
- **Published**: 2021-10-14 07:08:15+00:00
- **Updated**: 2021-10-14 07:08:15+00:00
- **Authors**: Vojtěch Čermák, Lukáš Adam
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel method for creating adversarial examples. Instead of perturbing pixels, we use an encoder-decoder representation of the input image and perturb intermediate layers in the decoder. This changes the high-level features provided by the generative model. Therefore, our perturbation possesses semantic meaning, such as a longer beak or green tints. We formulate this task as an optimization problem by minimizing the Wasserstein distance between the adversarial and initial images under a misclassification constraint. We employ the projected gradient method with a simple inexact projection. Due to the projection, all iterations are feasible, and our method always generates adversarial images. We perform numerical experiments on the MNIST and ImageNet datasets in both targeted and untargeted settings. We demonstrate that our adversarial images are much less vulnerable to steganographic defence techniques than pixel-based attacks. Moreover, we show that our method modifies key features such as edges and that defence techniques based on adversarial training are vulnerable to our attacks.



### Self-Supervised Domain Adaptation for Visual Navigation with Global Map Consistency
- **Arxiv ID**: http://arxiv.org/abs/2110.07184v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.07184v1)
- **Published**: 2021-10-14 07:14:36+00:00
- **Updated**: 2021-10-14 07:14:36+00:00
- **Authors**: Eun Sun Lee, Junho Kim, Young Min Kim
- **Comment**: Accepted to WACV 2022
- **Journal**: None
- **Summary**: We propose a light-weight, self-supervised adaptation for a visual navigation agent to generalize to unseen environment. Given an embodied agent trained in a noiseless environment, our objective is to transfer the agent to a noisy environment where actuation and odometry sensor noise is present. Our method encourages the agent to maximize the consistency between the global maps generated at different time steps in a round-trip trajectory. The proposed task is completely self-supervised, not requiring any supervision from ground-truth pose data or explicit noise model. In addition, optimization of the task objective is extremely light-weight, as training terminates within a few minutes on a commodity GPU. Our experiments show that the proposed task helps the agent to successfully transfer to new, noisy environments. The transferred agent exhibits improved localization and mapping accuracy, further leading to enhanced performance in downstream visual navigation tasks. Moreover, we demonstrate test-time adaptation with our self-supervised task to show its potential applicability in real-world deployment.



### Semi-supervised Multi-task Learning for Semantics and Depth
- **Arxiv ID**: http://arxiv.org/abs/2110.07197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07197v1)
- **Published**: 2021-10-14 07:43:39+00:00
- **Updated**: 2021-10-14 07:43:39+00:00
- **Authors**: Yufeng Wang, Yi-Hsuan Tsai, Wei-Chih Hung, Wenrui Ding, Shuo Liu, Ming-Hsuan Yang
- **Comment**: Accepted at WACV 2022
- **Journal**: None
- **Summary**: Multi-Task Learning (MTL) aims to enhance the model generalization by sharing representations between related tasks for better performance. Typical MTL methods are jointly trained with the complete multitude of ground-truths for all tasks simultaneously. However, one single dataset may not contain the annotations for each task of interest. To address this issue, we propose the Semi-supervised Multi-Task Learning (SemiMTL) method to leverage the available supervisory signals from different datasets, particularly for semantic segmentation and depth estimation tasks. To this end, we design an adversarial learning scheme in our semi-supervised training by leveraging unlabeled data to optimize all the task branches simultaneously and accomplish all tasks across datasets with partial annotations. We further present a domain-aware discriminator structure with various alignment formulations to mitigate the domain discrepancy issue among datasets. Finally, we demonstrate the effectiveness of the proposed method to learn across different datasets on challenging street view and remote sensing benchmarks.



### Towards Using Clothes Style Transfer for Scenario-aware Person Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2110.11894v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.11894v2)
- **Published**: 2021-10-14 07:49:00+00:00
- **Updated**: 2021-10-25 03:59:02+00:00
- **Authors**: Jingning Xu, Benlai Tang, Mingjie Wang, Siyuan Bian, Wenyi Guo, Xiang Yin, Zejun Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Clothes style transfer for person video generation is a challenging task, due to drastic variations of intra-person appearance and video scenarios. To tackle this problem, most recent AdaIN-based architectures are proposed to extract clothes and scenario features for generation. However, these approaches suffer from being short of fine-grained details and are prone to distort the origin person. To further improve the generation performance, we propose a novel framework with disentangled multi-branch encoders and a shared decoder. Moreover, to pursue the strong video spatio-temporal consistency, an inner-frame discriminator is delicately designed with input being cross-frame difference. Besides, the proposed framework possesses the property of scenario adaptation. Extensive experiments on the TEDXPeople benchmark demonstrate the superiority of our method over state-of-the-art approaches in terms of image quality and video coherence.



### Coarse to Fine: Video Retrieval before Moment Localization
- **Arxiv ID**: http://arxiv.org/abs/2110.07201v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07201v1)
- **Published**: 2021-10-14 07:54:36+00:00
- **Updated**: 2021-10-14 07:54:36+00:00
- **Authors**: Zijian Gao, Huanyu Liu, Jingyu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The current state-of-the-art methods for video corpus moment retrieval (VCMR) often use similarity-based feature alignment approach for the sake of convenience and speed. However, late fusion methods like cosine similarity alignment are unable to make full use of the information from both query texts and videos. In this paper, we combine feature alignment with feature fusion to promote the performance on VCMR.



### Unrolled Variational Bayesian Algorithm for Image Blind Deconvolution
- **Arxiv ID**: http://arxiv.org/abs/2110.07202v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2110.07202v1)
- **Published**: 2021-10-14 07:55:26+00:00
- **Updated**: 2021-10-14 07:55:26+00:00
- **Authors**: Yunshi Huang, Emilie Chouzenoux, Jean-Christophe Pesquet
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: In this paper, we introduce a variational Bayesian algorithm (VBA) for image blind deconvolution. Our generic framework incorporates smoothness priors on the unknown blur/image and possible affine constraints (e.g., sum to one) on the blur kernel. One of our main contributions is the integration of VBA within a neural network paradigm, following an unrolling methodology. The proposed architecture is trained in a supervised fashion, which allows us to optimally set two key hyperparameters of the VBA model and lead to further improvements in terms of resulting visual quality. Various experiments involving grayscale/color images and diverse kernel shapes, are performed. The numerical examples illustrate the high performance of our approach when compared to state-of-the-art techniques based on optimization, Bayesian estimation, or deep learning.



### Multi-Layer Pseudo-Supervision for Histopathology Tissue Semantic Segmentation using Patch-level Classification Labels
- **Arxiv ID**: http://arxiv.org/abs/2110.08048v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM, 68U10, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2110.08048v1)
- **Published**: 2021-10-14 08:02:07+00:00
- **Updated**: 2021-10-14 08:02:07+00:00
- **Authors**: Chu Han, Jiatai Lin, Jinhai Mai, Yi Wang, Qingling Zhang, Bingchao Zhao, Xin Chen, Xipeng Pan, Zhenwei Shi, Xiaowei Xu, Su Yao, Lixu Yan, Huan Lin, Zeyan Xu, Xiaomei Huang, Guoqiang Han, Changhong Liang, Zaiyi Liu
- **Comment**: 15 pages, 10 figures, journal
- **Journal**: None
- **Summary**: Tissue-level semantic segmentation is a vital step in computational pathology. Fully-supervised models have already achieved outstanding performance with dense pixel-level annotations. However, drawing such labels on the giga-pixel whole slide images is extremely expensive and time-consuming. In this paper, we use only patch-level classification labels to achieve tissue semantic segmentation on histopathology images, finally reducing the annotation efforts. We proposed a two-step model including a classification and a segmentation phases. In the classification phase, we proposed a CAM-based model to generate pseudo masks by patch-level labels. In the segmentation phase, we achieved tissue semantic segmentation by our proposed Multi-Layer Pseudo-Supervision. Several technical novelties have been proposed to reduce the information gap between pixel-level and patch-level annotations. As a part of this paper, we introduced a new weakly-supervised semantic segmentation (WSSS) dataset for lung adenocarcinoma (LUAD-HistoSeg). We conducted several experiments to evaluate our proposed model on two datasets. Our proposed model outperforms two state-of-the-art WSSS approaches. Note that we can achieve comparable quantitative and qualitative results with the fully-supervised model, with only around a 2\% gap for MIoU and FwIoU. By comparing with manual labeling, our model can greatly save the annotation time from hours to minutes. The source code is available at: \url{https://github.com/ChuHan89/WSSS-Tissue}.



### Task-Driven Deep Image Enhancement Network for Autonomous Driving in Bad Weather
- **Arxiv ID**: http://arxiv.org/abs/2110.07206v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.07206v1)
- **Published**: 2021-10-14 08:03:33+00:00
- **Updated**: 2021-10-14 08:03:33+00:00
- **Authors**: Younkwan Lee, Jihyo Jeon, Yeongmin Ko, Byunggwan Jeon, Moongu Jeon
- **Comment**: Accepted in ICRA21
- **Journal**: None
- **Summary**: Visual perception in autonomous driving is a crucial part of a vehicle to navigate safely and sustainably in different traffic conditions. However, in bad weather such as heavy rain and haze, the performance of visual perception is greatly affected by several degrading effects. Recently, deep learning-based perception methods have addressed multiple degrading effects to reflect real-world bad weather cases but have shown limited success due to 1) high computational costs for deployment on mobile devices and 2) poor relevance between image enhancement and visual perception in terms of the model ability. To solve these issues, we propose a task-driven image enhancement network connected to the high-level vision task, which takes in an image corrupted by bad weather as input. Specifically, we introduce a novel low memory network to reduce most of the layer connections of dense blocks for less memory and computational cost while maintaining high performance. We also introduce a new task-driven training strategy to robustly guide the high-level task model suitable for both high-quality restoration of images and highly accurate perception. Experiment results demonstrate that the proposed method improves the performance among lane and 2D object detection, and depth estimation largely under adverse weather in terms of both low memory and accuracy.



### HUMAN4D: A Human-Centric Multimodal Dataset for Motions and Immersive Media
- **Arxiv ID**: http://arxiv.org/abs/2110.07235v2
- **DOI**: 10.1109/ACCESS.2020.3026276
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.07235v2)
- **Published**: 2021-10-14 09:03:35+00:00
- **Updated**: 2021-10-19 21:19:03+00:00
- **Authors**: Anargyros Chatzitofis, Leonidas Saroglou, Prodromos Boutis, Petros Drakoulis, Nikolaos Zioulis, Shishir Subramanyam, Bart Kevelham, Caecilia Charbonnier, Pablo Cesar, Dimitrios Zarpalas, Stefanos Kollias, Petros Daras
- **Comment**: None
- **Journal**: IEEE Access, 8, 176241-176262, 2020
- **Summary**: We introduce HUMAN4D, a large and multimodal 4D dataset that contains a variety of human activities simultaneously captured by a professional marker-based MoCap, a volumetric capture and an audio recording system. By capturing 2 female and $2$ male professional actors performing various full-body movements and expressions, HUMAN4D provides a diverse set of motions and poses encountered as part of single- and multi-person daily, physical and social activities (jumping, dancing, etc.), along with multi-RGBD (mRGBD), volumetric and audio data. Despite the existence of multi-view color datasets captured with the use of hardware (HW) synchronization, to the best of our knowledge, HUMAN4D is the first and only public resource that provides volumetric depth maps with high synchronization precision due to the use of intra- and inter-sensor HW-SYNC. Moreover, a spatio-temporally aligned scanned and rigged 3D character complements HUMAN4D to enable joint research on time-varying and high-quality dynamic meshes. We provide evaluation baselines by benchmarking HUMAN4D with state-of-the-art human pose estimation and 3D compression methods. For the former, we apply 2D and 3D pose estimation algorithms both on single- and multi-view data cues. For the latter, we benchmark open-source 3D codecs on volumetric data respecting online volumetric video encoding and steady bit-rates. Furthermore, qualitative and quantitative visual comparison between mesh-based volumetric data reconstructed in different qualities showcases the available options with respect to 4D representations. HUMAN4D is introduced to the computer vision and graphics research communities to enable joint research on spatio-temporally aligned pose, volumetric, mRGBD and audio data cues. The dataset and its code are available https://tofis.github.io/myurls/human4d.



### Rethinking Point Cloud Filtering: A Non-Local Position Based Approach
- **Arxiv ID**: http://arxiv.org/abs/2110.07253v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.07253v1)
- **Published**: 2021-10-14 11:03:25+00:00
- **Updated**: 2021-10-14 11:03:25+00:00
- **Authors**: Jinxi Wang, Jincen Jiang, Xuequan Lu, Meili Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Existing position based point cloud filtering methods can hardly preserve sharp geometric features. In this paper, we rethink point cloud filtering from a non-learning non-local non-normal perspective, and propose a novel position based approach for feature-preserving point cloud filtering. Unlike normal based techniques, our method does not require the normal information. The core idea is to first design a similarity metric to search the non-local similar patches of a queried local patch. We then map the non-local similar patches into a canonical space and aggregate the non-local information. The aggregated outcome (i.e. coordinate) will be inversely mapped into the original space. Our method is simple yet effective. Extensive experiments validate our method, and show that it generally outperforms position based methods (deep learning and non-learning), and generates better or comparable outcomes to normal based techniques (deep learning and non-learning).



### DeepMoCap: Deep Optical Motion Capture Using Multiple Depth Sensors and Retro-Reflectors
- **Arxiv ID**: http://arxiv.org/abs/2110.07283v1
- **DOI**: 10.3390/s19020282
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.07283v1)
- **Published**: 2021-10-14 11:40:26+00:00
- **Updated**: 2021-10-14 11:40:26+00:00
- **Authors**: Anargyros Chatzitofis, Dimitrios Zarpalas, Stefanos Kollias, Petros Daras
- **Comment**: None
- **Journal**: Sensors, 19(2), 282, 2019
- **Summary**: In this paper, a marker-based, single-person optical motion capture method (DeepMoCap) is proposed using multiple spatio-temporally aligned infrared-depth sensors and retro-reflective straps and patches (reflectors). DeepMoCap explores motion capture by automatically localizing and labeling reflectors on depth images and, subsequently, on 3D space. Introducing a non-parametric representation to encode the temporal correlation among pairs of colorized depthmaps and 3D optical flow frames, a multi-stage Fully Convolutional Network (FCN) architecture is proposed to jointly learn reflector locations and their temporal dependency among sequential frames. The extracted reflector 2D locations are spatially mapped in 3D space, resulting in robust 3D optical data extraction. The subject's motion is efficiently captured by applying a template-based fitting technique on the extracted optical data. Two datasets have been created and made publicly available for evaluation purposes; one comprising multi-view depth and 3D optical flow annotated images (DMC2.5D), and a second, consisting of spatio-temporally aligned multi-view depth images along with skeleton, inertial and ground truth MoCap data (DMC3D). The FCN model outperforms its competitors on the DMC2.5D dataset using 2D Percentage of Correct Keypoints (PCK) metric, while the motion capture outcome is evaluated against RGB-D and inertial data fusion approaches on DMC3D, outperforming the next best method by 4.5% in total 3D PCK accuracy.



### View Vertically: A Hierarchical Network for Trajectory Prediction via Fourier Spectrums
- **Arxiv ID**: http://arxiv.org/abs/2110.07288v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07288v2)
- **Published**: 2021-10-14 11:48:31+00:00
- **Updated**: 2022-07-13 01:53:25+00:00
- **Authors**: Conghao Wong, Beihao Xia, Ziming Hong, Qinmu Peng, Wei Yuan, Qiong Cao, Yibo Yang, Xinge You
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: Understanding and forecasting future trajectories of agents are critical for behavior analysis, robot navigation, autonomous cars, and other related applications. Previous methods mostly treat trajectory prediction as time sequence generation. Different from them, this work studies agents' trajectories in a "vertical" view, i.e., modeling and forecasting trajectories from the spectral domain. Different frequency bands in the trajectory spectrums could hierarchically reflect agents' motion preferences at different scales. The low-frequency and high-frequency portions could represent their coarse motion trends and fine motion variations, respectively. Accordingly, we propose a hierarchical network V$^2$-Net, which contains two sub-networks, to hierarchically model and predict agents' trajectories with trajectory spectrums. The coarse-level keypoints estimation sub-network first predicts the "minimal" spectrums of agents' trajectories on several "key" frequency portions. Then the fine-level spectrum interpolation sub-network interpolates the spectrums to reconstruct the final predictions. Experimental results display the competitiveness and superiority of V$^2$-Net on both ETH-UCY benchmark and the Stanford Drone Dataset.



### Transformer for Polyp Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.07918v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07918v1)
- **Published**: 2021-10-14 11:58:57+00:00
- **Updated**: 2021-10-14 11:58:57+00:00
- **Authors**: Shijie Liu, Hongyu Zhou, Xiaozhou Shi, Junwen Pan
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, as the Transformer has performed increasingly well on NLP tasks, many researchers have ported the Transformer structure to vision tasks ,bridging the gap between NLP and CV tasks. In this work, we evaluate some deep learning network for the detection track. Because the ground truth is mask, so we can try both the current detection and segmentation method. We select the DETR as our baseline through experiment. Besides, we modify the train strategy to fit the dataset.



### FocusNet: Classifying Better by Focusing on Confusing Classes
- **Arxiv ID**: http://arxiv.org/abs/2110.07307v3
- **DOI**: 10.1016/j.patcog.2022.108709
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07307v3)
- **Published**: 2021-10-14 12:22:59+00:00
- **Updated**: 2022-10-05 07:45:38+00:00
- **Authors**: Xue Zhang, Zehua Sheng, Hui-Liang Shen
- **Comment**: Accepted by Pattern Recognition 2022
- **Journal**: Pattern Recognition 2022
- **Summary**: Nowadays, most classification networks use one-hot encoding to represent categorical data because of its simplicity. However, one-hot encoding may affect the generalization ability as it neglects inter-class correlations. We observe that, even when a neural network trained with one-hot labels produces incorrect predictions, it still pays attention to the target image region and reveals which classes confuse the network. Inspired by this observation, we propose a confusion-focusing mechanism to address the class-confusion issue. Our confusion-focusing mechanism is implemented by a two-branch network architecture. Its baseline branch generates confusing classes, and its FocusNet branch, whose architecture is flexible, discriminates correct labels from these confusing classes. We also introduce a novel focus-picking loss function to improve classification accuracy by encouraging FocusNet to focus on the most confusing classes. The experimental results validate that our FocusNet is effective for image classification on common datasets, and that our focus-picking loss function can also benefit the current neural networks in improving their classification accuracy.



### Modeling dynamic target deformation in camera calibration
- **Arxiv ID**: http://arxiv.org/abs/2110.07322v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.07322v1)
- **Published**: 2021-10-14 12:56:15+00:00
- **Updated**: 2021-10-14 12:56:15+00:00
- **Authors**: Annika Hagemann, Moritz Knorr, Christoph Stiller
- **Comment**: Accepted for publication at IEEE/CVF, WACV 2022
- **Journal**: None
- **Summary**: Most approaches to camera calibration rely on calibration targets of well-known geometry. During data acquisition, calibration target and camera system are typically moved w.r.t. each other, to allow image coverage and perspective versatility. We show that moving the target can lead to small temporary deformations of the target, which can introduce significant errors into the calibration result. While static inaccuracies of calibration targets have been addressed in previous works, to our knowledge, none of the existing approaches can capture time-varying, dynamic deformations. To achieve high-accuracy calibrations despite moving the target, we propose a way to explicitly model dynamic target deformations in camera calibration. This is achieved by using a low-dimensional deformation model with only few parameters per image, which can be optimized jointly with target poses and intrinsics. We demonstrate the effectiveness of modeling dynamic deformations using different calibration targets and show its significance in a structure-from-motion application.



### Domain generalization in deep learning for contrast-enhanced imaging
- **Arxiv ID**: http://arxiv.org/abs/2110.07360v3
- **DOI**: 10.1016/j.compbiomed.2022.106052
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.07360v3)
- **Published**: 2021-10-14 13:44:59+00:00
- **Updated**: 2022-09-06 14:15:33+00:00
- **Authors**: Carla Sendra-Balcells, Víctor M. Campello, Carlos Martín-Isla, David Viladés, Martín L. Descalzo, Andrea Guala, José F. Rodríguez-Palomares, Karim Lekadir
- **Comment**: None
- **Journal**: None
- **Summary**: The domain generalization problem has been widely investigated in deep learning for non-contrast imaging over the last years, but it received limited attention for contrast-enhanced imaging. However, there are marked differences in contrast imaging protocols across clinical centers, in particular in the time between contrast injection and image acquisition, while access to multi-center contrast-enhanced image data is limited compared to available datasets for non-contrast imaging. This calls for new tools for generalizing single-domain, single-center deep learning models across new unseen domains and clinical centers in contrast-enhanced imaging. In this paper, we present an exhaustive evaluation of deep learning techniques to achieve generalizability to unseen clinical centers for contrast-enhanced image segmentation. To this end, several techniques are investigated, optimized and systematically evaluated, including data augmentation, domain mixing, transfer learning and domain adaptation. To demonstrate the potential of domain generalization for contrast-enhanced imaging, the methods are evaluated for ventricular segmentation in contrast-enhanced cardiac magnetic resonance imaging (MRI). The results are obtained based on a multi-center cardiac contrast-enhanced MRI dataset acquired in four hospitals located in three countries (France, Spain and China). They show that the combination of data augmentation and transfer learning can lead to single-center models that generalize well to new clinical centers not included during training. Single-domain neural networks enriched with suitable generalization procedures can reach and even surpass the performance of multi-center, multi-vendor models in contrast-enhanced imaging, hence eliminating the need for comprehensive multi-center datasets to train generalizable models.



### Domain Adaptation on Semantic Segmentation with Separate Affine Transformation in Batch Normalization
- **Arxiv ID**: http://arxiv.org/abs/2110.07376v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.07376v1)
- **Published**: 2021-10-14 14:06:12+00:00
- **Updated**: 2021-10-14 14:06:12+00:00
- **Authors**: Junhao Yan, Woonsok Lee
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, unsupervised domain adaptation (UDA) for semantic segmentation has brought many researchers'attention. Many of them take an approach to design a complex system so as to better align the gap between source and target domain. Instead, we focus on the very basic structure of the deep neural network, Batch Normalization, and propose to replace the Sharing Affine Transformation with our proposed Separate Affine Transformation (SEAT). The proposed SEAT is simple, easily implemented and easy to integrate into existing adversarial learning based UDA methods. Also, to further improve the adaptation quality, we introduce multi level adaptation by adding the lower-level features to the higher-level ones before feeding them to the discriminator, without adding extra discriminator like others. Experiments show that the proposed methods is less complex without losing performance accuracy when compared with other UDA methods.



### HumBugDB: A Large-scale Acoustic Mosquito Dataset
- **Arxiv ID**: http://arxiv.org/abs/2110.07607v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS, E.0; I.2.1; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2110.07607v1)
- **Published**: 2021-10-14 14:18:17+00:00
- **Updated**: 2021-10-14 14:18:17+00:00
- **Authors**: Ivan Kiskin, Marianne Sinka, Adam D. Cobb, Waqas Rafique, Lawrence Wang, Davide Zilli, Benjamin Gutteridge, Rinita Dam, Theodoros Marinos, Yunpeng Li, Dickson Msaky, Emmanuel Kaindoa, Gerard Killeen, Eva Herreros-Moya, Kathy J. Willis, Stephen J. Roberts
- **Comment**: Accepted at the 35th Conference on Neural Information Processing
  Systems (NeurIPS 2021) Track on Datasets and Benchmarks. 10 pages main, 39
  pages including appendix. This paper accompanies the dataset found at
  https://zenodo.org/record/4904800 with corresponding code at
  https://github.com/HumBug-Mosquito/HumBugDB
- **Journal**: None
- **Summary**: This paper presents the first large-scale multi-species dataset of acoustic recordings of mosquitoes tracked continuously in free flight. We present 20 hours of audio recordings that we have expertly labelled and tagged precisely in time. Significantly, 18 hours of recordings contain annotations from 36 different species. Mosquitoes are well-known carriers of diseases such as malaria, dengue and yellow fever. Collecting this dataset is motivated by the need to assist applications which utilise mosquito acoustics to conduct surveys to help predict outbreaks and inform intervention policy. The task of detecting mosquitoes from the sound of their wingbeats is challenging due to the difficulty in collecting recordings from realistic scenarios. To address this, as part of the HumBug project, we conducted global experiments to record mosquitoes ranging from those bred in culture cages to mosquitoes captured in the wild. Consequently, the audio recordings vary in signal-to-noise ratio and contain a broad range of indoor and outdoor background environments from Tanzania, Thailand, Kenya, the USA and the UK. In this paper we describe in detail how we collected, labelled and curated the data. The data is provided from a PostgreSQL database, which contains important metadata such as the capture method, age, feeding status and gender of the mosquitoes. Additionally, we provide code to extract features and train Bayesian convolutional neural networks for two key tasks: the identification of mosquitoes from their corresponding background environments, and the classification of detected mosquitoes into species. Our extensive dataset is both challenging to machine learning researchers focusing on acoustic identification, and critical to entomologists, geo-spatial modellers and other domain experts to understand mosquito behaviour, model their distribution, and manage the threat they pose to humans.



### Self-Supervised Learning by Estimating Twin Class Distributions
- **Arxiv ID**: http://arxiv.org/abs/2110.07402v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.07402v4)
- **Published**: 2021-10-14 14:39:39+00:00
- **Updated**: 2021-12-06 06:52:45+00:00
- **Authors**: Feng Wang, Tao Kong, Rufeng Zhang, Huaping Liu, Hang Li
- **Comment**: None
- **Journal**: None
- **Summary**: We present TWIST, a simple and theoretically explainable self-supervised representation learning method by classifying large-scale unlabeled datasets in an end-to-end way. We employ a siamese network terminated by a softmax operation to produce twin class distributions of two augmented images. Without supervision, we enforce the class distributions of different augmentations to be consistent. However, simply minimizing the divergence between augmentations will cause collapsed solutions, i.e., outputting the same class probability distribution for all images. In this case, no information about the input image is left. To solve this problem, we propose to maximize the mutual information between the input and the class predictions. Specifically, we minimize the entropy of the distribution for each sample to make the class prediction for each sample assertive and maximize the entropy of the mean distribution to make the predictions of different samples diverse. In this way, TWIST can naturally avoid the collapsed solutions without specific designs such as asymmetric network, stop-gradient operation, or momentum encoder. As a result, TWIST outperforms state-of-the-art methods on a wide range of tasks. Especially, TWIST performs surprisingly well on semi-supervised learning, achieving 61.2% top-1 accuracy with 1% ImageNet labels using a ResNet-50 as backbone, surpassing previous best results by an absolute improvement of 6.2%. Codes and pre-trained models are given on: https://github.com/bytedance/TWIST



### RGB-D Image Inpainting Using Generative Adversarial Network with a Late Fusion Approach
- **Arxiv ID**: http://arxiv.org/abs/2110.07413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07413v1)
- **Published**: 2021-10-14 14:44:01+00:00
- **Updated**: 2021-10-14 14:44:01+00:00
- **Authors**: Ryo Fujii, Ryo Hachiuma, Hideo Saito
- **Comment**: Accepted at AVR 2020
- **Journal**: None
- **Summary**: Diminished reality is a technology that aims to remove objects from video images and fills in the missing region with plausible pixels. Most conventional methods utilize the different cameras that capture the same scene from different viewpoints to allow regions to be removed and restored. In this paper, we propose an RGB-D image inpainting method using generative adversarial network, which does not require multiple cameras. Recently, an RGB image inpainting method has achieved outstanding results by employing a generative adversarial network. However, RGB inpainting methods aim to restore only the texture of the missing region and, therefore, does not recover geometric information (i.e, 3D structure of the scene). We expand conventional image inpainting method to RGB-D image inpainting to jointly restore the texture and geometry of missing regions from a pair of RGB and depth images. Inspired by other tasks that use RGB and depth images (e.g., semantic segmentation and object detection), we propose late fusion approach that exploits the advantage of RGB and depth information each other. The experimental results verify the effectiveness of our proposed method.



### Automatic Modeling of Social Concepts Evoked by Art Images as Multimodal Frames
- **Arxiv ID**: http://arxiv.org/abs/2110.07420v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.DL, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2110.07420v1)
- **Published**: 2021-10-14 14:50:22+00:00
- **Updated**: 2021-10-14 14:50:22+00:00
- **Authors**: Delfina Sol Martinez Pandiani, Valentina Presutti
- **Comment**: First International Workshop on Multisensory Data and Knowledge at
  the 3rd Conference on Language, Data and Knowledge (2021)
- **Journal**: None
- **Summary**: Social concepts referring to non-physical objects--such as revolution, violence, or friendship--are powerful tools to describe, index, and query the content of visual data, including ever-growing collections of art images from the Cultural Heritage (CH) field. While much progress has been made towards complete image understanding in computer vision, automatic detection of social concepts evoked by images is still a challenge. This is partly due to the well-known semantic gap problem, worsened for social concepts given their lack of unique physical features, and reliance on more unspecific features than concrete concepts. In this paper, we propose the translation of recent cognitive theories about social concept representation into a software approach to represent them as multimodal frames, by integrating multisensory data. Our method focuses on the extraction, analysis, and integration of multimodal features from visual art material tagged with the concepts of interest. We define a conceptual model and present a novel ontology for formally representing social concepts as multimodal frames. Taking the Tate Gallery's collection as an empirical basis, we experiment our method on a corpus of art images to provide a proof of concept of its potential. We discuss further directions of research, and provide all software, data sources, and results.



### 3D Structure from 2D Microscopy images using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.07608v1
- **DOI**: 10.3389/fbinf.2021.740342
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.07608v1)
- **Published**: 2021-10-14 14:55:41+00:00
- **Updated**: 2021-10-14 14:55:41+00:00
- **Authors**: Benjamin J. Blundell, Christian Sieben, Suliana Manley, Ed Rosten, QueeLim Ch'ng, Susan Cox
- **Comment**: 32 Pages, 12 figures. Awaiting publication in 'Frontiers in
  Bioinformatics - Computational Bioimaging' -
  https://www.frontiersin.org/journals/bioinformatics
- **Journal**: None
- **Summary**: Understanding the structure of a protein complex is crucial indetermining its function. However, retrieving accurate 3D structures from microscopy images is highly challenging, particularly as many imaging modalities are two-dimensional. Recent advances in Artificial Intelligence have been applied to this problem, primarily using voxel based approaches to analyse sets of electron microscopy images. Herewe present a deep learning solution for reconstructing the protein com-plexes from a number of 2D single molecule localization microscopy images, with the solution being completely unconstrained. Our convolutional neural network coupled with a differentiable renderer predicts pose and derives a single structure. After training, the network is dis-carded, with the output of this method being a structural model which fits the data-set. We demonstrate the performance of our system on two protein complexes: CEP152 (which comprises part of the proximal toroid of the centriole) and centrioles.



### Possibilistic Fuzzy Local Information C-Means with Automated Feature Selection for Seafloor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.07433v1
- **DOI**: 10.1117/12.2305178
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.07433v1)
- **Published**: 2021-10-14 15:00:12+00:00
- **Updated**: 2021-10-14 15:00:12+00:00
- **Authors**: Joshua Peeples, Daniel Suen, Alina Zare, James Keller
- **Comment**: Proc. SPIE 10628, Detection and Sensing of Mines, Explosive Objects,
  and Obscured Targets XXIII (30 April 2018), 14 pages, 7 figures, 5 tables
- **Journal**: None
- **Summary**: The Possibilistic Fuzzy Local Information C-Means (PFLICM) method is presented as a technique to segment side-look synthetic aperture sonar (SAS) imagery into distinct regions of the sea-floor. In this work, we investigate and present the results of an automated feature selection approach for SAS image segmentation. The chosen features and resulting segmentation from the image will be assessed based on a select quantitative clustering validity criterion and the subset of the features that reach a desired threshold will be used for the segmentation process.



### Inverse Problems Leveraging Pre-trained Contrastive Representations
- **Arxiv ID**: http://arxiv.org/abs/2110.07439v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.07439v2)
- **Published**: 2021-10-14 15:06:30+00:00
- **Updated**: 2021-10-26 22:10:54+00:00
- **Authors**: Sriram Ravula, Georgios Smyrnis, Matt Jordan, Alexandros G. Dimakis
- **Comment**: Initial version. Final version to appear in Thirty-fifth Conference
  on Neural Information Processing Systems (NeurIPS 2021)
- **Journal**: None
- **Summary**: We study a new family of inverse problems for recovering representations of corrupted data. We assume access to a pre-trained representation learning network R(x) that operates on clean images, like CLIP. The problem is to recover the representation of an image R(x), if we are only given a corrupted version A(x), for some known forward operator A. We propose a supervised inversion method that uses a contrastive objective to obtain excellent representations for highly corrupted images. Using a linear probe on our robust representations, we achieve a higher accuracy than end-to-end supervised baselines when classifying images with various types of distortions, including blurring, additive noise, and random pixel masking. We evaluate on a subset of ImageNet and observe that our method is robust to varying levels of distortion. Our method outperforms end-to-end baselines even with a fraction of the labeled data in a wide range of forward operators.



### Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?
- **Arxiv ID**: http://arxiv.org/abs/2110.07472v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.07472v4)
- **Published**: 2021-10-14 15:46:53+00:00
- **Updated**: 2022-02-05 17:24:58+00:00
- **Authors**: Matthew Farrell, Blake Bordelon, Shubhendu Trivedi, Cengiz Pehlevan
- **Comment**: Version accepted to ICLR 2022
- **Journal**: None
- **Summary**: Equivariance has emerged as a desirable property of representations of objects subject to identity-preserving transformations that constitute a group, such as translations and rotations. However, the expressivity of a representation constrained by group equivariance is still not fully understood. We address this gap by providing a generalization of Cover's Function Counting Theorem that quantifies the number of linearly separable and group-invariant binary dichotomies that can be assigned to equivariant representations of objects. We find that the fraction of separable dichotomies is determined by the dimension of the space that is fixed by the group action. We show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling. While other operations do not change the fraction of separable dichotomies, local pooling decreases the fraction, despite being a highly nonlinear operation. Finally, we test our theory on intermediate representations of randomly initialized and fully trained convolutional neural networks and find perfect agreement.



### Non-contact Atrial Fibrillation Detection from Face Videos by Learning Systolic Peaks
- **Arxiv ID**: http://arxiv.org/abs/2110.07610v2
- **DOI**: 10.1109/JBHI.2022.3193117
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.07610v2)
- **Published**: 2021-10-14 16:04:57+00:00
- **Updated**: 2022-08-06 10:03:36+00:00
- **Authors**: Zhaodong Sun, Juhani Junttila, Mikko Tulppo, Tapio Seppänen, Xiaobai Li
- **Comment**: Published on JBHI
- **Journal**: None
- **Summary**: Objective: We propose a non-contact approach for atrial fibrillation (AF) detection from face videos. Methods: Face videos, electrocardiography (ECG), and contact photoplethysmography (PPG) from 100 healthy subjects and 100 AF patients are recorded. Data recordings from healthy subjects are all labeled as healthy. Two cardiologists evaluated ECG recordings of patients and labeled each recording as AF, sinus rhythm (SR), or atrial flutter (AFL). We use the 3D convolutional neural network for remote PPG monitoring and propose a novel loss function (Wasserstein distance) to use the timing of systolic peaks from contact PPG as the label for our model training. Then a set of heart rate variability (HRV) features are calculated from the inter-beat intervals, and a support vector machine (SVM) classifier is trained with HRV features. Results: Our proposed method can accurately extract systolic peaks from face videos for AF detection. The proposed method is trained with subject-independent 10-fold cross-validation with 30s video clips and tested on two tasks. 1) Classification of healthy versus AF: the accuracy, sensitivity, and specificity are 96.00%, 95.36%, and 96.12%. 2) Classification of SR versus AF: the accuracy, sensitivity, and specificity are 95.23%, 98.53%, and 91.12%. In addition, we also demonstrate the feasibility of non-contact AFL detection. Conclusion: We achieve good performance of non-contact AF detection by learning systolic peaks. Significance: non-contact AF detection can be used for self-screening of AF symptoms for suspectable populations at home or self-monitoring of AF recurrence after treatment for chronic patients.



### Simple Baseline for Single Human Motion Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2110.07495v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07495v1)
- **Published**: 2021-10-14 16:10:54+00:00
- **Updated**: 2021-10-14 16:10:54+00:00
- **Authors**: Chenxi Wang, Yunfeng Wang, Zixuan Huang, Zhiwen Chen
- **Comment**: ICCV SoMoF Workshop, 2021
- **Journal**: None
- **Summary**: Global human motion forecasting is important in many fields, which is the combination of global human trajectory prediction and local human pose prediction. Visual and social information are often used to boost model performance, however, they may consume too much computational resource. In this paper, we establish a simple but effective baseline for single human motion forecasting without visual and social information, equipped with useful training tricks. Our method "futuremotion_ICCV21" outperforms existing methods by a large margin on SoMoF benchmark. We hope our work provide new ideas for future research.



### TDACNN: Target-domain-free Domain Adaptation Convolutional Neural Network for Drift Compensation in Gas Sensors
- **Arxiv ID**: http://arxiv.org/abs/2110.07509v3
- **DOI**: 10.1016/j.snb.2022.131739
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.07509v3)
- **Published**: 2021-10-14 16:30:17+00:00
- **Updated**: 2022-03-26 04:08:06+00:00
- **Authors**: Yuelin Zhang, Sihao Xiang, Zehuan Wang, Xiaoyan Peng, Yutong Tian, Shukai Duan, Jia Yan
- **Comment**: Published in Sensors and Actuators B: Chemical, Volume 361, 15 June
  2022, 131739
- **Journal**: [J]. Sensors and Actuators B: Chemical, 2022: 131739
- **Summary**: Sensor drift is a long-existing unpredictable problem that deteriorates the performance of gaseous substance recognition, calling for an antidrift domain adaptation algorithm. However, the prerequisite for traditional methods to achieve fine results is to have data from both nondrift distributions (source domain) and drift distributions (target domain) for domain alignment, which is usually unrealistic and unachievable in real-life scenarios. To compensate for this, in this paper, deep learning based on a target-domain-free domain adaptation convolutional neural network (TDACNN) is proposed. The main concept is that CNNs extract not only the domain-specific features of samples but also the domain-invariant features underlying both the source and target domains. Making full use of these various levels of embedding features can lead to comprehensive utilization of different levels of characteristics, thus achieving drift compensation by the extracted intermediate features between two domains. In the TDACNN, a flexible multibranch backbone with a multiclassifier structure is proposed under the guidance of bionics, which utilizes multiple embedding features comprehensively without involving target domain data during training. A classifier ensemble method based on maximum mean discrepancy (MMD) is proposed to evaluate all the classifiers jointly based on the credibility of the pseudolabel. To optimize network training, an additive angular margin softmax loss with parameter dynamic adjustment is utilized. Experiments on two drift datasets under different settings demonstrate the superiority of TDACNN compared with several state-of-the-art methods.



### Contrastive Proposal Extension with LSTM Network for Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2110.07511v3
- **DOI**: 10.1109/TIP.2022.3216772
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07511v3)
- **Published**: 2021-10-14 16:31:57+00:00
- **Updated**: 2022-10-19 13:18:15+00:00
- **Authors**: Pei Lv, Suqi Hu, Tianran Hao
- **Comment**: 15 pages,12 figures, accepted to IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: Weakly supervised object detection (WSOD) has attracted more and more attention since it only uses image-level labels and can save huge annotation costs. Most of the WSOD methods use Multiple Instance Learning (MIL) as their basic framework, which regard it as an instance classification problem. However, these methods based on MIL tends to converge only on the most discriminate regions of different instances, rather than their corresponding complete regions, that is, insufficient integrity. Inspired by the habit of observing things by the human, we propose a new method by comparing the initial proposals and the extension ones to optimize those initial proposals. Specifically, we propose one new strategy for WSOD by involving contrastive proposal extension (CPE), which consists of multiple directional contrastive proposal extensions (D-CPE), and each D-CPE contains encoders based on LSTM network and corresponding decoders. Firstly, the boundary of initial proposals in MIL is extended to different positions according to well-designed sequential order. Then, CPE compares the extended proposal and the initial proposal by extracting the feature semantics of them using the encoders, and calculates the integrity of the initial proposal to optimize the score of the initial proposal. These contrastive contextual semantics will guide the basic WSOD to suppress bad proposals and improve the scores of good ones. In addition, a simple two-stream network is designed as the decoder to constrain the temporal coding of LSTM and improve the performance of WSOD further. Experiments on PASCAL VOC 2007, VOC 2012 and MS-COCO datasets show that our method has achieved the state-of-the-art results.



### Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset
- **Arxiv ID**: http://arxiv.org/abs/2110.07575v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2110.07575v1)
- **Published**: 2021-10-14 17:38:20+00:00
- **Updated**: 2021-10-14 17:38:20+00:00
- **Authors**: Ian Palmer, Andrew Rouditchenko, Andrei Barbu, Boris Katz, James Glass
- **Comment**: Presented at Interspeech 2021. This version contains additional
  experiments on the Spoken ObjectNet test set
- **Journal**: None
- **Summary**: Visually-grounded spoken language datasets can enable models to learn cross-modal correspondences with very weak supervision. However, modern audio-visual datasets contain biases that undermine the real-world performance of models trained on that data. We introduce Spoken ObjectNet, which is designed to remove some of these biases and provide a way to better evaluate how effectively models will perform in real-world scenarios. This dataset expands upon ObjectNet, which is a bias-controlled image dataset that features similar image classes to those present in ImageNet. We detail our data collection pipeline, which features several methods to improve caption quality, including automated language model checks. Lastly, we show baseline results on image retrieval and audio retrieval tasks. These results show that models trained on other datasets and then evaluated on Spoken ObjectNet tend to perform poorly due to biases in other datasets that the models have learned. We also show evidence that the performance decrease is due to the dataset controls, and not the transfer setting.



### Learning Temporal 3D Human Pose Estimation with Pseudo-Labels
- **Arxiv ID**: http://arxiv.org/abs/2110.07578v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07578v1)
- **Published**: 2021-10-14 17:40:45+00:00
- **Updated**: 2021-10-14 17:40:45+00:00
- **Authors**: Arij Bouazizi, Ulrich Kressel, Vasileios Belagiannis
- **Comment**: Accepted for publication at AVSS 2021. Project
  page:https://github.com/vru2020/TM_HPE/
- **Journal**: None
- **Summary**: We present a simple, yet effective, approach for self-supervised 3D human pose estimation. Unlike the prior work, we explore the temporal information next to the multi-view self-supervision. During training, we rely on triangulating 2D body pose estimates of a multiple-view camera system. A temporal convolutional neural network is trained with the generated 3D ground-truth and the geometric multi-view consistency loss, imposing geometrical constraints on the predicted 3D body skeleton. During inference, our model receives a sequence of 2D body pose estimates from a single-view to predict the 3D body pose for each of them. An extensive evaluation shows that our method achieves state-of-the-art performance in the Human3.6M and MPI-INF-3DHP benchmarks. Our code and models are publicly available at \url{https://github.com/vru2020/TM_HPE/}.



### Playing for 3D Human Recovery
- **Arxiv ID**: http://arxiv.org/abs/2110.07588v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07588v2)
- **Published**: 2021-10-14 17:49:42+00:00
- **Updated**: 2022-08-18 17:58:02+00:00
- **Authors**: Zhongang Cai, Mingyuan Zhang, Jiawei Ren, Chen Wei, Daxuan Ren, Zhengyu Lin, Haiyu Zhao, Lei Yang, Chen Change Loy, Ziwei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Image- and video-based 3D human recovery (i.e., pose and shape estimation) have achieved substantial progress. However, due to the prohibitive cost of motion capture, existing datasets are often limited in scale and diversity. In this work, we obtain massive human sequences by playing the video game with automatically annotated 3D ground truths. Specifically, we contribute GTA-Human, a large-scale 3D human dataset generated with the GTA-V game engine, featuring a highly diverse set of subjects, actions, and scenarios. More importantly, we study the use of game-playing data and obtain five major insights. First, game-playing data is surprisingly effective. A simple frame-based baseline trained on GTA-Human outperforms more sophisticated methods by a large margin. For video-based methods, GTA-Human is even on par with the in-domain training set. Second, we discover that synthetic data provides critical complements to the real data that is typically collected indoor. Our investigation into domain gap provides explanations for our data mixture strategies that are simple yet useful. Third, the scale of the dataset matters. The performance boost is closely related to the additional data available. A systematic study reveals the model sensitivity to data density from multiple key aspects. Fourth, the effectiveness of GTA-Human is also attributed to the rich collection of strong supervision labels (SMPL parameters), which are otherwise expensive to acquire in real datasets. Fifth, the benefits of synthetic data extend to larger models such as deeper convolutional neural networks (CNNs) and Transformers, for which a significant impact is also observed. We hope our work could pave the way for scaling up 3D human recovery to the real world. Homepage: https://caizhongang.github.io/projects/GTA-Human/



### Sub-word Level Lip Reading With Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/2110.07603v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2110.07603v2)
- **Published**: 2021-10-14 17:59:57+00:00
- **Updated**: 2021-12-03 11:35:51+00:00
- **Authors**: K R Prajwal, Triantafyllos Afouras, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this paper is to learn strong lip reading models that can recognise speech in silent videos. Most prior works deal with the open-set visual speech recognition problem by adapting existing automatic speech recognition techniques on top of trivially pooled visual features. Instead, in this paper we focus on the unique challenges encountered in lip reading and propose tailored solutions. To this end, we make the following contributions: (1) we propose an attention-based pooling mechanism to aggregate visual speech representations; (2) we use sub-word units for lip reading for the first time and show that this allows us to better model the ambiguities of the task; (3) we propose a model for Visual Speech Detection (VSD), trained on top of the lip reading network. Following the above, we obtain state-of-the-art results on the challenging LRS2 and LRS3 benchmarks when training on public datasets, and even surpass models trained on large-scale industrial datasets by using an order of magnitude less data. Our best model achieves 22.6% word error rate on the LRS2 dataset, a performance unprecedented for lip reading models, significantly reducing the performance gap between lip reading and automatic speech recognition. Moreover, on the AVA-ActiveSpeaker benchmark, our VSD model surpasses all visual-only baselines and even outperforms several recent audio-visual methods.



### NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2110.07604v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.07604v3)
- **Published**: 2021-10-14 17:59:58+00:00
- **Updated**: 2021-10-18 04:03:39+00:00
- **Authors**: Jason Y. Zhang, Gengshan Yang, Shubham Tulsiani, Deva Ramanan
- **Comment**: In NeurIPS 2021. v2-3: Fixed minor typos
- **Journal**: None
- **Summary**: Recent history has seen a tremendous growth of work exploring implicit representations of geometry and radiance, popularized through Neural Radiance Fields (NeRF). Such works are fundamentally based on a (implicit) volumetric representation of occupancy, allowing them to model diverse scene structure including translucent objects and atmospheric obscurants. But because the vast majority of real-world scenes are composed of well-defined surfaces, we introduce a surface analog of such implicit models called Neural Reflectance Surfaces (NeRS). NeRS learns a neural shape representation of a closed surface that is diffeomorphic to a sphere, guaranteeing water-tight reconstructions. Even more importantly, surface parameterizations allow NeRS to learn (neural) bidirectional surface reflectance functions (BRDFs) that factorize view-dependent appearance into environmental illumination, diffuse color (albedo), and specular "shininess." Finally, rather than illustrating our results on synthetic scenes or controlled in-the-lab capture, we assemble a novel dataset of multi-view images from online marketplaces for selling goods. Such "in-the-wild" multi-view image sets pose a number of challenges, including a small number of views with unknown/rough camera estimates. We demonstrate that surface-based neural reconstructions enable learning from such data, outperforming volumetric neural rendering-based reconstructions. We hope that NeRS serves as a first step toward building scalable, high-quality libraries of real-world shape, materials, and illumination. The project page with code and video visualizations can be found at https://jasonyzhang.com/ners.



### Non-deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.07641v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.07641v1)
- **Published**: 2021-10-14 18:03:56+00:00
- **Updated**: 2021-10-14 18:03:56+00:00
- **Authors**: Ankit Goyal, Alexey Bochkovskiy, Jia Deng, Vladlen Koltun
- **Comment**: None
- **Journal**: None
- **Summary**: Depth is the hallmark of deep neural networks. But more depth means more sequential computation and higher latency. This begs the question -- is it possible to build high-performing "non-deep" neural networks? We show that it is. To do so, we use parallel subnetworks instead of stacking one layer after another. This helps effectively reduce depth while maintaining high performance. By utilizing parallel substructures, we show, for the first time, that a network with a depth of just 12 can achieve top-1 accuracy over 80% on ImageNet, 96% on CIFAR10, and 81% on CIFAR100. We also show that a network with a low-depth (12) backbone can achieve an AP of 48% on MS-COCO. We analyze the scaling rules for our design and show how to increase performance without changing the network's depth. Finally, we provide a proof of concept for how non-deep networks could be used to build low-latency recognition systems. Code is available at https://github.com/imankgoyal/NonDeepNetworks.



### Talking Detection In Collaborative Learning Environments
- **Arxiv ID**: http://arxiv.org/abs/2110.07646v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.07646v1)
- **Published**: 2021-10-14 18:13:28+00:00
- **Updated**: 2021-10-14 18:13:28+00:00
- **Authors**: Wenjing Shi, Marios S. Pattichis, Sylvia Celedón-Pattichis, Carlos LópezLeiva
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of detecting talking activities in collaborative learning videos. Our approach uses head detection and projections of the log-magnitude of optical flow vectors to reduce the problem to a simple classification of small projection images without the need for training complex, 3-D activity classification systems. The small projection images are then easily classified using a simple majority vote of standard classifiers. For talking detection, our proposed approach is shown to significantly outperform single activity systems. We have an overall accuracy of 59% compared to 42% for Temporal Segment Network (TSN) and 45% for Convolutional 3D (C3D). In addition, our method is able to detect multiple talking instances from multiple speakers, while also detecting the speakers themselves.



### Interactive Analysis of CNN Robustness
- **Arxiv ID**: http://arxiv.org/abs/2110.07667v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2110.07667v1)
- **Published**: 2021-10-14 18:52:39+00:00
- **Updated**: 2021-10-14 18:52:39+00:00
- **Authors**: Stefan Sietzen, Mathias Lechner, Judy Borowski, Ramin Hasani, Manuela Waldner
- **Comment**: Accepted at Pacific Graphics 2021
- **Journal**: None
- **Summary**: While convolutional neural networks (CNNs) have found wide adoption as state-of-the-art models for image-related tasks, their predictions are often highly sensitive to small input perturbations, which the human vision is robust against. This paper presents Perturber, a web-based application that allows users to instantaneously explore how CNN activations and predictions evolve when a 3D input scene is interactively perturbed. Perturber offers a large variety of scene modifications, such as camera controls, lighting and shading effects, background modifications, object morphing, as well as adversarial attacks, to facilitate the discovery of potential vulnerabilities. Fine-tuned model versions can be directly compared for qualitative evaluation of their robustness. Case studies with machine learning experts have shown that Perturber helps users to quickly generate hypotheses about model vulnerabilities and to qualitatively compare model behavior. Using quantitative analyses, we could replicate users' insights with other CNN architectures and input images, yielding new insights about the vulnerability of adversarially trained models.



### Augmenting Imitation Experience via Equivariant Representations
- **Arxiv ID**: http://arxiv.org/abs/2110.07668v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.07668v1)
- **Published**: 2021-10-14 18:56:08+00:00
- **Updated**: 2021-10-14 18:56:08+00:00
- **Authors**: Dhruv Sharma, Alihusein Kuwajerwala, Florian Shkurti
- **Comment**: 7 pages (including references), 15 figures
- **Journal**: None
- **Summary**: The robustness of visual navigation policies trained through imitation often hinges on the augmentation of the training image-action pairs. Traditionally, this has been done by collecting data from multiple cameras, by using standard data augmentations from computer vision, such as adding random noise to each image, or by synthesizing training images. In this paper we show that there is another practical alternative for data augmentation for visual navigation based on extrapolating viewpoint embeddings and actions nearby the ones observed in the training data. Our method makes use of the geometry of the visual navigation problem in 2D and 3D and relies on policies that are functions of equivariant embeddings, as opposed to images. Given an image-action pair from a training navigation dataset, our neural network model predicts the latent representations of images at nearby viewpoints, using the equivariance property, and augments the dataset. We then train a policy on the augmented dataset. Our simulation results indicate that policies trained in this way exhibit reduced cross-track error, and require fewer interventions compared to policies trained using standard augmentation methods. We also show similar results in autonomous visual navigation by a real ground robot along a path of over 500m.



### Appearance Editing with Free-viewpoint Neural Rendering
- **Arxiv ID**: http://arxiv.org/abs/2110.07674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07674v1)
- **Published**: 2021-10-14 19:14:05+00:00
- **Updated**: 2021-10-14 19:14:05+00:00
- **Authors**: Pulkit Gera, Aakash KT, Dhawal Sirikonda, Parikshit Sakurikar, P. J. Narayanan
- **Comment**: None
- **Journal**: None
- **Summary**: We present a neural rendering framework for simultaneous view synthesis and appearance editing of a scene from multi-view images captured under known environment illumination. Existing approaches either achieve view synthesis alone or view synthesis along with relighting, without direct control over the scene's appearance. Our approach explicitly disentangles the appearance and learns a lighting representation that is independent of it. Specifically, we independently estimate the BRDF and use it to learn a lighting-only representation of the scene. Such disentanglement allows our approach to generalize to arbitrary changes in appearance while performing view synthesis. We show results of editing the appearance of a real scene, demonstrating that our approach produces plausible appearance editing. The performance of our view synthesis approach is demonstrated to be at par with state-of-the-art approaches on both real and synthetic data.



### Shaping embodied agent behavior with activity-context priors from egocentric video
- **Arxiv ID**: http://arxiv.org/abs/2110.07692v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.07692v1)
- **Published**: 2021-10-14 20:02:59+00:00
- **Updated**: 2021-10-14 20:02:59+00:00
- **Authors**: Tushar Nagarajan, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: Complex physical tasks entail a sequence of object interactions, each with its own preconditions -- which can be difficult for robotic agents to learn efficiently solely through their own experience. We introduce an approach to discover activity-context priors from in-the-wild egocentric video captured with human worn cameras. For a given object, an activity-context prior represents the set of other compatible objects that are required for activities to succeed (e.g., a knife and cutting board brought together with a tomato are conducive to cutting). We encode our video-based prior as an auxiliary reward function that encourages an agent to bring compatible objects together before attempting an interaction. In this way, our model translates everyday human experience into embodied agent skills. We demonstrate our idea using egocentric EPIC-Kitchens video of people performing unscripted kitchen activities to benefit virtual household robotic agents performing various complex tasks in AI2-iTHOR, significantly accelerating agent learning. Project page: http://vision.cs.utexas.edu/projects/ego-rewards/



### ASK: Adaptively Selecting Key Local Features for RGB-D Scene Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.07703v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07703v1)
- **Published**: 2021-10-14 20:26:58+00:00
- **Updated**: 2021-10-14 20:26:58+00:00
- **Authors**: Zhitong Xiong, Yuan Yuan, Qi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Indoor scene images usually contain scattered objects and various scene layouts, which make RGB-D scene classification a challenging task. Existing methods still have limitations for classifying scene images with great spatial variability. Thus, how to extract local patch-level features effectively using only image labels is still an open problem for RGB-D scene recognition. In this paper, we propose an efficient framework for RGB-D scene recognition, which adaptively selects important local features to capture the great spatial variability of scene images. Specifically, we design a differentiable local feature selection (DLFS) module, which can extract the appropriate number of key local scenerelated features. Discriminative local theme-level and object-level representations can be selected with the DLFS module from the spatially-correlated multi-modal RGB-D features. We take advantage of the correlation between RGB and depth modalities to provide more cues for selecting local features. To ensure that discriminative local features are selected, the variational mutual information maximization loss is proposed. Additionally, the DLFS module can be easily extended to select local features of different scales. By concatenating the local-orderless and global structured multi-modal features, the proposed framework can achieve state-of-the-art performance on public RGB-D scene recognition datasets.



### Gray Matter Segmentation in Ultra High Resolution 7 Tesla ex vivo T2w MRI of Human Brain Hemispheres
- **Arxiv ID**: http://arxiv.org/abs/2110.07711v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.07711v3)
- **Published**: 2021-10-14 21:01:18+00:00
- **Updated**: 2022-03-04 03:48:03+00:00
- **Authors**: Pulkit Khandelwal, Shokufeh Sadaghiani, Michael Tran Duong, Sadhana Ravikumar, Sydney Lim, Sanaz Arezoumandan, Claire Peterson, Eunice Chung, Madigan Bedard, Noah Capp, Ranjit Ittyerah, Elyse Migdal, Grace Choi, Emily Kopp, Bridget Loja, Eusha Hasan, Jiacheng Li, Karthik Prabhakaran, Gabor Mizsei, Marianna Gabrielyan, Theresa Schuck, John Robinson, Daniel Ohm, Edward Lee, John Q. Trojanowski, Corey McMillan, Murray Grossman, David Irwin, M. Dylan Tisdall, Sandhitsu R. Das, Laura E. M. Wisse, David A. Wolk, Paul A. Yushkevich
- **Comment**: Ex vivo analysis framework (work in progress 2022 at the University
  of Pennsylvania)
- **Journal**: None
- **Summary**: Ex vivo MRI of the brain provides remarkable advantages over in vivo MRI for visualizing and characterizing detailed neuroanatomy. However, automated cortical segmentation methods in ex vivo MRI are not well developed, primarily due to limited availability of labeled datasets, and heterogeneity in scanner hardware and acquisition protocols. In this work, we present a high resolution 7 Tesla dataset of 32 ex vivo human brain specimens. We benchmark the cortical mantle segmentation performance of nine neural network architectures, trained and evaluated using manually-segmented 3D patches sampled from specific cortical regions, and show excellent generalizing capabilities across whole brain hemispheres in different specimens, and also on unseen images acquired at different magnetic field strength and imaging sequences. Finally, we provide cortical thickness measurements across key regions in 3D ex vivo human brain images. Our code and processed datasets are publicly available at https://github.com/Pulkit-Khandelwal/picsl-ex-vivo-segmentation.



### Beyond Classification: Directly Training Spiking Neural Networks for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.07742v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07742v1)
- **Published**: 2021-10-14 21:53:03+00:00
- **Updated**: 2021-10-14 21:53:03+00:00
- **Authors**: Youngeun Kim, Joshua Chough, Priyadarshini Panda
- **Comment**: None
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) have recently emerged as the low-power alternative to Artificial Neural Networks (ANNs) because of their sparse, asynchronous, and binary event-driven processing. Due to their energy efficiency, SNNs have a high possibility of being deployed for real-world, resource-constrained systems such as autonomous vehicles and drones. However, owing to their non-differentiable and complex neuronal dynamics, most previous SNN optimization methods have been limited to image recognition. In this paper, we explore the SNN applications beyond classification and present semantic segmentation networks configured with spiking neurons. Specifically, we first investigate two representative SNN optimization techniques for recognition tasks (i.e., ANN-SNN conversion and surrogate gradient learning) on semantic segmentation datasets. We observe that, when converted from ANNs, SNNs suffer from high latency and low performance due to the spatial variance of features. Therefore, we directly train networks with surrogate gradient learning, resulting in lower latency and higher performance than ANN-SNN conversion. Moreover, we redesign two fundamental ANN segmentation architectures (i.e., Fully Convolutional Networks and DeepLab) for the SNN domain. We conduct experiments on two public semantic segmentation benchmarks including the PASCAL VOC2012 dataset and the DDD17 event-based dataset. In addition to showing the feasibility of SNNs for semantic segmentation, we show that SNNs can be more robust and energy-efficient compared to their ANN counterparts in this domain.



### A deep learning model for classification of diabetic retinopathy in eye fundus images based on retinal lesion detection
- **Arxiv ID**: http://arxiv.org/abs/2110.07745v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.07745v1)
- **Published**: 2021-10-14 22:04:59+00:00
- **Updated**: 2021-10-14 22:04:59+00:00
- **Authors**: Melissa delaPava, Hernán Ríos, Francisco J. Rodríguez, Oscar J. Perdomo, Fabio A. González
- **Comment**: 7 pages and 1 figure
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) is the result of a complication of diabetes affecting the retina. It can cause blindness, if left undiagnosed and untreated. An ophthalmologist performs the diagnosis by screening each patient and analyzing the retinal lesions via ocular imaging. In practice, such analysis is time-consuming and cumbersome to perform. This paper presents a model for automatic DR classification on eye fundus images. The approach identifies the main ocular lesions related to DR and subsequently diagnoses the illness. The proposed method follows the same workflow as the clinicians, providing information that can be interpreted clinically to support the prediction. A subset of the kaggle EyePACS and the Messidor-2 datasets, labeled with ocular lesions, is made publicly available. The kaggle EyePACS subset is used as a training set and the Messidor-2 as a test set for lesions and DR classification models. For DR diagnosis, our model has an area-under-the-curve, sensitivity, and specificity of 0.948, 0.886, and 0.875, respectively, which competes with state-of-the-art approaches.



### CT-SGAN: Computed Tomography Synthesis GAN
- **Arxiv ID**: http://arxiv.org/abs/2110.09288v2
- **DOI**: 10.1007/978-3-030-88210-5_6
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.09288v2)
- **Published**: 2021-10-14 22:20:40+00:00
- **Updated**: 2021-11-04 20:35:45+00:00
- **Authors**: Ahmad Pesaranghader, Yiping Wang, Mohammad Havaei
- **Comment**: In Proceedings of MICCAI Deep Generative Models workshop, October
  2021
- **Journal**: None
- **Summary**: Diversity in data is critical for the successful training of deep learning models. Leveraged by a recurrent generative adversarial network, we propose the CT-SGAN model that generates large-scale 3D synthetic CT-scan volumes ($\geq 224\times224\times224$) when trained on a small dataset of chest CT-scans. CT-SGAN offers an attractive solution to two major challenges facing machine learning in medical imaging: a small number of given i.i.d. training data, and the restrictions around the sharing of patient data preventing to rapidly obtain larger and more diverse datasets. We evaluate the fidelity of the generated images qualitatively and quantitatively using various metrics including Fr\'echet Inception Distance and Inception Score. We further show that CT-SGAN can significantly improve lung nodule detection accuracy by pre-training a classifier on a vast amount of synthetic data.



### "Knights": First Place Submission for VIPriors21 Action Recognition Challenge at ICCV 2021
- **Arxiv ID**: http://arxiv.org/abs/2110.07758v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07758v1)
- **Published**: 2021-10-14 22:47:31+00:00
- **Updated**: 2021-10-14 22:47:31+00:00
- **Authors**: Ishan Dave, Naman Biyani, Brandon Clark, Rohit Gupta, Yogesh Rawat, Mubarak Shah
- **Comment**: Challenge results are available at
  https://vipriors.github.io/challenges/#action-recognition
- **Journal**: None
- **Summary**: This technical report presents our approach "Knights" to solve the action recognition task on a small subset of Kinetics-400 i.e. Kinetics400ViPriors without using any extra-data. Our approach has 3 main components: state-of-the-art Temporal Contrastive self-supervised pretraining, video transformer models, and optical flow modality. Along with the use of standard test-time augmentation, our proposed solution achieves 73% on Kinetics400ViPriors test set, which is the best among all of the other entries Visual Inductive Priors for Data-Efficient Computer Vision's Action Recognition Challenge, ICCV 2021.



### 3D Reconstruction of Curvilinear Structures with Stereo Matching DeepConvolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.07766v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.07766v1)
- **Published**: 2021-10-14 23:05:47+00:00
- **Updated**: 2021-10-14 23:05:47+00:00
- **Authors**: Okan Altingövde, Anastasiia Mishchuk, Gulnaz Ganeeva, Emad Oveisi, Cecile Hebert, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Curvilinear structures frequently appear in microscopy imaging as the object of interest. Crystallographic defects, i.e., dislocations, are one of the curvilinear structures that have been repeatedly investigated under transmission electron microscopy (TEM) and their 3D structural information is of great importance for understanding the properties of materials. 3D information of dislocations is often obtained by tomography which is a cumbersome process since it is required to acquire many images with different tilt angles and similar imaging conditions. Although, alternative stereoscopy methods lower the number of required images to two, they still require human intervention and shape priors for accurate 3D estimation. We propose a fully automated pipeline for both detection and matching of curvilinear structures in stereo pairs by utilizing deep convolutional neural networks (CNNs) without making any prior assumption on 3D shapes. In this work, we mainly focus on 3D reconstruction of dislocations from stereo pairs of TEM images.



### 4D flight trajectory prediction using a hybrid Deep Learning prediction method based on ADS-B technology: a case study of Hartsfield-Jackson Atlanta International Airport(ATL)
- **Arxiv ID**: http://arxiv.org/abs/2110.07774v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.07774v1)
- **Published**: 2021-10-14 23:48:44+00:00
- **Updated**: 2021-10-14 23:48:44+00:00
- **Authors**: Hesam Sahfienya, Amelia C. Regan
- **Comment**: 17 pages, 10 figures
- **Journal**: None
- **Summary**: The core of any flight schedule is the trajectories. In particular, 4D trajectories are the most crucial component for flight attribute prediction. In particular, 4D trajectories are the most crucial component for flight attribute prediction. Each trajectory contains spatial and temporal features that are associated with uncertainties that make the prediction process complex. Today because of the increasing demand for air transportation, it is compulsory for airports and airlines to have an optimized schedule to use all of the airport's infrastructure potential. This is possible using advanced trajectory prediction methods. This paper proposes a novel hybrid deep learning model to extract the spatial and temporal features considering the uncertainty of the prediction model for Hartsfield-Jackson Atlanta International Airport(ATL). Automatic Dependent Surveillance-Broadcast (ADS-B) data are used as input to the models. This research is conducted in three steps: (a) data preprocessing; (b) prediction by a hybrid Convolutional Neural Network and Gated Recurrent Unit (CNN-GRU) along with a 3D-CNN model; (c) The third and last step is the comparison of the model's performance with the proposed model by comparing the experimental results. The deep model uncertainty is considered using the Mont-Carlo dropout (MC-Dropout). Mont-Carlo dropouts are added to the network layers to enhance the model's prediction performance by a robust approach of switching off between different neurons. The results show that the proposed model has low error measurements compared to the other models (i.e., 3D CNN, CNN-GRU). The model with MC-dropout reduces the error further by an average of 21 %.



