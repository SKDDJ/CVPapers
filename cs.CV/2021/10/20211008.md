# Arxiv Papers in cs.CV on 2021-10-08
### Automatic annotation of visual deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/2110.03851v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03851v1)
- **Published**: 2021-10-08 01:39:03+00:00
- **Updated**: 2021-10-08 01:39:03+00:00
- **Authors**: Ming Li, ChenHao Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision is widely used in the fields of driverless, face recognition and 3D reconstruction as a technology to help or replace human eye perception images or multidimensional data through computers. Nowadays, with the development and application of deep neural networks, the models of deep neural networks proposed for computer vision are becoming more and more abundant, and developers will use the already trained models on the way to solve problems, and need to consult the relevant documents to understand the use of the model. The class model, which creates the need to quickly and accurately find the relevant models that you need. The automatic annotation method of visual depth neural network proposed in this paper is based on natural language processing technology such as semantic analysis, which realizes automatic labeling of model application fields. In the three top international conferences on computer vision: ICCV, CVPR and ECCV, the average correct rate of application of the papers of 72 papers reached 90%, indicating the effectiveness of the automatic labeling system.



### 3D Meta-Segmentation Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2110.04297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.04297v1)
- **Published**: 2021-10-08 01:47:54+00:00
- **Updated**: 2021-10-08 01:47:54+00:00
- **Authors**: Yu Hao, Yi Fang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2110.03854
- **Journal**: None
- **Summary**: Though deep learning methods have shown great success in 3D point cloud part segmentation, they generally rely on a large volume of labeled training data, which makes the model suffer from unsatisfied generalization abilities to unseen classes with limited data. To address this problem, we present a novel meta-learning strategy that regards the 3D shape segmentation function as a task. By training over a number of 3D part segmentation tasks, our method is capable to learn the prior over the respective 3D segmentation function space which leads to an optimal model that is rapidly adapting to new part segmentation tasks. To implement our meta-learning strategy, we propose two novel modules: meta part segmentation learner and part segmentation learner. During the training process, the part segmentation learner is trained to complete a specific part segmentation task in the few-shot scenario. In the meantime, the meta part segmentation learner is trained to capture the prior from multiple similar part segmentation tasks. Based on the learned information of task distribution, our meta part segmentation learner is able to dynamically update the part segmentation learner with optimal parameters which enable our part segmentation learner to rapidly adapt and have great generalization ability on new part segmentation tasks. We demonstrate that our model achieves superior part segmentation performance with the few-shot setting on the widely used dataset: ShapeNet.



### Meta-Learning 3D Shape Segmentation Functions
- **Arxiv ID**: http://arxiv.org/abs/2110.03854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03854v1)
- **Published**: 2021-10-08 01:50:54+00:00
- **Updated**: 2021-10-08 01:50:54+00:00
- **Authors**: Yu Hao, Yi Fang
- **Comment**: None
- **Journal**: None
- **Summary**: Learning robust 3D shape segmentation functions with deep neural networks has emerged as a powerful paradigm, offering promising performance in producing a consistent part segmentation of each 3D shape. Generalizing across 3D shape segmentation functions requires robust learning of priors over the respective function space and enables consistent part segmentation of shapes in presence of significant 3D structure variations. Existing generalization methods rely on extensive training of 3D shape segmentation functions on large-scale labeled datasets. In this paper, we proposed to formalize the learning of a 3D shape segmentation function space as a meta-learning problem, aiming to predict a 3D segmentation model that can be quickly adapted to new shapes with no or limited training data. More specifically, we define each task as unsupervised learning of shape-conditioned 3D segmentation function which takes as input points in 3D space and predicts the part-segment labels. The 3D segmentation function is trained by a self-supervised 3D shape reconstruction loss without the need for part labels. Also, we introduce an auxiliary deep neural network as a meta-learner which takes as input a 3D shape and predicts the prior over the respective 3D segmentation function space. We show in experiments that our meta-learning approach, denoted as Meta-3DSeg, leads to improvements on unsupervised 3D shape segmentation over the conventional designs of deep neural networks for 3D shape segmentation functions.



### ABCP: Automatic Block-wise and Channel-wise Network Pruning via Joint Search
- **Arxiv ID**: http://arxiv.org/abs/2110.03858v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.8; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2110.03858v1)
- **Published**: 2021-10-08 02:15:49+00:00
- **Updated**: 2021-10-08 02:15:49+00:00
- **Authors**: Jiaqi Li, Haoran Li, Yaran Chen, Zixiang Ding, Nannan Li, Mingjun Ma, Zicheng Duan, Dongbing Zhao
- **Comment**: 12 pages, 9 figures, submitted to Journal of IEEE Transactions on
  Cybernetics
- **Journal**: None
- **Summary**: Currently, an increasing number of model pruning methods are proposed to resolve the contradictions between the computer powers required by the deep learning models and the resource-constrained devices. However, most of the traditional rule-based network pruning methods can not reach a sufficient compression ratio with low accuracy loss and are time-consuming as well as laborious. In this paper, we propose Automatic Block-wise and Channel-wise Network Pruning (ABCP) to jointly search the block-wise and channel-wise pruning action with deep reinforcement learning. A joint sample algorithm is proposed to simultaneously generate the pruning choice of each residual block and the channel pruning ratio of each convolutional layer from the discrete and continuous search space respectively. The best pruning action taking both the accuracy and the complexity of the model into account is obtained finally. Compared with the traditional rule-based pruning method, this pipeline saves human labor and achieves a higher compression ratio with lower accuracy loss. Tested on the mobile robot detection dataset, the pruned YOLOv3 model saves 99.5% FLOPs, reduces 99.5% parameters, and achieves 37.3 times speed up with only 2.8% mAP loss. The results of the transfer task on the sim2real detection dataset also show that our pruned model has much better robustness performance.



### Token Pooling in Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2110.03860v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03860v2)
- **Published**: 2021-10-08 02:22:50+00:00
- **Updated**: 2021-10-11 15:17:21+00:00
- **Authors**: Dmitrii Marin, Jen-Hao Rick Chang, Anurag Ranjan, Anish Prabhu, Mohammad Rastegari, Oncel Tuzel
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision. 2023
- **Summary**: Despite the recent success in many applications, the high computational requirements of vision transformers limit their use in resource-constrained settings. While many existing methods improve the quadratic complexity of attention, in most vision transformers, self-attention is not the major computation bottleneck, e.g., more than 80% of the computation is spent on fully-connected layers. To improve the computational complexity of all layers, we propose a novel token downsampling method, called Token Pooling, efficiently exploiting redundancies in the images and intermediate token representations. We show that, under mild assumptions, softmax-attention acts as a high-dimensional low-pass (smoothing) filter. Thus, its output contains redundancy that can be pruned to achieve a better trade-off between the computational cost and accuracy. Our new technique accurately approximates a set of tokens by minimizing the reconstruction error caused by downsampling. We solve this optimization problem via cost-efficient clustering. We rigorously analyze and compare to prior downsampling methods. Our experiments show that Token Pooling significantly improves the cost-accuracy trade-off over the state-of-the-art downsampling. Token Pooling is a simple and effective operator that can benefit many architectures. Applied to DeiT, it achieves the same ImageNet top-1 accuracy using 42% fewer computations.



### Boundary-aware Transformers for Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.03864v1
- **DOI**: 10.1007/978-3-030-87193-2_20
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03864v1)
- **Published**: 2021-10-08 02:43:34+00:00
- **Updated**: 2021-10-08 02:43:34+00:00
- **Authors**: Jiacheng Wang, Lan Wei, Liansheng Wang, Qichao Zhou, Lei Zhu, Jing Qin
- **Comment**: None
- **Journal**: Medical Image Computing and Computer Assisted Intervention 2021
- **Summary**: Skin lesion segmentation from dermoscopy images is of great importance for improving the quantitative analysis of skin cancer. However, the automatic segmentation of melanoma is a very challenging task owing to the large variation of melanoma and ambiguous boundaries of lesion areas. While convolutional neutral networks (CNNs) have achieved remarkable progress in this task, most of existing solutions are still incapable of effectively capturing global dependencies to counteract the inductive bias caused by limited receptive fields. Recently, transformers have been proposed as a promising tool for global context modeling by employing a powerful global attention mechanism, but one of their main shortcomings when applied to segmentation tasks is that they cannot effectively extract sufficient local details to tackle ambiguous boundaries. We propose a novel boundary-aware transformer (BAT) to comprehensively address the challenges of automatic skin lesion segmentation. Specifically, we integrate a new boundary-wise attention gate (BAG) into transformers to enable the whole network to not only effectively model global long-range dependencies via transformers but also, simultaneously, capture more local details by making full use of boundary-wise prior knowledge. Particularly, the auxiliary supervision of BAG is capable of assisting transformers to learn position embedding as it provides much spatial information. We conducted extensive experiments to evaluate the proposed BAT and experiments corroborate its effectiveness, consistently outperforming state-of-the-art methods in two famous datasets.



### Designing the Architecture of a Convolutional Neural Network Automatically for Diabetic Retinopathy Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2110.03877v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03877v2)
- **Published**: 2021-10-08 03:30:45+00:00
- **Updated**: 2022-11-07 22:00:32+00:00
- **Authors**: Fahman Saeed, Muhammad Hussain, Hatim A Aboalsamh, Fadwa Al Adel, Adi Mohammed Al Owaifeer
- **Comment**: 20 pages, 6 figures
- **Journal**: None
- **Summary**: The prevalence of diabetic retinopathy (DR) has reached 34.6% worldwide and is a major cause of blindness among middle-aged diabetic patients. Regular DR screening using fundus photography helps detect its complications and prevent its progression to advanced levels. As manual screening is time-consuming and subjective, machine learning (ML) and deep learning (DL) have been employed to aid graders. However, the existing CNN-based methods use either pre-trained CNN models or a brute force approach to design new CNN models, which are not customized to the complexity of fundus images. To overcome this issue, we introduce an approach for custom-design of CNN models, whose architectures are adapted to the structural patterns of fundus images and better represent the DR-relevant features. It takes the leverage of k-medoid clustering, principal component analysis (PCA), and inter-class and intra-class variations to automatically determine the depth and width of a CNN model. The designed models are lightweight, adapted to the internal structures of fundus images, and encode the discriminative patterns of DR lesions. The technique is validated on a local dataset from King Saud University Medical City, Saudi Arabia, and two challenging benchmark datasets from Kaggle: EyePACS and APTOS2019. The custom-designed models outperform the famous pre-trained CNN models like ResNet152, Densnet121, and ResNeSt50 with a significant decrease in the number of parameters and compete well with the state-of-the-art CNN-based DR screening methods. The proposed approach is helpful for DR screening under diverse clinical settings and referring the patients who may need further assessment and treatment to expert ophthalmologists.



### Active Learning of Neural Collision Handler for Complex 3D Mesh Deformations
- **Arxiv ID**: http://arxiv.org/abs/2110.07727v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.07727v1)
- **Published**: 2021-10-08 04:08:31+00:00
- **Updated**: 2021-10-08 04:08:31+00:00
- **Authors**: Qingyang Tan, Zherong Pan, Breannan Smith, Takaaki Shiratori, Dinesh Manocha
- **Comment**: None
- **Journal**: None
- **Summary**: We present a robust learning algorithm to detect and handle collisions in 3D deforming meshes. Our collision detector is represented as a bilevel deep autoencoder with an attention mechanism that identifies colliding mesh sub-parts. We use a numerical optimization algorithm to resolve penetrations guided by the network. Our learned collision handler can resolve collisions for unseen, high-dimensional meshes with thousands of vertices. To obtain stable network performance in such large and unseen spaces, we progressively insert new collision data based on the errors in network inferences. We automatically label these data using an analytical collision detector and progressively fine-tune our detection networks. We evaluate our method for collision handling of complex, 3D meshes coming from several datasets with different shapes and topologies, including datasets corresponding to dressed and undressed human poses, cloth simulations, and human hand poses acquired using multiview capture systems. Our approach outperforms supervised learning methods and achieves $93.8-98.1\%$ accuracy compared to the groundtruth by analytic methods. Compared to prior learning methods, our approach results in a $5.16\%-25.50\%$ lower false negative rate in terms of collision checking and a $9.65\%-58.91\%$ higher success rate in collision handling.



### Bounding-box deep calibration for high performance face detection
- **Arxiv ID**: http://arxiv.org/abs/2110.03892v2
- **DOI**: 10.1049/cvi2.12122
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03892v2)
- **Published**: 2021-10-08 04:41:41+00:00
- **Updated**: 2022-07-22 08:00:24+00:00
- **Authors**: Shi Luo, Xiongfei Li, Xiaoli Zhang
- **Comment**: 12 pages, 7 figures, 5 tables, 1 algorithm, 9 equation, 2 definition
- **Journal**: IET Computer Vision 2022
- **Summary**: Modern convolutional neural networks (CNNs)-based face detectors have achieved tremendous strides due to large annotated datasets. However, misaligned results with high detection confidence but low localization accuracy restrict the further improvement of detection performance. In this paper, the authors first predict high confidence detection results on the training set itself. Surprisingly, a considerable part of them exist in the same misalignment problem. Then, the authors carefully examine these cases and point out that annotation misalignment is the main reason. Later, a comprehensive discussion is given for the replacement rationality between predicted and annotated bounding-boxes. Finally, the authors propose a novel Bounding-Box Deep Calibration (BDC) method to reasonably replace misaligned annotations with model predicted bounding-boxes and offer calibrated annotations for the training set. Extensive experiments on multiple detectors and two popular benchmark datasets show the effectiveness of BDC on improving models' precision and recall rate, without adding extra inference time and memory consumption. Our simple and effective method provides a general strategy for improving face detection, especially for light-weight detectors in real-time situations.



### Neural Strokes: Stylized Line Drawing of 3D Shapes
- **Arxiv ID**: http://arxiv.org/abs/2110.03900v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.03900v1)
- **Published**: 2021-10-08 05:40:57+00:00
- **Updated**: 2021-10-08 05:40:57+00:00
- **Authors**: Difan Liu, Matthew Fisher, Aaron Hertzmann, Evangelos Kalogerakis
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: This paper introduces a model for producing stylized line drawings from 3D shapes. The model takes a 3D shape and a viewpoint as input, and outputs a drawing with textured strokes, with variations in stroke thickness, deformation, and color learned from an artist's style. The model is fully differentiable. We train its parameters from a single training drawing of another 3D shape. We show that, in contrast to previous image-based methods, the use of a geometric representation of 3D shape and 2D strokes allows the model to transfer important aspects of shape and texture style while preserving contours. Our method outputs the resulting drawing in a vector representation, enabling richer downstream analysis or editing in interactive applications.



### COVID-19 Monitoring System using Social Distancing and Face Mask Detection on Surveillance video datasets
- **Arxiv ID**: http://arxiv.org/abs/2110.03905v3
- **DOI**: 10.1109/ESCI50559.2021.9396783
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03905v3)
- **Published**: 2021-10-08 05:57:30+00:00
- **Updated**: 2022-12-16 15:45:18+00:00
- **Authors**: Sahana Srinivasan, Rujula Singh R, Ruchita R Biradar, Revathi SA
- **Comment**: I, Rujula Singh R, would like to apologize to the research community
  for the confusion caused by the inconsistency in author lists between
  multiple versions of this paper. I take full responsibility for this error
  and will be more diligent in the future to ensure the accuracy and
  consistency of our research publications
- **Journal**: 2021 International Conference on Emerging Smart Computing and
  Informatics (ESCI), 2021, pp. 449-455
- **Summary**: In the current times, the fear and danger of COVID-19 virus still stands large. Manual monitoring of social distancing norms is impractical with a large population moving about and with insufficient task force and resources to administer them. There is a need for a lightweight, robust and 24X7 video-monitoring system that automates this process. This paper proposes a comprehensive and effective solution to perform person detection, social distancing violation detection, face detection and face mask classification using object detection, clustering and Convolution Neural Network (CNN) based binary classifier. For this, YOLOv3, Density-based spatial clustering of applications with noise (DBSCAN), Dual Shot Face Detector (DSFD) and MobileNetV2 based binary classifier have been employed on surveillance video datasets. This paper also provides a comparative study of different face detection and face mask classification models. Finally, a video dataset labelling method is proposed along with the labelled video dataset to compensate for the lack of dataset in the community and is used for evaluation of the system. The system performance is evaluated in terms of accuracy, F1 score as well as the prediction time, which has to be low for practical applicability. The system performs with an accuracy of 91.2% and F1 score of 90.79% on the labelled video dataset and has an average prediction time of 7.12 seconds for 78 frames of a video.



### Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.03909v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03909v2)
- **Published**: 2021-10-08 06:07:21+00:00
- **Updated**: 2021-10-17 14:05:09+00:00
- **Authors**: Sungyong Baik, Janghoon Choi, Heewon Kim, Dohee Cho, Jaesik Min, Kyoung Mu Lee
- **Comment**: ICCV 2021 (Oral). Code at https://github.com/baiksung/MeTAL
- **Journal**: None
- **Summary**: In few-shot learning scenarios, the challenge is to generalize and perform well on new unseen examples when only very few labeled examples are available for each task. Model-agnostic meta-learning (MAML) has gained the popularity as one of the representative few-shot learning methods for its flexibility and applicability to diverse problems. However, MAML and its variants often resort to a simple loss function without any auxiliary loss function or regularization terms that can help achieve better generalization. The problem lies in that each application and task may require different auxiliary loss function, especially when tasks are diverse and distinct. Instead of attempting to hand-design an auxiliary loss function for each application and task, we introduce a new meta-learning framework with a loss function that adapts to each task. Our proposed framework, named Meta-Learning with Task-Adaptive Loss Function (MeTAL), demonstrates the effectiveness and the flexibility across various domains, such as few-shot classification and few-shot regression.



### Stereo Dense Scene Reconstruction and Accurate Localization for Learning-Based Navigation of Laparoscope in Minimally Invasive Surgery
- **Arxiv ID**: http://arxiv.org/abs/2110.03912v2
- **DOI**: 10.1109/TBME.2022.3195027
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03912v2)
- **Published**: 2021-10-08 06:12:18+00:00
- **Updated**: 2022-11-27 06:24:02+00:00
- **Authors**: Ruofeng Wei, Bin Li, Hangjie Mo, Bo Lu, Yonghao Long, Bohan Yang, Qi Dou, Yunhui Liu, Dong Sun
- **Comment**: None
- **Journal**: IEEE Transactions on Biomedical Engineering 2022
- **Summary**: Objective: The computation of anatomical information and laparoscope position is a fundamental block of surgical navigation in Minimally Invasive Surgery (MIS). Recovering a dense 3D structure of surgical scene using visual cues remains a challenge, and the online laparoscopic tracking primarily relies on external sensors, which increases system complexity. Methods: Here, we propose a learning-driven framework, in which an image-guided laparoscopic localization with 3D reconstructions of complex anatomical structures is obtained. To reconstruct the 3D structure of the whole surgical environment, we first fine-tune a learning-based stereoscopic depth perception method, which is robust to the texture-less and variant soft tissues, for depth estimation. Then, we develop a dense visual reconstruction algorithm to represent the scene by surfels, estimate the laparoscope poses and fuse the depth maps into a unified reference coordinate for tissue reconstruction. To estimate poses of new laparoscope views, we achieve a coarse-to-fine localization method, which incorporates our reconstructed 3D model. Results: We evaluate the reconstruction method and the localization module on three datasets, namely, the stereo correspondence and reconstruction of endoscopic data (SCARED), the ex-vivo phantom and tissue data collected with Universal Robot (UR) and Karl Storz Laparoscope, and the in-vivo DaVinci robotic surgery dataset, where the reconstructed 3D structures have rich details of surface texture with an accuracy error under 1.71 mm and the localization module can accurately track the laparoscope with only images as input. Conclusions: Experimental results demonstrate the superior performance of the proposed method in 3D anatomy reconstruction and laparoscopic localization. Significance: The proposed framework can be potentially extended to the current surgical navigation system.



### Multifocal Stereoscopic Projection Mapping
- **Arxiv ID**: http://arxiv.org/abs/2110.07726v1
- **DOI**: 10.1109/TVCG.2021.3106486
- **Categories**: **cs.CV**, cs.GR, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2110.07726v1)
- **Published**: 2021-10-08 06:13:10+00:00
- **Updated**: 2021-10-08 06:13:10+00:00
- **Authors**: Sorashi Kimura, Daisuke Iwai, Parinya Punpongsanon, Kosuke Sato
- **Comment**: None
- **Journal**: None
- **Summary**: Stereoscopic projection mapping (PM) allows a user to see a three-dimensional (3D) computer-generated (CG) object floating over physical surfaces of arbitrary shapes around us using projected imagery. However, the current stereoscopic PM technology only satisfies binocular cues and is not capable of providing correct focus cues, which causes a vergence--accommodation conflict (VAC). Therefore, we propose a multifocal approach to mitigate VAC in stereoscopic PM. Our primary technical contribution is to attach electrically focus-tunable lenses (ETLs) to active shutter glasses to control both vergence and accommodation. Specifically, we apply fast and periodical focal sweeps to the ETLs, which causes the "virtual image'" (as an optical term) of a scene observed through the ETLs to move back and forth during each sweep period. A 3D CG object is projected from a synchronized high-speed projector only when the virtual image of the projected imagery is located at a desired distance. This provides an observer with the correct focus cues required. In this study, we solve three technical issues that are unique to stereoscopic PM: (1) The 3D CG object is displayed on non-planar and even moving surfaces; (2) the physical surfaces need to be shown without the focus modulation; (3) the shutter glasses additionally need to be synchronized with the ETLs and the projector. We also develop a novel compensation technique to deal with the "lens breathing" artifact that varies the retinal size of the virtual image through focal length modulation. Further, using a proof-of-concept prototype, we demonstrate that our technique can present the virtual image of a target 3D CG object at the correct depth. Finally, we validate the advantage provided by our technique by comparing it with conventional stereoscopic PM using a user study on a depth-matching task.



### Optical Flow Estimation for Spiking Camera
- **Arxiv ID**: http://arxiv.org/abs/2110.03916v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03916v3)
- **Published**: 2021-10-08 06:16:45+00:00
- **Updated**: 2022-04-03 09:07:57+00:00
- **Authors**: Liwen Hu, Rui Zhao, Ziluo Ding, Lei Ma, Boxin Shi, Ruiqin Xiong, Tiejun Huang
- **Comment**: The first two authors contributed equally. Accepted to CVPR 2022
- **Journal**: None
- **Summary**: As a bio-inspired sensor with high temporal resolution, the spiking camera has an enormous potential in real applications, especially for motion estimation in high-speed scenes. However, frame-based and event-based methods are not well suited to spike streams from the spiking camera due to the different data modalities. To this end, we present, SCFlow, a tailored deep learning pipeline to estimate optical flow in high-speed scenes from spike streams. Importantly, a novel input representation is introduced which can adaptively remove the motion blur in spike streams according to the prior motion. Further, for training SCFlow, we synthesize two sets of optical flow data for the spiking camera, SPIkingly Flying Things and Photo-realistic High-speed Motion, denoted as SPIFT and PHM respectively, corresponding to random high-speed and well-designed scenes. Experimental results show that the SCFlow can predict optical flow from spike streams in different high-speed scenes. Moreover, SCFlow shows promising generalization on \textbf{real spike streams}. Codes and datasets refer to https://github.com/Acnext/Optical-Flow-For-Spiking-Camera.



### ViDT: An Efficient and Effective Fully Transformer-based Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2110.03921v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.03921v2)
- **Published**: 2021-10-08 06:32:05+00:00
- **Updated**: 2021-11-29 11:07:13+00:00
- **Authors**: Hwanjun Song, Deqing Sun, Sanghyuk Chun, Varun Jampani, Dongyoon Han, Byeongho Heo, Wonjae Kim, Ming-Hsuan Yang
- **Comment**: This is a revised version on Nov. 29
- **Journal**: None
- **Summary**: Transformers are transforming the landscape of computer vision, especially for recognition tasks. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to build an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and achieves 49.2AP owing to its high scalability for large models. We will release the code and trained models at https://github.com/naver-ai/vidt



### Accelerating Multi-Objective Neural Architecture Search by Random-Weight Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2110.05242v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2110.05242v1)
- **Published**: 2021-10-08 06:35:20+00:00
- **Updated**: 2021-10-08 06:35:20+00:00
- **Authors**: Shengran Hu, Ran Cheng, Cheng He, Zhichao Lu, Jing Wang, Miao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: For the goal of automated design of high-performance deep convolutional neural networks (CNNs), Neural Architecture Search (NAS) methodology is becoming increasingly important for both academia and industries.Due to the costly stochastic gradient descent (SGD) training of CNNs for performance evaluation, most existing NAS methods are computationally expensive for real-world deployments. To address this issue, we first introduce a new performance estimation metric, named Random-Weight Evaluation (RWE) to quantify the quality of CNNs in a cost-efficient manner. Instead of fully training the entire CNN, the RWE only trains its last layer and leaves the remainders with randomly initialized weights, which results in a single network evaluation in seconds.Second, a complexity metric is adopted for multi-objective NAS to balance the model size and performance. Overall, our proposed method obtains a set of efficient models with state-of-the-art performance in two real-world search spaces. Then the results obtained on the CIFAR-10 dataset are transferred to the ImageNet dataset to validate the practicality of the proposed algorithm. Moreover, ablation studies on NAS-Bench-301 datasets reveal the effectiveness of the proposed RWE in estimating the performance compared with existing methods.



### Directionally Decomposing Structured Light for Projector Calibration
- **Arxiv ID**: http://arxiv.org/abs/2110.03924v1
- **DOI**: 10.1109/TVCG.2021.3106511
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.03924v1)
- **Published**: 2021-10-08 06:44:01+00:00
- **Updated**: 2021-10-08 06:44:01+00:00
- **Authors**: Masatoki Sugimoto, Daisuke Iwai, Koki Ishida, Parinya Punpongsanon, Kosuke Sato
- **Comment**: None
- **Journal**: None
- **Summary**: Intrinsic projector calibration is essential in projection mapping (PM) applications, especially in dynamic PM. However, due to the shallow depth-of-field (DOF) of a projector, more work is needed to ensure accurate calibration. We aim to estimate the intrinsic parameters of a projector while avoiding the limitation of shallow DOF. As the core of our technique, we present a practical calibration device that requires a minimal working volume directly in front of the projector lens regardless of the projector's focusing distance and aperture size. The device consists of a flat-bed scanner and pinhole-array masks. For calibration, a projector projects a series of structured light patterns in the device. The pinholes directionally decompose the structured light, and only the projected rays that pass through the pinholes hit the scanner plane. For each pinhole, we extract a ray passing through the optical center of the projector. Consequently, we regard the projector as a pinhole projector that projects the extracted rays only, and we calibrate the projector by applying the standard camera calibration technique, which assumes a pinhole camera model. Using a proof-of-concept prototype, we demonstrate that our technique can calibrate projectors with different focusing distances and aperture sizes at the same accuracy as a conventional method. Finally, we confirm that our technique can provide intrinsic parameters accurate enough for a dynamic PM application, even when a projector is placed too far from a projection target for a conventional method to calibrate the projector using a fiducial object of reasonable size.



### Pose Refinement with Joint Optimization of Visual Points and Lines
- **Arxiv ID**: http://arxiv.org/abs/2110.03940v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.03940v2)
- **Published**: 2021-10-08 07:22:51+00:00
- **Updated**: 2022-07-26 07:57:38+00:00
- **Authors**: Shuang Gao, Jixiang Wan, Yishan Ping, Xudong Zhang, Shuzhou Dong, Yuchen Yang, Haikuan Ning, Jijunnan Li, Yandong Guo
- **Comment**: Accepted by IROS2022
- **Journal**: None
- **Summary**: High-precision camera re-localization technology in a pre-established 3D environment map is the basis for many tasks, such as Augmented Reality, Robotics and Autonomous Driving. The point-based visual re-localization approaches are well-developed in recent decades, but are insufficient in some feature-less cases. In this paper, we design a complete pipeline for camera pose refinement with points and lines, which contains the innovatively designed line extracting CNN named VLSE, the line matching and the pose optimization approaches. We adopt a novel line representation and customize a hybrid convolution block based on the Stacked Hourglass network, to detect accurate and stable line features on images. Then we apply a geometric-based strategy to obtain precise 2D-3D line correspondences using epipolar constraint and reprojection filtering. A following point-line joint cost function is constructed to optimize the camera pose with the initial coarse pose from the pure point-based localization. Sufficient experiments are conducted on open datasets, i.e, line extractor on Wireframe and YorkUrban, localization performance on InLoc duc1 and duc2, to confirm the effectiveness of our point-line joint pose optimization method.



### GaitPrivacyON: Privacy-Preserving Mobile Gait Biometrics using Unsupervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.03967v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2110.03967v2)
- **Published**: 2021-10-08 08:32:26+00:00
- **Updated**: 2022-07-19 10:31:52+00:00
- **Authors**: Paula Delgado-Santos, Ruben Tolosana, Richard Guest, Ruben Vera, Farzin Deravi, Aythami Morales
- **Comment**: None
- **Journal**: Pattern Recognition Letters, 2022
- **Summary**: Numerous studies in the literature have already shown the potential of biometrics on mobile devices for authentication purposes. However, it has been shown that, the learning processes associated to biometric systems might expose sensitive personal information about the subjects. This study proposes GaitPrivacyON, a novel mobile gait biometrics verification approach that provides accurate authentication results while preserving the sensitive information of the subject. It comprises two modules: i) a convolutional Autoencoder that transforms attributes of the biometric raw data, such as the gender or the activity being performed, into a new privacy-preserving representation; and ii) a mobile gait verification system based on the combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) with a Siamese architecture. The main advantage of GaitPrivacyON is that the first module (convolutional Autoencoder) is trained in an unsupervised way, without specifying the sensitive attributes of the subject to protect. The experimental results achieved using two popular databases (MotionSense and MobiAct) suggest the potential of GaitPrivacyON to significantly improve the privacy of the subject while keeping user authentication results higher than 99% Area Under the Curve (AUC). To the best of our knowledge, this is the first mobile gait verification approach that considers privacy-preserving methods trained in an unsupervised way.



### How to Build a Curb Dataset with LiDAR Data for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2110.03968v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2110.03968v1)
- **Published**: 2021-10-08 08:32:37+00:00
- **Updated**: 2021-10-08 08:32:37+00:00
- **Authors**: Dongfeng Bai, Tongtong Cao, Jingming Guo, Bingbing Liu
- **Comment**: 7 pages with 10 figures, submitted to 2022 IEEE International
  Conference on Robotics and Automation (ICRA)
- **Journal**: None
- **Summary**: Curbs are one of the essential elements of urban and highway traffic environments. Robust curb detection provides road structure information for motion planning in an autonomous driving system. Commonly, video cameras and 3D LiDARs are mounted on autonomous vehicles for curb detection. However, camera-based methods suffer from challenging illumination conditions. During the long period of time before wide application of Deep Neural Network (DNN) with point clouds, LiDAR-based curb detection methods are based on hand-crafted features, which suffer from poor detection in some complex scenes. Recently, DNN-based dynamic object detection using LiDAR data has become prevalent, while few works pay attention to curb detection with a DNN approach due to lack of labeled data. A dataset with curb annotations or an efficient curb labeling approach, hence, is of high demand...



### Score-based diffusion models for accelerated MRI
- **Arxiv ID**: http://arxiv.org/abs/2110.05243v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.05243v3)
- **Published**: 2021-10-08 08:42:03+00:00
- **Updated**: 2022-07-16 21:06:31+00:00
- **Authors**: Hyungjin Chung, Jong Chul Ye
- **Comment**: Code available at: https://github.com/HJ-harry/score-MRI
- **Journal**: None
- **Summary**: Score-based diffusion models provide a powerful way to model images using the gradient of the data distribution. Leveraging the learned score function as a prior, here we introduce a way to sample data from a conditional distribution given the measurements, such that the model can be readily used for solving inverse problems in imaging, especially for accelerated MRI. In short, we train a continuous time-dependent score function with denoising score matching. Then, at the inference stage, we iterate between numerical SDE solver and data consistency projection step to achieve reconstruction. Our model requires magnitude images only for training, and yet is able to reconstruct complex-valued data, and even extends to parallel imaging. The proposed method is agnostic to sub-sampling patterns, and can be used with any sampling schemes. Also, due to its generative nature, our approach can quantify uncertainty, which is not possible with standard regression settings. On top of all the advantages, our method also has very strong performance, even beating the models trained with full supervision. With extensive experiments, we verify the superiority of our method in terms of quality and practicality.



### Maximize the Exploration of Congeneric Semantics for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.03982v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.03982v1)
- **Published**: 2021-10-08 08:59:16+00:00
- **Updated**: 2021-10-08 08:59:16+00:00
- **Authors**: Ke Zhang, Sihong Chen, Qi Ju, Yong Jiang, Yucong Li, Xin He
- **Comment**: None
- **Journal**: None
- **Summary**: With the increase in the number of image data and the lack of corresponding labels, weakly supervised learning has drawn a lot of attention recently in computer vision tasks, especially in the fine-grained semantic segmentation problem. To alleviate human efforts from expensive pixel-by-pixel annotations, our method focuses on weakly supervised semantic segmentation (WSSS) with image-level tags, which are much easier to obtain. As a huge gap exists between pixel-level segmentation and image-level labels, how to reflect the image-level semantic information on each pixel is an important question. To explore the congeneric semantic regions from the same class to the maximum, we construct the patch-level graph neural network (P-GNN) based on the self-detected patches from different images that contain the same class labels. Patches can frame the objects as much as possible and include as little background as possible. The graph network that is established with patches as the nodes can maximize the mutual learning of similar objects. We regard the embedding vectors of patches as nodes, and use transformer-based complementary learning module to construct weighted edges according to the embedding similarity between different nodes. Moreover, to better supplement semantic information, we propose soft-complementary loss functions matched with the whole network structure. We conduct experiments on the popular PASCAL VOC 2012 benchmarks, and our model yields state-of-the-art performance.



### Automated Feature-Specific Tree Species Identification from Natural Images using Deep Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.03994v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.03994v1)
- **Published**: 2021-10-08 09:25:32+00:00
- **Updated**: 2021-10-08 09:25:32+00:00
- **Authors**: Dewald Homan, Johan A. du Preez
- **Comment**: 21 pages, 7 figures, submitted to Ecological Informatics
- **Journal**: None
- **Summary**: Prior work on plant species classification predominantly focuses on building models from isolated plant attributes. Hence, there is a need for tools that can assist in species identification in the natural world. We present a novel and robust two-fold approach capable of identifying trees in a real-world natural setting. Further, we leverage unlabelled data through deep semi-supervised learning and demonstrate superior performance to supervised learning. Our single-GPU implementation for feature recognition uses minimal annotated data and achieves accuracies of 93.96% and 93.11% for leaves and bark, respectively. Further, we extract feature-specific datasets of 50 species by employing this technique. Finally, our semi-supervised species classification method attains 94.04% top-5 accuracy for leaves and 83.04% top-5 accuracy for bark.



### Multi Proxy Anchor Family Loss for Several Types of Gradients
- **Arxiv ID**: http://arxiv.org/abs/2110.03997v8
- **DOI**: 10.1016/j.cviu.2023.103654
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.03997v8)
- **Published**: 2021-10-08 09:34:38+00:00
- **Updated**: 2023-06-11 10:10:56+00:00
- **Authors**: Shozo Saeki, Minoru Kawahara, Hirohisa Aman
- **Comment**: None
- **Journal**: Computer Vision and Image Understanding. 229 (2023) 103654
- **Summary**: The deep metric learning (DML) objective is to learn a neural network that maps into an embedding space where similar data are near and dissimilar data are far. However, conventional proxy-based losses for DML have two problems: gradient problem and application of the real-world dataset with multiple local centers. Additionally, the performance metrics of DML also have some issues with stability and flexibility. This paper proposes three multi-proxies anchor (MPA) family losses and a normalized discounted cumulative gain (nDCG@k) metric. This paper makes three contributions. (1) MPA-family losses can learn using a real-world dataset with multi-local centers. (2) MPA-family losses improve the training capacity of a neural network owing to solving the gradient problem. (3) MPA-family losses have data-wise or class-wise characteristics with respect to gradient generation. Finally, we demonstrate the effectiveness of MPA-family losses, and MPA-family losses achieves higher accuracy on two datasets for fine-grained images.



### Trident Pyramid Networks: The importance of processing at the feature pyramid level for better object detection
- **Arxiv ID**: http://arxiv.org/abs/2110.04004v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.04004v3)
- **Published**: 2021-10-08 09:59:59+00:00
- **Updated**: 2022-10-28 07:55:48+00:00
- **Authors**: Cédric Picron, Tinne Tuytelaars
- **Comment**: Accepted at BMVC 2022
- **Journal**: None
- **Summary**: Feature pyramids have become ubiquitous in multi-scale computer vision tasks such as object detection. Given their importance, a computer vision network can be divided into three parts: a backbone (generating a feature pyramid), a neck (refining the feature pyramid) and a head (generating the final output). Many existing networks operating on feature pyramids, named necks, are shallow and mostly focus on communication-based processing in the form of top-down and bottom-up operations. We present a new neck architecture called Trident Pyramid Network (TPN), that allows for a deeper design and for a better balance between communication-based processing and self-processing. We show consistent improvements when using our TPN neck on the COCO object detection benchmark, outperforming the popular BiFPN baseline by 0.5 AP, both when using the ResNet-50 and the ResNeXt-101-DCN backbone. Additionally, we empirically show that it is more beneficial to put additional computation into the TPN neck, rather than into the backbone, by outperforming a ResNet-101+FPN baseline with our ResNet-50+TPN network by 1.7 AP, while operating under similar computation budgets. This emphasizes the importance of performing computation at the feature pyramid level in modern-day object detection systems. Code is available at https://github.com/CedricPicron/TPN .



### An End-to-End Trainable Video Panoptic Segmentation Method usingTransformers
- **Arxiv ID**: http://arxiv.org/abs/2110.04009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.04009v1)
- **Published**: 2021-10-08 10:13:37+00:00
- **Updated**: 2021-10-08 10:13:37+00:00
- **Authors**: Jeongwon Ryu, Kwangjin Yoon
- **Comment**: This contains a brief abstract
- **Journal**: None
- **Summary**: In this paper, we present an algorithm to tackle a video panoptic segmentation problem, a newly emerging area of research. The video panoptic segmentation is a task that unifies the typical task of panoptic segmentation and multi-object tracking. In other words, it requires generating the instance tracking IDs along with panoptic segmentation results across video sequences. Our proposed video panoptic segmentation algorithm uses the transformer and it can be trained in end-to-end with an input of multiple video frames. We test our method on the STEP dataset and report its performance with recently proposed STQ metric. The method archived 57.81\% on the KITTI-STEP dataset and 31.8\% on the MOTChallenge-STEP dataset.



### Multidirectional Conjugate Gradients for Scalable Bundle Adjustment
- **Arxiv ID**: http://arxiv.org/abs/2110.04015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.04015v1)
- **Published**: 2021-10-08 10:21:44+00:00
- **Updated**: 2021-10-08 10:21:44+00:00
- **Authors**: Simon Weber, Nikolaus Demmel, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We revisit the problem of large-scale bundle adjustment and propose a technique called Multidirectional Conjugate Gradients that accelerates the solution of the normal equation by up to 61%. The key idea is that we enlarge the search space of classical preconditioned conjugate gradients to include multiple search directions. As a consequence, the resulting algorithm requires fewer iterations, leading to a significant speedup of large-scale reconstruction, in particular for denser problems where traditional approaches notoriously struggle. We provide a number of experimental ablation studies revealing the robustness to variations in the hyper-parameters and the speedup as a function of problem density.



### Chromatic Aberration Recovery on Arbitrary Images
- **Arxiv ID**: http://arxiv.org/abs/2110.04030v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2110.04030v1)
- **Published**: 2021-10-08 11:02:30+00:00
- **Updated**: 2021-10-08 11:02:30+00:00
- **Authors**: Daniel J. Blueman
- **Comment**: None
- **Journal**: None
- **Summary**: Digital imaging sensor technology has continued to outpace development in optical technology in modern imaging systems. The resulting quality loss attributable to lateral chromatic aberration is becoming increasingly significant as sensor resolution increases; other classes of aberration are less significant with classical image enhancement (e.g. sharpening), whereas lateral chromatic aberration becomes more significant. The goals of higher-performance and lighter lens systems drive a recent need to find new ways to overcome resulting image quality limitations.   This work demonstrates the robust and automatic minimisation of lateral chromatic aberration, recovering the loss of image quality using both artificial and real-world images. A series of test images are used to validate the functioning of the algorithm, and changes across a series of real-world images are used to evaluate the performance of the approach.



### UniNet: Unified Architecture Search with Convolution, Transformer, and MLP
- **Arxiv ID**: http://arxiv.org/abs/2110.04035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.04035v1)
- **Published**: 2021-10-08 11:09:40+00:00
- **Updated**: 2021-10-08 11:09:40+00:00
- **Authors**: Jihao Liu, Hongsheng Li, Guanglu Song, Xin Huang, Yu Liu
- **Comment**: technich report
- **Journal**: None
- **Summary**: Recently, transformer and multi-layer perceptron (MLP) architectures have achieved impressive results on various vision tasks. A few works investigated manually combining those operators to design visual network architectures, and can achieve satisfactory performances to some extent. In this paper, we propose to jointly search the optimal combination of convolution, transformer, and MLP for building a series of all-operator network architectures with high performances on visual tasks. We empirically identify that the widely-used strided convolution or pooling based down-sampling modules become the performance bottlenecks when the operators are combined to form a network. To better tackle the global context captured by the transformer and MLP operators, we propose two novel context-aware down-sampling modules, which can better adapt to the global information encoded by transformer and MLP operators. To this end, we jointly search all operators and down-sampling modules in a unified search space. Notably, Our searched network UniNet (Unified Network) outperforms state-of-the-art pure convolution-based architecture, EfficientNet, and pure transformer-based architecture, Swin-Transformer, on multiple public visual benchmarks, ImageNet classification, COCO object detection, and ADE20K semantic segmentation.



### Context-LGM: Leveraging Object-Context Relation for Context-Aware Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2110.04042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.04042v1)
- **Published**: 2021-10-08 11:31:58+00:00
- **Updated**: 2021-10-08 11:31:58+00:00
- **Authors**: Mingzhou Liu, Xinwei Sun, Fandong Zhang, Yizhou Yu, Yizhou Wang
- **Comment**: 13 pages, currently under review
- **Journal**: None
- **Summary**: Context, as referred to situational factors related to the object of interest, can help infer the object's states or properties in visual recognition. As such contextual features are too diverse (across instances) to be annotated, existing attempts simply exploit image labels as supervision to learn them, resulting in various contextual tricks, such as features pyramid, context attention, etc. However, without carefully modeling the context's properties, especially its relation to the object, their estimated context can suffer from large inaccuracy. To amend this problem, we propose a novel Contextual Latent Generative Model (Context-LGM), which considers the object-context relation and models it in a hierarchical manner. Specifically, we firstly introduce a latent generative model with a pair of correlated latent variables to respectively model the object and context, and embed their correlation via the generative process. Then, to infer contextual features, we reformulate the objective function of Variational Auto-Encoder (VAE), where contextual features are learned as a posterior distribution conditioned on the object. Finally, to implement this contextual posterior, we introduce a Transformer that takes the object's information as a reference and locates correlated contextual factors. The effectiveness of our method is verified by state-of-the-art performance on two context-aware object recognition tasks, i.e. lung cancer prediction and emotion recognition.



### Discover, Hallucinate, and Adapt: Open Compound Domain Adaptation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2110.04111v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.04111v1)
- **Published**: 2021-10-08 13:20:09+00:00
- **Updated**: 2021-10-08 13:20:09+00:00
- **Authors**: KwanYong Park, Sanghyun Woo, Inkyu Shin, In So Kweon
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) for semantic segmentation has been attracting attention recently, as it could be beneficial for various label-scarce real-world scenarios (e.g., robot control, autonomous driving, medical imaging, etc.). Despite the significant progress in this field, current works mainly focus on a single-source single-target setting, which cannot handle more practical settings of multiple targets or even unseen targets. In this paper, we investigate open compound domain adaptation (OCDA), which deals with mixed and novel situations at the same time, for semantic segmentation. We present a novel framework based on three main design principles: discover, hallucinate, and adapt. The scheme first clusters compound target data based on style, discovering multiple latent domains (discover). Then, it hallucinates multiple latent target domains in source by using image-translation (hallucinate). This step ensures the latent domains in the source and the target to be paired. Finally, target-to-source alignment is learned separately between domains (adapt). In high-level, our solution replaces a hard OCDA problem with much easier multiple UDA problems. We evaluate our solution on standard benchmark GTA to C-driving, and achieved new state-of-the-art results.



### The Layout Generation Algorithm of Graphic Design Based on Transformer-CVAE
- **Arxiv ID**: http://arxiv.org/abs/2110.06794v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.06794v2)
- **Published**: 2021-10-08 13:36:02+00:00
- **Updated**: 2022-01-06 16:59:13+00:00
- **Authors**: Mengxi Guo, Dangqing Huang, Xiaodong Xie
- **Comment**: The table on page 5 is wrong and does not indicate the metric
  corresponding to each value. At the same time, some of the data are
  questionable
- **Journal**: None
- **Summary**: Graphic design is ubiquitous in people's daily lives. For graphic design, the most time-consuming task is laying out various components in the interface. Repetitive manual layout design will waste a lot of time for professional graphic designers. Existing templates are usually rudimentary and not suitable for most designs, reducing efficiency and limiting creativity. This paper implemented the Transformer model and conditional variational autoencoder (CVAE) to the graphic design layout generation task. It proposed an end-to-end graphic design layout generation model named LayoutT-CVAE. We also proposed element disentanglement and feature-based disentanglement strategies and introduce new graphic design principles and similarity metrics into the model, which significantly increased the controllability and interpretability of the deep model. Compared with the existing state-of-art models, the layout generated by ours performs better on many metrics.



### Rapid head-pose detection for automated slice prescription of fetal-brain MRI
- **Arxiv ID**: http://arxiv.org/abs/2110.04140v1
- **DOI**: 10.1002/ima.22563
- **Categories**: **cs.CV**, eess.IV, physics.med-ph, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2110.04140v1)
- **Published**: 2021-10-08 13:59:05+00:00
- **Updated**: 2021-10-08 13:59:05+00:00
- **Authors**: Malte Hoffmann, Esra Abaci Turk, Borjan Gagoski, Leah Morgan, Paul Wighton, M. Dylan Tisdall, Martin Reuter, Elfar Adalsteinsson, P. Ellen Grant, Lawrence L. Wald, André J. W. van der Kouwe
- **Comment**: 19 pages, 10 figures, 2 tables, fetal MRI, head-pose detection, MSER,
  scan automation, scan prescription, slice positioning, final published
  version
- **Journal**: Int J Imaging Syst Technol, 31 (3), 2021, 1136-1154
- **Summary**: In fetal-brain MRI, head-pose changes between prescription and acquisition present a challenge to obtaining the standard sagittal, coronal and axial views essential to clinical assessment. As motion limits acquisitions to thick slices that preclude retrospective resampling, technologists repeat ~55-second stack-of-slices scans (HASTE) with incrementally reoriented field of view numerous times, deducing the head pose from previous stacks. To address this inefficient workflow, we propose a robust head-pose detection algorithm using full-uterus scout scans (EPI) which take ~5 seconds to acquire. Our ~2-second procedure automatically locates the fetal brain and eyes, which we derive from maximally stable extremal regions (MSERs). The success rate of the method exceeds 94% in the third trimester, outperforming a trained technologist by up to 20%. The pipeline may be used to automatically orient the anatomical sequence, removing the need to estimate the head pose from 2D views and reducing delays during which motion can occur.



### Explainability-Aware One Point Attack for Point Cloud Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.04158v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.04158v3)
- **Published**: 2021-10-08 14:29:02+00:00
- **Updated**: 2022-03-23 15:43:39+00:00
- **Authors**: Hanxiao Tan, Helena Kotthaus
- **Comment**: None
- **Journal**: None
- **Summary**: With the proposition of neural networks for point clouds, deep learning has started to shine in the field of 3D object recognition while researchers have shown an increased interest to investigate the reliability of point cloud networks by adversarial attacks. However, most of the existing studies aim to deceive humans or defense algorithms, while the few that address the operation principles of the models themselves remain flawed in terms of critical point selection. In this work, we propose two adversarial methods: One Point Attack (OPA) and Critical Traversal Attack (CTA), which incorporate the explainability technologies and aim to explore the intrinsic operating principle of point cloud networks and their sensitivity against critical points perturbations. Our results show that popular point cloud networks can be deceived with almost $100\%$ success rate by shifting only one point from the input instance. In addition, we show the interesting impact of different point attribution distributions on the adversarial robustness of point cloud networks. Finally, we discuss how our approaches facilitate the explainability study for point cloud networks. To the best of our knowledge, this is the first point-cloud-based adversarial approach concerning explainability. Our code is available at https://github.com/Explain3D/Exp-One-Point-Atk-PC.



### Semantic Image Alignment for Vehicle Localization
- **Arxiv ID**: http://arxiv.org/abs/2110.04162v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.04162v1)
- **Published**: 2021-10-08 14:40:15+00:00
- **Updated**: 2021-10-08 14:40:15+00:00
- **Authors**: Markus Herb, Matthias Lemberger, Marcel M. Schmitt, Alexander Kurz, Tobias Weiherer, Nassir Navab, Federico Tombari
- **Comment**: Accepted at 2021 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2021)
- **Journal**: None
- **Summary**: Accurate and reliable localization is a fundamental requirement for autonomous vehicles to use map information in higher-level tasks such as navigation or planning. In this paper, we present a novel approach to vehicle localization in dense semantic maps, including vectorized high-definition maps or 3D meshes, using semantic segmentation from a monocular camera. We formulate the localization task as a direct image alignment problem on semantic images, which allows our approach to robustly track the vehicle pose in semantically labeled maps by aligning virtual camera views rendered from the map to sequences of semantically segmented camera images. In contrast to existing visual localization approaches, the system does not require additional keypoint features, handcrafted localization landmark extractors or expensive LiDAR sensors. We demonstrate the wide applicability of our method on a diverse set of semantic mesh maps generated from stereo or LiDAR as well as manually annotated HD maps and show that it achieves reliable and accurate localization in real-time.



### PHNNs: Lightweight Neural Networks via Parameterized Hypercomplex Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2110.04176v2
- **DOI**: 10.1109/TNNLS.2022.3226772
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.04176v2)
- **Published**: 2021-10-08 14:57:19+00:00
- **Updated**: 2022-09-19 09:24:23+00:00
- **Authors**: Eleonora Grassucci, Aston Zhang, Danilo Comminiello
- **Comment**: Submitted to IEEE Transactions on Neural Networks and Learning
  Systems
- **Journal**: None
- **Summary**: Hypercomplex neural networks have proven to reduce the overall number of parameters while ensuring valuable performance by leveraging the properties of Clifford algebras. Recently, hypercomplex linear layers have been further improved by involving efficient parameterized Kronecker products. In this paper, we define the parameterization of hypercomplex convolutional layers and introduce the family of parameterized hypercomplex neural networks (PHNNs) that are lightweight and efficient large-scale models. Our method grasps the convolution rules and the filter organization directly from data without requiring a rigidly predefined domain structure to follow. PHNNs are flexible to operate in any user-defined or tuned domain, from 1D to $n$D regardless of whether the algebra rules are preset. Such a malleability allows processing multidimensional inputs in their natural domain without annexing further dimensions, as done, instead, in quaternion neural networks for 3D inputs like color images. As a result, the proposed family of PHNNs operates with $1/n$ free parameters as regards its analog in the real domain. We demonstrate the versatility of this approach to multiple domains of application by performing experiments on various image datasets as well as audio datasets in which our method outperforms real and quaternion-valued counterparts. Full code is available at: https://github.com/eleGAN23/HyperNets.



### Dataset Condensation with Distribution Matching
- **Arxiv ID**: http://arxiv.org/abs/2110.04181v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.04181v3)
- **Published**: 2021-10-08 15:02:30+00:00
- **Updated**: 2022-12-22 02:50:07+00:00
- **Authors**: Bo Zhao, Hakan Bilen
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision 2023 (WACV)
- **Summary**: Computational cost of training state-of-the-art deep models in many learning problems is rapidly increasing due to more sophisticated models and larger datasets. A recent promising direction for reducing training cost is dataset condensation that aims to replace the original large training set with a significantly smaller learned synthetic set while preserving the original information. While training deep models on the small set of condensed images can be extremely fast, their synthesis remains computationally expensive due to the complex bi-level optimization and second-order derivative computation. In this work, we propose a simple yet effective method that synthesizes condensed images by matching feature distributions of the synthetic and original training images in many sampled embedding spaces. Our method significantly reduces the synthesis cost while achieving comparable or better performance. Thanks to its efficiency, we apply our method to more realistic and larger datasets with sophisticated neural architectures and obtain a significant performance boost. We also show promising practical benefits of our method in continual learning and neural architecture search.



### Cross-modal Knowledge Distillation for Vision-to-Sensor Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.01849v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.01849v1)
- **Published**: 2021-10-08 15:06:38+00:00
- **Updated**: 2021-10-08 15:06:38+00:00
- **Authors**: Jianyuan Ni, Raunak Sarbajna, Yang Liu, Anne H. H. Ngu, Yan Yan
- **Comment**: 5 pages, 2 figures, submitted to ICASSP2022
- **Journal**: None
- **Summary**: Human activity recognition (HAR) based on multi-modal approach has been recently shown to improve the accuracy performance of HAR. However, restricted computational resources associated with wearable devices, i.e., smartwatch, failed to directly support such advanced methods. To tackle this issue, this study introduces an end-to-end Vision-to-Sensor Knowledge Distillation (VSKD) framework. In this VSKD framework, only time-series data, i.e., accelerometer data, is needed from wearable devices during the testing phase. Therefore, this framework will not only reduce the computational demands on edge devices, but also produce a learning model that closely matches the performance of the computational expensive multi-modal approach. In order to retain the local temporal relationship and facilitate visual deep learning models, we first convert time-series data to two-dimensional images by applying the Gramian Angular Field ( GAF) based encoding method. We adopted ResNet18 and multi-scale TRN with BN-Inception as teacher and student network in this study, respectively. A novel loss function, named Distance and Angle-wised Semantic Knowledge loss (DASK), is proposed to mitigate the modality variations between the vision and the sensor domain. Extensive experimental results on UTD-MHAD, MMAct, and Berkeley-MHAD datasets demonstrate the effectiveness and competitiveness of the proposed VSKD model which can deployed on wearable sensors.



### Exploiting the Intrinsic Neighborhood Structure for Source-free Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2110.04202v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.04202v3)
- **Published**: 2021-10-08 15:40:18+00:00
- **Updated**: 2021-11-29 13:04:37+00:00
- **Authors**: Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, Shangling Jui
- **Comment**: NeurIPS 2021. Fix four number errors in Tab.5 (first two tables, row
  3 and 4) and corresponding text
- **Journal**: None
- **Summary**: Domain adaptation (DA) aims to alleviate the domain shift between source domain and target domain. Most DA methods require access to the source data, but often that is not possible (e.g. due to data privacy or intellectual property). In this paper, we address the challenging source-free domain adaptation (SFDA) problem, where the source pretrained model is adapted to the target domain in the absence of source data. Our method is based on the observation that target data, which might no longer align with the source domain classifier, still forms clear clusters. We capture this intrinsic structure by defining local affinity of the target data, and encourage label consistency among data with high local affinity. We observe that higher affinity should be assigned to reciprocal neighbors, and propose a self regularization loss to decrease the negative impact of noisy neighbors. Furthermore, to aggregate information with more context, we consider expanded neighborhoods with small affinity values. In the experimental results we verify that the inherent structure of the target features is an important source of information for domain adaptation. We demonstrate that this local structure can be efficiently captured by considering the local neighbors, the reciprocal neighbors, and the expanded neighborhood. Finally, we achieve state-of-the-art performance on several 2D image and 3D point cloud recognition datasets. Code is available in https://github.com/Albert0147/SFDA_neighbors.



### Toward a Human-Level Video Understanding Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2110.04203v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2110.04203v2)
- **Published**: 2021-10-08 15:41:18+00:00
- **Updated**: 2021-10-18 01:46:06+00:00
- **Authors**: Yu-Jung Heo, Minsu Lee, Seongho Choi, Woo Suk Choi, Minjung Shin, Minjoon Jung, Jeh-Kwang Ryu, Byoung-Tak Zhang
- **Comment**: Presented at AI-HRI symposium as part of AAAI-FSS 2021
  (arXiv:2109.10836). The first two authors have equal contribution
- **Journal**: None
- **Summary**: We aim to develop an AI agent that can watch video clips and have a conversation with human about the video story. Developing video understanding intelligence is a significantly challenging task, and evaluation methods for adequately measuring and analyzing the progress of AI agent are lacking as well. In this paper, we propose the Video Turing Test to provide effective and practical assessments of video understanding intelligence as well as human-likeness evaluation of AI agents. We define a general format and procedure of the Video Turing Test and present a case study to confirm the effectiveness and usefulness of the proposed test.



### Combining Image Features and Patient Metadata to Enhance Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.05239v1
- **DOI**: 10.1109/EMBC46164.2021.9630047
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.05239v1)
- **Published**: 2021-10-08 15:43:31+00:00
- **Updated**: 2021-10-08 15:43:31+00:00
- **Authors**: Spencer A. Thomas
- **Comment**: paper has been accepted at the EMBC 2021 https://embc.embs.org/2021/
- **Journal**: None
- **Summary**: In this work, we compare the performance of six state-of-the-art deep neural networks in classification tasks when using only image features, to when these are combined with patient metadata. We utilise transfer learning from networks pretrained on ImageNet to extract image features from the ISIC HAM10000 dataset prior to classification. Using several classification performance metrics, we evaluate the effects of including metadata with the image features. Furthermore, we repeat our experiments with data augmentation. Our results show an overall enhancement in performance of each network as assessed by all metrics, only noting degradation in a vgg16 architecture. Our results indicate that this performance enhancement may be a general property of deep networks and should be explored in other areas. Moreover, these improvements come at a negligible additional cost in computation time, and therefore are a practical method for other applications.



### Salient ImageNet: How to discover spurious features in Deep Learning?
- **Arxiv ID**: http://arxiv.org/abs/2110.04301v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.04301v4)
- **Published**: 2021-10-08 15:55:05+00:00
- **Updated**: 2022-03-28 02:55:37+00:00
- **Authors**: Sahil Singla, Soheil Feizi
- **Comment**: Accepted at ICLR, 2022
- **Journal**: None
- **Summary**: Deep neural networks can be unreliable in the real world especially when they heavily use {\it spurious} features for their predictions. Focusing on image classifications, we define {\it core features} as the set of visual features that are always a part of the object definition while {\it spurious features} are the ones that are likely to {\it co-occur} with the object but not a part of it (e.g., attribute "fingers" for class "band aid"). Traditional methods for discovering spurious features either require extensive human annotations (thus, not scalable), or are useful on specific models. In this work, we introduce a {\it general} framework to discover a subset of spurious and core visual features used in inferences of a general model and localize them on a large number of images with minimal human supervision. Our methodology is based on this key idea: to identify spurious or core \textit{visual features} used in model predictions, we identify spurious or core \textit{neural features} (penultimate layer neurons of a robust model) via limited human supervision (e.g., using top 5 activating images per feature). We then show that these neural feature annotations {\it generalize} extremely well to many more images {\it without} any human supervision. We use the activation maps for these neural features as the soft masks to highlight spurious or core visual features. Using this methodology, we introduce the {\it Salient Imagenet} dataset containing core and spurious masks for a large set of samples from Imagenet. Using this dataset, we show that several popular Imagenet models rely heavily on various spurious features in their predictions, indicating the standard accuracy alone is not sufficient to fully assess model performance. Code and dataset for reproducing all experiments in the paper is available at \url{https://github.com/singlasahil14/salient_imagenet}.



### Inferring Offensiveness In Images From Natural Language Supervision
- **Arxiv ID**: http://arxiv.org/abs/2110.04222v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.04222v1)
- **Published**: 2021-10-08 16:19:21+00:00
- **Updated**: 2021-10-08 16:19:21+00:00
- **Authors**: Patrick Schramowski, Kristian Kersting
- **Comment**: None
- **Journal**: None
- **Summary**: Probing or fine-tuning (large-scale) pre-trained models results in state-of-the-art performance for many NLP tasks and, more recently, even for computer vision tasks when combined with image data. Unfortunately, these approaches also entail severe risks. In particular, large image datasets automatically scraped from the web may contain derogatory terms as categories and offensive images, and may also underrepresent specific classes. Consequently, there is an urgent need to carefully document datasets and curate their content. Unfortunately, this process is tedious and error-prone. We show that pre-trained transformers themselves provide a methodology for the automated curation of large-scale vision datasets. Based on human-annotated examples and the implicit knowledge of a CLIP based model, we demonstrate that one can select relevant prompts for rating the offensiveness of an image. In addition to e.g. privacy violation and pornographic content previously identified in ImageNet, we demonstrate that our approach identifies further inappropriate and potentially offensive content.



### Observations on K-image Expansion of Image-Mixing Augmentation for Classification
- **Arxiv ID**: http://arxiv.org/abs/2110.04248v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.04248v2)
- **Published**: 2021-10-08 16:58:20+00:00
- **Updated**: 2023-03-17 11:07:16+00:00
- **Authors**: Joonhyun Jeong, Sungmin Cha, Youngjoon Yoo, Sangdoo Yun, Taesup Moon, Jongwon Choi
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Image-mixing augmentations (e.g., Mixup and CutMix), which typically involve mixing two images, have become the de-facto training techniques for image classification. Despite their huge success in image classification, the number of images to be mixed has not been elucidated in the literature: only the naive K-image expansion has been shown to lead to performance degradation. This study derives a new K-image mixing augmentation based on the stick-breaking process under Dirichlet prior distribution. We demonstrate the superiority of our K-image expansion augmentation over conventional two-image mixing augmentation methods through extensive experiments and analyses: (1) more robust and generalized classifiers; (2) a more desirable loss landscape shape; (3) better adversarial robustness. Moreover, we show that our probabilistic model can measure the sample-wise uncertainty and boost the efficiency for network architecture search by achieving a 7-fold reduction in the search time. Code will be available at https://github.com/yjyoo3312/DCutMix-PyTorch.git.



### Active learning for interactive satellite image change detection
- **Arxiv ID**: http://arxiv.org/abs/2110.04250v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.04250v1)
- **Published**: 2021-10-08 16:59:12+00:00
- **Updated**: 2021-10-08 16:59:12+00:00
- **Authors**: Hichem Sahbi, Sebastien Deschamps, Andrei Stoian
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce in this paper a novel active learning algorithm for satellite image change detection. The proposed solution is interactive and based on a question and answer model, which asks an oracle (annotator) the most informative questions about the relevance of sampled satellite image pairs, and according to the oracle's responses, updates a decision function iteratively. We investigate a novel framework which models the probability that samples are relevant; this probability is obtained by minimizing an objective function capturing representativity, diversity and ambiguity. Only data with a high probability according to these criteria are selected and displayed to the oracle for further annotation. Extensive experiments on the task of satellite image change detection after natural hazards (namely tornadoes) show the relevance of the proposed method against the related work.



### LCS: Learning Compressible Subspaces for Adaptive Network Compression at Inference Time
- **Arxiv ID**: http://arxiv.org/abs/2110.04252v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.04252v1)
- **Published**: 2021-10-08 17:03:34+00:00
- **Updated**: 2021-10-08 17:03:34+00:00
- **Authors**: Elvis Nunez, Maxwell Horton, Anish Prabhu, Anurag Ranjan, Ali Farhadi, Mohammad Rastegari
- **Comment**: None
- **Journal**: None
- **Summary**: When deploying deep learning models to a device, it is traditionally assumed that available computational resources (compute, memory, and power) remain static. However, real-world computing systems do not always provide stable resource guarantees. Computational resources need to be conserved when load from other processes is high or battery power is low. Inspired by recent works on neural network subspaces, we propose a method for training a "compressible subspace" of neural networks that contains a fine-grained spectrum of models that range from highly efficient to highly accurate. Our models require no retraining, thus our subspace of models can be deployed entirely on-device to allow adaptive network compression at inference time. We present results for achieving arbitrarily fine-grained accuracy-efficiency trade-offs at inference time for structured and unstructured sparsity. We achieve accuracies on-par with standard models when testing our uncompressed models, and maintain high accuracy for sparsity rates above 90% when testing our compressed models. We also demonstrate that our algorithm extends to quantization at variable bit widths, achieving accuracy on par with individually trained networks.



### Collaging Class-specific GANs for Semantic Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2110.04281v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.04281v1)
- **Published**: 2021-10-08 17:46:56+00:00
- **Updated**: 2021-10-08 17:46:56+00:00
- **Authors**: Yuheng Li, Yijun Li, Jingwan Lu, Eli Shechtman, Yong Jae Lee, Krishna Kumar Singh
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: We propose a new approach for high resolution semantic image synthesis. It consists of one base image generator and multiple class-specific generators. The base generator generates high quality images based on a segmentation map. To further improve the quality of different objects, we create a bank of Generative Adversarial Networks (GANs) by separately training class-specific models. This has several benefits including -- dedicated weights for each class; centrally aligned data for each model; additional training data from other sources, potential of higher resolution and quality; and easy manipulation of a specific object in the scene. Experiments show that our approach can generate high quality images in high resolution while having flexibility of object-level control by using class-specific generators.



### Field Extraction from Forms with Unlabeled Data
- **Arxiv ID**: http://arxiv.org/abs/2110.04282v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.04282v2)
- **Published**: 2021-10-08 17:50:12+00:00
- **Updated**: 2022-04-11 21:07:06+00:00
- **Authors**: Mingfei Gao, Zeyuan Chen, Nikhil Naik, Kazuma Hashimoto, Caiming Xiong, Ran Xu
- **Comment**: Spa-NLP@ACL2022
- **Journal**: None
- **Summary**: We propose a novel framework to conduct field extraction from forms with unlabeled data. To bootstrap the training process, we develop a rule-based method for mining noisy pseudo-labels from unlabeled forms. Using the supervisory signal from the pseudo-labels, we extract a discriminative token representation from a transformer-based model by modeling the interaction between text in the form. To prevent the model from overfitting to label noise, we introduce a refinement module based on a progressive pseudo-label ensemble. Experimental results demonstrate the effectiveness of our framework.



### Toward a Visual Concept Vocabulary for GAN Latent Space
- **Arxiv ID**: http://arxiv.org/abs/2110.04292v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2110.04292v1)
- **Published**: 2021-10-08 17:58:19+00:00
- **Updated**: 2021-10-08 17:58:19+00:00
- **Authors**: Sarah Schwettmann, Evan Hernandez, David Bau, Samuel Klein, Jacob Andreas, Antonio Torralba
- **Comment**: 15 pages, 13 figures. Accepted to ICCV 2021. Project page:
  https://visualvocab.csail.mit.edu
- **Journal**: None
- **Summary**: A large body of recent work has identified transformations in the latent spaces of generative adversarial networks (GANs) that consistently and interpretably transform generated images. But existing techniques for identifying these transformations rely on either a fixed vocabulary of pre-specified visual concepts, or on unsupervised disentanglement techniques whose alignment with human judgments about perceptual salience is unknown. This paper introduces a new method for building open-ended vocabularies of primitive visual concepts represented in a GAN's latent space. Our approach is built from three components: (1) automatic identification of perceptually salient directions based on their layer selectivity; (2) human annotation of these directions with free-form, compositional natural language descriptions; and (3) decomposition of these annotations into a visual concept vocabulary, consisting of distilled directions labeled with single words. Experiments show that concepts learned with our approach are reliable and composable -- generalizing across classes, contexts, and observers, and enabling fine-grained manipulation of image style and content.



### Evaluating generative networks using Gaussian mixtures of image features
- **Arxiv ID**: http://arxiv.org/abs/2110.05240v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.05240v2)
- **Published**: 2021-10-08 17:58:29+00:00
- **Updated**: 2022-07-22 22:22:11+00:00
- **Authors**: Lorenzo Luzi, Carlos Ortiz Marrero, Nile Wynar, Richard G. Baraniuk, Michael J. Henry
- **Comment**: None
- **Journal**: None
- **Summary**: We develop a measure for evaluating the performance of generative networks given two sets of images. A popular performance measure currently used to do this is the Fr\'echet Inception Distance (FID). FID assumes that images featurized using the penultimate layer of Inception-v3 follow a Gaussian distribution, an assumption which cannot be violated if we wish to use FID as a metric. However, we show that Inception-v3 features of the ImageNet dataset are not Gaussian; in particular, every single marginal is not Gaussian. To remedy this problem, we model the featurized images using Gaussian mixture models (GMMs) and compute the 2-Wasserstein distance restricted to GMMs. We define a performance measure, which we call WaM, on two sets of images by using Inception-v3 (or another classifier) to featurize the images, estimate two GMMs, and use the restricted $2$-Wasserstein distance to compare the GMMs. We experimentally show the advantages of WaM over FID, including how FID is more sensitive than WaM to imperceptible image perturbations. By modelling the non-Gaussian features obtained from Inception-v3 as GMMs and using a GMM metric, we can more accurately evaluate generative network performance.



### 2nd Place Solution to Google Landmark Retrieval 2021
- **Arxiv ID**: http://arxiv.org/abs/2110.04294v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.04294v1)
- **Published**: 2021-10-08 17:59:58+00:00
- **Updated**: 2021-10-08 17:59:58+00:00
- **Authors**: Zhang Yuqi, Xu Xianzhe, Chen Weihua, Wang Yaohua, Zhang Fangyi, Wang Fan, Li Hao
- **Comment**: Kaggle Competition, ICCV workshop
- **Journal**: None
- **Summary**: This paper presents the 2nd place solution to the Google Landmark Retrieval 2021 Competition on Kaggle. The solution is based on a baseline with training tricks from person re-identification, a continent-aware sampling strategy is presented to select training images according to their country tags and a Landmark-Country aware reranking is proposed for the retrieval task. With these contributions, we achieve 0.52995 mAP@100 on private leaderboard. Code available at https://github.com/WesleyZhang1991/Google_Landmark_Retrieval_2021_2nd_Place_Solution



### COVID-19 Face Mask Recognition with Advanced Face Cut Algorithm for Human Safety Measures
- **Arxiv ID**: http://arxiv.org/abs/2110.04316v1
- **DOI**: 10.1109/ICCCNT51525.2021.9580061
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2110.04316v1)
- **Published**: 2021-10-08 18:03:36+00:00
- **Updated**: 2021-10-08 18:03:36+00:00
- **Authors**: Arkaprabha Basu, Md Firoj Ali
- **Comment**: 5 pages, 7 figures
- **Journal**: 2021 12th International Conference on Computing Communication and
  Networking Technologies (ICCCNT)
- **Summary**: In the last year, the outbreak of COVID-19 has deployed computer vision and machine learning algorithms in various fields to enhance human life interactions. COVID-19 is a highly contaminated disease that affects mainly the respiratory organs of the human body. We must wear a mask in this situation as the virus can be contaminated through the air and a non-masked person can be affected. Our proposal deploys a computer vision and deep learning framework to recognize face masks from images or videos. We have implemented a Boundary dependent face cut recognition algorithm that can cut the face from the image using 27 landmarks and then the preprocessed image can further be sent to the deep learning ResNet50 model. The experimental result shows a significant advancement of 3.4 percent compared to the YOLOV3 mask recognition architecture in just 10 epochs.



### Learning a Self-Expressive Network for Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/2110.04318v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.04318v1)
- **Published**: 2021-10-08 18:06:06+00:00
- **Updated**: 2021-10-08 18:06:06+00:00
- **Authors**: Shangzhi Zhang, Chong You, René Vidal, Chun-Guang Li
- **Comment**: 15 pages, 11 figures, 6 tables. The paper is the complete version of
  the CVPR2021's paper with a set of extra experimental results and a link to
  download the code
- **Journal**: None
- **Summary**: State-of-the-art subspace clustering methods are based on self-expressive model, which represents each data point as a linear combination of other data points. However, such methods are designed for a finite sample dataset and lack the ability to generalize to out-of-sample data. Moreover, since the number of self-expressive coefficients grows quadratically with the number of data points, their ability to handle large-scale datasets is often limited. In this paper, we propose a novel framework for subspace clustering, termed Self-Expressive Network (SENet), which employs a properly designed neural network to learn a self-expressive representation of the data. We show that our SENet can not only learn the self-expressive coefficients with desired properties on the training data, but also handle out-of-sample data. Besides, we show that SENet can also be leveraged to perform subspace clustering on large-scale datasets. Extensive experiments conducted on synthetic data and real world benchmark data validate the effectiveness of the proposed method. In particular, SENet yields highly competitive performance on MNIST, Fashion MNIST and Extended MNIST and state-of-the-art performance on CIFAR-10. The code is available at https://github.com/zhangsz1998/Self-Expressive-Network.



### Adversarial Token Attacks on Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2110.04337v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.04337v1)
- **Published**: 2021-10-08 19:00:16+00:00
- **Updated**: 2021-10-08 19:00:16+00:00
- **Authors**: Ameya Joshi, Gauri Jagatap, Chinmay Hegde
- **Comment**: None
- **Journal**: None
- **Summary**: Vision transformers rely on a patch token based self attention mechanism, in contrast to convolutional networks. We investigate fundamental differences between these two families of models, by designing a block sparsity based adversarial token attack. We probe and analyze transformer as well as convolutional models with token attacks of varying patch sizes. We infer that transformer models are more sensitive to token attacks than convolutional models, with ResNets outperforming Transformer models by up to $\sim30\%$ in robust accuracy for single token attacks.



### Generational Frameshifts in Technology: Computer Science and Neurosurgery, The VR Use Case
- **Arxiv ID**: http://arxiv.org/abs/2110.15719v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV, cs.NE, q-bio.OT
- **Links**: [PDF](http://arxiv.org/pdf/2110.15719v2)
- **Published**: 2021-10-08 20:02:17+00:00
- **Updated**: 2021-11-01 01:00:57+00:00
- **Authors**: Samuel R. Browd, Maya Sharma, Chetan Sharma
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: We are at a unique moment in history where there is a confluence of technologies which will synergistically come together to transform the practice of neurosurgery. These technological transformations will be all-encompassing, including improved tools and methods for intraoperative performance of neurosurgery, scalable solutions for asynchronous neurosurgical training and simulation, as well as broad aggregation of operative data allowing fundamental changes in quality assessment, billing, outcome measures, and dissemination of surgical best practices. The ability to perform surgery more safely and more efficiently while capturing the operative details and parsing each component of the operation will open an entirely new epoch advancing our field and all surgical specialties. The digitization of all components within the operating room will allow us to leverage the various fields within computer and computational science to obtain new insights that will improve care and delivery of the highest quality neurosurgery regardless of location. The democratization of neurosurgery is at hand and will be driven by our development, extraction, and adoption of these tools of the modern world. Virtual reality provides a good example of how consumer-facing technologies are finding a clear role in industry and medicine and serves as a notable example of the confluence of various computer science technologies creating a novel paradigm for scaling human ability and interactions. The authors describe the technology ecosystem that has come and highlight a myriad of computational and data sciences that will be necessary to enable the operating room of the near future.



### Quantum pixel representations and compression for $N$-dimensional images
- **Arxiv ID**: http://arxiv.org/abs/2110.04405v2
- **DOI**: 10.1038/s41598-022-11024-y
- **Categories**: **quant-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.04405v2)
- **Published**: 2021-10-08 23:32:00+00:00
- **Updated**: 2021-10-14 18:50:31+00:00
- **Authors**: Mercy G. Amankwah, Daan Camps, E. Wes Bethel, Roel Van Beeumen, Talita Perciano
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel and uniform framework for quantum pixel representations that overarches many of the most popular representations proposed in the recent literature, such as (I)FRQI, (I)NEQR, MCRQI, and (I)NCQI. The proposed QPIXL framework results in more efficient circuit implementations and significantly reduces the gate complexity for all considered quantum pixel representations. Our method only requires a linear number of gates in terms of the number of pixels and does not use ancilla qubits. Furthermore, the circuits only consist of Ry gates and CNOT gates making them practical in the NISQ era. Additionally, we propose a circuit and image compression algorithm that is shown to be highly effective, being able to reduce the necessary gates to prepare an FRQI state for example scientific images by up to 90% without sacrificing image quality. Our algorithms are made publicly available as part of QPIXL++, a Quantum Image Pixel Library.



### Unsupervised Pose-Aware Part Decomposition for 3D Articulated Objects
- **Arxiv ID**: http://arxiv.org/abs/2110.04411v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.04411v1)
- **Published**: 2021-10-08 23:53:56+00:00
- **Updated**: 2021-10-08 23:53:56+00:00
- **Authors**: Yuki Kawana, Yusuke Mukuta, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: Articulated objects exist widely in the real world. However, previous 3D generative methods for unsupervised part decomposition are unsuitable for such objects, because they assume a spatially fixed part location, resulting in inconsistent part parsing. In this paper, we propose PPD (unsupervised Pose-aware Part Decomposition) to address a novel setting that explicitly targets man-made articulated objects with mechanical joints, considering the part poses. We show that category-common prior learning for both part shapes and poses facilitates the unsupervised learning of (1) part decomposition with non-primitive-based implicit representation, and (2) part pose as joint parameters under single-frame shape supervision. We evaluate our method on synthetic and real datasets, and we show that it outperforms previous works in consistent part parsing of the articulated objects based on comparable part pose estimation performance to the supervised baseline.



### Robustness Evaluation of Transformer-based Form Field Extractors via Form Attacks
- **Arxiv ID**: http://arxiv.org/abs/2110.04413v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2110.04413v1)
- **Published**: 2021-10-08 23:58:24+00:00
- **Updated**: 2021-10-08 23:58:24+00:00
- **Authors**: Le Xue, Mingfei Gao, Zeyuan Chen, Caiming Xiong, Ran Xu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel framework to evaluate the robustness of transformer-based form field extraction methods via form attacks. We introduce 14 novel form transformations to evaluate the vulnerability of the state-of-the-art field extractors against form attacks from both OCR level and form level, including OCR location/order rearrangement, form background manipulation and form field-value augmentation. We conduct robustness evaluation using real invoices and receipts, and perform comprehensive research analysis. Experimental results suggest that the evaluated models are very susceptible to form perturbations such as the variation of field-values (~15% drop in F1 score), the disarrangement of input text order(~15% drop in F1 score) and the disruption of the neighboring words of field-values(~10% drop in F1 score). Guided by the analysis, we make recommendations to improve the design of field extractors and the process of data collection.



