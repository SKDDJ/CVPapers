# Arxiv Papers in cs.CV on 2021-12-27
### Video Joint Modelling Based on Hierarchical Transformer for Co-summarization
- **Arxiv ID**: http://arxiv.org/abs/2112.13478v2
- **DOI**: 10.1109/TPAMI.2022.3186506
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13478v2)
- **Published**: 2021-12-27 01:54:35+00:00
- **Updated**: 2022-06-29 06:42:24+00:00
- **Authors**: Li Haopeng, Ke Qiuhong, Gong Mingming, Zhang Rui
- **Comment**: None
- **Journal**: None
- **Summary**: Video summarization aims to automatically generate a summary (storyboard or video skim) of a video, which can facilitate large-scale video retrieval and browsing. Most of the existing methods perform video summarization on individual videos, which neglects the correlations among similar videos. Such correlations, however, are also informative for video understanding and video summarization. To address this limitation, we propose Video Joint Modelling based on Hierarchical Transformer (VJMHT) for co-summarization, which takes into consideration the semantic dependencies across videos. Specifically, VJMHT consists of two layers of Transformer: the first layer extracts semantic representation from individual shots of similar videos, while the second layer performs shot-level video joint modelling to aggregate cross-video semantic information. By this means, complete cross-video high-level patterns are explicitly modelled and learned for the summarization of individual videos. Moreover, Transformer-based video representation reconstruction is introduced to maximize the high-level similarity between the summary and the original video. Extensive experiments are conducted to verify the effectiveness of the proposed modules and the superiority of VJMHT in terms of F-measure and rank-based evaluation.



### A Compact Neural Network-based Algorithm for Robust Image Watermarking
- **Arxiv ID**: http://arxiv.org/abs/2112.13491v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13491v1)
- **Published**: 2021-12-27 03:20:45+00:00
- **Updated**: 2021-12-27 03:20:45+00:00
- **Authors**: Hong-Bo Xu, Rong Wang, Jia Wei, Shao-Ping Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Digital image watermarking seeks to protect the digital media information from unauthorized access, where the message is embedded into the digital image and extracted from it, even some noises or distortions are applied under various data processing including lossy image compression and interactive content editing. Traditional image watermarking solutions easily suffer from robustness when specified with some prior constraints, while recent deep learning-based watermarking methods could not tackle the information loss problem well under various separate pipelines of feature encoder and decoder. In this paper, we propose a novel digital image watermarking solution with a compact neural network, named Invertible Watermarking Network (IWN). Our IWN architecture is based on a single Invertible Neural Network (INN), this bijective propagation framework enables us to effectively solve the challenge of message embedding and extraction simultaneously, by taking them as a pair of inverse problems for each other and learning a stable invertible mapping. In order to enhance the robustness of our watermarking solution, we specifically introduce a simple but effective bit message normalization module to condense the bit message to be embedded, and a noise layer is designed to simulate various practical attacks under our IWN framework. Extensive experiments demonstrate the superiority of our solution under various distortions.



### Vision Transformer for Small-Size Datasets
- **Arxiv ID**: http://arxiv.org/abs/2112.13492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13492v1)
- **Published**: 2021-12-27 03:24:03+00:00
- **Updated**: 2021-12-27 03:24:03+00:00
- **Authors**: Seung Hoon Lee, Seunghyun Lee, Byung Cheol Song
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the Vision Transformer (ViT), which applied the transformer structure to the image classification task, has outperformed convolutional neural networks. However, the high performance of the ViT results from pre-training using a large-size dataset such as JFT-300M, and its dependence on a large dataset is interpreted as due to low locality inductive bias. This paper proposes Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA), which effectively solve the lack of locality inductive bias and enable it to learn from scratch even on small-size datasets. Moreover, SPT and LSA are generic and effective add-on modules that are easily applicable to various ViTs. Experimental results show that when both SPT and LSA were applied to the ViTs, the performance improved by an average of 2.96% in Tiny-ImageNet, which is a representative small-size dataset. Especially, Swin Transformer achieved an overwhelming performance improvement of 4.08% thanks to the proposed SPT and LSA.



### Estimating Parameters of the Tree Root in Heterogeneous Soil Environments via Mask-Guided Multi-Polarimetric Integration Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2112.13494v1
- **DOI**: 10.1109/TGRS.2021.3138974
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2112.13494v1)
- **Published**: 2021-12-27 03:28:30+00:00
- **Updated**: 2021-12-27 03:28:30+00:00
- **Authors**: Hai-Han Sun, Yee Hui Lee, Qiqi Dai, Chongyi Li, Genevieve Ow, Mohamed Lokman Mohd Yusof, Abdulkadir C. Yucel
- **Comment**: 14 pages, 12 figures
- **Journal**: None
- **Summary**: Ground-penetrating radar (GPR) has been used as a non-destructive tool for tree root inspection. Estimating root-related parameters from GPR radargrams greatly facilitates root health monitoring and imaging. However, the task of estimating root-related parameters is challenging as the root reflection is a complex function of multiple root parameters and root orientations. Existing methods can only estimate a single root parameter at a time without considering the influence of other parameters and root orientations, resulting in limited estimation accuracy under different root conditions. In addition, soil heterogeneity introduces clutter in GPR radargrams, making the data processing and interpretation even harder. To address these issues, a novel neural network architecture, called mask-guided multi-polarimetric integration neural network (MMI-Net), is proposed to automatically and simultaneously estimate multiple root-related parameters in heterogeneous soil environments. The MMI-Net includes two sub-networks: a MaskNet that predicts a mask to highlight the root reflection area to eliminate interfering environmental clutter, and a ParaNet that uses the predicted mask as guidance to integrate, extract, and emphasize informative features in multi-polarimetric radargrams for accurate estimation of five key root-related parameters. The parameters include the root depth, diameter, relative permittivity, horizontal and vertical orientation angles. Experimental results demonstrate that the proposed MMI-Net achieves high estimation accuracy in these root-related parameters. This is the first work that takes the combined contributions of root parameters and spatial orientations into account and simultaneously estimates multiple root-related parameters. The data and code implemented in the paper can be found at https://haihan-sun.github.io/GPR.html.



### MSHT: Multi-stage Hybrid Transformer for the ROSE Image Analysis of Pancreatic Cancer
- **Arxiv ID**: http://arxiv.org/abs/2112.13513v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.13513v1)
- **Published**: 2021-12-27 05:04:11+00:00
- **Updated**: 2021-12-27 05:04:11+00:00
- **Authors**: Tianyi Zhang, Yunlu Feng, Yu Zhao, Guangda Fan, Aiming Yang, Shangqin Lyu, Peng Zhang, Fan Song, Chenbin Ma, Yangyang Sun, Youdan Feng, Guanglei Zhang
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: Pancreatic cancer is one of the most malignant cancers in the world, which deteriorates rapidly with very high mortality. The rapid on-site evaluation (ROSE) technique innovates the workflow by immediately analyzing the fast stained cytopathological images with on-site pathologists, which enables faster diagnosis in this time-pressured process. However, the wider expansion of ROSE diagnosis has been hindered by the lack of experienced pathologists. To overcome this problem, we propose a hybrid high-performance deep learning model to enable the automated workflow, thus freeing the occupation of the valuable time of pathologists. By firstly introducing the Transformer block into this field with our particular multi-stage hybrid design, the spatial features generated by the convolutional neural network (CNN) significantly enhance the Transformer global modeling. Turning multi-stage spatial features as global attention guidance, this design combines the robustness from the inductive bias of CNN with the sophisticated global modeling power of Transformer. A dataset of 4240 ROSE images is collected to evaluate the method in this unexplored field. The proposed multi-stage hybrid Transformer (MSHT) achieves 95.68% in classification accuracy, which is distinctively higher than the state-of-the-art models. Facing the need for interpretability, MSHT outperforms its counterparts with more accurate attention regions. The results demonstrate that the MSHT can distinguish cancer samples accurately at an unprecedented image scale, laying the foundation for deploying automatic decision systems and enabling the expansion of ROSE in clinical practice. The code and records are available at: https://github.com/sagizty/Multi-Stage-Hybrid-Transformer.



### Dual Contrastive Learning for General Face Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/2112.13522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13522v1)
- **Published**: 2021-12-27 05:44:40+00:00
- **Updated**: 2021-12-27 05:44:40+00:00
- **Authors**: Ke Sun, Taiping Yao, Shen Chen, Shouhong Ding, Jilin L, Rongrong Ji
- **Comment**: This paper was accepted by AAAI 2022 Conference on Artificial
  Intelligence
- **Journal**: None
- **Summary**: With various facial manipulation techniques arising, face forgery detection has drawn growing attention due to security concerns. Previous works always formulate face forgery detection as a classification problem based on cross-entropy loss, which emphasizes category-level differences rather than the essential discrepancies between real and fake faces, limiting model generalization in unseen domains. To address this issue, we propose a novel face forgery detection framework, named Dual Contrastive Learning (DCL), which specially constructs positive and negative paired data and performs designed contrastive learning at different granularities to learn generalized feature representation. Concretely, combined with the hard sample selection strategy, Inter-Instance Contrastive Learning (Inter-ICL) is first proposed to promote task-related discriminative features learning by especially constructing instance pairs. Moreover, to further explore the essential discrepancies, Intra-Instance Contrastive Learning (Intra-ICL) is introduced to focus on the local content inconsistencies prevalent in the forged faces by constructing local-region pairs inside instances. Extensive experiments and visualizations on several datasets demonstrate the generalization of our method against the state-of-the-art competitors.



### Learning Generative Vision Transformer with Energy-Based Latent Space for Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/2112.13528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13528v1)
- **Published**: 2021-12-27 06:04:33+00:00
- **Updated**: 2021-12-27 06:04:33+00:00
- **Authors**: Jing Zhang, Jianwen Xie, Nick Barnes, Ping Li
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Vision transformer networks have shown superiority in many computer vision tasks. In this paper, we take a step further by proposing a novel generative vision transformer with latent variables following an informative energy-based prior for salient object detection. Both the vision transformer network and the energy-based prior model are jointly trained via Markov chain Monte Carlo-based maximum likelihood estimation, in which the sampling from the intractable posterior and prior distributions of the latent variables are performed by Langevin dynamics. Further, with the generative vision transformer, we can easily obtain a pixel-wise uncertainty map from an image, which indicates the model confidence in predicting saliency from the image. Different from the existing generative models which define the prior distribution of the latent variables as a simple isotropic Gaussian distribution, our model uses an energy-based informative prior which can be more expressive to capture the latent space of the data. We apply the proposed framework to both RGB and RGB-D salient object detection tasks. Extensive experimental results show that our framework can achieve not only accurate saliency predictions but also meaningful uncertainty maps that are consistent with the human perception.



### Adversarial Attack for Asynchronous Event-based Data
- **Arxiv ID**: http://arxiv.org/abs/2112.13534v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.13534v1)
- **Published**: 2021-12-27 06:23:43+00:00
- **Updated**: 2021-12-27 06:23:43+00:00
- **Authors**: Wooju Lee, Hyun Myung
- **Comment**: 8 pages, 6 figures, Thirty-Sixth AAAI Conference on Artificial
  Intelligence (AAAI-22)
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are vulnerable to adversarial examples that are carefully designed to cause the deep learning model to make mistakes. Adversarial examples of 2D images and 3D point clouds have been extensively studied, but studies on event-based data are limited. Event-based data can be an alternative to a 2D image under high-speed movements, such as autonomous driving. However, the given adversarial events make the current deep learning model vulnerable to safety issues. In this work, we generate adversarial examples and then train the robust models for event-based data, for the first time. Our algorithm shifts the time of the original events and generates additional adversarial events. Additional adversarial events are generated in two stages. First, null events are added to the event-based data to generate additional adversarial events. The perturbation size can be controlled with the number of null events. Second, the location and time of additional adversarial events are set to mislead DNNs in a gradient-based attack. Our algorithm achieves an attack success rate of 97.95\% on the N-Caltech101 dataset. Furthermore, the adversarial training model improves robustness on the adversarial event data compared to the original model.



### Meta-Learned Feature Critics for Domain Generalized Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.13538v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13538v1)
- **Published**: 2021-12-27 06:43:39+00:00
- **Updated**: 2021-12-27 06:43:39+00:00
- **Authors**: Zu-Yun Shiau, Wei-Wei Lin, Ci-Siang Lin, Yu-Chiang Frank Wang
- **Comment**: Accepted by ICIP 2021
- **Journal**: None
- **Summary**: How to handle domain shifts when recognizing or segmenting visual data across domains has been studied by learning and vision communities. In this paper, we address domain generalized semantic segmentation, in which the segmentation model is trained on multiple source domains and is expected to generalize to unseen data domains. We propose a novel meta-learning scheme with feature disentanglement ability, which derives domain-invariant features for semantic segmentation with domain generalization guarantees. In particular, we introduce a class-specific feature critic module in our framework, enforcing the disentangled visual features with domain generalization guarantees. Finally, our quantitative results on benchmark datasets confirm the effectiveness and robustness of our proposed model, performing favorably against state-of-the-art domain adaptation and generalization methods in segmentation.



### Few-Shot Classification in Unseen Domains by Episodic Meta-Learning Across Visual Domains
- **Arxiv ID**: http://arxiv.org/abs/2112.13539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13539v1)
- **Published**: 2021-12-27 06:54:11+00:00
- **Updated**: 2021-12-27 06:54:11+00:00
- **Authors**: Yuan-Chia Cheng, Ci-Siang Lin, Fu-En Yang, Yu-Chiang Frank Wang
- **Comment**: Accepted by ICIP 2021
- **Journal**: None
- **Summary**: Few-shot classification aims to carry out classification given only few labeled examples for the categories of interest. Though several approaches have been proposed, most existing few-shot learning (FSL) models assume that base and novel classes are drawn from the same data domain. When it comes to recognizing novel-class data in an unseen domain, this becomes an even more challenging task of domain generalized few-shot classification. In this paper, we present a unique learning framework for domain-generalized few-shot classification, where base classes are from homogeneous multiple source domains, while novel classes to be recognized are from target domains which are not seen during training. By advancing meta-learning strategies, our learning framework exploits data across multiple source domains to capture domain-invariant features, with FSL ability introduced by metric-learning based mechanisms across support and query data. We conduct extensive experiments to verify the effectiveness of our proposed learning framework and show learning from small yet homogeneous source data is able to perform preferably against learning from large-scale one. Moreover, we provide insights into choices of backbone models for domain-generalized few-shot classification.



### Image Edge Restoring Filter
- **Arxiv ID**: http://arxiv.org/abs/2112.13540v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13540v1)
- **Published**: 2021-12-27 07:02:01+00:00
- **Updated**: 2021-12-27 07:02:01+00:00
- **Authors**: Qian Liu, Yongpeng Li, Zhihang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In computer vision, image processing and computer graphics, image smoothing filtering is a very basic and important task and to be expected possessing good edge-preserving smoothing property. Here we address the problem that the edge-preserving ability of many popular local smoothing filters needs to be improved. In this paper, we propose the image Edge Restoring Filter (ERF) to restore the blur edge pixels in the output of local smoothing filters to be clear. The proposed filter can been implemented after many local smoothing filter (such as Box filter, Gaussian filter, Bilateral Filter, Guided Filter and so on). The combinations of "original local smoothing filters + ERF" have better edge-preserving smoothing property than the original local smoothing filters. Experiments on image smoothing, image denoising and image enhancement demonstrate the excellent edges restoring ability of the proposed filter and good edgepreserving smoothing property of the combination "original local smoothing filters + ERF". The proposed filter would benefit a great variety of applications given that smoothing filtering is a high frequently used and fundamental operation.



### ViR:the Vision Reservoir
- **Arxiv ID**: http://arxiv.org/abs/2112.13545v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.13545v2)
- **Published**: 2021-12-27 07:07:50+00:00
- **Updated**: 2021-12-29 06:30:56+00:00
- **Authors**: Xian Wei, Bin Wang, Mingsong Chen, Ji Yuan, Hai Lan, Jiehuang Shi, Xuan Tang, Bo Jin, Guozhang Chen, Dongping Yang
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: The most recent year has witnessed the success of applying the Vision Transformer (ViT) for image classification. However, there are still evidences indicating that ViT often suffers following two aspects, i) the high computation and the memory burden from applying the multiple Transformer layers for pre-training on a large-scale dataset, ii) the over-fitting when training on small datasets from scratch. To address these problems, a novel method, namely, Vision Reservoir computing (ViR), is proposed here for image classification, as a parallel to ViT. By splitting each image into a sequence of tokens with fixed length, the ViR constructs a pure reservoir with a nearly fully connected topology to replace the Transformer module in ViT. Two kinds of deep ViR models are subsequently proposed to enhance the network performance. Comparative experiments between the ViR and the ViT are carried out on several image classification benchmarks. Without any pre-training process, the ViR outperforms the ViT in terms of both model and computational complexity. Specifically, the number of parameters of the ViR is about 15% even 5% of the ViT, and the memory footprint is about 20% to 40% of the ViT. The superiority of the ViR performance is explained by Small-World characteristics, Lyapunov exponents, and memory capacity.



### PRIME: A few primitives can boost robustness to common corruptions
- **Arxiv ID**: http://arxiv.org/abs/2112.13547v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.13547v2)
- **Published**: 2021-12-27 07:17:51+00:00
- **Updated**: 2022-03-13 09:24:45+00:00
- **Authors**: Apostolos Modas, Rahul Rade, Guillermo Ortiz-Jiménez, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard
- **Comment**: Code available at: https://github.com/amodas/PRIME-augmentations
- **Journal**: European Conference on Computer Vision (ECCV) 2022
- **Summary**: Despite their impressive performance on image classification tasks, deep networks have a hard time generalizing to unforeseen corruptions of their data. To fix this vulnerability, prior works have built complex data augmentation strategies, combining multiple methods to enrich the training data. However, introducing intricate design choices or heuristics makes it hard to understand which elements of these methods are indeed crucial for improving robustness. In this work, we take a step back and follow a principled approach to achieve robustness to common corruptions. We propose PRIME, a general data augmentation scheme that relies on simple yet rich families of max-entropy image transformations. PRIME outperforms the prior art in terms of corruption robustness, while its simplicity and plug-and-play nature enable combination with other methods to further boost their robustness. We analyze PRIME to shed light on the importance of the mixing strategy on synthesizing corrupted images, and to reveal the robustness-accuracy trade-offs arising in the context of common corruptions. Finally, we show that the computational efficiency of our method allows it to be easily used in both on-line and off-line data augmentation schemes.



### Responsive Listening Head Generation: A Benchmark Dataset and Baseline
- **Arxiv ID**: http://arxiv.org/abs/2112.13548v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13548v3)
- **Published**: 2021-12-27 07:18:50+00:00
- **Updated**: 2022-07-20 10:54:23+00:00
- **Authors**: Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun Zhao, Tao Mei
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: We present a new listening head generation benchmark, for synthesizing responsive feedbacks of a listener (e.g., nod, smile) during a face-to-face conversation. As the indispensable complement to talking heads generation, listening head generation has seldomly been studied in literature. Automatically synthesizing listening behavior that actively responds to a talking head, is critical to applications such as digital human, virtual agents and social robots. In this work, we propose a novel dataset "ViCo", highlighting the listening head generation during a face-to-face conversation. A total number of 92 identities (67 speakers and 76 listeners) are involved in ViCo, featuring 483 clips in a paired "speaking-listening" pattern, where listeners show three listening styles based on their attitudes: positive, neutral, negative. Different from traditional speech-to-gesture or talking-head generation, listening head generation takes as input both the audio and visual signals from the speaker, and gives non-verbal feedbacks (e.g., head motions, facial expressions) in a real-time manner. Our dataset supports a wide range of applications such as human-to-human interaction, video-to-video translation, cross-modal understanding and generation. To encourage further research, we also release a listening head generation baseline, conditioning on different listening attitudes. Code & ViCo dataset: https://project.mhzhou.com/vico.



### Learning Robust and Lightweight Model through Separable Structured Transformations
- **Arxiv ID**: http://arxiv.org/abs/2112.13551v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13551v2)
- **Published**: 2021-12-27 07:25:26+00:00
- **Updated**: 2021-12-29 02:25:38+00:00
- **Authors**: Xian Wei, Yanhui Huang, Yangyu Xu, Mingsong Chen, Hai Lan, Yuanxiang Li, Zhongfeng Wang, Xuan Tang
- **Comment**: 18 pages, 5figures
- **Journal**: None
- **Summary**: With the proliferation of mobile devices and the Internet of Things, deep learning models are increasingly deployed on devices with limited computing resources and memory, and are exposed to the threat of adversarial noise. Learning deep models with both lightweight and robustness is necessary for these equipments. However, current deep learning solutions are difficult to learn a model that possesses these two properties without degrading one or the other. As is well known, the fully-connected layers contribute most of the parameters of convolutional neural networks. We perform a separable structural transformation of the fully-connected layer to reduce the parameters, where the large-scale weight matrix of the fully-connected layer is decoupled by the tensor product of several separable small-sized matrices. Note that data, such as images, no longer need to be flattened before being fed to the fully-connected layer, retaining the valuable spatial geometric information of the data. Moreover, in order to further enhance both lightweight and robustness, we propose a joint constraint of sparsity and differentiable condition number, which is imposed on these separable matrices. We evaluate the proposed approach on MLP, VGG-16 and Vision Transformer. The experimental results on datasets such as ImageNet, SVHN, CIFAR-100 and CIFAR10 show that we successfully reduce the amount of network parameters by 90%, while the robust accuracy loss is less than 1.5%, which is better than the SOTA methods based on the original fully-connected layer. Interestingly, it can achieve an overwhelming advantage even at a high compression rate, e.g., 200 times.



### Classification of Histopathology Images of Lung Cancer Using Convolutional Neural Network (CNN)
- **Arxiv ID**: http://arxiv.org/abs/2112.13553v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.13553v1)
- **Published**: 2021-12-27 07:43:58+00:00
- **Updated**: 2021-12-27 07:43:58+00:00
- **Authors**: Neha Baranwal, Preethi Doravari, Renu Kachhoria
- **Comment**: None
- **Journal**: None
- **Summary**: Cancer is the uncontrollable cell division of abnormal cells inside the human body, which can spread to other body organs. It is one of the non-communicable diseases (NCDs) and NCDs accounts for 71% of total deaths worldwide whereas lung cancer is the second most diagnosed cancer after female breast cancer. Cancer survival rate of lung cancer is only 19%. There are various methods for the diagnosis of lung cancer, such as X-ray, CT scan, PET-CT scan, bronchoscopy and biopsy. However, to know the subtype of lung cancer based on the tissue type H and E staining is widely used, where the staining is done on the tissue aspirated from a biopsy. Studies have reported that the type of histology is associated with prognosis and treatment in lung cancer. Therefore, early and accurate detection of lung cancer histology is an urgent need and as its treatment is dependent on the type of histology, molecular profile and stage of the disease, it is most essential to analyse the histopathology images of lung cancer. Hence, to speed up the vital process of diagnosis of lung cancer and reduce the burden on pathologists, Deep learning techniques are used. These techniques have shown improved efficacy in the analysis of histopathology slides of cancer. Several studies reported the importance of convolution neural networks (CNN) in the classification of histopathological pictures of various cancer types such as brain, skin, breast, lung, colorectal cancer. In this study tri-category classification of lung cancer images (normal, adenocarcinoma and squamous cell carcinoma) are carried out by using ResNet 50, VGG-19, Inception_ResNet_V2 and DenseNet for the feature extraction and triplet loss to guide the CNN such that it increases inter-cluster distance and reduces intra-cluster distance.



### MHATC: Autism Spectrum Disorder identification utilizing multi-head attention encoder along with temporal consolidation modules
- **Arxiv ID**: http://arxiv.org/abs/2201.00404v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.00404v1)
- **Published**: 2021-12-27 07:50:16+00:00
- **Updated**: 2021-12-27 07:50:16+00:00
- **Authors**: Ranjeet Ranjan Jha, Abhishek Bhardwaj, Devin Garg, Arnav Bhavsar, Aditya Nigam
- **Comment**: None
- **Journal**: None
- **Summary**: Resting-state fMRI is commonly used for diagnosing Autism Spectrum Disorder (ASD) by using network-based functional connectivity. It has been shown that ASD is associated with brain regions and their inter-connections. However, discriminating based on connectivity patterns among imaging data of the control population and that of ASD patients' brains is a non-trivial task. In order to tackle said classification task, we propose a novel deep learning architecture (MHATC) consisting of multi-head attention and temporal consolidation modules for classifying an individual as a patient of ASD. The devised architecture results from an in-depth analysis of the limitations of current deep neural network solutions for similar applications. Our approach is not only robust but computationally efficient, which can allow its adoption in a variety of other research and clinical settings.



### Algorithm for recognizing the contour of a honeycomb block
- **Arxiv ID**: http://arxiv.org/abs/2112.13846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.13846v1)
- **Published**: 2021-12-27 07:59:07+00:00
- **Updated**: 2021-12-27 07:59:07+00:00
- **Authors**: Maksim Viktorovich Kubrikov, Mikhail Vladimirovich Saramud, Ivan Alekseevich Paulin, Evgeniy Petrovich Talay
- **Comment**: 11 pages, in Russian, 13 figures, ICMTMTE
- **Journal**: None
- **Summary**: The article discusses an algorithm for recognizing the contour of fragments of a honeycomb block. The inapplicability of ready-made functions of the OpenCV library is shown. Two proposed algorithms are considered. The direct scanning algorithm finds the extreme white pixels in the binarized image, it works adequately on convex shapes of products, but does not find a contour on concave areas and in cavities of products. To solve this problem, a scanning algorithm using a sliding matrix is proposed, which works correctly on products of any shape.



### DAM-AL: Dilated Attention Mechanism with Attention Loss for 3D Infant Brain Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.13559v1
- **DOI**: 10.1145/3477314.3507112
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.13559v1)
- **Published**: 2021-12-27 08:05:59+00:00
- **Updated**: 2021-12-27 08:05:59+00:00
- **Authors**: Dinh-Hieu Hoang, Gia-Han Diep, Minh-Triet Tran, Ngan T. H Le
- **Comment**: None
- **Journal**: None
- **Summary**: While Magnetic Resonance Imaging (MRI) has played an essential role in infant brain analysis, segmenting MRI into a number of tissues such as gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF) is crucial and complex due to the extremely low intensity contrast between tissues at around 6-9 months of age as well as amplified noise, myelination, and incomplete volume. In this paper, we tackle those limitations by developing a new deep learning model, named DAM-AL, which contains two main contributions, i.e., dilated attention mechanism and hard-case attention loss. Our DAM-AL network is designed with skip block layers and atrous block convolution. It contains both channel-wise attention at high-level context features and spatial attention at low-level spatial structural features. Our attention loss consists of two terms corresponding to region information and hard samples attention. Our proposed DAM-AL has been evaluated on the infant brain iSeg 2017 dataset and the experiments have been conducted on both validation and testing sets. We have benchmarked DAM-AL on Dice coefficient and ASD metrics and compared it with state-of-the-art methods.



### Hard Example Guided Hashing for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2112.13565v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13565v1)
- **Published**: 2021-12-27 08:24:10+00:00
- **Updated**: 2021-12-27 08:24:10+00:00
- **Authors**: Hai Su, Meiyin Han, Junle Liang, Jun Liang, Songsen Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Compared with the traditional hashing methods, deep hashing methods generate hash codes with rich semantic information and greatly improves the performances in the image retrieval field. However, it is unsatisfied for current deep hashing methods to predict the similarity of hard examples. It exists two main factors affecting the ability of learning hard examples, which are weak key features extraction and the shortage of hard examples. In this paper, we give a novel end-to-end model to extract the key feature from hard examples and obtain hash code with the accurate semantic information. In addition, we redesign a hard pair-wise loss function to assess the hard degree and update penalty weights of examples. It effectively alleviates the shortage problem in hard examples. Experimental results on CIFAR-10 and NUS-WIDE demonstrate that our model outperformances the mainstream hashing-based image retrieval methods.



### Vegetation Stratum Occupancy Prediction from Airborne LiDAR 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2112.13583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13583v1)
- **Published**: 2021-12-27 09:33:08+00:00
- **Updated**: 2021-12-27 09:33:08+00:00
- **Authors**: Ekaterina Kalinicheva, Loic Landrieu, Clément Mallet, Nesrine Chehata
- **Comment**: None
- **Journal**: SilviLaser 2021 Conference
- **Summary**: We propose a new deep learning-based method for estimating the occupancy of vegetation strata from 3D point clouds captured from an aerial platform. Our model predicts rasterized occupancy maps for three vegetation strata: lower, medium, and higher strata. Our training scheme allows our network to only being supervized with values aggregated over cylindrical plots, which are easier to produce than pixel-wise or point-wise annotations. Our method outperforms handcrafted and deep learning baselines in terms of precision while simultaneously providing visual and interpretable predictions. We provide an open-source implementation of our method along along a dataset of 199 agricultural plots to train and evaluate occupancy regression algorithms.



### Multimodal Image Synthesis and Editing: The Generative AI Era
- **Arxiv ID**: http://arxiv.org/abs/2112.13592v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13592v6)
- **Published**: 2021-12-27 10:00:16+00:00
- **Updated**: 2023-08-24 16:17:21+00:00
- **Authors**: Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Shijian Lu, Lingjie Liu, Adam Kortylewski, Christian Theobalt, Eric Xing
- **Comment**: TPAMI 2023
- **Journal**: None
- **Summary**: As information exists in various modalities in real world, effective interaction and fusion among multimodal information plays a key role for the creation and perception of multimodal data in computer vision and deep learning research. With superb power in modeling the interaction among multimodal information, multimodal image synthesis and editing has become a hot research topic in recent years. Instead of providing explicit guidance for network training, multimodal guidance offers intuitive and flexible means for image synthesis and editing. On the other hand, this field is also facing several challenges in alignment of multimodal features, synthesis of high-resolution images, faithful evaluation metrics, etc. In this survey, we comprehensively contextualize the advance of the recent multimodal image synthesis and editing and formulate taxonomies according to data modalities and model types. We start with an introduction to different guidance modalities in image synthesis and editing, and then describe multimodal image synthesis and editing approaches extensively according to their model types. After that, we describe benchmark datasets and evaluation metrics as well as corresponding experimental results. Finally, we provide insights about the current research challenges and possible directions for future research. A project associated with this survey is available at https://github.com/fnzhan/Generative-AI.



### Depth estimation of endoscopy using sim-to-real transfer
- **Arxiv ID**: http://arxiv.org/abs/2112.13595v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T05, 62P10, I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2112.13595v1)
- **Published**: 2021-12-27 10:05:01+00:00
- **Updated**: 2021-12-27 10:05:01+00:00
- **Authors**: Bong Hyuk Jeong, Hang Keun Kim, Young Don Son
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: In order to use the navigation system effectively, distance information sensors such as depth sensors are essential. Since depth sensors are difficult to use in endoscopy, many groups propose a method using convolutional neural networks. In this paper, the ground truth of the depth image and the endoscopy image is generated through endoscopy simulation using the colon model segmented by CT colonography. Photo-realistic simulation images can be created using a sim-to-real approach using cycleGAN for endoscopy images. By training the generated dataset, we propose a quantitative endoscopy depth estimation network. The proposed method represents a better-evaluated score than the existing unsupervised training-based results.



### An Empirical Study of Adder Neural Networks for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2112.13608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13608v1)
- **Published**: 2021-12-27 11:03:13+00:00
- **Updated**: 2021-12-27 11:03:13+00:00
- **Authors**: Xinghao Chen, Chang Xu, Minjing Dong, Chunjing Xu, Yunhe Wang
- **Comment**: None
- **Journal**: NeurIPS 2021
- **Summary**: Adder neural networks (AdderNets) have shown impressive performance on image classification with only addition operations, which are more energy efficient than traditional convolutional neural networks built with multiplications. Compared with classification, there is a strong demand on reducing the energy consumption of modern object detectors via AdderNets for real-world applications such as autonomous driving and face detection. In this paper, we present an empirical study of AdderNets for object detection. We first reveal that the batch normalization statistics in the pre-trained adder backbone should not be frozen, since the relatively large feature variance of AdderNets. Moreover, we insert more shortcut connections in the neck part and design a new feature fusion architecture for avoiding the sparse features of adder layers. We present extensive ablation studies to explore several design choices of adder detectors. Comparisons with state-of-the-arts are conducted on COCO and PASCAL VOC benchmarks. Specifically, the proposed Adder FCOS achieves a 37.8\% AP on the COCO val set, demonstrating comparable performance to that of the convolutional counterpart with an about $1.4\times$ energy reduction.



### Generation of Synthetic Rat Brain MRI scans with a 3D Enhanced Alpha-GAN
- **Arxiv ID**: http://arxiv.org/abs/2112.13626v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2112.13626v3)
- **Published**: 2021-12-27 12:06:06+00:00
- **Updated**: 2022-01-04 10:27:19+00:00
- **Authors**: André Ferreira, Ricardo Magalhães, Sébastien Mériaux, Victor Alves
- **Comment**: 25 pages, 10 figures, 4 tables
- **Journal**: None
- **Summary**: Translational brain research using Magnetic Resonance Imaging (MRI) is becoming increasingly popular as animal models are an essential part of scientific studies and more ultra-high-field scanners are becoming available. Some disadvantages of MRI are the availability of MRI scanners and the time required for a full scanning session (it usually takes over 30 minutes). Privacy laws and the 3Rs ethics rule also make it difficult to create large datasets for training deep learning models. Generative Adversarial Networks (GANs) can perform data augmentation with higher quality than other techniques. In this work, the alpha-GAN architecture is used to test its ability to produce realistic 3D MRI scans of the rat brain. As far as the authors are aware, this is the first time that a GAN-based approach has been used for data augmentation in preclinical data. The generated scans are evaluated using various qualitative and quantitative metrics. A Turing test conducted by 4 experts has shown that the generated scans can trick almost any expert. The generated scans were also used to evaluate their impact on the performance of an existing deep learning model developed for segmenting the rat brain into white matter, grey matter and cerebrospinal fluid. The models were compared using the Dice score. The best results for whole brain and white matter segmentation were obtained when 174 real scans and 348 synthetic scans were used, with improvements of 0.0172 and 0.0129, respectively. Using 174 real scans and 87 synthetic scans resulted in improvements of 0.0038 and 0.0764 for grey matter and CSF segmentation, respectively. Thus, by using the proposed new normalisation layer and loss functions, it was possible to improve the realism of the generated rat MRI scans and it was shown that using the generated data improved the segmentation model more than using the conventional data augmentation.



### AdaptivePose: Human Parts as Adaptive Points
- **Arxiv ID**: http://arxiv.org/abs/2112.13635v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13635v1)
- **Published**: 2021-12-27 12:46:12+00:00
- **Updated**: 2021-12-27 12:46:12+00:00
- **Authors**: Yabo Xiao, Xiaojuan Wang, Dongdong Yu, Guoli Wang, Qian Zhang, Mingshu He
- **Comment**: Accepted by AAAI 2022. Code Will be released after the extention
- **Journal**: None
- **Summary**: Multi-person pose estimation methods generally follow top-down and bottom-up paradigms, both of which can be considered as two-stage approaches thus leading to the high computation cost and low efficiency. Towards a compact and efficient pipeline for multi-person pose estimation task, in this paper, we propose to represent the human parts as points and present a novel body representation, which leverages an adaptive point set including the human center and seven human-part related points to represent the human instance in a more fine-grained manner. The novel representation is more capable of capturing the various pose deformation and adaptively factorizes the long-range center-to-joint displacement thus delivers a single-stage differentiable network to more precisely regress multi-person pose, termed as AdaptivePose. For inference, our proposed network eliminates the grouping as well as refinements and only needs a single-step disentangling process to form multi-person pose. Without any bells and whistles, we achieve the best speed-accuracy trade-offs of 67.4% AP / 29.4 fps with DLA-34 and 71.3% AP / 9.1 fps with HRNet-W48 on COCO test-dev dataset.



### Self-normalized Classification of Parkinson's Disease DaTscan Images
- **Arxiv ID**: http://arxiv.org/abs/2112.13637v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.13637v1)
- **Published**: 2021-12-27 12:47:15+00:00
- **Updated**: 2021-12-27 12:47:15+00:00
- **Authors**: Yuan Zhou, Hemant D. Tagare
- **Comment**: To appear in IEEE BIBM 2021
- **Journal**: None
- **Summary**: Classifying SPECT images requires a preprocessing step which normalizes the images using a normalization region. The choice of the normalization region is not standard, and using different normalization regions introduces normalization region-dependent variability. This paper mathematically analyzes the effect of the normalization region to show that normalized-classification is exactly equivalent to a subspace separation of the half rays of the images under multiplicative equivalence. Using this geometry, a new self-normalized classification strategy is proposed. This strategy eliminates the normalizing region altogether. The theory is used to classify DaTscan images of 365 Parkinson's disease (PD) subjects and 208 healthy control (HC) subjects from the Parkinson's Progression Marker Initiative (PPMI). The theory is also used to understand PD progression from baseline to year 4.



### Augmenting Convolutional networks with attention-based aggregation
- **Arxiv ID**: http://arxiv.org/abs/2112.13692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13692v1)
- **Published**: 2021-12-27 14:05:41+00:00
- **Updated**: 2021-12-27 14:05:41+00:00
- **Authors**: Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Piotr Bojanowski, Armand Joulin, Gabriel Synnaeve, Hervé Jégou
- **Comment**: None
- **Journal**: None
- **Summary**: We show how to augment any convolutional network with an attention-based global map to achieve non-local reasoning. We replace the final average pooling by an attention-based aggregation layer akin to a single transformer block, that weights how the patches are involved in the classification decision. We plug this learned aggregation layer with a simplistic patch-based convolutional network parametrized by 2 parameters (width and depth). In contrast with a pyramidal design, this architecture family maintains the input patch resolution across all the layers. It yields surprisingly competitive trade-offs between accuracy and complexity, in particular in terms of memory consumption, as shown by our experiments on various computer vision tasks: object classification, image segmentation and detection.



### Using maps to predict economic activity
- **Arxiv ID**: http://arxiv.org/abs/2112.13850v2
- **DOI**: None
- **Categories**: **econ.GN**, cs.CV, cs.LG, q-fin.EC
- **Links**: [PDF](http://arxiv.org/pdf/2112.13850v2)
- **Published**: 2021-12-27 14:13:20+00:00
- **Updated**: 2022-04-01 14:27:58+00:00
- **Authors**: Imryoung Jeong, Hyunjoo Yang
- **Comment**: 24 pages including references and appendix, 9 figures, 1 table
- **Journal**: None
- **Summary**: We introduce a novel machine learning approach to leverage historical and contemporary maps and systematically predict economic statistics. Our simple algorithm extracts meaningful features from the maps based on their color compositions for predictions. We apply our method to grid-level population levels in Sub-Saharan Africa in the 1950s and South Korea in 1930, 1970, and 2015. Our results show that maps can reliably predict population density in the mid-20th century Sub-Saharan Africa using 9,886 map grids (5km by 5 km). Similarly, contemporary South Korean maps can generate robust predictions on income, consumption, employment, population density, and electric consumption. In addition, our method is capable of predicting historical South Korean population growth over a century.



### Weakly Supervised Visual-Auditory Fixation Prediction with Multigranularity Perception
- **Arxiv ID**: http://arxiv.org/abs/2112.13697v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13697v4)
- **Published**: 2021-12-27 14:13:30+00:00
- **Updated**: 2022-07-29 02:54:42+00:00
- **Authors**: Guotao Wang, Chenglizhao Chen, Deng-Ping Fan, Aimin Hao, Hong Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Thanks to the rapid advances in deep learning techniques and the wide availability of large-scale training sets, the performance of video saliency detection models has been improving steadily and significantly. However, deep learning-based visualaudio fixation prediction is still in its infancy. At present, only a few visual-audio sequences have been furnished, with real fixations being recorded in real visual-audio environments. Hence, it would be neither efficient nor necessary to recollect real fixations under the same visual-audio circumstances. To address this problem, this paper promotes a novel approach in a weakly supervised manner to alleviate the demand of large-scale training sets for visual-audio model training. By using only the video category tags, we propose the selective class activation mapping (SCAM) and its upgrade (SCAM+). In the spatial-temporal-audio circumstance, the former follows a coarse-to-fine strategy to select the most discriminative regions, and these regions are usually capable of exhibiting high consistency with the real human-eye fixations. The latter equips the SCAM with an additional multi-granularity perception mechanism, making the whole process more consistent with that of the real human visual system. Moreover, we distill knowledge from these regions to obtain complete new spatial-temporal-audio (STA) fixation prediction (FP) networks, enabling broad applications in cases where video tags are not available. Without resorting to any real human-eye fixation, the performances of these STA FP networks are comparable to those of fully supervised networks. The code and results are publicly available at https://github.com/guotaowang/STANet.



### Multi-Image Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2112.13706v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.13706v2)
- **Published**: 2021-12-27 14:28:04+00:00
- **Updated**: 2022-02-06 10:20:16+00:00
- **Authors**: Harsh Raj, Janhavi Dadhania, Akhilesh Bhardwaj, Prabuchandran KJ
- **Comment**: None
- **Journal**: None
- **Summary**: While a lot of work has been done on developing models to tackle the problem of Visual Question Answering, the ability of these models to relate the question to the image features still remain less explored. We present an empirical study of different feature extraction methods with different loss functions. We propose New dataset for the task of Visual Question Answering with multiple image inputs having only one ground truth, and benchmark our results on them. Our final model utilising Resnet + RCNN image features and Bert embeddings, inspired from stacked attention network gives 39% word accuracy and 99% image accuracy on CLEVER+TinyImagenet dataset.



### Visual Place Representation and Recognition from Depth Images
- **Arxiv ID**: http://arxiv.org/abs/2112.13707v1
- **DOI**: 10.1016/j.ijleo.2022.169109
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13707v1)
- **Published**: 2021-12-27 14:31:24+00:00
- **Updated**: 2021-12-27 14:31:24+00:00
- **Authors**: Farah Ibelaiden, Slimane Larabi
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposes a new method for place recognition based on the scene architecture. From depth video, we compute the 3D model and we derive and describe geometrically the 2D map from which the scene descriptor is deduced to constitute the core of the proposed algorithm. The obtained results show the efficiency and the robustness of the propounded descriptor to scene appearance changes and light variations.



### Rethinking the Data Annotation Process for Multi-view 3D Pose Estimation with Active Learning and Self-Training
- **Arxiv ID**: http://arxiv.org/abs/2112.13709v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13709v2)
- **Published**: 2021-12-27 14:34:25+00:00
- **Updated**: 2023-01-17 21:47:17+00:00
- **Authors**: Qi Feng, Kun He, He Wen, Cem Keskin, Yuting Ye
- **Comment**: IEEE WACV 2023 algorithms track. Code:
  https://github.com/facebookresearch/multi_view_active_learning
- **Journal**: None
- **Summary**: Pose estimation of the human body and hands is a fundamental problem in computer vision, and learning-based solutions require a large amount of annotated data. In this work, we improve the efficiency of the data annotation process for 3D pose estimation problems with Active Learning (AL) in a multi-view setting. AL selects examples with the highest value to annotate under limited annotation budgets (time and cost), but choosing the selection strategy is often nontrivial. We present a framework to efficiently extend existing single-view AL strategies. We then propose two novel AL strategies that make full use of multi-view geometry. Moreover, we demonstrate additional performance gains by incorporating pseudo-labels computed during the AL process, which is a form of self-training. Our system significantly outperforms simulated annotation baselines in 3D body and hand pose estimation on two large-scale benchmarks: CMU Panoptic Studio and InterHand2.6M. Notably, on CMU Panoptic Studio, we are able to reduce the turn-around time by 60% and annotation cost by 80% when compared to the conventional annotation process.



### SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos
- **Arxiv ID**: http://arxiv.org/abs/2112.13715v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13715v2)
- **Published**: 2021-12-27 14:53:30+00:00
- **Updated**: 2022-07-21 17:15:06+00:00
- **Authors**: Ailing Zeng, Lei Yang, Xuan Ju, Jiefeng Li, Jianyi Wang, Qiang Xu
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: When analyzing human motion videos, the output jitters from existing pose estimators are highly-unbalanced with varied estimation errors across frames. Most frames in a video are relatively easy to estimate and only suffer from slight jitters. In contrast, for rarely seen or occluded actions, the estimated positions of multiple joints largely deviate from the ground truth values for a consecutive sequence of frames, rendering significant jitters on them. To tackle this problem, we propose to attach a dedicated temporal-only refinement network to existing pose estimators for jitter mitigation, named SmoothNet. Unlike existing learning-based solutions that employ spatio-temporal models to co-optimize per-frame precision and temporal smoothness at all the joints, SmoothNet models the natural smoothness characteristics in body movements by learning the long-range temporal relations of every joint without considering the noisy correlations among joints. With a simple yet effective motion-aware fully-connected network, SmoothNet improves the temporal smoothness of existing pose estimators significantly and enhances the estimation accuracy of those challenging frames as a side-effect. Moreover, as a temporal-only model, a unique advantage of SmoothNet is its strong transferability across various types of estimators and datasets. Comprehensive experiments on five datasets with eleven popular backbone networks across 2D and 3D pose estimation and body recovery tasks demonstrate the efficacy of the proposed solution. Code is available at https://github.com/cure-lab/SmoothNet.



### A Multi-channel Training Method Boost the Performance
- **Arxiv ID**: http://arxiv.org/abs/2112.13727v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.13727v1)
- **Published**: 2021-12-27 15:18:16+00:00
- **Updated**: 2021-12-27 15:18:16+00:00
- **Authors**: Yingdong Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural network has made huge revolution and shown its superior performance on computer vision tasks such as classification and segmentation. Recent years, researches devote much effort to scaling down size of network while maintaining its ability, to adapt to the limited memory on embedded systems like mobile phone. In this paper, we propose a multi-channel training procedure which can highly facilitate the performance and robust of the target network. The proposed procedure contains two sets of networks and two information pipelines which can work independently hinge on the computation ability of the embedded platform, while in the mean time, the classification accuracy is also admirably enhanced.



### Multi-Domain Balanced Sampling Improves Out-of-Distribution Generalization of Chest X-ray Pathology Prediction Models
- **Arxiv ID**: http://arxiv.org/abs/2112.13734v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13734v2)
- **Published**: 2021-12-27 15:28:01+00:00
- **Updated**: 2021-12-28 02:36:40+00:00
- **Authors**: Enoch Tetteh, Joseph Viviano, Yoshua Bengio, David Krueger, Joseph Paul Cohen
- **Comment**: MED-NEURIPS 2021
- **Journal**: None
- **Summary**: Learning models that generalize under different distribution shifts in medical imaging has been a long-standing research challenge. There have been several proposals for efficient and robust visual representation learning among vision research practitioners, especially in the sensitive and critical biomedical domain. In this paper, we propose an idea for out-of-distribution generalization of chest X-ray pathologies that uses a simple balanced batch sampling technique. We observed that balanced sampling between the multiple training datasets improves the performance over baseline models trained without balancing.



### MSeg: A Composite Dataset for Multi-domain Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.13762v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13762v1)
- **Published**: 2021-12-27 16:16:35+00:00
- **Updated**: 2021-12-27 16:16:35+00:00
- **Authors**: John Lambert, Zhuang Liu, Ozan Sener, James Hays, Vladlen Koltun
- **Comment**: None
- **Journal**: None
- **Summary**: We present MSeg, a composite dataset that unifies semantic segmentation datasets from different domains. A naive merge of the constituent datasets yields poor performance due to inconsistent taxonomies and annotation practices. We reconcile the taxonomies and bring the pixel-level annotations into alignment by relabeling more than 220,000 object masks in more than 80,000 images, requiring more than 1.34 years of collective annotator effort. The resulting composite dataset enables training a single semantic segmentation model that functions effectively across domains and generalizes to datasets that were not seen during training. We adopt zero-shot cross-dataset transfer as a benchmark to systematically evaluate a model's robustness and show that MSeg training yields substantially more robust models in comparison to training on individual datasets or naive mixing of datasets without the presented contributions. A model trained on MSeg ranks first on the WildDash-v1 leaderboard for robust semantic segmentation, with no exposure to WildDash data during training. We evaluate our models in the 2020 Robust Vision Challenge (RVC) as an extreme generalization experiment. MSeg training sets include only three of the seven datasets in the RVC; more importantly, the evaluation taxonomy of RVC is different and more detailed. Surprisingly, our model shows competitive performance and ranks second. To evaluate how close we are to the grand aim of robust, efficient, and complete scene understanding, we go beyond semantic segmentation by training instance segmentation and panoptic segmentation models using our dataset. Moreover, we also evaluate various engineering design decisions and metrics, including resolution and computational efficiency. Although our models are far from this grand aim, our comprehensive evaluation is crucial for progress. We share all the models and code with the community.



### Improving Deep Image Matting via Local Smoothness Assumption
- **Arxiv ID**: http://arxiv.org/abs/2112.13809v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13809v2)
- **Published**: 2021-12-27 17:54:10+00:00
- **Updated**: 2022-04-05 20:02:11+00:00
- **Authors**: Rui Wang, Jun Xie, Jiacheng Han, Dezhen Qi
- **Comment**: 9 pages, accepted by IEEE ICME 2022
- **Journal**: None
- **Summary**: Natural image matting is a fundamental and challenging computer vision task. Conventionally, the problem is formulated as an underconstrained problem. Since the problem is ill-posed, further assumptions on the data distribution are required to make the problem well-posed. For classical matting methods, a commonly adopted assumption is the local smoothness assumption on foreground and background colors. However, the use of such assumptions was not systematically considered for deep learning based matting methods. In this work, we consider two local smoothness assumptions which can help improving deep image matting models. Based on the local smoothness assumptions, we propose three techniques, i.e., training set refinement, color augmentation and backpropagating refinement, which can improve the performance of the deep image matting model significantly. We conduct experiments to examine the effectiveness of the proposed algorithm. The experimental results show that the proposed method has favorable performance compared with existing matting methods.



### Infant Brain Age Classification: 2D CNN Outperforms 3D CNN in Small Dataset
- **Arxiv ID**: http://arxiv.org/abs/2112.13811v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.13811v1)
- **Published**: 2021-12-27 18:02:48+00:00
- **Updated**: 2021-12-27 18:02:48+00:00
- **Authors**: Mahdieh Shabanian, Markus Wenzel, John P. DeVincenzo
- **Comment**: 8 pages, 5 figures, 3 tables. arXiv admin note: text overlap with
  arXiv:2010.03963
- **Journal**: SPIE 2022 Medical Imaging Conference
- **Summary**: Determining if the brain is developing normally is a key component of pediatric neuroradiology and neurology. Brain magnetic resonance imaging (MRI) of infants demonstrates a specific pattern of development beyond simply myelination. While radiologists have used myelination patterns, brain morphology and size characteristics to determine age-adequate brain maturity, this requires years of experience in pediatric neuroradiology. With no standardized criteria, visual estimation of the structural maturity of the brain from MRI before three years of age remains dominated by inter-observer and intra-observer variability. A more objective estimation of brain developmental age could help physicians identify many neurodevelopmental conditions and diseases earlier and more reliably. Such data, however, is naturally hard to obtain, and the observer ground truth not much of a gold standard due to subjectivity of assessment. In this light, we explore the general feasibility to tackle this task, and the utility of different approaches, including two- and three-dimensional convolutional neural networks (CNN) that were trained on a fusion of T1-weighted, T2-weighted, and proton density (PD) weighted sequences from 84 individual subjects divided into four age groups from birth to 3 years of age. In the best performing approach, we achieved an accuracy of 0.90 [95% CI:0.86-0.94] using a 2D CNN on a central axial thick slab. We discuss the comparison to 3D networks and show how the performance compares to the use of only one sequence (T1w). In conclusion, despite the theoretical superiority of 3D CNN approaches, in limited-data situations, such approaches are inferior to simpler architectures. The code can be found in https://github.com/shabanian2018/Age_MRI-Classification



### Temporally Constrained Neural Networks (TCNN): A framework for semi-supervised video semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.13815v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2112.13815v1)
- **Published**: 2021-12-27 18:06:12+00:00
- **Updated**: 2021-12-27 18:06:12+00:00
- **Authors**: Deepak Alapatt, Pietro Mascagni, Armine Vardazaryan, Alain Garcia, Nariaki Okamoto, Didier Mutter, Jacques Marescaux, Guido Costamagna, Bernard Dallemagne, Nicolas Padoy
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: A major obstacle to building models for effective semantic segmentation, and particularly video semantic segmentation, is a lack of large and well annotated datasets. This bottleneck is particularly prohibitive in highly specialized and regulated fields such as medicine and surgery, where video semantic segmentation could have important applications but data and expert annotations are scarce. In these settings, temporal clues and anatomical constraints could be leveraged during training to improve performance. Here, we present Temporally Constrained Neural Networks (TCNN), a semi-supervised framework used for video semantic segmentation of surgical videos. In this work, we show that autoencoder networks can be used to efficiently provide both spatial and temporal supervisory signals to train deep learning models. We test our method on a newly introduced video dataset of laparoscopic cholecystectomy procedures, Endoscapes, and an adaptation of a public dataset of cataract surgeries, CaDIS. We demonstrate that lower-dimensional representations of predicted masks can be leveraged to provide a consistent improvement on both sparsely labeled datasets with no additional computational cost at inference time. Further, the TCNN framework is model-agnostic and can be used in conjunction with other model design choices with minimal additional complexity.



### Astronomical Image Colorization and upscaling with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2112.13865v1
- **DOI**: None
- **Categories**: **eess.IV**, astro-ph.IM, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.13865v1)
- **Published**: 2021-12-27 19:01:20+00:00
- **Updated**: 2021-12-27 19:01:20+00:00
- **Authors**: Shreyas Kalvankar, Hrushikesh Pandit, Pranav Parwate, Atharva Patil, Snehal Kamalapur
- **Comment**: 14 pages, 10 figures, 7 tables
- **Journal**: None
- **Summary**: Automatic colorization of images without human intervention has been a subject of interest in the machine learning community for a brief period of time. Assigning color to an image is a highly ill-posed problem because of its innate nature of possessing very high degrees of freedom; given an image, there is often no single color-combination that is correct. Besides colorization, another problem in reconstruction of images is Single Image Super Resolution, which aims at transforming low resolution images to a higher resolution. This research aims to provide an automated approach for the problem by focusing on a very specific domain of images, namely astronomical images, and process them using Generative Adversarial Networks (GANs). We explore the usage of various models in two different color spaces, RGB and L*a*b. We use transferred learning owing to a small data set, using pre-trained ResNet-18 as a backbone, i.e. encoder for the U-net and fine-tune it further. The model produces visually appealing images which hallucinate high resolution, colorized data in these results which does not exist in the original image. We present our results by evaluating the GANs quantitatively using distance metrics such as L1 distance and L2 distance in each of the color spaces across all channels to provide a comparative analysis. We use Frechet inception distance (FID) to compare the distribution of the generated images with the distribution of the real image to assess the model's performance.



### A Fistful of Words: Learning Transferable Visual Models from Bag-of-Words Supervision
- **Arxiv ID**: http://arxiv.org/abs/2112.13884v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13884v2)
- **Published**: 2021-12-27 20:02:10+00:00
- **Updated**: 2022-01-06 00:55:00+00:00
- **Authors**: Ajinkya Tejankar, Maziar Sanjabi, Bichen Wu, Saining Xie, Madian Khabsa, Hamed Pirsiavash, Hamed Firooz
- **Comment**: None
- **Journal**: None
- **Summary**: Using natural language as a supervision for training visual recognition models holds great promise. Recent works have shown that if such supervision is used in the form of alignment between images and captions in large training datasets, then the resulting aligned models perform well on zero-shot classification as downstream tasks2. In this paper, we focus on teasing out what parts of the language supervision are essential for training zero-shot image classification models. Through extensive and careful experiments, we show that: 1) A simple Bag-of-Words (BoW) caption could be used as a replacement for most of the image captions in the dataset. Surprisingly, we observe that this approach improves the zero-shot classification performance when combined with word balancing. 2) Using a BoW pretrained model, we can obtain more training data by generating pseudo-BoW captions on images that do not have a caption. Models trained on images with real and pseudo-BoW captions achieve stronger zero-shot performance. On ImageNet-1k zero-shot evaluation, our best model, that uses only 3M image-caption pairs, performs on-par with a CLIP model trained on 15M image-caption pairs (31.5% vs 31.3%).



### MedShift: identifying shift data for medical dataset curation
- **Arxiv ID**: http://arxiv.org/abs/2112.13885v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.13885v1)
- **Published**: 2021-12-27 20:06:23+00:00
- **Updated**: 2021-12-27 20:06:23+00:00
- **Authors**: Xiaoyuan Guo, Judy Wawira Gichoya, Hari Trivedi, Saptarshi Purkayastha, Imon Banerjee
- **Comment**: 35 pages, 28 figures, 2 tables
- **Journal**: None
- **Summary**: To curate a high-quality dataset, identifying data variance between the internal and external sources is a fundamental and crucial step. However, methods to detect shift or variance in data have not been significantly researched. Challenges to this are the lack of effective approaches to learn dense representation of a dataset and difficulties of sharing private data across medical institutions. To overcome the problems, we propose a unified pipeline called MedShift to detect the top-level shift samples and thus facilitate the medical curation. Given an internal dataset A as the base source, we first train anomaly detectors for each class of dataset A to learn internal distributions in an unsupervised way. Second, without exchanging data across sources, we run the trained anomaly detectors on an external dataset B for each class. The data samples with high anomaly scores are identified as shift data. To quantify the shiftness of the external dataset, we cluster B's data into groups class-wise based on the obtained scores. We then train a multi-class classifier on A and measure the shiftness with the classifier's performance variance on B by gradually dropping the group with the largest anomaly score for each class. Additionally, we adapt a dataset quality metric to help inspect the distribution differences for multiple medical sources. We verify the efficacy of MedShift with musculoskeletal radiographs (MURA) and chest X-rays datasets from more than one external source. Experiments show our proposed shift data detection pipeline can be beneficial for medical centers to curate high-quality datasets more efficiently. An interface introduction video to visualize our results is available at https://youtu.be/V3BF0P1sxQE.



### Free-Viewpoint RGB-D Human Performance Capture and Rendering
- **Arxiv ID**: http://arxiv.org/abs/2112.13889v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2112.13889v4)
- **Published**: 2021-12-27 20:13:53+00:00
- **Updated**: 2022-08-02 10:58:01+00:00
- **Authors**: Phong Nguyen-Ha, Nikolaos Sarafianos, Christoph Lassner, Janne Heikkila, Tony Tung
- **Comment**: Accepted at ECCV 2022, Project page:
  https://www.phongnhhn.info/HVS_Net/index.html
- **Journal**: None
- **Summary**: Capturing and faithfully rendering photo-realistic humans from novel views is a fundamental problem for AR/VR applications. While prior work has shown impressive performance capture results in laboratory settings, it is non-trivial to achieve casual free-viewpoint human capture and rendering for unseen identities with high fidelity, especially for facial expressions, hands, and clothes. To tackle these challenges we introduce a novel view synthesis framework that generates realistic renders from unseen views of any human captured from a single-view and sparse RGB-D sensor, similar to a low-cost depth camera, and without actor-specific models. We propose an architecture to create dense feature maps in novel views obtained by sphere-based neural rendering, and create complete renders using a global context inpainting model. Additionally, an enhancer network leverages the overall fidelity, even in occluded areas from the original view, producing crisp renders with fine details. We show that our method generates high-quality novel views of synthetic and real human actors given a single-stream, sparse RGB-D input. It generalizes to unseen identities, and new poses and faithfully reconstructs facial expressions. Our approach outperforms prior view synthesis methods and is robust to different levels of depth sparsity.



### SPViT: Enabling Faster Vision Transformers via Soft Token Pruning
- **Arxiv ID**: http://arxiv.org/abs/2112.13890v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.AR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.13890v2)
- **Published**: 2021-12-27 20:15:25+00:00
- **Updated**: 2022-09-20 22:20:30+00:00
- **Authors**: Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Mengshu Sun, Wei Niu, Xuan Shen, Geng Yuan, Bin Ren, Minghai Qin, Hao Tang, Yanzhi Wang
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Recently, Vision Transformer (ViT) has continuously established new milestones in the computer vision field, while the high computation and memory cost makes its propagation in industrial production difficult. Pruning, a traditional model compression paradigm for hardware efficiency, has been widely applied in various DNN structures. Nevertheless, it stays ambiguous on how to perform exclusive pruning on the ViT structure. Considering three key points: the structural characteristics, the internal data pattern of ViTs, and the related edge device deployment, we leverage the input token sparsity and propose a computation-aware soft pruning framework, which can be set up on vanilla Transformers of both flatten and CNN-type structures, such as Pooling-based ViT (PiT). More concretely, we design a dynamic attention-based multi-head token selector, which is a lightweight module for adaptive instance-wise token selection. We further introduce a soft pruning technique, which integrates the less informative tokens generated by the selector module into a package token that will participate in subsequent calculations rather than being completely discarded. Our framework is bound to the trade-off between accuracy and computation constraints of specific edge devices through our proposed computation-aware training strategy. Experimental results show that our framework significantly reduces the computation cost of ViTs while maintaining comparable performance on image classification. Moreover, our framework can guarantee the identified model to meet resource specifications of mobile devices and FPGA, and even achieve the real-time execution of DeiT-T on mobile platforms. For example, our method reduces the latency of DeiT-T to 26 ms (26%$\sim $41% superior to existing works) on the mobile device with 0.25%$\sim $4% higher top-1 accuracy on ImageNet.



### GPU-accelerated Faster Mean Shift with euclidean distance metrics
- **Arxiv ID**: http://arxiv.org/abs/2112.13891v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.13891v1)
- **Published**: 2021-12-27 20:18:24+00:00
- **Updated**: 2021-12-27 20:18:24+00:00
- **Authors**: Le You, Han Jiang, Jinyong Hu, Chorng Chang, Lingxi Chen, Xintong Cui, Mengyang Zhao
- **Comment**: 7 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:2007.14283
- **Journal**: None
- **Summary**: Handling clustering problems are important in data statistics, pattern recognition and image processing. The mean-shift algorithm, a common unsupervised algorithms, is widely used to solve clustering problems. However, the mean-shift algorithm is restricted by its huge computational resource cost. In previous research[10], we proposed a novel GPU-accelerated Faster Mean-shift algorithm, which greatly speed up the cosine-embedding clustering problem. In this study, we extend and improve the previous algorithm to handle Euclidean distance metrics. Different from conventional GPU-based mean-shift algorithms, our algorithm adopts novel Seed Selection & Early Stopping approaches, which greatly increase computing speed and reduce GPU memory consumption. In the simulation testing, when processing a 200K points clustering problem, our algorithm achieved around 3 times speedup compared to the state-of-the-art GPU-based mean-shift algorithms with optimized GPU memory consumption. Moreover, in this study, we implemented a plug-and-play model for faster mean-shift algorithm, which can be easily deployed. (Plug-and-play model is available: https://github.com/masqm/Faster-Mean-Shift-Euc)



### Non-Reference Quality Monitoring of Digital Images using Gradient Statistics and Feedforward Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2112.13893v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 94A08, I.4.5; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2112.13893v1)
- **Published**: 2021-12-27 20:21:55+00:00
- **Updated**: 2021-12-27 20:21:55+00:00
- **Authors**: Nisar Ahmed, Hafiz Muhammad Shahzad Asif, Hassan Khalid
- **Comment**: Fifth International Conference on Aerospace Science & Engineering
  (ICASE 2017) (ICASE Proceedings, Page No. 300-305)
- **Journal**: None
- **Summary**: Digital images contain a lot of redundancies, therefore, compressions are applied to reduce the image size without the loss of reasonable image quality. The same become more prominent in the case of videos that contains image sequences and higher compression ratios are achieved in low throughput networks. Assessment of the quality of images in such scenarios becomes of particular interest. Subjective evaluation in most of the scenarios becomes infeasible so objective evaluation is preferred. Among the three objective quality measures, full-reference and reduced-reference methods require an original image in some form to calculate the quality score which is not feasible in scenarios such as broadcasting or IP video. Therefore, a non-reference quality metric is proposed to assess the quality of digital images which calculates luminance and multiscale gradient statistics along with mean subtracted contrast normalized products as features to train a Feedforward Neural Network with Scaled Conjugate Gradient. The trained network has provided good regression and R2 measures and further testing on LIVE Image Quality Assessment database release-2 has shown promising results. Pearson, Kendall, and Spearman's correlation are calculated between predicted and actual quality scores and their results are comparable to the state-of-the-art systems. Moreover, the proposed metric is computationally faster than its counterparts and can be used for the quality assessment of image sequences.



### Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?
- **Arxiv ID**: http://arxiv.org/abs/2112.13906v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.13906v1)
- **Published**: 2021-12-27 21:19:23+00:00
- **Updated**: 2021-12-27 21:19:23+00:00
- **Authors**: Sedigheh Eslami, Gerard de Melo, Christoph Meinel
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive Language--Image Pre-training (CLIP) has shown remarkable success in learning with cross-modal supervision from extensive amounts of image--text pairs collected online. Thus far, the effectiveness of CLIP has been investigated primarily in general-domain multimodal problems. This work evaluates the effectiveness of CLIP for the task of Medical Visual Question Answering (MedVQA). To this end, we present PubMedCLIP, a fine-tuned version of CLIP for the medical domain based on PubMed articles. Our experiments are conducted on two MedVQA benchmark datasets and investigate two MedVQA methods, MEVF (Mixture of Enhanced Visual Features) and QCR (Question answering via Conditional Reasoning). For each of these, we assess the merits of visual representation learning using PubMedCLIP, the original CLIP, and state-of-the-art MAML (Model-Agnostic Meta-Learning) networks pre-trained only on visual data. We open source the code for our MedVQA pipeline and pre-training PubMedCLIP. CLIP and PubMedCLIP achieve improvements in comparison to MAML's visual encoder. PubMedCLIP achieves the best results with gains in the overall accuracy of up to 3%. Individual examples illustrate the strengths of PubMedCLIP in comparison to the previously widely used MAML networks. Visual representation learning with language supervision in PubMedCLIP leads to noticeable improvements for MedVQA. Our experiments reveal distributional differences in the two MedVQA benchmark datasets that have not been imparted in previous work and cause different back-end visual encoders in PubMedCLIP to exhibit different behavior on these datasets. Moreover, we witness fundamental performance differences of VQA in general versus medical domains.



### Improving Depth Estimation using Location Information
- **Arxiv ID**: http://arxiv.org/abs/2112.13925v1
- **DOI**: 10.1109/ICCES54031.2021.9686181
- **Categories**: **cs.CV**, cs.AI, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2112.13925v1)
- **Published**: 2021-12-27 22:30:14+00:00
- **Updated**: 2021-12-27 22:30:14+00:00
- **Authors**: Ahmed Zaitoon, Hossam El Din Abd El Munim, Hazem Abbas
- **Comment**: None
- **Journal**: 2021 16th International Conference on Computer Engineering and
  Systems (ICCES)
- **Summary**: The ability to accurately estimate depth information is crucial for many autonomous applications to recognize the surrounded environment and predict the depth of important objects. One of the most recently used techniques is monocular depth estimation where the depth map is inferred from a single image. This paper improves the self-supervised deep learning techniques to perform accurate generalized monocular depth estimation. The main idea is to train the deep model to take into account a sequence of the different frames, each frame is geotagged with its location information. This makes the model able to enhance depth estimation given area semantics. We demonstrate the effectiveness of our model to improve depth estimation results. The model is trained in a realistic environment and the results show improvements in the depth map after adding the location data to the model training phase.



### SPIDER: Searching Personalized Neural Architecture for Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.13939v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.13939v1)
- **Published**: 2021-12-27 23:42:15+00:00
- **Updated**: 2021-12-27 23:42:15+00:00
- **Authors**: Erum Mushtaq, Chaoyang He, Jie Ding, Salman Avestimehr
- **Comment**: None
- **Journal**: None
- **Summary**: Federated learning (FL) is an efficient learning framework that assists distributed machine learning when data cannot be shared with a centralized server due to privacy and regulatory restrictions. Recent advancements in FL use predefined architecture-based learning for all the clients. However, given that clients' data are invisible to the server and data distributions are non-identical across clients, a predefined architecture discovered in a centralized setting may not be an optimal solution for all the clients in FL. Motivated by this challenge, in this work, we introduce SPIDER, an algorithmic framework that aims to Search Personalized neural architecture for federated learning. SPIDER is designed based on two unique features: (1) alternately optimizing one architecture-homogeneous global model (Supernet) in a generic FL manner and one architecture-heterogeneous local model that is connected to the global model by weight sharing-based regularization (2) achieving architecture-heterogeneous local model by a novel neural architecture search (NAS) method that can select optimal subnet progressively using operation-level perturbation on the accuracy value as the criterion. Experimental results demonstrate that SPIDER outperforms other state-of-the-art personalization methods, and the searched personalized architectures are more inference efficient.



### PriFit: Learning to Fit Primitives Improves Few Shot Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.13942v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13942v2)
- **Published**: 2021-12-27 23:55:36+00:00
- **Updated**: 2022-06-23 13:17:44+00:00
- **Authors**: Gopal Sharma, Bidya Dash, Aruni RoyChowdhury, Matheus Gadelha, Marios Loizou, Liangliang Cao, Rui Wang, Erik Learned-Miller, Subhransu Maji, Evangelos Kalogerakis
- **Comment**: None
- **Journal**: None
- **Summary**: We present PriFit, a semi-supervised approach for label-efficient learning of 3D point cloud segmentation networks. PriFit combines geometric primitive fitting with point-based representation learning. Its key idea is to learn point representations whose clustering reveals shape regions that can be approximated well by basic geometric primitives, such as cuboids and ellipsoids. The learned point representations can then be re-used in existing network architectures for 3D point cloud segmentation, and improves their performance in the few-shot setting. According to our experiments on the widely used ShapeNet and PartNet benchmarks, PriFit outperforms several state-of-the-art methods in this setting, suggesting that decomposability into primitives is a useful prior for learning representations predictive of semantic parts. We present a number of ablative experiments varying the choice of geometric primitives and downstream tasks to demonstrate the effectiveness of the method.



