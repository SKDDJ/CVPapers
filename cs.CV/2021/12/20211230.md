# Arxiv Papers in cs.CV on 2021-12-30
### Few-shot Backdoor Defense Using Shapley Estimation
- **Arxiv ID**: http://arxiv.org/abs/2112.14889v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2112.14889v2)
- **Published**: 2021-12-30 02:27:03+00:00
- **Updated**: 2022-10-21 07:40:54+00:00
- **Authors**: Jiyang Guan, Zhuozhuo Tu, Ran He, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have achieved impressive performance in a variety of tasks over the last decade, such as autonomous driving, face recognition, and medical diagnosis. However, prior works show that deep neural networks are easily manipulated into specific, attacker-decided behaviors in the inference stage by backdoor attacks which inject malicious small hidden triggers into model training, raising serious security threats. To determine the triggered neurons and protect against backdoor attacks, we exploit Shapley value and develop a new approach called Shapley Pruning (ShapPruning) that successfully mitigates backdoor attacks from models in a data-insufficient situation (1 image per class or even free of data). Considering the interaction between neurons, ShapPruning identifies the few infected neurons (under 1% of all neurons) and manages to protect the model's structure and accuracy after pruning as many infected neurons as possible. To accelerate ShapPruning, we further propose discarding threshold and $\epsilon$-greedy strategy to accelerate Shapley estimation, making it possible to repair poisoned models with only several minutes. Experiments demonstrate the effectiveness and robustness of our method against various attacks and tasks compared to existing methods.



### Feature Generation and Hypothesis Verification for Reliable Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2112.14894v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2112.14894v1)
- **Published**: 2021-12-30 02:31:03+00:00
- **Updated**: 2021-12-30 02:31:03+00:00
- **Authors**: Shice Liu, Shitao Lu, Hongyi Xu, Jing Yang, Shouhong Ding, Lizhuang Ma
- **Comment**: Accepted by AAAI 2022
- **Journal**: None
- **Summary**: Although existing face anti-spoofing (FAS) methods achieve high accuracy in intra-domain experiments, their effects drop severely in cross-domain scenarios because of poor generalization. Recently, multifarious techniques have been explored, such as domain generalization and representation disentanglement. However, the improvement is still limited by two issues: 1) It is difficult to perfectly map all faces to a shared feature space. If faces from unknown domains are not mapped to the known region in the shared feature space, accidentally inaccurate predictions will be obtained. 2) It is hard to completely consider various spoof traces for disentanglement. In this paper, we propose a Feature Generation and Hypothesis Verification framework to alleviate the two issues. Above all, feature generation networks which generate hypotheses of real faces and known attacks are introduced for the first time in the FAS task. Subsequently, two hypothesis verification modules are applied to judge whether the input face comes from the real-face space and the real-face distribution respectively. Furthermore, some analyses of the relationship between our framework and Bayesian uncertainty estimation are given, which provides theoretical support for reliable defense in unknown domains. Experimental results show our framework achieves promising results and outperforms the state-of-the-art approaches on extensive public datasets.



### Runway Extraction and Improved Mapping from Space Imagery
- **Arxiv ID**: http://arxiv.org/abs/2201.00848v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.00848v1)
- **Published**: 2021-12-30 03:15:45+00:00
- **Updated**: 2021-12-30 03:15:45+00:00
- **Authors**: David A. Noever
- **Comment**: None
- **Journal**: None
- **Summary**: Change detection methods applied to monitoring key infrastructure like airport runways represent an important capability for disaster relief and urban planning. The present work identifies two generative adversarial networks (GAN) architectures that translate reversibly between plausible runway maps and satellite imagery. We illustrate the training capability using paired images (satellite-map) from the same point of view and using the Pix2Pix architecture or conditional GANs. In the absence of available pairs, we likewise show that CycleGAN architectures with four network heads (discriminator-generator pairs) can also provide effective style transfer from raw image pixels to outline or feature maps. To emphasize the runway and tarmac boundaries, we experimentally show that the traditional grey-tan map palette is not a required training input but can be augmented by higher contrast mapping palettes (red-black) for sharper runway boundaries. We preview a potentially novel use case (called "sketch2satellite") where a human roughly draws the current runway boundaries and automates the machine output of plausible satellite images. Finally, we identify examples of faulty runway maps where the published satellite and mapped runways disagree but an automated update renders the correct map using GANs.



### Retrieving Black-box Optimal Images from External Databases
- **Arxiv ID**: http://arxiv.org/abs/2112.14921v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.14921v1)
- **Published**: 2021-12-30 04:22:15+00:00
- **Updated**: 2021-12-30 04:22:15+00:00
- **Authors**: Ryoma Sato
- **Comment**: WSDM 2022
- **Journal**: None
- **Summary**: Suppose we have a black-box function (e.g., deep neural network) that takes an image as input and outputs a value that indicates preference. How can we retrieve optimal images with respect to this function from an external database on the Internet? Standard retrieval problems in the literature (e.g., item recommendations) assume that an algorithm has full access to the set of items. In other words, such algorithms are designed for service providers. In this paper, we consider the retrieval problem under different assumptions. Specifically, we consider how users with limited access to an image database can retrieve images using their own black-box functions. This formulation enables a flexible and finer-grained image search defined by each user. We assume the user can access the database through a search query with tight API limits. Therefore, a user needs to efficiently retrieve optimal images in terms of the number of queries. We propose an efficient retrieval algorithm Tiara for this problem. In the experiments, we confirm that our proposed method performs better than several baselines under various settings.



### Dense Depth Estimation from Multiple 360-degree Images Using Virtual Depth
- **Arxiv ID**: http://arxiv.org/abs/2112.14931v2
- **DOI**: 10.1007/s10489-022-03391-w
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.14931v2)
- **Published**: 2021-12-30 05:27:28+00:00
- **Updated**: 2022-03-10 00:54:33+00:00
- **Authors**: Seongyeop Yang, Kunhee Kim, Yeejin Lee
- **Comment**: 16 pages, 11 figures, Applied Intelligence
- **Journal**: None
- **Summary**: In this paper, we propose a dense depth estimation pipeline for multiview 360{\deg} images. The proposed pipeline leverages a spherical camera model that compensates for radial distortion in 360{\deg} images. The key contribution of this paper is the extension of a spherical camera model to multiview by introducing a translation scaling scheme. Moreover, we propose an effective dense depth estimation method by setting virtual depth and minimizing photonic reprojection error. We validate the performance of the proposed pipeline using the images of natural scenes as well as the synthesized dataset for quantitive evaluation. The experimental results verify that the proposed pipeline improves estimation accuracy compared to the current state-of-art dense depth estimation methods.



### SFU-HW-Tracks-v1: Object Tracking Dataset on Raw Video Sequences
- **Arxiv ID**: http://arxiv.org/abs/2112.14934v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.14934v1)
- **Published**: 2021-12-30 05:52:15+00:00
- **Updated**: 2021-12-30 05:52:15+00:00
- **Authors**: Takehiro Tanaka, Hyomin Choi, Ivan V. Bajić
- **Comment**: 4 pages, 3 figures, submitted to Data in Brief
- **Journal**: None
- **Summary**: We present a dataset that contains object annotations with unique object identities (IDs) for the High Efficiency Video Coding (HEVC) v1 Common Test Conditions (CTC) sequences. Ground-truth annotations for 13 sequences were prepared and released as the dataset called SFU-HW-Tracks-v1. For each video frame, ground truth annotations include object class ID, object ID, and bounding box location and its dimensions. The dataset can be used to evaluate object tracking performance on uncompressed video sequences and study the relationship between video compression and object tracking.



### Brain Signals Analysis Based Deep Learning Methods: Recent advances in the study of non-invasive brain signals
- **Arxiv ID**: http://arxiv.org/abs/2201.04229v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.04229v1)
- **Published**: 2021-12-30 06:44:35+00:00
- **Updated**: 2021-12-30 06:44:35+00:00
- **Authors**: Almabrok Essa, Hari Kotte
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: Brain signals constitute the information that are processed by millions of brain neurons (nerve cells and brain cells). These brain signals can be recorded and analyzed using various of non-invasive techniques such as the Electroencephalograph (EEG), Magneto-encephalograph (MEG) as well as brain-imaging techniques such as Magnetic Resonance Imaging (MRI), Computed Tomography (CT) and others, which will be discussed briefly in this paper. This paper discusses about the currently emerging techniques such as the usage of different Deep Learning (DL) algorithms for the analysis of these brain signals and how these algorithms will be helpful in determining the neurological status of a person by applying the signal decoding strategy.



### A Novel Generator with Auxiliary Branch for Improving GAN Performance
- **Arxiv ID**: http://arxiv.org/abs/2112.14968v1
- **DOI**: 10.1007/s10489-022-03666-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.14968v1)
- **Published**: 2021-12-30 08:38:49+00:00
- **Updated**: 2021-12-30 08:38:49+00:00
- **Authors**: Seung Park, Yong-Goo Shin
- **Comment**: None
- **Journal**: Applied Intelligence 2022
- **Summary**: The generator in the generative adversarial network (GAN) learns image generation in a coarse-to-fine manner in which earlier layers learn an overall structure of the image and the latter ones refine the details. To propagate the coarse information well, recent works usually build their generators by stacking up multiple residual blocks. Although the residual block can produce the high-quality image as well as be trained stably, it often impedes the information flow in the network. To alleviate this problem, this brief introduces a novel generator architecture that produces the image by combining features obtained through two different branches: the main and auxiliary branches. The goal of the main branch is to produce the image by passing through the multiple residual blocks, whereas the auxiliary branch is to convey the coarse information in the earlier layer to the later one. To combine the features in the main and auxiliary branches successfully, we also propose a gated feature fusion module that controls the information flow in those branches. To prove the superiority of the proposed method, this brief provides extensive experiments using various standard datasets including CIFAR-10, CIFAR-100, LSUN, CelebA-HQ, AFHQ, and tiny- ImageNet. Furthermore, we conducted various ablation studies to demonstrate the generalization ability of the proposed method. Quantitative evaluations prove that the proposed method exhibits impressive GAN performance in terms of Inception score (IS) and Frechet inception distance (FID). For instance, the proposed method boosts the FID and IS scores on the tiny-ImageNet dataset from 35.13 to 25.00 and 20.23 to 25.57, respectively.



### Contrastive Fine-grained Class Clustering via Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2112.14971v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.14971v3)
- **Published**: 2021-12-30 08:57:11+00:00
- **Updated**: 2022-03-13 02:43:21+00:00
- **Authors**: Yunji Kim, Jung-Woo Ha
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: Unsupervised fine-grained class clustering is a practical yet challenging task due to the difficulty of feature representations learning of subtle object details. We introduce C3-GAN, a method that leverages the categorical inference power of InfoGAN with contrastive learning. We aim to learn feature representations that encourage a dataset to form distinct cluster boundaries in the embedding space, while also maximizing the mutual information between the latent code and its image observation. Our approach is to train a discriminator, which is also used for inferring clusters, to optimize the contrastive loss, where image-latent pairs that maximize the mutual information are considered as positive pairs and the rest as negative pairs. Specifically, we map the input of a generator, which was sampled from the categorical distribution, to the embedding space of the discriminator and let them act as a cluster centroid. In this way, C3-GAN succeeded in learning a clustering-friendly embedding space where each cluster is distinctively separable. Experimental results show that C3-GAN achieved the state-of-the-art clustering performance on four fine-grained image datasets, while also alleviating the mode collapse phenomenon. Code is available at https://github.com/naver-ai/c3-gan.



### Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data
- **Arxiv ID**: http://arxiv.org/abs/2201.00849v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.00849v1)
- **Published**: 2021-12-30 09:20:07+00:00
- **Updated**: 2021-12-30 09:20:07+00:00
- **Authors**: Shenwang Jiang, Jianan Li, Ying Wang, Bo Huang, Zhang Zhang, Tingfa Xu
- **Comment**: Accepted by AAAI-2022
- **Journal**: None
- **Summary**: Corrupted labels and class imbalance are commonly encountered in practically collected training data, which easily leads to over-fitting of deep neural networks (DNNs). Existing approaches alleviate these issues by adopting a sample re-weighting strategy, which is to re-weight sample by designing weighting function. However, it is only applicable for training data containing only either one type of data biases. In practice, however, biased samples with corrupted labels and of tailed classes commonly co-exist in training data. How to handle them simultaneously is a key but under-explored problem. In this paper, we find that these two types of biased samples, though have similar transient loss, have distinguishable trend and characteristics in loss curves, which could provide valuable priors for sample weight assignment. Motivated by this, we delve into the loss curves and propose a novel probe-and-allocate training strategy: In the probing stage, we train the network on the whole biased training data without intervention, and record the loss curve of each sample as an additional attribute; In the allocating stage, we feed the resulting attribute to a newly designed curve-perception network, named CurveNet, to learn to identify the bias type of each sample and assign proper weights through meta-learning adaptively. The training speed of meta learning also blocks its application. To solve it, we propose a method named skip layer meta optimization (SLMO) to accelerate training speed by skipping the bottom layers. Extensive synthetic and real experiments well validate the proposed method, which achieves state-of-the-art performance on multiple challenging benchmarks.



### Contrastive Learning of Semantic and Visual Representations for Text Tracking
- **Arxiv ID**: http://arxiv.org/abs/2112.14976v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.14976v3)
- **Published**: 2021-12-30 09:22:13+00:00
- **Updated**: 2022-08-19 04:48:54+00:00
- **Authors**: Zhuang Li, Weijia Wu, Mike Zheng Shou, Jiahong Li, Size Li, Zhongyuan Wang, Hong Zhou
- **Comment**: Merge the paper with arXiv article 2207.08417. We will withdraw the
  two papers and create new one
- **Journal**: None
- **Summary**: Semantic representation is of great benefit to the video text tracking(VTT) task that requires simultaneously classifying, detecting, and tracking texts in the video. Most existing approaches tackle this task by appearance similarity in continuous frames, while ignoring the abundant semantic features. In this paper, we explore to robustly track video text with contrastive learning of semantic and visual representations. Correspondingly, we present an end-to-end video text tracker with Semantic and Visual Representations(SVRep), which detects and tracks texts by exploiting the visual and semantic relationships between different texts in a video sequence. Besides, with a light-weight architecture, SVRep achieves state-of-the-art performance while maintaining competitive inference speed. Specifically, with a backbone of ResNet-18, SVRep achieves an ${\rm ID_{F1}}$ of $\textbf{65.9\%}$, running at $\textbf{16.7}$ FPS, on the ICDAR2015(video) dataset with $\textbf{8.6\%}$ improvement than the previous state-of-the-art methods.



### Exploring the pattern of Emotion in children with ASD as an early biomarker through Recurring-Convolution Neural Network (R-CNN)
- **Arxiv ID**: http://arxiv.org/abs/2112.14983v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2112.14983v1)
- **Published**: 2021-12-30 09:35:05+00:00
- **Updated**: 2021-12-30 09:35:05+00:00
- **Authors**: Abirami S P, Kousalya G, Karthick R
- **Comment**: 8 figures and 2 tables. totally 18 pages
- **Journal**: None
- **Summary**: Autism Spectrum Disorder (ASD) is found to be a major concern among various occupational therapists. The foremost challenge of this neurodevelopmental disorder lies in the fact of analyzing and exploring various symptoms of the children at their early stage of development. Such early identification could prop up the therapists and clinicians to provide proper assistive support to make the children lead an independent life. Facial expressions and emotions perceived by the children could contribute to such early intervention of autism. In this regard, the paper implements in identifying basic facial expression and exploring their emotions upon a time variant factor. The emotions are analyzed by incorporating the facial expression identified through CNN using 68 landmark points plotted on the frontal face with a prediction network formed by RNN known as RCNN-FER system. The paper adopts R-CNN to take the advantage of increased accuracy and performance with decreased time complexity in predicting emotion as a textual network analysis. The papers proves better accuracy in identifying the emotion in autistic children when compared over simple machine learning models built for such identifications contributing to autistic society.



### THE Benchmark: Transferable Representation Learning for Monocular Height Estimation
- **Arxiv ID**: http://arxiv.org/abs/2112.14985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.14985v1)
- **Published**: 2021-12-30 09:40:26+00:00
- **Updated**: 2021-12-30 09:40:26+00:00
- **Authors**: Zhitong Xiong, Wei Huang, Jingtao Hu, Yilei Shi, Qi Wang, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Generating 3D city models rapidly is crucial for many applications. Monocular height estimation is one of the most efficient and timely ways to obtain large-scale geometric information. However, existing works focus primarily on training and testing models using unbiased datasets, which don't align well with real-world applications. Therefore, we propose a new benchmark dataset to study the transferability of height estimation models in a cross-dataset setting. To this end, we first design and construct a large-scale benchmark dataset for cross-dataset transfer learning on the height estimation task. This benchmark dataset includes a newly proposed large-scale synthetic dataset, a newly collected real-world dataset, and four existing datasets from different cities. Next, two new experimental protocols, zero-shot and few-shot cross-dataset transfer, are designed. For few-shot cross-dataset transfer, we enhance the window-based Transformer with the proposed scale-deformable convolution module to handle the severe scale-variation problem. To improve the generalizability of deep models in the zero-shot cross-dataset setting, a max-normalization-based Transformer network is designed to decouple the relative height map from the absolute heights. Experimental results have demonstrated the effectiveness of the proposed methods in both the traditional and cross-dataset transfer settings. The datasets and codes are publicly available at https://thebenchmarkh.github.io/.



### Knowledge Matters: Radiology Report Generation with General and Specific Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2112.15009v2
- **DOI**: 10.1016/j.media.2022.102510
- **Categories**: **eess.IV**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.15009v2)
- **Published**: 2021-12-30 10:36:04+00:00
- **Updated**: 2022-11-06 16:52:47+00:00
- **Authors**: Shuxin Yang, Xian Wu, Shen Ge, Shaohua Kevin Zhou, Li Xiao
- **Comment**: Medical Image Analysis
- **Journal**: None
- **Summary**: Automatic radiology report generation is critical in clinics which can relieve experienced radiologists from the heavy workload and remind inexperienced radiologists of misdiagnosis or missed diagnose. Existing approaches mainly formulate radiology report generation as an image captioning task and adopt the encoder-decoder framework. However, in the medical domain, such pure data-driven approaches suffer from the following problems: 1) visual and textual bias problem; 2) lack of expert knowledge. In this paper, we propose a knowledge-enhanced radiology report generation approach introduces two types of medical knowledge: 1) General knowledge, which is input independent and provides the broad knowledge for report generation; 2) Specific knowledge, which is input dependent and provides the fine-grained knowledge for report generation. To fully utilize both the general and specific knowledge, we also propose a knowledge-enhanced multi-head attention mechanism. By merging the visual features of the radiology image with general knowledge and specific knowledge, the proposed model can improve the quality of generated reports. Experimental results on two publicly available datasets IU-Xray and MIMIC-CXR show that the proposed knowledge enhanced approach outperforms state-of-the-art image captioning based methods. Ablation studies also demonstrate that both general and specific knowledge can help to improve the performance of radiology report generation.



### Radiology Report Generation with a Learned Knowledge Base and Multi-modal Alignment
- **Arxiv ID**: http://arxiv.org/abs/2112.15011v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.15011v2)
- **Published**: 2021-12-30 10:43:56+00:00
- **Updated**: 2022-06-01 14:52:58+00:00
- **Authors**: Shuxin Yang, Xian Wu, Shen Ge, S. Kevin Zhou, Li Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: In clinics, a radiology report is crucial for guiding a patient's treatment. However, writing radiology reports is a heavy burden for radiologists. To this end, we present an automatic, multi-modal approach for report generation from a chest x-ray. Our approach, motivated by the observation that the descriptions in radiology reports are highly correlated with specific information of the x-ray images, features two distinct modules: (i) Learned knowledge base: To absorb the knowledge embedded in the radiology reports, we build a knowledge base that can automatically distil and restore medical knowledge from textual embedding without manual labour; (ii) Multi-modal alignment: to promote the semantic alignment among reports, disease labels, and images, we explicitly utilize textual embedding to guide the learning of the visual feature space. We evaluate the performance of the proposed model using metrics from both natural language generation and clinic efficacy on the public IU-Xray and MIMIC-CXR datasets. Our ablation study shows that each module contributes to improving the quality of generated reports. Furthermore, with the assistance of both modules, our approach outperforms state-of-the-art methods over almost all the metrics.



### Investigating Pose Representations and Motion Contexts Modeling for 3D Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2112.15012v1
- **DOI**: 10.1109/TPAMI.2021.3139918
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.15012v1)
- **Published**: 2021-12-30 10:45:22+00:00
- **Updated**: 2021-12-30 10:45:22+00:00
- **Authors**: Zhenguang Liu, Shuang Wu, Shuyuan Jin, Shouling Ji, Qi Liu, Shijian Lu, Li Cheng
- **Comment**: Accepted to IEEE TPAMI, 27 Dec. 2021
- **Journal**: None
- **Summary**: Predicting human motion from historical pose sequence is crucial for a machine to succeed in intelligent interactions with humans. One aspect that has been obviated so far, is the fact that how we represent the skeletal pose has a critical impact on the prediction results. Yet there is no effort that investigates across different pose representation schemes. We conduct an indepth study on various pose representations with a focus on their effects on the motion prediction task. Moreover, recent approaches build upon off-the-shelf RNN units for motion prediction. These approaches process input pose sequence sequentially and inherently have difficulties in capturing long-term dependencies. In this paper, we propose a novel RNN architecture termed AHMR (Attentive Hierarchical Motion Recurrent network) for motion prediction which simultaneously models local motion contexts and a global context. We further explore a geodesic loss and a forward kinematics loss for the motion prediction task, which have more geometric significance than the widely employed L2 loss. Interestingly, we applied our method to a range of articulate objects including human, fish, and mouse. Empirical results show that our approach outperforms the state-of-the-art methods in short-term prediction and achieves much enhanced long-term prediction proficiency, such as retaining natural human-like motions over 50 seconds predictions. Our codes are released.



### Continually Learning Self-Supervised Representations with Projected Functional Regularization
- **Arxiv ID**: http://arxiv.org/abs/2112.15022v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.15022v2)
- **Published**: 2021-12-30 11:59:23+00:00
- **Updated**: 2022-05-02 14:35:54+00:00
- **Authors**: Alex Gomez-Villa, Bartlomiej Twardowski, Lu Yu, Andrew D. Bagdanov, Joost van de Weijer
- **Comment**: Accepted at Workshop on Continual Learning in Computer Vision (CVPR
  2022)
- **Journal**: None
- **Summary**: Recent self-supervised learning methods are able to learn high-quality image representations and are closing the gap with supervised approaches. However, these methods are unable to acquire new knowledge incrementally -- they are, in fact, mostly used only as a pre-training phase over IID data. In this work we investigate self-supervised methods in continual learning regimes without any replay mechanism. We show that naive functional regularization, also known as feature distillation, leads to lower plasticity and limits continual learning performance. Instead, we propose Projected Functional Regularization in which a separate temporal projection network ensures that the newly learned feature space preserves information of the previous one, while at the same time allowing for the learning of new features. This prevents forgetting while maintaining the plasticity of the learner. Comparison with other incremental learning approaches applied to self-supervision demonstrates that our method obtains competitive performance in different scenarios and on multiple datasets.



### Development of a face mask detection pipeline for mask-wearing monitoring in the era of the COVID-19 pandemic: A modular approach
- **Arxiv ID**: http://arxiv.org/abs/2112.15031v3
- **DOI**: 10.1109/JCSSE54890.2022.9836283
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.15031v3)
- **Published**: 2021-12-30 12:32:33+00:00
- **Updated**: 2022-08-01 11:37:25+00:00
- **Authors**: Benjaphan Sommana, Ukrit Watchareeruetai, Ankush Ganguly, Samuel W. F. Earp, Taya Kitiyakara, Suparee Boonmanunt, Ratchainant Thammasudjarit
- **Comment**: Accepted at the 19th International Joint Conference on Computer
  Science and Software Engineering (JCSSE 2022)
- **Journal**: None
- **Summary**: During the SARS-Cov-2 pandemic, mask-wearing became an effective tool to prevent spreading and contracting the virus. The ability to monitor the mask-wearing rate in the population would be useful for determining public health strategies against the virus. However, artificial intelligence technologies for detecting face masks have not been deployed at a large scale in real-life to measure the mask-wearing rate in public. In this paper, we present a two-step face mask detection approach consisting of two separate modules: 1) face detection and alignment and 2) face mask classification. This approach allowed us to experiment with different combinations of face detection and face mask classification modules. More specifically, we experimented with PyramidKey and RetinaFace as face detectors while maintaining a lightweight backbone for the face mask classification module. Moreover, we also provide a relabeled annotation of the test set of the AIZOO dataset, where we rectified the incorrect labels for some face images. The evaluation results on the AIZOO and Moxa 3K datasets showed that the proposed face mask detection pipeline surpassed the state-of-the-art methods. The proposed pipeline also yielded a higher mAP on the relabeled test set of the AIZOO dataset than the original test set. Since we trained the proposed model using in-the-wild face images, we can successfully deploy our model to monitor the mask-wearing rate using public CCTV images.



### Digital Rock Typing DRT Algorithm Formulation with Optimal Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.15068v2
- **DOI**: None
- **Categories**: **cs.LG**, astro-ph.EP, cs.CV, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/2112.15068v2)
- **Published**: 2021-12-30 14:04:50+00:00
- **Updated**: 2022-01-05 11:36:46+00:00
- **Authors**: Omar Alfarisi, Djamel Ouzzane, Mohamed Sassi, Tiejun Zhang
- **Comment**: 1-Acknowledgement section is updated. 2- References section is update
  with one additional reference
- **Journal**: None
- **Summary**: Each grid block in a 3D geological model requires a rock type that represents all physical and chemical properties of that block. The properties that classify rock types are lithology, permeability, and capillary pressure. Scientists and engineers determined these properties using conventional laboratory measurements, which embedded destructive methods to the sample or altered some of its properties (i.e., wettability, permeability, and porosity) because the measurements process includes sample crushing, fluid flow, or fluid saturation. Lately, Digital Rock Physics (DRT) has emerged to quantify these properties from micro-Computerized Tomography (uCT) and Magnetic Resonance Imaging (MRI) images. However, the literature did not attempt rock typing in a wholly digital context. We propose performing Digital Rock Typing (DRT) by: (1) integrating the latest DRP advances in a novel process that honors digital rock properties determination, while; (2) digitalizing the latest rock typing approaches in carbonate, and (3) introducing a novel carbonate rock typing process that utilizes computer vision capabilities to provide more insight about the heterogeneous carbonate rock texture.



### Pose Estimation of Specific Rigid Objects
- **Arxiv ID**: http://arxiv.org/abs/2112.15075v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.15075v1)
- **Published**: 2021-12-30 14:36:47+00:00
- **Updated**: 2021-12-30 14:36:47+00:00
- **Authors**: Tomas Hodan
- **Comment**: Tomas Hodan's PhD thesis defended on July 7, 2021. Supervisor: Prof.
  Jiri Matas. Reviewers: Prof. Vincent Lepetit, Prof. Markus Vincze, Dr.
  Slobodan Ilic. A recording of the defense: https://youtu.be/WAQmubEXCRM
- **Journal**: None
- **Summary**: In this thesis, we address the problem of estimating the 6D pose of rigid objects from a single RGB or RGB-D input image, assuming that 3D models of the objects are available. This problem is of great importance to many application fields such as robotic manipulation, augmented reality, and autonomous driving. First, we propose EPOS, a method for 6D object pose estimation from an RGB image. The key idea is to represent an object by compact surface fragments and predict the probability distribution of corresponding fragments at each pixel of the input image by a neural network. Each pixel is linked with a data-dependent number of fragments, which allows systematic handling of symmetries, and the 6D poses are estimated from the links by a RANSAC-based fitting method. EPOS outperformed all RGB and most RGB-D and D methods on several standard datasets. Second, we present HashMatch, an RGB-D method that slides a window over the input image and searches for a match against templates, which are pre-generated by rendering 3D object models in different orientations. The method applies a cascade of evaluation stages to each window location, which avoids exhaustive matching against all templates. Third, we propose ObjectSynth, an approach to synthesize photorealistic images of 3D object models for training methods based on neural networks. The images yield substantial improvements compared to commonly used images of objects rendered on top of random photographs. Fourth, we introduce T-LESS, the first dataset for 6D object pose estimation that includes 3D models and RGB-D images of industry-relevant objects. Fifth, we define BOP, a benchmark that captures the status quo in the field. BOP comprises eleven datasets in a unified format, an evaluation methodology, an online evaluation system, and public challenges held at international workshops organized at the ICCV and ECCV conferences.



### Automated 3D reconstruction of LoD2 and LoD1 models for all 10 million buildings of the Netherlands
- **Arxiv ID**: http://arxiv.org/abs/2201.01191v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.01191v1)
- **Published**: 2021-12-30 14:51:29+00:00
- **Updated**: 2021-12-30 14:51:29+00:00
- **Authors**: Ravi Peters, Balázs Dukai, Stelios Vitalis, Jordi van Liempt, Jantien Stoter
- **Comment**: Submitted to Journal of Photogrammetric Engineering & Remote Sensing
- **Journal**: None
- **Summary**: In this paper we present our workflow to automatically reconstruct 3D building models based on 2D building polygons and a LiDAR point cloud. The workflow generates models at different levels of detail (LoDs) to support data requirements of different applications from one consistent source. Specific attention has been paid to make the workflow robust to quickly run a new iteration in case of improvements in an algorithm or in case new input data become available. The quality of the reconstructed data highly depends on the quality of the input data and is monitored in several steps of the process. A 3D viewer has been developed to view and download the openly available 3D data at different LoDs in different formats. The workflow has been applied to all 10 million buildings of The Netherlands. The 3D service will be updated after new input data becomes available.



### Feature Extraction, Classification and Prediction for Hand Hygiene Gestures with KNN Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2112.15085v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.15085v2)
- **Published**: 2021-12-30 14:56:07+00:00
- **Updated**: 2022-05-25 18:28:09+00:00
- **Authors**: Rashmi Bakshi
- **Comment**: None
- **Journal**: None
- **Summary**: There are six, well-structured hand gestures for washing hands as provided by World Health Organisation guidelines. In this paper, hand features such as contours of the hands, the centroid of the hands, and extreme hand points along the largest contour are extracted for specific hand-washing gestures with the use of a computer vision library, OpenCV. For this project, a robust dataset of hand hygiene video recordings is built with the help of 30 research participants. In this work, a subset of the dataset was used as a pilot study to demonstrate the effectiveness of the KNN algorithm. Extracted hand features saved in a CSV file are passed to a KNN model with a cross-fold validation technique for the classification and prediction of the unlabelled data. A mean accuracy score of >95% is achieved and proves that the KNN algorithm with an appropriate input value of K=3 is efficient for hand hygiene gestures classification.



### Leveraging in-domain supervision for unsupervised image-to-image translation tasks via multi-stream generators
- **Arxiv ID**: http://arxiv.org/abs/2112.15091v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.15091v1)
- **Published**: 2021-12-30 15:29:36+00:00
- **Updated**: 2021-12-30 15:29:36+00:00
- **Authors**: Dvir Yerushalmi, Dov Danon, Amit H. Bermano
- **Comment**: None
- **Journal**: None
- **Summary**: Supervision for image-to-image translation (I2I) tasks is hard to come by, but bears significant effect on the resulting quality. In this paper, we observe that for many Unsupervised I2I (UI2I) scenarios, one domain is more familiar than the other, and offers in-domain prior knowledge, such as semantic segmentation. We argue that for complex scenes, figuring out the semantic structure of the domain is hard, especially with no supervision, but is an important part of a successful I2I operation. We hence introduce two techniques to incorporate this invaluable in-domain prior knowledge for the benefit of translation quality: through a novel Multi-Stream generator architecture, and through a semantic segmentation-based regularization loss term. In essence, we propose splitting the input data according to semantic masks, explicitly guiding the network to different behavior for the different regions of the image. In addition, we propose training a semantic segmentation network along with the translation task, and to leverage this output as a loss term that improves robustness. We validate our approach on urban data, demonstrating superior quality in the challenging UI2I tasks of converting day images to night ones. In addition, we also demonstrate how reinforcing the target dataset with our augmented images improves the training of downstream tasks such as the classical detection one.



### Benchmarking Chinese Text Recognition: Datasets, Baselines, and an Empirical Study
- **Arxiv ID**: http://arxiv.org/abs/2112.15093v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.15093v2)
- **Published**: 2021-12-30 15:30:52+00:00
- **Updated**: 2022-11-25 12:03:17+00:00
- **Authors**: Haiyang Yu, Jingye Chen, Bin Li, Jianqi Ma, Mengnan Guan, Xixi Xu, Xiaocong Wang, Shaobo Qu, Xiangyang Xue
- **Comment**: Code is available at
  https://github.com/FudanVI/benchmarking-chinese-text-recognition
- **Journal**: None
- **Summary**: The flourishing blossom of deep learning has witnessed the rapid development of text recognition in recent years. However, the existing text recognition methods are mainly proposed for English texts. As another widely-spoken language, Chinese text recognition (CTR) in all ways has extensive application markets. Based on our observations, we attribute the scarce attention on CTR to the lack of reasonable dataset construction standards, unified evaluation protocols, and results of the existing baselines. To fill this gap, we manually collect CTR datasets from publicly available competitions, projects, and papers. According to application scenarios, we divide the collected datasets into four categories including scene, web, document, and handwriting datasets. Besides, we standardize the evaluation protocols in CTR. With unified evaluation protocols, we evaluate a series of representative text recognition methods on the collected datasets to provide baselines. The experimental results indicate that the performance of baselines on CTR datasets is not as good as that on English datasets due to the characteristics of Chinese texts that are quite different from the Latin alphabet. Moreover, we observe that by introducing radical-level supervision as an auxiliary task, the performance of baselines can be further boosted. The code and datasets are made publicly available at https://github.com/FudanVI/benchmarking-chinese-text-recognition



### A general technique for the estimation of farm animal body part weights from CT scans and its applications in a rabbit breeding program
- **Arxiv ID**: http://arxiv.org/abs/2112.15095v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.15095v1)
- **Published**: 2021-12-30 15:36:06+00:00
- **Updated**: 2021-12-30 15:36:06+00:00
- **Authors**: Ádám Csóka, György Kovács, Virág Ács, Zsolt Matics, Zsolt Gerencsér, Zsolt Szendrő, István Nagy, Örs Petneházy, Imre Repa, Mariann Moizs, Tamás Donkó
- **Comment**: None
- **Journal**: None
- **Summary**: Various applications of farm animal imaging are based on the estimation of weights of certain body parts and cuts from the CT images of animals. In many cases, the complexity of the problem is increased by the enormous variability of postures in CT images due to the scanning of non-sedated, living animals. In this paper, we propose a general and robust approach for the estimation of the weights of cuts and body parts from the CT images of (possibly) living animals. We adapt multi-atlas based segmentation driven by elastic registration and joint feature and model selection for the regression component to cape with the large number of features and low number of samples. The proposed technique is evaluated and illustrated through real applications in rabbit breeding programs, showing r^2 scores 12% higher than previous techniques and methods that used to drive the selection so far. The proposed technique is easily adaptable to similar problems, consequently, it is shared in an open source software package for the benefit of the community.



### Colour alignment for relative colour constancy via non-standard references
- **Arxiv ID**: http://arxiv.org/abs/2112.15106v2
- **DOI**: 10.1109/TIP.2022.3214107
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.15106v2)
- **Published**: 2021-12-30 15:58:55+00:00
- **Updated**: 2022-10-03 08:19:41+00:00
- **Authors**: Yunfeng Zhao, Stuart Ferguson, Huiyu Zhou, Chris Elliott, Karen Rafferty
- **Comment**: 14 pages, 8 figures, 2 tables, accepted by IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: Relative colour constancy is an essential requirement for many scientific imaging applications. However, most digital cameras differ in their image formations and native sensor output is usually inaccessible, e.g., in smartphone camera applications. This makes it hard to achieve consistent colour assessment across a range of devices, and that undermines the performance of computer vision algorithms. To resolve this issue, we propose a colour alignment model that considers the camera image formation as a black-box and formulates colour alignment as a three-step process: camera response calibration, response linearisation, and colour matching. The proposed model works with non-standard colour references, i.e., colour patches without knowing the true colour values, by utilising a novel balance-of-linear-distances feature. It is equivalent to determining the camera parameters through an unsupervised process. It also works with a minimum number of corresponding colour patches across the images to be colour aligned to deliver the applicable processing. Two challenging image datasets collected by multiple cameras under various illumination and exposure conditions were used to evaluate the model. Performance benchmarks demonstrated that our model achieved superior performance compared to other popular and state-of-the-art methods.



### Improving the Behaviour of Vision Transformers with Token-consistent Stochastic Layers
- **Arxiv ID**: http://arxiv.org/abs/2112.15111v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.15111v3)
- **Published**: 2021-12-30 16:07:59+00:00
- **Updated**: 2022-07-14 10:22:24+00:00
- **Authors**: Nikola Popovic, Danda Pani Paudel, Thomas Probst, Luc Van Gool
- **Comment**: This article is under consideration at the Computer Vision and Image
  Understanding journal
- **Journal**: None
- **Summary**: We introduce token-consistent stochastic layers in vision transformers, without causing any severe drop in performance. The added stochasticity improves network calibration, robustness and strengthens privacy. We use linear layers with token-consistent stochastic parameters inside the multilayer perceptron blocks, without altering the architecture of the transformer. The stochastic parameters are sampled from the uniform distribution, both during training and inference. The applied linear operations preserve the topological structure, formed by the set of tokens passing through the shared multilayer perceptron. This operation encourages the learning of the recognition task to rely on the topological structures of the tokens, instead of their values, which in turn offers the desired robustness and privacy of the visual features. The effectiveness of the token-consistent stochasticity is demonstrated on three different applications, namely, network calibration, adversarial robustness, and feature privacy, by boosting the performance of the respective established baselines.



### Finding the Task-Optimal Low-Bit Sub-Distribution in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2112.15139v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.15139v4)
- **Published**: 2021-12-30 17:28:11+00:00
- **Updated**: 2022-05-27 16:04:28+00:00
- **Authors**: Runpei Dong, Zhanhong Tan, Mengdi Wu, Linfeng Zhang, Kaisheng Ma
- **Comment**: Accepted at ICML 2022
- **Journal**: None
- **Summary**: Quantized neural networks typically require smaller memory footprints and lower computation complexity, which is crucial for efficient deployment. However, quantization inevitably leads to a distribution divergence from the original network, which generally degrades the performance. To tackle this issue, massive efforts have been made, but most existing approaches lack statistical considerations and depend on several manual configurations. In this paper, we present an adaptive-mapping quantization method to learn an optimal latent sub-distribution that is inherent within models and smoothly approximated with a concrete Gaussian Mixture (GM). In particular, the network weights are projected in compliance with the GM-approximated sub-distribution. This sub-distribution evolves along with the weight update in a co-tuning schema guided by the direct task-objective optimization. Sufficient experiments on image classification and object detection over various modern architectures demonstrate the effectiveness, generalization property, and transferability of the proposed method. Besides, an efficient deployment flow for the mobile CPU is developed, achieving up to 7.46$\times$ inference acceleration on an octa-core ARM CPU. Our codes have been publicly released at \url{https://github.com/RunpeiDong/DGMS}.



### A Resolution Enhancement Plug-in for Deformable Registration of Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2112.15180v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2112.15180v1)
- **Published**: 2021-12-30 19:04:14+00:00
- **Updated**: 2021-12-30 19:04:14+00:00
- **Authors**: Kaicong Sun, Sven Simon
- **Comment**: None
- **Journal**: None
- **Summary**: Image registration is a fundamental task for medical imaging. Resampling of the intensity values is required during registration and better spatial resolution with finer and sharper structures can improve the resampling performance and hence the registration accuracy. Super-resolution (SR) is an algorithmic technique targeting at spatial resolution enhancement which can achieve an image resolution beyond the hardware limitation. In this work, we consider SR as a preprocessing technique and present a CNN-based resolution enhancement module (REM) which can be easily plugged into the registration network in a cascaded manner. Different residual schemes and network configurations of REM are investigated to obtain an effective architecture design of REM. In fact, REM is not confined to image registration, it can also be straightforwardly integrated into other vision tasks for enhanced resolution. The proposed REM is thoroughly evaluated for deformable registration on medical images quantitatively and qualitatively at different upscaling factors. Experiments on LPBA40 brain MRI dataset demonstrate that REM not only improves the registration accuracy, especially when the input images suffer from degraded spatial resolution, but also generates resolution enhanced images which can be exploited for successive diagnosis.



### Towards Robustness of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2112.15188v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.15188v1)
- **Published**: 2021-12-30 19:41:10+00:00
- **Updated**: 2021-12-30 19:41:10+00:00
- **Authors**: Steven Basart
- **Comment**: PhD Thesis
- **Journal**: None
- **Summary**: We introduce several new datasets namely ImageNet-A/O and ImageNet-R as well as a synthetic environment and testing suite we called CAOS. ImageNet-A/O allow researchers to focus in on the blind spots remaining in ImageNet. ImageNet-R was specifically created with the intention of tracking robust representation as the representations are no longer simply natural but include artistic, and other renditions. The CAOS suite is built off of CARLA simulator which allows for the inclusion of anomalous objects and can create reproducible synthetic environment and scenes for testing robustness. All of the datasets were created for testing robustness and measuring progress in robustness. The datasets have been used in various other works to measure their own progress in robustness and allowing for tangential progress that does not focus exclusively on natural accuracy.   Given these datasets, we created several novel methods that aim to advance robustness research. We build off of simple baselines in the form of Maximum Logit, and Typicality Score as well as create a novel data augmentation method in the form of DeepAugment that improves on the aforementioned benchmarks. Maximum Logit considers the logit values instead of the values after the softmax operation, while a small change produces noticeable improvements. The Typicality Score compares the output distribution to a posterior distribution over classes. We show that this improves performance over the baseline in all but the segmentation task. Speculating that perhaps at the pixel level the semantic information of a pixel is less meaningful than that of class level information. Finally the new augmentation technique of DeepAugment utilizes neural networks to create augmentations on images that are radically different than the traditional geometric and camera based transformations used previously.



### Visual and Object Geo-localization: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/2112.15202v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.15202v1)
- **Published**: 2021-12-30 20:46:53+00:00
- **Updated**: 2021-12-30 20:46:53+00:00
- **Authors**: Daniel Wilson, Xiaohan Zhang, Waqas Sultani, Safwan Wshah
- **Comment**: None
- **Journal**: None
- **Summary**: The concept of geo-localization refers to the process of determining where on earth some `entity' is located, typically using Global Positioning System (GPS) coordinates. The entity of interest may be an image, sequence of images, a video, satellite image, or even objects visible within the image. As massive datasets of GPS tagged media have rapidly become available due to smartphones and the internet, and deep learning has risen to enhance the performance capabilities of machine learning models, the fields of visual and object geo-localization have emerged due to its significant impact on a wide range of applications such as augmented reality, robotics, self-driving vehicles, road maintenance, and 3D reconstruction. This paper provides a comprehensive survey of geo-localization involving images, which involves either determining from where an image has been captured (Image geo-localization) or geo-locating objects within an image (Object geo-localization). We will provide an in-depth study, including a summary of popular algorithms, a description of proposed datasets, and an analysis of performance results to illustrate the current state of each field.



