# Arxiv Papers in cs.CV on 2021-12-10
### The Many Faces of Anger: A Multicultural Video Dataset of Negative Emotions in the Wild (MFA-Wild)
- **Arxiv ID**: http://arxiv.org/abs/2112.05267v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.05267v1)
- **Published**: 2021-12-10 00:30:10+00:00
- **Updated**: 2021-12-10 00:30:10+00:00
- **Authors**: Roya Javadi, Angelica Lim
- **Comment**: 8 pages, 13 figures, submitted to FG2021
- **Journal**: None
- **Summary**: The portrayal of negative emotions such as anger can vary widely between cultures and contexts, depending on the acceptability of expressing full-blown emotions rather than suppression to maintain harmony. The majority of emotional datasets collect data under the broad label ``anger", but social signals can range from annoyed, contemptuous, angry, furious, hateful, and more. In this work, we curated the first in-the-wild multicultural video dataset of emotions, and deeply explored anger-related emotional expressions by asking culture-fluent annotators to label the videos with 6 labels and 13 emojis in a multi-label framework. We provide a baseline multi-label classifier on our dataset, and show how emojis can be effectively used as a language-agnostic tool for annotation.



### Long-Range Thermal 3D Perception in Low Contrast Environments
- **Arxiv ID**: http://arxiv.org/abs/2112.05280v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.05280v1)
- **Published**: 2021-12-10 01:16:44+00:00
- **Updated**: 2021-12-10 01:16:44+00:00
- **Authors**: Andrey Filippov, Olga Filippova
- **Comment**: 13 pages, 16 figures
- **Journal**: None
- **Summary**: This report discusses the results of SBIR Phase I effort to prove the feasibility of dramatic improvement of the microbolometer-based Long Wave Infrared (LWIR) detectors sensitivity, especially for the 3D measurements. The resulting low SWaP-C thermal depth-sensing system will enable the situational awareness of Autonomous Air Vehicles for Advanced Air Mobility (AAM). It will provide robust 3D information of the surrounding environment, including low-contrast static and moving objects, at far distances in degraded visual conditions and GPS-denied areas. Our multi-sensor 3D perception enabled by COTS uncooled thermal sensors mitigates major weakness of LWIR sensors - low contrast by increasing the system sensitivity over an order of magnitude.   There were no available thermal image sets suitable for evaluating this technology, making datasets acquisition our first goal. We discuss the design and construction of the prototype system with sixteen 640pix x 512pix LWIR detectors, camera calibration to subpixel resolution, capture, and process synchronized image. The results show the 3.84x contrast increase for intrascene-only data and an additional 5.5x - with the interscene accumulation, reaching system noise-equivalent temperature difference (NETD) of 1.9 mK with the 40 mK sensors.



### RamBoAttack: A Robust Query Efficient Deep Neural Network Decision Exploit
- **Arxiv ID**: http://arxiv.org/abs/2112.05282v3
- **DOI**: 10.14722/ndss.2022.24200
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.05282v3)
- **Published**: 2021-12-10 01:25:24+00:00
- **Updated**: 2023-03-24 02:16:07+00:00
- **Authors**: Viet Quoc Vo, Ehsan Abbasnejad, Damith C. Ranasinghe
- **Comment**: Published in Network and Distributed System Security (NDSS) Symposium
  2022. Code is available at https://ramboattack.github.io/
- **Journal**: None
- **Summary**: Machine learning models are critically susceptible to evasion attacks from adversarial examples. Generally, adversarial examples, modified inputs deceptively similar to the original input, are constructed under whitebox settings by adversaries with full access to the model. However, recent attacks have shown a remarkable reduction in query numbers to craft adversarial examples using blackbox attacks. Particularly, alarming is the ability to exploit the classification decision from the access interface of a trained model provided by a growing number of Machine Learning as a Service providers including Google, Microsoft, IBM and used by a plethora of applications incorporating these models. The ability of an adversary to exploit only the predicted label from a model to craft adversarial examples is distinguished as a decision-based attack. In our study, we first deep dive into recent state-of-the-art decision-based attacks in ICLR and SP to highlight the costly nature of discovering low distortion adversarial employing gradient estimation methods. We develop a robust query efficient attack capable of avoiding entrapment in a local minimum and misdirection from noisy gradients seen in gradient estimation methods. The attack method we propose, RamBoAttack, exploits the notion of Randomized Block Coordinate Descent to explore the hidden classifier manifold, targeting perturbations to manipulate only localized input features to address the issues of gradient estimation methods. Importantly, the RamBoAttack is more robust to the different sample inputs available to an adversary and the targeted class. Overall, for a given target class, RamBoAttack is demonstrated to be more robust at achieving a lower distortion within a given query budget. We curate our extensive results using the large-scale high-resolution ImageNet dataset and open-source our attack, test samples and artifacts on GitHub.



### Image-to-Image Translation-based Data Augmentation for Robust EV Charging Inlet Detection
- **Arxiv ID**: http://arxiv.org/abs/2112.05290v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05290v2)
- **Published**: 2021-12-10 01:47:19+00:00
- **Updated**: 2022-02-03 01:58:51+00:00
- **Authors**: Yeonjun Bang, Yeejin Lee, Byeongkeun Kang
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: This work addresses the task of electric vehicle (EV) charging inlet detection for autonomous EV charging robots. Recently, automated EV charging systems have received huge attention to improve users' experience and to efficiently utilize charging infrastructures and parking lots. However, most related works have focused on system design, robot control, planning, and manipulation. Towards robust EV charging inlet detection, we propose a new dataset (EVCI dataset) and a novel data augmentation method that is based on image-to-image translation where typical image-to-image translation methods synthesize a new image in a different domain given an image. To the best of our knowledge, the EVCI dataset is the first EV charging inlet dataset. For the data augmentation method, we focus on being able to control synthesized images' captured environments (e.g., time, lighting) in an intuitive way. To achieve this, we first propose the environment guide vector that humans can intuitively interpret. We then propose a novel image-to-image translation network that translates a given image towards the environment described by the vector. Accordingly, it aims to synthesize a new image that has the same content as the given image while looking like captured in the provided environment by the environment guide vector. Lastly, we train a detection method using the augmented dataset. Through experiments on the EVCI dataset, we demonstrate that the proposed method outperforms the state-of-the-art methods. We also show that the proposed method is able to control synthesized images using an image and environment guide vectors.



### LCTR: On Awakening the Local Continuity of Transformer for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2112.05291v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05291v1)
- **Published**: 2021-12-10 01:48:40+00:00
- **Updated**: 2021-12-10 01:48:40+00:00
- **Authors**: Zhiwei Chen, Changan Wang, Yabiao Wang, Guannan Jiang, Yunhang Shen, Ying Tai, Chengjie Wang, Wei Zhang, Liujuan Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly supervised object localization (WSOL) aims to learn object localizer solely by using image-level labels. The convolution neural network (CNN) based techniques often result in highlighting the most discriminative part of objects while ignoring the entire object extent. Recently, the transformer architecture has been deployed to WSOL to capture the long-range feature dependencies with self-attention mechanism and multilayer perceptron structure. Nevertheless, transformers lack the locality inductive bias inherent to CNNs and therefore may deteriorate local feature details in WSOL. In this paper, we propose a novel framework built upon the transformer, termed LCTR (Local Continuity TRansformer), which targets at enhancing the local perception capability of global features among long-range feature dependencies. To this end, we propose a relational patch-attention module (RPAM), which considers cross-patch information on a global basis. We further design a cue digging module (CDM), which utilizes local features to guide the learning trend of the model for highlighting the weak local responses. Finally, comprehensive experiments are carried out on two widely used datasets, ie, CUB-200-2011 and ILSVRC, to verify the effectiveness of our method.



### 3D Scene Understanding at Urban Intersection using Stereo Vision and Digital Map
- **Arxiv ID**: http://arxiv.org/abs/2112.05295v1
- **DOI**: 10.1109/VTCSpring.2017.8108283
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05295v1)
- **Published**: 2021-12-10 02:05:15+00:00
- **Updated**: 2021-12-10 02:05:15+00:00
- **Authors**: Prarthana Bhattacharyya, Yanlei Gu, Jiali Bao, Xu Liu, Shunsuke Kamijo
- **Comment**: 6 pages, 6 figures
- **Journal**: 2017 IEEE 85th Vehicular Technology Conference (VTC Spring)
- **Summary**: The driving behavior at urban intersections is very complex. It is thus crucial for autonomous vehicles to comprehensively understand challenging urban traffic scenes in order to navigate intersections and prevent accidents. In this paper, we introduce a stereo vision and 3D digital map based approach to spatially and temporally analyze the traffic situation at urban intersections. Stereo vision is used to detect, classify and track obstacles, while a 3D digital map is used to improve ego-localization and provide context in terms of road-layout information. A probabilistic approach that temporally integrates these geometric, semantic, dynamic and contextual cues is presented. We qualitatively and quantitatively evaluate our proposed technique on real traffic data collected at an urban canyon in Tokyo to demonstrate the efficacy of the system in providing comprehensive awareness of the traffic surroundings.



### IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/2112.05298v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.05298v3)
- **Published**: 2021-12-10 02:10:54+00:00
- **Updated**: 2022-04-04 10:10:47+00:00
- **Authors**: Qi Li, Kaichun Mo, Yanchao Yang, Hang Zhao, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: Building embodied intelligent agents that can interact with 3D indoor environments has received increasing research attention in recent years. While most works focus on single-object or agent-object visual functionality and affordances, our work proposes to study a new kind of visual relationship that is also important to perceive and model -- inter-object functional relationships (e.g., a switch on the wall turns on or off the light, a remote control operates the TV). Humans often spend little or no effort to infer these relationships, even when entering a new room, by using our strong prior knowledge (e.g., we know that buttons control electrical devices) or using only a few exploratory interactions in cases of uncertainty (e.g., multiple switches and lights in the same room). In this paper, we take the first step in building AI system learning inter-object functional relationships in 3D indoor environments with key technical contributions of modeling prior knowledge by training over large-scale scenes and designing interactive policies for effectively exploring the training scenes and quickly adapting to novel test scenes. We create a new benchmark based on the AI2Thor and PartNet datasets and perform extensive experiments that prove the effectiveness of our proposed method. Results show that our model successfully learns priors and fast-interactive-adaptation strategies for exploring inter-object functional relationships in complex 3D scenes. Several ablation studies further validate the usefulness of each proposed module.



### Representing 3D Shapes with Probabilistic Directed Distance Fields
- **Arxiv ID**: http://arxiv.org/abs/2112.05300v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2112.05300v1)
- **Published**: 2021-12-10 02:15:47+00:00
- **Updated**: 2021-12-10 02:15:47+00:00
- **Authors**: Tristan Aumentado-Armstrong, Stavros Tsogkas, Sven Dickinson, Allan Jepson
- **Comment**: 22 pages
- **Journal**: None
- **Summary**: Differentiable rendering is an essential operation in modern vision, allowing inverse graphics approaches to 3D understanding to be utilized in modern machine learning frameworks. Explicit shape representations (voxels, point clouds, or meshes), while relatively easily rendered, often suffer from limited geometric fidelity or topological constraints. On the other hand, implicit representations (occupancy, distance, or radiance fields) preserve greater fidelity, but suffer from complex or inefficient rendering processes, limiting scalability. In this work, we endeavour to address both shortcomings with a novel shape representation that allows fast differentiable rendering within an implicit architecture. Building on implicit distance representations, we define Directed Distance Fields (DDFs), which map an oriented point (position and direction) to surface visibility and depth. Such a field can render a depth map with a single forward pass per pixel, enable differential surface geometry extraction (e.g., surface normals and curvatures) via network derivatives, be easily composed, and permit extraction of classical unsigned distance fields. Using probabilistic DDFs (PDDFs), we show how to model inherent discontinuities in the underlying field. Finally, we apply our method to fitting single shapes, unpaired 3D-aware generative image modelling, and single-image 3D reconstruction tasks, showcasing strong performance with simple architectural components via the versatility of our representation.



### Self-Ensemling for 3D Point Cloud Domain Adaption
- **Arxiv ID**: http://arxiv.org/abs/2112.05301v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05301v2)
- **Published**: 2021-12-10 02:18:09+00:00
- **Updated**: 2023-03-25 03:19:53+00:00
- **Authors**: Qing Li, Xiaojiang Peng, Chuan Yan, Pan Gao, Qi Hao
- **Comment**: None
- **Journal**: None
- **Summary**: Recently 3D point cloud learning has been a hot topic in computer vision and autonomous driving. Due to the fact that it is difficult to manually annotate a qualitative large-scale 3D point cloud dataset, unsupervised domain adaptation (UDA) is popular in 3D point cloud learning which aims to transfer the learned knowledge from the labeled source domain to the unlabeled target domain. However, the generalization and reconstruction errors caused by domain shift with simply-learned model are inevitable which substantially hinder the model's capability from learning good representations. To address these issues, we propose an end-to-end self-ensembling network (SEN) for 3D point cloud domain adaption tasks. Generally, our SEN resorts to the advantages of Mean Teacher and semi-supervised learning, and introduces a soft classification loss and a consistency loss, aiming to achieve consistent generalization and accurate reconstruction. In SEN, a student network is kept in a collaborative manner with supervised learning and self-supervised learning, and a teacher network conducts temporal consistency to learn useful representations and ensure the quality of point clouds reconstruction. Extensive experiments on several 3D point cloud UDA benchmarks show that our SEN outperforms the state-of-the-art methods on both classification and segmentation tasks. Moreover, further analysis demonstrates that our SEN also achieves better reconstruction results.



### Surrogate-based cross-correlation for particle image velocimetry
- **Arxiv ID**: http://arxiv.org/abs/2112.05303v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2112.05303v1)
- **Published**: 2021-12-10 02:45:42+00:00
- **Updated**: 2021-12-10 02:45:42+00:00
- **Authors**: Yong Lee, Fuqiang Gu, Zeyu Gong
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: This paper presents a novel surrogate-based cross-correlation (SBCC) framework to improve the correlation performance between two image signals. The basic idea behind the SBCC is that an optimized surrogate filter/image, supplanting one original image, will produce a more robust and more accurate correlation signal. The cross-correlation estimation of the SBCC is formularized with an objective function composed of surrogate loss and correlation consistency loss. The closed-form solution provides an efficient estimation. To our surprise, the SBCC framework could provide an alternative view to explain a set of generalized cross-correlation (GCC) methods and comprehend the meaning of parameters. With the help of our SBCC framework, we further propose four new specific cross-correlation methods, and provide some suggestions for improving existing GCC methods. A noticeable fact is that the SBCC could enhance the correlation robustness by incorporating other negative context images. Considering the sub-pixel accuracy and robustness requirement of particle image velocimetry (PIV), the contribution of each term in the objective function is investigated with particles' images. Compared with the state-of-the-art baseline methods, the SBCC methods exhibit improved performance (accuracy and robustness) on the synthetic dataset and several challenging real experimental PIV cases.



### Dynamic hardware system for cascade SVM classification of melanoma
- **Arxiv ID**: http://arxiv.org/abs/2112.05322v1
- **DOI**: 10.1007/s00521-018-3656-1
- **Categories**: **cs.AR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.05322v1)
- **Published**: 2021-12-10 03:56:35+00:00
- **Updated**: 2021-12-10 03:56:35+00:00
- **Authors**: Shereen Afifi, Hamid GholamHosseini, Roopak Sinha
- **Comment**: Journal paper, 9 pages, 4 figures, 4 tables
- **Journal**: Neural Computing & Applications 32 (2020) pp.1777-1788
- **Summary**: Melanoma is the most dangerous form of skin cancer, which is responsible for the majority of skin cancer-related deaths. Early diagnosis of melanoma can significantly reduce mortality rates and treatment costs. Therefore, skin cancer specialists are using image-based diagnostic tools for detecting melanoma earlier. We aim to develop a handheld device featured with low cost and high performance to enhance early detection of melanoma at the primary healthcare. But, developing this device is very challenging due to the complicated computations required by the embedded diagnosis system. Thus, we aim to exploit the recent hardware technology in reconfigurable computing to achieve a high-performance embedded system at low cost. Support vector machine (SVM) is a common classifier that shows high accuracy for classifying melanoma within the diagnosis system and is considered as the most compute-intensive task in the system. In this paper, we propose a dynamic hardware system for implementing a cascade SVM classifier on FPGA for early melanoma detection. A multi-core architecture is proposed to implement a two-stage cascade classifier using two classifiers with accuracies of 98% and 73%. The hardware implementation results were optimized by using the dynamic partial reconfiguration technology, where very low resource utilization of 1% slices and power consumption of 1.5 W were achieved. Consequently, the implemented dynamic hardware system meets vital embedded system constraints of high performance and low cost, resource utilization, and power consumption, while achieving efficient classification with high accuracy.



### Attention-based Transformation from Latent Features to Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2112.05324v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05324v1)
- **Published**: 2021-12-10 03:59:04+00:00
- **Updated**: 2021-12-10 03:59:04+00:00
- **Authors**: Kaiyi Zhang, Ximing Yang, Yuan Wu, Cheng Jin
- **Comment**: 9 pages, 7 figures, AAAI 2022
- **Journal**: None
- **Summary**: In point cloud generation and completion, previous methods for transforming latent features to point clouds are generally based on fully connected layers (FC-based) or folding operations (Folding-based). However, point clouds generated by FC-based methods are usually troubled by outliers and rough surfaces. For folding-based methods, their data flow is large, convergence speed is slow, and they are also hard to handle the generation of non-smooth surfaces. In this work, we propose AXform, an attention-based method to transform latent features to point clouds. AXform first generates points in an interim space, using a fully connected layer. These interim points are then aggregated to generate the target point cloud. AXform takes both parameter sharing and data flow into account, which makes it has fewer outliers, fewer network parameters, and a faster convergence speed. The points generated by AXform do not have the strong 2-manifold constraint, which improves the generation of non-smooth surfaces. When AXform is expanded to multiple branches for local generations, the centripetal constraint makes it has properties of self-clustering and space consistency, which further enables unsupervised semantic segmentation. We also adopt this scheme and design AXformNet for point cloud completion. Considerable experiments on different datasets show that our methods achieve state-of-the-art results.



### PyTorch Connectomics: A Scalable and Flexible Segmentation Framework for EM Connectomics
- **Arxiv ID**: http://arxiv.org/abs/2112.05754v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2112.05754v1)
- **Published**: 2021-12-10 04:02:23+00:00
- **Updated**: 2021-12-10 04:02:23+00:00
- **Authors**: Zudi Lin, Donglai Wei, Jeff Lichtman, Hanspeter Pfister
- **Comment**: Technical report
- **Journal**: None
- **Summary**: We present PyTorch Connectomics (PyTC), an open-source deep-learning framework for the semantic and instance segmentation of volumetric microscopy images, built upon PyTorch. We demonstrate the effectiveness of PyTC in the field of connectomics, which aims to segment and reconstruct neurons, synapses, and other organelles like mitochondria at nanometer resolution for understanding neuronal communication, metabolism, and development in animal brains. PyTC is a scalable and flexible toolbox that tackles datasets at different scales and supports multi-task and semi-supervised learning to better exploit expensive expert annotations and the vast amount of unlabeled data during training. Those functionalities can be easily realized in PyTC by changing the configuration options without coding and adapted to other 2D and 3D segmentation tasks for different tissues and imaging modalities. Quantitatively, our framework achieves the best performance in the CREMI challenge for synaptic cleft segmentation (outperforms existing best result by relatively 6.1$\%$) and competitive performance on mitochondria and neuronal nuclei segmentation. Code and tutorials are publicly available at https://connectomics.readthedocs.io.



### FaceFormer: Speech-Driven 3D Facial Animation with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2112.05329v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2112.05329v4)
- **Published**: 2021-12-10 04:21:59+00:00
- **Updated**: 2022-03-17 00:51:05+00:00
- **Authors**: Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping Wang, Taku Komura
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Speech-driven 3D facial animation is challenging due to the complex geometry of human faces and the limited availability of 3D audio-visual data. Prior works typically focus on learning phoneme-level features of short audio windows with limited context, occasionally resulting in inaccurate lip movements. To tackle this limitation, we propose a Transformer-based autoregressive model, FaceFormer, which encodes the long-term audio context and autoregressively predicts a sequence of animated 3D face meshes. To cope with the data scarcity issue, we integrate the self-supervised pre-trained speech representations. Also, we devise two biased attention mechanisms well suited to this specific task, including the biased cross-modal multi-head (MH) attention and the biased causal MH self-attention with a periodic positional encoding strategy. The former effectively aligns the audio-motion modalities, whereas the latter offers abilities to generalize to longer audio sequences. Extensive experiments and a perceptual user study show that our approach outperforms the existing state-of-the-arts. The code will be made available.



### Uncertainty, Edge, and Reverse-Attention Guided Generative Adversarial Network for Automatic Building Detection in Remotely Sensed Images
- **Arxiv ID**: http://arxiv.org/abs/2112.05335v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.05335v1)
- **Published**: 2021-12-10 04:45:24+00:00
- **Updated**: 2021-12-10 04:45:24+00:00
- **Authors**: Somrita Chattopadhyay, Avinash C. Kak
- **Comment**: 23 pages
- **Journal**: None
- **Summary**: Despite recent advances in deep-learning based semantic segmentation, automatic building detection from remotely sensed imagery is still a challenging problem owing to large variability in the appearance of buildings across the globe. The errors occur mostly around the boundaries of the building footprints, in shadow areas, and when detecting buildings whose exterior surfaces have reflectivity properties that are very similar to those of the surrounding regions. To overcome these problems, we propose a generative adversarial network based segmentation framework with uncertainty attention unit and refinement module embedded in the generator. The refinement module, composed of edge and reverse attention units, is designed to refine the predicted building map. The edge attention enhances the boundary features to estimate building boundaries with greater precision, and the reverse attention allows the network to explore the features missing in the previously estimated regions. The uncertainty attention unit assists the network in resolving uncertainties in classification. As a measure of the power of our approach, as of December 4, 2021, it ranks at the second place on DeepGlobe's public leaderboard despite the fact that main focus of our approach -- refinement of the building edges -- does not align exactly with the metrics used for leaderboard rankings. Our overall F1-score on DeepGlobe's challenging dataset is 0.745. We also report improvements on the previous-best results for the challenging INRIA Validation Dataset for which our network achieves an overall IoU of 81.28% and an overall accuracy of 97.03%. Along the same lines, for the official INRIA Test Dataset, our network scores 77.86% and 96.41% in overall IoU and accuracy.



### Tradeoffs Between Contrastive and Supervised Learning: An Empirical Study
- **Arxiv ID**: http://arxiv.org/abs/2112.05340v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.05340v1)
- **Published**: 2021-12-10 05:19:32+00:00
- **Updated**: 2021-12-10 05:19:32+00:00
- **Authors**: Ananya Karthik, Mike Wu, Noah Goodman, Alex Tamkin
- **Comment**: NeurIPS 2021 Workshop: Self-Supervised Learning - Theory and Practice
- **Journal**: None
- **Summary**: Contrastive learning has made considerable progress in computer vision, outperforming supervised pretraining on a range of downstream datasets. However, is contrastive learning the better choice in all situations? We demonstrate two cases where it is not. First, under sufficiently small pretraining budgets, supervised pretraining on ImageNet consistently outperforms a comparable contrastive model on eight diverse image classification datasets. This suggests that the common practice of comparing pretraining approaches at hundreds or thousands of epochs may not produce actionable insights for those with more limited compute budgets. Second, even with larger pretraining budgets we identify tasks where supervised learning prevails, perhaps because the object-centric bias of supervised pretraining makes the model more resilient to common corruptions and spurious foreground-background correlations. These results underscore the need to characterize tradeoffs of different pretraining objectives across a wider range of contexts and training regimes.



### Hyperdimensional Feature Fusion for Out-Of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2112.05341v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.05341v3)
- **Published**: 2021-12-10 05:21:18+00:00
- **Updated**: 2022-08-29 23:50:23+00:00
- **Authors**: Samuel Wilson, Tobias Fischer, Niko Sünderhauf, Feras Dayoub
- **Comment**: Accepted to WACV2023
- **Journal**: None
- **Summary**: We introduce powerful ideas from Hyperdimensional Computing into the challenging field of Out-of-Distribution (OOD) detection. In contrast to most existing work that performs OOD detection based on only a single layer of a neural network, we use similarity-preserving semi-orthogonal projection matrices to project the feature maps from multiple layers into a common vector space. By repeatedly applying the bundling operation $\oplus$, we create expressive class-specific descriptor vectors for all in-distribution classes. At test time, a simple and efficient cosine similarity calculation between descriptor vectors consistently identifies OOD samples with better performance than the current state-of-the-art. We show that the hyperdimensional fusion of multiple network layers is critical to achieve best general performance.



### Information Prebuilt Recurrent Reconstruction Network for Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2112.05755v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.05755v4)
- **Published**: 2021-12-10 05:32:23+00:00
- **Updated**: 2023-02-02 09:15:48+00:00
- **Authors**: Shuyun Wang, Ming Yu, Cuihong Xue, Yingchun Guo, Gang Yan
- **Comment**: 12 pages,9 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
- **Journal**: None
- **Summary**: The video super-resolution (VSR) method based on the recurrent convolutional network has strong temporal modeling capability for video sequences. However, the temporal receptive field of different recurrent units in the unidirectional recurrent network is unbalanced. Earlier reconstruction frames receive less spatio-temporal information, resulting in fuzziness or artifacts. Although the bidirectional recurrent network can alleviate this problem, it requires more memory space and fails to perform many tasks with low latency requirements. To solve the above problems, we propose an end-to-end information prebuilt recurrent reconstruction network (IPRRN), consisting of an information prebuilt network (IPNet) and a recurrent reconstruction network (RRNet). By integrating sufficient information from the front of the video to build the hidden state needed for the initially recurrent unit to help restore the earlier frames, the information prebuilt network balances the input information difference at different time steps. In addition, we demonstrate an efficient recurrent reconstruction network, which outperforms the existing unidirectional recurrent schemes in all aspects. Many experiments have verified the effectiveness of the network we propose, which can effectively achieve better quantitative and qualitative evaluation performance compared to the existing state-of-the-art methods.



### Enhancing Multi-Scale Implicit Learning in Image Super-Resolution with Integrated Positional Encoding
- **Arxiv ID**: http://arxiv.org/abs/2112.05756v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.05756v1)
- **Published**: 2021-12-10 06:09:55+00:00
- **Updated**: 2021-12-10 06:09:55+00:00
- **Authors**: Ying-Tian Liu, Yuan-Chen Guo, Song-Hai Zhang
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Is the center position fully capable of representing a pixel? There is nothing wrong to represent pixels with their centers in a discrete image representation, but it makes more sense to consider each pixel as the aggregation of signals from a local area in an image super-resolution (SR) context. Despite the great capability of coordinate-based implicit representation in the field of arbitrary-scale image SR, this area's nature of pixels is not fully considered. To this end, we propose integrated positional encoding (IPE), extending traditional positional encoding by aggregating frequency information over the pixel area. We apply IPE to the state-of-the-art arbitrary-scale image super-resolution method: local implicit image function (LIIF), presenting IPE-LIIF. We show the effectiveness of IPE-LIIF by quantitative and qualitative evaluations, and further demonstrate the generalization ability of IPE to larger image scales and multiple implicit-based methods. Code will be released.



### Exploring Pixel-level Self-supervision for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.05351v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05351v1)
- **Published**: 2021-12-10 06:13:45+00:00
- **Updated**: 2021-12-10 06:13:45+00:00
- **Authors**: Sung-Hoon Yoon, Hyeokjun Kweon, Jaeseok Jeong, Hyeonseong Kim, Shinjeong Kim, Kuk-Jin Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: Existing studies in weakly supervised semantic segmentation (WSSS) have utilized class activation maps (CAMs) to localize the class objects. However, since a classification loss is insufficient for providing precise object regions, CAMs tend to be biased towards discriminative patterns (i.e., sparseness) and do not provide precise object boundary information (i.e., impreciseness). To resolve these limitations, we propose a novel framework (composed of MainNet and SupportNet.) that derives pixel-level self-supervision from given image-level supervision. In our framework, with the help of the proposed Regional Contrastive Module (RCM) and Multi-scale Attentive Module (MAM), MainNet is trained by self-supervision from the SupportNet. The RCM extracts two forms of self-supervision from SupportNet: (1) class region masks generated from the CAMs and (2) class-wise prototypes obtained from the features according to the class region masks. Then, every pixel-wise feature of the MainNet is trained by the prototype in a contrastive manner, sharpening the resulting CAMs. The MAM utilizes CAMs inferred at multiple scales from the SupportNet as self-supervision to guide the MainNet. Based on the dissimilarity between the multi-scale CAMs from MainNet and SupportNet, CAMs from the MainNet are trained to expand to the less-discriminative regions. The proposed method shows state-of-the-art WSSS performance both on the train and validation sets on the PASCAL VOC 2012 dataset. For reproducibility, code will be available publicly soon.



### Learning to Learn Transferable Attack
- **Arxiv ID**: http://arxiv.org/abs/2112.06658v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.06658v1)
- **Published**: 2021-12-10 07:24:21+00:00
- **Updated**: 2021-12-10 07:24:21+00:00
- **Authors**: Shuman Fang, Jie Li, Xianming Lin, Rongrong Ji
- **Comment**: AAAI 2022
- **Journal**: None
- **Summary**: Transfer adversarial attack is a non-trivial black-box adversarial attack that aims to craft adversarial perturbations on the surrogate model and then apply such perturbations to the victim model. However, the transferability of perturbations from existing methods is still limited, since the adversarial perturbations are easily overfitting with a single surrogate model and specific data pattern. In this paper, we propose a Learning to Learn Transferable Attack (LLTA) method, which makes the adversarial perturbations more generalized via learning from both data and model augmentation. For data augmentation, we adopt simple random resizing and padding. For model augmentation, we randomly alter the back propagation instead of the forward propagation to eliminate the effect on the model prediction. By treating the attack of both specific data and a modified model as a task, we expect the adversarial perturbations to adopt enough tasks for generalization. To this end, the meta-learning algorithm is further introduced during the iteration of perturbation generation. Empirical results on the widely-used dataset demonstrate the effectiveness of our attack method with a 12.85% higher success rate of transfer attack compared with the state-of-the-art methods. We also evaluate our method on the real-world online system, i.e., Google Cloud Vision API, to further show the practical potentials of our method.



### Rethinking the Two-Stage Framework for Grounded Situation Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.05375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05375v1)
- **Published**: 2021-12-10 08:10:56+00:00
- **Updated**: 2021-12-10 08:10:56+00:00
- **Authors**: Meng Wei, Long Chen, Wei Ji, Xiaoyu Yue, Tat-Seng Chua
- **Comment**: Accepted by AAAI 2022
- **Journal**: None
- **Summary**: Grounded Situation Recognition (GSR), i.e., recognizing the salient activity (or verb) category in an image (e.g., buying) and detecting all corresponding semantic roles (e.g., agent and goods), is an essential step towards "human-like" event understanding. Since each verb is associated with a specific set of semantic roles, all existing GSR methods resort to a two-stage framework: predicting the verb in the first stage and detecting the semantic roles in the second stage. However, there are obvious drawbacks in both stages: 1) The widely-used cross-entropy (XE) loss for object recognition is insufficient in verb classification due to the large intra-class variation and high inter-class similarity among daily activities. 2) All semantic roles are detected in an autoregressive manner, which fails to model the complex semantic relations between different roles. To this end, we propose a novel SituFormer for GSR which consists of a Coarse-to-Fine Verb Model (CFVM) and a Transformer-based Noun Model (TNM). CFVM is a two-step verb prediction model: a coarse-grained model trained with XE loss first proposes a set of verb candidates, and then a fine-grained model trained with triplet loss re-ranks these candidates with enhanced verb features (not only separable but also discriminative). TNM is a transformer-based semantic role detection model, which detects all roles parallelly. Owing to the global relation modeling ability and flexibility of the transformer decoder, TNM can fully explore the statistical dependency of the roles. Extensive validations on the challenging SWiG benchmark show that SituFormer achieves a new state-of-the-art performance with significant gains under various metrics. Code is available at https://github.com/kellyiss/SituFormer.



### Cross-Modal Transferable Adversarial Attacks from Images to Videos
- **Arxiv ID**: http://arxiv.org/abs/2112.05379v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.05379v1)
- **Published**: 2021-12-10 08:19:03+00:00
- **Updated**: 2021-12-10 08:19:03+00:00
- **Authors**: Zhipeng Wei, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies have shown that adversarial examples hand-crafted on one white-box model can be used to attack other black-box models. Such cross-model transferability makes it feasible to perform black-box attacks, which has raised security concerns for real-world DNNs applications. Nevertheless, existing works mostly focus on investigating the adversarial transferability across different deep models that share the same modality of input data. The cross-modal transferability of adversarial perturbation has never been explored. This paper investigates the transferability of adversarial perturbation across different modalities, i.e., leveraging adversarial perturbation generated on white-box image models to attack black-box video models. Specifically, motivated by the observation that the low-level feature space between images and video frames are similar, we propose a simple yet effective cross-modal attack method, named as Image To Video (I2V) attack. I2V generates adversarial frames by minimizing the cosine similarity between features of pre-trained image models from adversarial and benign examples, then combines the generated adversarial frames to perform black-box attacks on video recognition models. Extensive experiments demonstrate that I2V can achieve high attack success rates on different black-box video recognition models. On Kinetics-400 and UCF-101, I2V achieves an average attack success rate of 77.88% and 65.68%, respectively, which sheds light on the feasibility of cross-modal adversarial attacks.



### UNIST: Unpaired Neural Implicit Shape Translation Network
- **Arxiv ID**: http://arxiv.org/abs/2112.05381v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2112.05381v2)
- **Published**: 2021-12-10 08:24:20+00:00
- **Updated**: 2022-03-30 16:30:56+00:00
- **Authors**: Qimin Chen, Johannes Merz, Aditya Sanghi, Hooman Shayani, Ali Mahdavi-Amiri, Hao Zhang
- **Comment**: CVPR 2022. project page: https://qiminchen.github.io/unist/
- **Journal**: None
- **Summary**: We introduce UNIST, the first deep neural implicit model for general-purpose, unpaired shape-to-shape translation, in both 2D and 3D domains. Our model is built on autoencoding implicit fields, rather than point clouds which represents the state of the art. Furthermore, our translation network is trained to perform the task over a latent grid representation which combines the merits of both latent-space processing and position awareness, to not only enable drastic shape transforms but also well preserve spatial features and fine local details for natural shape translations. With the same network architecture and only dictated by the input domain pairs, our model can learn both style-preserving content alteration and content-preserving style transfer. We demonstrate the generality and quality of the translation results, and compare them to well-known baselines. Code is available at https://qiminchen.github.io/unist/.



### Towards Full-to-Empty Room Generation with Structure-Aware Feature Encoding and Soft Semantic Region-Adaptive Normalization
- **Arxiv ID**: http://arxiv.org/abs/2112.05396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05396v1)
- **Published**: 2021-12-10 09:00:13+00:00
- **Updated**: 2021-12-10 09:00:13+00:00
- **Authors**: Vasileios Gkitsas, Nikolaos Zioulis, Vladimiros Sterzentsenko, Alexandros Doumanoglou, Dimitrios Zarpalas
- **Comment**: None
- **Journal**: None
- **Summary**: The task of transforming a furnished room image into a background-only is extremely challenging since it requires making large changes regarding the scene context while still preserving the overall layout and style. In order to acquire photo-realistic and structural consistent background, existing deep learning methods either employ image inpainting approaches or incorporate the learning of the scene layout as an individual task and leverage it later in a not fully differentiable semantic region-adaptive normalization module. To tackle these drawbacks, we treat scene layout generation as a feature linear transformation problem and propose a simple yet effective adjusted fully differentiable soft semantic region-adaptive normalization module (softSEAN) block. We showcase the applicability in diminished reality and depth estimation tasks, where our approach besides the advantages of mitigating training complexity and non-differentiability issues, surpasses the compared methods both quantitatively and qualitatively. Our softSEAN block can be used as a drop-in module for existing discriminative and generative models. Implementation is available on vcl3d.github.io/PanoDR/.



### The Large Labelled Logo Dataset (L3D): A Multipurpose and Hand-Labelled Continuously Growing Dataset
- **Arxiv ID**: http://arxiv.org/abs/2112.05404v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.05404v1)
- **Published**: 2021-12-10 09:21:44+00:00
- **Updated**: 2021-12-10 09:21:44+00:00
- **Authors**: Asier Gutiérrez-Fandiño, David Pérez-Fernández, Jordi Armengol-Estapé
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present the Large Labelled Logo Dataset (L3D), a multipurpose, hand-labelled, continuously growing dataset. It is composed of around 770k of color 256x256 RGB images extracted from the European Union Intellectual Property Office (EUIPO) open registry. Each of them is associated to multiple labels that classify the figurative and textual elements that appear in the images. These annotations have been classified by the EUIPO evaluators using the Vienna classification, a hierarchical classification of figurative marks. We suggest two direct applications of this dataset, namely, logo classification and logo generation.



### Multimedia Datasets for Anomaly Detection: A Review
- **Arxiv ID**: http://arxiv.org/abs/2112.05410v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2112.05410v3)
- **Published**: 2021-12-10 09:32:21+00:00
- **Updated**: 2022-04-04 04:39:52+00:00
- **Authors**: Pratibha Kumari, Anterpreet Kaur Bedi, Mukesh Saini
- **Comment**: 17 pages, 11 figures, 8 tables
- **Journal**: None
- **Summary**: Multimedia anomaly datasets play a crucial role in automated surveillance. They have a wide range of applications expanding from outlier objects/ situation detection to the detection of life-threatening events. For more than 1.5 decades, this field has attracted a lot of research attention, and as a result, more and more datasets dedicated to anomalous actions and object detection have been developed. Tapping these public anomaly datasets enable researchers to generate and compare various anomaly detection frameworks with the same input data. This paper presents a comprehensive survey on a variety of video, audio, as well as audio-visual datasets based on the application of anomaly detection. This survey aims to address the lack of a comprehensive comparison and analysis of multimedia public datasets based on anomaly detection. Also, it can assist researchers in selecting the best available dataset for bench-marking frameworks. Additionally, we discuss gaps in the existing dataset and insights for future direction towards developing multimodal anomaly detection datasets.



### Optimizing Edge Detection for Image Segmentation with Multicut Penalties
- **Arxiv ID**: http://arxiv.org/abs/2112.05416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05416v1)
- **Published**: 2021-12-10 09:48:03+00:00
- **Updated**: 2021-12-10 09:48:03+00:00
- **Authors**: Steffen Jung, Sebastian Ziegler, Amirhossein Kardoost, Margret Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: The Minimum Cost Multicut Problem (MP) is a popular way for obtaining a graph decomposition by optimizing binary edge labels over edge costs. While the formulation of a MP from independently estimated costs per edge is highly flexible and intuitive, solving the MP is NP-hard and time-expensive. As a remedy, recent work proposed to predict edge probabilities with awareness to potential conflicts by incorporating cycle constraints in the prediction process. We argue that such formulation, while providing a first step towards end-to-end learnable edge weights, is suboptimal, since it is built upon a loose relaxation of the MP. We therefore propose an adaptive CRF that allows to progressively consider more violated constraints and, in consequence, to issue solutions with higher validity. Experiments on the BSDS500 benchmark for natural image segmentation as well as on electron microscopic recordings show that our approach yields more precise edge detection and image segmentation.



### Predicting Physical World Destinations for Commands Given to Self-Driving Cars
- **Arxiv ID**: http://arxiv.org/abs/2112.05419v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.05419v1)
- **Published**: 2021-12-10 09:51:16+00:00
- **Updated**: 2021-12-10 09:51:16+00:00
- **Authors**: Dusan Grujicic, Thierry Deruyttere, Marie-Francine Moens, Matthew Blaschko
- **Comment**: Accepted at AAAI 2022. First two authors have contributed equally.
  Extended camera-ready version including the appendix and references to it in
  the main text
- **Journal**: None
- **Summary**: In recent years, we have seen significant steps taken in the development of self-driving cars. Multiple companies are starting to roll out impressive systems that work in a variety of settings. These systems can sometimes give the impression that full self-driving is just around the corner and that we would soon build cars without even a steering wheel. The increase in the level of autonomy and control given to an AI provides an opportunity for new modes of human-vehicle interaction. However, surveys have shown that giving more control to an AI in self-driving cars is accompanied by a degree of uneasiness by passengers. In an attempt to alleviate this issue, recent works have taken a natural language-oriented approach by allowing the passenger to give commands that refer to specific objects in the visual scene. Nevertheless, this is only half the task as the car should also understand the physical destination of the command, which is what we focus on in this paper. We propose an extension in which we annotate the 3D destination that the car needs to reach after executing the given command and evaluate multiple different baselines on predicting this destination location. Additionally, we introduce a model that outperforms the prior works adapted for this particular setting.



### Couplformer:Rethinking Vision Transformer with Coupling Attention Map
- **Arxiv ID**: http://arxiv.org/abs/2112.05425v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05425v1)
- **Published**: 2021-12-10 10:05:35+00:00
- **Updated**: 2021-12-10 10:05:35+00:00
- **Authors**: Hai Lan, Xihao Wang, Xian Wei
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: With the development of the self-attention mechanism, the Transformer model has demonstrated its outstanding performance in the computer vision domain. However, the massive computation brought from the full attention mechanism became a heavy burden for memory consumption. Sequentially, the limitation of memory reduces the possibility of improving the Transformer model. To remedy this problem, we propose a novel memory economy attention mechanism named Couplformer, which decouples the attention map into two sub-matrices and generates the alignment scores from spatial information. A series of different scale image classification tasks are applied to evaluate the effectiveness of our model. The result of experiments shows that on the ImageNet-1k classification task, the Couplformer can significantly decrease 28% memory consumption compared with regular Transformer while accessing sufficient accuracy requirements and outperforming 0.92% on Top-1 accuracy while occupying the same memory footprint. As a result, the Couplformer can serve as an efficient backbone in visual tasks, and provide a novel perspective on the attention mechanism for researchers.



### Edge-Enhanced Dual Discriminator Generative Adversarial Network for Fast MRI with Parallel Imaging Using Multi-view Information
- **Arxiv ID**: http://arxiv.org/abs/2112.05758v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.05758v1)
- **Published**: 2021-12-10 10:49:26+00:00
- **Updated**: 2021-12-10 10:49:26+00:00
- **Authors**: Jiahao Huang, Weiping Ding, Jun Lv, Jingwen Yang, Hao Dong, Javier Del Ser, Jun Xia, Tiaojuan Ren, Stephen Wong, Guang Yang
- **Comment**: 33 pages, 13 figures, Applied Intelligence
- **Journal**: None
- **Summary**: In clinical medicine, magnetic resonance imaging (MRI) is one of the most important tools for diagnosis, triage, prognosis, and treatment planning. However, MRI suffers from an inherent slow data acquisition process because data is collected sequentially in k-space. In recent years, most MRI reconstruction methods proposed in the literature focus on holistic image reconstruction rather than enhancing the edge information. This work steps aside this general trend by elaborating on the enhancement of edge information. Specifically, we introduce a novel parallel imaging coupled dual discriminator generative adversarial network (PIDD-GAN) for fast multi-channel MRI reconstruction by incorporating multi-view information. The dual discriminator design aims to improve the edge information in MRI reconstruction. One discriminator is used for holistic image reconstruction, whereas the other one is responsible for enhancing edge information. An improved U-Net with local and global residual learning is proposed for the generator. Frequency channel attention blocks (FCA Blocks) are embedded in the generator for incorporating attention mechanisms. Content loss is introduced to train the generator for better reconstruction quality. We performed comprehensive experiments on Calgary-Campinas public brain MR dataset and compared our method with state-of-the-art MRI reconstruction methods. Ablation studies of residual learning were conducted on the MICCAI13 dataset to validate the proposed modules. Results show that our PIDD-GAN provides high-quality reconstructed MR images, with well-preserved edge information. The time of single-image reconstruction is below 5ms, which meets the demand of faster processing.



### Monitoring and Adapting the Physical State of a Camera for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2112.05456v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.05456v2)
- **Published**: 2021-12-10 11:14:44+00:00
- **Updated**: 2023-02-10 08:39:54+00:00
- **Authors**: Maik Wischow, Guillermo Gallego, Ines Ernst, Anko Börner
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous vehicles and robots require increasingly more robustness and reliability to meet the demands of modern tasks. These requirements specially apply to cameras onboard such vehicles because they are the predominant sensors to acquire information about the environment and support actions. Cameras must maintain proper functionality and take automatic countermeasures if necessary. However, few works examine the practical use of a general condition monitoring approach for cameras and designs countermeasures in the context of an envisaged high-level application. We propose a generic and interpretable self-health-maintenance framework for cameras based on data- and physically-grounded models. To this end, we determine two reliable, real-time capable estimators for typical image effects of a camera in poor condition (blur, noise phenomena and most common combinations) by comparing traditional and retrained machine learning-based approaches in extensive experiments. Furthermore, we demonstrate on a real-world ground vehicle how one can adjust the camera parameters to achieve optimal whole-system capability based on experimental (non-linear and non-monotonic) input-output performance curves, using object detection, motion blur and sensor noise as examples. Our framework not only provides a practical ready-to-use solution to evaluate and maintain the health of cameras, but can also serve as a basis for extensions to tackle more sophisticated problems that combine additional data sources (e.g., sensor or environment parameters) empirically in order to attain fully reliable and robust machines.



### Critical configurations for three projective views
- **Arxiv ID**: http://arxiv.org/abs/2112.05478v3
- **DOI**: None
- **Categories**: **math.AG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.05478v3)
- **Published**: 2021-12-10 12:12:00+00:00
- **Updated**: 2023-06-21 11:06:04+00:00
- **Authors**: Martin Bråtelund
- **Comment**: 40 pages, 9 figures. This is a companion paper to arXiv:2112.05074.
  Accepted manuscript, to appear in Mathematica Scandinavica
- **Journal**: None
- **Summary**: The problem of structure from motion is concerned with recovering the 3-dimensional structure of an object from a set of 2-dimensional images taken by unknown cameras. Generally, all information can be uniquely recovered if enough images and point correspondences are provided, yet there are certain cases where unique recovery is impossible; these are called critical configurations. We use an algebraic approach to study the critical configurations for three projective cameras. We show that all critical configurations lie on the intersection of quadric surfaces, and classify exactly which intersections constitute a critical configuration.



### Visual Transformers with Primal Object Queries for Multi-Label Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2112.05485v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05485v2)
- **Published**: 2021-12-10 12:29:07+00:00
- **Updated**: 2022-05-15 10:21:09+00:00
- **Authors**: Vacit Oguz Yazici, Joost van de Weijer, Longlong Yu
- **Comment**: Accepted to ICPR 2022
- **Journal**: None
- **Summary**: Multi-label image classification is about predicting a set of class labels that can be considered as orderless sequential data. Transformers process the sequential data as a whole, therefore they are inherently good at set prediction. The first vision-based transformer model, which was proposed for the object detection task introduced the concept of object queries. Object queries are learnable positional encodings that are used by attention modules in decoder layers to decode the object classes or bounding boxes using the region of interests in an image. However, inputting the same set of object queries to different decoder layers hinders the training: it results in lower performance and delays convergence. In this paper, we propose the usage of primal object queries that are only provided at the start of the transformer decoder stack. In addition, we improve the mixup technique proposed for multi-label classification. The proposed transformer model with primal object queries improves the state-of-the-art class wise F1 metric by 2.1% and 1.8%; and speeds up the convergence by 79.0% and 38.6% on MS-COCO and NUS-WIDE datasets respectively.



### DronePose: The identification, segmentation, and orientation detection of drones via neural networks
- **Arxiv ID**: http://arxiv.org/abs/2112.05488v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05488v1)
- **Published**: 2021-12-10 12:34:53+00:00
- **Updated**: 2021-12-10 12:34:53+00:00
- **Authors**: Stirling Scholes, Alice Ruget, German Mora-Martin, Feng Zhu, Istvan Gyongy, Jonathan Leach
- **Comment**: None
- **Journal**: None
- **Summary**: The growing ubiquity of drones has raised concerns over the ability of traditional air-space monitoring technologies to accurately characterise such vehicles. Here, we present a CNN using a decision tree and ensemble structure to fully characterise drones in flight. Our system determines the drone type, orientation (in terms of pitch, roll, and yaw), and performs segmentation to classify different body parts (engines, body, and camera). We also provide a computer model for the rapid generation of large quantities of accurately labelled photo-realistic training data and demonstrate that this data is of sufficient fidelity to allow the system to accurately characterise real drones in flight. Our network will provide a valuable tool in the image processing chain where it may build upon existing drone detection technologies to provide complete drone characterisation over wide areas.



### Network Compression via Central Filter
- **Arxiv ID**: http://arxiv.org/abs/2112.05493v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.05493v2)
- **Published**: 2021-12-10 12:51:04+00:00
- **Updated**: 2021-12-13 05:23:18+00:00
- **Authors**: Yuanzhi Duan, Xiaofang Hu, Yue Zhou, Qiang Liu, Shukai Duan
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network pruning has remarkable performance for reducing the complexity of deep network models. Recent network pruning methods usually focused on removing unimportant or redundant filters in the network. In this paper, by exploring the similarities between feature maps, we propose a novel filter pruning method, Central Filter (CF), which suggests that a filter is approximately equal to a set of other filters after appropriate adjustments. Our method is based on the discovery that the average similarity between feature maps changes very little, regardless of the number of input images. Based on this finding, we establish similarity graphs on feature maps and calculate the closeness centrality of each node to select the Central Filter. Moreover, we design a method to directly adjust weights in the next layer corresponding to the Central Filter, effectively minimizing the error caused by pruning. Through experiments on various benchmark networks and datasets, CF yields state-of-the-art performance. For example, with ResNet-56, CF reduces approximately 39.7% of FLOPs by removing 47.1% of the parameters, with even 0.33% accuracy improvement on CIFAR-10. With GoogLeNet, CF reduces approximately 63.2% of FLOPs by removing 55.6% of the parameters, with only a small loss of 0.35% in top-1 accuracy on CIFAR-10. With ResNet-50, CF reduces approximately 47.9% of FLOPs by removing 36.9% of the parameters, with only a small loss of 1.07% in top-1 accuracy on ImageNet. The codes can be available at https://github.com/8ubpshLR23/Central-Filter.



### Graph-based Generative Face Anonymisation with Pose Preservation
- **Arxiv ID**: http://arxiv.org/abs/2112.05496v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05496v2)
- **Published**: 2021-12-10 12:58:17+00:00
- **Updated**: 2022-03-24 08:39:32+00:00
- **Authors**: Nicola Dall'Asen, Yiming Wang, Hao Tang, Luca Zanella, Elisa Ricci
- **Comment**: 21st International Conference on Image analysis and Processing
- **Journal**: None
- **Summary**: We propose AnonyGAN, a GAN-based solution for face anonymisation which replaces the visual information corresponding to a source identity with a condition identity provided as any single image. With the goal to maintain the geometric attributes of the source face, i.e., the facial pose and expression, and to promote more natural face generation, we propose to exploit a Bipartite Graph to explicitly model the relations between the facial landmarks of the source identity and the ones of the condition identity through a deep model. We further propose a landmark attention model to relax the manual selection of facial landmarks, allowing the network to weight the landmarks for the best visual naturalness and pose preservation. Finally, to facilitate the appearance learning, we propose a hybrid training strategy to address the challenge caused by the lack of direct pixel-level supervision. We evaluate our method and its variants on two public datasets, CelebA and LFW, in terms of visual naturalness, facial pose preservation and of its impacts on face detection and re-identification. We prove that AnonyGAN significantly outperforms the state-of-the-art methods in terms of visual naturalness, face detection and pose preservation.



### Sparse Depth Completion with Semantic Mesh Deformation Optimization
- **Arxiv ID**: http://arxiv.org/abs/2112.05498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05498v1)
- **Published**: 2021-12-10 13:01:06+00:00
- **Updated**: 2021-12-10 13:01:06+00:00
- **Authors**: Bing Zhou, Matias Aiskovich, Sinem Guven
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse depth measurements are widely available in many applications such as augmented reality, visual inertial odometry and robots equipped with low cost depth sensors. Although such sparse depth samples work well for certain applications like motion tracking, a complete depth map is usually preferred for broader applications, such as 3D object recognition, 3D reconstruction and autonomous driving. Despite the recent advancements in depth prediction from single RGB images with deeper neural networks, the existing approaches do not yield reliable results for practical use. In this work, we propose a neural network with post-optimization, which takes an RGB image and sparse depth samples as input and predicts the complete depth map. We make three major contributions to advance the state-of-the-art: an improved backbone network architecture named EDNet, a semantic edge-weighted loss function and a semantic mesh deformation optimization method. Our evaluation results outperform the existing work consistently on both indoor and outdoor datasets, and it significantly reduces the mean average error by up to 19.5% under the same settings of 200 sparse samples on NYU-Depth-V2 dataset.



### BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering
- **Arxiv ID**: http://arxiv.org/abs/2112.05504v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.05504v4)
- **Published**: 2021-12-10 13:16:21+00:00
- **Updated**: 2023-05-09 05:48:39+00:00
- **Authors**: Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, Dahua Lin
- **Comment**: Accepted to ECCV22; Previous version: CityNeRF: Building NeRF at City
  Scale; Project page can be found in https://city-super.github.io/citynerf
- **Journal**: None
- **Summary**: Neural radiance fields (NeRF) has achieved outstanding performance in modeling 3D objects and controlled scenes, usually under a single scale. In this work, we focus on multi-scale cases where large changes in imagery are observed at drastically different scales. This scenario vastly exists in real-world 3D environments, such as city scenes, with views ranging from satellite level that captures the overview of a city, to ground level imagery showing complex details of an architecture; and can also be commonly identified in landscape and delicate minecraft 3D models. The wide span of viewing positions within these scenes yields multi-scale renderings with very different levels of detail, which poses great challenges to neural radiance field and biases it towards compromised results. To address these issues, we introduce BungeeNeRF, a progressive neural radiance field that achieves level-of-detail rendering across drastically varied scales. Starting from fitting distant views with a shallow base block, as training progresses, new blocks are appended to accommodate the emerging details in the increasingly closer views. The strategy progressively activates high-frequency channels in NeRF's positional encoding inputs and successively unfolds more complex details as the training proceeds. We demonstrate the superiority of BungeeNeRF in modeling diverse multi-scale scenes with drastically varying views on multiple data sources (city models, synthetic, and drone captured data) and its support for high-quality rendering in different levels of detail.



### DeepRLS: A Recurrent Network Architecture with Least Squares Implicit Layers for Non-blind Image Deconvolution
- **Arxiv ID**: http://arxiv.org/abs/2112.05505v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2112.05505v1)
- **Published**: 2021-12-10 13:16:51+00:00
- **Updated**: 2021-12-10 13:16:51+00:00
- **Authors**: Iaroslav Koshelev, Daniil Selikhanovych, Stamatios Lefkimmiatis
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we study the problem of non-blind image deconvolution and propose a novel recurrent network architecture that leads to very competitive restoration results of high image quality. Motivated by the computational efficiency and robustness of existing large scale linear solvers, we manage to express the solution to this problem as the solution of a series of adaptive non-negative least-squares problems. This gives rise to our proposed Recurrent Least Squares Deconvolution Network (RLSDN) architecture, which consists of an implicit layer that imposes a linear constraint between its input and output. By design, our network manages to serve two important purposes simultaneously. The first is that it implicitly models an effective image prior that can adequately characterize the set of natural images, while the second is that it recovers the corresponding maximum a posteriori (MAP) estimate. Experiments on publicly available datasets, comparing recent state-of-the-art methods, show that our proposed RLSDN approach achieves the best reported performance both for grayscale and color images for all tested scenarios. Furthermore, we introduce a novel training strategy that can be adopted by any network architecture that involves the solution of linear systems as part of its pipeline. Our strategy eliminates completely the need to unroll the iterations required by the linear solver and, thus, it reduces significantly the memory footprint during training. Consequently, this enables the training of deeper network architectures which can further improve the reconstruction results.



### Global Attention Mechanism: Retain Information to Enhance Channel-Spatial Interactions
- **Arxiv ID**: http://arxiv.org/abs/2112.05561v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05561v1)
- **Published**: 2021-12-10 14:12:32+00:00
- **Updated**: 2021-12-10 14:12:32+00:00
- **Authors**: Yichao Liu, Zongru Shao, Nico Hoffmann
- **Comment**: 5 pages, 3 figures, 4 tables
- **Journal**: None
- **Summary**: A variety of attention mechanisms have been studied to improve the performance of various computer vision tasks. However, the prior methods overlooked the significance of retaining the information on both channel and spatial aspects to enhance the cross-dimension interactions. Therefore, we propose a global attention mechanism that boosts the performance of deep neural networks by reducing information reduction and magnifying the global interactive representations. We introduce 3D-permutation with multilayer-perceptron for channel attention alongside a convolutional spatial attention submodule. The evaluation of the proposed mechanism for the image classification task on CIFAR-100 and ImageNet-1K indicates that our method stably outperforms several recent attention mechanisms with both ResNet and lightweight MobileNet.



### GPU-accelerated image alignment for object detection in industrial applications
- **Arxiv ID**: http://arxiv.org/abs/2112.05576v1
- **DOI**: 10.1109/ARIS.2017.8297173
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05576v1)
- **Published**: 2021-12-10 14:43:24+00:00
- **Updated**: 2021-12-10 14:43:24+00:00
- **Authors**: Trung-Son Le, Chyi-Yeu Lin
- **Comment**: None
- **Journal**: None
- **Summary**: This research proposes a practical method for detecting featureless objects by using image alignment approach with a robust similarity measure in industrial applications. This similarity measure is robust against occlusion, illumination changes and background clutter. The performance of the proposed GPU (Graphics Processing Unit) accelerated algorithm is deemed successful in experiments of comparison between both CPU and GPU implementations



### Discrete neural representations for explainable anomaly detection
- **Arxiv ID**: http://arxiv.org/abs/2112.05585v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05585v1)
- **Published**: 2021-12-10 14:56:58+00:00
- **Updated**: 2021-12-10 14:56:58+00:00
- **Authors**: Stanislaw Szymanowicz, James Charles, Roberto Cipolla
- **Comment**: None
- **Journal**: Winter Conference on Applications of Computer Vision 2022
- **Summary**: The aim of this work is to detect and automatically generate high-level explanations of anomalous events in video. Understanding the cause of an anomalous event is crucial as the required response is dependant on its nature and severity. Recent works typically use object or action classifier to detect and provide labels for anomalous events. However, this constrains detection systems to a finite set of known classes and prevents generalisation to unknown objects or behaviours. Here we show how to robustly detect anomalies without the use of object or action classifiers yet still recover the high level reason behind the event. We make the following contributions: (1) a method using saliency maps to decouple the explanation of anomalous events from object and action classifiers, (2) show how to improve the quality of saliency maps using a novel neural architecture for learning discrete representations of video by predicting future frames and (3) beat the state-of-the-art anomaly explanation methods by 60\% on a subset of the public benchmark X-MAN dataset.



### Unified Multimodal Pre-training and Prompt-based Tuning for Vision-Language Understanding and Generation
- **Arxiv ID**: http://arxiv.org/abs/2112.05587v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.05587v2)
- **Published**: 2021-12-10 14:59:06+00:00
- **Updated**: 2021-12-15 05:55:13+00:00
- **Authors**: Tianyi Liu, Zuxuan Wu, Wenhan Xiong, Jingjing Chen, Yu-Gang Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing vision-language pre-training methods focus on understanding tasks and use BERT-like objectives (masked language modeling and image-text matching) during pretraining. Although they perform well in many understanding downstream tasks, e.g., visual question answering, image-text retrieval and visual entailment, they do not possess the ability to generate. To tackle this problem, we propose Unified multimodal pre-training for both Vision-Language understanding and generation (UniVL). The proposed UniVL is capable of handling both understanding tasks and generative tasks. We augment existing pretraining paradigms that only use random masks with causal masks, i.e., triangular masks that mask out future tokens, such that the pre-trained models can have autoregressive generation abilities by design. We formulate several previous understanding tasks as a text generation task and propose to use prompt-based method for fine-tuning on different downstream tasks. Our experiments show that there is a trade-off between understanding tasks and generation tasks while using the same model, and a feasible way to improve both tasks is to use more data. Our UniVL framework attains comparable performance to recent vision-language pre-training methods on both understanding tasks and generation tasks. Moreover, we demostrate that prompt-based finetuning is more data-efficient - it outperforms discriminative methods in few-shot scenarios.



### ST-MTL: Spatio-Temporal Multitask Learning Model to Predict Scanpath While Tracking Instruments in Robotic Surgery
- **Arxiv ID**: http://arxiv.org/abs/2112.08189v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.08189v1)
- **Published**: 2021-12-10 15:20:27+00:00
- **Updated**: 2021-12-10 15:20:27+00:00
- **Authors**: Mobarakol Islam, Vibashan VS, Chwee Ming Lim, Hongliang Ren
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Representation learning of the task-oriented attention while tracking instrument holds vast potential in image-guided robotic surgery. Incorporating cognitive ability to automate the camera control enables the surgeon to concentrate more on dealing with surgical instruments. The objective is to reduce the operation time and facilitate the surgery for both surgeons and patients. We propose an end-to-end trainable Spatio-Temporal Multi-Task Learning (ST-MTL) model with a shared encoder and spatio-temporal decoders for the real-time surgical instrument segmentation and task-oriented saliency detection. In the MTL model of shared parameters, optimizing multiple loss functions into a convergence point is still an open challenge. We tackle the problem with a novel asynchronous spatio-temporal optimization (ASTO) technique by calculating independent gradients for each decoder. We also design a competitive squeeze and excitation unit by casting a skip connection that retains weak features, excites strong features, and performs dynamic spatial and channel-wise feature recalibration. To capture better long term spatio-temporal dependencies, we enhance the long-short term memory (LSTM) module by concatenating high-level encoder features of consecutive frames. We also introduce Sinkhorn regularized loss to enhance task-oriented saliency detection by preserving computational efficiency. We generate the task-aware saliency maps and scanpath of the instruments on the dataset of the MICCAI 2017 robotic instrument segmentation challenge. Compared to the state-of-the-art segmentation and saliency methods, our model outperforms most of the evaluation metrics and produces an outstanding performance in the challenge.



### PERF: Performant, Explicit Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2112.05598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05598v1)
- **Published**: 2021-12-10 15:29:00+00:00
- **Updated**: 2021-12-10 15:29:00+00:00
- **Authors**: Sverker Rasmuson, Erik Sintorn, Ulf Assarsson
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel way of approaching image-based 3D reconstruction based on radiance fields. The problem of volumetric reconstruction is formulated as a non-linear least-squares problem and solved explicitly without the use of neural networks. This enables the use of solvers with a higher rate of convergence than what is typically used for neural networks, and fewer iterations are required until convergence. The volume is represented using a grid of voxels, with the scene surrounded by a hierarchy of environment maps. This makes it possible to get clean reconstructions of 360{\deg} scenes where the foreground and background is separated. A number of synthetic and real scenes from well known benchmark-suites are successfully reconstructed with quality on par with state-of-the-art methods, but at significantly reduced reconstruction times.



### Seq-Masks: Bridging the gap between appearance and gait modeling for video-based person re-identification
- **Arxiv ID**: http://arxiv.org/abs/2112.05626v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.7
- **Links**: [PDF](http://arxiv.org/pdf/2112.05626v1)
- **Published**: 2021-12-10 16:00:20+00:00
- **Updated**: 2021-12-10 16:00:20+00:00
- **Authors**: Zhigang Chang, Zhao Yang, Yongbiao Chen, Qin Zhou, Shibao Zheng
- **Comment**: ICASSP2021 Submission
- **Journal**: None
- **Summary**: ideo-based person re-identification (Re-ID) aims to match person images in video sequences captured by disjoint surveillance cameras. Traditional video-based person Re-ID methods focus on exploring appearance information, thus, vulnerable against illumination changes, scene noises, camera parameters, and especially clothes/carrying variations. Gait recognition provides an implicit biometric solution to alleviate the above headache. Nonetheless, it experiences severe performance degeneration as camera view varies. In an attempt to address these problems, in this paper, we propose a framework that utilizes the sequence masks (SeqMasks) in the video to integrate appearance information and gait modeling in a close fashion. Specifically, to sufficiently validate the effectiveness of our method, we build a novel dataset named MaskMARS based on MARS. Comprehensive experiments on our proposed large wild video Re-ID dataset MaskMARS evidenced our extraordinary performance and generalization capability. Validations on the gait recognition metric CASIA-B dataset further demonstrated the capability of our hybrid model.



### Preemptive Image Robustification for Protecting Users against Man-in-the-Middle Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2112.05634v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.05634v1)
- **Published**: 2021-12-10 16:06:03+00:00
- **Updated**: 2021-12-10 16:06:03+00:00
- **Authors**: Seungyong Moon, Gaon An, Hyun Oh Song
- **Comment**: Accepted and to appear at AAAI 2022
- **Journal**: None
- **Summary**: Deep neural networks have become the driving force of modern image recognition systems. However, the vulnerability of neural networks against adversarial attacks poses a serious threat to the people affected by these systems. In this paper, we focus on a real-world threat model where a Man-in-the-Middle adversary maliciously intercepts and perturbs images web users upload online. This type of attack can raise severe ethical concerns on top of simple performance degradation. To prevent this attack, we devise a novel bi-level optimization algorithm that finds points in the vicinity of natural images that are robust to adversarial perturbations. Experiments on CIFAR-10 and ImageNet show our method can effectively robustify natural images within the given modification budget. We also show the proposed method can improve robustness when jointly used with randomized smoothing.



### Learning Representations with Contrastive Self-Supervised Learning for Histopathology Applications
- **Arxiv ID**: http://arxiv.org/abs/2112.05760v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2112.05760v2)
- **Published**: 2021-12-10 16:08:57+00:00
- **Updated**: 2022-08-16 13:31:29+00:00
- **Authors**: Karin Stacke, Jonas Unger, Claes Lundström, Gabriel Eilertsen
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://www.melba-journal.org/papers/2022:023.html
- **Journal**: https://www.melba-journal.org/papers/2022:023.html
- **Summary**: Unsupervised learning has made substantial progress over the last few years, especially by means of contrastive self-supervised learning. The dominating dataset for benchmarking self-supervised learning has been ImageNet, for which recent methods are approaching the performance achieved by fully supervised training. The ImageNet dataset is however largely object-centric, and it is not clear yet what potential those methods have on widely different datasets and tasks that are not object-centric, such as in digital pathology. While self-supervised learning has started to be explored within this area with encouraging results, there is reason to look closer at how this setting differs from natural images and ImageNet. In this paper we make an in-depth analysis of contrastive learning for histopathology, pin-pointing how the contrastive objective will behave differently due to the characteristics of histopathology data. We bring forward a number of considerations, such as view generation for the contrastive objective and hyper-parameter tuning. In a large battery of experiments, we analyze how the downstream performance in tissue classification will be affected by these considerations. The results point to how contrastive learning can reduce the annotation effort within digital pathology, but that the specific dataset characteristics need to be considered. To take full advantage of the contrastive learning objective, different calibrations of view generation and hyper-parameters are required. Our results pave the way for realizing the full potential of self-supervised learning for histopathology applications.



### HeadNeRF: A Real-time NeRF-based Parametric Head Model
- **Arxiv ID**: http://arxiv.org/abs/2112.05637v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05637v3)
- **Published**: 2021-12-10 16:10:13+00:00
- **Updated**: 2022-04-30 13:57:53+00:00
- **Authors**: Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, Juyong Zhang
- **Comment**: Accepted by CVPR2022. Project page:
  https://crishy1995.github.io/HeadNeRF-Project/
- **Journal**: None
- **Summary**: In this paper, we propose HeadNeRF, a novel NeRF-based parametric head model that integrates the neural radiance field to the parametric representation of the human head. It can render high fidelity head images in real-time on modern GPUs, and supports directly controlling the generated images' rendering pose and various semantic attributes. Different from existing related parametric models, we use the neural radiance fields as a novel 3D proxy instead of the traditional 3D textured mesh, which makes that HeadNeRF is able to generate high fidelity images. However, the computationally expensive rendering process of the original NeRF hinders the construction of the parametric NeRF model. To address this issue, we adopt the strategy of integrating 2D neural rendering to the rendering process of NeRF and design novel loss terms. As a result, the rendering speed of HeadNeRF can be significantly accelerated, and the rendering time of one frame is reduced from 5s to 25ms. The well designed loss terms also improve the rendering accuracy, and the fine-level details of the human head, such as the gaps between teeth, wrinkles, and beards, can be represented and synthesized by HeadNeRF. Extensive experimental results and several applications demonstrate its effectiveness. The trained parametric model is available at https://github.com/CrisHY1995/headnerf.



### Roominoes: Generating Novel 3D Floor Plans From Existing 3D Rooms
- **Arxiv ID**: http://arxiv.org/abs/2112.05644v1
- **DOI**: 10.1111/cgf.14357
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2112.05644v1)
- **Published**: 2021-12-10 16:17:01+00:00
- **Updated**: 2021-12-10 16:17:01+00:00
- **Authors**: Kai Wang, Xianghao Xu, Leon Lei, Selena Ling, Natalie Lindsay, Angel X. Chang, Manolis Savva, Daniel Ritchie
- **Comment**: Symposium on Geometry Processing (SGP) 2021
- **Journal**: Computer Graphics Forum, 40: 57-69 (2021)
- **Summary**: Realistic 3D indoor scene datasets have enabled significant recent progress in computer vision, scene understanding, autonomous navigation, and 3D reconstruction. But the scale, diversity, and customizability of existing datasets is limited, and it is time-consuming and expensive to scan and annotate more. Fortunately, combinatorics is on our side: there are enough individual rooms in existing 3D scene datasets, if there was but a way to recombine them into new layouts. In this paper, we propose the task of generating novel 3D floor plans from existing 3D rooms. We identify three sub-tasks of this problem: generation of 2D layout, retrieval of compatible 3D rooms, and deformation of 3D rooms to fit the layout. We then discuss different strategies for solving the problem, and design two representative pipelines: one uses available 2D floor plans to guide selection and deformation of 3D rooms; the other learns to retrieve a set of compatible 3D rooms and combine them into novel layouts. We design a set of metrics that evaluate the generated results with respect to each of the three subtasks and show that different methods trade off performance on these subtasks. Finally, we survey downstream tasks that benefit from generated 3D scenes and discuss strategies in selecting the methods most appropriate for the demands of these tasks.



### Mask-invariant Face Recognition through Template-level Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2112.05646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05646v1)
- **Published**: 2021-12-10 16:19:28+00:00
- **Updated**: 2021-12-10 16:19:28+00:00
- **Authors**: Marco Huber, Fadi Boutros, Florian Kirchbuchner, Naser Damer
- **Comment**: Accepted at the 16th IEEE International Conference on Automatic Face
  and Gesture Recognition, FG 2021
- **Journal**: None
- **Summary**: The emergence of the global COVID-19 pandemic poses new challenges for biometrics. Not only are contactless biometric identification options becoming more important, but face recognition has also recently been confronted with the frequent wearing of masks. These masks affect the performance of previous face recognition systems, as they hide important identity information. In this paper, we propose a mask-invariant face recognition solution (MaskInv) that utilizes template-level knowledge distillation within a training paradigm that aims at producing embeddings of masked faces that are similar to those of non-masked faces of the same identities. In addition to the distilled knowledge, the student network benefits from additional guidance by margin-based identity classification loss, ElasticFace, using masked and non-masked faces. In a step-wise ablation study on two real masked face databases and five mainstream databases with synthetic masks, we prove the rationalization of our MaskInv approach. Our proposed solution outperforms previous state-of-the-art (SOTA) academic solutions in the recent MFRC-21 challenge in both scenarios, masked vs masked and masked vs non-masked, and also outperforms the previous solution on the MFR2 dataset. Furthermore, we demonstrate that the proposed model can still perform well on unmasked faces with only a minor loss in verification performance. The code, the trained models, as well as the evaluation protocol on the synthetically masked data are publicly available: https://github.com/fdbtrs/Masked-Face-Recognition-KD.



### Artificial Intellgence -- Application in Life Sciences and Beyond. The Upper Rhine Artificial Intelligence Symposium UR-AI 2021
- **Arxiv ID**: http://arxiv.org/abs/2112.05657v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2112.05657v1)
- **Published**: 2021-12-10 16:43:01+00:00
- **Updated**: 2021-12-10 16:43:01+00:00
- **Authors**: Karl-Herbert Schäfer, Franz Quint
- **Comment**: None
- **Journal**: None
- **Summary**: The TriRhenaTech alliance presents the accepted papers of the 'Upper-Rhine Artificial Intelligence Symposium' held on October 27th 2021 in Kaiserslautern, Germany. Topics of the conference are applications of Artificial Intellgence in life sciences, intelligent systems, industry 4.0, mobility and others. The TriRhenaTech alliance is a network of universities in the Upper-Rhine Trinational Metropolitan Region comprising of the German universities of applied sciences in Furtwangen, Kaiserslautern, Karlsruhe, Offenburg and Trier, the Baden-Wuerttemberg Cooperative State University Loerrach, the French university network Alsace Tech (comprised of 14 'grandes \'ecoles' in the fields of engineering, architecture and management) and the University of Applied Sciences and Arts Northwestern Switzerland. The alliance's common goal is to reinforce the transfer of knowledge, research, and technology, as well as the cross-border mobility of students.



### A Deep Learning Based Automated Hand Hygiene Training System
- **Arxiv ID**: http://arxiv.org/abs/2112.05667v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.05667v1)
- **Published**: 2021-12-10 17:01:44+00:00
- **Updated**: 2021-12-10 17:01:44+00:00
- **Authors**: Mobina Shahbandeh, Fatemeh Ghaffarpour, Sina Vali, Mohammad Amin Haghpanah, Amin Mousavi Torkamani, Mehdi Tale Masouleh, Ahmad Kalhor
- **Comment**: 6 pages, 13 figures
- **Journal**: None
- **Summary**: Hand hygiene is crucial for preventing viruses and infections. Due to the pervasive outbreak of COVID-19, wearing a mask and hand hygiene appear to be the most effective ways for the public to curb the spread of these viruses. The World Health Organization (WHO) recommends a guideline for alcohol-based hand rub in eight steps to ensure that all surfaces of hands are entirely clean. As these steps involve complex gestures, human assessment of them lacks enough accuracy. However, Deep Neural Network (DNN) and machine vision have made it possible to accurately evaluate hand rubbing quality for the purposes of training and feedback. In this paper, an automated deep learning based hand rub assessment system with real-time feedback is presented. The system evaluates the compliance with the 8-step guideline using a DNN architecture trained on a dataset of videos collected from volunteers with various skin tones and hand characteristics following the hand rubbing guideline. Various DNN architectures were tested, and an Inception-ResNet model led to the best results with 97% test accuracy. In the proposed system, an NVIDIA Jetson AGX Xavier embedded board runs the software. The efficacy of the system is evaluated in a concrete situation of being used by various users, and challenging steps are identified. In this experiment, the average time taken by the hand rubbing steps among volunteers is 27.2 seconds, which conforms to the WHO guidelines.



### VUT: Versatile UI Transformer for Multi-Modal Multi-Task User Interface Modeling
- **Arxiv ID**: http://arxiv.org/abs/2112.05692v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.05692v1)
- **Published**: 2021-12-10 17:37:26+00:00
- **Updated**: 2021-12-10 17:37:26+00:00
- **Authors**: Yang Li, Gang Li, Xin Zhou, Mostafa Dehghani, Alexey Gritsenko
- **Comment**: None
- **Journal**: None
- **Summary**: User interface modeling is inherently multimodal, which involves several distinct types of data: images, structures and language. The tasks are also diverse, including object detection, language generation and grounding. In this paper, we present VUT, a Versatile UI Transformer that takes multimodal input and simultaneously accomplishes 5 distinct tasks with the same model. Our model consists of a multimodal Transformer encoder that jointly encodes UI images and structures, and performs UI object detection when the UI structures are absent in the input. Our model also consists of an auto-regressive Transformer model that encodes the language input and decodes output, for both question-answering and command grounding with respect to the UI. Our experiments show that for most of the tasks, when trained jointly for multi-tasks, VUT substantially reduces the number of models and footprints needed for performing multiple tasks, while achieving accuracy exceeding or on par with baseline models trained for each individual task.



### Self-Supervised Transformers for fMRI representation
- **Arxiv ID**: http://arxiv.org/abs/2112.05761v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.05761v2)
- **Published**: 2021-12-10 18:04:26+00:00
- **Updated**: 2022-08-13 10:21:37+00:00
- **Authors**: Itzik Malkiel, Gony Rosenman, Lior Wolf, Talma Hendler
- **Comment**: None
- **Journal**: None
- **Summary**: We present TFF, which is a Transformer framework for the analysis of functional Magnetic Resonance Imaging (fMRI) data. TFF employs a two-phase training approach. First, self-supervised training is applied to a collection of fMRI scans, where the model is trained to reconstruct 3D volume data. Second, the pre-trained model is fine-tuned on specific tasks, utilizing ground truth labels. Our results show state-of-the-art performance on a variety of fMRI tasks, including age and gender prediction, as well as schizophrenia recognition. Our code for the training, network architecture, and results is attached as supplementary material.



### Neural Belief Propagation for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2112.05727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05727v1)
- **Published**: 2021-12-10 18:30:27+00:00
- **Updated**: 2021-12-10 18:30:27+00:00
- **Authors**: Daqi Liu, Miroslaw Bober, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: Scene graph generation aims to interpret an input image by explicitly modelling the potential objects and their relationships, which is predominantly solved by the message passing neural network models in previous methods. Currently, such approximation models generally assume the output variables are totally independent and thus ignore the informative structural higher-order interactions. This could lead to the inconsistent interpretations for an input image. In this paper, we propose a novel neural belief propagation method to generate the resulting scene graph. It employs a structural Bethe approximation rather than the mean field approximation to infer the associated marginals. To find a better bias-variance trade-off, the proposed model not only incorporates pairwise interactions but also higher order interactions into the associated scoring function. It achieves the state-of-the-art performance on various popular scene graph generation benchmarks.



### More Control for Free! Image Synthesis with Semantic Diffusion Guidance
- **Arxiv ID**: http://arxiv.org/abs/2112.05744v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2112.05744v4)
- **Published**: 2021-12-10 18:55:50+00:00
- **Updated**: 2022-12-05 15:37:39+00:00
- **Authors**: Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, Trevor Darrell
- **Comment**: WACV 2023. Project page https://xh-liu.github.io/sdg/
- **Journal**: None
- **Summary**: Controllable image synthesis models allow creation of diverse images based on text instructions or guidance from a reference image. Recently, denoising diffusion probabilistic models have been shown to generate more realistic imagery than prior methods, and have been successfully demonstrated in unconditional and class-conditional settings. We investigate fine-grained, continuous control of this model class, and introduce a novel unified framework for semantic diffusion guidance, which allows either language or image guidance, or both. Guidance is injected into a pretrained unconditional diffusion model using the gradient of image-text or image matching scores, without re-training the diffusion model. We explore CLIP-based language guidance as well as both content and style-based image guidance in a unified framework. Our text-guided synthesis approach can be applied to datasets without associated text annotations. We conduct experiments on FFHQ and LSUN datasets, and show results on fine-grained text-guided image synthesis, synthesis of images related to a style or content reference image, and examples with both textual and image guidance.



### Deep Learning based Framework for Automatic Diagnosis of Glaucoma based on analysis of Focal Notching in the Optic Nerve Head
- **Arxiv ID**: http://arxiv.org/abs/2112.05748v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.05748v1)
- **Published**: 2021-12-10 18:58:40+00:00
- **Updated**: 2021-12-10 18:58:40+00:00
- **Authors**: Sneha Dasgupta, Rishav Mukherjee, Kaushik Dutta, Anindya Sen
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic evaluation of the retinal fundus image is emerging as one of the most important tools for early detection and treatment of progressive eye diseases like Glaucoma. Glaucoma results to a progressive degeneration of vision and is characterized by the deformation of the shape of optic cup and the degeneration of the blood vessels resulting in the formation of a notch along the neuroretinal rim. In this paper, we propose a deep learning-based pipeline for automatic segmentation of optic disc (OD) and optic cup (OC) regions from Digital Fundus Images (DFIs), thereby extracting distinct features necessary for prediction of Glaucoma. This methodology has utilized focal notch analysis of neuroretinal rim along with cup-to-disc ratio values as classifying parameters to enhance the accuracy of Computer-aided design (CAD) systems in analyzing glaucoma. Support Vector-based Machine Learning algorithm is used for classification, which classifies DFIs as Glaucomatous or Normal based on the extracted features. The proposed pipeline was evaluated on the freely available DRISHTI-GS dataset with a resultant accuracy of 93.33% for detecting Glaucoma from DFIs.



### Label, Verify, Correct: A Simple Few Shot Object Detection Method
- **Arxiv ID**: http://arxiv.org/abs/2112.05749v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05749v2)
- **Published**: 2021-12-10 18:59:06+00:00
- **Updated**: 2022-03-29 14:51:18+00:00
- **Authors**: Prannay Kaul, Weidi Xie, Andrew Zisserman
- **Comment**: CVPR 2022, project page:
  https://www.robots.ox.ac.uk/~vgg/research/lvc/
- **Journal**: None
- **Summary**: The objective of this paper is few-shot object detection (FSOD) -- the task of expanding an object detector for a new category given only a few instances for training. We introduce a simple pseudo-labelling method to source high-quality pseudo-annotations from the training set, for each new category, vastly increasing the number of training instances and reducing class imbalance; our method finds previously unlabelled instances. Na\"ively training with model predictions yields sub-optimal performance; we present two novel methods to improve the precision of the pseudo-labelling process: first, we introduce a verification technique to remove candidate detections with incorrect class labels; second, we train a specialised model to correct poor quality bounding boxes. After these two novel steps, we obtain a large set of high-quality pseudo-annotations that allow our final detector to be trained end-to-end. Additionally, we demonstrate our method maintains base class performance, and the utility of simple augmentations in FSOD. While benchmarking on PASCAL VOC and MS-COCO, our method achieves state-of-the-art or second-best performance compared to existing approaches across all number of shots.



### Guided Generative Models using Weak Supervision for Detecting Object Spatial Arrangement in Overhead Images
- **Arxiv ID**: http://arxiv.org/abs/2112.05786v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.05786v1)
- **Published**: 2021-12-10 19:14:10+00:00
- **Updated**: 2021-12-10 19:14:10+00:00
- **Authors**: Weiwei Duan, Yao-Yi Chiang, Stefan Leyk, Johannes H. Uhl, Craig A. Knoblock
- **Comment**: None
- **Journal**: None
- **Summary**: The increasing availability and accessibility of numerous overhead images allows us to estimate and assess the spatial arrangement of groups of geospatial target objects, which can benefit many applications, such as traffic monitoring and agricultural monitoring. Spatial arrangement estimation is the process of identifying the areas which contain the desired objects in overhead images. Traditional supervised object detection approaches can estimate accurate spatial arrangement but require large amounts of bounding box annotations. Recent semi-supervised clustering approaches can reduce manual labeling but still require annotations for all object categories in the image. This paper presents the target-guided generative model (TGGM), under the Variational Auto-encoder (VAE) framework, which uses Gaussian Mixture Models (GMM) to estimate the distributions of both hidden and decoder variables in VAE. Modeling both hidden and decoder variables by GMM reduces the required manual annotations significantly for spatial arrangement estimation. Unlike existing approaches that the training process can only update the GMM as a whole in the optimization iterations (e.g., a "minibatch"), TGGM allows the update of individual GMM components separately in the same optimization iteration. Optimizing GMM components separately allows TGGM to exploit the semantic relationships in spatial data and requires only a few labels to initiate and guide the generative process. Our experiments shows that TGGM achieves results comparable to the state-of-the-art semi-supervised methods and outperforms unsupervised methods by 10% based on the $F_{1}$ scores, while requiring significantly fewer labeled data.



### A Label Correction Algorithm Using Prior Information for Automatic and Accurate Geospatial Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.05794v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.05794v1)
- **Published**: 2021-12-10 19:27:53+00:00
- **Updated**: 2021-12-10 19:27:53+00:00
- **Authors**: Weiwei Duan, Yao-Yi Chiang, Stefan Leyk, Johannes H. Uhl, Craig A. Knoblock
- **Comment**: None
- **Journal**: None
- **Summary**: Thousands of scanned historical topographic maps contain valuable information covering long periods of time, such as how the hydrography of a region has changed over time. Efficiently unlocking the information in these maps requires training a geospatial objects recognition system, which needs a large amount of annotated data. Overlapping geo-referenced external vector data with topographic maps according to their coordinates can annotate the desired objects' locations in the maps automatically. However, directly overlapping the two datasets causes misaligned and false annotations because the publication years and coordinate projection systems of topographic maps are different from the external vector data. We propose a label correction algorithm, which leverages the color information of maps and the prior shape information of the external vector data to reduce misaligned and false annotations. The experiments show that the precision of annotations from the proposed algorithm is 10% higher than the annotations from a state-of-the-art algorithm. Consequently, recognition results using the proposed algorithm's annotations achieve 9% higher correctness than using the annotations from the state-of-the-art algorithm.



### Benchmarking human visual search computational models in natural scenes: models comparison and reference datasets
- **Arxiv ID**: http://arxiv.org/abs/2112.05808v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.05808v1)
- **Published**: 2021-12-10 19:56:45+00:00
- **Updated**: 2021-12-10 19:56:45+00:00
- **Authors**: F. Travi, G. Ruarte, G. Bujia, J. E. Kamienkowski
- **Comment**: None
- **Journal**: None
- **Summary**: Visual search is an essential part of almost any everyday human goal-directed interaction with the environment. Nowadays, several algorithms are able to predict gaze positions during simple observation, but few models attempt to simulate human behavior during visual search in natural scenes. Furthermore, these models vary widely in their design and exhibit differences in the datasets and metrics with which they were evaluated. Thus, there is a need for a reference point, on which each model can be tested and from where potential improvements can be derived. In the present work, we select publicly available state-of-the-art visual search models in natural scenes and evaluate them on different datasets, employing the same metrics to estimate their efficiency and similarity with human subjects. In particular, we propose an improvement to the Ideal Bayesian Searcher through a combination with a neural network-based visual search model, enabling it to generalize to other datasets. The present work sheds light on the limitations of current models and how potential improvements can be accomplished by combining approaches. Moreover, it moves forward on providing a solution for the urgent need for benchmarking data and metrics to support the development of more general human visual search computational models.



### Deep ViT Features as Dense Visual Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2112.05814v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05814v3)
- **Published**: 2021-12-10 20:15:03+00:00
- **Updated**: 2022-10-15 21:18:49+00:00
- **Authors**: Shir Amir, Yossi Gandelsman, Shai Bagon, Tali Dekel
- **Comment**: Revised version - high res figures
- **Journal**: None
- **Summary**: We study the use of deep features extracted from a pretrained Vision Transformer (ViT) as dense visual descriptors. We observe and empirically demonstrate that such features, when extractedfrom a self-supervised ViT model (DINO-ViT), exhibit several striking properties, including: (i) the features encode powerful, well-localized semantic information, at high spatial granularity, such as object parts; (ii) the encoded semantic information is shared across related, yet different object categories, and (iii) positional bias changes gradually throughout the layers. These properties allow us to design simple methods for a variety of applications, including co-segmentation, part co-segmentation and semantic correspondences. To distill the power of ViT features from convoluted design choices, we restrict ourselves to lightweight zero-shot methodologies (e.g., binning and clustering) applied directly to the features. Since our methods require no additional training nor data, they are readily applicable across a variety of domains. We show by extensive qualitative and quantitative evaluation that our simple methodologies achieve competitive results with recent state-of-the-art supervised methods, and outperform previous unsupervised methods by a large margin. Code is available in dino-vit-features.github.io.



### Revisiting Consistency Regularization for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.05825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05825v1)
- **Published**: 2021-12-10 20:46:13+00:00
- **Updated**: 2021-12-10 20:46:13+00:00
- **Authors**: Yue Fan, Anna Kukleva, Bernt Schiele
- **Comment**: Published at GCPR2021 as a conference paper
- **Journal**: None
- **Summary**: Consistency regularization is one of the most widely-used techniques for semi-supervised learning (SSL). Generally, the aim is to train a model that is invariant to various data augmentations. In this paper, we revisit this idea and find that enforcing invariance by decreasing distances between features from differently augmented images leads to improved performance. However, encouraging equivariance instead, by increasing the feature distance, further improves performance. To this end, we propose an improved consistency regularization framework by a simple yet effective technique, FeatDistLoss, that imposes consistency and equivariance on the classifier and the feature level, respectively. Experimental results show that our model defines a new state of the art for various datasets and settings and outperforms previous work by a significant margin, particularly in low data regimes. Extensive experiments are conducted to analyze the method, and the code will be published.



### Quality-Aware Multimodal Biometric Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.05827v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.05827v1)
- **Published**: 2021-12-10 20:48:55+00:00
- **Updated**: 2021-12-10 20:48:55+00:00
- **Authors**: Sobhan Soleymani, Ali Dabouei, Fariborz Taherkhani, Seyed Mehdi Iranmanesh, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: IEEE Transactions on Biometrics, Behavior, and Identity Science
- **Journal**: None
- **Summary**: We present a quality-aware multimodal recognition framework that combines representations from multiple biometric traits with varying quality and number of samples to achieve increased recognition accuracy by extracting complimentary identification information based on the quality of the samples. We develop a quality-aware framework for fusing representations of input modalities by weighting their importance using quality scores estimated in a weakly-supervised fashion. This framework utilizes two fusion blocks, each represented by a set of quality-aware and aggregation networks. In addition to architecture modifications, we propose two task-specific loss functions: multimodal separability loss and multimodal compactness loss. The first loss assures that the representations of modalities for a class have comparable magnitudes to provide a better quality estimation, while the multimodal representations of different classes are distributed to achieve maximum discrimination in the embedding space. The second loss, which is considered to regularize the network weights, improves the generalization performance by regularizing the framework. We evaluate the performance by considering three multimodal datasets consisting of face, iris, and fingerprint modalities. The efficacy of the framework is demonstrated through comparison with the state-of-the-art algorithms. In particular, our framework outperforms the rank- and score-level fusion of modalities of BIOMDATA by more than 30% for true acceptance rate at false acceptance rate of $10^{-4}$.



### Short and Long Range Relation Based Spatio-Temporal Transformer for Micro-Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.05851v4
- **DOI**: 10.1109/TAFFC.2022.3213509
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05851v4)
- **Published**: 2021-12-10 22:10:31+00:00
- **Updated**: 2022-11-23 09:53:16+00:00
- **Authors**: Liangfei Zhang, Xiaopeng Hong, Ognjen Arandjelovic, Guoying Zhao
- **Comment**: 13 pages, 9 figures
- **Journal**: IEEE Transactions on Affective Computing, 2022
- **Summary**: Being spontaneous, micro-expressions are useful in the inference of a person's true emotions even if an attempt is made to conceal them. Due to their short duration and low intensity, the recognition of micro-expressions is a difficult task in affective computing. The early work based on handcrafted spatio-temporal features which showed some promise, has recently been superseded by different deep learning approaches which now compete for the state of the art performance. Nevertheless, the problem of capturing both local and global spatio-temporal patterns remains challenging. To this end, herein we propose a novel spatio-temporal transformer architecture -- to the best of our knowledge, the first purely transformer based approach (i.e. void of any convolutional network use) for micro-expression recognition. The architecture comprises a spatial encoder which learns spatial patterns, a temporal aggregator for temporal dimension analysis, and a classification head. A comprehensive evaluation on three widely used spontaneous micro-expression data sets, namely SMIC-HS, CASME II and SAMM, shows that the proposed approach consistently outperforms the state of the art, and is the first framework in the published literature on micro-expression recognition to achieve the unweighted F1-score greater than 0.9 on any of the aforementioned data sets.



### A Discriminative Channel Diversification Network for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2112.05861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.05861v1)
- **Published**: 2021-12-10 23:00:53+00:00
- **Updated**: 2021-12-10 23:00:53+00:00
- **Authors**: Krushi Patel, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Channel attention mechanisms in convolutional neural networks have been proven to be effective in various computer vision tasks. However, the performance improvement comes with additional model complexity and computation cost. In this paper, we propose a light-weight and effective attention module, called channel diversification block, to enhance the global context by establishing the channel relationship at the global level. Unlike other channel attention mechanisms, the proposed module focuses on the most discriminative features by giving more attention to the spatially distinguishable channels while taking account of the channel activation. Different from other attention models that plugin the module in between several intermediate layers, the proposed module is embedded at the end of the backbone networks, making it easy to implement. Extensive experiments on CIFAR-10, SVHN, and Tiny-ImageNet datasets demonstrate that the proposed module improves the performance of the baseline networks by a margin of 3% on average.



