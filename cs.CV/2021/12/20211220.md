# Arxiv Papers in cs.CV on 2021-12-20
### Driver Drowsiness Detection Using Ensemble Convolutional Neural Networks on YawDD
- **Arxiv ID**: http://arxiv.org/abs/2112.10298v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10298v1)
- **Published**: 2021-12-20 01:04:52+00:00
- **Updated**: 2021-12-20 01:04:52+00:00
- **Authors**: Rais Mohammad Salman, Mahbubur Rashid, Rupal Roy, Md Manjurul Ahsan, Zahed Siddique
- **Comment**: None
- **Journal**: None
- **Summary**: Driver drowsiness detection using videos/images is one of the most essential areas in today's time for driver safety. The development of deep learning techniques, notably Convolutional Neural Networks (CNN), applied in computer vision applications such as drowsiness detection, has shown promising results due to the tremendous increase in technology in the recent few decades. Eyes that are closed or blinking excessively, yawning, nodding, and occlusion are all key aspects of drowsiness. In this work, we have applied four different Convolutional Neural Network (CNN) techniques on the YawDD dataset to detect and examine the extent of drowsiness depending on the yawning frequency with specific pose and occlusion variation. Preliminary computational results show that our proposed Ensemble Convolutional Neural Network (ECNN) outperformed the traditional CNN-based approach by achieving an F1 score of 0.935, whereas the other three CNN, such as CNN1, CNN2, and CNN3 approaches gained 0.92, 0.90, and 0.912 F1 scores, respectively.



### Image quality enhancement of embedded holograms in holographic information hiding using deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/2112.11246v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2112.11246v1)
- **Published**: 2021-12-20 01:21:28+00:00
- **Updated**: 2021-12-20 01:21:28+00:00
- **Authors**: Tomoyoshi Shimobaba, Sota Oshima, Takashi Kakue, and Tomoyoshi Ito
- **Comment**: None
- **Journal**: None
- **Summary**: Holographic information hiding is a technique for embedding holograms or images into another hologram, used for copyright protection and steganography of holograms. Using deep neural networks, we offer a way to improve the visual quality of embedded holograms. The brightness of an embedded hologram is set to a fraction of that of the host hologram, resulting in a barely damaged reconstructed image of the host hologram. However, it is difficult to perceive because the embedded hologram's reconstructed image is darker than the reconstructed host image. In this study, we use deep neural networks to restore the darkened image.



### Model-based gait recognition using graph network on very large population database
- **Arxiv ID**: http://arxiv.org/abs/2112.10305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10305v1)
- **Published**: 2021-12-20 02:28:02+00:00
- **Updated**: 2021-12-20 02:28:02+00:00
- **Authors**: Zhihao Wang, Chaoying Tang
- **Comment**: None
- **Journal**: None
- **Summary**: At present, the existing gait recognition systems are focusing on developing methods to extract robust gait feature from silhouette images and they indeed achieved great success. However, gait can be sensitive to appearance features such as clothing and carried items. Compared with appearance-based method, model-based gait recognition is promising due to the robustness against these variations. In recent years, with the development of human pose estimation, the difficulty of model-based gait recognition methods has been mitigated. In this paper, to resist the increase of subjects and views variation, local features are built and a siamese network is proposed to maximize the distance of samples from the same subject. We leverage recent advances in action recognition to embed human pose sequence to a vector and introduce Spatial-Temporal Graph Convolution Blocks (STGCB) which has been commonly used in action recognition for gait recognition. Experiments on the very large population dataset named OUMVLP-Pose and the popular dataset, CASIA-B, show that our method archives some state-of-the-art (SOTA) performances in model-based gait recognition. The code and models of our method are available at https://github.com/timelessnaive/Gait-for-Large-Dataset after being accepted.



### Skin lesion segmentation and classification using deep learning and handcrafted features
- **Arxiv ID**: http://arxiv.org/abs/2112.10307v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.10307v1)
- **Published**: 2021-12-20 02:45:42+00:00
- **Updated**: 2021-12-20 02:45:42+00:00
- **Authors**: Redha Ali, Hussin K. Ragb
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: Accurate diagnostics of a skin lesion is a critical task in classification dermoscopic images. In this research, we form a new type of image features, called hybrid features, which has stronger discrimination ability than single method features. This study involves a new technique where we inject the handcrafted features or feature transfer into the fully connected layer of Convolutional Neural Network (CNN) model during the training process. Based on our literature review until now, no study has examined or investigated the impact on classification performance by injecting the handcrafted features into the CNN model during the training process. In addition, we also investigated the impact of segmentation mask and its effect on the overall classification performance. Our model achieves an 92.3% balanced multiclass accuracy, which is 6.8% better than the typical single method classifier architecture for deep learning.



### Contrastive Attention Network with Dense Field Estimation for Face Completion
- **Arxiv ID**: http://arxiv.org/abs/2112.10310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10310v1)
- **Published**: 2021-12-20 02:54:38+00:00
- **Updated**: 2021-12-20 02:54:38+00:00
- **Authors**: Xin Ma, Xiaoqiang Zhou, Huaibo Huang, Gengyun Jia, Zhenhua Chai, Xiaolin Wei
- **Comment**: Accepted by Pattern Recognition 2021. arXiv admin note: substantial
  text overlap with arXiv:2010.15643
- **Journal**: None
- **Summary**: Most modern face completion approaches adopt an autoencoder or its variants to restore missing regions in face images. Encoders are often utilized to learn powerful representations that play an important role in meeting the challenges of sophisticated learning tasks. Specifically, various kinds of masks are often presented in face images in the wild, forming complex patterns, especially in this hard period of COVID-19. It's difficult for encoders to capture such powerful representations under this complex situation. To address this challenge, we propose a self-supervised Siamese inference network to improve the generalization and robustness of encoders. It can encode contextual semantics from full-resolution images and obtain more discriminative representations. To deal with geometric variations of face images, a dense correspondence field is integrated into the network. We further propose a multi-scale decoder with a novel dual attention fusion module (DAF), which can combine the restored and known regions in an adaptive manner. This multi-scale architecture is beneficial for the decoder to utilize discriminative representations learned from encoders into images. Extensive experiments clearly demonstrate that the proposed approach not only achieves more appealing results compared with state-of-the-art methods but also improves the performance of masked face recognition dramatically.



### A New Adaptive Noise Covariance Matrices Estimation and Filtering Method: Application to Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2112.12082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12082v1)
- **Published**: 2021-12-20 03:11:48+00:00
- **Updated**: 2021-12-20 03:11:48+00:00
- **Authors**: Chao Jiang, Zhiling Wang, Shuhang Tan, Huawei Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Kalman filters are widely used for object tracking, where process and measurement noise are usually considered accurately known and constant. However, the exact known and constant assumptions do not always hold in practice. For example, when lidar is used to track noncooperative targets, the measurement noise is different under different distances and weather conditions. In addition, the process noise changes with the object's motion state, especially when the tracking object is a pedestrian, and the process noise changes more frequently. This paper proposes a new estimation-calibration-correction closed-loop estimation method to estimate the Kalman filter process and measurement noise covariance matrices online. First, we decompose the noise covariance matrix into an element distribution matrix and noise intensity and improve the Sage filter to estimate the element distribution matrix. Second, we propose a calibration method to accurately diagnose the noise intensity deviation. We then propose a correct method to adaptively correct the noise intensity online. Third, under the assumption that the system is detectable, the unbiased and convergence of the proposed method is mathematically proven. Simulation results prove the effectiveness and reliability of the proposed method. Finally, we apply the proposed method to multiobject tracking of lidar and evaluate it on the official KITTI server. The proposed method on the KITTI pedestrian multiobject tracking leaderboard (http://www.cvlibs.net/datasets /kitti/eval_tracking.php) surpasses all existing methods using lidar, proving the feasibility of the method in practical applications. This work provides a new way to improve the performance of the Kalman filter and multiobject tracking.



### Product Re-identification System in Fully Automated Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/2112.10324v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10324v2)
- **Published**: 2021-12-20 03:37:37+00:00
- **Updated**: 2021-12-21 05:52:36+00:00
- **Authors**: Chenggui Sun, Li Bin Song
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we introduce a method and present an improved neural work to perform product re-identification, which is an essential core function of a fully automated product defect detection system. Our method is based on feature distance. It is the combination of feature extraction neural networks, such as VGG16, AlexNet, with an image search engine - Vearch. The dataset that we used to develop product re-identification systems is a water-bottle dataset that consists of 400 images of 18 types of water bottles. This is a small dataset, which was the biggest challenge of our work. However, the combination of neural networks with Vearch shows potential to tackle the product re-identification problems. Especially, our new neural network - AlphaAlexNet that a neural network was improved based on AlexNet could improve the production identification accuracy by four percent. This indicates that an ideal production identification accuracy could be achieved when efficient feature extraction methods could be introduced and redesigned for image feature extractions of nearly identical products. In order to solve the biggest challenges caused by the small size of the dataset and the difficult nature of identifying productions that have little differences from each other. In our future work, we propose a new roadmap to tackle nearly-identical production identifications: to introduce or develop new algorithms that need very few images to train themselves.



### Incremental Cross-view Mutual Distillation for Self-supervised Medical CT Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2112.10325v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.10325v4)
- **Published**: 2021-12-20 03:38:37+00:00
- **Updated**: 2022-07-12 15:40:50+00:00
- **Authors**: Chaowei Fang, Liang Wang, Dingwen Zhang, Jun Xu, Yixuan Yuan, Junwei Han
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Due to the constraints of the imaging device and high cost in operation time, computer tomography (CT) scans are usually acquired with low intra-slice resolution. Improving the intra-slice resolution is beneficial to the disease diagnosis for both human experts and computer-aided systems. To this end, this paper builds a novel medical slice synthesis to increase the between-slice resolution. Considering that the ground-truth intermediate medical slices are always absent in clinical practice, we introduce the incremental cross-view mutual distillation strategy to accomplish this task in the self-supervised learning manner. Specifically, we model this problem from three different views: slice-wise interpolation from axial view and pixel-wise interpolation from coronal and sagittal views. Under this circumstance, the models learned from different views can distill valuable knowledge to guide the learning processes of each other. We can repeat this process to make the models synthesize intermediate slice data with increasing inter-slice resolution. To demonstrate the effectiveness of the proposed approach, we conduct comprehensive experiments on a large-scale CT dataset. Quantitative and qualitative comparison results show that our method outperforms state-of-the-art algorithms by clear margins.



### Generating Photo-realistic Images from LiDAR Point Clouds with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2112.11245v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.11245v1)
- **Published**: 2021-12-20 05:25:15+00:00
- **Updated**: 2021-12-20 05:25:15+00:00
- **Authors**: Nuriel Shalom Mor
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: We examined the feasibility of generative adversarial networks (GANs) to generate photo-realistic images from LiDAR point clouds. For this purpose, we created a dataset of point cloud image pairs and trained the GAN to predict photorealistic images from LiDAR point clouds containing reflectance and distance information. Our models learned how to predict realistically looking images from just point cloud data, even images with black cars. Black cars are difficult to detect directly from point clouds because of their low level of reflectivity. This approach might be used in the future to perform visual object recognition on photorealistic images generated from LiDAR point clouds. In addition to the conventional LiDAR system, a second system that generates photorealistic images from LiDAR point clouds would run simultaneously for visual object recognition in real-time. In this way, we might preserve the supremacy of LiDAR and benefit from using photo-realistic images for visual object recognition without the usage of any camera. In addition, this approach could be used to colorize point clouds without the usage of any camera images.



### DMS-GCN: Dynamic Mutiscale Spatiotemporal Graph Convolutional Networks for Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2112.10365v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10365v1)
- **Published**: 2021-12-20 07:07:03+00:00
- **Updated**: 2021-12-20 07:07:03+00:00
- **Authors**: Zigeng Yan, Di-Hua Zhai, Yuanqing Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Human motion prediction is an important and challenging task in many computer vision application domains. Recent work concentrates on utilizing the timing processing ability of recurrent neural networks (RNNs) to achieve smooth and reliable results in short-term prediction. However, as evidenced by previous work, RNNs suffer from errors accumulation, leading to unreliable results. In this paper, we propose a simple feed-forward deep neural network for motion prediction, which takes into account temporal smoothness and spatial dependencies between human body joints. We design a Multi-scale Spatio-temporal graph convolutional networks (GCNs) to implicitly establish the Spatio-temporal dependence in the process of human movement, where different scales fused dynamically during training. The entire model is suitable for all actions and follows a framework of encoder-decoder. The encoder consists of temporal GCNs to capture motion features between frames and semi-autonomous learned spatial GCNs to extract spatial structure among joint trajectories. The decoder uses temporal convolution networks (TCNs) to maintain its extensive ability. Extensive experiments show that our approach outperforms SOTA methods on the datasets of Human3.6M and CMU Mocap while only requiring much lesser parameters. Code will be available at https://github.com/yzg9353/DMSGCN.



### Deep Co-supervision and Attention Fusion Strategy for Automatic COVID-19 Lung Infection Segmentation on CT Images
- **Arxiv ID**: http://arxiv.org/abs/2112.10368v1
- **DOI**: 10.1016/j.patcog.2021.108452
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.10368v1)
- **Published**: 2021-12-20 07:32:39+00:00
- **Updated**: 2021-12-20 07:32:39+00:00
- **Authors**: Haigen Hu, Leizhao Shen, Qiu Guan, Xiaoxin Li, Qianwei Zhou, Su Ruan
- **Comment**: None
- **Journal**: Pattern Recognition,2022,124:108452
- **Summary**: Due to the irregular shapes,various sizes and indistinguishable boundaries between the normal and infected tissues, it is still a challenging task to accurately segment the infected lesions of COVID-19 on CT images. In this paper, a novel segmentation scheme is proposed for the infections of COVID-19 by enhancing supervised information and fusing multi-scale feature maps of different levels based on the encoder-decoder architecture. To this end, a deep collaborative supervision (Co-supervision) scheme is proposed to guide the network learning the features of edges and semantics. More specifically, an Edge Supervised Module (ESM) is firstly designed to highlight low-level boundary features by incorporating the edge supervised information into the initial stage of down-sampling. Meanwhile, an Auxiliary Semantic Supervised Module (ASSM) is proposed to strengthen high-level semantic information by integrating mask supervised information into the later stage. Then an Attention Fusion Module (AFM) is developed to fuse multiple scale feature maps of different levels by using an attention mechanism to reduce the semantic gaps between high-level and low-level feature maps. Finally, the effectiveness of the proposed scheme is demonstrated on four various COVID-19 CT datasets. The results show that the proposed three modules are all promising. Based on the baseline (ResUnet), using ESM, ASSM, or AFM alone can respectively increase Dice metric by 1.12\%, 1.95\%,1.63\% in our dataset, while the integration by incorporating three models together can rise 3.97\%. Compared with the existing approaches in various datasets, the proposed method can obtain better segmentation performance in some main metrics, and can achieve the best generalization and comprehensive performance.



### Hateful Memes Challenge: An Enhanced Multimodal Framework
- **Arxiv ID**: http://arxiv.org/abs/2112.11244v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.11244v1)
- **Published**: 2021-12-20 07:47:17+00:00
- **Updated**: 2021-12-20 07:47:17+00:00
- **Authors**: Aijing Gao, Bingjun Wang, Jiaqi Yin, Yating Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Hateful Meme Challenge proposed by Facebook AI has attracted contestants around the world. The challenge focuses on detecting hateful speech in multimodal memes. Various state-of-the-art deep learning models have been applied to this problem and the performance on challenge's leaderboard has also been constantly improved. In this paper, we enhance the hateful detection framework, including utilizing Detectron for feature extraction, exploring different setups of VisualBERT and UNITER models with different loss functions, researching the association between the hateful memes and the sensitive text features, and finally building ensemble method to boost model performance. The AUROC of our fine-tuned VisualBERT, UNITER, and ensemble method achieves 0.765, 0.790, and 0.803 on the challenge's test set, respectively, which beats the baseline models. Our code is available at https://github.com/yatingtian/hateful-meme



### Multimodal Adversarially Learned Inference with Factorized Discriminators
- **Arxiv ID**: http://arxiv.org/abs/2112.10384v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.10384v1)
- **Published**: 2021-12-20 08:18:49+00:00
- **Updated**: 2021-12-20 08:18:49+00:00
- **Authors**: Wenxue Chen, Jianke Zhu
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Learning from multimodal data is an important research topic in machine learning, which has the potential to obtain better representations. In this work, we propose a novel approach to generative modeling of multimodal data based on generative adversarial networks. To learn a coherent multimodal generative model, we show that it is necessary to align different encoder distributions with the joint decoder distribution simultaneously. To this end, we construct a specific form of the discriminator to enable our model to utilize data efficiently, which can be trained constrastively. By taking advantage of contrastive learning through factorizing the discriminator, we train our model on unimodal data. We have conducted experiments on the benchmark datasets, whose promising results show that our proposed approach outperforms the-state-of-the-art methods on a variety of metrics. The source code will be made publicly available.



### Evaluation and Comparison of Deep Learning Methods for Pavement Crack Identification with Visual Images
- **Arxiv ID**: http://arxiv.org/abs/2112.10390v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T07, I.5
- **Links**: [PDF](http://arxiv.org/pdf/2112.10390v1)
- **Published**: 2021-12-20 08:23:43+00:00
- **Updated**: 2021-12-20 08:23:43+00:00
- **Authors**: Kai-Liang Lu
- **Comment**: This work will be submitted for possible publication. It is a further
  study from 2012.14704v2
- **Journal**: Frontiers in Artificial Intelligence and Applications (CECNet2022)
- **Summary**: Compared with contact detection techniques, pavement crack identification with visual images via deep learning algorithms has the advantages of not being limited by the material of object to be detected, fast speed and low cost. The fundamental frameworks and typical model architectures of transfer learning (TL), encoder-decoder (ED), generative adversarial networks (GAN), and their common modules were first reviewed, and then the evolution of convolutional neural network (CNN) backbone models and GAN models were summarized. The crack classification, segmentation performance, and effect were tested on the SDNET2018 and CFD public data sets. In the aspect of patch sample classification, the fine-tuned TL models can be equivalent to or even slightly better than the ED models in accuracy, and the predicting time is faster; In the aspect of accurate crack location, both ED and GAN algorithms can achieve pixel-level segmentation and is expected to be detected in real time on low computing power platform. Furthermore, a weakly supervised learning framework of combined TL-SSGAN and its performance enhancement measures are proposed, which can maintain comparable crack identification performance with that of the supervised learning, while greatly reducing the number of labeled samples required.



### Projected Sliced Wasserstein Autoencoder-based Hyperspectral Images Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2112.11243v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11243v3)
- **Published**: 2021-12-20 09:21:02+00:00
- **Updated**: 2022-03-20 02:16:35+00:00
- **Authors**: Yurong Chen, Hui Zhang, Yaonan Wang, Q. M. Jonathan Wu, Yimin Yang
- **Comment**: I need revise this paper
- **Journal**: None
- **Summary**: Anomaly detection (AD) has been an active research area in various domains. Yet, the increasing data scale, complexity, and dimension turn the traditional methods into challenging. Recently, the deep generative model, such as the variational autoencoder (VAE), has sparked a renewed interest in the AD problem. However, the probability distribution divergence used as the regularization is too strong, which causes the model cannot capture the manifold of the true data. In this paper, we propose the Projected Sliced Wasserstein (PSW) autoencoder-based anomaly detection method. Rooted in the optimal transportation, the PSW distance is a weaker distribution measure compared with $f$-divergence. In particular, the computation-friendly eigen-decomposition method is leveraged to find the principal component for slicing the high-dimensional data. In this case, the Wasserstein distance can be calculated with the closed-form, even the prior distribution is not Gaussian. Comprehensive experiments conducted on various real-world hyperspectral anomaly detection benchmarks demonstrate the superior performance of the proposed method.



### UFPMP-Det: Toward Accurate and Efficient Object Detection on Drone Imagery
- **Arxiv ID**: http://arxiv.org/abs/2112.10415v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10415v2)
- **Published**: 2021-12-20 09:28:44+00:00
- **Updated**: 2022-01-01 10:18:13+00:00
- **Authors**: Yecheng Huang, Jiaxin Chen, Di Huang
- **Comment**: 8 pages, 6 figures, Accepted by AAAI2022
- **Journal**: None
- **Summary**: This paper proposes a novel approach to object detection on drone imagery, namely Multi-Proxy Detection Network with Unified Foreground Packing (UFPMP-Det). To deal with the numerous instances of very small scales, different from the common solution that divides the high-resolution input image into quite a number of chips with low foreground ratios to perform detection on them each, the Unified Foreground Packing (UFP) module is designed, where the sub-regions given by a coarse detector are initially merged through clustering to suppress background and the resulting ones are subsequently packed into a mosaic for a single inference, thus significantly reducing overall time cost. Furthermore, to address the more serious confusion between inter-class similarities and intra-class variations of instances, which deteriorates detection performance but is rarely discussed, the Multi-Proxy Detection Network (MP-Det) is presented to model object distributions in a fine-grained manner by employing multiple proxy learning, and the proxies are enforced to be diverse by minimizing a Bag-of-Instance-Words (BoIW) guided optimal transport loss. By such means, UFPMP-Det largely promotes both the detection accuracy and efficiency. Extensive experiments are carried out on the widely used VisDrone and UAVDT datasets, and UFPMP-Det reports new state-of-the-art scores at a much higher speed, highlighting its advantages.



### Learning with Label Noise for Image Retrieval by Selecting Interactions
- **Arxiv ID**: http://arxiv.org/abs/2112.10453v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10453v2)
- **Published**: 2021-12-20 11:27:48+00:00
- **Updated**: 2021-12-21 12:41:39+00:00
- **Authors**: Sarah Ibrahimi, Arnaud Sors, Rafael Sampaio de Rezende, Stéphane Clinchant
- **Comment**: Accepted at WACV 2022. 13 pages, 5 figures
- **Journal**: None
- **Summary**: Learning with noisy labels is an active research area for image classification. However, the effect of noisy labels on image retrieval has been less studied. In this work, we propose a noise-resistant method for image retrieval named Teacher-based Selection of Interactions, T-SINT, which identifies noisy interactions, ie. elements in the distance matrix, and selects correct positive and negative interactions to be considered in the retrieval loss by using a teacher-based training setup which contributes to the stability. As a result, it consistently outperforms state-of-the-art methods on high noise rates across benchmark datasets with synthetic noise and more realistic noise.



### Image Animation with Keypoint Mask
- **Arxiv ID**: http://arxiv.org/abs/2112.10457v2
- **DOI**: 10.13140/RG.2.2.16342.16968
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10457v2)
- **Published**: 2021-12-20 11:35:06+00:00
- **Updated**: 2021-12-21 22:15:23+00:00
- **Authors**: Or Toledano, Yanir Marmor, Dov Gertz
- **Comment**: None
- **Journal**: None
- **Summary**: Motion transfer is the task of synthesizing future video frames of a single source image according to the motion from a given driving video. In order to solve it, we face the challenging complexity of motion representation and the unknown relations between the driving video and the source image. Despite its difficulty, this problem attracted great interests from researches at the recent years, with gradual improvements. The goal is often thought as the decoupling of motion and appearance, which is may be solved by extracting the motion from keypoint movement. We chose to tackle the generic, unsupervised setting, where we need to apply animation to any arbitrary object, without any domain specific model for the structure of the input. In this work, we extract the structure from a keypoint heatmap, without an explicit motion representation. Then, the structures from the image and the video are extracted to warp the image according to the video, by a deep generator. We suggest two variants of the structure from different steps in the keypoint module, and show superior qualitative pose and quantitative scores.



### Reciprocal Normalization for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2112.10474v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10474v1)
- **Published**: 2021-12-20 12:17:22+00:00
- **Updated**: 2021-12-20 12:17:22+00:00
- **Authors**: Zhiyong Huang, Kekai Sheng, Ke Li, Jian Liang, Taiping Yao, Weiming Dong, Dengwen Zhou, Xing Sun
- **Comment**: The best feature normalization module for domain adaptation
- **Journal**: None
- **Summary**: Batch normalization (BN) is widely used in modern deep neural networks, which has been shown to represent the domain-related knowledge, and thus is ineffective for cross-domain tasks like unsupervised domain adaptation (UDA). Existing BN variant methods aggregate source and target domain knowledge in the same channel in normalization module. However, the misalignment between the features of corresponding channels across domains often leads to a sub-optimal transferability. In this paper, we exploit the cross-domain relation and propose a novel normalization method, Reciprocal Normalization (RN). Specifically, RN first presents a Reciprocal Compensation (RC) module to acquire the compensatory for each channel in both domains based on the cross-domain channel-wise correlation. Then RN develops a Reciprocal Aggregation (RA) module to adaptively aggregate the feature with its cross-domain compensatory components. As an alternative to BN, RN is more suitable for UDA problems and can be easily integrated into popular domain adaptation methods. Experiments show that the proposed RN outperforms existing normalization counterparts by a large margin and helps state-of-the-art adaptation approaches achieve better results. The source code is available on https://github.com/Openning07/reciprocal-normalization-for-DA.



### a novel attention-based network for fast salient object detection
- **Arxiv ID**: http://arxiv.org/abs/2112.10481v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10481v1)
- **Published**: 2021-12-20 12:30:20+00:00
- **Updated**: 2021-12-20 12:30:20+00:00
- **Authors**: Bin Zhang, Yang Wu, Xiaojing Zhang, Ming Ma
- **Comment**: None
- **Journal**: None
- **Summary**: In the current salient object detection network, the most popular method is using U-shape structure. However, the massive number of parameters leads to more consumption of computing and storage resources which are not feasible to deploy on the limited memory device. Some others shallow layer network will not maintain the same accuracy compared with U-shape structure and the deep network structure with more parameters will not converge to a global minimum loss with great speed. To overcome all of these disadvantages, we proposed a new deep convolution network architecture with three contributions: (1) using smaller convolution neural networks (CNNs) to compress the model in our improved salient object features compression and reinforcement extraction module (ISFCREM) to reduce parameters of the model. (2) introducing channel attention mechanism in ISFCREM to weigh different channels for improving the ability of feature representation. (3) applying a new optimizer to accumulate the long-term gradient information during training to adaptively tune the learning rate. The results demonstrate that the proposed method can compress the model to 1/3 of the original size nearly without losing the accuracy and converging faster and more smoothly on six widely used datasets of salient object detection compared with the others models. Our code is published in https://gitee.com/binzhangbinzhangbin/code-a-novel-attention-based-network-for-fast-salient-object-detection.git



### ScanQA: 3D Question Answering for Spatial Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2112.10482v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10482v3)
- **Published**: 2021-12-20 12:30:55+00:00
- **Updated**: 2022-05-07 21:55:42+00:00
- **Authors**: Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, Motoaki Kawanabe
- **Comment**: CVPR2022. The first three authors are equally contributed. Project
  page: https://github.com/ATR-DBI/ScanQA
- **Journal**: None
- **Summary**: We propose a new 3D spatial understanding task of 3D Question Answering (3D-QA). In the 3D-QA task, models receive visual information from the entire 3D scene of the rich RGB-D indoor scan and answer the given textual questions about the 3D scene. Unlike the 2D-question answering of VQA, the conventional 2D-QA models suffer from problems with spatial understanding of object alignment and directions and fail the object identification from the textual questions in 3D-QA. We propose a baseline model for 3D-QA, named ScanQA model, where the model learns a fused descriptor from 3D object proposals and encoded sentence embeddings. This learned descriptor correlates the language expressions with the underlying geometric features of the 3D scan and facilitates the regression of 3D bounding boxes to determine described objects in textual questions and outputs correct answers. We collected human-edited question-answer pairs with free-form answers that are grounded to 3D objects in each 3D scene. Our new ScanQA dataset contains over 40K question-answer pairs from the 800 indoor scenes drawn from the ScanNet dataset. To the best of our knowledge, the proposed 3D-QA task is the first large-scale effort to perform object-grounded question-answering in 3D environments.



### Fusion and Orthogonal Projection for Improved Face-Voice Association
- **Arxiv ID**: http://arxiv.org/abs/2112.10483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10483v1)
- **Published**: 2021-12-20 12:33:33+00:00
- **Updated**: 2021-12-20 12:33:33+00:00
- **Authors**: Muhammad Saad Saeed, Muhammad Haris Khan, Shah Nawaz, Muhammad Haroon Yousaf, Alessio Del Bue
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of learning association between face and voice, which is gaining interest in the computer vision community lately. Prior works adopt pairwise or triplet loss formulations to learn an embedding space amenable for associated matching and verification tasks. Albeit showing some progress, such loss formulations are, however, restrictive due to dependency on distance-dependent margin parameter, poor run-time training complexity, and reliance on carefully crafted negative mining procedures. In this work, we hypothesize that enriched feature representation coupled with an effective yet efficient supervision is necessary in realizing a discriminative joint embedding space for improved face-voice association. To this end, we propose a light-weight, plug-and-play mechanism that exploits the complementary cues in both modalities to form enriched fused embeddings and clusters them based on their identity labels via orthogonality constraints. We coin our proposed mechanism as fusion and orthogonal projection (FOP) and instantiate in a two-stream pipeline. The overall resulting framework is evaluated on a large-scale VoxCeleb dataset with a multitude of tasks, including cross-modal verification and matching. Results show that our method performs favourably against the current state-of-the-art methods and our proposed supervision formulation is more effective and efficient than the ones employed by the contemporary methods.



### Scale-Net: Learning to Reduce Scale Differences for Large-Scale Invariant Image Matching
- **Arxiv ID**: http://arxiv.org/abs/2112.10485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10485v1)
- **Published**: 2021-12-20 12:35:36+00:00
- **Updated**: 2021-12-20 12:35:36+00:00
- **Authors**: Yujie Fu, Yihong Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Most image matching methods perform poorly when encountering large scale changes in images. To solve this problem, firstly, we propose a scale-difference-aware image matching method (SDAIM) that reduces image scale differences before local feature extraction, via resizing both images of an image pair according to an estimated scale ratio. Secondly, in order to accurately estimate the scale ratio, we propose a covisibility-attention-reinforced matching module (CVARM) and then design a novel neural network, termed as Scale-Net, based on CVARM. The proposed CVARM can lay more stress on covisible areas within the image pair and suppress the distraction from those areas visible in only one image. Quantitative and qualitative experiments confirm that the proposed Scale-Net has higher scale ratio estimation accuracy and much better generalization ability compared with all the existing scale ratio estimation methods. Further experiments on image matching and relative pose estimation tasks demonstrate that our SDAIM and Scale-Net are able to greatly boost the performance of representative local features and state-of-the-art local feature matching methods.



### HarmoFL: Harmonizing Local and Global Drifts in Federated Learning on Heterogeneous Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2112.10775v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.10775v3)
- **Published**: 2021-12-20 13:25:48+00:00
- **Updated**: 2022-04-24 07:06:49+00:00
- **Authors**: Meirui Jiang, Zirui Wang, Qi Dou
- **Comment**: Accepted at AAAI 2022
- **Journal**: None
- **Summary**: Multiple medical institutions collaboratively training a model using federated learning (FL) has become a promising solution for maximizing the potential of data-driven models, yet the non-independent and identically distributed (non-iid) data in medical images is still an outstanding challenge in real-world practice. The feature heterogeneity caused by diverse scanners or protocols introduces a drift in the learning process, in both local (client) and global (server) optimizations, which harms the convergence as well as model performance. Many previous works have attempted to address the non-iid issue by tackling the drift locally or globally, but how to jointly solve the two essentially coupled drifts is still unclear. In this work, we concentrate on handling both local and global drifts and introduce a new harmonizing framework called HarmoFL. First, we propose to mitigate the local update drift by normalizing amplitudes of images transformed into the frequency domain to mimic a unified imaging setting, in order to generate a harmonized feature space across local clients. Second, based on harmonized features, we design a client weight perturbation guiding each local model to reach a flat optimum, where a neighborhood area of the local optimal solution has a uniformly low loss. Without any extra communication cost, the perturbation assists the global model to optimize towards a converged optimal solution by aggregating several local flat optima. We have theoretically analyzed the proposed method and empirically conducted extensive experiments on three medical image classification and segmentation tasks, showing that HarmoFL outperforms a set of recent state-of-the-art methods with promising convergence behavior. Code is available at https://github.com/med-air/HarmoFL.



### Unsupervised deep learning techniques for powdery mildew recognition based on multispectral imaging
- **Arxiv ID**: http://arxiv.org/abs/2112.11242v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2112.11242v1)
- **Published**: 2021-12-20 13:29:13+00:00
- **Updated**: 2021-12-20 13:29:13+00:00
- **Authors**: Alessandro Benfenati, Paola Causin, Roberto Oberti, Giovanni Stefanello
- **Comment**: None
- **Journal**: None
- **Summary**: Objectives. Sustainable management of plant diseases is an open challenge which has relevant economic and environmental impact. Optimal strategies rely on human expertise for field scouting under favourable conditions to assess the current presence and extent of disease symptoms. This labor-intensive task is complicated by the large field area to be scouted, combined with the millimeter-scale size of the early symptoms to be detected. In view of this, image-based detection of early disease symptoms is an attractive approach to automate this process, enabling a potential high throughput monitoring at sustainable costs.   Methods. Deep learning has been successfully applied in various domains to obtain an automatic selection of the relevant image features by learning filters via a training procedure. Deep learning has recently entered also the domain of plant disease detection: following this idea, in this work we present a deep learning approach to automatically recognize powdery mildew on cucumber leaves. We focus on unsupervised deep learning techniques applied to multispectral imaging data and we propose the use of autoencoder architectures to investigate two strategies for disease detection: i) clusterization of features in a compressed space; ii) anomaly detection.   Results. The two proposed approaches have been assessed by quantitative indices. The clusterization approach is not fully capable by itself to provide accurate predictions but it does cater relevant information. Anomaly detection has instead a significant potential of resolution which could be further exploited as a prior for supervised architectures with a very limited number of labeled samples.



### Object Recognition as Classification via Visual Properties
- **Arxiv ID**: http://arxiv.org/abs/2112.10531v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.10531v2)
- **Published**: 2021-12-20 13:50:07+00:00
- **Updated**: 2021-12-28 19:07:10+00:00
- **Authors**: Fausto Giunchiglia, Mayukh Bagchi
- **Comment**: None
- **Journal**: None
- **Summary**: We base our work on the teleosemantic modelling of concepts as abilities implementing the distinct functions of recognition and classification. Accordingly, we model two types of concepts - substance concepts suited for object recognition exploiting visual properties, and classification concepts suited for classification of substance concepts exploiting linguistically grounded properties. The goal in this paper is to demonstrate that object recognition can be construed as classification via visual properties, as distinct from work in mainstream computer vision. Towards that, we present an object recognition process based on Ranganathan's four-phased faceted knowledge organization process, grounded in the teleosemantic distinctions of substance concept and classification concept. We also briefly introduce the ongoing project MultiMedia UKC, whose aim is to build an object recognition resource following our proposed process



### Implicit Neural Representation Learning for Hyperspectral Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2112.10541v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.10541v1)
- **Published**: 2021-12-20 14:07:54+00:00
- **Updated**: 2021-12-20 14:07:54+00:00
- **Authors**: Kaiwei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) super-resolution without additional auxiliary image remains a constant challenge due to its high-dimensional spectral patterns, where learning an effective spatial and spectral representation is a fundamental issue. Recently, Implicit Neural Representations (INRs) are making strides as a novel and effective representation, especially in the reconstruction task. Therefore, in this work, we propose a novel HSI reconstruction model based on INR which represents HSI by a continuous function mapping a spatial coordinate to its corresponding spectral radiance values. In particular, as a specific implementation of INR, the parameters of parametric model are predicted by a hypernetwork that operates on feature extraction using convolution network. It makes the continuous functions map the spatial coordinates to pixel values in a content-aware manner. Moreover, periodic spatial encoding are deeply integrated with the reconstruction procedure, which makes our model capable of recovering more high frequency details. To verify the efficacy of our model, we conduct experiments on three HSI datasets (CAVE, NUS, and NTIRE2018). Experimental results show that the proposed model can achieve competitive reconstruction performance in comparison with the state-of-the-art methods. In addition, we provide an ablation study on the effect of individual components of our model. We hope this paper could server as a potent reference for future research.



### Dynamic Hypergraph Convolutional Networks for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.10570v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10570v1)
- **Published**: 2021-12-20 14:46:14+00:00
- **Updated**: 2021-12-20 14:46:14+00:00
- **Authors**: Jinfeng Wei, Yunxin Wang, Mengli Guo, Pei Lv, Xiaoshan Yang, Mingliang Xu
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: Graph convolutional networks (GCNs) based methods have achieved advanced performance on skeleton-based action recognition task. However, the skeleton graph cannot fully represent the motion information contained in skeleton data. In addition, the topology of the skeleton graph in the GCN-based methods is manually set according to natural connections, and it is fixed for all samples, which cannot well adapt to different situations. In this work, we propose a novel dynamic hypergraph convolutional networks (DHGCN) for skeleton-based action recognition. DHGCN uses hypergraph to represent the skeleton structure to effectively exploit the motion information contained in human joints. Each joint in the skeleton hypergraph is dynamically assigned the corresponding weight according to its moving, and the hypergraph topology in our model can be dynamically adjusted to different samples according to the relationship between the joints. Experimental results demonstrate that the performance of our model achieves competitive performance on three datasets: Kinetics-Skeleton 400, NTU RGB+D 60, and NTU RGB+D 120.



### General Greedy De-bias Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.10572v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.10572v5)
- **Published**: 2021-12-20 14:47:32+00:00
- **Updated**: 2023-01-19 03:02:35+00:00
- **Authors**: Xinzhe Han, Shuhui Wang, Chi Su, Qingming Huang, Qi Tian
- **Comment**: This work has been accepted by IEEE T-PAMI. Copyright is transferred
  without notice, after which this version may no longer be accessible
- **Journal**: None
- **Summary**: Neural networks often make predictions relying on the spurious correlations from the datasets rather than the intrinsic properties of the task of interest, facing sharp degradation on out-of-distribution (OOD) test data. Existing de-bias learning frameworks try to capture specific dataset bias by annotations but they fail to handle complicated OOD scenarios. Others implicitly identify the dataset bias by special design low capability biased models or losses, but they degrade when the training and testing data are from the same distribution. In this paper, we propose a General Greedy De-bias learning framework (GGD), which greedily trains the biased models and the base model. The base model is encouraged to focus on examples that are hard to solve with biased models, thus remaining robust against spurious correlations in the test stage. GGD largely improves models' OOD generalization ability on various tasks, but sometimes over-estimates the bias level and degrades on the in-distribution test. We further re-analyze the ensemble process of GGD and introduce the Curriculum Regularization inspired by curriculum learning, which achieves a good trade-off between in-distribution and out-of-distribution performance. Extensive experiments on image classification, adversarial question answering, and visual question answering demonstrate the effectiveness of our method. GGD can learn a more robust base model under the settings of both task-specific biased models with prior knowledge and self-ensemble biased model without prior knowledge.



### Image-free multi-character recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.10587v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10587v1)
- **Published**: 2021-12-20 15:06:49+00:00
- **Updated**: 2021-12-20 15:06:49+00:00
- **Authors**: Huayi Wang, Chunli Zhu, Liheng Bian
- **Comment**: 17pages, 4figures
- **Journal**: None
- **Summary**: The recently developed image-free sensing technique maintains the advantages of both the light hardware and software, which has been applied in simple target classification and motion tracking. In practical applications, however, there usually exist multiple targets in the field of view, where existing trials fail to produce multi-semantic information. In this letter, we report a novel image-free sensing technique to tackle the multi-target recognition challenge for the first time. Different from the convolutional layer stack of image-free single-pixel networks, the reported CRNN network utilities the bidirectional LSTM architecture to predict the distribution of multiple characters simultaneously. The framework enables to capture the long-range dependencies, providing a high recognition accuracy of multiple characters. We demonstrated the technique's effectiveness in license plate detection, which achieved 87.60% recognition accuracy at a 5% sampling rate with a higher than 100 FPS refresh rate.



### Real-Time Optical Flow for Vehicular Perception with Low- and High-Resolution Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/2112.10591v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2112.10591v1)
- **Published**: 2021-12-20 15:09:20+00:00
- **Updated**: 2021-12-20 15:09:20+00:00
- **Authors**: Vincent Brebion, Julien Moreau, Franck Davoine
- **Comment**: 13 pages, journal paper
- **Journal**: None
- **Summary**: Event cameras capture changes of illumination in the observed scene rather than accumulating light to create images. Thus, they allow for applications under high-speed motion and complex lighting conditions, where traditional framebased sensors show their limits with blur and over- or underexposed pixels. Thanks to these unique properties, they represent nowadays an highly attractive sensor for ITS-related applications. Event-based optical flow (EBOF) has been studied following the rise in popularity of these neuromorphic cameras. The recent arrival of high-definition neuromorphic sensors, however, challenges the existing approaches, because of the increased resolution of the events pixel array and a much higher throughput. As an answer to these points, we propose an optimized framework for computing optical flow in real-time with both low- and high-resolution event cameras. We formulate a novel dense representation for the sparse events flow, in the form of the "inverse exponential distance surface". It serves as an interim frame, designed for the use of proven, state-of-the-art frame-based optical flow computation methods. We evaluate our approach on both low- and high-resolution driving sequences, and show that it often achieves better results than the current state of the art, while also reaching higher frame rates, 250Hz at 346 x 260 pixels and 77Hz at 1280 x 720 pixels.



### DeePaste -- Inpainting for Pasting
- **Arxiv ID**: http://arxiv.org/abs/2112.10600v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10600v2)
- **Published**: 2021-12-20 15:13:45+00:00
- **Updated**: 2021-12-26 12:40:17+00:00
- **Authors**: Levi Kassel Michael Werman
- **Comment**: None
- **Journal**: None
- **Summary**: One of the challenges of supervised learning training is the need to procure an substantial amount of tagged data. A well-known method of solving this problem is to use synthetic data in a copy-paste fashion, so that we cut objects and paste them onto relevant backgrounds. Pasting the objects naively results in artifacts that cause models to give poor results on real data. We present a new method for cleanly pasting objects on different backgrounds so that the dataset created gives competitive performance on real data. The main emphasis is on the treatment of the border of the pasted object using inpainting. We show state-of-the-art results both on instance detection and foreground segmentation



### A Multi-user Oriented Live Free-viewpoint Video Streaming System Based On View Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2112.10603v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.10603v2)
- **Published**: 2021-12-20 15:17:57+00:00
- **Updated**: 2021-12-22 06:43:47+00:00
- **Authors**: Jingchuan Hu, Shuai Guo, Kai Zhou, Yu Dong, Jun Xu, Li Song
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: As an important application form of immersive multimedia services, free-viewpoint video(FVV) enables users with great immersive experience by strong interaction. However, the computational complexity of virtual view synthesis algorithms poses a significant challenge to the real-time performance of an FVV system. Furthermore, the individuality of user interaction makes it difficult to serve multiple users simultaneously for a system with conventional architecture. In this paper, we novelly introduce a CNN-based view interpolation algorithm to synthesis dense virtual views in real time. Based on this, we also build an end-to-end live free-viewpoint system with a multi-user oriented streaming strategy. Our system can utilize a single edge server to serve multiple users at the same time without having to bring a large view synthesis load on the client side. We analyze the whole system and show that our approaches give the user a pleasant immersive experience, in terms of both visual quality and latency.



### Learning to integrate vision data into road network data
- **Arxiv ID**: http://arxiv.org/abs/2112.10624v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10624v2)
- **Published**: 2021-12-20 15:38:49+00:00
- **Updated**: 2022-03-02 15:32:40+00:00
- **Authors**: Oliver Stromann, Alireza Razavi, Michael Felsberg
- **Comment**: None
- **Journal**: None
- **Summary**: Road networks are the core infrastructure for connected and autonomous vehicles, but creating meaningful representations for machine learning applications is a challenging task. In this work, we propose to integrate remote sensing vision data into road network data for improved embeddings with graph neural networks. We present a segmentation of road edges based on spatio-temporal road and traffic characteristics, which allows to enrich the attribute set of road networks with visual features of satellite imagery and digital surface models. We show that both, the segmentation and the integration of vision data can increase performance on a road type classification task, and we achieve state-of-the-art performance on the OSM+DiDi Chuxing dataset on Chengdu, China.



### Raw High-Definition Radar for Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.10646v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.10646v3)
- **Published**: 2021-12-20 16:15:26+00:00
- **Updated**: 2022-04-13 13:48:20+00:00
- **Authors**: Julien Rebut, Arthur Ouaknine, Waqas Malik, Patrick Pérez
- **Comment**: 12 pages, 7 figures, 6 tables
- **Journal**: CVPR2022
- **Summary**: With their robustness to adverse weather conditions and ability to measure speeds, radar sensors have been part of the automotive landscape for more than two decades. Recent progress toward High Definition (HD) Imaging radar has driven the angular resolution below the degree, thus approaching laser scanning performance. However, the amount of data a HD radar delivers and the computational cost to estimate the angular positions remain a challenge. In this paper, we propose a novel HD radar sensing model, FFT-RadNet, that eliminates the overhead of computing the range-azimuth-Doppler 3D tensor, learning instead to recover angles from a range-Doppler spectrum. FFT-RadNet is trained both to detect vehicles and to segment free driving space. On both tasks, it competes with the most recent radar-based models while requiring less compute and memory. Also, we collected and annotated 2-hour worth of raw data from synchronized automotive-grade sensors (camera, laser, HD radar) in various environments (city street, highway, countryside road). This unique dataset, nick-named RADIal for "Radar, Lidar et al.", is available at https://github.com/valeoai/RADIal.



### HyperSegNAS: Bridging One-Shot Neural Architecture Search with 3D Medical Image Segmentation using HyperNet
- **Arxiv ID**: http://arxiv.org/abs/2112.10652v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.10652v2)
- **Published**: 2021-12-20 16:21:09+00:00
- **Updated**: 2022-03-24 13:56:32+00:00
- **Authors**: Cheng Peng, Andriy Myronenko, Ali Hatamizadeh, Vish Nath, Md Mahfuzur Rahman Siddiquee, Yufan He, Daguang Xu, Rama Chellappa, Dong Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation of 3D medical images is a challenging task due to the high variability of the shape and pattern of objects (such as organs or tumors). Given the recent success of deep learning in medical image segmentation, Neural Architecture Search (NAS) has been introduced to find high-performance 3D segmentation network architectures. However, because of the massive computational requirements of 3D data and the discrete optimization nature of architecture search, previous NAS methods require a long search time or necessary continuous relaxation, and commonly lead to sub-optimal network architectures. While one-shot NAS can potentially address these disadvantages, its application in the segmentation domain has not been well studied in the expansive multi-scale multi-path search space. To enable one-shot NAS for medical image segmentation, our method, named HyperSegNAS, introduces a HyperNet to assist super-net training by incorporating architecture topology information. Such a HyperNet can be removed once the super-net is trained and introduces no overhead during architecture search. We show that HyperSegNAS yields better performing and more intuitive architectures compared to the previous state-of-the-art (SOTA) segmentation networks; furthermore, it can quickly and accurately find good architecture candidates under different computing constraints. Our method is evaluated on public datasets from the Medical Segmentation Decathlon (MSD) challenge, and achieves SOTA performances.



### Attention-Based Sensor Fusion for Human Activity Recognition Using IMU Signals
- **Arxiv ID**: http://arxiv.org/abs/2112.11224v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2112.11224v1)
- **Published**: 2021-12-20 17:00:27+00:00
- **Updated**: 2021-12-20 17:00:27+00:00
- **Authors**: Wenjin Tao, Haodong Chen, Md Moniruzzaman, Ming C. Leu, Zhaozheng Yi, Ruwen Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Human Activity Recognition (HAR) using wearable devices such as smart watches embedded with Inertial Measurement Unit (IMU) sensors has various applications relevant to our daily life, such as workout tracking and health monitoring. In this paper, we propose a novel attention-based approach to human activity recognition using multiple IMU sensors worn at different body locations. Firstly, a sensor-wise feature extraction module is designed to extract the most discriminative features from individual sensors with Convolutional Neural Networks (CNNs). Secondly, an attention-based fusion mechanism is developed to learn the importance of sensors at different body locations and to generate an attentive feature representation. Finally, an inter-sensor feature extraction module is applied to learn the inter-sensor correlations, which are connected to a classifier to output the predicted classes of activities. The proposed approach is evaluated using five public datasets and it outperforms state-of-the-art methods on a wide variety of activity categories.



### SelFSR: Self-Conditioned Face Super-Resolution in the Wild via Flow Field Degradation Network
- **Arxiv ID**: http://arxiv.org/abs/2112.10683v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.10683v1)
- **Published**: 2021-12-20 17:04:00+00:00
- **Updated**: 2021-12-20 17:04:00+00:00
- **Authors**: Xianfang Zeng, Jiangning Zhang, Liang Liu, Guangzhong Tian, Yong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In spite of the success on benchmark datasets, most advanced face super-resolution models perform poorly in real scenarios since the remarkable domain gap between the real images and the synthesized training pairs. To tackle this problem, we propose a novel domain-adaptive degradation network for face super-resolution in the wild. This degradation network predicts a flow field along with an intermediate low resolution image. Then, the degraded counterpart is generated by warping the intermediate image. With the preference of capturing motion blur, such a model performs better at preserving identity consistency between the original images and the degraded. We further present the self-conditioned block for super-resolution network. This block takes the input image as a condition term to effectively utilize facial structure information, eliminating the reliance on explicit priors, e.g. facial landmarks or boundary. Our model achieves state-of-the-art performance on both CelebA and real-world face dataset. The former demonstrates the powerful generative ability of our proposed architecture while the latter shows great identity consistency and perceptual quality in real-world images.



### Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual Fly-Throughs
- **Arxiv ID**: http://arxiv.org/abs/2112.10703v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.10703v2)
- **Published**: 2021-12-20 17:40:48+00:00
- **Updated**: 2022-03-28 22:21:38+00:00
- **Authors**: Haithem Turki, Deva Ramanan, Mahadev Satyanarayanan
- **Comment**: CVPR 2022 Project page: https://meganerf.cmusatyalab.org GitHub:
  https://github.com/cmusatyalab/mega-nerf
- **Journal**: None
- **Summary**: We use neural radiance fields (NeRFs) to build interactive 3D environments from large-scale visual captures spanning buildings or even multiple city blocks collected primarily from drones. In contrast to single object scenes (on which NeRFs are traditionally evaluated), our scale poses multiple challenges including (1) the need to model thousands of images with varying lighting conditions, each of which capture only a small subset of the scene, (2) prohibitively large model capacities that make it infeasible to train on a single GPU, and (3) significant challenges for fast rendering that would enable interactive fly-throughs.   To address these challenges, we begin by analyzing visibility statistics for large-scale scenes, motivating a sparse network structure where parameters are specialized to different regions of the scene. We introduce a simple geometric clustering algorithm for data parallelism that partitions training images (or rather pixels) into different NeRF submodules that can be trained in parallel.   We evaluate our approach on existing datasets (Quad 6k and UrbanScene3D) as well as against our own drone footage, improving training speed by 3x and PSNR by 12%. We also evaluate recent NeRF fast renderers on top of Mega-NeRF and introduce a novel method that exploits temporal coherence. Our technique achieves a 40x speedup over conventional NeRF rendering while remaining within 0.8 db in PSNR quality, exceeding the fidelity of existing fast renderers.



### Learning Spatio-Temporal Specifications for Dynamical Systems
- **Arxiv ID**: http://arxiv.org/abs/2112.10714v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, cs.SY, eess.SY, I.5.3, I.5.4, B.1.0
- **Links**: [PDF](http://arxiv.org/pdf/2112.10714v1)
- **Published**: 2021-12-20 18:03:01+00:00
- **Updated**: 2021-12-20 18:03:01+00:00
- **Authors**: Suhail Alsalehi, Erfan Aasi, Ron Weiss, Calin Belta
- **Comment**: 12 pages, submitted to L4DC 2021
- **Journal**: PMLR 168:968-980, 2022
- **Summary**: Learning dynamical systems properties from data provides important insights that help us understand such systems and mitigate undesired outcomes. In this work, we propose a framework for learning spatio-temporal (ST) properties as formal logic specifications from data. We introduce SVM-STL, an extension of Signal Signal Temporal Logic (STL), capable of specifying spatial and temporal properties of a wide range of dynamical systems that exhibit time-varying spatial patterns. Our framework utilizes machine learning techniques to learn SVM-STL specifications from system executions given by sequences of spatial patterns. We present methods to deal with both labeled and unlabeled data. In addition, given system requirements in the form of SVM-STL specifications, we provide an approach for parameter synthesis to find parameters that maximize the satisfaction of such specifications. Our learning framework and parameter synthesis approach are showcased in an example of a reaction-diffusion system.



### BAPose: Bottom-Up Pose Estimation with Disentangled Waterfall Representations
- **Arxiv ID**: http://arxiv.org/abs/2112.10716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10716v1)
- **Published**: 2021-12-20 18:07:09+00:00
- **Updated**: 2021-12-20 18:07:09+00:00
- **Authors**: Bruno Artacho, Andreas Savakis
- **Comment**: None
- **Journal**: None
- **Summary**: We propose BAPose, a novel bottom-up approach that achieves state-of-the-art results for multi-person pose estimation. Our end-to-end trainable framework leverages a disentangled multi-scale waterfall architecture and incorporates adaptive convolutions to infer keypoints more precisely in crowded scenes with occlusions. The multi-scale representations, obtained by the disentangled waterfall module in BAPose, leverage the efficiency of progressive filtering in the cascade architecture, while maintaining multi-scale fields-of-view comparable to spatial pyramid configurations. Our results on the challenging COCO and CrowdPose datasets demonstrate that BAPose is an efficient and robust framework for multi-person pose estimation, achieving significant improvements on state-of-the-art accuracy.



### Learning Physics Properties of Fabrics and Garments with a Physics Similarity Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2112.10727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10727v1)
- **Published**: 2021-12-20 18:19:12+00:00
- **Updated**: 2021-12-20 18:19:12+00:00
- **Authors**: Li Duan, Lewis Boyd, Gerardo Aragon-Camarasa
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose to predict the physics parameters of real fabrics and garments by learning their physics similarities between simulated fabrics via a Physics Similarity Network (PhySNet). For this, we estimate wind speeds generated by an electric fan and the area weight to predict bending stiffness of simulated and real fabrics and garments. We found that PhySNet coupled with a Bayesian optimiser can predict physics parameters and improve the state-of-art by 34%for real fabrics and 68% for real garments.



### MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding
- **Arxiv ID**: http://arxiv.org/abs/2112.10728v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.10728v2)
- **Published**: 2021-12-20 18:23:30+00:00
- **Updated**: 2022-05-04 05:45:41+00:00
- **Authors**: Revanth Gangi Reddy, Xilin Rui, Manling Li, Xudong Lin, Haoyang Wen, Jaemin Cho, Lifu Huang, Mohit Bansal, Avirup Sil, Shih-Fu Chang, Alexander Schwing, Heng Ji
- **Comment**: Accepted at AAAI 2022
- **Journal**: None
- **Summary**: Recently, there has been an increasing interest in building question answering (QA) models that reason across multiple modalities, such as text and images. However, QA using images is often limited to just picking the answer from a pre-defined set of options. In addition, images in the real world, especially in news, have objects that are co-referential to the text, with complementary information from both modalities. In this paper, we present a new QA evaluation benchmark with 1,384 questions over news articles that require cross-media grounding of objects in images onto text. Specifically, the task involves multi-hop questions that require reasoning over image-caption pairs to identify the grounded visual object being referred to and then predicting a span from the news body text to answer the question. In addition, we introduce a novel multimedia data augmentation framework, based on cross-media knowledge extraction and synthetic question-answer generation, to automatically augment data that can provide weak supervision for this task. We evaluate both pipeline-based and end-to-end pretraining-based multimedia QA models on our benchmark, and show that they achieve promising performance, while considerably lagging behind human performance hence leaving large room for future work on this challenging new task.



### Are Large-scale Datasets Necessary for Self-Supervised Pre-training?
- **Arxiv ID**: http://arxiv.org/abs/2112.10740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10740v1)
- **Published**: 2021-12-20 18:41:32+00:00
- **Updated**: 2021-12-20 18:41:32+00:00
- **Authors**: Alaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Hervé Jegou, Edouard Grave
- **Comment**: None
- **Journal**: None
- **Summary**: Pre-training models on large scale datasets, like ImageNet, is a standard practice in computer vision. This paradigm is especially effective for tasks with small training sets, for which high-capacity models tend to overfit. In this work, we consider a self-supervised pre-training scenario that only leverages the target task data. We consider datasets, like Stanford Cars, Sketch or COCO, which are order(s) of magnitude smaller than Imagenet. Our study shows that denoising autoencoders, such as BEiT or a variant that we introduce in this paper, are more robust to the type and size of the pre-training data than popular self-supervised methods trained by comparing image embeddings.We obtain competitive performance compared to ImageNet pre-training on a variety of classification datasets, from different domains. On COCO, when pre-training solely using COCO images, the detection and instance segmentation performance surpasses the supervised ImageNet pre-training in a comparable setting.



### GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2112.10741v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.10741v3)
- **Published**: 2021-12-20 18:42:55+00:00
- **Updated**: 2022-03-08 18:18:49+00:00
- **Authors**: Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen
- **Comment**: 20 pages, 18 figures
- **Journal**: None
- **Summary**: Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.



### High-Resolution Image Synthesis with Latent Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2112.10752v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10752v2)
- **Published**: 2021-12-20 18:55:25+00:00
- **Updated**: 2022-04-13 11:38:44+00:00
- **Authors**: Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .



### Discovering State Variables Hidden in Experimental Data
- **Arxiv ID**: http://arxiv.org/abs/2112.10755v1
- **DOI**: None
- **Categories**: **math.DS**, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2112.10755v1)
- **Published**: 2021-12-20 18:56:59+00:00
- **Updated**: 2021-12-20 18:56:59+00:00
- **Authors**: Boyuan Chen, Kuang Huang, Sunand Raghupathi, Ishaan Chandratreya, Qiang Du, Hod Lipson
- **Comment**: Project website with code, data, and overview video is at:
  https://www.cs.columbia.edu/~bchen/neural-state-variables
- **Journal**: None
- **Summary**: All physical laws are described as relationships between state variables that give a complete and non-redundant description of the relevant system dynamics. However, despite the prevalence of computing power and AI, the process of identifying the hidden state variables themselves has resisted automation. Most data-driven methods for modeling physical phenomena still assume that observed data streams already correspond to relevant state variables. A key challenge is to identify the possible sets of state variables from scratch, given only high-dimensional observational data. Here we propose a new principle for determining how many state variables an observed system is likely to have, and what these variables might be, directly from video streams. We demonstrate the effectiveness of this approach using video recordings of a variety of physical dynamical systems, ranging from elastic double pendulums to fire flames. Without any prior knowledge of the underlying physics, our algorithm discovers the intrinsic dimension of the observed dynamics and identifies candidate sets of state variables. We suggest that this approach could help catalyze the understanding, prediction and control of increasingly complex systems. Project website is at: https://www.cs.columbia.edu/~bchen/neural-state-variables



### 3D-aware Image Synthesis via Learning Structural and Textural Representations
- **Arxiv ID**: http://arxiv.org/abs/2112.10759v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10759v2)
- **Published**: 2021-12-20 18:59:40+00:00
- **Updated**: 2022-04-18 12:26:06+00:00
- **Authors**: Yinghao Xu, Sida Peng, Ceyuan Yang, Yujun Shen, Bolei Zhou
- **Comment**: CVPR 2022 camera-ready, Project page:
  https://genforce.github.io/volumegan/
- **Journal**: None
- **Summary**: Making generative models 3D-aware bridges the 2D image space and the 3D physical world yet remains challenging. Recent attempts equip a Generative Adversarial Network (GAN) with a Neural Radiance Field (NeRF), which maps 3D coordinates to pixel values, as a 3D prior. However, the implicit function in NeRF has a very local receptive field, making the generator hard to become aware of the global structure. Meanwhile, NeRF is built on volume rendering which can be too costly to produce high-resolution results, increasing the optimization difficulty. To alleviate these two problems, we propose a novel framework, termed as VolumeGAN, for high-fidelity 3D-aware image synthesis, through explicitly learning a structural representation and a textural representation. We first learn a feature volume to represent the underlying structure, which is then converted to a feature field using a NeRF-like model. The feature field is further accumulated into a 2D feature map as the textural representation, followed by a neural renderer for appearance synthesis. Such a design enables independent control of the shape and the appearance. Extensive experiments on a wide range of datasets show that our approach achieves sufficiently higher image quality and better 3D control than the previous methods.



### StyleSwin: Transformer-based GAN for High-resolution Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2112.10762v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10762v2)
- **Published**: 2021-12-20 18:59:51+00:00
- **Updated**: 2022-07-21 01:15:06+00:00
- **Authors**: Bowen Zhang, Shuyang Gu, Bo Zhang, Jianmin Bao, Dong Chen, Fang Wen, Yong Wang, Baining Guo
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Despite the tantalizing success in a broad of vision tasks, transformers have not yet demonstrated on-par ability as ConvNets in high-resolution image generative modeling. In this paper, we seek to explore using pure transformers to build a generative adversarial network for high-resolution image synthesis. To this end, we believe that local attention is crucial to strike the balance between computational efficiency and modeling capacity. Hence, the proposed generator adopts Swin transformer in a style-based architecture. To achieve a larger receptive field, we propose double attention which simultaneously leverages the context of the local and the shifted windows, leading to improved generation quality. Moreover, we show that offering the knowledge of the absolute position that has been lost in window-based transformers greatly benefits the generation quality. The proposed StyleSwin is scalable to high resolutions, with both the coarse geometry and fine structures benefit from the strong expressivity of transformers. However, blocking artifacts occur during high-resolution synthesis because performing the local attention in a block-wise manner may break the spatial coherency. To solve this, we empirically investigate various solutions, among which we find that employing a wavelet discriminator to examine the spectral discrepancy effectively suppresses the artifacts. Extensive experiments show the superiority over prior transformer-based GANs, especially on high resolutions, e.g., 1024x1024. The StyleSwin, without complex training strategies, excels over StyleGAN on CelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the promise of using transformers for high-resolution image generation. The code and models will be available at https://github.com/microsoft/StyleSwin.



### Mask2Former for Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.10764v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.10764v1)
- **Published**: 2021-12-20 18:59:59+00:00
- **Updated**: 2021-12-20 18:59:59+00:00
- **Authors**: Bowen Cheng, Anwesa Choudhuri, Ishan Misra, Alexander Kirillov, Rohit Girdhar, Alexander G. Schwing
- **Comment**: Code and models: https://github.com/facebookresearch/Mask2Former
- **Journal**: None
- **Summary**: We find Mask2Former also achieves state-of-the-art performance on video instance segmentation without modifying the architecture, the loss or even the training pipeline. In this report, we show universal image segmentation architectures trivially generalize to video segmentation by directly predicting 3D segmentation volumes. Specifically, Mask2Former sets a new state-of-the-art of 60.4 AP on YouTubeVIS-2019 and 52.6 AP on YouTubeVIS-2021. We believe Mask2Former is also capable of handling video semantic and panoptic segmentation, given its versatility in image segmentation. We hope this will make state-of-the-art video segmentation research more accessible and bring more attention to designing universal image and video segmentation architectures.



### Lite Vision Transformer with Enhanced Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/2112.10809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10809v1)
- **Published**: 2021-12-20 19:11:53+00:00
- **Updated**: 2021-12-20 19:11:53+00:00
- **Authors**: Chenglin Yang, Yilin Wang, Jianming Zhang, He Zhang, Zijun Wei, Zhe Lin, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the impressive representation capacity of vision transformer models, current light-weight vision transformer models still suffer from inconsistent and incorrect dense predictions at local regions. We suspect that the power of their self-attention mechanism is limited in shallower and thinner networks. We propose Lite Vision Transformer (LVT), a novel light-weight transformer network with two enhanced self-attention mechanisms to improve the model performances for mobile deployment. For the low-level features, we introduce Convolutional Self-Attention (CSA). Unlike previous approaches of merging convolution and self-attention, CSA introduces local self-attention into the convolution within a kernel of size 3x3 to enrich low-level features in the first stage of LVT. For the high-level features, we propose Recursive Atrous Self-Attention (RASA), which utilizes the multi-scale context when calculating the similarity map and a recursive mechanism to increase the representation capability with marginal extra parameter cost. The superiority of LVT is demonstrated on ImageNet recognition, ADE20K semantic segmentation, and COCO panoptic segmentation. The code is made publicly available.



### One Sketch for All: One-Shot Personalized Sketch Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.10838v2
- **DOI**: 10.1109/TIP.2022.3160076
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10838v2)
- **Published**: 2021-12-20 20:10:44+00:00
- **Updated**: 2022-03-24 19:01:15+00:00
- **Authors**: Anran Qi, Yulia Gryaditskaya, Tao Xiang, Yi-Zhe Song
- **Comment**: IEEE Transactions on Image Processing, 2022
- **Journal**: None
- **Summary**: We present the first one-shot personalized sketch segmentation method. We aim to segment all sketches belonging to the same category provisioned with a single sketch with a given part annotation while (i) preserving the parts semantics embedded in the exemplar, and (ii) being robust to input style and abstraction. We refer to this scenario as personalized. With that, we importantly enable a much-desired personalization capability for downstream fine-grained sketch analysis tasks. To train a robust segmentation module, we deform the exemplar sketch to each of the available sketches of the same category. Our method generalizes to sketches not observed during training. Our central contribution is a sketch-specific hierarchical deformation network. Given a multi-level sketch-strokes encoding obtained via a graph convolutional network, our method estimates rigid-body transformation from the target to the exemplar, on the upper level. Finer deformation from the exemplar to the globally warped target sketch is further obtained through stroke-wise deformations, on the lower level. Both levels of deformation are guided by mean squared distances between the keypoints learned without supervision, ensuring that the stroke semantics are preserved. We evaluate our method against the state-of-the-art segmentation and perceptual grouping baselines re-purposed for the one-shot setting and against two few-shot 3D shape segmentation methods. We show that our method outperforms all the alternatives by more than $10\%$ on average. Ablation studies further demonstrate that our method is robust to personalization: changes in input part semantics and style differences.



### Encoding Hierarchical Information in Neural Networks helps in Subpopulation Shift
- **Arxiv ID**: http://arxiv.org/abs/2112.10844v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10844v2)
- **Published**: 2021-12-20 20:26:26+00:00
- **Updated**: 2022-06-13 21:33:46+00:00
- **Authors**: Amitangshu Mukherjee, Isha Garg, Kaushik Roy
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: Over the past decade, deep neural networks have proven to be adept in image classification tasks, often surpassing humans in terms of accuracy. However, standard neural networks often fail to understand the concept of hierarchical structures and dependencies among different classes for vision related tasks. Humans on the other hand, seem to intuitively learn categories conceptually, progressively growing from understanding high-level concepts down to granular levels of categories. One of the issues arising from the inability of neural networks to encode such dependencies within its learned structure is that of subpopulation shift -- where models are queried with novel unseen classes taken from a shifted population of the training set categories. Since the neural network treats each class as independent from all others, it struggles to categorize shifting populations that are dependent at higher levels of the hierarchy. In this work, we study the aforementioned problems through the lens of a novel conditional supervised training framework. We tackle subpopulation shift by a structured learning procedure that incorporates hierarchical information conditionally through labels. Furthermore, we introduce a notion of graphical distance to model the catastrophic effect of mispredictions. We show that learning in this structured hierarchical manner results in networks that are more robust against subpopulation shifts, with an improvement up to 3\% in terms of accuracy and up to 11\% in terms of graphical distance over standard models on subpopulation shift benchmarks.



### Translational Concept Embedding for Generalized Compositional Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.10871v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10871v1)
- **Published**: 2021-12-20 21:27:51+00:00
- **Updated**: 2021-12-20 21:27:51+00:00
- **Authors**: He Huang, Wei Tang, Jiawei Zhang, Philip S. Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Generalized compositional zero-shot learning means to learn composed concepts of attribute-object pairs in a zero-shot fashion, where a model is trained on a set of seen concepts and tested on a combined set of seen and unseen concepts. This task is very challenging because of not only the gap between seen and unseen concepts but also the contextual dependency between attributes and objects. This paper introduces a new approach, termed translational concept embedding, to solve these two difficulties in a unified framework. It models the effect of applying an attribute to an object as adding a translational attribute feature to an object prototype. We explicitly take into account of the contextual dependency between attributes and objects by generating translational attribute features conditionally dependent on the object prototypes. Furthermore, we design a ratio variance constraint loss to promote the model's generalization ability on unseen concepts. It regularizes the distances between concepts by utilizing knowledge from their pretrained word embeddings. We evaluate the performance of our model under both the unbiased and biased concept classification tasks, and show that our model is able to achieve good balance in predicting unseen and seen concepts.



### Spatiotemporal Motion Synchronization for Snowboard Big Air
- **Arxiv ID**: http://arxiv.org/abs/2112.10909v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10909v1)
- **Published**: 2021-12-20 23:30:33+00:00
- **Updated**: 2021-12-20 23:30:33+00:00
- **Authors**: Seiji Matsumura, Dan Mikami, Naoki Saijo, Makio Kashino
- **Comment**: None
- **Journal**: None
- **Summary**: During the training for snowboard big air, one of the most popular winter sports, athletes and coaches extensively shoot and check their jump attempts using a single camera or smartphone. However, by watching videos sequentially, it is difficult to compare the precise difference in performance between two trials. Therefore, side-by-side display or overlay of two videos may be helpful for training. To accomplish this, the spatial and temporal alignment of multiple performances must be ensured. In this study, we propose a conventional but plausible solution using the existing image processing techniques for snowboard big air training. We conducted interviews with expert snowboarders who stated that the spatiotemporally aligned videos enabled them to precisely identify slight differences in body movements. The results suggest that the proposed method can be used during the training of snowboard big air.



