# Arxiv Papers in cs.CV on 2021-12-24
### A formal approach to good practices in Pseudo-Labeling for Unsupervised Domain Adaptive Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2112.12887v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12887v3)
- **Published**: 2021-12-24 00:29:02+00:00
- **Updated**: 2022-02-02 11:05:01+00:00
- **Authors**: Fabian Dubourvieux, Romaric Audigier, Angélique Loesch, Samia Ainouz, Stéphane Canu
- **Comment**: This paper is a preprint under submission at CVIU for review
- **Journal**: None
- **Summary**: The use of pseudo-labels prevails in order to tackle Unsupervised Domain Adaptive (UDA) Re-Identification (re-ID) with the best performance. Indeed, this family of approaches has given rise to several UDA re-ID specific frameworks, which are effective. In these works, research directions to improve Pseudo-Labeling UDA re-ID performance are varied and mostly based on intuition and experiments: refining pseudo-labels, reducing the impact of errors in pseudo-labels... It can be hard to deduce from them general good practices, which can be implemented in any Pseudo-Labeling method, to consistently improve its performance. To address this key question, a new theoretical view on Pseudo-Labeling UDA re-ID is proposed. The contributions are threefold: (i) A novel theoretical framework for Pseudo-Labeling UDA re-ID, formalized through a new general learning upper-bound on the UDA re-ID performance. (ii) General good practices for Pseudo-Labeling, directly deduced from the interpretation of the proposed theoretical framework, in order to improve the target re-ID performance. (iii) Extensive experiments on challenging person and vehicle cross-dataset re-ID tasks, showing consistent performance improvements for various state-of-the-art methods and various proposed implementations of good practices.



### Cluster-guided Image Synthesis with Unconditional Models
- **Arxiv ID**: http://arxiv.org/abs/2112.12911v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12911v1)
- **Published**: 2021-12-24 02:18:34+00:00
- **Updated**: 2021-12-24 02:18:34+00:00
- **Authors**: Markos Georgopoulos, James Oldfield, Grigorios G Chrysos, Yannis Panagakis
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are the driving force behind the state-of-the-art in image generation. Despite their ability to synthesize high-resolution photo-realistic images, generating content with on-demand conditioning of different granularity remains a challenge. This challenge is usually tackled by annotating massive datasets with the attributes of interest, a laborious task that is not always a viable option. Therefore, it is vital to introduce control into the generation process of unsupervised generative models. In this work, we focus on controllable image generation by leveraging GANs that are well-trained in an unsupervised fashion. To this end, we discover that the representation space of intermediate layers of the generator forms a number of clusters that separate the data according to semantically meaningful attributes (e.g., hair color and pose). By conditioning on the cluster assignments, the proposed method is able to control the semantic class of the generated image. Our approach enables sampling from each cluster by Implicit Maximum Likelihood Estimation (IMLE). We showcase the efficacy of our approach on faces (CelebA-HQ and FFHQ), animals (Imagenet) and objects (LSUN) using different pre-trained generative models. The results highlight the ability of our approach to condition image generation on attributes like gender, pose and hair style on faces, as well as a variety of features on different object classes.



### Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.12916v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.12916v1)
- **Published**: 2021-12-24 02:43:42+00:00
- **Updated**: 2021-12-24 02:43:42+00:00
- **Authors**: Yue He, Chen Chen, Jing Zhang, Juhua Liu, Fengxiang He, Chaoyue Wang, Bo Du
- **Comment**: Accepted by AAAI-22
- **Journal**: None
- **Summary**: Existing Scene Text Recognition (STR) methods typically use a language model to optimize the joint probability of the 1D character sequence predicted by a visual recognition (VR) model, which ignore the 2D spatial context of visual semantics within and between character instances, making them not generalize well to arbitrary shape scene text. To address this issue, we make the first attempt to perform textual reasoning based on visual semantics in this paper. Technically, given the character segmentation maps predicted by a VR model, we construct a subgraph for each instance, where nodes represent the pixels in it and edges are added between nodes based on their spatial similarity. Then, these subgraphs are sequentially connected by their root nodes and merged into a complete graph. Based on this graph, we devise a graph convolutional network for textual reasoning (GTR) by supervising it with a cross-entropy loss. GTR can be easily plugged in representative STR models to improve their performance owing to better textual reasoning. Specifically, we construct our model, namely S-GTR, by paralleling GTR to the language model in a segmentation-based STR baseline, which can effectively exploit the visual-linguistic complementarity via mutual learning. S-GTR sets new state-of-the-art on six challenging STR benchmarks and generalizes well to multi-linguistic datasets. Code is available at https://github.com/adeline-cs/GTR.



### Multi-initialization Optimization Network for Accurate 3D Human Pose and Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/2112.12917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12917v1)
- **Published**: 2021-12-24 02:43:58+00:00
- **Updated**: 2021-12-24 02:43:58+00:00
- **Authors**: Zhiwei Liu, Xiangyu Zhu, Lu Yang, Xiang Yan, Ming Tang, Zhen Lei, Guibo Zhu, Xuetao Feng, Yan Wang, Jinqiao Wang
- **Comment**: accepted by ACM Multimedia 2021
- **Journal**: None
- **Summary**: 3D human pose and shape recovery from a monocular RGB image is a challenging task. Existing learning based methods highly depend on weak supervision signals, e.g. 2D and 3D joint location, due to the lack of in-the-wild paired 3D supervision. However, considering the 2D-to-3D ambiguities existed in these weak supervision labels, the network is easy to get stuck in local optima when trained with such labels. In this paper, we reduce the ambituity by optimizing multiple initializations. Specifically, we propose a three-stage framework named Multi-Initialization Optimization Network (MION). In the first stage, we strategically select different coarse 3D reconstruction candidates which are compatible with the 2D keypoints of input sample. Each coarse reconstruction can be regarded as an initialization leads to one optimization branch. In the second stage, we design a mesh refinement transformer (MRT) to respectively refine each coarse reconstruction result via a self-attention mechanism. Finally, a Consistency Estimation Network (CEN) is proposed to find the best result from mutiple candidates by evaluating if the visual evidence in RGB image matches a given 3D reconstruction. Experiments demonstrate that our Multi-Initialization Optimization Network outperforms existing 3D mesh based methods on multiple public benchmarks.



### BMPQ: Bit-Gradient Sensitivity Driven Mixed-Precision Quantization of DNNs from Scratch
- **Arxiv ID**: http://arxiv.org/abs/2112.13843v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.13843v1)
- **Published**: 2021-12-24 03:16:58+00:00
- **Updated**: 2021-12-24 03:16:58+00:00
- **Authors**: Souvik Kundu, Shikai Wang, Qirui Sun, Peter A. Beerel, Massoud Pedram
- **Comment**: 4 pages, 2 figures, 2 tables
- **Journal**: None
- **Summary**: Large DNNs with mixed-precision quantization can achieve ultra-high compression while retaining high classification performance. However, because of the challenges in finding an accurate metric that can guide the optimization process, these methods either sacrifice significant performance compared to the 32-bit floating-point (FP-32) baseline or rely on a compute-expensive, iterative training policy that requires the availability of a pre-trained baseline. To address this issue, this paper presents BMPQ, a training method that uses bit gradients to analyze layer sensitivities and yield mixed-precision quantized models. BMPQ requires a single training iteration but does not need a pre-trained baseline. It uses an integer linear program (ILP) to dynamically adjust the precision of layers during training, subject to a fixed hardware budget. To evaluate the efficacy of BMPQ, we conduct extensive experiments with VGG16 and ResNet18 on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. Compared to the baseline FP-32 models, BMPQ can yield models that have 15.4x fewer parameter bits with a negligible drop in accuracy. Compared to the SOTA "during training", mixed-precision training scheme, our models are 2.1x, 2.2x, and 2.9x smaller, on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively, with an improved accuracy of up to 14.54%.



### Not All Voxels Are Equal: Semantic Scene Completion from the Point-Voxel Perspective
- **Arxiv ID**: http://arxiv.org/abs/2112.12925v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12925v2)
- **Published**: 2021-12-24 03:25:40+00:00
- **Updated**: 2023-03-20 12:30:36+00:00
- **Authors**: Xiaokang Chen, Jiaxiang Tang, Jingbo Wang, Gang Zeng
- **Comment**: Accepted to AAAI 2022
- **Journal**: None
- **Summary**: We revisit Semantic Scene Completion (SSC), a useful task to predict the semantic and occupancy representation of 3D scenes, in this paper. A number of methods for this task are always based on voxelized scene representations for keeping local scene structure. However, due to the existence of visible empty voxels, these methods always suffer from heavy computation redundancy when the network goes deeper, and thus limit the completion quality. To address this dilemma, we propose our novel point-voxel aggregation network for this task. Firstly, we transfer the voxelized scenes to point clouds by removing these visible empty voxels and adopt a deep point stream to capture semantic information from the scene efficiently. Meanwhile, a light-weight voxel stream containing only two 3D convolution layers preserves local structures of the voxelized scenes. Furthermore, we design an anisotropic voxel aggregation operator to fuse the structure details from the voxel stream into the point stream, and a semantic-aware propagation module to enhance the up-sampling process in the point stream by semantic labels. We demonstrate that our model surpasses state-of-the-arts on two benchmarks by a large margin, with only depth images as the input.



### Learning Aligned Cross-Modal Representation for Generalized Zero-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2112.12927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12927v1)
- **Published**: 2021-12-24 03:35:37+00:00
- **Updated**: 2021-12-24 03:35:37+00:00
- **Authors**: Zhiyu Fang, Xiaobin Zhu, Chun Yang, Zheng Han, Jingyan Qin, Xu-Cheng Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Learning a common latent embedding by aligning the latent spaces of cross-modal autoencoders is an effective strategy for Generalized Zero-Shot Classification (GZSC). However, due to the lack of fine-grained instance-wise annotations, it still easily suffer from the domain shift problem for the discrepancy between the visual representation of diversified images and the semantic representation of fixed attributes. In this paper, we propose an innovative autoencoder network by learning Aligned Cross-Modal Representations (dubbed ACMR) for GZSC. Specifically, we propose a novel Vision-Semantic Alignment (VSA) method to strengthen the alignment of cross-modal latent features on the latent subspaces guided by a learned classifier. In addition, we propose a novel Information Enhancement Module (IEM) to reduce the possibility of latent variables collapse meanwhile encouraging the discriminative ability of latent variables. Extensive experiments on publicly available datasets demonstrate the state-of-the-art performance of our method.



### Realtime Global Attention Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.12939v1
- **DOI**: 10.1109/LRA.2022.3140443
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12939v1)
- **Published**: 2021-12-24 04:24:18+00:00
- **Updated**: 2021-12-24 04:24:18+00:00
- **Authors**: Xi Mo, Xiangyu Chen
- **Comment**: Ver1.0 for RA-L with ICRA presentation
- **Journal**: IEEE Robotics and Automation Letters 7(2022).1574-1580
- **Summary**: In this paper, we proposed an end-to-end realtime global attention neural network (RGANet) for the challenging task of semantic segmentation. Different from the encoding strategy deployed by self-attention paradigms, the proposed global attention module encodes global attention via depth-wise convolution and affine transformations. The integration of these global attention modules into a hierarchy architecture maintains high inferential performance. In addition, an improved evaluation metric, namely MGRID, is proposed to alleviate the negative effect of non-convex, widely scattered ground-truth areas. Results from extensive experiments on state-of-the-art architectures for semantic segmentation manifest the leading performance of proposed approaches for robotic monocular visual perception.



### Deep ensembles in bioimage segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.12955v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.12955v1)
- **Published**: 2021-12-24 05:54:21+00:00
- **Updated**: 2021-12-24 05:54:21+00:00
- **Authors**: Loris Nanni, Daniela Cuza, Alessandra Lumini, Andrea Loreggia, Sheryl Brahnam
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation consists in classifying each pixel of an image by assigning it to a specific label chosen from a set of all the available ones. During the last few years, a lot of attention shifted to this kind of task. Many computer vision researchers tried to apply autoencoder structures to develop models that can learn the semantics of the image as well as a low-level representation of it. In an autoencoder architecture, given an input, an encoder computes a low dimensional representation of the input that is then used by a decoder to reconstruct the original data. In this work, we propose an ensemble of convolutional neural networks (CNNs). In ensemble methods, many different models are trained and then used for classification, the ensemble aggregates the outputs of the single classifiers. The approach leverages on differences of various classifiers to improve the performance of the whole system. Diversity among the single classifiers is enforced by using different loss functions. In particular, we present a new loss function that results from the combination of Dice and Structural Similarity Index. The proposed ensemble is implemented by combining different backbone networks using the DeepLabV3+ and HarDNet environment. The proposal is evaluated through an extensive empirical evaluation on two real-world scenarios: polyp and skin segmentation. All the code is available online at https://github.com/LorisNanni.



### SGTR: End-to-end Scene Graph Generation with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2112.12970v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.12970v3)
- **Published**: 2021-12-24 07:10:18+00:00
- **Updated**: 2022-03-31 08:21:19+00:00
- **Authors**: Rongjie Li, Songyang Zhang, Xuming He
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Scene Graph Generation (SGG) remains a challenging visual understanding task due to its compositional property. Most previous works adopt a bottom-up two-stage or a point-based one-stage approach, which often suffers from high time complexity or sub-optimal designs. In this work, we propose a novel SGG method to address the aforementioned issues, formulating the task as a bipartite graph construction problem. To solve the problem, we develop a transformer-based end-to-end framework that first generates the entity and predicate proposal set, followed by inferring directed edges to form the relation triplets. In particular, we develop a new entity-aware predicate representation based on a structural predicate generator that leverages the compositional property of relationships. Moreover, we design a graph assembling module to infer the connectivity of the bipartite scene graph based on our entity-aware structure, enabling us to generate the scene graph in an end-to-end manner. Extensive experimental results show that our design is able to achieve the state-of-the-art or comparable performance on two challenging benchmarks, surpassing most of the existing approaches and enjoying higher efficiency in inference. We hope our model can serve as a strong baseline for the Transformer-based scene graph generation. Code is available: https://github.com/Scarecrow0/SGTR



### Doppler velocity-based algorithm for Clustering and Velocity Estimation of moving objects
- **Arxiv ID**: http://arxiv.org/abs/2112.12984v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.12984v1)
- **Published**: 2021-12-24 07:57:28+00:00
- **Updated**: 2021-12-24 07:57:28+00:00
- **Authors**: Mian Guo, Kai Zhong, Xiaozhi Wang
- **Comment**: 7 pages, 9 figures, 2 tables, 2 algorithms, CACRE2022
- **Journal**: None
- **Summary**: We propose a Doppler velocity-based cluster and velocity estimation algorithm based on the characteristics of FMCW LiDAR which achieves highly accurate, single-scan, and real-time motion state detection and velocity estimation. We prove the continuity of the Doppler velocity on the same object. Based on this principle, we achieve the distinction between moving objects and stationary background via region growing clustering algorithm. The obtained stationary background will be used to estimate the velocity of the FMCW LiDAR by the least-squares method. Then we estimate the velocity of the moving objects using the estimated LiDAR velocity and the Doppler velocity of moving objects obtained by clustering. To ensure real-time processing, we set the appropriate least-squares parameters. Meanwhile, to verify the effectiveness of the algorithm, we create the FMCW LiDAR model on the autonomous driving simulation platform CARLA for spawning data. The results show that our algorithm can process at least a 4.5million points and estimate the velocity of 150 moving objects per second under the arithmetic power of the Ryzen 3600x CPU, with a motion state detection accuracy of over 99% and estimated velocity accuracy of 0.1 m/s.



### iSeg3D: An Interactive 3D Shape Segmentation Tool
- **Arxiv ID**: http://arxiv.org/abs/2112.12988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12988v1)
- **Published**: 2021-12-24 08:15:52+00:00
- **Updated**: 2021-12-24 08:15:52+00:00
- **Authors**: Sucheng Qian, Liu Liu, Wenqiang Xu, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: A large-scale dataset is essential for learning good features in 3D shape understanding, but there are only a few datasets that can satisfy deep learning training. One of the major reasons is that current tools for annotating per-point semantic labels using polygons or scribbles are tedious and inefficient. To facilitate segmentation annotations in 3D shapes, we propose an effective annotation tool, named iSeg for 3D shape. It can obtain a satisfied segmentation result with minimal human clicks (< 10). Under our observation, most objects can be considered as the composition of finite primitive shapes, and we train iSeg3D model on our built primitive-composed shape data to learn the geometric prior knowledge in a self-supervised manner. Given human interactions, the learned knowledge can be used to segment parts on arbitrary shapes, in which positive clicks help associate the primitives into the semantic parts and negative clicks can avoid over-segmentation. Besides, We also provide an online human-in-loop fine-tuning module that enables the model perform better segmentation with less clicks. Experiments demonstrate the effectiveness of iSeg3D on PartNet shape segmentation. Data and codes will be made publicly available.



### Domain-Aware Continual Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.12989v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.12989v1)
- **Published**: 2021-12-24 08:17:18+00:00
- **Updated**: 2021-12-24 08:17:18+00:00
- **Authors**: Kai Yi, Mohamed Elhoseiny
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Domain Aware Continual Zero-Shot Learning (DACZSL), the task of visually recognizing images of unseen categories in unseen domains sequentially. We created DACZSL on top of the DomainNet dataset by dividing it into a sequence of tasks, where classes are incrementally provided on seen domains during training and evaluation is conducted on unseen domains for both seen and unseen classes. We also proposed a novel Domain-Invariant CZSL Network (DIN), which outperforms state-of-the-art baseline models that we adapted to DACZSL setting. We adopt a structure-based approach to alleviate forgetting knowledge from previous tasks with a small per-task private network in addition to a global shared network. To encourage the private network to capture the domain and task-specific representation, we train our model with a novel adversarial knowledge disentanglement setting to make our global network task-invariant and domain-invariant over all the tasks. Our method also learns a class-wise learnable prompt to obtain better class-level text representation, which is used to represent side information to enable zero-shot prediction of future unseen classes. Our code and benchmarks will be made publicly available.



### US-GAN: On the importance of Ultimate Skip Connection for Facial Expression Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2112.13002v2
- **DOI**: 10.1007/s11042-023-15268-2
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.13002v2)
- **Published**: 2021-12-24 08:56:50+00:00
- **Updated**: 2023-04-08 01:42:24+00:00
- **Authors**: Arbish Akram, Nazar Khan
- **Comment**: None
- **Journal**: Multimed Tools Appl (2023)
- **Summary**: We demonstrate the benefit of using an ultimate skip (US) connection for facial expression synthesis using generative adversarial networks (GAN). A direct connection transfers identity, facial, and color details from input to output while suppressing artifacts. The intermediate layers can therefore focus on expression generation only. This leads to a light-weight US-GAN model comprised of encoding layers, a single residual block, decoding layers, and an ultimate skip connection from input to output. US-GAN has $3\times$ fewer parameters than state-of-the-art models and is trained on $2$ orders of magnitude smaller dataset. It yields $7\%$ increase in face verification score (FVS) and $27\%$ decrease in average content distance (ACD). Based on a randomized user-study, US-GAN outperforms the state of the art by $25\%$ in face realism, $43\%$ in expression quality, and $58\%$ in identity preservation.



### Continuous Spectral Reconstruction from RGB Images via Implicit Neural Representation
- **Arxiv ID**: http://arxiv.org/abs/2112.13003v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13003v2)
- **Published**: 2021-12-24 09:08:23+00:00
- **Updated**: 2022-08-31 07:27:29+00:00
- **Authors**: Ruikang Xu, Mingde Yao, Chang Chen, Lizhi Wang, Zhiwei Xiong
- **Comment**: Accepted to ECCV Workshop 2022
- **Journal**: None
- **Summary**: Existing methods for spectral reconstruction usually learn a discrete mapping from RGB images to a number of spectral bands. However, this modeling strategy ignores the continuous nature of spectral signature. In this paper, we propose Neural Spectral Reconstruction (NeSR) to lift this limitation, by introducing a novel continuous spectral representation. To this end, we embrace the concept of implicit function and implement a parameterized embodiment with a neural network. Specifically, we first adopt a backbone network to extract spatial features of RGB inputs. Based on it, we devise Spectral Profile Interpolation (SPI) module and Neural Attention Mapping (NAM) module to enrich deep features, where the spatial-spectral correlation is involved for a better representation. Then, we view the number of sampled spectral bands as the coordinate of continuous implicit function, so as to learn the projection from deep features to spectral intensities. Extensive experiments demonstrate the distinct advantage of NeSR in reconstruction accuracy over baseline methods. Moreover, NeSR extends the flexibility of spectral reconstruction by enabling an arbitrary number of spectral bands as the target output.



### Benchmarking Pedestrian Odometry: The Brown Pedestrian Odometry Dataset (BPOD)
- **Arxiv ID**: http://arxiv.org/abs/2112.13018v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13018v1)
- **Published**: 2021-12-24 10:11:32+00:00
- **Updated**: 2021-12-24 10:11:32+00:00
- **Authors**: David Charatan, Hongyi Fan, Benjamin Kimia
- **Comment**: None
- **Journal**: None
- **Summary**: We present the Brown Pedestrian Odometry Dataset (BPOD) for benchmarking visual odometry algorithms in head-mounted pedestrian settings. This dataset was captured using synchronized global and rolling shutter stereo cameras in 12 diverse indoor and outdoor locations on Brown University's campus. Compared to existing datasets, BPOD contains more image blur and self-rotation, which are common in pedestrian odometry but rare elsewhere. Ground-truth trajectories are generated from stick-on markers placed along the pedestrian's path, and the pedestrian's position is documented using a third-person video. We evaluate the performance of representative direct, feature-based, and learning-based VO methods on BPOD. Our results show that significant development is needed to successfully capture pedestrian trajectories. The link to the dataset is here: \url{https://doi.org/10.26300/c1n7-7p93



### Raw Produce Quality Detection with Shifted Window Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/2112.13845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.13845v1)
- **Published**: 2021-12-24 10:16:28+00:00
- **Updated**: 2021-12-24 10:16:28+00:00
- **Authors**: Oh Joon Kwon, Byungsoo Kim, Youngduck Choi
- **Comment**: None
- **Journal**: None
- **Summary**: Global food insecurity is expected to worsen in the coming decades with the accelerated rate of climate change and the rapidly increasing population. In this vein, it is important to remove inefficiencies at every level of food production. The recent advances in deep learning can help reduce such inefficiencies, yet their application has not yet become mainstream throughout the industry, inducing economic costs at a massive scale. To this point, modern techniques such as CNNs (Convolutional Neural Networks) have been applied to RPQD (Raw Produce Quality Detection) tasks. On the other hand, Transformer's successful debut in the vision among other modalities led us to expect a better performance with these Transformer-based models in RPQD. In this work, we exclusively investigate the recent state-of-the-art Swin (Shifted Windows) Transformer which computes self-attention in both intra- and inter-window fashion. We compare Swin Transformer against CNN models on four RPQD image datasets, each containing different kinds of raw produce: fruits and vegetables, fish, pork, and beef. We observe that Swin Transformer not only achieves better or competitive performance but also is data- and compute-efficient, making it ideal for actual deployment in real-world setting. To the best of our knowledge, this is the first large-scale empirical study on RPQD task, which we hope will gain more attention in future works.



### Grounding Linguistic Commands to Navigable Regions
- **Arxiv ID**: http://arxiv.org/abs/2112.13031v1
- **DOI**: 10.1109/IROS51168.2021.9636172
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.13031v1)
- **Published**: 2021-12-24 11:11:44+00:00
- **Updated**: 2021-12-24 11:11:44+00:00
- **Authors**: Nivedita Rufus, Kanishk Jain, Unni Krishnan R Nair, Vineet Gandhi, K Madhava Krishna
- **Comment**: None
- **Journal**: 2021 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), 2021, pp. 8593-8600
- **Summary**: Humans have a natural ability to effortlessly comprehend linguistic commands such as "park next to the yellow sedan" and instinctively know which region of the road the vehicle should navigate. Extending this ability to autonomous vehicles is the next step towards creating fully autonomous agents that respond and act according to human commands. To this end, we propose the novel task of Referring Navigable Regions (RNR), i.e., grounding regions of interest for navigation based on the linguistic command. RNR is different from Referring Image Segmentation (RIS), which focuses on grounding an object referred to by the natural language expression instead of grounding a navigable region. For example, for a command "park next to the yellow sedan," RIS will aim to segment the referred sedan, and RNR aims to segment the suggested parking region on the road. We introduce a new dataset, Talk2Car-RegSeg, which extends the existing Talk2car dataset with segmentation masks for the regions described by the linguistic commands. A separate test split with concise manoeuvre-oriented commands is provided to assess the practicality of our dataset. We benchmark the proposed dataset using a novel transformer-based architecture. We present extensive ablations and show superior performance over baselines on multiple evaluation metrics. A downstream path planner generating trajectories based on RNR outputs confirms the efficacy of the proposed framework.



### Channel-Wise Attention-Based Network for Self-Supervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2112.13047v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.13047v1)
- **Published**: 2021-12-24 12:18:54+00:00
- **Updated**: 2021-12-24 12:18:54+00:00
- **Authors**: Jiaxing Yan, Hong Zhao, Penghui Bu, YuSheng Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning has shown very promising results for monocular depth estimation. Scene structure and local details both are significant clues for high-quality depth estimation. Recent works suffer from the lack of explicit modeling of scene structure and proper handling of details information, which leads to a performance bottleneck and blurry artefacts in predicted results. In this paper, we propose the Channel-wise Attention-based Depth Estimation Network (CADepth-Net) with two effective contributions: 1) The structure perception module employs the self-attention mechanism to capture long-range dependencies and aggregates discriminative features in channel dimensions, explicitly enhances the perception of scene structure, obtains the better scene understanding and rich feature representation. 2) The detail emphasis module re-calibrates channel-wise feature maps and selectively emphasizes the informative features, aiming to highlight crucial local details information and fuse different level features more efficiently, resulting in more precise and sharper depth prediction. Furthermore, the extensive experiments validate the effectiveness of our method and show that our model achieves the state-of-the-art results on the KITTI benchmark and Make3D datasets.



### Self-Gated Memory Recurrent Network for Efficient Scalable HDR Deghosting
- **Arxiv ID**: http://arxiv.org/abs/2112.13050v1
- **DOI**: 10.1109/TCI.2021.3112920
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13050v1)
- **Published**: 2021-12-24 12:36:33+00:00
- **Updated**: 2021-12-24 12:36:33+00:00
- **Authors**: K. Ram Prabhakar, Susmit Agrawal, R. Venkatesh Babu
- **Comment**: 12 pages
- **Journal**: IEEE Transactions on Computational Imaging (Volume 7, 2021)
  1228-1239
- **Summary**: We propose a novel recurrent network-based HDR deghosting method for fusing arbitrary length dynamic sequences. The proposed method uses convolutional and recurrent architectures to generate visually pleasing, ghosting-free HDR images. We introduce a new recurrent cell architecture, namely Self-Gated Memory (SGM) cell, that outperforms the standard LSTM cell while containing fewer parameters and having faster running times. In the SGM cell, the information flow through a gate is controlled by multiplying the gate's output by a function of itself. Additionally, we use two SGM cells in a bidirectional setting to improve output quality. The proposed approach achieves state-of-the-art performance compared to existing HDR deghosting methods quantitatively across three publicly available datasets while simultaneously achieving scalability to fuse variable-length input sequence without necessitating re-training. Through extensive ablations, we demonstrate the importance of individual components in our proposed approach. The code is available at https://val.cds.iisc.ac.in/HDR/HDRRNN/index.html.



### Generalized Wasserstein Dice Loss, Test-time Augmentation, and Transformers for the BraTS 2021 challenge
- **Arxiv ID**: http://arxiv.org/abs/2112.13054v1
- **DOI**: 10.1007/978-3-031-09002-8_17
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.13054v1)
- **Published**: 2021-12-24 13:01:44+00:00
- **Updated**: 2021-12-24 13:01:44+00:00
- **Authors**: Lucas Fidon, Suprosanna Shit, Ivan Ezhov, Johannes C. Paetzold, Sébastien Ourselin, Tom Vercauteren
- **Comment**: BraTS 2021 challenge
- **Journal**: None
- **Summary**: Brain tumor segmentation from multiple Magnetic Resonance Imaging (MRI) modalities is a challenging task in medical image computation. The main challenges lie in the generalizability to a variety of scanners and imaging protocols. In this paper, we explore strategies to increase model robustness without increasing inference time. Towards this aim, we explore finding a robust ensemble from models trained using different losses, optimizers, and train-validation data split. Importantly, we explore the inclusion of a transformer in the bottleneck of the U-Net architecture. While we find transformer in the bottleneck performs slightly worse than the baseline U-Net in average, the generalized Wasserstein Dice loss consistently produces superior results. Further, we adopt an efficient test time augmentation strategy for faster and robust inference. Our final ensemble of seven 3D U-Nets with test-time augmentation produces an average dice score of 89.4% and an average Hausdorff 95% distance of 10.0 mm when evaluated on the BraTS 2021 testing dataset. Our code and trained models are publicly available at https://github.com/LucasFidon/TRABIT_BraTS2021.



### NIP: Neuron-level Inverse Perturbation Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2112.13060v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.13060v2)
- **Published**: 2021-12-24 13:37:42+00:00
- **Updated**: 2022-11-07 05:44:57+00:00
- **Authors**: Ruoxi Chen, Haibo Jin, Jinyin Chen, Haibin Zheng, Yue Yu, Shouling Ji
- **Comment**: There are some problems in the figure so we need to withdraw this
  paper. We will upload the new version after revision
- **Journal**: None
- **Summary**: Although deep learning models have achieved unprecedented success, their vulnerabilities towards adversarial attacks have attracted increasing attention, especially when deployed in security-critical domains. To address the challenge, numerous defense strategies, including reactive and proactive ones, have been proposed for robustness improvement. From the perspective of image feature space, some of them cannot reach satisfying results due to the shift of features. Besides, features learned by models are not directly related to classification results. Different from them, We consider defense method essentially from model inside and investigated the neuron behaviors before and after attacks. We observed that attacks mislead the model by dramatically changing the neurons that contribute most and least to the correct label. Motivated by it, we introduce the concept of neuron influence and further divide neurons into front, middle and tail part. Based on it, we propose neuron-level inverse perturbation(NIP), the first neuron-level reactive defense method against adversarial attacks. By strengthening front neurons and weakening those in the tail part, NIP can eliminate nearly all adversarial perturbations while still maintaining high benign accuracy. Besides, it can cope with different sizes of perturbations via adaptivity, especially larger ones. Comprehensive experiments conducted on three datasets and six models show that NIP outperforms the state-of-the-art baselines against eleven adversarial attacks. We further provide interpretable proofs via neuron activation and visualization for better understanding.



### CatchBackdoor: Backdoor Testing by Critical Trojan Neural Path Identification via Differential Fuzzing
- **Arxiv ID**: http://arxiv.org/abs/2112.13064v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.13064v2)
- **Published**: 2021-12-24 13:57:03+00:00
- **Updated**: 2023-02-21 14:02:52+00:00
- **Authors**: Haibo Jin, Ruoxi Chen, Jinyin Chen, Yao Cheng, Chong Fu, Ting Wang, Yue Yu, Zhaoyan Ming
- **Comment**: There are some problems in the experiment so we need to withdraw this
  paper. We will upload the new version after revision
- **Journal**: None
- **Summary**: The success of deep neural networks (DNNs) in real-world applications has benefited from abundant pre-trained models. However, the backdoored pre-trained models can pose a significant trojan threat to the deployment of downstream DNNs. Existing DNN testing methods are mainly designed to find incorrect corner case behaviors in adversarial settings but fail to discover the backdoors crafted by strong trojan attacks. Observing the trojan network behaviors shows that they are not just reflected by a single compromised neuron as proposed by previous work but attributed to the critical neural paths in the activation intensity and frequency of multiple neurons. This work formulates the DNN backdoor testing and proposes the CatchBackdoor framework. Via differential fuzzing of critical neurons from a small number of benign examples, we identify the trojan paths and particularly the critical ones, and generate backdoor testing examples by simulating the critical neurons in the identified paths. Extensive experiments demonstrate the superiority of CatchBackdoor, with higher detection performance than existing methods. CatchBackdoor works better on detecting backdoors by stealthy blending and adaptive attacks, which existing methods fail to detect. Moreover, our experiments show that CatchBackdoor may reveal the potential backdoors of models in Model Zoo.



### Virtuoso: Video-based Intelligence for real-time tuning on SOCs
- **Arxiv ID**: http://arxiv.org/abs/2112.13076v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.13076v1)
- **Published**: 2021-12-24 14:47:41+00:00
- **Updated**: 2021-12-24 14:47:41+00:00
- **Authors**: Jayoung Lee, PengCheng Wang, Ran Xu, Venkat Dasari, Noah Weston, Yin Li, Saurabh Bagchi, Somali Chaterji
- **Comment**: 28 pages, 15 figures, 4 tables, ACM-TODAES
- **Journal**: None
- **Summary**: Efficient and adaptive computer vision systems have been proposed to make computer vision tasks, such as image classification and object detection, optimized for embedded or mobile devices. These solutions, quite recent in their origin, focus on optimizing the model (a deep neural network, DNN) or the system by designing an adaptive system with approximation knobs. In spite of several recent efforts, we show that existing solutions suffer from two major drawbacks. First, the system does not consider energy consumption of the models while making a decision on which model to run. Second, the evaluation does not consider the practical scenario of contention on the device, due to other co-resident workloads. In this work, we propose an efficient and adaptive video object detection system, Virtuoso, which is jointly optimized for accuracy, energy efficiency, and latency. Underlying Virtuoso is a multi-branch execution kernel that is capable of running at different operating points in the accuracy-energy-latency axes, and a lightweight runtime scheduler to select the best fit execution branch to satisfy the user requirement. To fairly compare with Virtuoso, we benchmark 15 state-of-the-art or widely used protocols, including Faster R-CNN (FRCNN), YOLO v3, SSD, EfficientDet, SELSA, MEGA, REPP, FastAdapt, and our in-house adaptive variants of FRCNN+, YOLO+, SSD+, and EfficientDet+ (our variants have enhanced efficiency for mobiles). With this comprehensive benchmark, Virtuoso has shown superiority to all the above protocols, leading the accuracy frontier at every efficiency level on NVIDIA Jetson mobile GPUs. Specifically, Virtuoso has achieved an accuracy of 63.9%, which is more than 10% higher than some of the popular object detection models, FRCNN at 51.1%, and YOLO at 49.5%.



### Multi-Scale Feature Fusion: Learning Better Semantic Segmentation for Road Pothole Detection
- **Arxiv ID**: http://arxiv.org/abs/2112.13082v1
- **DOI**: 10.1109/ICAS49788.2021.9551165
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.13082v1)
- **Published**: 2021-12-24 15:07:47+00:00
- **Updated**: 2021-12-24 15:07:47+00:00
- **Authors**: Jiahe Fan, Mohammud J. Bocus, Brett Hosking, Rigen Wu, Yanan Liu, Sergey Vityazev, Rui Fan
- **Comment**: 2021 IEEE International Conference on Autonomous Systems (ICAS)
- **Journal**: None
- **Summary**: This paper presents a novel pothole detection approach based on single-modal semantic segmentation. It first extracts visual features from input images using a convolutional neural network. A channel attention module then reweighs the channel features to enhance the consistency of different feature maps. Subsequently, we employ an atrous spatial pyramid pooling module (comprising of atrous convolutions in series, with progressive rates of dilation) to integrate the spatial context information. This helps better distinguish between potholes and undamaged road areas. Finally, the feature maps in the adjacent layers are fused using our proposed multi-scale feature fusion module. This further reduces the semantic gap between different feature channel layers. Extensive experiments were carried out on the Pothole-600 dataset to demonstrate the effectiveness of our proposed method. The quantitative comparisons suggest that our method achieves the state-of-the-art (SoTA) performance on both RGB images and transformed disparity images, outperforming three SoTA single-modal semantic segmentation networks.



### SimViT: Exploring a Simple Vision Transformer with sliding windows
- **Arxiv ID**: http://arxiv.org/abs/2112.13085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.13085v1)
- **Published**: 2021-12-24 15:18:20+00:00
- **Updated**: 2021-12-24 15:18:20+00:00
- **Authors**: Gang Li, Di Xu, Xing Cheng, Lingyu Si, Changwen Zheng
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: Although vision Transformers have achieved excellent performance as backbone models in many vision tasks, most of them intend to capture global relations of all tokens in an image or a window, which disrupts the inherent spatial and local correlations between patches in 2D structure. In this paper, we introduce a simple vision Transformer named SimViT, to incorporate spatial structure and local information into the vision Transformers. Specifically, we introduce Multi-head Central Self-Attention(MCSA) instead of conventional Multi-head Self-Attention to capture highly local relations. The introduction of sliding windows facilitates the capture of spatial structure. Meanwhile, SimViT extracts multi-scale hierarchical features from different layers for dense prediction tasks. Extensive experiments show the SimViT is effective and efficient as a general-purpose backbone model for various image processing tasks. Especially, our SimViT-Micro only needs 3.3M parameters to achieve 71.1% top-1 accuracy on ImageNet-1k dataset, which is the smallest size vision Transformer model by now. Our code will be available in https://github.com/ucasligang/SimViT.



### Invertible Network for Unpaired Low-light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2112.13107v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.13107v1)
- **Published**: 2021-12-24 17:00:54+00:00
- **Updated**: 2021-12-24 17:00:54+00:00
- **Authors**: Jize Zhang, Haolin Wang, Xiaohe Wu, Wangmeng Zuo
- **Comment**: None
- **Journal**: None
- **Summary**: Existing unpaired low-light image enhancement approaches prefer to employ the two-way GAN framework, in which two CNN generators are deployed for enhancement and degradation separately. However, such data-driven models ignore the inherent characteristics of transformation between the low and normal light images, leading to unstable training and artifacts. Here, we propose to leverage the invertible network to enhance low-light image in forward process and degrade the normal-light one inversely with unpaired learning. The generated and real images are then fed into discriminators for adversarial learning. In addition to the adversarial loss, we design various loss functions to ensure the stability of training and preserve more image details. Particularly, a reversibility loss is introduced to alleviate the over-exposure problem. Moreover, we present a progressive self-guided enhancement process for low-light images and achieve favorable performance against the SOTAs.



### Ultrasound Speckle Suppression and Denoising using MRI-derived Normalizing Flow Priors
- **Arxiv ID**: http://arxiv.org/abs/2112.13110v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.13110v1)
- **Published**: 2021-12-24 17:21:24+00:00
- **Updated**: 2021-12-24 17:21:24+00:00
- **Authors**: Vincent van de Schaft, Ruud J. G. van Sloun
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Ultrasonography offers an inexpensive, widely-accessible and compact medical imaging solution. However, compared to other imaging modalities such as CT and MRI, ultrasound images notoriously suffer from strong speckle noise, which originates from the random interference of sub-wavelength scattering. This deteriorates ultrasound image quality and makes interpretation challenging. We here propose a new unsupervised ultrasound speckle reduction and image denoising method based on maximum-a-posteriori estimation with deep generative priors that are learned from high-quality MRI images. To model the generative tissue reflectivity prior, we exploit normalizing flows, which in recent years have shown to be very powerful in modeling signal priors across a variety of applications. To facilitate generaliation, we factorize the prior and train our flow model on patches from the NYU fastMRI (fully-sampled) dataset. This prior is then used for inference in an iterative denoising scheme. We first validate the utility of our learned priors on noisy MRI data (no prior domain shift), and then turn to evaluating performance on both simulated and in-vivo ultrasound images from the PICMUS and CUBDL datasets. The results show that the method outperforms other (unsupervised) ultrasound denoising methods (NLM and OBNLM) both quantitatively and qualitatively.



### The Curse of Zero Task Diversity: On the Failure of Transfer Learning to Outperform MAML and their Empirical Equivalence
- **Arxiv ID**: http://arxiv.org/abs/2112.13121v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2112.13121v4)
- **Published**: 2021-12-24 18:42:58+00:00
- **Updated**: 2022-11-28 20:54:49+00:00
- **Authors**: Brando Miranda, Yu-Xiong Wang, Sanmi Koyejo
- **Comment**: An updated version with updated correction is at arXiv:2208.01545 and
  it's acompanying neurips submission is at
  https://brando90.github.io/brandomiranda/publications.html
- **Journal**: None
- **Summary**: Recently, it has been observed that a transfer learning solution might be all we need to solve many few-shot learning benchmarks -- thus raising important questions about when and how meta-learning algorithms should be deployed. In this paper, we seek to clarify these questions by proposing a novel metric -- the diversity coefficient -- to measure the diversity of tasks in a few-shot learning benchmark. We hypothesize that the diversity coefficient of the few-shot learning benchmark is predictive of whether meta-learning solutions will succeed or not. Using the diversity coefficient, we show that the MiniImagenet benchmark has zero diversity. This novel insight contextualizes claims that transfer learning solutions are better than meta-learned solutions. Specifically, we empirically find that a diversity coefficient of zero correlates with a high similarity between transfer learning and Model-Agnostic Meta-Learning (MAML) learned solutions in terms of meta-accuracy (at meta-test time). Therefore, we conjecture meta-learned solutions have the same meta-test performance as transfer learning when the diversity coefficient is zero. Our work provides the first test of whether diversity correlates with meta-learning success.



### Does MAML Only Work via Feature Re-use? A Data Centric Perspective
- **Arxiv ID**: http://arxiv.org/abs/2112.13137v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2112.13137v1)
- **Published**: 2021-12-24 20:18:38+00:00
- **Updated**: 2021-12-24 20:18:38+00:00
- **Authors**: Brando Miranda, Yu-Xiong Wang, Sanmi Koyejo
- **Comment**: 15 pages, 12 figures
- **Journal**: None
- **Summary**: Recent work has suggested that a good embedding is all we need to solve many few-shot learning benchmarks. Furthermore, other work has strongly suggested that Model Agnostic Meta-Learning (MAML) also works via this same method - by learning a good embedding. These observations highlight our lack of understanding of what meta-learning algorithms are doing and when they work. In this work, we provide empirical results that shed some light on how meta-learned MAML representations function. In particular, we identify three interesting properties: 1) In contrast to previous work, we show that it is possible to define a family of synthetic benchmarks that result in a low degree of feature re-use - suggesting that current few-shot learning benchmarks might not have the properties needed for the success of meta-learning algorithms; 2) meta-overfitting occurs when the number of classes (or concepts) are finite, and this issue disappears once the task has an unbounded number of concepts (e.g., online learning); 3) more adaptation at meta-test time with MAML does not necessarily result in a significant representation change or even an improvement in meta-test performance - even when training on our proposed synthetic benchmarks. Finally, we suggest that to understand meta-learning algorithms better, we must go beyond tracking only absolute performance and, in addition, formally quantify the degree of meta-learning and track both metrics together. Reporting results in future work this way will help us identify the sources of meta-overfitting more accurately and help us design more flexible meta-learning algorithms that learn beyond fixed feature re-use. Finally, we conjecture the core challenge of re-thinking meta-learning is in the design of few-shot learning data sets and benchmarks - rather than in the algorithms, as suggested by previous work.



### Reconstructing Compact Building Models from Point Clouds Using Deep Implicit Fields
- **Arxiv ID**: http://arxiv.org/abs/2112.13142v3
- **DOI**: 10.1016/j.isprsjprs.2022.09.017
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2112.13142v3)
- **Published**: 2021-12-24 21:32:32+00:00
- **Updated**: 2022-09-26 13:24:05+00:00
- **Authors**: Zhaiyu Chen, Hugo Ledoux, Seyran Khademi, Liangliang Nan
- **Comment**: Accepted for publication in ISPRS Journal of Photogrammetry and
  Remote Sensing
- **Journal**: None
- **Summary**: While three-dimensional (3D) building models play an increasingly pivotal role in many real-world applications, obtaining a compact representation of buildings remains an open problem. In this paper, we present a novel framework for reconstructing compact, watertight, polygonal building models from point clouds. Our framework comprises three components: (a) a cell complex is generated via adaptive space partitioning that provides a polyhedral embedding as the candidate set; (b) an implicit field is learned by a deep neural network that facilitates building occupancy estimation; (c) a Markov random field is formulated to extract the outer surface of a building via combinatorial optimization. We evaluate and compare our method with state-of-the-art methods in generic reconstruction, model-based reconstruction, geometry simplification, and primitive assembly. Experiments on both synthetic and real-world point clouds have demonstrated that, with our neural-guided strategy, high-quality building models can be obtained with significant advantages in fidelity, compactness, and computational efficiency. Our method also shows robustness to noise and insufficient measurements, and it can directly generalize from synthetic scans to real-world measurements. The source code of this work is freely available at https://github.com/chenzhaiyu/points2poly.



### Fast and Scalable Computation of the Forward and Inverse Discrete Periodic Radon Transform
- **Arxiv ID**: http://arxiv.org/abs/2112.13149v1
- **DOI**: 10.1109/TIP.2015.2501725
- **Categories**: **cs.AR**, cs.CV, cs.DC, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2112.13149v1)
- **Published**: 2021-12-24 22:33:13+00:00
- **Updated**: 2021-12-24 22:33:13+00:00
- **Authors**: Cesar Carranza, Daniel Llamocca, Marios Pattichis
- **Comment**: This paper has been published as follows: C. Carranza, D. Llamocca,
  and M. Pattichis. "Fast and scalable computation of the forward and inverse
  discrete periodic radon transform", IEEE Transactions on Image Processing,
  25(1):119-133, Jan 2016
- **Journal**: IEEE Transactions on Image Processing, 25(1):119-133, Jan 2016
- **Summary**: The Discrete Periodic Radon Transform (DPRT) has been extensively used in applications that involve image reconstructions from projections. This manuscript introduces a fast and scalable approach for computing the forward and inverse DPRT that is based on the use of: (i) a parallel array of fixed-point adder trees, (ii) circular shift registers to remove the need for accessing external memory components when selecting the input data for the adder trees, (iii) an image block-based approach to DPRT computation that can fit the proposed architecture to available resources, and (iv) fast transpositions that are computed in one or a few clock cycles that do not depend on the size of the input image. As a result, for an $N\times N$ image ($N$ prime), the proposed approach can compute up to $N^{2}$ additions per clock cycle. Compared to previous approaches, the scalable approach provides the fastest known implementations for different amounts of computational resources. For example, for a $251\times 251$ image, for approximately $25\%$ fewer flip-flops than required for a systolic implementation, we have that the scalable DPRT is computed 36 times faster. For the fastest case, we introduce optimized architectures that can compute the DPRT and its inverse in just $2N+\left\lceil \log_{2}N\right\rceil+1$ and $2N+3\left\lceil \log_{2}N\right\rceil+B+2$ cycles respectively, where $B$ is the number of bits used to represent each input pixel. On the other hand, the scalable DPRT approach requires more 1-bit additions than for the systolic implementation and provides a trade-off between speed and additional 1-bit additions. All of the proposed DPRT architectures were implemented in VHDL and validated using an FPGA implementation.



### Fast 2D Convolutions and Cross-Correlations Using Scalable Architectures
- **Arxiv ID**: http://arxiv.org/abs/2112.13150v1
- **DOI**: 10.1109/TIP.2017.2678799
- **Categories**: **cs.AR**, cs.CV, cs.DC, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2112.13150v1)
- **Published**: 2021-12-24 22:34:51+00:00
- **Updated**: 2021-12-24 22:34:51+00:00
- **Authors**: Cesar Carranza, Daniel Llamocca, Marios Pattichis
- **Comment**: The paper develops the fastest known methods for computing 2D
  convolutions in hardware
- **Journal**: IEEE Transactions on Image Processing 26.5 (2017): 2230-2245
- **Summary**: The manuscript describes fast and scalable architectures and associated algorithms for computing convolutions and cross-correlations. The basic idea is to map 2D convolutions and cross-correlations to a collection of 1D convolutions and cross-correlations in the transform domain. This is accomplished through the use of the Discrete Periodic Radon Transform (DPRT) for general kernels and the use of SVD-LU decompositions for low-rank kernels. The approach uses scalable architectures that can be fitted into modern FPGA and Zynq-SOC devices. Based on different types of available resources, for $P\times P$ blocks, 2D convolutions and cross-correlations can be computed in just $O(P)$ clock cycles up to $O(P^2)$ clock cycles. Thus, there is a trade-off between performance and required numbers and types of resources. We provide implementations of the proposed architectures using modern programmable devices (Virtex-7 and Zynq-SOC). Based on the amounts and types of required resources, we show that the proposed approaches significantly outperform current methods.



