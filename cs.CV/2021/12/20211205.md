# Arxiv Papers in cs.CV on 2021-12-05
### SSAGCN: Social Soft Attention Graph Convolution Network for Pedestrian Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2112.02459v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.02459v1)
- **Published**: 2021-12-05 01:49:18+00:00
- **Updated**: 2021-12-05 01:49:18+00:00
- **Authors**: Pei Lv, Wentong Wang, Yunxin Wang, Yuzhen Zhang, Mingliang Xu, Changsheng Xu
- **Comment**: 14 pages, 8 figures
- **Journal**: None
- **Summary**: Pedestrian trajectory prediction is an important technique of autonomous driving, which has become a research hot-spot in recent years. Previous methods mainly rely on the position relationship of pedestrians to model social interaction, which is obviously not enough to represent the complex cases in real situations. In addition, most of existing work usually introduce the scene interaction module as an independent branch and embed the social interaction features in the process of trajectory generation, rather than simultaneously carrying out the social interaction and scene interaction, which may undermine the rationality of trajectory prediction. In this paper, we propose one new prediction model named Social Soft Attention Graph Convolution Network (SSAGCN) which aims to simultaneously handle social interactions among pedestrians and scene interactions between pedestrians and environments. In detail, when modeling social interaction, we propose a new \emph{social soft attention function}, which fully considers various interaction factors among pedestrians. And it can distinguish the influence of pedestrians around the agent based on different factors under various situations. For the physical interaction, we propose one new \emph{sequential scene sharing mechanism}. The influence of the scene on one agent at each moment can be shared with other neighbors through social soft attention, therefore the influence of the scene is expanded both in spatial and temporal dimension. With the help of these improvements, we successfully obtain socially and physically acceptable predicted trajectories. The experiments on public available datasets prove the effectiveness of SSAGCN and have achieved state-of-the-art results.



### Visual Persuasion in COVID-19 Social Media Content: A Multi-Modal Characterization
- **Arxiv ID**: http://arxiv.org/abs/2112.13910v1
- **DOI**: 10.1145/3487553.3524647
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.13910v1)
- **Published**: 2021-12-05 02:15:01+00:00
- **Updated**: 2021-12-05 02:15:01+00:00
- **Authors**: Mesut Erhan Unal, Adriana Kovashka, Wen-Ting Chung, Yu-Ru Lin
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Social media content routinely incorporates multi-modal design to covey information and shape meanings, and sway interpretations toward desirable implications, but the choices and outcomes of using both texts and visual images have not been sufficiently studied. This work proposes a computational approach to analyze the outcome of persuasive information in multi-modal content, focusing on two aspects, popularity and reliability, in COVID-19-related news articles shared on Twitter. The two aspects are intertwined in the spread of misinformation: for example, an unreliable article that aims to misinform has to attain some popularity. This work has several contributions. First, we propose a multi-modal (image and text) approach to effectively identify popularity and reliability of information sources simultaneously. Second, we identify textual and visual elements that are predictive to information popularity and reliability. Third, by modeling cross-modal relations and similarity, we are able to uncover how unreliable articles construct multi-modal meaning in a distorted, biased fashion. Our work demonstrates how to use multi-modal analysis for understanding influential content and has implications to social media literacy and engagement.



### Neighborhood Spatial Aggregation MC Dropout for Efficient Uncertainty-aware Semantic Segmentation in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2201.07676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07676v1)
- **Published**: 2021-12-05 02:22:32+00:00
- **Updated**: 2021-12-05 02:22:32+00:00
- **Authors**: Chao Qi, Jianqin Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Uncertainty-aware semantic segmentation of the point clouds includes the predictive uncertainty estimation and the uncertainty-guided model optimization. One key challenge in the task is the efficiency of point-wise predictive distribution establishment. The widely-used MC dropout establishes the distribution by computing the standard deviation of samples using multiple stochastic forward propagations, which is time-consuming for tasks based on point clouds containing massive points. Hence, a framework embedded with NSA-MC dropout, a variant of MC dropout, is proposed to establish distributions in just one forward pass. Specifically, the NSA-MC dropout samples the model many times through a space-dependent way, outputting point-wise distribution by aggregating stochastic inference results of neighbors. Based on this, aleatoric and predictive uncertainties acquire from the predictive distribution. The aleatoric uncertainty is integrated into the loss function to penalize noisy points, avoiding the over-fitting of the model to some degree. Besides, the predictive uncertainty quantifies the confidence degree of predictions. Experimental results show that our framework obtains better segmentation results of real-world point clouds and efficiently quantifies the credibility of results. Our NSA-MC dropout is several times faster than MC dropout, and the inference time does not establish a coupling relation with the sampling times. The code will be available if the paper is accepted.



### Pose-guided Feature Disentangling for Occluded Person Re-identification Based on Transformer
- **Arxiv ID**: http://arxiv.org/abs/2112.02466v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.02466v2)
- **Published**: 2021-12-05 03:23:31+00:00
- **Updated**: 2021-12-11 08:04:37+00:00
- **Authors**: Tao Wang, Hong Liu, Pinhao Song, Tianyu Guo, Wei Shi
- **Comment**: Accepted by AAAI2022
- **Journal**: None
- **Summary**: Occluded person re-identification is a challenging task as human body parts could be occluded by some obstacles (e.g. trees, cars, and pedestrians) in certain scenes. Some existing pose-guided methods solve this problem by aligning body parts according to graph matching, but these graph-based methods are not intuitive and complicated. Therefore, we propose a transformer-based Pose-guided Feature Disentangling (PFD) method by utilizing pose information to clearly disentangle semantic components (e.g. human body or joint parts) and selectively match non-occluded parts correspondingly. First, Vision Transformer (ViT) is used to extract the patch features with its strong capability. Second, to preliminarily disentangle the pose information from patch information, the matching and distributing mechanism is leveraged in Pose-guided Feature Aggregation (PFA) module. Third, a set of learnable semantic views are introduced in transformer decoder to implicitly enhance the disentangled body part features. However, those semantic views are not guaranteed to be related to the body without additional supervision. Therefore, Pose-View Matching (PVM) module is proposed to explicitly match visible body parts and automatically separate occlusion features. Fourth, to better prevent the interference of occlusions, we design a Pose-guided Push Loss to emphasize the features of visible body parts. Extensive experiments over five challenging datasets for two tasks (occluded and holistic Re-ID) demonstrate that our proposed PFD is superior promising, which performs favorably against state-of-the-art methods. Code is available at https://github.com/WangTaoAs/PFD_Net



### RADA: Robust Adversarial Data Augmentation for Camera Localization in Challenging Weather
- **Arxiv ID**: http://arxiv.org/abs/2112.02469v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2112.02469v1)
- **Published**: 2021-12-05 03:49:11+00:00
- **Updated**: 2021-12-05 03:49:11+00:00
- **Authors**: Jialu Wang, Muhamad Risqi U. Saputra, Chris Xiaoxuan Lu, Niki Trigon, Andrew Markham
- **Comment**: None
- **Journal**: None
- **Summary**: Camera localization is a fundamental and crucial problem for many robotic applications. In recent years, using deep-learning for camera-based localization has become a popular research direction. However, they lack robustness to large domain shifts, which can be caused by seasonal or illumination changes between training and testing data sets. Data augmentation is an attractive approach to tackle this problem, as it does not require additional data to be provided. However, existing augmentation methods blindly perturb all pixels and therefore cannot achieve satisfactory performance. To overcome this issue, we proposed RADA, a system whose aim is to concentrate on perturbing the geometrically informative parts of the image. As a result, it learns to generate minimal image perturbations that are still capable of perplexing the network. We show that when these examples are utilized as augmentation, it greatly improves robustness. We show that our method outperforms previous augmentation techniques and achieves up to two times higher accuracy than the SOTA localization models (e.g., AtLoc and MapNet) when tested on `unseen' challenging weather conditions.



### Noise Distribution Adaptive Self-Supervised Image Denoising using Tweedie Distribution and Score Matching
- **Arxiv ID**: http://arxiv.org/abs/2112.03696v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2112.03696v1)
- **Published**: 2021-12-05 04:36:08+00:00
- **Updated**: 2021-12-05 04:36:08+00:00
- **Authors**: Kwanyoung Kim, Taesung Kwon, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Tweedie distributions are a special case of exponential dispersion models, which are often used in classical statistics as distributions for generalized linear models. Here, we reveal that Tweedie distributions also play key roles in modern deep learning era, leading to a distribution independent self-supervised image denoising formula without clean reference images. Specifically, by combining with the recent Noise2Score self-supervised image denoising approach and the saddle point approximation of Tweedie distribution, we can provide a general closed-form denoising formula that can be used for large classes of noise distributions without ever knowing the underlying noise distribution. Similar to the original Noise2Score, the new approach is composed of two successive steps: score matching using perturbed noisy images, followed by a closed form image denoising formula via distribution-independent Tweedie's formula. This also suggests a systematic algorithm to estimate the noise model and noise parameters for a given noisy image data set. Through extensive experiments, we demonstrate that the proposed method can accurately estimate noise models and parameters, and provide the state-of-the-art self-supervised image denoising performance in the benchmark dataset and real-world dataset.



### Deblurring via Stochastic Refinement
- **Arxiv ID**: http://arxiv.org/abs/2112.02475v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.02475v2)
- **Published**: 2021-12-05 04:36:09+00:00
- **Updated**: 2021-12-28 14:23:23+00:00
- **Authors**: Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G. Dimakis, Peyman Milanfar
- **Comment**: None
- **Journal**: None
- **Summary**: Image deblurring is an ill-posed problem with multiple plausible solutions for a given input image. However, most existing methods produce a deterministic estimate of the clean image and are trained to minimize pixel-level distortion. These metrics are known to be poorly correlated with human perception, and often lead to unrealistic reconstructions. We present an alternative framework for blind deblurring based on conditional diffusion models. Unlike existing techniques, we train a stochastic sampler that refines the output of a deterministic predictor and is capable of producing a diverse set of plausible reconstructions for a given input. This leads to a significant improvement in perceptual quality over existing state-of-the-art methods across multiple standard benchmarks. Our predict-and-refine approach also enables much more efficient sampling compared to typical diffusion models. Combined with a carefully tuned network architecture and inference procedure, our method is competitive in terms of distortion metrics such as PSNR. These results show clear benefits of our diffusion-based method for deblurring and challenge the widely used strategy of producing a single, deterministic reconstruction.



### Safe Distillation Box
- **Arxiv ID**: http://arxiv.org/abs/2112.03695v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.03695v1)
- **Published**: 2021-12-05 05:01:55+00:00
- **Updated**: 2021-12-05 05:01:55+00:00
- **Authors**: Jingwen Ye, Yining Mao, Jie Song, Xinchao Wang, Cheng Jin, Mingli Song
- **Comment**: Accepted by AAAI2022
- **Journal**: None
- **Summary**: Knowledge distillation (KD) has recently emerged as a powerful strategy to transfer knowledge from a pre-trained teacher model to a lightweight student, and has demonstrated its unprecedented success over a wide spectrum of applications. In spite of the encouraging results, the KD process per se poses a potential threat to network ownership protection, since the knowledge contained in network can be effortlessly distilled and hence exposed to a malicious user. In this paper, we propose a novel framework, termed as Safe Distillation Box (SDB), that allows us to wrap a pre-trained model in a virtual box for intellectual property protection. Specifically, SDB preserves the inference capability of the wrapped model to all users, but precludes KD from unauthorized users. For authorized users, on the other hand, SDB carries out a knowledge augmentation scheme to strengthen the KD performances and the results of the student model. In other words, all users may employ a model in SDB for inference, but only authorized users get access to KD from the model. The proposed SDB imposes no constraints over the model architecture, and may readily serve as a plug-and-play solution to protect the ownership of a pre-trained network. Experiments across various datasets and architectures demonstrate that, with SDB, the performance of an unauthorized KD drops significantly while that of an authorized gets enhanced, demonstrating the effectiveness of SDB.



### Classification of COVID-19 on chest X-Ray images using Deep Learning model with Histogram Equalization and Lungs Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.02478v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.3; I.4.6; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2112.02478v3)
- **Published**: 2021-12-05 05:04:38+00:00
- **Updated**: 2022-07-11 10:58:07+00:00
- **Authors**: Aman Swaraj, Karan Verma
- **Comment**: Total number of words of the manuscript- 6577 The number of words of
  the abstract- 238 The number of figures- 8 The number of tables- 10
- **Journal**: None
- **Summary**: Background and Objective: Artificial intelligence (AI) methods coupled with biomedical analysis has a critical role during pandemics as it helps to release the overwhelming pressure from healthcare systems and physicians. As the ongoing COVID-19 crisis worsens in countries having dense populations and inadequate testing kits like Brazil and India, radiological imaging can act as an important diagnostic tool to accurately classify covid-19 patients and prescribe the necessary treatment in due time. With this motivation, we present our study based on deep learning architecture for detecting covid-19 infected lungs using chest X-rays. Dataset: We collected a total of 2470 images for three different class labels, namely, healthy lungs, ordinary pneumonia, and covid-19 infected pneumonia, out of which 470 X-ray images belong to the covid-19 category. Methods: We first pre-process all the images using histogram equalization techniques and segment them using U-net architecture. VGG-16 network is then used for feature extraction from the pre-processed images which is further sampled by SMOTE oversampling technique to achieve a balanced dataset. Finally, the class-balanced features are classified using a support vector machine (SVM) classifier with 10-fold cross-validation and the accuracy is evaluated. Result and Conclusion: Our novel approach combining well-known pre-processing techniques, feature extraction methods, and dataset balancing method, lead us to an outstanding rate of recognition of 98% for COVID-19 images over a dataset of 2470 X-ray images. Our model is therefore fit to be utilized in healthcare facilities for screening purposes.



### Face Trees for Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.02487v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.02487v1)
- **Published**: 2021-12-05 06:35:12+00:00
- **Updated**: 2021-12-05 06:35:12+00:00
- **Authors**: Mojtaba Kolahdouzi, Alireza Sepas-Moghaddam, Ali Etemad
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an end-to-end architecture for facial expression recognition. Our model learns an optimal tree topology for facial landmarks, whose traversal generates a sequence from which we obtain an embedding to feed a sequential learner. The proposed architecture incorporates two main streams, one focusing on landmark positions to learn the structure of the face, while the other focuses on patches around the landmarks to learn texture information. Each stream is followed by an attention mechanism and the outputs are fed to a two-stream fusion component to perform the final classification. We conduct extensive experiments on two large-scale publicly available facial expression datasets, AffectNet and FER2013, to evaluate the efficacy of our approach. Our method outperforms other solutions in the area and sets new state-of-the-art expression recognition rates on these datasets.



### Exploring Complicated Search Spaces with Interleaving-Free Sampling
- **Arxiv ID**: http://arxiv.org/abs/2112.02488v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.02488v1)
- **Published**: 2021-12-05 06:42:48+00:00
- **Updated**: 2021-12-05 06:42:48+00:00
- **Authors**: Yunjie Tian, Lingxi Xie, Jiemin Fang, Jianbin Jiao, Qixiang Ye, Qi Tian
- **Comment**: 9 pages, 8 figures, 6 tables
- **Journal**: None
- **Summary**: The existing neural architecture search algorithms are mostly working on search spaces with short-distance connections. We argue that such designs, though safe and stable, obstacles the search algorithms from exploring more complicated scenarios. In this paper, we build the search algorithm upon a complicated search space with long-distance connections, and show that existing weight-sharing search algorithms mostly fail due to the existence of \textbf{interleaved connections}. Based on the observation, we present a simple yet effective algorithm named \textbf{IF-NAS}, where we perform a periodic sampling strategy to construct different sub-networks during the search procedure, avoiding the interleaved connections to emerge in any of them. In the proposed search space, IF-NAS outperform both random sampling and previous weight-sharing search algorithms by a significant margin. IF-NAS also generalizes to the micro cell-based spaces which are much easier. Our research emphasizes the importance of macro structure and we look forward to further efforts along this direction.



### Implicit Neural Deformation for Sparse-View Face Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2112.02494v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.02494v2)
- **Published**: 2021-12-05 07:02:53+00:00
- **Updated**: 2022-10-03 06:13:47+00:00
- **Authors**: Moran Li, Haibin Huang, Yi Zheng, Mengtian Li, Nong Sang, Chongyang Ma
- **Comment**: 10 pages, 6 figures, The 30th Pacific Conference on Computer Graphics
  and Applications. Pacific Graphics(PG) 2022
- **Journal**: None
- **Summary**: In this work, we present a new method for 3D face reconstruction from sparse-view RGB images. Unlike previous methods which are built upon 3D morphable models (3DMMs) with limited details, we leverage an implicit representation to encode rich geometric features. Our overall pipeline consists of two major components, including a geometry network, which learns a deformable neural signed distance function (SDF) as the 3D face representation, and a rendering network, which learns to render on-surface points of the neural SDF to match the input images via self-supervised optimization. To handle in-the-wild sparse-view input of the same target with different expressions at test time, we propose residual latent code to effectively expand the shape space of the learned implicit face representation as well as a novel view-switch loss to enforce consistency among different views. Our experimental results on several benchmark datasets demonstrate that our approach outperforms alternative baselines and achieves superior face reconstruction results compared to state-of-the-art methods.



### MovieNet-PS: A Large-Scale Person Search Dataset in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2112.02500v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.02500v4)
- **Published**: 2021-12-05 07:38:53+00:00
- **Updated**: 2023-02-28 11:19:31+00:00
- **Authors**: Jie Qin, Peng Zheng, Yichao Yan, Rong Quan, Xiaogang Cheng, Bingbing Ni
- **Comment**: ICASSP 2023
- **Journal**: None
- **Summary**: Person search aims to jointly localize and identify a query person from natural, uncropped images, which has been actively studied over the past few years. In this paper, we delve into the rich context information globally and locally surrounding the target person, which we refer to as scene and group context, respectively. Unlike previous works that treat the two types of context individually, we exploit them in a unified global-local context network (GLCNet) with the intuitive aim of feature enhancement. Specifically, re-ID embeddings and context features are simultaneously learned in a multi-stage fashion, ultimately leading to enhanced, discriminative features for person search. We conduct the experiments on two person search benchmarks (i.e., CUHK-SYSU and PRW) as well as extend our approach to a more challenging setting (i.e., character search on MovieNet). Extensive experimental results demonstrate the consistent improvement of the proposed GLCNet over the state-of-the-art methods on all three datasets. Our source codes, pre-trained models, and the new dataset are publicly available at: https://github.com/ZhengPeng7/GLCNet.



### Adaptive Channel Encoding Transformer for Point Cloud Analysis
- **Arxiv ID**: http://arxiv.org/abs/2112.02507v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.02507v4)
- **Published**: 2021-12-05 08:18:00+00:00
- **Updated**: 2022-07-17 01:49:22+00:00
- **Authors**: Guoquan Xu, Hezhi Cao, Yifan Zhang, Yanxin Ma, Jianwei Wan, Ke Xu
- **Comment**: ICANN2022
- **Journal**: None
- **Summary**: Transformer plays an increasingly important role in various computer vision areas and remarkable achievements have also been made in point cloud analysis. Since they mainly focus on point-wise transformer, an adaptive channel encoding transformer is proposed in this paper. Specifically, a channel convolution called Transformer-Conv is designed to encode the channel. It can encode feature channels by capturing the potential relationship between coordinates and features. Compared with simply assigning attention weight to each channel, our method aims to encode the channel adaptively. In addition, our network adopts the neighborhood search method of low-level and high-level dual semantic receptive fields to improve the performance. Extensive experiments show that our method is superior to state-of-the-art point cloud classification and segmentation methods on three benchmark datasets.



### Uncertainty-Guided Mutual Consistency Learning for Semi-Supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.02508v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.02508v2)
- **Published**: 2021-12-05 08:19:41+00:00
- **Updated**: 2022-08-25 23:32:56+00:00
- **Authors**: Yichi Zhang, Rushi Jiao, Qingcheng Liao, Dongyang Li, Jicong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image segmentation is a fundamental and critical step in many clinical approaches. Semi-supervised learning has been widely applied to medical image segmentation tasks since it alleviates the heavy burden of acquiring expert-examined annotations and takes the advantage of unlabeled data which is much easier to acquire. Although consistency learning has been proven to be an effective approach by enforcing an invariance of predictions under different distributions, existing approaches cannot make full use of region-level shape constraint and boundary-level distance information from unlabeled data. In this paper, we propose a novel uncertainty-guided mutual consistency learning framework to effectively exploit unlabeled data by integrating intra-task consistency learning from up-to-date predictions for self-ensembling and cross-task consistency learning from task-level regularization to exploit geometric shape information. The framework is guided by the estimated segmentation uncertainty of models to select out relatively certain predictions for consistency learning, so as to effectively exploit more reliable information from unlabeled data. Experiments on two publicly available benchmark datasets showed that: 1) Our proposed method can achieve significant performance improvement by leveraging unlabeled data, with up to 4.13% and 9.82% in Dice coefficient compared to supervised baseline on left atrium segmentation and brain tumor segmentation, respectively. 2) Compared with other semi-supervised segmentation methods, our proposed method achieve better segmentation performance under the same backbone network and task settings on both datasets, demonstrating the effectiveness and robustness of our method and potential transferability for other medical image segmentation tasks.



### Adaptive Channel Encoding for Point Cloud Analysis
- **Arxiv ID**: http://arxiv.org/abs/2112.02509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.02509v1)
- **Published**: 2021-12-05 08:20:27+00:00
- **Updated**: 2021-12-05 08:20:27+00:00
- **Authors**: Guoquan Xu, Hezhi Cao, Yifan Zhang, Jianwei Wan, Ke Xu, Yanxin Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Attention mechanism plays a more and more important role in point cloud analysis and channel attention is one of the hotspots. With so much channel information, it is difficult for neural networks to screen useful channel information. Thus, an adaptive channel encoding mechanism is proposed to capture channel relationships in this paper. It improves the quality of the representation generated by the network by explicitly encoding the interdependence between the channels of its features. Specifically, a channel-wise convolution (Channel-Conv) is proposed to adaptively learn the relationship between coordinates and features, so as to encode the channel. Different from the popular attention weight schemes, the Channel-Conv proposed in this paper realizes adaptability in convolution operation, rather than simply assigning different weights for channels. Extensive experiments on existing benchmarks verify our method achieves the state of the arts.



### Neural Photometry-guided Visual Attribute Transfer
- **Arxiv ID**: http://arxiv.org/abs/2112.02520v1
- **DOI**: 10.1109/TVCG.2021.3133081
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, 68T07 (Primary) 68T45, 68U05 (Secondary), I.2.6; I.4.10; I.3.3; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2112.02520v1)
- **Published**: 2021-12-05 09:22:28+00:00
- **Updated**: 2021-12-05 09:22:28+00:00
- **Authors**: Carlos Rodriguez-Pardo, Elena Garces
- **Comment**: 13 pages. To be published in Transactions on Visualizations and
  Computer Graphics. Project website:
  http://carlosrodriguezpardo.es/projects/NeuralPhotometricTransfer/
- **Journal**: None
- **Summary**: We present a deep learning-based method for propagating spatially-varying visual material attributes (e.g. texture maps or image stylizations) to larger samples of the same or similar materials. For training, we leverage images of the material taken under multiple illuminations and a dedicated data augmentation policy, making the transfer robust to novel illumination conditions and affine deformations. Our model relies on a supervised image-to-image translation framework and is agnostic to the transferred domain; we showcase a semantic segmentation, a normal map, and a stylization. Following an image analogies approach, the method only requires the training data to contain the same visual structures as the input guidance. Our approach works at interactive rates, making it suitable for material edit applications. We thoroughly evaluate our learning methodology in a controlled setup providing quantitative measures of performance. Last, we demonstrate that training the model on a single material is enough to generalize to materials of the same type without the need for massive datasets.



### Snapshot HDR Video Construction Using Coded Mask
- **Arxiv ID**: http://arxiv.org/abs/2112.02522v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.02522v1)
- **Published**: 2021-12-05 09:32:11+00:00
- **Updated**: 2021-12-05 09:32:11+00:00
- **Authors**: Masheal Alghamdi, Qiang Fu, Ali Thabet, Wolfgang Heidrich
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: This paper study the reconstruction of High Dynamic Range (HDR) video from snapshot-coded LDR video. Constructing an HDR video requires restoring the HDR values for each frame and maintaining the consistency between successive frames. HDR image acquisition from single image capture, also known as snapshot HDR imaging, can be achieved in several ways. For example, the reconfigurable snapshot HDR camera is realized by introducing an optical element into the optical stack of the camera; by placing a coded mask at a small standoff distance in front of the sensor. High-quality HDR image can be recovered from the captured coded image using deep learning methods. This study utilizes 3D-CNNs to perform a joint demosaicking, denoising, and HDR video reconstruction from coded LDR video. We enforce more temporally consistent HDR video reconstruction by introducing a temporal loss function that considers the short-term and long-term consistency. The obtained results are promising and could lead to affordable HDR video capture using conventional cameras.



### STSM: Spatio-Temporal Shift Module for Efficient Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.02523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.02523v1)
- **Published**: 2021-12-05 09:40:49+00:00
- **Updated**: 2021-12-05 09:40:49+00:00
- **Authors**: Zhaoqilin Yang, Gaoyun An
- **Comment**: 9 pages,4 figures
- **Journal**: None
- **Summary**: The modeling, computational cost, and accuracy of traditional Spatio-temporal networks are the three most concentrated research topics in video action recognition. The traditional 2D convolution has a low computational cost, but it cannot capture the time relationship; the convolutional neural networks (CNNs) model based on 3D convolution can obtain good performance, but its computational cost is high, and the amount of parameters is large. In this paper, we propose a plug-and-play Spatio-temporal Shift Module (STSM), which is a generic module that is both effective and high-performance. Specifically, after STSM is inserted into other networks, the performance of the network can be improved without increasing the number of calculations and parameters. In particular, when the network is 2D CNNs, our STSM module allows the network to learn efficient Spatio-temporal features. We conducted extensive evaluations of the proposed module, conducted numerous experiments to study its effectiveness in video action recognition, and achieved state-of-the-art results on the kinetics-400 and Something-Something V2 datasets.



### End-to-End Segmentation via Patch-wise Polygons Prediction
- **Arxiv ID**: http://arxiv.org/abs/2112.02535v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.02535v1)
- **Published**: 2021-12-05 10:42:40+00:00
- **Updated**: 2021-12-05 10:42:40+00:00
- **Authors**: Tal Shaharabany, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: The leading segmentation methods represent the output map as a pixel grid. We study an alternative representation in which the object edges are modeled, per image patch, as a polygon with $k$ vertices that is coupled with per-patch label probabilities. The vertices are optimized by employing a differentiable neural renderer to create a raster image. The delineated region is then compared with the ground truth segmentation. Our method obtains multiple state-of-the-art results: 76.26\% mIoU on the Cityscapes validation, 90.92\% IoU on the Vaihingen building segmentation benchmark, 66.82\% IoU for the MoNU microscopy dataset, and 90.91\% for the bird benchmark CUB. Our code for training and reproducing these results is attached as supplementary.



### Hard Sample Aware Noise Robust Learning for Histopathology Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2112.03694v1
- **DOI**: 10.1109/TMI.2021.3125459
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, q-bio.QM, I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2112.03694v1)
- **Published**: 2021-12-05 11:07:55+00:00
- **Updated**: 2021-12-05 11:07:55+00:00
- **Authors**: Chuang Zhu, Wenkai Chen, Ting Peng, Ying Wang, Mulan Jin
- **Comment**: 14 pages, 20figures, IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Deep learning-based histopathology image classification is a key technique to help physicians in improving the accuracy and promptness of cancer diagnosis. However, the noisy labels are often inevitable in the complex manual annotation process, and thus mislead the training of the classification model. In this work, we introduce a novel hard sample aware noise robust learning method for histopathology image classification. To distinguish the informative hard samples from the harmful noisy ones, we build an easy/hard/noisy (EHN) detection model by using the sample training history. Then we integrate the EHN into a self-training architecture to lower the noise rate through gradually label correction. With the obtained almost clean dataset, we further propose a noise suppressing and hard enhancing (NSHE) scheme to train the noise robust model. Compared with the previous works, our method can save more clean samples and can be directly applied to the real-world noisy dataset scenario without using a clean subset. Experimental results demonstrate that the proposed scheme outperforms the current state-of-the-art methods in both the synthetic and real-world noisy datasets. The source code and data are available at https://github.com/bupt-ai-cz/HSA-NRL/.



### Generative Modeling of Turbulence
- **Arxiv ID**: http://arxiv.org/abs/2112.02548v2
- **DOI**: 10.1063/5.0082562
- **Categories**: **physics.flu-dyn**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.02548v2)
- **Published**: 2021-12-05 11:39:14+00:00
- **Updated**: 2022-03-04 17:50:53+00:00
- **Authors**: Claudia Drygala, Benjamin Winhart, Francesca di Mare, Hanno Gottschalk
- **Comment**: None
- **Journal**: None
- **Summary**: We present a mathematically well founded approach for the synthetic modeling of turbulent flows using generative adversarial networks (GAN). Based on the analysis of chaotic, deterministic systems in terms of ergodicity, we outline a mathematical proof that GAN can actually learn to sample state snapshots form the invariant measure of the chaotic system. Based on this analysis, we study a hierarchy of chaotic systems starting with the Lorenz attractor and then carry on to the modeling of turbulent flows with GAN. As training data, we use fields of velocity fluctuations obtained from large eddy simulations (LES). Two architectures are investigated in detail: we use a deep, convolutional GAN (DCGAN) to synthesise the turbulent flow around a cylinder. We furthermore simulate the flow around a low pressure turbine stator using the pix2pixHD architecture for a conditional DCGAN being conditioned on the position of a rotating wake in front of the stator. The settings of adversarial training and the effects of using specific GAN architectures are explained. We thereby show that GAN are efficient in simulating turbulence in technically challenging flow problems on the basis of a moderate amount of training data. GAN training and inference times significantly fall short when compared with classical numerical methods, in particular LES, while still providing turbulent flows in high resolution. We furthermore analyse the statistical properties of the synthesized and LES flow fields, which agree excellently. We also show the ability of the conditional GAN to generalize over changes of geometry by generating turbulent flow fields for positions of the wake that are not included in the training data.



### Learning Tracking Representations via Dual-Branch Fully Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/2112.02571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.02571v1)
- **Published**: 2021-12-05 13:44:33+00:00
- **Updated**: 2021-12-05 13:44:33+00:00
- **Authors**: Fei Xie, Chunyu Wang, Guangting Wang, Wankou Yang, Wenjun Zeng
- **Comment**: ICCV21 Workshops
- **Journal**: None
- **Summary**: We present a Siamese-like Dual-branch network based on solely Transformers for tracking. Given a template and a search image, we divide them into non-overlapping patches and extract a feature vector for each patch based on its matching results with others within an attention window. For each token, we estimate whether it contains the target object and the corresponding size. The advantage of the approach is that the features are learned from matching, and ultimately, for matching. So the features are aligned with the object tracking task. The method achieves better or comparable results as the best-performing methods which first use CNN to extract features and then use Transformer to fuse them. It outperforms the state-of-the-art methods on the GOT-10k and VOT2020 benchmarks. In addition, the method achieves real-time inference speed (about $40$ fps) on one GPU. The code and models will be released.



### PolyphonicFormer: Unified Query Learning for Depth-aware Video Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.02582v4
- **DOI**: 10.1007/978-3-031-19812-0_34
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.02582v4)
- **Published**: 2021-12-05 14:31:47+00:00
- **Updated**: 2022-12-28 03:26:33+00:00
- **Authors**: Haobo Yuan, Xiangtai Li, Yibo Yang, Guangliang Cheng, Jing Zhang, Yunhai Tong, Lefei Zhang, Dacheng Tao
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: The Depth-aware Video Panoptic Segmentation (DVPS) is a new challenging vision problem that aims to predict panoptic segmentation and depth in a video simultaneously. The previous work solves this task by extending the existing panoptic segmentation method with an extra dense depth prediction and instance tracking head. However, the relationship between the depth and panoptic segmentation is not well explored -- simply combining existing methods leads to competition and needs carefully weight balancing. In this paper, we present PolyphonicFormer, a vision transformer to unify these sub-tasks under the DVPS task and lead to more robust results. Our principal insight is that the depth can be harmonized with the panoptic segmentation with our proposed new paradigm of predicting instance level depth maps with object queries. Then the relationship between the two tasks via query-based learning is explored. From the experiments, we demonstrate the benefits of our design from both depth estimation and panoptic segmentation aspects. Since each thing query also encodes the instance-wise information, it is natural to perform tracking directly with appearance learning. Our method achieves state-of-the-art results on two DVPS datasets (Semantic KITTI, Cityscapes), and ranks 1st on the ICCV-2021 BMTT Challenge video + depth track. Code is available at https://github.com/HarborYuan/PolyphonicFormer .



### Constrained Adaptive Projection with Pretrained Features for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2112.02597v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.02597v2)
- **Published**: 2021-12-05 15:29:59+00:00
- **Updated**: 2022-04-25 08:28:23+00:00
- **Authors**: Xingtai Gui, Di Wu, Yang Chang, Shicai Fan
- **Comment**: Accepted to IJCAI 2022 Main Track. This version includes 6 pages of
  main paper, 2 pages of Appendix
- **Journal**: None
- **Summary**: Anomaly detection aims to separate anomalies from normal samples, and the pretrained network is promising for anomaly detection. However, adapting the pretrained features would be confronted with the risk of pattern collapse when finetuning on one-class training data. In this paper, we propose an anomaly detection framework called constrained adaptive projection with pretrained features (CAP). Combined with pretrained features, a simple linear projection head applied on a specific input and its k most similar pretrained normal representations is designed for feature adaptation, and a reformed self-attention is leveraged to mine the inner-relationship among one-class semantic features. A loss function is proposed to avoid potential pattern collapse. Concretely, it considers the similarity between a specific data and its corresponding adaptive normal representation, and incorporates a constraint term slightly aligning pretrained and adaptive spaces. Our method achieves state-ofthe-art anomaly detection performance on semantic anomaly detection and sensory anomaly detection benchmarks including 96.5% AUROC on CIFAR- 100 dataset, 97.0% AUROC on CIFAR-10 dataset and 89.9% AUROC on MvTec dataset.



### PSI: A Pedestrian Behavior Dataset for Socially Intelligent Autonomous Car
- **Arxiv ID**: http://arxiv.org/abs/2112.02604v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.02604v2)
- **Published**: 2021-12-05 15:54:57+00:00
- **Updated**: 2022-06-11 21:08:21+00:00
- **Authors**: Tina Chen, Taotao Jing, Renran Tian, Yaobin Chen, Joshua Domeyer, Heishiro Toyoda, Rini Sherony, Zhengming Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Prediction of pedestrian behavior is critical for fully autonomous vehicles to drive in busy city streets safely and efficiently. The future autonomous cars need to fit into mixed conditions with not only technical but also social capabilities. As more algorithms and datasets have been developed to predict pedestrian behaviors, these efforts lack the benchmark labels and the capability to estimate the temporal-dynamic intent changes of the pedestrians, provide explanations of the interaction scenes, and support algorithms with social intelligence. This paper proposes and shares another benchmark dataset called the IUPUI-CSRC Pedestrian Situated Intent (PSI) data with two innovative labels besides comprehensive computer vision labels. The first novel label is the dynamic intent changes for the pedestrians to cross in front of the ego-vehicle, achieved from 24 drivers with diverse backgrounds. The second one is the text-based explanations of the driver reasoning process when estimating pedestrian intents and predicting their behaviors during the interaction period. These innovative labels can enable several computer vision tasks, including pedestrian intent/behavior prediction, vehicle-pedestrian interaction segmentation, and video-to-language mapping for explainable algorithms. The released dataset can fundamentally improve the development of pedestrian behavior prediction models and develop socially intelligent autonomous cars to interact with pedestrians efficiently. The dataset has been evaluated with different tasks and is released to the public to access.



### Real-time Virtual Intraoperative CT for Image Guided Surgery
- **Arxiv ID**: http://arxiv.org/abs/2112.02608v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.02608v1)
- **Published**: 2021-12-05 16:06:34+00:00
- **Updated**: 2021-12-05 16:06:34+00:00
- **Authors**: Yangming Li, Neeraja Konuthula, Ian M. Humphreys, Kris Moe, Blake Hannaford, Randall Bly
- **Comment**: None
- **Journal**: None
- **Summary**: Abstract. Purpose: This paper presents a scheme for generating virtual intraoperative CT scans in order to improve surgical completeness in Endoscopic Sinus Surgeries (ESS). Approach: The work presents three methods, the tip motion-based, the tip trajectory-based, and the instrument based, along with non-parametric smoothing and Gaussian Process Regression, for virtual intraoperative CT generation. Results: The proposed methods studied and compared on ESS performed on cadavers. Surgical results show all three methods improve the Dice Similarity Coefficients > 86%, with F-score > 92% and precision > 89.91%. The tip trajectory-based method was found to have best performance and reached 96.87% precision in surgical completeness evaluation. Conclusions: This work demonstrated that virtual intraoperative CT scans improves the consistency between the actual surgical scene and the reference model, and improves surgical completeness in ESS. Comparing with actual intraoperative CT scans, the proposed scheme has no impact on existing surgical protocols, does not require extra hardware other than the one is already available in most ESS overcome the high costs, the repeated radiation, and the elongated anesthesia caused by actual intraoperative CTs, and is practical in ESS.



### Dynamic Token Normalization Improves Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2112.02624v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.02624v2)
- **Published**: 2021-12-05 17:04:59+00:00
- **Updated**: 2022-10-14 05:25:34+00:00
- **Authors**: Wenqi Shao, Yixiao Ge, Zhaoyang Zhang, Xuyuan Xu, Xiaogang Wang, Ying Shan, Ping Luo
- **Comment**: Published at ICLR'22; 18 pages, 12 Tables, 9 Figures
- **Journal**: None
- **Summary**: Vision Transformer (ViT) and its variants (e.g., Swin, PVT) have achieved great success in various computer vision tasks, owing to their capability to learn long-range contextual information. Layer Normalization (LN) is an essential ingredient in these models. However, we found that the ordinary LN makes tokens at different positions similar in magnitude because it normalizes embeddings within each token. It is difficult for Transformers to capture inductive bias such as the positional context in an image with LN. We tackle this problem by proposing a new normalizer, termed Dynamic Token Normalization (DTN), where normalization is performed both within each token (intra-token) and across different tokens (inter-token). DTN has several merits. Firstly, it is built on a unified formulation and thus can represent various existing normalization methods. Secondly, DTN learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context. {Thirdly, by simply replacing LN layers, DTN can be readily plugged into various vision transformers, such as ViT, Swin, PVT, LeViT, T2T-ViT, BigBird and Reformer. Extensive experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead. For example, DTN outperforms LN by $0.5\%$ - $1.2\%$ top-1 accuracy on ImageNet, by $1.2$ - $1.4$ box AP in object detection on COCO benchmark, by $2.3\%$ - $3.9\%$ mCE in robustness experiments on ImageNet-C, and by $0.5\%$ - $0.8\%$ accuracy in Long ListOps on Long-Range Arena.} Codes will be made public at \url{https://github.com/wqshao126/DTN}



### Boosting Mobile CNN Inference through Semantic Memory
- **Arxiv ID**: http://arxiv.org/abs/2112.02644v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2112.02644v1)
- **Published**: 2021-12-05 18:18:31+00:00
- **Updated**: 2021-12-05 18:18:31+00:00
- **Authors**: Yun Li, Chen Zhang, Shihao Han, Li Lyna Zhang, Baoqun Yin, Yunxin Liu, Mengwei Xu
- **Comment**: 13 pages, 13 figures
- **Journal**: None
- **Summary**: Human brains are known to be capable of speeding up visual recognition of repeatedly presented objects through faster memory encoding and accessing procedures on activated neurons. For the first time, we borrow and distill such a capability into a semantic memory design, namely SMTM, to improve on-device CNN inference. SMTM employs a hierarchical memory architecture to leverage the long-tail distribution of objects of interest, and further incorporates several novel techniques to put it into effects: (1) it encodes high-dimensional feature maps into low-dimensional, semantic vectors for low-cost yet accurate cache and lookup; (2) it uses a novel metric in determining the exit timing considering different layers' inherent characteristics; (3) it adaptively adjusts the cache size and semantic vectors to fit the scene dynamics. SMTM is prototyped on commodity CNN engine and runs on both mobile CPU and GPU. Extensive experiments on large-scale datasets and models show that SMTM can significantly speed up the model inference over standard approach (up to 2X) and prior cache designs (up to 1.5X), with acceptable accuracy loss.



### Learning Query Expansion over the Nearest Neighbor Graph
- **Arxiv ID**: http://arxiv.org/abs/2112.02666v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.02666v1)
- **Published**: 2021-12-05 19:48:42+00:00
- **Updated**: 2021-12-05 19:48:42+00:00
- **Authors**: Benjamin Klein, Lior Wolf
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Query Expansion (QE) is a well established method for improving retrieval metrics in image search applications. When using QE, the search is conducted on a new query vector, constructed using an aggregation function over the query and images from the database. Recent works gave rise to QE techniques in which the aggregation function is learned, whereas previous techniques were based on hand-crafted aggregation functions, e.g., taking the mean of the query's nearest neighbors. However, most QE methods have focused on aggregation functions that work directly over the query and its immediate nearest neighbors. In this work, a hierarchical model, Graph Query Expansion (GQE), is presented, which is learned in a supervised manner and performs aggregation over an extended neighborhood of the query, thus increasing the information used from the database when computing the query expansion, and using the structure of the nearest neighbors graph. The technique achieves state-of-the-art results over known benchmarks.



### DIY Graphics Tab: A Cost-Effective Alternative to Graphics Tablet for Educators
- **Arxiv ID**: http://arxiv.org/abs/2112.03269v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.03269v1)
- **Published**: 2021-12-05 20:49:32+00:00
- **Updated**: 2021-12-05 20:49:32+00:00
- **Authors**: Mohammad Imrul Jubair, Arafat Ibne Yousuf, Tashfiq Ahmed, Hasanath Jamy, Foisal Reza, Mohsena Ashraf
- **Comment**: Accepted in AAAI2022 workshop
- **Journal**: None
- **Summary**: Everyday, more and more people are turning to online learning, which has altered our traditional classroom method. Recording lectures has always been a normal task for online educators, and it has lately become even more important during the epidemic because actual lessons are still being postponed in several countries. When recording lectures, a graphics tablet is a great substitute for a whiteboard because of its portability and ability to interface with computers. This graphic tablet, however, is too expensive for the majority of instructors. In this paper, we propose a computer vision-based alternative to the graphics tablet for instructors and educators, which functions largely in the same way as a graphic tablet but just requires a pen, paper, and a laptop's webcam. We call it "Do-It-Yourself Graphics Tab" or "DIY Graphics Tab". Our system receives a sequence of images of a person's writing on paper acquired by a camera as input and outputs the screen containing the contents of the writing from the paper. The task is not straightforward since there are many obstacles such as occlusion due to the person's hand, random movement of the paper, poor lighting condition, perspective distortion due to the angle of view, etc. A pipeline is used to route the input recording through our system, which conducts instance segmentation and preprocessing before generating the appropriate output. We also conducted user experience evaluations from the teachers and students, and their responses are examined in this paper.



### Joint Symmetry Detection and Shape Matching for Non-Rigid Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2112.02713v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.02713v2)
- **Published**: 2021-12-05 23:49:14+00:00
- **Updated**: 2022-04-19 11:44:30+00:00
- **Authors**: Abhishek Sharma, Maks Ovsjanikov
- **Comment**: Under Review. arXiv admin note: substantial text overlap with
  arXiv:2110.02994
- **Journal**: None
- **Summary**: Despite the success of deep functional maps in non-rigid 3D shape matching, there exists no learning framework that models both self-symmetry and shape matching simultaneously. This is despite the fact that errors due to symmetry mismatch are a major challenge in non-rigid shape matching. In this paper, we propose a novel framework that simultaneously learns both self symmetry as well as a pairwise map between a pair of shapes. Our key idea is to couple a self symmetry map and a pairwise map through a regularization term that provides a joint constraint on both of them, thereby, leading to more accurate maps. We validate our method on several benchmarks where it outperforms many competitive baselines on both tasks.



