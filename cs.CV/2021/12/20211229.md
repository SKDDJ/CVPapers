# Arxiv Papers in cs.CV on 2021-12-29
### Closer Look at the Transferability of Adversarial Examples: How They Fool Different Models Differently
- **Arxiv ID**: http://arxiv.org/abs/2112.14337v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.14337v3)
- **Published**: 2021-12-29 00:18:59+00:00
- **Updated**: 2022-10-20 03:47:04+00:00
- **Authors**: Futa Waseda, Sosuke Nishikawa, Trung-Nghia Le, Huy H. Nguyen, Isao Echizen
- **Comment**: 25 pages, 13 figures, Accepted at the IEEE Winter Conference on
  Applications of Computer Vision (WACV) 2023
- **Journal**: None
- **Summary**: Deep neural networks are vulnerable to adversarial examples (AEs), which have adversarial transferability: AEs generated for the source model can mislead another (target) model's predictions. However, the transferability has not been understood in terms of to which class target model's predictions were misled (i.e., class-aware transferability). In this paper, we differentiate the cases in which a target model predicts the same wrong class as the source model ("same mistake") or a different wrong class ("different mistake") to analyze and provide an explanation of the mechanism. We find that (1) AEs tend to cause same mistakes, which correlates with "non-targeted transferability"; however, (2) different mistakes occur even between similar models, regardless of the perturbation size. Furthermore, we present evidence that the difference between same mistakes and different mistakes can be explained by non-robust features, predictive but human-uninterpretable patterns: different mistakes occur when non-robust features in AEs are used differently by models. Non-robust features can thus provide consistent explanations for the class-aware transferability of AEs.



### Super-Efficient Super Resolution for Fast Adversarial Defense at the Edge
- **Arxiv ID**: http://arxiv.org/abs/2112.14340v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.14340v1)
- **Published**: 2021-12-29 00:35:41+00:00
- **Updated**: 2021-12-29 00:35:41+00:00
- **Authors**: Kartikeya Bhardwaj, Dibakar Gope, James Ward, Paul Whatmough, Danny Loh
- **Comment**: This preprint is for personal use only. The official article will
  appear in proceedings of Design, Automation & Test in Europe (DATE), 2022, as
  part of the Special Initiative on Autonomous Systems Design (ASD)
- **Journal**: None
- **Summary**: Autonomous systems are highly vulnerable to a variety of adversarial attacks on Deep Neural Networks (DNNs). Training-free model-agnostic defenses have recently gained popularity due to their speed, ease of deployment, and ability to work across many DNNs. To this end, a new technique has emerged for mitigating attacks on image classification DNNs, namely, preprocessing adversarial images using super resolution -- upscaling low-quality inputs into high-resolution images. This defense requires running both image classifiers and super resolution models on constrained autonomous systems. However, super resolution incurs a heavy computational cost. Therefore, in this paper, we investigate the following question: Does the robustness of image classifiers suffer if we use tiny super resolution models? To answer this, we first review a recent work called Super-Efficient Super Resolution (SESR) that achieves similar or better image quality than prior art while requiring 2x to 330x fewer Multiply-Accumulate (MAC) operations. We demonstrate that despite being orders of magnitude smaller than existing models, SESR achieves the same level of robustness as significantly larger networks. Finally, we estimate end-to-end performance of super resolution-based defenses on a commercial Arm Ethos-U55 micro-NPU. Our findings show that SESR achieves nearly 3x higher FPS than a baseline while achieving similar robustness.



### Background-aware Classification Activation Map for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2112.14379v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.14379v1)
- **Published**: 2021-12-29 03:12:09+00:00
- **Updated**: 2021-12-29 03:12:09+00:00
- **Authors**: Lei Zhu, Qi She, Qian Chen, Xiangxi Meng, Mufeng Geng, Lujia Jin, Zhe Jiang, Bin Qiu, Yunfei You, Yibao Zhang, Qiushi Ren, Yanye Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly supervised object localization (WSOL) relaxes the requirement of dense annotations for object localization by using image-level classification masks to supervise its learning process. However, current WSOL methods suffer from excessive activation of background locations and need post-processing to obtain the localization mask. This paper attributes these issues to the unawareness of background cues, and propose the background-aware classification activation map (B-CAM) to simultaneously learn localization scores of both object and background with only image-level labels. In our B-CAM, two image-level features, aggregated by pixel-level features of potential background and object locations, are used to purify the object feature from the object-related background and to represent the feature of the pure-background sample, respectively. Then based on these two features, both the object classifier and the background classifier are learned to determine the binary object localization mask. Our B-CAM can be trained in end-to-end manner based on a proposed stagger classification loss, which not only improves the objects localization but also suppresses the background activation. Experiments show that our B-CAM outperforms one-stage WSOL methods on the CUB-200, OpenImages and VOC2012 datasets.



### Cross-Domain Empirical Risk Minimization for Unbiased Long-tailed Classification
- **Arxiv ID**: http://arxiv.org/abs/2112.14380v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.14380v1)
- **Published**: 2021-12-29 03:18:47+00:00
- **Updated**: 2021-12-29 03:18:47+00:00
- **Authors**: Beier Zhu, Yulei Niu, Xian-Sheng Hua, Hanwang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We address the overlooked unbiasedness in existing long-tailed classification methods: we find that their overall improvement is mostly attributed to the biased preference of tail over head, as the test distribution is assumed to be balanced; however, when the test is as imbalanced as the long-tailed training data -- let the test respect Zipf's law of nature -- the tail bias is no longer beneficial overall because it hurts the head majorities. In this paper, we propose Cross-Domain Empirical Risk Minimization (xERM) for training an unbiased model to achieve strong performances on both test distributions, which empirically demonstrates that xERM fundamentally improves the classification by learning better feature representation rather than the head vs. tail game. Based on causality, we further theoretically explain why xERM achieves unbiasedness: the bias caused by the domain selection is removed by adjusting the empirical risks on the imbalanced domain and the balanced but unseen domain. Codes are available at https://github.com/BeierZhu/xERM.



### COTReg:Coupled Optimal Transport based Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2112.14381v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.14381v2)
- **Published**: 2021-12-29 03:20:18+00:00
- **Updated**: 2022-10-07 13:44:34+00:00
- **Authors**: Guofeng Mei, Xiaoshui Huang, Litao Yu, Jian Zhang, Mohammed Bennamoun
- **Comment**: None
- **Journal**: None
- **Summary**: Generating a set of high-quality correspondences or matches is one of the most critical steps in point cloud registration. This paper proposes a learning framework COTReg by jointly considering the pointwise and structural matchings to predict correspondences of 3D point cloud registration. Specifically, we transform the two matchings into a Wasserstein distance-based and a Gromov-Wasserstein distance-based optimizations, respectively. Thus the task of establishing the correspondences can be naturally reshaped to a coupled optimal transport problem. Furthermore, we design a network to predict the confidence score of being an inlier for each point of the point clouds, which provides the overlap region information to generate correspondences. Our correspondence prediction pipeline can be easily integrated into either learning-based features like FCGF or traditional descriptors like FPFH. We conducted comprehensive experiments on 3DMatch, KITTI, 3DCSR, and ModelNet40 benchmarks, showing the state-of-art performance of the proposed method.



### Self-Supervised Robustifying Guidance for Monocular 3D Face Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2112.14382v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.14382v3)
- **Published**: 2021-12-29 03:30:50+00:00
- **Updated**: 2022-10-21 04:38:14+00:00
- **Authors**: Hitika Tiwari, Min-Hung Chen, Yi-Min Tsai, Hsien-Kai Kuo, Hung-Jen Chen, Kevin Jou, K. S. Venkatesh, Yong-Sheng Chen
- **Comment**: Accepted by The 33rd British Machine Vision Conference (BMVC) 2022.
  Evaluation code and datasets:
  https://github.com/ArcTrinity9/Datasets-ReaChOcc-and-SynChOcc
- **Journal**: None
- **Summary**: Despite the recent developments in 3D Face Reconstruction from occluded and noisy face images, the performance is still unsatisfactory. Moreover, most existing methods rely on additional dependencies, posing numerous constraints over the training procedure. Therefore, we propose a Self-Supervised RObustifying GUidancE (ROGUE) framework to obtain robustness against occlusions and noise in the face images. The proposed network contains 1) the Guidance Pipeline to obtain the 3D face coefficients for the clean faces and 2) the Robustification Pipeline to acquire the consistency between the estimated coefficients for occluded or noisy images and the clean counterpart. The proposed image- and feature-level loss functions aid the ROGUE learning process without posing additional dependencies. To facilitate model evaluation, we propose two challenging occlusion face datasets, ReaChOcc and SynChOcc, containing real-world and synthetic occlusion-based face images for robustness evaluation. Also, a noisy variant of the test dataset of CelebA is produced for evaluation. Our method outperforms the current state-of-the-art method by large margins (e.g., for the perceptual errors, a reduction of 23.8% for real-world occlusions, 26.4% for synthetic occlusions, and 22.7% for noisy images), demonstrating the effectiveness of the proposed approach. The occlusion datasets and the corresponding evaluation code are released publicly at https://github.com/ArcTrinity9/Datasets-ReaChOcc-and-SynChOcc.



### Deep Graph Clustering via Dual Correlation Reduction
- **Arxiv ID**: http://arxiv.org/abs/2112.14772v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.14772v1)
- **Published**: 2021-12-29 04:05:38+00:00
- **Updated**: 2021-12-29 04:05:38+00:00
- **Authors**: Yue Liu, Wenxuan Tu, Sihang Zhou, Xinwang Liu, Linxuan Song, Xihong Yang, En Zhu
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Deep graph clustering, which aims to reveal the underlying graph structure and divide the nodes into different groups, has attracted intensive attention in recent years. However, we observe that, in the process of node encoding, existing methods suffer from representation collapse which tends to map all data into the same representation. Consequently, the discriminative capability of the node representation is limited, leading to unsatisfied clustering performance. To address this issue, we propose a novel self-supervised deep graph clustering method termed Dual Correlation Reduction Network (DCRN) by reducing information correlation in a dual manner. Specifically, in our method, we first design a siamese network to encode samples. Then by forcing the cross-view sample correlation matrix and cross-view feature correlation matrix to approximate two identity matrices, respectively, we reduce the information correlation in the dual-level, thus improving the discriminative capability of the resulting features. Moreover, in order to alleviate representation collapse caused by over-smoothing in GCN, we introduce a propagation regularization term to enable the network to gain long-distance information with the shallow network structure. Extensive experimental results on six benchmark datasets demonstrate the effectiveness of the proposed DCRN against the existing state-of-the-art methods.



### Overcoming Mode Collapse with Adaptive Multi Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2112.14406v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.14406v1)
- **Published**: 2021-12-29 05:57:55+00:00
- **Updated**: 2021-12-29 05:57:55+00:00
- **Authors**: Karttikeya Mangalam, Rohin Garg
- **Comment**: BMVC 2021 Poster
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are a class of generative models used for various applications, but they have been known to suffer from the mode collapse problem, in which some modes of the target distribution are ignored by the generator. Investigative study using a new data generation procedure indicates that the mode collapse of the generator is driven by the discriminator's inability to maintain classification accuracy on previously seen samples, a phenomenon called Catastrophic Forgetting in continual learning. Motivated by this observation, we introduce a novel training procedure that adaptively spawns additional discriminators to remember previous modes of generation. On several datasets, we show that our training scheme can be plugged-in to existing GAN frameworks to mitigate mode collapse and improve standard metrics for GAN evaluation.



### Invertible Image Dataset Protection
- **Arxiv ID**: http://arxiv.org/abs/2112.14420v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.14420v1)
- **Published**: 2021-12-29 06:56:43+00:00
- **Updated**: 2021-12-29 06:56:43+00:00
- **Authors**: Kejiang Chen, Xianhan Zeng, Qichao Ying, Sheng Li, Zhenxing Qian, Xinpeng Zhang
- **Comment**: Submitted to ICME 2022. Authors are from University of Science and
  Technology of China, Fudan University, China. A potential extended version of
  this work is under way
- **Journal**: None
- **Summary**: Deep learning has achieved enormous success in various industrial applications. Companies do not want their valuable data to be stolen by malicious employees to train pirated models. Nor do they wish the data analyzed by the competitors after using them online. We propose a novel solution for dataset protection in this scenario by robustly and reversibly transform the images into adversarial images. We develop a reversible adversarial example generator (RAEG) that introduces slight changes to the images to fool traditional classification models. Even though malicious attacks train pirated models based on the defensed versions of the protected images, RAEG can significantly weaken the functionality of these models. Meanwhile, the reversibility of RAEG ensures the performance of authorized models. Extensive experiments demonstrate that RAEG can better protect the data with slight distortion against adversarial defense than previous methods.



### A Color Image Steganography Based on Frequency Sub-band Selection
- **Arxiv ID**: http://arxiv.org/abs/2112.14437v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.14437v1)
- **Published**: 2021-12-29 07:52:56+00:00
- **Updated**: 2021-12-29 07:52:56+00:00
- **Authors**: Hai Su, Shan Yang, Shuqing Zhang, Songsen Yu
- **Comment**: 19 pages,17 figures
- **Journal**: None
- **Summary**: Color image steganography based on deep learning is the art of hiding information in the color image. Among them, image hiding steganography(hiding image with image) has attracted much attention in recent years because of its great steganographic capacity. However, images generated by image hiding steganography may show some obvious color distortion or artificial texture traces. We propose a color image steganographic model based on frequency sub-band selection to solve the above problems. Firstly, we discuss the relationship between the characteristics of different color spaces/frequency sub-bands and the generated image quality. Then, we select the B channel of the RGB image as the embedding channel and the high-frequency sub-band as the embedding domain. DWT(discrete wavelet transformation) transforms B channel information and secret gray image into frequency domain information, and then the secret image is embedded and extracted in the frequency domain. Comprehensive experiments demonstrate that images generated by our model have better image quality, and the imperceptibility is significantly increased.



### ACDNet: Adaptively Combined Dilated Convolution for Monocular Panorama Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2112.14440v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2112.14440v2)
- **Published**: 2021-12-29 08:04:19+00:00
- **Updated**: 2022-04-01 04:16:15+00:00
- **Authors**: Chuanqing Zhuang, Zhengda Lu, Yiqun Wang, Jun Xiao, Ying Wang
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Depth estimation is a crucial step for 3D reconstruction with panorama images in recent years. Panorama images maintain the complete spatial information but introduce distortion with equirectangular projection. In this paper, we propose an ACDNet based on the adaptively combined dilated convolution to predict the dense depth map for a monocular panoramic image. Specifically, we combine the convolution kernels with different dilations to extend the receptive field in the equirectangular projection. Meanwhile, we introduce an adaptive channel-wise fusion module to summarize the feature maps and get diverse attention areas in the receptive field along the channels. Due to the utilization of channel-wise attention in constructing the adaptive channel-wise fusion module, the network can capture and leverage the cross-channel contextual information efficiently. Finally, we conduct depth estimation experiments on three datasets (both virtual and real-world) and the experimental results demonstrate that our proposed ACDNet substantially outperforms the current state-of-the-art (SOTA) methods. Our codes and model parameters are accessed in https://github.com/zcq15/ACDNet.



### Semantic Feature Extraction for Generalized Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.14478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.14478v1)
- **Published**: 2021-12-29 09:52:30+00:00
- **Updated**: 2021-12-29 09:52:30+00:00
- **Authors**: Junhan Kim, Kyuhong Shim, Byonghyo Shim
- **Comment**: Accepted at AAAI2022
- **Journal**: None
- **Summary**: Generalized zero-shot learning (GZSL) is a technique to train a deep learning model to identify unseen classes using the attribute. In this paper, we put forth a new GZSL technique that improves the GZSL classification performance greatly. Key idea of the proposed approach, henceforth referred to as semantic feature extraction-based GZSL (SE-GZSL), is to use the semantic feature containing only attribute-related information in learning the relationship between the image and the attribute. In doing so, we can remove the interference, if any, caused by the attribute-irrelevant information contained in the image feature. To train a network extracting the semantic feature, we present two novel loss functions, 1) mutual information-based loss to capture all the attribute-related information in the image feature and 2) similarity-based loss to remove unwanted attribute-irrelevant information. From extensive experiments using various datasets, we show that the proposed SE-GZSL technique outperforms conventional GZSL approaches by a large margin.



### Two-phase training mitigates class imbalance for camera trap image classification with CNNs
- **Arxiv ID**: http://arxiv.org/abs/2112.14491v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.14491v1)
- **Published**: 2021-12-29 10:47:45+00:00
- **Updated**: 2021-12-29 10:47:45+00:00
- **Authors**: Farjad Malik, Simon Wouters, Ruben Cartuyvels, Erfan Ghadery, Marie-Francine Moens
- **Comment**: None
- **Journal**: None
- **Summary**: By leveraging deep learning to automatically classify camera trap images, ecologists can monitor biodiversity conservation efforts and the effects of climate change on ecosystems more efficiently. Due to the imbalanced class-distribution of camera trap datasets, current models are biased towards the majority classes. As a result, they obtain good performance for a few majority classes but poor performance for many minority classes. We used two-phase training to increase the performance for these minority classes. We trained, next to a baseline model, four models that implemented a different versions of two-phase training on a subset of the highly imbalanced Snapshot Serengeti dataset. Our results suggest that two-phase training can improve performance for many minority classes, with limited loss in performance for the other classes. We find that two-phase training based on majority undersampling increases class-specific F1-scores up to 3.0%. We also find that two-phase training outperforms using only oversampling or undersampling by 6.1% in F1-score on average. Finally, we find that a combination of over- and undersampling leads to a better performance than using them individually.



### Spatial Distribution Patterns of Clownfish in Recirculating Aquaculture Systems
- **Arxiv ID**: http://arxiv.org/abs/2112.14513v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2112.14513v3)
- **Published**: 2021-12-29 11:39:56+00:00
- **Updated**: 2022-12-19 10:40:01+00:00
- **Authors**: Fahad Aljehani, Ibrahima N'Doye, Micaela S. Justo, John E. Majoris, Michael L. Berumen, Taous-Meriem Laleg-Kirati
- **Comment**: 14 pages, 15 figures
- **Journal**: None
- **Summary**: Successful aquaculture systems can reduce the pressure and help secure the most diverse and productive Red Sea coral reef ecosystem to maintain a healthy and functional ecosystem within a sustainable blue economy. Interestingly, recirculating aquaculture systems are currently emerging in fish farm production practices. On the other hand, monitoring and detecting fish behaviors provide essential information on fish welfare and contribute to an intelligent production in global aquaculture. This work proposes an efficient approach to analyze the spatial distribution status and motion patterns of juvenile clownfish \textit{(Amphiprion bicinctus)} maintained in aquaria at three stocking densities (1, 5, and 10 individuals/aquarium). The estimated displacement is crucial in assessing the dispersion and velocity to express the clownfish's spatial distribution and movement behavior in a recirculating aquaculture system. Indeed, we aim to compute the velocity, magnitude, and turning angle using an optical flow method to assist aquaculturists in efficiently monitoring and identifying fish behavior. We test the system design on a database containing two days of video streams of juvenile clownfish maintained in aquaria. The proposed displacement estimation reveals good performance in measuring clownfish's motion and dispersion characteristics leading to assessing the potential signs of stress behaviors. We demonstrate the effectiveness of the proposed technique for quantifying variation in clownfish activity levels between recordings taken in the morning and afternoon at different stocking densities. It provides practical baseline support for online predicting and monitoring feeding behavior in ornamental fish aquaculture.



### Res2NetFuse: A Fusion Method for Infrared and Visible Images
- **Arxiv ID**: http://arxiv.org/abs/2112.14540v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2112.14540v3)
- **Published**: 2021-12-29 13:34:48+00:00
- **Updated**: 2023-08-27 04:14:25+00:00
- **Authors**: Xu Song, Xiao-Jun Wu, Hui Li, Jun Sun, Vasile Palade
- **Comment**: There are some errors that need to be corrected
- **Journal**: None
- **Summary**: This paper presents a novel Res2Net-based fusion framework for infrared and visible images. The proposed fusion model has three parts: an encoder, a fusion layer and a decoder, respectively. The Res2Net-based encoder is used to extract multi-scale features of source images, the paper introducing a new training strategy for training a Res2Net-based encoder that uses only a single image. Then, a new fusion strategy is developed based on the attention model. Finally, the fused image is reconstructed by the decoder. The proposed approach is also analyzed in detail. Experiments show that our method achieves state-of-the-art fusion performance in objective and subjective assessment by comparing with the existing methods.



### Onsite Non-Line-of-Sight Imaging via Online Calibrations
- **Arxiv ID**: http://arxiv.org/abs/2112.14555v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.14555v1)
- **Published**: 2021-12-29 13:48:32+00:00
- **Updated**: 2021-12-29 13:48:32+00:00
- **Authors**: Zhengqing Pan, Ruiqian Li, Tian Gao, Zi Wang, Ping Liu, Siyuan Shen, Tao Wu, Jingyi Yu, Shiying Li
- **Comment**: None
- **Journal**: None
- **Summary**: There has been an increasing interest in deploying non-line-of-sight (NLOS) imaging systems for recovering objects behind an obstacle. Existing solutions generally pre-calibrate the system before scanning the hidden objects. Onsite adjustments of the occluder, object and scanning pattern require re-calibration. We present an online calibration technique that directly decouples the acquired transients at onsite scanning into the LOS and hidden components. We use the former to directly (re)calibrate the system upon changes of scene/obstacle configurations, scanning regions, and scanning patterns whereas the latter for hidden object recovery via spatial, frequency or learning based techniques. Our technique avoids using auxiliary calibration apparatus such as mirrors or checkerboards and supports both laboratory validations and real-world deployments.



### HPRN: Holistic Prior-embedded Relation Network for Spectral Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2112.14608v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.14608v2)
- **Published**: 2021-12-29 15:43:20+00:00
- **Updated**: 2022-02-08 08:13:56+00:00
- **Authors**: Chaoxiong Wu, Jiaojiao Li, Rui Song, Yunsong Li, Qian Du
- **Comment**: None
- **Journal**: None
- **Summary**: Spectral super-resolution (SSR) refers to the hyperspectral image (HSI) recovery from an RGB counterpart. Due to the one-to-many nature of the SSR problem, a single RGB image can be reprojected to many HSIs. The key to tackle this ill-posed problem is to plug into multi-source prior information such as the natural spatial context-prior of RGB images, deep feature-prior or inherent statistical-prior of HSIs, etc., so as to effectively alleviate the degree of ill-posedness. However, most current approaches only consider the general and limited priors in their customized convolutional neural networks (CNNs), which leads to the inability to guarantee the confidence and fidelity of reconstructed spectra. In this paper, we propose a novel holistic prior-embedded relation network (HPRN) to integrate comprehensive priors to regularize and optimize the solution space of SSR. Basically, the core framework is delicately assembled by several multi-residual relation blocks (MRBs) that fully facilitate the transmission and utilization of the low-frequency content prior of RGBs. Innovatively, the semantic prior of RGB inputs is introduced to mark category attributes, and a semantic-driven spatial relation module (SSRM) is invented to perform the feature aggregation of clustered similar range for refining recovered characteristics. Additionally, we develop a transformer-based channel relation module (TCRM), which breaks the habit of employing scalars as the descriptors of channel-wise relations in the previous deep feature-prior, and replaces them with certain vectors to make the mapping function more robust and smoother. In order to maintain the mathematical correlation and spectral consistency between hyperspectral bands, the second-order prior constraints (SOPC) are incorporated into the loss function to guide the HSI reconstruction.



### Implementation of Convolutional Neural Network Architecture on 3D Multiparametric Magnetic Resonance Imaging for Prostate Cancer Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2112.14644v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.14644v1)
- **Published**: 2021-12-29 16:47:52+00:00
- **Updated**: 2021-12-29 16:47:52+00:00
- **Authors**: Ping-Chang Lin, Teodora Szasz, Hakizumwami B. Runesha
- **Comment**: None
- **Journal**: None
- **Summary**: Prostate cancer is one of the most common causes of cancer deaths in men. There is a growing demand for noninvasively and accurately diagnostic methods that facilitate the current standard prostate cancer risk assessment in clinical practice. Still, developing computer-aided classification tools in prostate cancer diagnostics from multiparametric magnetic resonance images continues to be a challenge. In this work, we propose a novel deep learning approach for automatic classification of prostate lesions in the corresponding magnetic resonance images by constructing a two-stage multimodal multi-stream convolutional neural network (CNN)-based architecture framework. Without implementing sophisticated image preprocessing steps or third-party software, our framework achieved the classification performance with the area under a Receiver Operating Characteristic (ROC) curve value of 0.87. The result outperformed most of the submitted methods and shared the highest value reported by the PROSTATEx Challenge organizer. Our proposed CNN-based framework reflects the potential of assisting medical image interpretation in prostate cancer and reducing unnecessary biopsies.



### On the Instability of Relative Pose Estimation and RANSAC's Role
- **Arxiv ID**: http://arxiv.org/abs/2112.14651v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2112.14651v1)
- **Published**: 2021-12-29 16:56:06+00:00
- **Updated**: 2021-12-29 16:56:06+00:00
- **Authors**: Hongyi Fan, Joe Kileel, Benjamin Kimia
- **Comment**: 27 pages, 11 figures, 2 tables
- **Journal**: None
- **Summary**: In this paper we study the numerical instabilities of the 5- and 7-point problems for essential and fundamental matrix estimation in multiview geometry. In both cases we characterize the ill-posed world scenes where the condition number for epipolar estimation is infinite. We also characterize the ill-posed instances in terms of the given image data. To arrive at these results, we present a general framework for analyzing the conditioning of minimal problems in multiview geometry, based on Riemannian manifolds. Experiments with synthetic and real-world data then reveal a striking conclusion: that Random Sample Consensus (RANSAC) in Structure-from-Motion (SfM) does not only serve to filter out outliers, but RANSAC also selects for well-conditioned image data, sufficiently separated from the ill-posed locus that our theory predicts. Our findings suggest that, in future work, one could try to accelerate and increase the success of RANSAC by testing only well-conditioned image data.



### Gendered Differences in Face Recognition Accuracy Explained by Hairstyles, Makeup, and Facial Morphology
- **Arxiv ID**: http://arxiv.org/abs/2112.14656v1
- **DOI**: 10.1109/TIFS.2021.3135750
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.14656v1)
- **Published**: 2021-12-29 17:07:33+00:00
- **Updated**: 2021-12-29 17:07:33+00:00
- **Authors**: Vítor Albiero, Kai Zhang, Michael C. King, Kevin W. Bowyer
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2008.06989
- **Journal**: None
- **Summary**: Media reports have accused face recognition of being ''biased'', ''sexist'' and ''racist''. There is consensus in the research literature that face recognition accuracy is lower for females, who often have both a higher false match rate and a higher false non-match rate. However, there is little published research aimed at identifying the cause of lower accuracy for females. For instance, the 2019 Face Recognition Vendor Test that documents lower female accuracy across a broad range of algorithms and datasets also lists ''Analyze cause and effect'' under the heading ''What we did not do''. We present the first experimental analysis to identify major causes of lower face recognition accuracy for females on datasets where previous research has observed this result. Controlling for equal amount of visible face in the test images mitigates the apparent higher false non-match rate for females. Additional analysis shows that makeup-balanced datasets further improves females to achieve lower false non-match rates. Finally, a clustering experiment suggests that images of two different females are inherently more similar than of two different males, potentially accounting for a difference in false match rates.



### MetaGraspNet_v0: A Large-Scale Benchmark Dataset for Vision-driven Robotic Grasping via Physics-based Metaverse Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2112.14663v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.14663v3)
- **Published**: 2021-12-29 17:23:24+00:00
- **Updated**: 2022-08-30 17:53:40+00:00
- **Authors**: Yuhao Chen, E. Zhixuan Zeng, Maximilian Gilles, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: There has been increasing interest in smart factories powered by robotics systems to tackle repetitive, laborious tasks. One impactful yet challenging task in robotics-powered smart factory applications is robotic grasping: using robotic arms to grasp objects autonomously in different settings. Robotic grasping requires a variety of computer vision tasks such as object detection, segmentation, grasp prediction, pick planning, etc. While significant progress has been made in leveraging of machine learning for robotic grasping, particularly with deep learning, a big challenge remains in the need for large-scale, high-quality RGBD datasets that cover a wide diversity of scenarios and permutations. To tackle this big, diverse data problem, we are inspired by the recent rise in the concept of metaverse, which has greatly closed the gap between virtual worlds and the physical world. Metaverses allow us to create digital twins of real-world manufacturing scenarios and to virtually create different scenarios from which large volumes of data can be generated for training models. In this paper, we present MetaGraspNet: a large-scale benchmark dataset for vision-driven robotic grasping via physics-based metaverse synthesis. The proposed dataset contains 100,000 images and 25 different object types and is split into 5 difficulties to evaluate object detection and segmentation model performance in different grasping scenarios. We also propose a new layout-weighted performance metric alongside the dataset for evaluating object detection and segmentation performance in a manner that is more appropriate for robotic grasp applications compared to existing general-purpose performance metrics. Our benchmark dataset is available open-source on Kaggle, with the first phase consisting of detailed object detection, segmentation, layout annotations, and a layout-weighted performance metric script.



### StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2
- **Arxiv ID**: http://arxiv.org/abs/2112.14683v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.14683v4)
- **Published**: 2021-12-29 17:58:29+00:00
- **Updated**: 2022-05-31 20:39:09+00:00
- **Authors**: Ivan Skorokhodov, Sergey Tulyakov, Mohamed Elhoseiny
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Videos show continuous events, yet most $-$ if not all $-$ video synthesis frameworks treat them discretely in time. In this work, we think of videos of what they should be $-$ time-continuous signals, and extend the paradigm of neural representations to build a continuous-time video generator. For this, we first design continuous motion representations through the lens of positional embeddings. Then, we explore the question of training on very sparse videos and demonstrate that a good generator can be learned by using as few as 2 frames per clip. After that, we rethink the traditional image + video discriminators pair and design a holistic discriminator that aggregates temporal information by simply concatenating frames' features. This decreases the training cost and provides richer learning signal to the generator, making it possible to train directly on 1024$^2$ videos for the first time. We build our model on top of StyleGAN2 and it is just ${\approx}5\%$ more expensive to train at the same resolution while achieving almost the same image quality. Moreover, our latent space features similar properties, enabling spatial manipulations that our method can propagate in time. We can generate arbitrarily long videos at arbitrary high frame rate, while prior work struggles to generate even 64 frames at a fixed rate. Our model is tested on four modern 256$^2$ and one 1024$^2$-resolution video synthesis benchmarks. In terms of sheer metrics, it performs on average ${\approx}30\%$ better than the closest runner-up. Project website: https://universome.github.io.



### Disentanglement and Generalization Under Correlation Shifts
- **Arxiv ID**: http://arxiv.org/abs/2112.14754v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2112.14754v2)
- **Published**: 2021-12-29 18:55:17+00:00
- **Updated**: 2022-12-23 16:53:06+00:00
- **Authors**: Christina M. Funke, Paul Vicol, Kuan-Chieh Wang, Matthias Kümmerer, Richard Zemel, Matthias Bethge
- **Comment**: CoLLAs 2022
- **Journal**: None
- **Summary**: Correlations between factors of variation are prevalent in real-world data. Exploiting such correlations may increase predictive performance on noisy data; however, often correlations are not robust (e.g., they may change between domains, datasets, or applications) and models that exploit them do not generalize when correlations shift. Disentanglement methods aim to learn representations which capture different factors of variation in latent subspaces. A common approach involves minimizing the mutual information between latent subspaces, such that each encodes a single underlying attribute. However, this fails when attributes are correlated. We solve this problem by enforcing independence between subspaces conditioned on the available attributes, which allows us to remove only dependencies that are not due to the correlation structure present in the training data. We achieve this via an adversarial approach to minimize the conditional mutual information (CMI) between subspaces with respect to categorical variables. We first show theoretically that CMI minimization is a good objective for robust disentanglement on linear problems. We then apply our method on real-world datasets based on MNIST and CelebA, and show that it yields models that are disentangled and robust under correlation shift, including in weakly supervised settings.



### A Simple Baseline for Open-Vocabulary Semantic Segmentation with Pre-trained Vision-language Model
- **Arxiv ID**: http://arxiv.org/abs/2112.14757v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.14757v2)
- **Published**: 2021-12-29 18:56:18+00:00
- **Updated**: 2022-12-29 16:36:55+00:00
- **Authors**: Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue Cao, Han Hu, Xiang Bai
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, open-vocabulary image classification by vision language pre-training has demonstrated incredible achievements, that the model can classify arbitrary categories without seeing additional annotated images of that category. However, it is still unclear how to make the open-vocabulary recognition work well on broader vision problems. This paper targets open-vocabulary semantic segmentation by building it on an off-the-shelf pre-trained vision-language model, i.e., CLIP. However, semantic segmentation and the CLIP model perform on different visual granularity, that semantic segmentation processes on pixels while CLIP performs on images. To remedy the discrepancy in processing granularity, we refuse the use of the prevalent one-stage FCN based framework, and advocate a two-stage semantic segmentation framework, with the first stage extracting generalizable mask proposals and the second stage leveraging an image based CLIP model to perform open-vocabulary classification on the masked image crops which are generated in the first stage. Our experimental results show that this two-stage framework can achieve superior performance than FCN when trained only on COCO Stuff dataset and evaluated on other datasets without fine-tuning. Moreover, this simple framework also surpasses previous state-of-the-arts of zero-shot semantic segmentation by a large margin: +29.5 hIoU on the Pascal VOC 2012 dataset, and +8.9 hIoU on the COCO Stuff dataset. With its simplicity and strong performance, we hope this framework to serve as a baseline to facilitate future research. The code are made publicly available at~\url{https://github.com/MendelXu/zsseg.baseline}.



### Deep Learning meets Liveness Detection: Recent Advancements and Challenges
- **Arxiv ID**: http://arxiv.org/abs/2112.14796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.14796v1)
- **Published**: 2021-12-29 19:24:58+00:00
- **Updated**: 2021-12-29 19:24:58+00:00
- **Authors**: Arian Sabaghi, Marzieh Oghbaie, Kooshan Hashemifard, Mohammad Akbari
- **Comment**: None
- **Journal**: None
- **Summary**: Facial biometrics has been recently received tremendous attention as a convenient replacement for traditional authentication systems. Consequently, detecting malicious attempts has found great significance, leading to extensive studies in face anti-spoofing~(FAS),i.e., face presentation attack detection. Deep feature learning and techniques, as opposed to hand-crafted features, have promised a dramatic increase in the FAS systems' accuracy, tackling the key challenges of materializing the real-world application of such systems. Hence, a new research area dealing with the development of more generalized as well as accurate models is increasingly attracting the attention of the research community and industry. In this paper, we present a comprehensive survey on the literature related to deep-feature-based FAS methods since 2017. To shed light on this topic, a semantic taxonomy based on various features and learning methodologies is represented. Further, we cover predominant public datasets for FAS in chronological order, their evolutional progress, and the evaluation criteria (both intra-dataset and inter-dataset). Finally, we discuss the open research challenges and future directions.



### Learning Spatially-Adaptive Squeeze-Excitation Networks for Image Synthesis and Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.14804v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.14804v3)
- **Published**: 2021-12-29 19:38:30+00:00
- **Updated**: 2022-10-03 14:54:59+00:00
- **Authors**: Jianghao Shen, Tianfu Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Learning light-weight yet expressive deep networks in both image synthesis and image recognition remains a challenging problem. Inspired by a more recent observation that it is the data-specificity that makes the multi-head self-attention (MHSA) in the Transformer model so powerful, this paper proposes to extend the widely adopted light-weight Squeeze-Excitation (SE) module to be spatially-adaptive to reinforce its data specificity, as a convolutional alternative of the MHSA, while retaining the efficiency of SE and the inductive basis of convolution. It presents two designs of spatially-adaptive squeeze-excitation (SASE) modules for image synthesis and image recognition respectively. For image synthesis tasks, the proposed SASE is tested in both low-shot and one-shot learning tasks. It shows better performance than prior arts. For image recognition tasks, the proposed SASE is used as a drop-in replacement for convolution layers in ResNets and achieves much better accuracy than the vanilla ResNets, and slightly better than the MHSA counterparts such as the Swin-Transformer and Pyramid-Transformer in the ImageNet-1000 dataset, with significantly smaller models.



