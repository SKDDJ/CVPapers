# Arxiv Papers in cs.CV on 2021-12-18
### A Streaming Volumetric Image Generation Framework for Development and Evaluation of Out-of-Core Methods
- **Arxiv ID**: http://arxiv.org/abs/2112.09809v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.09809v2)
- **Published**: 2021-12-18 00:00:48+00:00
- **Updated**: 2022-05-04 13:59:58+00:00
- **Authors**: Dominik Drees, Xiaoyi Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Advances in 3D imaging technology in recent years have allowed for increasingly high resolution volumetric images of large specimen. The resulting datasets of hundreds of Gigabytes in size call for new scalable and memory efficient approaches in the field of image processing, where some progress has been made already. At the same time, quantitative evaluation of these new methods is difficult both in terms of the availability of specific data sizes and in the generation of associated ground truth data. In this paper we present an algorithmic framework that can be used to efficiently generate test (and ground truth) volume data, optionally even in a streaming fashion. As the proposed nested sweeps algorithm is fast, it can be used to generate test data on demand. We analyze the asymptotic run time of the presented algorithm and compare it experimentally to alternative approaches as well as a hypothetical best-case baseline method. In a case study, the framework is applied to the popular VascuSynth software for vascular image generation, making it capable of efficiently producing larger-than-main memory volumes which is demonstrated by generating a trillion voxel (1TB) image. Implementations of the presented framework are available online in the form of the modified version of Vascusynth and the code used for the experimental evaluation. In addition, the test data generation procedure has been integrated into the popular volume rendering and processing framework Voreen.



### Exploiting Long-Term Dependencies for Generating Dynamic Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/2112.09828v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.09828v2)
- **Published**: 2021-12-18 03:02:11+00:00
- **Updated**: 2022-10-19 16:58:46+00:00
- **Authors**: Shengyu Feng, Subarna Tripathi, Hesham Mostafa, Marcel Nassar, Somdeb Majumdar
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: Dynamic scene graph generation from a video is challenging due to the temporal dynamics of the scene and the inherent temporal fluctuations of predictions. We hypothesize that capturing long-term temporal dependencies is the key to effective generation of dynamic scene graphs. We propose to learn the long-term dependencies in a video by capturing the object-level consistency and inter-object relationship dynamics over object-level long-term tracklets using transformers. Experimental results demonstrate that our Dynamic Scene Graph Detection Transformer (DSG-DETR) outperforms state-of-the-art methods by a significant margin on the benchmark dataset Action Genome. Our ablation studies validate the effectiveness of each component of the proposed approach. The source code is available at https://github.com/Shengyu-Feng/DSG-DETR.



### Face Deblurring Based on Separable Normalization and Adaptive Denormalization
- **Arxiv ID**: http://arxiv.org/abs/2112.09833v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.09833v1)
- **Published**: 2021-12-18 03:42:23+00:00
- **Updated**: 2021-12-18 03:42:23+00:00
- **Authors**: Xian Zhang, Hao Zhang, Jiancheng Lv, Xiaojie Li
- **Comment**: None
- **Journal**: None
- **Summary**: Face deblurring aims to restore a clear face image from a blurred input image with more explicit structure and facial details. However, most conventional image and face deblurring methods focus on the whole generated image resolution without consideration of special face part texture and generally produce unsufficient details. Considering that faces and backgrounds have different distribution information, in this study, we designed an effective face deblurring network based on separable normalization and adaptive denormalization (SNADNet). First, We fine-tuned the face parsing network to obtain an accurate face structure. Then, we divided the face parsing feature into face foreground and background. Moreover, we constructed a new feature adaptive denormalization to regularize fafcial structures as a condition of the auxiliary to generate more harmonious and undistorted face structure. In addition, we proposed a texture extractor and multi-patch discriminator to enhance the generated facial texture information. Experimental results on both CelebA and CelebA-HQ datasets demonstrate that the proposed face deblurring network restores face structure with more facial details and performs favorably against state-of-the-art methods in terms of structured similarity indexing method (SSIM), peak signal-to-noise ratio (PSNR), Frechet inception distance (FID) and L1, and qualitative comparisons.



### Calorie Aware Automatic Meal Kit Generation from an Image
- **Arxiv ID**: http://arxiv.org/abs/2112.09839v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.09839v1)
- **Published**: 2021-12-18 04:16:12+00:00
- **Updated**: 2021-12-18 04:16:12+00:00
- **Authors**: Ahmad Babaeian Jelodar, Yu Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Calorie and nutrition research has attained increased interest in recent years. But, due to the complexity of the problem, literature in this area focuses on a limited subset of ingredients or dish types and simple convolutional neural networks or traditional machine learning. Simultaneously, estimation of ingredient portions can help improve calorie estimation and meal re-production from a given image. In this paper, given a single cooking image, a pipeline for calorie estimation and meal re-production for different servings of the meal is proposed. The pipeline contains two stages. In the first stage, a set of ingredients associated with the meal in the given image are predicted. In the second stage, given image features and ingredients, portions of the ingredients and finally the total meal calorie are simultaneously estimated using a deep transformer-based model. Portion estimation introduced in the model helps improve calorie estimation and is also beneficial for meal re-production in different serving sizes. To demonstrate the benefits of the pipeline, the model can be used for meal kits generation. To evaluate the pipeline, the large scale dataset Recipe1M is used. Prior to experiments, the Recipe1M dataset is parsed and explicitly annotated with portions of ingredients. Experiments show that using ingredients and their portions significantly improves calorie estimation. Also, a visual interface is created in which a user can interact with the pipeline to reach accurate calorie estimations and generate a meal kit for cooking purposes.



### Enhanced Object Detection in Floor-plan through Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2112.09844v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T45, I.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2112.09844v1)
- **Published**: 2021-12-18 05:06:22+00:00
- **Updated**: 2021-12-18 05:06:22+00:00
- **Authors**: Dev Khare, N S Kamal, Barathi Ganesh HB, V Sowmya, V V Sajith Variyar
- **Comment**: 3rd International Conference on Machine Learning, Image Processing,
  Network Security and Data Sciences
- **Journal**: None
- **Summary**: Building Information Modelling (BIM) software use scalable vector formats to enable flexible designing of floor plans in the industry. Floor plans in the architectural domain can come from many sources that may or may not be in scalable vector format. The conversion of floor plan images to fully annotated vector images is a process that can now be realized by computer vision. Novel datasets in this field have been used to train Convolutional Neural Network (CNN) architectures for object detection. Image enhancement through Super-Resolution (SR) is also an established CNN based network in computer vision that is used for converting low resolution images to high resolution ones. This work focuses on creating a multi-component module that stacks a SR model on a floor plan object detection model. The proposed stacked model shows greater performance than the corresponding vanilla object detection model. For the best case, the the inclusion of SR showed an improvement of 39.47% in object detection over the vanilla network. Data and code are made publicly available at https://github.com/rbg-research/Floor-Plan-Detection.



### LegoDNN: Block-grained Scaling of Deep Neural Networks for Mobile Vision
- **Arxiv ID**: http://arxiv.org/abs/2112.09852v1
- **DOI**: 10.1145/3447993.3483249
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.09852v1)
- **Published**: 2021-12-18 06:04:03+00:00
- **Updated**: 2021-12-18 06:04:03+00:00
- **Authors**: Rui Han, Qinglong Zhang, Chi Harold Liu, Guoren Wang, Jian Tang, Lydia Y. Chen
- **Comment**: 13 pages, 15 figures
- **Journal**: In MobiCom'21, pages 406-419, 2021. ACM
- **Summary**: Deep neural networks (DNNs) have become ubiquitous techniques in mobile and embedded systems for applications such as image/object recognition and classification. The trend of executing multiple DNNs simultaneously exacerbate the existing limitations of meeting stringent latency/accuracy requirements on resource constrained mobile devices. The prior art sheds light on exploring the accuracy-resource tradeoff by scaling the model sizes in accordance to resource dynamics. However, such model scaling approaches face to imminent challenges: (i) large space exploration of model sizes, and (ii) prohibitively long training time for different model combinations. In this paper, we present LegoDNN, a lightweight, block-grained scaling solution for running multi-DNN workloads in mobile vision systems. LegoDNN guarantees short model training times by only extracting and training a small number of common blocks (e.g. 5 in VGG and 8 in ResNet) in a DNN. At run-time, LegoDNN optimally combines the descendant models of these blocks to maximize accuracy under specific resources and latency constraints, while reducing switching overhead via smart block-level scaling of the DNN. We implement LegoDNN in TensorFlow Lite and extensively evaluate it against state-of-the-art techniques (FLOP scaling, knowledge distillation and model compression) using a set of 12 popular DNN models. Evaluation results show that LegoDNN provides 1,296x to 279,936x more options in model sizes without increasing training time, thus achieving as much as 31.74% improvement in inference accuracy and 71.07% reduction in scaling energy consumptions.



### Space Non-cooperative Object Active Tracking with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.09854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.09854v1)
- **Published**: 2021-12-18 06:12:24+00:00
- **Updated**: 2021-12-18 06:12:24+00:00
- **Authors**: Dong Zhou, Guanghui Sun, Wenxiao Lei
- **Comment**: None
- **Journal**: None
- **Summary**: Active visual tracking of space non-cooperative object is significant for future intelligent spacecraft to realise space debris removal, asteroid exploration, autonomous rendezvous and docking. However, existing works often consider this task into different subproblems (e.g. image preprocessing, feature extraction and matching, position and pose estimation, control law design) and optimize each module alone, which are trivial and sub-optimal. To this end, we propose an end-to-end active visual tracking method based on DQN algorithm, named as DRLAVT. It can guide the chasing spacecraft approach to arbitrary space non-cooperative target merely relied on color or RGBD images, which significantly outperforms position-based visual servoing baseline algorithm that adopts state-of-the-art 2D monocular tracker, SiamRPN. Extensive experiments implemented with diverse network architectures, different perturbations and multiple targets demonstrate the advancement and robustness of DRLAVT. In addition, We further prove our method indeed learnt the motion patterns of target with deep reinforcement learning through hundreds of trial-and-errors.



### An effective coaxiality measurement for twist drill based on line structured light sensor
- **Arxiv ID**: http://arxiv.org/abs/2112.09873v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.09873v3)
- **Published**: 2021-12-18 08:10:54+00:00
- **Updated**: 2022-04-08 01:10:45+00:00
- **Authors**: Ailing Cheng, Jiaojiao Ye, Fei Yang, Shufang Lu, Fei Gao
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible.13 pages, 22 figures
- **Journal**: None
- **Summary**: Aiming at the accurate and effective coaxiality measurement for twist drill with irregular surface, an optical measurement mechanism is proposed in this paper. First, A high-precision rotation instrument based on four core units is designed, which can obtain the 3-D point cloud data of full angle for the twist drill. Second, in the data processing stage, an improved robust Gaussian mixture model is established for accurate and rapid blade back segmentation. To improve measurement efficiency, a rapid reconstruction method of the twist drill axis based on orthogonal synthesis is provided to locate the axial position of the maximum deviation from the benchmark by utilizing the extracted blade back data. Finally, by calculating the maximum radial Euclidean distance from the benchmark, the coaxiality error of the twist drill is obtained. Comparing with other measurement methods, experimental results show that our proposed method is effective with high precision of 3 um and high efficiency of less than 3 s/pc. The result demonstrate that the proposed method is effective, robust and automatic, it can be applied in many actual industrial scene.



### Adversarial Memory Networks for Action Prediction
- **Arxiv ID**: http://arxiv.org/abs/2112.09875v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.09875v1)
- **Published**: 2021-12-18 08:16:21+00:00
- **Updated**: 2021-12-18 08:16:21+00:00
- **Authors**: Zhiqiang Tao, Yue Bai, Handong Zhao, Sheng Li, Yu Kong, Yun Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Action prediction aims to infer the forthcoming human action with partially-observed videos, which is a challenging task due to the limited information underlying early observations. Existing methods mainly adopt a reconstruction strategy to handle this task, expecting to learn a single mapping function from partial observations to full videos to facilitate the prediction process. In this study, we propose adversarial memory networks (AMemNet) to generate the "full video" feature conditioning on a partial video query from two new aspects. Firstly, a key-value structured memory generator is designed to memorize different partial videos as key memories and dynamically write full videos in value memories with gating mechanism and querying attention. Secondly, we develop a class-aware discriminator to guide the memory generator to deliver not only realistic but also discriminative full video features upon adversarial training. The final prediction result of AMemNet is given by late fusion over RGB and optical flow streams. Extensive experimental results on two benchmark video datasets, UCF-101 and HMDB51, are provided to demonstrate the effectiveness of the proposed AMemNet model over state-of-the-art methods.



### Does Explainable Machine Learning Uncover the Black Box in Vision Applications?
- **Arxiv ID**: http://arxiv.org/abs/2112.09898v1
- **DOI**: 10.1016/j.imavis.2021.104353
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.09898v1)
- **Published**: 2021-12-18 10:37:52+00:00
- **Updated**: 2021-12-18 10:37:52+00:00
- **Authors**: Manish Narwaria
- **Comment**: Image and Vision Computing, Volume 118, 2022, 104353, ISSN 0262-8856,
  https://doi.org/10.1016/j.imavis.2021.104353
- **Journal**: Image and Vision Computing, Volume 118, 2022, 104353, ISSN
  0262-8856
- **Summary**: Machine learning (ML) in general and deep learning (DL) in particular has become an extremely popular tool in several vision applications (like object detection, super resolution, segmentation, object tracking etc.). Almost in parallel, the issue of explainability in ML (i.e. the ability to explain/elaborate the way a trained ML model arrived at its decision) in vision has also received fairly significant attention from various quarters. However, we argue that the current philosophy behind explainable ML suffers from certain limitations, and the resulting explanations may not meaningfully uncover black box ML models. To elaborate our assertion, we first raise a few fundamental questions which have not been adequately discussed in the corresponding literature. We also provide perspectives on how explainablity in ML can benefit by relying on more rigorous principles in the related areas.



### 3D Instance Segmentation of MVS Buildings
- **Arxiv ID**: http://arxiv.org/abs/2112.09902v2
- **DOI**: 10.1109/TGRS.2022.3183567
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.09902v2)
- **Published**: 2021-12-18 11:12:38+00:00
- **Updated**: 2022-06-22 07:49:49+00:00
- **Authors**: Jiazhou Chen, Yanghui Xu, Shufang Lu, Ronghua Liang, Liangliang Nan
- **Comment**: 14 figures, 12 figures
- **Journal**: None
- **Summary**: We present a novel 3D instance segmentation framework for Multi-View Stereo (MVS) buildings in urban scenes. Unlike existing works focusing on semantic segmentation of urban scenes, the emphasis of this work lies in detecting and segmenting 3D building instances even if they are attached and embedded in a large and imprecise 3D surface model. Multi-view RGB images are first enhanced to RGBH images by adding a heightmap and are segmented to obtain all roof instances using a fine-tuned 2D instance segmentation neural network. Instance masks from different multi-view images are then clustered into global masks. Our mask clustering accounts for spatial occlusion and overlapping, which can eliminate segmentation ambiguities among multi-view images. Based on these global masks, 3D roof instances are segmented out by mask back-projections and extended to the entire building instances through a Markov random field optimization. A new dataset that contains instance-level annotation for both 3D urban scenes (roofs and buildings) and drone images (roofs) is provided. To the best of our knowledge, it is the first outdoor dataset dedicated to 3D instance segmentation with much more annotations of attached 3D buildings than existing datasets. Quantitative evaluations and ablation studies have shown the effectiveness of all major steps and the advantages of our multi-view framework over the orthophoto-based method.



### Anomaly Discovery in Semantic Segmentation via Distillation Comparison Networks
- **Arxiv ID**: http://arxiv.org/abs/2112.09908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.09908v1)
- **Published**: 2021-12-18 11:32:47+00:00
- **Updated**: 2021-12-18 11:32:47+00:00
- **Authors**: Huan Zhou, Shi Gong, Yu Zhou, Zengqiang Zheng, Ronghua Liu, Xiang Bai
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: This paper aims to address the problem of anomaly discovery in semantic segmentation. Our key observation is that semantic classification plays a critical role in existing approaches, while the incorrectly classified pixels are easily regarded as anomalies. Such a phenomenon frequently appears and is rarely discussed, which significantly reduces the performance of anomaly discovery. To this end, we propose a novel Distillation Comparison Network (DiCNet). It comprises of a teacher branch which is a semantic segmentation network that removed the semantic classification head, and a student branch that is distilled from the teacher branch through a distribution distillation. We show that the distillation guarantees the semantic features of the two branches hold consistency in the known classes, while reflect inconsistency in the unknown class. Therefore, we leverage the semantic feature discrepancy between the two branches to discover the anomalies. DiCNet abandons the semantic classification head in the inference process, and hence significantly alleviates the issue caused by incorrect semantic classification. Extensive experimental results on StreetHazards dataset and BDD-Anomaly dataset are conducted to verify the superior performance of DiCNet. In particular, DiCNet obtains a 6.3% improvement in AUPR and a 5.2% improvement in FPR95 on StreetHazards dataset, achieves a 4.2% improvement in AUPR and a 6.8% improvement in FPR95 on BDD-Anomaly dataset. Codes are available at https://github.com/zhouhuan-hust/DiCNet.



### Fast and Robust Registration of Partially Overlapping Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2112.09922v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.09922v1)
- **Published**: 2021-12-18 12:39:05+00:00
- **Updated**: 2021-12-18 12:39:05+00:00
- **Authors**: Eduardo Arnold, Sajjad Mozaffari, Mehrdad Dianati
- **Comment**: Accepted at IEEE Robotics and Automation Letters (RA-L). 8 pages, 6
  figures, 3 tables
- **Journal**: None
- **Summary**: Real-time registration of partially overlapping point clouds has emerging applications in cooperative perception for autonomous vehicles and multi-agent SLAM. The relative translation between point clouds in these applications is higher than in traditional SLAM and odometry applications, which challenges the identification of correspondences and a successful registration. In this paper, we propose a novel registration method for partially overlapping point clouds where correspondences are learned using an efficient point-wise feature encoder, and refined using a graph-based attention network. This attention network exploits geometrical relationships between key points to improve the matching in point clouds with low overlap. At inference time, the relative pose transformation is obtained by robustly fitting the correspondences through sample consensus. The evaluation is performed on the KITTI dataset and a novel synthetic dataset including low-overlapping point clouds with displacements of up to 30m. The proposed method achieves on-par performance with state-of-the-art methods on the KITTI dataset, and outperforms existing methods for low overlapping point clouds. Additionally, the proposed method achieves significantly faster inference times, as low as 410ms, between 5 and 35 times faster than competing methods. Our code and dataset are available at https://github.com/eduardohenriquearnold/fastreg.



### DeepUME: Learning the Universal Manifold Embedding for Robust Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2112.09938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.09938v1)
- **Published**: 2021-12-18 14:06:42+00:00
- **Updated**: 2021-12-18 14:06:42+00:00
- **Authors**: Natalie Lang, Joseph M. Francos
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Registration of point clouds related by rigid transformations is one of the fundamental problems in computer vision. However, a solution to the practical scenario of aligning sparsely and differently sampled observations in the presence of noise is still lacking. We approach registration in this scenario with a fusion of the closed-form Universal Mani-fold Embedding (UME) method and a deep neural network. The two are combined into a single unified framework, named DeepUME, trained end-to-end and in an unsupervised manner. To successfully provide a global solution in the presence of large transformations, we employ an SO(3)-invariant coordinate system to learn both a joint-resampling strategy of the point clouds and SO(3)-invariant features. These features are then utilized by the geometric UME method for transformation estimation. The parameters of DeepUME are optimized using a metric designed to overcome an ambiguity problem emerging in the registration of symmetric shapes, when noisy scenarios are considered. We show that our hybrid method outperforms state-of-the-art registration methods in various scenarios, and generalizes well to unseen data sets. Our code is publicly available.



### Rapid Face Mask Detection and Person Identification Model based on Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2112.09951v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.09951v1)
- **Published**: 2021-12-18 15:29:14+00:00
- **Updated**: 2021-12-18 15:29:14+00:00
- **Authors**: Abdullah Ahmad Khan, Mohd. Belal, GhufranUllah
- **Comment**: 12 pages , 15 figures , International Conference
- **Journal**: None
- **Summary**: As Covid-19 has been constantly getting mutated and in three or four months a new variant gets introduced to us and it comes with more deadly problems. The things that prevent us from getting Covid is getting vaccinated and wearing a face mask. In this paper, we have implemented a new Face Mask Detection and Person Recognition model named Insight face which is based on SoftMax loss classification algorithm Arc Face loss and names it as RFMPI-DNN(Rapid Face Detection and Peron Identification Model based on Deep Neural Networks) to detect face mask and person identity rapidly as compared to other models available. To compare our new model, we have used previous MobileNet_V2 model and face recognition module for effective comparison on the basis of time. The proposed model implemented in the system has outperformed the model compared in this paper in every aspect



### Pre-Training Transformers for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2112.09965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.09965v1)
- **Published**: 2021-12-18 16:52:48+00:00
- **Updated**: 2021-12-18 16:52:48+00:00
- **Authors**: Burhan Ul Tayyab, Nicholas Chua
- **Comment**: None
- **Journal**: None
- **Summary**: The Visual Domain Adaptation Challenge 2021 called for unsupervised domain adaptation methods that could improve the performance of models by transferring the knowledge obtained from source datasets to out-of-distribution target datasets. In this paper, we utilize BeiT [1] and demonstrate its capability of capturing key attributes from source datasets and apply it to target datasets in a semi-supervised manner. Our method was able to outperform current state-of-the-art (SoTA) techniques and was able to achieve 1st place on the ViSDA Domain Adaptation Challenge with ACC of 56.29% and AUROC of 69.79%.



### 3D Structural Analysis of the Optic Nerve Head to Robustly Discriminate Between Papilledema and Optic Disc Drusen
- **Arxiv ID**: http://arxiv.org/abs/2112.09970v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.09970v1)
- **Published**: 2021-12-18 17:05:53+00:00
- **Updated**: 2021-12-18 17:05:53+00:00
- **Authors**: Michaël J. A. Girard, Satish K. Panda, Tin Aung Tun, Elisabeth A. Wibroe, Raymond P. Najjar, Aung Tin, Alexandre H. Thiéry, Steffen Hamann, Clare Fraser, Dan Milea
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: (1) To develop a deep learning algorithm to identify major tissue structures of the optic nerve head (ONH) in 3D optical coherence tomography (OCT) scans; (2) to exploit such information to robustly differentiate among healthy, optic disc drusen (ODD), and papilledema ONHs.   It was a cross-sectional comparative study with confirmed ODD (105 eyes), papilledema due to high intracranial pressure (51 eyes), and healthy controls (100 eyes). 3D scans of the ONHs were acquired using OCT, then processed to improve deep-tissue visibility. At first, a deep learning algorithm was developed using 984 B-scans (from 130 eyes) in order to identify: major neural/connective tissues, and ODD regions. The performance of our algorithm was assessed using the Dice coefficient (DC). In a 2nd step, a classification algorithm (random forest) was designed using 150 OCT volumes to perform 3-class classifications (1: ODD, 2: papilledema, 3: healthy) strictly from their drusen and prelamina swelling scores (derived from the segmentations). To assess performance, we reported the area under the receiver operating characteristic curves (AUCs) for each class.   Our segmentation algorithm was able to isolate neural and connective tissues, and ODD regions whenever present. This was confirmed by an average DC of 0.93$\pm$0.03 on the test set, corresponding to good performance. Classification was achieved with high AUCs, i.e. 0.99$\pm$0.01 for the detection of ODD, 0.99 $\pm$ 0.01 for the detection of papilledema, and 0.98$\pm$0.02 for the detection of healthy ONHs.   Our AI approach accurately discriminated ODD from papilledema, using a single OCT scan. Our classification performance was excellent, with the caveat that validation in a much larger population is warranted. Our approach may have the potential to establish OCT as the mainstay of diagnostic imaging in neuro-ophthalmology.



### Tell me what you see: A zero-shot action recognition method based on natural language descriptions
- **Arxiv ID**: http://arxiv.org/abs/2112.09976v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.09976v1)
- **Published**: 2021-12-18 17:44:07+00:00
- **Updated**: 2021-12-18 17:44:07+00:00
- **Authors**: Valter Estevam, Rayson Laroca, David Menotti, Helio Pedrini
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, several approaches have explored the detection and classification of objects in videos to perform Zero-Shot Action Recognition with remarkable results. In these methods, class-object relationships are used to associate visual patterns with the semantic side information because these relationships also tend to appear in texts. Therefore, word vector methods would reflect them in their latent representations. Inspired by these methods and by video captioning's ability to describe events not only with a set of objects but with contextual information, we propose a method in which video captioning models, called observers, provide different and complementary descriptive sentences. We demonstrate that representing videos with descriptive sentences instead of deep features, in ZSAR, is viable and naturally alleviates the domain adaptation problem, as we reached state-of-the-art (SOTA) performance on the UCF101 dataset and competitive performance on HMDB51 without their training sets. We also demonstrate that word vectors are unsuitable for building the semantic embedding space of our descriptions. Thus, we propose to represent the classes with sentences extracted from documents acquired with search engines on the Internet, without any human evaluation on the quality of descriptions. Lastly, we build a shared semantic space employing BERT-based embedders pre-trained in the paraphrasing task on multiple text datasets. We show that this pre-training is essential for bridging the semantic gap. The projection onto this space is straightforward for both types of information, visual and semantic, because they are sentences, enabling the classification with nearest neighbour rule in this shared space. Our code is available at https://github.com/valterlej/zsarcap.



### Deeper Learning with CoLU Activation
- **Arxiv ID**: http://arxiv.org/abs/2112.12078v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2112.12078v1)
- **Published**: 2021-12-18 21:11:11+00:00
- **Updated**: 2021-12-18 21:11:11+00:00
- **Authors**: Advait Vagerwal
- **Comment**: 7 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: In neural networks, non-linearity is introduced by activation functions. One commonly used activation function is Rectified Linear Unit (ReLU). ReLU has been a popular choice as an activation but has flaws. State-of-the-art functions like Swish and Mish are now gaining attention as a better choice as they combat many flaws presented by other activation functions. CoLU is an activation function similar to Swish and Mish in properties. It is defined as f(x)=x/(1-xe^-(x+e^x)). It is smooth, continuously differentiable, unbounded above, bounded below, non-saturating, and non-monotonic. Based on experiments done with CoLU with different activation functions, it is observed that CoLU usually performs better than other functions on deeper neural networks. While training different neural networks on MNIST on an incrementally increasing number of convolutional layers, CoLU retained the highest accuracy for more layers. On a smaller network with 8 convolutional layers, CoLU had the highest mean accuracy, closely followed by ReLU. On VGG-13 trained on Fashion-MNIST, CoLU had a 4.20% higher accuracy than Mish and 3.31% higher accuracy than ReLU. On ResNet-9 trained on Cifar-10, CoLU had 0.05% higher accuracy than Swish, 0.09% higher accuracy than Mish, and 0.29% higher accuracy than ReLU. It is observed that activation functions may behave better than other activation functions based on different factors including the number of layers, types of layers, number of parameters, learning rate, optimizer, etc. Further research can be done on these factors and activation functions for more optimal activation functions and more knowledge on their behavior.



### Cross-Domain Federated Learning in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2112.10001v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.10001v1)
- **Published**: 2021-12-18 21:12:06+00:00
- **Updated**: 2021-12-18 21:12:06+00:00
- **Authors**: Vishwa S Parekh, Shuhao Lai, Vladimir Braverman, Jeff Leal, Steven Rowe, Jay J Pillai, Michael A Jacobs
- **Comment**: Under Review for MIDL 2022
- **Journal**: None
- **Summary**: Federated learning is increasingly being explored in the field of medical imaging to train deep learning models on large scale datasets distributed across different data centers while preserving privacy by avoiding the need to transfer sensitive patient information. In this manuscript, we explore federated learning in a multi-domain, multi-task setting wherein different participating nodes may contain datasets sourced from different domains and are trained to solve different tasks. We evaluated cross-domain federated learning for the tasks of object detection and segmentation across two different experimental settings: multi-modal and multi-organ. The result from our experiments on cross-domain federated learning framework were very encouraging with an overlap similarity of 0.79 for organ localization and 0.65 for lesion segmentation. Our results demonstrate the potential of federated learning in developing multi-domain, multi-task deep learning models without sharing data from different domains.



### Image Segmentation Using Text and Image Prompts
- **Arxiv ID**: http://arxiv.org/abs/2112.10003v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10003v2)
- **Published**: 2021-12-18 21:27:19+00:00
- **Updated**: 2022-03-30 15:42:46+00:00
- **Authors**: Timo Lüddecke, Alexander S. Ecker
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Image segmentation is usually addressed by training a model for a fixed set of object classes. Incorporating additional classes or more complex queries later is expensive as it requires re-training the model on a dataset that encompasses these expressions. Here we propose a system that can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text or an image. This approach enables us to create a unified model (trained once) for three common segmentation tasks, which come with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation. We build upon the CLIP model as a backbone which we extend with a transformer-based decoder that enables dense prediction. After training on an extended version of the PhraseCut dataset, our system generates a binary segmentation map for an image based on a free-text prompt or on an additional image expressing the query. We analyze different variants of the latter image-based prompts in detail. This novel hybrid input allows for dynamic adaptation not only to the three segmentation tasks mentioned above, but to any binary segmentation task where a text or image query can be formulated. Finally, we find our system to adapt well to generalized queries involving affordances or properties. Code is available at https://eckerlab.org/code/clipseg.



### Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks
- **Arxiv ID**: http://arxiv.org/abs/2112.10017v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2112.10017v1)
- **Published**: 2021-12-18 22:37:30+00:00
- **Updated**: 2021-12-18 22:37:30+00:00
- **Authors**: Zixuan Ke, Bing Liu, Xingchang Huang
- **Comment**: None
- **Journal**: NeurIPS 2020
- **Summary**: Existing research on continual learning of a sequence of tasks focused on dealing with catastrophic forgetting, where the tasks are assumed to be dissimilar and have little shared knowledge. Some work has also been done to transfer previously learned knowledge to the new task when the tasks are similar and have shared knowledge. To the best of our knowledge, no technique has been proposed to learn a sequence of mixed similar and dissimilar tasks that can deal with forgetting and also transfer knowledge forward and backward. This paper proposes such a technique to learn both types of tasks in the same network. For dissimilar tasks, the algorithm focuses on dealing with forgetting, and for similar tasks, the algorithm focuses on selectively transferring the knowledge learned from some similar previous tasks to improve the new task learning. Additionally, the algorithm automatically detects whether a new task is similar to any previous tasks. Empirical evaluation using sequences of mixed tasks demonstrates the effectiveness of the proposed model.



### Supervised laser-speckle image sampling of skin tissue to detect very early stage of diabetes by its effects on skin subcellular properties
- **Arxiv ID**: http://arxiv.org/abs/2112.10024v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2112.10024v1)
- **Published**: 2021-12-18 23:08:53+00:00
- **Updated**: 2021-12-18 23:08:53+00:00
- **Authors**: Ahmet Orun, Luke Vella Critien, Jennifer Carter, Martin Stacey
- **Comment**: None
- **Journal**: None
- **Summary**: This paper investigates the effectiveness of an expert system based on K-nearest neighbors algorithm for laser speckle image sampling applied to the early detection of diabetes. With the latest developments in artificial intelligent guided laser speckle imaging technologies, it may be possible to optimise laser parameters, such as wavelength, energy level and image texture measures in association with a suitable AI technique to interact effectively with the subcellular properties of a skin tissue to detect early signs of diabetes. The new approach is potentially more effective than the classical skin glucose level observation because of its optimised combination of laser physics and AI techniques, and additionally, it allows non-expert individuals to perform more frequent skin tissue tests for an early detection of diabetes.



