# Arxiv Papers in cs.CV on 2021-12-23
### Robust and Precise Facial Landmark Detection by Self-Calibrated Pose Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2112.12328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12328v1)
- **Published**: 2021-12-23 02:51:08+00:00
- **Updated**: 2021-12-23 02:51:08+00:00
- **Authors**: Jun Wan, Hui Xi, Jie Zhou, Zhihui Lai, Witold Pedrycz, Xu Wang, Hang Sun
- **Comment**: Accept by IEEE Transactions on Cybernetics, December 2021
- **Journal**: None
- **Summary**: Current fully-supervised facial landmark detection methods have progressed rapidly and achieved remarkable performance. However, they still suffer when coping with faces under large poses and heavy occlusions for inaccurate facial shape constraints and insufficient labeled training samples. In this paper, we propose a semi-supervised framework, i.e., a Self-Calibrated Pose Attention Network (SCPAN) to achieve more robust and precise facial landmark detection in challenging scenarios. To be specific, a Boundary-Aware Landmark Intensity (BALI) field is proposed to model more effective facial shape constraints by fusing boundary and landmark intensity field information. Moreover, a Self-Calibrated Pose Attention (SCPA) model is designed to provide a self-learned objective function that enforces intermediate supervision without label information by introducing a self-calibrated mechanism and a pose attention mask. We show that by integrating the BALI fields and SCPA model into a novel self-calibrated pose attention network, more facial prior knowledge can be learned and the detection accuracy and robustness of our method for faces with large poses and heavy occlusions have been improved. The experimental results obtained for challenging benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in the literature.



### MVDG: A Unified Multi-view Framework for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2112.12329v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12329v2)
- **Published**: 2021-12-23 02:51:35+00:00
- **Updated**: 2022-08-08 05:33:42+00:00
- **Authors**: Jian Zhang, Lei Qi, Yinghuan Shi, Yang Gao
- **Comment**: Accepted by ECCV2022. The code is available at
  https://github.com/koncle/MVDG
- **Journal**: None
- **Summary**: To generalize the model trained in source domains to unseen target domains, domain generalization (DG) has recently attracted lots of attention. Since target domains can not be involved in training, overfitting source domains is inevitable. As a popular regularization technique, the meta-learning training scheme has shown its ability to resist overfitting. However, in the training stage, current meta-learning-based methods utilize only one task along a single optimization trajectory, which might produce a biased and noisy optimization direction. Beyond the training stage, overfitting could also cause unstable prediction in the test stage. In this paper, we propose a novel multi-view DG framework to effectively reduce the overfitting in both the training and test stage. Specifically, in the training stage, we develop a multi-view regularized meta-learning algorithm that employs multiple optimization trajectories to produce a suitable optimization direction for model updating. We also theoretically show that the generalization bound could be reduced by increasing the number of tasks in each trajectory. In the test stage, we utilize multiple augmented images to yield a multi-view prediction to alleviate unstable prediction, which significantly promotes model reliability. Extensive experiments on three benchmark datasets validate that our method can find a flat minimum to enhance generalization and outperform several state-of-the-art approaches.



### Revisiting Transformation Invariant Geometric Deep Learning: Are Initial Representations All You Need?
- **Arxiv ID**: http://arxiv.org/abs/2112.12345v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.12345v1)
- **Published**: 2021-12-23 03:52:33+00:00
- **Updated**: 2021-12-23 03:52:33+00:00
- **Authors**: Ziwei Zhang, Xin Wang, Zeyang Zhang, Peng Cui, Wenwu Zhu
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Geometric deep learning, i.e., designing neural networks to handle the ubiquitous geometric data such as point clouds and graphs, have achieved great successes in the last decade. One critical inductive bias is that the model can maintain invariance towards various transformations such as translation, rotation, and scaling. The existing graph neural network (GNN) approaches can only maintain permutation-invariance, failing to guarantee invariance with respect to other transformations. Besides GNNs, other works design sophisticated transformation-invariant layers, which are computationally expensive and difficult to be extended. To solve this problem, we revisit why the existing neural networks cannot maintain transformation invariance when handling geometric data. Our findings show that transformation-invariant and distance-preserving initial representations are sufficient to achieve transformation invariance rather than needing sophisticated neural layer designs. Motivated by these findings, we propose Transformation Invariant Neural Networks (TinvNN), a straightforward and general framework for geometric data. Specifically, we realize transformation-invariant and distance-preserving initial point representations by modifying multi-dimensional scaling before feeding the representations into neural networks. We prove that TinvNN can strictly guarantee transformation invariance, being general and flexible enough to be combined with the existing neural networks. Extensive experimental results on point cloud analysis and combinatorial optimization demonstrate the effectiveness and general applicability of our proposed method. Based on the experimental results, we advocate that TinvNN should be considered a new starting point and an essential baseline for further studies of transformation-invariant geometric deep learning.



### Learning Hierarchical Attention for Weakly-supervised Chest X-Ray Abnormality Localization and Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2112.12349v1
- **DOI**: 10.1109/TMI.2020.3042773
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12349v1)
- **Published**: 2021-12-23 04:12:51+00:00
- **Updated**: 2021-12-23 04:12:51+00:00
- **Authors**: Xi Ouyang, Srikrishna Karanam, Ziyan Wu, Terrence Chen, Jiayu Huo, Xiang Sean Zhou, Qian Wang, Jie-Zhi Cheng
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging 2021
- **Summary**: We consider the problem of abnormality localization for clinical applications. While deep learning has driven much recent progress in medical imaging, many clinical challenges are not fully addressed, limiting its broader usage. While recent methods report high diagnostic accuracies, physicians have concerns trusting these algorithm results for diagnostic decision-making purposes because of a general lack of algorithm decision reasoning and interpretability. One potential way to address this problem is to further train these models to localize abnormalities in addition to just classifying them. However, doing this accurately will require a large amount of disease localization annotations by clinical experts, a task that is prohibitively expensive to accomplish for most applications. In this work, we take a step towards addressing these issues by means of a new attention-driven weakly supervised algorithm comprising a hierarchical attention mining framework that unifies activation- and gradient-based visual attention in a holistic manner. Our key algorithmic innovations include the design of explicit ordinal attention constraints, enabling principled model training in a weakly-supervised fashion, while also facilitating the generation of visual-attention-driven model explanations by means of localization cues. On two large-scale chest X-ray datasets (NIH ChestX-ray14 and CheXpert), we demonstrate significant localization performance improvements over the current state of the art while also achieving competitive classification performance. Our code is available on https://github.com/oyxhust/HAM.



### A Random Point Initialization Approach to Image Segmentation with Variational Level-sets
- **Arxiv ID**: http://arxiv.org/abs/2112.12355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12355v1)
- **Published**: 2021-12-23 04:37:44+00:00
- **Updated**: 2021-12-23 04:37:44+00:00
- **Authors**: J. N. Mueller, J. N. Corcoran
- **Comment**: 17 pages, 27 figures
- **Journal**: None
- **Summary**: Image segmentation is an essential component in many image processing and computer vision tasks. The primary goal of image segmentation is to simplify an image for easier analysis, and there are two broad approaches for achieving this: edge based methods, which extract the boundaries of specific known objects, and region based methods, which partition the image into regions that are statistically homogeneous. One of the more prominent edge finding methods, known as the level set method, evolves a zero-level contour in the image plane with gradient descent until the contour has converged to the object boundaries. While the classical level set method and its variants have proved successful in segmenting real images, they are susceptible to becoming stuck in noisy regions of the image plane without a priori knowledge of the image and they are unable to provide details beyond object outer boundary locations. We propose a modification to the variational level set image segmentation method that can quickly detect object boundaries by making use of random point initialization. We demonstrate the efficacy of our approach by comparing the performance of our method on real images to that of the prominent Canny Method.



### Dual Path Structural Contrastive Embeddings for Learning Novel Objects
- **Arxiv ID**: http://arxiv.org/abs/2112.12359v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12359v3)
- **Published**: 2021-12-23 04:43:31+00:00
- **Updated**: 2022-01-04 06:07:49+00:00
- **Authors**: Bingbin Li, Elvis Han Cui, Yanan Li, Donghui Wang, Weng Kee Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Learning novel classes from a very few labeled samples has attracted increasing attention in machine learning areas. Recent research on either meta-learning based or transfer-learning based paradigm demonstrates that gaining information on a good feature space can be an effective solution to achieve favorable performance on few-shot tasks. In this paper, we propose a simple but effective paradigm that decouples the tasks of learning feature representations and classifiers and only learns the feature embedding architecture from base classes via the typical transfer-learning training strategy. To maintain both the generalization ability across base and novel classes and discrimination ability within each class, we propose a dual path feature learning scheme that effectively combines structural similarity with contrastive feature construction. In this way, both inner-class alignment and inter-class uniformity can be well balanced, and result in improved performance. Experiments on three popular benchmarks show that when incorporated with a simple prototype based classifier, our method can still achieve promising results for both standard and generalized few-shot problems in either an inductive or transductive inference setting.



### DENSE: Data-Free One-Shot Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.12371v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.12371v2)
- **Published**: 2021-12-23 05:43:29+00:00
- **Updated**: 2022-11-21 08:17:40+00:00
- **Authors**: Jie Zhang, Chen Chen, Bo Li, Lingjuan Lyu, Shuang Wu, Shouhong Ding, Chunhua Shen, Chao Wu
- **Comment**: Accepted by NeurIPS 2022
- **Journal**: None
- **Summary**: One-shot Federated Learning (FL) has recently emerged as a promising approach, which allows the central server to learn a model in a single communication round. Despite the low communication cost, existing one-shot FL methods are mostly impractical or face inherent limitations, \eg a public dataset is required, clients' models are homogeneous, and additional data/model information need to be uploaded. To overcome these issues, we propose a novel two-stage \textbf{D}ata-fre\textbf{E} o\textbf{N}e-\textbf{S}hot federated l\textbf{E}arning (DENSE) framework, which trains the global model by a data generation stage and a model distillation stage. DENSE is a practical one-shot FL method that can be applied in reality due to the following advantages: (1) DENSE requires no additional information compared with other methods (except the model parameters) to be transferred between clients and the server; (2) DENSE does not require any auxiliary dataset for training; (3) DENSE considers model heterogeneity in FL, \ie different clients can have different model architectures. Experiments on a variety of real-world datasets demonstrate the superiority of our method.For example, DENSE outperforms the best baseline method Fed-ADI by 5.08\% on CIFAR10 dataset.



### DILF-EN framework for Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.12385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12385v1)
- **Published**: 2021-12-23 06:49:24+00:00
- **Updated**: 2021-12-23 06:49:24+00:00
- **Authors**: Mohammed Asad Karim, Indu Joshi, Pratik Mazumder, Pravendra Singh
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Deep learning models suffer from catastrophic forgetting of the classes in the older phases as they get trained on the classes introduced in the new phase in the class-incremental learning setting. In this work, we show that the effect of catastrophic forgetting on the model prediction varies with the change in orientation of the same image, which is a novel finding. Based on this, we propose a novel data-ensemble approach that combines the predictions for the different orientations of the image to help the model retain further information regarding the previously seen classes and thereby reduce the effect of forgetting on the model predictions. However, we cannot directly use the data-ensemble approach if the model is trained using traditional techniques. Therefore, we also propose a novel dual-incremental learning framework that involves jointly training the network with two incremental learning objectives, i.e., the class-incremental learning objective and our proposed data-incremental learning objective. In the dual-incremental learning framework, each image belongs to two classes, i.e., the image class (for class-incremental learning) and the orientation class (for data-incremental learning). In class-incremental learning, each new phase introduces a new set of classes, and the model cannot access the complete training data from the older phases. In our proposed data-incremental learning, the orientation classes remain the same across all the phases, and the data introduced by the new phase in class-incremental learning acts as new training data for these orientation classes. We empirically demonstrate that the dual-incremental learning framework is vital to the data-ensemble approach. We apply our proposed approach to state-of-the-art class-incremental learning methods and empirically show that our framework significantly improves the performance of these methods.



### KFWC: A Knowledge-Driven Deep Learning Model for Fine-grained Classification of Wet-AMD
- **Arxiv ID**: http://arxiv.org/abs/2112.12386v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.12386v1)
- **Published**: 2021-12-23 07:01:59+00:00
- **Updated**: 2021-12-23 07:01:59+00:00
- **Authors**: Haihong E, Jiawen He, Tianyi Hu, Lifei Wang, Lifei Yuan, Ruru Zhang, Meina Song
- **Comment**: None
- **Journal**: None
- **Summary**: Automated diagnosis using deep neural networks can help ophthalmologists detect the blinding eye disease wet Age-related Macular Degeneration (AMD). Wet-AMD has two similar subtypes, Neovascular AMD and Polypoidal Choroidal Vessels (PCV). However, due to the difficulty in data collection and the similarity between images, most studies have only achieved the coarse-grained classification of wet-AMD rather than a finer-grained one of wet-AMD subtypes. To solve this issue, in this paper we propose a Knowledge-driven Fine-grained Wet-AMD Classification Model (KFWC), to classify fine-grained diseases with insufficient data. With the introduction of a priori knowledge of 10 lesion signs of input images into the KFWC, we aim to accelerate the KFWC by means of multi-label classification pre-training, to locate the decisive image features in the fine-grained disease classification task and therefore achieve better classification. Simultaneously, the KFWC can also provide good interpretability and effectively alleviate the pressure of data collection and annotation in the field of fine-grained disease classification for wet-AMD. The experiments demonstrate the effectiveness of the KFWC which reaches 99.71% in AU-ROC scores, and its considerable improvements over the data-driven w/o Knowledge and ophthalmologists, with the rates of 6.69% over the strongest baseline and 4.14% over ophthalmologists.



### Cloud Removal from Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2112.15483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.15483v1)
- **Published**: 2021-12-23 07:25:19+00:00
- **Updated**: 2021-12-23 07:25:19+00:00
- **Authors**: Rutvik Chauhan, Antarpuneet Singh, Sujoy Saha
- **Comment**: None
- **Journal**: None
- **Summary**: In this report, we have analyzed available cloud detection technique using sentinel hub. We have also implemented spatial attention generative adversarial network and improved quality of generated image compared to previous solution [7].



### Learning Implicit Body Representations from Double Diffusion Based Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2112.12390v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12390v2)
- **Published**: 2021-12-23 07:30:22+00:00
- **Updated**: 2022-01-17 09:26:41+00:00
- **Authors**: Guangming Yao, Hongzhi Wu, Yi Yuan, Lincheng Li, Kun Zhou, Xin Yu
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: In this paper, we present a novel double diffusion based neural radiance field, dubbed DD-NeRF, to reconstruct human body geometry and render the human body appearance in novel views from a sparse set of images. We first propose a double diffusion mechanism to achieve expressive representations of input images by fully exploiting human body priors and image appearance details at two levels. At the coarse level, we first model the coarse human body poses and shapes via an unclothed 3D deformable vertex model as guidance. At the fine level, we present a multi-view sampling network to capture subtle geometric deformations and image detailed appearances, such as clothing and hair, from multiple input views. Considering the sparsity of the two level features, we diffuse them into feature volumes in the canonical space to construct neural radiance fields. Then, we present a signed distance function (SDF) regression network to construct body surfaces from the diffused features. Thanks to our double diffused representations, our method can even synthesize novel views of unseen subjects. Experiments on various datasets demonstrate that our approach outperforms the state-of-the-art in both geometric reconstruction and novel view synthesis.



### Iteratively Selecting an Easy Reference Frame Makes Unsupervised Video Object Segmentation Easier
- **Arxiv ID**: http://arxiv.org/abs/2112.12402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12402v1)
- **Published**: 2021-12-23 07:54:15+00:00
- **Updated**: 2021-12-23 07:54:15+00:00
- **Authors**: Youngjo Lee, Hongje Seong, Euntai Kim
- **Comment**: Accepted to AAAI 2022
- **Journal**: None
- **Summary**: Unsupervised video object segmentation (UVOS) is a per-pixel binary labeling problem which aims at separating the foreground object from the background in the video without using the ground truth (GT) mask of the foreground object. Most of the previous UVOS models use the first frame or the entire video as a reference frame to specify the mask of the foreground object. Our question is why the first frame should be selected as a reference frame or why the entire video should be used to specify the mask. We believe that we can select a better reference frame to achieve the better UVOS performance than using only the first frame or the entire video as a reference frame. In our paper, we propose Easy Frame Selector (EFS). The EFS enables us to select an 'easy' reference frame that makes the subsequent VOS become easy, thereby improving the VOS performance. Furthermore, we propose a new framework named as Iterative Mask Prediction (IMP). In the framework, we repeat applying EFS to the given video and selecting an 'easier' reference frame from the video than the previous iteration, increasing the VOS performance incrementally. The IMP consists of EFS, Bi-directional Mask Prediction (BMP), and Temporal Information Updating (TIU). From the proposed framework, we achieve state-of-the-art performance in three UVOS benchmark sets: DAVIS16, FBMS, and SegTrack-V2.



### Radiomic biomarker extracted from PI-RADS 3 patients support more eìcient and robust prostate cancer diagnosis: a multi-center study
- **Arxiv ID**: http://arxiv.org/abs/2112.13686v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2112.13686v1)
- **Published**: 2021-12-23 08:07:37+00:00
- **Updated**: 2021-12-23 08:07:37+00:00
- **Authors**: Longfei Li, Rui Yang, Xin Chen, Cheng Li, Hairong Zheng, Yusong Lin, Zaiyi Liu, Shanshan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Prostate Imaging Reporting and Data System (PI-RADS) based on multi-parametric MRI classi\^ees patients into 5 categories (PI-RADS 1-5) for routine clinical diagnosis guidance. However, there is no consensus on whether PI-RADS 3 patients should go through biopsies. Mining features from these hard samples (HS) is meaningful for physicians to achieve accurate diagnoses. Currently, the mining of HS biomarkers is insu\`icient, and the e\'eectiveness and robustness of HS biomarkers for prostate cancer diagnosis have not been explored. In this study, biomarkers from di\'eerent data distributions are constructed. Results show that HS biomarkers can achieve better performances in di\'eerent data distributions.



### InstaIndoor and Multi-modal Deep Learning for Indoor Scene Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.12409v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12409v1)
- **Published**: 2021-12-23 08:11:22+00:00
- **Updated**: 2021-12-23 08:11:22+00:00
- **Authors**: Andreea Glavan, Estefania Talavera
- **Comment**: None
- **Journal**: None
- **Summary**: Indoor scene recognition is a growing field with great potential for behaviour understanding, robot localization, and elderly monitoring, among others. In this study, we approach the task of scene recognition from a novel standpoint, using multi-modal learning and video data gathered from social media. The accessibility and variety of social media videos can provide realistic data for modern scene recognition techniques and applications. We propose a model based on fusion of transcribed speech to text and visual features, which is used for classification on a novel dataset of social media videos of indoor scenes named InstaIndoor. Our model achieves up to 70% accuracy and 0.7 F1-Score. Furthermore, we highlight the potential of our approach by benchmarking on a YouTube-8M subset of indoor scenes as well, where it achieves 74% accuracy and 0.74 F1-Score. We hope the contributions of this work pave the way to novel research in the challenging field of indoor scene recognition.



### Adaptive Modeling Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2112.12431v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.12431v1)
- **Published**: 2021-12-23 09:52:30+00:00
- **Updated**: 2021-12-23 09:52:30+00:00
- **Authors**: Zhiwen Yan, Teck Khim Ng
- **Comment**: 10 pages, 3 figures
- **Journal**: None
- **Summary**: Adversarial training, the process of training a deep learning model with adversarial data, is one of the most successful adversarial defense methods for deep learning models. We have found that the robustness to white-box attack of an adversarially trained model can be further improved if we fine tune this model in inference stage to adapt to the adversarial input, with the extra information in it. We introduce an algorithm that "post trains" the model at inference stage between the original output class and a "neighbor" class, with existing training data. The accuracy of pre-trained Fast-FGSM CIFAR10 classifier base model against white-box projected gradient attack (PGD) can be significantly improved from 46.8% to 64.5% with our algorithm.



### Your Face Mirrors Your Deepest Beliefs-Predicting Personality and Morals through Facial Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.12455v1
- **DOI**: 10.3390/fi14010005
- **Categories**: **cs.CV**, cs.CY, cs.LG, H.4.0; I.2.10; J.4
- **Links**: [PDF](http://arxiv.org/pdf/2112.12455v1)
- **Published**: 2021-12-23 10:46:51+00:00
- **Updated**: 2021-12-23 10:46:51+00:00
- **Authors**: P. A. Gloor, A. Fronzetti Colladon, E. Altuntas, C. Cetinkaya, M. F. Kaiser, L. Ripperger, T. Schaefer
- **Comment**: None
- **Journal**: Future Internet 14(1), 5 (2022)
- **Summary**: Can we really "read the mind in the eyes"? Moreover, can AI assist us in this task? This paper answers these two questions by introducing a machine learning system that predicts personality characteristics of individuals on the basis of their face. It does so by tracking the emotional response of the individual's face through facial emotion recognition (FER) while watching a series of 15 short videos of different genres. To calibrate the system, we invited 85 people to watch the videos, while their emotional responses were analyzed through their facial expression. At the same time, these individuals also took four well-validated surveys of personality characteristics and moral values: the revised NEO FFI personality inventory, the Haidt moral foundations test, the Schwartz personal value system, and the domain-specific risk-taking scale (DOSPERT). We found that personality characteristics and moral values of an individual can be predicted through their emotional response to the videos as shown in their face, with an accuracy of up to 86% using gradient-boosted trees. We also found that different personality characteristics are better predicted by different videos, in other words, there is no single video that will provide accurate predictions for all personality characteristics, but it is the response to the mix of different videos that allows for accurate prediction.



### Pose Adaptive Dual Mixup for Few-Shot Single-View 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2112.12484v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12484v1)
- **Published**: 2021-12-23 12:22:08+00:00
- **Updated**: 2021-12-23 12:22:08+00:00
- **Authors**: Ta-Ying Cheng, Hsuan-Ru Yang, Niki Trigoni, Hwann-Tzong Chen, Tyng-Luh Liu
- **Comment**: To appear in the Thirty-Sixth AAAI Conference on Artificial
  Intelligence (AAAI), February 2022
- **Journal**: None
- **Summary**: We present a pose adaptive few-shot learning procedure and a two-stage data interpolation regularization, termed Pose Adaptive Dual Mixup (PADMix), for single-image 3D reconstruction. While augmentations via interpolating feature-label pairs are effective in classification tasks, they fall short in shape predictions potentially due to inconsistencies between interpolated products of two images and volumes when rendering viewpoints are unknown. PADMix targets this issue with two sets of mixup procedures performed sequentially. We first perform an input mixup which, combined with a pose adaptive learning procedure, is helpful in learning 2D feature extraction and pose adaptive latent encoding. The stagewise training allows us to build upon the pose invariant representations to perform a follow-up latent mixup under one-to-one correspondences between features and ground-truth volumes. PADMix significantly outperforms previous literature on few-shot settings over the ShapeNet dataset and sets new benchmarks on the more challenging real-world Pix3D dataset.



### LaTr: Layout-Aware Transformer for Scene-Text VQA
- **Arxiv ID**: http://arxiv.org/abs/2112.12494v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12494v2)
- **Published**: 2021-12-23 12:41:26+00:00
- **Updated**: 2021-12-24 11:06:59+00:00
- **Authors**: Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Appalaraju, R. Manmatha
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel multimodal architecture for Scene Text Visual Question Answering (STVQA), named Layout-Aware Transformer (LaTr). The task of STVQA requires models to reason over different modalities. Thus, we first investigate the impact of each modality, and reveal the importance of the language module, especially when enriched with layout information. Accounting for this, we propose a single objective pre-training scheme that requires only text and spatial cues. We show that applying this pre-training scheme on scanned documents has certain advantages over using natural images, despite the domain gap. Scanned documents are easy to procure, text-dense and have a variety of layouts, helping the model learn various spatial cues (e.g. left-of, below etc.) by tying together language and layout information. Compared to existing approaches, our method performs vocabulary-free decoding and, as shown, generalizes well beyond the training vocabulary. We further demonstrate that LaTr improves robustness towards OCR errors, a common reason for failure cases in STVQA. In addition, by leveraging a vision transformer, we eliminate the need for an external object detector. LaTr outperforms state-of-the-art STVQA methods on multiple datasets. In particular, +7.6% on TextVQA, +10.8% on ST-VQA and +4.0% on OCR-VQA (all absolute accuracy numbers).



### FedFR: Joint Optimization Federated Framework for Generic and Personalized Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.12496v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.12496v3)
- **Published**: 2021-12-23 12:42:38+00:00
- **Updated**: 2022-03-21 06:55:43+00:00
- **Authors**: Chih-Ting Liu, Chien-Yi Wang, Shao-Yi Chien, Shang-Hong Lai
- **Comment**: This paper was accepted by AAAI 2022 Conference on Artificial
  Intelligence and selected as an oral paper
- **Journal**: None
- **Summary**: Current state-of-the-art deep learning based face recognition (FR) models require a large number of face identities for central training. However, due to the growing privacy awareness, it is prohibited to access the face images on user devices to continually improve face recognition models. Federated Learning (FL) is a technique to address the privacy issue, which can collaboratively optimize the model without sharing the data between clients. In this work, we propose a FL based framework called FedFR to improve the generic face representation in a privacy-aware manner. Besides, the framework jointly optimizes personalized models for the corresponding clients via the proposed Decoupled Feature Customization module. The client-specific personalized model can serve the need of optimized face recognition experience for registered identities at the local device. To the best of our knowledge, we are the first to explore the personalized face recognition in FL setup. The proposed framework is validated to be superior to previous approaches on several generic and personalized face recognition benchmarks with diverse FL scenarios. The source codes and our proposed personalized FR benchmark under FL setup are available at https://github.com/jackie840129/FedFR.



### Attentive Multi-View Deep Subspace Clustering Net
- **Arxiv ID**: http://arxiv.org/abs/2112.12506v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.12506v1)
- **Published**: 2021-12-23 12:57:26+00:00
- **Updated**: 2021-12-23 12:57:26+00:00
- **Authors**: Run-kun Lu, Jian-wei Liu, Xin Zuo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel Attentive Multi-View Deep Subspace Nets (AMVDSN), which deeply explores underlying consistent and view-specific information from multiple views and fuse them by considering each view's dynamic contribution obtained by attention mechanism. Unlike most multi-view subspace learning methods that they directly reconstruct data points on raw data or only consider consistency or complementarity when learning representation in deep or shallow space, our proposed method seeks to find a joint latent representation that explicitly considers both consensus and view-specific information among multiple views, and then performs subspace clustering on learned joint latent representation.Besides, different views contribute differently to representation learning, we therefore introduce attention mechanism to derive dynamic weight for each view, which performs much better than previous fusion methods in the field of multi-view subspace clustering. The proposed algorithm is intuitive and can be easily optimized just by using Stochastic Gradient Descent (SGD) because of the neural network framework, which also provides strong non-linear characterization capability compared with traditional subspace clustering approaches. The experimental results on seven real-world data sets have demonstrated the effectiveness of our proposed algorithm against some state-of-the-art subspace learning approaches.



### PyCIL: A Python Toolbox for Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.12533v2
- **DOI**: 10.1007/s11432-022-3600-y
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.12533v2)
- **Published**: 2021-12-23 13:41:24+00:00
- **Updated**: 2022-10-26 06:58:45+00:00
- **Authors**: Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, De-Chuan Zhan
- **Comment**: Accepted to SCIENCE CHINA Information Sciences. Code is available at
  https://github.com/G-U-N/PyCIL
- **Journal**: None
- **Summary**: Traditional machine learning systems are deployed under the closed-world setting, which requires the entire training data before the offline training process. However, real-world applications often face the incoming new classes, and a model should incorporate them continually. The learning paradigm is called Class-Incremental Learning (CIL). We propose a Python toolbox that implements several key algorithms for class-incremental learning to ease the burden of researchers in the machine learning community. The toolbox contains implementations of a number of founding works of CIL such as EWC and iCaRL, but also provides current state-of-the-art algorithms that can be used for conducting novel fundamental research. This toolbox, named PyCIL for Python Class-Incremental Learning, is available at https://github.com/G-U-N/PyCIL



### FourierMask: Instance Segmentation using Fourier Mapping in Implicit Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2112.12535v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.12535v2)
- **Published**: 2021-12-23 13:42:32+00:00
- **Updated**: 2022-03-17 14:48:47+00:00
- **Authors**: Hamd ul Moqeet Riaz, Nuri Benbarka, Timon Hoefer, Andreas Zell
- **Comment**: None
- **Journal**: None
- **Summary**: We present FourierMask, which employs Fourier series combined with implicit neural representations to generate instance segmentation masks. We apply a Fourier mapping (FM) to the coordinate locations and utilize the mapped features as inputs to an implicit representation (coordinate-based multi-layer perceptron (MLP)). FourierMask learns to predict the coefficients of the FM for a particular instance, and therefore adapts the FM to a specific object. This allows FourierMask to be generalized to predict instance segmentation masks from natural images. Since implicit functions are continuous in the domain of input coordinates, we illustrate that by sub-sampling the input pixel coordinates, we can generate higher resolution masks during inference. Furthermore, we train a renderer MLP (FourierRend) on the uncertain predictions of FourierMask and illustrate that it significantly improves the quality of the masks. FourierMask shows competitive results on the MS COCO dataset compared to the baseline Mask R-CNN at the same output resolution and surpasses it on higher resolution.



### On the relationship between calibrated predictors and unbiased volume estimation
- **Arxiv ID**: http://arxiv.org/abs/2112.12560v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.12560v1)
- **Published**: 2021-12-23 14:22:19+00:00
- **Updated**: 2021-12-23 14:22:19+00:00
- **Authors**: Teodora Popordanoska, Jeroen Bertels, Dirk Vandermeulen, Frederik Maes, Matthew B. Blaschko
- **Comment**: Published at MICCAI 2021
- **Journal**: None
- **Summary**: Machine learning driven medical image segmentation has become standard in medical image analysis. However, deep learning models are prone to overconfident predictions. This has led to a renewed focus on calibrated predictions in the medical imaging and broader machine learning communities. Calibrated predictions are estimates of the probability of a label that correspond to the true expected value of the label conditioned on the confidence. Such calibrated predictions have utility in a range of medical imaging applications, including surgical planning under uncertainty and active learning systems. At the same time it is often an accurate volume measurement that is of real importance for many medical applications. This work investigates the relationship between model calibration and volume estimation. We demonstrate both mathematically and empirically that if the predictor is calibrated per image, we can obtain the correct volume by taking an expectation of the probability scores per pixel/voxel of the image. Furthermore, we show that convex combinations of calibrated classifiers preserve volume estimation, but do not preserve calibration. Therefore, we conclude that having a calibrated predictor is a sufficient, but not necessary condition for obtaining an unbiased estimate of the volume. We validate our theoretical findings empirically on a collection of 18 different (calibrated) training strategies on the tasks of glioma volume estimation on BraTS 2018, and ischemic stroke lesion volume estimation on ISLES 2018 datasets.



### Boosting Generative Zero-Shot Learning by Synthesizing Diverse Features with Attribute Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.12573v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12573v1)
- **Published**: 2021-12-23 14:32:51+00:00
- **Updated**: 2021-12-23 14:32:51+00:00
- **Authors**: Xiaojie Zhao, Yuming Shen, Shidong Wang, Haofeng Zhang
- **Comment**: Accepted by AAAI2022
- **Journal**: None
- **Summary**: The recent advance in deep generative models outlines a promising perspective in the realm of Zero-Shot Learning (ZSL). Most generative ZSL methods use category semantic attributes plus a Gaussian noise to generate visual features. After generating unseen samples, this family of approaches effectively transforms the ZSL problem into a supervised classification scheme. However, the existing models use a single semantic attribute, which contains the complete attribute information of the category. The generated data also carry the complete attribute information, but in reality, visual samples usually have limited attributes. Therefore, the generated data from attribute could have incomplete semantics. Based on this fact, we propose a novel framework to boost ZSL by synthesizing diverse features. This method uses augmented semantic attributes to train the generative model, so as to simulate the real distribution of visual features. We evaluate the proposed model on four benchmark datasets, observing significant performance improvement against the state-of-the-art.



### NeRD++: Improved 3D-mirror symmetry learning from a single image
- **Arxiv ID**: http://arxiv.org/abs/2112.12579v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12579v2)
- **Published**: 2021-12-23 14:37:52+00:00
- **Updated**: 2022-10-07 08:34:42+00:00
- **Authors**: Yancong Lin, Silvia-Laura Pintea, Jan van Gemert
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Many objects are naturally symmetric, and this symmetry can be exploited to infer unseen 3D properties from a single 2D image. Recently, NeRD is proposed for accurate 3D mirror plane estimation from a single image. Despite the unprecedented accuracy, it relies on large annotated datasets for training and suffers from slow inference. Here we aim to improve its data and compute efficiency. We do away with the computationally expensive 4D feature volumes and instead explicitly compute the feature correlation of the pixel correspondences across depth, thus creating a compact 3D volume. We also design multi-stage spherical convolutions to identify the optimal mirror plane on the hemisphere, whose inductive bias offers gains in data-efficiency. Experiments on both synthetic and real-world datasets show the benefit of our proposed changes for improved data efficiency and inference speed.



### Towards Universal GAN Image Detection
- **Arxiv ID**: http://arxiv.org/abs/2112.12606v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12606v1)
- **Published**: 2021-12-23 14:49:27+00:00
- **Updated**: 2021-12-23 14:49:27+00:00
- **Authors**: Davide Cozzolino, Diego Gragnaniello, Giovanni Poggi, Luisa Verdoliva
- **Comment**: None
- **Journal**: None
- **Summary**: The ever higher quality and wide diffusion of fake images have spawn a quest for reliable forensic tools. Many GAN image detectors have been proposed, recently. In real world scenarios, however, most of them show limited robustness and generalization ability. Moreover, they often rely on side information not available at test time, that is, they are not universal. We investigate these problems and propose a new GAN image detector based on a limited sub-sampling architecture and a suitable contrastive learning paradigm. Experiments carried out in challenging conditions prove the proposed method to be a first step towards universal GAN image detection, ensuring also good robustness to common image impairments, and good generalization to unseen architectures.



### Predição da Idade Cerebral a partir de Imagens de Ressonância Magnética utilizando Redes Neurais Convolucionais
- **Arxiv ID**: http://arxiv.org/abs/2112.12609v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.12609v1)
- **Published**: 2021-12-23 14:51:45+00:00
- **Updated**: 2021-12-23 14:51:45+00:00
- **Authors**: Victor H. R. Oliveira, Augusto Antunes, Alexandre S. Soares, Arthur D. Reys, Robson Z. Júnior, Saulo D. S. Pedro, Danilo Silva
- **Comment**: 3 pages, 3 figures, in Portuguese, accepted at XVIII Congresso
  Brasileiro de Inform\'atica em Sa\'ude (CBIS 2021)
- **Journal**: None
- **Summary**: In this work, deep learning techniques for brain age prediction from magnetic resonance images are investigated, aiming to assist in the identification of biomarkers of the natural aging process. The identification of biomarkers is useful for detecting an early-stage neurodegenerative process, as well as for predicting age-related or non-age-related cognitive decline. Two techniques are implemented and compared in this work: a 3D Convolutional Neural Network applied to the volumetric image and a 2D Convolutional Neural Network applied to slices from the axial plane, with subsequent fusion of individual predictions. The best result was obtained by the 2D model, which achieved a mean absolute error of 3.83 years.   --   Neste trabalho s\~ao investigadas t\'ecnicas de aprendizado profundo para a predi\c{c}\~ao da idade cerebral a partir de imagens de resson\^ancia magn\'etica, visando auxiliar na identifica\c{c}\~ao de biomarcadores do processo natural de envelhecimento. A identifica\c{c}\~ao de biomarcadores \'e \'util para a detec\c{c}\~ao de um processo neurodegenerativo em est\'agio inicial, al\'em de possibilitar prever um decl\'inio cognitivo relacionado ou n\~ao \`a idade. Duas t\'ecnicas s\~ao implementadas e comparadas neste trabalho: uma Rede Neural Convolucional 3D aplicada na imagem volum\'etrica e uma Rede Neural Convolucional 2D aplicada a fatias do plano axial, com posterior fus\~ao das predi\c{c}\~oes individuais. O melhor resultado foi obtido pelo modelo 2D, que alcan\c{c}ou um erro m\'edio absoluto de 3.83 anos.



### PandaSet: Advanced Sensor Suite Dataset for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2112.12610v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.12610v1)
- **Published**: 2021-12-23 14:52:12+00:00
- **Updated**: 2021-12-23 14:52:12+00:00
- **Authors**: Pengchuan Xiao, Zhenlei Shao, Steven Hao, Zishuo Zhang, Xiaolin Chai, Judy Jiao, Zesong Li, Jian Wu, Kai Sun, Kun Jiang, Yunlong Wang, Diange Yang
- **Comment**: This paper has been published on ITSC'2021, please check the website
  of the PandaSet for more information: https://pandaset.org/
- **Journal**: None
- **Summary**: The accelerating development of autonomous driving technology has placed greater demands on obtaining large amounts of high-quality data. Representative, labeled, real world data serves as the fuel for training deep learning networks, critical for improving self-driving perception algorithms. In this paper, we introduce PandaSet, the first dataset produced by a complete, high-precision autonomous vehicle sensor kit with a no-cost commercial license. The dataset was collected using one 360{\deg} mechanical spinning LiDAR, one forward-facing, long-range LiDAR, and 6 cameras. The dataset contains more than 100 scenes, each of which is 8 seconds long, and provides 28 types of labels for object classification and 37 types of labels for semantic segmentation. We provide baselines for LiDAR-only 3D object detection, LiDAR-camera fusion 3D object detection and LiDAR point cloud segmentation. For more details about PandaSet and the development kit, see https://scale.com/open-datasets/pandaset.



### Manifold Learning Benefits GANs
- **Arxiv ID**: http://arxiv.org/abs/2112.12618v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.12618v2)
- **Published**: 2021-12-23 14:59:05+00:00
- **Updated**: 2022-04-01 17:43:49+00:00
- **Authors**: Yao Ni, Piotr Koniusz, Richard Hartley, Richard Nock
- **Comment**: CVPR 2022, 32 pages full version
- **Journal**: None
- **Summary**: In this paper, we improve Generative Adversarial Networks by incorporating a manifold learning step into the discriminator. We consider locality-constrained linear and subspace-based manifolds, and locality-constrained non-linear manifolds. In our design, the manifold learning and coding steps are intertwined with layers of the discriminator, with the goal of attracting intermediate feature representations onto manifolds. We adaptively balance the discrepancy between feature representations and their manifold view, which is a trade-off between denoising on the manifold and refining the manifold. We find that locality-constrained non-linear manifolds outperform linear manifolds due to their non-uniform density and smoothness. We also substantially outperform state-of-the-art baselines.



### Comparison and Analysis of Image-to-Image Generative Adversarial Networks: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2112.12625v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12625v2)
- **Published**: 2021-12-23 15:11:18+00:00
- **Updated**: 2022-08-27 00:32:29+00:00
- **Authors**: Sagar Saxena, Mohammad Nayeem Teli
- **Comment**: 36 pages, 22 figures, Preprint; format changed, typos corrected
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have recently introduced effective methods of performing Image-to-Image translations. These models can be applied and generalized to a variety of domains in Image-to-Image translation without changing any parameters. In this paper, we survey and analyze eight Image-to-Image Generative Adversarial Networks: Pix2Pix, CycleGAN, CoGAN, StarGAN, MUNIT, StarGAN2, DA-GAN, and Self Attention GAN. Each of these models presented state-of-the-art results and introduced new techniques to build Image-to-Image GANs. In addition to a survey of the models, we also survey the 18 datasets they were trained on and the 9 metrics they were evaluated on. Finally, we present results of a controlled experiment for 6 of these models on a common set of metrics and datasets. The results were mixed and showed that, on certain datasets, tasks, and metrics, some models outperformed others. The last section of this paper discusses those results and establishes areas of future research. As researchers continue to innovate new Image-to-Image GANs, it is important to gain a good understanding of the existing methods, datasets, and metrics. This paper provides a comprehensive overview and discussion to help build this foundation.



### InDuDoNet+: A Deep Unfolding Dual Domain Network for Metal Artifact Reduction in CT Images
- **Arxiv ID**: http://arxiv.org/abs/2112.12660v2
- **DOI**: 10.1016/j.media.2022.102729
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.12660v2)
- **Published**: 2021-12-23 15:52:37+00:00
- **Updated**: 2022-12-26 12:38:15+00:00
- **Authors**: Hong Wang, Yuexiang Li, Haimiao Zhang, Deyu Meng, Yefeng Zheng
- **Comment**: None
- **Journal**: Medical Image Analysis 2022
- **Summary**: During the computed tomography (CT) imaging process, metallic implants within patients often cause harmful artifacts, which adversely degrade the visual quality of reconstructed CT images and negatively affect the subsequent clinical diagnosis. For the metal artifact reduction (MAR) task, current deep learning based methods have achieved promising performance. However, most of them share two main common limitations: 1) the CT physical imaging geometry constraint is not comprehensively incorporated into deep network structures; 2) the entire framework has weak interpretability for the specific MAR task; hence, the role of each network module is difficult to be evaluated. To alleviate these issues, in the paper, we construct a novel deep unfolding dual domain network, termed InDuDoNet+, into which CT imaging process is finely embedded. Concretely, we derive a joint spatial and Radon domain reconstruction model and propose an optimization algorithm with only simple operators for solving it. By unfolding the iterative steps involved in the proposed algorithm into the corresponding network modules, we easily build the InDuDoNet+ with clear interpretability. Furthermore, we analyze the CT values among different tissues, and merge the prior observations into a prior network for our InDuDoNet+, which significantly improve its generalization performance.Comprehensive experiments on synthesized data and clinical data substantiate the superiority of the proposed methods as well as the superior generalization performance beyond the current state-of-the-art (SOTA) MAR methods. . Code is available at \url{https://github.com/hongwang01/InDuDoNet_plus}.



### Omni-Seg: A Single Dynamic Network for Multi-label Renal Pathology Image Segmentation using Partially Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/2112.12665v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.12665v2)
- **Published**: 2021-12-23 16:02:03+00:00
- **Updated**: 2022-03-23 17:59:15+00:00
- **Authors**: Ruining Deng, Quan Liu, Can Cui, Zuhayr Asad, Haichun Yang, Yuankai Huo
- **Comment**: None
- **Journal**: None
- **Summary**: Computer-assisted quantitative analysis on Giga-pixel pathology images has provided a new avenue in histology examination. The innovations have been largely focused on cancer pathology (i.e., tumor segmentation and characterization). In non-cancer pathology, the learning algorithms can be asked to examine more comprehensive tissue types simultaneously, as a multi-label setting. The prior arts typically needed to train multiple segmentation networks in order to match the domain-specific knowledge for heterogeneous tissue types (e.g., glomerular tuft, glomerular unit, proximal tubular, distal tubular, peritubular capillaries, and arteries). In this paper, we propose a dynamic single segmentation network (Omni-Seg) that learns to segment multiple tissue types using partially labeled images (i.e., only one tissue type is labeled for each training image) for renal pathology. By learning from ~150,000 patch-wise pathological images from six tissue types, the proposed Omni-Seg network achieved superior segmentation accuracy and less resource consumption when compared to the previous the multiple-network and multi-head design. In the testing stage, the proposed method obtains "completely labeled" tissue segmentation results using only "partially labeled" training images. The source code is available at https://github.com/ddrrnn123/Omni-Seg



### 3D Skeleton-based Few-shot Action Recognition with JEANIE is not so Naïve
- **Arxiv ID**: http://arxiv.org/abs/2112.12668v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.12668v1)
- **Published**: 2021-12-23 16:09:23+00:00
- **Updated**: 2021-12-23 16:09:23+00:00
- **Authors**: Lei Wang, Jun Liu, Piotr Koniusz
- **Comment**: Full 17 page version
- **Journal**: None
- **Summary**: In this paper, we propose a Few-shot Learning pipeline for 3D skeleton-based action recognition by Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE). To factor out misalignment between query and support sequences of 3D body joints, we propose an advanced variant of Dynamic Time Warping which jointly models each smooth path between the query and support frames to achieve simultaneously the best alignment in the temporal and simulated camera viewpoint spaces for end-to-end learning under the limited few-shot training data. Sequences are encoded with a temporal block encoder based on Simple Spectral Graph Convolution, a lightweight linear Graph Neural Network backbone (we also include a setting with a transformer). Finally, we propose a similarity-based loss which encourages the alignment of sequences of the same class while preventing the alignment of unrelated sequences. We demonstrate state-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D Multiview Activity II.



### TagLab: A human-centric AI system for interactive semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.12702v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, I.2.1; I.3.6
- **Links**: [PDF](http://arxiv.org/pdf/2112.12702v1)
- **Published**: 2021-12-23 16:50:06+00:00
- **Updated**: 2021-12-23 16:50:06+00:00
- **Authors**: Gaia Pavoni, Massimiliano Corsini, Federico Ponchio, Alessandro Muntoni, Paolo Cignoni
- **Comment**: Accepted at Human Centered AI workshop at NeurIPS 2021,
  https://sites.google.com/view/hcai-human-centered-ai-neurips/home
- **Journal**: None
- **Summary**: Fully automatic semantic segmentation of highly specific semantic classes and complex shapes may not meet the accuracy standards demanded by scientists. In such cases, human-centered AI solutions, able to assist operators while preserving human control over complex tasks, are a good trade-off to speed up image labeling while maintaining high accuracy levels. TagLab is an open-source AI-assisted software for annotating large orthoimages which takes advantage of different degrees of automation; it speeds up image annotation from scratch through assisted tools, creates custom fully automatic semantic segmentation models, and, finally, allows the quick edits of automatic predictions. Since the orthoimages analysis applies to several scientific disciplines, TagLab has been designed with a flexible labeling pipeline. We report our results in two different scenarios, marine ecology, and architectural heritage.



### Digital Editions as Distant Supervision for Layout Analysis of Printed Books
- **Arxiv ID**: http://arxiv.org/abs/2112.12703v1
- **DOI**: 10.1007/978-3-030-86331-9_30
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12703v1)
- **Published**: 2021-12-23 16:51:53+00:00
- **Updated**: 2021-12-23 16:51:53+00:00
- **Authors**: Alejandro H. Toselli, Si Wu, David A. Smith
- **Comment**: 15 pages, 2 figures. International Conference on Document Analysis
  and Recognition. Springer, Cham, 2021
- **Journal**: None
- **Summary**: Archivists, textual scholars, and historians often produce digital editions of historical documents. Using markup schemes such as those of the Text Encoding Initiative and EpiDoc, these digital editions often record documents' semantic regions (such as notes and figures) and physical features (such as page and line breaks) as well as transcribing their textual content. We describe methods for exploiting this semantic markup as distant supervision for training and evaluating layout analysis models. In experiments with several model architectures on the half-million pages of the Deutsches Textarchiv (DTA), we find a high correlation of these region-level evaluation methods with pixel-level and word-level metrics. We discuss the possibilities for improving accuracy with self-training and the ability of models trained on the DTA to generalize to other historical printed books.



### AI-based Reconstruction for Fast MRI -- A Systematic Review and Meta-analysis
- **Arxiv ID**: http://arxiv.org/abs/2112.12744v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2112.12744v1)
- **Published**: 2021-12-23 17:56:41+00:00
- **Updated**: 2021-12-23 17:56:41+00:00
- **Authors**: Yutong Chen, Carola-Bibiane Schönlieb, Pietro Liò, Tim Leiner, Pier Luigi Dragotti, Ge Wang, Daniel Rueckert, David Firmin, Guang Yang
- **Comment**: 42 pages, 5 figures, Proceedings of the IEEE
- **Journal**: None
- **Summary**: Compressed sensing (CS) has been playing a key role in accelerating the magnetic resonance imaging (MRI) acquisition process. With the resurgence of artificial intelligence, deep neural networks and CS algorithms are being integrated to redefine the state of the art of fast MRI. The past several years have witnessed substantial growth in the complexity, diversity, and performance of deep learning-based CS techniques that are dedicated to fast MRI. In this meta-analysis, we systematically review the deep learning-based CS techniques for fast MRI, describe key model designs, highlight breakthroughs, and discuss promising directions. We have also introduced a comprehensive analysis framework and a classification system to assess the pivotal role of deep learning in CS-based acceleration for MRI.



### Assessing the Impact of Attention and Self-Attention Mechanisms on the Classification of Skin Lesions
- **Arxiv ID**: http://arxiv.org/abs/2112.12748v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2112.12748v1)
- **Published**: 2021-12-23 18:02:48+00:00
- **Updated**: 2021-12-23 18:02:48+00:00
- **Authors**: Rafael Pedro, Arlindo L. Oliveira
- **Comment**: None
- **Journal**: None
- **Summary**: Attention mechanisms have raised significant interest in the research community, since they promise significant improvements in the performance of neural network architectures. However, in any specific problem, we still lack a principled way to choose specific mechanisms and hyper-parameters that lead to guaranteed improvements. More recently, self-attention has been proposed and widely used in transformer-like architectures, leading to significant breakthroughs in some applications. In this work we focus on two forms of attention mechanisms: attention modules and self-attention. Attention modules are used to reweight the features of each layer input tensor. Different modules have different ways to perform this reweighting in fully connected or convolutional layers. The attention models studied are completely modular and in this work they will be used with the popular ResNet architecture. Self-Attention, originally proposed in the area of Natural Language Processing makes it possible to relate all the items in an input sequence. Self-Attention is becoming increasingly popular in Computer Vision, where it is sometimes combined with convolutional layers, although some recent architectures do away entirely with convolutions. In this work, we study and perform an objective comparison of a number of different attention mechanisms in a specific computer vision task, the classification of samples in the widely used Skin Cancer MNIST dataset. The results show that attention modules do sometimes improve the performance of convolutional neural network architectures, but also that this improvement, although noticeable and statistically significant, is not consistent in different settings. The results obtained with self-attention mechanisms, on the other hand, show consistent and significant improvements, leading to the best results even in architectures with a reduced number of parameters.



### SLIP: Self-supervision meets Language-Image Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2112.12750v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12750v1)
- **Published**: 2021-12-23 18:07:13+00:00
- **Updated**: 2021-12-23 18:07:13+00:00
- **Authors**: Norman Mu, Alexander Kirillov, David Wagner, Saining Xie
- **Comment**: Code: https://github.com/facebookresearch/SLIP
- **Journal**: None
- **Summary**: Recent work has shown that self-supervised pre-training leads to improvements over supervised learning on challenging visual recognition tasks. CLIP, an exciting new approach to learning with language supervision, demonstrates promising performance on a wide variety of benchmarks. In this work, we explore whether self-supervised learning can aid in the use of language supervision for visual representation learning. We introduce SLIP, a multi-task learning framework for combining self-supervised learning and CLIP pre-training. After pre-training with Vision Transformers, we thoroughly evaluate representation quality and compare performance to both CLIP and self-supervised learning under three distinct settings: zero-shot transfer, linear classification, and end-to-end finetuning. Across ImageNet and a battery of additional datasets, we find that SLIP improves accuracy by a large margin. We validate our results further with experiments on different model sizes, training schedules, and pre-training datasets. Our findings show that SLIP enjoys the best of both worlds: better performance than self-supervision (+8.1% linear accuracy) and language supervision (+5.2% zero-shot accuracy).



### BANMo: Building Animatable 3D Neural Models from Many Casual Videos
- **Arxiv ID**: http://arxiv.org/abs/2112.12761v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2112.12761v3)
- **Published**: 2021-12-23 18:30:31+00:00
- **Updated**: 2023-04-03 13:57:31+00:00
- **Authors**: Gengshan Yang, Minh Vo, Natalia Neverova, Deva Ramanan, Andrea Vedaldi, Hanbyul Joo
- **Comment**: CVPR 2022 camera-ready version (last update: May 2022)
- **Journal**: None
- **Summary**: Prior work for articulated 3D shape reconstruction often relies on specialized sensors (e.g., synchronized multi-camera systems), or pre-built 3D deformable models (e.g., SMAL or SMPL). Such methods are not able to scale to diverse sets of objects in the wild. We present BANMo, a method that requires neither a specialized sensor nor a pre-defined template shape. BANMo builds high-fidelity, articulated 3D models (including shape and animatable skinning weights) from many monocular casual videos in a differentiable rendering framework. While the use of many videos provides more coverage of camera views and object articulations, they introduce significant challenges in establishing correspondence across scenes with different backgrounds, illumination conditions, etc. Our key insight is to merge three schools of thought; (1) classic deformable shape models that make use of articulated bones and blend skinning, (2) volumetric neural radiance fields (NeRFs) that are amenable to gradient-based optimization, and (3) canonical embeddings that generate correspondences between pixels and an articulated model. We introduce neural blend skinning models that allow for differentiable and invertible articulated deformations. When combined with canonical embeddings, such models allow us to establish dense correspondences across videos that can be self-supervised with cycle consistency. On real and synthetic datasets, BANMo shows higher-fidelity 3D reconstructions than prior works for humans and animals, with the ability to render realistic images from novel viewpoints and poses. Project webpage: banmo-www.github.io .



### Cross Modal Retrieval with Querybank Normalisation
- **Arxiv ID**: http://arxiv.org/abs/2112.12777v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12777v3)
- **Published**: 2021-12-23 18:51:58+00:00
- **Updated**: 2022-04-18 19:41:32+00:00
- **Authors**: Simion-Vlad Bogolin, Ioana Croitoru, Hailin Jin, Yang Liu, Samuel Albanie
- **Comment**: Accepted at CVPR 2022
- **Journal**: None
- **Summary**: Profiting from large-scale training datasets, advances in neural architecture design and efficient inference, joint embeddings have become the dominant approach for tackling cross-modal retrieval. In this work we first show that, despite their effectiveness, state-of-the-art joint embeddings suffer significantly from the longstanding "hubness problem" in which a small number of gallery embeddings form the nearest neighbours of many queries. Drawing inspiration from the NLP literature, we formulate a simple but effective framework called Querybank Normalisation (QB-Norm) that re-normalises query similarities to account for hubs in the embedding space. QB-Norm improves retrieval performance without requiring retraining. Differently from prior work, we show that QB-Norm works effectively without concurrent access to any test set queries. Within the QB-Norm framework, we also propose a novel similarity normalisation method, the Dynamic Inverted Softmax, that is significantly more robust than existing approaches. We showcase QB-Norm across a range of cross modal retrieval models and benchmarks where it consistently enhances strong baselines beyond the state of the art. Code is available at https://vladbogo.github.io/QB-Norm/.



### SeMask: Semantically Masked Transformers for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.12782v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.12782v3)
- **Published**: 2021-12-23 18:56:02+00:00
- **Updated**: 2022-04-13 09:30:58+00:00
- **Authors**: Jitesh Jain, Anukriti Singh, Nikita Orlov, Zilong Huang, Jiachen Li, Steven Walton, Humphrey Shi
- **Comment**: Updated experiments with Mix-Transformer (MiT) on ADE20K and added an
  analysis section
- **Journal**: None
- **Summary**: Finetuning a pretrained backbone in the encoder part of an image transformer network has been the traditional approach for the semantic segmentation task. However, such an approach leaves out the semantic context that an image provides during the encoding stage. This paper argues that incorporating semantic information of the image into pretrained hierarchical transformer-based backbones while finetuning improves the performance considerably. To achieve this, we propose SeMask, a simple and effective framework that incorporates semantic information into the encoder with the help of a semantic attention operation. In addition, we use a lightweight semantic decoder during training to provide supervision to the intermediate semantic prior maps at every stage. Our experiments demonstrate that incorporating semantic priors enhances the performance of the established hierarchical encoders with a slight increase in the number of FLOPs. We provide empirical proof by integrating SeMask into Swin Transformer and Mix Transformer backbones as our encoder paired with different decoders. Our framework achieves a new state-of-the-art of 58.25% mIoU on the ADE20K dataset and improvements of over 3% in the mIoU metric on the Cityscapes dataset. The code and checkpoints are publicly available at https://github.com/Picsart-AI-Research/SeMask-Segmentation .



### NinjaDesc: Content-Concealing Visual Descriptors via Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.12785v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12785v2)
- **Published**: 2021-12-23 18:58:58+00:00
- **Updated**: 2022-03-29 16:06:02+00:00
- **Authors**: Tony Ng, Hyo Jin Kim, Vincent Lee, Daniel DeTone, Tsun-Yi Yang, Tianwei Shen, Eddy Ilg, Vassileios Balntas, Krystian Mikolajczyk, Chris Sweeney
- **Comment**: Accepted at CVPR 2022. Supplementary material included after
  references. 15 pages, 14 figures, 6 tables
- **Journal**: None
- **Summary**: In the light of recent analyses on privacy-concerning scene revelation from visual descriptors, we develop descriptors that conceal the input image content. In particular, we propose an adversarial learning framework for training visual descriptors that prevent image reconstruction, while maintaining the matching accuracy. We let a feature encoding network and image reconstruction network compete with each other, such that the feature encoder tries to impede the image reconstruction with its generated descriptors, while the reconstructor tries to recover the input image from the descriptors. The experimental results demonstrate that the visual descriptors obtained with our method significantly deteriorate the image reconstruction quality with minimal impact on correspondence matching and camera localization performance.



### ELSA: Enhanced Local Self-Attention for Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2112.12786v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2112.12786v1)
- **Published**: 2021-12-23 18:59:48+00:00
- **Updated**: 2021-12-23 18:59:48+00:00
- **Authors**: Jingkai Zhou, Pichao Wang, Fan Wang, Qiong Liu, Hao Li, Rong Jin
- **Comment**: Project at \url{https://github.com/damo-cv/ELSA}
- **Journal**: None
- **Summary**: Self-attention is powerful in modeling long-range dependencies, but it is weak in local finer-level feature learning. The performance of local self-attention (LSA) is just on par with convolution and inferior to dynamic filters, which puzzles researchers on whether to use LSA or its counterparts, which one is better, and what makes LSA mediocre. To clarify these, we comprehensively investigate LSA and its counterparts from two sides: \emph{channel setting} and \emph{spatial processing}. We find that the devil lies in the generation and application of spatial attention, where relative position embeddings and the neighboring filter application are key factors. Based on these findings, we propose the enhanced local self-attention (ELSA) with Hadamard attention and the ghost head. Hadamard attention introduces the Hadamard product to efficiently generate attention in the neighboring case, while maintaining the high-order mapping. The ghost head combines attention maps with static matrices to increase channel capacity. Experiments demonstrate the effectiveness of ELSA. Without architecture / hyperparameter modification, drop-in replacing LSA with ELSA boosts Swin Transformer \cite{swin} by up to +1.4 on top-1 accuracy. ELSA also consistently benefits VOLO \cite{volo} from D1 to D5, where ELSA-VOLO-D5 achieves 87.2 on the ImageNet-1K without extra training images. In addition, we evaluate ELSA in downstream tasks. ELSA significantly improves the baseline by up to +1.9 box Ap / +1.3 mask Ap on the COCO, and by up to +1.9 mIoU on the ADE20K. Code is available at \url{https://github.com/damo-cv/ELSA}.



### Self-Attention Generative Adversarial Network for Iterative Reconstruction of CT Images
- **Arxiv ID**: http://arxiv.org/abs/2112.12810v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.12810v1)
- **Published**: 2021-12-23 19:20:38+00:00
- **Updated**: 2021-12-23 19:20:38+00:00
- **Authors**: Ruiwen Xing, Thomas Humphries, Dong Si
- **Comment**: 16 pages, 8 figures, 5 tables
- **Journal**: None
- **Summary**: Computed tomography (CT) uses X-ray measurements taken from sensors around the body to generate tomographic images of the human body. Conventional reconstruction algorithms can be used if the X-ray data are adequately sampled and of high quality; however, concerns such as reducing dose to the patient, or geometric limitations on data acquisition, may result in low quality or incomplete data. Images reconstructed from these data using conventional methods are of poor quality, due to noise and other artifacts. The aim of this study is to train a single neural network to reconstruct high-quality CT images from noisy or incomplete CT scan data, including low-dose, sparse-view, and limited-angle scenarios. To accomplish this task, we train a generative adversarial network (GAN) as a signal prior, to be used in conjunction with the iterative simultaneous algebraic reconstruction technique (SART) for CT data. The network includes a self-attention block to model long-range dependencies in the data. We compare our Self-Attention GAN for CT image reconstruction with several state-of-the-art approaches, including denoising cycle GAN, CIRCLE GAN, and a total variation superiorized algorithm. Our approach is shown to have comparable overall performance to CIRCLE GAN, while outperforming the other two approaches.



### MDN-VO: Estimating Visual Odometry with Confidence
- **Arxiv ID**: http://arxiv.org/abs/2112.12812v1
- **DOI**: 10.1109/IROS51168.2021.9636827
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.12812v1)
- **Published**: 2021-12-23 19:26:04+00:00
- **Updated**: 2021-12-23 19:26:04+00:00
- **Authors**: Nimet Kaygusuz, Oscar Mendez, Richard Bowden
- **Comment**: None
- **Journal**: 2021 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), 2021, pp. 3528-3533
- **Summary**: Visual Odometry (VO) is used in many applications including robotics and autonomous systems. However, traditional approaches based on feature matching are computationally expensive and do not directly address failure cases, instead relying on heuristic methods to detect failure. In this work, we propose a deep learning-based VO model to efficiently estimate 6-DoF poses, as well as a confidence model for these estimates. We utilise a CNN - RNN hybrid model to learn feature representations from image sequences. We then employ a Mixture Density Network (MDN) which estimates camera motion as a mixture of Gaussians, based on the extracted spatio-temporal representations. Our model uses pose labels as a source of supervision, but derives uncertainties in an unsupervised manner. We evaluate the proposed model on the KITTI and nuScenes datasets and report extensive quantitative and qualitative results to analyse the performance of both pose and uncertainty estimation. Our experiments show that the proposed model exceeds state-of-the-art performance in addition to detecting failure cases using the predicted pose uncertainty.



### Multi-Camera Sensor Fusion for Visual Odometry using Deep Uncertainty Estimation
- **Arxiv ID**: http://arxiv.org/abs/2112.12818v1
- **DOI**: 10.1109/ITSC48978.2021.9565079
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.12818v1)
- **Published**: 2021-12-23 19:44:45+00:00
- **Updated**: 2021-12-23 19:44:45+00:00
- **Authors**: Nimet Kaygusuz, Oscar Mendez, Richard Bowden
- **Comment**: None
- **Journal**: 2021 IEEE International Intelligent Transportation Systems
  Conference (ITSC), 2021, pp. 2944-2949
- **Summary**: Visual Odometry (VO) estimation is an important source of information for vehicle state estimation and autonomous driving. Recently, deep learning based approaches have begun to appear in the literature. However, in the context of driving, single sensor based approaches are often prone to failure because of degraded image quality due to environmental factors, camera placement, etc. To address this issue, we propose a deep sensor fusion framework which estimates vehicle motion using both pose and uncertainty estimations from multiple on-board cameras. We extract spatio-temporal feature representations from a set of consecutive images using a hybrid CNN - RNN model. We then utilise a Mixture Density Network (MDN) to estimate the 6-DoF pose as a mixture of distributions and a fusion module to estimate the final pose using MDN outputs from multi-cameras. We evaluate our approach on the publicly available, large scale autonomous vehicle dataset, nuScenes. The results show that the proposed fusion approach surpasses the state-of-the-art, and provides robust estimates and accurate trajectories compared to individual camera-based estimations.



### Dense Out-of-Distribution Detection by Robust Learning on Synthetic Negative Data
- **Arxiv ID**: http://arxiv.org/abs/2112.12833v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12833v3)
- **Published**: 2021-12-23 20:35:10+00:00
- **Updated**: 2023-07-31 09:35:22+00:00
- **Authors**: Matej Grcić, Petra Bevandić, Zoran Kalafatić, Siniša Šegvić
- **Comment**: None
- **Journal**: None
- **Summary**: Standard machine learning is unable to accommodate inputs which do not belong to the training distribution. The resulting models often give rise to confident incorrect predictions which may lead to devastating consequences. This problem is especially demanding in the context of dense prediction since input images may be only partially anomalous. Previous work has addressed dense out-of-distribution detection by discriminative training with respect to off-the-shelf negative datasets. However, real negative data are unlikely to cover all modes of the entire visual world. To this end, we extend this approach by generating synthetic negative patches along the border of the inlier manifold. We leverage a jointly trained normalizing flow due to coverage-oriented learning objective and the capability to generate samples at different resolutions. We detect anomalies according to a principled information-theoretic criterion which can be consistently applied through training and inference. The resulting models set the new state of the art on benchmarks for out-of-distribution detection in road-driving scenes and remote sensing imagery, in spite of minimal computational overhead.



### Faster Deep Ensemble Averaging for Quantification of DNA Damage from Comet Assay Images With Uncertainty Estimates
- **Arxiv ID**: http://arxiv.org/abs/2112.12839v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.12839v1)
- **Published**: 2021-12-23 20:48:28+00:00
- **Updated**: 2021-12-23 20:48:28+00:00
- **Authors**: Srikanth Namuduri, Prateek Mehta, Lise Barbe, Stephanie Lam, Zohreh Faghihmonzavi, Steve Finkbeiner, Shekhar Bhansali
- **Comment**: None
- **Journal**: None
- **Summary**: Several neurodegenerative diseases involve the accumulation of cellular DNA damage. Comet assays are a popular way of estimating the extent of DNA damage. Current literature on the use of deep learning to quantify DNA damage presents an empirical approach to hyper-parameter optimization and does not include uncertainty estimates. Deep ensemble averaging is a standard approach to estimating uncertainty but it requires several iterations of network training, which makes it time-consuming. Here we present an approach to quantify the extent of DNA damage that combines deep learning with a rigorous and comprehensive method to optimize the hyper-parameters with the help of statistical tests. We also use an architecture that allows for a faster computation of deep ensemble averaging and performs statistical tests applicable to networks using transfer learning. We applied our approach to a comet assay dataset with more than 1300 images and achieved an $R^2$ of 0.84, where the output included the confidence interval for each prediction. The proposed architecture is an improvement over the current approaches since it speeds up the uncertainty estimation by 30X while being statistically more rigorous.



### Impact of class imbalance on chest x-ray classifiers: towards better evaluation practices for discrimination and calibration performance
- **Arxiv ID**: http://arxiv.org/abs/2112.12843v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12843v2)
- **Published**: 2021-12-23 20:57:47+00:00
- **Updated**: 2022-03-14 12:29:28+00:00
- **Authors**: Candelaria Mosquera, Luciana Ferrer, Diego Milone, Daniel Luna, Enzo Ferrante
- **Comment**: Conference on Health, Inference, and Learning (CHIL) 2022 - Invited
  non-archival presentation
- **Journal**: None
- **Summary**: This work aims to analyze standard evaluation practices adopted by the research community when assessing chest x-ray classifiers, particularly focusing on the impact of class imbalance in such appraisals. Our analysis considers a comprehensive definition of model performance, covering not only discriminative performance but also model calibration, a topic of research that has received increasing attention during the last years within the machine learning community. Firstly, we conducted a literature study to analyze common scientific practices and confirmed that: (1) even when dealing with highly imbalanced datasets, the community tends to use metrics that are dominated by the majority class; and (2) it is still uncommon to include calibration studies for chest x-ray classifiers, albeit its importance in the context of healthcare. Secondly, we perform a systematic experiment on two major chest x-ray datasets to explore the behavior of several performance metrics under different class ratios and show that widely adopted metrics can conceal the performance in the minority class. Finally, we recommend the inclusion of complementary metrics to better reflect the system's performance in such scenarios. Our study indicates that current evaluation practices adopted by the research community for chest x-ray computer-aided diagnosis systems may not reflect their performance in real clinical scenarios, and suggest alternatives to improve this situation.



### HSPACE: Synthetic Parametric Humans Animated in Complex Environments
- **Arxiv ID**: http://arxiv.org/abs/2112.12867v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12867v2)
- **Published**: 2021-12-23 22:27:55+00:00
- **Updated**: 2022-01-06 11:45:09+00:00
- **Authors**: Eduard Gabriel Bazavan, Andrei Zanfir, Mihai Zanfir, William T. Freeman, Rahul Sukthankar, Cristian Sminchisescu
- **Comment**: None
- **Journal**: None
- **Summary**: Advances in the state of the art for 3d human sensing are currently limited by the lack of visual datasets with 3d ground truth, including multiple people, in motion, operating in real-world environments, with complex illumination or occlusion, and potentially observed by a moving camera. Sophisticated scene understanding would require estimating human pose and shape as well as gestures, towards representations that ultimately combine useful metric and behavioral signals with free-viewpoint photo-realistic visualisation capabilities. To sustain progress, we build a large-scale photo-realistic dataset, Human-SPACE (HSPACE), of animated humans placed in complex synthetic indoor and outdoor environments. We combine a hundred diverse individuals of varying ages, gender, proportions, and ethnicity, with hundreds of motions and scenes, as well as parametric variations in body shape (for a total of 1,600 different humans), in order to generate an initial dataset of over 1 million frames. Human animations are obtained by fitting an expressive human body model, GHUM, to single scans of people, followed by novel re-targeting and positioning procedures that support the realistic animation of dressed humans, statistical variation of body proportions, and jointly consistent scene placement of multiple moving people. Assets are generated automatically, at scale, and are compatible with existing real time rendering and game engines. The dataset with evaluation server will be made available for research. Our large-scale analysis of the impact of synthetic data, in connection with real data and weak supervision, underlines the considerable potential for continuing quality improvements and limiting the sim-to-real gap, in this practical setting, in connection with increased model capacity.



