# Arxiv Papers in cs.CV on 2021-12-22
### AdaptPose: Cross-Dataset Adaptation for 3D Human Pose Estimation by Learnable Motion Generation
- **Arxiv ID**: http://arxiv.org/abs/2112.11593v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11593v2)
- **Published**: 2021-12-22 00:27:52+00:00
- **Updated**: 2022-03-15 18:21:16+00:00
- **Authors**: Mohsen Gholami, Bastian Wandt, Helge Rhodin, Rabab Ward, Z. Jane Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of cross-dataset generalization of 3D human pose estimation models. Testing a pre-trained 3D pose estimator on a new dataset results in a major performance drop. Previous methods have mainly addressed this problem by improving the diversity of the training data. We argue that diversity alone is not sufficient and that the characteristics of the training data need to be adapted to those of the new dataset such as camera viewpoint, position, human actions, and body size. To this end, we propose AdaptPose, an end-to-end framework that generates synthetic 3D human motions from a source dataset and uses them to fine-tune a 3D pose estimator. AdaptPose follows an adversarial training scheme. From a source 3D pose the generator generates a sequence of 3D poses and a camera orientation that is used to project the generated poses to a novel view. Without any 3D labels or camera information AdaptPose successfully learns to create synthetic 3D poses from the target dataset while only being trained on 2D poses. In experiments on the Human3.6M, MPI-INF-3DHP, 3DPW, and Ski-Pose datasets our method outperforms previous work in cross-dataset evaluations by 14% and previous semi-supervised learning methods that use partial 3D annotations by 16%.



### EyePAD++: A Distillation-based approach for joint Eye Authentication and Presentation Attack Detection using Periocular Images
- **Arxiv ID**: http://arxiv.org/abs/2112.11610v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11610v2)
- **Published**: 2021-12-22 01:22:08+00:00
- **Updated**: 2021-12-29 03:24:05+00:00
- **Authors**: Prithviraj Dhar, Amit Kumar, Kirsten Kaplan, Khushi Gupta, Rakesh Ranjan, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: A practical eye authentication (EA) system targeted for edge devices needs to perform authentication and be robust to presentation attacks, all while remaining compute and latency efficient. However, existing eye-based frameworks a) perform authentication and Presentation Attack Detection (PAD) independently and b) involve significant pre-processing steps to extract the iris region. Here, we introduce a joint framework for EA and PAD using periocular images. While a deep Multitask Learning (MTL) network can perform both the tasks, MTL suffers from the forgetting effect since the training datasets for EA and PAD are disjoint. To overcome this, we propose Eye Authentication with PAD (EyePAD), a distillation-based method that trains a single network for EA and PAD while reducing the effect of forgetting. To further improve the EA performance, we introduce a novel approach called EyePAD++ that includes training an MTL network on both EA and PAD data, while distilling the `versatility' of the EyePAD network through an additional distillation step. Our proposed methods outperform the SOTA in PAD and obtain near-SOTA performance in eye-to-eye verification, without any pre-processing. We also demonstrate the efficacy of EyePAD and EyePAD++ in user-to-user verification with PAD across network backbones and image quality.



### MOSAIC: Mobile Segmentation via decoding Aggregated Information and encoded Context
- **Arxiv ID**: http://arxiv.org/abs/2112.11623v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.11623v1)
- **Published**: 2021-12-22 02:01:45+00:00
- **Updated**: 2021-12-22 02:01:45+00:00
- **Authors**: Weijun Wang, Andrew Howard
- **Comment**: None
- **Journal**: None
- **Summary**: We present a next-generation neural network architecture, MOSAIC, for efficient and accurate semantic image segmentation on mobile devices. MOSAIC is designed using commonly supported neural operations by diverse mobile hardware platforms for flexible deployment across various mobile platforms. With a simple asymmetric encoder-decoder structure which consists of an efficient multi-scale context encoder and a light-weight hybrid decoder to recover spatial details from aggregated information, MOSAIC achieves new state-of-the-art performance while balancing accuracy and computational cost. Deployed on top of a tailored feature extraction backbone based on a searched classification network, MOSAIC achieves a 5% absolute accuracy gain surpassing the current industry standard MLPerf models and state-of-the-art architectures.



### Convolutional neural network based on transfer learning for breast cancer screening
- **Arxiv ID**: http://arxiv.org/abs/2112.11629v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.11629v1)
- **Published**: 2021-12-22 02:27:12+00:00
- **Updated**: 2021-12-22 02:27:12+00:00
- **Authors**: Hussin Ragb, Redha Ali, Elforjani Jera, Nagi Buaossa
- **Comment**: 9 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:2009.08831
- **Journal**: None
- **Summary**: Breast cancer is the most common cancer in the world and the most prevalent cause of death among women worldwide. Nevertheless, it is also one of the most treatable malignancies if detected early. In this paper, a deep convolutional neural network-based algorithm is proposed to aid in accurately identifying breast cancer from ultrasonic images. In this algorithm, several neural networks are fused in a parallel architecture to perform the classification process and the voting criteria are applied in the final classification decision between the candidate object classes where the output of each neural network is representing a single vote. Several experiments were conducted on the breast ultrasound dataset consisting of 537 Benign, 360 malignant, and 133 normal images. These experiments show an optimistic result and a capability of the proposed model to outperform many state-of-the-art algorithms on several measures. Using k-fold cross-validation and a bagging classifier ensemble, we achieved an accuracy of 99.5% and a sensitivity of 99.6%.



### JoJoGAN: One Shot Face Stylization
- **Arxiv ID**: http://arxiv.org/abs/2112.11641v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11641v4)
- **Published**: 2021-12-22 03:13:16+00:00
- **Updated**: 2022-03-06 21:25:50+00:00
- **Authors**: Min Jin Chong, David Forsyth
- **Comment**: code at https://github.com/mchong6/JoJoGAN
- **Journal**: None
- **Summary**: A style mapper applies some fixed style to its input images (so, for example, taking faces to cartoons). This paper describes a simple procedure -- JoJoGAN -- to learn a style mapper from a single example of the style. JoJoGAN uses a GAN inversion procedure and StyleGAN's style-mixing property to produce a substantial paired dataset from a single example style. The paired dataset is then used to fine-tune a StyleGAN. An image can then be style mapped by GAN-inversion followed by the fine-tuned StyleGAN. JoJoGAN needs just one reference and as little as 30 seconds of training time. JoJoGAN can use extreme style references (say, animal faces) successfully. Furthermore, one can control what aspects of the style are used and how much of the style is applied. Qualitative and quantitative evaluation show that JoJoGAN produces high quality high resolution images that vastly outperform the current state-of-the-art.



### Exploring Credibility Scoring Metrics of Perception Systems for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2112.11643v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.11643v1)
- **Published**: 2021-12-22 03:17:14+00:00
- **Updated**: 2021-12-22 03:17:14+00:00
- **Authors**: Viren Khandal, Arth Vidyarthi
- **Comment**: In 14th International Conference on COMmunication Systems & NETworkS
  (COMSNETS) Intelligent Transportation Systems 2022
- **Journal**: None
- **Summary**: Autonomous and semi-autonomous vehicles' perception algorithms can encounter situations with erroneous object detection, such as misclassification of objects on the road, which can lead to safety violations and potentially fatal consequences. While there has been substantial work in the robustness of object detection algorithms and online metric learning, there is little research on benchmarking scoring metrics to determine any possible indicators of potential misclassification. An emphasis is put on exploring the potential of taking these scoring metrics online in order to allow the AV to make perception-based decisions given real-time constraints. In this work, we explore which, if any, metrics act as online indicators of when perception algorithms and object detectors are failing. Our work provides insight on better design principles and characteristics of online metrics to accurately evaluate the credibility of object detectors. Our approach employs non-adversarial and realistic perturbations to images, on which we evaluate various quantitative metrics. We found that offline metrics can be designed to account for real-world corruptions such as poor weather conditions and that the analysis of such metrics can provide a segue into designing online metrics. This is a clear next step as it can allow for error-free autonomous vehicle perception and safer time-critical and safety-critical decision-making.



### Out-of-distribution Detection with Boundary Aware Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.11648v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11648v3)
- **Published**: 2021-12-22 03:35:54+00:00
- **Updated**: 2022-07-08 06:32:39+00:00
- **Authors**: Sen Pei, Xin Zhang, Bin Fan, Gaofeng Meng
- **Comment**: None
- **Journal**: ECCV 2022 Poster
- **Summary**: There is an increasing need to determine whether inputs are out-of-distribution (\emph{OOD}) for safely deploying machine learning models in the open world scenario. Typical neural classifiers are based on the closed world assumption, where the training data and the test data are drawn \emph{i.i.d.} from the same distribution, and as a result, give over-confident predictions even faced with \emph{OOD} inputs. For tackling this problem, previous studies either use real outliers for training or generate synthetic \emph{OOD} data under strong assumptions, which are either costly or intractable to generalize. In this paper, we propose boundary aware learning (\textbf{BAL}), a novel framework that can learn the distribution of \emph{OOD} features adaptively. The key idea of BAL is to generate \emph{OOD} features from trivial to hard progressively with a generator, meanwhile, a discriminator is trained for distinguishing these synthetic \emph{OOD} features and in-distribution (\emph{ID}) features. Benefiting from the adversarial training scheme, the discriminator can well separate \emph{ID} and \emph{OOD} features, allowing more robust \emph{OOD} detection. The proposed BAL achieves \emph{state-of-the-art} performance on classification benchmarks, reducing up to 13.9\% FPR95 compared with previous methods.



### Ghost-dil-NetVLAD: A Lightweight Neural Network for Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.11679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11679v1)
- **Published**: 2021-12-22 06:05:02+00:00
- **Updated**: 2021-12-22 06:05:02+00:00
- **Authors**: Qingyuan Gong, Yu Liu, Liqiang Zhang, Renhe Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Visual place recognition (VPR) is a challenging task with the unbalance between enormous computational cost and high recognition performance. Thanks to the practical feature extraction ability of the lightweight convolution neural networks (CNNs) and the train-ability of the vector of locally aggregated descriptors (VLAD) layer, we propose a lightweight weakly supervised end-to-end neural network consisting of a front-ended perception model called GhostCNN and a learnable VLAD layer as a back-end. GhostCNN is based on Ghost modules that are lightweight CNN-based architectures. They can generate redundant feature maps using linear operations instead of the traditional convolution process, making a good trade-off between computation resources and recognition accuracy. To enhance our proposed lightweight model further, we add dilated convolutions to the Ghost module to get features containing more spatial semantic information, improving accuracy. Finally, rich experiments conducted on a commonly used public benchmark and our private dataset validate that the proposed neural network reduces the FLOPs and parameters of VGG16-NetVLAD by 99.04% and 80.16%, respectively. Besides, both models achieve similar accuracy.



### Cost Aggregation Is All You Need for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.11685v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11685v1)
- **Published**: 2021-12-22 06:18:51+00:00
- **Updated**: 2021-12-22 06:18:51+00:00
- **Authors**: Sunghwan Hong, Seokju Cho, Jisu Nam, Seungryong Kim
- **Comment**: The trained weights and codes are available at:
  https://seokju-cho.github.io/VAT/
- **Journal**: None
- **Summary**: We introduce a novel cost aggregation network, dubbed Volumetric Aggregation with Transformers (VAT), to tackle the few-shot segmentation task by using both convolutions and transformers to efficiently handle high dimensional correlation maps between query and support. In specific, we propose our encoder consisting of volume embedding module to not only transform the correlation maps into more tractable size but also inject some convolutional inductive bias and volumetric transformer module for the cost aggregation. Our encoder has a pyramidal structure to let the coarser level aggregation to guide the finer level and enforce to learn complementary matching scores. We then feed the output into our affinity-aware decoder along with the projected feature maps for guiding the segmentation process. Combining these components, we conduct experiments to demonstrate the effectiveness of the proposed method, and our method sets a new state-of-the-art for all the standard benchmarks in few-shot segmentation task. Furthermore, we find that the proposed method attains state-of-the-art performance even for the standard benchmarks in semantic correspondence task although not specifically designed for this task. We also provide an extensive ablation study to validate our architectural choices. The trained weights and codes are available at: https://seokju-cho.github.io/VAT/.



### Multi-Centroid Representation Network for Domain Adaptive Person Re-ID
- **Arxiv ID**: http://arxiv.org/abs/2112.11689v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11689v1)
- **Published**: 2021-12-22 06:40:21+00:00
- **Updated**: 2021-12-22 06:40:21+00:00
- **Authors**: Yuhang Wu, Tengteng Huang, Haotian Yao, Chi Zhang, Yuanjie Shao, Chuchu Han, Changxin Gao, Nong Sang
- **Comment**: Accepted by AAAI2022
- **Journal**: None
- **Summary**: Recently, many approaches tackle the Unsupervised Domain Adaptive person re-identification (UDA re-ID) problem through pseudo-label-based contrastive learning. During training, a uni-centroid representation is obtained by simply averaging all the instance features from a cluster with the same pseudo label. However, a cluster may contain images with different identities (label noises) due to the imperfect clustering results, which makes the uni-centroid representation inappropriate. In this paper, we present a novel Multi-Centroid Memory (MCM) to adaptively capture different identity information within the cluster. MCM can effectively alleviate the issue of label noises by selecting proper positive/negative centroids for the query image. Moreover, we further propose two strategies to improve the contrastive learning process. First, we present a Domain-Specific Contrastive Learning (DSCL) mechanism to fully explore intradomain information by comparing samples only from the same domain. Second, we propose Second-Order Nearest Interpolation (SONI) to obtain abundant and informative negative samples. We integrate MCM, DSCL, and SONI into a unified framework named Multi-Centroid Representation Network (MCRN). Extensive experiments demonstrate the superiority of MCRN over state-of-the-art approaches on multiple UDA re-ID tasks and fully unsupervised re-ID tasks.



### Comprehensive Visual Question Answering on Point Clouds through Compositional Scene Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2112.11691v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11691v3)
- **Published**: 2021-12-22 06:43:21+00:00
- **Updated**: 2023-05-22 02:55:52+00:00
- **Authors**: Xu Yan, Zhihao Yuan, Yuhao Du, Yinghong Liao, Yao Guo, Zhen Li, Shuguang Cui
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Question Answering on 3D Point Cloud (VQA-3D) is an emerging yet challenging field that aims at answering various types of textual questions given an entire point cloud scene. To tackle this problem, we propose the CLEVR3D, a large-scale VQA-3D dataset consisting of 171K questions from 8,771 3D scenes. Specifically, we develop a question engine leveraging 3D scene graph structures to generate diverse reasoning questions, covering the questions of objects' attributes (i.e., size, color, and material) and their spatial relationships. Through such a manner, we initially generated 44K questions from 1,333 real-world scenes. Moreover, a more challenging setup is proposed to remove the confounding bias and adjust the context from a common-sense layout. Such a setup requires the network to achieve comprehensive visual understanding when the 3D scene is different from the general co-occurrence context (e.g., chairs always exist with tables). To this end, we further introduce the compositional scene manipulation strategy and generate 127K questions from 7,438 augmented 3D scenes, which can improve VQA-3D models for real-world comprehension. Built upon the proposed dataset, we baseline several VQA-3D models, where experimental results verify that the CLEVR3D can significantly boost other 3D scene understanding tasks. Our code and dataset will be made publicly available at https://github.com/yanx27/CLEVR3D.



### Few-Shot Object Detection: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/2112.11699v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.11699v2)
- **Published**: 2021-12-22 07:08:53+00:00
- **Updated**: 2022-09-15 10:44:22+00:00
- **Authors**: Mona Köhler, Markus Eisenbach, Horst-Michael Gross
- **Comment**: 27 pages, 13 figures, submitted to IEEE Transactions on Neural
  Networks and Learning Systems
- **Journal**: None
- **Summary**: Humans are able to learn to recognize new objects even from a few examples. In contrast, training deep-learning-based object detectors requires huge amounts of annotated data. To avoid the need to acquire and annotate these huge amounts of data, few-shot object detection aims to learn from few object instances of new categories in the target domain. In this survey, we provide an overview of the state of the art in few-shot object detection. We categorize approaches according to their training scheme and architectural layout. For each type of approaches, we describe the general realization as well as concepts to improve the performance on novel categories. Whenever appropriate, we give short takeaways regarding these concepts in order to highlight the best ideas. Eventually, we introduce commonly used datasets and their evaluation protocols and analyze reported benchmark results. As a result, we emphasize common challenges in evaluation and identify the most promising current trends in this emerging field of few-shot object detection.



### Adaptive Contrast for Image Regression in Computer-Aided Disease Assessment
- **Arxiv ID**: http://arxiv.org/abs/2112.11700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11700v1)
- **Published**: 2021-12-22 07:13:02+00:00
- **Updated**: 2021-12-22 07:13:02+00:00
- **Authors**: Weihang Dai, Xiaomeng Li, Wan Hang Keith Chiu, Michael D. Kuo, Kwang-Ting Cheng
- **Comment**: Accepted in IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Image regression tasks for medical applications, such as bone mineral density (BMD) estimation and left-ventricular ejection fraction (LVEF) prediction, play an important role in computer-aided disease assessment. Most deep regression methods train the neural network with a single regression loss function like MSE or L1 loss. In this paper, we propose the first contrastive learning framework for deep image regression, namely AdaCon, which consists of a feature learning branch via a novel adaptive-margin contrastive loss and a regression prediction branch. Our method incorporates label distance relationships as part of the learned feature representations, which allows for better performance in downstream regression tasks. Moreover, it can be used as a plug-and-play module to improve performance of existing regression methods. We demonstrate the effectiveness of AdaCon on two medical image regression tasks, ie, bone mineral density estimation from X-ray images and left-ventricular ejection fraction prediction from echocardiogram videos. AdaCon leads to relative improvements of 3.3% and 5.9% in MAE over state-of-the-art BMD estimation and LVEF prediction methods, respectively.



### Entropy Regularized Iterative Weighted Shrinkage-Thresholding Algorithm (ERIWSTA): An Application to CT Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2112.11706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11706v1)
- **Published**: 2021-12-22 07:34:45+00:00
- **Updated**: 2021-12-22 07:34:45+00:00
- **Authors**: Bingxue Wu, Jiao Wei, Chen Li, Yudong Yao, Yueyang Teng
- **Comment**: None
- **Journal**: None
- **Summary**: The iterative weighted shrinkage-thresholding algorithm (IWSTA) has shown superiority to the classic unweighted iterative shrinkage-thresholding algorithm (ISTA) for solving linear inverse problems, which address the attributes differently. This paper proposes a new entropy regularized IWSTA (ERIWSTA) that adds an entropy regularizer to the cost function to measure the uncertainty of the weights to stimulate attributes to participate in problem solving. Then, the weights are solved with a Lagrange multiplier method to obtain a simple iterative update. The weights can be explained as the probability of the contribution of an attribute to the problem solution. Experimental results on CT image restoration show that the proposed method has better performance in terms of convergence speed and restoration accuracy than the existing methods.



### Fusion of medical imaging and electronic health records with attention and multi-head machanisms
- **Arxiv ID**: http://arxiv.org/abs/2112.11710v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11710v1)
- **Published**: 2021-12-22 07:39:26+00:00
- **Updated**: 2021-12-22 07:39:26+00:00
- **Authors**: Cheng Jiang, Yihao Chen, Jianbo Chang, Ming Feng, Renzhi Wang, Jianhua Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Doctors often make diagonostic decisions based on patient's image scans, such as magnetic resonance imaging (MRI), and patient's electronic health records (EHR) such as age, gender, blood pressure and so on. Despite a lot of automatic methods have been proposed for either image or text analysis in computer vision or natural language research areas, much fewer studies have been developed for the fusion of medical image and EHR data for medical problems. Among existing early or intermediate fusion methods, concatenation of features from both modalities is still a mainstream. For a better exploiting of image and EHR data, we propose a multi-modal attention module which use EHR data to help the selection of important regions during image feature extraction process conducted by traditional CNN. Moreover, we propose to incorporate multi-head machnism to gated multimodal unit (GMU) to make it able to parallelly fuse image and EHR features in different subspaces. With the help of the two modules, existing CNN architecture can be enhanced using both modalities. Experiments on predicting Glasgow outcome scale (GOS) of intracerebral hemorrhage patients and classifying Alzheimer's Disease showed the proposed method can automatically focus on task-related areas and achieve better results by making better use of image and EHR features.



### High-Accuracy RGB-D Face Recognition via Segmentation-Aware Face Depth Estimation and Mask-Guided Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2112.11713v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11713v1)
- **Published**: 2021-12-22 07:46:23+00:00
- **Updated**: 2021-12-22 07:46:23+00:00
- **Authors**: Meng-Tzu Chiu, Hsun-Ying Cheng, Chien-Yi Wang, Shang-Hong Lai
- **Comment**: IEEE International Conference on Automatic Face and Gesture
  Recognition (FG) 2021
- **Journal**: None
- **Summary**: Deep learning approaches have achieved highly accurate face recognition by training the models with very large face image datasets. Unlike the availability of large 2D face image datasets, there is a lack of large 3D face datasets available to the public. Existing public 3D face datasets were usually collected with few subjects, leading to the over-fitting problem. This paper proposes two CNN models to improve the RGB-D face recognition task. The first is a segmentation-aware depth estimation network, called DepthNet, which estimates depth maps from RGB face images by including semantic segmentation information for more accurate face region localization. The other is a novel mask-guided RGB-D face recognition model that contains an RGB recognition branch, a depth map recognition branch, and an auxiliary segmentation mask branch with a spatial attention module. Our DepthNet is used to augment a large 2D face image dataset to a large RGB-D face dataset, which is used for training an accurate RGB-D face recognition model. Furthermore, the proposed mask-guided RGB-D face recognition model can fully exploit the depth map and segmentation mask information and is more robust against pose variation than previous methods. Our experimental results show that DepthNet can produce more reliable depth maps from face images with the segmentation mask. Our mask-guided face recognition model outperforms state-of-the-art methods on several public 3D face datasets.



### Comparing radiologists' gaze and saliency maps generated by interpretability methods for chest x-rays
- **Arxiv ID**: http://arxiv.org/abs/2112.11716v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.11716v3)
- **Published**: 2021-12-22 07:57:18+00:00
- **Updated**: 2023-04-20 01:58:51+00:00
- **Authors**: Ricardo Bigolin Lanfredi, Ambuj Arora, Trafton Drew, Joyce D. Schroeder, Tolga Tasdizen
- **Comment**: This paper was presented as an Extended Abstract at the Gaze Meets ML
  2022 Workshop, a NeurIPS 2022 workshop
- **Journal**: None
- **Summary**: The interpretability of medical image analysis models is considered a key research field. We use a dataset of eye-tracking data from five radiologists to compare the outputs of interpretability methods and the heatmaps representing where radiologists looked. We conduct a class-independent analysis of the saliency maps generated by two methods selected from the literature: Grad-CAM and attention maps from an attention-gated model. For the comparison, we use shuffled metrics, which avoid biases from fixation locations. We achieve scores comparable to an interobserver baseline in one shuffled metric, highlighting the potential of saliency maps from Grad-CAM to mimic a radiologist's attention over an image. We also divide the dataset into subsets to evaluate in which cases similarities are higher.



### Generalized Local Optimality for Video Steganalysis in Motion Vector Domain
- **Arxiv ID**: http://arxiv.org/abs/2112.11729v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2112.11729v1)
- **Published**: 2021-12-22 08:44:12+00:00
- **Updated**: 2021-12-22 08:44:12+00:00
- **Authors**: Liming Zhai, Lina Wang, Yanzhen Ren, Yang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The local optimality of motion vectors (MVs) is an intrinsic property in video coding, and any modifications to the MVs will inevitably destroy this optimality, making it a sensitive indicator of steganography in the MV domain. Thus the local optimality is commonly used to design steganalytic features, and the estimation for local optimality has become a top priority in video steganalysis. However, the local optimality in existing works is often estimated inaccurately or using an unreasonable assumption, limiting its capability in steganalysis. In this paper, we propose to estimate the local optimality in a more reasonable and comprehensive fashion, and generalize the concept of local optimality in two aspects. First, the local optimality measured in a rate-distortion sense is jointly determined by MV and predicted motion vector (PMV), and the variability of PMV will affect the estimation for local optimality. Hence we generalize the local optimality from a static estimation to a dynamic one. Second, the PMV is a special case of MV, and can also reflect the embedding traces in MVs. So we generalize the local optimality from the MV domain to the PMV domain. Based on the two generalizations of local optimality, we construct new types of steganalytic features and also propose feature symmetrization rules to reduce feature dimension. Extensive experiments performed on three databases demonstrate the effectiveness of the proposed features, which achieve state-of-the-art in both accuracy and robustness in various conditions, including cover source mismatch, video prediction methods, video codecs, and video resolutions.



### Simple and Effective Balance of Contrastive Losses
- **Arxiv ID**: http://arxiv.org/abs/2112.11743v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.11743v1)
- **Published**: 2021-12-22 09:23:27+00:00
- **Updated**: 2021-12-22 09:23:27+00:00
- **Authors**: Arnaud Sors, Rafael Sampaio de Rezende, Sarah Ibrahimi, Jean-Marc Andreoli
- **Comment**: 15 pages, 10 figures
- **Journal**: None
- **Summary**: Contrastive losses have long been a key ingredient of deep metric learning and are now becoming more popular due to the success of self-supervised learning. Recent research has shown the benefit of decomposing such losses into two sub-losses which act in a complementary way when learning the representation network: a positive term and an entropy term. Although the overall loss is thus defined as a combination of two terms, the balance of these two terms is often hidden behind implementation details and is largely ignored and sub-optimal in practice. In this work, we approach the balance of contrastive losses as a hyper-parameter optimization problem, and propose a coordinate descent-based search method that efficiently find the hyper-parameters that optimize evaluation performance. In the process, we extend existing balance analyses to the contrastive margin loss, include batch size in the balance, and explain how to aggregate loss elements from the batch to maintain near-optimal performance over a larger range of batch sizes. Extensive experiments with benchmarks from deep metric learning and self-supervised learning show that optimal hyper-parameters are found faster with our method than with other common search methods.



### Class-aware Sounding Objects Localization via Audiovisual Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2112.11749v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2112.11749v1)
- **Published**: 2021-12-22 09:34:33+00:00
- **Updated**: 2021-12-22 09:34:33+00:00
- **Authors**: Di Hu, Yake Wei, Rui Qian, Weiyao Lin, Ruihua Song, Ji-Rong Wen
- **Comment**: accepted by TPAMI 2021. Code:
  https://github.com/GeWu-Lab/CSOL_TPAMI2021
- **Journal**: None
- **Summary**: Audiovisual scenes are pervasive in our daily life. It is commonplace for humans to discriminatively localize different sounding objects but quite challenging for machines to achieve class-aware sounding objects localization without category annotations, i.e., localizing the sounding object and recognizing its category. To address this problem, we propose a two-stage step-by-step learning framework to localize and recognize sounding objects in complex audiovisual scenarios using only the correspondence between audio and vision. First, we propose to determine the sounding area via coarse-grained audiovisual correspondence in the single source cases. Then visual features in the sounding area are leveraged as candidate object representations to establish a category-representation object dictionary for expressive visual character extraction. We generate class-aware object localization maps in cocktail-party scenarios and use audiovisual correspondence to suppress silent areas by referring to this dictionary. Finally, we employ category-level audiovisual consistency as the supervision to achieve fine-grained audio and sounding object distribution alignment. Experiments on both realistic and synthesized videos show that our model is superior in localizing and recognizing objects as well as filtering out silent ones. We also transfer the learned audiovisual network into the unsupervised object detection task, obtaining reasonable performance.



### Exploring Inter-frequency Guidance of Image for Lightweight Gaussian Denoising
- **Arxiv ID**: http://arxiv.org/abs/2112.11779v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.11779v1)
- **Published**: 2021-12-22 10:35:53+00:00
- **Updated**: 2021-12-22 10:35:53+00:00
- **Authors**: Zhuang Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Image denoising is of vital importance in many imaging or computer vision related areas. With the convolutional neural networks showing strong capability in computer vision tasks, the performance of image denoising has also been brought up by CNN based methods. Though CNN based image denoisers show promising results on this task, most of the current CNN based methods try to learn the mapping from noisy image to clean image directly, which lacks the explicit exploration of prior knowledge of images and noises. Natural images are observed to obey the reciprocal power law, implying the low-frequency band of image tend to occupy most of the energy. Thus in the condition of AGWN (additive gaussian white noise) deterioration, low-frequency band tend to preserve a higher PSNR than high-frequency band. Considering the spatial morphological consistency of different frequency bands, low-frequency band with more fidelity can be used as a guidance to refine the more contaminated high-frequency bands. Based on this thought, we proposed a novel network architecture denoted as IGNet, in order to refine the frequency bands from low to high in a progressive manner. Firstly, it decomposes the feature maps into high- and low-frequency subbands using DWT (discrete wavelet transform) iteratively, and then each low band features are used to refine the high band features. Finally, the refined feature maps are processed by a decoder to recover the clean result. With this design, more inter-frequency prior and information are utilized, thus the model size can be lightened while still perserves competitive results. Experiments on several public datasets show that our model obtains competitive performance comparing with other state-of-the-art methods yet with a lightweight structure.



### BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View
- **Arxiv ID**: http://arxiv.org/abs/2112.11790v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11790v3)
- **Published**: 2021-12-22 10:48:06+00:00
- **Updated**: 2022-06-16 09:15:52+00:00
- **Authors**: Junjie Huang, Guan Huang, Zheng Zhu, Yun Ye, Dalong Du
- **Comment**: Multi-camera 3D Object Detection
- **Journal**: None
- **Summary**: Autonomous driving perceives its surroundings for decision making, which is one of the most complex scenarios in visual perception. The success of paradigm innovation in solving the 2D object detection task inspires us to seek an elegant, feasible, and scalable paradigm for fundamentally pushing the performance boundary in this area. To this end, we contribute the BEVDet paradigm in this paper. BEVDet performs 3D object detection in Bird-Eye-View (BEV), where most target values are defined and route planning can be handily performed. We merely reuse existing modules to build its framework but substantially develop its performance by constructing an exclusive data augmentation strategy and upgrading the Non-Maximum Suppression strategy. In the experiment, BEVDet offers an excellent trade-off between accuracy and time-efficiency. As a fast version, BEVDet-Tiny scores 31.2% mAP and 39.2% NDS on the nuScenes val set. It is comparable with FCOS3D, but requires just 11% computational budget of 215.3 GFLOPs and runs 9.2 times faster at 15.6 FPS. Another high-precision version dubbed BEVDet-Base scores 39.3% mAP and 47.2% NDS, significantly exceeding all published results. With a comparable inference speed, it surpasses FCOS3D by a large margin of +9.8% mAP and +10.0% NDS. The source code is publicly available for further research at https://github.com/HuangJunJie2017/BEVDet .



### YOLO-Z: Improving small object detection in YOLOv5 for autonomous vehicles
- **Arxiv ID**: http://arxiv.org/abs/2112.11798v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11798v4)
- **Published**: 2021-12-22 11:03:43+00:00
- **Updated**: 2023-01-03 09:18:41+00:00
- **Authors**: Aduen Benjumea, Izzeddin Teeti, Fabio Cuzzolin, Andrew Bradley
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: As autonomous vehicles and autonomous racing rise in popularity, so does the need for faster and more accurate detectors. While our naked eyes are able to extract contextual information almost instantly, even from far away, image resolution and computational resources limitations make detecting smaller objects (that is, objects that occupy a small pixel area in the input image) a genuinely challenging task for machines and a wide-open research field. This study explores how the popular YOLOv5 object detector can be modified to improve its performance in detecting smaller objects, with a particular application in autonomous racing. To achieve this, we investigate how replacing certain structural elements of the model (as well as their connections and other parameters) can affect performance and inference time. In doing so, we propose a series of models at different scales, which we name `YOLO-Z', and which display an improvement of up to 6.9% in mAP when detecting smaller objects at 50% IOU, at the cost of just a 3ms increase in inference time compared to the original YOLOv5. Our objective is to inform future research on the potential of adjusting a popular detector such as YOLOv5 to address specific tasks and provide insights on how specific changes can impact small object detection. Such findings, applied to the broader context of autonomous vehicles, could increase the amount of contextual information available to such systems.



### Neuroevolution deep learning architecture search for estimation of river surface elevation from photogrammetric Digital Surface Models
- **Arxiv ID**: http://arxiv.org/abs/2112.12510v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.12510v3)
- **Published**: 2021-12-22 11:25:23+00:00
- **Updated**: 2022-12-04 17:21:14+00:00
- **Authors**: Radosław Szostak, Marcin Pietroń, Mirosław Zimnoch, Przemysław Wachniew, Paweł Ćwiąkała, Edyta Puniach
- **Comment**: extended version of NeurIPS 2021 Workshop paper - ML4PhysicalSciences
- **Journal**: None
- **Summary**: Development of the new methods of surface water observation is crucial in the perspective of increasingly frequent extreme hydrological events related to global warming and increasing demand for water. Orthophotos and digital surface models (DSMs) obtained using UAV photogrammetry can be used to determine the Water Surface Elevation (WSE) of a river. However, this task is difficult due to disturbances of the water surface on DSMs caused by limitations of photogrammetric algorithms. In this study, machine learning was used to extract a WSE value from disturbed photogrammetric data. A brand new dataset has been prepared specifically for this purpose by hydrology and photogrammetry experts. The new method is an important step toward automating water surface level measurements with high spatial and temporal resolution. Such data can be used to validate and calibrate of hydrological, hydraulic and hydrodynamic models making hydrological forecasts more accurate, in particular predicting extreme and dangerous events such as floods or droughts. For our knowledge this is the first approach in which dataset was created for this purpose and deep learning models were used for this task. Additionally, neuroevolution algorithm was set to explore different architectures to find local optimal models and non-gradient search was performed to fine-tune the model parameters. The achieved results have better accuracy compared to manual methods of determining WSE from photogrammetric DSMs.



### Binary Image Skeletonization Using 2-Stage U-Net
- **Arxiv ID**: http://arxiv.org/abs/2112.11824v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11824v1)
- **Published**: 2021-12-22 11:58:42+00:00
- **Updated**: 2021-12-22 11:58:42+00:00
- **Authors**: Mohamed A. Ghanem, Alaa A. Anani
- **Comment**: Computer Vision Course Project [AUC, Spring 21]
- **Journal**: None
- **Summary**: Object Skeletonization is the process of extracting skeletal, line-like representations of shapes. It provides a very useful tool for geometric shape understanding and minimal shape representation. It also has a wide variety of applications, most notably in anatomical research and activity detection. Several mathematical algorithmic approaches have been developed to solve this problem, and some of them have been proven quite robust. However, a lesser amount of attention has been invested into deep learning solutions for it. In this paper, we use a 2-stage variant of the famous U-Net architecture to split the problem space into two sub-problems: shape minimization and corrective skeleton thinning. Our model produces results that are visually much better than the baseline SkelNetOn model. We propose a new metric, M-CCORR, based on normalized correlation coefficients as an alternative to F1 for this challenge as it solves the problem of class imbalance, managing to recognize skeleton similarity without suffering from F1's over-sensitivity to pixel-shifts.



### Deep learning for brain metastasis detection and segmentation in longitudinal MRI data
- **Arxiv ID**: http://arxiv.org/abs/2112.11833v5
- **DOI**: 10.1002/mp.15863
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.11833v5)
- **Published**: 2021-12-22 12:18:43+00:00
- **Updated**: 2022-09-16 12:07:37+00:00
- **Authors**: Yixing Huang, Christoph Bert, Philipp Sommer, Benjamin Frey, Udo Gaipl, Luitpold V. Distel, Thomas Weissmann, Michael Uder, Manuel A. Schmidt, Arnd Dörfler, Andreas Maier, Rainer Fietkau, Florian Putz
- **Comment**: Implementation is available to public at
  https://github.com/YixingHuang/DeepMedicPlus
- **Journal**: Medical Physics 2022
- **Summary**: Brain metastases occur frequently in patients with metastatic cancer. Early and accurate detection of brain metastases is very essential for treatment planning and prognosis in radiation therapy. To improve brain metastasis detection performance with deep learning, a custom detection loss called volume-level sensitivity-specificity (VSS) is proposed, which rates individual metastasis detection sensitivity and specificity in (sub-)volume levels. As sensitivity and precision are always a trade-off in a metastasis level, either a high sensitivity or a high precision can be achieved by adjusting the weights in the VSS loss without decline in dice score coefficient for segmented metastases. To reduce metastasis-like structures being detected as false positive metastases, a temporal prior volume is proposed as an additional input of DeepMedic. The modified network is called DeepMedic+ for distinction. Our proposed VSS loss improves the sensitivity of brain metastasis detection for DeepMedic, increasing the sensitivity from 85.3% to 97.5%. Alternatively, it improves the precision from 69.1% to 98.7%. Comparing DeepMedic+ with DeepMedic with the same VSS loss, 44.4% of the false positive metastases are reduced in the high sensitivity model and the precision reaches 99.6% for the high specificity model. The mean dice coefficient for all metastases is about 0.81. With the ensemble of the high sensitivity and high specificity models, on average only 1.5 false positive metastases per patient needs further check, while the majority of true positive metastases are confirmed. The ensemble learning is able to distinguish high confidence true positive metastases from metastases candidates that require special expert review or further follow-up, being particularly well-fit to the requirements of expert support in real clinical practice.



### Bottom-up approaches for multi-person pose estimation and it's applications: A brief review
- **Arxiv ID**: http://arxiv.org/abs/2112.11834v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.11834v1)
- **Published**: 2021-12-22 12:20:26+00:00
- **Updated**: 2021-12-22 12:20:26+00:00
- **Authors**: Milan Kresović, Thong Duy Nguyen
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: Human Pose Estimation (HPE) is one of the fundamental problems in computer vision. It has applications ranging from virtual reality, human behavior analysis, video surveillance, anomaly detection, self-driving to medical assistance. The main objective of HPE is to obtain the person's posture from the given input. Among different paradigms for HPE, one paradigm is called bottom-up multi-person pose estimation. In the bottom-up approach, initially, all the key points of the targets are detected, and later in the optimization stage, the detected key points are associated with the corresponding targets. This review paper discussed the recent advancements in bottom-up approaches for the HPE and listed the possible high-quality datasets used to train the models. Additionally, a discussion of the prominent bottom-up approaches and their quantitative results on the standard performance matrices are given. Finally, the limitations of the existing methods are highlighted, and guidelines of the future research directions are given.



### NVS-MonoDepth: Improving Monocular Depth Prediction with Novel View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2112.12577v1
- **DOI**: 10.1109/3DV53792.2021.00093
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12577v1)
- **Published**: 2021-12-22 12:21:08+00:00
- **Updated**: 2021-12-22 12:21:08+00:00
- **Authors**: Zuria Bauer, Zuoyue Li, Sergio Orts-Escolano, Miguel Cazorla, Marc Pollefeys, Martin R. Oswald
- **Comment**: 8 pages (main paper), 9 pages (supplementary material), 14 figures, 4
  tables
- **Journal**: 2021 International Conference on 3D Vision (3DV)
- **Summary**: Building upon the recent progress in novel view synthesis, we propose its application to improve monocular depth estimation. In particular, we propose a novel training method split in three main steps. First, the prediction results of a monocular depth network are warped to an additional view point. Second, we apply an additional image synthesis network, which corrects and improves the quality of the warped RGB image. The output of this network is required to look as similar as possible to the ground-truth view by minimizing the pixel-wise RGB reconstruction error. Third, we reapply the same monocular depth estimation onto the synthesized second view point and ensure that the depth predictions are consistent with the associated ground truth depth. Experimental results prove that our method achieves state-of-the-art or comparable performance on the KITTI and NYU-Depth-v2 datasets with a lightweight and simple vanilla U-Net architecture.



### A Discriminative Single-Shot Segmentation Network for Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2112.11846v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11846v2)
- **Published**: 2021-12-22 12:48:51+00:00
- **Updated**: 2021-12-27 08:08:02+00:00
- **Authors**: Alan Lukežič, Jiří Matas, Matej Kristan
- **Comment**: Extended version of the D3S tracker (CVPR2020). Accepted to IEEE
  TPAMI. arXiv admin note: substantial text overlap with arXiv:1911.08862
- **Journal**: None
- **Summary**: Template-based discriminative trackers are currently the dominant tracking paradigm due to their robustness, but are restricted to bounding box tracking and a limited range of transformation models, which reduces their localization accuracy. We propose a discriminative single-shot segmentation tracker -- D3S2, which narrows the gap between visual object tracking and video object segmentation. A single-shot network applies two target models with complementary geometric properties, one invariant to a broad range of transformations, including non-rigid deformations, the other assuming a rigid object to simultaneously achieve robust online target segmentation. The overall tracking reliability is further increased by decoupling the object and feature scale estimation. Without per-dataset finetuning, and trained only for segmentation as the primary output, D3S2 outperforms all published trackers on the recent short-term tracking benchmark VOT2020 and performs very close to the state-of-the-art trackers on the GOT-10k, TrackingNet, OTB100 and LaSoT. D3S2 outperforms the leading segmentation tracker SiamMask on video object segmentation benchmarks and performs on par with top video object segmentation algorithms.



### Multimodal Analysis of memes for sentiment extraction
- **Arxiv ID**: http://arxiv.org/abs/2112.11850v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.11850v1)
- **Published**: 2021-12-22 12:57:05+00:00
- **Updated**: 2021-12-22 12:57:05+00:00
- **Authors**: Nayan Varma Alluri, Neeli Dheeraj Krishna
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Memes are one of the most ubiquitous forms of social media communication. The study and processing of memes, which are intrinsically multimedia, is a popular topic right now. The study presented in this research is based on the Memotion dataset, which involves categorising memes based on irony, comedy, motivation, and overall-sentiment. Three separate innovative transformer-based techniques have been developed, and their outcomes have been thoroughly reviewed.The best algorithm achieved a macro F1 score of 0.633 for humour classification, 0.55 for motivation classification, 0.61 for sarcasm classification, and 0.575 for overall sentiment of the meme out of all our techniques.



### Geodesic squared exponential kernel for non-rigid shape registration
- **Arxiv ID**: http://arxiv.org/abs/2112.11853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11853v1)
- **Published**: 2021-12-22 13:01:00+00:00
- **Updated**: 2021-12-22 13:01:00+00:00
- **Authors**: Florent Jousse, Xavier Pennec, Hervé Delingette, Matilde Gonzalez
- **Comment**: 2021 16TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE
  RECOGNITION (FG 2021) PROCEEDINGS, Dec 2021, JODHPUR, India
- **Journal**: None
- **Summary**: This work addresses the problem of non-rigid registration of 3D scans, which is at the core of shape modeling techniques. Firstly, we propose a new kernel based on geodesic distances for the Gaussian Process Morphable Models (GPMMs) framework. The use of geodesic distances into the kernel makes it more adapted to the topological and geometric characteristics of the surface and leads to more realistic deformations around holes and curved areas. Since the kernel possesses hyperparameters we have optimized them for the task of face registration on the FaceWarehouse dataset. We show that the Geodesic squared exponential kernel performs significantly better than state of the art kernels for the task of face registration on all the 20 expressions of the FaceWarehouse dataset. Secondly, we propose a modification of the loss function used in the non-rigid ICP registration algorithm, that allows to weight the correspondences according to the confidence given to them. As a use case, we show that we can make the registration more robust to outliers in the 3D scans, such as non-skin parts.



### Few-shot Font Generation with Weakly Supervised Localized Representations
- **Arxiv ID**: http://arxiv.org/abs/2112.11895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11895v1)
- **Published**: 2021-12-22 14:26:53+00:00
- **Updated**: 2021-12-22 14:26:53+00:00
- **Authors**: Song Park, Sanghyuk Chun, Junbum Cha, Bado Lee, Hyunjung Shim
- **Comment**: First two authors contributed equally. This is a journal extension of
  our AAAI 2021 paper arXiv:2009.11042; Code: https://github.com/clovaai/lffont
  and https://github.com/clovaai/fewshot-font-generation
- **Journal**: None
- **Summary**: Automatic few-shot font generation aims to solve a well-defined, real-world problem because manual font designs are expensive and sensitive to the expertise of designers. Existing methods learn to disentangle style and content elements by developing a universal style representation for each font style. However, this approach limits the model in representing diverse local styles, because it is unsuitable for complicated letter systems, for example, Chinese, whose characters consist of a varying number of components (often called "radical") -- with a highly complex structure. In this paper, we propose a novel font generation method that learns localized styles, namely component-wise style representations, instead of universal styles. The proposed style representations enable the synthesis of complex local details in text designs. However, learning component-wise styles solely from a few reference glyphs is infeasible when a target script has a large number of components, for example, over 200 for Chinese. To reduce the number of required reference glyphs, we represent component-wise styles by a product of component and style factors, inspired by low-rank matrix factorization. Owing to the combination of strong representation and a compact factorization strategy, our method shows remarkably better few-shot font generation results (with only eight reference glyphs) than other state-of-the-art methods. Moreover, strong locality supervision, for example, location of each component, skeleton, or strokes, was not utilized. The source code is available at https://github.com/clovaai/lffont and https://github.com/clovaai/fewshot-font-generation.



### Meta-Learning and Self-Supervised Pretraining for Real World Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2112.11929v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.11929v1)
- **Published**: 2021-12-22 14:48:22+00:00
- **Updated**: 2021-12-22 14:48:22+00:00
- **Authors**: Ileana Rugina, Rumen Dangovski, Mark Veillette, Pooya Khorrami, Brian Cheung, Olga Simek, Marin Soljačić
- **Comment**: 10 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: Recent advances in deep learning, in particular enabled by hardware advances and big data, have provided impressive results across a wide range of computational problems such as computer vision, natural language, or reinforcement learning. Many of these improvements are however constrained to problems with large-scale curated data-sets which require a lot of human labor to gather. Additionally, these models tend to generalize poorly under both slight distributional shifts and low-data regimes. In recent years, emerging fields such as meta-learning or self-supervised learning have been closing the gap between proof-of-concept results and real-life applications of machine learning by extending deep-learning to the semi-supervised and few-shot domains. We follow this line of work and explore spatio-temporal structure in a recently introduced image-to-image translation problem in order to: i) formulate a novel multi-task few-shot image generation benchmark and ii) explore data augmentations in contrastive pre-training for image translation downstream tasks. We present several baselines for the few-shot problem and discuss trade-offs between different approaches. Our code is available at https://github.com/irugina/meta-image-translation.



### Automatic Estimation of Anthropometric Human Body Measurements
- **Arxiv ID**: http://arxiv.org/abs/2112.11992v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.11992v1)
- **Published**: 2021-12-22 16:13:59+00:00
- **Updated**: 2021-12-22 16:13:59+00:00
- **Authors**: Dana Škorvánková, Adam Riečický, Martin Madaras
- **Comment**: None
- **Journal**: None
- **Summary**: Research tasks related to human body analysis have been drawing a lot of attention in computer vision area over the last few decades, considering its potential benefits on our day-to-day life. Anthropometry is a field defining physical measures of a human body size, form, and functional capacities. Specifically, the accurate estimation of anthropometric body measurements from visual human body data is one of the challenging problems, where the solution would ease many different areas of applications, including ergonomics, garment manufacturing, etc. This paper formulates a research in the field of deep learning and neural networks, to tackle the challenge of body measurements estimation from various types of visual input data (such as 2D images or 3D point clouds). Also, we deal with the lack of real human data annotated with ground truth body measurements required for training and evaluation, by generating a synthetic dataset of various human body shapes and performing a skeleton-driven annotation.



### DA-FDFtNet: Dual Attention Fake Detection Fine-tuning Network to Detect Various AI-Generated Fake Images
- **Arxiv ID**: http://arxiv.org/abs/2112.12001v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12001v1)
- **Published**: 2021-12-22 16:25:24+00:00
- **Updated**: 2021-12-22 16:25:24+00:00
- **Authors**: Young Oh Bang, Simon S. Woo
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the advancement of Generative Adversarial Networks (GAN), Autoencoders, and other AI technologies, it has been much easier to create fake images such as "Deepfakes". More recent research has introduced few-shot learning, which uses a small amount of training data to produce fake images and videos more effectively. Therefore, the ease of generating manipulated images and the difficulty of distinguishing those images can cause a serious threat to our society, such as propagating fake information. However, detecting realistic fake images generated by the latest AI technology is challenging due to the reasons mentioned above. In this work, we propose Dual Attention Fake Detection Fine-tuning Network (DA-FDFtNet) to detect the manipulated fake face images from the real face data. Our DA-FDFtNet integrates the pre-trained model with Fine-Tune Transformer, MBblockV3, and a channel attention module to improve the performance and robustness across different types of fake images. In particular, Fine-Tune Transformer consists of multiple numbers of an image-based self-attention module and a down-sampling layer. The channel attention module is also connected with the pre-trained model to capture the fake images feature space. We experiment with our DA-FDFtNet with the FaceForensics++ dataset and various GAN-generated datasets, and we show that our approach outperforms the previous baseline models.



### Looking Beyond Corners: Contrastive Learning of Visual Representations for Keypoint Detection and Description Extraction
- **Arxiv ID**: http://arxiv.org/abs/2112.12002v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12002v2)
- **Published**: 2021-12-22 16:27:11+00:00
- **Updated**: 2022-07-11 16:15:30+00:00
- **Authors**: Henrique Siqueira, Patrick Ruhkamp, Ibrahim Halfaoui, Markus Karmann, Onay Urfalioglu
- **Comment**: Accepted at IEEE WCCI 2022
- **Journal**: None
- **Summary**: Learnable keypoint detectors and descriptors are beginning to outperform classical hand-crafted feature extraction methods. Recent studies on self-supervised learning of visual representations have driven the increasing performance of learnable models based on deep networks. By leveraging traditional data augmentations and homography transformations, these networks learn to detect corners under adverse conditions such as extreme illumination changes. However, their generalization capabilities are limited to corner-like features detected a priori by classical methods or synthetically generated data.   In this paper, we propose the Correspondence Network (CorrNet) that learns to detect repeatable keypoints and to extract discriminative descriptions via unsupervised contrastive learning under spatial constraints. Our experiments show that CorrNet is not only able to detect low-level features such as corners, but also high-level features that represent similar objects present in a pair of input images through our proposed joint guided backpropagation of their latent space. Our approach obtains competitive results under viewpoint changes and achieves state-of-the-art performance under illumination changes.



### Barely-Supervised Learning: Semi-Supervised Learning with very few labeled images
- **Arxiv ID**: http://arxiv.org/abs/2112.12004v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12004v1)
- **Published**: 2021-12-22 16:29:10+00:00
- **Updated**: 2021-12-22 16:29:10+00:00
- **Authors**: Thomas Lucas, Philippe Weinzaepfel, Gregory Rogez
- **Comment**: None
- **Journal**: None
- **Summary**: This paper tackles the problem of semi-supervised learning when the set of labeled samples is limited to a small number of images per class, typically less than 10, problem that we refer to as barely-supervised learning. We analyze in depth the behavior of a state-of-the-art semi-supervised method, FixMatch, which relies on a weakly-augmented version of an image to obtain supervision signal for a more strongly-augmented version. We show that it frequently fails in barely-supervised scenarios, due to a lack of training signal when no pseudo-label can be predicted with high confidence. We propose a method to leverage self-supervised methods that provides training signal in the absence of confident pseudo-labels. We then propose two methods to refine the pseudo-label selection process which lead to further improvements. The first one relies on a per-sample history of the model predictions, akin to a voting scheme. The second iteratively updates class-dependent confidence thresholds to better explore classes that are under-represented in the pseudo-labels. Our experiments show that our approach performs significantly better on STL-10 in the barely-supervised regime, e.g. with 4 or 8 labeled images per class.



### Community Detection in Medical Image Datasets: Using Wavelets and Spectral Methods
- **Arxiv ID**: http://arxiv.org/abs/2112.12021v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.12021v2)
- **Published**: 2021-12-22 16:47:10+00:00
- **Updated**: 2022-04-26 12:10:37+00:00
- **Authors**: Roozbeh Yousefzadeh
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image datasets can have large number of images representing patients with different health conditions and various disease severity. When dealing with raw unlabeled image datasets, the large number of samples often makes it hard for experts and non-experts to understand the variety of images present in a dataset. Supervised learning methods rely on labeled images which requires a considerable effort by medical experts to first understand the communities of images present in the data and then labeling the images. Here, we propose an algorithm to facilitate the automatic identification of communities in medical image datasets. We further demonstrate that such analysis can be insightful in a supervised setting when the images are already labeled. Such insights are useful because, health and disease severity can be considered a continuous spectrum, and within each class, there usually are finer communities worthy of investigation, especially when they have similarities to communities in other classes. In our approach, we use wavelet decomposition of images in tandem with spectral methods. We show that the eigenvalues of a graph Laplacian can reveal the number of notable communities in an image dataset. Moreover, analyzing the similarities may be used to infer a spectrum representing the severity of the disease. In our experiments, we use a dataset of images labeled with different conditions for COVID patients. We detect 25 communities in the dataset and then observe that only 6 of those communities contain patients with pneumonia. We also investigate the contents of a colorectal cancer histology dataset.



### Learning and Crafting for the Wide Multiple Baseline Stereo
- **Arxiv ID**: http://arxiv.org/abs/2112.12027v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12027v1)
- **Published**: 2021-12-22 16:52:55+00:00
- **Updated**: 2021-12-22 16:52:55+00:00
- **Authors**: Dmytro Mishkin
- **Comment**: After-defence version with additional fixes based on reviewer
  commends. 144 pages
- **Journal**: None
- **Summary**: This thesis introduces the wide multiple baseline stereo (WxBS) problem. WxBS, a generalization of the standard wide baseline stereo problem, considers the matching of images that simultaneously differ in more than one image acquisition factor such as viewpoint, illumination, sensor type, or where object appearance changes significantly, e.g., over time. A new dataset with the ground truth, evaluation metric and baselines has been introduced.   The thesis presents the following improvements of the WxBS pipeline. (i) A loss function, called HardNeg, for learning a local image descriptor that relies on hard negative mining within a mini-batch and on the maximization of the distance between the closest positive and the closest negative patches. (ii) The descriptor trained with the HardNeg loss, called HardNet, is compact and shows state-of-the-art performance in standard matching, patch verification and retrieval benchmarks. (iii) A method for learning the affine shape, orientation, and potentially other parameters related to geometric and appearance properties of local features. (iv) A tentative correspondences generation strategy which generalizes the standard first to second closest distance ratio is presented. The selection strategy, which shows performance superior to the standard method, is applicable to either hard-engineered descriptors like SIFT, LIOP, and MROGH or deeply learned like HardNet. (v) A feedback loop is introduced for the two-view matching problem, resulting in MODS -- matching with on-demand view synthesis -- algorithm. MODS is an algorithm that handles a viewing angle difference even larger than the previous state-of-the-art ASIFT algorithm, without a significant increase of computational cost over "standard" wide and narrow baseline approaches. Last, but not least, a comprehensive benchmark for local features and robust estimation algorithms is introduced.



### Multi-View Partial (MVP) Point Cloud Challenge 2021 on Completion and Registration: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2112.12053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12053v1)
- **Published**: 2021-12-22 17:24:53+00:00
- **Updated**: 2021-12-22 17:24:53+00:00
- **Authors**: Liang Pan, Tong Wu, Zhongang Cai, Ziwei Liu, Xumin Yu, Yongming Rao, Jiwen Lu, Jie Zhou, Mingye Xu, Xiaoyuan Luo, Kexue Fu, Peng Gao, Manning Wang, Yali Wang, Yu Qiao, Junsheng Zhou, Xin Wen, Peng Xiang, Yu-Shen Liu, Zhizhong Han, Yuanjie Yan, Junyi An, Lifa Zhu, Changwei Lin, Dongrui Liu, Xin Li, Francisco Gómez-Fernández, Qinlong Wang, Yang Yang
- **Comment**: 15 pages, 13 figures, ICCV2021 Workshop Technique Report, the
  codebase webpage: https://github.com/paul007pl/MVP_Benchmark
- **Journal**: None
- **Summary**: As real-scanned point clouds are mostly partial due to occlusions and viewpoints, reconstructing complete 3D shapes based on incomplete observations becomes a fundamental problem for computer vision. With a single incomplete point cloud, it becomes the partial point cloud completion problem. Given multiple different observations, 3D reconstruction can be addressed by performing partial-to-partial point cloud registration. Recently, a large-scale Multi-View Partial (MVP) point cloud dataset has been released, which consists of over 100,000 high-quality virtual-scanned partial point clouds. Based on the MVP dataset, this paper reports methods and results in the Multi-View Partial Point Cloud Challenge 2021 on Completion and Registration. In total, 128 participants registered for the competition, and 31 teams made valid submissions. The top-ranked solutions will be analyzed, and then we will discuss future research directions.



### Improved skin lesion recognition by a Self-Supervised Curricular Deep Learning approach
- **Arxiv ID**: http://arxiv.org/abs/2112.12086v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12086v1)
- **Published**: 2021-12-22 17:45:47+00:00
- **Updated**: 2021-12-22 17:45:47+00:00
- **Authors**: Kirill Sirotkin, Marcos Escudero-Viñolo, Pablo Carballeira, Juan Carlos SanMiguel
- **Comment**: 11 pages, 8 figures, submitted to the Journal of Biomedical and
  Health Informatics (Special Issue on Skin Image Analysis in the Age of Deep
  Learning)
- **Journal**: None
- **Summary**: State-of-the-art deep learning approaches for skin lesion recognition often require pretraining on larger and more varied datasets, to overcome the generalization limitations derived from the reduced size of the skin lesion imaging datasets. ImageNet is often used as the pretraining dataset, but its transferring potential is hindered by the domain gap between the source dataset and the target dermatoscopic scenario. In this work, we introduce a novel pretraining approach that sequentially trains a series of Self-Supervised Learning pretext tasks and only requires the unlabeled skin lesion imaging data. We present a simple methodology to establish an ordering that defines a pretext task curriculum. For the multi-class skin lesion classification problem, and ISIC-2019 dataset, we provide experimental evidence showing that: i) a model pretrained by a curriculum of pretext tasks outperforms models pretrained by individual pretext tasks, and ii) a model pretrained by the optimal pretext task curriculum outperforms a model pretrained on ImageNet. We demonstrate that this performance gain is related to the fact that the curriculum of pretext tasks better focuses the attention of the final model on the skin lesion. Beyond performance improvement, this strategy allows for a large reduction in the training time with respect to ImageNet pretraining, which is especially advantageous for network architectures tailored for a specific problem.



### Reflash Dropout in Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2112.12089v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12089v3)
- **Published**: 2021-12-22 17:47:32+00:00
- **Updated**: 2022-04-20 06:14:24+00:00
- **Authors**: Xiangtao Kong, Xina Liu, Jinjin Gu, Yu Qiao, Chao Dong
- **Comment**: CVPR2022 paper + supplementary file
- **Journal**: None
- **Summary**: Dropout is designed to relieve the overfitting problem in high-level vision tasks but is rarely applied in low-level vision tasks, like image super-resolution (SR). As a classic regression problem, SR exhibits a different behaviour as high-level tasks and is sensitive to the dropout operation. However, in this paper, we show that appropriate usage of dropout benefits SR networks and improves the generalization ability. Specifically, dropout is better embedded at the end of the network and is significantly helpful for the multi-degradation settings. This discovery breaks our common sense and inspires us to explore its working mechanism. We further use two analysis tools -- one is from recent network interpretation works, and the other is specially designed for this task. The analysis results provide side proofs to our experimental findings and show us a new perspective to understand SR networks.



### NICE-SLAM: Neural Implicit Scalable Encoding for SLAM
- **Arxiv ID**: http://arxiv.org/abs/2112.12130v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12130v2)
- **Published**: 2021-12-22 18:45:44+00:00
- **Updated**: 2022-04-21 17:33:59+00:00
- **Authors**: Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin R. Oswald, Marc Pollefeys
- **Comment**: CVPR 2022, first two authors contributed equally. Project page:
  https://pengsongyou.github.io/nice-slam
- **Journal**: None
- **Summary**: Neural implicit representations have recently shown encouraging results in various domains, including promising progress in simultaneous localization and mapping (SLAM). Nevertheless, existing methods produce over-smoothed scene reconstructions and have difficulty scaling up to large scenes. These limitations are mainly due to their simple fully-connected network architecture that does not incorporate local information in the observations. In this paper, we present NICE-SLAM, a dense SLAM system that incorporates multi-level local information by introducing a hierarchical scene representation. Optimizing this representation with pre-trained geometric priors enables detailed reconstruction on large indoor scenes. Compared to recent neural implicit SLAM systems, our approach is more scalable, efficient, and robust. Experiments on five challenging datasets demonstrate competitive results of NICE-SLAM in both mapping and tracking quality. Project page: https://pengsongyou.github.io/nice-slam



### Can Deep Neural Networks be Converted to Ultra Low-Latency Spiking Neural Networks?
- **Arxiv ID**: http://arxiv.org/abs/2112.12133v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12133v1)
- **Published**: 2021-12-22 18:47:45+00:00
- **Updated**: 2021-12-22 18:47:45+00:00
- **Authors**: Gourav Datta, Peter A. Beerel
- **Comment**: Accepted to DATE 2022
- **Journal**: None
- **Summary**: Spiking neural networks (SNNs), that operate via binary spikes distributed over time, have emerged as a promising energy efficient ML paradigm for resource-constrained devices. However, the current state-of-the-art (SOTA) SNNs require multiple time steps for acceptable inference accuracy, increasing spiking activity and, consequently, energy consumption. SOTA training strategies for SNNs involve conversion from a non-spiking deep neural network (DNN). In this paper, we determine that SOTA conversion strategies cannot yield ultra low latency because they incorrectly assume that the DNN and SNN pre-activation values are uniformly distributed. We propose a new training algorithm that accurately captures these distributions, minimizing the error between the DNN and converted SNN. The resulting SNNs have ultra low latency and high activation sparsity, yielding significant improvements in compute efficiency. In particular, we evaluate our framework on image recognition tasks from CIFAR-10 and CIFAR-100 datasets on several VGG and ResNet architectures. We obtain top-1 accuracy of 64.19% with only 2 time steps on the CIFAR-100 dataset with ~159.2x lower compute energy compared to an iso-architecture standard DNN. Compared to other SOTA SNN models, our models perform inference 2.5-8x faster (i.e., with fewer time steps).



### Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2112.12141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12141v1)
- **Published**: 2021-12-22 18:57:16+00:00
- **Updated**: 2021-12-22 18:57:16+00:00
- **Authors**: Jingxiao Zheng, Xinwei Shi, Alexander Gorban, Junhua Mao, Yang Song, Charles R. Qi, Ting Liu, Visesh Chari, Andre Cornman, Yin Zhou, Congcong Li, Dragomir Anguelov
- **Comment**: None
- **Journal**: None
- **Summary**: 3D human pose estimation (HPE) in autonomous vehicles (AV) differs from other use cases in many factors, including the 3D resolution and range of data, absence of dense depth maps, failure modes for LiDAR, relative location between the camera and LiDAR, and a high bar for estimation accuracy. Data collected for other use cases (such as virtual reality, gaming, and animation) may therefore not be usable for AV applications. This necessitates the collection and annotation of a large amount of 3D data for HPE in AV, which is time-consuming and expensive. In this paper, we propose one of the first approaches to alleviate this problem in the AV setting. Specifically, we propose a multi-modal approach which uses 2D labels on RGB images as weak supervision to perform 3D HPE. The proposed multi-modal architecture incorporates LiDAR and camera inputs with an auxiliary segmentation branch. On the Waymo Open Dataset, our approach achieves a 22% relative improvement over camera-only 2D HPE baseline, and 6% improvement over LiDAR-only model. Finally, careful ablation studies and parts based analysis illustrate the advantages of each of our contributions.



### Scaling Open-Vocabulary Image Segmentation with Image-Level Labels
- **Arxiv ID**: http://arxiv.org/abs/2112.12143v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12143v2)
- **Published**: 2021-12-22 18:57:54+00:00
- **Updated**: 2022-07-20 21:56:52+00:00
- **Authors**: Golnaz Ghiasi, Xiuye Gu, Yin Cui, Tsung-Yi Lin
- **Comment**: Accepted at ECCV 2022
- **Journal**: None
- **Summary**: We design an open-vocabulary image segmentation model to organize an image into meaningful regions indicated by arbitrary texts. Recent works (CLIP and ALIGN), despite attaining impressive open-vocabulary classification accuracy with image-level caption labels, are unable to segment visual concepts with pixels. We argue that these models miss an important step of visual grouping, which organizes pixels into groups before learning visual-semantic alignments. We propose OpenSeg to address the above issue while still making use of scalable image-level supervision of captions. First, it learns to propose segmentation masks for possible organizations. Then it learns visual-semantic alignments by aligning each word in a caption to one or a few predicted masks. We find the mask representations are the key to support learning image segmentation from captions, making it possible to scale up the dataset and vocabulary sizes. OpenSeg significantly outperforms the recent open-vocabulary method of LSeg by +19.9 mIoU on PASCAL dataset, thanks to its scalability.



### Recur, Attend or Convolve? On Whether Temporal Modeling Matters for Cross-Domain Robustness in Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.12175v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12175v4)
- **Published**: 2021-12-22 19:11:53+00:00
- **Updated**: 2022-10-11 11:44:32+00:00
- **Authors**: Sofia Broomé, Ernest Pokropek, Boyu Li, Hedvig Kjellström
- **Comment**: None
- **Journal**: None
- **Summary**: Most action recognition models today are highly parameterized, and evaluated on datasets with appearance-wise distinct classes. It has also been shown that 2D Convolutional Neural Networks (CNNs) tend to be biased toward texture rather than shape in still image recognition tasks, in contrast to humans. Taken together, this raises suspicion that large video models partly learn spurious spatial texture correlations rather than to track relevant shapes over time to infer generalizable semantics from their movement. A natural way to avoid parameter explosion when learning visual patterns over time is to make use of recurrence. Biological vision consists of abundant recurrent circuitry, and is superior to computer vision in terms of domain shift generalization. In this article, we empirically study whether the choice of low-level temporal modeling has consequences for texture bias and cross-domain robustness. In order to enable a light-weight and systematic assessment of the ability to capture temporal structure, not revealed from single frames, we provide the Temporal Shape (TS) dataset, as well as modified domains of Diving48 allowing for the investigation of spatial texture bias in video models. The combined results of our experiments indicate that sound physical inductive bias such as recurrence in temporal modeling may be advantageous when robustness to domain shift is important for the task.



### Multimodal Personality Recognition using Cross-Attention Transformer and Behaviour Encoding
- **Arxiv ID**: http://arxiv.org/abs/2112.12180v3
- **DOI**: 10.5220/0010841400003124
- **Categories**: **cs.CV**, cs.AI, 68T05, 68T10, I.5
- **Links**: [PDF](http://arxiv.org/pdf/2112.12180v3)
- **Published**: 2021-12-22 19:14:55+00:00
- **Updated**: 2023-01-12 15:01:11+00:00
- **Authors**: Tanay Agrawal, Dhruv Agarwal, Michal Balazia, Neelabh Sinha, Francois Bremond
- **Comment**: Preprint. Final paper accepted at the 17th International Conference
  on Computer Vision Theory and Applications (VISAPP), virtual, February, 2022.
  8 pages
- **Journal**: None
- **Summary**: Personality computing and affective computing have gained recent interest in many research areas. The datasets for the task generally have multiple modalities like video, audio, language and bio-signals. In this paper, we propose a flexible model for the task which exploits all available data. The task involves complex relations and to avoid using a large model for video processing specifically, we propose the use of behaviour encoding which boosts performance with minimal change to the model. Cross-attention using transformers has become popular in recent times and is utilised for fusion of different modalities. Since long term relations may exist, breaking the input into chunks is not desirable, thus the proposed model processes the entire input together. Our experiments show the importance of each of the above contributions



### Fine-grained Multi-Modal Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.12182v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.12182v1)
- **Published**: 2021-12-22 19:17:45+00:00
- **Updated**: 2021-12-22 19:17:45+00:00
- **Authors**: Duo Wang, Salah Karout
- **Comment**: Accepted at BMVC 2021
- **Journal**: None
- **Summary**: Multi-Modal Self-Supervised Learning from videos has been shown to improve model's performance on various downstream tasks. However, such Self-Supervised pre-training requires large batch sizes and a large amount of computation resources due to the noise present in the uncurated data. This is partly due to the fact that the prevalent training scheme is trained on coarse-grained setting, in which vectors representing the whole video clips or natural language sentences are used for computing similarity. Such scheme makes training noisy as part of the video clips can be totally not correlated with the other-modality input such as text description. In this paper, we propose a fine-grained multi-modal self-supervised training scheme that computes the similarity between embeddings at finer-scale (such as individual feature map embeddings and embeddings of phrases), and uses attention mechanisms to reduce noisy pairs' weighting in the loss function. We show that with the proposed pre-training scheme, we can train smaller models, with smaller batch-size and much less computational resources to achieve downstream tasks performances comparable to State-Of-The-Art, for tasks including action recognition and text-image retrievals.



### Improved 2D Keypoint Detection in Out-of-Balance and Fall Situations -- combining input rotations and a kinematic model
- **Arxiv ID**: http://arxiv.org/abs/2112.12193v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12193v1)
- **Published**: 2021-12-22 19:49:09+00:00
- **Updated**: 2021-12-22 19:49:09+00:00
- **Authors**: Michael Zwölfer, Dieter Heinrich, Kurt Schindelwig, Bastian Wandt, Helge Rhodin, Joerg Spoerri, Werner Nachbauer
- **Comment**: extended abstract, 4 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: Injury analysis may be one of the most beneficial applications of deep learning based human pose estimation. To facilitate further research on this topic, we provide an injury specific 2D dataset for alpine skiing, covering in total 533 images. We further propose a post processing routine, that combines rotational information with a simple kinematic model. We could improve detection results in fall situations by up to 21% regarding the PCK@0.2 metric.



### Maximum Entropy on Erroneous Predictions (MEEP): Improving model calibration for medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.12218v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.12218v3)
- **Published**: 2021-12-22 20:34:20+00:00
- **Updated**: 2023-06-02 14:47:01+00:00
- **Authors**: Agostina Larrazabal, Cesar Martinez, Jose Dolz, Enzo Ferrante
- **Comment**: Accepted for publication at MICCAI 2023
- **Journal**: None
- **Summary**: Modern deep neural networks achieved remarkable progress in medical image segmentation tasks. However, it has recently been observed that they tend to produce overconfident estimates, even in situations of high uncertainty, leading to poorly calibrated and unreliable models. In this work we introduce Maximum Entropy on Erroneous Predictions (MEEP), a training strategy for segmentation networks which selectively penalizes overconfident predictions, focusing only on misclassified pixels. Our method is agnostic to the neural architecture, does not increase model complexity and can be coupled with multiple segmentation loss functions. We benchmark the proposed strategy in two challenging segmentation tasks: white matter hyperintensity lesions in magnetic resonance images (MRI) of the brain, and atrial segmentation in cardiac MRI. The experimental results demonstrate that coupling MEEP with standard segmentation losses leads to improvements not only in terms of model calibration, but also in segmentation quality.



### SAMCNet for Spatial-configuration-based Classification: A Summary of Results
- **Arxiv ID**: http://arxiv.org/abs/2112.12219v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.12219v2)
- **Published**: 2021-12-22 20:45:24+00:00
- **Updated**: 2022-03-18 16:23:50+00:00
- **Authors**: Majid Farhadloo, Carl Molnar, Gaoxiang Luo, Yan Li, Shashi Shekhar, Rachel L. Maus, Svetomir N. Markovic, Raymond Moore, Alexey Leontovich
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of spatial-configuration-based classification is to build a classifier to distinguish two classes (e.g., responder, non-responder) based on the spatial arrangements (e.g., spatial interactions between different point categories) given multi-category point data from two classes. This problem is important for generating hypotheses in medical pathology towards discovering new immunotherapies for cancer treatment as well as for other applications in biomedical research and microbial ecology. This problem is challenging due to an exponential number of category subsets which may vary in the strength of spatial interactions. Most prior efforts on using human selected spatial association measures may not be sufficient for capturing the relevant (e.g., surrounded by) spatial interactions which may be of biological significance. In addition, the related deep neural networks are limited to category pairs and do not explore larger subsets of point categories. To overcome these limitations, we propose a Spatial-interaction Aware Multi-Category deep neural Network (SAMCNet) architecture and contribute novel local reference frame characterization and point pair prioritization layers for spatial-configuration-based classification. Extensive experimental results on multiple cancer datasets show that the proposed architecture provides higher prediction accuracy over baseline methods.



### Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2112.12252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12252v1)
- **Published**: 2021-12-22 22:41:02+00:00
- **Updated**: 2021-12-22 22:41:02+00:00
- **Authors**: Benjamin Kiefer, David Ott, Andreas Zell
- **Comment**: The first two authors contributed equally. Github repository will be
  made public soon
- **Journal**: None
- **Summary**: Acquiring data to train deep learning-based object detectors on Unmanned Aerial Vehicles (UAVs) is expensive, time-consuming and may even be prohibited by law in specific environments. On the other hand, synthetic data is fast and cheap to access. In this work, we explore the potential use of synthetic data in object detection from UAVs across various application environments. For that, we extend the open-source framework DeepGTAV to work for UAV scenarios. We capture various large-scale high-resolution synthetic data sets in several domains to demonstrate their use in real-world object detection from UAVs by analyzing multiple training strategies across several models. Furthermore, we analyze several different data generation and sampling parameters to provide actionable engineering advice for further scientific research. The DeepGTAV framework is available at https://git.io/Jyf5j.



### Human Activity Recognition on wrist-worn accelerometers using self-supervised neural networks
- **Arxiv ID**: http://arxiv.org/abs/2112.12272v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.12272v1)
- **Published**: 2021-12-22 23:35:20+00:00
- **Updated**: 2021-12-22 23:35:20+00:00
- **Authors**: Niranjan Sridhar, Lance Myers
- **Comment**: None
- **Journal**: None
- **Summary**: Measures of Activity of Daily Living (ADL) are an important indicator of overall health but difficult to measure in-clinic. Automated and accurate human activity recognition (HAR) using wrist-worn accelerometers enables practical and cost efficient remote monitoring of ADL. Key obstacles in developing high quality HAR is the lack of large labeled datasets and the performance loss when applying models trained on small curated datasets to the continuous stream of heterogeneous data in real-life. In this work we design a self-supervised learning paradigm to create a robust representation of accelerometer data that can generalize across devices and subjects. We demonstrate that this representation can separate activities of daily living and achieve strong HAR accuracy (on multiple benchmark datasets) using very few labels. We also propose a segmentation algorithm which can identify segments of salient activity and boost HAR accuracy on continuous real-life data.



