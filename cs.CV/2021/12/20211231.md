# Arxiv Papers in cs.CV on 2021-12-31
### Data-Free Knowledge Transfer: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2112.15278v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.15278v1)
- **Published**: 2021-12-31 03:39:42+00:00
- **Updated**: 2021-12-31 03:39:42+00:00
- **Authors**: Yuang Liu, Wei Zhang, Jun Wang, Jianyong Wang
- **Comment**: 20 pages, 8 figures
- **Journal**: None
- **Summary**: In the last decade, many deep learning models have been well trained and made a great success in various fields of machine intelligence, especially for computer vision and natural language processing. To better leverage the potential of these well-trained models in intra-domain or cross-domain transfer learning situations, knowledge distillation (KD) and domain adaptation (DA) are proposed and become research highlights. They both aim to transfer useful information from a well-trained model with original training data. However, the original data is not always available in many cases due to privacy, copyright or confidentiality. Recently, the data-free knowledge transfer paradigm has attracted appealing attention as it deals with distilling valuable knowledge from well-trained models without requiring to access to the training data. In particular, it mainly consists of the data-free knowledge distillation (DFKD) and source data-free domain adaptation (SFDA). On the one hand, DFKD aims to transfer the intra-domain knowledge of original data from a cumbersome teacher network to a compact student network for model compression and efficient inference. On the other hand, the goal of SFDA is to reuse the cross-domain knowledge stored in a well-trained source model and adapt it to a target domain. In this paper, we provide a comprehensive survey on data-free knowledge transfer from the perspectives of knowledge distillation and unsupervised domain adaptation, to help readers have a better understanding of the current research status and ideas. Applications and challenges of the two areas are briefly reviewed, respectively. Furthermore, we provide some insights to the subject of future research.



### ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation
- **Arxiv ID**: http://arxiv.org/abs/2112.15283v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2112.15283v1)
- **Published**: 2021-12-31 03:53:33+00:00
- **Updated**: 2021-12-31 03:53:33+00:00
- **Authors**: Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: Conventional methods for the image-text generation tasks mainly tackle the naturally bidirectional generation tasks separately, focusing on designing task-specific frameworks to improve the quality and fidelity of the generated samples. Recently, Vision-Language Pre-training models have greatly improved the performance of the image-to-text generation tasks, but large-scale pre-training models for text-to-image synthesis task are still under-developed. In this paper, we propose ERNIE-ViLG, a unified generative pre-training framework for bidirectional image-text generation with transformer model. Based on the image quantization models, we formulate both image generation and text generation as autoregressive generative tasks conditioned on the text/image input. The bidirectional image-text generative modeling eases the semantic alignments across vision and language. For the text-to-image generation process, we further propose an end-to-end training method to jointly learn the visual sequence generator and the image reconstructor. To explore the landscape of large-scale pre-training for bidirectional text-image generation, we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for text-to-image synthesis and best results on COCO-CN and AIC-ICC for image captioning.



### CSformer: Bridging Convolution and Transformer for Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/2112.15299v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.15299v1)
- **Published**: 2021-12-31 04:37:11+00:00
- **Updated**: 2021-12-31 04:37:11+00:00
- **Authors**: Dongjie Ye, Zhangkai Ni, Hanli Wang, Jian Zhang, Shiqi Wang, Sam Kwong
- **Comment**: None
- **Journal**: None
- **Summary**: Convolution neural networks (CNNs) have succeeded in compressive image sensing. However, due to the inductive bias of locality and weight sharing, the convolution operations demonstrate the intrinsic limitations in modeling the long-range dependency. Transformer, designed initially as a sequence-to-sequence model, excels at capturing global contexts due to the self-attention-based architectures even though it may be equipped with limited localization abilities. This paper proposes CSformer, a hybrid framework that integrates the advantages of leveraging both detailed spatial information from CNN and the global context provided by transformer for enhanced representation learning. The proposed approach is an end-to-end compressive image sensing method, composed of adaptive sampling and recovery. In the sampling module, images are measured block-by-block by the learned sampling matrix. In the reconstruction stage, the measurement is projected into dual stems. One is the CNN stem for modeling the neighborhood relationships by convolution, and the other is the transformer stem for adopting global self-attention mechanism. The dual branches structure is concurrent, and the local features and global representations are fused under different resolutions to maximize the complementary of features. Furthermore, we explore a progressive strategy and window-based transformer block to reduce the parameter and computational complexity. The experimental results demonstrate the effectiveness of the dedicated transformer-based architecture for compressive sensing, which achieves superior performance compared to state-of-the-art methods on different datasets.



### SplitBrain: Hybrid Data and Model Parallel Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.15317v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2112.15317v1)
- **Published**: 2021-12-31 06:25:38+00:00
- **Updated**: 2021-12-31 06:25:38+00:00
- **Authors**: Farley Lai, Asim Kadav, Erik Kruus
- **Comment**: None
- **Journal**: None
- **Summary**: The recent success of deep learning applications has coincided with those widely available powerful computational resources for training sophisticated machine learning models with huge datasets. Nonetheless, training large models such as convolutional neural networks using model parallelism (as opposed to data parallelism) is challenging because the complex nature of communication between model shards makes it difficult to partition the computation efficiently across multiple machines with an acceptable trade-off. This paper presents SplitBrain, a high performance distributed deep learning framework supporting hybrid data and model parallelism. Specifically, SplitBrain provides layer-specific partitioning that co-locates compute intensive convolutional layers while sharding memory demanding layers. A novel scalable group communication is proposed to further improve the training throughput with reduced communication overhead. The results show that SplitBrain can achieve nearly linear speedup while saving up to 67\% of memory consumption for data and model parallel VGG over CIFAR-10.



### InverseMV: Composing Piano Scores with a Convolutional Video-Music Transformer
- **Arxiv ID**: http://arxiv.org/abs/2112.15320v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.MM, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2112.15320v1)
- **Published**: 2021-12-31 06:39:28+00:00
- **Updated**: 2021-12-31 06:39:28+00:00
- **Authors**: Chin-Tung Lin, Mu Yang
- **Comment**: Rejected by ISMIR 2020
- **Journal**: None
- **Summary**: Many social media users prefer consuming content in the form of videos rather than text. However, in order for content creators to produce videos with a high click-through rate, much editing is needed to match the footage to the music. This posts additional challenges for more amateur video makers. Therefore, we propose a novel attention-based model VMT (Video-Music Transformer) that automatically generates piano scores from video frames. Using music generated from models also prevent potential copyright infringements that often come with using existing music. To the best of our knowledge, there is no work besides the proposed VMT that aims to compose music for video. Additionally, there lacks a dataset with aligned video and symbolic music. We release a new dataset composed of over 7 hours of piano scores with fine alignment between pop music videos and MIDI files. We conduct experiments with human evaluation on VMT, SeqSeq model (our baseline), and the original piano version soundtrack. VMT achieves consistent improvements over the baseline on music smoothness and video relevance. In particular, with the relevance scores and our case study, our model has shown the capability of multimodality on frame-level actors' movement for music generation. Our VMT model, along with the new dataset, presents a promising research direction toward composing the matching soundtrack for videos. We have released our code at https://github.com/linchintung/VMT



### DeepVisualInsight: Time-Travelling Visualization for Spatio-Temporal Causality of Deep Classification Training
- **Arxiv ID**: http://arxiv.org/abs/2201.01155v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/2201.01155v1)
- **Published**: 2021-12-31 07:05:31+00:00
- **Updated**: 2021-12-31 07:05:31+00:00
- **Authors**: Xianglin Yang, Yun Lin, Ruofan Liu, Zhenfeng He, Chao Wang, Jin Song Dong, Hong Mei
- **Comment**: Accepted in AAAI'22
- **Journal**: None
- **Summary**: Understanding how the predictions of deep learning models are formed during the training process is crucial to improve model performance and fix model defects, especially when we need to investigate nontrivial training strategies such as active learning, and track the root cause of unexpected training results such as performance degeneration.   In this work, we propose a time-travelling visual solution DeepVisualInsight (DVI), aiming to manifest the spatio-temporal causality while training a deep learning image classifier. The spatio-temporal causality demonstrates how the gradient-descent algorithm and various training data sampling techniques can influence and reshape the layout of learnt input representation and the classification boundaries in consecutive epochs. Such causality allows us to observe and analyze the whole learning process in the visible low dimensional space. Technically, we propose four spatial and temporal properties and design our visualization solution to satisfy them. These properties preserve the most important information when inverse-)projecting input samples between the visible low-dimensional and the invisible high-dimensional space, for causal analyses. Our extensive experiments show that, comparing to baseline approaches, we achieve the best visualization performance regarding the spatial/temporal properties and visualization efficiency. Moreover, our case study shows that our visual solution can well reflect the characteristics of various training scenarios, showing good potential of DVI as a debugging tool for analyzing deep learning training processes.



### Deconfounded Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/2112.15324v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2112.15324v1)
- **Published**: 2021-12-31 07:14:59+00:00
- **Updated**: 2021-12-31 07:14:59+00:00
- **Authors**: Jianqiang Huang, Yu Qin, Jiaxin Qi, Qianru Sun, Hanwang Zhang
- **Comment**: AAAI 2022 Accepted
- **Journal**: None
- **Summary**: We focus on the confounding bias between language and location in the visual grounding pipeline, where we find that the bias is the major visual reasoning bottleneck. For example, the grounding process is usually a trivial language-location association without visual reasoning, e.g., grounding any language query containing sheep to the nearly central regions, due to that most queries about sheep have ground-truth locations at the image center. First, we frame the visual grounding pipeline into a causal graph, which shows the causalities among image, query, target location and underlying confounder. Through the causal graph, we know how to break the grounding bottleneck: deconfounded visual grounding. Second, to tackle the challenge that the confounder is unobserved in general, we propose a confounder-agnostic approach called: Referring Expression Deconfounder (RED), to remove the confounding bias. Third, we implement RED as a simple language attention, which can be applied in any grounding method. On popular benchmarks, RED improves various state-of-the-art grounding methods by a significant margin. Code will soon be available at: https://github.com/JianqiangH/Deconfounded_VG.



### On Distinctive Properties of Universal Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2112.15329v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.15329v1)
- **Published**: 2021-12-31 07:35:04+00:00
- **Updated**: 2021-12-31 07:35:04+00:00
- **Authors**: Sung Min Park, Kuo-An Wei, Kai Xiao, Jerry Li, Aleksander Madry
- **Comment**: None
- **Journal**: None
- **Summary**: We identify properties of universal adversarial perturbations (UAPs) that distinguish them from standard adversarial perturbations. Specifically, we show that targeted UAPs generated by projected gradient descent exhibit two human-aligned properties: semantic locality and spatial invariance, which standard targeted adversarial perturbations lack. We also demonstrate that UAPs contain significantly less signal for generalization than standard adversarial perturbations -- that is, UAPs leverage non-robust features to a smaller extent than standard adversarial perturbations.



### P2P-Loc: Point to Point Tiny Person Localization
- **Arxiv ID**: http://arxiv.org/abs/2112.15344v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.15344v2)
- **Published**: 2021-12-31 08:24:43+00:00
- **Updated**: 2022-01-05 08:27:34+00:00
- **Authors**: Xuehui Yu, Di Wu, Qixiang Ye, Jianbin Jiao, Zhenjun Han
- **Comment**: None
- **Journal**: None
- **Summary**: Bounding-box annotation form has been the most frequently used method for visual object localization tasks. However, bounding-box annotation relies on a large amount of precisely annotating bounding boxes, and it is expensive and laborious. It is impossible to be employed in practical scenarios and even redundant for some applications (such as tiny person localization) that the size would not matter. Therefore, we propose a novel point-based framework for the person localization task by annotating each person as a coarse point (CoarsePoint) instead of an accurate bounding box that can be any point within the object extent. Then, the network predicts the person's location as a 2D coordinate in the image. Although this greatly simplifies the data annotation pipeline, the CoarsePoint annotation inevitably decreases label reliability (label uncertainty) and causes network confusion during training. As a result, we propose a point self-refinement approach that iteratively updates point annotations in a self-paced way. The proposed refinement system alleviates the label uncertainty and progressively improves localization performance. Experimental results show that our approach has achieved comparable object localization performance while saving up to 80$\%$ of annotation cost.



### Learning to Predict 3D Lane Shape and Camera Pose from a Single Image via Geometry Constraints
- **Arxiv ID**: http://arxiv.org/abs/2112.15351v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.15351v1)
- **Published**: 2021-12-31 08:59:27+00:00
- **Updated**: 2021-12-31 08:59:27+00:00
- **Authors**: Ruijin Liu, Dapeng Chen, Tie Liu, Zhiliang Xiong, Zejian Yuan
- **Comment**: 14 pages, 10 figures, accepted by AAAI 2022
- **Journal**: None
- **Summary**: Detecting 3D lanes from the camera is a rising problem for autonomous vehicles. In this task, the correct camera pose is the key to generating accurate lanes, which can transform an image from perspective-view to the top-view. With this transformation, we can get rid of the perspective effects so that 3D lanes would look similar and can accurately be fitted by low-order polynomials. However, mainstream 3D lane detectors rely on perfect camera poses provided by other sensors, which is expensive and encounters multi-sensor calibration issues. To overcome this problem, we propose to predict 3D lanes by estimating camera pose from a single image with a two-stage framework. The first stage aims at the camera pose task from perspective-view images. To improve pose estimation, we introduce an auxiliary 3D lane task and geometry constraints to benefit from multi-task learning, which enhances consistencies between 3D and 2D, as well as compatibility in the above two tasks. The second stage targets the 3D lane task. It uses previously estimated pose to generate top-view images containing distance-invariant lane appearances for predicting accurate 3D lanes. Experiments demonstrate that, without ground truth camera pose, our method outperforms the state-of-the-art perfect-camera-pose-based methods and has the fewest parameters and computations. Codes are available at https://github.com/liuruijin17/CLGo.



### Sparse LiDAR Assisted Self-supervised Stereo Disparity Estimation
- **Arxiv ID**: http://arxiv.org/abs/2112.15355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.15355v1)
- **Published**: 2021-12-31 09:11:21+00:00
- **Updated**: 2021-12-31 09:11:21+00:00
- **Authors**: Xiaoming Zhao, Weihai Chen, Xingming Wu, Peter C. Y. Chen, Zhengguo Li
- **Comment**: None
- **Journal**: None
- **Summary**: Deep stereo matching has made significant progress in recent years. However, state-of-the-art methods are based on expensive 4D cost volume, which limits their use in real-world applications. To address this issue, 3D correlation maps and iterative disparity updates have been proposed. Regarding that in real-world platforms, such as self-driving cars and robots, the Lidar is usually installed. Thus we further introduce the sparse Lidar point into the iterative updates, which alleviates the burden of network updating the disparity from zero states. Furthermore, we propose training the network in a self-supervised way so that it can be trained on any captured data for better generalization ability. Experiments and comparisons show that the presented method is effective and achieves comparable results with related methods.



### Conditional Generative Data-free Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2112.15358v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.15358v4)
- **Published**: 2021-12-31 09:23:40+00:00
- **Updated**: 2022-08-12 15:54:38+00:00
- **Authors**: Xinyi Yu, Ling Yan, Yang Yang, Libo Zhou, Linlin Ou
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge distillation has made remarkable achievements in model compression. However, most existing methods require the original training data, which is usually unavailable due to privacy and security issues. In this paper, we propose a conditional generative data-free knowledge distillation (CGDD) framework for training lightweight networks without any training data. This method realizes efficient knowledge distillation based on conditional image generation. Specifically, we treat the preset labels as ground truth to train a conditional generator in a semi-supervised manner. The trained generator can produce specified classes of training images. For training the student network, we force it to extract the knowledge hidden in teacher feature maps, which provide crucial cues for the learning process. Moreover, an adversarial training framework for promoting distillation performance is constructed by designing several loss functions. This framework helps the student model to explore larger data space. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on different datasets. Compared with other data-free works, our work obtains state-of-the-art results on CIFAR100, Caltech101, and different versions of ImageNet datasets. The codes will be released.



### Modeling Mask Uncertainty in Hyperspectral Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2112.15362v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.15362v4)
- **Published**: 2021-12-31 09:39:13+00:00
- **Updated**: 2022-09-24 00:17:42+00:00
- **Authors**: Jiamian Wang, Yulun Zhang, Xin Yuan, Ziyi Meng, Zhiqiang Tao
- **Comment**: ECCV 2022 Oral Paper
- **Journal**: None
- **Summary**: Recently, hyperspectral imaging (HSI) has attracted increasing research attention, especially for the ones based on a coded aperture snapshot spectral imaging (CASSI) system. Existing deep HSI reconstruction models are generally trained on paired data to retrieve original signals upon 2D compressed measurements given by a particular optical hardware mask in CASSI, during which the mask largely impacts the reconstruction performance and could work as a "model hyperparameter" governing on data augmentations. This mask-specific training style will lead to a hardware miscalibration issue, which sets up barriers to deploying deep HSI models among different hardware and noisy environments. To address this challenge, we introduce mask uncertainty for HSI with a complete variational Bayesian learning treatment and explicitly model it through a mask decomposition inspired by real hardware. Specifically, we propose a novel Graph-based Self-Tuning (GST) network to reason uncertainties adapting to varying spatial structures of masks among different hardware. Moreover, we develop a bilevel optimization framework to balance HSI reconstruction and uncertainty estimation, accounting for the hyperparameter property of masks. Extensive experimental results and model discussions validate the effectiveness (over 33/30 dB) of the proposed GST method under two miscalibration scenarios and demonstrate a highly competitive performance compared with the state-of-the-art well-calibrated methods. Our code and pre-trained model are available at https://github.com/Jiamian-Wang/mask_uncertainty_spectral_SCI



### Weakly Supervised Change Detection Using Guided Anisotropic Difusion
- **Arxiv ID**: http://arxiv.org/abs/2112.15367v1
- **DOI**: 10.1007/s10994-021-06008-4
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.15367v1)
- **Published**: 2021-12-31 10:03:47+00:00
- **Updated**: 2021-12-31 10:03:47+00:00
- **Authors**: Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch, Yann Gousseau
- **Comment**: Machine Learning Journal 2021. arXiv admin note: substantial text
  overlap with arXiv:1904.08208
- **Journal**: None
- **Summary**: Large scale datasets created from crowdsourced labels or openly available data have become crucial to provide training data for large scale learning algorithms. While these datasets are easier to acquire, the data are frequently noisy and unreliable, which is motivating research on weakly supervised learning techniques. In this paper we propose original ideas that help us to leverage such datasets in the context of change detection. First, we propose the guided anisotropic diffusion (GAD) algorithm, which improves semantic segmentation results using the input images as guides to perform edge preserving filtering. We then show its potential in two weakly-supervised learning strategies tailored for change detection. The first strategy is an iterative learning method that combines model optimisation and data cleansing using GAD to extract the useful information from a large scale change detection dataset generated from open vector data. The second one incorporates GAD within a novel spatial attention layer that increases the accuracy of weakly supervised networks trained to perform pixel-level predictions from image-level labels. Improvements with respect to state-of-the-art are demonstrated on 4 different public datasets.



### Efficient Single Image Super-Resolution Using Dual Path Connections with Multiple Scale Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.15386v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2112.15386v3)
- **Published**: 2021-12-31 11:03:59+00:00
- **Updated**: 2022-10-28 05:07:39+00:00
- **Authors**: Bin-Cheng Yang, Gangshan Wu
- **Comment**: 21 pages, 9 figures, 5 tables
- **Journal**: None
- **Summary**: Deep convolutional neural networks have been demonstrated to be effective for SISR in recent years. On the one hand, residual connections and dense connections have been used widely to ease forward information and backward gradient flows to boost performance. However, current methods use residual connections and dense connections separately in most network layers in a sub-optimal way. On the other hand, although various networks and methods have been designed to improve computation efficiency, save parameters, or utilize training data of multiple scale factors for each other to boost performance, it either do super-resolution in HR space to have a high computation cost or can not share parameters between models of different scale factors to save parameters and inference time. To tackle these challenges, we propose an efficient single image super-resolution network using dual path connections with multiple scale learning named as EMSRDPN. By introducing dual path connections inspired by Dual Path Networks into EMSRDPN, it uses residual connections and dense connections in an integrated way in most network layers. Dual path connections have the benefits of both reusing common features of residual connections and exploring new features of dense connections to learn a good representation for SISR. To utilize the feature correlation of multiple scale factors, EMSRDPN shares all network units in LR space between different scale factors to learn shared features and only uses a separate reconstruction unit for each scale factor, which can utilize training data of multiple scale factors to help each other to boost performance, meanwhile which can save parameters and support shared inference for multiple scale factors to improve efficiency. Experiments show EMSRDPN achieves better performance and comparable or even better parameter and inference efficiency over SOTA methods.



### InfoNeRF: Ray Entropy Minimization for Few-Shot Neural Volume Rendering
- **Arxiv ID**: http://arxiv.org/abs/2112.15399v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.15399v2)
- **Published**: 2021-12-31 11:56:01+00:00
- **Updated**: 2022-04-10 08:48:45+00:00
- **Authors**: Mijeong Kim, Seonguk Seo, Bohyung Han
- **Comment**: CVPR 2022, Website: http://cv.snu.ac.kr/research/InfoNeRF
- **Journal**: None
- **Summary**: We present an information-theoretic regularization technique for few-shot novel view synthesis based on neural implicit representation. The proposed approach minimizes potential reconstruction inconsistency that happens due to insufficient viewpoints by imposing the entropy constraint of the density in each ray. In addition, to alleviate the potential degenerate issue when all training images are acquired from almost redundant viewpoints, we further incorporate the spatially smoothness constraint into the estimated images by restricting information gains from a pair of rays with slightly different viewpoints. The main idea of our algorithm is to make reconstructed scenes compact along individual rays and consistent across rays in the neighborhood. The proposed regularizers can be plugged into most of existing neural volume rendering techniques based on NeRF in a straightforward way. Despite its simplicity, we achieve consistently improved performance compared to existing neural view synthesis methods by large margins on multiple standard benchmarks.



### Relational Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship
- **Arxiv ID**: http://arxiv.org/abs/2112.15402v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.15402v3)
- **Published**: 2021-12-31 12:05:22+00:00
- **Updated**: 2023-08-03 14:00:42+00:00
- **Authors**: Quanziang Wang, Renzhen Wang, Yuexiang Li, Dong Wei, Kai Ma, Yefeng Zheng, Deyu Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Continual learning is a promising machine learning paradigm to learn new tasks while retaining previously learned knowledge over streaming training data. Till now, rehearsal-based methods, keeping a small part of data from old tasks as a memory buffer, have shown good performance in mitigating catastrophic forgetting for previously learned knowledge. However, most of these methods typically treat each new task equally, which may not adequately consider the relationship or similarity between old and new tasks. Furthermore, these methods commonly neglect sample importance in the continual training process and result in sub-optimal performance on certain tasks. To address this challenging problem, we propose Relational Experience Replay (RER), a bi-level learning framework, to adaptively tune task-wise relationships and sample importance within each task to achieve a better `stability' and `plasticity' trade-off. As such, the proposed method is capable of accumulating new knowledge while consolidating previously learned old knowledge during continual learning. Extensive experiments conducted on three publicly available datasets (i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet) show that the proposed method can consistently improve the performance of all baselines and surpass current state-of-the-art methods.



### Disjoint Contrastive Regression Learning for Multi-Sourced Annotations
- **Arxiv ID**: http://arxiv.org/abs/2112.15411v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.15411v1)
- **Published**: 2021-12-31 12:39:04+00:00
- **Updated**: 2021-12-31 12:39:04+00:00
- **Authors**: Xiaoqian Ruan, Gaoang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale datasets are important for the development of deep learning models. Such datasets usually require a heavy workload of annotations, which are extremely time-consuming and expensive. To accelerate the annotation procedure, multiple annotators may be employed to label different subsets of the data. However, the inconsistency and bias among different annotators are harmful to the model training, especially for qualitative and subjective tasks.To address this challenge, in this paper, we propose a novel contrastive regression framework to address the disjoint annotations problem, where each sample is labeled by only one annotator and multiple annotators work on disjoint subsets of the data. To take account of both the intra-annotator consistency and inter-annotator inconsistency, two strategies are employed.Firstly, a contrastive-based loss is applied to learn the relative ranking among different samples of the same annotator, with the assumption that the ranking of samples from the same annotator is unanimous. Secondly, we apply the gradient reversal layer to learn robust representations that are invariant to different annotators. Experiments on the facial expression prediction task, as well as the image quality assessment task, verify the effectiveness of our proposed framework.



### Facial-Sketch Synthesis: A New Challenge
- **Arxiv ID**: http://arxiv.org/abs/2112.15439v6
- **DOI**: 10.1007/s11633-022-1349-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.15439v6)
- **Published**: 2021-12-31 13:19:21+00:00
- **Updated**: 2022-07-11 22:07:30+00:00
- **Authors**: Deng-Ping Fan, Ziling Huang, Peng Zheng, Hong Liu, Xuebin Qin, Luc Van Gool
- **Comment**: Accepted to Machine Intelligence Research (MIR)
- **Journal**: None
- **Summary**: This paper aims to conduct a comprehensive study on facial-sketch synthesis (FSS). However, due to the high costs of obtaining hand-drawn sketch datasets, there lacks a complete benchmark for assessing the development of FSS algorithms over the last decade. We first introduce a high-quality dataset for FSS, named FS2K, which consists of 2,104 image-sketch pairs spanning three types of sketch styles, image backgrounds, lighting conditions, skin colors, and facial attributes. FS2K differs from previous FSS datasets in difficulty, diversity, and scalability and should thus facilitate the progress of FSS research. Second, we present the largest-scale FSS investigation by reviewing 89 classical methods, including 25 handcrafted feature-based facial-sketch synthesis approaches, 29 general translation methods, and 35 image-to-sketch approaches. Besides, we elaborate comprehensive experiments on the existing 19 cutting-edge models. Third, we present a simple baseline for FSS, named FSGAN. With only two straightforward components, i.e., facial-aware masking and style-vector expansion, FSGAN surpasses the performance of all previous state-of-the-art models on the proposed FS2K dataset by a large margin. Finally, we conclude with lessons learned over the past years and point out several unsolved challenges. Our code is available at https://github.com/DengPingFan/FSGAN.



### Accurate and Real-time 3D Pedestrian Detection Using an Efficient Attentive Pillar Network
- **Arxiv ID**: http://arxiv.org/abs/2112.15458v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.15458v3)
- **Published**: 2021-12-31 13:41:37+00:00
- **Updated**: 2022-11-17 07:45:31+00:00
- **Authors**: Duy-Tho Le, Hengcan Shi, Hamid Rezatofighi, Jianfei Cai
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Efficiently and accurately detecting people from 3D point cloud data is of great importance in many robotic and autonomous driving applications. This fundamental perception task is still very challenging due to (i) significant deformations of human body pose and gesture over time and (ii) point cloud sparsity and scarcity for pedestrian class objects. Recent efficient 3D object detection approaches rely on pillar features to detect objects from point cloud data. However, these pillar features do not carry sufficient expressive representations to deal with all the aforementioned challenges in detecting people. To address this shortcoming, we first introduce a stackable Pillar Aware Attention (PAA) module for enhanced pillar features extraction while suppressing noises in the point clouds. By integrating multi-point-channel-pooling, point-wise, channel-wise, and task-aware attention into a simple module, the representation capabilities are boosted while requiring little additional computing resources. We also present Mini-BiFPN, a small yet effective feature network that creates bidirectional information flow and multi-level cross-scale feature fusion to better integrate multi-resolution features. Our proposed framework, namely PiFeNet, has been evaluated on three popular large-scale datasets for 3D pedestrian Detection, i.e. KITTI, JRDB, and nuScenes achieving state-of-the-art (SOTA) performance on KITTI Bird-eye-view (BEV) and JRDB and very competitive performance on nuScenes. Our approach has inference speed of 26 frame-per-second (FPS), making it a real-time detector. The code for our PiFeNet is available at https://github.com/ldtho/PiFeNet.



### Scene-Adaptive Attention Network for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2112.15509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.15509v1)
- **Published**: 2021-12-31 15:03:17+00:00
- **Updated**: 2021-12-31 15:03:17+00:00
- **Authors**: Xing Wei, Yuanrui Kang, Jihao Yang, Yunfeng Qiu, Dahu Shi, Wenming Tan, Yihong Gong
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, significant progress has been made on the research of crowd counting. However, as the challenging scale variations and complex scenes existed in crowds, neither traditional convolution networks nor recent Transformer architectures with fixed-size attention could handle the task well. To address this problem, this paper proposes a scene-adaptive attention network, termed SAANet. First of all, we design a deformable attention in-built Transformer backbone, which learns adaptive feature representations with deformable sampling locations and dynamic attention weights. Then we propose the multi-level feature fusion and count-attentive feature enhancement modules further, to strengthen feature representation under the global image context. The learned representations could attend to the foreground and are adaptive to different scales of crowds. We conduct extensive experiments on four challenging crowd counting benchmarks, demonstrating that our method achieves state-of-the-art performance. Especially, our method currently ranks No.1 on the public leaderboard of the NWPU-Crowd benchmark. We hope our method could be a strong baseline to support future research in crowd counting. The source code will be released to the community.



### Transfer learning for cancer diagnosis in histopathological images
- **Arxiv ID**: http://arxiv.org/abs/2112.15523v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.15523v1)
- **Published**: 2021-12-31 15:43:00+00:00
- **Updated**: 2021-12-31 15:43:00+00:00
- **Authors**: Sandhya Aneja, Nagender Aneja, Pg Emeroylariffion Abas, Abdul Ghani Naim
- **Comment**: None
- **Journal**: IAES International Journal of Artificial Intelligence (IJ-AI),
  Vol. 11, No. 1, March 2022, pp. 129~136
- **Summary**: Transfer learning allows us to exploit knowledge gained from one task to assist in solving another but relevant task. In modern computer vision research, the question is which architecture performs better for a given dataset. In this paper, we compare the performance of 14 pre-trained ImageNet models on the histopathologic cancer detection dataset, where each model has been configured as a naive model, feature extractor model, or fine-tuned model. Densenet161 has been shown to have high precision whilst Resnet101 has a high recall. A high precision model is suitable to be used when follow-up examination cost is high, whilst low precision but a high recall/sensitivity model can be used when the cost of follow-up examination is low. Results also show that transfer learning helps to converge a model faster.



### on the effectiveness of generative adversarial network on anomaly detection
- **Arxiv ID**: http://arxiv.org/abs/2112.15541v1
- **DOI**: 10.1007/978-3-030-61609-0_38
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.15541v1)
- **Published**: 2021-12-31 16:35:47+00:00
- **Updated**: 2021-12-31 16:35:47+00:00
- **Authors**: Laya Rafiee Sevyeri, Thomas Fevens
- **Comment**: This paper is an improved version of an existing paper published by
  the same authors in ICANN2020
- **Journal**: None
- **Summary**: Identifying anomalies refers to detecting samples that do not resemble the training data distribution. Many generative models have been used to find anomalies, and among them, generative adversarial network (GAN)-based approaches are currently very popular. GANs mainly rely on the rich contextual information of these models to identify the actual training distribution. Following this analogy, we suggested a new unsupervised model based on GANs --a combination of an autoencoder and a GAN. Further, a new scoring function was introduced to target anomalies where a linear combination of the internal representation of the discriminator and the generator's visual representation, plus the encoded representation of the autoencoder, come together to define the proposed anomaly score. The model was further evaluated on benchmark datasets such as SVHN, CIFAR10, and MNIST, as well as a public medical dataset of leukemia images. In all the experiments, our model outperformed its existing counterparts while slightly improving the inference time.



### Improving Baselines in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2112.15550v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.15550v1)
- **Published**: 2021-12-31 16:59:03+00:00
- **Updated**: 2021-12-31 16:59:03+00:00
- **Authors**: Kazuki Irie, Imanol Schlag, Róbert Csordás, Jürgen Schmidhuber
- **Comment**: Presented at NeurIPS 2021 Workshop on Distribution Shifts,
  https://openreview.net/forum?id=9vxOrkNTs1x
- **Journal**: None
- **Summary**: We share our experience with the recently released WILDS benchmark, a collection of ten datasets dedicated to developing models and training strategies which are robust to domain shifts. Several experiments yield a couple of critical observations which we believe are of general interest for any future work on WILDS. Our study focuses on two datasets: iWildCam and FMoW. We show that (1) Conducting separate cross-validation for each evaluation metric is crucial for both datasets, (2) A weak correlation between validation and test performance might make model development difficult for iWildCam, (3) Minor changes in the training of hyper-parameters improve the baseline by a relatively large margin (mainly on FMoW), (4) There is a strong correlation between certain domains and certain target labels (mainly on iWildCam). To the best of our knowledge, no prior work on these datasets has reported these observations despite their obvious importance. Our code is public.



### An Unsupervised Domain Adaptation Model based on Dual-module Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2112.15555v1
- **DOI**: 10.1016/j.neucom.2021.12.060
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.15555v1)
- **Published**: 2021-12-31 17:07:54+00:00
- **Updated**: 2021-12-31 17:07:54+00:00
- **Authors**: Yiju Yang, Tianxiao Zhang, Guanyu Li, Taejoon Kim, Guanghui Wang
- **Comment**: arXiv admin note: text overlap with arXiv:2108.00610
- **Journal**: Neurocomputing, Volume 475, 28 February 2022, Pages 102-111
- **Summary**: In this paper, we propose a dual-module network architecture that employs a domain discriminative feature module to encourage the domain invariant feature module to learn more domain invariant features. The proposed architecture can be applied to any model that utilizes domain invariant features for unsupervised domain adaptation to improve its ability to extract domain invariant features. We conduct experiments with the Domain-Adversarial Training of Neural Networks (DANN) model as a representative algorithm. In the training process, we supply the same input to the two modules and then extract their feature distribution and prediction results respectively. We propose a discrepancy loss to find the discrepancy of the prediction results and the feature distribution between the two modules. Through the adversarial training by maximizing the loss of their feature distribution and minimizing the discrepancy of their prediction results, the two modules are encouraged to learn more domain discriminative and domain invariant features respectively. Extensive comparative evaluations are conducted and the proposed approach outperforms the state-of-the-art in most unsupervised domain adaptation tasks.



### PCACE: A Statistical Approach to Ranking Neurons for CNN Interpretability
- **Arxiv ID**: http://arxiv.org/abs/2112.15571v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.15571v1)
- **Published**: 2021-12-31 17:54:57+00:00
- **Updated**: 2021-12-31 17:54:57+00:00
- **Authors**: Sílvia Casacuberta, Esra Suel, Seth Flaxman
- **Comment**: None
- **Journal**: Responsible AI and DeepSpatial workshops at the 27th SIGKDD
  Conference on Knowledge Discovery and Data Mining (KDD 2021)
- **Summary**: In this paper we introduce a new problem within the growing literature of interpretability for convolution neural networks (CNNs). While previous work has focused on the question of how to visually interpret CNNs, we ask what it is that we care to interpret, that is, which layers and neurons are worth our attention? Due to the vast size of modern deep learning network architectures, automated, quantitative methods are needed to rank the relative importance of neurons so as to provide an answer to this question. We present a new statistical method for ranking the hidden neurons in any convolutional layer of a network. We define importance as the maximal correlation between the activation maps and the class score. We provide different ways in which this method can be used for visualization purposes with MNIST and ImageNet, and show a real-world application of our method to air pollution prediction with street-level images.



### 3-D Material Style Transfer for Reconstructing Unknown Appearance in Complex Natural Materials
- **Arxiv ID**: http://arxiv.org/abs/2112.15589v1
- **DOI**: None
- **Categories**: **cs.CV**, I.3; I.3.5; I.3.7; I.3.8
- **Links**: [PDF](http://arxiv.org/pdf/2112.15589v1)
- **Published**: 2021-12-31 18:51:37+00:00
- **Updated**: 2021-12-31 18:51:37+00:00
- **Authors**: Shashank Ranjan, Corey Toler-Franklin
- **Comment**: 15 pages, 22 figures
- **Journal**: None
- **Summary**: We propose a 3-D material style transfer framework for reconstructing invisible (or faded) appearance properties in complex natural materials. Our algorithm addresses the technical challenge of transferring appearance properties from one object to another of the same material when both objects have intricate, noncorresponding color patterns. Eggshells, exoskeletons, and minerals, for example, have patterns composed of highly randomized layers of organic and inorganic compounds. These materials pose a challenge as the distribution of compounds that determine surface color changes from object to object and within local pattern regions. Our solution adapts appearance observations from a material property distribution in an exemplar to the material property distribution of a target object to reconstruct its unknown appearance. We use measured reflectance in 3-D bispectral textures to record changing material property distributions. Our novel implementation of spherical harmonics uses principles from chemistry and biology to learn relationships between color (hue and saturation) and material composition and concentration in an exemplar. The encoded relationships are transformed to the property distribution of a target for color recovery and material assignment. Quantitative and qualitative evaluation methods show that we replicate color patterns more accurately than methods that only rely on shape correspondences and coarse-level perceptual differences. We demonstrate applications of our work for reconstructing color in extinct fossils, restoring faded artifacts and generating synthetic textures.



### Multi-Dimensional Model Compression of Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.00043v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.00043v1)
- **Published**: 2021-12-31 19:54:18+00:00
- **Updated**: 2021-12-31 19:54:18+00:00
- **Authors**: Zejiang Hou, Sun-Yuan Kung
- **Comment**: None
- **Journal**: None
- **Summary**: Vision transformers (ViT) have recently attracted considerable attentions, but the huge computational cost remains an issue for practical deployment. Previous ViT pruning methods tend to prune the model along one dimension solely, which may suffer from excessive reduction and lead to sub-optimal model quality. In contrast, we advocate a multi-dimensional ViT compression paradigm, and propose to harness the redundancy reduction from attention head, neuron and sequence dimensions jointly. We firstly propose a statistical dependence based pruning criterion that is generalizable to different dimensions for identifying deleterious components. Moreover, we cast the multi-dimensional compression as an optimization, learning the optimal pruning policy across the three dimensions that maximizes the compressed model's accuracy under a computational budget. The problem is solved by our adapted Gaussian process search with expected improvement. Experimental results show that our method effectively reduces the computational cost of various ViT models. For example, our method reduces 40\% FLOPs without top-1 accuracy loss for DeiT and T2T-ViT models, outperforming previous state-of-the-arts.



### iCaps: Iterative Category-level Object Pose and Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/2201.00059v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.00059v1)
- **Published**: 2021-12-31 21:15:05+00:00
- **Updated**: 2021-12-31 21:15:05+00:00
- **Authors**: Xinke Deng, Junyi Geng, Timothy Bretl, Yu Xiang, Dieter Fox
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a category-level 6D object pose and shape estimation approach iCaps, which allows tracking 6D poses of unseen objects in a category and estimating their 3D shapes. We develop a category-level auto-encoder network using depth images as input, where feature embeddings from the auto-encoder encode poses of objects in a category. The auto-encoder can be used in a particle filter framework to estimate and track 6D poses of objects in a category. By exploiting an implicit shape representation based on signed distance functions, we build a LatentNet to estimate a latent representation of the 3D shape given the estimated pose of an object. Then the estimated pose and shape can be used to update each other in an iterative way. Our category-level 6D object pose and shape estimation pipeline only requires 2D detection and segmentation for initialization. We evaluate our approach on a publicly available dataset and demonstrate its effectiveness. In particular, our method achieves comparably high accuracy on shape estimation.



### Croesus: Multi-Stage Processing and Transactions for Video-Analytics in Edge-Cloud Systems
- **Arxiv ID**: http://arxiv.org/abs/2201.00063v1
- **DOI**: None
- **Categories**: **eess.SY**, cs.CV, cs.DC, cs.LG, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/2201.00063v1)
- **Published**: 2021-12-31 21:38:05+00:00
- **Updated**: 2021-12-31 21:38:05+00:00
- **Authors**: Samaa Gazzaz, Vishal Chakraborty, Faisal Nawab
- **Comment**: Published in ICDE2022
- **Journal**: None
- **Summary**: Emerging edge applications require both a fast response latency and complex processing. This is infeasible without expensive hardware that can process complex operations -- such as object detection -- within a short time. Many approach this problem by addressing the complexity of the models -- via model compression, pruning and quantization -- or compressing the input. In this paper, we propose a different perspective when addressing the performance challenges. Croesus is a multi-stage approach to edge-cloud systems that provides the ability to find the balance between accuracy and performance. Croesus consists of two stages (that can be generalized to multiple stages): an initial and a final stage. The initial stage performs the computation in real-time using approximate/best-effort computation at the edge. The final stage performs the full computation at the cloud, and uses the results to correct any errors made at the initial stage. In this paper, we demonstrate the implications of such an approach on a video analytics use-case and show how multi-stage processing yields a better balance between accuracy and performance. Moreover, we study the safety of multi-stage transactions via two proposals: multi-stage serializability (MS-SR) and multi-stage invariant confluence with Apologies (MS-IA).



