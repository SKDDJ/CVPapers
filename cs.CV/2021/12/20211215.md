# Arxiv Papers in cs.CV on 2021-12-15
### Structure-Aware Image Segmentation with Homotopy Warping
- **Arxiv ID**: http://arxiv.org/abs/2112.07812v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2112.07812v3)
- **Published**: 2021-12-15 00:33:15+00:00
- **Updated**: 2022-10-13 02:44:28+00:00
- **Authors**: Xiaoling Hu
- **Comment**: 21 pages, 12 figures
- **Journal**: None
- **Summary**: Besides per-pixel accuracy, topological correctness is also crucial for the segmentation of images with fine-scale structures, e.g., satellite images and biomedical images. In this paper, by leveraging the theory of digital topology, we identify pixels in an image that are critical for topology. By focusing on these critical pixels, we propose a new homotopy warping loss to train deep image segmentation networks for better topological accuracy. To efficiently identify these topologically critical pixels, we propose a new algorithm exploiting the distance transform. The proposed algorithm, as well as the loss function, naturally generalize to different topological structures in both 2D and 3D settings. The proposed loss function helps deep nets achieve better performance in terms of topology-aware metrics, outperforming state-of-the-art structure/topology-aware segmentation methods.



### Weed Recognition using Deep Learning Techniques on Class-imbalanced Imagery
- **Arxiv ID**: http://arxiv.org/abs/2112.07819v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.07819v1)
- **Published**: 2021-12-15 01:00:05+00:00
- **Updated**: 2021-12-15 01:00:05+00:00
- **Authors**: A S M Mahmudul Hasan, Ferdous Sohel, Dean Diepeveen, Hamid Laga, Michael G. K. Jones
- **Comment**: The paper is accepted by Crop and Pasture Science journal
  (https://www.publish.csiro.au/CP/justaccepted/CP21626)
- **Journal**: None
- **Summary**: Most weed species can adversely impact agricultural productivity by competing for nutrients required by high-value crops. Manual weeding is not practical for large cropping areas. Many studies have been undertaken to develop automatic weed management systems for agricultural crops. In this process, one of the major tasks is to recognise the weeds from images. However, weed recognition is a challenging task. It is because weed and crop plants can be similar in colour, texture and shape which can be exacerbated further by the imaging conditions, geographic or weather conditions when the images are recorded. Advanced machine learning techniques can be used to recognise weeds from imagery. In this paper, we have investigated five state-of-the-art deep neural networks, namely VGG16, ResNet-50, Inception-V3, Inception-ResNet-v2 and MobileNetV2, and evaluated their performance for weed recognition. We have used several experimental settings and multiple dataset combinations. In particular, we constructed a large weed-crop dataset by combining several smaller datasets, mitigating class imbalance by data augmentation, and using this dataset in benchmarking the deep neural networks. We investigated the use of transfer learning techniques by preserving the pre-trained weights for extracting the features and fine-tuning them using the images of crop and weed datasets. We found that VGG16 performed better than others on small-scale datasets, while ResNet-50 performed better than other deep networks on the large combined dataset.



### Value Retrieval with Arbitrary Queries for Form-like Documents
- **Arxiv ID**: http://arxiv.org/abs/2112.07820v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.07820v2)
- **Published**: 2021-12-15 01:12:02+00:00
- **Updated**: 2022-04-15 20:42:21+00:00
- **Authors**: Mingfei Gao, Le Xue, Chetan Ramaiah, Chen Xing, Ran Xu, Caiming Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: We propose value retrieval with arbitrary queries for form-like documents to reduce human effort of processing forms. Unlike previous methods that only address a fixed set of field items, our method predicts target value for an arbitrary query based on the understanding of the layout and semantics of a form. To further boost model performance, we propose a simple document language modeling (SimpleDLM) strategy to improve document understanding on large-scale model pre-training. Experimental results show that our method outperforms previous designs significantly and the SimpleDLM further improves our performance on value retrieval by around 17% F1 score compared with the state-of-the-art pre-training method. Code is available at https://github.com/salesforce/QVR-SimpleDLM.



### Mining Minority-class Examples With Uncertainty Estimates
- **Arxiv ID**: http://arxiv.org/abs/2112.07835v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.07835v1)
- **Published**: 2021-12-15 02:05:02+00:00
- **Updated**: 2021-12-15 02:05:02+00:00
- **Authors**: Gursimran Singh, Lingyang Chu, Lanjun Wang, Jian Pei, Qi Tian, Yong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In the real world, the frequency of occurrence of objects is naturally skewed forming long-tail class distributions, which results in poor performance on the statistically rare classes. A promising solution is to mine tail-class examples to balance the training dataset. However, mining tail-class examples is a very challenging task. For instance, most of the otherwise successful uncertainty-based mining approaches struggle due to distortion of class probabilities resulting from skewness in data. In this work, we propose an effective, yet simple, approach to overcome these challenges. Our framework enhances the subdued tail-class activations and, thereafter, uses a one-class data-centric approach to effectively identify tail-class examples. We carry out an exhaustive evaluation of our framework on three datasets spanning over two computer vision tasks. Substantial improvements in the minority-class mining and fine-tuned model's performance strongly corroborate the value of our proposed solution.



### Gaze Estimation with Eye Region Segmentation and Self-Supervised Multistream Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.07878v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.07878v1)
- **Published**: 2021-12-15 04:44:45+00:00
- **Updated**: 2021-12-15 04:44:45+00:00
- **Authors**: Zunayed Mahmud, Paul Hungler, Ali Etemad
- **Comment**: 5 pages, 1 figure, 3 tables, Accepted in AAAI-22 Workshop on
  Human-Centric Self-Supervised Learning
- **Journal**: None
- **Summary**: We present a novel multistream network that learns robust eye representations for gaze estimation. We first create a synthetic dataset containing eye region masks detailing the visible eyeball and iris using a simulator. We then perform eye region segmentation with a U-Net type model which we later use to generate eye region masks for real-world eye images. Next, we pretrain an eye image encoder in the real domain with self-supervised contrastive learning to learn generalized eye representations. Finally, this pretrained eye encoder, along with two additional encoders for visible eyeball region and iris, are used in parallel in our multistream framework to extract salient features for gaze estimation from real-world images. We demonstrate the performance of our method on the EYEDIAP dataset in two different evaluation settings and achieve state-of-the-art results, outperforming all the existing benchmarks on this dataset. We also conduct additional experiments to validate the robustness of our self-supervised network with respect to different amounts of labeled data used for training.



### Does a Face Mask Protect my Privacy?: Deep Learning to Predict Protected Attributes from Masked Face Images
- **Arxiv ID**: http://arxiv.org/abs/2112.07879v2
- **DOI**: 10.1007/978-3-030-97546-3_8
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2112.07879v2)
- **Published**: 2021-12-15 04:46:19+00:00
- **Updated**: 2022-01-26 06:37:51+00:00
- **Authors**: Sachith Seneviratne, Nuran Kasthuriarachchi, Sanka Rasnayaka, Danula Hettiachchi, Ridwan Shariffdeen
- **Comment**: Accepted to AJCAI 2021 - 34th Australasian Joint Conference on
  Artificial Intelligence, Feb 2022, Sydney, Australia.
  http://ajcai2021.net/program
- **Journal**: None
- **Summary**: Contactless and efficient systems are implemented rapidly to advocate preventive methods in the fight against the COVID-19 pandemic. Despite the positive benefits of such systems, there is potential for exploitation by invading user privacy. In this work, we analyse the privacy invasiveness of face biometric systems by predicting privacy-sensitive soft-biometrics using masked face images. We train and apply a CNN based on the ResNet-50 architecture with 20,003 synthetic masked images and measure the privacy invasiveness. Despite the popular belief of the privacy benefits of wearing a mask among people, we show that there is no significant difference to privacy invasiveness when a mask is worn. In our experiments we were able to accurately predict sex (94.7%),race (83.1%) and age (MAE 6.21 and RMSE 8.33) from masked face images. Our proposed approach can serve as a baseline utility to evaluate the privacy-invasiveness of artificial intelligence systems that make use of privacy-sensitive information. We open-source all contributions for re-producibility and broader use by the research community.



### Robust Depth Completion with Uncertainty-Driven Loss Functions
- **Arxiv ID**: http://arxiv.org/abs/2112.07895v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.07895v2)
- **Published**: 2021-12-15 05:22:34+00:00
- **Updated**: 2021-12-28 04:35:12+00:00
- **Authors**: Yufan Zhu, Weisheng Dong, Leida Li, Jinjian Wu, Xin Li, Guangming Shi
- **Comment**: accepted by AAAI2022
- **Journal**: None
- **Summary**: Recovering a dense depth image from sparse LiDAR scans is a challenging task. Despite the popularity of color-guided methods for sparse-to-dense depth completion, they treated pixels equally during optimization, ignoring the uneven distribution characteristics in the sparse depth map and the accumulated outliers in the synthesized ground truth. In this work, we introduce uncertainty-driven loss functions to improve the robustness of depth completion and handle the uncertainty in depth completion. Specifically, we propose an explicit uncertainty formulation for robust depth completion with Jeffrey's prior. A parametric uncertain-driven loss is introduced and translated to new loss functions that are robust to noisy or missing data. Meanwhile, we propose a multiscale joint prediction model that can simultaneously predict depth and uncertainty maps. The estimated uncertainty map is also used to perform adaptive prediction on the pixels with high uncertainty, leading to a residual map for refining the completion results. Our method has been tested on KITTI Depth Completion Benchmark and achieved the state-of-the-art robustness performance in terms of MAE, IMAE, and IRMSE metrics.



### Homography Decomposition Networks for Planar Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2112.07909v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.07909v4)
- **Published**: 2021-12-15 06:13:32+00:00
- **Updated**: 2022-02-20 14:34:05+00:00
- **Authors**: Xinrui Zhan, Yueran Liu, Jianke Zhu, Yang Li
- **Comment**: Accepted at AAAI 2022, preprint version
- **Journal**: None
- **Summary**: Planar object tracking plays an important role in AI applications, such as robotics, visual servoing, and visual SLAM. Although the previous planar trackers work well in most scenarios, it is still a challenging task due to the rapid motion and large transformation between two consecutive frames. The essential reason behind this problem is that the condition number of such a non-linear system changes unstably when the searching range of the homography parameter space becomes larger. To this end, we propose a novel Homography Decomposition Networks(HDN) approach that drastically reduces and stabilizes the condition number by decomposing the homography transformation into two groups. Specifically, a similarity transformation estimator is designed to predict the first group robustly by a deep convolution equivariant network. By taking advantage of the scale and rotation estimation with high confidence, a residual transformation is estimated by a simple regression model. Furthermore, the proposed end-to-end network is trained in a semi-supervised fashion. Extensive experiments show that our proposed approach outperforms the state-of-the-art planar tracking methods at a large margin on the challenging POT, UCSB and POIC datasets.



### Decoupling Zero-Shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.07910v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.07910v2)
- **Published**: 2021-12-15 06:21:47+00:00
- **Updated**: 2022-04-15 10:28:07+00:00
- **Authors**: Jian Ding, Nan Xue, Gui-Song Xia, Dengxin Dai
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Zero-shot semantic segmentation (ZS3) aims to segment the novel categories that have not been seen in the training. Existing works formulate ZS3 as a pixel-level zeroshot classification problem, and transfer semantic knowledge from seen classes to unseen ones with the help of language models pre-trained only with texts. While simple, the pixel-level ZS3 formulation shows the limited capability to integrate vision-language models that are often pre-trained with image-text pairs and currently demonstrate great potential for vision tasks. Inspired by the observation that humans often perform segment-level semantic labeling, we propose to decouple the ZS3 into two sub-tasks: 1) a classagnostic grouping task to group the pixels into segments. 2) a zero-shot classification task on segments. The former task does not involve category information and can be directly transferred to group pixels for unseen classes. The latter task performs at segment-level and provides a natural way to leverage large-scale vision-language models pre-trained with image-text pairs (e.g. CLIP) for ZS3. Based on the decoupling formulation, we propose a simple and effective zero-shot semantic segmentation model, called ZegFormer, which outperforms the previous methods on ZS3 standard benchmarks by large margins, e.g., 22 points on the PASCAL VOC and 3 points on the COCO-Stuff in terms of mIoU for unseen classes. Code will be released at https://github.com/dingjiansw101/ZegFormer.



### A Comparative Analysis of Machine Learning Approaches for Automated Face Mask Detection During COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2112.07913v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.07913v1)
- **Published**: 2021-12-15 06:30:50+00:00
- **Updated**: 2021-12-15 06:30:50+00:00
- **Authors**: Junaed Younus Khan, Md Abdullah Al Alamin
- **Comment**: None
- **Journal**: None
- **Summary**: The World Health Organization (WHO) has recommended wearing face masks as one of the most effective measures to prevent COVID-19 transmission. In many countries, it is now mandatory to wear face masks, specially in public places. Since manual monitoring of face masks is often infeasible in the middle of the crowd, automatic detection can be beneficial. To facilitate that, we explored a number of deep learning models (i.e., VGG1, VGG19, ResNet50) for face-mask detection and evaluated them on two benchmark datasets. We also evaluated transfer learning (i.e., VGG19, ResNet50 pre-trained on ImageNet) in this context. We find that while the performances of all the models are quite good, transfer learning models achieve the best performance. Transfer learning improves the performance by 0.10\%--0.40\% with 30\% less training time. Our experiment also shows these high-performing models are not quite robust for real-world cases where the test dataset comes from a different distribution. Without any fine-tuning, the performance of these models drops by 47\% in cross-domain settings.



### SPTS: Single-Point Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/2112.07917v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.07917v6)
- **Published**: 2021-12-15 06:44:21+00:00
- **Updated**: 2022-08-29 15:20:49+00:00
- **Authors**: Dezhi Peng, Xinyu Wang, Yuliang Liu, Jiaxin Zhang, Mingxin Huang, Songxuan Lai, Shenggao Zhu, Jing Li, Dahua Lin, Chunhua Shen, Xiang Bai, Lianwen Jin
- **Comment**: Accepted by ACM MM 2022
- **Journal**: None
- **Summary**: Existing scene text spotting (i.e., end-to-end text detection and recognition) methods rely on costly bounding box annotations (e.g., text-line, word-level, or character-level bounding boxes). For the first time, we demonstrate that training scene text spotting models can be achieved with an extremely low-cost annotation of a single-point for each instance. We propose an end-to-end scene text spotting method that tackles scene text spotting as a sequence prediction task. Given an image as input, we formulate the desired detection and recognition results as a sequence of discrete tokens and use an auto-regressive Transformer to predict the sequence. The proposed method is simple yet effective, which can achieve state-of-the-art results on widely used benchmarks. Most significantly, we show that the performance is not very sensitive to the positions of the point annotation, meaning that it can be much easier to be annotated or even be automatically generated than the bounding box that requires precise positions. We believe that such a pioneer attempt indicates a significant opportunity for scene text spotting applications of a much larger scale than previously possible. The code is available at https://github.com/shannanyinxiang/SPTS.



### M-FasterSeg: An Efficient Semantic Segmentation Network Based on Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2112.07918v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.07918v3)
- **Published**: 2021-12-15 06:46:55+00:00
- **Updated**: 2022-05-30 02:37:44+00:00
- **Authors**: Junjun Wu, Huiyu Kuang, Qinghua Lu, Zeqin Lin, Qingwu Shi, Xilin Liu, Xiaoman Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Image semantic segmentation technology is one of the key technologies for intelligent systems to understand natural scenes. As one of the important research directions in the field of visual intelligence, this technology has broad application scenarios in the fields of mobile robots, drones, smart driving, and smart security. However, in the actual application of mobile robots, problems such as inaccurate segmentation semantic label prediction and loss of edge information of segmented objects and background may occur. This paper proposes an improved structure of a semantic segmentation network based on a deep learning network that combines self-attention neural network and neural network architecture search methods. First, a neural network search method NAS (Neural Architecture Search) is used to find a semantic segmentation network with multiple resolution branches. In the search process, combine the self-attention network structure module to adjust the searched neural network structure, and then combine the semantic segmentation network searched by different branches to form a fast semantic segmentation network structure, and input the picture into the network structure to get the final forecast result. The experimental results on the Cityscapes dataset show that the accuracy of the algorithm is 69.8%, and the segmentation speed is 48/s. It achieves a good balance between real-time and accuracy, can optimize edge segmentation, and has a better performance in complex scenes. Good robustness is suitable for practical application.



### Temporal Shuffling for Defending Deep Action Recognition Models against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2112.07921v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.07921v1)
- **Published**: 2021-12-15 06:57:01+00:00
- **Updated**: 2021-12-15 06:57:01+00:00
- **Authors**: Jaehui Hwang, Huan Zhang, Jun-Ho Choi, Cho-Jui Hsieh, Jong-Seok Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, video-based action recognition methods using convolutional neural networks (CNNs) achieve remarkable recognition performance. However, there is still lack of understanding about the generalization mechanism of action recognition models. In this paper, we suggest that action recognition models rely on the motion information less than expected, and thus they are robust to randomization of frame orders. Based on this observation, we develop a novel defense method using temporal shuffling of input videos against adversarial attacks for action recognition models. Another observation enabling our defense method is that adversarial perturbations on videos are sensitive to temporal destruction. To the best of our knowledge, this is the first attempt to design a defense method specific to video-based action recognition models.



### Imagine by Reasoning: A Reasoning-Based Implicit Semantic Data Augmentation for Long-Tailed Classification
- **Arxiv ID**: http://arxiv.org/abs/2112.07928v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.07928v2)
- **Published**: 2021-12-15 07:14:39+00:00
- **Updated**: 2022-02-11 03:30:07+00:00
- **Authors**: Xiaohua Chen, Yucan Zhou, Dayan Wu, Wanqian Zhang, Yu Zhou, Bo Li, Weiping Wang
- **Comment**: Accepted by AAAI2022
- **Journal**: None
- **Summary**: Real-world data often follows a long-tailed distribution, which makes the performance of existing classification algorithms degrade heavily. A key issue is that samples in tail categories fail to depict their intra-class diversity. Humans can imagine a sample in new poses, scenes, and view angles with their prior knowledge even if it is the first time to see this category. Inspired by this, we propose a novel reasoning-based implicit semantic data augmentation method to borrow transformation directions from other classes. Since the covariance matrix of each category represents the feature transformation directions, we can sample new directions from similar categories to generate definitely different instances. Specifically, the long-tailed distributed data is first adopted to train a backbone and a classifier. Then, a covariance matrix for each category is estimated, and a knowledge graph is constructed to store the relations of any two categories. Finally, tail samples are adaptively enhanced via propagating information from all the similar categories in the knowledge graph. Experimental results on CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018 have demonstrated the effectiveness of our proposed method compared with the state-of-the-art methods.



### From Noise to Feature: Exploiting Intensity Distribution as a Novel Soft Biometric Trait for Finger Vein Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.07931v1
- **DOI**: 10.1109/TIFS.2018.2866330
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.07931v1)
- **Published**: 2021-12-15 07:23:21+00:00
- **Updated**: 2021-12-15 07:23:21+00:00
- **Authors**: Wenxiong Kang, Yuting Lu, Dejian Li, Wei Jia
- **Comment**: 11 pages
- **Journal**: IEEE transactions on information forensics and security 14.4
  (2018): 858-869
- **Summary**: Most finger vein feature extraction algorithms achieve satisfactory performance due to their texture representation abilities, despite simultaneously ignoring the intensity distribution that is formed by the finger tissue, and in some cases, processing it as background noise. In this paper, we exploit this kind of noise as a novel soft biometric trait for achieving better finger vein recognition performance. First, a detailed analysis of the finger vein imaging principle and the characteristics of the image are presented to show that the intensity distribution that is formed by the finger tissue in the background can be extracted as a soft biometric trait for recognition. Then, two finger vein background layer extraction algorithms and three soft biometric trait extraction algorithms are proposed for intensity distribution feature extraction. Finally, a hybrid matching strategy is proposed to solve the issue of dimension difference between the primary and soft biometric traits on the score level. A series of rigorous contrast experiments on three open-access databases demonstrates that our proposed method is feasible and effective for finger vein recognition.



### Efficient Geometry-aware 3D Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2112.07945v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.07945v2)
- **Published**: 2021-12-15 08:01:43+00:00
- **Updated**: 2022-04-27 18:25:14+00:00
- **Authors**: Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, Gordon Wetzstein
- **Comment**: Project page: https://matthew-a-chan.github.io/EG3D
- **Journal**: None
- **Summary**: Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute-intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit-implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.



### Transcoded Video Restoration by Temporal Spatial Auxiliary Network
- **Arxiv ID**: http://arxiv.org/abs/2112.07948v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.07948v1)
- **Published**: 2021-12-15 08:10:23+00:00
- **Updated**: 2021-12-15 08:10:23+00:00
- **Authors**: Li Xu, Gang He, Jinjia Zhou, Jie Lei, Weiying Xie, Yunsong Li, Yu-Wing Tai
- **Comment**: Accepted by AAAI2022
- **Journal**: None
- **Summary**: In most video platforms, such as Youtube, and TikTok, the played videos usually have undergone multiple video encodings such as hardware encoding by recording devices, software encoding by video editing apps, and single/multiple video transcoding by video application servers. Previous works in compressed video restoration typically assume the compression artifacts are caused by one-time encoding. Thus, the derived solution usually does not work very well in practice. In this paper, we propose a new method, temporal spatial auxiliary network (TSAN), for transcoded video restoration. Our method considers the unique traits between video encoding and transcoding, and we consider the initial shallow encoded videos as the intermediate labels to assist the network to conduct self-supervised attention training. In addition, we employ adjacent multi-frame information and propose the temporal deformable alignment and pyramidal spatial fusion for transcoded video restoration. The experimental results demonstrate that the performance of the proposed method is superior to that of the previous techniques. The code is available at https://github.com/icecherylXuli/TSAN.



### Object Pursuit: Building a Space of Objects via Discriminative Weight Generation
- **Arxiv ID**: http://arxiv.org/abs/2112.07954v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.07954v3)
- **Published**: 2021-12-15 08:25:30+00:00
- **Updated**: 2022-04-03 03:10:34+00:00
- **Authors**: Chuanyu Pan, Yanchao Yang, Kaichun Mo, Yueqi Duan, Leonidas Guibas
- **Comment**: 24 pages. This paper has been accepted by ICLR2022 (OpenReview:
  https://openreview.net/forum?id=lbauk6wK2-y)
- **Journal**: None
- **Summary**: We propose a framework to continuously learn object-centric representations for visual learning and understanding. Existing object-centric representations either rely on supervisions that individualize objects in the scene, or perform unsupervised disentanglement that can hardly deal with complex scenes in the real world. To mitigate the annotation burden and relax the constraints on the statistical complexity of the data, our method leverages interactions to effectively sample diverse variations of an object and the corresponding training signals while learning the object-centric representations. Throughout learning, objects are streamed one by one in random order with unknown identities, and are associated with latent codes that can synthesize discriminative weights for each object through a convolutional hypernetwork. Moreover, re-identification of learned objects and forgetting prevention are employed to make the learning process efficient and robust. We perform an extensive study of the key features of the proposed framework and analyze the characteristics of the learned representations. Furthermore, we demonstrate the capability of the proposed framework in learning representations that can improve label efficiency in downstream tasks. Our code and trained models are made publicly available at: https://github.com/pptrick/Object-Pursuit.



### FEAR: Fast, Efficient, Accurate and Robust Visual Tracker
- **Arxiv ID**: http://arxiv.org/abs/2112.07957v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.07957v2)
- **Published**: 2021-12-15 08:28:55+00:00
- **Updated**: 2022-07-19 09:03:17+00:00
- **Authors**: Vasyl Borsuk, Roman Vei, Orest Kupyn, Tetiana Martyniuk, Igor Krashenyi, Jiři Matas
- **Comment**: None
- **Journal**: None
- **Summary**: We present FEAR, a family of fast, efficient, accurate, and robust Siamese visual trackers. We present a novel and efficient way to benefit from dual-template representation for object model adaption, which incorporates temporal information with only a single learnable parameter. We further improve the tracker architecture with a pixel-wise fusion block. By plugging-in sophisticated backbones with the abovementioned modules, FEAR-M and FEAR-L trackers surpass most Siamese trackers on several academic benchmarks in both accuracy and efficiency. Employed with the lightweight backbone, the optimized version FEAR-XS offers more than 10 times faster tracking than current Siamese trackers while maintaining near state-of-the-art results. FEAR-XS tracker is 2.4x smaller and 4.3x faster than LightTrack with superior accuracy. In addition, we expand the definition of the model efficiency by introducing FEAR benchmark that assesses energy consumption and execution speed. We show that energy consumption is a limiting factor for trackers on mobile devices. Source code, pretrained models, and evaluation protocol are available at https://github.com/PinataFarms/FEARTracker.



### A learning-based approach to feature recognition of Engineering shapes
- **Arxiv ID**: http://arxiv.org/abs/2112.07962v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T01 68T01 68T01 68T01 (primary), 68U07 (secondary), J.6; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/2112.07962v1)
- **Published**: 2021-12-15 08:35:18+00:00
- **Updated**: 2021-12-15 08:35:18+00:00
- **Authors**: Lakshmi Priya Muraleedharan, Ramanathan Muthuganapathy
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a machine learning approach to recognise engineering shape features such as holes, slots, etc. in a CAD mesh model. With the advent of digital archiving, newer manufacturing techniques such as 3D printing, scanning of components and reverse engineering, CAD data is proliferated in the form of mesh model representation. As the number of nodes and edges become larger in a mesh model as well as the possibility of presence of noise, direct application of graph-based approaches would not only be expensive but also difficult to be tuned for noisy data. Hence, this calls for newer approaches to be devised for feature recognition for CAD models represented in the form of mesh. Here, we show that a discrete version of Gauss map can be used as a signature for a feature learning. We show that this approach not only requires fewer memory requirements but also the training time is quite less. As no network architecture is involved, the number of hyperparameters are much lesser and can be tuned in a much faster time. The recognition accuracy is also very similar to that of the one obtained using 3D convolutional neural networks (CNN) but in much lesser running time and storage requirements. A comparison has been done with other non-network based machine learning approaches to show that our approach has the highest accuracy. We also show the recognition results for CAD models having multiple features as well as complex/interacting features obtained from public benchmarks. The ability to handle noisy data has also been demonstrated.



### Towards General and Efficient Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.07963v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.07963v2)
- **Published**: 2021-12-15 08:35:28+00:00
- **Updated**: 2022-03-20 19:47:04+00:00
- **Authors**: Yichen Xie, Masayoshi Tomizuka, Wei Zhan
- **Comment**: None
- **Journal**: None
- **Summary**: Active learning selects the most informative samples to exploit limited annotation budgets. Existing work follows a cumbersome pipeline that repeats the time-consuming model training and batch data selection multiple times. In this paper, we challenge this status quo by proposing a novel general and efficient active learning (GEAL) method following our designed new pipeline. Utilizing a publicly available pretrained model, our method selects data from different datasets with a single-pass inference of the same model without extra training or supervision. To capture subtle local information, we propose knowledge clusters extracted from intermediate features. Free from the troublesome batch selection strategy, all data samples are selected in one-shot through a distance-based sampling in the fine-grained knowledge cluster level. This whole process is faster than prior arts by hundreds of times. Extensive experiments verify the effectiveness of our method on object detection, image classification, and semantic segmentation. Our code is publicly available in https://github.com/yichen928/GEAL_active_learning.



### Modality-Aware Triplet Hard Mining for Zero-shot Sketch-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2112.07966v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.07966v2)
- **Published**: 2021-12-15 08:36:44+00:00
- **Updated**: 2021-12-16 02:13:22+00:00
- **Authors**: Zongheng Huang, YiFan Sun, Chuchu Han, Changxin Gao, Nong Sang
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: This paper tackles the Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) problem from the viewpoint of cross-modality metric learning. This task has two characteristics: 1) the zero-shot setting requires a metric space with good within-class compactness and the between-class discrepancy for recognizing the novel classes and 2) the sketch query and the photo gallery are in different modalities. The metric learning viewpoint benefits ZS-SBIR from two aspects. First, it facilitates improvement through recent good practices in deep metric learning (DML). By combining two fundamental learning approaches in DML, e.g., classification training and pairwise training, we set up a strong baseline for ZS-SBIR. Without bells and whistles, this baseline achieves competitive retrieval accuracy. Second, it provides an insight that properly suppressing the modality gap is critical. To this end, we design a novel method named Modality-Aware Triplet Hard Mining (MATHM). MATHM enhances the baseline with three types of pairwise learning, e.g., a cross-modality sample pair, a within-modality sample pair, and their combination.\We also design an adaptive weighting method to balance these three components during training dynamically. Experimental results confirm that MATHM brings another round of significant improvement based on the strong baseline and sets up new state-of-the-art performance. For example, on the TU-Berlin dataset, we achieve 47.88+2.94% mAP@all and 58.28+2.34% Prec@100. Code will be publicly available at: https://github.com/huangzongheng/MATHM.



### Predicting Media Memorability: Comparing Visual, Textual and Auditory Features
- **Arxiv ID**: http://arxiv.org/abs/2112.07969v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.07969v1)
- **Published**: 2021-12-15 08:42:49+00:00
- **Updated**: 2021-12-15 08:42:49+00:00
- **Authors**: Lorin Sweeney, Graham Healy, Alan F. Smeaton
- **Comment**: 3 pages
- **Journal**: None
- **Summary**: This paper describes our approach to the Predicting Media Memorability task in MediaEval 2021, which aims to address the question of media memorability by setting the task of automatically predicting video memorability. This year we tackle the task from a comparative standpoint, looking to gain deeper insights into each of three explored modalities, and using our results from last year's submission (2020) as a point of reference. Our best performing short-term memorability model (0.132) tested on the TRECVid2019 dataset -- just like last year -- was a frame based CNN that was not trained on any TRECVid data, and our best short-term memorability model (0.524) tested on the Memento10k dataset, was a Bayesian Ride Regressor fit with DenseNet121 visual features.



### Detail-aware Deep Clothing Animations Infused with Multi-source Attributes
- **Arxiv ID**: http://arxiv.org/abs/2112.07974v1
- **DOI**: 10.1111/cgf.14651
- **Categories**: **cs.CV**, cs.GR, 68U05 (Primary), 68T07 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/2112.07974v1)
- **Published**: 2021-12-15 08:50:49+00:00
- **Updated**: 2021-12-15 08:50:49+00:00
- **Authors**: Tianxing Li, Rui Shi, Takashi Kanai
- **Comment**: 14 pages, 12 figures
- **Journal**: None
- **Summary**: This paper presents a novel learning-based clothing deformation method to generate rich and reasonable detailed deformations for garments worn by bodies of various shapes in various animations. In contrast to existing learning-based methods, which require numerous trained models for different garment topologies or poses and are unable to easily realize rich details, we use a unified framework to produce high fidelity deformations efficiently and easily. To address the challenging issue of predicting deformations influenced by multi-source attributes, we propose three strategies from novel perspectives. Specifically, we first found that the fit between the garment and the body has an important impact on the degree of folds. We then designed an attribute parser to generate detail-aware encodings and infused them into the graph neural network, therefore enhancing the discrimination of details under diverse attributes. Furthermore, to achieve better convergence and avoid overly smooth deformations, we proposed output reconstruction to mitigate the complexity of the learning task. Experiment results show that our proposed deformation method achieves better performance over existing methods in terms of generalization ability and quality of details.



### Temporal Action Proposal Generation with Background Constraint
- **Arxiv ID**: http://arxiv.org/abs/2112.07984v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.07984v1)
- **Published**: 2021-12-15 09:20:49+00:00
- **Updated**: 2021-12-15 09:20:49+00:00
- **Authors**: Haosen Yang, Wenhao Wu, Lining Wang, Sheng Jin, Boyang Xia, Hongxun Yao, Hujie Huang
- **Comment**: Accepted by AAAI2022. arXiv admin note: text overlap with
  arXiv:2105.12043
- **Journal**: None
- **Summary**: Temporal action proposal generation (TAPG) is a challenging task that aims to locate action instances in untrimmed videos with temporal boundaries. To evaluate the confidence of proposals, the existing works typically predict action score of proposals that are supervised by the temporal Intersection-over-Union (tIoU) between proposal and the ground-truth. In this paper, we innovatively propose a general auxiliary Background Constraint idea to further suppress low-quality proposals, by utilizing the background prediction score to restrict the confidence of proposals. In this way, the Background Constraint concept can be easily plug-and-played into existing TAPG methods (e.g., BMN, GTAD). From this perspective, we propose the Background Constraint Network (BCNet) to further take advantage of the rich information of action and background. Specifically, we introduce an Action-Background Interaction module for reliable confidence evaluation, which models the inconsistency between action and background by attention mechanisms at the frame and clip levels. Extensive experiments are conducted on two popular benchmarks, i.e., ActivityNet-1.3 and THUMOS14. The results demonstrate that our method outperforms state-of-the-art methods. Equipped with the existing action classifier, our method also achieves remarkable performance on the temporal action localization task.



### Self-Ensembling GAN for Cross-Domain Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.07999v2
- **DOI**: 10.1109/TMM.2022.3229976
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.07999v2)
- **Published**: 2021-12-15 09:50:25+00:00
- **Updated**: 2022-12-30 09:42:20+00:00
- **Authors**: Yonghao Xu, Fengxiang He, Bo Du, Dacheng Tao, Liangpei Zhang
- **Comment**: None
- **Journal**: IEEE Trans. Multimedia, 2022
- **Summary**: Deep neural networks (DNNs) have greatly contributed to the performance gains in semantic segmentation. Nevertheless, training DNNs generally requires large amounts of pixel-level labeled data, which is expensive and time-consuming to collect in practice. To mitigate the annotation burden, this paper proposes a self-ensembling generative adversarial network (SE-GAN) exploiting cross-domain data for semantic segmentation. In SE-GAN, a teacher network and a student network constitute a self-ensembling model for generating semantic segmentation maps, which together with a discriminator, forms a GAN. Despite its simplicity, we find SE-GAN can significantly boost the performance of adversarial training and enhance the stability of the model, the latter of which is a common barrier shared by most adversarial training-based methods. We theoretically analyze SE-GAN and provide an $\mathcal O(1/\sqrt{N})$ generalization bound ($N$ is the training sample size), which suggests controlling the discriminator's hypothesis complexity to enhance the generalizability. Accordingly, we choose a simple network as the discriminator. Extensive and systematic experiments in two standard settings demonstrate that the proposed method significantly outperforms current state-of-the-art approaches. The source code of our model is available online (https://github.com/YonghaoXu/SE-GAN).



### Autoencoder-based background reconstruction and foreground segmentation with background noise estimation
- **Arxiv ID**: http://arxiv.org/abs/2112.08001v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2112.08001v2)
- **Published**: 2021-12-15 09:51:00+00:00
- **Updated**: 2022-05-10 15:52:53+00:00
- **Authors**: Bruno Sauvalle, Arnaud de La Fortelle
- **Comment**: None
- **Journal**: None
- **Summary**: Even after decades of research, dynamic scene background reconstruction and foreground object segmentation are still considered as open problems due various challenges such as illumination changes, camera movements, or background noise caused by air turbulence or moving trees. We propose in this paper to model the background of a frame sequence as a low dimensional manifold using an autoencoder and compare the reconstructed background provided by this autoencoder with the original image to compute the foreground/background segmentation masks. The main novelty of the proposed model is that the autoencoder is also trained to predict the background noise, which allows to compute for each frame a pixel-dependent threshold to perform the foreground segmentation. Although the proposed model does not use any temporal or motion information, it exceeds the state of the art for unsupervised background subtraction on the CDnet 2014 and LASIESTA datasets, with a significant improvement on videos where the camera is moving. It is also able to perform background reconstruction on some non-video image datasets.



### Consistent Depth Prediction under Various Illuminations using Dilated Cross Attention
- **Arxiv ID**: http://arxiv.org/abs/2112.08006v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.08006v1)
- **Published**: 2021-12-15 10:02:46+00:00
- **Updated**: 2021-12-15 10:02:46+00:00
- **Authors**: Zitian Zhang, Chuhua Xian
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: In this paper, we aim to solve the problem of consistent depth prediction in complex scenes under various illumination conditions. The existing indoor datasets based on RGB-D sensors or virtual rendering have two critical limitations - sparse depth maps (NYU Depth V2) and non-realistic illumination (SUN CG, SceneNet RGB-D). We propose to use internet 3D indoor scenes and manually tune their illuminations to render photo-realistic RGB photos and their corresponding depth and BRDF maps, obtaining a new indoor depth dataset called Vari dataset. We propose a simple convolutional block named DCA by applying depthwise separable dilated convolution on encoded features to process global information and reduce parameters. We perform cross attention on these dilated features to retain the consistency of depth prediction under different illuminations. Our method is evaluated by comparing it with current state-of-the-art methods on Vari dataset and a significant improvement is observed in our experiments. We also conduct the ablation study, finetune our model on NYU Depth V2 and also evaluate on real-world data to further validate the effectiveness of our DCA block. The code, pre-trained weights and Vari dataset are open-sourced.



### LTB curves with Lipschitz turn are par-regular
- **Arxiv ID**: http://arxiv.org/abs/2112.09567v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV, cs.DM
- **Links**: [PDF](http://arxiv.org/pdf/2112.09567v1)
- **Published**: 2021-12-15 10:10:07+00:00
- **Updated**: 2021-12-15 10:10:07+00:00
- **Authors**: Etienne Le Quentrec, Loïc Mazo, Étienne Baudrier, Mohamed Tajine
- **Comment**: None
- **Journal**: None
- **Summary**: Preserving the topology during a digitization process is a requirement of first importance. To this end, it is classical in Digital Geometry to assume the shape borders to be par-regular. Par-regularity was proved to be equivalent to having positive reach or to belong to the class C 1,1 of curves with Lipschitz derivative. Recently, we proposed to use a larger class that encompasses polygons with obtuse angles, the locally turn-bounded curves. The aim of this technical report is to define the class of par-regular curves inside the class of locally turn-bounded curves using only the notion of turn, that is of integral curvature. To be more precise, in a previous article, we have already proved that par-regular curves are locally turn-bounded. Incidentally this proof lead us to show that the turn of par-regular curves is a Lipschitz function of their length. We call the class of curves verifying this latter property the curves with Lipschitz turn. In this technical report, we prove the converse assertion : locally turn-bounded curves with Lipschitz turn are par-regular. The equivalence is stated in Theorem 3.1 and the converse assertion is proved in Lemma 3.2. In section 1, we recall the definition of par-regularity and equivalently of sets with positive reach. In section 2, we present the notions of curves locally turn-bounded and of curves with Lipschitz turn. Throughout this latter section, some of intermediate steps (Lemmas 2.3 and 2.11) are proved just after the introduction of their related notions. The last section (section 3) is dedicated to the proof of the equivalence of the notions.



### MissMarple : A Novel Socio-inspired Feature-transfer Learning Deep Network for Image Splicing Detection
- **Arxiv ID**: http://arxiv.org/abs/2112.08018v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.08018v1)
- **Published**: 2021-12-15 10:32:47+00:00
- **Updated**: 2021-12-15 10:32:47+00:00
- **Authors**: Angelina L. Gokhale, Dhanya Pramod, Sudeep D. Thepade, Ravi Kulkarni
- **Comment**: 27 pages, 6 figures and 15 tables
- **Journal**: None
- **Summary**: In this paper we propose a novel socio-inspired convolutional neural network (CNN) deep learning model for image splicing detection. Based on the premise that learning from the detection of coarsely spliced image regions can improve the detection of visually imperceptible finely spliced image forgeries, the proposed model referred to as, MissMarple, is a twin CNN network involving feature-transfer learning. Results obtained from training and testing the proposed model using the benchmark datasets like Columbia splicing, WildWeb, DSO1 and a proposed dataset titled AbhAS consisting of realistic splicing forgeries revealed improvement in detection accuracy over the existing deep learning models.



### Segmentation-Reconstruction-Guided Facial Image De-occlusion
- **Arxiv ID**: http://arxiv.org/abs/2112.08022v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.08022v1)
- **Published**: 2021-12-15 10:40:08+00:00
- **Updated**: 2021-12-15 10:40:08+00:00
- **Authors**: Xiangnan Yin, Di Huang, Zehua Fu, Yunhong Wang, Liming Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Occlusions are very common in face images in the wild, leading to the degraded performance of face-related tasks. Although much effort has been devoted to removing occlusions from face images, the varying shapes and textures of occlusions still challenge the robustness of current methods. As a result, current methods either rely on manual occlusion masks or only apply to specific occlusions. This paper proposes a novel face de-occlusion model based on face segmentation and 3D face reconstruction, which automatically removes all kinds of face occlusions with even blurred boundaries,e.g., hairs. The proposed model consists of a 3D face reconstruction module, a face segmentation module, and an image generation module. With the face prior and the occlusion mask predicted by the first two, respectively, the image generation module can faithfully recover the missing facial textures. To supervise the training, we further build a large occlusion dataset, with both manually labeled and synthetic occlusions. Qualitative and quantitative results demonstrate the effectiveness and robustness of the proposed method.



### LookinGood^π: Real-time Person-independent Neural Re-rendering for High-quality Human Performance Capture
- **Arxiv ID**: http://arxiv.org/abs/2112.08037v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.08037v2)
- **Published**: 2021-12-15 11:00:21+00:00
- **Updated**: 2023-02-16 14:18:56+00:00
- **Authors**: Xiqi Yang, Kewei Yang, Kang Chen, Weidong Zhang, Weiwei Xu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose LookinGood^{\pi}, a novel neural re-rendering approach that is aimed to (1) improve the rendering quality of the low-quality reconstructed results from human performance capture system in real-time; (2) improve the generalization ability of the neural rendering network on unseen people. Our key idea is to utilize the rendered image of reconstructed geometry as the guidance to assist the prediction of person-specific details from few reference images, thus enhancing the re-rendered result. In light of this, we design a two-branch network. A coarse branch is designed to fix some artifacts (i.e. holes, noise) and obtain a coarse version of the rendered input, while a detail branch is designed to predict "correct" details from the warped references. The guidance of the rendered image is realized by blending features from two branches effectively in the training of the detail branch, which improves both the warping accuracy and the details' fidelity. We demonstrate that our method outperforms state-of-the-art methods at producing high-fidelity images on unseen people.



### Exploring the Asynchronous of the Frequency Spectra of GAN-generated Facial Images
- **Arxiv ID**: http://arxiv.org/abs/2112.08050v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2112.08050v1)
- **Published**: 2021-12-15 11:34:11+00:00
- **Updated**: 2021-12-15 11:34:11+00:00
- **Authors**: Binh M. Le, Simon S. Woo
- **Comment**: International Workshop on Safety and Security of Deep Learning IJCAI,
  2021
- **Journal**: None
- **Summary**: The rapid progression of Generative Adversarial Networks (GANs) has raised a concern of their misuse for malicious purposes, especially in creating fake face images. Although many proposed methods succeed in detecting GAN-based synthetic images, they are still limited by the need for large quantities of the training fake image dataset and challenges for the detector's generalizability to unknown facial images. In this paper, we propose a new approach that explores the asynchronous frequency spectra of color channels, which is simple but effective for training both unsupervised and supervised learning models to distinguish GAN-based synthetic images. We further investigate the transferability of a training model that learns from our suggested features in one source domain and validates on another target domains with prior knowledge of the features' distribution. Our experimental results show that the discrepancy of spectra in the frequency domain is a practical artifact to effectively detect various types of GAN-based generated images.



### Leveraging Image-based Generative Adversarial Networks for Time Series Generation
- **Arxiv ID**: http://arxiv.org/abs/2112.08060v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2112.08060v2)
- **Published**: 2021-12-15 11:55:11+00:00
- **Updated**: 2023-08-31 12:41:13+00:00
- **Authors**: Justin Hellermann, Stefan Lessmann
- **Comment**: None
- **Journal**: None
- **Summary**: Generative models for images have gained significant attention in computer vision and natural language processing due to their ability to generate realistic samples from complex data distributions. To leverage the advances of image-based generative models for the time series domain, we propose a two-dimensional image representation for time series, the Extended Intertemporal Return Plot (XIRP). Our approach captures the intertemporal time series dynamics in a scale-invariant and invertible way, reducing training time and improving sample quality. We benchmark synthetic XIRPs obtained by an off-the-shelf Wasserstein GAN with gradient penalty (WGAN-GP) to other image representations and models regarding similarity and predictive ability metrics. Our novel, validated image representation for time series consistently and significantly outperforms a state-of-the-art RNN-based generative model regarding predictive ability. Further, we introduce an improved stochastic inversion to substantially improve simulation quality regardless of the representation and provide the prospect of transfer potentials in other domains.



### Depth Refinement for Improved Stereo Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2112.08070v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.08070v1)
- **Published**: 2021-12-15 12:21:08+00:00
- **Updated**: 2021-12-15 12:21:08+00:00
- **Authors**: Amit Bracha, Noam Rotstein, David Bensaïd, Ron Slossberg, Ron Kimmel
- **Comment**: None
- **Journal**: None
- **Summary**: Depth estimation is a cornerstone of a vast number of applications requiring 3D assessment of the environment, such as robotics, augmented reality, and autonomous driving to name a few. One prominent technique for depth estimation is stereo matching which has several advantages: it is considered more accessible than other depth-sensing technologies, can produce dense depth estimates in real-time, and has benefited greatly from the advances of deep learning in recent years. However, current techniques for depth estimation from stereoscopic images still suffer from a built-in drawback. To reconstruct depth, a stereo matching algorithm first estimates the disparity map between the left and right images before applying a geometric triangulation. A simple analysis reveals that the depth error is quadratically proportional to the object's distance. Therefore, constant disparity errors are translated to large depth errors for objects far from the camera. To mitigate this quadratic relation, we propose a simple but effective method that uses a refinement network for depth estimation. We show analytical and empirical results suggesting that the proposed learning procedure reduces this quadratic relation. We evaluate the proposed refinement procedure on well-known benchmarks and datasets, like Sceneflow and KITTI datasets, and demonstrate significant improvements in the depth accuracy metric.



### Image-Adaptive YOLO for Object Detection in Adverse Weather Conditions
- **Arxiv ID**: http://arxiv.org/abs/2112.08088v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.08088v3)
- **Published**: 2021-12-15 12:54:17+00:00
- **Updated**: 2022-07-04 09:10:22+00:00
- **Authors**: Wenyu Liu, Gaofeng Ren, Runsheng Yu, Shi Guo, Jianke Zhu, Lei Zhang
- **Comment**: AAAI 2022, Preprint version with Appendix
- **Journal**: None
- **Summary**: Though deep learning-based object detection methods have achieved promising results on the conventional datasets, it is still challenging to locate objects from the low-quality images captured in adverse weather conditions. The existing methods either have difficulties in balancing the tasks of image enhancement and object detection, or often ignore the latent information beneficial for detection. To alleviate this problem, we propose a novel Image-Adaptive YOLO (IA-YOLO) framework, where each image can be adaptively enhanced for better detection performance. Specifically, a differentiable image processing (DIP) module is presented to take into account the adverse weather conditions for YOLO detector, whose parameters are predicted by a small convolutional neural net-work (CNN-PP). We learn CNN-PP and YOLOv3 jointly in an end-to-end fashion, which ensures that CNN-PP can learn an appropriate DIP to enhance the image for detection in a weakly supervised manner. Our proposed IA-YOLO approach can adaptively process images in both normal and adverse weather conditions. The experimental results are very encouraging, demonstrating the effectiveness of our proposed IA-YOLO method in both foggy and low-light scenarios.



### Vision Transformer Based Video Hashing Retrieval for Tracing the Source of Fake Videos
- **Arxiv ID**: http://arxiv.org/abs/2112.08117v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.08117v2)
- **Published**: 2021-12-15 13:35:55+00:00
- **Updated**: 2022-09-06 04:17:44+00:00
- **Authors**: Pengfei Pei, Xianfeng Zhao, Yun Cao, Jinchuan Li, Xuyuan Lai
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the spread of fake videos has brought great influence on individuals and even countries. It is important to provide robust and reliable results for fake videos. The results of conventional detection methods are not reliable and not robust for unseen videos. Another alternative and more effective way is to find the original video of the fake video. For example, fake videos from the Russia-Ukraine war and the Hong Kong law revision storm are refuted by finding the original video. We use an improved retrieval method to find the original video, named ViTHash. Specifically, tracing the source of fake videos requires finding the unique one, which is difficult when there are only small differences in the original videos. To solve the above problems, we designed a novel loss Hash Triplet Loss. In addition, we designed a tool called Localizator to compare the difference between the original traced video and the fake video. We have done extensive experiments on FaceForensics++, Celeb-DF and DeepFakeDetection, and we also have done additional experiments on our built three datasets: DAVIS2016-TL (video inpainting), VSTL (video splicing) and DFTL (similar videos). Experiments have shown that our performance is better than state-of-the-art methods, especially in cross-dataset mode. Experiments also demonstrated that ViTHash is effective in various forgery detection: video inpainting, video splicing and deepfakes. Our code and datasets have been released on GitHub: \url{https://github.com/lajlksdf/vtl}.



### Self-Supervised Monocular Depth and Ego-Motion Estimation in Endoscopy: Appearance Flow to the Rescue
- **Arxiv ID**: http://arxiv.org/abs/2112.08122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.08122v1)
- **Published**: 2021-12-15 13:51:10+00:00
- **Updated**: 2021-12-15 13:51:10+00:00
- **Authors**: Shuwei Shao, Zhongcai Pei, Weihai Chen, Wentao Zhu, Xingming Wu, Dianmin Sun, Baochang Zhang
- **Comment**: Accepted by Medical Image Analysis
- **Journal**: None
- **Summary**: Recently, self-supervised learning technology has been applied to calculate depth and ego-motion from monocular videos, achieving remarkable performance in autonomous driving scenarios. One widely adopted assumption of depth and ego-motion self-supervised learning is that the image brightness remains constant within nearby frames. Unfortunately, the endoscopic scene does not meet this assumption because there are severe brightness fluctuations induced by illumination variations, non-Lambertian reflections and interreflections during data collection, and these brightness fluctuations inevitably deteriorate the depth and ego-motion estimation accuracy. In this work, we introduce a novel concept referred to as appearance flow to address the brightness inconsistency problem. The appearance flow takes into consideration any variations in the brightness pattern and enables us to develop a generalized dynamic image constraint. Furthermore, we build a unified self-supervised framework to estimate monocular depth and ego-motion simultaneously in endoscopic scenes, which comprises a structure module, a motion module, an appearance module and a correspondence module, to accurately reconstruct the appearance and calibrate the image brightness. Extensive experiments are conducted on the SCARED dataset and EndoSLAM dataset, and the proposed unified framework exceeds other self-supervised approaches by a large margin. To validate our framework's generalization ability on different patients and cameras, we train our model on SCARED but test it on the SERV-CT and Hamlyn datasets without any fine-tuning, and the superior results reveal its strong generalization ability. Code will be available at: \url{https://github.com/ShuweiShao/AF-SfMLearner}.



### Improving Self-supervised Learning with Automated Unsupervised Outlier Arbitration
- **Arxiv ID**: http://arxiv.org/abs/2112.08132v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.08132v1)
- **Published**: 2021-12-15 14:05:23+00:00
- **Updated**: 2021-12-15 14:05:23+00:00
- **Authors**: Yu Wang, Jingyang Lin, Jingjing Zou, Yingwei Pan, Ting Yao, Tao Mei
- **Comment**: NeurIPS 2021; Code is publicly available at:
  https://github.com/ssl-codelab/uota
- **Journal**: None
- **Summary**: Our work reveals a structured shortcoming of the existing mainstream self-supervised learning methods. Whereas self-supervised learning frameworks usually take the prevailing perfect instance level invariance hypothesis for granted, we carefully investigate the pitfalls behind. Particularly, we argue that the existing augmentation pipeline for generating multiple positive views naturally introduces out-of-distribution (OOD) samples that undermine the learning of the downstream tasks. Generating diverse positive augmentations on the input does not always pay off in benefiting downstream tasks. To overcome this inherent deficiency, we introduce a lightweight latent variable model UOTA, targeting the view sampling issue for self-supervised learning. UOTA adaptively searches for the most important sampling region to produce views, and provides viable choice for outlier-robust self-supervised learning approaches. Our method directly generalizes to many mainstream self-supervised learning approaches, regardless of the loss's nature contrastive or not. We empirically show UOTA's advantage over the state-of-the-art self-supervised paradigms with evident margin, which well justifies the existence of the OOD sample issue embedded in the existing approaches. Especially, we theoretically prove that the merits of the proposal boil down to guaranteed estimator variance and bias reduction. Code is available: at https://github.com/ssl-codelab/uota.



### Multi-View Depth Estimation by Fusing Single-View Depth Probability with Multi-View Geometry
- **Arxiv ID**: http://arxiv.org/abs/2112.08177v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.08177v2)
- **Published**: 2021-12-15 14:56:53+00:00
- **Updated**: 2022-03-29 17:57:07+00:00
- **Authors**: Gwangbin Bae, Ignas Budvytis, Roberto Cipolla
- **Comment**: CVPR 2022 (oral)
- **Journal**: None
- **Summary**: Multi-view depth estimation methods typically require the computation of a multi-view cost-volume, which leads to huge memory consumption and slow inference. Furthermore, multi-view matching can fail for texture-less surfaces, reflective surfaces and moving objects. For such failure modes, single-view depth estimation methods are often more reliable. To this end, we propose MaGNet, a novel framework for fusing single-view depth probability with multi-view geometry, to improve the accuracy, robustness and efficiency of multi-view depth estimation. For each frame, MaGNet estimates a single-view depth probability distribution, parameterized as a pixel-wise Gaussian. The distribution estimated for the reference frame is then used to sample per-pixel depth candidates. Such probabilistic sampling enables the network to achieve higher accuracy while evaluating fewer depth candidates. We also propose depth consistency weighting for the multi-view matching score, to ensure that the multi-view depth is consistent with the single-view predictions. The proposed method achieves state-of-the-art performance on ScanNet, 7-Scenes and KITTI. Qualitative evaluation demonstrates that our method is more robust against challenging artifacts such as texture-less/reflective surfaces and moving objects. Our code and model weights are available at https://github.com/baegwangbin/MaGNet.



### Quantitative analysis of visual representation of sign elements in COVID-19 context
- **Arxiv ID**: http://arxiv.org/abs/2112.08219v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2112.08219v1)
- **Published**: 2021-12-15 15:54:53+00:00
- **Updated**: 2021-12-15 15:54:53+00:00
- **Authors**: María Jesús Cano-Martínez, Miguel Carrasco, Joaquín Sandoval, César González-Martín
- **Comment**: None
- **Journal**: None
- **Summary**: Representation is the way in which human beings re-present the reality of what is happening, both externally and internally. Thus, visual representation as a means of communication uses elements to build a narrative, just as spoken and written language do. We propose using computer analysis to perform a quantitative analysis of the elements used in the visual creations that have been produced in reference to the epidemic, using the images compiled in The Covid Art Museum's Instagram account to analyze the different elements used to represent subjective experiences with regard to a global event. This process has been carried out with techniques based on machine learning to detect objects in the images so that the algorithm can be capable of learning and detecting the objects contained in each study image. This research reveals that the elements that are repeated in images to create narratives and the relations of association that are established in the sample, concluding that, despite the subjectivity that all creation entails, there are certain parameters of shared and reduced decisions when it comes to selecting objects to be included in visual representations



### An Experimental Study of the Impact of Pre-training on the Pruning of a Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2112.08227v1
- **DOI**: 10.1145/3378184.3378224
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.08227v1)
- **Published**: 2021-12-15 16:02:15+00:00
- **Updated**: 2021-12-15 16:02:15+00:00
- **Authors**: Nathan Hubens, Matei Mancas, Bernard Gosselin, Marius Preda, Titus Zaharia
- **Comment**: 7 pages, published at APPIS 2020
- **Journal**: None
- **Summary**: In recent years, deep neural networks have known a wide success in various application domains. However, they require important computational and memory resources, which severely hinders their deployment, notably on mobile devices or for real-time applications. Neural networks usually involve a large number of parameters, which correspond to the weights of the network. Such parameters, obtained with the help of a training process, are determinant for the performance of the network. However, they are also highly redundant. The pruning methods notably attempt to reduce the size of the parameter set, by identifying and removing the irrelevant weights. In this paper, we examine the impact of the training strategy on the pruning efficiency. Two training modalities are considered and compared: (1) fine-tuned and (2) from scratch. The experimental results obtained on four datasets (CIFAR10, CIFAR100, SVHN and Caltech101) and for two different CNNs (VGG16 and MobileNet) demonstrate that a network that has been pre-trained on a large corpus (e.g. ImageNet) and then fine-tuned on a particular dataset can be pruned much more efficiently (up to 80% of parameter reduction) than the same network trained from scratch.



### RA V-Net: Deep learning network for automated liver segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.08232v2
- **DOI**: 10.1088/1361-6560/ac7193
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.08232v2)
- **Published**: 2021-12-15 16:10:14+00:00
- **Updated**: 2021-12-16 03:30:04+00:00
- **Authors**: Zhiqi Lee, Sumin Qi, Chongchong Fan, Ziwei Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of the liver is a prerequisite for the diagnosis of disease. Automated segmentation is an important application of computer-aided detection and diagnosis of liver disease. In recent years, automated processing of medical images has gained breakthroughs. However, the low contrast of abdominal scan CT images and the complexity of liver morphology make accurate automatic segmentation challenging. In this paper, we propose RA V-Net, which is an improved medical image automatic segmentation model based on U-Net. It has the following three main innovations. CofRes Module (Composite Original Feature Residual Module) is proposed. With more complex convolution layers and skip connections to make it obtain a higher level of image feature extraction capability and prevent gradient disappearance or explosion. AR Module (Attention Recovery Module) is proposed to reduce the computational effort of the model. In addition, the spatial features between the data pixels of the encoding and decoding modules are sensed by adjusting the channels and LSTM convolution. Finally, the image features are effectively retained. CA Module (Channel Attention Module) is introduced, which used to extract relevant channels with dependencies and strengthen them by matrix dot product, while weakening irrelevant channels without dependencies. The purpose of channel attention is achieved. The attention mechanism provided by LSTM convolution and CA Module are strong guarantees for the performance of the neural network. The accuracy of U-Net network: 0.9862, precision: 0.9118, DSC: 0.8547, JSC: 0.82. The evaluation metrics of RA V-Net, accuracy: 0.9968, precision: 0.9597, DSC: 0.9654, JSC: 0.9414. The most representative metric for the segmentation effect is DSC, which improves 0.1107 over U-Net, and JSC improves 0.1214.



### Putting People in their Place: Monocular Regression of 3D People in Depth
- **Arxiv ID**: http://arxiv.org/abs/2112.08274v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.08274v3)
- **Published**: 2021-12-15 17:08:17+00:00
- **Updated**: 2022-04-20 02:05:54+00:00
- **Authors**: Yu Sun, Wu Liu, Qian Bao, Yili Fu, Tao Mei, Michael J. Black
- **Comment**: CVPR 2022; Code https://github.com/Arthur151/ROMP ; Dataset
  https://github.com/Arthur151/Relative_Human
- **Journal**: None
- **Summary**: Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird's-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a "Relative Human" (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code and dataset are released for research purposes.



### SeqFormer: Sequential Transformer for Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.08275v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.08275v2)
- **Published**: 2021-12-15 17:09:18+00:00
- **Updated**: 2022-07-21 17:28:33+00:00
- **Authors**: Junfeng Wu, Yi Jiang, Song Bai, Wenqing Zhang, Xiang Bai
- **Comment**: ECCV 2022, Oral
- **Journal**: None
- **Summary**: In this work, we present SeqFormer for video instance segmentation. SeqFormer follows the principle of vision transformer that models instance relationships among video frames. Nevertheless, we observe that a stand-alone instance query suffices for capturing a time sequence of instances in a video, but attention mechanisms shall be done with each frame independently. To achieve this, SeqFormer locates an instance in each frame and aggregates temporal information to learn a powerful representation of a video-level instance, which is used to predict the mask sequences on each frame dynamically. Instance tracking is achieved naturally without tracking branches or post-processing. On YouTube-VIS, SeqFormer achieves 47.4 AP with a ResNet-50 backbone and 49.0 AP with a ResNet-101 backbone without bells and whistles. Such achievement significantly exceeds the previous state-of-the-art performance by 4.6 and 4.4, respectively. In addition, integrated with the recently-proposed Swin transformer, SeqFormer achieves a much higher AP of 59.3. We hope SeqFormer could be a strong baseline that fosters future research in video instance segmentation, and in the meantime, advances this field with a more robust, accurate, neat model. The code is available at https://github.com/wjf5203/SeqFormer.



### Detecting Object States vs Detecting Objects: A New Dataset and a Quantitative Experimental Study
- **Arxiv ID**: http://arxiv.org/abs/2112.08281v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.08281v2)
- **Published**: 2021-12-15 17:19:14+00:00
- **Updated**: 2022-08-18 20:43:12+00:00
- **Authors**: Filippos Gouidis, Theodore Patkos, Antonis Argyros, Dimitris Plexousakis
- **Comment**: Submitted to the Proceedings of the 17th International Joint
  Conference on Computer Vision, Imaging and Computer Graphics Theory and
  Applications (VISAPP)
- **Journal**: None
- **Summary**: The detection of object states in images (State Detection - SD) is a problem of both theoretical and practical importance and it is tightly interwoven with other important computer vision problems, such as action recognition and affordance detection. It is also highly relevant to any entity that needs to reason and act in dynamic domains, such as robotic systems and intelligent agents. Despite its importance, up to now, the research on this problem has been limited. In this paper, we attempt a systematic study of the SD problem. First, we introduce the Object State Detection Dataset (OSDD), a new publicly available dataset consisting of more than 19,000 annotations for 18 object categories and 9 state classes. Second, using a standard deep learning framework used for Object Detection (OD), we conduct a number of appropriately designed experiments, towards an in-depth study of the behavior of the SD problem. This study enables the setup of a baseline on the performance of SD, as well as its relative performance in comparison to OD, in a variety of scenarios. Overall, the experimental outcomes confirm that SD is harder than OD and that tailored SD methods need to be developed for addressing effectively this significant problem.



### Lifelong Generative Modelling Using Dynamic Expansion Graph Model
- **Arxiv ID**: http://arxiv.org/abs/2112.08370v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.08370v1)
- **Published**: 2021-12-15 17:35:27+00:00
- **Updated**: 2021-12-15 17:35:27+00:00
- **Authors**: Fei Ye, Adrian G. Bors
- **Comment**: Accepted in Proceedings of the 36th AAAI Conference on Artificial
  Intelligence (AAAI 2022)
- **Journal**: None
- **Summary**: Variational Autoencoders (VAEs) suffer from degenerated performance, when learning several successive tasks. This is caused by catastrophic forgetting. In order to address the knowledge loss, VAEs are using either Generative Replay (GR) mechanisms or Expanding Network Architectures (ENA). In this paper we study the forgetting behaviour of VAEs using a joint GR and ENA methodology, by deriving an upper bound on the negative marginal log-likelihood. This theoretical analysis provides new insights into how VAEs forget the previously learnt knowledge during lifelong learning. The analysis indicates the best performance achieved when considering model mixtures, under the ENA framework, where there are no restrictions on the number of components. However, an ENA-based approach may require an excessive number of parameters. This motivates us to propose a novel Dynamic Expansion Graph Model (DEGM). DEGM expands its architecture, according to the novelty associated with each new databases, when compared to the information already learnt by the network from previous tasks. DEGM training optimizes knowledge structuring, characterizing the joint probabilistic representations corresponding to the past and more recently learned tasks. We demonstrate that DEGM guarantees optimal performance for each task while also minimizing the required number of parameters. Supplementary materials (SM) and source code are available in https://github.com/dtuzi123/Expansion-Graph-Model.



### ForgeryNet -- Face Forgery Analysis Challenge 2021: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2112.08325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.08325v1)
- **Published**: 2021-12-15 18:17:55+00:00
- **Updated**: 2021-12-15 18:17:55+00:00
- **Authors**: Yinan He, Lu Sheng, Jing Shao, Ziwei Liu, Zhaofan Zou, Zhizhi Guo, Shan Jiang, Curitis Sun, Guosheng Zhang, Keyao Wang, Haixiao Yue, Zhibin Hong, Wanguo Wang, Zhenyu Li, Qi Wang, Zhenli Wang, Ronghao Xu, Mingwen Zhang, Zhiheng Wang, Zhenhang Huang, Tianming Zhang, Ningning Zhao
- **Comment**: Technical report. Challenge website:
  https://competitions.codalab.org/competitions/33386
- **Journal**: None
- **Summary**: The rapid progress of photorealistic synthesis techniques has reached a critical point where the boundary between real and manipulated images starts to blur. Recently, a mega-scale deep face forgery dataset, ForgeryNet which comprised of 2.9 million images and 221,247 videos has been released. It is by far the largest publicly available in terms of data-scale, manipulations (7 image-level approaches, 8 video-level approaches), perturbations (36 independent and more mixed perturbations), and annotations (6.3 million classification labels, 2.9 million manipulated area annotations, and 221,247 temporal forgery segment labels). This paper reports methods and results in the ForgeryNet - Face Forgery Analysis Challenge 2021, which employs the ForgeryNet benchmark. The model evaluation is conducted offline on the private test set. A total of 186 participants registered for the competition, and 11 teams made valid submissions. We will analyze the top-ranked solutions and present some discussion on future work directions.



### CPPE-5: Medical Personal Protective Equipment Dataset
- **Arxiv ID**: http://arxiv.org/abs/2112.09569v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.09569v2)
- **Published**: 2021-12-15 18:45:55+00:00
- **Updated**: 2023-02-18 08:51:42+00:00
- **Authors**: Rishit Dagli, Ali Mustufa Shaikh
- **Comment**: 18 pages, 6 tables, 6 figures. Code and models are available at
  https://git.io/cppe5-dataset
- **Journal**: None
- **Summary**: We present a new challenging dataset, CPPE - 5 (Medical Personal Protective Equipment), with the goal to allow the study of subordinate categorization of medical personal protective equipments, which is not possible with other popular data sets that focus on broad-level categories (such as PASCAL VOC, ImageNet, Microsoft COCO, OpenImages, etc). To make it easy for models trained on this dataset to be used in practical scenarios in complex scenes, our dataset mainly contains images that show complex scenes with several objects in each scene in their natural context. The image collection for this dataset focuses on: obtaining as many non-iconic images as possible and making sure all the images are real-life images, unlike other existing datasets in this area. Our dataset includes 5 object categories (coveralls, face shields, gloves, masks, and goggles), and each image is annotated with a set of bounding boxes and positive labels. We present a detailed analysis of the dataset in comparison to other popular broad category datasets as well as datasets focusing on personal protective equipments, we also find that at present there exist no such publicly available datasets. Finally, we also analyze performance and compare model complexities on baseline and state-of-the-art models for bounding box results. Our code, data, and trained models are available at https://git.io/cppe5-dataset.



### Reliable Multi-Object Tracking in the Presence of Unreliable Detections
- **Arxiv ID**: http://arxiv.org/abs/2112.08345v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.08345v2)
- **Published**: 2021-12-15 18:53:27+00:00
- **Updated**: 2022-11-07 23:46:24+00:00
- **Authors**: Travis Mandel, Mark Jimenez, Emily Risley, Taishi Nammoto, Rebekka Williams, Max Panoff, Meynard Ballesteros, Bobbie Suarez
- **Comment**: The full journal version of this article (published in Pattern
  Recognition, Vol. 135) can be found at
  https://www.sciencedirect.com/science/article/pii/S0031320322005878. The
  article is open access. The source code and dataset can be found at
  https://github.com/tmandel/fish-detrac
- **Journal**: None
- **Summary**: Recent multi-object tracking (MOT) systems have leveraged highly accurate object detectors; however, training such detectors requires large amounts of labeled data. Although such data is widely available for humans and vehicles, it is significantly more scarce for other animal species. We present Robust Confidence Tracking (RCT), an algorithm designed to maintain robust performance even when detection quality is poor. In contrast to prior methods which discard detection confidence information, RCT takes a fundamentally different approach, relying on the exact detection confidence values to initialize tracks, extend tracks, and filter tracks. In particular, RCT is able to minimize identity switches by efficiently using low-confidence detections (along with a single object tracker) to keep continuous track of objects. To evaluate trackers in the presence of unreliable detections, we present a challenging real-world underwater fish tracking dataset, FISHTRAC. In an evaluation on FISHTRAC as well as the UA-DETRAC dataset, we find that RCT outperforms other algorithms when provided with imperfect detections, including state-of-the-art deep single and multi-object trackers as well as more classic approaches. Specifically, RCT has the best average HOTA across methods that successfully return results for all sequences, and has significantly less identity switches than other methods.



### 3D Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2112.08359v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.08359v2)
- **Published**: 2021-12-15 18:59:59+00:00
- **Updated**: 2022-11-29 01:51:28+00:00
- **Authors**: Shuquan Ye, Dongdong Chen, Songfang Han, Jing Liao
- **Comment**: To Appear at IEEE Transactions on Visualization and Computer Graphics
  (TVCG) 2022
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) has witnessed tremendous progress in recent years. However, most efforts only focus on the 2D image question answering tasks. In this paper, we present the first attempt at extending VQA to the 3D domain, which can facilitate artificial intelligence's perception of 3D real-world scenarios. Different from image based VQA, 3D Question Answering (3DQA) takes the color point cloud as input and requires both appearance and 3D geometry comprehension ability to answer the 3D-related questions. To this end, we propose a novel transformer-based 3DQA framework "3DQA-TR", which consists of two encoders for exploiting the appearance and geometry information, respectively. The multi-modal information of appearance, geometry, and the linguistic question can finally attend to each other via a 3D-Linguistic Bert to predict the target answers. To verify the effectiveness of our proposed 3DQA framework, we further develop the first 3DQA dataset "ScanQA", which builds on the ScanNet dataset and contains $\sim$6K questions, $\sim$30K answers for $806$ scenes. Extensive experiments on this dataset demonstrate the obvious superiority of our proposed 3DQA framework over existing VQA frameworks, and the effectiveness of our major designs. Our code and dataset will be made publicly available to facilitate the research in this direction.



### Positional Encoding Augmented GAN for the Assessment of Wind Flow for Pedestrian Comfort in Urban Areas
- **Arxiv ID**: http://arxiv.org/abs/2112.08447v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, 68T07, 68T45, 68T01, I.2.0; I.6.0
- **Links**: [PDF](http://arxiv.org/pdf/2112.08447v2)
- **Published**: 2021-12-15 19:37:11+00:00
- **Updated**: 2022-01-04 09:48:15+00:00
- **Authors**: Henrik Hoeiness, Kristoffer Gjerde, Luca Oggiano, Knut Erik Teigen Giljarhus, Massimiliano Ruocco
- **Comment**: None
- **Journal**: None
- **Summary**: Approximating wind flows using computational fluid dynamics (CFD) methods can be time-consuming. Creating a tool for interactively designing prototypes while observing the wind flow change requires simpler models to simulate faster. Instead of running numerical approximations resulting in detailed calculations, data-driven methods and deep learning might be able to give similar results in a fraction of the time. This work rephrases the problem from computing 3D flow fields using CFD to a 2D image-to-image translation-based problem on the building footprints to predict the flow field at pedestrian height level. We investigate the use of generative adversarial networks (GAN), such as Pix2Pix [1] and CycleGAN [2] representing state-of-the-art for image-to-image translation task in various domains as well as U-Net autoencoder [3]. The models can learn the underlying distribution of a dataset in a data-driven manner, which we argue can help the model learn the underlying Reynolds-averaged Navier-Stokes (RANS) equations from CFD. We experiment on novel simulated datasets on various three-dimensional bluff-shaped buildings with and without height information. Moreover, we present an extensive qualitative and quantitative evaluation of the generated images for a selection of models and compare their performance with the simulations delivered by CFD. We then show that adding positional data to the input can produce more accurate results by proposing a general framework for injecting such information on the different architectures. Furthermore, we show that the models performances improve by applying attention mechanisms and spectral normalization to facilitate stable training.



### Dense Video Captioning Using Unsupervised Semantic Information
- **Arxiv ID**: http://arxiv.org/abs/2112.08455v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.08455v1)
- **Published**: 2021-12-15 20:03:42+00:00
- **Updated**: 2021-12-15 20:03:42+00:00
- **Authors**: Valter Estevam, Rayson Laroca, Helio Pedrini, David Menotti
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a method to learn unsupervised semantic visual information based on the premise that complex events (e.g., minutes) can be decomposed into simpler events (e.g., a few seconds), and that these simple events are shared across several complex events. We split a long video into short frame sequences to extract their latent representation with three-dimensional convolutional neural networks. A clustering method is used to group representations producing a visual codebook (i.e., a long video is represented by a sequence of integers given by the cluster labels). A dense representation is learned by encoding the co-occurrence probability matrix for the codebook entries. We demonstrate how this representation can leverage the performance of the dense video captioning task in a scenario with only visual features. As a result of this approach, we are able to replace the audio signal in the Bi-Modal Transformer (BMT) method and produce temporal proposals with comparable performance. Furthermore, we concatenate the visual signal with our descriptor in a vanilla transformer method to achieve state-of-the-art performance in captioning compared to the methods that explore only visual features, as well as a competitive performance with multi-modal methods. Our code is available at https://github.com/valterlej/dvcusi.



### Rethinking Nearest Neighbors for Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/2112.08459v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.08459v2)
- **Published**: 2021-12-15 20:15:01+00:00
- **Updated**: 2021-12-17 20:48:46+00:00
- **Authors**: Menglin Jia, Bor-Chun Chen, Zuxuan Wu, Claire Cardie, Serge Belongie, Ser-Nam Lim
- **Comment**: Modified paragraph spacing
- **Journal**: None
- **Summary**: Neural network classifiers have become the de-facto choice for current "pre-train then fine-tune" paradigms of visual classification. In this paper, we investigate k-Nearest-Neighbor (k-NN) classifiers, a classical model-free learning method from the pre-deep learning era, as an augmentation to modern neural network based approaches. As a lazy learning method, k-NN simply aggregates the distance between the test image and top-k neighbors in a training set. We adopt k-NN with pre-trained visual representations produced by either supervised or self-supervised methods in two steps: (1) Leverage k-NN predicted probabilities as indications for easy vs. hard examples during training. (2) Linearly interpolate the k-NN predicted distribution with that of the augmented classifier. Via extensive experiments on a wide range of classification tasks, our study reveals the generality and flexibility of k-NN integration with additional insights: (1) k-NN achieves competitive results, sometimes even outperforming a standard linear classifier. (2) Incorporating k-NN is especially beneficial for tasks where parametric classifiers perform poorly and / or in low-data regimes. We hope these discoveries will encourage people to rethink the role of pre-deep learning, classical methods in computer vision. Our code is available at: https://github.com/KMnP/nn-revisit.



### Insta-VAX: A Multimodal Benchmark for Anti-Vaccine and Misinformation Posts Detection on Social Media
- **Arxiv ID**: http://arxiv.org/abs/2112.08470v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.08470v1)
- **Published**: 2021-12-15 20:34:57+00:00
- **Updated**: 2021-12-15 20:34:57+00:00
- **Authors**: Mingyang Zhou, Mahasweta Chakraborti, Sijia Qian, Zhou Yu, Jingwen Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Sharing of anti-vaccine posts on social media, including misinformation posts, has been shown to create confusion and reduce the publics confidence in vaccines, leading to vaccine hesitancy and resistance. Recent years have witnessed the fast rise of such anti-vaccine posts in a variety of linguistic and visual forms in online networks, posing a great challenge for effective content moderation and tracking. Extending previous work on leveraging textual information to understand vaccine information, this paper presents Insta-VAX, a new multi-modal dataset consisting of a sample of 64,957 Instagram posts related to human vaccines. We applied a crowdsourced annotation procedure verified by two trained expert judges to this dataset. We then bench-marked several state-of-the-art NLP and computer vision classifiers to detect whether the posts show anti-vaccine attitude and whether they contain misinformation. Extensive experiments and analyses demonstrate the multimodal models can classify the posts more accurately than the uni-modal models, but still need improvement especially on visual context understanding and external knowledge cooperation. The dataset and classifiers contribute to monitoring and tracking of vaccine discussions for social scientific and public health efforts in combating the problem of vaccine misinformation.



### StyleMC: Multi-Channel Based Fast Text-Guided Image Generation and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2112.08493v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.08493v1)
- **Published**: 2021-12-15 21:37:10+00:00
- **Updated**: 2021-12-15 21:37:10+00:00
- **Authors**: Umut Kocasari, Alara Dirik, Mert Tiftikci, Pinar Yanardag
- **Comment**: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV 2022)
- **Journal**: None
- **Summary**: Discovering meaningful directions in the latent space of GANs to manipulate semantic attributes typically requires large amounts of labeled data. Recent work aims to overcome this limitation by leveraging the power of Contrastive Language-Image Pre-training (CLIP), a joint text-image model. While promising, these methods require several hours of preprocessing or training to achieve the desired manipulations. In this paper, we present StyleMC, a fast and efficient method for text-driven image generation and manipulation. StyleMC uses a CLIP-based loss and an identity loss to manipulate images via a single text prompt without significantly affecting other attributes. Unlike prior work, StyleMC requires only a few seconds of training per text prompt to find stable global directions, does not require prompt engineering and can be used with any pre-trained StyleGAN2 model. We demonstrate the effectiveness of our method and compare it to state-of-the-art methods. Our code can be found at http://catlab-team.github.io/stylemc.



### Predicting Levels of Household Electricity Consumption in Low-Access Settings
- **Arxiv ID**: http://arxiv.org/abs/2112.08497v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.08497v1)
- **Published**: 2021-12-15 21:42:36+00:00
- **Updated**: 2021-12-15 21:42:36+00:00
- **Authors**: Simone Fobi, Joel Mugyenyi, Nathaniel J. Williams, Vijay Modi, Jay Taneja
- **Comment**: Accepted to be published in Proceedings of IEEE Winter Conference on
  Applications of Computer Vision (WACV) 2022
- **Journal**: None
- **Summary**: In low-income settings, the most critical piece of information for electric utilities is the anticipated consumption of a customer. Electricity consumption assessment is difficult to do in settings where a significant fraction of households do not yet have an electricity connection. In such settings the absolute levels of anticipated consumption can range from 5-100 kWh/month, leading to high variability amongst these customers. Precious resources are at stake if a significant fraction of low consumers are connected over those with higher consumption.   This is the first study of it's kind in low-income settings that attempts to predict a building's consumption and not that of an aggregate administrative area. We train a Convolutional Neural Network (CNN) over pre-electrification daytime satellite imagery with a sample of utility bills from 20,000 geo-referenced electricity customers in Kenya (0.01% of Kenya's residential customers). This is made possible with a two-stage approach that uses a novel building segmentation approach to leverage much larger volumes of no-cost satellite imagery to make the most of scarce and expensive customer data. Our method shows that competitive accuracies can be achieved at the building level, addressing the challenge of consumption variability. This work shows that the building's characteristics and it's surrounding context are both important in predicting consumption levels. We also evaluate the addition of lower resolution geospatial datasets into the training process, including nighttime lights and census-derived data. The results are already helping inform site selection and distribution-level planning, through granular predictions at the level of individual structures in Kenya and there is no reason this cannot be extended to other countries.



