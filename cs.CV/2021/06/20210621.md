# Arxiv Papers in cs.CV on 2021-06-21
### Large-scale image segmentation based on distributed clustering algorithms
- **Arxiv ID**: http://arxiv.org/abs/2106.10795v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10795v1)
- **Published**: 2021-06-21 01:11:49+00:00
- **Updated**: 2021-06-21 01:11:49+00:00
- **Authors**: Ran Lu, Aleksandar Zlateski, H. Sebastian Seung
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Many approaches to 3D image segmentation are based on hierarchical clustering of supervoxels into image regions. Here we describe a distributed algorithm capable of handling a tremendous number of supervoxels. The algorithm works recursively, the regions are divided into chunks that are processed independently in parallel by multiple workers. At each round of the recursive procedure, the chunk size in all dimensions are doubled until a single chunk encompasses the entire image. The final result is provably independent of the chunking scheme, and the same as if the entire image were processed without division into chunks. This is nontrivial because a pair of adjacent regions is scored by some statistical property (e.g. mean or median) of the affinities at the interface, and the interface may extend over arbitrarily many chunks. The trick is to delay merge decisions for regions that touch chunk boundaries, and only complete them in a later round after the regions are fully contained within a chunk. We demonstrate the algorithm by clustering an affinity graph with over 1.5 trillion edges between 135 billion supervoxels derived from a 3D electron microscopic brain image.



### Improved Padding in CNNs for Quantitative Susceptibility Mapping
- **Arxiv ID**: http://arxiv.org/abs/2106.15331v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15331v1)
- **Published**: 2021-06-21 01:35:00+00:00
- **Updated**: 2021-06-21 01:35:00+00:00
- **Authors**: Juan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep learning methods have been proposed for quantitative susceptibility mapping (QSM) data processing: background field removal, field-to-source inversion, and single-step QSM reconstruction. However, the conventional padding mechanism used in convolutional neural networks (CNNs) can introduce spatial artifacts, especially in QSM background field removal and single-step QSM which requires inference from total fields with extreme large values at the edge boundaries of volume of interest. To address this issue, we propose an improved padding technique which utilizes the neighboring valid voxels to estimate the invalid voxels of feature maps at volume boundaries in the neural networks. Studies using simulated and in-vivo data show that the proposed padding greatly improves estimation accuracy and reduces artifacts in the results in the tasks of background field removal, field-to-source inversion, and single-step QSM reconstruction.



### DiGS : Divergence guided shape implicit neural representation for unoriented point clouds
- **Arxiv ID**: http://arxiv.org/abs/2106.10811v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.10811v3)
- **Published**: 2021-06-21 02:10:03+00:00
- **Updated**: 2023-05-17 07:45:15+00:00
- **Authors**: Yizhak Ben-Shabat, Chamin Hewa Koneputugodage, Stephen Gould
- **Comment**: None
- **Journal**: None
- **Summary**: Shape implicit neural representations (INRs) have recently shown to be effective in shape analysis and reconstruction tasks. Existing INRs require point coordinates to learn the implicit level sets of the shape. When a normal vector is available for each point, a higher fidelity representation can be learned, however normal vectors are often not provided as raw data. Furthermore, the method's initialization has been shown to play a crucial role for surface reconstruction. In this paper, we propose a divergence guided shape representation learning approach that does not require normal vectors as input. We show that incorporating a soft constraint on the divergence of the distance function favours smooth solutions that reliably orients gradients to match the unknown normal at each point, in some cases even better than approaches that use ground truth normal vectors directly. Additionally, we introduce a novel geometric initialization method for sinusoidal INRs that further improves convergence to the desired solution. We evaluate the effectiveness of our approach on the task of surface reconstruction and shape space learning and show SOTA performance compared to other unoriented methods. Code and model parameters available at our project page https://chumbyte.github.io/DiGS-Site/.



### ToAlign: Task-oriented Alignment for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2106.10812v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10812v3)
- **Published**: 2021-06-21 02:17:48+00:00
- **Updated**: 2021-10-26 12:29:49+00:00
- **Authors**: Guoqiang Wei, Cuiling Lan, Wenjun Zeng, Zhizheng Zhang, Zhibo Chen
- **Comment**: Accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: Unsupervised domain adaptive classifcation intends to improve the classifcation performance on unlabeled target domain. To alleviate the adverse effect of domain shift, many approaches align the source and target domains in the feature space. However, a feature is usually taken as a whole for alignment without explicitly making domain alignment proactively serve the classifcation task, leading to sub-optimal solution. In this paper, we propose an effective Task-oriented Alignment (ToAlign) for unsupervised domain adaptation (UDA). We study what features should be aligned across domains and propose to make the domain alignment proactively serve classifcation by performing feature decomposition and alignment under the guidance of the prior knowledge induced from the classifcation task itself. Particularly, we explicitly decompose a feature in the source domain into a task-related/discriminative feature that should be aligned, and a task-irrelevant feature that should be avoided/ignored, based on the classifcation meta-knowledge. Extensive experimental results on various benchmarks (e.g., Offce-Home, Visda-2017, and DomainNet) under different domain adaptation settings demonstrate the effectiveness of ToAlign which helps achieve the state-of-the-art performance. The code is publicly available at https://github.com/microsoft/UDA



### Structured Sparse R-CNN for Direct Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2106.10815v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10815v2)
- **Published**: 2021-06-21 02:24:20+00:00
- **Updated**: 2022-03-28 15:37:07+00:00
- **Authors**: Yao Teng, Limin Wang
- **Comment**: Camera-ready version of CVPR 2022
- **Journal**: None
- **Summary**: Scene graph generation (SGG) is to detect object pairs with their relations in an image. Existing SGG approaches often use multi-stage pipelines to decompose this task into object detection, relation graph construction, and dense or dense-to-sparse relation prediction. Instead, from a perspective on SGG as a direct set prediction, this paper presents a simple, sparse, and unified framework, termed as Structured Sparse R-CNN. The key to our method is a set of learnable triplet queries and a structured triplet detector which could be jointly optimized from the training set in an end-to-end manner. Specifically, the triplet queries encode the general prior for object pairs with their relations, and provide an initial guess of scene graphs for subsequent refinement. The triplet detector presents a cascaded architecture to progressively refine the detected scene graphs with the customized dynamic heads. In addition, to relieve the training difficulty of our method, we propose a relaxed and enhanced training strategy based on knowledge distillation from a Siamese Sparse R-CNN. We perform experiments on several datasets: Visual Genome and Open Images V4/V6, and the results demonstrate that our method achieves the state-of-the-art performance. In addition, we also perform in-depth ablation studies to provide insights on our structured modeling in triplet detector design and training strategies. The code and models are made available at https://github.com/MCG-NJU/Structured-Sparse-RCNN.



### 3D Object Detection for Autonomous Driving: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2106.10823v3
- **DOI**: 10.1016/j.patcog.2022.108796
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10823v3)
- **Published**: 2021-06-21 03:17:20+00:00
- **Updated**: 2022-05-25 03:14:19+00:00
- **Authors**: Rui Qian, Xin Lai, Xirong Li
- **Comment**: The manuscript is accepted by Pattern Recognition on 14 May 2022
- **Journal**: None
- **Summary**: Autonomous driving is regarded as one of the most promising remedies to shield human beings from severe crashes. To this end, 3D object detection serves as the core basis of perception stack especially for the sake of path planning, motion prediction, and collision avoidance etc. Taking a quick glance at the progress we have made, we attribute challenges to visual appearance recovery in the absence of depth information from images, representation learning from partially occluded unstructured point clouds, and semantic alignments over heterogeneous features from cross modalities. Despite existing efforts, 3D object detection for autonomous driving is still in its infancy. Recently, a large body of literature have been investigated to address this 3D vision task. Nevertheless, few investigations have looked into collecting and structuring this growing knowledge. We therefore aim to fill this gap in a comprehensive survey, encompassing all the main concerns including sensors, datasets, performance metrics and the recent state-of-the-art detection methods, together with their pros and cons. Furthermore, we provide quantitative comparisons with the state of the art. A case study on fifteen selected representative methods is presented, involved with runtime analysis, error analysis, and robustness analysis. Finally, we provide concluding remarks after an in-depth analysis of the surveyed works and identify promising directions for future work.



### Two-Stream Consensus Network: Submission to HACS Challenge 2021 Weakly-Supervised Learning Track
- **Arxiv ID**: http://arxiv.org/abs/2106.10829v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10829v3)
- **Published**: 2021-06-21 03:36:36+00:00
- **Updated**: 2022-04-17 18:31:23+00:00
- **Authors**: Yuanhao Zhai, Le Wang, David Doermann, Junsong Yuan
- **Comment**: Second place solution to the HACS Weakly-Supervised Temporal Action
  Localization Challenge 2021. arXiv admin note: text overlap with
  arXiv:2010.11594
- **Journal**: None
- **Summary**: This technical report presents our solution to the HACS Temporal Action Localization Challenge 2021, Weakly-Supervised Learning Track. The goal of weakly-supervised temporal action localization is to temporally locate and classify action of interest in untrimmed videos given only video-level labels. We adopt the two-stream consensus network (TSCN) as the main framework in this challenge. The TSCN consists of a two-stream base model training procedure and a pseudo ground truth learning procedure. The base model training encourages the model to predict reliable predictions based on single modality (i.e., RGB or optical flow), based on the fusion of which a pseudo ground truth is generated and in turn used as supervision to train the base models. On the HACS v1.1.1 dataset, without fine-tuning the feature-extraction I3D models, our method achieves 22.20% on the validation set and 21.68% on the testing set in terms of average mAP. Our solution ranked the 2nd in this challenge, and we hope our method can serve as a baseline for future academic research.



### Interpretable Face Manipulation Detection via Feature Whitening
- **Arxiv ID**: http://arxiv.org/abs/2106.10834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10834v1)
- **Published**: 2021-06-21 03:51:43+00:00
- **Updated**: 2021-06-21 03:51:43+00:00
- **Authors**: Yingying Hua, Daichi Zhang, Pengju Wang, Shiming Ge
- **Comment**: None
- **Journal**: None
- **Summary**: Why should we trust the detections of deep neural networks for manipulated faces? Understanding the reasons is important for users in improving the fairness, reliability, privacy and trust of the detection models. In this work, we propose an interpretable face manipulation detection approach to achieve the trustworthy and accurate inference. The approach could make the face manipulation detection process transparent by embedding the feature whitening module. This module aims to whiten the internal working mechanism of deep networks through feature decorrelation and feature constraint. The experimental results demonstrate that our proposed approach can strike a balance between the detection accuracy and the model interpretability.



### Active Learning for Deep Neural Networks on Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/2106.10836v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/2106.10836v2)
- **Published**: 2021-06-21 03:55:33+00:00
- **Updated**: 2023-03-22 09:12:09+00:00
- **Authors**: Yuya Senzaki, Christian Hamelain
- **Comment**: None
- **Journal**: None
- **Summary**: When dealing with deep neural network (DNN) applications on edge devices, continuously updating the model is important. Although updating a model with real incoming data is ideal, using all of them is not always feasible due to limits, such as labeling and communication costs. Thus, it is necessary to filter and select the data to use for training (i.e., active learning) on the device. In this paper, we formalize a practical active learning problem for DNNs on edge devices and propose a general task-agnostic framework to tackle this problem, which reduces it to a stream submodular maximization. This framework is light enough to be run with low computational resources, yet provides solutions whose quality is theoretically guaranteed thanks to the submodular property. Through this framework, we can configure data selection criteria flexibly, including using methods proposed in previous active learning studies. We evaluate our approach on both classification and object detection tasks in a practical setting to simulate a real-life scenario. The results of our study show that the proposed framework outperforms all other methods in both tasks, while running at a practical speed on real devices.



### Trainable Class Prototypes for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.10846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.10846v1)
- **Published**: 2021-06-21 04:19:56+00:00
- **Updated**: 2021-06-21 04:19:56+00:00
- **Authors**: Jianyi Li, Guizhong Liu
- **Comment**: 8 pages, 2 figures,and 3 Tables. arXiv admin note: substantial text
  overlap with arXiv:2008.09942
- **Journal**: None
- **Summary**: Metric learning is a widely used method for few shot learning in which the quality of prototypes plays a key role in the algorithm. In this paper we propose the trainable prototypes for distance measure instead of the artificial ones within the meta-training and task-training framework. Also to avoid the disadvantages that the episodic meta-training brought, we adopt non-episodic meta-training based on self-supervised learning. Overall we solve the few-shot tasks in two phases: meta-training a transferable feature extractor via self-supervised learning and training the prototypes for metric classification. In addition, the simple attention mechanism is used in both meta-training and task-training. Our method achieves state-of-the-art performance in a variety of established few-shot tasks on the standard few-shot visual classification dataset, with about 20% increase compared to the available unsupervised few-shot learning methods.



### Robust Pooling through the Data Mode
- **Arxiv ID**: http://arxiv.org/abs/2106.10850v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10850v1)
- **Published**: 2021-06-21 04:35:24+00:00
- **Updated**: 2021-06-21 04:35:24+00:00
- **Authors**: Ayman Mukhaimar, Ruwan Tennakoon, Chow Yin Lai, Reza Hoseinnezhad, AlirezaBab-Hadiashar
- **Comment**: under consideration at Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: The task of learning from point cloud data is always challenging due to the often occurrence of noise and outliers in the data. Such data inaccuracies can significantly influence the performance of state-of-the-art deep learning networks and their ability to classify or segment objects. While there are some robust deep learning approaches, they are computationally too expensive for real-time applications. This paper proposes a deep learning solution that includes a novel robust pooling layer which greatly enhances network robustness and performs significantly faster than state-of-the-art approaches. The proposed pooling layer looks for data a mode/cluster using two methods, RANSAC, and histogram, as clusters are indicative of models. We tested the pooling layer into frameworks such as Point-based and graph-based neural networks, and the tests showed enhanced robustness as compared to robust state-of-the-art methods.



### CUDA-GHR: Controllable Unsupervised Domain Adaptation for Gaze and Head Redirection
- **Arxiv ID**: http://arxiv.org/abs/2106.10852v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10852v4)
- **Published**: 2021-06-21 04:39:42+00:00
- **Updated**: 2022-09-19 18:48:17+00:00
- **Authors**: Swati Jindal, Xin Eric Wang
- **Comment**: Accepted at WACV2023, Camera-ready version
- **Journal**: None
- **Summary**: The robustness of gaze and head pose estimation models is highly dependent on the amount of labeled data. Recently, generative modeling has shown excellent results in generating photo-realistic images, which can alleviate the need for annotations. However, adopting such generative models to new domains while maintaining their ability to provide fine-grained control over different image attributes, \eg, gaze and head pose directions, has been a challenging problem. This paper proposes CUDA-GHR, an unsupervised domain adaptation framework that enables fine-grained control over gaze and head pose directions while preserving the appearance-related factors of the person. Our framework simultaneously learns to adapt to new domains and disentangle visual attributes such as appearance, gaze direction, and head orientation by utilizing a label-rich source domain and an unlabeled target domain. Extensive experiments on the benchmarking datasets show that the proposed method can outperform state-of-the-art techniques on both quantitative and qualitative evaluations. Furthermore, we demonstrate the effectiveness of generated image-label pairs in the target domain for pretraining networks for the downstream task of gaze and head pose estimation. The source code and pre-trained models are available at https://github.com/jswati31/cuda-ghr.



### Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single Panorama
- **Arxiv ID**: http://arxiv.org/abs/2106.10859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10859v1)
- **Published**: 2021-06-21 05:08:34+00:00
- **Updated**: 2021-06-21 05:08:34+00:00
- **Authors**: Ching-Yu Hsu, Cheng Sun, Hwann-Tzong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first method to the application of parallax-enabled novel panoramic view synthesis. Recent works for novel view synthesis focus on perspective images with limited field-of-view and require sufficient pictures captured in a specific condition. Conversely, OmniNeRF can generate panorama images for unknown viewpoints given a single equirectangular image as training data. To this end, we propose to augment the single RGB-D panorama by projecting back and forth between a 3D world and different 2D panoramic coordinates at different virtual camera positions. By doing so, we are able to optimize an Omnidirectional Neural Radiance Field with visible pixels collecting from omnidirectional viewing angles at a fixed center for the estimation of new viewing angles from varying camera positions. As a result, the proposed OmniNeRF achieves convincing renderings of novel panoramic views that exhibit the parallax effect. We showcase the effectiveness of each of our proposals on both synthetic and real-world datasets.



### An End-to-End Khmer Optical Character Recognition using Sequence-to-Sequence with Attention
- **Arxiv ID**: http://arxiv.org/abs/2106.10875v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10875v1)
- **Published**: 2021-06-21 06:18:23+00:00
- **Updated**: 2021-06-21 06:18:23+00:00
- **Authors**: Rina Buoy, Sokchea Kor, Nguonly Taing
- **Comment**: 16 pages, 18 figures
- **Journal**: None
- **Summary**: This paper presents an end-to-end deep convolutional recurrent neural network solution for Khmer optical character recognition (OCR) task. The proposed solution uses a sequence-to-sequence (Seq2Seq) architecture with attention mechanism. The encoder extracts visual features from an input text-line image via layers of residual convolutional blocks and a layer of gated recurrent units (GRU). The features are encoded in a single context vector and a sequence of hidden states which are fed to the decoder for decoding one character at a time until a special end-of-sentence (EOS) token is reached. The attention mechanism allows the decoder network to adaptively select parts of the input image while predicting a target character. The Seq2Seq Khmer OCR network was trained on a large collection of computer-generated text-line images for seven common Khmer fonts. The proposed model's performance outperformed the state-of-art Tesseract OCR engine for Khmer language on the 3000-images test set by achieving a character error rate (CER) of 1% vs 3%.



### Total Generate: Cycle in Cycle Generative Adversarial Networks for Generating Human Faces, Hands, Bodies, and Natural Scenes
- **Arxiv ID**: http://arxiv.org/abs/2106.10876v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.10876v1)
- **Published**: 2021-06-21 06:20:16+00:00
- **Updated**: 2021-06-21 06:20:16+00:00
- **Authors**: Hao Tang, Nicu Sebe
- **Comment**: Accepted to TMM, an extended version of a paper published in ACM MM
  2019. arXiv admin note: substantial text overlap with arXiv:1908.00999
- **Journal**: None
- **Summary**: We propose a novel and unified Cycle in Cycle Generative Adversarial Network (C2GAN) for generating human faces, hands, bodies, and natural scenes. Our proposed C2GAN is a cross-modal model exploring the joint exploitation of the input image data and guidance data in an interactive manner. C2GAN contains two different generators, i.e., an image-generation generator and a guidance-generation generator. Both generators are mutually connected and trained in an end-to-end fashion and explicitly form three cycled subnets, i.e., one image generation cycle and two guidance generation cycles. Each cycle aims at reconstructing the input domain and simultaneously produces a useful output involved in the generation of another cycle. In this way, the cycles constrain each other implicitly providing complementary information from both image and guidance modalities and bringing an extra supervision gradient across the cycles, facilitating a more robust optimization of the whole model. Extensive results on four guided image-to-image translation subtasks demonstrate that the proposed C2GAN is effective in generating more realistic images compared with state-of-the-art models. The code is available at https://github.com/Ha0Tang/C2GAN.



### A Comprehensive Survey of Image-Based Food Recognition and Volume Estimation Methods for Dietary Assessment
- **Arxiv ID**: http://arxiv.org/abs/2106.11776v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11776v3)
- **Published**: 2021-06-21 06:30:06+00:00
- **Updated**: 2021-09-03 18:30:49+00:00
- **Authors**: Ghalib Tahir, Chu Kiong Loo
- **Comment**: None
- **Journal**: None
- **Summary**: Dietary studies showed that dietary-related problem such as obesity is associated with other chronic diseases like hypertension, irregular blood sugar levels, and increased risk of heart attacks. The primary cause of these problems is poor lifestyle choices and unhealthy dietary habits, which are manageable using interactive mHealth apps. However, traditional dietary monitoring systems using manual food logging suffer from imprecision, underreporting, time consumption, and low adherence. Recent dietary monitoring systems tackle these challenges by automatic assessment of dietary intake through machine learning methods. This survey discusses the most performing methodologies that have been developed so far for automatic food recognition and volume estimation. First, we will present the rationale of visual-based methods for food recognition. The core of the paper is the presentation, discussion and evaluation of these methods on popular food image databases. Following that, we discussed the mobile applications that are implementing these methods. The survey ends with a discussion of research gaps and open issues in this area.



### Affect-driven Ordinal Engagement Measurement from Video
- **Arxiv ID**: http://arxiv.org/abs/2106.10882v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2106.10882v2)
- **Published**: 2021-06-21 06:49:17+00:00
- **Updated**: 2022-11-07 03:42:54+00:00
- **Authors**: Ali Abedi, Shehroz Khan
- **Comment**: 13 pages, 8 figures, 7 tables
- **Journal**: None
- **Summary**: In education and intervention programs, user engagement has been identified as a major factor in successful program completion. Automatic measurement of user engagement provides helpful information for instructors to meet program objectives and individualize program delivery. In this paper, we present a novel approach for video-based engagement measurement in virtual learning programs. We propose to use affect states, continuous values of valence and arousal extracted from consecutive video frames, along with a new latent affective feature vector and behavioral features for engagement measurement. Deep-learning sequential models are trained and validated on the extracted frame-level features. In addition, due to the fact that engagement is an ordinal variable, we develop the ordinal versions of the above models in order to address the problem of engagement measurement as an ordinal classification problem. We evaluated the performance of the proposed method on the only two publicly available video engagement measurement datasets, DAiSEE and EmotiW-EW, containing videos of students in online learning programs. Our experiments show a state-of-the-art engagement level classification accuracy of 67.4% on the DAiSEE dataset, and a regression mean squared error of 0.0508 on the EmotiW-EW dataset. Our ablation study shows the effectiveness of incorporating affect states and ordinality of engagement in engagement measurement.



### Knowledge Distillation via Instance-level Sequence Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.10885v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10885v1)
- **Published**: 2021-06-21 06:58:26+00:00
- **Updated**: 2021-06-21 06:58:26+00:00
- **Authors**: Haoran Zhao, Xin Sun, Junyu Dong, Zihe Dong, Qiong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, distillation approaches are suggested to extract general knowledge from a teacher network to guide a student network. Most of the existing methods transfer knowledge from the teacher network to the student via feeding the sequence of random mini-batches sampled uniformly from the data. Instead, we argue that the compact student network should be guided gradually using samples ordered in a meaningful sequence. Thus, it can bridge the gap of feature representation between the teacher and student network step by step. In this work, we provide a curriculum learning knowledge distillation framework via instance-level sequence learning. It employs the student network of the early epoch as a snapshot to create a curriculum for the student network's next training phase. We carry out extensive experiments on CIFAR-10, CIFAR-100, SVHN and CINIC-10 datasets. Compared with several state-of-the-art methods, our framework achieves the best performance with fewer iterations.



### Trust It or Not: Confidence-Guided Automatic Radiology Report Generation
- **Arxiv ID**: http://arxiv.org/abs/2106.10887v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10887v3)
- **Published**: 2021-06-21 07:02:12+00:00
- **Updated**: 2022-02-03 14:58:10+00:00
- **Authors**: Yixin Wang, Zihao Lin, Zhe Xu, Haoyu Dong, Jiang Tian, Jie Luo, Zhongchao Shi, Yang Zhang, Jianping Fan, Zhiqiang He
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Medical imaging plays a pivotal role in diagnosis and treatment in clinical practice. Inspired by the significant progress in automatic image captioning, various deep learning (DL)-based methods have been proposed to generate radiology reports for medical images. Despite promising results, previous works overlook the uncertainties of their models and are thus unable to provide clinicians with the reliability/confidence of the generated radiology reports to assist their decision-making. In this paper, we propose a novel method to explicitly quantify both the visual uncertainty and the textual uncertainty for DL-based radiology report generation. Such multi-modal uncertainties can sufficiently capture the model confidence degree at both the report level and the sentence level, and thus they are further leveraged to weight the losses for more comprehensive model optimization. Experimental results have demonstrated that the proposed method for model uncertainty characterization and estimation can produce more reliable confidence scores for radiology report generation, and the modified loss function, which takes into account the uncertainties, leads to better model performance on two public radiology report datasets. In addition, the quality of the automatically generated reports was manually evaluated by human raters and the results also indicate that the proposed uncertainties can reflect the variance of clinical diagnosis.



### Brain tumor grade classification Using LSTM Neural Networks with Domain Pre-Transforms
- **Arxiv ID**: http://arxiv.org/abs/2106.10889v1
- **DOI**: 10.1109/MWSCAS47672.2021.9531760.
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10889v1)
- **Published**: 2021-06-21 07:04:52+00:00
- **Updated**: 2021-06-21 07:04:52+00:00
- **Authors**: Maedeh Sadat Fasihi, Wasfy B. Mikhael
- **Comment**: 4 pages, 5 figures, MWSCAS2021
- **Journal**: International Midwest Symposium on Circuits and Systems 1 (2021)
  529-532
- **Summary**: The performance of image classification methodsheavily relies on the high-quality annotations, which are noteasily affordable, particularly for medical data. To alleviate thislimitation, in this study, we propose a weakly supervised imageclassification method based on combination of hand-craftedfeatures. We hypothesize that integration of these hand-craftedfeatures alongside Long short-term memory (LSTM) classifiercan reduce the adverse effects of weak labels in classificationaccuracy. Our proposed algorithm is based on selecting theappropriate domain representations of the data in Wavelet andDiscrete Cosine Transform (DCT) domains. This informationis then fed into LSTM network to account for the sequentialnature of the data. The proposed efficient, low dimensionalfeatures exploit the power of shallow deep learning modelsto achieve higher performance with lower computational cost.In order to show efficacy of the proposed strategy, we haveexperimented classification of brain tumor grades and achievedthe state of the art performance with the resolution of 256 x 256. We also conducted a comprehensive set of experiments toanalyze the effect of each component on the performance.



### PIANO: A Parametric Hand Bone Model from Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/2106.10893v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10893v1)
- **Published**: 2021-06-21 07:21:20+00:00
- **Updated**: 2021-06-21 07:21:20+00:00
- **Authors**: Yuwei Li, Minye Wu, Yuyao Zhang, Lan Xu, Jingyi Yu
- **Comment**: Accepted to IJCAI 2021
- **Journal**: None
- **Summary**: Hand modeling is critical for immersive VR/AR, action understanding, or human healthcare. Existing parametric models account only for hand shape, pose, or texture, without modeling the anatomical attributes like bone, which is essential for realistic hand biomechanics analysis. In this paper, we present PIANO, the first parametric bone model of human hands from MRI data. Our PIANO model is biologically correct, simple to animate, and differentiable, achieving more anatomically precise modeling of the inner hand kinematic structure in a data-driven manner than the traditional hand models based on the outer surface only. Furthermore, our PIANO model can be applied in neural network layers to enable training with a fine-grained semantic loss, which opens up the new task of data-driven fine-grained hand bone anatomic and semantic understanding from MRI or even RGB images. We make our model publicly available.



### Self-Supervised Tracking via Target-Aware Data Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2106.10900v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10900v3)
- **Published**: 2021-06-21 07:40:34+00:00
- **Updated**: 2022-12-30 08:39:39+00:00
- **Authors**: Xin Li, Wenjie Pei, Yaowei Wang, Zhenyu He, Huchuan Lu, Ming-Hsuan Yang
- **Comment**: 11 pages, 7 figures, Accepted by IEEE Transactions on Neural Networks
  and Learning Systems
- **Journal**: None
- **Summary**: While deep-learning based tracking methods have achieved substantial progress, they entail large-scale and high-quality annotated data for sufficient training. To eliminate expensive and exhaustive annotation, we study self-supervised learning for visual tracking. In this work, we develop the Crop-Transform-Paste operation, which is able to synthesize sufficient training data by simulating various appearance variations during tracking, including appearance variations of objects and background interference. Since the target state is known in all synthesized data, existing deep trackers can be trained in routine ways using the synthesized data without human annotation. The proposed target-aware data-synthesis method adapts existing tracking approaches within a self-supervised learning framework without algorithmic changes. Thus, the proposed self-supervised learning mechanism can be seamlessly integrated into existing tracking frameworks to perform training. Extensive experiments show that our method 1) achieves favorable performance against supervised learning schemes under the cases with limited annotations; 2) helps deal with various tracking challenges such as object deformation, occlusion, or background clutter due to its manipulability; 3) performs favorably against state-of-the-art unsupervised tracking methods; 4) boosts the performance of various state-of-the-art supervised learning frameworks, including SiamRPN++, DiMP, and TransT.



### Surgical data science for safe cholecystectomy: a protocol for segmentation of hepatocystic anatomy and assessment of the critical view of safety
- **Arxiv ID**: http://arxiv.org/abs/2106.10916v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10916v2)
- **Published**: 2021-06-21 08:27:38+00:00
- **Updated**: 2021-09-20 16:59:55+00:00
- **Authors**: Pietro Mascagni, Deepak Alapatt, Alain Garcia, Nariaki Okamoto, Armine Vardazaryan, Guido Costamagna, Bernard Dallemagne, Nicolas Padoy
- **Comment**: 24 pages, 34 figures. v2: Minor revisions and code linked
- **Journal**: None
- **Summary**: Minimally invasive image-guided surgery heavily relies on vision. Deep learning models for surgical video analysis could therefore support visual tasks such as assessing the critical view of safety (CVS) in laparoscopic cholecystectomy (LC), potentially contributing to surgical safety and efficiency. However, the performance, reliability and reproducibility of such models are deeply dependent on the quality of data and annotations used in their development. Here, we present a protocol, checklists, and visual examples to promote consistent annotation of hepatocystic anatomy and CVS criteria. We believe that sharing annotation guidelines can help build trustworthy multicentric datasets for assessing generalizability of performance, thus accelerating the clinical translation of deep learning models for surgical video analysis.



### Trinity: A No-Code AI platform for complex spatial datasets
- **Arxiv ID**: http://arxiv.org/abs/2106.11756v5
- **DOI**: None
- **Categories**: **cs.SE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11756v5)
- **Published**: 2021-06-21 08:28:34+00:00
- **Updated**: 2021-07-01 06:22:23+00:00
- **Authors**: C. V. Krishnakumar Iyer, Feili Hou, Henry Wang, Yonghong Wang, Kay Oh, Swetava Ganguli, Vipul Pandey
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: We present a no-code Artificial Intelligence (AI) platform called Trinity with the main design goal of enabling both machine learning researchers and non-technical geospatial domain experts to experiment with domain-specific signals and datasets for solving a variety of complex problems on their own. This versatility to solve diverse problems is achieved by transforming complex Spatio-temporal datasets to make them consumable by standard deep learning models, in this case, Convolutional Neural Networks (CNNs), and giving the ability to formulate disparate problems in a standard way, eg. semantic segmentation. With an intuitive user interface, a feature store that hosts derivatives of complex feature engineering, a deep learning kernel, and a scalable data processing mechanism, Trinity provides a powerful platform for domain experts to share the stage with scientists and engineers in solving business-critical problems. It enables quick prototyping, rapid experimentation and reduces the time to production by standardizing model building and deployment. In this paper, we present our motivation behind Trinity and its design along with showcasing sample applications to motivate the idea of lowering the bar to using AI.



### Cross-layer Navigation Convolutional Neural Network for Fine-grained Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.10920v1
- **DOI**: None
- **Categories**: **cs.CV**, 14J60 (Primary) 14F05, 14J26 (Secondary), F.2.2; I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2106.10920v1)
- **Published**: 2021-06-21 08:38:27+00:00
- **Updated**: 2021-06-21 08:38:27+00:00
- **Authors**: Chenyu Guo, Jiyang Xie, Kongming Liang, Xian Sun, Zhanyu Ma
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Fine-grained visual classification (FGVC) aims to classify sub-classes of objects in the same super-class (e.g., species of birds, models of cars). For the FGVC tasks, the essential solution is to find discriminative subtle information of the target from local regions. TraditionalFGVC models preferred to use the refined features,i.e., high-level semantic information for recognition and rarely use low-level in-formation. However, it turns out that low-level information which contains rich detail information also has effect on improving performance. Therefore, in this paper, we propose cross-layer navigation convolutional neural network for feature fusion. First, the feature maps extracted by the backbone network are fed into a convolutional long short-term memory model sequentially from high-level to low-level to perform feature aggregation. Then, attention mechanisms are used after feature fusion to extract spatial and channel information while linking the high-level semantic information and the low-level texture features, which can better locate the discriminative regions for the FGVC. In the experiments, three commonly used FGVC datasets, including CUB-200-2011, Stanford-Cars, andFGVC-Aircraft datasets, are used for evaluation and we demonstrate the superiority of the proposed method by comparing it with other referred FGVC methods to show that this method achieves superior results.



### Unsupervised Deep Learning by Injecting Low-Rank and Sparse Priors
- **Arxiv ID**: http://arxiv.org/abs/2106.10923v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10923v1)
- **Published**: 2021-06-21 08:41:02+00:00
- **Updated**: 2021-06-21 08:41:02+00:00
- **Authors**: Tomoya Sakai
- **Comment**: None
- **Journal**: None
- **Summary**: What if deep neural networks can learn from sparsity-inducing priors? When the networks are designed by combining layer modules (CNN, RNN, etc), engineers less exploit the inductive bias, i.e., existing well-known rules or prior knowledge, other than annotated training data sets. We focus on employing sparsity-inducing priors in deep learning to encourage the network to concisely capture the nature of high-dimensional data in an unsupervised way. In order to use non-differentiable sparsity-inducing norms as loss functions, we plug their proximal mappings into the automatic differentiation framework. We demonstrate unsupervised learning of U-Net for background subtraction using low-rank and sparse priors. The U-Net can learn moving objects in a training sequence without any annotation, and successfully detect the foreground objects in test sequences.



### TCIC: Theme Concepts Learning Cross Language and Vision for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2106.10936v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2106.10936v1)
- **Published**: 2021-06-21 09:12:55+00:00
- **Updated**: 2021-06-21 09:12:55+00:00
- **Authors**: Zhihao Fan, Zhongyu Wei, Siyuan Wang, Ruize Wang, Zejun Li, Haijun Shan, Xuanjing Huang
- **Comment**: IJCAI2021
- **Journal**: None
- **Summary**: Existing research for image captioning usually represents an image using a scene graph with low-level facts (objects and relations) and fails to capture the high-level semantics. In this paper, we propose a Theme Concepts extended Image Captioning (TCIC) framework that incorporates theme concepts to represent high-level cross-modality semantics. In practice, we model theme concepts as memory vectors and propose Transformer with Theme Nodes (TTN) to incorporate those vectors for image captioning. Considering that theme concepts can be learned from both images and captions, we propose two settings for their representations learning based on TTN. On the vision side, TTN is configured to take both scene graph based features and theme concepts as input for visual representation learning. On the language side, TTN is configured to take both captions and theme concepts as input for text representation re-construction. Both settings aim to generate target captions with the same transformer-based decoder. During the training, we further align representations of theme concepts learned from images and corresponding captions to enforce the cross-modality learning. Experimental results on MS COCO show the effectiveness of our approach compared to some state-of-the-art models.



### A Game-Theoretic Taxonomy of Visual Concepts in DNNs
- **Arxiv ID**: http://arxiv.org/abs/2106.10938v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10938v1)
- **Published**: 2021-06-21 09:16:51+00:00
- **Updated**: 2021-06-21 09:16:51+00:00
- **Authors**: Xu Cheng, Chuntung Chu, Yi Zheng, Jie Ren, Quanshi Zhang
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: In this paper, we rethink how a DNN encodes visual concepts of different complexities from a new perspective, i.e. the game-theoretic multi-order interactions between pixels in an image. Beyond the categorical taxonomy of objects and the cognitive taxonomy of textures and shapes, we provide a new taxonomy of visual concepts, which helps us interpret the encoding of shapes and textures, in terms of concept complexities. In this way, based on multi-order interactions, we find three distinctive signal-processing behaviors of DNNs encoding textures. Besides, we also discover the flexibility for a DNN to encode shapes is lower than the flexibility of encoding textures. Furthermore, we analyze how DNNs encode outlier samples, and explore the impacts of network architectures on interactions. Additionally, we clarify the crucial role of the multi-order interactions in real-world applications. The code will be released when the paper is accepted.



### Hard hat wearing detection based on head keypoint localization
- **Arxiv ID**: http://arxiv.org/abs/2106.10944v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2106.10944v2)
- **Published**: 2021-06-21 09:31:33+00:00
- **Updated**: 2022-06-24 10:27:20+00:00
- **Authors**: Bartosz Wójcik, Mateusz Żarski, Kamil Książek, Jarosław Adam Miszczak, Mirosław Jan Skibniewski
- **Comment**: 17 pages, 9 figures and 9 tables
- **Journal**: None
- **Summary**: In recent years, a lot of attention is paid to deep learning methods in the context of vision-based construction site safety systems, especially regarding personal protective equipment. However, despite all this attention, there is still no reliable way to establish the relationship between workers and their hard hats. To answer this problem a combination of deep learning, object detection and head keypoint localization, with simple rule-based reasoning is proposed in this article. In tests, this solution surpassed the previous methods based on the relative bounding box position of different instances, as well as direct detection of hard hat wearers and non-wearers. The results show that the conjunction of novel deep learning methods with humanly-interpretable rule-based systems can result in a solution that is both reliable and can successfully mimic manual, on-site supervision. This work is the next step in the development of fully autonomous construction site safety systems and shows that there is still room for improvement in this area.



### Leveraging Conditional Generative Models in a General Explanation Framework of Classifier Decisions
- **Arxiv ID**: http://arxiv.org/abs/2106.10947v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10947v1)
- **Published**: 2021-06-21 09:41:54+00:00
- **Updated**: 2021-06-21 09:41:54+00:00
- **Authors**: Martin Charachon, Paul-Henry Cournède, Céline Hudelot, Roberto Ardon
- **Comment**: None
- **Journal**: None
- **Summary**: Providing a human-understandable explanation of classifiers' decisions has become imperative to generate trust in their use for day-to-day tasks. Although many works have addressed this problem by generating visual explanation maps, they often provide noisy and inaccurate results forcing the use of heuristic regularization unrelated to the classifier in question. In this paper, we propose a new general perspective of the visual explanation problem overcoming these limitations. We show that visual explanation can be produced as the difference between two generated images obtained via two specific conditional generative models. Both generative models are trained using the classifier to explain and a database to enforce the following properties: (i) All images generated by the first generator are classified similarly to the input image, whereas the second generator's outputs are classified oppositely. (ii) Generated images belong to the distribution of real images. (iii) The distances between the input image and the corresponding generated images are minimal so that the difference between the generated elements only reveals relevant information for the studied classifier. Using symmetrical and cyclic constraints, we present two different approximations and implementations of the general formulation. Experimentally, we demonstrate significant improvements w.r.t the state-of-the-art on three different public data sets. In particular, the localization of regions influencing the classifier is consistent with human annotations.



### Multiple Object Tracking with Mixture Density Networks for Trajectory Estimation
- **Arxiv ID**: http://arxiv.org/abs/2106.10950v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10950v2)
- **Published**: 2021-06-21 09:45:27+00:00
- **Updated**: 2021-06-22 01:55:48+00:00
- **Authors**: Andreu Girbau, Xavier Giró-i-Nieto, Ignasi Rius, Ferran Marqués
- **Comment**: Best paper runner up on CVPR 2021 RVSU workshop
- **Journal**: None
- **Summary**: Multiple object tracking faces several challenges that may be alleviated with trajectory information. Knowing the posterior locations of an object helps disambiguating and solving situations such as occlusions, re-identification, and identity switching. In this work, we show that trajectory estimation can become a key factor for tracking, and present TrajE, a trajectory estimator based on recurrent mixture density networks, as a generic module that can be added to existing object trackers. To provide several trajectory hypotheses, our method uses beam search. Also, relying on the same estimated trajectory, we propose to reconstruct a track after an occlusion occurs. We integrate TrajE into two state of the art tracking algorithms, CenterTrack [63] and Tracktor [3]. Their respective performances in the MOTChallenge 2017 test set are boosted 6.3 and 0.3 points in MOTA score, and 1.8 and 3.1 in IDF1, setting a new state of the art for the CenterTrack+TrajE configuration



### Segmentation of cell-level anomalies in electroluminescence images of photovoltaic modules
- **Arxiv ID**: http://arxiv.org/abs/2106.10962v2
- **DOI**: 10.1016/j.solener.2021.03.058
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.10962v2)
- **Published**: 2021-06-21 10:17:40+00:00
- **Updated**: 2022-01-14 13:06:58+00:00
- **Authors**: Urtzi Otamendi, Iñigo Martinez, Marco Quartulli, Igor G. Olaizola, Elisabeth Viles, Werther Cambarau
- **Comment**: 16 pages, 14 figures
- **Journal**: Solar Energy, Volume 220, 2021
- **Summary**: In the operation & maintenance (O&M) of photovoltaic (PV) plants, the early identification of failures has become crucial to maintain productivity and prolong components' life. Of all defects, cell-level anomalies can lead to serious failures and may affect surrounding PV modules in the long run. These fine defects are usually captured with high spatial resolution electroluminescence (EL) imaging. The difficulty of acquiring such images has limited the availability of data. For this work, multiple data resources and augmentation techniques have been used to surpass this limitation. Current state-of-the-art detection methods extract barely low-level information from individual PV cell images, and their performance is conditioned by the available training data. In this article, we propose an end-to-end deep learning pipeline that detects, locates and segments cell-level anomalies from entire photovoltaic modules via EL images. The proposed modular pipeline combines three deep learning techniques: 1. object detection (modified Faster-RNN), 2. image classification (EfficientNet) and 3. weakly supervised segmentation (autoencoder). The modular nature of the pipeline allows to upgrade the deep learning models to the further improvements in the state-of-the-art and also extend the pipeline towards new functionalities.



### Perception-Aware Multi-Sensor Fusion for 3D LiDAR Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.15277v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15277v2)
- **Published**: 2021-06-21 10:47:26+00:00
- **Updated**: 2021-08-18 04:04:45+00:00
- **Authors**: Zhuangwei Zhuang, Rong Li, Kui Jia, Qicheng Wang, Yuanqing Li, Mingkui Tan
- **Comment**: 20 pages. ICCV2021
- **Journal**: None
- **Summary**: 3D LiDAR (light detection and ranging) semantic segmentation is important in scene understanding for many applications, such as auto-driving and robotics. For example, for autonomous cars equipped with RGB cameras and LiDAR, it is crucial to fuse complementary information from different sensors for robust and accurate segmentation. Existing fusion-based methods, however, may not achieve promising performance due to the vast difference between the two modalities. In this work, we investigate a collaborative fusion scheme called perception-aware multi-sensor fusion (PMF) to exploit perceptual information from two modalities, namely, appearance information from RGB images and spatio-depth information from point clouds. To this end, we first project point clouds to the camera coordinates to provide spatio-depth information for RGB images. Then, we propose a two-stream network to extract features from the two modalities, separately, and fuse the features by effective residual-based fusion modules. Moreover, we propose additional perception-aware losses to measure the perceptual difference between the two modalities. Extensive experiments on two benchmark data sets show the superiority of our method. For example, on nuScenes, our PMF outperforms the state-of-the-art method by 0.8 in mIoU.



### SHREC 2021: Track on Skeleton-based Hand Gesture Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2106.10980v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.10980v1)
- **Published**: 2021-06-21 10:57:49+00:00
- **Updated**: 2021-06-21 10:57:49+00:00
- **Authors**: Ariel Caputo, Andrea Giachetti, Simone Soso, Deborah Pintani, Andrea D'Eusanio, Stefano Pini, Guido Borghi, Alessandro Simoni, Roberto Vezzani, Rita Cucchiara, Andrea Ranieri, Franca Giannini, Katia Lupinetti, Marina Monti, Mehran Maghoumi, Joseph J. LaViola Jr, Minh-Quan Le, Hai-Dang Nguyen, Minh-Triet Tran
- **Comment**: 12 pages, to be published on Computers & Graphics
- **Journal**: None
- **Summary**: Gesture recognition is a fundamental tool to enable novel interaction paradigms in a variety of application scenarios like Mixed Reality environments, touchless public kiosks, entertainment systems, and more. Recognition of hand gestures can be nowadays performed directly from the stream of hand skeletons estimated by software provided by low-cost trackers (Ultraleap) and MR headsets (Hololens, Oculus Quest) or by video processing software modules (e.g. Google Mediapipe). Despite the recent advancements in gesture and action recognition from skeletons, it is unclear how well the current state-of-the-art techniques can perform in a real-world scenario for the recognition of a wide set of heterogeneous gestures, as many benchmarks do not test online recognition and use limited dictionaries. This motivated the proposal of the SHREC 2021: Track on Skeleton-based Hand Gesture Recognition in the Wild. For this contest, we created a novel dataset with heterogeneous gestures featuring different types and duration. These gestures have to be found inside sequences in an online recognition scenario. This paper presents the result of the contest, showing the performances of the techniques proposed by four research groups on the challenging task compared with a simple baseline method.



### ImageNet Pre-training also Transfers Non-Robustness
- **Arxiv ID**: http://arxiv.org/abs/2106.10989v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.10989v4)
- **Published**: 2021-06-21 11:16:13+00:00
- **Updated**: 2022-12-06 03:28:02+00:00
- **Authors**: Jiaming Zhang, Jitao Sang, Qi Yi, Yunfan Yang, Huiwen Dong, Jian Yu
- **Comment**: Accepted by AAAI2023
- **Journal**: None
- **Summary**: ImageNet pre-training has enabled state-of-the-art results on many tasks. In spite of its recognized contribution to generalization, we observed in this study that ImageNet pre-training also transfers adversarial non-robustness from pre-trained model into fine-tuned model in the downstream classification tasks. We first conducted experiments on various datasets and network backbones to uncover the adversarial non-robustness in fine-tuned model. Further analysis was conducted on examining the learned knowledge of fine-tuned model and standard model, and revealed that the reason leading to the non-robustness is the non-robust features transferred from ImageNet pre-trained model. Finally, we analyzed the preference for feature learning of the pre-trained model, explored the factors influencing robustness, and introduced a simple robust ImageNet pre-training solution. Our code is available at \url{https://github.com/jiamingzhang94/ImageNet-Pretraining-transfers-non-robustness}.



### Estimating MRI Image Quality via Image Reconstruction Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2106.10992v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10992v1)
- **Published**: 2021-06-21 11:22:17+00:00
- **Updated**: 2021-06-21 11:22:17+00:00
- **Authors**: Richard Shaw, Carole H. Sudre, Sebastien Ourselin, M. Jorge Cardoso
- **Comment**: None
- **Journal**: None
- **Summary**: Quality control (QC) in medical image analysis is time-consuming and laborious, leading to increased interest in automated methods. However, what is deemed suitable quality for algorithmic processing may be different from human-perceived measures of visual quality. In this work, we pose MR image quality assessment from an image reconstruction perspective. We train Bayesian CNNs using a heteroscedastic uncertainty model to recover clean images from noisy data, providing measures of uncertainty over the predictions. This framework enables us to divide data corruption into learnable and non-learnable components and leads us to interpret the predictive uncertainty as an estimation of the achievable recovery of an image. Thus, we argue that quality control for visual assessment cannot be equated to quality control for algorithmic processing. We validate this statement in a multi-task experiment combining artefact recovery with uncertainty prediction and grey matter segmentation. Recognising this distinction between visual and algorithmic quality has the impact that, depending on the downstream task, less data can be excluded based on ``visual quality" reasons alone.



### How to Reach Real-Time AI on Consumer Devices? Solutions for Programmable and Custom Architectures
- **Arxiv ID**: http://arxiv.org/abs/2106.15021v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15021v1)
- **Published**: 2021-06-21 11:23:12+00:00
- **Updated**: 2021-06-21 11:23:12+00:00
- **Authors**: Stylianos I. Venieris, Ioannis Panopoulos, Ilias Leontiadis, Iakovos S. Venieris
- **Comment**: Invited paper at the 32nd IEEE International Conference on
  Application-Specific Systems, Architectures and Processors (ASAP), 2021
- **Journal**: None
- **Summary**: The unprecedented performance of deep neural networks (DNNs) has led to large strides in various Artificial Intelligence (AI) inference tasks, such as object and speech recognition. Nevertheless, deploying such AI models across commodity devices faces significant challenges: large computational cost, multiple performance objectives, hardware heterogeneity and a common need for high accuracy, together pose critical problems to the deployment of DNNs across the various embedded and mobile devices in the wild. As such, we have yet to witness the mainstream usage of state-of-the-art deep learning algorithms across consumer devices. In this paper, we provide preliminary answers to this potentially game-changing question by presenting an array of design techniques for efficient AI systems. We start by examining the major roadblocks when targeting both programmable processors and custom accelerators. Then, we present diverse methods for achieving real-time performance following a cross-stack approach. These span model-, system- and hardware-level techniques, and their combination. Our findings provide illustrative examples of AI systems that do not overburden mobile hardware, while also indicating how they can improve inference accuracy. Moreover, we showcase how custom ASIC- and FPGA-based accelerators can be an enabling factor for next-generation AI applications, such as multi-DNN systems. Collectively, these results highlight the critical need for further exploration as to how the various cross-stack solutions can be best combined in order to bring the latest advances in deep learning close to users, in a robust and efficient manner.



### Delving into the pixels of adversarial samples
- **Arxiv ID**: http://arxiv.org/abs/2106.10996v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.10996v1)
- **Published**: 2021-06-21 11:28:06+00:00
- **Updated**: 2021-06-21 11:28:06+00:00
- **Authors**: Blerta Lindqvist
- **Comment**: None
- **Journal**: None
- **Summary**: Despite extensive research into adversarial attacks, we do not know how adversarial attacks affect image pixels. Knowing how image pixels are affected by adversarial attacks has the potential to lead us to better adversarial defenses. Motivated by instances that we find where strong attacks do not transfer, we delve into adversarial examples at pixel level to scrutinize how adversarial attacks affect image pixel values. We consider several ImageNet architectures, InceptionV3, VGG19 and ResNet50, as well as several strong attacks. We find that attacks can have different effects at pixel level depending on classifier architecture. In particular, input pre-processing plays a previously overlooked role in the effect that attacks have on pixels. Based on the insights of pixel-level examination, we find new ways to detect some of the strongest current attacks.



### Interventional Video Grounding with Dual Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.11013v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2106.11013v2)
- **Published**: 2021-06-21 12:11:28+00:00
- **Updated**: 2021-07-07 15:10:07+00:00
- **Authors**: Guoshun Nan, Rui Qiao, Yao Xiao, Jun Liu, Sicong Leng, Hao Zhang, Wei Lu
- **Comment**: Accepted in CVPR 2021
- **Journal**: None
- **Summary**: Video grounding aims to localize a moment from an untrimmed video for a given textual query. Existing approaches focus more on the alignment of visual and language stimuli with various likelihood-based matching or regression strategies, i.e., P(Y|X). Consequently, these models may suffer from spurious correlations between the language and video features due to the selection bias of the dataset. 1) To uncover the causality behind the model and data, we first propose a novel paradigm from the perspective of the causal inference, i.e., interventional video grounding (IVG) that leverages backdoor adjustment to deconfound the selection bias based on structured causal model (SCM) and do-calculus P(Y|do(X)). Then, we present a simple yet effective method to approximate the unobserved confounder as it cannot be directly sampled from the dataset. 2) Meanwhile, we introduce a dual contrastive learning approach (DCL) to better align the text and video by maximizing the mutual information (MI) between query and video clips, and the MI between start/end frames of a target moment and the others within a video to learn more informative visual representations. Experiments on three standard benchmarks show the effectiveness of our approaches. Our code is available on GitHub: https://github.com/nanguoshun/IVG.



### One Million Scenes for Autonomous Driving: ONCE Dataset
- **Arxiv ID**: http://arxiv.org/abs/2106.11037v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11037v3)
- **Published**: 2021-06-21 12:28:08+00:00
- **Updated**: 2021-10-25 08:08:25+00:00
- **Authors**: Jiageng Mao, Minzhe Niu, Chenhan Jiang, Hanxue Liang, Jingheng Chen, Xiaodan Liang, Yamin Li, Chaoqiang Ye, Wei Zhang, Zhenguo Li, Jie Yu, Hang Xu, Chunjing Xu
- **Comment**: NeurIPS 2021 Datasets and Benchmarks Track
- **Journal**: None
- **Summary**: Current perception models in autonomous driving have become notorious for greatly relying on a mass of annotated data to cover unseen cases and address the long-tail problem. On the other hand, learning from unlabeled large-scale collected data and incrementally self-training powerful recognition models have received increasing attention and may become the solutions of next-generation industry-level powerful and robust perception models in autonomous driving. However, the research community generally suffered from data inadequacy of those essential real-world scene data, which hampers the future exploration of fully/semi/self-supervised methods for 3D perception. In this paper, we introduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the autonomous driving scenario. The ONCE dataset consists of 1 million LiDAR scenes and 7 million corresponding camera images. The data is selected from 144 driving hours, which is 20x longer than the largest 3D autonomous driving dataset available (e.g. nuScenes and Waymo), and it is collected across a range of different areas, periods and weather conditions. To facilitate future research on exploiting unlabeled data for 3D detection, we additionally provide a benchmark in which we reproduce and evaluate a variety of self-supervised and semi-supervised methods on the ONCE dataset. We conduct extensive analyses on those methods and provide valuable observations on their performance related to the scale of used data. Data, code, and more information are available at https://once-for-auto-driving.github.io/index.html.



### CataNet: Predicting remaining cataract surgery duration
- **Arxiv ID**: http://arxiv.org/abs/2106.11048v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11048v1)
- **Published**: 2021-06-21 12:35:34+00:00
- **Updated**: 2021-06-21 12:35:34+00:00
- **Authors**: Andrés Marafioti, Michel Hayoz, Mathias Gallardo, Pablo Márquez Neila, Sebastian Wolf, Martin Zinkernagel, Raphael Sznitman
- **Comment**: Accepted at MICCAI 2021
- **Journal**: None
- **Summary**: Cataract surgery is a sight saving surgery that is performed over 10 million times each year around the world. With such a large demand, the ability to organize surgical wards and operating rooms efficiently is critical to delivery this therapy in routine clinical care. In this context, estimating the remaining surgical duration (RSD) during procedures is one way to help streamline patient throughput and workflows. To this end, we propose CataNet, a method for cataract surgeries that predicts in real time the RSD jointly with two influential elements: the surgeon's experience, and the current phase of the surgery. We compare CataNet to state-of-the-art RSD estimation methods, showing that it outperforms them even when phase and experience are not considered. We investigate this improvement and show that a significant contributor is the way we integrate the elapsed time into CataNet's feature extractor.



### Visual Probing: Cognitive Framework for Explaining Self-Supervised Image Representations
- **Arxiv ID**: http://arxiv.org/abs/2106.11054v3
- **DOI**: 10.1109/ACCESS.2023.3242982
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11054v3)
- **Published**: 2021-06-21 12:40:31+00:00
- **Updated**: 2022-08-21 08:16:34+00:00
- **Authors**: Witold Oleszkiewicz, Dominika Basaj, Igor Sieradzki, Michał Górszczak, Barbara Rychalska, Koryna Lewandowska, Tomasz Trzciński, Bartosz Zieliński
- **Comment**: Submitted to IEEE Access
- **Journal**: None
- **Summary**: Recently introduced self-supervised methods for image representation learning provide on par or superior results to their fully supervised competitors, yet the corresponding efforts to explain the self-supervised approaches lag behind. Motivated by this observation, we introduce a novel visual probing framework for explaining the self-supervised models by leveraging probing tasks employed previously in natural language processing. The probing tasks require knowledge about semantic relationships between image parts. Hence, we propose a systematic approach to obtain analogs of natural language in vision, such as visual words, context, and taxonomy. Our proposal is grounded in Marr's computational theory of vision and concerns features like textures, shapes, and lines. We show the effectiveness and applicability of those analogs in the context of explaining self-supervised representations. Our key findings emphasize that relations between language and vision can serve as an effective yet intuitive tool for discovering how machine learning models work, independently of data modality. Our work opens a plethora of research pathways towards more explainable and transparent AI.



### CLIP2Video: Mastering Video-Text Retrieval via Image CLIP
- **Arxiv ID**: http://arxiv.org/abs/2106.11097v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11097v1)
- **Published**: 2021-06-21 13:30:33+00:00
- **Updated**: 2021-06-21 13:30:33+00:00
- **Authors**: Han Fang, Pengfei Xiong, Luhui Xu, Yu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: We present CLIP2Video network to transfer the image-language pre-training model to video-text retrieval in an end-to-end manner. Leading approaches in the domain of video-and-language learning try to distill the spatio-temporal video features and multi-modal interaction between videos and languages from a large-scale video-text dataset. Different from them, we leverage pretrained image-language model, simplify it as a two-stage framework with co-learning of image-text and enhancing temporal relations between video frames and video-text respectively, make it able to train on comparatively small datasets. Specifically, based on the spatial semantics captured by Contrastive Language-Image Pretraining (CLIP) model, our model involves a Temporal Difference Block to capture motions at fine temporal video frames, and a Temporal Alignment Block to re-align the tokens of video clips and phrases and enhance the multi-modal correlation. We conduct thorough ablation studies, and achieve state-of-the-art performance on major text-to-video and video-to-text retrieval benchmarks, including new records of retrieval accuracy on MSR-VTT, MSVD and VATEX.



### Obstacle Detection for BVLOS Drones
- **Arxiv ID**: http://arxiv.org/abs/2106.11098v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11098v2)
- **Published**: 2021-06-21 13:31:54+00:00
- **Updated**: 2021-06-22 13:31:13+00:00
- **Authors**: Jan Moros Esteban, Jaap van de Loosdrecht, Maya Aghaei
- **Comment**: 7 pages, 7 figures, Supervisors: Maya Aghaei Gavari and Jaap van de
  Loosdrecht
- **Journal**: None
- **Summary**: With the introduction of new regulations in the European Union, the future of Beyond Visual Line Of Sight (BVLOS) drones is set to bloom. This led to the creation of the theBEAST project, which aims to create an autonomous security drone, with focus on those regulations and on safety. This technical paper describes the first steps of a module within this project, which revolves around detecting obstacles so they can be avoided in a fail-safe landing. A deep learning powered object detection method is the subject of our research, and various experiments are held to maximize its performance, such as comparing various data augmentation techniques or YOLOv3 and YOLOv5. According to the results of the experiments, we conclude that although object detection is a promising approach to resolve this problem, more volume of data is required for potential usage in a real-life application.



### Distilling effective supervision for robust medical image segmentation with noisy labels
- **Arxiv ID**: http://arxiv.org/abs/2106.11099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11099v1)
- **Published**: 2021-06-21 13:33:38+00:00
- **Updated**: 2021-06-21 13:33:38+00:00
- **Authors**: Jialin Shi, Ji Wu
- **Comment**: Accepted to MICCAI 2021
- **Journal**: None
- **Summary**: Despite the success of deep learning methods in medical image segmentation tasks, the human-level performance relies on massive training data with high-quality annotations, which are expensive and time-consuming to collect. The fact is that there exist low-quality annotations with label noise, which leads to suboptimal performance of learned models. Two prominent directions for segmentation learning with noisy labels include pixel-wise noise robust training and image-level noise robust training. In this work, we propose a novel framework to address segmenting with noisy labels by distilling effective supervision information from both pixel and image levels. In particular, we explicitly estimate the uncertainty of every pixel as pixel-wise noise estimation, and propose pixel-wise robust learning by using both the original labels and pseudo labels. Furthermore, we present an image-level robust learning method to accommodate more information as the complements to pixel-level learning. We conduct extensive experiments on both simulated and real-world noisy datasets. The results demonstrate the advantageous performance of our method compared to state-of-the-art baselines for medical image segmentation with noisy labels.



### SODA10M: A Large-Scale 2D Self/Semi-Supervised Object Detection Dataset for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2106.11118v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11118v3)
- **Published**: 2021-06-21 13:55:57+00:00
- **Updated**: 2021-11-08 03:42:16+00:00
- **Authors**: Jianhua Han, Xiwen Liang, Hang Xu, Kai Chen, Lanqing Hong, Jiageng Mao, Chaoqiang Ye, Wei Zhang, Zhenguo Li, Xiaodan Liang, Chunjing Xu
- **Comment**: NeurIPS 2021 Datasets and Benchmarks Track
- **Journal**: None
- **Summary**: Aiming at facilitating a real-world, ever-evolving and scalable autonomous driving system, we present a large-scale dataset for standardizing the evaluation of different self-supervised and semi-supervised approaches by learning from raw data, which is the first and largest dataset to date. Existing autonomous driving systems heavily rely on `perfect' visual perception models (i.e., detection) trained using extensive annotated data to ensure safety. However, it is unrealistic to elaborately label instances of all scenarios and circumstances (i.e., night, extreme weather, cities) when deploying a robust autonomous driving system. Motivated by recent advances of self-supervised and semi-supervised learning, a promising direction is to learn a robust detection model by collaboratively exploiting large-scale unlabeled data and few labeled data. Existing datasets either provide only a small amount of data or covers limited domains with full annotation, hindering the exploration of large-scale pre-trained models. Here, we release a Large-Scale 2D Self/semi-supervised Object Detection dataset for Autonomous driving, named as SODA10M, containing 10 million unlabeled images and 20K images labeled with 6 representative object categories. To improve diversity, the images are collected within 27833 driving hours under different weather conditions, periods and location scenes of 32 different cities. We provide extensive experiments and deep analyses of existing popular self/semi-supervised approaches, and give some interesting findings in autonomous driving scope. Experiments show that SODA10M can serve as a promising pre-training dataset for different self-supervised learning methods, which gives superior performance when fine-tuning with different downstream tasks (i.e., detection, semantic/instance segmentation) in autonomous driving domain. More information can refer to https://soda-2d.github.io.



### Graceful Degradation and Related Fields
- **Arxiv ID**: http://arxiv.org/abs/2106.11119v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11119v2)
- **Published**: 2021-06-21 13:56:41+00:00
- **Updated**: 2021-06-24 12:30:26+00:00
- **Authors**: Jack Dymond
- **Comment**: None
- **Journal**: None
- **Summary**: When machine learning models encounter data which is out of the distribution on which they were trained they have a tendency to behave poorly, most prominently over-confidence in erroneous predictions. Such behaviours will have disastrous effects on real-world machine learning systems. In this field graceful degradation refers to the optimisation of model performance as it encounters this out-of-distribution data. This work presents a definition and discussion of graceful degradation and where it can be applied in deployed visual systems. Following this a survey of relevant areas is undertaken, novelly splitting the graceful degradation problem into active and passive approaches. In passive approaches, graceful degradation is handled and achieved by the model in a self-contained manner, in active approaches the model is updated upon encountering epistemic uncertainties. This work communicates the importance of the problem and aims to prompt the development of machine learning strategies that are aware of graceful degradation.



### FP-Age: Leveraging Face Parsing Attention for Facial Age Estimation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2106.11145v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11145v2)
- **Published**: 2021-06-21 14:31:32+00:00
- **Updated**: 2022-03-04 16:42:33+00:00
- **Authors**: Yiming Lin, Jie Shen, Yujiang Wang, Maja Pantic
- **Comment**: Accepted by Transactions of Image Processing. Code and data are
  available on https://github.com/ibug-group/fpage
- **Journal**: None
- **Summary**: Image-based age estimation aims to predict a person's age from facial images. It is used in a variety of real-world applications. Although end-to-end deep models have achieved impressive results for age estimation on benchmark datasets, their performance in-the-wild still leaves much room for improvement due to the challenges caused by large variations in head pose, facial expressions, and occlusions. To address this issue, we propose a simple yet effective method to explicitly incorporate facial semantics into age estimation, so that the model would learn to correctly focus on the most informative facial components from unaligned facial images regardless of head pose and non-rigid deformation. To this end, we design a face parsing-based network to learn semantic information at different scales and a novel face parsing attention module to leverage these semantic features for age estimation. To evaluate our method on in-the-wild data, we also introduce a new challenging large-scale benchmark called IMDB-Clean. This dataset is created by semi-automatically cleaning the noisy IMDB-WIKI dataset using a constrained clustering method. Through comprehensive experiment on IMDB-Clean and other benchmark datasets, under both intra-dataset and cross-dataset evaluation protocols, we show that our method consistently outperforms all existing age estimation methods and achieves a new state-of-the-art performance. To the best of our knowledge, our work presents the first attempt of leveraging face parsing attention to achieve semantic-aware age estimation, which may be inspiring to other high level facial analysis tasks. Code and data are available on \url{https://github.com/ibug-group/fpage}.



### OadTR: Online Action Detection with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2106.11149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11149v1)
- **Published**: 2021-06-21 14:39:35+00:00
- **Updated**: 2021-06-21 14:39:35+00:00
- **Authors**: Xiang Wang, Shiwei Zhang, Zhiwu Qing, Yuanjie Shao, Zhengrong Zuo, Changxin Gao, Nong Sang
- **Comment**: Code is available at https://github.com/wangxiang1230/OadTR
- **Journal**: None
- **Summary**: Most recent approaches for online action detection tend to apply Recurrent Neural Network (RNN) to capture long-range temporal structure. However, RNN suffers from non-parallelism and gradient vanishing, hence it is hard to be optimized. In this paper, we propose a new encoder-decoder framework based on Transformers, named OadTR, to tackle these problems. The encoder attached with a task token aims to capture the relationships and global interactions between historical observations. The decoder extracts auxiliary information by aggregating anticipated future clip representations. Therefore, OadTR can recognize current actions by encoding historical information and predicting future context simultaneously. We extensively evaluate the proposed OadTR on three challenging datasets: HDD, TVSeries, and THUMOS14. The experimental results show that OadTR achieves higher training and inference speeds than current RNN based approaches, and significantly outperforms the state-of-the-art methods in terms of both mAP and mcAP. Code is available at https://github.com/wangxiang1230/OadTR.



### Automatic Plant Cover Estimation with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.11154v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11154v3)
- **Published**: 2021-06-21 14:52:01+00:00
- **Updated**: 2021-07-28 17:28:31+00:00
- **Authors**: Matthias Körschens, Paul Bodesheim, Christine Römermann, Solveig Franziska Bucher, Mirco Migliavacca, Josephine Ulrich, Joachim Denzler
- **Comment**: None
- **Journal**: None
- **Summary**: Monitoring the responses of plants to environmental changes is essential for plant biodiversity research. This, however, is currently still being done manually by botanists in the field. This work is very laborious, and the data obtained is, though following a standardized method to estimate plant coverage, usually subjective and has a coarse temporal resolution. To remedy these caveats, we investigate approaches using convolutional neural networks (CNNs) to automatically extract the relevant data from images, focusing on plant community composition and species coverages of 9 herbaceous plant species. To this end, we investigate several standard CNN architectures and different pretraining methods. We find that we outperform our previous approach at higher image resolutions using a custom CNN with a mean absolute error of 5.16%. In addition to these investigations, we also conduct an error analysis based on the temporal aspect of the plant cover images. This analysis gives insight into where problems for automatic approaches lie, like occlusion and likely misclassifications caused by temporal changes.



### 3D Shape Registration Using Spectral Graph Embedding and Probabilistic Matching
- **Arxiv ID**: http://arxiv.org/abs/2106.11166v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2106.11166v1)
- **Published**: 2021-06-21 15:02:31+00:00
- **Updated**: 2021-06-21 15:02:31+00:00
- **Authors**: Avinash Sharma, Radu Horaud, Diana Mateus
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of 3D shape registration and we propose a novel technique based on spectral graph theory and probabilistic matching. The task of 3D shape analysis involves tracking, recognition, registration, etc. Analyzing 3D data in a single framework is still a challenging task considering the large variability of the data gathered with different acquisition devices. 3D shape registration is one such challenging shape analysis task. The main contribution of this chapter is to extend the spectral graph matching methods to very large graphs by combining spectral graph matching with Laplacian embedding. Since the embedded representation of a graph is obtained by dimensionality reduction we claim that the existing spectral-based methods are not easily applicable. We discuss solutions for the exact and inexact graph isomorphism problems and recall the main spectral properties of the combinatorial graph Laplacian; We provide a novel analysis of the commute-time embedding that allows us to interpret the latter in terms of the PCA of a graph, and to select the appropriate dimension of the associated embedded metric space; We derive a unit hyper-sphere normalization for the commute-time embedding that allows us to register two shapes with different samplings; We propose a novel method to find the eigenvalue-eigenvector ordering and the eigenvector signs using the eigensignature (histogram) which is invariant to the isometric shape deformations and fits well in the spectral graph matching framework, and we present a probabilistic shape matching formulation using an expectation maximization point registration algorithm which alternates between aligning the eigenbases and finding a vertex-to-vertex assignment.



### On the potential of sequential and non-sequential regression models for Sentinel-1-based biomass prediction in Tanzanian miombo forests
- **Arxiv ID**: http://arxiv.org/abs/2106.15020v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15020v2)
- **Published**: 2021-06-21 15:05:35+00:00
- **Updated**: 2022-05-31 12:20:34+00:00
- **Authors**: Sara Björk, Stian Normann Anfinsen, Erik Næsset, Terje Gobakken, Eliakimu Zahabu
- **Comment**: None
- **Journal**: None
- **Summary**: This study derives regression models for above-ground biomass (AGB) estimation in miombo woodlands of Tanzania that utilise the high availability and low cost of Sentinel-1 data. The limited forest canopy penetration of C-band SAR sensors along with the sparseness of available ground truth restrict their usefulness in traditional AGB regression models. Therefore, we propose to use AGB predictions based on airborne laser scanning (ALS) data as a surrogate response variable for SAR data. This dramatically increases the available training data and opens for flexible regression models that capture fine-scale AGB dynamics. This becomes a sequential modelling approach, where the first regression stage has linked in situ data to ALS data and produced the AGB prediction map; We perform the subsequent stage, where this map is related to Sentinel-1 data. We develop a traditional, parametric regression model and alternative non-parametric models for this stage. The latter uses a conditional generative adversarial network (cGAN) to translate Sentinel-1 images into ALS-based AGB prediction maps. The convolution filters in the neural networks make them contextual. We compare the sequential models to traditional, non-sequential regression models, all trained on limited AGB ground reference data. Results show that our newly proposed non-sequential Sentinel-1-based regression model performs better quantitatively than the sequential models, but achieves less sensitivity to fine-scale AGB dynamics. The contextual cGAN-based sequential models best reproduce the distribution of ALS-based AGB predictions. They also reach a lower RMSE against in situ AGB data than the parametric sequential model, indicating a potential for further development.



### TNT: Text-Conditioned Network with Transductive Inference for Few-Shot Video Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.11173v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11173v2)
- **Published**: 2021-06-21 15:08:08+00:00
- **Updated**: 2021-12-15 14:18:25+00:00
- **Authors**: Andrés Villa, Juan-Manuel Perez-Rua, Victor Escorcia, Vladimir Araujo, Juan Carlos Niebles, Alvaro Soto
- **Comment**: 18 pages including references, 6 figures, and 3 tables
- **Journal**: None
- **Summary**: Recently, few-shot video classification has received an increasing interest. Current approaches mostly focus on effectively exploiting the temporal dimension in videos to improve learning under low data regimes. However, most works have largely ignored that videos are often accompanied by rich textual descriptions that can also be an essential source of information to handle few-shot recognition cases. In this paper, we propose to leverage these human-provided textual descriptions as privileged information when training a few-shot video classification model. Specifically, we formulate a text-based task conditioner to adapt video features to the few-shot learning task. Furthermore, our model follows a transductive setting to improve the task-adaptation ability of the model by using the support textual descriptions and query instances to update a set of class prototypes. Our model achieves state-of-the-art performance on four challenging benchmarks commonly used to evaluate few-shot video action classification models.



### Does Optimal Source Task Performance Imply Optimal Pre-training for a Target Task?
- **Arxiv ID**: http://arxiv.org/abs/2106.11174v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11174v2)
- **Published**: 2021-06-21 15:09:04+00:00
- **Updated**: 2022-04-12 16:44:47+00:00
- **Authors**: Steven Gutstein, Brent Lance, Sanjay Shakkottai
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-tuning of pre-trained deep nets is commonly used to improve accuracies and training times for neural nets. It is generally assumed that pre-training a net for optimal source task performance best prepares it for fine-tuning to learn an arbitrary target task. This is generally not true. Stopping source task training, prior to optimal performance, can create a pre-trained net better suited for fine-tuning to learn a new task. We perform several experiments demonstrating this effect, as well as the influence of the amount of training and of learning rate. Additionally, our results indicate that this reflects a general loss of learning ability that even extends to relearning the source task.



### Multi-level Feature Learning for Contrastive Multi-view Clustering
- **Arxiv ID**: http://arxiv.org/abs/2106.11193v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11193v2)
- **Published**: 2021-06-21 15:32:34+00:00
- **Updated**: 2022-03-25 06:50:48+00:00
- **Authors**: Jie Xu, Huayi Tang, Yazhou Ren, Liang Peng, Xiaofeng Zhu, Lifang He
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-view clustering can explore common semantics from multiple views and has attracted increasing attention. However, existing works punish multiple objectives in the same feature space, where they ignore the conflict between learning consistent common semantics and reconstructing inconsistent view-private information. In this paper, we propose a new framework of multi-level feature learning for contrastive multi-view clustering to address the aforementioned issue. Our method learns different levels of features from the raw features, including low-level features, high-level features, and semantic labels/features in a fusion-free manner, so that it can effectively achieve the reconstruction objective and the consistency objectives in different feature spaces. Specifically, the reconstruction objective is conducted on the low-level features. Two consistency objectives based on contrastive learning are conducted on the high-level features and the semantic labels, respectively. They make the high-level features effectively explore the common semantics and the semantic labels achieve the multi-view clustering. As a result, the proposed framework can reduce the adverse influence of view-private information. Extensive experiments on public datasets demonstrate that our method achieves state-of-the-art clustering effectiveness.



### Temporal Early Exits for Efficient Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.11208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11208v1)
- **Published**: 2021-06-21 15:49:46+00:00
- **Updated**: 2021-06-21 15:49:46+00:00
- **Authors**: Amin Sabet, Jonathon Hare, Bashir Al-Hashimi, Geoff V. Merrett
- **Comment**: None
- **Journal**: None
- **Summary**: Transferring image-based object detectors to the domain of video remains challenging under resource constraints. Previous efforts utilised optical flow to allow unchanged features to be propagated, however, the overhead is considerable when working with very slowly changing scenes from applications such as surveillance. In this paper, we propose temporal early exits to reduce the computational complexity of per-frame video object detection. Multiple temporal early exit modules with low computational overhead are inserted at early layers of the backbone network to identify the semantic differences between consecutive frames. Full computation is only required if the frame is identified as having a semantic change to previous frames; otherwise, detection results from previous frames are reused. Experiments on CDnet show that our method significantly reduces the computational complexity and execution of per-frame video object detection up to $34 \times$ compared to existing methods with an acceptable reduction of 2.2\% in mAP.



### Multi-VAE: Learning Disentangled View-common and View-peculiar Visual Representations for Multi-view Clustering
- **Arxiv ID**: http://arxiv.org/abs/2106.11232v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11232v2)
- **Published**: 2021-06-21 16:23:28+00:00
- **Updated**: 2021-07-07 14:29:15+00:00
- **Authors**: Jie Xu, Yazhou Ren, Huayi Tang, Xiaorong Pu, Xiaofeng Zhu, Ming Zeng, Lifang He
- **Comment**: Because some important information about the authors hasn't been
  confirmed, and our manuscript need to be improved and revised. The new
  version may need a long time to modified, so we decide to withdrew it
- **Journal**: None
- **Summary**: Multi-view clustering, a long-standing and important research problem, focuses on mining complementary information from diverse views. However, existing works often fuse multiple views' representations or handle clustering in a common feature space, which may result in their entanglement especially for visual representations. To address this issue, we present a novel VAE-based multi-view clustering framework (Multi-VAE) by learning disentangled visual representations. Concretely, we define a view-common variable and multiple view-peculiar variables in the generative model. The prior of view-common variable obeys approximately discrete Gumbel Softmax distribution, which is introduced to extract the common cluster factor of multiple views. Meanwhile, the prior of view-peculiar variable follows continuous Gaussian distribution, which is used to represent each view's peculiar visual factors. By controlling the mutual information capacity to disentangle the view-common and view-peculiar representations, continuous visual information of multiple views can be separated so that their common discrete cluster information can be effectively mined. Experimental results demonstrate that Multi-VAE enjoys the disentangled and explainable visual representations, while obtaining superior clustering performance compared with state-of-the-art methods.



### Can poachers find animals from public camera trap images?
- **Arxiv ID**: http://arxiv.org/abs/2106.11236v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, q-bio.PE
- **Links**: [PDF](http://arxiv.org/pdf/2106.11236v1)
- **Published**: 2021-06-21 16:31:47+00:00
- **Updated**: 2021-06-21 16:31:47+00:00
- **Authors**: Sara Beery, Elizabeth Bondi
- **Comment**: CV4Animals Workshop at CVPR 2021
- **Journal**: None
- **Summary**: To protect the location of camera trap data containing sensitive, high-target species, many ecologists randomly obfuscate the latitude and longitude of the camera when publishing their data. For example, they may publish a random location within a 1km radius of the true camera location for each camera in their network. In this paper, we investigate the robustness of geo-obfuscation for maintaining camera trap location privacy, and show via a case study that a few simple, intuitive heuristics and publicly available satellite rasters can be used to reduce the area likely to contain the camera by 87% (assuming random obfuscation within 1km), demonstrating that geo-obfuscation may be less effective than previously believed.



### 2D vs. 3D LiDAR-based Person Detection on Mobile Robots
- **Arxiv ID**: http://arxiv.org/abs/2106.11239v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11239v2)
- **Published**: 2021-06-21 16:35:49+00:00
- **Updated**: 2022-07-25 12:27:30+00:00
- **Authors**: Dan Jia, Alexander Hermans, Bastian Leibe
- **Comment**: Shortened version accepted at the International Conference on
  Intelligent Robots and Systems (IROS) 2022
- **Journal**: None
- **Summary**: Person detection is a crucial task for mobile robots navigating in human-populated environments. LiDAR sensors are promising for this task, thanks to their accurate depth measurements and large field of view. Two types of LiDAR sensors exist: the 2D LiDAR sensors, which scan a single plane, and the 3D LiDAR sensors, which scan multiple planes, thus forming a volume. How do they compare for the task of person detection? To answer this, we conduct a series of experiments, using the public, large-scale JackRabbot dataset and the state-of-the-art 2D and 3D LiDAR-based person detectors (DR-SPAAM and CenterPoint respectively). Our experiments include multiple aspects, ranging from the basic performance and speed comparison, to more detailed analysis on localization accuracy and robustness against distance and scene clutter. The insights from these experiments highlight the strengths and weaknesses of 2D and 3D LiDAR sensors as sources for person detection, and are especially valuable for designing mobile robots that will operate in close proximity to surrounding humans (e.g. service or social robot).



### VIMPAC: Video Pre-Training via Masked Token Prediction and Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.11250v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11250v1)
- **Published**: 2021-06-21 16:48:19+00:00
- **Updated**: 2021-06-21 16:48:19+00:00
- **Authors**: Hao Tan, Jie Lei, Thomas Wolf, Mohit Bansal
- **Comment**: Under review, 23 Pages
- **Journal**: None
- **Summary**: Video understanding relies on perceiving the global content and modeling its internal connections (e.g., causality, movement, and spatio-temporal correspondence). To learn these interactions, we apply a mask-then-predict pre-training task on discretized video tokens generated via VQ-VAE. Unlike language, where the text tokens are more independent, neighboring video tokens typically have strong correlations (e.g., consecutive video frames usually look very similar), and hence uniformly masking individual tokens will make the task too trivial to learn useful representations. To deal with this issue, we propose a block-wise masking strategy where we mask neighboring video tokens in both spatial and temporal domains. We also add an augmentation-free contrastive learning method to further capture the global content by predicting whether the video clips are sampled from the same video. We pre-train our model on uncurated videos and show that our pre-trained model can reach state-of-the-art results on several video understanding datasets (e.g., SSV2, Diving48). Lastly, we provide detailed analyses on model scalability and pre-training method design. Code is released at https://github.com/airsplay/vimpac.



### Applying VertexShuffle Toward 360-Degree Video Super-Resolution on Focused-Icosahedral-Mesh
- **Arxiv ID**: http://arxiv.org/abs/2106.11253v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.11253v1)
- **Published**: 2021-06-21 16:53:57+00:00
- **Updated**: 2021-06-21 16:53:57+00:00
- **Authors**: Na Li, Yao Liu
- **Comment**: This paper introduce a new mesh representation and a new upsampling
  method on a mesh
- **Journal**: None
- **Summary**: With the emerging of 360-degree image/video, augmented reality (AR) and virtual reality (VR), the demand for analysing and processing spherical signals get tremendous increase. However, plenty of effort paid on planar signals that projected from spherical signals, which leading to some problems, e.g. waste of pixels, distortion. Recent advances in spherical CNN have opened up the possibility of directly analysing spherical signals. However, they pay attention to the full mesh which makes it infeasible to deal with situations in real-world application due to the extremely large bandwidth requirement. To address the bandwidth waste problem associated with 360-degree video streaming and save computation, we exploit Focused Icosahedral Mesh to represent a small area and construct matrices to rotate spherical content to the focused mesh area. We also proposed a novel VertexShuffle operation that can significantly improve both the performance and the efficiency compared to the original MeshConv Transpose operation introduced in UGSCNN. We further apply our proposed methods on super resolution model, which is the first to propose a spherical super-resolution model that directly operates on a mesh representation of spherical pixels of 360-degree data. To evaluate our model, we also collect a set of high-resolution 360-degree videos to generate a spherical image dataset. Our experiments indicate that our proposed spherical super-resolution model achieves significant benefits in terms of both performance and inference time compared to the baseline spherical super-resolution model that uses the simple MeshConv Transpose operation. In summary, our model achieves great super-resolution performance on 360-degree inputs, achieving 32.79 dB PSNR on average when super-resoluting 16x vertices on the mesh.



### Neural Marching Cubes
- **Arxiv ID**: http://arxiv.org/abs/2106.11272v3
- **DOI**: 10.1145/3478513.3480518
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11272v3)
- **Published**: 2021-06-21 17:18:52+00:00
- **Updated**: 2021-09-03 19:46:18+00:00
- **Authors**: Zhiqin Chen, Hao Zhang
- **Comment**: Accepted to SIGGRAPH Asia 2021. Code:
  https://github.com/czq142857/NMC, supplementary material:
  https://www.sfu.ca/~zhiqinc/nmc/NMC_supplementary.zip
- **Journal**: ACM Trans. Graph. 40, 6, Article 251 (December 2021), 15 pages
- **Summary**: We introduce Neural Marching Cubes (NMC), a data-driven approach for extracting a triangle mesh from a discretized implicit field. Classical MC is defined by coarse tessellation templates isolated to individual cubes. While more refined tessellations have been proposed, they all make heuristic assumptions, such as trilinearity, when determining the vertex positions and local mesh topologies in each cube. In principle, none of these approaches can reconstruct geometric features that reveal coherence or dependencies between nearby cubes (e.g., a sharp edge), as such information is unaccounted for, resulting in poor estimates of the true underlying implicit field. To tackle these challenges, we re-cast MC from a deep learning perspective, by designing tessellation templates more apt at preserving geometric features, and learning the vertex positions and mesh topologies from training meshes, to account for contextual information from nearby cubes. We develop a compact per-cube parameterization to represent the output triangle mesh, while being compatible with neural processing, so that a simple 3D convolutional network can be employed for the training. We show that all topological cases in each cube that are applicable to our design can be easily derived using our representation, and the resulting tessellations can also be obtained naturally and efficiently by following a few design guidelines. In addition, our network learns local features with limited receptive fields, hence it generalizes well to new shapes and new datasets. We evaluate our neural MC approach by quantitative and qualitative comparisons to all well-known MC variants. In particular, we demonstrate the ability of our network to recover sharp features such as edges and corners, a long-standing issue of MC and its variants. Our network also reconstructs local mesh topologies more accurately than previous approaches.



### Attention-based Neural Network for Driving Environment Complexity Perception
- **Arxiv ID**: http://arxiv.org/abs/2106.11277v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11277v1)
- **Published**: 2021-06-21 17:27:11+00:00
- **Updated**: 2021-06-21 17:27:11+00:00
- **Authors**: Ce Zhang, Azim Eskandarian, Xuelai Du
- **Comment**: Accepted by 2021 IEEE Intelligent Transportation Systems Conference
- **Journal**: None
- **Summary**: Environment perception is crucial for autonomous vehicle (AV) safety. Most existing AV perception algorithms have not studied the surrounding environment complexity and failed to include the environment complexity parameter. This paper proposes a novel attention-based neural network model to predict the complexity level of the surrounding driving environment. The proposed model takes naturalistic driving videos and corresponding vehicle dynamics parameters as input. It consists of a Yolo-v3 object detection algorithm, a heat map generation algorithm, CNN-based feature extractors, and attention-based feature extractors for both video and time-series vehicle dynamics data inputs to extract features. The output from the proposed algorithm is a surrounding environment complexity parameter. The Berkeley DeepDrive dataset (BDD Dataset) and subjectively labeled surrounding environment complexity levels are used for model training and validation to evaluate the algorithm. The proposed attention-based network achieves 91.22% average classification accuracy to classify the surrounding environment complexity. It proves that the environment complexity level can be accurately predicted and applied for future AVs' environment perception studies.



### The Arm-Swing Is Discriminative in Video Gait Recognition for Athlete Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2106.11280v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11280v1)
- **Published**: 2021-06-21 17:28:07+00:00
- **Updated**: 2021-06-21 17:28:07+00:00
- **Authors**: Yapkan Choi, Yeshwanth Napolean, Jan C. van Gemert
- **Comment**: ICIP 2021
- **Journal**: None
- **Summary**: In this paper we evaluate running gait as an attribute for video person re-identification in a long-distance running event. We show that running gait recognition achieves competitive performance compared to appearance-based approaches in the cross-camera retrieval task and that gait and appearance features are complementary to each other. For gait, the arm swing during running is less distinguishable when using binary gait silhouettes, due to ambiguity in the torso region. We propose to use human semantic parsing to create partial gait silhouettes where the torso is left out. Leaving out the torso improves recognition results by allowing the arm swing to be more visible in the frontal and oblique viewing angles, which offers hints that arm swings are somewhat personal. Experiments show an increase of 3.2% mAP on the CampusRun and increased accuracy with 4.8% in the frontal and rear view on CASIA-B, compared to using the full body silhouettes.



### TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?
- **Arxiv ID**: http://arxiv.org/abs/2106.11297v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11297v4)
- **Published**: 2021-06-21 17:55:59+00:00
- **Updated**: 2022-04-03 15:42:57+00:00
- **Authors**: Michael S. Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, Anelia Angelova
- **Comment**: This is the full version of the paper, extending its conference paper
  at NeurIPS 2021. Version 1.1 of the code is released
- **Journal**: NeurIPS 2021
- **Summary**: In this paper, we introduce a novel visual representation learning which relies on a handful of adaptively learned tokens, and which is applicable to both image and video understanding tasks. Instead of relying on hand-designed splitting strategies to obtain visual tokens and processing a large number of densely sampled patches for attention, our approach learns to mine important tokens in visual data. This results in efficiently and effectively finding a few important visual tokens and enables modeling of pairwise attention between such tokens, over a longer temporal horizon for videos, or the spatial content in images. Our experiments demonstrate strong performance on several challenging benchmarks for both image and video recognition tasks. Importantly, due to our tokens being adaptive, we accomplish competitive results at significantly reduced compute amount. We obtain comparable results to the state-of-the-arts on ImageNet while being computationally more efficient. We also confirm the effectiveness of the approach on multiple video datasets, including Kinetics-400, Kinetics-600, Charades, and AViD.   The code is available at: https://github.com/google-research/scenic/tree/main/scenic/projects/token_learner



### Understanding Object Dynamics for Interactive Image-to-Video Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2106.11303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11303v1)
- **Published**: 2021-06-21 17:57:39+00:00
- **Updated**: 2021-06-21 17:57:39+00:00
- **Authors**: Andreas Blattmann, Timo Milbich, Michael Dorkenwald, Björn Ommer
- **Comment**: CVPR 2021, project page available at https://bit.ly/3cxfA2L
- **Journal**: None
- **Summary**: What would be the effect of locally poking a static scene? We present an approach that learns naturally-looking global articulations caused by a local manipulation at a pixel level. Training requires only videos of moving objects but no information of the underlying manipulation of the physical scene. Our generative model learns to infer natural object dynamics as a response to user interaction and learns about the interrelations between different object body regions. Given a static image of an object and a local poking of a pixel, the approach then predicts how the object would deform over time. In contrast to existing work on video prediction, we do not synthesize arbitrary realistic videos but enable local interactive control of the deformation. Our model is not restricted to particular object categories and can transfer dynamics onto novel unseen object instances. Extensive experiments on diverse objects demonstrate the effectiveness of our approach compared to common video prediction frameworks. Project page is available at https://bit.ly/3cxfA2L .



### Simple Distillation Baselines for Improving Small Self-supervised Models
- **Arxiv ID**: http://arxiv.org/abs/2106.11304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11304v1)
- **Published**: 2021-06-21 17:58:05+00:00
- **Updated**: 2021-06-21 17:58:05+00:00
- **Authors**: Jindong Gu, Wei Liu, Yonglong Tian
- **Comment**: None
- **Journal**: None
- **Summary**: While large self-supervised models have rivalled the performance of their supervised counterparts, small models still struggle. In this report, we explore simple baselines for improving small self-supervised models via distillation, called SimDis. Specifically, we present an offline-distillation baseline, which establishes a new state-of-the-art, and an online-distillation baseline, which achieves similar performance with minimal computational overhead. We hope these baselines will provide useful experience for relevant future research. Code is available at: https://github.com/JindongGu/SimDis/



### Fast Simultaneous Gravitational Alignment of Multiple Point Sets
- **Arxiv ID**: http://arxiv.org/abs/2106.11308v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11308v1)
- **Published**: 2021-06-21 17:59:40+00:00
- **Updated**: 2021-06-21 17:59:40+00:00
- **Authors**: Vladislav Golyanik, Soshi Shimada, Christian Theobalt
- **Comment**: Project webpage: http://gvv.mpi-inf.mpg.de/projects/MBGA/
- **Journal**: 3DV 2020
- **Summary**: The problem of simultaneous rigid alignment of multiple unordered point sets which is unbiased towards any of the inputs has recently attracted increasing interest, and several reliable methods have been newly proposed. While being remarkably robust towards noise and clustered outliers, current approaches require sophisticated initialisation schemes and do not scale well to large point sets. This paper proposes a new resilient technique for simultaneous registration of multiple point sets by interpreting the latter as particle swarms rigidly moving in the mutually induced force fields. Thanks to the improved simulation with altered physical laws and acceleration of globally multiply-linked point interactions with a 2^D-tree (D is the space dimensionality), our Multi-Body Gravitational Approach (MBGA) is robust to noise and missing data while supporting more massive point sets than previous methods (with 10^5 points and more). In various experimental settings, MBGA is shown to outperform several baseline point set alignment approaches in terms of accuracy and runtime. We make our source code available for the community to facilitate the reproducibility of the results.



### How Do Adam and Training Strategies Help BNNs Optimization?
- **Arxiv ID**: http://arxiv.org/abs/2106.11309v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11309v1)
- **Published**: 2021-06-21 17:59:51+00:00
- **Updated**: 2021-06-21 17:59:51+00:00
- **Authors**: Zechun Liu, Zhiqiang Shen, Shichao Li, Koen Helwegen, Dong Huang, Kwang-Ting Cheng
- **Comment**: ICML 2021. Code and models are available at
  https://github.com/liuzechun/AdamBNN
- **Journal**: None
- **Summary**: The best performing Binary Neural Networks (BNNs) are usually attained using Adam optimization and its multi-step training variants. However, to the best of our knowledge, few studies explore the fundamental reasons why Adam is superior to other optimizers like SGD for BNN optimization or provide analytical explanations that support specific training strategies. To address this, in this paper we first investigate the trajectories of gradients and weights in BNNs during the training process. We show the regularization effect of second-order momentum in Adam is crucial to revitalize the weights that are dead due to the activation saturation in BNNs. We find that Adam, through its adaptive learning rate strategy, is better equipped to handle the rugged loss surface of BNNs and reaches a better optimum with higher generalization ability. Furthermore, we inspect the intriguing role of the real-valued weights in binary networks, and reveal the effect of weight decay on the stability and sluggishness of BNN optimization. Through extensive experiments and analysis, we derive a simple training scheme, building on existing Adam-based optimization, which achieves 70.5% top-1 accuracy on the ImageNet dataset using the same architecture as the state-of-the-art ReActNet while achieving 1.1% higher accuracy. Code and models are available at https://github.com/liuzechun/AdamBNN.



### Towards Long-Form Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/2106.11310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11310v1)
- **Published**: 2021-06-21 17:59:52+00:00
- **Updated**: 2021-06-21 17:59:52+00:00
- **Authors**: Chao-Yuan Wu, Philipp Krähenbühl
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Our world offers a never-ending stream of visual stimuli, yet today's vision systems only accurately recognize patterns within a few seconds. These systems understand the present, but fail to contextualize it in past or future events. In this paper, we study long-form video understanding. We introduce a framework for modeling long-form videos and develop evaluation protocols on large-scale datasets. We show that existing state-of-the-art short-term models are limited for long-form tasks. A novel object-centric transformer-based video recognition architecture performs significantly better on 7 diverse tasks. It also outperforms comparable state-of-the-art on the AVA dataset.



### Image simulation for space applications with the SurRender software
- **Arxiv ID**: http://arxiv.org/abs/2106.11322v1
- **DOI**: None
- **Categories**: **astro-ph.EP**, astro-ph.IM, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11322v1)
- **Published**: 2021-06-21 18:00:01+00:00
- **Updated**: 2021-06-21 18:00:01+00:00
- **Authors**: Jérémy Lebreton, Roland Brochard, Matthieu Baudry, Grégory Jonniaux, Adrien Hadj Salah, Keyvan Kanani, Matthieu Le Goff, Aurore Masson, Nicolas Ollagnier, Paolo Panicucci, Amsha Proag, Cyril Robin
- **Comment**: 11th International ESA Conference on Guidance, Navigation & Control
  Systems, 22 - 25 June 2021 16 pages, 8 figures
- **Journal**: None
- **Summary**: Image Processing algorithms for vision-based navigation require reliable image simulation capacities. In this paper we explain why traditional rendering engines may present limitations that are potentially critical for space applications. We introduce Airbus SurRender software v7 and provide details on features that make it a very powerful space image simulator. We show how SurRender is at the heart of the development processes of our computer vision solutions and we provide a series of illustrations of rendered images for various use cases ranging from Moon and Solar System exploration, to in orbit rendezvous and planetary robotics.



### Context-aware PolyUNet for Liver and Lesion Segmentation from Abdominal CT Images
- **Arxiv ID**: http://arxiv.org/abs/2106.11330v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11330v1)
- **Published**: 2021-06-21 18:01:04+00:00
- **Updated**: 2021-06-21 18:01:04+00:00
- **Authors**: Liping Zhang, Simon Chun-Ho Yu
- **Comment**: 7 pages and 3 figures
- **Journal**: None
- **Summary**: Accurate liver and lesion segmentation from computed tomography (CT) images are highly demanded in clinical practice for assisting the diagnosis and assessment of hepatic tumor disease. However, automatic liver and lesion segmentation from contrast-enhanced CT volumes is extremely challenging due to the diversity in contrast, resolution, and quality of images. Previous methods based on UNet for 2D slice-by-slice or 3D volume-by-volume segmentation either lack sufficient spatial contexts or suffer from high GPU computational cost, which limits the performance. To tackle these issues, we propose a novel context-aware PolyUNet for accurate liver and lesion segmentation. It jointly explores structural diversity and consecutive t-adjacent slices to enrich feature expressive power and spatial contextual information while avoiding the overload of GPU memory consumption. In addition, we utilize zoom out/in and two-stage refinement strategy to exclude the irrelevant contexts and focus on the specific region for the fine-grained segmentation. Our method achieved very competitive performance at the MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge among all tasks with a single model and ranked the $3^{rd}$, $12^{th}$, $2^{nd}$, and $5^{th}$ places in the liver segmentation, lesion segmentation, lesion detection, and tumor burden estimation, respectively.



### Dive into Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.11342v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11342v5)
- **Published**: 2021-06-21 18:19:46+00:00
- **Updated**: 2023-08-22 17:02:42+00:00
- **Authors**: Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola
- **Comment**: (HTML) https://D2L.ai (GitHub) https://github.com/d2l-ai/d2l-en/
- **Journal**: None
- **Summary**: This open-source book represents our attempt to make deep learning approachable, teaching readers the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code. Our goal is to offer a resource that could (i) be freely available for everyone; (ii) offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist; (iii) include runnable code, showing readers how to solve problems in practice; (iv) allow for rapid updates, both by us and also by the community at large; (v) be complemented by a forum for interactive discussion of technical details and to answer questions.



### f-Domain-Adversarial Learning: Theory and Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2106.11344v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11344v1)
- **Published**: 2021-06-21 18:21:09+00:00
- **Updated**: 2021-06-21 18:21:09+00:00
- **Authors**: David Acuna, Guojun Zhang, Marc T. Law, Sanja Fidler
- **Comment**: ICML 2021
- **Journal**: None
- **Summary**: Unsupervised domain adaptation is used in many machine learning applications where, during training, a model has access to unlabeled data in the target domain, and a related labeled dataset. In this paper, we introduce a novel and general domain-adversarial framework. Specifically, we derive a novel generalization bound for domain adaptation that exploits a new measure of discrepancy between distributions based on a variational characterization of f-divergences. It recovers the theoretical results from Ben-David et al. (2010a) as a special case and supports divergences used in practice. Based on this bound, we derive a new algorithmic framework that introduces a key correction in the original adversarial training method of Ganin et al. (2016). We show that many regularizers and ad-hoc objectives introduced over the last years in this framework are then not required to achieve performance comparable to (if not better than) state-of-the-art domain-adversarial methods. Experimental analysis conducted on real-world natural language and computer vision datasets show that our framework outperforms existing baselines, and obtains the best results for f-divergences that were not considered previously in domain-adversarial learning.



### GAIA: A Transfer Learning System of Object Detection that Fits Your Needs
- **Arxiv ID**: http://arxiv.org/abs/2106.11346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11346v1)
- **Published**: 2021-06-21 18:24:20+00:00
- **Updated**: 2021-06-21 18:24:20+00:00
- **Authors**: Xingyuan Bu, Junran Peng, Junjie Yan, Tieniu Tan, Zhaoxiang Zhang
- **Comment**: CVPR2021. The first two authors contribute equally. Code is released
  at https://github.com/GAIA-vision
- **Journal**: None
- **Summary**: Transfer learning with pre-training on large-scale datasets has played an increasingly significant role in computer vision and natural language processing recently. However, as there exist numerous application scenarios that have distinctive demands such as certain latency constraints and specialized data distributions, it is prohibitively expensive to take advantage of large-scale pre-training for per-task requirements. In this paper, we focus on the area of object detection and present a transfer learning system named GAIA, which could automatically and efficiently give birth to customized solutions according to heterogeneous downstream needs. GAIA is capable of providing powerful pre-trained weights, selecting models that conform to downstream demands such as latency constraints and specified data domains, and collecting relevant data for practitioners who have very few datapoints for their tasks. With GAIA, we achieve promising results on COCO, Objects365, Open Images, Caltech, CityPersons, and UODB which is a collection of datasets including KITTI, VOC, WiderFace, DOTA, Clipart, Comic, and more. Taking COCO as an example, GAIA is able to efficiently produce models covering a wide range of latency from 16ms to 53ms, and yields AP from 38.2 to 46.5 without whistles and bells. To benefit every practitioner in the community of object detection, GAIA is released at https://github.com/GAIA-vision.



### FDeblur-GAN: Fingerprint Deblurring using Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2106.11354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11354v1)
- **Published**: 2021-06-21 18:37:20+00:00
- **Updated**: 2021-06-21 18:37:20+00:00
- **Authors**: Amol S. Joshi, Ali Dabouei, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: 8 Pages, Accepted in IJCB Conference
- **Journal**: None
- **Summary**: While working with fingerprint images acquired from crime scenes, mobile cameras, or low-quality sensors, it becomes difficult for automated identification systems to verify the identity due to image blur and distortion. We propose a fingerprint deblurring model FDeblur-GAN, based on the conditional Generative Adversarial Networks (cGANs) and multi-stage framework of the stack GAN. Additionally, we integrate two auxiliary sub-networks into the model for the deblurring task. The first sub-network is a ridge extractor model. It is added to generate ridge maps to ensure that fingerprint information and minutiae are preserved in the deblurring process and prevent the model from generating erroneous minutiae. The second sub-network is a verifier that helps the generator to preserve the ID information during the generation process. Using a database of blurred fingerprints and corresponding ridge maps, the deep network learns to deblur from the input blurry samples. We evaluate the proposed method in combination with two different fingerprint matching algorithms. We achieved an accuracy of 95.18% on our fingerprint database for the task of matching deblurred and ground truth fingerprints.



### Photozilla: A Large-Scale Photography Dataset and Visual Embedding for 20 Photography Styles
- **Arxiv ID**: http://arxiv.org/abs/2106.11359v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11359v1)
- **Published**: 2021-06-21 18:45:06+00:00
- **Updated**: 2021-06-21 18:45:06+00:00
- **Authors**: Trisha Singhal, Junhua Liu, Lucienne T. M. Blessing, Kwan Hui Lim
- **Comment**: In the Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops, 2021. (Poster)
- **Journal**: None
- **Summary**: The advent of social media platforms has been a catalyst for the development of digital photography that engendered a boom in vision applications. With this motivation, we introduce a large-scale dataset termed 'Photozilla', which includes over 990k images belonging to 10 different photographic styles. The dataset is then used to train 3 classification models to automatically classify the images into the relevant style which resulted in an accuracy of ~96%. With the rapid evolution of digital photography, we have seen new types of photography styles emerging at an exponential rate. On that account, we present a novel Siamese-based network that uses the trained classification models as the base architecture to adapt and classify unseen styles with only 25 training samples. We report an accuracy of over 68% for identifying 10 other distinct types of photography styles. This dataset can be found at https://trisha025.github.io/Photozilla/



### Boggart: Towards General-Purpose Acceleration of Retrospective Video Analytics
- **Arxiv ID**: http://arxiv.org/abs/2106.15315v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2106.15315v2)
- **Published**: 2021-06-21 19:21:16+00:00
- **Updated**: 2022-05-01 19:59:03+00:00
- **Authors**: Neil Agarwal, Ravi Netravali
- **Comment**: None
- **Journal**: None
- **Summary**: Commercial retrospective video analytics platforms have increasingly adopted general interfaces to support the custom queries and convolutional neural networks (CNNs) that different applications require. However, existing optimizations were designed for settings where CNNs were platform- (not user-) determined, and fail to meet at least one of the following key platform goals when that condition is violated: reliable accuracy, low latency, and minimal wasted work.   We present Boggart, a system that simultaneously meets all three goals while supporting the generality that today's platforms seek. Prior to queries being issued, Boggart carefully employs traditional computer vision algorithms to generate indices that are imprecise, but are fundamentally comprehensive across different CNNs/queries. For each issued query, Boggart employs new techniques to quickly characterize the imprecision of its index, and sparingly run CNNs (and propagate the results to other frames) in a way that bounds accuracy drops. Our results highlight that Boggart's improved generality comes at low cost, with speedups that match (and most often, exceed) prior, model-specific approaches.



### BEyond observation: an approach for ObjectNav
- **Arxiv ID**: http://arxiv.org/abs/2106.11379v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.11379v1)
- **Published**: 2021-06-21 19:27:16+00:00
- **Updated**: 2021-06-21 19:27:16+00:00
- **Authors**: Daniel V. Ruiz, Eduardo Todt
- **Comment**: Presented at the 2th Embodied AI Workshop at CVPR 2021
- **Journal**: None
- **Summary**: With the rise of automation, unmanned vehicles became a hot topic both as commercial products and as a scientific research topic. It composes a multi-disciplinary field of robotics that encompasses embedded systems, control theory, path planning, Simultaneous Localization and Mapping (SLAM), scene reconstruction, and pattern recognition. In this work, we present our exploratory research of how sensor data fusion and state-of-the-art machine learning algorithms can perform the Embodied Artificial Intelligence (E-AI) task called Visual Semantic Navigation. This task, a.k.a Object-Goal Navigation (ObjectNav) consists of autonomous navigation using egocentric visual observations to reach an object belonging to the target semantic class without prior knowledge of the environment. Our method reached fourth place on the Habitat Challenge 2021 ObjectNav on the Minival phase and the Test-Standard Phase.



### Mapping Slums with Medium Resolution Satellite Imagery: a Comparative Analysis of Multi-Spectral Data and Grey-level Co-occurrence Matrix Techniques
- **Arxiv ID**: http://arxiv.org/abs/2106.11395v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2106.11395v1)
- **Published**: 2021-06-21 20:11:27+00:00
- **Updated**: 2021-06-21 20:11:27+00:00
- **Authors**: Agatha C. H. de Mattos, Gavin McArdle, Michela Bertolotto
- **Comment**: Accepted at the 3rd Workshop on Artificial Intelligence for Social
  Good (IJCAI 2021)
- **Journal**: None
- **Summary**: The UN-Habitat estimates that over one billion people live in slums around the world. However, state-of-the-art techniques to detect the location of slum areas employ high-resolution satellite imagery, which is costly to obtain and process. As a result, researchers have started to look at utilising free and open-access medium resolution satellite imagery. Yet, there is no clear consensus on which data preparation and machine learning approaches are the most appropriate to use with such imagery data. In this paper, we evaluate two techniques (multi-spectral data and grey-level co-occurrence matrix feature extraction) on an open-access dataset consisting of labelled Sentinel-2 images with a spatial resolution of 10 meters. Both techniques were paired with a canonical correlation forests classifier. The results show that the grey-level co-occurrence matrix performed better than multi-spectral data for all four cities. It had an average accuracy for the slum class of 97% and a mean intersection over union of 94%, while multi-spectral data had 75% and 64% for the respective metrics. These results indicate that open-access satellite imagery with a resolution of at least 10 meters may be suitable for keeping track of development goals such as the detection of slums in cities.



### BiAdam: Fast Adaptive Bilevel Optimization Methods
- **Arxiv ID**: http://arxiv.org/abs/2106.11396v4
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11396v4)
- **Published**: 2021-06-21 20:16:40+00:00
- **Updated**: 2023-02-18 22:36:36+00:00
- **Authors**: Feihu Huang, Junyi Li, Shangqian Gao
- **Comment**: 25 pages, 1 table and 7 figures
- **Journal**: None
- **Summary**: Bilevel optimization recently has attracted increased interest in machine learning due to its many applications such as hyper-parameter optimization and meta learning. Although many bilevel methods recently have been proposed, these methods do not consider using adaptive learning rates. It is well known that adaptive learning rates can accelerate optimization algorithms. To fill this gap, in the paper, we propose a novel fast adaptive bilevel framework to solve stochastic bilevel optimization problems that the outer problem is possibly nonconvex and the inner problem is strongly convex. Our framework uses unified adaptive matrices including many types of adaptive learning rates, and can flexibly use the momentum and variance reduced techniques. In particular, we provide a useful convergence analysis framework for the bilevel optimization. Specifically, we propose a fast single-loop adaptive bilevel optimization (BiAdam) algorithm, which achieves a sample complexity of $\tilde{O}(\epsilon^{-4})$ for finding an $\epsilon$-stationary solution. Meanwhile, we propose an accelerated version of BiAdam algorithm (VR-BiAdam), which reaches the best known sample complexity of $\tilde{O}(\epsilon^{-3})$. To the best of our knowledge, we first study the adaptive bilevel optimization methods with adaptive learning rates. Experimental results on data hyper-cleaning and hyper-representation learning tasks demonstrate the efficiency of our algorithms.



### Spatio-Temporal Multi-Task Learning Transformer for Joint Moving Object Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.11401v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11401v1)
- **Published**: 2021-06-21 20:30:44+00:00
- **Updated**: 2021-06-21 20:30:44+00:00
- **Authors**: Eslam Mohamed, Ahmed El-Sallab
- **Comment**: None
- **Journal**: None
- **Summary**: Moving objects have special importance for Autonomous Driving tasks. Detecting moving objects can be posed as Moving Object Segmentation, by segmenting the object pixels, or Moving Object Detection, by generating a bounding box for the moving targets. In this paper, we present a Multi-Task Learning architecture, based on Transformers, to jointly perform both tasks through one network. Due to the importance of the motion features to the task, the whole setup is based on a Spatio-Temporal aggregation. We evaluate the performance of the individual tasks architecture versus the MTL setup, both with early shared encoders, and late shared encoder-decoder transformers. For the latter, we present a novel joint tasks query decoder transformer, that enables us to have tasks dedicated heads out of the shared model. To evaluate our approach, we use the KITTI MOD [29] data set. Results show1.5% mAP improvement for Moving Object Detection, and 2%IoU improvement for Moving Object Segmentation, over the individual tasks networks.



### MODETR: Moving Object Detection with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2106.11422v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11422v1)
- **Published**: 2021-06-21 21:56:46+00:00
- **Updated**: 2021-06-21 21:56:46+00:00
- **Authors**: Eslam Mohamed, Ahmad El-Sallab
- **Comment**: None
- **Journal**: Machine Learning for Autonomous Driving Workshop at the 34th
  Conference on Neural Information Processing Systems (NeurIPS 2020),
  Vancouver, Canada
- **Summary**: Moving Object Detection (MOD) is a crucial task for the Autonomous Driving pipeline. MOD is usually handled via 2-stream convolutional architectures that incorporates both appearance and motion cues, without considering the inter-relations between the spatial or motion features. In this paper, we tackle this problem through multi-head attention mechanisms, both across the spatial and motion streams. We propose MODETR; a Moving Object DEtection TRansformer network, comprised of multi-stream transformer encoders for both spatial and motion modalities, and an object transformer decoder that produces the moving objects bounding boxes using set predictions. The whole architecture is trained end-to-end using bi-partite loss. Several methods of incorporating motion cues with the Transformer model are explored, including two-stream RGB and Optical Flow (OF) methods, and multi-stream architectures that take advantage of sequence information. To incorporate the temporal information, we propose a new Temporal Positional Encoding (TPE) approach to extend the Spatial Positional Encoding(SPE) in DETR. We explore two architectural choices for that, balancing between speed and time. To evaluate the our network, we perform the MOD task on the KITTI MOD [6] data set. Results show significant 5% mAP of the Transformer network for MOD over the state-of-the art methods. Moreover, the proposed TPE encoding provides 10% mAP improvement over the SPE baseline.



### Normalized Avatar Synthesis Using StyleGAN and Perceptual Refinement
- **Arxiv ID**: http://arxiv.org/abs/2106.11423v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2106.11423v1)
- **Published**: 2021-06-21 21:57:16+00:00
- **Updated**: 2021-06-21 21:57:16+00:00
- **Authors**: Huiwen Luo, Koki Nagano, Han-Wei Kung, Mclean Goldwhite, Qingguo Xu, Zejian Wang, Lingyu Wei, Liwen Hu, Hao Li
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: We introduce a highly robust GAN-based framework for digitizing a normalized 3D avatar of a person from a single unconstrained photo. While the input image can be of a smiling person or taken in extreme lighting conditions, our method can reliably produce a high-quality textured model of a person's face in neutral expression and skin textures under diffuse lighting condition. Cutting-edge 3D face reconstruction methods use non-linear morphable face models combined with GAN-based decoders to capture the likeness and details of a person but fail to produce neutral head models with unshaded albedo textures which is critical for creating relightable and animation-friendly avatars for integration in virtual environments. The key challenges for existing methods to work is the lack of training and ground truth data containing normalized 3D faces. We propose a two-stage approach to address this problem. First, we adopt a highly robust normalized 3D face generator by embedding a non-linear morphable face model into a StyleGAN2 network. This allows us to generate detailed but normalized facial assets. This inference is then followed by a perceptual refinement step that uses the generated assets as regularization to cope with the limited available training samples of normalized faces. We further introduce a Normalized Face Dataset, which consists of a combination photogrammetry scans, carefully selected photographs, and generated fake people with neutral expressions in diffuse lighting conditions. While our prepared dataset contains two orders of magnitude less subjects than cutting edge GAN-based 3D facial reconstruction methods, we show that it is possible to produce high-quality normalized face models for very challenging unconstrained input images, and demonstrate superior performance to the current state-of-the-art.



### Incremental Deep Neural Network Learning using Classification Confidence Thresholding
- **Arxiv ID**: http://arxiv.org/abs/2106.11437v1
- **DOI**: 10.1109/TNNLS.2021.3087104
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11437v1)
- **Published**: 2021-06-21 22:46:28+00:00
- **Updated**: 2021-06-21 22:46:28+00:00
- **Authors**: Justin Leo, Jugal Kalita
- **Comment**: Accepted to IEEE TNNLS
- **Journal**: TNNLS 33 (2022) 7706-7716
- **Summary**: Most modern neural networks for classification fail to take into account the concept of the unknown. Trained neural networks are usually tested in an unrealistic scenario with only examples from a closed set of known classes. In an attempt to develop a more realistic model, the concept of working in an open set environment has been introduced. This in turn leads to the concept of incremental learning where a model with its own architecture and initial trained set of data can identify unknown classes during the testing phase and autonomously update itself if evidence of a new class is detected. Some problems that arise in incremental learning are inefficient use of resources to retrain the classifier repeatedly and the decrease of classification accuracy as multiple classes are added over time. This process of instantiating new classes is repeated as many times as necessary, accruing errors. To address these problems, this paper proposes the Classification Confidence Threshold approach to prime neural networks for incremental learning to keep accuracies high by limiting forgetting. A lean method is also used to reduce resources used in the retraining of the neural network. The proposed method is based on the idea that a network is able to incrementally learn a new class even when exposed to a limited number samples associated with the new class. This method can be applied to most existing neural networks with minimal changes to network architecture.



### Encoder-Decoder Architectures for Clinically Relevant Coronary Artery Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.11447v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11447v1)
- **Published**: 2021-06-21 23:32:11+00:00
- **Updated**: 2021-06-21 23:32:11+00:00
- **Authors**: João Lourenço Silva, Miguel Nobre Menezes, Tiago Rodrigues, Beatriz Silva, Fausto J. Pinto, Arlindo L. Oliveira
- **Comment**: None
- **Journal**: None
- **Summary**: Coronary X-ray angiography is a crucial clinical procedure for the diagnosis and treatment of coronary artery disease, which accounts for roughly 16% of global deaths every year. However, the images acquired in these procedures have low resolution and poor contrast, making lesion detection and assessment challenging. Accurate coronary artery segmentation not only helps mitigate these problems, but also allows the extraction of relevant anatomical features for further analysis by quantitative methods. Although automated segmentation of coronary arteries has been proposed before, previous approaches have used non-optimal segmentation criteria, leading to less useful results. Most methods either segment only the major vessel, discarding important information from the remaining ones, or segment the whole coronary tree based mostly on contrast information, producing a noisy output that includes vessels that are not relevant for diagnosis. We adopt a better-suited clinical criterion and segment vessels according to their clinical relevance. Additionally, we simultaneously perform catheter segmentation, which may be useful for diagnosis due to the scale factor provided by the catheter's known diameter, and is a task that has not yet been performed with good results. To derive the optimal approach, we conducted an extensive comparative study of encoder-decoder architectures trained on a combination of focal loss and a variant of generalized dice loss. Based on the EfficientNet and the UNet++ architectures, we propose a line of efficient and high-performance segmentation models using a new decoder architecture, the EfficientUNet++, whose best-performing version achieved average dice scores of 0.8904 and 0.7526 for the artery and catheter classes, respectively, and an average generalized dice score of 0.9234.



