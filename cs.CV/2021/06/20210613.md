# Arxiv Papers in cs.CV on 2021-06-13
### The Spatio-Temporal Poisson Point Process: A Simple Model for the Alignment of Event Camera Data
- **Arxiv ID**: http://arxiv.org/abs/2106.06887v1
- **DOI**: 10.1109/ICCV48922.2021.01324
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06887v1)
- **Published**: 2021-06-13 00:43:27+00:00
- **Updated**: 2021-06-13 00:43:27+00:00
- **Authors**: Cheng Gu, Erik Learned-Miller, Daniel Sheldon, Guillermo Gallego, Pia Bideau
- **Comment**: None
- **Journal**: IEEE International Conference on Computer Vision 2021
- **Summary**: Event cameras, inspired by biological vision systems, provide a natural and data efficient representation of visual information. Visual information is acquired in the form of events that are triggered by local brightness changes. Each pixel location of the camera's sensor records events asynchronously and independently with very high temporal resolution. However, because most brightness changes are triggered by relative motion of the camera and the scene, the events recorded at a single sensor location seldom correspond to the same world point. To extract meaningful information from event cameras, it is helpful to register events that were triggered by the same underlying world point. In this work we propose a new model of event data that captures its natural spatio-temporal structure. We start by developing a model for aligned event data. That is, we develop a model for the data as though it has been perfectly registered already. In particular, we model the aligned data as a spatio-temporal Poisson point process. Based on this model, we develop a maximum likelihood approach to registering events that are not yet aligned. That is, we find transformations of the observed events that make them as likely as possible under our model. In particular we extract the camera rotation that leads to the best event alignment. We show new state of the art accuracy for rotational velocity estimation on the DAVIS 240C dataset. In addition, our method is also faster and has lower computational complexity than several competing methods.



### SAR Image Change Detection Based on Multiscale Capsule Network
- **Arxiv ID**: http://arxiv.org/abs/2106.06896v5
- **DOI**: 10.1109/LGRS.2020.2977838
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.06896v5)
- **Published**: 2021-06-13 01:56:28+00:00
- **Updated**: 2021-09-25 09:38:17+00:00
- **Authors**: Yunhao Gao, Feng Gao, Junyu Dong, Heng-Chao Li
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Traditional change detection methods based on convolutional neural networks (CNNs) face the challenges of speckle noise and deformation sensitivity for synthetic aperture radar images. To mitigate these issues, we proposed a Multiscale Capsule Network (Ms-CapsNet) to extract the discriminative information between the changed and unchanged pixels. On the one hand, the capsule module is employed to exploit the spatial relationship of features. Therefore, equivariant properties can be achieved by aggregating the features from different positions. On the other hand, an adaptive fusion convolution (AFC) module is designed for the proposed Ms-CapsNet. Higher semantic features can be captured for the primary capsules. Feature extracted by the AFC module significantly improves the robustness to speckle noise. The effectiveness of the proposed Ms-CapsNet is verified on three real SAR datasets. The comparison experiments with four state-of-the-art methods demonstrated the efficiency of the proposed method. Our codes are available at https://github.com/summitgao/SAR_CD_MS_CapsNet.



### Domain Generalization on Medical Imaging Classification using Episodic Training with Task Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.06908v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.06908v1)
- **Published**: 2021-06-13 03:56:59+00:00
- **Updated**: 2021-06-13 03:56:59+00:00
- **Authors**: Chenxin Li, Qi Qi, Xinghao Ding, Yue Huang, Dong Liang, Yizhou Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Medical imaging datasets usually exhibit domain shift due to the variations of scanner vendors, imaging protocols, etc. This raises the concern about the generalization capacity of machine learning models. Domain generalization (DG), which aims to learn a model from multiple source domains such that it can be directly generalized to unseen test domains, seems particularly promising to medical imaging community. To address DG, recent model-agnostic meta-learning (MAML) has been introduced, which transfers the knowledge from previous training tasks to facilitate the learning of novel testing tasks. However, in clinical practice, there are usually only a few annotated source domains available, which decreases the capacity of training task generation and thus increases the risk of overfitting to training tasks in the paradigm. In this paper, we propose a novel DG scheme of episodic training with task augmentation on medical imaging classification. Based on meta-learning, we develop the paradigm of episodic training to construct the knowledge transfer from episodic training-task simulation to the real testing task of DG. Motivated by the limited number of source domains in real-world medical deployment, we consider the unique task-level overfitting and we propose task augmentation to enhance the variety during training task generation to alleviate it. With the established learning framework, we further exploit a novel meta-objective to regularize the deep embedding of training domains. To validate the effectiveness of the proposed method, we perform experiments on histopathological images and abdominal CT images.



### An Interaction-based Convolutional Neural Network (ICNN) Towards Better Understanding of COVID-19 X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2106.06911v1
- **DOI**: 10.3390/a14110337
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06911v1)
- **Published**: 2021-06-13 04:41:17+00:00
- **Updated**: 2021-06-13 04:41:17+00:00
- **Authors**: Shaw-Hwa Lo, Yiqiao Yin
- **Comment**: None
- **Journal**: Algorithms 2021
- **Summary**: The field of Explainable Artificial Intelligence (XAI) aims to build explainable and interpretable machine learning (or deep learning) methods without sacrificing prediction performance. Convolutional Neural Networks (CNNs) have been successful in making predictions, especially in image classification. However, these famous deep learning models use tens of millions of parameters based on a large number of pre-trained filters which have been repurposed from previous data sets. We propose a novel Interaction-based Convolutional Neural Network (ICNN) that does not make assumptions about the relevance of local information. Instead, we use a model-free Influence Score (I-score) to directly extract the influential information from images to form important variable modules. We demonstrate that the proposed method produces state-of-the-art prediction performance of 99.8% on a real-world data set classifying COVID-19 Chest X-ray images without sacrificing the explanatory power of the model. This proposed design can efficiently screen COVID-19 patients before human diagnosis, and will be the benchmark for addressing future XAI problems in large-scale data sets.



### Heterogeneous Federated Learning using Dynamic Model Pruning and Adaptive Gradient
- **Arxiv ID**: http://arxiv.org/abs/2106.06921v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.06921v2)
- **Published**: 2021-06-13 05:27:43+00:00
- **Updated**: 2023-02-09 16:44:10+00:00
- **Authors**: Sixing Yu, Phuong Nguyen, Ali Anwar, Ali Jannesari
- **Comment**: Preprint of the CCGrid 2023 Submission
- **Journal**: None
- **Summary**: Federated Learning (FL) has emerged as a new paradigm for training machine learning models distributively without sacrificing data security and privacy. Learning models on edge devices such as mobile phones is one of the most common use cases for FL. However, Non-identical independent distributed~(non-IID) data in edge devices easily leads to training failures. Especially, over-parameterized machine learning models can easily be over-fitted on such data, hence, resulting in inefficient federated learning and poor model performance. To overcome the over-fitting issue, we proposed an adaptive dynamic pruning approach for FL, which can dynamically slim the model by dropping out unimportant parameters, hence, preventing over-fittings. Since the machine learning model's parameters react differently for different training samples, adaptive dynamic pruning will evaluate the salience of the model's parameter according to the input training sample, and only retain the salient parameter's gradients when doing back-propagation. We performed comprehensive experiments to evaluate our approach. The results show that our approach by removing the redundant parameters in neural networks can significantly reduce the over-fitting issue and greatly improves the training efficiency. In particular, when training the ResNet-32 on CIFAR-10, our approach reduces the communication cost by 57\%. We further demonstrate the inference acceleration capability of the proposed algorithm. Our approach reduces up to 50\% FLOPs inference of DNNs on edge devices while maintaining the model's quality.



### Deep Learning for Predictive Analytics in Reversible Steganography
- **Arxiv ID**: http://arxiv.org/abs/2106.06924v3
- **DOI**: 10.1109/ACCESS.2023.3233976
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.06924v3)
- **Published**: 2021-06-13 05:32:17+00:00
- **Updated**: 2023-03-07 14:05:05+00:00
- **Authors**: Ching-Chun Chang, Xu Wang, Sisheng Chen, Isao Echizen, Victor Sanchez, Chang-Tsun Li
- **Comment**: None
- **Journal**: IEEE Access (2023), vol. 11, pp. 3494-3510
- **Summary**: Deep learning is regarded as a promising solution for reversible steganography. There is an accelerating trend of representing a reversible steo-system by monolithic neural networks, which bypass intermediate operations in traditional pipelines of reversible steganography. This end-to-end paradigm, however, suffers from imperfect reversibility. By contrast, the modular paradigm that incorporates neural networks into modules of traditional pipelines can stably guarantee reversibility with mathematical explainability. Prediction-error modulation is a well-established reversible steganography pipeline for digital images. It consists of a predictive analytics module and a reversible coding module. Given that reversibility is governed independently by the coding module, we narrow our focus to the incorporation of neural networks into the analytics module, which serves the purpose of predicting pixel intensities and a pivotal role in determining capacity and imperceptibility. The objective of this study is to evaluate the impacts of different training configurations upon predictive accuracy of neural networks and provide practical insights. In particular, we investigate how different initialisation strategies for input images may affect the learning process and how different training strategies for dual-layer prediction respond to the problem of distributional shift. Furthermore, we compare steganographic performance of various model architectures with different loss functions.



### Inverting Adversarially Robust Networks for Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2106.06927v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2106.06927v5)
- **Published**: 2021-06-13 05:51:00+00:00
- **Updated**: 2022-10-21 17:59:55+00:00
- **Authors**: Renan A. Rojas-Gomez, Raymond A. Yeh, Minh N. Do, Anh Nguyen
- **Comment**: Accepted at the 16th Asian Conference on Computer Vision (ACCV 2022)
- **Journal**: None
- **Summary**: Despite unconditional feature inversion being the foundation of many image synthesis applications, training an inverter demands a high computational budget, large decoding capacity and imposing conditions such as autoregressive priors. To address these limitations, we propose the use of adversarially robust representations as a perceptual primitive for feature inversion. We train an adversarially robust encoder to extract disentangled and perceptually-aligned image representations, making them easily invertible. By training a simple generator with the mirror architecture of the encoder, we achieve superior reconstruction quality and generalization over standard models. Based on this, we propose an adversarially robust autoencoder and demonstrate its improved performance on style transfer, image denoising and anomaly detection tasks. Compared to recent ImageNet feature inversion methods, our model attains improved performance with significantly less complexity.



### Cross-Modal Attention Consistency for Video-Audio Unsupervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.06939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06939v1)
- **Published**: 2021-06-13 07:41:15+00:00
- **Updated**: 2021-06-13 07:41:15+00:00
- **Authors**: Shaobo Min, Qi Dai, Hongtao Xie, Chuang Gan, Yongdong Zhang, Jingdong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-modal correlation provides an inherent supervision for video unsupervised representation learning. Existing methods focus on distinguishing different video clips by visual and audio representations. We human visual perception could attend to regions where sounds are made, and our auditory perception could also ground their frequencies of sounding objects, which we call bidirectional local correspondence. Such supervision is intuitive but not well explored in the contrastive learning framework. This paper introduces a pretext task, Cross-Modal Attention Consistency (CMAC), for exploring the bidirectional local correspondence property. The CMAC approach aims to align the regional attention generated purely from the visual signal with the target attention generated under the guidance of acoustic signal, and do a similar alignment for frequency grounding on the acoustic attention. Accompanied by a remoulded cross-modal contrastive loss where we consider additional within-modal interactions, the CMAC approach works effectively for enforcing the bidirectional alignment. Extensive experiments on six downstream benchmarks demonstrate that CMAC can improve the state-of-the-art performance on both visual and audio modalities.



### A Stronger Baseline for Ego-Centric Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.06942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06942v1)
- **Published**: 2021-06-13 08:11:31+00:00
- **Updated**: 2021-06-13 08:11:31+00:00
- **Authors**: Zhiwu Qing, Ziyuan Huang, Xiang Wang, Yutong Feng, Shiwei Zhang, Jianwen Jiang, Mingqian Tang, Changxin Gao, Marcelo H. Ang Jr, Nong Sang
- **Comment**: CVPRW21, EPIC-KITCHENS-100 Competition Report
- **Journal**: None
- **Summary**: This technical report analyzes an egocentric video action detection method we used in the 2021 EPIC-KITCHENS-100 competition hosted in CVPR2021 Workshop. The goal of our task is to locate the start time and the end time of the action in the long untrimmed video, and predict action category. We adopt sliding window strategy to generate proposals, which can better adapt to short-duration actions. In addition, we show that classification and proposals are conflict in the same network. The separation of the two tasks boost the detection performance with high efficiency. By simply employing these strategy, we achieved 16.10\% performance on the test set of EPIC-KITCHENS-100 Action Detection challenge using a single model, surpassing the baseline method by 11.7\% in terms of average mAP.



### Boosting Randomized Smoothing with Variance Reduced Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2106.06946v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.06946v3)
- **Published**: 2021-06-13 08:40:27+00:00
- **Updated**: 2022-03-30 16:04:51+00:00
- **Authors**: Miklós Z. Horváth, Mark Niklas Müller, Marc Fischer, Martin Vechev
- **Comment**: ICLR 2022 Spotlight Paper
- **Journal**: None
- **Summary**: Randomized Smoothing (RS) is a promising method for obtaining robustness certificates by evaluating a base model under noise. In this work, we: (i) theoretically motivate why ensembles are a particularly suitable choice as base models for RS, and (ii) empirically confirm this choice, obtaining state-of-the-art results in multiple settings. The key insight of our work is that the reduced variance of ensembles over the perturbations introduced in RS leads to significantly more consistent classifications for a given input. This, in turn, leads to substantially increased certifiable radii for samples close to the decision boundary. Additionally, we introduce key optimizations which enable an up to 55-fold decrease in sample complexity of RS for predetermined radii, thus drastically reducing its computational overhead. Experimentally, we show that ensembles of only 3 to 10 classifiers consistently improve on their strongest constituting model with respect to their average certified radius (ACR) by 5% to 21% on both CIFAR10 and ImageNet, achieving a new state-of-the-art ACR of 0.86 and 1.11, respectively. We release all code and models required to reproduce our results at https://github.com/eth-sri/smoothing-ensembles.



### Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs
- **Arxiv ID**: http://arxiv.org/abs/2106.06959v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06959v5)
- **Published**: 2021-06-13 10:29:42+00:00
- **Updated**: 2022-06-25 13:44:07+00:00
- **Authors**: Jaewoong Choi, Junho Lee, Changyeon Yoon, Jung Ho Park, Geonho Hwang, Myungjoo Kang
- **Comment**: 24 pages, 19 figures
- **Journal**: International Conference on Learning Representations, 2022
- **Summary**: The discovery of the disentanglement properties of the latent space in GANs motivated a lot of research to find the semantically meaningful directions on it. In this paper, we suggest that the disentanglement property is closely related to the geometry of the latent space. In this regard, we propose an unsupervised method for finding the semantic-factorizing directions on the intermediate latent space of GANs based on the local geometry. Intuitively, our proposed method, called Local Basis, finds the principal variation of the latent space in the neighborhood of the base latent variable. Experimental results show that the local principal variation corresponds to the semantic factorization and traversing along it provides strong robustness to image traversal. Moreover, we suggest an explanation for the limited success in finding the global traversal directions in the latent space, especially W-space of StyleGAN2. We show that W-space is warped globally by comparing the local geometry, discovered from Local Basis, through the metric on Grassmannian Manifold. The global warpage implies that the latent space is not well-aligned globally and therefore the global traversal directions are bound to show limited success on it.



### DMSANet: Dual Multi Scale Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2106.08382v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.08382v2)
- **Published**: 2021-06-13 10:31:31+00:00
- **Updated**: 2021-08-11 06:49:12+00:00
- **Authors**: Abhinav Sagar
- **Comment**: 11 pages, 3 figures, 8 tables, Submitted to WACV 2022
- **Journal**: None
- **Summary**: Attention mechanism of late has been quite popular in the computer vision community. A lot of work has been done to improve the performance of the network, although almost always it results in increased computational complexity. In this paper, we propose a new attention module that not only achieves the best performance but also has lesser parameters compared to most existing models. Our attention module can easily be integrated with other convolutional neural networks because of its lightweight nature. The proposed network named Dual Multi Scale Attention Network (DMSANet) is comprised of two parts: the first part is used to extract features at various scales and aggregate them, the second part uses spatial and channel attention modules in parallel to adaptively integrate local features with their global dependencies. We benchmark our network performance for Image Classification on ImageNet dataset, Object Detection and Instance Segmentation both on MS COCO dataset.



### Representation and Correlation Enhanced Encoder-Decoder Framework for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.06960v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06960v2)
- **Published**: 2021-06-13 10:36:56+00:00
- **Updated**: 2021-11-14 05:53:11+00:00
- **Authors**: Mengmeng Cui, Wei Wang, Jinjin Zhang, Liang Wang
- **Comment**: 15 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: Attention-based encoder-decoder framework is widely used in the scene text recognition task. However, for the current state-of-the-art(SOTA) methods, there is room for improvement in terms of the efficient usage of local visual and global context information of the input text image, as well as the robust correlation between the scene processing module(encoder) and the text processing module(decoder). In this paper, we propose a Representation and Correlation Enhanced Encoder-Decoder Framework(RCEED) to address these deficiencies and break performance bottleneck. In the encoder module, local visual feature, global context feature, and position information are aligned and fused to generate a small-size comprehensive feature map. In the decoder module, two methods are utilized to enhance the correlation between scene and text feature space. 1) The decoder initialization is guided by the holistic feature and global glimpse vector exported from the encoder. 2) The feature enriched glimpse vector produced by the Multi-Head General Attention is used to assist the RNN iteration and the character prediction at each time step. Meanwhile, we also design a Layernorm-Dropout LSTM cell to improve model's generalization towards changeable texts. Extensive experiments on the benchmarks demonstrate the advantageous performance of RCEED in scene text recognition tasks, especially the irregular ones.



### Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation
- **Arxiv ID**: http://arxiv.org/abs/2106.06963v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2106.06963v2)
- **Published**: 2021-06-13 11:10:02+00:00
- **Updated**: 2021-06-26 17:52:54+00:00
- **Authors**: Fenglin Liu, Xian Wu, Shen Ge, Wei Fan, Yuexian Zou
- **Comment**: Accepted by CVPR 2021 (2021 IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR2021))
- **Journal**: None
- **Summary**: Automatically generating radiology reports can improve current clinical practice in diagnostic radiology. On one hand, it can relieve radiologists from the heavy burden of report writing; On the other hand, it can remind radiologists of abnormalities and avoid the misdiagnosis and missed diagnosis. Yet, this task remains a challenging job for data-driven neural networks, due to the serious visual and textual data biases. To this end, we propose a Posterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to imitate the working patterns of radiologists, who will first examine the abnormal regions and assign the disease topic tags to the abnormal regions, and then rely on the years of prior medical knowledge and prior working experience accumulations to write reports. Thus, the PPKED includes three modules: Posterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and Multi-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior knowledge, which provides explicit abnormal visual regions to alleviate visual data bias; PrKE explores the prior knowledge from the prior medical knowledge graph (medical knowledge) and prior radiology reports (working experience) to alleviate textual data bias. The explored knowledge is distilled by the MKD to generate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our method is able to outperform previous state-of-the-art models on these two datasets.



### Contrastive Attention for Automatic Chest X-ray Report Generation
- **Arxiv ID**: http://arxiv.org/abs/2106.06965v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2106.06965v5)
- **Published**: 2021-06-13 11:20:31+00:00
- **Updated**: 2023-04-11 06:19:27+00:00
- **Authors**: Fenglin Liu, Changchang Yin, Xian Wu, Shen Ge, Yuexian Zou, Ping Zhang, Yuexian Zou, Xu Sun
- **Comment**: Appear in Findings of ACL 2021 (The Joint Conference of the 59th
  Annual Meeting of the Association for Computational Linguistics and the 11th
  International Joint Conference on Natural Language Processing (ACL-IJCNLP
  2021))
- **Journal**: None
- **Summary**: Recently, chest X-ray report generation, which aims to automatically generate descriptions of given chest X-ray images, has received growing research interests. The key challenge of chest X-ray report generation is to accurately capture and describe the abnormal regions. In most cases, the normal regions dominate the entire chest X-ray image, and the corresponding descriptions of these normal regions dominate the final report. Due to such data bias, learning-based models may fail to attend to abnormal regions. In this work, to effectively capture and describe abnormal regions, we propose the Contrastive Attention (CA) model. Instead of solely focusing on the current input image, the CA model compares the current input image with normal images to distill the contrastive information. The acquired contrastive information can better represent the visual features of abnormal regions. According to the experiments on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into several existing models can boost their performance across most metrics. In addition, according to the analysis, the CA model can help existing models better attend to the abnormal regions and provide more accurate descriptions which are crucial for an interpretable diagnosis. Specifically, we achieve the state-of-the-art results on the two public datasets.



### Feedback Pyramid Attention Networks for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2106.06966v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.06966v1)
- **Published**: 2021-06-13 11:32:53+00:00
- **Updated**: 2021-06-13 11:32:53+00:00
- **Authors**: Huapeng Wu, Jie Gui, Jun Zhang, James T. Kwok, Zhihui Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, convolutional neural network (CNN) based image super-resolution (SR) methods have achieved significant performance improvement. However, most CNN-based methods mainly focus on feed-forward architecture design and neglect to explore the feedback mechanism, which usually exists in the human visual system. In this paper, we propose feedback pyramid attention networks (FPAN) to fully exploit the mutual dependencies of features. Specifically, a novel feedback connection structure is developed to enhance low-level feature expression with high-level information. In our method, the output of each layer in the first stage is also used as the input of the corresponding layer in the next state to re-update the previous low-level filters. Moreover, we introduce a pyramid non-local structure to model global contextual information in different scales and improve the discriminative representation of the network. Extensive experimental results on various datasets demonstrate the superiority of our FPAN in comparison with the state-of-the-art SR methods.



### NLHD: A Pixel-Level Non-Local Retinex Model for Low-Light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2106.06971v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.06971v2)
- **Published**: 2021-06-13 11:48:14+00:00
- **Updated**: 2021-06-15 15:08:50+00:00
- **Authors**: Hao Hou, Yingkun Hou, Yuxuan Shi, Benzheng Wei, Jun Xu
- **Comment**: 14 pages, 11 figures
- **Journal**: None
- **Summary**: Retinex model has been applied to low-light image enhancement in many existing methods. More appropriate decomposition of a low-light image can help achieve better image enhancement. In this paper, we propose a new pixel-level non-local Haar transform based illumination and reflectance decomposition method (NLHD). The unique low-frequency coefficient of Haar transform on each similar pixel group is used to reconstruct the illumination component, and the rest of all high-frequency coefficients are employed to reconstruct the reflectance component. The complete similarity of pixels in a matched similar pixel group and the simple separable Haar transform help to obtain more appropriate image decomposition; thus, the image is hardly sharpened in the image brightness enhancement procedure. The exponential transform and logarithmic transform are respectively implemented on the illumination component. Then a minimum fusion strategy on the results of these two transforms is utilized to achieve more natural illumination component enhancement. It can alleviate the mosaic artifacts produced in the darker regions by the exponential transform with a gamma value less than 1 and reduce information loss caused by excessive enhancement of the brighter regions due to the logarithmic transform. Finally, the Retinex model is applied to the enhanced illumination and reflectance to achieve image enhancement. We also develop a local noise level estimation based noise suppression method and a non-local saturation reduction based color deviation correction method. These two methods can respectively attenuate noise or color deviation usually presented in the enhanced results of the extremely dark low-light images. Experiments on benchmark datasets show that the proposed method can achieve better low-light image enhancement results on subjective and objective evaluations than most existing methods.



### An Approach Towards Physics Informed Lung Ultrasound Image Scoring Neural Network for Diagnostic Assistance in COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2106.06980v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.06980v1)
- **Published**: 2021-06-13 13:01:53+00:00
- **Updated**: 2021-06-13 13:01:53+00:00
- **Authors**: Mahesh Raveendranatha Panicker, Yale Tung Chen, Gayathri M, Madhavanunni A N, Kiran Vishnu Narayan, C Kesavadas, A P Vinod
- **Comment**: 8 pages, 8 figures, 3 tables, submitted to Springer SIVP Special
  Issue for COVID19
- **Journal**: None
- **Summary**: Ultrasound is fast becoming an inevitable diagnostic tool for regular and continuous monitoring of the lung with the recent outbreak of COVID-19. In this work, a novel approach is presented to extract acoustic propagation-based features to automatically highlight the region below pleura, which is an important landmark in lung ultrasound (LUS). Subsequently, a multichannel input formed by using the acoustic physics-based feature maps is fused to train a neural network, referred to as LUSNet, to classify the LUS images into five classes of varying severity of lung infection to track the progression of COVID-19. In order to ensure that the proposed approach is agnostic to the type of acquisition, the LUSNet, which consists of a U-net architecture is trained in an unsupervised manner with the acoustic feature maps to ensure that the encoder-decoder architecture is learning features in the pleural region of interest. A novel combination of the U-net output and the U-net encoder output is employed for the classification of severity of infection in the lung. A detailed analysis of the proposed approach on LUS images over the infection to full recovery period of ten confirmed COVID-19 subjects shows an average five-fold cross-validation accuracy, sensitivity, and specificity of 97%, 93%, and 98% respectively over 5000 frames of COVID-19 videos. The analysis also shows that, when the input dataset is limited and diverse as in the case of COVID-19 pandemic, an aided effort of combining acoustic propagation-based features along with the gray scale images, as proposed in this work, improves the performance of the neural network significantly and also aids the labelling and triaging process.



### Learning the Imaging Landmarks: Unsupervised Key point Detection in Lung Ultrasound Videos
- **Arxiv ID**: http://arxiv.org/abs/2106.06987v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.06987v1)
- **Published**: 2021-06-13 13:27:12+00:00
- **Updated**: 2021-06-13 13:27:12+00:00
- **Authors**: Arpan Tripathi, Mahesh Raveendranatha Panicker, Abhilash R Hareendranathan, Yale Tung Chen, Jacob L Jaremko, Kiran Vishnu Narayan, Kesavadas C
- **Comment**: 5 pages, 6 figures, submitted to IEEE EMBC 2021
- **Journal**: None
- **Summary**: Lung ultrasound (LUS) is an increasingly popular diagnostic imaging modality for continuous and periodic monitoring of lung infection, given its advantages of non-invasiveness, non-ionizing nature, portability and easy disinfection. The major landmarks assessed by clinicians for triaging using LUS are pleura, A and B lines. There have been many efforts for the automatic detection of these landmarks. However, restricting to a few pre-defined landmarks may not reveal the actual imaging biomarkers particularly in case of new pathologies like COVID-19. Rather, the identification of key landmarks should be driven by data given the availability of a plethora of neural network algorithms. This work is a first of its kind attempt towards unsupervised detection of the key LUS landmarks in LUS videos of COVID-19 subjects during various stages of infection. We adapted the relatively newer approach of transporter neural networks to automatically mark and track pleura, A and B lines based on their periodic motion and relatively stable appearance in the videos. Initial results on unsupervised pleura detection show an accuracy of 91.8% employing 1081 LUS video frames.



### NDPNet: A novel non-linear data projection network for few-shot fine-grained image classification
- **Arxiv ID**: http://arxiv.org/abs/2106.06988v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.06988v3)
- **Published**: 2021-06-13 13:33:09+00:00
- **Updated**: 2021-07-09 11:22:38+00:00
- **Authors**: Weichuan Zhang, Xuefang Liu, Zhe Xue, Yongsheng Gao, Changming Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Metric-based few-shot fine-grained image classification (FSFGIC) aims to learn a transferable feature embedding network by estimating the similarities between query images and support classes from very few examples. In this work, we propose, for the first time, to introduce the non-linear data projection concept into the design of FSFGIC architecture in order to address the limited sample problem in few-shot learning and at the same time to increase the discriminability of the model for fine-grained image classification. Specifically, we first design a feature re-abstraction embedding network that has the ability to not only obtain the required semantic features for effective metric learning but also re-enhance such features with finer details from input images. Then the descriptors of the query images and the support classes are projected into different non-linear spaces in our proposed similarity metric learning network to learn discriminative projection factors. This design can effectively operate in the challenging and restricted condition of a FSFGIC task for making the distance between the samples within the same class smaller and the distance between samples from different classes larger and for reducing the coupling relationship between samples from different categories. Furthermore, a novel similarity measure based on the proposed non-linear data project is presented for evaluating the relationships of feature information between a query image and a support set. It is worth to note that our proposed architecture can be easily embedded into any episodic training mechanisms for end-to-end training from scratch. Extensive experiments on FSFGIC tasks demonstrate the superiority of the proposed methods over the state-of-the-art benchmarks.



### Is Perfect Filtering Enough Leading to Perfect Phase Correction for dMRI data?
- **Arxiv ID**: http://arxiv.org/abs/2106.06992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06992v1)
- **Published**: 2021-06-13 13:38:32+00:00
- **Updated**: 2021-06-13 13:38:32+00:00
- **Authors**: Liu Feihong, Yang Junwei, He Xiaowei, Zhou Luping, Feng Jun, Shen Dinggang
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Being complex-valued and low in signal-to-noise ratios, magnitude-based diffusion MRI is confounded by the noise-floor that falsely elevates signal magnitude and incurs bias to the commonly used diffusion indices, such as fractional anisotropy (FA). To avoid noise-floor, most existing phase correction methods explore improving filters to estimate the noise-free background phase. In this work, after diving into the phase correction procedures, we argue that even a perfect filter is insufficient for phase correction because the correction procedures are incapable of distinguishing sign-symbols of noise, resulting in artifacts (\textit{i.e.}, arbitrary signal loss). With this insight, we generalize the definition of noise-floor to a complex polar coordinate system and propose a calibration procedure that could conveniently distinguish noise sign symbols. The calibration procedure is conceptually simple and easy to implement without relying on any external technique while keeping distinctly effective.



### Pyramidal Dense Attention Networks for Lightweight Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2106.06996v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.06996v1)
- **Published**: 2021-06-13 13:49:41+00:00
- **Updated**: 2021-06-13 13:49:41+00:00
- **Authors**: Huapeng Wu, Jie Gui, Jun Zhang, James T. Kwok, Zhihui Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep convolutional neural network methods have achieved an excellent performance in image superresolution (SR), but they can not be easily applied to embedded devices due to large memory cost. To solve this problem, we propose a pyramidal dense attention network (PDAN) for lightweight image super-resolution in this paper. In our method, the proposed pyramidal dense learning can gradually increase the width of the densely connected layer inside a pyramidal dense block to extract deep features efficiently. Meanwhile, the adaptive group convolution that the number of groups grows linearly with dense convolutional layers is introduced to relieve the parameter explosion. Besides, we also present a novel joint attention to capture cross-dimension interaction between the spatial dimensions and channel dimension in an efficient way for providing rich discriminative feature representations. Extensive experimental results show that our method achieves superior performance in comparison with the state-of-the-art lightweight SR methods.



### Experimental Analysis of Trajectory Control Using Computer Vision and Artificial Intelligence for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2106.07003v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.07003v1)
- **Published**: 2021-06-13 14:23:18+00:00
- **Updated**: 2021-06-13 14:23:18+00:00
- **Authors**: Ammar N. Abbas, Muhammad Asad Irshad, Hossam Hassan Ammar
- **Comment**: None
- **Journal**: None
- **Summary**: Perception of the lane boundaries is crucial for the tasks related to autonomous trajectory control. In this paper, several methodologies for lane detection are discussed with an experimental illustration: Hough transformation, Blob analysis, and Bird's eye view. Following the abstraction of lane marks from the boundary, the next approach is applying a control law based on the perception to control steering and speed control. In the following, a comparative analysis is made between an open-loop response, PID control, and a neural network control law through graphical statistics. To get the perception of the surrounding a wireless streaming camera connected to Raspberry Pi is used. After pre-processing the signal received by the camera the output is sent back to the Raspberry Pi that processes the input and communicates the control to the motors through Arduino via serial communication.



### Noise2Score: Tweedie's Approach to Self-Supervised Image Denoising without Clean Images
- **Arxiv ID**: http://arxiv.org/abs/2106.07009v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07009v2)
- **Published**: 2021-06-13 14:41:09+00:00
- **Updated**: 2021-10-27 09:02:42+00:00
- **Authors**: Kwanyoung Kim, Jong Chul Ye
- **Comment**: Camera ready version for NeurIPS 2021
- **Journal**: None
- **Summary**: Recently, there has been extensive research interest in training deep networks to denoise images without clean reference. However, the representative approaches such as Noise2Noise, Noise2Void, Stein's unbiased risk estimator (SURE), etc. seem to differ from one another and it is difficult to find the coherent mathematical structure. To address this, here we present a novel approach, called Noise2Score, which reveals a missing link in order to unite these seemingly different approaches. Specifically, we show that image denoising problems without clean images can be addressed by finding the mode of the posterior distribution and that the Tweedie's formula offers an explicit solution through the score function (i.e. the gradient of log likelihood). Our method then uses the recent finding that the score function can be stably estimated from the noisy images using the amortized residual denoising autoencoder, the method of which is closely related to Noise2Noise or Nose2Void. Our Noise2Score approach is so universal that the same network training can be used to remove noises from images that are corrupted by any exponential family distributions and noise parameters. Using extensive experiments with Gaussian, Poisson, and Gamma noises, we show that Noise2Score significantly outperforms the state-of-the-art self-supervised denoising methods in the benchmark data set such as (C)BSD68, Set12, and Kodak, etc.



### Siamese Network Training Using Artificial Triplets By Sampling and Image Transformation
- **Arxiv ID**: http://arxiv.org/abs/2106.07015v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.07015v2)
- **Published**: 2021-06-13 14:47:52+00:00
- **Updated**: 2021-09-26 07:24:49+00:00
- **Authors**: Ammar N. Abbas, David Moser
- **Comment**: None
- **Journal**: None
- **Summary**: The device used in this work detects the objects over the surface of the water using two thermal cameras which aid the users to detect and avoid the objects in scenarios where the human eyes cannot (night, fog, etc.). To avoid the obstacle collision autonomously, it is required to track the objects in real-time and assign a specific identity to each object to determine its dynamics (trajectory, velocity, etc.) for making estimated collision predictions. In the following work, a Machine Learning (ML) approach for Computer Vision (CV) called Convolutional Neural Network (CNN) was used using TensorFlow as the high-level programming environment in Python. To validate the algorithm a test set was generated using an annotation tool that was created during the work for proper evaluation. Once validated, the algorithm was deployed on the platform and tested with the sequence generated by the test boat.



### Generation of the NIR spectral Band for Satellite Images with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.07020v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.07020v1)
- **Published**: 2021-06-13 15:14:57+00:00
- **Updated**: 2021-06-13 15:14:57+00:00
- **Authors**: Svetlana Illarionova, Dmitrii Shadrin, Alexey Trekin, Vladimir Ignatiev, Ivan Oseledets
- **Comment**: None
- **Journal**: None
- **Summary**: The near-infrared (NIR) spectral range (from 780 to 2500 nm) of the multispectral remote sensing imagery provides vital information for the landcover classification, especially concerning the vegetation assessment. Despite the usefulness of NIR, common RGB is not always accompanied by it. Modern achievements in image processing via deep neural networks allow generating artificial spectral information, such as for the image colorization problem. In this research, we aim to investigate whether this approach can produce not only visually similar images but also an artificial spectral band that can improve the performance of computer vision algorithms for solving remote sensing tasks. We study the generative adversarial network (GAN) approach in the task of the NIR band generation using just RGB channels of high-resolution satellite imagery. We evaluate the impact of a generated channel on the model performance for solving the forest segmentation task. Our results show an increase in model accuracy when using generated NIR comparing to the baseline model that uses only RGB (0.947 and 0.914 F1-score accordingly). Conducted study shows the advantages of generating the extra band and its implementation in applied challenges reducing the required amount of labeled data.



### Styleformer: Transformer based Generative Adversarial Networks with Style Vector
- **Arxiv ID**: http://arxiv.org/abs/2106.07023v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.07023v3)
- **Published**: 2021-06-13 15:30:39+00:00
- **Updated**: 2022-04-05 09:52:11+00:00
- **Authors**: Jeeseung Park, Younggeun Kim
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: We propose Styleformer, which is a style-based generator for GAN architecture, but a convolution-free transformer-based generator. In our paper, we explain how a transformer can generate high-quality images, overcoming the disadvantage that convolution operations are difficult to capture global features in an image. Furthermore, we change the demodulation of StyleGAN2 and modify the existing transformer structure (e.g., residual connection, layer normalization) to create a strong style-based generator with a convolution-free structure. We also make Styleformer lighter by applying Linformer, enabling Styleformer to generate higher resolution images and result in improvements in terms of speed and memory. We experiment with the low-resolution image dataset such as CIFAR-10, as well as the high-resolution image dataset like LSUN-church. Styleformer records FID 2.82 and IS 9.94 on CIFAR-10, a benchmark dataset, which is comparable performance to the current state-of-the-art and outperforms all GAN-based generative models, including StyleGAN2-ADA with fewer parameters on the unconditional setting. We also both achieve new state-of-the-art with FID 15.17, IS 11.01, and FID 3.66, respectively on STL-10 and CelebA. We release our code at https://github.com/Jeeseung-Park/Styleformer.



### Reborn Mechanism: Rethinking the Negative Phase Information Flow in Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2106.07026v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07026v2)
- **Published**: 2021-06-13 15:33:49+00:00
- **Updated**: 2021-07-07 17:37:09+00:00
- **Authors**: Zhicheng Cai, Kaizhu Huang, Chenglei Peng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel nonlinear activation mechanism typically for convolutional neural network (CNN), named as reborn mechanism. In sharp contrast to ReLU which cuts off the negative phase value, the reborn mechanism enjoys the capacity to reborn and reconstruct dead neurons. Compared to other improved ReLU functions, reborn mechanism introduces a more proper way to utilize the negative phase information. Extensive experiments validate that this activation mechanism is able to enhance the model representation ability more significantly and make the better use of the input data information while maintaining the advantages of the original ReLU function. Moreover, reborn mechanism enables a non-symmetry that is hardly achieved by traditional CNNs and can act as a channel compensation method, offering competitive or even better performance but with fewer learned parameters than traditional methods. Reborn mechanism was tested on various benchmark datasets, all obtaining better performance than previous nonlinear activation functions.



### Weakly-supervised High-resolution Segmentation of Mammography Images for Breast Cancer Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2106.07049v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07049v2)
- **Published**: 2021-06-13 17:25:21+00:00
- **Updated**: 2021-06-15 03:46:23+00:00
- **Authors**: Kangning Liu, Yiqiu Shen, Nan Wu, Jakub Chłędowski, Carlos Fernandez-Granda, Krzysztof J. Geras
- **Comment**: The last two authors contributed equally. Accepted to Medical Imaging
  with Deep Learning (MIDL) 2021
- **Journal**: None
- **Summary**: In the last few years, deep learning classifiers have shown promising results in image-based medical diagnosis. However, interpreting the outputs of these models remains a challenge. In cancer diagnosis, interpretability can be achieved by localizing the region of the input image responsible for the output, i.e. the location of a lesion. Alternatively, segmentation or detection models can be trained with pixel-wise annotations indicating the locations of malignant lesions. Unfortunately, acquiring such labels is labor-intensive and requires medical expertise. To overcome this difficulty, weakly-supervised localization can be utilized. These methods allow neural network classifiers to output saliency maps highlighting the regions of the input most relevant to the classification task (e.g. malignant lesions in mammograms) using only image-level labels (e.g. whether the patient has cancer or not) during training. When applied to high-resolution images, existing methods produce low-resolution saliency maps. This is problematic in applications in which suspicious lesions are small in relation to the image size. In this work, we introduce a novel neural network architecture to perform weakly-supervised segmentation of high-resolution images. The proposed model selects regions of interest via coarse-level localization, and then performs fine-grained segmentation of those regions. We apply this model to breast cancer diagnosis with screening mammography, and validate it on a large clinically-realistic dataset. Measured by Dice similarity score, our approach outperforms existing methods by a large margin in terms of localization performance of benign and malignant lesions, relatively improving the performance by 39.6% and 20.0%, respectively. Code and the weights of some of the models are available at https://github.com/nyukat/GLAM



### HistoTransfer: Understanding Transfer Learning for Histopathology
- **Arxiv ID**: http://arxiv.org/abs/2106.07068v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07068v1)
- **Published**: 2021-06-13 18:55:23+00:00
- **Updated**: 2021-06-13 18:55:23+00:00
- **Authors**: Yash Sharma, Lubaina Ehsan, Sana Syed, Donald E. Brown
- **Comment**: Accepted at IEEE International Conference on Biomedical and Health
  Informatics (BHI'21). arXiv admin note: text overlap with arXiv:2103.10626
- **Journal**: None
- **Summary**: Advancement in digital pathology and artificial intelligence has enabled deep learning-based computer vision techniques for automated disease diagnosis and prognosis. However, WSIs present unique computational and algorithmic challenges. WSIs are gigapixel-sized, making them infeasible to be used directly for training deep neural networks. Hence, for modeling, a two-stage approach is adopted: Patch representations are extracted first, followed by the aggregation for WSI prediction. These approaches require detailed pixel-level annotations for training the patch encoder. However, obtaining these annotations is time-consuming and tedious for medical experts. Transfer learning is used to address this gap and deep learning architectures pre-trained on ImageNet are used for generating patch-level representation. Even though ImageNet differs significantly from histopathology data, pre-trained networks have been shown to perform impressively on histopathology data. Also, progress in self-supervised and multi-task learning coupled with the release of multiple histopathology data has led to the release of histopathology-specific networks. In this work, we compare the performance of features extracted from networks trained on ImageNet and histopathology data. We use an attention pooling network over these extracted features for slide-level aggregation. We investigate if features learned using more complex networks lead to gain in performance. We use a simple top-k sampling approach for fine-tuning framework and study the representation similarity between frozen and fine-tuned networks using Centered Kernel Alignment. Further, to examine if intermediate block representation is better suited for feature extraction and ImageNet architectures are unnecessarily large for histopathology, we truncate the blocks of ResNet18 and DenseNet121 and examine the performance.



### Revisiting consistency for semi-supervised semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.07075v5
- **DOI**: 10.3390/s23020940
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07075v5)
- **Published**: 2021-06-13 19:31:59+00:00
- **Updated**: 2023-01-20 10:52:30+00:00
- **Authors**: Ivan Grubišić, Marin Oršić, Siniša Šegvić
- **Comment**: The source code is available at
  https://github.com/Ivan1248/semisup-seg-efficient
- **Journal**: Sensors. 2023; 23(2):940
- **Summary**: Semi-supervised learning an attractive technique in practical deployments of deep models since it relaxes the dependence on labeled data. It is especially important in the scope of dense prediction because pixel-level annotation requires significant effort. This paper considers semi-supervised algorithms that enforce consistent predictions over perturbed unlabeled inputs. We study the advantages of perturbing only one of the two model instances and preventing the backward pass through the unperturbed instance. We also propose a competitive perturbation model as a composition of geometric warp and photometric jittering. We experiment with efficient models due to their importance for real-time and low-power applications. Our experiments show clear advantages of (1) one-way consistency, (2) perturbing only the student branch, and (3) strong photometric and geometric perturbations. Our perturbation model outperforms recent work and most of the contribution comes from photometric component. Experiments with additional data from the large coarsely annotated subset of Cityscapes suggest that semi-supervised training can outperform supervised training with the coarse labels.



### Survey: Image Mixing and Deleting for Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.07085v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07085v4)
- **Published**: 2021-06-13 20:32:24+00:00
- **Updated**: 2023-02-06 19:21:18+00:00
- **Authors**: Humza Naveed, Saeed Anwar, Munawar Hayat, Kashif Javed, Ajmal Mian
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks are prone to overfitting and memorizing data patterns. To avoid over-fitting and enhance their generalization and performance, various methods have been suggested in the literature, including dropout, regularization, label smoothing, etc. One such method is augmentation which introduces different types of corruption in the data to prevent the model from overfitting and to memorize patterns present in the data. A sub-area of data augmentation is image mixing and deleting. This specific type of augmentation either deletes image regions or mixes two images to hide or make particular characteristics of images confusing for the network, forcing it to emphasize the overall structure of the object in an image. Models trained with this approach have proven to perform and generalize well compared to those trained without image mixing or deleting. An added benefit that comes with this method of training is robustness against image corruption. Due to its low computational cost and recent success, researchers have proposed many image mixing and deleting techniques. We furnish an in-depth survey of image mixing and deleting techniques and provide categorization via their most distinguishing features. We initiate our discussion with some fundamental relevant concepts. Next, we present essentials, such as each category's strengths and limitations, describing their working mechanism, basic formulations, and applications. We also discuss the general challenges and recommend possible future research directions for image mixing and deleting data augmentation techniques. Datasets and codes for evaluation are publicly available here.



### On-Off Center-Surround Receptive Fields for Accurate and Robust Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.07091v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2106.07091v1)
- **Published**: 2021-06-13 20:55:16+00:00
- **Updated**: 2021-06-13 20:55:16+00:00
- **Authors**: Zahra Babaiee, Ramin Hasani, Mathias Lechner, Daniela Rus, Radu Grosu
- **Comment**: 21 Pages. Accepted for publication in the proceedings of the 38th
  International Conference on Machine Learning (ICML) 2021
- **Journal**: None
- **Summary**: Robustness to variations in lighting conditions is a key objective for any deep vision system. To this end, our paper extends the receptive field of convolutional neural networks with two residual components, ubiquitous in the visual processing system of vertebrates: On-center and off-center pathways, with excitatory center and inhibitory surround; OOCS for short. The on-center pathway is excited by the presence of a light stimulus in its center but not in its surround, whereas the off-center one is excited by the absence of a light stimulus in its center but not in its surround. We design OOCS pathways via a difference of Gaussians, with their variance computed analytically from the size of the receptive fields. OOCS pathways complement each other in their response to light stimuli, ensuring this way a strong edge-detection capability, and as a result, an accurate and robust inference under challenging lighting conditions. We provide extensive empirical evidence showing that networks supplied with the OOCS edge representation gain accuracy and illumination-robustness compared to standard deep models.



### Reducing Effects of Swath Gaps on Unsupervised Machine Learning Models for NASA MODIS Instruments
- **Arxiv ID**: http://arxiv.org/abs/2106.07113v2
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.EP, astro-ph.IM
- **Links**: [PDF](http://arxiv.org/pdf/2106.07113v2)
- **Published**: 2021-06-13 23:50:05+00:00
- **Updated**: 2021-07-31 04:02:41+00:00
- **Authors**: Sarah Chen, Esther Cao, Anirudh Koul, Siddha Ganju, Satyarth Praveen, Meher Anand Kasam
- **Comment**: Accepted to COSPAR 2021 Workshop on Cloud Computing for Space
  Sciences
- **Journal**: None
- **Summary**: Due to the nature of their pathways, NASA Terra and NASA Aqua satellites capture imagery containing swath gaps, which are areas of no data. Swath gaps can overlap the region of interest (ROI) completely, often rendering the entire imagery unusable by Machine Learning (ML) models. This problem is further exacerbated when the ROI rarely occurs (e.g. a hurricane) and, on occurrence, is partially overlapped with a swath gap. With annotated data as supervision, a model can learn to differentiate between the area of focus and the swath gap. However, annotation is expensive and currently the vast majority of existing data is unannotated. Hence, we propose an augmentation technique that considerably removes the existence of swath gaps in order to allow CNNs to focus on the ROI, and thus successfully use data with swath gaps for training. We experiment on the UC Merced Land Use Dataset, where we add swath gaps through empty polygons (up to 20 percent areas) and then apply augmentation techniques to fill the swath gaps. We compare the model trained with our augmentation techniques on the swath gap-filled data with the model trained on the original swath gap-less data and note highly augmented performance. Additionally, we perform a qualitative analysis using activation maps that visualizes the effectiveness of our trained network in not paying attention to the swath gaps. We also evaluate our results with a human baseline and show that, in certain cases, the filled swath gaps look so realistic that even a human evaluator did not distinguish between original satellite images and swath gap-filled images. Since this method is aimed at unlabeled data, it is widely generalizable and impactful for large scale unannotated datasets from various space data domains.



