# Arxiv Papers in cs.CV on 2021-06-26
### Midpoint Regularization: from High Uncertainty Training to Conservative Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.13913v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.13913v1)
- **Published**: 2021-06-26 00:31:46+00:00
- **Updated**: 2021-06-26 00:31:46+00:00
- **Authors**: Hongyu Guo
- **Comment**: Accepted to ECML-PKDD 2021. arXiv admin note: substantial text
  overlap with arXiv:2012.01559
- **Journal**: None
- **Summary**: Label Smoothing (LS) improves model generalization through penalizing models from generating overconfident output distributions. For each training sample the LS strategy smooths the one-hot encoded training signal by distributing its distribution mass over the non-ground truth classes. We extend this technique by considering example pairs, coined PLS. PLS first creates midpoint samples by averaging random sample pairs and then learns a smoothing distribution during training for each of these midpoint samples, resulting in midpoints with high uncertainty labels for training. We empirically show that PLS significantly outperforms LS, achieving up to 30% of relative classification error reduction. We also visualize that PLS produces very low winning softmax scores for both in and out of distribution samples.



### CAMS: Color-Aware Multi-Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2106.13920v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.13920v2)
- **Published**: 2021-06-26 01:15:09+00:00
- **Updated**: 2021-09-04 19:09:06+00:00
- **Authors**: Mahmoud Afifi, Abdullah Abuolaim, Mostafa Hussien, Marcus A. Brubaker, Michael S. Brown
- **Comment**: None
- **Journal**: None
- **Summary**: Image style transfer aims to manipulate the appearance of a source image, or "content" image, to share similar texture and colors of a target "style" image. Ideally, the style transfer manipulation should also preserve the semantic content of the source image. A commonly used approach to assist in transferring styles is based on Gram matrix optimization. One problem of Gram matrix-based optimization is that it does not consider the correlation between colors and their styles. Specifically, certain textures or structures should be associated with specific colors. This is particularly challenging when the target style image exhibits multiple style types. In this work, we propose a color-aware multi-style transfer method that generates aesthetically pleasing results while preserving the style-color correlation between style and generated images. We achieve this desired outcome by introducing a simple but efficient modification to classic Gram matrix-based style transfer optimization. A nice feature of our method is that it enables the users to manually select the color associations between the target style and content image for more transfer flexibility. We validated our method with several qualitative comparisons, including a user study conducted with 30 participants. In comparison with prior work, our method is simple, easy to implement, and achieves visually appealing results when targeting images that have multiple styles. Source code is available at https://github.com/mahmoudnafifi/color-aware-style-transfer.



### Dual-Stream Reciprocal Disentanglement Learning for Domain Adaptation Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2106.13929v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.13929v2)
- **Published**: 2021-06-26 03:05:23+00:00
- **Updated**: 2021-10-01 16:27:34+00:00
- **Authors**: Huafeng Li, Kaixiong Xu, Jinxing Li, Guangming Lu, Yong Xu, Zhengtao Yu, David Zhang
- **Comment**: 14 pages,8 figures
- **Journal**: None
- **Summary**: Since human-labeled samples are free for the target set, unsupervised person re-identification (Re-ID) has attracted much attention in recent years, by additionally exploiting the source set. However, due to the differences on camera styles, illumination and backgrounds, there exists a large gap between source domain and target domain, introducing a great challenge on cross-domain matching. To tackle this problem, in this paper we propose a novel method named Dual-stream Reciprocal Disentanglement Learning (DRDL), which is quite efficient in learning domain-invariant features. In DRDL, two encoders are first constructed for id-related and id-unrelated feature extractions, which are respectively measured by their associated classifiers. Furthermore, followed by an adversarial learning strategy, both streams reciprocally and positively effect each other, so that the id-related features and id-unrelated features are completely disentangled from a given image, allowing the encoder to be powerful enough to obtain the discriminative but domain-invariant features. In contrast to existing approaches, our proposed method is free from image generation, which not only reduces the computational complexity remarkably, but also removes redundant information from id-related features. Extensive experiments substantiate the superiority of our proposed method compared with the state-of-the-arts. The source code has been released in https://github.com/lhf12278/DRDL.



### Inverting and Understanding Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2106.13933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.13933v1)
- **Published**: 2021-06-26 03:31:59+00:00
- **Updated**: 2021-06-26 03:31:59+00:00
- **Authors**: Ang Cao, Justin Johnson
- **Comment**: Preprints
- **Journal**: None
- **Summary**: As a core problem in computer vision, the performance of object detection has improved drastically in the past few years. Despite their impressive performance, object detectors suffer from a lack of interpretability. Visualization techniques have been developed and widely applied to introspect the decisions made by other kinds of deep learning models; however, visualizing object detectors has been underexplored. In this paper, we propose using inversion as a primary tool to understand modern object detectors and develop an optimization-based approach to layout inversion, allowing us to generate synthetic images recognized by trained detectors as containing a desired configuration of objects. We reveal intriguing properties of detectors by applying our layout inversion technique to a variety of modern object detectors, and further investigate them via validation experiments: they rely on qualitatively different features for classification and regression; they learn canonical motifs of commonly co-occurring objects; they use diff erent visual cues to recognize objects of varying sizes. We hope our insights can help practitioners improve object detectors.



### Domain Adaptive YOLO for One-Stage Cross-Domain Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.13939v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.13939v2)
- **Published**: 2021-06-26 04:17:42+00:00
- **Updated**: 2021-07-04 02:34:06+00:00
- **Authors**: Shizhao Zhang, Hongya Tuo, Jian Hu, Zhongliang Jing
- **Comment**: None
- **Journal**: None
- **Summary**: Domain shift is a major challenge for object detectors to generalize well to real world applications. Emerging techniques of domain adaptation for two-stage detectors help to tackle this problem. However, two-stage detectors are not the first choice for industrial applications due to its long time consumption. In this paper, a novel Domain Adaptive YOLO (DA-YOLO) is proposed to improve cross-domain performance for one-stage detectors. Image level features alignment is used to strictly match for local features like texture, and loosely match for global features like illumination. Multi-scale instance level features alignment is presented to reduce instance domain shift effectively , such as variations in object appearance and viewpoint. A consensus regularization to these domain classifiers is employed to help the network generate domain-invariant detections. We evaluate our proposed method on popular datasets like Cityscapes, KITTI, SIM10K and etc.. The results demonstrate significant improvement when tested under different cross-domain scenarios.



### Core Challenges in Embodied Vision-Language Planning
- **Arxiv ID**: http://arxiv.org/abs/2106.13948v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.13948v4)
- **Published**: 2021-06-26 05:18:58+00:00
- **Updated**: 2022-05-24 18:52:43+00:00
- **Authors**: Jonathan Francis, Nariaki Kitamura, Felix Labelle, Xiaopeng Lu, Ingrid Navarro, Jean Oh
- **Comment**: Journal of Artificial Intelligence Research 74 (2022) 459-515
- **Journal**: None
- **Summary**: Recent advances in the areas of multimodal machine learning and artificial intelligence (AI) have led to the development of challenging tasks at the intersection of Computer Vision, Natural Language Processing, and Embodied AI. Whereas many approaches and previous survey pursuits have characterised one or two of these dimensions, there has not been a holistic analysis at the center of all three. Moreover, even when combinations of these topics are considered, more focus is placed on describing, e.g., current architectural methods, as opposed to also illustrating high-level challenges and opportunities for the field. In this survey paper, we discuss Embodied Vision-Language Planning (EVLP) tasks, a family of prominent embodied navigation and manipulation problems that jointly use computer vision and natural language. We propose a taxonomy to unify these tasks and provide an in-depth analysis and comparison of the new and current algorithmic approaches, metrics, simulated environments, as well as the datasets used for EVLP tasks. Finally, we present the core challenges that we believe new EVLP works should seek to address, and we advocate for task construction that enables model generalizability and furthers real-world deployment.



### Spectral-Spatial Global Graph Reasoning for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.13952v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.13952v2)
- **Published**: 2021-06-26 06:24:51+00:00
- **Updated**: 2023-04-06 16:50:23+00:00
- **Authors**: Di Wang, Bo Du, Liangpei Zhang
- **Comment**: Accepted by IEEE TNNLS
- **Journal**: None
- **Summary**: Convolutional neural networks have been widely applied to hyperspectral image classification. However, traditional convolutions can not effectively extract features for objects with irregular distributions. Recent methods attempt to address this issue by performing graph convolutions on spatial topologies, but fixed graph structures and local perceptions limit their performances. To tackle these problems, in this paper, different from previous approaches, we perform the superpixel generation on intermediate features during network training to adaptively produce homogeneous regions, obtain graph structures, and further generate spatial descriptors, which are served as graph nodes. Besides spatial objects, we also explore the graph relationships between channels by reasonably aggregating channels to generate spectral descriptors. The adjacent matrices in these graph convolutions are obtained by considering the relationships among all descriptors to realize global perceptions. By combining the extracted spatial and spectral graph features, we finally obtain a spectral-spatial graph reasoning network (SSGRN). The spatial and spectral parts of SSGRN are separately called spatial and spectral graph reasoning subnetworks. Comprehensive experiments on four public datasets demonstrate the competitiveness of the proposed methods compared with other state-of-the-art graph convolution-based approaches.



### In-N-Out: Towards Good Initialization for Inpainting and Outpainting
- **Arxiv ID**: http://arxiv.org/abs/2106.13953v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.13953v3)
- **Published**: 2021-06-26 06:26:15+00:00
- **Updated**: 2021-09-17 10:52:50+00:00
- **Authors**: Changho Jo, Woobin Im, Sung-Eui Yoon
- **Comment**: 14 pages (10 pages without references), 7 figures
- **Journal**: None
- **Summary**: In computer vision, recovering spatial information by filling in masked regions, e.g., inpainting, has been widely investigated for its usability and wide applicability to other various applications: image inpainting, image extrapolation, and environment map estimation. Most of them are studied separately depending on the applications. Our focus, however, is on accommodating the opposite task, e.g., image outpainting, which would benefit the target applications, e.g., image inpainting. Our self-supervision method, In-N-Out, is summarized as a training approach that leverages the knowledge of the opposite task into the target model. We empirically show that In-N-Out -- which explores the complementary information -- effectively takes advantage over the traditional pipelines where only task-specific learning takes place in training. In experiments, we compare our method to the traditional procedure and analyze the effectiveness of our method on different applications: image inpainting, image extrapolation, and environment map estimation. For these tasks, we demonstrate that In-N-Out consistently improves the performance of the recent works with In-N-Out self-supervision to their training procedure. Also, we show that our approach achieves better results than an existing training approach for outpainting.



### Functional Classwise Principal Component Analysis: A Novel Classification Framework
- **Arxiv ID**: http://arxiv.org/abs/2106.13959v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.13959v1)
- **Published**: 2021-06-26 07:10:58+00:00
- **Updated**: 2021-06-26 07:10:58+00:00
- **Authors**: Avishek Chatterjee, Satyaki Mazumder, Koel Das
- **Comment**: None
- **Journal**: None
- **Summary**: In recent times, functional data analysis (FDA) has been successfully applied in the field of high dimensional data classification. In this paper, we present a novel classification framework using functional data and classwise Principal Component Analysis (PCA). Our proposed method can be used in high dimensional time series data which typically suffers from small sample size problem. Our method extracts a piece wise linear functional feature space and is particularly suitable for hard classification problems.The proposed framework converts time series data into functional data and uses classwise functional PCA for feature extraction followed by classification using a Bayesian linear classifier. We demonstrate the efficacy of our proposed method by applying it to both synthetic data sets and real time series data from diverse fields including but not limited to neuroscience, food science, medical sciences and chemometrics.



### Multi-stage Optimization based Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2106.15357v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15357v1)
- **Published**: 2021-06-26 07:59:52+00:00
- **Updated**: 2021-06-26 07:59:52+00:00
- **Authors**: Xiaosen Wang, Chuanbiao Song, Liwei Wang, Kun He
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: In the field of adversarial robustness, there is a common practice that adopts the single-step adversarial training for quickly developing adversarially robust models. However, the single-step adversarial training is most likely to cause catastrophic overfitting, as after a few training epochs it will be hard to generate strong adversarial examples to continuously boost the adversarial robustness. In this work, we aim to avoid the catastrophic overfitting by introducing multi-step adversarial examples during the single-step adversarial training. Then, to balance the large training overhead of generating multi-step adversarial examples, we propose a Multi-stage Optimization based Adversarial Training (MOAT) method that periodically trains the model on mixed benign examples, single-step adversarial examples, and multi-step adversarial examples stage by stage. In this way, the overall training overhead is reduced significantly, meanwhile, the model could avoid catastrophic overfitting. Extensive experiments on CIFAR-10 and CIFAR-100 datasets demonstrate that under similar amount of training overhead, the proposed MOAT exhibits better robustness than either single-step or multi-step adversarial training methods.



### OffRoadTranSeg: Semi-Supervised Segmentation using Transformers on OffRoad environments
- **Arxiv ID**: http://arxiv.org/abs/2106.13963v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.13963v1)
- **Published**: 2021-06-26 08:05:09+00:00
- **Updated**: 2021-06-26 08:05:09+00:00
- **Authors**: Anukriti Singh, Kartikeya Singh, P. B. Sujit
- **Comment**: None
- **Journal**: None
- **Summary**: We present OffRoadTranSeg, the first end-to-end framework for semi-supervised segmentation in unstructured outdoor environment using transformers and automatic data selection for labelling. The offroad segmentation is a scene understanding approach that is widely used in autonomous driving. The popular offroad segmentation method is to use fully connected convolution layers and large labelled data, however, due to class imbalance, there will be several mismatches and also some classes may not be detected. Our approach is to do the task of offroad segmentation in a semi-supervised manner. The aim is to provide a model where self supervised vision transformer is used to fine-tune offroad datasets with self-supervised data collection for labelling using depth estimation. The proposed method is validated on RELLIS-3D and RUGD offroad datasets. The experiments show that OffRoadTranSeg outperformed other state of the art models, and also solves the RELLIS-3D class imbalance problem.



### Exploring Temporal Context and Human Movement Dynamics for Online Action Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/2106.13967v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.13967v1)
- **Published**: 2021-06-26 08:34:19+00:00
- **Updated**: 2021-06-26 08:34:19+00:00
- **Authors**: Vasiliki I. Vasileiou, Nikolaos Kardaris, Petros Maragos
- **Comment**: EUSIPCO-2021
- **Journal**: None
- **Summary**: Nowadays, the interaction between humans and robots is constantly expanding, requiring more and more human motion recognition applications to operate in real time. However, most works on temporal action detection and recognition perform these tasks in offline manner, i.e. temporally segmented videos are classified as a whole. In this paper, based on the recently proposed framework of Temporal Recurrent Networks, we explore how temporal context and human movement dynamics can be effectively employed for online action detection. Our approach uses various state-of-the-art architectures and appropriately combines the extracted features in order to improve action detection. We evaluate our method on a challenging but widely used dataset for temporal action localization, THUMOS'14. Our experiments show significant improvement over the baseline method, achieving state-of-the art results on THUMOS'14.



### Semantics-aware Multi-modal Domain Translation:From LiDAR Point Clouds to Panoramic Color Images
- **Arxiv ID**: http://arxiv.org/abs/2106.13974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.13974v1)
- **Published**: 2021-06-26 08:52:17+00:00
- **Updated**: 2021-06-26 08:52:17+00:00
- **Authors**: Tiago Cortinhal, Fatih Kurnaz, Eren Aksoy
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a simple yet effective framework to address the domain translation problem between different sensor modalities with unique data formats. By relying only on the semantics of the scene, our modular generative framework can, for the first time, synthesize a panoramic color image from a given full 3D LiDAR point cloud. The framework starts with semantic segmentation of the point cloud, which is initially projected onto a spherical surface. The same semantic segmentation is applied to the corresponding camera image. Next, our new conditional generative model adversarially learns to translate the predicted LiDAR segment maps to the camera image counterparts. Finally, generated image segments are processed to render the panoramic scene images. We provide a thorough quantitative evaluation on the SemanticKitti dataset and show that our proposed framework outperforms other strong baseline models.   Our source code is available at https://github.com/halmstad-University/TITAN-NET



### Descriptive Modeling of Textiles using FE Simulations and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.13982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.13982v1)
- **Published**: 2021-06-26 09:32:24+00:00
- **Updated**: 2021-06-26 09:32:24+00:00
- **Authors**: Arturo Mendoza, Roger Trullo, Yanneck Wielhorski
- **Comment**: Preprint submitted to Composites Science and Technology. A. Mendoza
  and R. Trullo contributed equally to this work
- **Journal**: None
- **Summary**: In this work we propose a novel and fully automated method for extracting the yarn geometrical features in woven composites so that a direct parametrization of the textile reinforcement is achieved (e.g., FE mesh). Thus, our aim is not only to perform yarn segmentation from tomographic images but rather to provide a complete descriptive modeling of the fabric. As such, this direct approach improves on previous methods that use voxel-wise masks as intermediate representations followed by re-meshing operations (yarn envelope estimation). The proposed approach employs two deep neural network architectures (U-Net and Mask RCNN). First, we train the U-Net to generate synthetic CT images from the corresponding FE simulations. This allows to generate large quantities of annotated data without requiring costly manual annotations. This data is then used to train the Mask R-CNN, which is focused on predicting contour points around each of the yarns in the image. Experimental results show that our method is accurate and robust for performing yarn instance segmentation on CT images, this is further validated by quantitative and qualitative analyses.



### ShapeEditer: a StyleGAN Encoder for Face Swapping
- **Arxiv ID**: http://arxiv.org/abs/2106.13984v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.13984v1)
- **Published**: 2021-06-26 09:38:45+00:00
- **Updated**: 2021-06-26 09:38:45+00:00
- **Authors**: Shuai Yang, Kai Qiao
- **Comment**: 13 pages, 3 figures
- **Journal**: None
- **Summary**: In this paper, we propose a novel encoder, called ShapeEditor, for high-resolution, realistic and high-fidelity face exchange. First of all, in order to ensure sufficient clarity and authenticity, our key idea is to use an advanced pretrained high-quality random face image generator, i.e. StyleGAN, as backbone. Secondly, we design ShapeEditor, a two-step encoder, to make the swapped face integrate the identity and attribute of the input faces. In the first step, we extract the identity vector of the source image and the attribute vector of the target image respectively; in the second step, we map the concatenation of identity vector and attribute vector into the $\mathcal{W+}$ potential space. In addition, for learning to map into the latent space of StyleGAN, we propose a set of self-supervised loss functions with which the training data do not need to be labeled manually. Extensive experiments on the test dataset show that the results of our method not only have a great advantage in clarity and authenticity than other state-of-the-art methods, but also reflect the sufficient integration of identity and attribute.



### Mining atmospheric data
- **Arxiv ID**: http://arxiv.org/abs/2106.13992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.13992v1)
- **Published**: 2021-06-26 10:04:35+00:00
- **Updated**: 2021-06-26 10:04:35+00:00
- **Authors**: Chaabane Djeraba, Jérôme Riedi
- **Comment**: 5 pages, 1 figure
- **Journal**: None
- **Summary**: This paper overviews two interdependent issues important for mining remote sensing data (e.g. images) obtained from atmospheric monitoring missions. The first issue relates the building new public datasets and benchmarks, which are hot priority of the remote sensing community. The second issue is the investigation of deep learning methodologies for atmospheric data classification based on vast amount of data without annotations and with localized annotated data provided by sparse observing networks at the surface. The targeted application is air quality assessment and prediction. Air quality is defined as the pollution level linked with several atmospheric constituents such as gases and aerosols. There are dependency relationships between the bad air quality, caused by air pollution, and the public health. The target application is the development of a fast prediction model for local and regional air quality assessment and tracking. The results of mining data will have significant implication for citizen and decision makers by providing a fast prediction and reliable air quality monitoring system able to cover the local and regional scale through intelligent extrapolation of sparse ground-based in situ measurement networks.



### Semi-Supervised Deep Ensembles for Blind Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2106.14008v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14008v2)
- **Published**: 2021-06-26 11:59:46+00:00
- **Updated**: 2021-06-29 19:36:30+00:00
- **Authors**: Zhihua Wang, Dingquan Li, Kede Ma
- **Comment**: 6 pages, 1 figure, 5 tables
- **Journal**: None
- **Summary**: Ensemble methods are generally regarded to be better than a single model if the base learners are deemed to be "accurate" and "diverse." Here we investigate a semi-supervised ensemble learning strategy to produce generalizable blind image quality assessment models. We train a multi-head convolutional network for quality prediction by maximizing the accuracy of the ensemble (as well as the base learners) on labeled data, and the disagreement (i.e., diversity) among them on unlabeled data, both implemented by the fidelity loss. We conduct extensive experiments to demonstrate the advantages of employing unlabeled data for BIQA, especially in model generalization and failure identification.



### UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.14019v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14019v1)
- **Published**: 2021-06-26 13:27:14+00:00
- **Updated**: 2021-06-26 13:27:14+00:00
- **Authors**: Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Trung Bui, Kyomin Jung
- **Comment**: ACL 2021
- **Journal**: None
- **Summary**: Despite the success of various text generation metrics such as BERTScore, it is still difficult to evaluate the image captions without enough reference captions due to the diversity of the descriptions. In this paper, we introduce a new metric UMIC, an Unreferenced Metric for Image Captioning which does not require reference captions to evaluate image captions. Based on Vision-and-Language BERT, we train UMIC to discriminate negative captions via contrastive learning. Also, we observe critical problems of the previous benchmark dataset (i.e., human annotations) on image captioning metric, and introduce a new collection of human annotations on the generated captions. We validate UMIC on four datasets, including our new dataset, and show that UMIC has a higher correlation than all previous metrics that require multiple references. We release the benchmark dataset and pre-trained models to compute the UMIC.



### BiX-NAS: Searching Efficient Bi-directional Architecture for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.14033v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14033v3)
- **Published**: 2021-06-26 14:33:04+00:00
- **Updated**: 2021-07-01 09:03:36+00:00
- **Authors**: Xinyi Wang, Tiange Xiang, Chaoyi Zhang, Yang Song, Dongnan Liu, Heng Huang, Weidong Cai
- **Comment**: MICCAI2021
- **Journal**: None
- **Summary**: The recurrent mechanism has recently been introduced into U-Net in various medical image segmentation tasks. Existing studies have focused on promoting network recursion via reusing building blocks. Although network parameters could be greatly saved, computational costs still increase inevitably in accordance with the pre-set iteration time. In this work, we study a multi-scale upgrade of a bi-directional skip connected network and then automatically discover an efficient architecture by a novel two-phase Neural Architecture Search (NAS) algorithm, namely BiX-NAS. Our proposed method reduces the network computational cost by sifting out ineffective multi-scale features at different levels and iterations. We evaluate BiX-NAS on two segmentation tasks using three different medical image datasets, and the experimental results show that our BiX-NAS searched architecture achieves the state-of-the-art performance with significantly lower computational cost.



### Identifying High Accuracy Regions in Traffic Camera Images to Enhance the Estimation of Road Traffic Metrics: A Quadtree-Based Method
- **Arxiv ID**: http://arxiv.org/abs/2106.14049v3
- **DOI**: 10.1177/03611981221096117
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2106.14049v3)
- **Published**: 2021-06-26 15:46:45+00:00
- **Updated**: 2022-06-14 14:27:21+00:00
- **Authors**: Yue Lin, Ningchuan Xiao
- **Comment**: Transportation Research Record (2022)
- **Journal**: None
- **Summary**: The growing number of real-time camera feeds in urban areas has made it possible to provide high-quality traffic data for effective transportation planning, operations, and management. However, deriving reliable traffic metrics from these camera feeds has been a challenge due to the limitations of current vehicle detection techniques, as well as the various camera conditions such as height and resolution. In this work, a quadtree based algorithm is developed to continuously partition the image extent until only regions with high detection accuracy are remained. These regions are referred to as the high-accuracy identification regions (HAIR) in this paper. We demonstrate how the use of the HAIR can improve the accuracy of traffic density estimates using images from traffic cameras at different heights and resolutions in Central Ohio. Our experiments show that the proposed algorithm can be used to derive robust HAIR where vehicle detection accuracy is 41 percent higher than that in the original image extent. The use of the HAIR also significantly improves the traffic density estimation with an overall decrease of 49 percent in root mean squared error.



### A Graph-based approach to derive the geodesic distance on Statistical manifolds: Application to Multimedia Information Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2106.14060v1
- **DOI**: 10.1109/WINCOM50532.2020.9272434
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14060v1)
- **Published**: 2021-06-26 16:39:54+00:00
- **Updated**: 2021-06-26 16:39:54+00:00
- **Authors**: Zakariae Abbad, Ahmed Drissi El Maliani, Said Ouatik El Alaoui, Mohammed El Hassouni
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we leverage the properties of non-Euclidean Geometry to define the Geodesic distance (GD) on the space of statistical manifolds. The Geodesic distance is a real and intuitive similarity measure that is a good alternative to the purely statistical and extensively used Kullback-Leibler divergence (KLD). Despite the effectiveness of the GD, a closed-form does not exist for many manifolds, since the geodesic equations are hard to solve. This explains that the major studies have been content to use numerical approximations. Nevertheless, most of those do not take account of the manifold properties, which leads to a loss of information and thus to low performances. We propose an approximation of the Geodesic distance through a graph-based method. This latter permits to well represent the structure of the statistical manifold, and respects its geometrical properties. Our main aim is to compare the graph-based approximation to the state of the art approximations. Thus, the proposed approach is evaluated for two statistical manifolds, namely the Weibull manifold and the Gamma manifold, considering the Content-Based Texture Retrieval application on different databases.



### TANet++: Triple Attention Network with Filtered Pointcloud on 3D Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.15366v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15366v2)
- **Published**: 2021-06-26 16:56:35+00:00
- **Updated**: 2021-07-01 15:42:17+00:00
- **Authors**: Cong Ma
- **Comment**: 3pages, 2 figures
- **Journal**: None
- **Summary**: TANet is one of state-of-the-art 3D object detection method on KITTI and JRDB benchmark, the network contains a Triple Attention module and Coarse-to-Fine Regression module to improve the robustness and accuracy of 3D Detection. However, since the original input data (point clouds) contains a lot of noise during collecting the data, which will further affect the training of the model. For example, the object is far from the robot, the sensor is difficult to obtain enough pointcloud. If the objects only contains few point clouds, and the samples are fed into model with the normal samples together during training, the detector will be difficult to distinguish the individual with few pointcloud belong to object or background. In this paper, we propose TANet++ to improve the performance on 3D Detection, which adopt a novel training strategy on training the TANet. In order to reduce the negative impact by the weak samples, the training strategy previously filtered the training data, and then the TANet++ is trained by the rest of data. The experimental results shows that AP score of TANet++ is 8.98 higher than TANet on JRDB benchmark.



### Saying the Unseen: Video Descriptions via Dialog Agents
- **Arxiv ID**: http://arxiv.org/abs/2106.14069v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14069v1)
- **Published**: 2021-06-26 17:36:31+00:00
- **Updated**: 2021-06-26 17:36:31+00:00
- **Authors**: Ye Zhu, Yu Wu, Yi Yang, Yan Yan
- **Comment**: Accepted as a regular paper at TPAMI 2021
- **Journal**: None
- **Summary**: Current vision and language tasks usually take complete visual data (e.g., raw images or videos) as input, however, practical scenarios may often consist the situations where part of the visual information becomes inaccessible due to various reasons e.g., restricted view with fixed camera or intentional vision block for security concerns. As a step towards the more practical application scenarios, we introduce a novel task that aims to describe a video using the natural language dialog between two agents as a supplementary information source given incomplete visual data. Different from most existing vision-language tasks where AI systems have full access to images or video clips, which may reveal sensitive information such as recognizable human faces or voices, we intentionally limit the visual input for AI systems and seek a more secure and transparent information medium, i.e., the natural language dialog, to supplement the missing visual information. Specifically, one of the intelligent agents - Q-BOT - is given two semantic segmented frames from the beginning and the end of the video, as well as a finite number of opportunities to ask relevant natural language questions before describing the unseen video. A-BOT, the other agent who has access to the entire video, assists Q-BOT to accomplish the goal by answering the asked questions. We introduce two different experimental settings with either a generative (i.e., agents generate questions and answers freely) or a discriminative (i.e., agents select the questions and answers from candidates) internal dialog generation process. With the proposed unified QA-Cooperative networks, we experimentally demonstrate the knowledge transfer process between the two dialog agents and the effectiveness of using the natural language dialog as a supplement for incomplete implicit visions.



### Vision-driven Compliant Manipulation for Reliable, High-Precision Assembly Tasks
- **Arxiv ID**: http://arxiv.org/abs/2106.14070v1
- **DOI**: 10.15607/RSS.2021.XVII.070
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2106.14070v1)
- **Published**: 2021-06-26 17:54:16+00:00
- **Updated**: 2021-06-26 17:54:16+00:00
- **Authors**: Andrew S. Morgan, Bowen Wen, Junchi Liang, Abdeslam Boularias, Aaron M. Dollar, Kostas Bekris
- **Comment**: None
- **Journal**: None
- **Summary**: Highly constrained manipulation tasks continue to be challenging for autonomous robots as they require high levels of precision, typically less than 1mm, which is often incompatible with what can be achieved by traditional perception systems. This paper demonstrates that the combination of state-of-the-art object tracking with passively adaptive mechanical hardware can be leveraged to complete precision manipulation tasks with tight, industrially-relevant tolerances (0.25mm). The proposed control method closes the loop through vision by tracking the relative 6D pose of objects in the relevant workspace. It adjusts the control reference of both the compliant manipulator and the hand to complete object insertion tasks via within-hand manipulation. Contrary to previous efforts for insertion, our method does not require expensive force sensors, precision manipulators, or time-consuming, online learning, which is data hungry. Instead, this effort leverages mechanical compliance and utilizes an object agnostic manipulation model of the hand learned offline, off-the-shelf motion planning, and an RGBD-based object tracker trained solely with synthetic data. These features allow the proposed system to easily generalize and transfer to new tasks and environments. This paper describes in detail the system components and showcases its efficacy with extensive experiments involving tight tolerance peg-in-hole insertion tasks of various geometries as well as open-world constrained placement tasks.



### Interflow: Aggregating Multi-layer Feature Mappings with Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2106.14073v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14073v3)
- **Published**: 2021-06-26 18:22:01+00:00
- **Updated**: 2021-07-13 07:50:20+00:00
- **Authors**: Zhicheng Cai
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Traditionally, CNN models possess hierarchical structures and utilize the feature mapping of the last layer to obtain the prediction output. However, it can be difficulty to settle the optimal network depth and make the middle layers learn distinguished features. This paper proposes the Interflow algorithm specially for traditional CNN models. Interflow divides CNNs into several stages according to the depth and makes predictions by the feature mappings in each stage. Subsequently, we input these prediction branches into a well-designed attention module, which learns the weights of these prediction branches, aggregates them and obtains the final output. Interflow weights and fuses the features learned in both shallower and deeper layers, making the feature information at each stage processed reasonably and effectively, enabling the middle layers to learn more distinguished features, and enhancing the model representation ability. In addition, Interflow can alleviate gradient vanishing problem, lower the difficulty of network depth selection, and lighten possible over-fitting problem by introducing attention mechanism. Besides, it can avoid network degradation as a byproduct. Compared with the original model, the CNN model with Interflow achieves higher test accuracy on multiple benchmark datasets.



### Generalized Zero-Shot Learning using Multimodal Variational Auto-Encoder with Semantic Concepts
- **Arxiv ID**: http://arxiv.org/abs/2106.14082v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.14082v1)
- **Published**: 2021-06-26 20:08:37+00:00
- **Updated**: 2021-06-26 20:08:37+00:00
- **Authors**: Nihar Bendre, Kevin Desai, Peyman Najafirad
- **Comment**: 5 pages, 2 figures, 2 tables
- **Journal**: None
- **Summary**: With the ever-increasing amount of data, the central challenge in multimodal learning involves limitations of labelled samples. For the task of classification, techniques such as meta-learning, zero-shot learning, and few-shot learning showcase the ability to learn information about novel classes based on prior knowledge. Recent techniques try to learn a cross-modal mapping between the semantic space and the image space. However, they tend to ignore the local and global semantic knowledge. To overcome this problem, we propose a Multimodal Variational Auto-Encoder (M-VAE) which can learn the shared latent space of image features and the semantic space. In our approach we concatenate multimodal data to a single embedding before passing it to the VAE for learning the latent space. We propose the use of a multi-modal loss during the reconstruction of the feature embedding through the decoder. Our approach is capable to correlating modalities and exploit the local and global semantic knowledge for novel sample predictions. Our experimental results using a MLP classifier on four benchmark datasets show that our proposed model outperforms the current state-of-the-art approaches for generalized zero-shot learning.



### Radar Voxel Fusion for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.14087v1
- **DOI**: 10.3390/app11125598
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14087v1)
- **Published**: 2021-06-26 20:34:12+00:00
- **Updated**: 2021-06-26 20:34:12+00:00
- **Authors**: Felix Nobis, Ehsan Shafiei, Phillip Karle, Johannes Betz, Markus Lienkamp
- **Comment**: None
- **Journal**: None
- **Summary**: Automotive traffic scenes are complex due to the variety of possible scenarios, objects, and weather conditions that need to be handled. In contrast to more constrained environments, such as automated underground trains, automotive perception systems cannot be tailored to a narrow field of specific tasks but must handle an ever-changing environment with unforeseen events. As currently no single sensor is able to reliably perceive all relevant activity in the surroundings, sensor data fusion is applied to perceive as much information as possible. Data fusion of different sensors and sensor modalities on a low abstraction level enables the compensation of sensor weaknesses and misdetections among the sensors before the information-rich sensor data are compressed and thereby information is lost after a sensor-individual object detection. This paper develops a low-level sensor fusion network for 3D object detection, which fuses lidar, camera, and radar data. The fusion network is trained and evaluated on the nuScenes data set. On the test set, fusion of radar data increases the resulting AP (Average Precision) detection score by about 5.1% in comparison to the baseline lidar network. The radar sensor fusion proves especially beneficial in inclement conditions such as rain and night scenes. Fusing additional camera data contributes positively only in conjunction with the radar fusion, which shows that interdependencies of the sensors are important for the detection result. Additionally, the paper proposes a novel loss to handle the discontinuity of a simple yaw representation for object detection. Our updated loss increases the detection and orientation estimation performance for all sensor input configurations. The code for this research has been made available on GitHub.



### Real-time 3D Object Detection using Feature Map Flow
- **Arxiv ID**: http://arxiv.org/abs/2106.14101v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.14101v1)
- **Published**: 2021-06-26 22:20:31+00:00
- **Updated**: 2021-06-26 22:20:31+00:00
- **Authors**: Youshaa Murhij, Dmitry Yudin
- **Comment**: CVPR 2021 Workshop on autonomous driving (Waymo Real-time 3D
  Detection)
- **Journal**: None
- **Summary**: In this paper, we present a real-time 3D detection approach considering time-spatial feature map aggregation from different time steps of deep neural model inference (named feature map flow, FMF). Proposed approach improves the quality of 3D detection center-based baseline and provides real-time performance on the nuScenes and Waymo benchmark. Code is available at https://github.com/YoushaaMurhij/FMFNet



### Image Classification with CondenseNeXt for ARM-Based Computing Platforms
- **Arxiv ID**: http://arxiv.org/abs/2106.14102v1
- **DOI**: 10.1109/IEMTRONICS52119.2021.9422541
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.14102v1)
- **Published**: 2021-06-26 22:22:03+00:00
- **Updated**: 2021-06-26 22:22:03+00:00
- **Authors**: Priyank Kalgaonkar, Mohamed El-Sharkawy
- **Comment**: 6 pages, 7 figures, conference, published IEEE Conference paper
- **Journal**: None
- **Summary**: In this paper, we demonstrate the implementation of our ultra-efficient deep convolutional neural network architecture: CondenseNeXt on NXP BlueBox, an autonomous driving development platform developed for self-driving vehicles. We show that CondenseNeXt is remarkably efficient in terms of FLOPs, designed for ARM-based embedded computing platforms with limited computational resources and can perform image classification without the need of a CUDA enabled GPU. CondenseNeXt utilizes the state-of-the-art depthwise separable convolution and model compression techniques to achieve a remarkable computational efficiency. Extensive analyses are conducted on CIFAR-10, CIFAR-100 and ImageNet datasets to verify the performance of CondenseNeXt Convolutional Neural Network (CNN) architecture. It achieves state-of-the-art image classification performance on three benchmark datasets including CIFAR-10 (4.79% top-1 error), CIFAR-100 (21.98% top-1 error) and ImageNet (7.91% single model, single crop top-5 error). CondenseNeXt achieves final trained model size improvement of 2.9+ MB and up to 59.98% reduction in forward FLOPs compared to CondenseNet and can perform image classification on ARM-Based computing platforms without needing a CUDA enabled GPU support, with outstanding efficiency.



### Can An Image Classifier Suffice For Action Recognition?
- **Arxiv ID**: http://arxiv.org/abs/2106.14104v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14104v3)
- **Published**: 2021-06-26 22:28:30+00:00
- **Updated**: 2022-04-25 18:34:03+00:00
- **Authors**: Quanfu Fan, Chun-Fu, Chen, Rameswar Panda
- **Comment**: None
- **Journal**: None
- **Summary**: We explore a new perspective on video understanding by casting the video recognition problem as an image recognition task. Our approach rearranges input video frames into super images, which allow for training an image classifier directly to fulfill the task of action recognition, in exactly the same way as image classification. With such a simple idea, we show that transformer-based image classifiers alone can suffice for action recognition. In particular, our approach demonstrates strong and promising performance against SOTA methods on several public datasets including Kinetics400, Moments In Time, Something-Something V2 (SSV2), Jester and Diving48. We also experiment with the prevalent ResNet image classifiers in computer vision to further validate our idea. The results on both Kinetics400 and SSV2 are comparable to some of the best-performed CNN approaches based on spatio-temporal modeling. Our source codes and models are available at https://github.com/IBM/sifar-pytorch.



