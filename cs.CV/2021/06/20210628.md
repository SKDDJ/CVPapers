# Arxiv Papers in cs.CV on 2021-06-28
### The Deep Neural Network based Photometry and Astrometry Framework for Wide Field Small Aperture Telescopes
- **Arxiv ID**: http://arxiv.org/abs/2106.14349v2
- **DOI**: None
- **Categories**: **astro-ph.IM**, astro-ph.GA, astro-ph.SR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14349v2)
- **Published**: 2021-06-28 00:34:15+00:00
- **Updated**: 2022-04-25 14:29:20+00:00
- **Authors**: Peng Jia, Yongyang Sun, Zhimin Yang, Rui Sun, Qiang Liu
- **Comment**: Submitted to the AJ and welcome to any comments. Complete code and
  data can be downloaded from https://zenodo.org/record/6374479
- **Journal**: None
- **Summary**: Wide field small aperture telescopes (WFSATs) are preferable observation instruments for time domain astronomy, because they could obtain images of celestial objects with high cadence in a cost-effective way. An automatic data processing algorithm which could detect celestial objects and obtain their positions and magnitudes from observed images is important for further scientific research. In this paper, we extend the ability of a deep neural network based astronomical target detection algorithm to make it suitable for photometry and astrometry, by adding two new branches. Because the photometry and astrometry neural network are data-driven regression algorithms, limited training data with limited diversity would introduce the epistemic uncertainty to final regression results. Therefore, we further investigate the epistemic uncertainty of our algorithm and have found that differences of background noises and differences of point spread functions between the training data and the real would introduce uncertainties to final measurements. To reduce this effect, we propose to use transfer learning strategy to train the neural network with real data. The algorithm proposed in this paper could obtain types, positions and magnitudes of celestial objects with high accuracy and cost around 0.125 second to process an image, regardless of its size. The algorithm could be integrated into data processing pipelines of WFSATs to increase their response speed and detection ability to time-domain astronomical events.



### Deep Learning Image Recognition for Non-images
- **Arxiv ID**: http://arxiv.org/abs/2106.14350v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14350v2)
- **Published**: 2021-06-28 00:36:36+00:00
- **Updated**: 2022-02-09 22:08:16+00:00
- **Authors**: Boris Kovalerchuk, Divya Chandrika Kalla, Bedant Agarwal
- **Comment**: 33 pages, 17 figures, 18 tables
- **Journal**: None
- **Summary**: Powerful deep learning algorithms open an opportunity for solving non-image Machine Learning (ML) problems by transforming these problems to into the image recognition problems. The CPC-R algorithm presented in this chapter converts non-image data into images by visualizing non-image data. Then deep learning CNN algorithms solve the learning problems on these images. The design of the CPC-R algorithm allows preserving all high-dimensional information in 2-D images. The use of pair values mapping instead of single value mapping used in the alternative approaches allows encoding each n-D point with 2 times fewer visual elements. The attributes of an n-D point are divided into pairs of its values and each pair is visualized as 2-D points in the same 2-D Cartesian coordinates. Next, grey scale or color intensity values are assigned to each pair to encode the order of pairs. This is resulted in the heatmap image. The computational experiments with CPC-R are conducted for different CNN architectures, and methods to optimize the CPC-R images showing that the combined CPC-R and deep learning CNN algorithms are able to solve non-image ML problems reaching high accuracy on the benchmark datasets. This chapter expands our prior work by adding more experiments to test accuracy of classification, exploring saliency and informativeness of discovered features to test their interpretability, and generalizing the approach.



### Rail-5k: a Real-World Dataset for Rail Surface Defects Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.14366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14366v1)
- **Published**: 2021-06-28 01:53:52+00:00
- **Updated**: 2021-06-28 01:53:52+00:00
- **Authors**: Zihao Zhang, Shaozuo Yu, Siwei Yang, Yu Zhou, Bingchen Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents the Rail-5k dataset for benchmarking the performance of visual algorithms in a real-world application scenario, namely the rail surface defects detection task. We collected over 5k high-quality images from railways across China, and annotated 1100 images with the help from railway experts to identify the most common 13 types of rail defects. The dataset can be used for two settings both with unique challenges, the first is the fully-supervised setting using the 1k+ labeled images for training, fine-grained nature and long-tailed distribution of defect classes makes it hard for visual algorithms to tackle. The second is the semi-supervised learning setting facilitated by the 4k unlabeled images, these 4k images are uncurated containing possible image corruptions and domain shift with the labeled images, which can not be easily tackle by previous semi-supervised learning methods. We believe our dataset could be a valuable benchmark for evaluating robustness and reliability of visual algorithms.



### Multi-Compound Transformer for Accurate Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.14385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14385v1)
- **Published**: 2021-06-28 03:45:44+00:00
- **Updated**: 2021-06-28 03:45:44+00:00
- **Authors**: Yuanfeng Ji, Ruimao Zhang, Huijie Wang, Zhen Li, Lingyun Wu, Shaoting Zhang, Ping Luo
- **Comment**: Accepted by MICCAI2021
- **Journal**: None
- **Summary**: The recent vision transformer(i.e.for image classification) learns non-local attentive interaction of different patch tokens. However, prior arts miss learning the cross-scale dependencies of different pixels, the semantic correspondence of different labels, and the consistency of the feature representations and semantic embeddings, which are critical for biomedical segmentation. In this paper, we tackle the above issues by proposing a unified transformer network, termed Multi-Compound Transformer (MCTrans), which incorporates rich feature learning and semantic structure mining into a unified framework. Specifically, MCTrans embeds the multi-scale convolutional features as a sequence of tokens and performs intra- and inter-scale self-attention, rather than single-scale attention in previous works. In addition, a learnable proxy embedding is also introduced to model semantic relationship and feature enhancement by using self-attention and cross-attention, respectively. MCTrans can be easily plugged into a UNet-like network and attains a significant improvement over the state-of-the-art methods in biomedical image segmentation in six standard benchmarks. For example, MCTrans outperforms UNet by 3.64%, 3.71%, 4.34%, 2.8%, 1.88%, 1.57% in Pannuke, CVC-Clinic, CVC-Colon, Etis, Kavirs, ISIC2018 dataset, respectively. Code is available at https://github.com/JiYuanFeng/MCTrans.



### Kimera-Multi: Robust, Distributed, Dense Metric-Semantic SLAM for Multi-Robot Systems
- **Arxiv ID**: http://arxiv.org/abs/2106.14386v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2106.14386v2)
- **Published**: 2021-06-28 03:56:40+00:00
- **Updated**: 2021-12-17 22:20:22+00:00
- **Authors**: Yulun Tian, Yun Chang, Fernando Herrera Arias, Carlos Nieto-Granda, Jonathan P. How, Luca Carlone
- **Comment**: Accepted by IEEE Transactions on Robotics (18 pages, 15 figures)
- **Journal**: None
- **Summary**: This paper presents Kimera-Multi, the first multi-robot system that (i) is robust and capable of identifying and rejecting incorrect inter and intra-robot loop closures resulting from perceptual aliasing, (ii) is fully distributed and only relies on local (peer-to-peer) communication to achieve distributed localization and mapping, and (iii) builds a globally consistent metric-semantic 3D mesh model of the environment in real-time, where faces of the mesh are annotated with semantic labels. Kimera-Multi is implemented by a team of robots equipped with visual-inertial sensors. Each robot builds a local trajectory estimate and a local mesh using Kimera. When communication is available, robots initiate a distributed place recognition and robust pose graph optimization protocol based on a novel distributed graduated non-convexity algorithm. The proposed protocol allows the robots to improve their local trajectory estimates by leveraging inter-robot loop closures while being robust to outliers. Finally, each robot uses its improved trajectory estimate to correct the local mesh using mesh deformation techniques.   We demonstrate Kimera-Multi in photo-realistic simulations, SLAM benchmarking datasets, and challenging outdoor datasets collected using ground robots. Both real and simulated experiments involve long trajectories (e.g., up to 800 meters per robot). The experiments show that Kimera-Multi (i) outperforms the state of the art in terms of robustness and accuracy, (ii) achieves estimation errors comparable to a centralized SLAM system while being fully distributed, (iii) is parsimonious in terms of communication bandwidth, (iv) produces accurate metric-semantic 3D meshes, and (v) is modular and can be also used for standard 3D reconstruction (i.e., without semantic labels) or for trajectory estimation (i.e., without reconstructing a 3D mesh).



### A 3D CNN Network with BERT For Automatic COVID-19 Diagnosis From CT-Scan Images
- **Arxiv ID**: http://arxiv.org/abs/2106.14403v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14403v3)
- **Published**: 2021-06-28 05:35:26+00:00
- **Updated**: 2021-10-04 15:47:38+00:00
- **Authors**: Weijun Tan, Jingfeng Liu
- **Comment**: None
- **Journal**: 2021 ICCV Workshops
- **Summary**: We present an automatic COVID1-19 diagnosis framework from lung CT-scan slice images. In this framework, the slice images of a CT-scan volume are first proprocessed using segmentation techniques to filter out images of closed lung, and to remove the useless background. Then a resampling method is used to select one or multiple sets of a fixed number of slice images for training and validation. A 3D CNN network with BERT is used to classify this set of selected slice images. In this network, an embedding feature is also extracted. In cases where there are more than one set of slice images in a volume, the features of all sets are extracted and pooled into a global feature vector for the whole CT-scan volume. A simple multiple-layer perceptron (MLP) network is used to further classify the aggregated feature vector. The models are trained and evaluated on the provided training and validation datasets. On the validation dataset, the accuracy is 0.9278 and the F1 score is 0.9261.



### Progressive Class-based Expansion Learning For Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.14412v1
- **DOI**: 10.1109/LSP.2021.3094174
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14412v1)
- **Published**: 2021-06-28 06:11:32+00:00
- **Updated**: 2021-06-28 06:11:32+00:00
- **Authors**: Hui Wang, Hanbin Zhao, Xi Li
- **Comment**: Accepted to Journal of IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: In this paper, we propose a novel image process scheme called class-based expansion learning for image classification, which aims at improving the supervision-stimulation frequency for the samples of the confusing classes. Class-based expansion learning takes a bottom-up growing strategy in a class-based expansion optimization fashion, which pays more attention to the quality of learning the fine-grained classification boundaries for the preferentially selected classes. Besides, we develop a class confusion criterion to select the confusing class preferentially for training. In this way, the classification boundaries of the confusing classes are frequently stimulated, resulting in a fine-grained form. Experimental results demonstrate the effectiveness of the proposed scheme on several benchmarks.



### Co$^2$L: Contrastive Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.14413v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14413v1)
- **Published**: 2021-06-28 06:14:38+00:00
- **Updated**: 2021-06-28 06:14:38+00:00
- **Authors**: Hyuntak Cha, Jaeho Lee, Jinwoo Shin
- **Comment**: 14 pages, 5 figures
- **Journal**: None
- **Summary**: Recent breakthroughs in self-supervised learning show that such algorithms learn visual representations that can be transferred better to unseen tasks than joint-training methods relying on task-specific supervision. In this paper, we found that the similar holds in the continual learning con-text: contrastively learned representations are more robust against the catastrophic forgetting than jointly trained representations. Based on this novel observation, we propose a rehearsal-based continual learning algorithm that focuses on continually learning and maintaining transferable representations. More specifically, the proposed scheme (1) learns representations using the contrastive learning objective, and (2) preserves learned representations using a self-supervised distillation step. We conduct extensive experimental validations under popular benchmark image classification datasets, where our method sets the new state-of-the-art performance.



### Prior-Induced Information Alignment for Image Matting
- **Arxiv ID**: http://arxiv.org/abs/2106.14439v1
- **DOI**: 10.1109/TMM.2021.3087007.
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14439v1)
- **Published**: 2021-06-28 07:46:59+00:00
- **Updated**: 2021-06-28 07:46:59+00:00
- **Authors**: Yuhao Liu, Jiake Xie, Yu Qiao, Yong Tang and, Xin Yang
- **Comment**: IEEE TMM
- **Journal**: None
- **Summary**: Image matting is an ill-posed problem that aims to estimate the opacity of foreground pixels in an image. However, most existing deep learning-based methods still suffer from the coarse-grained details. In general, these algorithms are incapable of felicitously distinguishing the degree of exploration between deterministic domains (certain FG and BG pixels) and undetermined domains (uncertain in-between pixels), or inevitably lose information in the continuous sampling process, leading to a sub-optimal result. In this paper, we propose a novel network named Prior-Induced Information Alignment Matting Network (PIIAMatting), which can efficiently model the distinction of pixel-wise response maps and the correlation of layer-wise feature maps. It mainly consists of a Dynamic Gaussian Modulation mechanism (DGM) and an Information Alignment strategy (IA). Specifically, the DGM can dynamically acquire a pixel-wise domain response map learned from the prior distribution. The response map can present the relationship between the opacity variation and the convergence process during training. On the other hand, the IA comprises an Information Match Module (IMM) and an Information Aggregation Module (IAM), jointly scheduled to match and aggregate the adjacent layer-wise features adaptively. Besides, we also develop a Multi-Scale Refinement (MSR) module to integrate multi-scale receptive field information at the refinement stage to recover the fluctuating appearance details. Extensive quantitative and qualitative evaluations demonstrate that the proposed PIIAMatting performs favourably against state-of-the-art image matting methods on the Alphamatting.com, Composition-1K and Distinctions-646 dataset.



### VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects
- **Arxiv ID**: http://arxiv.org/abs/2106.14440v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.14440v2)
- **Published**: 2021-06-28 07:47:31+00:00
- **Updated**: 2022-04-01 13:31:19+00:00
- **Authors**: Ruihai Wu, Yan Zhao, Kaichun Mo, Zizheng Guo, Yian Wang, Tianhao Wu, Qingnan Fan, Xuelin Chen, Leonidas Guibas, Hao Dong
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: Perceiving and manipulating 3D articulated objects (e.g., cabinets, doors) in human environments is an important yet challenging task for future home-assistant robots. The space of 3D articulated objects is exceptionally rich in their myriad semantic categories, diverse shape geometry, and complicated part functionality. Previous works mostly abstract kinematic structure with estimated joint parameters and part poses as the visual representations for manipulating 3D articulated objects. In this paper, we propose object-centric actionable visual priors as a novel perception-interaction handshaking point that the perception system outputs more actionable guidance than kinematic structure estimation, by predicting dense geometry-aware, interaction-aware, and task-aware visual action affordance and trajectory proposals. We design an interaction-for-perception framework VAT-Mart to learn such actionable visual representations by simultaneously training a curiosity-driven reinforcement learning policy exploring diverse interaction trajectories and a perception module summarizing and generalizing the explored knowledge for pointwise predictions among diverse shapes. Experiments prove the effectiveness of the proposed approach using the large-scale PartNet-Mobility dataset in SAPIEN environment and show promising generalization capabilities to novel test shapes, unseen object categories, and real-world data. Project page: https://hyperplane-lab.github.io/vat-mart



### Feature Combination Meets Attention: Baidu Soccer Embeddings and Transformer based Temporal Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.14447v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.14447v1)
- **Published**: 2021-06-28 08:00:21+00:00
- **Updated**: 2021-06-28 08:00:21+00:00
- **Authors**: Xin Zhou, Le Kang, Zhiyu Cheng, Bo He, Jingyu Xin
- **Comment**: Tech Report. Authors Xin Zhou, Le Kang, and Zhiyu Cheng made equal
  contributions
- **Journal**: None
- **Summary**: With rapidly evolving internet technologies and emerging tools, sports related videos generated online are increasing at an unprecedentedly fast pace. To automate sports video editing/highlight generation process, a key task is to precisely recognize and locate the events in the long untrimmed videos. In this tech report, we present a two-stage paradigm to detect what and when events happen in soccer broadcast videos. Specifically, we fine-tune multiple action recognition models on soccer data to extract high-level semantic features, and design a transformer based temporal detection module to locate the target events. This approach achieved the state-of-the-art performance in both two tasks, i.e., action spotting and replay grounding, in the SoccerNet-v2 Challenge, under CVPR 2021 ActivityNet workshop. Our soccer embedding features are released at https://github.com/baidu-research/vidpress-sports. By sharing these features with the broader community, we hope to accelerate the research into soccer video understanding.



### Efficient Realistic Data Generation Framework leveraging Deep Learning-based Human Digitization
- **Arxiv ID**: http://arxiv.org/abs/2106.15409v2
- **DOI**: 10.1007/978-3-030-80568-5
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15409v2)
- **Published**: 2021-06-28 08:07:31+00:00
- **Updated**: 2021-06-30 20:05:23+00:00
- **Authors**: C. Symeonidis, P. Nousi, P. Tosidis, K. Tsampazis, N. Passalis, A. Tefas, N. Nikolaidis
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of supervised deep learning algorithms depends significantly on the scale, quality and diversity of the data used for their training. Collecting and manually annotating large amount of data can be both time-consuming and costly tasks to perform. In the case of tasks related to visual human-centric perception, the collection and distribution of such data may also face restrictions due to legislation regarding privacy. In addition, the design and testing of complex systems, e.g., robots, which often employ deep learning-based perception models, may face severe difficulties as even state-of-the-art methods trained on real and large-scale datasets cannot always perform adequately due to not having been adapted to the visual differences between the virtual and the real world data. As an attempt to tackle and mitigate the effect of these issues, we present a method that automatically generates realistic synthetic data with annotations for a) person detection, b) face recognition, and c) human pose estimation. The proposed method takes as input real background images and populates them with human figures in various poses. Instead of using hand-made 3D human models, we propose the use of models generated through deep learning methods, further reducing the dataset creation costs, while maintaining a high level of realism. In addition, we provide open-source and easy to use tools that implement the proposed pipeline, allowing for generating highly-realistic synthetic datasets for a variety of tasks. A benchmarking and evaluation in the corresponding tasks shows that synthetic data can be effectively used as a supplement to real data.



### Recurrent neural network transducer for Japanese and Chinese offline handwritten text recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.14459v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14459v1)
- **Published**: 2021-06-28 08:16:44+00:00
- **Updated**: 2021-06-28 08:16:44+00:00
- **Authors**: Trung Tan Ngo, Hung Tuan Nguyen, Nam Tuan Ly, Masaki Nakagawa
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an RNN-Transducer model for recognizing Japanese and Chinese offline handwritten text line images. As far as we know, it is the first approach that adopts the RNN-Transducer model for offline handwritten text recognition. The proposed model consists of three main components: a visual feature encoder that extracts visual features from an input image by CNN and then encodes the visual features by BLSTM; a linguistic context encoder that extracts and encodes linguistic features from the input image by embedded layers and LSTM; and a joint decoder that combines and then decodes the visual features and the linguistic features into the final label sequence by fully connected and softmax layers. The proposed model takes advantage of both visual and linguistic information from the input image. In the experiments, we evaluated the performance of the proposed model on the two datasets: Kuzushiji and SCUT-EPT. Experimental results show that the proposed model achieves state-of-the-art performance on all datasets.



### Exploring convolutional neural networks with transfer learning for diagnosing Lyme disease from skin lesion images
- **Arxiv ID**: http://arxiv.org/abs/2106.14465v2
- **DOI**: 10.1016/j.cmpb.2022.106624
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.14465v2)
- **Published**: 2021-06-28 08:28:21+00:00
- **Updated**: 2022-02-15 09:17:08+00:00
- **Authors**: Sk Imran Hossain, Jocelyn de Goër de Herve, Md Shahriar Hassan, Delphine Martineau, Evelina Petrosyan, Violaine Corbain, Jean Beytout, Isabelle Lebert, Elisabeth Baux, Céline Cazorla, Carole Eldin, Yves Hansmann, Solene Patrat-Delon, Thierry Prazuck, Alice Raffetin, Pierre Tattevin, Gwenaël Vourc'H, Olivier Lesens, Engelbert Nguifo
- **Comment**: None
- **Journal**: None
- **Summary**: Lyme disease which is one of the most common infectious vector-borne diseases manifests itself in most cases with erythema migrans (EM) skin lesions. Recent studies show that convolutional neural networks (CNNs) perform well to identify skin lesions from images. Lightweight CNN based pre-scanner applications for resource-constrained mobile devices can help users with early diagnosis of Lyme disease and prevent the transition to a severe late form thanks to appropriate antibiotic therapy. Also, resource-intensive CNN based robust computer applications can assist non-expert practitioners with an accurate diagnosis. The main objective of this study is to extensively analyze the effectiveness of CNNs for diagnosing Lyme disease from images and to find out the best CNN architectures considering resource constraints. First, we created an EM dataset with the help of expert dermatologists from Clermont-Ferrand University Hospital Center of France. Second, we benchmarked this dataset for twenty-three CNN architectures customized from VGG, ResNet, DenseNet, MobileNet, Xception, NASNet, and EfficientNet architectures in terms of predictive performance, computational complexity, and statistical significance. Third, to improve the performance of the CNNs, we used custom transfer learning from ImageNet pre-trained models as well as pre-trained the CNNs with the skin lesion dataset HAM10000. Fourth, for model explainability, we utilized Gradient-weighted Class Activation Mapping to visualize the regions of input that are significant to the CNNs for making predictions. Fifth, we provided guidelines for model selection based on predictive performance and computational complexity.



### Dizygotic Conditional Variational AutoEncoder for Multi-Modal and Partial Modality Absent Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.14467v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.14467v1)
- **Published**: 2021-06-28 08:29:55+00:00
- **Updated**: 2021-06-28 08:29:55+00:00
- **Authors**: Yi Zhang, Sheng Huang, Xi Peng, Dan Yang
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Data augmentation is a powerful technique for improving the performance of the few-shot classification task. It generates more samples as supplements, and then this task can be transformed into a common supervised learning issue for solution. However, most mainstream data augmentation based approaches only consider the single modality information, which leads to the low diversity and quality of generated features. In this paper, we present a novel multi-modal data augmentation approach named Dizygotic Conditional Variational AutoEncoder (DCVAE) for addressing the aforementioned issue. DCVAE conducts feature synthesis via pairing two Conditional Variational AutoEncoders (CVAEs) with the same seed but different modality conditions in a dizygotic symbiosis manner. Subsequently, the generated features of two CVAEs are adaptively combined to yield the final feature, which can be converted back into its paired conditions while ensuring these conditions are consistent with the original conditions not only in representation but also in function. DCVAE essentially provides a new idea of data augmentation in various multi-modal scenarios by exploiting the complement of different modality prior information. Extensive experimental results demonstrate our work achieves state-of-the-art performances on miniImageNet, CIFAR-FS and CUB datasets, and is able to work well in the partial modality absence case.



### False Negative Reduction in Video Instance Segmentation using Uncertainty Estimates
- **Arxiv ID**: http://arxiv.org/abs/2106.14474v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14474v1)
- **Published**: 2021-06-28 08:38:55+00:00
- **Updated**: 2021-06-28 08:38:55+00:00
- **Authors**: Kira Maag
- **Comment**: None
- **Journal**: None
- **Summary**: Instance segmentation of images is an important tool for automated scene understanding. Neural networks are usually trained to optimize their overall performance in terms of accuracy. Meanwhile, in applications such as automated driving, an overlooked pedestrian seems more harmful than a falsely detected one. In this work, we present a false negative detection method for image sequences based on inconsistencies in time series of tracked instances given the availability of image sequences in online applications. As the number of instances can be greatly increased by this algorithm, we apply a false positive pruning using uncertainty estimates aggregated over instances. To this end, instance-wise metrics are constructed which characterize uncertainty and geometry of a given instance or are predicated on depth estimation. The proposed method serves as a post-processing step applicable to any neural network that can also be trained on single frames only. In our tests, we obtain an improved trade-off between false negative and false positive instances by our fused detection approach in comparison to the use of an ordinary score value provided by the instance segmentation network during inference.



### A More Compact Object Detector Head Network with Feature Enhancement and Relational Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2106.14475v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14475v2)
- **Published**: 2021-06-28 08:38:57+00:00
- **Updated**: 2021-07-03 16:08:12+00:00
- **Authors**: Wenchao Zhang, Chong Fu, Xiangshi Chang, Tengfei Zhao, Xiang Li, Chiu-Wing Sham
- **Comment**: None
- **Journal**: None
- **Summary**: Modeling implicit feature interaction patterns is of significant importance to object detection tasks. However, in the two-stage detectors, due to the excessive use of hand-crafted components, it is very difficult to reason about the implicit relationship of the instance features. To tackle this problem, we analyze three different levels of feature interaction relationships, namely, the dependency relationship between the cropped local features and global features, the feature autocorrelation within the instance, and the cross-correlation relationship between the instances. To this end, we propose a more compact object detector head network (CODH), which can not only preserve global context information and condense the information density, but also allows instance-wise feature enhancement and relational reasoning in a larger matrix space. Without bells and whistles, our method can effectively improve the detection performance while significantly reducing the parameters of the model, e.g., with our method, the parameters of the head network is 0.6 times smaller than the state-of-the-art Cascade R-CNN, yet the performance boost is 1.3% on COCO test-dev. Without losing generality, we can also build a more lighter head network for other multi-stage detectors by assembling our method.



### Adventurer's Treasure Hunt: A Transparent System for Visually Grounded Compositional Visual Question Answering based on Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/2106.14476v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14476v1)
- **Published**: 2021-06-28 08:39:34+00:00
- **Updated**: 2021-06-28 08:39:34+00:00
- **Authors**: Daniel Reich, Felix Putze, Tanja Schultz
- **Comment**: None
- **Journal**: None
- **Summary**: With the expressed goal of improving system transparency and visual grounding in the reasoning process in VQA, we present a modular system for the task of compositional VQA based on scene graphs. Our system is called "Adventurer's Treasure Hunt" (or ATH), named after an analogy we draw between our model's search procedure for an answer and an adventurer's search for treasure. We developed ATH with three characteristic features in mind: 1. By design, ATH allows us to explicitly quantify the impact of each of the sub-components on overall VQA performance, as well as their performance on their individual sub-task. 2. By modeling the search task after a treasure hunt, ATH inherently produces an explicit, visually grounded inference path for the processed question. 3. ATH is the first GQA-trained VQA system that dynamically extracts answers by querying the visual knowledge base directly, instead of selecting one from a specially learned classifier's output distribution over a pre-fixed answer vocabulary. We report detailed results on all components and their contributions to overall VQA performance on the GQA dataset and show that ATH achieves the highest visual grounding score among all examined systems.



### Cheating Detection Pipeline for Online Interviews and Exams
- **Arxiv ID**: http://arxiv.org/abs/2106.14483v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.14483v2)
- **Published**: 2021-06-28 08:52:20+00:00
- **Updated**: 2021-11-14 12:02:10+00:00
- **Authors**: Azmi Can Özgen, Mahiye Uluyağmur Öztürk, Umut Bayraktar
- **Comment**: None
- **Journal**: None
- **Summary**: Remote examination and job interviews have gained popularity and become indispensable because of both pandemics and the advantage of remote working circumstances. Most companies and academic institutions utilize these systems for their recruitment processes and also for online exams. However, one of the critical problems of the remote examination systems is conducting the exams in a reliable environment. In this work, we present a cheating analysis pipeline for online interviews and exams. The system only requires a video of the candidate, which is recorded during the exam. Then cheating detection pipeline is employed to detect another person, electronic device usage, and candidate absence status. The pipeline consists of face detection, face recognition, object detection, and face tracking algorithms. To evaluate the performance of the pipeline we collected a private video dataset. The video dataset includes both cheating activities and clean videos. Ultimately, our pipeline presents an efficient and fast guideline to detect and analyze cheating activities in an online interview and exam video.



### Making Images Real Again: A Comprehensive Survey on Deep Image Composition
- **Arxiv ID**: http://arxiv.org/abs/2106.14490v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14490v3)
- **Published**: 2021-06-28 09:09:14+00:00
- **Updated**: 2023-08-07 04:47:05+00:00
- **Authors**: Li Niu, Wenyan Cong, Liu Liu, Yan Hong, Bo Zhang, Jing Liang, Liqing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: As a common image editing operation, image composition aims to combine the foreground from one image and another background image, resulting in a composite image. However, there are many issues that could make the composite images unrealistic. These issues can be summarized as the inconsistency between foreground and background, which includes appearance inconsistency (e.g., incompatible illumination), geometry inconsistency (e.g., unreasonable size), and semantic inconsistency (e.g., mismatched semantic context). Image composition task could be decomposed into multiple sub-tasks, in which each sub-task targets at one or more issues. Specifically, object placement aims to find reasonable scale, location, and shape for the foreground. Image blending aims to address the unnatural boundary between foreground and background. Image harmonization aims to adjust the illumination statistics of foreground. Shadow generation aims to generate plausible shadow for the foreground. These sub-tasks can be executed sequentially or parallelly to acquire realistic composite images. To the best of our knowledge, there is no previous survey on image composition. In this paper, we conduct comprehensive survey over the sub-tasks and combinatorial task of image composition. For each one, we summarize the existing methods, available datasets, and common evaluation metrics. Datasets and codes for image composition are summarized at https://github.com/bcmi/Awesome-Image-Composition.



### R2RNet: Low-light Image Enhancement via Real-low to Real-normal Network
- **Arxiv ID**: http://arxiv.org/abs/2106.14501v2
- **DOI**: 10.1016/j.jvcir.2022.103712
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14501v2)
- **Published**: 2021-06-28 09:33:13+00:00
- **Updated**: 2021-11-12 02:47:37+00:00
- **Authors**: Jiang Hai, Zhu Xuan, Songchen Han, Ren Yang, Yutong Hao, Fengzhu Zou, Fang Lin
- **Comment**: 12 pages, 9 figures
- **Journal**: Journal of Visual Communication and Image Representation, 2022
- **Summary**: Images captured in weak illumination conditions could seriously degrade the image quality. Solving a series of degradation of low-light images can effectively improve the visual quality of images and the performance of high-level visual tasks. In this study, a novel Retinex-based Real-low to Real-normal Network (R2RNet) is proposed for low-light image enhancement, which includes three subnets: a Decom-Net, a Denoise-Net, and a Relight-Net. These three subnets are used for decomposing, denoising, contrast enhancement and detail preservation, respectively. Our R2RNet not only uses the spatial information of the image to improve the contrast but also uses the frequency information to preserve the details. Therefore, our model acheived more robust results for all degraded images. Unlike most previous methods that were trained on synthetic images, we collected the first Large-Scale Real-World paired low/normal-light images dataset (LSRW dataset) to satisfy the training requirements and make our model have better generalization performance in real-world scenes. Extensive experiments on publicly available datasets demonstrated that our method outperforms the existing state-of-the-art methods both quantitatively and visually. In addition, our results showed that the performance of the high-level visual task (i.e. face detection) can be effectively improved by using the enhanced results obtained by our method in low-light conditions. Our codes and the LSRW dataset are available at: https://github.com/abcdef2000/R2RNet.



### A Diffeomorphic Aging Model for Adult Human Brain from Cross-Sectional Data
- **Arxiv ID**: http://arxiv.org/abs/2106.14516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14516v1)
- **Published**: 2021-06-28 10:04:05+00:00
- **Updated**: 2021-06-28 10:04:05+00:00
- **Authors**: Alphin J Thottupattu, Jayanthi Sivaswamy, Venkateswaran P. Krishnan
- **Comment**: None
- **Journal**: None
- **Summary**: Normative aging trends of the brain can serve as an important reference in the assessment of neurological structural disorders. Such models are typically developed from longitudinal brain image data -- follow-up data of the same subject over different time points. In practice, obtaining such longitudinal data is difficult. We propose a method to develop an aging model for a given population, in the absence of longitudinal data, by using images from different subjects at different time points, the so-called cross-sectional data. We define an aging model as a diffeomorphic deformation on a structural template derived from the data and propose a method that develops topology preserving aging model close to natural aging. The proposed model is successfully validated on two public cross-sectional datasets which provide templates constructed from different sets of subjects at different age points.



### Contrastive Counterfactual Visual Explanations With Overdetermination
- **Arxiv ID**: http://arxiv.org/abs/2106.14556v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.14556v3)
- **Published**: 2021-06-28 10:24:17+00:00
- **Updated**: 2022-06-09 11:34:45+00:00
- **Authors**: Adam White, Kwun Ho Ngan, James Phelan, Saman Sadeghi Afgeh, Kevin Ryan, Constantino Carlos Reyes-Aldasoro, Artur d'Avila Garcez
- **Comment**: None
- **Journal**: None
- **Summary**: A novel explainable AI method called CLEAR Image is introduced in this paper. CLEAR Image is based on the view that a satisfactory explanation should be contrastive, counterfactual and measurable. CLEAR Image explains an image's classification probability by contrasting the image with a corresponding image generated automatically via adversarial learning. This enables both salient segmentation and perturbations that faithfully determine each segment's importance. CLEAR Image was successfully applied to a medical imaging case study where it outperformed methods such as Grad-CAM and LIME by an average of 27% using a novel pointing game metric. CLEAR Image excels in identifying cases of "causal overdetermination" where there are multiple patches in an image, any one of which is sufficient by itself to cause the classification probability to be close to one.



### Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity
- **Arxiv ID**: http://arxiv.org/abs/2106.14568v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14568v4)
- **Published**: 2021-06-28 10:48:20+00:00
- **Updated**: 2022-02-07 12:21:13+00:00
- **Authors**: Shiwei Liu, Tianlong Chen, Zahra Atashgahi, Xiaohan Chen, Ghada Sokar, Elena Mocanu, Mykola Pechenizkiy, Zhangyang Wang, Decebal Constantin Mocanu
- **Comment**: published in International Conference on Learning Representations
  (ICLR 2022)
- **Journal**: Proceedings of the International Conference on Machine Learning
  (ICLR 2022)
- **Summary**: The success of deep ensembles on improving predictive performance, uncertainty estimation, and out-of-distribution robustness has been extensively studied in the machine learning literature. Albeit the promising results, naively training multiple deep neural networks and combining their predictions at inference leads to prohibitive computational costs and memory requirements. Recently proposed efficient ensemble approaches reach the performance of the traditional deep ensembles with significantly lower costs. However, the training resources required by these approaches are still at least the same as training a single dense model. In this work, we draw a unique connection between sparse neural network training and deep ensembles, yielding a novel efficient ensemble learning framework called FreeTickets. Instead of training multiple dense networks and averaging them, we directly train sparse subnetworks from scratch and extract diverse yet accurate subnetworks during this efficient, sparse-to-sparse training. Our framework, FreeTickets, is defined as the ensemble of these relatively cheap sparse subnetworks. Despite being an ensemble method, FreeTickets has even fewer parameters and training FLOPs than a single dense model. This seemingly counter-intuitive outcome is due to the ultra training/inference efficiency of dynamic sparse training. FreeTickets surpasses the dense baseline in all the following criteria: prediction accuracy, uncertainty estimation, out-of-distribution (OoD) robustness, as well as efficiency for both training and inference. Impressively, FreeTickets outperforms the naive deep ensemble with ResNet50 on ImageNet using around only 1/5 of the training FLOPs required by the latter. We have released our source code at https://github.com/VITA-Group/FreeTickets.



### Privacy-Preserving Image Acquisition Using Trainable Optical Kernel
- **Arxiv ID**: http://arxiv.org/abs/2106.14577v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV, I.2.10; I.5.0
- **Links**: [PDF](http://arxiv.org/pdf/2106.14577v1)
- **Published**: 2021-06-28 11:08:14+00:00
- **Updated**: 2021-06-28 11:08:14+00:00
- **Authors**: Yamin Sepehri, Pedram Pad, Pascal Frossard, L. Andrea Dunbar
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: Preserving privacy is a growing concern in our society where sensors and cameras are ubiquitous. In this work, for the first time, we propose a trainable image acquisition method that removes the sensitive identity revealing information in the optical domain before it reaches the image sensor. The method benefits from a trainable optical convolution kernel which transmits the desired information while filters out the sensitive content. As the sensitive content is suppressed before it reaches the image sensor, it does not enter the digital domain therefore is unretrievable by any sort of privacy attack. This is in contrast with the current digital privacy-preserving methods that are all vulnerable to direct access attack. Also, in contrast with the previous optical privacy-preserving methods that cannot be trained, our method is data-driven and optimized for the specific application at hand. Moreover, there is no additional computation, memory, or power burden on the acquisition system since this processing happens passively in the optical domain and can even be used together and on top of the fully digital privacy-preserving systems. The proposed approach is adaptable to different digital neural networks and content. We demonstrate it for several scenarios such as smile detection as the desired attribute while the gender is filtered out as the sensitive content. We trained the optical kernel in conjunction with two adversarial neural networks where the analysis network tries to detect the desired attribute and the adversarial network tries to detect the sensitive content. We show that this method can reduce 65.1% of sensitive content when it is selected to be the gender and it only loses 7.3% of the desired content. Moreover, we reconstruct the original faces using the deep reconstruction method that confirms the ineffectiveness of reconstruction attacks to obtain the sensitive content.



### Recent Advances in Fibrosis and Scar Segmentation from Cardiac MRI: A State-of-the-Art Review and Future Perspectives
- **Arxiv ID**: http://arxiv.org/abs/2106.15707v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2106.15707v1)
- **Published**: 2021-06-28 11:30:35+00:00
- **Updated**: 2021-06-28 11:30:35+00:00
- **Authors**: Yinzhe Wu, Zeyu Tang, Binghuan Li, David Firmin, Guang Yang
- **Comment**: 3 figure, 8 tables, 46 pages
- **Journal**: None
- **Summary**: Segmentation of cardiac fibrosis and scar are essential for clinical diagnosis and can provide invaluable guidance for the treatment of cardiac diseases. Late Gadolinium enhancement (LGE) cardiovascular magnetic resonance (CMR) has been successful for its efficacy in guiding the clinical diagnosis and treatment reliably. For LGE CMR, many methods have demonstrated success in accurately segmenting scarring regions. Co-registration with other non-contrast-agent (non-CA) modalities, balanced steady-state free precession (bSSFP) and cine magnetic resonance imaging (MRI) for example, can further enhance the efficacy of automated segmentation of cardiac anatomies. Many conventional methods have been proposed to provide automated or semi-automated segmentation of scars. With the development of deep learning in recent years, we can also see more advanced methods that are more efficient in providing more accurate segmentations. This paper conducts a state-of-the-art review of conventional and current state-of-the-art approaches utilising different modalities for accurate cardiac fibrosis and scar segmentation.



### ACN: Adversarial Co-training Network for Brain Tumor Segmentation with Missing Modalities
- **Arxiv ID**: http://arxiv.org/abs/2106.14591v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14591v2)
- **Published**: 2021-06-28 11:53:11+00:00
- **Updated**: 2021-06-29 08:08:11+00:00
- **Authors**: Yixin Wang, Yang Zhang, Yang Liu, Zihao Lin, Jiang Tian, Cheng Zhong, Zhongchao Shi, Jianping Fan, Zhiqiang He
- **Comment**: MICCAI 2021
- **Journal**: None
- **Summary**: Accurate segmentation of brain tumors from magnetic resonance imaging (MRI) is clinically relevant in diagnoses, prognoses and surgery treatment, which requires multiple modalities to provide complementary morphological and physiopathologic information. However, missing modality commonly occurs due to image corruption, artifacts, different acquisition protocols or allergies to certain contrast agents in clinical practice. Though existing efforts demonstrate the possibility of a unified model for all missing situations, most of them perform poorly when more than one modality is missing. In this paper, we propose a novel Adversarial Co-training Network (ACN) to solve this issue, in which a series of independent yet related models are trained dedicated to each missing situation with significantly better results. Specifically, ACN adopts a novel co-training network, which enables a coupled learning process for both full modality and missing modality to supplement each other's domain and feature representations, and more importantly, to recover the `missing' information of absent modalities. Then, two unsupervised modules, i.e., entropy and knowledge adversarial learning modules are proposed to minimize the domain gap while enhancing prediction reliability and encouraging the alignment of latent representations, respectively. We also adapt modality-mutual information knowledge transfer learning to ACN to retain the rich mutual information among modalities. Extensive experiments on BraTS2018 dataset show that our proposed method significantly outperforms all state-of-the-art methods under any missing situation.



### Dataset and Benchmarking of Real-Time Embedded Object Detection for RoboCup SSL
- **Arxiv ID**: http://arxiv.org/abs/2106.14597v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.14597v1)
- **Published**: 2021-06-28 12:09:36+00:00
- **Updated**: 2021-06-28 12:09:36+00:00
- **Authors**: Roberto Fernandes, Walber M. Rodrigues, Edna Barros
- **Comment**: None
- **Journal**: None
- **Summary**: When producing a model to object detection in a specific context, the first obstacle is to have a dataset labeling the desired classes. In RoboCup, some leagues already have more than one dataset to train and evaluate a model. However, in the Small Size League (SSL), there is not such dataset available yet. This paper presents an open-source dataset to be used as a benchmark for real-time object detection in SSL. This work also presented a pipeline to train, deploy, and evaluate Convolutional Neural Networks (CNNs) models in a low-power embedded system. This pipeline was used to evaluate the proposed dataset with state-of-art optimized models. In this dataset, the MobileNet SSD v1 achieves 44.88% AP (68.81% AP50) at 94 Frames Per Second (FPS) while running on an SSL robot.



### Fractal Pyramid Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.14694v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.14694v1)
- **Published**: 2021-06-28 13:15:30+00:00
- **Updated**: 2021-06-28 13:15:30+00:00
- **Authors**: Zhiqiang Deng, Huimin Yu, Yangqi Long
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new network architecture, the Fractal Pyramid Networks (PFNs) for pixel-wise prediction tasks as an alternative to the widely used encoder-decoder structure. In the encoder-decoder structure, the input is processed by an encoding-decoding pipeline that tries to get a semantic large-channel feature. Different from that, our proposed PFNs hold multiple information processing pathways and encode the information to multiple separate small-channel features. On the task of self-supervised monocular depth estimation, even without ImageNet pretrained, our models can compete or outperform the state-of-the-art methods on the KITTI dataset with much fewer parameters. Moreover, the visual quality of the prediction is significantly improved. The experiment of semantic segmentation provides evidence that the PFNs can be applied to other pixel-wise prediction tasks, and demonstrates that our models can catch more global structure information.



### Fast computation of mutual information in the frequency domain with applications to global multimodal image alignment
- **Arxiv ID**: http://arxiv.org/abs/2106.14699v1
- **DOI**: 10.1016/j.patrec.2022.05.022
- **Categories**: **cs.CV**, eess.IV, 92C55, 94A08, 94A15, 94A17, 68U10, 68W01
- **Links**: [PDF](http://arxiv.org/pdf/2106.14699v1)
- **Published**: 2021-06-28 13:27:05+00:00
- **Updated**: 2021-06-28 13:27:05+00:00
- **Authors**: Johan Öfverstedt, Joakim Lindblad, Nataša Sladoje
- **Comment**: 7 pages, 4 figures, 2 tables. The article is under consideration at
  Pattern Recognition Letters
- **Journal**: Pattern Recognition Letters, Vol. 159, pp. 196-203, 2022
- **Summary**: Multimodal image alignment is the process of finding spatial correspondences between images formed by different imaging techniques or under different conditions, to facilitate heterogeneous data fusion and correlative analysis. The information-theoretic concept of mutual information (MI) is widely used as a similarity measure to guide multimodal alignment processes, where most works have focused on local maximization of MI that typically works well only for small displacements; this points to a need for global maximization of MI, which has previously been computationally infeasible due to the high run-time complexity of existing algorithms. We propose an efficient algorithm for computing MI for all discrete displacements (formalized as the cross-mutual information function (CMIF)), which is based on cross-correlation computed in the frequency domain. We show that the algorithm is equivalent to a direct method while asymptotically superior in terms of run-time. Furthermore, we propose a method for multimodal image alignment for transformation models with few degrees of freedom (e.g. rigid) based on the proposed CMIF-algorithm. We evaluate the efficacy of the proposed method on three distinct benchmark datasets, of aerial images, cytological images, and histological images, and we observe excellent success-rates (in recovering known rigid transformations), overall outperforming alternative methods, including local optimization of MI as well as several recent deep learning-based approaches. We also evaluate the run-times of a GPU implementation of the proposed algorithm and observe speed-ups from 100 to more than 10,000 times for realistic image sizes compared to a GPU implementation of a direct method. Code is shared as open-source at \url{github.com/MIDA-group/globalign}.



### Motion Projection Consistency Based 3D Human Pose Estimation with Virtual Bones from Monocular Videos
- **Arxiv ID**: http://arxiv.org/abs/2106.14706v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14706v2)
- **Published**: 2021-06-28 13:37:57+00:00
- **Updated**: 2022-09-14 20:46:38+00:00
- **Authors**: Guangming Wang, Honghao Zeng, Ziliang Wang, Zhe Liu, Hesheng Wang
- **Comment**: 10 pages, 7 figures. Accepted by TCDS 2022
- **Journal**: None
- **Summary**: Real-time 3D human pose estimation is crucial for human-computer interaction. It is cheap and practical to estimate 3D human pose only from monocular video. However, recent bone splicing based 3D human pose estimation method brings about the problem of cumulative error. In this paper, the concept of virtual bones is proposed to solve such a challenge. The virtual bones are imaginary bones between non-adjacent joints. They do not exist in reality, but they bring new loop constraints for the estimation of 3D human joints. The proposed network in this paper predicts real bones and virtual bones, simultaneously. The final length of real bones is constrained and learned by the loop constructed by the predicted real bones and virtual bones. Besides, the motion constraints of joints in consecutive frames are considered. The consistency between the 2D projected position displacement predicted by the network and the captured real 2D displacement by the camera is proposed as a new projection consistency loss for the learning of 3D human pose. The experiments on the Human3.6M dataset demonstrate the good performance of the proposed method. Ablation studies demonstrate the effectiveness of the proposed inter-frame projection consistency constraints and intra-frame loop constraints.



### Weighted multi-level deep learning analysis and framework for processing breast cancer WSIs
- **Arxiv ID**: http://arxiv.org/abs/2106.14708v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.6; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2106.14708v1)
- **Published**: 2021-06-28 13:38:11+00:00
- **Updated**: 2021-06-28 13:38:11+00:00
- **Authors**: Peter Bokor, Lukas Hudec, Ondrej Fabian, Wanda Benesova
- **Comment**: 9 pages, 12 images, 3 tables with results, We have an intention to
  submit this paper to the current journal focused on computer methods/deep
  learning in biomedicine
- **Journal**: None
- **Summary**: Prevention and early diagnosis of breast cancer (BC) is an essential prerequisite for the selection of proper treatment. The substantial pressure due to the increase of demand for faster and more precise diagnostic results drives for automatic solutions. In the past decade, deep learning techniques have demonstrated their power over several domains, and Computer-Aided (CAD) diagnostic became one of them. However, when it comes to the analysis of Whole Slide Images (WSI), most of the existing works compute predictions from levels independently. This is, however, in contrast to the histopathologist expert approach who requires to see a global architecture of tissue structures important in BC classification.   We present a deep learning-based solution and framework for processing WSI based on a novel approach utilizing the advantages of image levels. We apply the weighing of information extracted from several levels into the final classification of the malignancy. Our results demonstrate the profitability of global information with an increase of accuracy from 72.2% to 84.8%.



### Tiled sparse coding in eigenspaces for the COVID-19 diagnosis in chest X-ray images
- **Arxiv ID**: http://arxiv.org/abs/2106.14724v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.14724v1)
- **Published**: 2021-06-28 13:50:31+00:00
- **Updated**: 2021-06-28 13:50:31+00:00
- **Authors**: Juan E. Arco, Andrés Ortiz, Javier Ramírez, Juan M Gorriz
- **Comment**: 14 pages, 5 figures
- **Journal**: None
- **Summary**: The ongoing crisis of the COVID-19 (Coronavirus disease 2019) pandemic has changed the world. According to the World Health Organization (WHO), 4 million people have died due to this disease, whereas there have been more than 180 million confirmed cases of COVID-19. The collapse of the health system in many countries has demonstrated the need of developing tools to automatize the diagnosis of the disease from medical imaging. Previous studies have used deep learning for this purpose. However, the performance of this alternative highly depends on the size of the dataset employed for training the algorithm. In this work, we propose a classification framework based on sparse coding in order to identify the pneumonia patterns associated with different pathologies. Specifically, each chest X-ray (CXR) image is partitioned into different tiles. The most relevant features extracted from PCA are then used to build the dictionary within the sparse coding procedure. Once images are transformed and reconstructed from the elements of the dictionary, classification is performed from the reconstruction errors of individual patches associated with each image. Performance is evaluated in a real scenario where simultaneously differentiation between four different pathologies: control vs bacterial pneumonia vs viral pneumonia vs COVID-19. The accuracy when identifying the presence of pneumonia is 93.85%, whereas 88.11% is obtained in the 4-class classification context. The excellent results and the pioneering use of sparse coding in this scenario evidence the applicability of this approach as an aid for clinicians in a real-world environment.



### Real-Time Multi-View 3D Human Pose Estimation using Semantic Feedback to Smart Edge Sensors
- **Arxiv ID**: http://arxiv.org/abs/2106.14729v1
- **DOI**: 10.15607/RSS.2021.XVII.040
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.14729v1)
- **Published**: 2021-06-28 14:00:00+00:00
- **Updated**: 2021-06-28 14:00:00+00:00
- **Authors**: Simon Bultmann, Sven Behnke
- **Comment**: Accepted for Robotics: Science and Systems (RSS), July 2021, 10
  pages, 6 figures
- **Journal**: Proceedings of Robotics: Science and Systems, July 2021
- **Summary**: We present a novel method for estimation of 3D human poses from a multi-camera setup, employing distributed smart edge sensors coupled with a backend through a semantic feedback loop. 2D joint detection for each camera view is performed locally on a dedicated embedded inference processor. Only the semantic skeleton representation is transmitted over the network and raw images remain on the sensor board. 3D poses are recovered from 2D joints on a central backend, based on triangulation and a body model which incorporates prior knowledge of the human skeleton. A feedback channel from backend to individual sensors is implemented on a semantic level. The allocentric 3D pose is backprojected into the sensor views where it is fused with 2D joint detections. The local semantic model on each sensor can thus be improved by incorporating global context information. The whole pipeline is capable of real-time operation. We evaluate our method on three public datasets, where we achieve state-of-the-art results and show the benefits of our feedback architecture, as well as in our own setup for multi-person experiments. Using the feedback signal improves the 2D joint detections and in turn the estimated 3D poses.



### Unsupervised Discovery of Actions in Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/2106.14733v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14733v1)
- **Published**: 2021-06-28 14:05:01+00:00
- **Updated**: 2021-06-28 14:05:01+00:00
- **Authors**: AJ Piergiovanni, Anelia Angelova, Michael S. Ryoo, Irfan Essa
- **Comment**: Full paper
- **Journal**: None
- **Summary**: In this paper we address the problem of automatically discovering atomic actions in unsupervised manner from instructional videos. Instructional videos contain complex activities and are a rich source of information for intelligent agents, such as, autonomous robots or virtual assistants, which can, for example, automatically `read' the steps from an instructional video and execute them. However, videos are rarely annotated with atomic activities, their boundaries or duration. We present an unsupervised approach to learn atomic actions of structured human tasks from a variety of instructional videos. We propose a sequential stochastic autoregressive model for temporal segmentation of videos, which learns to represent and discover the sequential relationship between different atomic actions of the task, and which provides automatic and unsupervised self-labeling for videos. Our approach outperforms the state-of-the-art unsupervised methods with large margins. We will open source the code.



### Speech2Properties2Gestures: Gesture-Property Prediction as a Tool for Generating Representational Gestures from Speech
- **Arxiv ID**: http://arxiv.org/abs/2106.14736v2
- **DOI**: 10.1145/3472306.3478333
- **Categories**: **cs.HC**, cs.CV, cs.GR, cs.LG, I.2.7; I.2.6; I.3.7
- **Links**: [PDF](http://arxiv.org/pdf/2106.14736v2)
- **Published**: 2021-06-28 14:07:59+00:00
- **Updated**: 2021-08-13 13:29:29+00:00
- **Authors**: Taras Kucherenko, Rajmund Nagy, Patrik Jonell, Michael Neff, Hedvig Kjellström, Gustav Eje Henter
- **Comment**: Accepted for publication at the ACM International Conference on
  Intelligent Virtual Agents (IVA 2021)
- **Journal**: International Conference on Intelligent Virtual Agents 2021
- **Summary**: We propose a new framework for gesture generation, aiming to allow data-driven approaches to produce more semantically rich gestures. Our approach first predicts whether to gesture, followed by a prediction of the gesture properties. Those properties are then used as conditioning for a modern probabilistic gesture-generation model capable of high-quality output. This empowers the approach to generate gestures that are both diverse and representational. Follow-ups and more information can be found on the project page: https://svito-zar.github.io/speech2properties2gestures/ .



### Real-Time Human Pose Estimation on a Smart Walker using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.14739v1
- **DOI**: 10.1016/j.eswa.2021.115498
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2106.14739v1)
- **Published**: 2021-06-28 14:11:48+00:00
- **Updated**: 2021-06-28 14:11:48+00:00
- **Authors**: Manuel Palermo, Sara Moccia, Lucia Migliorelli, Emanuele Frontoni, Cristina P. Santos
- **Comment**: Accepted for publication in Expert Systems with Applications
- **Journal**: None
- **Summary**: Rehabilitation is important to improve quality of life for mobility-impaired patients. Smart walkers are a commonly used solution that should embed automatic and objective tools for data-driven human-in-the-loop control and monitoring. However, present solutions focus on extracting few specific metrics from dedicated sensors with no unified full-body approach. We investigate a general, real-time, full-body pose estimation framework based on two RGB+D camera streams with non-overlapping views mounted on a smart walker equipment used in rehabilitation. Human keypoint estimation is performed using a two-stage neural network framework. The 2D-Stage implements a detection module that locates body keypoints in the 2D image frames. The 3D-Stage implements a regression module that lifts and relates the detected keypoints in both cameras to the 3D space relative to the walker. Model predictions are low-pass filtered to improve temporal consistency. A custom acquisition method was used to obtain a dataset, with 14 healthy subjects, used for training and evaluating the proposed framework offline, which was then deployed on the real walker equipment. An overall keypoint detection error of 3.73 pixels for the 2D-Stage and 44.05mm for the 3D-Stage were reported, with an inference time of 26.6ms when deployed on the constrained hardware of the walker. We present a novel approach to patient monitoring and data-driven human-in-the-loop control in the context of smart walkers. It is able to extract a complete and compact body representation in real-time and from inexpensive sensors, serving as a common base for downstream metrics extraction solutions, and Human-Robot interaction applications. Despite promising results, more data should be collected on users with impairments, to assess its performance as a rehabilitation tool in real-world scenarios.



### One-Shot Affordance Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.14747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14747v1)
- **Published**: 2021-06-28 14:22:52+00:00
- **Updated**: 2021-06-28 14:22:52+00:00
- **Authors**: Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Affordance detection refers to identifying the potential action possibilities of objects in an image, which is an important ability for robot perception and manipulation. To empower robots with this ability in unseen scenarios, we consider the challenging one-shot affordance detection problem in this paper, i.e., given a support image that depicts the action purpose, all objects in a scene with the common affordance should be detected. To this end, we devise a One-Shot Affordance Detection (OS-AD) network that firstly estimates the purpose and then transfers it to help detect the common affordance from all candidate images. Through collaboration learning, OS-AD can capture the common characteristics between objects having the same underlying affordance and learn a good adaptation capability for perceiving unseen affordances. Besides, we build a Purpose-driven Affordance Dataset (PAD) by collecting and labeling 4k images from 31 affordance and 72 object categories. Experimental results demonstrate the superiority of our model over previous representative ones in terms of both objective metrics and visual quality. The benchmark suite is at ProjectPage.



### A Theory-Driven Self-Labeling Refinement Method for Contrastive Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.14749v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2106.14749v1)
- **Published**: 2021-06-28 14:24:52+00:00
- **Updated**: 2021-06-28 14:24:52+00:00
- **Authors**: Pan Zhou, Caiming Xiong, Xiao-Tong Yuan, Steven Hoi
- **Comment**: under review. arXiv admin note: substantial text overlap with
  arXiv:1903.11680 by other authors
- **Journal**: None
- **Summary**: For an image query, unsupervised contrastive learning labels crops of the same image as positives, and other image crops as negatives. Although intuitive, such a native label assignment strategy cannot reveal the underlying semantic similarity between a query and its positives and negatives, and impairs performance, since some negatives are semantically similar to the query or even share the same semantic class as the query. In this work, we first prove that for contrastive learning, inaccurate label assignment heavily impairs its generalization for semantic instance discrimination, while accurate labels benefit its generalization. Inspired by this theory, we propose a novel self-labeling refinement approach for contrastive learning. It improves the label quality via two complementary modules: (i) self-labeling refinery (SLR) to generate accurate labels and (ii) momentum mixup (MM) to enhance similarity between query and its positive. SLR uses a positive of a query to estimate semantic similarity between a query and its positive and negatives, and combines estimated similarity with vanilla label assignment in contrastive learning to iteratively generate more accurate and informative soft labels. We theoretically show that our SLR can exactly recover the true semantic labels of label-corrupted data, and supervises networks to achieve zero prediction error on classification tasks. MM randomly combines queries and positives to increase semantic similarity between the generated virtual queries and their positives so as to improves label accuracy. Experimental results on CIFAR10, ImageNet, VOC and COCO show the effectiveness of our method. PyTorch code and model will be released online.



### Hyperspectral Remote Sensing Image Classification Based on Multi-scale Cross Graphic Convolution
- **Arxiv ID**: http://arxiv.org/abs/2106.14804v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.14804v1)
- **Published**: 2021-06-28 15:28:09+00:00
- **Updated**: 2021-06-28 15:28:09+00:00
- **Authors**: Yunsong Zhao, Yin Li, Zhihan Chen, Tianchong Qiu, Guojin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The mining and utilization of features directly affect the classification performance of models used in the classification and recognition of hyperspectral remote sensing images. Traditional models usually conduct feature mining from a single perspective, with the features mined being limited and the internal relationships between them being ignored. Consequently, useful features are lost and classification results are unsatisfactory. To fully mine and utilize image features, a new multi-scale feature-mining learning algorithm (MGRNet) is proposed. The model uses principal component analysis to reduce the dimensionality of the original hyperspectral image (HSI) to retain 99.99% of its semantic information and extract dimensionality reduction features. Using a multi-scale convolution algorithm, the input dimensionality reduction features were mined to obtain shallow features, which then served as inputs into a multi-scale graph convolution algorithm to construct the internal relationships between eigenvalues at different scales. We then carried out cross fusion of multi-scale information obtained by graph convolution, before inputting the new information obtained into the residual network algorithm for deep feature mining. Finally, a flexible maximum transfer function classifier was used to predict the final features and complete the classification. Experiments on three common hyperspectral datasets showed the MGRNet algorithm proposed in this paper to be superior to traditional methods in recognition accuracy.



### Dataset Bias Mitigation Through Analysis of CNN Training Scores
- **Arxiv ID**: http://arxiv.org/abs/2106.14829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14829v1)
- **Published**: 2021-06-28 16:07:49+00:00
- **Updated**: 2021-06-28 16:07:49+00:00
- **Authors**: Ekberjan Derman
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: Training datasets are crucial for convolutional neural network-based algorithms, which directly impact their overall performance. As such, using a well-structured dataset that has minimum level of bias is always desirable. In this paper, we proposed a novel, domain-independent approach, called score-based resampling (SBR), to locate the under-represented samples of the original training dataset based on the model prediction scores obtained with that training set. In our method, once trained, we use the same CNN model to infer on its own training samples, obtain prediction scores, and based on the distance between predicted and ground-truth, we identify samples that are far away from their ground-truth and augment them in the original training set. The temperature term of the Sigmoid function is decreased to better differentiate scores. For experimental evaluation, we selected one Kaggle dataset for gender classification. We first used a CNN-based classifier with relatively standard structure, trained on the training images, and evaluated on the provided validation samples of the original dataset. Then, we assessed it on a totally new test dataset consisting of light male, light female, dark male, and dark female groups. The obtained accuracies varied, revealing the existence of categorical bias against certain groups in the original dataset. Subsequently, we trained the model after resampling based on our proposed approach. We compared our method with a previously proposed variational autoencoder (VAE) based algorithm. The obtained results confirmed the validity of our proposed method regrading identifying under-represented samples among original dataset to decrease categorical bias of classifying certain groups. Although tested for gender classification, the proposed algorithm can be used for investigating dataset structure of any CNN-based tasks.



### Understanding Dynamics of Nonlinear Representation Learning and Its Application
- **Arxiv ID**: http://arxiv.org/abs/2106.14836v4
- **DOI**: 10.1162/neco_a_01483
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2106.14836v4)
- **Published**: 2021-06-28 16:31:30+00:00
- **Updated**: 2022-04-09 05:54:02+00:00
- **Authors**: Kenji Kawaguchi, Linjun Zhang, Zhun Deng
- **Comment**: None
- **Journal**: Neural computation, volume 34, pages 991-1018 (2022)
- **Summary**: Representations of the world environment play a crucial role in artificial intelligence. It is often inefficient to conduct reasoning and inference directly in the space of raw sensory representations, such as pixel values of images. Representation learning allows us to automatically discover suitable representations from raw sensory data. For example, given raw sensory data, a deep neural network learns nonlinear representations at its hidden layers, which are subsequently used for classification (or regression) at its output layer. This happens implicitly during training through minimizing a supervised or unsupervised loss in common practical regimes of deep learning, unlike the neural tangent kernel (NTK) regime. In this paper, we study the dynamics of such implicit nonlinear representation learning, which is beyond the NTK regime. We identify a pair of a new assumption and a novel condition, called the common model structure assumption and the data-architecture alignment condition. Under the common model structure assumption, the data-architecture alignment condition is shown to be sufficient for the global convergence and necessary for the global optimality. Moreover, our theory explains how and when increasing the network size does and does not improve the training behaviors in the practical regime. Our results provide practical guidance for designing a model structure: e.g., the common model structure assumption can be used as a justification for using a particular model structure instead of others. We also derive a new training framework based on the theory. The proposed framework is empirically shown to maintain competitive (practical) test performances while providing global convergence guarantees for deep residual neural networks with convolutions, skip connections, and batch normalization with standard benchmark datasets, including CIFAR-10, CIFAR-100, and SVHN.



### CLIPDraw: Exploring Text-to-Drawing Synthesis through Language-Image Encoders
- **Arxiv ID**: http://arxiv.org/abs/2106.14843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14843v1)
- **Published**: 2021-06-28 16:43:26+00:00
- **Updated**: 2021-06-28 16:43:26+00:00
- **Authors**: Kevin Frans, L. B. Soros, Olaf Witkowski
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents CLIPDraw, an algorithm that synthesizes novel drawings based on natural language input. CLIPDraw does not require any training; rather a pre-trained CLIP language-image encoder is used as a metric for maximizing similarity between the given description and a generated drawing. Crucially, CLIPDraw operates over vector strokes rather than pixel images, a constraint that biases drawings towards simpler human-recognizable shapes. Results compare between CLIPDraw and other synthesis-through-optimization methods, as well as highlight various interesting behaviors of CLIPDraw, such as satisfying ambiguous text in multiple ways, reliably producing drawings in diverse artistic styles, and scaling from simple to complex visual representations as stroke count is increased. Code for experimenting with the method is available at: https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb



### Progressive Joint Low-light Enhancement and Noise Removal for Raw Images
- **Arxiv ID**: http://arxiv.org/abs/2106.14844v4
- **DOI**: 10.1109/TIP.2022.3155948
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14844v4)
- **Published**: 2021-06-28 16:43:52+00:00
- **Updated**: 2022-09-02 14:08:01+00:00
- **Authors**: Yucheng Lu, Seung-Won Jung
- **Comment**: None
- **Journal**: None
- **Summary**: Low-light imaging on mobile devices is typically challenging due to insufficient incident light coming through the relatively small aperture, resulting in a low signal-to-noise ratio. Most of the previous works on low-light image processing focus either only on a single task such as illumination adjustment, color enhancement, or noise removal; or on a joint illumination adjustment and denoising task that heavily relies on short-long exposure image pairs collected from specific camera models, and thus these approaches are less practical and generalizable in real-world settings where camera-specific joint enhancement and restoration is required. To tackle this problem, in this paper, we propose a low-light image processing framework that performs joint illumination adjustment, color enhancement, and denoising. Considering the difficulty in model-specific data collection and the ultra-high definition of the captured images, we design two branches: a coefficient estimation branch as well as a joint enhancement and denoising branch. The coefficient estimation branch works in a low-resolution space and predicts the coefficients for enhancement via bilateral learning, whereas the joint enhancement and denoising branch works in a full-resolution space and progressively performs joint enhancement and denoising. In contrast to existing methods, our framework does not need to recollect massive data when being adapted to another camera model, which significantly reduces the efforts required to fine-tune our approach for practical usage. Through extensive experiments, we demonstrate its great potential in real-world low-light imaging applications when compared with current state-of-the-art methods.



### Iris Presentation Attack Detection by Attention-based and Deep Pixel-wise Binary Supervision Network
- **Arxiv ID**: http://arxiv.org/abs/2106.14845v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14845v1)
- **Published**: 2021-06-28 16:47:08+00:00
- **Updated**: 2021-06-28 16:47:08+00:00
- **Authors**: Meiling Fang, Naser Damer, Fadi Boutros, Florian Kirchbuchner, Arjan Kuijper
- **Comment**: To appear at the 2021 International Joint Conference on Biometrics
  (IJCB 2021)
- **Journal**: None
- **Summary**: Iris presentation attack detection (PAD) plays a vital role in iris recognition systems. Most existing CNN-based iris PAD solutions 1) perform only binary label supervision during the training of CNNs, serving global information learning but weakening the capture of local discriminative features, 2) prefer the stacked deeper convolutions or expert-designed networks, raising the risk of overfitting, 3) fuse multiple PAD systems or various types of features, increasing difficulty for deployment on mobile devices. Hence, we propose a novel attention-based deep pixel-wise binary supervision (A-PBS) method. Pixel-wise supervision is first able to capture the fine-grained pixel/patch-level cues. Then, the attention mechanism guides the network to automatically find regions that most contribute to an accurate PAD decision. Extensive experiments are performed on LivDet-Iris 2017 and three other publicly available databases to show the effectiveness and robustness of proposed A-PBS methods. For instance, the A-PBS model achieves an HTER of 6.50% on the IIITD-WVU database outperforming state-of-the-art methods.



### K-Net: Towards Unified Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.14855v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.14855v2)
- **Published**: 2021-06-28 17:18:21+00:00
- **Updated**: 2021-11-01 17:40:49+00:00
- **Authors**: Wenwei Zhang, Jiangmiao Pang, Kai Chen, Chen Change Loy
- **Comment**: Camera ready for NeurIPS2021
- **Journal**: None
- **Summary**: Semantic, instance, and panoptic segmentations have been addressed using different and specialized frameworks despite their underlying connections. This paper presents a unified, simple, and effective framework for these essentially similar tasks. The framework, named K-Net, segments both instances and semantic categories consistently by a group of learnable kernels, where each kernel is responsible for generating a mask for either a potential instance or a stuff class. To remedy the difficulties of distinguishing various instances, we propose a kernel update strategy that enables each kernel dynamic and conditional on its meaningful group in the input image. K-Net can be trained in an end-to-end manner with bipartite matching, and its training and inference are naturally NMS-free and box-free. Without bells and whistles, K-Net surpasses all previous published state-of-the-art single-model results of panoptic segmentation on MS COCO test-dev split and semantic segmentation on ADE20K val split with 55.2% PQ and 54.3% mIoU, respectively. Its instance segmentation performance is also on par with Cascade Mask R-CNN on MS COCO with 60%-90% faster inference speeds. Code and models will be released at https://github.com/ZwwWayne/K-Net/.



### Modeling Clothing as a Separate Layer for an Animatable Human Avatar
- **Arxiv ID**: http://arxiv.org/abs/2106.14879v3
- **DOI**: 10.1145/3478513.3480545
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2106.14879v3)
- **Published**: 2021-06-28 17:58:40+00:00
- **Updated**: 2021-10-05 00:20:09+00:00
- **Authors**: Donglai Xiang, Fabian Prada, Timur Bagautdinov, Weipeng Xu, Yuan Dong, He Wen, Jessica Hodgins, Chenglei Wu
- **Comment**: Camera ready for SIGGRAPH Asia 2021 Technical Papers.
  https://research.fb.com/publications/modeling-clothing-as-a-separate-layer-for-an-animatable-human-avatar/
- **Journal**: None
- **Summary**: We have recently seen great progress in building photorealistic animatable full-body codec avatars, but generating high-fidelity animation of clothing is still difficult. To address these difficulties, we propose a method to build an animatable clothed body avatar with an explicit representation of the clothing on the upper body from multi-view captured videos. We use a two-layer mesh representation to register each 3D scan separately with the body and clothing templates. In order to improve the photometric correspondence across different frames, texture alignment is then performed through inverse rendering of the clothing geometry and texture predicted by a variational autoencoder. We then train a new two-layer codec avatar with separate modeling of the upper clothing and the inner body layer. To learn the interaction between the body dynamics and clothing states, we use a temporal convolution network to predict the clothing latent code based on a sequence of input skeletal poses. We show photorealistic animation output for three different actors, and demonstrate the advantage of our clothed-body avatars over the single-layer avatars used in previous work. We also show the benefit of an explicit clothing model that allows the clothing texture to be edited in the animation output.



### HDMapGen: A Hierarchical Graph Generative Model of High Definition Maps
- **Arxiv ID**: http://arxiv.org/abs/2106.14880v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14880v1)
- **Published**: 2021-06-28 17:59:30+00:00
- **Updated**: 2021-06-28 17:59:30+00:00
- **Authors**: Lu Mi, Hang Zhao, Charlie Nash, Xiaohan Jin, Jiyang Gao, Chen Sun, Cordelia Schmid, Nir Shavit, Yuning Chai, Dragomir Anguelov
- **Comment**: None
- **Journal**: None
- **Summary**: High Definition (HD) maps are maps with precise definitions of road lanes with rich semantics of the traffic rules. They are critical for several key stages in an autonomous driving system, including motion forecasting and planning. However, there are only a small amount of real-world road topologies and geometries, which significantly limits our ability to test out the self-driving stack to generalize onto new unseen scenarios. To address this issue, we introduce a new challenging task to generate HD maps. In this work, we explore several autoregressive models using different data representations, including sequence, plain graph, and hierarchical graph. We propose HDMapGen, a hierarchical graph generation model capable of producing high-quality and diverse HD maps through a coarse-to-fine approach. Experiments on the Argoverse dataset and an in-house dataset show that HDMapGen significantly outperforms baseline methods. Additionally, we demonstrate that HDMapGen achieves high scalability and efficiency.



### Early Convolutions Help Transformers See Better
- **Arxiv ID**: http://arxiv.org/abs/2106.14881v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14881v3)
- **Published**: 2021-06-28 17:59:33+00:00
- **Updated**: 2021-10-25 19:54:23+00:00
- **Authors**: Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollár, Ross Girshick
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Vision transformer (ViT) models exhibit substandard optimizability. In particular, they are sensitive to the choice of optimizer (AdamW vs. SGD), optimizer hyperparameters, and training schedule length. In comparison, modern convolutional neural networks are easier to optimize. Why is this the case? In this work, we conjecture that the issue lies with the patchify stem of ViT models, which is implemented by a stride-p p*p convolution (p=16 by default) applied to the input image. This large-kernel plus large-stride convolution runs counter to typical design choices of convolutional layers in neural networks. To test whether this atypical design choice causes an issue, we analyze the optimization behavior of ViT models with their original patchify stem versus a simple counterpart where we replace the ViT stem by a small number of stacked stride-two 3*3 convolutions. While the vast majority of computation in the two ViT designs is identical, we find that this small change in early visual processing results in markedly different training behavior in terms of the sensitivity to optimization settings as well as the final model accuracy. Using a convolutional stem in ViT dramatically increases optimization stability and also improves peak performance (by ~1-2% top-1 accuracy on ImageNet-1k), while maintaining flops and runtime. The improvement can be observed across the wide spectrum of model complexities (from 1G to 36G flops) and dataset scales (from ImageNet-1k to ImageNet-21k). These findings lead us to recommend using a standard, lightweight convolutional stem for ViT models in this regime as a more robust architectural choice compared to the original ViT model design.



### Rethinking Token-Mixing MLP for MLP-based Vision Backbone
- **Arxiv ID**: http://arxiv.org/abs/2106.14882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14882v1)
- **Published**: 2021-06-28 17:59:57+00:00
- **Updated**: 2021-06-28 17:59:57+00:00
- **Authors**: Tan Yu, Xu Li, Yunfeng Cai, Mingming Sun, Ping Li
- **Comment**: None
- **Journal**: None
- **Summary**: In the past decade, we have witnessed rapid progress in the machine vision backbone. By introducing the inductive bias from the image processing, convolution neural network (CNN) has achieved excellent performance in numerous computer vision tasks and has been established as \emph{de facto} backbone. In recent years, inspired by the great success achieved by Transformer in NLP tasks, vision Transformer models emerge. Using much less inductive bias, they have achieved promising performance in computer vision tasks compared with their CNN counterparts. More recently, researchers investigate using the pure-MLP architecture to build the vision backbone to further reduce the inductive bias, achieving good performance. The pure-MLP backbone is built upon channel-mixing MLPs to fuse the channels and token-mixing MLPs for communications between patches. In this paper, we re-think the design of the token-mixing MLP. We discover that token-mixing MLPs in existing MLP-based backbones are spatial-specific, and thus it is sensitive to spatial translation. Meanwhile, the channel-agnostic property of the existing token-mixing MLPs limits their capability in mixing tokens. To overcome those limitations, we propose an improved structure termed as Circulant Channel-Specific (CCS) token-mixing MLP, which is spatial-invariant and channel-specific. It takes fewer parameters but achieves higher classification accuracy on ImageNet1K benchmark.



### Striking the Right Balance: Recall Loss for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.14917v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14917v2)
- **Published**: 2021-06-28 18:02:03+00:00
- **Updated**: 2022-02-03 23:48:12+00:00
- **Authors**: Junjiao Tian, Niluthpol Mithun, Zach Seymour, Han-Pang Chiu, Zsolt Kira
- **Comment**: Paper accepted to ICRA2022
- **Journal**: IEEE International Conference on Robotics and Automation 2022
- **Summary**: Class imbalance is a fundamental problem in computer vision applications such as semantic segmentation. Specifically, uneven class distributions in a training dataset often result in unsatisfactory performance on under-represented classes. Many works have proposed to weight the standard cross entropy loss function with pre-computed weights based on class statistics, such as the number of samples and class margins. There are two major drawbacks to these methods: 1) constantly up-weighting minority classes can introduce excessive false positives in semantic segmentation; 2) a minority class is not necessarily a hard class. The consequence is low precision due to excessive false positives. In this regard, we propose a hard-class mining loss by reshaping the vanilla cross entropy loss such that it weights the loss for each class dynamically based on instantaneous recall performance. We show that the novel recall loss changes gradually between the standard cross entropy loss and the inverse frequency weighted loss. Recall loss also leads to improved mean accuracy while offering competitive mean Intersection over Union (IoU) performance. On Synthia dataset, recall loss achieves $9\%$ relative improvement on mean accuracy with competitive mean IoU using DeepLab-ResNet18 compared to the cross entropy loss. Code available at https://github.com/PotatoTian/recall-semseg.



### Cosmic-CoNN: A Cosmic Ray Detection Deep-Learning Framework, Dataset, and Toolkit
- **Arxiv ID**: http://arxiv.org/abs/2106.14922v3
- **DOI**: None
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.14922v3)
- **Published**: 2021-06-28 18:04:56+00:00
- **Updated**: 2022-10-06 06:14:32+00:00
- **Authors**: Chengyuan Xu, Curtis McCully, Boning Dong, D. Andrew Howell, Pradeep Sen
- **Comment**: 17 pages, 10 figures, 4 tables. Submitted to AAS Journals. See
  https://github.com/cy-xu/cosmic-conn for the open-source software and
  https://zenodo.org/record/5034763 for the dataset
- **Journal**: None
- **Summary**: Rejecting cosmic rays (CRs) is essential for the scientific interpretation of CCD-captured data, but detecting CRs in single-exposure images has remained challenging. Conventional CR detectors require experimental parameter tuning for different instruments, and recent deep learning methods only produce instrument-specific models that suffer from performance loss on telescopes not included in the training data. We present Cosmic-CoNN, a generic CR detector deployed for 24 telescopes at the Las Cumbres Observatory, which is made possible by the three contributions in this work: 1) We build a large and diverse ground-based CR dataset leveraging thousands of images from a global telescope network. 2) We propose a novel loss function and a neural network optimized for telescope imaging data to train generic CR detection models. At 95% recall, our model achieves a precision of 93.70% on Las Cumbres imaging data and maintains a consistent performance on new ground-based instruments never used for training. Specifically, the Cosmic-CoNN model trained on the Las Cumbres CR dataset maintains high precisions of 92.03% and 96.69% on Gemini GMOS-N/S 1x1 and 2x2 binning images, respectively. 3) We build a suite of tools including an interactive CR mask visualization and editing interface, console commands, and Python APIs to make automatic, robust CR detection widely accessible by the community of astronomers. Our dataset, open-source codebase, and trained models are available at https://github.com/cy-xu/cosmic-conn.



### Fast Training of Neural Lumigraph Representations using Meta Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.14942v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.14942v2)
- **Published**: 2021-06-28 18:55:50+00:00
- **Updated**: 2021-10-26 23:50:07+00:00
- **Authors**: Alexander W. Bergman, Petr Kellnhofer, Gordon Wetzstein
- **Comment**: Project website:
  http://www.computationalimaging.org/publications/metanlr/
- **Journal**: None
- **Summary**: Novel view synthesis is a long-standing problem in machine learning and computer vision. Significant progress has recently been made in developing neural scene representations and rendering techniques that synthesize photorealistic images from arbitrary views. These representations, however, are extremely slow to train and often also slow to render. Inspired by neural variants of image-based rendering, we develop a new neural rendering approach with the goal of quickly learning a high-quality representation which can also be rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a unique combination of a neural shape representation and 2D CNN-based image feature extraction, aggregation, and re-projection. To push representation convergence times down to minutes, we leverage meta learning to learn neural shape and image feature priors which accelerate training. The optimized shape and image features can then be extracted using traditional graphics techniques and rendered in real time. We show that MetaNLR++ achieves similar or better novel view synthesis results in a fraction of the time that competing methods require.



### Achieving Real-Time Object Detection on MobileDevices with Neural Pruning Search
- **Arxiv ID**: http://arxiv.org/abs/2106.14943v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.14943v1)
- **Published**: 2021-06-28 18:59:20+00:00
- **Updated**: 2021-06-28 18:59:20+00:00
- **Authors**: Pu Zhao, Wei Niu, Geng Yuan, Yuxuan Cai, Bin Ren, Yanzhi Wang, Xue Lin
- **Comment**: Presented on the HiPEAC 2021 workshop (cogarch 2021)
- **Journal**: None
- **Summary**: Object detection plays an important role in self-driving cars for security development. However, mobile systems on self-driving cars with limited computation resources lead to difficulties for object detection. To facilitate this, we propose a compiler-aware neural pruning search framework to achieve high-speed inference on autonomous vehicles for 2D and 3D object detection. The framework automatically searches the pruning scheme and rate for each layer to find a best-suited pruning for optimizing detection accuracy and speed performance under compiler optimization. Our experiments demonstrate that for the first time, the proposed method achieves (close-to) real-time, 55ms and 99ms inference times for YOLOv4 based 2D object detection and PointPillars based 3D detection, respectively, on an off-the-shelf mobile phone with minor (or no) accuracy loss.



### Data augmentation for deep learning based accelerated MRI reconstruction with limited data
- **Arxiv ID**: http://arxiv.org/abs/2106.14947v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.2; I.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2106.14947v1)
- **Published**: 2021-06-28 19:08:46+00:00
- **Updated**: 2021-06-28 19:08:46+00:00
- **Authors**: Zalan Fabian, Reinhard Heckel, Mahdi Soltanolkotabi
- **Comment**: 27 pages, 19 figures, to be published in ICML2021
- **Journal**: None
- **Summary**: Deep neural networks have emerged as very successful tools for image restoration and reconstruction tasks. These networks are often trained end-to-end to directly reconstruct an image from a noisy or corrupted measurement of that image. To achieve state-of-the-art performance, training on large and diverse sets of images is considered critical. However, it is often difficult and/or expensive to collect large amounts of training images. Inspired by the success of Data Augmentation (DA) for classification problems, in this paper, we propose a pipeline for data augmentation for accelerated MRI reconstruction and study its effectiveness at reducing the required training data in a variety of settings. Our DA pipeline, MRAugment, is specifically designed to utilize the invariances present in medical imaging measurements as naive DA strategies that neglect the physics of the problem fail. Through extensive studies on multiple datasets we demonstrate that in the low-data regime DA prevents overfitting and can match or even surpass the state of the art while using significantly fewer training data, whereas in the high-data regime it has diminishing returns. Furthermore, our findings show that DA can improve the robustness of the model against various shifts in the test distribution.



### Deep Learning for Face Anti-Spoofing: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2106.14948v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14948v3)
- **Published**: 2021-06-28 19:12:00+00:00
- **Updated**: 2022-09-02 15:30:35+00:00
- **Authors**: Zitong Yu, Yunxiao Qin, Xiaobai Li, Chenxu Zhao, Zhen Lei, Guoying Zhao
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI)
- **Journal**: None
- **Summary**: Face anti-spoofing (FAS) has lately attracted increasing attention due to its vital role in securing face recognition systems from presentation attacks (PAs). As more and more realistic PAs with novel types spring up, traditional FAS methods based on handcrafted features become unreliable due to their limited representation capacity. With the emergence of large-scale academic datasets in the recent decade, deep learning based FAS achieves remarkable performance and dominates this area. However, existing reviews in this field mainly focus on the handcrafted features, which are outdated and uninspiring for the progress of FAS community. In this paper, to stimulate future research, we present the first comprehensive review of recent advances in deep learning based FAS. It covers several novel and insightful components: 1) besides supervision with binary label (e.g., '0' for bonafide vs. '1' for PAs), we also investigate recent methods with pixel-wise supervision (e.g., pseudo depth map); 2) in addition to traditional intra-dataset evaluation, we collect and analyze the latest methods specially designed for domain generalization and open-set FAS; and 3) besides commercial RGB camera, we summarize the deep learning applications under multi-modal (e.g., depth and infrared) or specialized (e.g., light field and flash) sensors. We conclude this survey by emphasizing current open issues and highlighting potential prospects.



### Object Detection Based Handwriting Localization
- **Arxiv ID**: http://arxiv.org/abs/2106.14989v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.14989v1)
- **Published**: 2021-06-28 21:25:20+00:00
- **Updated**: 2021-06-28 21:25:20+00:00
- **Authors**: Yuli Wu, Yucheng Hu, Suting Miao
- **Comment**: ICDAR 2021 Workshop: Industrial Applications of Document Analysis and
  Recognition
- **Journal**: None
- **Summary**: We present an object detection based approach to localize handwritten regions from documents, which initially aims to enhance the anonymization during the data transmission. The concatenated fusion of original and preprocessed images containing both printed texts and handwritten notes or signatures are fed into the convolutional neural network, where the bounding boxes are learned to detect the handwriting. Afterwards, the handwritten regions can be processed (e.g. replaced with redacted signatures) to conceal the personally identifiable information (PII). This processing pipeline based on the deep learning network Cascade R-CNN works at 10 fps on a GPU during the inference, which ensures the enhanced anonymization with minimal computational overheads. Furthermore, the impressive generalizability has been empirically showcased: the trained model based on the English-dominant dataset works well on the fictitious unseen invoices, even in Chinese. The proposed approach is also expected to facilitate other tasks such as handwriting recognition and signature verification.



### Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals
- **Arxiv ID**: http://arxiv.org/abs/2106.15004v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.15004v2)
- **Published**: 2021-06-28 22:23:14+00:00
- **Updated**: 2021-09-15 22:54:24+00:00
- **Authors**: Nachiket Deo, Eric M. Wolff, Oscar Beijbom
- **Comment**: None
- **Journal**: None
- **Summary**: Accurately predicting the future motion of surrounding vehicles requires reasoning about the inherent uncertainty in driving behavior. This uncertainty can be loosely decoupled into lateral (e.g., keeping lane, turning) and longitudinal (e.g., accelerating, braking). We present a novel method that combines learned discrete policy rollouts with a focused decoder on subsets of the lane graph. The policy rollouts explore different goals given current observations, ensuring that the model captures lateral variability. Longitudinal variability is captured by our latent variable model decoder that is conditioned on various subsets of the lane graph. Our model achieves state-of-the-art performance on the nuScenes motion prediction dataset, and qualitatively demonstrates excellent scene compliance. Detailed ablations highlight the importance of the policy rollouts and the decoder architecture.



### An Uncertainty Estimation Framework for Probabilistic Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.15007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15007v1)
- **Published**: 2021-06-28 22:29:59+00:00
- **Updated**: 2021-06-28 22:29:59+00:00
- **Authors**: Zongyao Lyu, Nolan B. Gutierrez, William J. Beksi
- **Comment**: To be published in the 2021 International Conference on Automation
  Science and Engineering (CASE)
- **Journal**: None
- **Summary**: In this paper, we introduce a new technique that combines two popular methods to estimate uncertainty in object detection. Quantifying uncertainty is critical in real-world robotic applications. Traditional detection models can be ambiguous even when they provide a high-probability output. Robot actions based on high-confidence, yet unreliable predictions, may result in serious repercussions. Our framework employs deep ensembles and Monte Carlo dropout for approximating predictive uncertainty, and it improves upon the uncertainty estimation quality of the baseline method. The proposed approach is evaluated on publicly available synthetic image datasets captured from sequences of video.



### Understanding Cognitive Fatigue from fMRI Scans with Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.15009v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15009v3)
- **Published**: 2021-06-28 22:38:51+00:00
- **Updated**: 2021-09-19 17:30:30+00:00
- **Authors**: Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Fillia Makedon, Glenn Wylie
- **Comment**: 8 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: Functional magnetic resonance imaging (fMRI) is a neuroimaging technique that records neural activations in the brain by capturing the blood oxygen level in different regions based on the task performed by a subject. Given fMRI data, the problem of predicting the state of cognitive fatigue in a person has not been investigated to its full extent. This paper proposes tackling this issue as a multi-class classification problem by dividing the state of cognitive fatigue into six different levels, ranging from no-fatigue to extreme fatigue conditions. We built a spatio-temporal model that uses convolutional neural networks (CNN) for spatial feature extraction and a long short-term memory (LSTM) network for temporal modeling of 4D fMRI scans. We also applied a self-supervised method called MoCo (Momentum Contrast) to pre-train our model on a public dataset BOLD5000 and fine-tuned it on our labeled dataset to predict cognitive fatigue. Our novel dataset contains fMRI scans from Traumatic Brain Injury (TBI) patients and healthy controls (HCs) while performing a series of N-back cognitive tasks. This method establishes a state-of-the-art technique to analyze cognitive fatigue from fMRI data and beats previous approaches to solve this problem.



### Are conditional GANs explicitly conditional?
- **Arxiv ID**: http://arxiv.org/abs/2106.15011v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.15011v3)
- **Published**: 2021-06-28 22:49:27+00:00
- **Updated**: 2022-03-03 10:36:31+00:00
- **Authors**: Houssem eddine Boulahbal, Adrian Voicila, Andrew Comport
- **Comment**: Accepted at BMVC2021
- **Journal**: None
- **Summary**: This paper proposes two important contributions for conditional Generative Adversarial Networks (cGANs) to improve the wide variety of applications that exploit this architecture. The first main contribution is an analysis of cGANs to show that they are not explicitly conditional. In particular, it will be shown that the discriminator and subsequently the cGAN does not automatically learn the conditionality between inputs. The second contribution is a new method, called a contrario cGAN, that explicitly models conditionality for both parts of the adversarial architecture via a novel a contrario loss that involves training the discriminator to learn unconditional (adverse) examples. This leads to a novel type of data augmentation approach for GANs (a contrario learning) which allows to restrict the search space of the generator to conditional outputs using adverse examples. Extensive experimentation is carried out to evaluate the conditionality of the discriminator by proposing a probability distribution analysis. Comparisons with the cGAN architecture for different applications show significant improvements in performance on well known datasets including, semantic image synthesis, image segmentation, monocular depth prediction and "single label"-to-image using different metrics including Fr\'echet Inception Distance (FID), mean Intersection over Union (mIoU), Root Mean Square Error log (RMSE log) and Number of statistically-Different Bins (NDB).



