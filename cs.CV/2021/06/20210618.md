# Arxiv Papers in cs.CV on 2021-06-18
### Effective Model Sparsification by Scheduled Grow-and-Prune Methods
- **Arxiv ID**: http://arxiv.org/abs/2106.09857v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2106.09857v3)
- **Published**: 2021-06-18 01:03:13+00:00
- **Updated**: 2022-03-04 15:08:32+00:00
- **Authors**: Xiaolong Ma, Minghai Qin, Fei Sun, Zejiang Hou, Kun Yuan, Yi Xu, Yanzhi Wang, Yen-Kuang Chen, Rong Jin, Yuan Xie
- **Comment**: ICLR 2022 camera ready
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are effective in solving many real-world problems. Larger DNN models usually exhibit better quality (e.g., accuracy) but their excessive computation results in long inference time. Model sparsification can reduce the computation and memory cost while maintaining model quality. Most existing sparsification algorithms unidirectionally remove weights, while others randomly or greedily explore a small subset of weights in each layer for pruning. The limitations of these algorithms reduce the level of achievable sparsity. In addition, many algorithms still require pre-trained dense models and thus suffer from large memory footprint. In this paper, we propose a novel scheduled grow-and-prune (GaP) methodology without having to pre-train a dense model. It addresses the shortcomings of the previous works by repeatedly growing a subset of layers to dense and then pruning them back to sparse after some training. Experiments show that the models pruned using the proposed methods match or beat the quality of the highly optimized dense models at 80% sparsity on a variety of tasks, such as image classification, objective detection, 3D object part segmentation, and translation. They also outperform other state-of-the-art (SOTA) methods for model sparsification. As an example, a 90% non-uniform sparse ResNet-50 model obtained via GaP achieves 77.9% top-1 accuracy on ImageNet, improving the previous SOTA results by 1.5%. Code available at: https://github.com/boone891214/GaP.



### RSG: A Simple but Effective Module for Learning Imbalanced Datasets
- **Arxiv ID**: http://arxiv.org/abs/2106.09859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.09859v1)
- **Published**: 2021-06-18 01:10:27+00:00
- **Updated**: 2021-06-18 01:10:27+00:00
- **Authors**: Jianfeng Wang, Thomas Lukasiewicz, Xiaolin Hu, Jianfei Cai, Zhenghua Xu
- **Comment**: To appear at CVPR 2021. We propose a flexible data generation/data
  augmentation module for long-tailed classification. Codes are available at:
  https://github.com/Jianf-Wang/RSG
- **Journal**: None
- **Summary**: Imbalanced datasets widely exist in practice and area great challenge for training deep neural models with agood generalization on infrequent classes. In this work, wepropose a new rare-class sample generator (RSG) to solvethis problem. RSG aims to generate some new samplesfor rare classes during training, and it has in particularthe following advantages: (1) it is convenient to use andhighly versatile, because it can be easily integrated intoany kind of convolutional neural network, and it works wellwhen combined with different loss functions, and (2) it isonly used during the training phase, and therefore, no ad-ditional burden is imposed on deep neural networks duringthe testing phase. In extensive experimental evaluations, weverify the effectiveness of RSG. Furthermore, by leveragingRSG, we obtain competitive results on Imbalanced CIFARand new state-of-the-art results on Places-LT, ImageNet-LT, and iNaturalist 2018. The source code is available at https://github.com/Jianf-Wang/RSG.



### Medical Image Analysis on Left Atrial LGE MRI for Atrial Fibrillation Studies: A Review
- **Arxiv ID**: http://arxiv.org/abs/2106.09862v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.09862v3)
- **Published**: 2021-06-18 01:31:06+00:00
- **Updated**: 2022-01-10 11:14:05+00:00
- **Authors**: Lei Li, Veronika A. Zimmer, Julia A. Schnabel, Xiahai Zhuang
- **Comment**: 30 pages
- **Journal**: None
- **Summary**: Late gadolinium enhancement magnetic resonance imaging (LGE MRI) is commonly used to visualize and quantify left atrial (LA) scars. The position and extent of scars provide important information of the pathophysiology and progression of atrial fibrillation (AF). Hence, LA scar segmentation and quantification from LGE MRI can be useful in computer-assisted diagnosis and treatment stratification of AF patients. Since manual delineation can be time-consuming and subject to intra- and inter-expert variability, automating this computing is highly desired, which nevertheless is still challenging and under-researched.   This paper aims to provide a systematic review on computing methods for LA cavity, wall, scar and ablation gap segmentation and quantification from LGE MRI, and the related literature for AF studies. Specifically, we first summarize AF-related imaging techniques, particularly LGE MRI. Then, we review the methodologies of the four computing tasks in detail, and summarize the validation strategies applied in each task. Finally, the possible future developments are outlined, with a brief survey on the potential clinical applications of the aforementioned methods. The review shows that the research into this topic is still in early stages. Although several methods have been proposed, especially for LA segmentation, there is still large scope for further algorithmic developments due to performance issues related to the high variability of enhancement appearance and differences in image acquisition.



### Analyzing Adversarial Robustness of Deep Neural Networks in Pixel Space: a Semantic Perspective
- **Arxiv ID**: http://arxiv.org/abs/2106.09872v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.soc-ph, 68T-06
- **Links**: [PDF](http://arxiv.org/pdf/2106.09872v1)
- **Published**: 2021-06-18 02:16:01+00:00
- **Updated**: 2021-06-18 02:16:01+00:00
- **Authors**: Lina Wang, Xingshu Chen, Yulong Wang, Yawei Yue, Yi Zhu, Xuemei Zeng, Wei Wang
- **Comment**: 13 pages, 6figures
- **Journal**: None
- **Summary**: The vulnerability of deep neural networks to adversarial examples, which are crafted maliciously by modifying the inputs with imperceptible perturbations to misled the network produce incorrect outputs, reveals the lack of robustness and poses security concerns. Previous works study the adversarial robustness of image classifiers on image level and use all the pixel information in an image indiscriminately, lacking of exploration of regions with different semantic meanings in the pixel space of an image. In this work, we fill this gap and explore the pixel space of the adversarial image by proposing an algorithm to looking for possible perturbations pixel by pixel in different regions of the segmented image. The extensive experimental results on CIFAR-10 and ImageNet verify that searching for the modified pixel in only some pixels of an image can successfully launch the one-pixel adversarial attacks without requiring all the pixels of the entire image, and there exist multiple vulnerable points scattered in different regions of an image. We also demonstrate that the adversarial robustness of different regions on the image varies with the amount of semantic information contained.



### Towards Clustering-friendly Representations: Subspace Clustering via Graph Filtering
- **Arxiv ID**: http://arxiv.org/abs/2106.09874v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.09874v1)
- **Published**: 2021-06-18 02:21:36+00:00
- **Updated**: 2021-06-18 02:21:36+00:00
- **Authors**: Zhengrui Ma, Zhao Kang, Guangchun Luo, Ling Tian
- **Comment**: Published in ACM Multimedia 2020
- **Journal**: None
- **Summary**: Finding a suitable data representation for a specific task has been shown to be crucial in many applications. The success of subspace clustering depends on the assumption that the data can be separated into different subspaces. However, this simple assumption does not always hold since the raw data might not be separable into subspaces. To recover the ``clustering-friendly'' representation and facilitate the subsequent clustering, we propose a graph filtering approach by which a smooth representation is achieved. Specifically, it injects graph similarity into data features by applying a low-pass filter to extract useful data representations for clustering. Extensive experiments on image and document clustering datasets demonstrate that our method improves upon state-of-the-art subspace clustering techniques. Especially, its comparable performance with deep learning methods emphasizes the effectiveness of the simple graph filtering scheme for many real-world applications. An ablation study shows that graph filtering can remove noise, preserve structure in the image, and increase the separability of classes.



### Smoothed Multi-View Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/2106.09875v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.09875v1)
- **Published**: 2021-06-18 02:24:19+00:00
- **Updated**: 2021-06-18 02:24:19+00:00
- **Authors**: Peng Chen, Liang Liu, Zhengrui Ma, Zhao Kang
- **Comment**: Accepted by International Conference on Neural Computing for Advanced
  Applications 2021
- **Journal**: None
- **Summary**: In recent years, multi-view subspace clustering has achieved impressive performance due to the exploitation of complementary imformation across multiple views. However, multi-view data can be very complicated and are not easy to cluster in real-world applications. Most existing methods operate on raw data and may not obtain the optimal solution. In this work, we propose a novel multi-view clustering method named smoothed multi-view subspace clustering (SMVSC) by employing a novel technique, i.e., graph filtering, to obtain a smooth representation for each view, in which similar data points have similar feature values. Specifically, it retains the graph geometric features through applying a low-pass filter. Consequently, it produces a ``clustering-friendly" representation and greatly facilitates the downstream clustering task. Extensive experiments on benchmark datasets validate the superiority of our approach. Analysis shows that graph filtering increases the separability of classes.



### Quantized Neural Networks via {-1, +1} Encoding Decomposition and Acceleration
- **Arxiv ID**: http://arxiv.org/abs/2106.09886v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.09886v1)
- **Published**: 2021-06-18 03:11:15+00:00
- **Updated**: 2021-06-18 03:11:15+00:00
- **Authors**: Qigong Sun, Xiufang Li, Fanhua Shang, Hongying Liu, Kang Yang, Licheng Jiao, Zhouchen Lin
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1905.13389
- **Journal**: None
- **Summary**: The training of deep neural networks (DNNs) always requires intensive resources for both computation and data storage. Thus, DNNs cannot be efficiently applied to mobile phones and embedded devices, which severely limits their applicability in industrial applications. To address this issue, we propose a novel encoding scheme using {-1, +1} to decompose quantized neural networks (QNNs) into multi-branch binary networks, which can be efficiently implemented by bitwise operations (i.e., xnor and bitcount) to achieve model compression, computational acceleration, and resource saving. By using our method, users can achieve different encoding precisions arbitrarily according to their requirements and hardware resources. The proposed mechanism is highly suitable for the use of FPGA and ASIC in terms of data storage and computation, which provides a feasible idea for smart chips. We validate the effectiveness of our method on large-scale image classification (e.g., ImageNet), object detection, and semantic segmentation tasks. In particular, our method with low-bit encoding can still achieve almost the same performance as its high-bit counterparts.



### Medical Matting: A New Perspective on Medical Segmentation with Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2106.09887v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.09887v3)
- **Published**: 2021-06-18 03:13:52+00:00
- **Updated**: 2022-01-01 07:32:44+00:00
- **Authors**: Lin Wang, Lie Ju, Xin Wang, Wanji He, Donghao Zhang, Yelin Huang, Zhiwen Yang, Xuan Yao, Xin Zhao, Xiufen Ye, Zongyuan Ge
- **Comment**: Jounal version of our MICCAI 2021 paper, in review
- **Journal**: None
- **Summary**: It is difficult to accurately label ambiguous and complex shaped targets manually by binary masks. The weakness of binary mask under-expression is highlighted in medical image segmentation, where blurring is prevalent. In the case of multiple annotations, reaching a consensus for clinicians by binary masks is more challenging. Moreover, these uncertain areas are related to the lesions' structure and may contain anatomical information beneficial to diagnosis. However, current studies on uncertainty mainly focus on the uncertainty in model training and data labels. None of them investigate the influence of the ambiguous nature of the lesion itself.Inspired by image matting, this paper introduces alpha matte as a soft mask to represent uncertain areas in medical scenes and accordingly puts forward a new uncertainty quantification method to fill the gap of uncertainty research for lesion structure. In this work, we introduce a new architecture to generate binary masks and alpha mattes in a multitasking framework, which outperforms all state-of-the-art matting algorithms compared. The proposed uncertainty map is able to highlight the ambiguous regions and a novel multitasking loss weighting strategy we presented can improve performance further and demonstrate their concrete benefits. To fully-evaluate the effectiveness of our proposed method, we first labelled three medical datasets with alpha matte to address the shortage of available matting datasets in medical scenes and prove the alpha matte to be a more efficient labeling method than a binary mask from both qualitative and quantitative aspects.



### GEM: A General Evaluation Benchmark for Multimodal Tasks
- **Arxiv ID**: http://arxiv.org/abs/2106.09889v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.09889v1)
- **Published**: 2021-06-18 03:14:13+00:00
- **Updated**: 2021-06-18 03:14:13+00:00
- **Authors**: Lin Su, Nan Duan, Edward Cui, Lei Ji, Chenfei Wu, Huaishao Luo, Yongfei Liu, Ming Zhong, Taroon Bharti, Arun Sacheti
- **Comment**: Accepted by Findings of ACL 2021
- **Journal**: None
- **Summary**: In this paper, we present GEM as a General Evaluation benchmark for Multimodal tasks. Different from existing datasets such as GLUE, SuperGLUE, XGLUE and XTREME that mainly focus on natural language tasks, GEM is a large-scale vision-language benchmark, which consists of GEM-I for image-language tasks and GEM-V for video-language tasks. Comparing with existing multimodal datasets such as MSCOCO and Flicker30K for image-language tasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the largest vision-language dataset covering image-language tasks and video-language tasks at the same time, but also labeled in multiple languages. We also provide two baseline models for this benchmark. We will release the dataset, code and baseline models, aiming to advance the development of multilingual multimodal research.



### Development of a conversing and body temperature scanning autonomously navigating robot to help screen for COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2106.09894v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2106.09894v1)
- **Published**: 2021-06-18 03:30:11+00:00
- **Updated**: 2021-06-18 03:30:11+00:00
- **Authors**: Ryan Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Throughout the COVID-19 pandemic, the most common symptom displayed by patients has been a fever, leading to the use of temperature scanning as a preemptive measure to detect potential carriers of the virus. Human employees with handheld thermometers have been used to fulfill this task, however this puts them at risk as they cannot be physically distanced and the sequential nature of this method leads to great inconveniences and inefficiency. The proposed solution is an autonomously navigating robot capable of conversing and scanning people's temperature to detect fevers and help screen for COVID-19. To satisfy this objective, the robot must be able to (1) navigate autonomously, (2) detect and track people, and (3) get individuals' temperature reading and converse with them if it exceeds 38{\deg}C. An autonomously navigating mobile robot is used with a manipulator controlled using a face tracking algorithm, and an end effector consisting of a thermal camera, smartphone, and chatbot. The goal is to develop a functioning solution that performs the above tasks. In addition, technical challenges encountered and their engineering solutions will be presented, and recommendations will be made for enhancements that could be incorporated when approaching commercialization.



### Light Lies: Optical Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2106.09908v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.09908v2)
- **Published**: 2021-06-18 04:20:49+00:00
- **Updated**: 2021-07-14 05:57:41+00:00
- **Authors**: Kyulim Kim, JeongSoo Kim, Seungri Song, Jun-Ho Choi, Chulmin Joo, Jong-Seok Lee
- **Comment**: 11 pages, 4 figures, author names corrected
- **Journal**: None
- **Summary**: A significant amount of work has been done on adversarial attacks that inject imperceptible noise to images to deteriorate the image classification performance of deep models. However, most of the existing studies consider attacks in the digital (pixel) domain where an image acquired by an image sensor with sampling and quantization has been recorded. This paper, for the first time, introduces an optical adversarial attack, which physically alters the light field information arriving at the image sensor so that the classification model yields misclassification. More specifically, we modulate the phase of the light in the Fourier domain using a spatial light modulator placed in the photographic system. The operative parameters of the modulator are obtained by gradient-based optimization to maximize cross-entropy and minimize distortions. We present experiments based on both simulation and a real hardware optical system, from which the feasibility of the proposed optical attack is demonstrated. It is also verified that the proposed attack is completely different from common optical-domain distortions such as spherical aberration, defocus, and astigmatism in terms of both perturbation patterns and classification results.



### A Unified Generative Adversarial Network Training via Self-Labeling and Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/2106.09914v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.09914v1)
- **Published**: 2021-06-18 04:40:26+00:00
- **Updated**: 2021-06-18 04:40:26+00:00
- **Authors**: Tomoki Watanabe, Paolo Favaro
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel GAN training scheme that can handle any level of labeling in a unified manner. Our scheme introduces a form of artificial labeling that can incorporate manually defined labels, when available, and induce an alignment between them. To define the artificial labels, we exploit the assumption that neural network generators can be trained more easily to map nearby latent vectors to data with semantic similarities, than across separate categories. We use generated data samples and their corresponding artificial conditioning labels to train a classifier. The classifier is then used to self-label real data. To boost the accuracy of the self-labeling, we also use the exponential moving average of the classifier. However, because the classifier might still make mistakes, especially at the beginning of the training, we also refine the labels through self-attention, by using the labeling of real data samples only when the classifier outputs a high classification probability score. We evaluate our approach on CIFAR-10, STL-10 and SVHN, and show that both self-labeling and self-attention consistently improve the quality of generated data. More surprisingly, we find that the proposed scheme can even outperform class-conditional GANs.



### A Framework for Real-time Traffic Trajectory Tracking, Speed Estimation, and Driver Behavior Calibration at Urban Intersections Using Virtual Traffic Lanes
- **Arxiv ID**: http://arxiv.org/abs/2106.09932v1
- **DOI**: 10.1109/ITSC48978.2021.9564525
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.09932v1)
- **Published**: 2021-06-18 06:15:53+00:00
- **Updated**: 2021-06-18 06:15:53+00:00
- **Authors**: Awad Abdelhalim, Montasir Abbas, Bhavi Bharat Kotha, Alfred Wicks
- **Comment**: None
- **Journal**: None
- **Summary**: In a previous study, we presented VT-Lane, a three-step framework for real-time vehicle detection, tracking, and turn movement classification at urban intersections. In this study, we present a case study incorporating the highly accurate trajectories and movement classification obtained via VT-Lane for the purpose of speed estimation and driver behavior calibration for traffic at urban intersections. First, we use a highly instrumented vehicle to verify the estimated speeds obtained from video inference. The results of the speed validation show that our method can estimate the average travel speed of detected vehicles in real-time with an error of 0.19 m/sec, which is equivalent to 2% of the average observed travel speeds in the intersection of the study. Instantaneous speeds (at the resolution of 30 Hz) were found to be estimated with an average error of 0.21 m/sec and 0.86 m/sec respectively for free-flowing and congested traffic conditions. We then use the estimated speeds to calibrate the parameters of a driver behavior model for the vehicles in the area of study. The results show that the calibrated model replicates the driving behavior with an average error of 0.45 m/sec, indicating the high potential for using this framework for automated, large-scale calibration of car-following models from roadside traffic video data, which can lead to substantial improvements in traffic modeling via microscopic simulation.



### Universum GANs: Improving GANs through contradictions
- **Arxiv ID**: http://arxiv.org/abs/2106.09946v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2106.09946v2)
- **Published**: 2021-06-18 06:51:35+00:00
- **Updated**: 2022-09-20 18:36:01+00:00
- **Authors**: Sauptik Dhar, Javad Heydari, Samarth Tripathi, Unmesh Kurup, Mohak Shah
- **Comment**: Generative Adversarial Networks, Universum Learning, Semi-Supervised
  Learning, Synthetic Data
- **Journal**: None
- **Summary**: Limited availability of labeled-data makes any supervised learning problem challenging. Alternative learning settings like semi-supervised and universum learning alleviate the dependency on labeled data, but still require a large amount of unlabeled data, which may be unavailable or expensive to acquire. GAN-based data generation methods have recently shown promise by generating synthetic samples to improve learning. However, most existing GAN based approaches either provide poor discriminator performance under limited labeled data settings; or results in low quality generated data. In this paper, we propose a Universum GAN game which provides improved discriminator accuracy under limited data settings, while generating high quality realistic data. We further propose an evolving discriminator loss which improves its convergence and generalization performance. We derive the theoretical guarantees and provide empirical results in support of our approach.



### Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2106.09947v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.09947v3)
- **Published**: 2021-06-18 06:57:58+00:00
- **Updated**: 2022-10-11 15:32:26+00:00
- **Authors**: Maura Pintor, Luca Demetrio, Angelo Sotgiu, Ambra Demontis, Nicholas Carlini, Battista Biggio, Fabio Roli
- **Comment**: Accepted at NeurIPS 2022
- **Journal**: None
- **Summary**: Evaluating robustness of machine-learning models to adversarial examples is a challenging problem. Many defenses have been shown to provide a false sense of robustness by causing gradient-based attacks to fail, and they have been broken under more rigorous evaluations. Although guidelines and best practices have been suggested to improve current adversarial robustness evaluations, the lack of automatic testing and debugging tools makes it difficult to apply these recommendations in a systematic manner. In this work, we overcome these limitations by: (i) categorizing attack failures based on how they affect the optimization of gradient-based attacks, while also unveiling two novel failures affecting many popular attack implementations and past evaluations; (ii) proposing six novel indicators of failure, to automatically detect the presence of such failures in the attack optimization process; and (iii) suggesting a systematic protocol to apply the corresponding fixes. Our extensive experimental analysis, involving more than 15 models in 3 distinct application domains, shows that our indicators of failure can be used to debug and improve current adversarial robustness evaluations, thereby providing a first concrete step towards automatizing and systematizing them. Our open-source code is available at: https://github.com/pralab/IndicatorsOfAttackFailure.



### Novelty Detection via Contrastive Learning with Negative Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.09958v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.09958v1)
- **Published**: 2021-06-18 07:26:15+00:00
- **Updated**: 2021-06-18 07:26:15+00:00
- **Authors**: Chengwei Chen, Yuan Xie, Shaohui Lin, Ruizhi Qiao, Jian Zhou, Xin Tan, Yi Zhang, Lizhuang Ma
- **Comment**: None
- **Journal**: IJCAI2021
- **Summary**: Novelty detection is the process of determining whether a query example differs from the learned training distribution. Previous methods attempt to learn the representation of the normal samples via generative adversarial networks (GANs). However, they will suffer from instability training, mode dropping, and low discriminative ability. Recently, various pretext tasks (e.g. rotation prediction and clustering) have been proposed for self-supervised learning in novelty detection. However, the learned latent features are still low discriminative. We overcome such problems by introducing a novel decoder-encoder framework. Firstly, a generative network (a.k.a. decoder) learns the representation by mapping the initialized latent vector to an image. In particular, this vector is initialized by considering the entire distribution of training data to avoid the problem of mode-dropping. Secondly, a contrastive network (a.k.a. encoder) aims to ``learn to compare'' through mutual information estimation, which directly helps the generative network to obtain a more discriminative representation by using a negative data augmentation strategy. Extensive experiments show that our model has significant superiority over cutting-edge novelty detectors and achieves new state-of-the-art results on some novelty detection benchmarks, e.g. CIFAR10 and DCASE. Moreover, our model is more stable for training in a non-adversarial manner, compared to other adversarial based novelty detection methods.



### Multi-Granularity Network with Modal Attention for Dense Affective Understanding
- **Arxiv ID**: http://arxiv.org/abs/2106.09964v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.09964v1)
- **Published**: 2021-06-18 07:37:06+00:00
- **Updated**: 2021-06-18 07:37:06+00:00
- **Authors**: Baoming Yan, Lin Wang, Ke Gao, Bo Gao, Xiao Liu, Chao Ban, Jiang Yang, Xiaobo Li
- **Comment**: Oral presentation at AUVi Workshop - CVPR 2021
- **Journal**: None
- **Summary**: Video affective understanding, which aims to predict the evoked expressions by the video content, is desired for video creation and recommendation. In the recent EEV challenge, a dense affective understanding task is proposed and requires frame-level affective prediction. In this paper, we propose a multi-granularity network with modal attention (MGN-MA), which employs multi-granularity features for better description of the target frame. Specifically, the multi-granularity features could be divided into frame-level, clips-level and video-level features, which corresponds to visual-salient content, semantic-context and video theme information. Then the modal attention fusion module is designed to fuse the multi-granularity features and emphasize more affection-relevant modals. Finally, the fused feature is fed into a Mixtures Of Experts (MOE) classifier to predict the expressions. Further employing model-ensemble post-processing, the proposed method achieves the correlation score of 0.02292 in the EEV challenge.



### HifiFace: 3D Shape and Semantic Prior Guided High Fidelity Face Swapping
- **Arxiv ID**: http://arxiv.org/abs/2106.09965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.09965v1)
- **Published**: 2021-06-18 07:39:09+00:00
- **Updated**: 2021-06-18 07:39:09+00:00
- **Authors**: Yuhan Wang, Xu Chen, Junwei Zhu, Wenqing Chu, Ying Tai, Chengjie Wang, Jilin Li, Yongjian Wu, Feiyue Huang, Rongrong Ji
- **Comment**: Accepted to IJCAI 2021, project website: https://johann.wang/HifiFace
- **Journal**: None
- **Summary**: In this work, we propose a high fidelity face swapping method, called HifiFace, which can well preserve the face shape of the source face and generate photo-realistic results. Unlike other existing face swapping works that only use face recognition model to keep the identity similarity, we propose 3D shape-aware identity to control the face shape with the geometric supervision from 3DMM and 3D face reconstruction method. Meanwhile, we introduce the Semantic Facial Fusion module to optimize the combination of encoder and decoder features and make adaptive blending, which makes the results more photo-realistic. Extensive experiments on faces in the wild demonstrate that our method can preserve better identity, especially on the face shape, and can generate more photo-realistic results than previous state-of-the-art methods.



### Towards interpreting computer vision based on transformation invariant optimization
- **Arxiv ID**: http://arxiv.org/abs/2106.09982v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2106.09982v1)
- **Published**: 2021-06-18 08:04:10+00:00
- **Updated**: 2021-06-18 08:04:10+00:00
- **Authors**: Chen Li, Jinzhe Jiang, Xin Zhang, Tonghuan Zhang, Yaqian Zhao, Dongdong Jiang, RenGang Li
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: Interpreting how does deep neural networks (DNNs) make predictions is a vital field in artificial intelligence, which hinders wide applications of DNNs. Visualization of learned representations helps we humans understand the vision of DNNs. In this work, visualized images that can activate the neural network to the target classes are generated by back-propagation method. Here, rotation and scaling operations are applied to introduce the transformation invariance in the image generating process, which we find a significant improvement on visualization effect. Finally, we show some cases that such method can help us to gain insight into neural networks.



### Advanced Hough-based method for on-device document localization
- **Arxiv ID**: http://arxiv.org/abs/2106.09987v1
- **DOI**: 10.18287/2412-6179-CO-895
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.09987v1)
- **Published**: 2021-06-18 08:17:45+00:00
- **Updated**: 2021-06-18 08:17:45+00:00
- **Authors**: D. V. Tropin, A. M. Ershov, D. P. Nikolaev, V. V. Arlazarov
- **Comment**: This is a preprint of the article submitted for publication in the
  journal "Computer Optics"
- **Journal**: None
- **Summary**: The demand for on-device document recognition systems increases in conjunction with the emergence of more strict privacy and security requirements. In such systems, there is no data transfer from the end device to a third-party information processing servers. The response time is vital to the user experience of on-device document recognition. Combined with the unavailability of discrete GPUs, powerful CPUs, or a large RAM capacity on consumer-grade end devices such as smartphones, the time limitations put significant constraints on the computational complexity of the applied algorithms for on-device execution.   In this work, we consider document location in an image without prior knowledge of the document content or its internal structure. In accordance with the published works, at least 5 systems offer solutions for on-device document location. All these systems use a location method which can be considered Hough-based. The precision of such systems seems to be lower than that of the state-of-the-art solutions which were not designed to account for the limited computational resources.   We propose an advanced Hough-based method. In contrast with other approaches, it accounts for the geometric invariants of the central projection model and combines both edge and color features for document boundary detection. The proposed method allowed for the second best result for SmartDoc dataset in terms of precision, surpassed by U-net like neural network. When evaluated on a more challenging MIDV-500 dataset, the proposed algorithm guaranteed the best precision compared to published methods. Our method retained the applicability to on-device computations.



### Accumulative Poisoning Attacks on Real-time Data
- **Arxiv ID**: http://arxiv.org/abs/2106.09993v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.09993v2)
- **Published**: 2021-06-18 08:29:53+00:00
- **Updated**: 2021-10-26 13:45:48+00:00
- **Authors**: Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, Jun Zhu
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Collecting training data from untrusted sources exposes machine learning services to poisoning adversaries, who maliciously manipulate training data to degrade the model accuracy. When trained on offline datasets, poisoning adversaries have to inject the poisoned data in advance before training, and the order of feeding these poisoned batches into the model is stochastic. In contrast, practical systems are more usually trained/fine-tuned on sequentially captured real-time data, in which case poisoning adversaries could dynamically poison each data batch according to the current model state. In this paper, we focus on the real-time settings and propose a new attacking strategy, which affiliates an accumulative phase with poisoning attacks to secretly (i.e., without affecting accuracy) magnify the destructive effect of a (poisoned) trigger batch. By mimicking online learning and federated learning on MNIST and CIFAR-10, we show that model accuracy significantly drops by a single update step on the trigger batch after the accumulative phase. Our work validates that a well-designed but straightforward attacking strategy can dramatically amplify the poisoning effects, with no need to explore complex techniques.



### Source-free Domain Adaptation via Avatar Prototype Generation and Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2106.15326v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15326v1)
- **Published**: 2021-06-18 08:30:54+00:00
- **Updated**: 2021-06-18 08:30:54+00:00
- **Authors**: Zhen Qiu, Yifan Zhang, Hongbin Lin, Shuaicheng Niu, Yanxia Liu, Qing Du, Mingkui Tan
- **Comment**: Accepted by IJCAI 2021
- **Journal**: None
- **Summary**: We study a practical domain adaptation task, called source-free unsupervised domain adaptation (UDA) problem, in which we cannot access source domain data due to data privacy issues but only a pre-trained source model and unlabeled target data are available. This task, however, is very difficult due to one key challenge: the lack of source data and target domain labels makes model adaptation very challenging. To address this, we propose to mine the hidden knowledge in the source model and exploit it to generate source avatar prototypes (i.e., representative features for each source class) as well as target pseudo labels for domain alignment. To this end, we propose a Contrastive Prototype Generation and Adaptation (CPGA) method. Specifically, CPGA consists of two stages: (1) prototype generation: by exploring the classification boundary information of the source model, we train a prototype generator to generate avatar prototypes via contrastive learning. (2) prototype adaptation: based on the generated source prototypes and target pseudo labels, we develop a new robust contrastive prototype adaptation strategy to align each pseudo-labeled target data to the corresponding source prototypes. Extensive experiments on three UDA benchmark datasets demonstrate the effectiveness and superiority of the proposed method.



### Equivariance-bridged SO(2)-Invariant Representation Learning using Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2106.09996v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.09996v2)
- **Published**: 2021-06-18 08:37:45+00:00
- **Updated**: 2021-10-21 01:15:22+00:00
- **Authors**: Sungwon Hwang, Hyungtae Lim, Hyun Myung
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Training a Convolutional Neural Network (CNN) to be robust against rotation has mostly been done with data augmentation. In this paper, another progressive vision of research direction is highlighted to encourage less dependence on data augmentation by achieving structural rotational invariance of a network. The deep equivariance-bridged SO(2) invariant network is proposed to echo such vision. First, Self-Weighted Nearest Neighbors Graph Convolutional Network (SWN-GCN) is proposed to implement Graph Convolutional Network (GCN) on the graph representation of an image to acquire rotationally equivariant representation, as GCN is more suitable for constructing deeper network than spectral graph convolution-based approaches. Then, invariant representation is eventually obtained with Global Average Pooling (GAP), a permutation-invariant operation suitable for aggregating high-dimensional representations, over the equivariant set of vertices retrieved from SWN-GCN. Our method achieves the state-of-the-art image classification performance on rotated MNIST and CIFAR-10 images, where the models are trained with a non-augmented dataset only. Quantitative validations over invariance of the representations also demonstrate strong invariance of deep representations of SWN-GCN over rotations.



### Improved Radar Localization on Lidar Maps Using Shared Embedding
- **Arxiv ID**: http://arxiv.org/abs/2106.10000v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10000v1)
- **Published**: 2021-06-18 08:40:04+00:00
- **Updated**: 2021-06-18 08:40:04+00:00
- **Authors**: Huan Yin, Yue Wang, Rong Xiong
- **Comment**: Extended abstract. Spotlight Talk at Radar Perception for All-Weather
  Autonomy Workshop of ICRA 2021
- **Journal**: None
- **Summary**: We present a heterogeneous localization framework for solving radar global localization and pose tracking on pre-built lidar maps. To bridge the gap of sensing modalities, deep neural networks are constructed to create shared embedding space for radar scans and lidar maps. Herein learned feature embeddings are supportive for similarity measurement, thus improving map retrieval and data matching respectively. In RobotCar and MulRan datasets, we demonstrate the effectiveness of the proposed framework with the comparison to Scan Context and RaLL. In addition, the proposed pose tracking pipeline is with less neural networks compared to the original RaLL.



### Shape Prior Non-Uniform Sampling Guided Real-time Stereo 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.10013v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10013v3)
- **Published**: 2021-06-18 09:14:55+00:00
- **Updated**: 2021-06-22 03:35:10+00:00
- **Authors**: Aqi Gao, Jiale Cao, Yanwei Pang
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Pseudo-LiDAR based 3D object detectors have gained popularity due to their high accuracy. However, these methods need dense depth supervision and suffer from inferior speed. To solve these two issues, a recently introduced RTS3D builds an efficient 4D Feature-Consistency Embedding (FCE) space for the intermediate representation of object without depth supervision. FCE space splits the entire object region into 3D uniform grid latent space for feature sampling point generation, which ignores the importance of different object regions. However, we argue that, compared with the inner region, the outer region plays a more important role for accurate 3D detection. To encode more information from the outer region, we propose a shape prior non-uniform sampling strategy that performs dense sampling in outer region and sparse sampling in inner region. As a result, more points are sampled from the outer region and more useful features are extracted for 3D detection. Further, to enhance the feature discrimination of each sampling point, we propose a high-level semantic enhanced FCE module to exploit more contextual information and suppress noise better. Experiments on the KITTI dataset are performed to show the effectiveness of the proposed method. Compared with the baseline RTS3D, our proposed method has 2.57% improvement on AP3d almost without extra network parameters. Moreover, our proposed method outperforms the state-of-the-art methods without extra supervision at a real-time speed.



### EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2021: Team M3EM Technical Report
- **Arxiv ID**: http://arxiv.org/abs/2106.10026v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10026v3)
- **Published**: 2021-06-18 10:03:30+00:00
- **Updated**: 2021-07-01 02:56:46+00:00
- **Authors**: Lijin Yang, Yifei Huang, Yusuke Sugano, Yoichi Sato
- **Comment**: None
- **Journal**: None
- **Summary**: In this report, we describe the technical details of our submission to the 2021 EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition. Leveraging multiple modalities has been proved to benefit the Unsupervised Domain Adaptation (UDA) task. In this work, we present Multi-Modal Mutual Enhancement Module (M3EM), a deep module for jointly considering information from multiple modalities to find the most transferable representations across domains. We achieve this by implementing two sub-modules for enhancing each modality using the context of other modalities. The first sub-module exchanges information across modalities through the semantic space, while the second sub-module finds the most transferable spatial region based on the consensus of all modalities.



### Learning and Meshing from Deep Implicit Surface Networks Using an Efficient Implementation of Analytic Marching
- **Arxiv ID**: http://arxiv.org/abs/2106.10031v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2106.10031v1)
- **Published**: 2021-06-18 10:06:28+00:00
- **Updated**: 2021-06-18 10:06:28+00:00
- **Authors**: Jiabao Lei, Kui Jia, Yi Ma
- **Comment**: arXiv admin note: text overlap with arXiv:2002.06597
- **Journal**: None
- **Summary**: Reconstruction of object or scene surfaces has tremendous applications in computer vision, computer graphics, and robotics. In this paper, we study a fundamental problem in this context about recovering a surface mesh from an implicit field function whose zero-level set captures the underlying surface. To achieve the goal, existing methods rely on traditional meshing algorithms; while promising, they suffer from loss of precision learned in the implicit surface networks, due to the use of discrete space sampling in marching cubes. Given that an MLP with activations of Rectified Linear Unit (ReLU) partitions its input space into a number of linear regions, we are motivated to connect this local linearity with a same property owned by the desired result of polygon mesh. More specifically, we identify from the linear regions, partitioned by an MLP based implicit function, the analytic cells and analytic faces that are associated with the function's zero-level isosurface. We prove that under mild conditions, the identified analytic faces are guaranteed to connect and form a closed, piecewise planar surface. Based on the theorem, we propose an algorithm of analytic marching, which marches among analytic cells to exactly recover the mesh captured by an implicit surface network. We also show that our theory and algorithm are equally applicable to advanced MLPs with shortcut connections and max pooling. Given the parallel nature of analytic marching, we contribute AnalyticMesh, a software package that supports efficient meshing of implicit surface networks via CUDA parallel computing, and mesh simplification for efficient downstream processing. We apply our method to different settings of generative shape modeling using implicit surface networks. Extensive experiments demonstrate our advantages over existing methods in terms of both meshing accuracy and efficiency.



### Training or Architecture? How to Incorporate Invariance in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.10044v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10044v1)
- **Published**: 2021-06-18 10:31:00+00:00
- **Updated**: 2021-06-18 10:31:00+00:00
- **Authors**: Kanchana Vaishnavi Gandikota, Jonas Geiping, Zorah Lähner, Adam Czapliński, Michael Moeller
- **Comment**: None
- **Journal**: None
- **Summary**: Many applications require the robustness, or ideally the invariance, of a neural network to certain transformations of input data. Most commonly, this requirement is addressed by either augmenting the training data, using adversarial training, or defining network architectures that include the desired invariance automatically. Unfortunately, the latter often relies on the ability to enlist all possible transformations, which make such approaches largely infeasible for infinite sets of transformations, such as arbitrary rotations or scaling. In this work, we propose a method for provably invariant network architectures with respect to group actions by choosing one element from a (possibly continuous) orbit based on a fixed criterion. In a nutshell, we intend to 'undo' any possible transformation before feeding the data into the actual network. We analyze properties of such approaches, extend them to equivariant networks, and demonstrate their advantages in terms of robustness as well as computational efficiency in several numerical examples. In particular, we investigate the robustness with respect to rotations of images (which can possibly hold up to discretization artifacts only) as well as the provable rotational and scaling invariance of 3D point cloud classification.



### Light Pollution Reduction in Nighttime Photography
- **Arxiv ID**: http://arxiv.org/abs/2106.10046v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10046v1)
- **Published**: 2021-06-18 10:38:13+00:00
- **Updated**: 2021-06-18 10:38:13+00:00
- **Authors**: Chang Liu, Xiaolin Wu
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Nighttime photographers are often troubled by light pollution of unwanted artificial lights. Artificial lights, after scattered by aerosols in the atmosphere, can inundate the starlight and degrade the quality of nighttime images, by reducing contrast and dynamic range and causing hazes. In this paper we develop a physically-based light pollution reduction (LPR) algorithm that can substantially alleviate the aforementioned degradations of perceptual quality and restore the pristine state of night sky. The key to the success of the proposed LPR algorithm is an inverse method to estimate the spatial radiance distribution and spectral signature of ground artificial lights. Extensive experiments are carried out to evaluate the efficacy and limitations of the LPR algorithm.



### Patch-Based Image Restoration using Expectation Propagation
- **Arxiv ID**: http://arxiv.org/abs/2106.15327v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2106.15327v2)
- **Published**: 2021-06-18 10:45:15+00:00
- **Updated**: 2021-11-10 16:35:03+00:00
- **Authors**: Dan Yao, Stephen McLaughlin, Yoann Altmann
- **Comment**: 27 pages
- **Journal**: None
- **Summary**: This paper presents a new Expectation Propagation (EP) framework for image restoration using patch-based prior distributions. While Monte Carlo techniques are classically used to sample from intractable posterior distributions, they can suffer from scalability issues in high-dimensional inference problems such as image restoration. To address this issue, EP is used here to approximate the posterior distributions using products of multivariate Gaussian densities. Moreover, imposing structural constraints on the covariance matrices of these densities allows for greater scalability and distributed computation. While the method is naturally suited to handle additive Gaussian observation noise, it can also be extended to non-Gaussian noise. Experiments conducted for denoising, inpainting and deconvolution problems with Gaussian and Poisson noise illustrate the potential benefits of such flexible approximate Bayesian method for uncertainty quantification in imaging problems, at a reduced computational cost compared to sampling techniques.



### Contrastive Learning of Generalized Game Representations
- **Arxiv ID**: http://arxiv.org/abs/2106.10060v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.10060v1)
- **Published**: 2021-06-18 11:17:54+00:00
- **Updated**: 2021-06-18 11:17:54+00:00
- **Authors**: Chintan Trivedi, Antonios Liapis, Georgios N. Yannakakis
- **Comment**: 8 pages, 7 figures, CoG
- **Journal**: None
- **Summary**: Representing games through their pixels offers a promising approach for building general-purpose and versatile game models. While games are not merely images, neural network models trained on game pixels often capture differences of the visual style of the image rather than the content of the game. As a result, such models cannot generalize well even within similar games of the same genre. In this paper we build on recent advances in contrastive learning and showcase its benefits for representation learning in games. Learning to contrast images of games not only classifies games in a more efficient manner; it also yields models that separate games in a more meaningful fashion by ignoring the visual style and focusing, instead, on their content. Our results in a large dataset of sports video games containing 100k images across 175 games and 10 game genres suggest that contrastive learning is better suited for learning generalized game representations compared to conventional supervised learning. The findings of this study bring us closer to universal visual encoders for games that can be reused across previously unseen games without requiring retraining or fine-tuning.



### Paradigm selection for Data Fusion of SAR and Multispectral Sentinel data applied to Land-Cover Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.11056v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11056v1)
- **Published**: 2021-06-18 11:36:54+00:00
- **Updated**: 2021-06-18 11:36:54+00:00
- **Authors**: Alessandro Sebastianelli, Maria Pia Del Rosso, Pierre Philippe Mathieu, Silvia Liberata Ullo
- **Comment**: This work has been submitted to the IEEE Geoscience and Remote
  Sensing Letters for possible publication. Copyright may be transferred
  without notice, after which this version may no longer be accessible
- **Journal**: None
- **Summary**: Data fusion is a well-known technique, becoming more and more popular in the Artificial Intelligence for Earth Observation (AI4EO) domain mainly due to its ability of reinforcing AI4EO applications by combining multiple data sources and thus bringing better results. On the other hand, like other methods for satellite data analysis, data fusion itself is also benefiting and evolving thanks to the integration of Artificial Intelligence (AI). In this letter, four data fusion paradigms, based on Convolutional Neural Networks (CNNs), are analyzed and implemented. The goals are to provide a systematic procedure for choosing the best data fusion framework, resulting in the best classification results, once the basic structure for the CNN has been defined, and to help interested researchers in their work when data fusion applied to remote sensing is involved. The procedure has been validated for land-cover classification but it can be transferred to other cases.



### Residual Contrastive Learning for Image Reconstruction: Learning Transferable Representations from Noisy Images
- **Arxiv ID**: http://arxiv.org/abs/2106.10070v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.10070v2)
- **Published**: 2021-06-18 11:37:05+00:00
- **Updated**: 2022-04-27 17:57:34+00:00
- **Authors**: Nanqing Dong, Matteo Maggioni, Yongxin Yang, Eduardo Pérez-Pellitero, Ales Leonardis, Steven McDonagh
- **Comment**: Accepted by IJCAI 2022
- **Journal**: None
- **Summary**: This paper is concerned with contrastive learning (CL) for low-level image restoration and enhancement tasks. We propose a new label-efficient learning paradigm based on residuals, residual contrastive learning (RCL), and derive an unsupervised visual representation learning framework, suitable for low-level vision tasks with noisy inputs. While supervised image reconstruction aims to minimize residual terms directly, RCL alternatively builds a connection between residuals and CL by defining a novel instance discrimination pretext task, using residuals as the discriminative feature. Our formulation mitigates the severe task misalignment between instance discrimination pretext tasks and downstream image reconstruction tasks, present in existing CL frameworks. Experimentally, we find that RCL can learn robust and transferable representations that improve the performance of various downstream tasks, such as denoising and super resolution, in comparison with recent self-supervised methods designed specifically for noisy inputs. Additionally, our unsupervised pre-training can significantly reduce annotation costs whilst maintaining performance competitive with fully-supervised image reconstruction.



### Combined Person Classification with Airborne Optical Sectioning
- **Arxiv ID**: http://arxiv.org/abs/2106.10077v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10077v1)
- **Published**: 2021-06-18 11:56:17+00:00
- **Updated**: 2021-06-18 11:56:17+00:00
- **Authors**: Indrajit Kurmi, David C. Schedl, Oliver Bimber
- **Comment**: 9 Pages, 7 Figures, 1 Table. This work has been submitted to the IEEE
  for possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
- **Journal**: None
- **Summary**: Fully autonomous drones have been demonstrated to find lost or injured persons under strongly occluding forest canopy. Airborne Optical Sectioning (AOS), a novel synthetic aperture imaging technique, together with deep-learning-based classification enables high detection rates under realistic search-and-rescue conditions. We demonstrate that false detections can be significantly suppressed and true detections boosted by combining classifications from multiple AOS rather than single integral images. This improves classification rates especially in the presence of occlusion. To make this possible, we modified the AOS imaging process to support large overlaps between subsequent integrals, enabling real-time and on-board scanning and processing of groundspeeds up to 10 m/s.



### Debiased Subjective Assessment of Real-World Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2106.10080v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10080v2)
- **Published**: 2021-06-18 12:03:35+00:00
- **Updated**: 2021-06-21 03:42:52+00:00
- **Authors**: Cao Peibei, Wang Zhangyang, Ma Kede
- **Comment**: None
- **Journal**: None
- **Summary**: In real-world image enhancement, it is often challenging (if not impossible) to acquire ground-truth data, preventing the adoption of distance metrics for objective quality assessment. As a result, one often resorts to subjective quality assessment, the most straightforward and reliable means of evaluating image enhancement. Conventional subjective testing requires manually pre-selecting a small set of visual examples, which may suffer from three sources of biases: 1) sampling bias due to the extremely sparse distribution of the selected samples in the image space; 2) algorithmic bias due to potential overfitting the selected samples; 3) subjective bias due to further potential cherry-picking test results. This eventually makes the field of real-world image enhancement more of an art than a science. Here we take steps towards debiasing conventional subjective assessment by automatically sampling a set of adaptive and diverse images for subsequent testing. This is achieved by casting sample selection into a joint maximization of the discrepancy between the enhancers and the diversity among the selected input images. Careful visual inspection on the resulting enhanced images provides a debiased ranking of the enhancement algorithms. We demonstrate our subjective assessment method using three popular and practically demanding image enhancement tasks: dehazing, super-resolution, and low-light enhancement.



### Discerning Generic Event Boundaries in Long-Form Wild Videos
- **Arxiv ID**: http://arxiv.org/abs/2106.10090v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.10090v1)
- **Published**: 2021-06-18 12:28:19+00:00
- **Updated**: 2021-06-18 12:28:19+00:00
- **Authors**: Ayush K Rai, Tarun Krishna, Julia Dietlmeier, Kevin McGuinness, Alan F Smeaton, Noel E O'Connor
- **Comment**: Technical Report for Generic Event Boundary Challenge - LOVEU
  Challenge (CVPR 2021)
- **Journal**: None
- **Summary**: Detecting generic, taxonomy-free event boundaries invideos represents a major stride forward towards holisticvideo understanding. In this paper we present a technique forgeneric event boundary detection based on a two stream in-flated 3D convolutions architecture, which can learn spatio-temporal features from videos. Our work is inspired from theGeneric Event Boundary Detection Challenge (part of CVPR2021 Long Form Video Understanding- LOVEU Workshop).Throughout the paper we provide an in-depth analysis ofthe experiments performed along with an interpretation ofthe results obtained.



### hSMAL: Detailed Horse Shape and Pose Reconstruction for Motion Pattern Recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.10102v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10102v1)
- **Published**: 2021-06-18 12:53:40+00:00
- **Updated**: 2021-06-18 12:53:40+00:00
- **Authors**: Ci Li, Nima Ghorbani, Sofia Broomé, Maheen Rashid, Michael J. Black, Elin Hernlund, Hedvig Kjellström, Silvia Zuffi
- **Comment**: CV4Animals Workshop in CVPR 2021
- **Journal**: None
- **Summary**: In this paper we present our preliminary work on model-based behavioral analysis of horse motion. Our approach is based on the SMAL model, a 3D articulated statistical model of animal shape. We define a novel SMAL model for horses based on a new template, skeleton and shape space learned from $37$ horse toys. We test the accuracy of our hSMAL model in reconstructing a horse from 3D mocap data and images. We apply the hSMAL model to the problem of lameness detection from video, where we fit the model to images to recover 3D pose and train an ST-GCN network on pose data. A comparison with the same network trained on mocap points illustrates the benefit of our approach.



### Towards Distraction-Robust Active Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2106.10110v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.10110v1)
- **Published**: 2021-06-18 13:05:25+00:00
- **Updated**: 2021-06-18 13:05:25+00:00
- **Authors**: Fangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang
- **Comment**: To appear in ICML2021
- **Journal**: None
- **Summary**: In active visual tracking, it is notoriously difficult when distracting objects appear, as distractors often mislead the tracker by occluding the target or bringing a confusing appearance. To address this issue, we propose a mixed cooperative-competitive multi-agent game, where a target and multiple distractors form a collaborative team to play against a tracker and make it fail to follow. Through learning in our game, diverse distracting behaviors of the distractors naturally emerge, thereby exposing the tracker's weakness, which helps enhance the distraction-robustness of the tracker. For effective learning, we then present a bunch of practical methods, including a reward function for distractors, a cross-modal teacher-student learning strategy, and a recurrent attention mechanism for the tracker. The experimental results show that our tracker performs desired distraction-robust active visual tracking and can be well generalized to unseen environments. We also show that the multi-agent game can be used to adversarially test the robustness of trackers.



### Virtual Temporal Samples for Recurrent Neural Networks: applied to semantic segmentation in agriculture
- **Arxiv ID**: http://arxiv.org/abs/2106.10118v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.10118v2)
- **Published**: 2021-06-18 13:15:54+00:00
- **Updated**: 2021-09-05 20:29:23+00:00
- **Authors**: Alireza Ahmadi, Michael Halstead, Chris McCool
- **Comment**: None
- **Journal**: None
- **Summary**: This paper explores the potential for performing temporal semantic segmentation in the context of agricultural robotics without temporally labelled data. We achieve this by proposing to generate virtual temporal samples from labelled still images. By exploiting the relatively static scene and assuming that the robot (camera) moves we are able to generate virtually labelled temporal sequences with no extra annotation effort. Normally, to train a recurrent neural network (RNN), labelled samples from a video (temporal) sequence are required which is laborious and has stymied work in this direction. By generating virtual temporal samples, we demonstrate that it is possible to train a lightweight RNN to perform semantic segmentation on two challenging agricultural datasets. Our results show that by training a temporal semantic segmenter using virtual samples we can increase the performance by an absolute amount of $4.6$ and $4.9$ on sweet pepper and sugar beet datasets, respectively. This indicates that our virtual data augmentation technique is able to accurately classify agricultural images temporally without the use of complicated synthetic data generation techniques nor with the overhead of labelling large amounts of temporal sequences.



### Self-supervised Video Representation Learning with Cross-Stream Prototypical Contrasting
- **Arxiv ID**: http://arxiv.org/abs/2106.10137v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10137v3)
- **Published**: 2021-06-18 13:57:51+00:00
- **Updated**: 2021-10-20 14:51:14+00:00
- **Authors**: Martine Toering, Ioannis Gatopoulos, Maarten Stol, Vincent Tao Hu
- **Comment**: WACV2022
- **Journal**: None
- **Summary**: Instance-level contrastive learning techniques, which rely on data augmentation and a contrastive loss function, have found great success in the domain of visual representation learning. They are not suitable for exploiting the rich dynamical structure of video however, as operations are done on many augmented instances. In this paper we propose "Video Cross-Stream Prototypical Contrasting", a novel method which predicts consistent prototype assignments from both RGB and optical flow views, operating on sets of samples. Specifically, we alternate the optimization process; while optimizing one of the streams, all views are mapped to one set of stream prototype vectors. Each of the assignments is predicted with all views except the one matching the prediction, pushing representations closer to their assigned prototypes. As a result, more efficient video embeddings with ingrained motion information are learned, without the explicit need for optical flow computation during inference. We obtain state-of-the-art results on nearest-neighbour video retrieval and action recognition, outperforming previous best by +3.2% on UCF101 using the S3D backbone (90.5% Top-1 acc), and by +7.2% on UCF101 and +15.1% on HMDB51 using the R(2+1)D backbone.



### Deep Learning for Multi-View Stereo via Plane Sweep: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2106.15328v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15328v2)
- **Published**: 2021-06-18 14:10:44+00:00
- **Updated**: 2021-07-29 06:44:44+00:00
- **Authors**: Qingtian Zhu, Chen Min, Zizhuang Wei, Yisong Chen, Guoping Wang
- **Comment**: None
- **Journal**: None
- **Summary**: 3D reconstruction has lately attracted increasing attention due to its wide application in many areas, such as autonomous driving, robotics and virtual reality. As a dominant technique in artificial intelligence, deep learning has been successfully adopted to solve various computer vision problems. However, deep learning for 3D reconstruction is still at its infancy due to its unique challenges and varying pipelines. To stimulate future research, this paper presents a review of recent progress in deep learning methods for Multi-view Stereo (MVS), which is considered as a crucial task of image-based 3D reconstruction. It also presents comparative results on several publicly available datasets, with insightful observations and inspiring future research directions.



### All You Can Embed: Natural Language based Vehicle Retrieval with Spatio-Temporal Transformers
- **Arxiv ID**: http://arxiv.org/abs/2106.10153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10153v1)
- **Published**: 2021-06-18 14:38:51+00:00
- **Updated**: 2021-06-18 14:38:51+00:00
- **Authors**: Carmelo Scribano, Davide Sapienza, Giorgia Franchini, Micaela Verucchi, Marko Bertogna
- **Comment**: CVPR 2021 AI CITY CHALLENGE Natural Language-Based Vehicle Retrieval
- **Journal**: None
- **Summary**: Combining Natural Language with Vision represents a unique and interesting challenge in the domain of Artificial Intelligence. The AI City Challenge Track 5 for Natural Language-Based Vehicle Retrieval focuses on the problem of combining visual and textual information, applied to a smart-city use case. In this paper, we present All You Can Embed (AYCE), a modular solution to correlate single-vehicle tracking sequences with natural language. The main building blocks of the proposed architecture are (i) BERT to provide an embedding of the textual descriptions, (ii) a convolutional backbone along with a Transformer model to embed the visual information. For the training of the retrieval model, a variation of the Triplet Margin Loss is proposed to learn a distance measure between the visual and language embeddings. The code is publicly available at https://github.com/cscribano/AYCE_2021.



### World-GAN: a Generative Model for Minecraft Worlds
- **Arxiv ID**: http://arxiv.org/abs/2106.10155v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2106.10155v1)
- **Published**: 2021-06-18 14:45:39+00:00
- **Updated**: 2021-06-18 14:45:39+00:00
- **Authors**: Maren Awiszus, Frederik Schubert, Bodo Rosenhahn
- **Comment**: 8 pages, 8 figures, IEEE Conference on Games (CoG) 2021
- **Journal**: None
- **Summary**: This work introduces World-GAN, the first method to perform data-driven Procedural Content Generation via Machine Learning in Minecraft from a single example. Based on a 3D Generative Adversarial Network (GAN) architecture, we are able to create arbitrarily sized world snippets from a given sample. We evaluate our approach on creations from the community as well as structures generated with the Minecraft World Generator. Our method is motivated by the dense representations used in Natural Language Processing (NLP) introduced with word2vec [1]. The proposed block2vec representations make World-GAN independent from the number of different blocks, which can vary a lot in Minecraft, and enable the generation of larger levels. Finally, we demonstrate that changing this new representation space allows us to change the generated style of an already trained generator. World-GAN enables its users to generate Minecraft worlds based on parts of their creations.



### Toward Fault Detection in Industrial Welding Processes with Deep Learning and Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.10160v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.10160v1)
- **Published**: 2021-06-18 14:52:49+00:00
- **Updated**: 2021-06-18 14:52:49+00:00
- **Authors**: Jibinraj Antony, Dr. Florian Schlather, Georgij Safronov, Markus Schmitz, Prof. Dr. Kristof Van Laerhoven
- **Comment**: None
- **Journal**: None
- **Summary**: With the rise of deep learning models in the field of computer vision, new possibilities for their application in industrial processes proves to return great benefits. Nevertheless, the actual fit of machine learning for highly standardised industrial processes is still under debate. This paper addresses the challenges on the industrial realization of the AI tools, considering the use case of Laser Beam Welding quality control as an example. We use object detection algorithms from the TensorFlow object detection API and adapt them to our use case using transfer learning. The baseline models we develop are used as benchmarks and evaluated and compared to models that undergo dataset scaling and hyperparameter tuning. We find that moderate scaling of the dataset via image augmentation leads to improvements in intersection over union (IoU) and recall, whereas high levels of augmentation and scaling may lead to deterioration of results. Finally, we put our results into perspective of the underlying use case and evaluate their fit.



### Steerable Partial Differential Operators for Equivariant Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.10163v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10163v3)
- **Published**: 2021-06-18 14:58:19+00:00
- **Updated**: 2022-04-23 18:09:15+00:00
- **Authors**: Erik Jenner, Maurice Weiler
- **Comment**: Published at ICLR 2022, code available at
  https://github.com/ejnnr/steerable_pdos
- **Journal**: None
- **Summary**: Recent work in equivariant deep learning bears strong similarities to physics. Fields over a base space are fundamental entities in both subjects, as are equivariant maps between these fields. In deep learning, however, these maps are usually defined by convolutions with a kernel, whereas they are partial differential operators (PDOs) in physics. Developing the theory of equivariant PDOs in the context of deep learning could bring these subjects even closer together and lead to a stronger flow of ideas. In this work, we derive a $G$-steerability constraint that completely characterizes when a PDO between feature vector fields is equivariant, for arbitrary symmetry groups $G$. We then fully solve this constraint for several important groups. We use our solutions as equivariant drop-in replacements for convolutional layers and benchmark them in that role. Finally, we develop a framework for equivariant maps based on Schwartz distributions that unifies classical convolutions and differential operators and gives insight about the relation between the two.



### Non-Iterative Phase Retrieval With Cascaded Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.10195v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10195v1)
- **Published**: 2021-06-18 15:52:12+00:00
- **Updated**: 2021-06-18 15:52:12+00:00
- **Authors**: Tobias Uelwer, Tobias Hoffmann, Stefan Harmeling
- **Comment**: Accepted at the 30th International Conference on Artificial Neural
  Networks (ICANN 2021)
- **Journal**: None
- **Summary**: Fourier phase retrieval is the problem of reconstructing a signal given only the magnitude of its Fourier transformation. Optimization-based approaches, like the well-established Gerchberg-Saxton or the hybrid input output algorithm, struggle at reconstructing images from magnitudes that are not oversampled. This motivates the application of learned methods, which allow reconstruction from non-oversampled magnitude measurements after a learning phase. In this paper, we want to push the limits of these learned methods by means of a deep neural network cascade that reconstructs the image successively on different resolutions from its non-oversampled Fourier magnitude. We evaluate our method on four different datasets (MNIST, EMNIST, Fashion-MNIST, and KMNIST) and demonstrate that it yields improved performance over other non-iterative methods and optimization-based methods.



### A Dynamic Spatial-temporal Attention Network for Early Anticipation of Traffic Accidents
- **Arxiv ID**: http://arxiv.org/abs/2106.10197v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.10197v2)
- **Published**: 2021-06-18 15:58:53+00:00
- **Updated**: 2021-12-21 00:43:09+00:00
- **Authors**: Muhammad Monjurul Karim, Yu Li, Ruwen Qin, Zhaozheng Yin
- **Comment**: 10 pages, 4 figures, submitted to a journal
- **Journal**: None
- **Summary**: The rapid advancement of sensor technologies and artificial intelligence are creating new opportunities for traffic safety enhancement. Dashboard cameras (dashcams) have been widely deployed on both human driving vehicles and automated driving vehicles. A computational intelligence model that can accurately and promptly predict accidents from the dashcam video will enhance the preparedness for accident prevention. The spatial-temporal interaction of traffic agents is complex. Visual cues for predicting a future accident are embedded deeply in dashcam video data. Therefore, the early anticipation of traffic accidents remains a challenge. Inspired by the attention behavior of humans in visually perceiving accident risks, this paper proposes a Dynamic Spatial-Temporal Attention (DSTA) network for the early accident anticipation from dashcam videos. The DSTA-network learns to select discriminative temporal segments of a video sequence with a Dynamic Temporal Attention (DTA) module. It also learns to focus on the informative spatial regions of frames with a Dynamic Spatial Attention (DSA) module. A Gated Recurrent Unit (GRU) is trained jointly with the attention modules to predict the probability of a future accident. The evaluation of the DSTA-network on two benchmark datasets confirms that it has exceeded the state-of-the-art performance. A thorough ablation study that assesses the DSTA-network at the component level reveals how the network achieves such performance. Furthermore, this paper proposes a method to fuse the prediction scores from two complementary models and verifies its effectiveness in further boosting the performance of early accident anticipation.



### Reliability and Validity of Image-Based and Self-Reported Skin Phenotype Metrics
- **Arxiv ID**: http://arxiv.org/abs/2106.11240v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11240v1)
- **Published**: 2021-06-18 16:12:24+00:00
- **Updated**: 2021-06-18 16:12:24+00:00
- **Authors**: John J. Howard, Yevgeniy B. Sirotin, Jerry L. Tipton, Arun R. Vemury
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: With increasing adoption of face recognition systems, it is important to ensure adequate performance of these technologies across demographic groups. Recently, phenotypes such as skin-tone, have been proposed as superior alternatives to traditional race categories when exploring performance differentials. However, there is little consensus regarding how to appropriately measure skin-tone in evaluations of biometric performance or in AI more broadly. In this study, we explore the relationship between face-area-lightness-measures (FALMs) estimated from images and ground-truth skin readings collected using a device designed to measure human skin. FALMs estimated from different images of the same individual varied significantly relative to ground-truth FALM. This variation was only reduced by greater control of acquisition (camera, background, and environment). Next, we compare ground-truth FALM to Fitzpatrick Skin Types (FST) categories obtained using the standard, in-person, medical survey and show FST is poorly predictive of skin-tone. Finally, we show how noisy estimation of FALM leads to errors selecting explanatory factors for demographic differentials. These results demonstrate that measures of skin-tone for biometric performance evaluations must come from objective, characterized, and controlled sources. Further, despite this being a currently practiced approach, estimating FST categories and FALMs from uncontrolled imagery does not provide an appropriate measure of skin-tone.



### Residual Error: a New Performance Measure for Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2106.10212v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2106.10212v1)
- **Published**: 2021-06-18 16:34:23+00:00
- **Updated**: 2021-06-18 16:34:23+00:00
- **Authors**: Hossein Aboutalebi, Mohammad Javad Shafiee, Michelle Karg, Christian Scharfenberger, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the significant advances in deep learning over the past decade, a major challenge that limits the wide-spread adoption of deep learning has been their fragility to adversarial attacks. This sensitivity to making erroneous predictions in the presence of adversarially perturbed data makes deep neural networks difficult to adopt for certain real-world, mission-critical applications. While much of the research focus has revolved around adversarial example creation and adversarial hardening, the area of performance measures for assessing adversarial robustness is not well explored. Motivated by this, this study presents the concept of residual error, a new performance measure for not only assessing the adversarial robustness of a deep neural network at the individual sample level, but also can be used to differentiate between adversarial and non-adversarial examples to facilitate for adversarial example detection. Furthermore, we introduce a hybrid model for approximating the residual error in a tractable manner. Experimental results using the case of image classification demonstrates the effectiveness and efficacy of the proposed residual error metric for assessing several well-known deep neural network architectures. These results thus illustrate that the proposed measure could be a useful tool for not only assessing the robustness of deep neural networks used in mission-critical scenarios, but also in the design of adversarially robust models.



### A Coarse-to-Fine Instance Segmentation Network with Learning Boundary Representation
- **Arxiv ID**: http://arxiv.org/abs/2106.10213v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.10213v1)
- **Published**: 2021-06-18 16:37:28+00:00
- **Updated**: 2021-06-18 16:37:28+00:00
- **Authors**: Feng Luo, Bin-Bin Gao, Jiangpeng Yan, Xiu Li
- **Comment**: 8 pages, Accepted by IJCNN 2021
- **Journal**: None
- **Summary**: Boundary-based instance segmentation has drawn much attention since of its attractive efficiency. However, existing methods suffer from the difficulty in long-distance regression. In this paper, we propose a coarse-to-fine module to address the problem. Approximate boundary points are generated at the coarse stage and then features of these points are sampled and fed to a refined regressor for fine prediction. It is end-to-end trainable since differential sampling operation is well supported in the module. Furthermore, we design a holistic boundary-aware branch and introduce instance-agnostic supervision to assist regression. Equipped with ResNet-101, our approach achieves 31.7\% mask AP on COCO dataset with single-scale training and testing, outperforming the baseline 1.3\% mask AP with less than 1\% additional parameters and GFLOPs. Experiments also show that our proposed method achieves competitive performance compared to existing boundary-based methods with a lightweight design and a simple pipeline.



### VSAC: Efficient and Accurate Estimator for H and F
- **Arxiv ID**: http://arxiv.org/abs/2106.10240v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10240v2)
- **Published**: 2021-06-18 17:04:57+00:00
- **Updated**: 2021-09-13 15:00:32+00:00
- **Authors**: Maksym Ivashechkin, Daniel Barath, Jiri Matas
- **Comment**: None
- **Journal**: None
- **Summary**: We present VSAC, a RANSAC-type robust estimator with a number of novelties. It benefits from the introduction of the concept of independent inliers that improves significantly the efficacy of the dominant plane handling and, also, allows near error-free rejection of incorrect models, without false positives. The local optimization process and its application is improved so that it is run on average only once. Further technical improvements include adaptive sequential hypothesis verification and efficient model estimation via Gaussian elimination. Experiments on four standard datasets show that VSAC is significantly faster than all its predecessors and runs on average in 1-2 ms, on a CPU. It is two orders of magnitude faster and yet as precise as MAGSAC++, the currently most accurate estimator of two-view geometry. In the repeated runs on EVD, HPatches, PhotoTourism, and Kusvod2 datasets, it never failed.



### Bridging the Gap Between Object Detection and User Intent via Query-Modulation
- **Arxiv ID**: http://arxiv.org/abs/2106.10258v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.10258v2)
- **Published**: 2021-06-18 17:47:53+00:00
- **Updated**: 2022-08-03 15:39:05+00:00
- **Authors**: Marco Fornoni, Chaochao Yan, Liangchen Luo, Kimberly Wilber, Alex Stark, Yin Cui, Boqing Gong, Andrew Howard
- **Comment**: None
- **Journal**: None
- **Summary**: When interacting with objects through cameras, or pictures, users often have a specific intent. For example, they may want to perform a visual search. With most object detection models relying on image pixels as their sole input, undesired results are not uncommon. Most typically: lack of a high-confidence detection on the object of interest, or detection with a wrong class label. The issue is especially severe when operating capacity-constrained mobile object detectors on-device. In this paper we investigate techniques to modulate mobile detectors to explicitly account for the user intent, expressed as an embedding of a simple query. Compared to standard detectors, query-modulated detectors show superior performance at detecting objects for a given user query. Thanks to large-scale training data synthesized from standard object detection annotations, query-modulated detectors also outperform a specialized referring expression recognition system. Query-modulated detectors can also be trained to simultaneously solve for both localizing a user query and standard detection, even outperforming standard mobile detectors at the canonical COCO task.



### How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2106.10270v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.10270v2)
- **Published**: 2021-06-18 17:58:20+00:00
- **Updated**: 2022-06-23 10:51:04+00:00
- **Authors**: Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, Lucas Beyer
- **Comment**: Andreas, Alex, Xiaohua and Lucas contributed equally. We release more
  than 50'000 ViT models trained under diverse settings on various datasets.
  Available at https://github.com/google-research/big_vision,
  https://github.com/google-research/vision_transformer and
  https://github.com/rwightman/pytorch-image-models TMLR review at
  https://openreview.net/forum?id=4nPswr1KcP
- **Journal**: Transactions on Machine Learning Research (05/2022)
- **Summary**: Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation ("AugReg" for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.



### End-to-end Temporal Action Detection with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2106.10271v4
- **DOI**: 10.1109/TIP.2022.3195321
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10271v4)
- **Published**: 2021-06-18 17:58:34+00:00
- **Updated**: 2022-08-11 14:04:47+00:00
- **Authors**: Xiaolong Liu, Qimeng Wang, Yao Hu, Xu Tang, Shiwei Zhang, Song Bai, Xiang Bai
- **Comment**: Accepted by IEEE Transactions on Image Processing (TIP). Code:
  https://github.com/xlliu7/TadTR
- **Journal**: None
- **Summary**: Temporal action detection (TAD) aims to determine the semantic label and the temporal interval of every action instance in an untrimmed video. It is a fundamental and challenging task in video understanding. Previous methods tackle this task with complicated pipelines. They often need to train multiple networks and involve hand-designed operations, such as non-maximal suppression and anchor generation, which limit the flexibility and prevent end-to-end learning. In this paper, we propose an end-to-end Transformer-based method for TAD, termed TadTR. Given a small set of learnable embeddings called action queries, TadTR adaptively extracts temporal context information from the video for each query and directly predicts action instances with the context. To adapt Transformer to TAD, we propose three improvements to enhance its locality awareness. The core is a temporal deformable attention module that selectively attends to a sparse set of key snippets in a video. A segment refinement mechanism and an actionness regression head are designed to refine the boundaries and confidence of the predicted instances, respectively. With such a simple pipeline, TadTR requires lower computation cost than previous detectors, while preserving remarkable performance. As a self-contained detector, it achieves state-of-the-art performance on THUMOS14 (56.7% mAP) and HACS Segments (32.09% mAP). Combined with an extra action classifier, it obtains 36.75% mAP on ActivityNet-1.3. Code is available at https://github.com/xlliu7/TadTR.



### Towards Single Stage Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.10309v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.10309v2)
- **Published**: 2021-06-18 18:34:50+00:00
- **Updated**: 2021-06-28 15:23:42+00:00
- **Authors**: Peri Akiva, Kristin Dana
- **Comment**: None
- **Journal**: None
- **Summary**: The costly process of obtaining semantic segmentation labels has driven research towards weakly supervised semantic segmentation (WSSS) methods, using only image-level, point, or box labels. The lack of dense scene representation requires methods to increase complexity to obtain additional semantic information about the scene, often done through multiple stages of training and refinement. Current state-of-the-art (SOTA) models leverage image-level labels to produce class activation maps (CAMs) which go through multiple stages of refinement before they are thresholded to make pseudo-masks for supervision. The multi-stage approach is computationally expensive, and dependency on image-level labels for CAMs generation lacks generalizability to more complex scenes. In contrary, our method offers a single-stage approach generalizable to arbitrary dataset, that is trainable from scratch, without any dependency on pre-trained backbones, classification, or separate refinement tasks. We utilize point annotations to generate reliable, on-the-fly pseudo-masks through refined and filtered features. While our method requires point annotations that are only slightly more expensive than image-level annotations, we are to demonstrate SOTA performance on benchmark datasets (PascalVOC 2012), as well as significantly outperform other SOTA WSSS methods on recent real-world datasets (CRAID, CityPersons, IAD).



### A system of vision sensor based deep neural networks for complex driving scene analysis in support of crash risk assessment and prevention
- **Arxiv ID**: http://arxiv.org/abs/2106.10319v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.10319v1)
- **Published**: 2021-06-18 19:07:59+00:00
- **Updated**: 2021-06-18 19:07:59+00:00
- **Authors**: Muhammad Monjurul Karim, Yu Li, Ruwen Qin, Zhaozheng Yin
- **Comment**: 11 Pages, 8 Figures, Presented in TRB conference
- **Journal**: None
- **Summary**: To assist human drivers and autonomous vehicles in assessing crash risks, driving scene analysis using dash cameras on vehicles and deep learning algorithms is of paramount importance. Although these technologies are increasingly available, driving scene analysis for this purpose still remains a challenge. This is mainly due to the lack of annotated large image datasets for analyzing crash risk indicators and crash likelihood, and the lack of an effective method to extract lots of required information from complex driving scenes. To fill the gap, this paper develops a scene analysis system. The Multi-Net of the system includes two multi-task neural networks that perform scene classification to provide four labels for each scene. The DeepLab v3 and YOLO v3 are combined by the system to detect and locate risky pedestrians and the nearest vehicles. All identified information can provide the situational awareness to autonomous vehicles or human drivers for identifying crash risks from the surrounding traffic. To address the scarcity of annotated image datasets for studying traffic crashes, two completely new datasets have been developed by this paper and made available to the public, which were proved to be effective in training the proposed deep neural networks. The paper further evaluates the performance of the Multi-Net and the efficiency of the developed system. Comprehensive scene analysis is further illustrated with representative examples. Results demonstrate the effectiveness of the developed system and datasets for driving scene analysis, and their supportiveness for crash risk assessment and crash prevention.



### Single View Physical Distance Estimation using Human Pose
- **Arxiv ID**: http://arxiv.org/abs/2106.10335v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10335v1)
- **Published**: 2021-06-18 19:50:40+00:00
- **Updated**: 2021-06-18 19:50:40+00:00
- **Authors**: Xiaohan Fei, Henry Wang, Xiangyu Zeng, Lin Lee Cheong, Meng Wang, Joseph Tighe
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a fully automated system that simultaneously estimates the camera intrinsics, the ground plane, and physical distances between people from a single RGB image or video captured by a camera viewing a 3-D scene from a fixed vantage point. To automate camera calibration and distance estimation, we leverage priors about human pose and develop a novel direct formulation for pose-based auto-calibration and distance estimation, which shows state-of-the-art performance on publicly available datasets. The proposed approach enables existing camera systems to measure physical distances without needing a dedicated calibration process or range sensors, and is applicable to a broad range of use cases such as social distancing and workplace safety. Furthermore, to enable evaluation and drive research in this area, we contribute to the publicly available MEVA dataset with additional distance annotations, resulting in MEVADA -- the first evaluation benchmark in the world for the pose-based auto-calibration and distance estimation problem.



### Analysis and Tuning of a Voice Assistant System for Dysfluent Speech
- **Arxiv ID**: http://arxiv.org/abs/2106.11759v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.AI, cs.CL, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2106.11759v1)
- **Published**: 2021-06-18 20:58:34+00:00
- **Updated**: 2021-06-18 20:58:34+00:00
- **Authors**: Vikramjit Mitra, Zifang Huang, Colin Lea, Lauren Tooley, Sarah Wu, Darren Botten, Ashwini Palekar, Shrinath Thelapurath, Panayiotis Georgiou, Sachin Kajarekar, Jefferey Bigham
- **Comment**: 5 pages, 1 page reference, 2 figures
- **Journal**: None
- **Summary**: Dysfluencies and variations in speech pronunciation can severely degrade speech recognition performance, and for many individuals with moderate-to-severe speech disorders, voice operated systems do not work. Current speech recognition systems are trained primarily with data from fluent speakers and as a consequence do not generalize well to speech with dysfluencies such as sound or word repetitions, sound prolongations, or audible blocks. The focus of this work is on quantitative analysis of a consumer speech recognition system on individuals who stutter and production-oriented approaches for improving performance for common voice assistant tasks (i.e., "what is the weather?"). At baseline, this system introduces a significant number of insertion and substitution errors resulting in intended speech Word Error Rates (isWER) that are 13.64\% worse (absolute) for individuals with fluency disorders. We show that by simply tuning the decoding parameters in an existing hybrid speech recognition system one can improve isWER by 24\% (relative) for individuals with fluency disorders. Tuning these parameters translates to 3.6\% better domain recognition and 1.7\% better intent recognition relative to the default setup for the 18 study participants across all stuttering severities.



### Direct Reconstruction of Linear Parametric Images from Dynamic PET Using Nonlocal Deep Image Prior
- **Arxiv ID**: http://arxiv.org/abs/2106.10359v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2106.10359v1)
- **Published**: 2021-06-18 21:30:22+00:00
- **Updated**: 2021-06-18 21:30:22+00:00
- **Authors**: Kuang Gong, Ciprian Catana, Jinyi Qi, Quanzheng Li
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: Direct reconstruction methods have been developed to estimate parametric images directly from the measured PET sinograms by combining the PET imaging model and tracer kinetics in an integrated framework. Due to limited counts received, signal-to-noise-ratio (SNR) and resolution of parametric images produced by direct reconstruction frameworks are still limited. Recently supervised deep learning methods have been successfully applied to medical imaging denoising/reconstruction when large number of high-quality training labels are available. For static PET imaging, high-quality training labels can be acquired by extending the scanning time. However, this is not feasible for dynamic PET imaging, where the scanning time is already long enough. In this work, we proposed an unsupervised deep learning framework for direct parametric reconstruction from dynamic PET, which was tested on the Patlak model and the relative equilibrium Logan model. The patient's anatomical prior image, which is readily available from PET/CT or PET/MR scans, was supplied as the network input to provide a manifold constraint, and also utilized to construct a kernel layer to perform non-local feature denoising. The linear kinetic model was embedded in the network structure as a 1x1 convolution layer. The training objective function was based on the PET statistical model. Evaluations based on dynamic datasets of 18F-FDG and 11C-PiB tracers show that the proposed framework can outperform the traditional and the kernel method-based direct reconstruction methods.



### The Animal ID Problem: Continual Curation
- **Arxiv ID**: http://arxiv.org/abs/2106.10377v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.10377v1)
- **Published**: 2021-06-18 22:32:11+00:00
- **Updated**: 2021-06-18 22:32:11+00:00
- **Authors**: Charles V. Stewart, Jason R. Parham, Jason Holmberg, Tanya Y. Berger-Wolf
- **Comment**: 4 pages, 2 figures, non-archival in 2021 CVPR workshop
- **Journal**: None
- **Summary**: Hoping to stimulate new research in individual animal identification from images, we propose to formulate the problem as the human-machine Continual Curation of images and animal identities. This is an open world recognition problem, where most new animals enter the system after its algorithms are initially trained and deployed. Continual Curation, as defined here, requires (1) an improvement in the effectiveness of current recognition methods, (2) a pairwise verification algorithm that allows the possibility of no decision, and (3) an algorithmic decision mechanism that seeks human input to guide the curation process. Error metrics must evaluate the ability of recognition algorithms to identify not only animals that have been seen just once or twice but also recognize new animals not in the database. An important measure of overall system performance is accuracy as a function of the amount of human input required.



### Dynamical Deep Generative Latent Modeling of 3D Skeletal Motion
- **Arxiv ID**: http://arxiv.org/abs/2106.10393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10393v1)
- **Published**: 2021-06-18 23:58:49+00:00
- **Updated**: 2021-06-18 23:58:49+00:00
- **Authors**: Amirreza Farnoosh, Sarah Ostadabbas
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a Bayesian switching dynamical model for segmentation of 3D pose data over time that uncovers interpretable patterns in the data and is generative. Our model decomposes highly correlated skeleton data into a set of few spatial basis of switching temporal processes in a low-dimensional latent framework. We parameterize these temporal processes with regard to a switching deep vector autoregressive prior in order to accommodate both multimodal and higher-order nonlinear inter-dependencies. This results in a dynamical deep generative latent model that parses the meaningful intrinsic states in the dynamics of 3D pose data using approximate variational inference, and enables a realistic low-level dynamical generation and segmentation of complex skeleton movements. Our experiments on four biological motion data containing bat flight, salsa dance, walking, and golf datasets substantiate superior performance of our model in comparison with the state-of-the-art methods.



