# Arxiv Papers in cs.CV on 2021-06-19
### Sparse Training via Boosting Pruning Plasticity with Neuroregeneration
- **Arxiv ID**: http://arxiv.org/abs/2106.10404v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10404v4)
- **Published**: 2021-06-19 02:09:25+00:00
- **Updated**: 2022-02-06 15:09:51+00:00
- **Authors**: Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Lu Yin, Huanyu Kou, Li Shen, Mykola Pechenizkiy, Zhangyang Wang, Decebal Constantin Mocanu
- **Comment**: Published on the thirty-fifth Conference on Neural Information
  Processing Systems (NeurIPS 2021). Code can be found
  https://github.com/Shiweiliuiiiiiii/GraNet
- **Journal**: Conference on Neural Information Processing Systems (NeurIPS 2021)
- **Summary**: Works on lottery ticket hypothesis (LTH) and single-shot network pruning (SNIP) have raised a lot of attention currently on post-training pruning (iterative magnitude pruning), and before-training pruning (pruning at initialization). The former method suffers from an extremely large computation cost and the latter usually struggles with insufficient performance. In comparison, during-training pruning, a class of pruning methods that simultaneously enjoys the training/inference efficiency and the comparable performance, temporarily, has been less explored. To better understand during-training pruning, we quantitatively study the effect of pruning throughout training from the perspective of pruning plasticity (the ability of the pruned networks to recover the original performance). Pruning plasticity can help explain several other empirical observations about neural network pruning in literature. We further find that pruning plasticity can be substantially improved by injecting a brain-inspired mechanism called neuroregeneration, i.e., to regenerate the same number of connections as pruned. We design a novel gradual magnitude pruning (GMP) method, named gradual pruning with zero-cost neuroregeneration (\textbf{GraNet}), that advances state of the art. Perhaps most impressively, its sparse-to-sparse version for the first time boosts the sparse-to-sparse training performance over various dense-to-sparse methods with ResNet-50 on ImageNet without extending the training time. We release all codes in https://github.com/Shiweiliuiiiiiii/GraNet.



### AdaZoom: Adaptive Zoom Network for Multi-Scale Object Detection in Large Scenes
- **Arxiv ID**: http://arxiv.org/abs/2106.10409v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10409v1)
- **Published**: 2021-06-19 03:30:22+00:00
- **Updated**: 2021-06-19 03:30:22+00:00
- **Authors**: Jingtao Xu, Yali Li, Shengjin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Detection in large-scale scenes is a challenging problem due to small objects and extreme scale variation. It is essential to focus on the image regions of small objects. In this paper, we propose a novel Adaptive Zoom (AdaZoom) network as a selective magnifier with flexible shape and focal length to adaptively zoom the focus regions for object detection. Based on policy gradient, we construct a reinforcement learning framework for focus region generation, with the reward formulated by object distributions. The scales and aspect ratios of the generated regions are adaptive to the scales and distribution of objects inside. We apply variable magnification according to the scale of the region for adaptive multi-scale detection. We further propose collaborative training to complementarily promote the performance of AdaZoom and the detection network. To validate the effectiveness, we conduct extensive experiments on VisDrone2019, UAVDT, and DOTA datasets. The experiments show AdaZoom brings a consistent and significant improvement over different detection networks, achieving state-of-the-art performance on these datasets, especially outperforming the existing methods by AP of 4.64% on Vis-Drone2019.



### Deep Generative Learning via Schr√∂dinger Bridge
- **Arxiv ID**: http://arxiv.org/abs/2106.10410v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10410v2)
- **Published**: 2021-06-19 03:35:42+00:00
- **Updated**: 2021-07-30 07:50:28+00:00
- **Authors**: Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, Can Yang
- **Comment**: None
- **Journal**: ICML, 2021
- **Summary**: We propose to learn a generative model via entropy interpolation with a Schr\"{o}dinger Bridge. The generative learning task can be formulated as interpolating between a reference distribution and a target distribution based on the Kullback-Leibler divergence. At the population level, this entropy interpolation is characterized via an SDE on $[0,1]$ with a time-varying drift term. At the sample level, we derive our Schr\"{o}dinger Bridge algorithm by plugging the drift term estimated by a deep score estimator and a deep density ratio estimator into the Euler-Maruyama method. Under some mild smoothness assumptions of the target distribution, we prove the consistency of both the score estimator and the density ratio estimator, and then establish the consistency of the proposed Schr\"{o}dinger Bridge approach. Our theoretical results guarantee that the distribution learned by our approach converges to the target distribution. Experimental results on multimodal synthetic data and benchmark data support our theoretical findings and indicate that the generative model via Schr\"{o}dinger Bridge is comparable with state-of-the-art GANs, suggesting a new formulation of generative learning. We demonstrate its usefulness in image interpolation and image inpainting.



### Multi-Contextual Design of Convolutional Neural Network for Steganalysis
- **Arxiv ID**: http://arxiv.org/abs/2106.10430v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10430v2)
- **Published**: 2021-06-19 05:38:52+00:00
- **Updated**: 2021-11-04 17:05:55+00:00
- **Authors**: Brijesh Singh, Arijit Sur, Pinaki Mitra
- **Comment**: Under Review
- **Journal**: None
- **Summary**: In recent times, deep learning-based steganalysis classifiers became popular due to their state-of-the-art performance. Most deep steganalysis classifiers usually extract noise residuals using high-pass filters as preprocessing steps and feed them to their deep model for classification. It is observed that recent steganographic embedding does not always restrict their embedding in the high-frequency zone; instead, they distribute it as per embedding policy. Therefore, besides noise residual, learning the embedding zone is another challenging task. In this work, unlike the conventional approaches, the proposed model first extracts the noise residual using learned denoising kernels to boost the signal-to-noise ratio. After preprocessing, the sparse noise residuals are fed to a novel Multi-Contextual Convolutional Neural Network (M-CNET) that uses heterogeneous context size to learn the sparse and low-amplitude representation of noise residuals. The model performance is further improved by incorporating the Self-Attention module to focus on the areas prone to steganalytic embedding. A set of comprehensive experiments is performed to show the proposed scheme's efficacy over the prior arts. Besides, an ablation study is given to justify the contribution of various modules of the proposed architecture.



### Neural Network Facial Authentication for Public Electric Vehicle Charging Station
- **Arxiv ID**: http://arxiv.org/abs/2106.10432v1
- **DOI**: 10.33093/jetap.2021.3.1
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10432v1)
- **Published**: 2021-06-19 05:48:42+00:00
- **Updated**: 2021-06-19 05:48:42+00:00
- **Authors**: Muhamad Amin Husni Abdul Haris, Sin Liang Lim
- **Comment**: None
- **Journal**: JETAP Vol.3 No.1 (2021) 17-21
- **Summary**: This study is to investigate and compare the facial recognition accuracy performance of Dlib ResNet against a K-Nearest Neighbour (KNN) classifier. Particularly when used against a dataset from an Asian ethnicity as Dlib ResNet was reported to have an accuracy deficiency when it comes to Asian faces. The comparisons are both implemented on the facial vectors extracted using the Histogram of Oriented Gradients (HOG) method and use the same dataset for a fair comparison. Authentication of a user by facial recognition in an electric vehicle (EV) charging station demonstrates a practical use case for such an authentication system.



### Fingerprinting Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.11760v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11760v3)
- **Published**: 2021-06-19 06:25:10+00:00
- **Updated**: 2023-05-29 05:43:47+00:00
- **Authors**: Guanlin Li, Guowen Xu, Han Qiu, Shangwei Guo, Run Wang, Jiwei Li, Tianwei Zhang, Rongxing Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have been widely used in various application scenarios. Since the production of a commercial GAN requires substantial computational and human resources, the copyright protection of GANs is urgently needed. In this paper, we present the first fingerprinting scheme for the Intellectual Property (IP) protection of GANs. We break through the stealthiness and robustness bottlenecks suffered by previous fingerprinting methods for classification models being naively transferred to GANs. Specifically, we innovatively construct a composite deep learning model from the target GAN and a classifier. Then we generate fingerprint samples from this composite model, and embed them in the classifier for effective ownership verification. This scheme inspires some concrete methodologies to practically protect the modern GAN models. Theoretical analysis proves that these methods can satisfy different security requirements necessary for IP protection. We also conduct extensive experiments to show that our solutions outperform existing strategies.



### One-to-many Approach for Improving Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2106.10437v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.10437v4)
- **Published**: 2021-06-19 06:41:29+00:00
- **Updated**: 2021-08-19 02:48:51+00:00
- **Authors**: Sieun Park, Eunho Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, there has been discussions on the ill-posed nature of super-resolution that multiple possible reconstructions exist for a given low-resolution image. Using normalizing flows, SRflow[23] achieves state-of-the-art perceptual quality by learning the distribution of the output instead of a deterministic output to one estimate. In this paper, we adapt the concepts of SRFlow to improve GAN-based super-resolution by properly implementing the one-to-many property. We modify the generator to estimate a distribution as a mapping from random noise. We improve the content loss that hampers the perceptual training objectives. We also propose additional training techniques to further enhance the perceptual quality of generated images. Using our proposed methods, we were able to improve the performance of ESRGAN[1] in x4 perceptual SR and achieve the state-of-the-art LPIPS score in x16 perceptual extreme SR by applying our methods to RFB-ESRGAN[21].



### Cloud based Scalable Object Recognition from Video Streams using Orientation Fusion and Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.15329v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15329v1)
- **Published**: 2021-06-19 07:15:15+00:00
- **Updated**: 2021-06-19 07:15:15+00:00
- **Authors**: Muhammad Usman Yaseen, Ashiq Anjum, Giancarlo Fortino, Antonio Liotta, Amir Hussain
- **Comment**: None
- **Journal**: None
- **Summary**: Object recognition from live video streams comes with numerous challenges such as the variation in illumination conditions and poses. Convolutional neural networks (CNNs) have been widely used to perform intelligent visual object recognition. Yet, CNNs still suffer from severe accuracy degradation, particularly on illumination-variant datasets. To address this problem, we propose a new CNN method based on orientation fusion for visual object recognition. The proposed cloud-based video analytics system pioneers the use of bi-dimensional empirical mode decomposition to split a video frame into intrinsic mode functions (IMFs). We further propose these IMFs to endure Reisz transform to produce monogenic object components, which are in turn used for the training of CNNs. Past works have demonstrated how the object orientation component may be used to pursue accuracy levels as high as 93\%. Herein we demonstrate how a feature-fusion strategy of the orientation components leads to further improving visual recognition accuracy to 97\%. We also assess the scalability of our method, looking at both the number and the size of the video streams under scrutiny. We carry out extensive experimentation on the publicly available Yale dataset, including also a self generated video datasets, finding significant improvements (both in accuracy and scale), in comparison to AlexNet, LeNet and SE-ResNeXt, which are the three most commonly used deep learning models for visual object recognition and classification.



### Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2106.10446v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.10446v1)
- **Published**: 2021-06-19 07:48:55+00:00
- **Updated**: 2021-06-19 07:48:55+00:00
- **Authors**: Ahjeong Seo, Gi-Cheon Kang, Joonhan Park, Byoung-Tak Zhang
- **Comment**: ACL 2021
- **Journal**: None
- **Summary**: Video Question Answering is a task which requires an AI agent to answer questions grounded in video. This task entails three key challenges: (1) understand the intention of various questions, (2) capturing various elements of the input video (e.g., object, action, causality), and (3) cross-modal grounding between language and vision information. We propose Motion-Appearance Synergistic Networks (MASN), which embed two cross-modal features grounded on motion and appearance information and selectively utilize them depending on the question's intentions. MASN consists of a motion module, an appearance module, and a motion-appearance fusion module. The motion module computes the action-oriented cross-modal joint representations, while the appearance module focuses on the appearance aspect of the input video. Finally, the motion-appearance fusion module takes each output of the motion module and the appearance module as input, and performs question-guided fusion. As a result, MASN achieves new state-of-the-art performance on the TGIF-QA and MSVD-QA datasets. We also conduct qualitative analysis by visualizing the inference results of MASN. The code is available at https://github.com/ahjeongseo/MASN-pytorch.



### MSN: Efficient Online Mask Selection Network for Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.10452v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.10452v1)
- **Published**: 2021-06-19 08:33:29+00:00
- **Updated**: 2021-06-19 08:33:29+00:00
- **Authors**: Vidit Goel, Jiachen Li, Shubhika Garg, Harsh Maheshwari, Humphrey Shi
- **Comment**: 3rd Place Solution to the YouTube-VIS Challenge at CVPR 2021
- **Journal**: None
- **Summary**: In this work we present a novel solution for Video Instance Segmentation(VIS), that is automatically generating instance level segmentation masks along with object class and tracking them in a video. Our method improves the masks from segmentation and propagation branches in an online manner using the Mask Selection Network (MSN) hence limiting the noise accumulation during mask tracking. We propose an effective design of MSN by using patch-based convolutional neural network. The network is able to distinguish between very subtle differences between the masks and choose the better masks out of the associated masks accurately. Further, we make use of temporal consistency and process the video sequences in both forward and reverse manner as a post processing step to recover lost objects. The proposed method can be used to adapt any video object segmentation method for the task of VIS. Our method achieves a score of 49.1 mAP on 2021 YouTube-VIS Challenge and was ranked third place among more than 30 global teams. Our code will be available at https://github.com/SHI-Labs/Mask-Selection-Networks.



### Humble Teachers Teach Better Students for Semi-Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.10456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10456v1)
- **Published**: 2021-06-19 09:05:10+00:00
- **Updated**: 2021-06-19 09:05:10+00:00
- **Authors**: Yihe Tang, Weifeng Chen, Yijun Luo, Yuting Zhang
- **Comment**: CVPR 2021 camera-ready. Code: https://github.com/lryta/HumbleTeacher
- **Journal**: None
- **Summary**: We propose a semi-supervised approach for contemporary object detectors following the teacher-student dual model framework. Our method is featured with 1) the exponential moving averaging strategy to update the teacher from the student online, 2) using plenty of region proposals and soft pseudo-labels as the student's training targets, and 3) a light-weighted detection-specific data ensemble for the teacher to generate more reliable pseudo-labels. Compared to the recent state-of-the-art -- STAC, which uses hard labels on sparsely selected hard pseudo samples, the teacher in our model exposes richer information to the student with soft-labels on many proposals. Our model achieves COCO-style AP of 53.04% on VOC07 val set, 8.4% better than STAC, when using VOC12 as unlabeled data. On MS-COCO, it outperforms prior work when only a small percentage of data is taken as labeled. It also reaches 53.8% AP on MS-COCO test-dev with 3.1% gain over the fully supervised ResNet-152 Cascaded R-CNN, by tapping into unlabeled data of a similar size to the labeled data.



### Place recognition survey: An update on deep learning approaches
- **Arxiv ID**: http://arxiv.org/abs/2106.10458v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10458v3)
- **Published**: 2021-06-19 09:17:15+00:00
- **Updated**: 2022-03-01 18:15:25+00:00
- **Authors**: Tiago Barros, Ricardo Pereira, Lu√≠s Garrote, Cristiano Premebida, Urbano J. Nunes
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous Vehicles (AV) are becoming more capable of navigating in complex environments with dynamic and changing conditions. A key component that enables these intelligent vehicles to overcome such conditions and become more autonomous is the sophistication of the perception and localization systems. As part of the localization system, place recognition has benefited from recent developments in other perception tasks such as place categorization or object recognition, namely with the emergence of deep learning (DL) frameworks. This paper surveys recent approaches and methods used in place recognition, particularly those based on deep learning. The contributions of this work are twofold: surveying recent sensors such as 3D LiDARs and RADARs, applied in place recognition; and categorizing the various DL-based place recognition works into supervised, unsupervised, semi-supervised, parallel, and hierarchical categories. First, this survey introduces key place recognition concepts to contextualize the reader. Then, sensor characteristics are addressed. This survey proceeds by elaborating on the various DL-based works, presenting summaries for each framework. Some lessons learned from this survey include: the importance of NetVLAD for supervised end-to-end learning; the advantages of unsupervised approaches in place recognition, namely for cross-domain applications; or the increasing tendency of recent works to seek, not only for higher performance but also for higher efficiency.



### Prediction of the facial growth direction with Machine Learning methods
- **Arxiv ID**: http://arxiv.org/abs/2106.10464v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10464v1)
- **Published**: 2021-06-19 10:12:12+00:00
- **Updated**: 2021-06-19 10:12:12+00:00
- **Authors**: Stanis≈Çaw Ka≈∫mierczak, Zofia Juszka, Piotr Fudalej, Jacek Ma≈Ñdziuk
- **Comment**: None
- **Journal**: None
- **Summary**: First attempts of prediction of the facial growth (FG) direction were made over half of a century ago. Despite numerous attempts and elapsed time, a satisfactory method has not been established yet and the problem still poses a challenge for medical experts. To our knowledge, this paper is the first Machine Learning approach to the prediction of FG direction. Conducted data analysis reveals the inherent complexity of the problem and explains the reasons of difficulty in FG direction prediction based on 2D X-ray images. To perform growth forecasting, we employ a wide range of algorithms, from logistic regression, through tree ensembles to neural networks and consider three, slightly different, problem formulations. The resulting classification accuracy varies between 71% and 75%.



### Interactive Object Segmentation with Dynamic Click Transform
- **Arxiv ID**: http://arxiv.org/abs/2106.10465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10465v1)
- **Published**: 2021-06-19 10:13:37+00:00
- **Updated**: 2021-06-19 10:13:37+00:00
- **Authors**: Chun-Tse Lin, Wei-Chih Tu, Chih-Ting Liu, Shao-Yi Chien
- **Comment**: This paper was accepted by IEEE International Conference on Image
  Processing (ICIP) 2021
- **Journal**: None
- **Summary**: In the interactive segmentation, users initially click on the target object to segment the main body and then provide corrections on mislabeled regions to iteratively refine the segmentation masks. Most existing methods transform these user-provided clicks into interaction maps and concatenate them with image as the input tensor. Typically, the interaction maps are determined by measuring the distance of each pixel to the clicked points, ignoring the relation between clicks and mislabeled regions. We propose a Dynamic Click Transform Network~(DCT-Net), consisting of Spatial-DCT and Feature-DCT, to better represent user interactions. Spatial-DCT transforms each user-provided click with individual diffusion distance according to the target scale, and Feature-DCT normalizes the extracted feature map to a specific distribution predicted from the clicked points. We demonstrate the effectiveness of our proposed method and achieve favorable performance compared to the state-of-the-art on three standard benchmark datasets.



### TNCR: Table Net Detection and Classification Dataset
- **Arxiv ID**: http://arxiv.org/abs/2106.15322v1
- **DOI**: 10.1016/j.neucom.2021.11.101
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.15322v1)
- **Published**: 2021-06-19 10:48:58+00:00
- **Updated**: 2021-06-19 10:48:58+00:00
- **Authors**: Abdelrahman Abdallah, Alexander Berendeyev, Islam Nuradin, Daniyar Nurseitov
- **Comment**: None
- **Journal**: Neurocomputing, Volume 473, 7 February 2022, Pages 79-97
- **Summary**: We present TNCR, a new table dataset with varying image quality collected from free websites. The TNCR dataset can be used for table detection in scanned document images and their classification into 5 different classes. TNCR contains 9428 high-quality labeled images. In this paper, we have implemented state-of-the-art deep learning-based methods for table detection to create several strong baselines. Cascade Mask R-CNN with ResNeXt-101-64x4d Backbone Network achieves the highest performance compared to other methods with a precision of 79.7%, recall of 89.8%, and f1 score of 84.4% on the TNCR dataset. We have made TNCR open source in the hope of encouraging more deep learning approaches to table detection, classification, and structure recognition. The dataset and trained model checkpoints are available at https://github.com/abdoelsayed2016/TNCR_Dataset.



### Informative Class Activation Maps
- **Arxiv ID**: http://arxiv.org/abs/2106.10472v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.10472v2)
- **Published**: 2021-06-19 11:02:59+00:00
- **Updated**: 2021-08-14 12:37:31+00:00
- **Authors**: Zhenyue Qin, Dongwoo Kim, Tom Gedeon
- **Comment**: ICML Workshop 2021
- **Journal**: None
- **Summary**: We study how to evaluate the quantitative information content of a region within an image for a particular label. To this end, we bridge class activation maps with information theory. We develop an informative class activation map (infoCAM). Given a classification task, infoCAM depict how to accumulate information of partial regions to that of the entire image toward a label. Thus, we can utilise infoCAM to locate the most informative features for a label. When applied to an image classification task, infoCAM performs better than the traditional classification map in the weakly supervised object localisation task. We achieve state-of-the-art results on Tiny-ImageNet.



### Practical Transferability Estimation for Image Classification Tasks
- **Arxiv ID**: http://arxiv.org/abs/2106.10479v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.10479v2)
- **Published**: 2021-06-19 11:59:11+00:00
- **Updated**: 2021-06-30 10:26:37+00:00
- **Authors**: Yang Tan, Yang Li, Shao-Lun Huang
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Transferability estimation is an essential problem in transfer learning to predict how good the performance is when transferring a source model (or source task) to a target task. Recent analytical transferability metrics have been widely used for source model selection and multi-task learning. A major challenge is how to make transfereability estimation robust under the cross-domain cross-task settings. The recently proposed OTCE score solves this problem by considering both domain and task differences, with the help of transfer experiences on auxiliary tasks, which causes an efficiency overhead. In this work, we propose a practical transferability metric called JC-NCE score that dramatically improves the robustness of the task difference estimation in OTCE, thus removing the need for auxiliary tasks. Specifically, we build the joint correspondences between source and target data via solving an optimal transport problem with a ground cost considering both the sample distance and label distance, and then compute the transferability score as the negative conditional entropy of the matched labels. Extensive validations under the intra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE score outperforms the auxiliary-task free version of OTCE for 7% and 12%, respectively, and is also more robust than other existing transferability metrics on average.



### Unbalanced Feature Transport for Exemplar-based Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2106.10482v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10482v1)
- **Published**: 2021-06-19 12:07:48+00:00
- **Updated**: 2021-06-19 12:07:48+00:00
- **Authors**: Fangneng Zhan, Yingchen Yu, Kaiwen Cui, Gongjie Zhang, Shijian Lu, Jianxiong Pan, Changgong Zhang, Feiying Ma, Xuansong Xie, Chunyan Miao
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Despite the great success of GANs in images translation with different conditioned inputs such as semantic segmentation and edge maps, generating high-fidelity realistic images with reference styles remains a grand challenge in conditional image-to-image translation. This paper presents a general image translation framework that incorporates optimal transport for feature alignment between conditional inputs and style exemplars in image translation. The introduction of optimal transport mitigates the constraint of many-to-one feature matching significantly while building up accurate semantic correspondences between conditional inputs and exemplars. We design a novel unbalanced optimal transport to address the transport between features with deviational distributions which exists widely between conditional inputs and exemplars. In addition, we design a semantic-activation normalization scheme that injects style features of exemplars into the image translation process successfully. Extensive experiments over multiple image translation tasks show that our method achieves superior image translation qualitatively and quantitatively as compared with the state-of-the-art.



### CompConv: A Compact Convolution Module for Efficient Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.10486v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10486v2)
- **Published**: 2021-06-19 12:31:57+00:00
- **Updated**: 2021-07-03 11:03:35+00:00
- **Authors**: Chen Zhang, Yinghao Xu, Yujun Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have achieved remarkable success in various computer vision tasks but rely on tremendous computational cost. To solve this problem, existing approaches either compress well-trained large-scale models or learn lightweight models with carefully designed network structures. In this work, we make a close study of the convolution operator, which is the basic unit used in CNNs, to reduce its computing load. In particular, we propose a compact convolution module, called CompConv, to facilitate efficient feature learning. With the divide-and-conquer strategy, CompConv is able to save a great many computations as well as parameters to produce a certain dimensional feature map. Furthermore, CompConv discreetly integrates the input features into the outputs to efficiently inherit the input information. More importantly, the novel CompConv is a plug-and-play module that can be directly applied to modern CNN structures to replace the vanilla convolution layers without further effort. Extensive experimental results suggest that CompConv can adequately compress the benchmark CNN structures yet barely sacrifice the performance, surpassing other competitors.



### CenterAtt: Fast 2-stage Center Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2106.10493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10493v1)
- **Published**: 2021-06-19 13:03:14+00:00
- **Updated**: 2021-06-19 13:03:14+00:00
- **Authors**: Jianyun Xu, Xin Tang, Jian Dou, Xu Shu, Yushi Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In this technical report, we introduce the methods of HIKVISION_LiDAR_Det in the challenge of waymo open dataset real-time 3D detection. Our solution for the competition are built upon Centerpoint 3D detection framework. Several variants of CenterPoint are explored, including center attention head and feature pyramid network neck. In order to achieve real time detection, methods like batchnorm merge, half-precision floating point network and GPU-accelerated voxelization process are adopted. By using these methods, our team ranks 6th among all the methods on real-time 3D detection challenge in the waymo open dataset.



### Exploring Visual Context for Weakly Supervised Person Search
- **Arxiv ID**: http://arxiv.org/abs/2106.10506v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10506v2)
- **Published**: 2021-06-19 14:47:13+00:00
- **Updated**: 2021-09-26 11:19:43+00:00
- **Authors**: Yichao Yan, Jinpeng Li, Shengcai Liao, Jie Qin, Bingbing Ni, Xiaokang Yang, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Person search has recently emerged as a challenging task that jointly addresses pedestrian detection and person re-identification. Existing approaches follow a fully supervised setting where both bounding box and identity annotations are available. However, annotating identities is labor-intensive, limiting the practicability and scalability of current frameworks. This paper inventively considers weakly supervised person search with only bounding box annotations. We proposed to address this novel task by investigating three levels of context clues (i.e., detection, memory and scene) in unconstrained natural images. The first two are employed to promote local and global discriminative capabilities, while the latter enhances clustering accuracy. Despite its simple design, our CGPS achieves 80.0% in mAP on CUHK-SYSU, boosting the baseline model by 8.8%. Surprisingly, it even achieves comparable performance with several supervised person search models. Our code is available at https://github.com/ljpadam/CGPS



### GLIB: Towards Automated Test Oracle for Graphically-Rich Applications
- **Arxiv ID**: http://arxiv.org/abs/2106.10507v3
- **DOI**: 10.1145/3468264.3468586
- **Categories**: **cs.SE**, cs.CV, cs.LG, 68N01, 68T45, D.2.5; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2106.10507v3)
- **Published**: 2021-06-19 14:50:43+00:00
- **Updated**: 2021-07-15 22:39:34+00:00
- **Authors**: Ke Chen, Yufei Li, Yingfeng Chen, Changjie Fan, Zhipeng Hu, Wei Yang
- **Comment**: ESEC/FSE 2021
- **Journal**: None
- **Summary**: Graphically-rich applications such as games are ubiquitous with attractive visual effects of Graphical User Interface (GUI) that offers a bridge between software applications and end-users. However, various types of graphical glitches may arise from such GUI complexity and have become one of the main component of software compatibility issues. Our study on bug reports from game development teams in NetEase Inc. indicates that graphical glitches frequently occur during the GUI rendering and severely degrade the quality of graphically-rich applications such as video games. Existing automated testing techniques for such applications focus mainly on generating various GUI test sequences and check whether the test sequences can cause crashes. These techniques require constant human attention to captures non-crashing bugs such as bugs causing graphical glitches. In this paper, we present the first step in automating the test oracle for detecting non-crashing bugs in graphically-rich applications. Specifically, we propose \texttt{GLIB} based on a code-based data augmentation technique to detect game GUI glitches. We perform an evaluation of \texttt{GLIB} on 20 real-world game apps (with bug reports available) and the result shows that \texttt{GLIB} can achieve 100\% precision and 99.5\% recall in detecting non-crashing bugs such as game GUI glitches. Practical application of \texttt{GLIB} on another 14 real-world games (without bug reports) further demonstrates that \texttt{GLIB} can effectively uncover GUI glitches, with 48 of 53 bugs reported by \texttt{GLIB} having been confirmed and fixed so far.



### Video Summarization through Reinforcement Learning with a 3D Spatio-Temporal U-Net
- **Arxiv ID**: http://arxiv.org/abs/2106.10528v1
- **DOI**: 10.1109/TIP.2022.3143699
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10528v1)
- **Published**: 2021-06-19 16:27:19+00:00
- **Updated**: 2021-06-19 16:27:19+00:00
- **Authors**: Tianrui Liu, Qingjie Meng, Jun-Jie Huang, Athanasios Vlontzos, Daniel Rueckert, Bernhard Kainz
- **Comment**: None
- **Journal**: None
- **Summary**: Intelligent video summarization algorithms allow to quickly convey the most relevant information in videos through the identification of the most essential and explanatory content while removing redundant video frames. In this paper, we introduce the 3DST-UNet-RL framework for video summarization. A 3D spatio-temporal U-Net is used to efficiently encode spatio-temporal information of the input videos for downstream reinforcement learning (RL). An RL agent learns from spatio-temporal latent scores and predicts actions for keeping or rejecting a video frame in a video summary. We investigate if real/inflated 3D spatio-temporal CNN features are better suited to learn representations from videos than commonly used 2D image features. Our framework can operate in both, a fully unsupervised mode and a supervised training mode. We analyse the impact of prescribed summary lengths and show experimental evidence for the effectiveness of 3DST-UNet-RL on two commonly used general video summarization benchmarks. We also applied our method on a medical video summarization task. The proposed video summarization method has the potential to save storage costs of ultrasound screening videos as well as to increase efficiency when browsing patient video data during retrospective analysis or audit without loosing essential information



### Reversible Colour Density Compression of Images using cGANs
- **Arxiv ID**: http://arxiv.org/abs/2106.10542v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10542v1)
- **Published**: 2021-06-19 17:44:39+00:00
- **Updated**: 2021-06-19 17:44:39+00:00
- **Authors**: Arun Jose, Abraham Francis
- **Comment**: 7 pages, 2 figures
- **Journal**: None
- **Summary**: Image compression using colour densities is historically impractical to decompress losslessly. We examine the use of conditional generative adversarial networks in making this transformation more feasible, through learning a mapping between the images and a loss function to train on. We show that this method is effective at producing visually lossless generations, indicating that efficient colour compression is viable.



### VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment and Analysis
- **Arxiv ID**: http://arxiv.org/abs/2106.10548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10548v1)
- **Published**: 2021-06-19 18:28:16+00:00
- **Updated**: 2021-06-19 18:28:16+00:00
- **Authors**: Argho Sarkar, Maryam Rahnemoonfar
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: Visual Question Answering system integrated with Unmanned Aerial Vehicle (UAV) has a lot of potentials to advance the post-disaster damage assessment purpose. Providing assistance to affected areas is highly dependent on real-time data assessment and analysis. Scope of the Visual Question Answering is to understand the scene and provide query related answer which certainly faster the recovery process after any disaster. In this work, we address the importance of \textit{visual question answering (VQA)} task for post-disaster damage assessment by presenting our recently developed VQA dataset called \textit{HurMic-VQA} collected during hurricane Michael, and comparing the performances of baseline VQA models.



### Supervised learning for crop/weed classification based on color and texture features
- **Arxiv ID**: http://arxiv.org/abs/2106.10581v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.10581v1)
- **Published**: 2021-06-19 22:31:54+00:00
- **Updated**: 2021-06-19 22:31:54+00:00
- **Authors**: Faiza Mekhalfa, Fouad Yacef
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision techniques have attracted a great interest in precision agriculture, recently. The common goal of all computer vision-based precision agriculture tasks is to detect the objects of interest (e.g., crop, weed) and discriminating them from the background. The Weeds are unwanted plants growing among crops competing for nutrients, water, and sunlight, causing losses to crop yields. Weed detection and mapping is critical for site-specific weed management to reduce the cost of labor and impact of herbicides. This paper investigates the use of color and texture features for discrimination of Soybean crops and weeds. Feature extraction methods including two color spaces (RGB, HSV), gray level Co-occurrence matrix (GLCM), and Local Binary Pattern (LBP) are used to train the Support Vector Machine (SVM) classifier. The experiment was carried out on image dataset of soybean crop, obtained from an unmanned aerial vehicle (UAV), which is publicly available. The results from the experiment showed that the highest accuracy (above 96%) was obtained from the combination of color and LBP features.



### Exploring Vision Transformers for Fine-grained Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.10587v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.10587v2)
- **Published**: 2021-06-19 23:57:31+00:00
- **Updated**: 2021-06-30 01:04:12+00:00
- **Authors**: Marcos V. Conde, Kerem Turgutlu
- **Comment**: 4 pages, 5 figures, 4 tables. Published in IEEE Computer Society
  Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) 2021
  - FGVC8. For code see https://github.com/mv-lab/ViT-FGVC8 and for other
  workshop papers see https://sites.google.com/view/fgvc8/papers
- **Journal**: None
- **Summary**: Existing computer vision research in categorization struggles with fine-grained attributes recognition due to the inherently high intra-class variances and low inter-class variances. SOTA methods tackle this challenge by locating the most informative image regions and rely on them to classify the complete image. The most recent work, Vision Transformer (ViT), shows its strong performance in both traditional and fine-grained classification tasks. In this work, we propose a multi-stage ViT framework for fine-grained image classification tasks, which localizes the informative image regions without requiring architectural changes using the inherent multi-head self-attention mechanism. We also introduce attention-guided augmentations for improving the model's capabilities. We demonstrate the value of our approach by experimenting with four popular fine-grained benchmarks: CUB-200-2011, Stanford Cars, Stanford Dogs, and FGVC7 Plant Pathology. We also prove our model's interpretability via qualitative results.



### Low-Power Multi-Camera Object Re-Identification using Hierarchical Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.10588v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.10588v1)
- **Published**: 2021-06-19 23:59:26+00:00
- **Updated**: 2021-06-19 23:59:26+00:00
- **Authors**: Abhinav Goel, Caleb Tung, Xiao Hu, Haobo Wang, James C. Davis, George K. Thiruvathukal, Yung-Hsiang Lu
- **Comment**: Accepted to ISLPED 2021
- **Journal**: None
- **Summary**: Low-power computer vision on embedded devices has many applications. This paper describes a low-power technique for the object re-identification (reID) problem: matching a query image against a gallery of previously seen images. State-of-the-art techniques rely on large, computationally-intensive Deep Neural Networks (DNNs). We propose a novel hierarchical DNN architecture that uses attribute labels in the training dataset to perform efficient object reID. At each node in the hierarchy, a small DNN identifies a different attribute of the query image. The small DNN at each leaf node is specialized to re-identify a subset of the gallery: only the images with the attributes identified along the path from the root to a leaf. Thus, a query image is re-identified accurately after processing with a few small DNNs. We compare our method with state-of-the-art object reID techniques. With a 4% loss in accuracy, our approach realizes significant resource savings: 74% less memory, 72% fewer operations, and 67% lower query latency, yielding 65% less energy consumption.



