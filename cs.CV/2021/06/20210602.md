# Arxiv Papers in cs.CV on 2021-06-02
### Rotation Equivariant Feature Image Pyramid Network for Object Detection in Optical Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/2106.00880v3
- **DOI**: 10.1109/TGRS.2021.3112481
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00880v3)
- **Published**: 2021-06-02 01:33:49+00:00
- **Updated**: 2021-09-06 03:09:00+00:00
- **Authors**: Pourya Shamsolmoali, Masoumeh Zareapoor, Jocelyn Chanussot, Huiyu Zhou, Jie Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Detection of objects is extremely important in various aerial vision-based applications. Over the last few years, the methods based on convolution neural networks have made substantial progress. However, because of the large variety of object scales, densities, and arbitrary orientations, the current detectors struggle with the extraction of semantically strong features for small-scale objects by a predefined convolution kernel. To address this problem, we propose the rotation equivariant feature image pyramid network (REFIPN), an image pyramid network based on rotation equivariance convolution. The proposed model adopts single-shot detector in parallel with a lightweight image pyramid module to extract representative features and generate regions of interest in an optimization approach. The proposed network extracts feature in a wide range of scales and orientations by using novel convolution filters. These features are used to generate vector fields and determine the weight and angle of the highest-scoring orientation for all spatial locations on an image. By this approach, the performance for small-sized object detection is enhanced without sacrificing the performance for large-sized object detection. The performance of the proposed model is validated on two commonly used aerial benchmarks and the results show our proposed model can achieve state-of-the-art performance with satisfactory efficiency.



### TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.00908v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00908v2)
- **Published**: 2021-06-02 02:57:54+00:00
- **Updated**: 2021-10-31 14:08:35+00:00
- **Authors**: Zhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, Jian Zhang, Xiangyang Ji, Yongbing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, the current MIL methods are usually based on independent and identical distribution hypothesis, thus neglect the correlation among different instances. To address this problem, we proposed a new framework, called correlated MIL, and provided a proof for convergence. Based on this framework, we devised a Transformer based MIL (TransMIL), which explored both morphological and spatial information. The proposed TransMIL can effectively deal with unbalanced/balanced and binary/multiple classification with great visualization and interpretability. We conducted various experiments for three different computational pathology problems and achieved better performance and faster convergence compared with state-of-the-art methods. The test AUC for the binary tumor classification can be up to 93.09% over CAMELYON16 dataset. And the AUC over the cancer subtypes classification can be up to 96.03% and 98.82% over TCGA-NSCLC dataset and TCGA-RCC dataset, respectively. Implementation is available at: https://github.com/szc19990412/TransMIL.



### Translational Symmetry-Aware Facade Parsing for 3D Building Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2106.00912v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00912v1)
- **Published**: 2021-06-02 03:10:51+00:00
- **Updated**: 2021-06-02 03:10:51+00:00
- **Authors**: Hantang Liu, Wentong Li, Jianke Zhu
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Effectively parsing the facade is essential to 3D building reconstruction, which is an important computer vision problem with a large amount of applications in high precision map for navigation, computer aided design, and city generation for digital entertainments. To this end, the key is how to obtain the shape grammars from 2D images accurately and efficiently. Although enjoying the merits of promising results on the semantic parsing, deep learning methods cannot directly make use of the architectural rules, which play an important role for man-made structures. In this paper, we present a novel translational symmetry-based approach to improving the deep neural networks. Our method employs deep learning models as the base parser, and a module taking advantage of translational symmetry is used to refine the initial parsing results. In contrast to conventional semantic segmentation or bounding box prediction, we propose a novel scheme to fuse segmentation with anchor-free detection in a single stage network, which enables the efficient training and better convergence. After parsing the facades into shape grammars, we employ an off-the-shelf rendering engine like Blender to reconstruct the realistic high-quality 3D models using procedural modeling. We conduct experiments on three public datasets, where our proposed approach outperforms the state-of-the-art methods. In addition, we have illustrated the 3D building models built from 2D facade images.



### Consumer Image Quality Prediction using Recurrent Neural Networks for Spatial Pooling
- **Arxiv ID**: http://arxiv.org/abs/2106.00918v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00918v1)
- **Published**: 2021-06-02 03:31:44+00:00
- **Updated**: 2021-06-02 03:31:44+00:00
- **Authors**: Jari Korhonen, Yicheng Su, Junyong You
- **Comment**: None
- **Journal**: None
- **Summary**: Promising results for subjective image quality prediction have been achieved during the past few years by using convolutional neural networks (CNN). However, the use of CNNs for high resolution image quality assessment remains a challenge, since typical CNN architectures have been designed for small resolution input images. In this study, we propose an image quality model that attempts to mimic the attention mechanism of human visual system (HVS) by using a recurrent neural network (RNN) for spatial pooling of the features extracted from different spatial areas (patches) by a deep CNN-based feature extractor. The experimental study, conducted by using images with different resolutions from two recently published image quality datasets, indicates that the quality prediction accuracy of the proposed method is competitive against benchmark models representing the state-of-the-art, and the proposed method also performs consistently on different resolution versions of the same dataset.



### Self-supervised Lesion Change Detection and Localisation in Longitudinal Multiple Sclerosis Brain Imaging
- **Arxiv ID**: http://arxiv.org/abs/2106.00919v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00919v1)
- **Published**: 2021-06-02 03:34:10+00:00
- **Updated**: 2021-06-02 03:34:10+00:00
- **Authors**: Minh-Son To, Ian G Sarno, Chee Chong, Mark Jenkinson, Gustavo Carneiro
- **Comment**: Provisional accepted for MICCAI 2021
- **Journal**: None
- **Summary**: Longitudinal imaging forms an essential component in the management and follow-up of many medical conditions. The presence of lesion changes on serial imaging can have significant impact on clinical decision making, highlighting the important role for automated change detection. Lesion changes can represent anomalies in serial imaging, which implies a limited availability of annotations and a wide variety of possible changes that need to be considered. Hence, we introduce a new unsupervised anomaly detection and localisation method trained exclusively with serial images that do not contain any lesion changes. Our training automatically synthesises lesion changes in serial images, introducing detection and localisation pseudo-labels that are used to self-supervise the training of our model. Given the rarity of these lesion changes in the synthesised images, we train the model with the imbalance robust focal Tversky loss. When compared to supervised models trained on different datasets, our method shows competitive performance in the detection and localisation of new demyelinating lesions on longitudinal magnetic resonance imaging in multiple sclerosis patients. Code for the models will be made available on GitHub.



### End-to-End Information Extraction by Character-Level Embedding and Multi-Stage Attentional U-Net
- **Arxiv ID**: http://arxiv.org/abs/2106.00952v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00952v3)
- **Published**: 2021-06-02 05:42:51+00:00
- **Updated**: 2021-09-09 07:36:35+00:00
- **Authors**: Tuan-Anh Nguyen Dang, Dat-Thanh Nguyen
- **Comment**: Accepted to BMVC 2019
- **Journal**: 30th British Machine Vision Conference (BMVC) 2019
- **Summary**: Information extraction from document images has received a lot of attention recently, due to the need for digitizing a large volume of unstructured documents such as invoices, receipts, bank transfers, etc. In this paper, we propose a novel deep learning architecture for end-to-end information extraction on the 2D character-grid embedding of the document, namely the \textit{Multi-Stage Attentional U-Net}. To effectively capture the textual and spatial relations between 2D elements, our model leverages a specialized multi-stage encoder-decoders design, in conjunction with efficient uses of the self-attention mechanism and the box convolution. Experimental results on different datasets show that our model outperforms the baseline U-Net architecture by a large margin while using 40\% fewer parameters. Moreover, it also significantly improved the baseline in erroneous OCR and limited training data scenario, thus becomes practical for real-world applications.



### Feedback Network for Mutually Boosted Stereo Image Super-Resolution and Disparity Estimation
- **Arxiv ID**: http://arxiv.org/abs/2106.00985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00985v1)
- **Published**: 2021-06-02 07:05:17+00:00
- **Updated**: 2021-06-02 07:05:17+00:00
- **Authors**: Qinyan Dai, Juncheng Li, Qiaosi Yi, Faming Fang, Guixu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Under stereo settings, the problem of image super-resolution (SR) and disparity estimation are interrelated that the result of each problem could help to solve the other. The effective exploitation of correspondence between different views facilitates the SR performance, while the high-resolution (HR) features with richer details benefit the correspondence estimation. According to this motivation, we propose a Stereo Super-Resolution and Disparity Estimation Feedback Network (SSRDE-FNet), which simultaneously handles the stereo image super-resolution and disparity estimation in a unified framework and interact them with each other to further improve their performance. Specifically, the SSRDE-FNet is composed of two dual recursive sub-networks for left and right views. Besides the cross-view information exploitation in the low-resolution (LR) space, HR representations produced by the SR process are utilized to perform HR disparity estimation with higher accuracy, through which the HR features can be aggregated to generate a finer SR result. Afterward, the proposed HR Disparity Information Feedback (HRDIF) mechanism delivers information carried by HR disparity back to previous layers to further refine the SR image reconstruction. Extensive experiments demonstrate the effectiveness and advancement of SSRDE-FNet.



### Tips and Tricks to Improve CNN-based Chest X-ray Diagnosis: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2106.00997v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00997v1)
- **Published**: 2021-06-02 07:46:02+00:00
- **Updated**: 2021-06-02 07:46:02+00:00
- **Authors**: Changhee Han, Takayuki Okamoto, Koichi Takeuchi, Dimitris Katsios, Andrey Grushnikov, Masaaki Kobayashi, Antoine Choppin, Yutaka Kurashina, Yuki Shimahara
- **Comment**: 7 pages, 2 figures, to be published in Japanese Journal of Medical
  Imaging and Information Sciences
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) intrinsically requires large-scale data whereas Chest X-Ray (CXR) images tend to be data/annotation-scarce, leading to over-fitting. Therefore, based on our development experience and related work, this paper thoroughly introduces tricks to improve generalization in the CXR diagnosis: how to (i) leverage additional data, (ii) augment/distillate data, (iii) regularize training, and (iv) conduct efficient segmentation. As a development example based on such optimization techniques, we also feature LPIXEL's CNN-based CXR solution, EIRL Chest Nodule, which improved radiologists/non-radiologists' nodule detection sensitivity by 0.100/0.131, respectively, while maintaining specificity.



### Towards Unified Surgical Skill Assessment
- **Arxiv ID**: http://arxiv.org/abs/2106.01035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.01035v1)
- **Published**: 2021-06-02 09:06:43+00:00
- **Updated**: 2021-06-02 09:06:43+00:00
- **Authors**: Daochang Liu, Qiyue Li, Tingting Jiang, Yizhou Wang, Rulin Miao, Fei Shan, Ziyu Li
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Surgical skills have a great influence on surgical safety and patients' well-being. Traditional assessment of surgical skills involves strenuous manual efforts, which lacks efficiency and repeatability. Therefore, we attempt to automatically predict how well the surgery is performed using the surgical video. In this paper, a unified multi-path framework for automatic surgical skill assessment is proposed, which takes care of multiple composing aspects of surgical skills, including surgical tool usage, intraoperative event pattern, and other skill proxies. The dependency relationships among these different aspects are specially modeled by a path dependency module in the framework. We conduct extensive experiments on the JIGSAWS dataset of simulated surgical tasks, and a new clinical dataset of real laparoscopic surgeries. The proposed framework achieves promising results on both datasets, with the state-of-the-art on the simulated dataset advanced from 0.71 Spearman's correlation to 0.80. It is also shown that combining multiple skill aspects yields better performance than relying on a single aspect.



### A Novel Edge Detection Operator for Identifying Buildings in Augmented Reality Applications
- **Arxiv ID**: http://arxiv.org/abs/2106.01055v1
- **DOI**: 10.1007/978-3-030-59506-7_18
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.01055v1)
- **Published**: 2021-06-02 10:06:50+00:00
- **Updated**: 2021-06-02 10:06:50+00:00
- **Authors**: Ciprian Orhei, Silviu Vert, Radu Vasiu
- **Comment**: None
- **Journal**: None
- **Summary**: Augmented Reality is an environment-enhancing technology, widely applied in many domains, such as tourism and culture. One of the major challenges in this field is precise detection and extraction of building information through Computer Vision techniques. Edge detection is one of the building blocks operations for many feature extraction solutions in Computer Vision. AR systems use edge detection for building extraction or for extraction of facade details from buildings. In this paper, we propose a novel filter operator for edge detection that aims to extract building contours or facade features better. The proposed filter gives more weight for finding vertical and horizontal edges that is an important feature for our aim.



### Rethinking Cross-modal Interaction from a Top-down Perspective for Referring Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.01061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.01061v1)
- **Published**: 2021-06-02 10:26:13+00:00
- **Updated**: 2021-06-02 10:26:13+00:00
- **Authors**: Chen Liang, Yu Wu, Tianfei Zhou, Wenguan Wang, Zongxin Yang, Yunchao Wei, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Referring video object segmentation (RVOS) aims to segment video objects with the guidance of natural language reference. Previous methods typically tackle RVOS through directly grounding linguistic reference over the image lattice. Such bottom-up strategy fails to explore object-level cues, easily leading to inferior results. In this work, we instead put forward a two-stage, top-down RVOS solution. First, an exhaustive set of object tracklets is constructed by propagating object masks detected from several sampled frames to the entire video. Second, a Transformer-based tracklet-language grounding module is proposed, which models instance-level visual relations and cross-modal interactions simultaneously and efficiently. Our model ranks first place on CVPR2021 Referring Youtube-VOS challenge.



### Online Coreset Selection for Rehearsal-based Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.01085v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.01085v4)
- **Published**: 2021-06-02 11:39:25+00:00
- **Updated**: 2022-03-18 07:21:44+00:00
- **Authors**: Jaehong Yoon, Divyam Madaan, Eunho Yang, Sung Ju Hwang
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: A dataset is a shred of crucial evidence to describe a task. However, each data point in the dataset does not have the same potential, as some of the data points can be more representative or informative than others. This unequal importance among the data points may have a large impact in rehearsal-based continual learning, where we store a subset of the training examples (coreset) to be replayed later to alleviate catastrophic forgetting. In continual learning, the quality of the samples stored in the coreset directly affects the model's effectiveness and efficiency. The coreset selection problem becomes even more important under realistic settings, such as imbalanced continual learning or noisy data scenarios. To tackle this problem, we propose Online Coreset Selection (OCS), a simple yet effective method that selects the most representative and informative coreset at each iteration and trains them in an online manner. Our proposed method maximizes the model's adaptation to a current dataset while selecting high-affinity samples to past tasks, which directly inhibits catastrophic forgetting. We validate the effectiveness of our coreset selection mechanism over various standard, imbalanced, and noisy datasets against strong continual learning baselines, demonstrating that it improves task adaptation and prevents catastrophic forgetting in a sample-efficient manner.



### TSI: Temporal Saliency Integration for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.01088v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.01088v4)
- **Published**: 2021-06-02 11:43:49+00:00
- **Updated**: 2021-12-17 05:58:51+00:00
- **Authors**: Haisheng Su, Kunchang Li, Jinyuan Feng, Dongliang Wang, Weihao Gan, Wei Wu, Yu Qiao
- **Comment**: Submitted to CVPR 2022
- **Journal**: None
- **Summary**: Efficient spatiotemporal modeling is an important yet challenging problem for video action recognition. Existing state-of-the-art methods exploit neighboring feature differences to obtain motion clues for short-term temporal modeling with a simple convolution. However, only one local convolution is incapable of handling various kinds of actions because of the limited receptive field. Besides, action-irrelated noises brought by camera movement will also harm the quality of extracted motion features. In this paper, we propose a Temporal Saliency Integration (TSI) block, which mainly contains a Salient Motion Excitation (SME) module and a Cross-perception Temporal Integration (CTI) module. Specifically, SME aims to highlight the motion-sensitive area through spatial-level local-global motion modeling, where the saliency alignment and pyramidal motion modeling are conducted successively between adjacent frames to capture motion dynamics with fewer noises caused by misaligned background. CTI is designed to perform multi-perception temporal modeling through a group of separate 1D convolutions respectively. Meanwhile, temporal interactions across different perceptions are integrated with the attention mechanism. Through these two modules, long short-term temporal relationships can be encoded efficiently by introducing limited additional parameters. Extensive experiments are conducted on several popular benchmarks (i.e., Something-Something V1 & V2, Kinetics-400, UCF-101, and HMDB-51), which demonstrate the effectiveness of our proposed method.



### Prediction of the Position of External Markers Using a Recurrent Neural Network Trained With Unbiased Online Recurrent Optimization for Safe Lung Cancer Radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/2106.01100v6
- **DOI**: 10.1016/j.cmpb.2022.106908
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2106.01100v6)
- **Published**: 2021-06-02 12:07:31+00:00
- **Updated**: 2022-11-21 10:54:10+00:00
- **Authors**: Michel Pohl, Mitsuru Uesaka, Hiroyuki Takahashi, Kazuyuki Demachi, Ritu Bhusal Chhatkuli
- **Comment**: 24 pages, 16 figures, corrected mistakes and improved notations in
  Eqs. 1, 3, 7, 29-35, footnote 11, line 18 in Algorithm 1, and Appendix A.2
- **Journal**: Computer Methods and Programs in Biomedicine, Volume 222, 2022,
  p.106908
- **Summary**: During lung radiotherapy, the position of infrared reflective objects on the chest can be recorded to estimate the tumor location. However, radiotherapy systems have a latency inherent to robot control limitations that impedes the radiation delivery precision. Prediction with online learning of recurrent neural networks (RNN) allows for adaptation to non-stationary respiratory signals, but classical methods such as RTRL and truncated BPTT are respectively slow and biased. This study investigates the capabilities of unbiased online recurrent optimization (UORO) to forecast respiratory motion and enhance safety in lung radiotherapy.   We used 9 observation records of the 3D position of 3 external markers on the chest and abdomen of healthy individuals breathing during intervals from 73s to 222s. The sampling frequency was 10Hz, and the amplitudes of the recorded trajectories range from 6mm to 40mm in the superior-inferior direction. We forecast the 3D location of each marker simultaneously with a horizon value between 0.1s and 2.0s, using an RNN trained with UORO. We compare its performance with an RNN trained with RTRL, LMS, and offline linear regression. We provide closed-form expressions for quantities involved in the loss gradient calculation in UORO, thereby making its implementation efficient. Training and cross-validation were performed during the first minute of each sequence.   On average over the horizon values considered and the 9 sequences, UORO achieves the lowest root-mean-square (RMS) error and maximum error among the compared algorithms. These errors are respectively equal to 1.3mm and 8.8mm, and the prediction time per time step was lower than 2.8ms (Dell Intel core i9-9900K 3.60 GHz). Linear regression has the lowest RMS error for the horizon values 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s, and UORO for horizon values greater than 0.6s.



### Deep Learning based Full-reference and No-reference Quality Assessment Models for Compressed UGC Videos
- **Arxiv ID**: http://arxiv.org/abs/2106.01111v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.01111v1)
- **Published**: 2021-06-02 12:23:16+00:00
- **Updated**: 2021-06-02 12:23:16+00:00
- **Authors**: Wei Sun, Tao Wang, Xiongkuo Min, Fuwang Yi, Guangtao Zhai
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a deep learning based video quality assessment (VQA) framework to evaluate the quality of the compressed user's generated content (UGC) videos. The proposed VQA framework consists of three modules, the feature extraction module, the quality regression module, and the quality pooling module. For the feature extraction module, we fuse the features from intermediate layers of the convolutional neural network (CNN) network into final quality-aware feature representation, which enables the model to make full use of visual information from low-level to high-level. Specifically, the structure and texture similarities of feature maps extracted from all intermediate layers are calculated as the feature representation for the full reference (FR) VQA model, and the global mean and standard deviation of the final feature maps fused by intermediate feature maps are calculated as the feature representation for the no reference (NR) VQA model. For the quality regression module, we use the fully connected (FC) layer to regress the quality-aware features into frame-level scores. Finally, a subjectively-inspired temporal pooling strategy is adopted to pool frame-level scores into the video-level score. The proposed model achieves the best performance among the state-of-the-art FR and NR VQA models on the Compressed UGC VQA database and also achieves pretty good performance on the in-the-wild UGC VQA databases.



### Deep Learning Based Analysis of Prostate Cancer from MP-MRI
- **Arxiv ID**: http://arxiv.org/abs/2106.01835v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.01835v1)
- **Published**: 2021-06-02 12:42:35+00:00
- **Updated**: 2021-06-02 12:42:35+00:00
- **Authors**: Pedro C. Neto
- **Comment**: None
- **Journal**: None
- **Summary**: The diagnosis of prostate cancer faces a problem with overdiagnosis that leads to damaging side effects due to unnecessary treatment. Research has shown that the use of multi-parametric magnetic resonance images to conduct biopsies can drastically help to mitigate the overdiagnosis, thus reducing the side effects on healthy patients. This study aims to investigate the use of deep learning techniques to explore computer-aid diagnosis based on MRI as input. Several diagnosis problems ranging from classification of lesions as being clinically significant or not to the detection and segmentation of lesions are addressed with deep learning based approaches.   This thesis tackled two main problems regarding the diagnosis of prostate cancer. Firstly, XmasNet was used to conduct two large experiments on the classification of lesions. Secondly, detection and segmentation experiments were conducted, first on the prostate and afterward on the prostate cancer lesions. The former experiments explored the lesions through a two-dimensional space, while the latter explored models to work with three-dimensional inputs. For this task, the 3D models explored were the 3D U-Net and a pretrained 3D ResNet-18. A rigorous analysis of all these problems was conducted with a total of two networks, two cropping techniques, two resampling techniques, two crop sizes, five input sizes and data augmentations experimented for lesion classification. While for segmentation two models, two input sizes and data augmentations were experimented. However, while the binary classification of the clinical significance of lesions and the detection and segmentation of the prostate already achieve the desired results (0.870 AUC and 0.915 dice score respectively), the classification of the PIRADS score and the segmentation of lesions still have a large margin to improve (0.664 accuracy and 0.690 dice score respectively).



### Towards Robust Classification Model by Counterfactual and Invariant Data Generation
- **Arxiv ID**: http://arxiv.org/abs/2106.01127v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.01127v2)
- **Published**: 2021-06-02 12:48:29+00:00
- **Updated**: 2021-06-03 06:14:35+00:00
- **Authors**: Chun-Hao Chang, George Alexandru Adam, Anna Goldenberg
- **Comment**: Accepted in 2021 CVPR
- **Journal**: None
- **Summary**: Despite the success of machine learning applications in science, industry, and society in general, many approaches are known to be non-robust, often relying on spurious correlations to make predictions. Spuriousness occurs when some features correlate with labels but are not causal; relying on such features prevents models from generalizing to unseen environments where such correlations break. In this work, we focus on image classification and propose two data generation processes to reduce spuriousness. Given human annotations of the subset of the features responsible (causal) for the labels (e.g. bounding boxes), we modify this causal set to generate a surrogate image that no longer has the same label (i.e. a counterfactual image). We also alter non-causal features to generate images still recognized as the original labels, which helps to learn a model invariant to these features. In several challenging datasets, our data generations outperform state-of-the-art methods in accuracy when spurious correlations break, and increase the saliency focus on causal features providing better explanations.



### Benchmarking CNN on 3D Anatomical Brain MRI: Architectures, Data Augmentation and Deep Ensemble Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.01132v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.01132v2)
- **Published**: 2021-06-02 13:00:35+00:00
- **Updated**: 2023-04-17 12:48:33+00:00
- **Authors**: Benoit Dufumier, Pietro Gori, Ilaria Battaglia, Julie Victor, Antoine Grigis, Edouard Duchesnay
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Deep Learning (DL) and specifically CNN models have become a de facto method for a wide range of vision tasks, outperforming traditional machine learning (ML) methods. Consequently, they drew a lot of attention in the neuroimaging field in particular for phenotype prediction or computer-aided diagnosis. However, most of the current studies often deal with small single-site cohorts, along with a specific pre-processing pipeline and custom CNN architectures, which make them difficult to compare to. We propose an extensive benchmark of recent state-of-the-art (SOTA) 3D CNN, evaluating also the benefits of data augmentation and deep ensemble learning, on both Voxel-Based Morphometry (VBM) pre-processing and quasi-raw images. Experiments were conducted on a large multi-site 3D brain anatomical MRI data-set comprising N=10k scans on 3 challenging tasks: age prediction, sex classification, and schizophrenia diagnosis. We found that all models provide significantly better predictions with VBM images than quasi-raw data. This finding evolved as the training set approaches 10k samples where quasi-raw data almost reach the performance of VBM. Moreover, we showed that linear models perform comparably with SOTA CNN on VBM data. We also demonstrated that DenseNet and tiny-DenseNet, a lighter version that we proposed, provide a good compromise in terms of performance in all data regime. Therefore, we suggest to employ them as the architectures by default. Critically, we also showed that current CNN are still very biased towards the acquisition site, even when trained with N=10k multi-site images. In this context, VBM pre-processing provides an efficient way to limit this site effect. Surprisingly, we did not find any clear benefit from data augmentation techniques. Finally, we proved that deep ensemble learning is well suited to re-calibrate big CNN models without sacrificing performance.



### Online and Real-Time Tracking in a Surveillance Scenario
- **Arxiv ID**: http://arxiv.org/abs/2106.01153v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.01153v1)
- **Published**: 2021-06-02 13:43:25+00:00
- **Updated**: 2021-06-02 13:43:25+00:00
- **Authors**: Oliver Urbann, Oliver Bredtmann, Maximilian Otten, Jan-Philip Richter, Thilo Bauer, David Zibriczky
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an approach for tracking in a surveillance scenario. Typical aspects for this scenario are a 24/7 operation with a static camera mounted above the height of a human with many objects or people. The Multiple Object Tracking Benchmark 20 (MOT20) reflects this scenario best. We can show that our approach is real-time capable on this benchmark and outperforms all other real-time capable approaches in HOTA, MOTA, and IDF1. We achieve this by contributing a fast Siamese network reformulated for linear runtime (instead of quadratic) to generate fingerprints from detections. Thus, it is possible to associate the detections to Kalman filters based on multiple tracking specific ratings: Cosine similarity of fingerprints, Intersection over Union, and pixel distance ratio in the image.



### Digital homotopy relations and digital homology theories
- **Arxiv ID**: http://arxiv.org/abs/2106.01171v1
- **DOI**: None
- **Categories**: **math.AT**, cs.CV, math.GN, 55P10, 68R10
- **Links**: [PDF](http://arxiv.org/pdf/2106.01171v1)
- **Published**: 2021-06-02 14:10:46+00:00
- **Updated**: 2021-06-02 14:10:46+00:00
- **Authors**: P. Christopher Staecker
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1907.00473,
  arXiv:1903.00706
- **Journal**: None
- **Summary**: In this paper we prove results relating to two homotopy relations and four homology theories developed in the topology of digital images.   We introduce a new type of homotopy relation for digitally continuous functions which we call "strong homotopy." Both digital homotopy and strong homotopy are natural digitizations of classical topological homotopy: the difference between them is analogous to the difference between digital 4-adjacency and 8-adjacency in the plane.   We also consider four different digital homology theories: a simplicial homology theory by Arslan et al which is the homology of the clique complex, a singular simplicial homology theory by D. W. Lee, a cubical homology theory by Jamil and Ali, and a new kind of cubical homology for digital images with $c_1$-adjacency which is easily computed, and generalizes a construction by Karaca \& Ege. We show that the two simplicial homology theories are isomorphic to each other, but distinct from the two cubical theories.   We also show that homotopic maps have the same induced homomorphisms in the cubical homology theory, and strong homotopic maps additionally have the same induced homomorphisms in the simplicial theory.



### ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.01178v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.01178v3)
- **Published**: 2021-06-02 14:20:24+00:00
- **Updated**: 2021-10-15 12:54:42+00:00
- **Authors**: Danila Rukhovich, Anna Vorontsova, Anton Konushin
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce the task of multi-view RGB-based 3D object detection as an end-to-end optimization problem. To address this problem, we propose ImVoxelNet, a novel fully convolutional method of 3D object detection based on monocular or multi-view RGB images. The number of monocular images in each multi-view input can variate during training and inference; actually, this number might be unique for each multi-view input. ImVoxelNet successfully handles both indoor and outdoor scenes, which makes it general-purpose. Specifically, it achieves state-of-the-art results in car detection on KITTI (monocular) and nuScenes (multi-view) benchmarks among all methods that accept RGB images. Moreover, it surpasses existing RGB-based 3D object detection methods on the SUN RGB-D dataset. On ScanNet, ImVoxelNet sets a new benchmark for multi-view 3D object detection. The source code and the trained models are available at https://github.com/saic-vul/imvoxelnet.



### DFGC 2021: A DeepFake Game Competition
- **Arxiv ID**: http://arxiv.org/abs/2106.01217v1
- **DOI**: 10.1109/IJCB52358.2021.9484387
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.01217v1)
- **Published**: 2021-06-02 15:10:13+00:00
- **Updated**: 2021-06-02 15:10:13+00:00
- **Authors**: Bo Peng, Hongxing Fan, Wei Wang, Jing Dong, Yuezun Li, Siwei Lyu, Qi Li, Zhenan Sun, Han Chen, Baoying Chen, Yanjie Hu, Shenghai Luo, Junrui Huang, Yutong Yao, Boyuan Liu, Hefei Ling, Guosheng Zhang, Zhiliang Xu, Changtao Miao, Changlei Lu, Shan He, Xiaoyan Wu, Wanyi Zhuang
- **Comment**: None
- **Journal**: 2021 IEEE International Joint Conference on Biometrics (IJCB),
  2021, pp. 1-8
- **Summary**: This paper presents a summary of the DFGC 2021 competition. DeepFake technology is developing fast, and realistic face-swaps are increasingly deceiving and hard to detect. At the same time, DeepFake detection methods are also improving. There is a two-party game between DeepFake creators and detectors. This competition provides a common platform for benchmarking the adversarial game between current state-of-the-art DeepFake creation and detection methods. In this paper, we present the organization, results and top solutions of this competition and also share our insights obtained during this event. We also release the DFGC-21 testing dataset collected from our participants to further benefit the research community.



### Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision
- **Arxiv ID**: http://arxiv.org/abs/2106.01226v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.01226v2)
- **Published**: 2021-06-02 15:21:56+00:00
- **Updated**: 2021-06-04 04:01:04+00:00
- **Authors**: Xiaokang Chen, Yuhui Yuan, Gang Zeng, Jingdong Wang
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: In this paper, we study the semi-supervised semantic segmentation problem via exploring both labeled data and extra unlabeled data. We propose a novel consistency regularization approach, called cross pseudo supervision (CPS). Our approach imposes the consistency on two segmentation networks perturbed with different initialization for the same input image. The pseudo one-hot label map, output from one perturbed segmentation network, is used to supervise the other segmentation network with the standard cross-entropy loss, and vice versa. The CPS consistency has two roles: encourage high similarity between the predictions of two perturbed networks for the same input image, and expand training data by using the unlabeled data with pseudo labels. Experiment results show that our approach achieves the state-of-the-art semi-supervised segmentation performance on Cityscapes and PASCAL VOC 2012. Code is available at https://git.io/CPS.



### Steerable 3D Spherical Neurons
- **Arxiv ID**: http://arxiv.org/abs/2106.13863v7
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.13863v7)
- **Published**: 2021-06-02 16:30:02+00:00
- **Updated**: 2022-06-14 12:36:53+00:00
- **Authors**: Pavlo Melnyk, Michael Felsberg, Mårten Wadenbäck
- **Comment**: ICML2022
- **Journal**: Proceedings of the 39th International Conference on Machine
  Learning, PMLR 162:15330-15339, 2022.
  https://proceedings.mlr.press/v162/melnyk22a.html
- **Summary**: Emerging from low-level vision theory, steerable filters found their counterpart in prior work on steerable convolutional neural networks equivariant to rigid transformations. In our work, we propose a steerable feed-forward learning-based approach that consists of neurons with spherical decision surfaces and operates on point clouds. Such spherical neurons are obtained by conformal embedding of Euclidean space and have recently been revisited in the context of learning representations of point sets. Focusing on 3D geometry, we exploit the isometry property of spherical neurons and derive a 3D steerability constraint. After training spherical neurons to classify point clouds in a canonical orientation, we use a tetrahedron basis to quadruplicate the neurons and construct rotation-equivariant spherical filter banks. We then apply the derived constraint to interpolate the filter bank outputs and, thus, obtain a rotation-invariant network. Finally, we use a synthetic point set and real-world 3D skeleton data to verify our theoretical findings. The code is available at https://github.com/pavlo-melnyk/steerable-3d-neurons.



### Data augmentation and pre-trained networks for extremely low data regimes unsupervised visual inspection
- **Arxiv ID**: http://arxiv.org/abs/2106.01277v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML, I.2.10; I.4.6; I.4.8; I.4.9; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2106.01277v1)
- **Published**: 2021-06-02 16:37:20+00:00
- **Updated**: 2021-06-02 16:37:20+00:00
- **Authors**: Pierre Gutierrez, Antoine Cordier, Thaïs Caldeira, Théophile Sautory
- **Comment**: 16 pages, 8 figures, 9 tables, SPIE proceedings of Optical Metrology
  conference (https://spie.org/conferences-and-exhibitions/optical-metrology)
- **Journal**: None
- **Summary**: The use of deep features coming from pre-trained neural networks for unsupervised anomaly detection purposes has recently gathered momentum in the computer vision field. In particular, industrial inspection applications can take advantage of such features, as demonstrated by the multiple successes of related methods on the MVTec Anomaly Detection (MVTec AD) dataset. These methods make use of neural networks pre-trained on auxiliary classification tasks such as ImageNet. However, to our knowledge, no comparative study of robustness to the low data regimes between these approaches has been conducted yet. For quality inspection applications, the handling of limited sample sizes may be crucial as large quantities of images are not available for small series. In this work, we aim to compare three approaches based on deep pre-trained features when varying the quantity of available data in MVTec AD: KNN, Mahalanobis, and PaDiM. We show that although these methods are mostly robust to small sample sizes, they still can benefit greatly from using data augmentation in the original image space, which allows to deal with very small production runs.



### The Semi-Supervised iNaturalist Challenge at the FGVC8 Workshop
- **Arxiv ID**: http://arxiv.org/abs/2106.01364v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.01364v1)
- **Published**: 2021-06-02 17:59:41+00:00
- **Updated**: 2021-06-02 17:59:41+00:00
- **Authors**: Jong-Chyi Su, Subhransu Maji
- **Comment**: Tech report for Semi-iNat 2021 challenge, competition page:
  https://github.com/cvl-umass/semi-inat-2021
- **Journal**: None
- **Summary**: Semi-iNat is a challenging dataset for semi-supervised classification with a long-tailed distribution of classes, fine-grained categories, and domain shifts between labeled and unlabeled data. This dataset is behind the second iteration of the semi-supervised recognition challenge to be held at the FGVC8 workshop at CVPR 2021. Different from the previous one, this dataset (i) includes images of species from different kingdoms in the natural taxonomy, (ii) is at a larger scale -- with 810 in-class and 1629 out-of-class species for a total of 330k images, and (iii) does not provide in/out-of-class labels, but provides coarse taxonomic labels (kingdom and phylum) for the unlabeled images. This document describes baseline results and the details of the dataset which is available here: \url{https://github.com/cvl-umass/semi-inat-2021}.



### Container: Context Aggregation Network
- **Arxiv ID**: http://arxiv.org/abs/2106.01401v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.01401v2)
- **Published**: 2021-06-02 18:09:11+00:00
- **Updated**: 2021-10-18 06:52:31+00:00
- **Authors**: Peng Gao, Jiasen Lu, Hongsheng Li, Roozbeh Mottaghi, Aniruddha Kembhavi
- **Comment**: NeuIPS 2021
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are ubiquitous in computer vision, with a myriad of effective and efficient variations. Recently, Transformers -- originally introduced in natural language processing -- have been increasingly adopted in computer vision. While early adopters continue to employ CNN backbones, the latest networks are end-to-end CNN-free Transformer solutions. A recent surprising finding shows that a simple MLP based solution without any traditional convolutional or Transformer components can produce effective visual representations. While CNNs, Transformers and MLP-Mixers may be considered as completely disparate architectures, we provide a unified view showing that they are in fact special cases of a more general method to aggregate spatial context in a neural network stack. We present the \model (CONText AggregatIon NEtwoRk), a general-purpose building block for multi-head context aggregation that can exploit long-range interactions \emph{a la} Transformers while still exploiting the inductive bias of the local convolution operation leading to faster convergence speeds, often seen in CNNs. In contrast to Transformer-based methods that do not scale well to downstream tasks that rely on larger input image resolutions, our efficient network, named \modellight, can be employed in object detection and instance segmentation networks such as DETR, RetinaNet and Mask-RCNN to obtain an impressive detection mAP of 38.9, 43.8, 45.1 and mask mAP of 41.3, providing large improvements of 6.6, 7.3, 6.9 and 6.6 pts respectively, compared to a ResNet-50 backbone with a comparable compute and parameter size. Our method also achieves promising results on self-supervised learning compared to DeiT on the DINO framework. Code is released at \url{https://github.com/allenai/container}.



### One Representation to Rule Them All: Identifying Out-of-Support Examples in Few-shot Learning with Generic Representations
- **Arxiv ID**: http://arxiv.org/abs/2106.01423v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, math.MG
- **Links**: [PDF](http://arxiv.org/pdf/2106.01423v1)
- **Published**: 2021-06-02 19:07:27+00:00
- **Updated**: 2021-06-02 19:07:27+00:00
- **Authors**: Henry Kvinge, Scott Howland, Nico Courts, Lauren A. Phillips, John Buckheit, Zachary New, Elliott Skomski, Jung H. Lee, Sandeep Tiwari, Jessica Hibler, Courtney D. Corley, Nathan O. Hodas
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: The field of few-shot learning has made remarkable strides in developing powerful models that can operate in the small data regime. Nearly all of these methods assume every unlabeled instance encountered will belong to a handful of known classes for which one has examples. This can be problematic for real-world use cases where one routinely finds 'none-of-the-above' examples. In this paper we describe this challenge of identifying what we term 'out-of-support' (OOS) examples. We describe how this problem is subtly different from out-of-distribution detection and describe a new method of identifying OOS examples within the Prototypical Networks framework using a fixed point which we call the generic representation. We show that our method outperforms other existing approaches in the literature as well as other approaches that we propose in this paper. Finally, we investigate how the use of such a generic point affects the geometry of a model's feature space.



### Learning to Select: A Fully Attentive Approach for Novel Object Captioning
- **Arxiv ID**: http://arxiv.org/abs/2106.01424v1
- **DOI**: 10.1145/3460426.3463587
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2106.01424v1)
- **Published**: 2021-06-02 19:11:21+00:00
- **Updated**: 2021-06-02 19:11:21+00:00
- **Authors**: Marco Cagrandi, Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, Rita Cucchiara
- **Comment**: ICMR 2021
- **Journal**: None
- **Summary**: Image captioning models have lately shown impressive results when applied to standard datasets. Switching to real-life scenarios, however, constitutes a challenge due to the larger variety of visual concepts which are not covered in existing training sets. For this reason, novel object captioning (NOC) has recently emerged as a paradigm to test captioning models on objects which are unseen during the training phase. In this paper, we present a novel approach for NOC that learns to select the most relevant objects of an image, regardless of their adherence to the training set, and to constrain the generative process of a language model accordingly. Our architecture is fully-attentive and end-to-end trainable, also when incorporating constraints. We perform experiments on the held-out COCO dataset, where we demonstrate improvements over the state of the art, both in terms of adaptability to novel objects and caption quality.



### Unsharp Mask Guided Filtering
- **Arxiv ID**: http://arxiv.org/abs/2106.01428v1
- **DOI**: 10.1109/TIP.2021.3106812
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.01428v1)
- **Published**: 2021-06-02 19:15:34+00:00
- **Updated**: 2021-06-02 19:15:34+00:00
- **Authors**: Zenglin Shi, Yunlu Chen, Efstratios Gavves, Pascal Mettes, Cees G. M. Snoek
- **Comment**: IEEE Transactions on Image Processing, 2021
- **Journal**: None
- **Summary**: The goal of this paper is guided image filtering, which emphasizes the importance of structure transfer during filtering by means of an additional guidance image. Where classical guided filters transfer structures using hand-designed functions, recent guided filters have been considerably advanced through parametric learning of deep networks. The state-of-the-art leverages deep networks to estimate the two core coefficients of the guided filter. In this work, we posit that simultaneously estimating both coefficients is suboptimal, resulting in halo artifacts and structure inconsistencies. Inspired by unsharp masking, a classical technique for edge enhancement that requires only a single coefficient, we propose a new and simplified formulation of the guided filter. Our formulation enjoys a filtering prior from a low-pass filter and enables explicit structure transfer by estimating a single coefficient. Based on our proposed formulation, we introduce a successive guided filtering network, which provides multiple filtering results from a single network, allowing for a trade-off between accuracy and efficiency. Extensive ablations, comparisons and analysis show the effectiveness and efficiency of our formulation and network, resulting in state-of-the-art results across filtering tasks like upsampling, denoising, and cross-modality filtering. Code is available at \url{https://github.com/shizenglin/Unsharp-Mask-Guided-Filtering}.



### NTIRE 2021 Challenge on High Dynamic Range Imaging: Dataset, Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2106.01439v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.01439v1)
- **Published**: 2021-06-02 19:45:16+00:00
- **Updated**: 2021-06-02 19:45:16+00:00
- **Authors**: Eduardo Pérez-Pellitero, Sibi Catley-Chandar, Aleš Leonardis, Radu Timofte
- **Comment**: To appear in CVPRW 2021 (NTIRE)
- **Journal**: None
- **Summary**: This paper reviews the first challenge on high-dynamic range (HDR) imaging that was part of the New Trends in Image Restoration and Enhancement (NTIRE) workshop, held in conjunction with CVPR 2021. This manuscript focuses on the newly introduced dataset, the proposed methods and their results. The challenge aims at estimating a HDR image from one or multiple respective low-dynamic range (LDR) observations, which might suffer from under- or over-exposed regions and different sources of noise. The challenge is composed by two tracks: In Track 1 only a single LDR image is provided as input, whereas in Track 2 three differently-exposed LDR images with inter-frame motion are available. In both tracks, the ultimate goal is to achieve the best objective HDR reconstruction in terms of PSNR with respect to a ground-truth image, evaluated both directly and with a canonical tonemapping operation.



### SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis
- **Arxiv ID**: http://arxiv.org/abs/2106.01444v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.01444v2)
- **Published**: 2021-06-02 19:58:20+00:00
- **Updated**: 2022-01-08 04:42:48+00:00
- **Authors**: Joshua Feinglass, Yezhou Yang
- **Comment**: Accepted to and presented at ACL 2021 Main Conference (Oral)
- **Journal**: None
- **Summary**: The open-ended nature of visual captioning makes it a challenging area for evaluation. The majority of proposed models rely on specialized training to improve human-correlation, resulting in limited adoption, generalizability, and explainabilty. We introduce "typicality", a new formulation of evaluation rooted in information theory, which is uniquely suited for problems lacking a definite ground truth. Typicality serves as our framework to develop a novel semantic comparison, SPARCS, as well as referenceless fluency evaluation metrics. Over the course of our analysis, two separate dimensions of fluency naturally emerge: style, captured by metric SPURTS, and grammar, captured in the form of grammatical outlier penalties. Through extensive experiments and ablation studies on benchmark datasets, we show how these decomposed dimensions of semantics and fluency provide greater system-level insight into captioner differences. Our proposed metrics along with their combination, SMURF, achieve state-of-the-art correlation with human judgment when compared with other rule-based evaluation metrics.



### Long Term Object Detection and Tracking in Collaborative Learning Environments
- **Arxiv ID**: http://arxiv.org/abs/2106.07556v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07556v1)
- **Published**: 2021-06-02 20:15:14+00:00
- **Updated**: 2021-06-02 20:15:14+00:00
- **Authors**: Sravani Teeparthi
- **Comment**: Master's thesis
- **Journal**: None
- **Summary**: Human activity recognition in videos is a challenging problem that has drawn a lot of interest, particularly when the goal requires the analysis of a large video database. AOLME project provides a collaborative learning environment for middle school students to explore mathematics, computer science, and engineering by processing digital images and videos. As part of this project, around 2200 hours of video data was collected for analysis. Because of the size of the dataset, it is hard to analyze all the videos of the dataset manually. Thus, there is a huge need for reliable computer-based methods that can detect activities of interest. My thesis is focused on the development of accurate methods for detecting and tracking objects in long videos. All the models are validated on videos from 7 different sessions, ranging from 45 minutes to 90 minutes. The keyboard detector achieved a very high average precision (AP) of 92% at 0.5 intersection over union (IoU). Furthermore, a combined system of the detector with a fast tracker KCF (159fps) was developed so that the algorithm runs significantly faster without sacrificing accuracy. For a video of 23 minutes having resolution 858X480 @ 30 fps, the detection alone runs at 4.7Xthe real-time, and the combined algorithm runs at 21Xthe real-time for an average IoU of 0.84 and 0.82, respectively. The hand detector achieved average precision (AP) of 72% at 0.5 IoU. The detection results were improved to 81% using optimal data augmentation parameters. The hand detector runs at 4.7Xthe real-time with AP of 81% at 0.5 IoU. The hand detection method was integrated with projections and clustering for accurate proposal generation. This approach reduced the number of false-positive hand detections by 80%. The overall hand detection system runs at 4Xthe real-time, capturing all the activity regions of the current collaborative group.



### Domain Adaptation for Facial Expression Classifier via Domain Discrimination and Gradient Reversal
- **Arxiv ID**: http://arxiv.org/abs/2106.01467v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.01467v1)
- **Published**: 2021-06-02 20:58:24+00:00
- **Updated**: 2021-06-02 20:58:24+00:00
- **Authors**: Kamil Akhmetov
- **Comment**: None
- **Journal**: None
- **Summary**: Bringing empathy to a computerized system could significantly improve the quality of human-computer communications, as soon as machines would be able to understand customer intentions and better serve their needs. According to different studies (Literature Review), visual information is one of the most important channels of human interaction and contains significant behavioral signals, that may be captured from facial expressions. Therefore, it is consistent and natural that the research in the field of Facial Expression Recognition (FER) has acquired increased interest over the past decade due to having diverse application area including health-care, sociology, psychology, driver-safety, virtual reality, cognitive sciences, security, entertainment, marketing, etc. We propose a new architecture for the task of FER and examine the impact of domain discrimination loss regularization on the learning process. With regard to observations, including both classical training conditions and unsupervised domain adaptation scenarios, important aspects of the considered domain adaptation approach integration are traced. The results may serve as a foundation for further research in the field.



### Artificial Perceptual Learning: Image Categorization with Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/2106.07559v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07559v1)
- **Published**: 2021-06-02 21:12:20+00:00
- **Updated**: 2021-06-02 21:12:20+00:00
- **Authors**: Chengliang Tang, María Uriarte, Helen Jin, Douglas C. Morton, Tian Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning has achieved much success on supervised learning tasks with large sets of well-annotated training samples. However, in many practical situations, such strong and high-quality supervision provided by training data is unavailable due to the expensive and labor-intensive labeling process. Automatically identifying and recognizing object categories in a large volume of unlabeled images with weak supervision remains an important, yet unsolved challenge in computer vision. In this paper, we propose a novel machine learning framework, artificial perceptual learning (APL), to tackle the problem of weakly supervised image categorization. The proposed APL framework is constructed using state-of-the-art machine learning algorithms as building blocks to mimic the cognitive development process known as infant categorization. We develop and illustrate the proposed framework by implementing a wide-field fine-grain ecological survey of tree species over an 8,000-hectare area of the El Yunque rainforest in Puerto Rico. It is based on unlabeled high-resolution aerial images of the tree canopy. Misplaced ground-based labels were available for less than 1% of these images, which serve as the only weak supervision for this learning framework. We validate the proposed framework using a small set of images with high quality human annotations and show that the proposed framework attains human-level cognitive economy.



### Multiscale Domain Adaptive YOLO for Cross-Domain Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.01483v2
- **DOI**: 10.1109/ICIP42928.2021.9506039
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.01483v2)
- **Published**: 2021-06-02 21:50:25+00:00
- **Updated**: 2022-02-10 21:06:41+00:00
- **Authors**: Mazin Hnewa, Hayder Radha
- **Comment**: None
- **Journal**: IEEE International Conference on Image Processing (ICIP), 2021,
  pp. 3323-3327,
- **Summary**: The area of domain adaptation has been instrumental in addressing the domain shift problem encountered by many applications. This problem arises due to the difference between the distributions of source data used for training in comparison with target data used during realistic testing scenarios. In this paper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO) framework that employs multiple domain adaptation paths and corresponding domain classifiers at different scales of the recently introduced YOLOv4 object detector to generate domain-invariant features. We train and test our proposed method using popular datasets. Our experiments show significant improvements in object detection performance when training YOLOv4 using the proposed MS-DAYOLO and when tested on target data representing challenging weather conditions for autonomous driving applications.



### LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes
- **Arxiv ID**: http://arxiv.org/abs/2106.01487v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.01487v2)
- **Published**: 2021-06-02 21:57:52+00:00
- **Updated**: 2021-10-07 02:17:11+00:00
- **Authors**: Aditya Kusupati, Matthew Wallingford, Vivek Ramanujan, Raghav Somani, Jae Sung Park, Krishna Pillutla, Prateek Jain, Sham Kakade, Ali Farhadi
- **Comment**: NeurIPS 2021 Camera Ready. 19 pages, 6 figures
- **Journal**: None
- **Summary**: Learning binary representations of instances and classes is a classical problem with several high potential applications. In modern settings, the compression of high-dimensional neural representations to low-dimensional binary codes is a challenging task and often require large bit-codes to be accurate. In this work, we propose a novel method for Learning Low-dimensional binary Codes (LLC) for instances as well as classes. Our method does not require any side-information, like annotated attributes or label meta-data, and learns extremely low-dimensional binary codes (~20 bits for ImageNet-1K). The learnt codes are super-efficient while still ensuring nearly optimal classification accuracy for ResNet50 on ImageNet-1K. We demonstrate that the learnt codes capture intrinsically important features in the data, by discovering an intuitive taxonomy over classes. We further quantitatively measure the quality of our codes by applying it to the efficient image retrieval as well as out-of-distribution (OOD) detection problems. For ImageNet-100 retrieval problem, our learnt binary codes outperform 16 bit HashNet using only 10 bits and also are as accurate as 10 dimensional real representations. Finally, our learnt binary codes can perform OOD detection, out-of-the-box, as accurately as a baseline that needs ~3000 samples to tune its threshold, while we require none. Code is open-sourced at https://github.com/RAIVNLab/LLC.



### Not All Knowledge Is Created Equal: Mutual Distillation of Confident Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2106.01489v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.01489v3)
- **Published**: 2021-06-02 22:06:55+00:00
- **Updated**: 2022-11-16 14:33:15+00:00
- **Authors**: Ziyun Li, Xinshao Wang, Di Hu, Neil M. Robertson, David A. Clifton, Christoph Meinel, Haojin Yang
- **Comment**: NeurIPS 2022 Workshop(Trustworthy and Socially Responsible Machine
  Learning) paper
- **Journal**: None
- **Summary**: Mutual knowledge distillation (MKD) improves a model by distilling knowledge from another model. However, \textit{not all knowledge is certain and correct}, especially under adverse conditions. For example, label noise usually leads to less reliable models due to undesired memorization \cite{zhang2017understanding,arpit2017closer}. Wrong knowledge misleads the learning rather than helps. This problem can be handled by two aspects: (i) improving the reliability of a model where the knowledge is from (i.e., knowledge source's reliability); (ii) selecting reliable knowledge for distillation. In the literature, making a model more reliable is widely studied while selective MKD receives little attention. Therefore, we focus on studying selective MKD. Concretely, a generic MKD framework, \underline{C}onfident knowledge selection followed by \underline{M}utual \underline{D}istillation (CMD), is designed. The key component of CMD is a generic knowledge selection formulation, making the selection threshold either static (CMD-S) or progressive (CMD-P). Additionally, CMD covers two special cases: zero-knowledge and all knowledge, leading to a unified MKD framework. Extensive experiments are present to demonstrate the effectiveness of CMD and thoroughly justify the design of CMD. For example, CMD-P obtains new state-of-the-art results in robustness against label noise.



### Personalizing Pre-trained Models
- **Arxiv ID**: http://arxiv.org/abs/2106.01499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.01499v1)
- **Published**: 2021-06-02 22:58:47+00:00
- **Updated**: 2021-06-02 22:58:47+00:00
- **Authors**: Mina Khan, P Srivatsa, Advait Rane, Shriram Chenniappa, Asadali Hazariwala, Pattie Maes
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised or weakly supervised models trained on large-scale datasets have shown sample-efficient transfer to diverse datasets in few-shot settings. We consider how upstream pretrained models can be leveraged for downstream few-shot, multilabel, and continual learning tasks. Our model CLIPPER (CLIP PERsonalized) uses image representations from CLIP, a large-scale image representation learning model trained using weak natural language supervision. We developed a technique, called Multi-label Weight Imprinting (MWI), for multi-label, continual, and few-shot learning, and CLIPPER uses MWI with image representations from CLIP. We evaluated CLIPPER on 10 single-label and 5 multi-label datasets. Our model shows robust and competitive performance, and we set new benchmarks for few-shot, multi-label, and continual learning. Our lightweight technique is also compute-efficient and enables privacy-preserving applications as the data is not sent to the upstream model for fine-tuning.



### DeepCompress: Efficient Point Cloud Geometry Compression
- **Arxiv ID**: http://arxiv.org/abs/2106.01504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.01504v1)
- **Published**: 2021-06-02 23:18:11+00:00
- **Updated**: 2021-06-02 23:18:11+00:00
- **Authors**: Ryan Killea, Yun Li, Saeed Bastani, Paul McLachlan
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: Point clouds are a basic data type that is increasingly of interest as 3D content becomes more ubiquitous. Applications using point clouds include virtual, augmented, and mixed reality and autonomous driving. We propose a more efficient deep learning-based encoder architecture for point clouds compression that incorporates principles from established 3D object detection and image compression architectures. Through an ablation study, we show that incorporating the learned activation function from Computational Efficient Neural Image Compression (CENIC) and designing more parameter-efficient convolutional blocks yields dramatic gains in efficiency and performance. Our proposed architecture incorporates Generalized Divisive Normalization activations and propose a spatially separable InceptionV4-inspired block. We then evaluate rate-distortion curves on the standard JPEG Pleno 8i Voxelized Full Bodies dataset to evaluate our model's performance. Our proposed modifications outperform the baseline approaches by a small margin in terms of Bjontegard delta rate and PSNR values, yet reduces necessary encoder convolution operations by 8 percent and reduces total encoder parameters by 20 percent. Our proposed architecture, when considered on its own, has a small penalty of 0.02 percent in Chamfer's Distance and 0.32 percent increased bit rate in Point to Plane Distance for the same peak signal-to-noise ratio.



### Barbershop: GAN-based Image Compositing using Segmentation Masks
- **Arxiv ID**: http://arxiv.org/abs/2106.01505v2
- **DOI**: 10.1145/3478513.3480537
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2106.01505v2)
- **Published**: 2021-06-02 23:20:43+00:00
- **Updated**: 2021-10-16 18:51:36+00:00
- **Authors**: Peihao Zhu, Rameen Abdal, John Femiani, Peter Wonka
- **Comment**: Project page: https://zpdesu.github.io/Barbershop/ Video:
  https://youtu.be/ZU-yrAvoJfQ
- **Journal**: None
- **Summary**: Seamlessly blending features from multiple images is extremely challenging because of complex relationships in lighting, geometry, and partial occlusion which cause coupling between different parts of the image. Even though recent work on GANs enables synthesis of realistic hair or faces, it remains difficult to combine them into a single, coherent, and plausible image rather than a disjointed set of image patches. We present a novel solution to image blending, particularly for the problem of hairstyle transfer, based on GAN-inversion. We propose a novel latent space for image blending which is better at preserving detail and encoding spatial information, and propose a new GAN-embedding algorithm which is able to slightly modify images to conform to a common segmentation mask. Our novel representation enables the transfer of the visual properties from multiple reference images including specific details such as moles and wrinkles, and because we do image blending in a latent-space we are able to synthesize images that are coherent. Our approach avoids blending artifacts present in other approaches and finds a globally consistent image. Our results demonstrate a significant improvement over the current state of the art in a user study, with users preferring our blending solution over 95 percent of the time.



