# Arxiv Papers in cs.CV on 2021-06-07
### Video Imprint
- **Arxiv ID**: http://arxiv.org/abs/2106.03283v1
- **DOI**: 10.1109/TPAMI.2018.2866114
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03283v1)
- **Published**: 2021-06-07 00:32:47+00:00
- **Updated**: 2021-06-07 00:32:47+00:00
- **Authors**: Zhanning Gao, Le Wang, Nebojsa Jojic, Zhenxing Niu, Nanning Zheng, Gang Hua
- **Comment**: None
- **Journal**: IEEE transactions on pattern analysis and machine intelligence,
  41(12), 3086-3099 (2018)
- **Summary**: A new unified video analytics framework (ER3) is proposed for complex event retrieval, recognition and recounting, based on the proposed video imprint representation, which exploits temporal correlations among image features across video frames. With the video imprint representation, it is convenient to reverse map back to both temporal and spatial locations in video frames, allowing for both key frame identification and key areas localization within each frame. In the proposed framework, a dedicated feature alignment module is incorporated for redundancy removal across frames to produce the tensor representation, i.e., the video imprint. Subsequently, the video imprint is individually fed into both a reasoning network and a feature aggregation module, for event recognition/recounting and event retrieval tasks, respectively. Thanks to its attention mechanism inspired by the memory networks used in language modeling, the proposed reasoning network is capable of simultaneous event category recognition and localization of the key pieces of evidence for event recounting. In addition, the latent structure in our reasoning network highlights the areas of the video imprint, which can be directly used for event recounting. With the event retrieval task, the compact video representation aggregated from the video imprint contributes to better retrieval results than existing state-of-the-art methods.



### Video Instance Segmentation using Inter-Frame Communication Transformers
- **Arxiv ID**: http://arxiv.org/abs/2106.03299v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03299v1)
- **Published**: 2021-06-07 02:08:39+00:00
- **Updated**: 2021-06-07 02:08:39+00:00
- **Authors**: Sukjun Hwang, Miran Heo, Seoung Wug Oh, Seon Joo Kim
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel end-to-end solution for video instance segmentation (VIS) based on transformers. Recently, the per-clip pipeline shows superior performance over per-frame methods leveraging richer information from multiple frames. However, previous per-clip models require heavy computation and memory usage to achieve frame-to-frame communications, limiting practicality. In this work, we propose Inter-frame Communication Transformers (IFC), which significantly reduces the overhead for information-passing between frames by efficiently encoding the context within the input clip. Specifically, we propose to utilize concise memory tokens as a mean of conveying information as well as summarizing each frame scene. The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens. We validate our method on the latest benchmark sets and achieved the state-of-the-art performance (AP 44.6 on YouTube-VIS 2019 val set using the offline inference) while having a considerably fast runtime (89.4 FPS). Our method can also be applied to near-online inference for processing a video in real-time with only a small delay. The code will be made available.



### Zero-Shot Knowledge Distillation from a Decision-Based Black-Box Model
- **Arxiv ID**: http://arxiv.org/abs/2106.03310v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03310v1)
- **Published**: 2021-06-07 02:46:31+00:00
- **Updated**: 2021-06-07 02:46:31+00:00
- **Authors**: Zi Wang
- **Comment**: Accepted to ICML 2021
- **Journal**: None
- **Summary**: Knowledge distillation (KD) is a successful approach for deep neural network acceleration, with which a compact network (student) is trained by mimicking the softmax output of a pre-trained high-capacity network (teacher). In tradition, KD usually relies on access to the training samples and the parameters of the white-box teacher to acquire the transferred knowledge. However, these prerequisites are not always realistic due to storage costs or privacy issues in real-world applications. Here we propose the concept of decision-based black-box (DB3) knowledge distillation, with which the student is trained by distilling the knowledge from a black-box teacher (parameters are not accessible) that only returns classes rather than softmax outputs. We start with the scenario when the training set is accessible. We represent a sample's robustness against other classes by computing its distances to the teacher's decision boundaries and use it to construct the soft label for each training sample. After that, the student can be trained via standard KD. We then extend this approach to a more challenging scenario in which even accessing the training data is not feasible. We propose to generate pseudo samples distinguished by the teacher's decision boundaries to the largest extent and construct soft labels for them, which are used as the transfer set. We evaluate our approaches on various benchmark networks and datasets and experiment results demonstrate their effectiveness. Codes are available at: https://github.com/zwang84/zsdb3kd.



### Exploring to establish an appropriate model for image aesthetic assessment via CNN-based RSRL: An empirical study
- **Arxiv ID**: http://arxiv.org/abs/2106.03316v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03316v2)
- **Published**: 2021-06-07 03:20:00+00:00
- **Updated**: 2021-06-28 01:37:31+00:00
- **Authors**: Ying Dai
- **Comment**: None
- **Journal**: Journal of Imaging 2022, 8(4), 85
- **Summary**: To establish an appropriate model for photo aesthetic assessment, in this paper, a D-measure which reflects the disentanglement degree of the final layer FC nodes of CNN is introduced. By combining F-measure with D-measure to obtain a FD measure, an algorithm of determining the optimal model from the multiple photo score prediction models generated by CNN-based repetitively self-revised learning(RSRL) is proposed. Furthermore, the first fixation perspective(FFP) and the assessment interest region(AIR) of the models are defined and calculated. The experimental results show that the FD measure is effective for establishing the appropriate model from the multiple score prediction models with different CNN structures. Moreover, the FD-determined optimal models with the comparatively high FD always have the FFP an AIR which are close to the human's aesthetic perception when enjoying photos.



### A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.03323v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03323v5)
- **Published**: 2021-06-07 03:51:25+00:00
- **Updated**: 2022-12-20 07:23:37+00:00
- **Authors**: Jie Gui, Xiaofeng Cong, Yuan Cao, Wenqi Ren, Jun Zhang, Jing Zhang, Jiuxin Cao, Dacheng Tao
- **Comment**: This paper is accepted by ACM Computing Surveys
- **Journal**: None
- **Summary**: With the development of convolutional neural networks, hundreds of deep learning based dehazing methods have been proposed. In this paper, we provide a comprehensive survey on supervised, semi-supervised, and unsupervised single image dehazing. We first discuss the physical model, datasets, network modules, loss functions, and evaluation metrics that are commonly used. Then, the main contributions of various dehazing algorithms are categorized and summarized. Further, quantitative and qualitative experiments of various baseline methods are carried out. Finally, the unsolved issues and challenges that can inspire the future research are pointed out. A collection of useful dehazing materials is available at \url{https://github.com/Xiaofeng-life/AwesomeDehazing}.



### Contextual Guided Segmentation Framework for Semi-supervised Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.03330v2
- **DOI**: 10.1007/s00138-022-01278-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03330v2)
- **Published**: 2021-06-07 04:16:50+00:00
- **Updated**: 2022-04-11 08:46:16+00:00
- **Authors**: Trung-Nghia Le, Tam V. Nguyen, Minh-Triet Tran
- **Comment**: Project page: https://sites.google.com/view/ltnghia/research/vos
- **Journal**: Machine Vision and Applications 2022
- **Summary**: In this paper, we propose Contextual Guided Segmentation (CGS) framework for video instance segmentation in three passes. In the first pass, i.e., preview segmentation, we propose Instance Re-Identification Flow to estimate main properties of each instance (i.e., human/non-human, rigid/deformable, known/unknown category) by propagating its preview mask to other frames. In the second pass, i.e., contextual segmentation, we introduce multiple contextual segmentation schemes. For human instance, we develop skeleton-guided segmentation in a frame along with object flow to correct and refine the result across frames. For non-human instance, if the instance has a wide variation in appearance and belongs to known categories (which can be inferred from the initial mask), we adopt instance segmentation. If the non-human instance is nearly rigid, we train FCNs on synthesized images from the first frame of a video sequence. In the final pass, i.e., guided segmentation, we develop a novel fined-grained segmentation method on non-rectangular regions of interest (ROIs). The natural-shaped ROI is generated by applying guided attention from the neighbor frames of the current one to reduce the ambiguity in the segmentation of different overlapping instances. Forward mask propagation is followed by backward mask propagation to further restore missing instance fragments due to re-appeared instances, fast motion, occlusion, or heavy deformation. Finally, instances in each frame are merged based on their depth values, together with human and non-human object interaction and rare instance priority. Experiments conducted on the DAVIS Test-Challenge dataset demonstrate the effectiveness of our proposed framework. We achieved the 3rd consistently in the DAVIS Challenges 2017-2019 with 75.4%, 72.4%, and 78.4% in terms of global score, region similarity, and contour accuracy, respectively.



### SelfDoc: Self-Supervised Document Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.03331v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2106.03331v1)
- **Published**: 2021-06-07 04:19:49+00:00
- **Updated**: 2021-06-07 04:19:49+00:00
- **Authors**: Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I. Morariu, Handong Zhao, Rajiv Jain, Varun Manjunatha, Hongfu Liu
- **Comment**: To appear in CVPR'2021
- **Journal**: None
- **Summary**: We propose SelfDoc, a task-agnostic pre-training framework for document image understanding. Because documents are multimodal and are intended for sequential reading, our framework exploits the positional, textual, and visual information of every semantically meaningful component in a document, and it models the contextualization between each block of content. Unlike existing document pre-training models, our model is coarse-grained instead of treating individual words as input, therefore avoiding an overly fine-grained with excessive contextualization. Beyond that, we introduce cross-modal learning in the model pre-training phase to fully leverage multimodal information from unlabeled documents. For downstream usage, we propose a novel modality-adaptive attention mechanism for multimodal feature fusion by adaptively emphasizing language and vision signals. Our framework benefits from self-supervised pre-training on documents without requiring annotations by a feature masking training strategy. It achieves superior performance on multiple downstream tasks with significantly fewer document images used in the pre-training stage compared to previous works.



### Wide-Baseline Relative Camera Pose Estimation with Directional Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.03336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03336v1)
- **Published**: 2021-06-07 04:46:09+00:00
- **Updated**: 2021-06-07 04:46:09+00:00
- **Authors**: Kefan Chen, Noah Snavely, Ameesh Makadia
- **Comment**: None
- **Journal**: None
- **Summary**: Modern deep learning techniques that regress the relative camera pose between two images have difficulty dealing with challenging scenarios, such as large camera motions resulting in occlusions and significant changes in perspective that leave little overlap between images. These models continue to struggle even with the benefit of large supervised training datasets. To address the limitations of these models, we take inspiration from techniques that show regressing keypoint locations in 2D and 3D can be improved by estimating a discrete distribution over keypoint locations. Analogously, in this paper we explore improving camera pose regression by instead predicting a discrete distribution over camera poses. To realize this idea, we introduce DirectionNet, which estimates discrete distributions over the 5D relative pose space using a novel parameterization to make the estimation problem tractable. Specifically, DirectionNet factorizes relative camera pose, specified by a 3D rotation and a translation direction, into a set of 3D direction vectors. Since 3D directions can be identified with points on the sphere, DirectionNet estimates discrete distributions on the sphere as its output. We evaluate our model on challenging synthetic and real pose estimation datasets constructed from Matterport3D and InteriorNet. Promising results show a near 50% reduction in error over direct regression methods.



### ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias
- **Arxiv ID**: http://arxiv.org/abs/2106.03348v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03348v4)
- **Published**: 2021-06-07 05:31:06+00:00
- **Updated**: 2021-12-24 02:57:08+00:00
- **Authors**: Yufei Xu, Qiming Zhang, Jing Zhang, Dacheng Tao
- **Comment**: Accepted by NeurIPS 2021, 23 pages, including downstream task results
  (detection, semantic segmentation, human pose, and video object segmentation)
- **Journal**: None
- **Summary**: Transformers have shown great potential in various computer vision tasks owing to their strong capability in modeling long-range dependency using the self-attention mechanism. Nevertheless, vision transformers treat an image as 1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual structures and dealing with scale variance. Alternatively, they require large-scale training data and longer training schedules to learn the IB implicitly. In this paper, we propose a novel Vision Transformer Advanced by Exploring intrinsic IB from convolutions, ie, ViTAE. Technically, ViTAE has several spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Moreover, in each transformer layer, ViTAE has a convolution block in parallel to the multi-head self-attention module, whose features are fused and fed into the feed-forward network. Consequently, it has the intrinsic locality IB and is able to learn local features and global dependencies collaboratively. Experiments on ImageNet as well as downstream tasks prove the superiority of ViTAE over the baseline transformer and concurrent works. Source code and pretrained models will be available at GitHub.



### Continual Active Learning for Efficient Adaptation of Machine Learning Models to Changing Image Acquisition
- **Arxiv ID**: http://arxiv.org/abs/2106.03351v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03351v1)
- **Published**: 2021-06-07 05:39:06+00:00
- **Updated**: 2021-06-07 05:39:06+00:00
- **Authors**: Matthias Perkonigg, Johannes Hofmanninger, Georg Langs
- **Comment**: Accepted for publication at the 27th international conference on
  Information Processing in Medical Imaging (IPMI) 2021
- **Journal**: None
- **Summary**: Imaging in clinical routine is subject to changing scanner protocols, hardware, or policies in a typically heterogeneous set of acquisition hardware. Accuracy and reliability of deep learning models suffer from those changes as data and targets become inconsistent with their initial static training set. Continual learning can adapt to a continuous data stream of a changing imaging environment. Here, we propose a method for continual active learning on a data stream of medical images. It recognizes shifts or additions of new imaging sources - domains -, adapts training accordingly, and selects optimal examples for labelling. Model training has to cope with a limited labelling budget, resembling typical real world scenarios. We demonstrate our method on T1-weighted magnetic resonance images from three different scanners with the task of brain age estimation. Results demonstrate that the proposed method outperforms naive active learning while requiring less manual labelling.



### Commutative Lie Group VAE for Disentanglement Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.03375v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03375v1)
- **Published**: 2021-06-07 07:03:14+00:00
- **Updated**: 2021-06-07 07:03:14+00:00
- **Authors**: Xinqi Zhu, Chang Xu, Dacheng Tao
- **Comment**: Accepted in ICML2021
- **Journal**: None
- **Summary**: We view disentanglement learning as discovering an underlying structure that equivariantly reflects the factorized variations shown in data. Traditionally, such a structure is fixed to be a vector space with data variations represented by translations along individual latent dimensions. We argue this simple structure is suboptimal since it requires the model to learn to discard the properties (e.g. different scales of changes, different levels of abstractness) of data variations, which is an extra work than equivariance learning. Instead, we propose to encode the data variations with groups, a structure not only can equivariantly represent variations, but can also be adaptively optimized to preserve the properties of data variations. Considering it is hard to conduct training on group structures, we focus on Lie groups and adopt a parameterization using Lie algebra. Based on the parameterization, some disentanglement learning constraints are naturally derived. A simple model named Commutative Lie Group VAE is introduced to realize the group-based disentanglement learning. Experiments show that our model can effectively learn disentangled representations without supervision, and can achieve state-of-the-art performance without extra constraints.



### ContourRender: Detecting Arbitrary Contour Shape For Instance Segmentation In One Pass
- **Arxiv ID**: http://arxiv.org/abs/2106.03382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03382v1)
- **Published**: 2021-06-07 07:23:03+00:00
- **Updated**: 2021-06-07 07:23:03+00:00
- **Authors**: Tutian Tang, Wenqiang Xu, Ruolin Ye, Yan-Feng Wang, Cewu Lu
- **Comment**: Tech report
- **Journal**: None
- **Summary**: Direct contour regression for instance segmentation is a challenging task. Previous works usually achieve it by learning to progressively refine the contour prediction or adopting a shape representation with limited expressiveness. In this work, we argue that the difficulty in regressing the contour points in one pass is mainly due to the ambiguity when discretizing a smooth contour into a polygon. To address the ambiguity, we propose a novel differentiable rendering-based approach named \textbf{ContourRender}. During training, it first predicts a contour generated by an invertible shape signature, and then optimizes the contour with the more stable silhouette by converting it to a contour mesh and rendering the mesh to a 2D map.   This method significantly improves the quality of contour without iterations or cascaded refinements. Moreover, as optimization is not needed during inference, the inference speed will not be influenced.   Experiments show the proposed ContourRender outperforms all the contour-based instance segmentation approaches on COCO, while stays competitive with the iteration-based state-of-the-art on Cityscapes. In addition, we specifically select a subset from COCO val2017 named COCO ContourHard-val to further demonstrate the contour quality improvements. Codes, models, and dataset split will be released.



### DINs: Deep Interactive Networks for Neurofibroma Segmentation in Neurofibromatosis Type 1 on Whole-Body MRI
- **Arxiv ID**: http://arxiv.org/abs/2106.03388v1
- **DOI**: 10.1109/JBHI.2021.3087735
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03388v1)
- **Published**: 2021-06-07 07:29:29+00:00
- **Updated**: 2021-06-07 07:29:29+00:00
- **Authors**: Jian-Wei Zhang, Wei Chen, K. Ina Ly, Xubin Zhang, Fan Yan, Justin Jordan, Gordon Harris, Scott Plotkin, Pengyi Hao, Wenli Cai
- **Comment**: Accepted by IEEE Journal of Biomedical and Health Informatics (JBHI)
- **Journal**: IEEE Journal of Biomedical and Health Informatics, 2021
- **Summary**: Neurofibromatosis type 1 (NF1) is an autosomal dominant tumor predisposition syndrome that involves the central and peripheral nervous systems. Accurate detection and segmentation of neurofibromas are essential for assessing tumor burden and longitudinal tumor size changes. Automatic convolutional neural networks (CNNs) are sensitive and vulnerable as tumors' variable anatomical location and heterogeneous appearance on MRI. In this study, we propose deep interactive networks (DINs) to address the above limitations. User interactions guide the model to recognize complicated tumors and quickly adapt to heterogeneous tumors. We introduce a simple but effective Exponential Distance Transform (ExpDT) that converts user interactions into guide maps regarded as the spatial and appearance prior. Comparing with popular Euclidean and geodesic distances, ExpDT is more robust to various image sizes, which reserves the distribution of interactive inputs. Furthermore, to enhance the tumor-related features, we design a deep interactive module to propagate the guides into deeper layers. We train and evaluate DINs on three MRI data sets from NF1 patients. The experiment results yield significant improvements of 44% and 14% in DSC comparing with automated and other interactive methods, respectively. We also experimentally demonstrate the efficiency of DINs in reducing user burden when comparing with conventional interactive methods. The source code of our method is available at \url{https://github.com/Jarvis73/DINs}.



### Resolution learning in deep convolutional networks using scale-space theory
- **Arxiv ID**: http://arxiv.org/abs/2106.03412v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03412v2)
- **Published**: 2021-06-07 08:23:02+00:00
- **Updated**: 2021-06-30 14:08:16+00:00
- **Authors**: Silvia L. Pintea, Nergis Tomen, Stanley F. Goes, Marco Loog, Jan C. van Gemert
- **Comment**: None
- **Journal**: None
- **Summary**: Resolution in deep convolutional neural networks (CNNs) is typically bounded by the receptive field size through filter sizes, and subsampling layers or strided convolutions on feature maps. The optimal resolution may vary significantly depending on the dataset. Modern CNNs hard-code their resolution hyper-parameters in the network architecture which makes tuning such hyper-parameters cumbersome. We propose to do away with hard-coded resolution hyper-parameters and aim to learn the appropriate resolution from data. We use scale-space theory to obtain a self-similar parametrization of filters and make use of the N-Jet: a truncated Taylor series to approximate a filter by a learned combination of Gaussian derivative filters. The parameter sigma of the Gaussian basis controls both the amount of detail the filter encodes and the spatial extent of the filter. Since sigma is a continuous parameter, we can optimize it with respect to the loss. The proposed N-Jet layer achieves comparable performance when used in state-of-the art architectures, while learning the correct resolution in each layer automatically. We evaluate our N-Jet layer on both classification and segmentation, and we show that learning sigma is especially beneficial for inputs at multiple sizes.



### Multi-Target Domain Adaptation with Collaborative Consistency Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.03418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03418v1)
- **Published**: 2021-06-07 08:36:20+00:00
- **Updated**: 2021-06-07 08:36:20+00:00
- **Authors**: Takashi Isobe, Xu Jia, Shuaijun Chen, Jianzhong He, Yongjie Shi, Jianzhuang Liu, Huchuan Lu, Shengjin Wang
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Recently unsupervised domain adaptation for the semantic segmentation task has become more and more popular due to high-cost of pixel-level annotation on real-world images. However, most domain adaptation methods are only restricted to single-source-single-target pair, and can not be directly extended to multiple target domains. In this work, we propose a collaborative learning framework to achieve unsupervised multi-target domain adaptation. An unsupervised domain adaptation expert model is first trained for each source-target pair and is further encouraged to collaborate with each other through a bridge built between different target domains. These expert models are further improved by adding the regularization of making the consistent pixel-wise prediction for each sample with the same structured context. To obtain a single model that works across multiple target domains, we propose to simultaneously learn a student model which is trained to not only imitate the output of each expert on the corresponding target domain, but also to pull different expert close to each other with regularization on their weights. Extensive experiments demonstrate that the proposed method can effectively exploit rich structured information contained in both labeled source domain and multiple unlabeled target domains. Not only does it perform well across multiple target domains but also performs favorably against state-of-the-art unsupervised domain adaptation methods specially trained on a single source-target pair



### Source-Free Open Compound Domain Adaptation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.03422v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03422v1)
- **Published**: 2021-06-07 08:38:41+00:00
- **Updated**: 2021-06-07 08:38:41+00:00
- **Authors**: Yuyang Zhao, Zhun Zhong, Zhiming Luo, Gim Hee Lee, Nicu Sebe
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we introduce a new concept, named source-free open compound domain adaptation (SF-OCDA), and study it in semantic segmentation. SF-OCDA is more challenging than the traditional domain adaptation but it is more practical. It jointly considers (1) the issues of data privacy and data storage and (2) the scenario of multiple target domains and unseen open domains. In SF-OCDA, only the source pre-trained model and the target data are available to learn the target model. The model is evaluated on the samples from the target and unseen open domains. To solve this problem, we present an effective framework by separating the training process into two stages: (1) pre-training a generalized source model and (2) adapting a target model with self-supervised learning. In our framework, we propose the Cross-Patch Style Swap (CPSS) to diversify samples with various patch styles in the feature-level, which can benefit the training of both stages. First, CPSS can significantly improve the generalization ability of the source model, providing more accurate pseudo-labels for the latter stage. Second, CPSS can reduce the influence of noisy pseudo-labels and also avoid the model overfitting to the target domain during self-supervised learning, consistently boosting the performance on the target and open domains. Experiments demonstrate that our method produces state-of-the-art results on the C-Driving dataset. Furthermore, our model also achieves the leading performance on CityScapes for domain generalization.



### Channel DropBlock: An Improved Regularization Method for Fine-Grained Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.03432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03432v1)
- **Published**: 2021-06-07 09:03:02+00:00
- **Updated**: 2021-06-07 09:03:02+00:00
- **Authors**: Yifeng Ding, Shuwei Dong, Yujun Tong, Zhanyu Ma, Bo Xiao, Haibin Ling
- **Comment**: None
- **Journal**: None
- **Summary**: Classifying the sub-categories of an object from the same super-category (e.g., bird) in a fine-grained visual classification (FGVC) task highly relies on mining multiple discriminative features. Existing approaches mainly tackle this problem by introducing attention mechanisms to locate the discriminative parts or feature encoding approaches to extract the highly parameterized features in a weakly-supervised fashion. In this work, we propose a lightweight yet effective regularization method named Channel DropBlock (CDB), in combination with two alternative correlation metrics, to address this problem. The key idea is to randomly mask out a group of correlated channels during training to destruct features from co-adaptations and thus enhance feature representations. Extensive experiments on three benchmark FGVC datasets show that CDB effectively improves the performance.



### Unsupervised Learning for Cuboid Shape Abstraction via Joint Segmentation from Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2106.03437v1
- **DOI**: 10.1145/3450626.3459873
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2106.03437v1)
- **Published**: 2021-06-07 09:15:16+00:00
- **Updated**: 2021-06-07 09:15:16+00:00
- **Authors**: Kaizhi Yang, Xuejin Chen
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Representing complex 3D objects as simple geometric primitives, known as shape abstraction, is important for geometric modeling, structural analysis, and shape synthesis. In this paper, we propose an unsupervised shape abstraction method to map a point cloud into a compact cuboid representation. We jointly predict cuboid allocation as part segmentation and cuboid shapes and enforce the consistency between the segmentation and shape abstraction for self-learning. For the cuboid abstraction task, we transform the input point cloud into a set of parametric cuboids using a variational auto-encoder network. The segmentation network allocates each point into a cuboid considering the point-cuboid affinity. Without manual annotations of parts in point clouds, we design four novel losses to jointly supervise the two branches in terms of geometric similarity and cuboid compactness. We evaluate our method on multiple shape collections and demonstrate its superiority over existing shape abstraction methods. Moreover, based on our network architecture and learned representations, our approach supports various applications including structured shape generation, shape interpolation, and structural shape clustering.



### supervised adptive threshold network for instance segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.03450v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03450v1)
- **Published**: 2021-06-07 09:25:44+00:00
- **Updated**: 2021-06-07 09:25:44+00:00
- **Authors**: Kuikun Liu, Jie Yang, Cai Sun, Haoyuan Chi
- **Comment**: None
- **Journal**: None
- **Summary**: Currently, instance segmentation is attracting more and more attention in machine learning region. However, there exists some defects on the information propagation in previous Mask R-CNN and other network models. In this paper, we propose supervised adaptive threshold network for instance segmentation. Specifically, we adopt the Mask R-CNN method based on adaptive threshold, and by establishing a layered adaptive network structure, it performs adaptive binarization on the probability graph generated by Mask RCNN to obtain better segmentation effect and reduce the error rate. At the same time, an adaptive feature pool is designed to make the transmission between different layers of the network more accurate and effective, reduce the loss in the process of feature transmission, and further improve the mask method. Experiments on benchmark data sets indicate that the effectiveness of the proposed model



### Shape As Points: A Differentiable Poisson Solver
- **Arxiv ID**: http://arxiv.org/abs/2106.03452v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2106.03452v2)
- **Published**: 2021-06-07 09:28:38+00:00
- **Updated**: 2021-11-08 10:23:06+00:00
- **Authors**: Songyou Peng, Chiyu "Max" Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, Andreas Geiger
- **Comment**: NeurIPS 2021 (Oral). Project page: https://pengsongyou.github.io/sap.
  Code: https://github.com/autonomousvision/shape_as_points
- **Journal**: None
- **Summary**: In recent years, neural implicit representations gained popularity in 3D reconstruction due to their expressiveness and flexibility. However, the implicit nature of neural implicit representations results in slow inference time and requires careful initialization. In this paper, we revisit the classic yet ubiquitous point cloud representation and introduce a differentiable point-to-mesh layer using a differentiable formulation of Poisson Surface Reconstruction (PSR) that allows for a GPU-accelerated fast solution of the indicator function given an oriented point cloud. The differentiable PSR layer allows us to efficiently and differentiably bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field, enabling end-to-end optimization of surface reconstruction metrics such as Chamfer distance. This duality between points and meshes hence allows us to represent shapes as oriented point clouds, which are explicit, lightweight and expressive. Compared to neural implicit representations, our Shape-As-Points (SAP) model is more interpretable, lightweight, and accelerates inference time by one order of magnitude. Compared to other explicit representations such as points, patches, and meshes, SAP produces topology-agnostic, watertight manifold surfaces. We demonstrate the effectiveness of SAP on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction.



### Knowledge-aware Deep Framework for Collaborative Skin Lesion Segmentation and Melanoma Recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.03455v2
- **DOI**: 10.1016/j.patcog.2021.108075
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03455v2)
- **Published**: 2021-06-07 09:33:45+00:00
- **Updated**: 2021-12-02 11:59:35+00:00
- **Authors**: Xiaohong Wang, Xudong Jiang, Henghui Ding, Yuqian Zhao, Jun Liu
- **Comment**: Pattern Recognition
- **Journal**: None
- **Summary**: Deep learning techniques have shown their superior performance in dermatologist clinical inspection. Nevertheless, melanoma diagnosis is still a challenging task due to the difficulty of incorporating the useful dermatologist clinical knowledge into the learning process. In this paper, we propose a novel knowledge-aware deep framework that incorporates some clinical knowledge into collaborative learning of two important melanoma diagnosis tasks, i.e., skin lesion segmentation and melanoma recognition. Specifically, to exploit the knowledge of morphological expressions of the lesion region and also the periphery region for melanoma identification, a lesion-based pooling and shape extraction (LPSE) scheme is designed, which transfers the structure information obtained from skin lesion segmentation into melanoma recognition. Meanwhile, to pass the skin lesion diagnosis knowledge from melanoma recognition to skin lesion segmentation, an effective diagnosis guided feature fusion (DGFF) strategy is designed. Moreover, we propose a recursive mutual learning mechanism that further promotes the inter-task cooperation, and thus iteratively improves the joint learning capability of the model for both skin lesion segmentation and melanoma recognition. Experimental results on two publicly available skin lesion datasets show the effectiveness of the proposed method for melanoma analysis.



### FINet: Dual Branches Feature Interaction for Partial-to-Partial Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2106.03479v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03479v3)
- **Published**: 2021-06-07 10:15:02+00:00
- **Updated**: 2022-04-06 07:50:55+00:00
- **Authors**: Hao Xu, Nianjin Ye, Guanghui Liu, Bing Zeng, Shuaicheng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Data association is important in the point cloud registration. In this work, we propose to solve the partial-to-partial registration from a new perspective, by introducing multi-level feature interactions between the source and the reference clouds at the feature extraction stage, such that the registration can be realized without the attentions or explicit mask estimation for the overlapping detection as adopted previously. Specifically, we present FINet, a feature interaction-based structure with the capability to enable and strengthen the information associating between the inputs at multiple stages. To achieve this, we first split the features into two components, one for rotation and one for translation, based on the fact that they belong to different solution spaces, yielding a dual branches structure. Second, we insert several interaction modules at the feature extractor for the data association. Third, we propose a transformation sensitivity loss to obtain rotation-attentive and translation-attentive features. Experiments demonstrate that our method performs higher precision and robustness compared to the state-of-the-art traditional and learning-based methods. Code is available at https://github.com/megvii-research/FINet.



### Redundant representations help generalization in wide neural networks
- **Arxiv ID**: http://arxiv.org/abs/2106.03485v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03485v4)
- **Published**: 2021-06-07 10:18:54+00:00
- **Updated**: 2023-04-29 09:39:14+00:00
- **Authors**: Diego Doimo, Aldo Glielmo, Sebastian Goldt, Alessandro Laio
- **Comment**: None
- **Journal**: Advances in Neural Information Processing Systems 35 (2022)
- **Summary**: Deep neural networks (DNNs) defy the classical bias-variance trade-off: adding parameters to a DNN that interpolates its training data will typically improve its generalization performance. Explaining the mechanism behind this ``benign overfitting'' in deep networks remains an outstanding challenge. Here, we study the last hidden layer representations of various state-of-the-art convolutional neural networks and find that if the last hidden representation is wide enough, its neurons tend to split into groups that carry identical information, and differ from each other only by statistically independent noise. The number of such groups increases linearly with the width of the layer, but only if the width is above a critical value. We show that redundant neurons appear only when the training process reaches interpolation and the training error is zero.



### Exploiting Emotional Dependencies with Graph Convolutional Networks for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.03487v2
- **DOI**: 10.1109/FG52635.2021.9667014
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03487v2)
- **Published**: 2021-06-07 10:20:05+00:00
- **Updated**: 2021-08-27 10:27:02+00:00
- **Authors**: Panagiotis Antoniadis, Panagiotis P. Filntisis, Petros Maragos
- **Comment**: 9 pages, 8 figures, 5 tables, revised submission to the 16th IEEE
  International Conference on Automatic Face and Gesture Recognition
- **Journal**: 2021 16th IEEE International Conference on Automatic Face and
  Gesture Recognition (FG 2021)
- **Summary**: Over the past few years, deep learning methods have shown remarkable results in many face-related tasks including automatic facial expression recognition (FER) in-the-wild. Meanwhile, numerous models describing the human emotional states have been proposed by the psychology community. However, we have no clear evidence as to which representation is more appropriate and the majority of FER systems use either the categorical or the dimensional model of affect. Inspired by recent work in multi-label classification, this paper proposes a novel multi-task learning (MTL) framework that exploits the dependencies between these two models using a Graph Convolutional Network (GCN) to recognize facial expressions in-the-wild. Specifically, a shared feature representation is learned for both discrete and continuous recognition in a MTL setting. Moreover, the facial expression classifiers and the valence-arousal regressors are learned through a GCN that explicitly captures the dependencies between them. To evaluate the performance of our method under real-world conditions we perform extensive experiments on the AffectNet and Aff-Wild2 datasets. The results of our experiments show that our method is capable of improving the performance across different datasets and backbone architectures. Finally, we also surpass the previous state-of-the-art methods on the categorical model of AffectNet.



### Self-Supervision & Meta-Learning for One-Shot Unsupervised Cross-Domain Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.03496v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03496v3)
- **Published**: 2021-06-07 10:33:04+00:00
- **Updated**: 2022-09-01 08:52:44+00:00
- **Authors**: F. Cappio Borlino, S. Polizzotto, B. Caputo, T. Tommasi
- **Comment**: Accepted for Publication at Computer Vision and Image Understanding
  (CVIU) Journal
- **Journal**: None
- **Summary**: Deep detection approaches are powerful in controlled conditions, but appear brittle and fail when source models are used off-the-shelf on unseen domains. Most of the existing works on domain adaptation simplify the setting and access jointly both a large source dataset and a sizable amount of target samples. However this scenario is unrealistic in many practical cases as when monitoring image feeds from social media: only a pretrained source model is available and every target image uploaded by the users belongs to a different domain not foreseen during training. We address this challenging setting by presenting an object detection algorithm able to exploit a pre-trained source model and perform unsupervised adaptation by using only one target sample seen at test time. Our multi-task architecture includes a self-supervised branch that we exploit to meta-train the whole model with single-sample cross-domain episodes, and prepare to the test condition. At deployment time the self-supervised task is iteratively solved on any incoming sample to one-shot adapt on it. We introduce a new dataset of social media image feeds and present a thorough benchmark with the most recent cross-domain detection methods showing the advantages of our approach.



### Efficient training for future video generation based on hierarchical disentangled representation of latent variables
- **Arxiv ID**: http://arxiv.org/abs/2106.03502v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03502v2)
- **Published**: 2021-06-07 10:43:23+00:00
- **Updated**: 2021-06-08 15:22:18+00:00
- **Authors**: Naoya Fushishita, Antonio Tejero-de-Pablos, Yusuke Mukuta, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: Generating videos predicting the future of a given sequence has been an area of active research in recent years. However, an essential problem remains unsolved: most of the methods require large computational cost and memory usage for training. In this paper, we propose a novel method for generating future prediction videos with less memory usage than the conventional methods. This is a critical stepping stone in the path towards generating videos with high image quality, similar to that of generated images in the latest works in the field of image generation. We achieve high-efficiency by training our method in two stages: (1) image reconstruction to encode video frames into latent variables, and (2) latent variable prediction to generate the future sequence. Our method achieves an efficient compression of video into low-dimensional latent variables by decomposing each frame according to its hierarchical structure. That is, we consider that video can be separated into background and foreground objects, and that each object holds time-varying and time-independent information independently. Our experiments show that the proposed method can efficiently generate future prediction videos, even for complex datasets that cannot be handled by previous methods.



### The Distance Transform and its Computation
- **Arxiv ID**: http://arxiv.org/abs/2106.03503v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03503v2)
- **Published**: 2021-06-07 10:46:26+00:00
- **Updated**: 2023-02-24 11:14:37+00:00
- **Authors**: Tilo Strutz
- **Comment**: 24 pages, 22 figures, 1 table, 9 listings
- **Journal**: None
- **Summary**: Distance transformation is an image processing technique used for many different applications. Related to a binary image, the general idea is to determine the distance of all background points to the nearest object point (or vice versa). In this tutorial, different approaches are explained in detail and compared using examples. Corresponding source code is provided to facilitate own investigations. A particular objective of this tutorial is to clarify the difference between arbitrary distance transforms and exact Euclidean distance transformations.



### Self-supervised Depth Estimation Leveraging Global Perception and Geometric Smoothness Using On-board Videos
- **Arxiv ID**: http://arxiv.org/abs/2106.03505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03505v1)
- **Published**: 2021-06-07 10:53:27+00:00
- **Updated**: 2021-06-07 10:53:27+00:00
- **Authors**: Shaocheng Jia, Xin Pei, Wei Yao, S. C. Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised depth estimation has drawn much attention in recent years as it does not require labeled data but image sequences. Moreover, it can be conveniently used in various applications, such as autonomous driving, robotics, realistic navigation, and smart cities. However, extracting global contextual information from images and predicting a geometrically natural depth map remain challenging. In this paper, we present DLNet for pixel-wise depth estimation, which simultaneously extracts global and local features with the aid of our depth Linformer block. This block consists of the Linformer and innovative soft split multi-layer perceptron blocks. Moreover, a three-dimensional geometry smoothness loss is proposed to predict a geometrically natural depth map by imposing the second-order smoothness constraint on the predicted three-dimensional point clouds, thereby realizing improved performance as a byproduct. Finally, we explore the multi-scale prediction strategy and propose the maximum margin dual-scale prediction strategy for further performance improvement. In experiments on the KITTI and Make3D benchmarks, the proposed DLNet achieves performance competitive to those of the state-of-the-art methods, reducing time and space complexities by more than $62\%$ and $56\%$, respectively. Extensive testing on various real-world situations further demonstrates the strong practicality and generalization capability of the proposed model.



### Multi-Exit Semantic Segmentation Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.03527v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03527v3)
- **Published**: 2021-06-07 11:37:03+00:00
- **Updated**: 2022-07-31 21:10:48+00:00
- **Authors**: Alexandros Kouris, Stylianos I. Venieris, Stefanos Laskaridis, Nicholas D. Lane
- **Comment**: (Extended version) Accepted at ECCV 2022
- **Journal**: None
- **Summary**: Semantic segmentation arises as the backbone of many vision systems, spanning from self-driving cars and robot navigation to augmented reality and teleconferencing. Frequently operating under stringent latency constraints within a limited resource envelope, optimising for efficient execution becomes important. At the same time, the heterogeneous capabilities of the target platforms and the diverse constraints of different applications require the design and training of multiple target-specific segmentation models, leading to excessive maintenance costs. To this end, we propose a framework for converting state-of-the-art segmentation CNNs to Multi-Exit Semantic Segmentation (MESS) networks: specially trained models that employ parametrised early exits along their depth to i) dynamically save computation during inference on easier samples and ii) save training and maintenance cost by offering a post-training customisable speed-accuracy trade-off. Designing and training such networks naively can hurt performance. Thus, we propose a novel two-staged training scheme for multi-exit networks. Furthermore, the parametrisation of MESS enables co-optimising the number, placement and architecture of the attached segmentation heads along with the exit policy, upon deployment via exhaustive search in <1 GPUh. This allows MESS to rapidly adapt to the device capabilities and application requirements for each target use-case, offering a train-once-deploy-everywhere solution. MESS variants achieve latency gains of up to 2.83x with the same accuracy, or 5.33 pp higher accuracy for the same computational budget, compared to the original backbone network. Lastly, MESS delivers orders of magnitude faster architectural customisation, compared to state-of-the-art techniques.



### End-to-end reconstruction meets data-driven regularization for inverse problems
- **Arxiv ID**: http://arxiv.org/abs/2106.03538v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2106.03538v1)
- **Published**: 2021-06-07 12:05:06+00:00
- **Updated**: 2021-06-07 12:05:06+00:00
- **Authors**: Subhadip Mukherjee, Marcello Carioni, Ozan Öktem, Carola-Bibiane Schönlieb
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an unsupervised approach for learning end-to-end reconstruction operators for ill-posed inverse problems. The proposed method combines the classical variational framework with iterative unrolling, which essentially seeks to minimize a weighted combination of the expected distortion in the measurement space and the Wasserstein-1 distance between the distributions of the reconstruction and ground-truth. More specifically, the regularizer in the variational setting is parametrized by a deep neural network and learned simultaneously with the unrolled reconstruction operator. The variational problem is then initialized with the reconstruction of the unrolled operator and solved iteratively till convergence. Notably, it takes significantly fewer iterations to converge, thanks to the excellent initialization obtained via the unrolled operator. The resulting approach combines the computational efficiency of end-to-end unrolled reconstruction with the well-posedness and noise-stability guarantees of the variational setting. Moreover, we demonstrate with the example of X-ray computed tomography (CT) that our approach outperforms state-of-the-art unsupervised methods, and that it outperforms or is on par with state-of-the-art supervised learned reconstruction approaches.



### An Intelligent Hybrid Model for Identity Document Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.04345v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.04345v1)
- **Published**: 2021-06-07 13:08:00+00:00
- **Updated**: 2021-06-07 13:08:00+00:00
- **Authors**: Nouna Khandan
- **Comment**: None
- **Journal**: None
- **Summary**: Digitization, i.e., the process of converting information into a digital format, may provide various opportunities (e.g., increase in productivity, disaster recovery, and environmentally friendly solutions) and challenges for businesses. In this context, one of the main challenges would be to accurately classify numerous scanned documents uploaded every day by customers as usual business processes. For example, processes in banking (e.g., applying for loans) or the Government Registry of BDM (Births, Deaths, and Marriages) applications may involve uploading several documents such as a driver's license and passport. There are not many studies available to address the challenge as an application of image classification. Although some studies are available which used various methods, a more accurate model is still required. The current study has proposed a robust fusion model to define the type of identity documents accurately. The proposed approach is based on two different methods in which images are classified based on their visual features and text features. A novel model based on statistics and regression has been proposed to calculate the confidence level for the feature-based classifier. A fuzzy-mean fusion model has been proposed to combine the classifier results based on their confidence score. The proposed approach has been implemented using Python and experimentally validated on synthetic and real-world datasets. The performance of the proposed model is evaluated using the Receiver Operating Characteristic (ROC) curve analysis.



### Adversarial Attack and Defense in Deep Ranking
- **Arxiv ID**: http://arxiv.org/abs/2106.03614v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03614v1)
- **Published**: 2021-06-07 13:41:45+00:00
- **Updated**: 2021-06-07 13:41:45+00:00
- **Authors**: Mo Zhou, Le Wang, Zhenxing Niu, Qilin Zhang, Nanning Zheng, Gang Hua
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Network classifiers are vulnerable to adversarial attack, where an imperceptible perturbation could result in misclassification. However, the vulnerability of DNN-based image ranking systems remains under-explored. In this paper, we propose two attacks against deep ranking systems, i.e., Candidate Attack and Query Attack, that can raise or lower the rank of chosen candidates by adversarial perturbations. Specifically, the expected ranking order is first represented as a set of inequalities, and then a triplet-like objective function is designed to obtain the optimal perturbation. Conversely, an anti-collapse triplet defense is proposed to improve the ranking model robustness against all proposed attacks, where the model learns to prevent the positive and negative samples being pulled close to each other by adversarial attack. To comprehensively measure the empirical adversarial robustness of a ranking model with our defense, we propose an empirical robustness score, which involves a set of representative attacks against ranking models. Our adversarial ranking attacks and defenses are evaluated on MNIST, Fashion-MNIST, CUB200-2011, CARS196 and Stanford Online Products datasets. Experimental results demonstrate that a typical deep ranking system can be effectively compromised by our attacks. Nevertheless, our defense can significantly improve the ranking system robustness, and simultaneously mitigate a wide range of attacks.



### Document-level Relation Extraction as Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.03618v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03618v2)
- **Published**: 2021-06-07 13:44:44+00:00
- **Updated**: 2021-08-22 16:18:41+00:00
- **Authors**: Ningyu Zhang, Xiang Chen, Xin Xie, Shumin Deng, Chuanqi Tan, Mosha Chen, Fei Huang, Luo Si, Huajun Chen
- **Comment**: Accepted by IJCAI 2021
- **Journal**: None
- **Summary**: Document-level relation extraction aims to extract relations among multiple entity pairs from a document. Previously proposed graph-based or transformer-based models utilize the entities independently, regardless of global information among relational triples. This paper approaches the problem by predicting an entity-level relation matrix to capture local and global information, parallel to the semantic segmentation task in computer vision. Herein, we propose a Document U-shaped Network for document-level relation extraction. Specifically, we leverage an encoder module to capture the context information of entities and a U-shaped segmentation module over the image-style feature map to capture global interdependency among triples. Experimental results show that our approach can obtain state-of-the-art performance on three benchmark datasets DocRED, CDR, and GDA.



### A structured latent space for human body motion generation
- **Arxiv ID**: http://arxiv.org/abs/2106.04387v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04387v4)
- **Published**: 2021-06-07 13:56:46+00:00
- **Updated**: 2022-09-01 13:38:44+00:00
- **Authors**: Mathieu Marsot, Stefanie Wuhrer, Jean-Sebastien Franco, Stephane Durocher
- **Comment**: Published at 3DV 2022
- **Journal**: None
- **Summary**: We propose a framework to learn a structured latent space to represent 4D human body motion, where each latent vector encodes a full motion of the whole 3D human shape. On one hand several data-driven skeletal animation models exist proposing motion spaces of temporally dense motion signals, but based on geometrically sparse kinematic representations. On the other hand many methods exist to build shape spaces of dense 3D geometry, but for static frames. We bring together both concepts, proposing a motion space that is dense both temporally and geometrically. Once trained, our model generates a multi-frame sequence of dense 3D meshes based on a single point in a low-dimensional latent space. This latent space is built to be structured, such that similar motions form clusters. It also embeds variations of duration in the latent vector, allowing semantically close sequences that differ only by temporal unfolding to share similar latent vectors. We demonstrate experimentally the structural properties of our latent space, and show it can be used to generate plausible interpolations between different actions. We also apply our model to 4D human motion completion, showing its promising abilities to learn spatio-temporal features of human motion.



### Efficient Iterative Amortized Inference for Learning Symmetric and Disentangled Multi-Object Representations
- **Arxiv ID**: http://arxiv.org/abs/2106.03630v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03630v1)
- **Published**: 2021-06-07 14:02:49+00:00
- **Updated**: 2021-06-07 14:02:49+00:00
- **Authors**: Patrick Emami, Pan He, Sanjay Ranka, Anand Rangarajan
- **Comment**: Published in ICML'21. Code and data:
  https://github.com/pemami4911/EfficientMORL
- **Journal**: None
- **Summary**: Unsupervised multi-object representation learning depends on inductive biases to guide the discovery of object-centric representations that generalize. However, we observe that methods for learning these representations are either impractical due to long training times and large memory consumption or forego key inductive biases. In this work, we introduce EfficientMORL, an efficient framework for the unsupervised learning of object-centric representations. We show that optimization challenges caused by requiring both symmetry and disentanglement can in fact be addressed by high-cost iterative amortized inference by designing the framework to minimize its dependence on it. We take a two-stage approach to inference: first, a hierarchical variational autoencoder extracts symmetric and disentangled representations through bottom-up inference, and second, a lightweight network refines the representations with top-down feedback. The number of refinement steps taken during training is reduced following a curriculum, so that at test time with zero steps the model achieves 99.1% of the refined decomposition performance. We demonstrate strong object decomposition and disentanglement on the standard multi-object benchmark while achieving nearly an order of magnitude faster training and test time inference over the previous state-of-the-art model.



### Making EfficientNet More Efficient: Exploring Batch-Independent Normalization, Group Convolutions and Reduced Resolution Training
- **Arxiv ID**: http://arxiv.org/abs/2106.03640v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2106.03640v4)
- **Published**: 2021-06-07 14:10:52+00:00
- **Updated**: 2021-08-26 10:23:01+00:00
- **Authors**: Dominic Masters, Antoine Labatie, Zach Eaton-Rosen, Carlo Luschi
- **Comment**: None
- **Journal**: None
- **Summary**: Much recent research has been dedicated to improving the efficiency of training and inference for image classification. This effort has commonly focused on explicitly improving theoretical efficiency, often measured as ImageNet validation accuracy per FLOP. These theoretical savings have, however, proven challenging to achieve in practice, particularly on high-performance training accelerators.   In this work, we focus on improving the practical efficiency of the state-of-the-art EfficientNet models on a new class of accelerator, the Graphcore IPU. We do this by extending this family of models in the following ways: (i) generalising depthwise convolutions to group convolutions; (ii) adding proxy-normalized activations to match batch normalization performance with batch-independent statistics; (iii) reducing compute by lowering the training resolution and inexpensively fine-tuning at higher resolution. We find that these three methods improve the practical efficiency for both training and inference. Code available at https://github.com/graphcore/graphcore-research/tree/main/Making_EfficientNet_More_Efficient .



### Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2106.03650v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03650v1)
- **Published**: 2021-06-07 14:22:07+00:00
- **Updated**: 2021-06-07 14:22:07+00:00
- **Authors**: Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, Bin Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Very recently, Window-based Transformers, which computed self-attention within non-overlapping local windows, demonstrated promising results on image classification, semantic segmentation, and object detection. However, less study has been devoted to the cross-window connection which is the key element to improve the representation ability. In this work, we revisit the spatial shuffle as an efficient way to build connections among windows. As a result, we propose a new vision transformer, named Shuffle Transformer, which is highly efficient and easy to implement by modifying two lines of code. Furthermore, the depth-wise convolution is introduced to complement the spatial shuffle for enhancing neighbor-window connections. The proposed architectures achieve excellent performance on a wide range of visual tasks including image-level classification, object detection, and semantic segmentation. Code will be released for reproduction.



### Recovery Analysis for Plug-and-Play Priors using the Restricted Eigenvalue Condition
- **Arxiv ID**: http://arxiv.org/abs/2106.03668v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2106.03668v2)
- **Published**: 2021-06-07 14:45:38+00:00
- **Updated**: 2021-10-26 16:07:01+00:00
- **Authors**: Jiaming Liu, M. Salman Asif, Brendt Wohlberg, Ulugbek S. Kamilov
- **Comment**: 27 pages, 13 figures
- **Journal**: None
- **Summary**: The plug-and-play priors (PnP) and regularization by denoising (RED) methods have become widely used for solving inverse problems by leveraging pre-trained deep denoisers as image priors. While the empirical imaging performance and the theoretical convergence properties of these algorithms have been widely investigated, their recovery properties have not previously been theoretically analyzed. We address this gap by showing how to establish theoretical recovery guarantees for PnP/RED by assuming that the solution of these methods lies near the fixed-points of a deep neural network. We also present numerical results comparing the recovery performance of PnP/RED in compressive sensing against that of recent compressive sensing algorithms based on generative models. Our numerical results suggest that PnP with a pre-trained artifact removal network provides significantly better results compared to the existing state-of-the-art methods.



### Open source disease analysis system of cactus by artificial intelligence and image processing
- **Arxiv ID**: http://arxiv.org/abs/2106.03669v1
- **DOI**: 10.1145/3468784.3469075
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.03669v1)
- **Published**: 2021-06-07 14:46:23+00:00
- **Updated**: 2021-06-07 14:46:23+00:00
- **Authors**: Kanlayanee Kaweesinsakul, Siranee Nuchitprasitchai, Joshua M. Pearce
- **Comment**: Preprint for IAIT2021
- **Journal**: None
- **Summary**: There is a growing interest in cactus cultivation because of numerous cacti uses from houseplants to food and medicinal applications. Various diseases impact the growth of cacti. To develop an automated model for the analysis of cactus disease and to be able to quickly treat and prevent damage to the cactus. The Faster R-CNN and YOLO algorithm technique were used to analyze cactus diseases automatically distributed into six groups: 1) anthracnose, 2) canker, 3) lack of care, 4) aphid, 5) rusts and 6) normal group. Based on the experimental results the YOLOv5 algorithm was found to be more effective at detecting and identifying cactus disease than the Faster R-CNN algorithm. Data training and testing with YOLOv5S model resulted in a precision of 89.7% and an accuracy (recall) of 98.5%, which is effective enough for further use in a number of applications in cactus cultivation. Overall the YOLOv5 algorithm had a test time per image of only 26 milliseconds. Therefore, the YOLOv5 algorithm was found to suitable for mobile applications and this model could be further developed into a program for analyzing cactus disease.



### Towards a Multi-purpose Robotic Nursing Assistant
- **Arxiv ID**: http://arxiv.org/abs/2106.03683v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2106.03683v1)
- **Published**: 2021-06-07 15:00:12+00:00
- **Updated**: 2021-06-07 15:00:12+00:00
- **Authors**: Krishna Chaitanya Kodur, Kaustubh Rajpathak, Akilesh Rajavenkatanarayanan, Maria Kyrarini, Fillia Makedon
- **Comment**: accepted at ICRA 2021 Workshop on No-Touch Care for Worker Safety
  During Pandemic Response
- **Journal**: None
- **Summary**: Robotic nursing aid is one of the heavily researched areas in robotics nowadays. Several robotic assistants exist that only focus on a specific function related to nurses assistance or functions related to patient aid. There is a need for a unified system that not only performs tasks that would assist nurses and reduce their burden but also perform tasks that help a patient. In recent times, due to the COVID-19 pandemic, there is also an increase in the need for robotic assistants that have teleoperation capabilities to provide better protection against the virus spread. To address these requirements, we propose a novel Multi-purpose Intelligent Nurse Aid (MINA) robotic system that is capable of providing walking assistance to the patients and perform teleoperation tasks with an easy-to-use and intuitive Graphical User Interface (GUI). This paper also presents preliminary results from the walking assistant task that improves upon the current state-of-the-art methods and shows the developed GUI for teleoperation.



### Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing
- **Arxiv ID**: http://arxiv.org/abs/2106.03686v3
- **DOI**: 10.3390/s22083065
- **Categories**: **eess.SP**, cs.CV, cs.IT, cs.LG, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2106.03686v3)
- **Published**: 2021-06-07 15:00:33+00:00
- **Updated**: 2021-12-17 22:06:48+00:00
- **Authors**: Udaya S. K. P. Miriya Thanthrige, Peter Jung, Aydin Sezgin
- **Comment**: None
- **Journal**: Sensors 2022,
- **Summary**: We address the detection of material defects, which are inside a layered material structure using compressive sensing based multiple-input and multiple-output (MIMO) wireless radar. Here, the strong clutter due to the reflection of the layered structure's surface often makes the detection of the defects challenging. Thus, sophisticated signal separation methods are required for improved defect detection. In many scenarios, the number of defects that we are interested in is limited and the signaling response of the layered structure can be modeled as a low-rank structure. Therefore, we propose joint rank and sparsity minimization for defect detection. In particular, we propose a non-convex approach based on the iteratively reweighted nuclear and $\ell_1-$norm (a double-reweighted approach) to obtain a higher accuracy compared to the conventional nuclear norm and $\ell_1-$norm minimization. To this end, an iterative algorithm is designed to estimate the low-rank and sparse contributions. Further, we propose deep learning to learn the parameters of the algorithm (i.e., algorithm unfolding) to improve the accuracy and the speed of convergence of the algorithm. Our numerical results show that the proposed approach outperforms the conventional approaches in terms of mean square errors of the recovered low-rank and sparse components and the speed of convergence.



### Deep Learning 3D Dose Prediction for Conventional Lung IMRT Using Consistent/Unbiased Automated Plans
- **Arxiv ID**: http://arxiv.org/abs/2106.03705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03705v1)
- **Published**: 2021-06-07 15:15:05+00:00
- **Updated**: 2021-06-07 15:15:05+00:00
- **Authors**: Navdeep Dahiya, Gourav Jhanwar, Anthony Yezzi, Masoud Zarepisheh, Saad Nadeem
- **Comment**: 16 pages, 4 figures
- **Journal**: None
- **Summary**: Deep learning (DL) 3D dose prediction has recently gained a lot of attention. However, the variability of plan quality in the training dataset, generated manually by planners with wide range of expertise, can dramatically effect the quality of the final predictions. Moreover, any changes in the clinical criteria requires a new set of manually generated plans by planners to build a new prediction model. In this work, we instead use consistent plans generated by our in-house automated planning system (named ``ECHO'') to train the DL model. ECHO (expedited constrained hierarchical optimization) generates consistent/unbiased plans by solving large-scale constrained optimization problems sequentially. If the clinical criteria changes, a new training data set can be easily generated offline using ECHO, with no or limited human intervention, making the DL-based prediction model easily adaptable to the changes in the clinical practice. We used 120 conventional lung patients (100 for training, 20 for testing) with different beam configurations and trained our DL-model using manually-generated as well as automated ECHO plans. We evaluated different inputs: (1) CT+(PTV/OAR)contours, and (2) CT+contours+beam configurations, and different loss functions: (1) MAE (mean absolute error), and (2) MAE+DVH (dose volume histograms). The quality of the predictions was compared using different DVH metrics as well as dose-score and DVH-score, recently introduced by the AAPM knowledge-based planning grand challenge. The best results were obtained using automated ECHO plans and CT+contours+beam as training inputs and MAE+DVH as loss function.



### Refiner: Refining Self-attention for Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2106.03714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03714v1)
- **Published**: 2021-06-07 15:24:54+00:00
- **Updated**: 2021-06-07 15:24:54+00:00
- **Authors**: Daquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu, Zihang Jiang, Yuan Li, Xiaojie Jin, Qibin Hou, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Vision Transformers (ViTs) have shown competitive accuracy in image classification tasks compared with CNNs. Yet, they generally require much more data for model pre-training. Most of recent works thus are dedicated to designing more complex architectures or training methods to address the data-efficiency issue of ViTs. However, few of them explore improving the self-attention mechanism, a key factor distinguishing ViTs from CNNs. Different from existing works, we introduce a conceptually simple scheme, called refiner, to directly refine the self-attention maps of ViTs. Specifically, refiner explores attention expansion that projects the multi-head attention maps to a higher-dimensional space to promote their diversity. Further, refiner applies convolutions to augment local patterns of the attention maps, which we show is equivalent to a distributed local attention features are aggregated locally with learnable kernels and then globally aggregated with self-attention. Extensive experiments demonstrate that refiner works surprisingly well. Significantly, it enables ViTs to achieve 86% top-1 classification accuracy on ImageNet with only 81M parameters.



### Incremental False Negative Detection for Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.03719v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03719v6)
- **Published**: 2021-06-07 15:29:14+00:00
- **Updated**: 2022-03-16 07:51:00+00:00
- **Authors**: Tsai-Shien Chen, Wei-Chih Hung, Hung-Yu Tseng, Shao-Yi Chien, Ming-Hsuan Yang
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: Self-supervised learning has recently shown great potential in vision tasks through contrastive learning, which aims to discriminate each image, or instance, in the dataset. However, such instance-level learning ignores the semantic relationship among instances and sometimes undesirably repels the anchor from the semantically similar samples, termed as "false negatives". In this work, we show that the unfavorable effect from false negatives is more significant for the large-scale datasets with more semantic concepts. To address the issue, we propose a novel self-supervised contrastive learning framework that incrementally detects and explicitly removes the false negative samples. Specifically, following the training process, our method dynamically detects increasing high-quality false negatives considering that the encoder gradually improves and the embedding space becomes more semantically structural. Next, we discuss two strategies to explicitly remove the detected false negatives during contrastive learning. Extensive experiments show that our framework outperforms other self-supervised contrastive learning methods on multiple benchmarks in a limited resource setup.



### Person Re-Identification with a Locally Aware Transformer
- **Arxiv ID**: http://arxiv.org/abs/2106.03720v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03720v2)
- **Published**: 2021-06-07 15:31:19+00:00
- **Updated**: 2021-06-08 17:59:48+00:00
- **Authors**: Charu Sharma, Siddhant R. Kapil, David Chapman
- **Comment**: 10 pages, 2 figure, submitted to NeurIPS 2021
- **Journal**: None
- **Summary**: Person Re-Identification is an important problem in computer vision-based surveillance applications, in which the same person is attempted to be identified from surveillance photographs in a variety of nearby zones. At present, the majority of Person re-ID techniques are based on Convolutional Neural Networks (CNNs), but Vision Transformers are beginning to displace pure CNNs for a variety of object recognition tasks. The primary output of a vision transformer is a global classification token, but vision transformers also yield local tokens which contain additional information about local regions of the image. Techniques to make use of these local tokens to improve classification accuracy are an active area of research. We propose a novel Locally Aware Transformer (LA-Transformer) that employs a Parts-based Convolution Baseline (PCB)-inspired strategy for aggregating globally enhanced local classification tokens into an ensemble of $\sqrt{N}$ classifiers, where $N$ is the number of patches. An additional novelty is that we incorporate blockwise fine-tuning which further improves re-ID accuracy. LA-Transformer with blockwise fine-tuning achieves rank-1 accuracy of $98.27 \%$ with standard deviation of $0.13$ on the Market-1501 and $98.7\%$ with standard deviation of $0.2$ on the CUHK03 dataset respectively, outperforming all other state-of-the-art published methods at the time of writing.



### Deep Neural Network-based Enhancement for Image and Video Streaming Systems: A Survey and Future Directions
- **Arxiv ID**: http://arxiv.org/abs/2106.03727v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2106.03727v1)
- **Published**: 2021-06-07 15:42:36+00:00
- **Updated**: 2021-06-07 15:42:36+00:00
- **Authors**: Royson Lee, Stylianos I. Venieris, Nicholas D. Lane
- **Comment**: Accepted for publication at the ACM Computing Surveys (CSUR) journal,
  2021. arXiv admin note: text overlap with arXiv:2010.05838
- **Journal**: None
- **Summary**: Internet-enabled smartphones and ultra-wide displays are transforming a variety of visual apps spanning from on-demand movies and 360{\deg} videos to video-conferencing and live streaming. However, robustly delivering visual content under fluctuating networking conditions on devices of diverse capabilities remains an open problem. In recent years, advances in the field of deep learning on tasks such as super-resolution and image enhancement have led to unprecedented performance in generating high-quality images from low-quality ones, a process we refer to as neural enhancement. In this paper, we survey state-of-the-art content delivery systems that employ neural enhancement as a key component in achieving both fast response time and high visual quality. We first present the components and architecture of existing content delivery systems, highlighting their challenges and motivating the use of neural enhancement models as a countermeasure. We then cover the deployment challenges of these models and analyze existing systems and their design decisions in efficiently overcoming these technical challenges. Additionally, we underline the key trends and common approaches across systems that target diverse use-cases. Finally, we present promising future directions based on the latest insights from deep learning research to further boost the quality of experience of content delivery systems.



### Reveal of Vision Transformers Robustness against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2106.03734v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03734v2)
- **Published**: 2021-06-07 15:59:49+00:00
- **Updated**: 2021-09-20 11:48:45+00:00
- **Authors**: Ahmed Aldahdooh, Wassim Hamidouche, Olivier Deforges
- **Comment**: None
- **Journal**: None
- **Summary**: The major part of the vanilla vision transformer (ViT) is the attention block that brings the power of mimicking the global context of the input image. For better performance, ViT needs large-scale training data. To overcome this data hunger limitation, many ViT-based networks, or hybrid-ViT, have been proposed to include local context during the training. The robustness of ViTs and its variants against adversarial attacks has not been widely investigated in the literature like CNNs. This work studies the robustness of ViT variants 1) against different Lp-based adversarial attacks in comparison with CNNs, 2) under adversarial examples (AEs) after applying preprocessing defense methods and 3) under the adaptive attacks using expectation over transformation (EOT) framework. To that end, we run a set of experiments on 1000 images from ImageNet-1k and then provide an analysis that reveals that vanilla ViT or hybrid-ViT are more robust than CNNs. For instance, we found that 1) Vanilla ViTs or hybrid-ViTs are more robust than CNNs under Lp-based attacks and under adaptive attacks. 2) Unlike hybrid-ViTs, Vanilla ViTs are not responding to preprocessing defenses that mainly reduce the high frequency components. Furthermore, feature maps, attention maps, and Grad-CAM visualization jointly with image quality measures, and perturbations' energy spectrum are provided for an insight understanding of attention-based models.



### Unsupervised Action Segmentation for Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/2106.03738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03738v1)
- **Published**: 2021-06-07 16:02:06+00:00
- **Updated**: 2021-06-07 16:02:06+00:00
- **Authors**: AJ Piergiovanni, Anelia Angelova, Michael S. Ryoo, Irfan Essa
- **Comment**: 4 page abstract for LUV workshop
- **Journal**: None
- **Summary**: In this paper we address the problem of automatically discovering atomic actions in unsupervised manner from instructional videos, which are rarely annotated with atomic actions. We present an unsupervised approach to learn atomic actions of structured human tasks from a variety of instructional videos based on a sequential stochastic autoregressive model for temporal segmentation of videos. This learns to represent and discover the sequential relationship between different atomic actions of the task, and which provides automatic and unsupervised self-labeling.



### Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence
- **Arxiv ID**: http://arxiv.org/abs/2106.03743v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2106.03743v6)
- **Published**: 2021-06-07 16:08:48+00:00
- **Updated**: 2022-04-03 15:42:50+00:00
- **Authors**: Antoine Labatie, Dominic Masters, Zach Eaton-Rosen, Carlo Luschi
- **Comment**: NeurIPS 2021 camera-ready
- **Journal**: None
- **Summary**: We investigate the reasons for the performance degradation incurred with batch-independent normalization. We find that the prototypical techniques of layer normalization and instance normalization both induce the appearance of failure modes in the neural network's pre-activations: (i) layer normalization induces a collapse towards channel-wise constant functions; (ii) instance normalization induces a lack of variability in instance statistics, symptomatic of an alteration of the expressivity. To alleviate failure mode (i) without aggravating failure mode (ii), we introduce the technique "Proxy Normalization" that normalizes post-activations using a proxy distribution. When combined with layer normalization or group normalization, this batch-independent normalization emulates batch normalization's behavior and consistently matches or exceeds its performance.



### Efficient Training of Visual Transformers with Small Datasets
- **Arxiv ID**: http://arxiv.org/abs/2106.03746v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03746v2)
- **Published**: 2021-06-07 16:14:06+00:00
- **Updated**: 2021-11-14 21:16:01+00:00
- **Authors**: Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno Lepri, Marco De Nadai
- **Comment**: None
- **Journal**: Proceedings of the 35th Conference on Neural Information
  Processing Systems (NeurIPS) 2021
- **Summary**: Visual Transformers (VTs) are emerging as an architectural paradigm alternative to Convolutional networks (CNNs). Differently from CNNs, VTs can capture global relations between image elements and they potentially have a larger representation capacity. However, the lack of the typical convolutional inductive bias makes these models more data-hungry than common CNNs. In fact, some local properties of the visual domain which are embedded in the CNN architectural design, in VTs should be learned from samples. In this paper, we empirically analyse different VTs, comparing their robustness in a small training-set regime, and we show that, despite having a comparable accuracy when trained on ImageNet, their performance on smaller datasets can be largely different. Moreover, we propose a self-supervised task which can extract additional information from images with only a negligible computational overhead. This task encourages the VTs to learn spatial relations within an image and makes the VT training much more robust when training data are scarce. Our task is used jointly with the standard (supervised) training and it does not depend on specific architectural choices, thus it can be easily plugged in the existing VTs. Using an extensive evaluation with different VTs and datasets, we show that our method can improve (sometimes dramatically) the final accuracy of the VTs. Our code is available at: https://github.com/yhlleo/VTs-Drloc.



### HERS Superpixels: Deep Affinity Learning for Hierarchical Entropy Rate Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.03755v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP, stat.ML, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2106.03755v2)
- **Published**: 2021-06-07 16:20:04+00:00
- **Updated**: 2021-11-18 18:13:45+00:00
- **Authors**: Hankui Peng, Angelica I. Aviles-Rivero, Carola-Bibiane Schönlieb
- **Comment**: None
- **Journal**: None
- **Summary**: Superpixels serve as a powerful preprocessing tool in numerous computer vision tasks. By using superpixel representation, the number of image primitives can be largely reduced by orders of magnitudes. With the rise of deep learning in recent years, a few works have attempted to feed deeply learned features / graphs into existing classical superpixel techniques. However, none of them are able to produce superpixels in near real-time, which is crucial to the applicability of superpixels in practice. In this work, we propose a two-stage graph-based framework for superpixel segmentation. In the first stage, we introduce an efficient Deep Affinity Learning (DAL) network that learns pairwise pixel affinities by aggregating multi-scale information. In the second stage, we propose a highly efficient superpixel method called Hierarchical Entropy Rate Segmentation (HERS). Using the learned affinities from the first stage, HERS builds a hierarchical tree structure that can produce any number of highly adaptive superpixels instantaneously. We demonstrate, through visual and numerical experiments, the effectiveness and efficiency of our method compared to various state-of-the-art superpixel methods.



### FairCal: Fairness Calibration for Face Verification
- **Arxiv ID**: http://arxiv.org/abs/2106.03761v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2106.03761v4)
- **Published**: 2021-06-07 16:26:26+00:00
- **Updated**: 2022-03-30 17:41:19+00:00
- **Authors**: Tiago Salvador, Stephanie Cairns, Vikram Voleti, Noah Marshall, Adam Oberman
- **Comment**: Accepted at ICLR 2022
- **Journal**: None
- **Summary**: Despite being widely used, face recognition models suffer from bias: the probability of a false positive (incorrect face match) strongly depends on sensitive attributes such as the ethnicity of the face. As a result, these models can disproportionately and negatively impact minority groups, particularly when used by law enforcement. The majority of bias reduction methods have several drawbacks: they use an end-to-end retraining approach, may not be feasible due to privacy issues, and often reduce accuracy. An alternative approach is post-processing methods that build fairer decision classifiers using the features of pre-trained models, thus avoiding the cost of retraining. However, they still have drawbacks: they reduce accuracy (AGENDA, PASS, FTC), or require retuning for different false positive rates (FSN). In this work, we introduce the Fairness Calibration (FairCal) method, a post-training approach that simultaneously: (i) increases model accuracy (improving the state-of-the-art), (ii) produces fairly-calibrated probabilities, (iii) significantly reduces the gap in the false positive rates, (iv) does not require knowledge of the sensitive attribute, and (v) does not require retraining, training an additional model, or retuning. We apply it to the task of Face Verification, and obtain state-of-the-art results with all the above advantages.



### Few-Shot Unsupervised Image-to-Image Translation on complex scenes
- **Arxiv ID**: http://arxiv.org/abs/2106.03770v1
- **DOI**: 10.11648/j.ajcst.20210404.12
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03770v1)
- **Published**: 2021-06-07 16:33:19+00:00
- **Updated**: 2021-06-07 16:33:19+00:00
- **Authors**: Luca Barras, Samuel Chassot, Daniel Filipe Nunes Silva
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation methods have received a lot of attention in the last few years. Multiple techniques emerged tackling the initial challenge from different perspectives. Some focus on learning as much as possible from several target style images for translations while other make use of object detection in order to produce more realistic results on content-rich scenes. In this work, we assess how a method that has initially been developed for single object translation performs on more diverse and content-rich images. Our work is based on the FUNIT[1] framework and we train it with a more diverse dataset. This helps understanding how such method behaves beyond their initial frame of application. We present a way to extend a dataset based on object detection. Moreover, we propose a way to adapt the FUNIT framework in order to leverage the power of object detection that one can see in other methods.



### Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2106.03772v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03772v1)
- **Published**: 2021-06-07 16:36:50+00:00
- **Updated**: 2021-06-07 16:36:50+00:00
- **Authors**: Yiding Yang, Zhou Ren, Haoxiang Li, Chunluan Zhou, Xinchao Wang, Gang Hua
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Multi-person pose estimation and tracking serve as crucial steps for video understanding. Most state-of-the-art approaches rely on first estimating poses in each frame and only then implementing data association and refinement. Despite the promising results achieved, such a strategy is inevitably prone to missed detections especially in heavily-cluttered scenes, since this tracking-by-detection paradigm is, by nature, largely dependent on visual evidences that are absent in the case of occlusion. In this paper, we propose a novel online approach to learning the pose dynamics, which are independent of pose detections in current fame, and hence may serve as a robust estimation even in challenging scenarios including occlusion. Specifically, we derive this prediction of dynamics through a graph neural network~(GNN) that explicitly accounts for both spatial-temporal and visual information. It takes as input the historical pose tracklets and directly predicts the corresponding poses in the following frame for each tracklet. The predicted poses will then be aggregated with the detected poses, if any, at the same frame so as to produce the final pose, potentially recovering the occluded joints missed by the estimator. Experiments on PoseTrack 2017 and PoseTrack 2018 datasets demonstrate that the proposed method achieves results superior to the state of the art on both human pose estimation and tracking tasks.



### Digital Taxonomist: Identifying Plant Species in Community Scientists' Photographs
- **Arxiv ID**: http://arxiv.org/abs/2106.03774v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03774v2)
- **Published**: 2021-06-07 16:38:02+00:00
- **Updated**: 2021-10-05 16:44:55+00:00
- **Authors**: Riccardo de Lutio, Yihang She, Stefano D'Aronco, Stefania Russo, Philipp Brun, Jan D. Wegner, Konrad Schindler
- **Comment**: Accepted for publication in the ISPRS Journal of Photogrammetry and
  Remote Sensing
- **Journal**: None
- **Summary**: Automatic identification of plant specimens from amateur photographs could improve species range maps, thus supporting ecosystems research as well as conservation efforts. However, classifying plant specimens based on image data alone is challenging: some species exhibit large variations in visual appearance, while at the same time different species are often visually similar; additionally, species observations follow a highly imbalanced, long-tailed distribution due to differences in abundance as well as observer biases. On the other hand, most species observations are accompanied by side information about the spatial, temporal and ecological context. Moreover, biological species are not an unordered list of classes but embedded in a hierarchical taxonomic structure. We propose a multimodal deep learning model that takes into account these additional cues in a unified framework. Our Digital Taxonomist is able to identify plant species in photographs better than a classifier trained on the image content alone, the performance gained is over 6 percent points in terms of accuracy.



### CDN-MEDAL: Two-stage Density and Difference Approximation Framework for Motion Analysis
- **Arxiv ID**: http://arxiv.org/abs/2106.03776v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03776v4)
- **Published**: 2021-06-07 16:39:42+00:00
- **Updated**: 2021-09-21 11:49:47+00:00
- **Authors**: Synh Viet-Uyen Ha, Cuong Tien Nguyen, Hung Ngoc Phan, Nhat Minh Chung, Phuong Hoai Ha
- **Comment**: 13 pages, 5 figures, to be submitted to IEEE TMM
- **Journal**: None
- **Summary**: Background modeling and subtraction is a promising research area with a variety of applications for video surveillance. Recent years have witnessed a proliferation of effective learning-based deep neural networks in this area. However, the techniques have only provided limited descriptions of scenes' properties while requiring heavy computations, as their single-valued mapping functions are learned to approximate the temporal conditional averages of observed target backgrounds and foregrounds. On the other hand, statistical learning in imagery domains has been a prevalent approach with high adaptation to dynamic context transformation, notably using Gaussian Mixture Models (GMM) with its generalization capabilities. By leveraging both, we propose a novel method called CDN-MEDAL-net for background modeling and subtraction with two convolutional neural networks. The first architecture, CDN-GM, is grounded on an unsupervised GMM statistical learning strategy to describe observed scenes' salient features. The second one, MEDAL-net, implements a light-weighted pipeline of online video background subtraction. Our two-stage architecture is small, but it is very effective with rapid convergence to representations of intricate motion patterns. Our experiments show that the proposed approach is not only capable of effectively extracting regions of moving objects in unseen cases, but it is also very efficient.



### Pointwise visual field estimation from optical coherence tomography in glaucoma: a structure-function analysis using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2106.03793v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03793v1)
- **Published**: 2021-06-07 16:58:38+00:00
- **Updated**: 2021-06-07 16:58:38+00:00
- **Authors**: Ruben Hemelings, Bart Elen, João Barbosa Breda, Erwin Bellon, Matthew B Blaschko, Patrick De Boever, Ingeborg Stalmans
- **Comment**: None
- **Journal**: None
- **Summary**: Background/Aims: Standard Automated Perimetry (SAP) is the gold standard to monitor visual field (VF) loss in glaucoma management, but is prone to intra-subject variability. We developed and validated a deep learning (DL) regression model that estimates pointwise and overall VF loss from unsegmented optical coherence tomography (OCT) scans. Methods: Eight DL regression models were trained with various retinal imaging modalities: circumpapillary OCT at 3.5mm, 4.1mm, 4.7mm diameter, and scanning laser ophthalmoscopy (SLO) en face images to estimate mean deviation (MD) and 52 threshold values. This retrospective study used data from patients who underwent a complete glaucoma examination, including a reliable Humphrey Field Analyzer (HFA) 24-2 SITA Standard VF exam and a SPECTRALIS OCT scan using the Glaucoma Module Premium Edition. Results: A total of 1378 matched OCT-VF pairs of 496 patients (863 eyes) were included for training and evaluation of the DL models. Average sample MD was -7.53dB (from -33.8dB to +2.0dB). For 52 VF threshold values estimation, the circumpapillary OCT scan with the largest radius (4.7mm) achieved the best performance among all individual models (Pearson r=0.77, 95% CI=[0.72-0.82]). For MD, prediction averaging of OCT-trained models (3.5mm, 4.1mm, 4.7mm) resulted in a Pearson r of 0.78 [0.73-0.83] on the validation set and comparable performance on the test set (Pearson r=0.79 [0.75-0.82]). Conclusion: DL on unsegmented OCT scans accurately predicts pointwise and mean deviation of 24-2 VF in glaucoma patients. Automated VF from unsegmented OCT could be a solution for patients unable to produce reliable perimetry results.



### DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Reconstruction and Rendering
- **Arxiv ID**: http://arxiv.org/abs/2106.03798v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03798v4)
- **Published**: 2021-06-07 17:08:17+00:00
- **Updated**: 2022-03-27 18:14:48+00:00
- **Authors**: Ruizhi Shao, Hongwen Zhang, He Zhang, Mingjia Chen, Yanpei Cao, Tao Yu, Yebin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce DoubleField, a novel framework combining the merits of both surface field and radiance field for high-fidelity human reconstruction and rendering. Within DoubleField, the surface field and radiance field are associated together by a shared feature embedding and a surface-guided sampling strategy. Moreover, a view-to-view transformer is introduced to fuse multi-view features and learn view-dependent features directly from high-resolution inputs. With the modeling power of DoubleField and the view-to-view transformer, our method significantly improves the reconstruction quality of both geometry and appearance, while supporting direct inference, scene-specific high-resolution finetuning, and fast rendering. The efficacy of DoubleField is validated by the quantitative evaluations on several datasets and the qualitative results in a real-world sparse multi-view system, showing its superior capability for high-quality human model reconstruction and photo-realistic free-viewpoint human rendering. Data and source code will be made public for the research purpose. Please refer to our project page: http://www.liuyebin.com/dbfield/dbfield.html.



### Visual Transformer for Task-aware Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.03801v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.03801v1)
- **Published**: 2021-06-07 17:13:59+00:00
- **Updated**: 2021-06-07 17:13:59+00:00
- **Authors**: Razvan Caramalau, Binod Bhattarai, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Pool-based sampling in active learning (AL) represents a key framework for an-notating informative data when dealing with deep learning models. In this paper, we present a novel pipeline for pool-based Active Learning. Unlike most previous works, our method exploits accessible unlabelled examples during training to estimate their co-relation with the labelled examples. Another contribution of this paper is to adapt Visual Transformer as a sampler in the AL pipeline. Visual Transformer models non-local visual concept dependency between labelled and unlabelled examples, which is crucial to identifying the influencing unlabelled examples. Also, compared to existing methods where the learner and the sampler are trained in a multi-stage manner, we propose to train them in a task-aware jointly manner which enables transforming the latent space into two separate tasks: one that classifies the labelled examples; the other that distinguishes the labelling direction. We evaluated our work on four different challenging benchmarks of classification and detection tasks viz. CIFAR10, CIFAR100,FashionMNIST, RaFD, and Pascal VOC 2007. Our extensive empirical and qualitative evaluations demonstrate the superiority of our method compared to the existing methods. Code available: https://github.com/razvancaramalau/Visual-Transformer-for-Task-aware-Active-Learning



### Deep Medial Fields
- **Arxiv ID**: http://arxiv.org/abs/2106.03804v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03804v1)
- **Published**: 2021-06-07 17:15:38+00:00
- **Updated**: 2021-06-07 17:15:38+00:00
- **Authors**: Daniel Rebain, Ke Li, Vincent Sitzmann, Soroosh Yazdani, Kwang Moo Yi, Andrea Tagliasacchi
- **Comment**: None
- **Journal**: None
- **Summary**: Implicit representations of geometry, such as occupancy fields or signed distance fields (SDF), have recently re-gained popularity in encoding 3D solid shape in a functional form. In this work, we introduce medial fields: a field function derived from the medial axis transform (MAT) that makes available information about the underlying 3D geometry that is immediately useful for a number of downstream tasks. In particular, the medial field encodes the local thickness of a 3D shape, and enables O(1) projection of a query point onto the medial axis. To construct the medial field we require nothing but the SDF of the shape itself, thus allowing its straightforward incorporation in any application that relies on signed distance fields. Working in unison with the O(1) surface projection supported by the SDF, the medial field opens the door for an entirely new set of efficient, shape-aware operations on implicit representations. We present three such applications, including a modification to sphere tracing that renders implicit representations with better convergence properties, a fast construction method for memory-efficient rigid-body collision proxies, and an efficient approximation of ambient occlusion that remains stable with respect to viewpoint variations.



### 3DB: A Framework for Debugging Computer Vision Models
- **Arxiv ID**: http://arxiv.org/abs/2106.03805v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2106.03805v1)
- **Published**: 2021-06-07 17:16:12+00:00
- **Updated**: 2021-06-07 17:16:12+00:00
- **Authors**: Guillaume Leclerc, Hadi Salman, Andrew Ilyas, Sai Vemprala, Logan Engstrom, Vibhav Vineet, Kai Xiao, Pengchuan Zhang, Shibani Santurkar, Greg Yang, Ashish Kapoor, Aleksander Madry
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce 3DB: an extendable, unified framework for testing and debugging vision models using photorealistic simulation. We demonstrate, through a wide range of use cases, that 3DB allows users to discover vulnerabilities in computer vision systems and gain insights into how models make decisions. 3DB captures and generalizes many robustness analyses from prior work, and enables one to study their interplay. Finally, we find that the insights generated by the system transfer to the physical world.   We are releasing 3DB as a library (https://github.com/3db/3db) alongside a set of example analyses, guides, and documentation: https://3db.github.io/3db/ .



### High Resolution Solar Image Generation using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.03814v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03814v1)
- **Published**: 2021-06-07 17:24:33+00:00
- **Updated**: 2021-06-07 17:24:33+00:00
- **Authors**: Ankan Dash, Junyi Ye, Guiling Wang
- **Comment**: 10 pages, 3 figures
- **Journal**: None
- **Summary**: We applied Deep Learning algorithm known as Generative Adversarial Networks (GANs) to perform solar image-to-image translation. That is, from Solar Dynamics Observatory (SDO)/Helioseismic and Magnetic Imager(HMI) line of sight magnetogram images to SDO/Atmospheric Imaging Assembly(AIA) 0304-{\AA} images. The Ultraviolet(UV)/Extreme Ultraviolet(EUV) observations like the SDO/AIA0304-{\AA} images were only made available to scientists in the late 1990s even though the magenetic field observations like the SDO/HMI have been available since the 1970s. Therefore by leveraging Deep Learning algorithms like GANs we can give scientists access to complete datasets for analysis. For generating high resolution solar images we use the Pix2PixHD and Pix2Pix algorithms. The Pix2PixHD algorithm was specifically designed for high resolution image generation tasks, and the Pix2Pix algorithm is by far the most widely used image to image translation algorithm. For training and testing we used the data for the year 2012, 2013 and 2014. The results show that our deep learning models are capable of generating high resolution(1024 x 1024 pixels) AIA0304 images from HMI magnetograms. Specifically, the pixel-to-pixel Pearson Correlation Coefficient of the images generated by Pix2PixHD and original images is as high as 0.99. The number is 0.962 if Pix2Pix is used to generate images. The results we get for our Pix2PixHD model is better than the results obtained by previous works done by others to generate AIA0304 images. Thus, we can use these models to generate AIA0304 images when the AIA0304 data is not available which can be used for understanding space weather and giving researchers the capability to predict solar events such as Solar Flares and Coronal Mass Ejections. As far as we know, our work is the first attempt to leverage Pix2PixHD algorithm for SDO/HMI to SDO/AIA0304 image-to-image translation.



### Active Speaker Detection as a Multi-Objective Optimization with Uncertainty-based Multimodal Fusion
- **Arxiv ID**: http://arxiv.org/abs/2106.03821v2
- **DOI**: 10.21437/Interspeech.2021-80
- **Categories**: **cs.SD**, cs.CL, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2106.03821v2)
- **Published**: 2021-06-07 17:38:55+00:00
- **Updated**: 2021-09-15 12:32:37+00:00
- **Authors**: Baptiste Pouthier, Laurent Pilati, Leela K. Gudupudi, Charles Bouveyron, Frederic Precioso
- **Comment**: In INTERSPEECH 2021
- **Journal**: Proc. Interspeech 2021, 2381-2385
- **Summary**: It is now well established from a variety of studies that there is a significant benefit from combining video and audio data in detecting active speakers. However, either of the modalities can potentially mislead audiovisual fusion by inducing unreliable or deceptive information. This paper outlines active speaker detection as a multi-objective learning problem to leverage best of each modalities using a novel self-attention, uncertainty-based multimodal fusion scheme. Results obtained show that the proposed multi-objective learning architecture outperforms traditional approaches in improving both mAP and AUC scores. We further demonstrate that our fusion strategy surpasses, in active speaker detection, other modality fusion methods reported in various disciplines. We finally show that the proposed method significantly improves the state-of-the-art on the AVA-ActiveSpeaker dataset.



### NTIRE 2021 Challenge on Burst Super-Resolution: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2106.03839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03839v1)
- **Published**: 2021-06-07 17:55:28+00:00
- **Updated**: 2021-06-07 17:55:28+00:00
- **Authors**: Goutam Bhat, Martin Danelljan, Radu Timofte, Kazutoshi Akita, Wooyeong Cho, Haoqiang Fan, Lanpeng Jia, Daeshik Kim, Bruno Lecouat, Youwei Li, Shuaicheng Liu, Ziluan Liu, Ziwei Luo, Takahiro Maeda, Julien Mairal, Christian Micheloni, Xuan Mo, Takeru Oba, Pavel Ostyakov, Jean Ponce, Sanghyeok Son, Jian Sun, Norimichi Ukita, Rao Muhammad Umer, Youliang Yan, Lei Yu, Magauiya Zhussip, Xueyi Zou
- **Comment**: NTIRE 2021 Burst Super-Resolution challenge report
- **Journal**: None
- **Summary**: This paper reviews the NTIRE2021 challenge on burst super-resolution. Given a RAW noisy burst as input, the task in the challenge was to generate a clean RGB image with 4 times higher resolution. The challenge contained two tracks; Track 1 evaluating on synthetically generated data, and Track 2 using real-world bursts from mobile camera. In the final testing phase, 6 teams submitted results using a diverse set of solutions. The top-performing methods set a new state-of-the-art for the burst super-resolution task.



### Mean-Shifted Contrastive Loss for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.03844v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03844v2)
- **Published**: 2021-06-07 17:58:03+00:00
- **Updated**: 2022-11-28 15:19:19+00:00
- **Authors**: Tal Reiss, Yedid Hoshen
- **Comment**: AAAI 2023
- **Journal**: None
- **Summary**: Deep anomaly detection methods learn representations that separate between normal and anomalous images. Although self-supervised representation learning is commonly used, small dataset sizes limit its effectiveness. It was previously shown that utilizing external, generic datasets (e.g. ImageNet classification) can significantly improve anomaly detection performance. One approach is outlier exposure, which fails when the external datasets do not resemble the anomalies. We take the approach of transferring representations pre-trained on external datasets for anomaly detection. Anomaly detection performance can be significantly improved by fine-tuning the pre-trained representations on the normal training images. In this paper, we first demonstrate and analyze that contrastive learning, the most popular self-supervised learning paradigm cannot be naively applied to pre-trained features. The reason is that pre-trained feature initialization causes poor conditioning for standard contrastive objectives, resulting in bad optimization dynamics. Based on our analysis, we provide a modified contrastive objective, the Mean-Shifted Contrastive Loss. Our method is highly effective and achieves a new state-of-the-art anomaly detection performance including $98.6\%$ ROC-AUC on the CIFAR-10 dataset.



### GAN Cocktail: mixing GANs without dataset access
- **Arxiv ID**: http://arxiv.org/abs/2106.03847v2
- **DOI**: 10.1007/978-3-031-20050-2_13
- **Categories**: **cs.LG**, cs.CV, cs.GR, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2106.03847v2)
- **Published**: 2021-06-07 17:59:04+00:00
- **Updated**: 2022-07-11 08:37:52+00:00
- **Authors**: Omri Avrahami, Dani Lischinski, Ohad Fried
- **Comment**: ECCV 2022. Project page is available at:
  https://omriavrahami.com/GAN-cocktail-page/
- **Journal**: None
- **Summary**: Today's generative models are capable of synthesizing high-fidelity images, but each model specializes on a specific target domain. This raises the need for model merging: combining two or more pretrained generative models into a single unified one. In this work we tackle the problem of model merging, given two constraints that often come up in the real world: (1) no access to the original training data, and (2) without increasing the size of the neural network. To the best of our knowledge, model merging under these constraints has not been studied thus far. We propose a novel, two-stage solution. In the first stage, we transform the weights of all the models to the same parameter space by a technique we term model rooting. In the second stage, we merge the rooted models by averaging their weights and fine-tuning them for each specific domain, using only data generated by the original trained models. We demonstrate that our approach is superior to baseline methods and to existing transfer learning techniques, and investigate several applications.



### SIMONe: View-Invariant, Temporally-Abstracted Object Representations via Unsupervised Video Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2106.03849v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03849v2)
- **Published**: 2021-06-07 17:59:23+00:00
- **Updated**: 2021-12-06 16:39:36+00:00
- **Authors**: Rishabh Kabra, Daniel Zoran, Goker Erdogan, Loic Matthey, Antonia Creswell, Matthew Botvinick, Alexander Lerchner, Christopher P. Burgess
- **Comment**: Animated figures are available at
  https://sites.google.com/view/simone-scene-understanding/
- **Journal**: None
- **Summary**: To help agents reason about scenes in terms of their building blocks, we wish to extract the compositional structure of any given scene (in particular, the configuration and characteristics of objects comprising the scene). This problem is especially difficult when scene structure needs to be inferred while also estimating the agent's location/viewpoint, as the two variables jointly give rise to the agent's observations. We present an unsupervised variational approach to this problem. Leveraging the shared structure that exists across different scenes, our model learns to infer two sets of latent representations from RGB video input alone: a set of "object" latents, corresponding to the time-invariant, object-level contents of the scene, as well as a set of "frame" latents, corresponding to global time-varying elements such as viewpoint. This factorization of latents allows our model, SIMONe, to represent object attributes in an allocentric manner which does not depend on viewpoint. Moreover, it allows us to disentangle object dynamics and summarize their trajectories as time-abstracted, view-invariant, per-object properties. We demonstrate these capabilities, as well as the model's performance in terms of view synthesis and instance segmentation, across three procedurally generated video datasets.



### Shifting Transformation Learning for Out-of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.03899v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03899v2)
- **Published**: 2021-06-07 18:18:26+00:00
- **Updated**: 2021-10-08 19:40:44+00:00
- **Authors**: Sina Mohseni, Arash Vahdat, Jay Yadawa
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting out-of-distribution (OOD) samples plays a key role in open-world and safety-critical applications such as autonomous systems and healthcare. Recently, self-supervised representation learning techniques (via contrastive learning and pretext learning) have shown effective in improving OOD detection. However, one major issue with such approaches is the choice of shifting transformations and pretext tasks which depends on the in-domain distribution. In this paper, we propose a simple framework that leverages a shifting transformation learning setting for learning multiple shifted representations of the training set for improved OOD detection. To address the problem of selecting optimal shifting transformation and pretext tasks, we propose a simple mechanism for automatically selecting the transformations and modulating their effect on representation learning without requiring any OOD training samples. In extensive experiments, we show that our simple framework outperforms state-of-the-art OOD detection models on several image datasets. We also characterize the criteria for a desirable OOD detector for real-world applications and demonstrate the efficacy of our proposed technique against state-of-the-art OOD detection techniques.



### AutoPtosis
- **Arxiv ID**: http://arxiv.org/abs/2106.03905v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03905v2)
- **Published**: 2021-06-07 18:32:31+00:00
- **Updated**: 2021-06-09 15:41:00+00:00
- **Authors**: Abdullah Aleem, Manoj Prabhakar Nallabothula, Pete Setabutr, Joelle A. Hallak, Darvin Yi
- **Comment**: None
- **Journal**: None
- **Summary**: Blepharoptosis, or ptosis as it is more commonly referred to, is a condition of the eyelid where the upper eyelid droops. The current diagnosis for ptosis involves cumbersome manual measurements that are time-consuming and prone to human error. In this paper, we present AutoPtosis, an artificial intelligence based system with interpretable results for rapid diagnosis of ptosis. We utilize a diverse dataset collected from the Illinois Ophthalmic Database Atlas (I-ODA) to develop a robust deep learning model for prediction and also develop a clinically inspired model that calculates the marginal reflex distance and iris ratio. AutoPtosis achieved 95.5% accuracy on physician verified data that had an equal class balance. The proposed algorithm can help in the rapid and timely diagnosis of ptosis, significantly reduce the burden on the healthcare system, and save the patients and clinics valuable resources.



### XIRL: Cross-embodiment Inverse Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.03911v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03911v3)
- **Published**: 2021-06-07 18:45:07+00:00
- **Updated**: 2021-12-13 12:40:16+00:00
- **Authors**: Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette Bohg, Debidatta Dwibedi
- **Comment**: Oral Accept, CoRL '21
- **Journal**: None
- **Summary**: We investigate the visual cross-embodiment imitation setting, in which agents learn policies from videos of other agents (such as humans) demonstrating the same task, but with stark differences in their embodiments -- shape, actions, end-effector dynamics, etc. In this work, we demonstrate that it is possible to automatically discover and learn vision-based reward functions from cross-embodiment demonstration videos that are robust to these differences. Specifically, we present a self-supervised method for Cross-embodiment Inverse Reinforcement Learning (XIRL) that leverages temporal cycle-consistency constraints to learn deep visual embeddings that capture task progression from offline videos of demonstrations across multiple expert agents, each performing the same task differently due to embodiment differences. Prior to our work, producing rewards from self-supervised embeddings typically required alignment with a reference trajectory, which may be difficult to acquire under stark embodiment differences. We show empirically that if the embeddings are aware of task progress, simply taking the negative distance between the current state and goal state in the learned embedding space is useful as a reward for training policies with reinforcement learning. We find our learned reward function not only works for embodiments seen during training, but also generalizes to entirely new embodiments. Additionally, when transferring real-world human demonstrations to a simulated robot, we find that XIRL is more sample efficient than current best methods. Qualitative results, code, and datasets are available at https://x-irl.github.io



### How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2106.03932v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2106.03932v2)
- **Published**: 2021-06-07 19:44:56+00:00
- **Updated**: 2021-09-07 06:22:37+00:00
- **Authors**: Okan Köpüklü, Maja Taseska, Gerhard Rigoll
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Successful active speaker detection requires a three-stage pipeline: (i) audio-visual encoding for all speakers in the clip, (ii) inter-speaker relation modeling between a reference speaker and the background speakers within each frame, and (iii) temporal modeling for the reference speaker. Each stage of this pipeline plays an important role for the final performance of the created architecture. Based on a series of controlled experiments, this work presents several practical guidelines for audio-visual active speaker detection. Correspondingly, we present a new architecture called ASDNet, which achieves a new state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5% outperforming the second best with a large margin of 4.7%. Our code and pretrained models are publicly available.



### Progressive Multi-scale Fusion Network for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.03941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03941v1)
- **Published**: 2021-06-07 20:02:39+00:00
- **Updated**: 2021-06-07 20:02:39+00:00
- **Authors**: Guangyu Ren, Yanchu Xie, Tianhong Dai, Tania Stathaki
- **Comment**: None
- **Journal**: None
- **Summary**: Salient object detection(SOD) aims at locating the most significant object within a given image. In recent years, great progress has been made in applying SOD on many vision tasks. The depth map could provide additional spatial prior and boundary cues to boost the performance. Combining the depth information with image data obtained from standard visual cameras has been widely used in recent SOD works, however, introducing depth information in a suboptimal fusion strategy may have negative influence in the performance of SOD. In this paper, we discuss about the advantages of the so-called progressive multi-scale fusion method and propose a mask-guided feature aggregation module(MGFA). The proposed framework can effectively combine the two features of different modalities and, furthermore, alleviate the impact of erroneous depth features, which are inevitably caused by the variation of depth quality. We further introduce a mask-guided refinement module(MGRM) to complement the high-level semantic features and reduce the irrelevant features from multi-scale fusion, leading to an overall refinement of detection. Experiments on five challenging benchmarks demonstrate that the proposed method outperforms 11 state-of-the-art methods under different evaluation metrics.



### Novel View Video Prediction Using a Dual Representation
- **Arxiv ID**: http://arxiv.org/abs/2106.03956v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03956v1)
- **Published**: 2021-06-07 20:41:33+00:00
- **Updated**: 2021-06-07 20:41:33+00:00
- **Authors**: Sarah Shiraz, Krishna Regmi, Shruti Vyas, Yogesh S. Rawat, Mubarak Shah
- **Comment**: Accepted in ICIP 2021
- **Journal**: None
- **Summary**: We address the problem of novel view video prediction; given a set of input video clips from a single/multiple views, our network is able to predict the video from a novel view. The proposed approach does not require any priors and is able to predict the video from wider angular distances, upto 45 degree, as compared to the recent studies predicting small variations in viewpoint. Moreover, our method relies only onRGB frames to learn a dual representation which is used to generate the video from a novel viewpoint. The dual representation encompasses a view-dependent and a global representation which incorporates complementary details to enable novel view video prediction. We demonstrate the effectiveness of our framework on two real world datasets: NTU-RGB+D and CMU Panoptic. A comparison with the State-of-the-art novel view video prediction methods shows an improvement of 26.1% in SSIM, 13.6% in PSNR, and 60% inFVD scores without using explicit priors from target views.



### Generative Flows with Invertible Attentions
- **Arxiv ID**: http://arxiv.org/abs/2106.03959v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03959v4)
- **Published**: 2021-06-07 20:43:04+00:00
- **Updated**: 2022-03-31 04:35:39+00:00
- **Authors**: Rhea Sanjay Sukthanker, Zhiwu Huang, Suryansh Kumar, Radu Timofte, Luc Van Gool
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Flow-based generative models have shown an excellent ability to explicitly learn the probability density function of data via a sequence of invertible transformations. Yet, learning attentions in generative flows remains understudied, while it has made breakthroughs in other domains. To fill the gap, this paper introduces two types of invertible attention mechanisms, i.e., map-based and transformer-based attentions, for both unconditional and conditional generative flows. The key idea is to exploit a masked scheme of these two attentions to learn long-range data dependencies in the context of generative flows. The masked scheme allows for invertible attention modules with tractable Jacobian determinants, enabling its seamless integration at any positions of the flow-based models. The proposed attention mechanisms lead to more efficient generative flows, due to their capability of modeling the long-term data dependencies. Evaluation on multiple image synthesis tasks shows that the proposed attention flows result in efficient models and compare favorably against the state-of-the-art unconditional and conditional generative flows.



### Weakly Supervised Volumetric Image Segmentation with Deformed Templates
- **Arxiv ID**: http://arxiv.org/abs/2106.03987v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03987v3)
- **Published**: 2021-06-07 22:09:34+00:00
- **Updated**: 2022-06-03 12:58:29+00:00
- **Authors**: Udaranga Wickramasinghe, Patrick M. Jensen, Mian Shah, Jiancheng Yang, Pascal Fua
- **Comment**: 12 Pages
- **Journal**: None
- **Summary**: There are many approaches to weakly-supervised training of networks to segment 2D images. By contrast, existing approaches to segmenting volumetric images rely on full-supervision of a subset of 2D slices of the 3D volume. We propose an approach to volume segmentation that is truly weakly-supervised in the sense that we only need to provide a sparse set of 3D points on the surface of target objects instead of detailed 2D masks. We use the 3D points to deform a 3D template so that it roughly matches the target object outlines and we introduce an architecture that exploits the supervision it provides to train a network to find accurate boundaries. We evaluate our approach on Computed Tomography (CT), Magnetic Resonance Imagery (MRI) and Electron Microscopy (EM) image datasets and show that it substantially reduces the required amount of effort.



### Task-Generic Hierarchical Human Motion Prior using VAEs
- **Arxiv ID**: http://arxiv.org/abs/2106.04004v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2106.04004v1)
- **Published**: 2021-06-07 23:11:42+00:00
- **Updated**: 2021-06-07 23:11:42+00:00
- **Authors**: Jiaman Li, Ruben Villegas, Duygu Ceylan, Jimei Yang, Zhengfei Kuang, Hao Li, Yajie Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: A deep generative model that describes human motions can benefit a wide range of fundamental computer vision and graphics tasks, such as providing robustness to video-based human pose estimation, predicting complete body movements for motion capture systems during occlusions, and assisting key frame animation with plausible movements. In this paper, we present a method for learning complex human motions independent of specific tasks using a combined global and local latent space to facilitate coarse and fine-grained modeling. Specifically, we propose a hierarchical motion variational autoencoder (HM-VAE) that consists of a 2-level hierarchical latent space. While the global latent space captures the overall global body motion, the local latent space enables to capture the refined poses of the different body parts. We demonstrate the effectiveness of our hierarchical motion variational autoencoder in a variety of tasks including video-based human pose estimation, motion completion from partial observations, and motion synthesis from sparse key-frames. Even though, our model has not been trained for any of these tasks specifically, it provides superior performance than task-specific alternatives. Our general-purpose human motion prior model can fix corrupted human body animations and generate complete movements from incomplete observations.



### On the Coupling of Depth and Egomotion Networks for Self-Supervised Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/2106.04007v3
- **DOI**: 10.1109/LRA.2022.3176087
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.04007v3)
- **Published**: 2021-06-07 23:30:45+00:00
- **Updated**: 2022-07-11 14:07:39+00:00
- **Authors**: Brandon Wagstaff, Valentin Peretroukhin, Jonathan Kelly
- **Comment**: In IEEE Robotics and Automation Letters (RA-L) and presented at the
  IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS'22), Kyoto, Japan, Oct. 23-27, 2022
- **Journal**: IEEE Robotics and Automation Letters (RA-L), Vol. 7, No. 3, pp.
  6766 - 6773, Jul. 2022
- **Summary**: Structure from motion (SfM) has recently been formulated as a self-supervised learning problem, where neural network models of depth and egomotion are learned jointly through view synthesis. Herein, we address the open problem of how to best couple, or link, the depth and egomotion network components, so that information such as a common scale factor can be shared between the networks. Towards this end, we introduce several notions of coupling, categorize existing approaches, and present a novel tightly-coupled approach that leverages the interdependence of depth and egomotion at training time and at test time. Our approach uses iterative view synthesis to recursively update the egomotion network input, permitting contextual information to be passed between the components. We demonstrate through substantial experiments that our approach promotes consistency between the depth and egomotion predictions at test time, improves generalization, and leads to state-of-the-art accuracy on indoor and outdoor depth and egomotion evaluation benchmarks.



### FEAR: A Simple Lightweight Method to Rank Architectures
- **Arxiv ID**: http://arxiv.org/abs/2106.04010v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.04010v1)
- **Published**: 2021-06-07 23:38:21+00:00
- **Updated**: 2021-06-07 23:38:21+00:00
- **Authors**: Debadeepta Dey, Shital Shah, Sebastien Bubeck
- **Comment**: 31 pages, 8 figures
- **Journal**: None
- **Summary**: The fundamental problem in Neural Architecture Search (NAS) is to efficiently find high-performing architectures from a given search space. We propose a simple but powerful method which we call FEAR, for ranking architectures in any search space. FEAR leverages the viewpoint that neural networks are powerful non-linear feature extractors. First, we train different architectures in the search space to the same training or validation error. Then, we compare the usefulness of the features extracted by each architecture. We do so with a quick training keeping most of the architecture frozen. This gives fast estimates of the relative performance. We validate FEAR on Natsbench topology search space on three different datasets against competing baselines and show strong ranking correlation especially compared to recently proposed zero-cost methods. FEAR particularly excels at ranking high-performance architectures in the search space. When used in the inner loop of discrete search algorithms like random search, FEAR can cut down the search time by approximately 2.4X without losing accuracy. We additionally empirically study very recently proposed zero-cost measures for ranking and find that they breakdown in ranking performance as training proceeds and also that data-agnostic ranking scores which ignore the dataset do not generalize across dissimilar datasets.



