# Arxiv Papers in cs.CV on 2021-06-11
### Spectral Unsupervised Domain Adaptation for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.06112v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06112v3)
- **Published**: 2021-06-11 01:31:52+00:00
- **Updated**: 2022-06-05 06:08:57+00:00
- **Authors**: Jingyi Zhang, Jiaxing Huang, Zichen Tian, Shijian Lu
- **Comment**: Accepted to CVPR2022
- **Journal**: None
- **Summary**: Though unsupervised domain adaptation (UDA) has achieved very impressive progress recently, it remains a great challenge due to missing target annotations and the rich discrepancy between source and target distributions. We propose Spectral UDA (SUDA), an effective and efficient UDA technique that works in the spectral space and can generalize across different visual recognition tasks. SUDA addresses the UDA challenges from two perspectives. First, it introduces a spectrum transformer (ST) that mitigates inter-domain discrepancies by enhancing domain-invariant spectra while suppressing domain-variant spectra of source and target samples simultaneously. Second, it introduces multi-view spectral learning that learns useful unsupervised representations by maximizing mutual information among multiple ST-generated spectral views of each target sample. Extensive experiments show that SUDA achieves superior accuracy consistently across different visual tasks in object detection, semantic segmentation and image classification. Additionally, SUDA also works with the transformer-based network and achieves state-of-the-art performance on object detection.



### Instance-Level Task Parameters: A Robust Multi-task Weighting Framework
- **Arxiv ID**: http://arxiv.org/abs/2106.06129v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.06129v1)
- **Published**: 2021-06-11 02:35:42+00:00
- **Updated**: 2021-06-11 02:35:42+00:00
- **Authors**: Pavan Kumar Anasosalu Vasu, Shreyas Saxena, Oncel Tuzel
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works have shown that deep neural networks benefit from multi-task learning by learning a shared representation across several related tasks. However, performance of such systems depend on relative weighting between various losses involved during training. Prior works on loss weighting schemes assume that instances are equally easy or hard for all tasks. In order to break this assumption, we let the training process dictate the optimal weighting of tasks for every instance in the dataset. More specifically, we equip every instance in the dataset with a set of learnable parameters (instance-level task parameters) where the cardinality is equal to the number of tasks learned by the model. These parameters model the weighting of each task for an instance. They are updated by gradient descent and do not require hand-crafted rules. We conduct extensive experiments on SURREAL and CityScapes datasets, for human shape and pose estimation, depth estimation and semantic segmentation tasks. In these tasks, our approach outperforms recent dynamic loss weighting approaches, e.g. reducing surface estimation errors by 8.97% on SURREAL. When applied to datasets where one or more tasks can have noisy annotations, the proposed method learns to prioritize learning from clean labels for a given task, e.g. reducing surface estimation errors by up to 60%. We also show that we can reliably detect corrupt labels for a given task as a by-product from learned instance-level task parameters.



### Refining Pseudo Labels with Clustering Consensus over Generations for Unsupervised Object Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2106.06133v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06133v2)
- **Published**: 2021-06-11 02:42:42+00:00
- **Updated**: 2021-08-23 11:10:50+00:00
- **Authors**: Xiao Zhang, Yixiao Ge, Yu Qiao, Hongsheng Li
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Unsupervised object re-identification targets at learning discriminative representations for object retrieval without any annotations. Clustering-based methods conduct training with the generated pseudo labels and currently dominate this research direction. However, they still suffer from the issue of pseudo label noise. To tackle the challenge, we propose to properly estimate pseudo label similarities between consecutive training generations with clustering consensus and refine pseudo labels with temporally propagated and ensembled pseudo labels. To the best of our knowledge, this is the first attempt to leverage the spirit of temporal ensembling to improve classification with dynamically changing classes over generations. The proposed pseudo label refinery strategy is simple yet effective and can be seamlessly integrated into existing clustering-based unsupervised re-identification methods. With our proposed approach, state-of-the-art method can be further boosted with up to 8.8% mAP improvements on the challenging MSMT17 dataset.



### Team RUC_AIM3 Technical Report at ActivityNet 2021: Entities Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2106.06138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06138v1)
- **Published**: 2021-06-11 02:50:25+00:00
- **Updated**: 2021-06-11 02:50:25+00:00
- **Authors**: Ludan Ruan, Jieting Chen, Yuqing Song, Shizhe Chen, Qin Jin
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: Entities Object Localization (EOL) aims to evaluate how grounded or faithful a description is, which consists of caption generation and object grounding. Previous works tackle this problem by jointly training the two modules in a framework, which limits the complexity of each module. Therefore, in this work, we propose to divide these two modules into two stages and improve them respectively to boost the whole system performance. For the caption generation, we propose a Unified Multi-modal Pre-training Model (UMPM) to generate event descriptions with rich objects for better localization. For the object grounding, we fine-tune the state-of-the-art detection model MDETR and design a post processing method to make the grounding results more faithful. Our overall system achieves the state-of-the-art performances on both sub-tasks in Entities Object Localization challenge at Activitynet 2021, with 72.57 localization accuracy on the testing set of sub-task I and 0.2477 F1_all_per_sent on the hidden testing set of sub-task II.



### PyGAD: An Intuitive Genetic Algorithm Python Library
- **Arxiv ID**: http://arxiv.org/abs/2106.06158v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2106.06158v1)
- **Published**: 2021-06-11 04:08:30+00:00
- **Updated**: 2021-06-11 04:08:30+00:00
- **Authors**: Ahmed Fawzy Gad
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces PyGAD, an open-source easy-to-use Python library for building the genetic algorithm. PyGAD supports a wide range of parameters to give the user control over everything in its life cycle. This includes, but is not limited to, population, gene value range, gene data type, parent selection, crossover, and mutation. PyGAD is designed as a general-purpose optimization library that allows the user to customize the fitness function. Its usage consists of 3 main steps: build the fitness function, create an instance of the pygad.GA class, and calling the pygad.GA.run() method. The library supports training deep learning models created either with PyGAD itself or with frameworks like Keras and PyTorch. Given its stable state, PyGAD is also in active development to respond to the user's requested features and enhancement received on GitHub https://github.com/ahmedfgad/GeneticAlgorithmPython. PyGAD comes with documentation https://pygad.readthedocs.io for further details and examples.



### Learning the Precise Feature for Cluster Assignment
- **Arxiv ID**: http://arxiv.org/abs/2106.06159v1
- **DOI**: 10.1109/TCYB.2021.3079914
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.06159v1)
- **Published**: 2021-06-11 04:08:54+00:00
- **Updated**: 2021-06-11 04:08:54+00:00
- **Authors**: Yanhai Gan, Xinghui Dong, Huiyu Zhou, Feng Gao, Junyu Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Clustering is one of the fundamental tasks in computer vision and pattern recognition. Recently, deep clustering methods (algorithms based on deep learning) have attracted wide attention with their impressive performance. Most of these algorithms combine deep unsupervised representation learning and standard clustering together. However, the separation of representation learning and clustering will lead to suboptimal solutions because the two-stage strategy prevents representation learning from adapting to subsequent tasks (e.g., clustering according to specific cues). To overcome this issue, efforts have been made in the dynamic adaption of representation and cluster assignment, whereas current state-of-the-art methods suffer from heuristically constructed objectives with representation and cluster assignment alternatively optimized. To further standardize the clustering problem, we audaciously formulate the objective of clustering as finding a precise feature as the cue for cluster assignment. Based on this, we propose a general-purpose deep clustering framework which radically integrates representation learning and clustering into a single pipeline for the first time. The proposed framework exploits the powerful ability of recently developed generative models for learning intrinsic features, and imposes an entropy minimization on the distribution of the cluster assignment by a dedicated variational algorithm. Experimental results show that the performance of the proposed method is superior, or at least comparable to, the state-of-the-art methods on the handwritten digit recognition, fashion recognition, face recognition and object recognition benchmark datasets.



### Calibration and Auto-Refinement for Light Field Cameras
- **Arxiv ID**: http://arxiv.org/abs/2106.06181v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06181v1)
- **Published**: 2021-06-11 05:49:14+00:00
- **Updated**: 2021-06-11 05:49:14+00:00
- **Authors**: Yuriy Anisimov, Gerd Reis, Didier Stricker
- **Comment**: Presented on 29. International Conference on Computer Graphics,
  Visualization and Computer Vision 2021 (WSCG 2021)
- **Journal**: None
- **Summary**: The ability to create an accurate three-dimensional reconstruction of a captured scene draws attention to the principles of light fields. This paper presents an approach for light field camera calibration and rectification, based on pairwise pattern-based parameters extraction. It is followed by a correspondence-based algorithm for camera parameters refinement from arbitrary scenes using the triangulation filter and nonlinear optimization. The effectiveness of our approach is validated on both real and synthetic data.



### MlTr: Multi-label Classification with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2106.06195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06195v1)
- **Published**: 2021-06-11 06:53:09+00:00
- **Updated**: 2021-06-11 06:53:09+00:00
- **Authors**: Xing Cheng, Hezheng Lin, Xiangyu Wu, Fan Yang, Dong Shen, Zhongyuan Wang, Nian Shi, Honglin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The task of multi-label image classification is to recognize all the object labels presented in an image. Though advancing for years, small objects, similar objects and objects with high conditional probability are still the main bottlenecks of previous convolutional neural network(CNN) based models, limited by convolutional kernels' representational capacity. Recent vision transformer networks utilize the self-attention mechanism to extract the feature of pixel granularity, which expresses richer local semantic information, while is insufficient for mining global spatial dependence. In this paper, we point out the three crucial problems that CNN-based methods encounter and explore the possibility of conducting specific transformer modules to settle them. We put forward a Multi-label Transformer architecture(MlTr) constructed with windows partitioning, in-window pixel attention, cross-window attention, particularly improving the performance of multi-label image classification tasks. The proposed MlTr shows state-of-the-art results on various prevalent multi-label datasets such as MS-COCO, Pascal-VOC, and NUS-WIDE with 88.5%, 95.8%, and 65.5% respectively. The code will be available soon at https://github.com/starmemda/MlTr/



### A deep learning approach to clustering visual arts
- **Arxiv ID**: http://arxiv.org/abs/2106.06234v2
- **DOI**: 10.1007/s11263-022-01664-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06234v2)
- **Published**: 2021-06-11 08:35:26+00:00
- **Updated**: 2022-08-17 06:38:36+00:00
- **Authors**: Giovanna Castellano, Gennaro Vessio
- **Comment**: Published on Int J Comput Vis (2022)
- **Journal**: None
- **Summary**: Clustering artworks is difficult for several reasons. On the one hand, recognizing meaningful patterns based on domain knowledge and visual perception is extremely hard. On the other hand, applying traditional clustering and feature reduction techniques to the highly dimensional pixel space can be ineffective. To address these issues, in this paper we propose DELIUS: a DEep learning approach to cLustering vIsUal artS. The method uses a pre-trained convolutional network to extract features and then feeds these features into a deep embedded clustering model, where the task of mapping the input data to a latent space is jointly optimized with the task of finding a set of cluster centroids in this latent space. Quantitative and qualitative experimental results show the effectiveness of the proposed method. DELIUS can be useful for several tasks related to art analysis, in particular visual link retrieval and historical knowledge discovery in painting datasets.



### KRADA: Known-region-aware Domain Alignment for Open-set Domain Adaptation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.06237v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.06237v3)
- **Published**: 2021-06-11 08:43:59+00:00
- **Updated**: 2023-02-19 07:02:37+00:00
- **Authors**: Chenhong Zhou, Feng Liu, Chen Gong, Rongfei Zeng, Tongliang Liu, William K. Cheung, Bo Han
- **Comment**: 18 pages
- **Journal**: Transactions on Machine Learning Research, 2023
- **Summary**: In semantic segmentation, we aim to train a pixel-level classifier to assign category labels to all pixels in an image, where labeled training images and unlabeled test images are from the same distribution and share the same label set. However, in an open world, the unlabeled test images probably contain unknown categories and have different distributions from the labeled images. Hence, in this paper, we consider a new, more realistic, and more challenging problem setting where the pixel-level classifier has to be trained with labeled images and unlabeled open-world images -- we name it open-set domain adaptation segmentation (OSDAS). In OSDAS, the trained classifier is expected to identify unknown-class pixels and classify known-class pixels well. To solve OSDAS, we first investigate which distribution that unknown-class pixels obey. Then, motivated by the goodness-of-fit test, we use statistical measurements to show how a pixel fits the distribution of an unknown class and select highly-fitted pixels to form the unknown region in each test image. Eventually, we propose an end-to-end learning framework, known-region-aware domain alignment (KRADA), to distinguish unknown classes while aligning the distributions of known classes in labeled and unlabeled open-world images. The effectiveness of KRADA has been verified on two synthetic tasks and one COVID-19 segmentation task.



### AugNet: End-to-End Unsupervised Visual Representation Learning with Image Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.06250v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06250v1)
- **Published**: 2021-06-11 09:02:30+00:00
- **Updated**: 2021-06-11 09:02:30+00:00
- **Authors**: Mingxiang Chen, Zhanguo Chang, Haonan Lu, Bitao Yang, Zhuang Li, Liufang Guo, Zhecheng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the achievements in artificial intelligence so far were accomplished by supervised learning which requires numerous annotated training data and thus costs innumerable manpower for labeling. Unsupervised learning is one of the effective solutions to overcome such difficulties. In our work, we propose AugNet, a new deep learning training paradigm to learn image features from a collection of unlabeled pictures. We develop a method to construct the similarities between pictures as distance metrics in the embedding space by leveraging the inter-correlation between augmented versions of samples. Our experiments demonstrate that the method is able to represent the image in low dimensional space and performs competitively in downstream tasks such as image classification and image similarity comparison. Specifically, we achieved over 60% and 27% accuracy on the STL10 and CIFAR100 datasets with unsupervised clustering, respectively. Moreover, unlike many deep-learning-based image retrieval algorithms, our approach does not require access to external annotated datasets to train the feature extractor, but still shows comparable or even better feature representation ability and easy-to-use characteristics. In our evaluations, the method outperforms all the state-of-the-art image retrieval algorithms on some out-of-domain image datasets. The code for the model implementation is available at https://github.com/chenmingxiang110/AugNet.



### Survey of Image Based Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.06307v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.06307v1)
- **Published**: 2021-06-11 10:56:43+00:00
- **Updated**: 2021-06-11 10:56:43+00:00
- **Authors**: Usman Nazir, He Wang, Murtaza Taj
- **Comment**: None
- **Journal**: None
- **Summary**: In this survey paper, we analyze image based graph neural networks and propose a three-step classification approach. We first convert the image into superpixels using the Quickshift algorithm so as to reduce 30% of the input data. The superpixels are subsequently used to generate a region adjacency graph. Finally, the graph is passed through a state-of-art graph convolutional neural network to get classification scores. We also analyze the spatial and spectral convolution filtering techniques in graph neural networks. Spectral-based models perform better than spatial-based models and classical CNN with lesser compute cost.



### Topology-Preserved Human Reconstruction with Details
- **Arxiv ID**: http://arxiv.org/abs/2106.06313v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06313v3)
- **Published**: 2021-06-11 11:13:42+00:00
- **Updated**: 2022-02-01 09:12:48+00:00
- **Authors**: Lixiang Lin, Jianke Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: It is challenging to directly estimate the human geometry from a single image due to the high diversity and complexity of body shapes with the various clothing styles. Most of model-based approaches are limited to predict the shape and pose of a minimally clothed body with over-smoothing surface. While capturing the fine detailed geometries, the model-free methods are lack of the fixed mesh topology. To address these issues, we propose a novel topology-preserved human reconstruction approach by bridging the gap between model-based and model-free human reconstruction. We present an end-to-end neural network that simultaneously predicts the pixel-aligned implicit surface and an explicit mesh model built by graph convolutional neural network. Experiments on DeepHuman and our collected dataset showed that our approach is effective. The code will be made publicly available.



### ViT-Inception-GAN for Image Colourising
- **Arxiv ID**: http://arxiv.org/abs/2106.06321v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.06321v1)
- **Published**: 2021-06-11 11:41:14+00:00
- **Updated**: 2021-06-11 11:41:14+00:00
- **Authors**: Tejas Bana, Jatan Loya, Siddhant Kulkarni
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Studies involving colourising images has been garnering researchers' keen attention over time, assisted by significant advances in various Machine Learning techniques and compute power availability. Traditionally, colourising images have been an intricate task that gave a substantial degree of freedom during the assignment of chromatic information. In our proposed method, we attempt to colourise images using Vision Transformer - Inception - Generative Adversarial Network (ViT-I-GAN), which has an Inception-v3 fusion embedding in the generator. For a stable and robust network, we have used Vision Transformer (ViT) as the discriminator. We trained the model on the Unsplash and the COCO dataset for demonstrating the improvement made by the Inception-v3 embedding. We have compared the results between ViT-GANs with and without Inception-v3 embedding.



### SimSwap: An Efficient Framework For High Fidelity Face Swapping
- **Arxiv ID**: http://arxiv.org/abs/2106.06340v1
- **DOI**: 10.1145/3394171.3413630
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06340v1)
- **Published**: 2021-06-11 12:23:10+00:00
- **Updated**: 2021-06-11 12:23:10+00:00
- **Authors**: Renwang Chen, Xuanhong Chen, Bingbing Ni, Yanhao Ge
- **Comment**: Accepted by ACMMM 2020
- **Journal**: Proceedings of the 28th ACM International Conference on
  Multimedia. 2020
- **Summary**: We propose an efficient framework, called Simple Swap (SimSwap), aiming for generalized and high fidelity face swapping. In contrast to previous approaches that either lack the ability to generalize to arbitrary identity or fail to preserve attributes like facial expression and gaze direction, our framework is capable of transferring the identity of an arbitrary source face into an arbitrary target face while preserving the attributes of the target face. We overcome the above defects in the following two ways. First, we present the ID Injection Module (IIM) which transfers the identity information of the source face into the target face at feature level. By using this module, we extend the architecture of an identity-specific face swapping algorithm to a framework for arbitrary face swapping. Second, we propose the Weak Feature Matching Loss which efficiently helps our framework to preserve the facial attributes in an implicit way. Extensive experiments on wild faces demonstrate that our SimSwap is able to achieve competitive identity performance while preserving attributes better than previous state-of-the-art methods. The code is already available on github: https://github.com/neuralchen/SimSwap.



### Part-aware Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.06351v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06351v1)
- **Published**: 2021-06-11 12:48:07+00:00
- **Updated**: 2021-06-11 12:48:07+00:00
- **Authors**: Daan de Geus, Panagiotis Meletis, Chenyang Lu, Xiaoxiao Wen, Gijs Dubbelman
- **Comment**: CVPR 2021. Code and data: https://github.com/tue-mps/panoptic_parts
- **Journal**: None
- **Summary**: In this work, we introduce the new scene understanding task of Part-aware Panoptic Segmentation (PPS), which aims to understand a scene at multiple levels of abstraction, and unifies the tasks of scene parsing and part parsing. For this novel task, we provide consistent annotations on two commonly used datasets: Cityscapes and Pascal VOC. Moreover, we present a single metric to evaluate PPS, called Part-aware Panoptic Quality (PartPQ). For this new task, using the metric and annotations, we set multiple baselines by merging results of existing state-of-the-art methods for panoptic segmentation and part segmentation. Finally, we conduct several experiments that evaluate the importance of the different levels of abstraction in this single task.



### Conterfactual Generative Zero-Shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.06360v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T07, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2106.06360v1)
- **Published**: 2021-06-11 13:01:03+00:00
- **Updated**: 2021-06-11 13:01:03+00:00
- **Authors**: Feihong Shen, Jun Liu, Ping Hu
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: zero-shot learning is an essential part of computer vision. As a classical downstream task, zero-shot semantic segmentation has been studied because of its applicant value. One of the popular zero-shot semantic segmentation methods is based on the generative model Most new proposed works added structures on the same architecture to enhance this model. However, we found that, from the view of causal inference, the result of the original model has been influenced by spurious statistical relationships. Thus the performance of the prediction shows severe bias. In this work, we consider counterfactual methods to avoid the confounder in the original model. Based on this method, we proposed a new framework for zero-shot semantic segmentation. Our model is compared with baseline models on two real-world datasets, Pascal-VOC and Pascal-Context. The experiment results show proposed models can surpass previous confounded models and can still make use of additional structures to improve the performance. We also design a simple structure based on Graph Convolutional Networks (GCN) in this work.



### Small Object Detection for Near Real-Time Egocentric Perception in a Manual Assembly Scenario
- **Arxiv ID**: http://arxiv.org/abs/2106.06403v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2106.06403v1)
- **Published**: 2021-06-11 13:59:44+00:00
- **Updated**: 2021-06-11 13:59:44+00:00
- **Authors**: Hooman Tavakoli, Snehal Walunj, Parsha Pahlevannejad, Christiane Plociennik, Martin Ruskowski
- **Comment**: Accepted for presentation at EPIC@CVPR2021 workshop
- **Journal**: None
- **Summary**: Detecting small objects in video streams of head-worn augmented reality devices in near real-time is a huge challenge: training data is typically scarce, the input video stream can be of limited quality, and small objects are notoriously hard to detect. In industrial scenarios, however, it is often possible to leverage contextual knowledge for the detection of small objects. Furthermore, CAD data of objects are typically available and can be used to generate synthetic training data. We describe a near real-time small object detection pipeline for egocentric perception in a manual assembly scenario: We generate a training data set based on CAD data and realistic backgrounds in Unity. We then train a YOLOv4 model for a two-stage detection process: First, the context is recognized, then the small object of interest is detected. We evaluate our pipeline on the augmented reality device Microsoft Hololens 2.



### Attention-based Partial Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.06415v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06415v2)
- **Published**: 2021-06-11 14:16:06+00:00
- **Updated**: 2021-06-14 15:26:13+00:00
- **Authors**: Stefan Hörmann, Zeyuan Zhang, Martin Knoche, Torben Teepe, Gerhard Rigoll
- **Comment**: To be published in IEEE ICIP 2021
- **Journal**: None
- **Summary**: Photos of faces captured in unconstrained environments, such as large crowds, still constitute challenges for current face recognition approaches as often faces are occluded by objects or people in the foreground. However, few studies have addressed the task of recognizing partial faces. In this paper, we propose a novel approach to partial face recognition capable of recognizing faces with different occluded areas. We achieve this by combining attentional pooling of a ResNet's intermediate feature maps with a separate aggregation module. We further adapt common losses to partial faces in order to ensure that the attention maps are diverse and handle occluded parts. Our thorough analysis demonstrates that we outperform all baselines under multiple benchmark protocols, including naturally and synthetically occluded partial faces. This suggests that our method successfully focuses on the relevant parts of the occluded face.



### Scale-invariant scale-channel networks: Deep networks that generalise to previously unseen scales
- **Arxiv ID**: http://arxiv.org/abs/2106.06418v2
- **DOI**: 10.1007/s10851-022-01082-2
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.06418v2)
- **Published**: 2021-06-11 14:22:26+00:00
- **Updated**: 2022-02-22 10:35:46+00:00
- **Authors**: Ylva Jansson, Tony Lindeberg
- **Comment**: 31 pages, 15 figures, 6 tables. arXiv admin note: substantial text
  overlap with arXiv:2004.01536
- **Journal**: Journal of Mathematical Imaging and Vision, 2022
- **Summary**: The ability to handle large scale variations is crucial for many real world visual tasks. A straightforward approach for handling scale in a deep network is to process an image at several scales simultaneously in a set of scale channels. Scale invariance can then, in principle, be achieved by using weight sharing between the scale channels together with max or average pooling over the outputs from the scale channels. The ability of such scale channel networks to generalise to scales not present in the training set over significant scale ranges has, however, not previously been explored.   In this paper, we present a systematic study of this methodology by implementing different types of scale channel networks and evaluating their ability to generalise to previously unseen scales. We develop a formalism for analysing the covariance and invariance properties of scale channel networks, and explore how different design choices, unique to scaling transformations, affect the overall performance of scale channel networks. We first show that two previously proposed scale channel network designs do not generalise well to scales not present in the training set. We explain theoretically and demonstrate experimentally why generalisation fails in these cases.   We then propose a new type of foveated scale channel architecture}, where the scale channels process increasingly larger parts of the image with decreasing resolution. This new type of scale channel network is shown to generalise extremely well, provided sufficient image resolution and the absence of boundary effects. Our proposed FovMax and FovAvg networks perform almost identically over a scale range of 8, also when training on single scale training data, and do also give improved performance when learning from datasets with large scale variations in the small sample regime.



### A Framework to Enhance Generalization of Deep Metric Learning methods using General Discriminative Feature Learning and Class Adversarial Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.06420v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, 6804 (Primary)
- **Links**: [PDF](http://arxiv.org/pdf/2106.06420v1)
- **Published**: 2021-06-11 14:24:40+00:00
- **Updated**: 2021-06-11 14:24:40+00:00
- **Authors**: Karrar Al-Kaabi, Reza Monsefi, Davood Zabihzadeh
- **Comment**: Includes: 31 Pages, 5 Tables, 15 Figures
- **Journal**: None
- **Summary**: Metric learning algorithms aim to learn a distance function that brings the semantically similar data items together and keeps dissimilar ones at a distance. The traditional Mahalanobis distance learning is equivalent to find a linear projection. In contrast, Deep Metric Learning (DML) methods are proposed that automatically extract features from data and learn a non-linear transformation from input space to a semantically embedding space. Recently, many DML methods are proposed focused to enhance the discrimination power of the learned metric by providing novel sampling strategies or loss functions. This approach is very helpful when both the training and test examples are coming from the same set of categories. However, it is less effective in many applications of DML such as image retrieval and person-reidentification. Here, the DML should learn general semantic concepts from observed classes and employ them to rank or identify objects from unseen categories. Neglecting the generalization ability of the learned representation and just emphasizing to learn a more discriminative embedding on the observed classes may lead to the overfitting problem. To address this limitation, we propose a framework to enhance the generalization power of existing DML methods in a Zero-Shot Learning (ZSL) setting by general yet discriminative representation learning and employing a class adversarial neural network. To learn a more general representation, we propose to employ feature maps of intermediate layers in a deep neural network and enhance their discrimination power through an attention mechanism. Besides, a class adversarial network is utilized to enforce the deep model to seek class invariant features for the DML task. We evaluate our work on widely used machine vision datasets in a ZSL setting.



### An Image Forensic Technique Based on JPEG Ghosts
- **Arxiv ID**: http://arxiv.org/abs/2106.06439v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2106.06439v2)
- **Published**: 2021-06-11 14:52:43+00:00
- **Updated**: 2021-06-19 02:42:08+00:00
- **Authors**: Divakar Singh
- **Comment**: 8 pages, 10 figures, 2 tables
- **Journal**: None
- **Summary**: The unprecedented growth in the easy availability of photo-editing tools has endangered the power of digital images.An image was supposed to be worth more than a thousand words,but now this can be said only if it can be authenticated orthe integrity of the image can be proved to be intact. In thispaper, we propose a digital image forensic technique for JPEG images. It can detect any forgery in the image if the forged portion called a ghost image is having a compression quality different from that of the cover image. It is based on resaving the JPEG image at different JPEG qualities, and the detection of the forged portion is maximum when it is saved at the same JPEG quality as the cover image. Also, we can precisely predictthe JPEG quality of the cover image by analyzing the similarity using Structural Similarity Index Measure (SSIM) or the energyof the images. The first maxima in SSIM or the first minima inenergy correspond to the cover image JPEG quality. We created adataset for varying JPEG compression qualities of the ghost and the cover images and validated the scalability of the experimental results.We also, experimented with varied attack scenarios, e.g. high-quality ghost image embedded in low quality of cover image,low-quality ghost image embedded in high-quality of cover image,and ghost image and cover image both at the same quality.The proposed method is able to localize the tampered portions accurately even for forgeries as small as 10x10 sized pixel blocks.Our technique is also robust against other attack scenarios like copy-move forgery, inserting text into image, rescaling (zoom-out/zoom-in) ghost image and then pasting on cover image.



### Learning Compositional Shape Priors for Few-Shot 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2106.06440v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.06440v2)
- **Published**: 2021-06-11 14:55:49+00:00
- **Updated**: 2021-06-16 11:18:32+00:00
- **Authors**: Mateusz Michalkiewicz, Stavros Tsogkas, Sarah Parisot, Mahsa Baktashmotlagh, Anders Eriksson, Eugene Belilovsky
- **Comment**: 13 pages, 12 figures. arXiv admin note: substantial text overlap with
  arXiv:2004.06302
- **Journal**: None
- **Summary**: The impressive performance of deep convolutional neural networks in single-view 3D reconstruction suggests that these models perform non-trivial reasoning about the 3D structure of the output space. Recent work has challenged this belief, showing that, on standard benchmarks, complex encoder-decoder architectures perform similarly to nearest-neighbor baselines or simple linear decoder models that exploit large amounts of per-category data. However, building large collections of 3D shapes for supervised training is a laborious process; a more realistic and less constraining task is inferring 3D shapes for categories with few available training examples, calling for a model that can successfully generalize to novel object classes. In this work we experimentally demonstrate that naive baselines fail in this few-shot learning setting, in which the network must learn informative shape priors for inference of new categories. We propose three ways to learn a class-specific global shape prior, directly from data. Using these techniques, we are able to capture multi-scale information about the 3D shape, and account for intra-class variability by virtue of an implicit compositional structure. Experiments on the popular ShapeNet dataset show that our method outperforms a zero-shot baseline by over 40%, and the current state-of-the-art by over 10%, in terms of relative performance, in the few-shot setting.



### K-shot NAS: Learnable Weight-Sharing for NAS with K-shot Supernets
- **Arxiv ID**: http://arxiv.org/abs/2106.06442v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.06442v1)
- **Published**: 2021-06-11 14:57:36+00:00
- **Updated**: 2021-06-11 14:57:36+00:00
- **Authors**: Xiu Su, Shan You, Mingkai Zheng, Fei Wang, Chen Qian, Changshui Zhang, Chang Xu
- **Comment**: Accepted by ICML 2021
- **Journal**: None
- **Summary**: In one-shot weight sharing for NAS, the weights of each operation (at each layer) are supposed to be identical for all architectures (paths) in the supernet. However, this rules out the possibility of adjusting operation weights to cater for different paths, which limits the reliability of the evaluation results. In this paper, instead of counting on a single supernet, we introduce $K$-shot supernets and take their weights for each operation as a dictionary. The operation weight for each path is represented as a convex combination of items in a dictionary with a simplex code. This enables a matrix approximation of the stand-alone weight matrix with a higher rank ($K>1$). A \textit{simplex-net} is introduced to produce architecture-customized code for each path. As a result, all paths can adaptively learn how to share weights in the $K$-shot supernets and acquire corresponding weights for better evaluation. $K$-shot supernets and simplex-net can be iteratively trained, and we further extend the search to the channel dimension. Extensive experiments on benchmark datasets validate that K-shot NAS significantly improves the evaluation accuracy of paths and thus brings in impressive performance improvements.



### Neural Network Modeling of Probabilities for Coding the Octree Representation of Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2106.06482v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.06482v4)
- **Published**: 2021-06-11 16:07:46+00:00
- **Updated**: 2021-10-11 13:58:16+00:00
- **Authors**: Emre Can Kaya, Ioan Tabus
- **Comment**: 6 pages, 3 figures, MMSP 2021 camera-ready
- **Journal**: None
- **Summary**: This paper describes a novel lossless point cloud compression algorithm that uses a neural network for estimating the coding probabilities for the occupancy status of voxels, depending on wide three dimensional contexts around the voxel to be encoded. The point cloud is represented as an octree, with each resolution layer being sequentially encoded and decoded using arithmetic coding, starting from the lowest resolution, until the final resolution is reached. The occupancy probability of each voxel of the splitting pattern at each node of the octree is modeled by a neural network, having at its input the already encoded occupancy status of several octree nodes (belonging to the past and current resolutions), corresponding to a 3D context surrounding the node to be encoded. The algorithm has a fast and a slow version, the fast version selecting differently several voxels of the context, which allows an increased parallelization by sending larger batches of templates to be estimated by the neural network, at both encoder and decoder. The proposed algorithms yield state-of-the-art results on benchmark datasets. The implementation will be made available at https://github.com/marmus12/nnctx



### Pedestrian Attribute Recognition in Video Surveillance Scenarios Based on View-attribute Attention Localization
- **Arxiv ID**: http://arxiv.org/abs/2106.06485v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06485v2)
- **Published**: 2021-06-11 16:09:31+00:00
- **Updated**: 2021-12-20 01:22:52+00:00
- **Authors**: Weichen Chen, Xinyi Yu, Linlin Ou
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrian attribute recognition in surveillance scenarios is still a challenging task due to the inaccurate localization of specific attributes. In this paper, we propose a novel view-attribute localization method based on attention (VALA), which utilizes view information to guide the recognition process to focus on specific attributes and attention mechanism to localize specific attribute-corresponding areas. Concretely, view information is leveraged by the view prediction branch to generate four view weights that represent the confidences for attributes from different views. View weights are then delivered back to compose specific view-attributes, which will participate and supervise deep feature extraction. In order to explore the spatial location of a view-attribute, regional attention is introduced to aggregate spatial information and encode inter-channel dependencies of the view feature. Subsequently, a fine attentive attribute-specific region is localized, and regional weights for the view-attribute from different spatial locations are gained by the regional attention. The final view-attribute recognition outcome is obtained by combining the view weights with the regional weights. Experiments on three wide datasets (RAP, RAPv2, and PA-100K) demonstrate the effectiveness of our approach compared with state-of-the-art methods.



### Shallow Optical Flow Three-Stream CNN for Macro- and Micro-Expression Spotting from Long Videos
- **Arxiv ID**: http://arxiv.org/abs/2106.06489v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, I.4; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2106.06489v1)
- **Published**: 2021-06-11 16:19:48+00:00
- **Updated**: 2021-06-11 16:19:48+00:00
- **Authors**: Gen-Bing Liong, John See, Lai-Kuan Wong
- **Comment**: Accepted for publication in ICIP2021. 9 pages, including 3 pages of
  supplemental notes
- **Journal**: None
- **Summary**: Facial expressions vary from the visible to the subtle. In recent years, the analysis of micro-expressions $-$ a natural occurrence resulting from the suppression of one's true emotions, has drawn the attention of researchers with a broad range of potential applications. However, spotting microexpressions in long videos becomes increasingly challenging when intertwined with normal or macro-expressions. In this paper, we propose a shallow optical flow three-stream CNN (SOFTNet) model to predict a score that captures the likelihood of a frame being in an expression interval. By fashioning the spotting task as a regression problem, we introduce pseudo-labeling to facilitate the learning process. We demonstrate the efficacy and efficiency of the proposed approach on the recent MEGC 2020 benchmark, where state-of-the-art performance is achieved on CAS(ME)$^{2}$ with equally promising results on SAMM Long Videos.



### Efficient Deep Learning Architectures for Fast Identification of Bacterial Strains in Resource-Constrained Devices
- **Arxiv ID**: http://arxiv.org/abs/2106.06505v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T07 (Primary), 68U10 (Secondary), I.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2106.06505v1)
- **Published**: 2021-06-11 16:59:22+00:00
- **Updated**: 2021-06-11 16:59:22+00:00
- **Authors**: R. Gallardo García, S. Jarquín Rodríguez, B. Beltrán Martínez, C. Hernández Gracidas, R. Martínez Torres
- **Comment**: 22 pages, 2 figures, 5 tables. Submitted to Multimedia Tools and
  Applications, issue 1218 - Engineering Tools and Applications in Medical
  Imaging (currently in reviewing process)
- **Journal**: None
- **Summary**: This work presents twelve fine-tuned deep learning architectures to solve the bacterial classification problem over the Digital Image of Bacterial Species Dataset. The base architectures were mainly published as mobile or efficient solutions to the ImageNet challenge, and all experiments presented in this work consisted of making several modifications to the original designs, in order to make them able to solve the bacterial classification problem by using fine-tuning and transfer learning techniques. This work also proposes a novel data augmentation technique for this dataset, which is based on the idea of artificial zooming, strongly increasing the performance of every tested architecture, even doubling it in some cases. In order to get robust and complete evaluations, all experiments were performed with 10-fold cross-validation and evaluated with five different metrics: top-1 and top-5 accuracy, precision, recall, and F1 score. This paper presents a complete comparison of the twelve different architectures, cross-validated with the original and the augmented version of the dataset, the results are also compared with several literature methods. Overall, eight of the eleven architectures surpassed the 0.95 scores in top-1 accuracy with our data augmentation method, being 0.9738 the highest top-1 accuracy. The impact of the data augmentation technique is reported with relative improvement scores.



### Step-Wise Hierarchical Alignment Network for Image-Text Matching
- **Arxiv ID**: http://arxiv.org/abs/2106.06509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06509v1)
- **Published**: 2021-06-11 17:05:56+00:00
- **Updated**: 2021-06-11 17:05:56+00:00
- **Authors**: Zhong Ji, Kexin Chen, Haoran Wang
- **Comment**: Accepted by IJCAI 2021
- **Journal**: None
- **Summary**: Image-text matching plays a central role in bridging the semantic gap between vision and language. The key point to achieve precise visual-semantic alignment lies in capturing the fine-grained cross-modal correspondence between image and text. Most previous methods rely on single-step reasoning to discover the visual-semantic interactions, which lacks the ability of exploiting the multi-level information to locate the hierarchical fine-grained relevance. Different from them, in this work, we propose a step-wise hierarchical alignment network (SHAN) that decomposes image-text matching into multi-step cross-modal reasoning process. Specifically, we first achieve local-to-local alignment at fragment level, following by performing global-to-local and global-to-global alignment at context level sequentially. This progressive alignment strategy supplies our model with more complementary and sufficient semantic clues to understand the hierarchical correlations between image and text. The experimental results on two benchmark datasets demonstrate the superiority of our proposed method.



### Recovery of Meteorites Using an Autonomous Drone and Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.06523v1
- **DOI**: 10.1111/maps.13663
- **Categories**: **astro-ph.EP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.06523v1)
- **Published**: 2021-06-11 17:36:33+00:00
- **Updated**: 2021-06-11 17:36:33+00:00
- **Authors**: Robert I. Citron, Peter Jenniskens, Christopher Watkins, Sravanthi Sinha, Amar Shah, Chedy Raissi, Hadrien Devillepoix, Jim Albers
- **Comment**: 16 pages, 9 Figures
- **Journal**: Meteoritics & Planetary Science (2021)
- **Summary**: The recovery of freshly fallen meteorites from tracked and triangulated meteors is critical to determining their source asteroid families. However, locating meteorite fragments in strewn fields remains a challenge with very few meteorites being recovered from the meteors triangulated in past and ongoing meteor camera networks. We examined if locating meteorites can be automated using machine learning and an autonomous drone. Drones can be programmed to fly a grid search pattern and take systematic pictures of the ground over a large survey area. Those images can be analyzed using a machine learning classifier to identify meteorites in the field among many other features. Here, we describe a proof-of-concept meteorite classifier that deploys off-line a combination of different convolution neural networks to recognize meteorites from images taken by drones in the field. The system was implemented in a conceptual drone setup and tested in the suspected strewn field of a recent meteorite fall near Walker Lake, Nevada.



### HR-NAS: Searching Efficient High-Resolution Neural Architectures with Lightweight Transformers
- **Arxiv ID**: http://arxiv.org/abs/2106.06560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06560v1)
- **Published**: 2021-06-11 18:11:36+00:00
- **Updated**: 2021-06-11 18:11:36+00:00
- **Authors**: Mingyu Ding, Xiaochen Lian, Linjie Yang, Peng Wang, Xiaojie Jin, Zhiwu Lu, Ping Luo
- **Comment**: Accepted by CVPR 2021 (Oral)
- **Journal**: None
- **Summary**: High-resolution representations (HR) are essential for dense prediction tasks such as segmentation, detection, and pose estimation. Learning HR representations is typically ignored in previous Neural Architecture Search (NAS) methods that focus on image classification. This work proposes a novel NAS method, called HR-NAS, which is able to find efficient and accurate networks for different tasks, by effectively encoding multiscale contextual information while maintaining high-resolution representations. In HR-NAS, we renovate the NAS search space as well as its searching strategy. To better encode multiscale image contexts in the search space of HR-NAS, we first carefully design a lightweight transformer, whose computational complexity can be dynamically changed with respect to different objective functions and computation budgets. To maintain high-resolution representations of the learned networks, HR-NAS adopts a multi-branch architecture that provides convolutional encoding of multiple feature resolutions, inspired by HRNet. Last, we proposed an efficient fine-grained search strategy to train HR-NAS, which effectively explores the search space, and finds optimal architectures given various tasks and computation resources. HR-NAS is capable of achieving state-of-the-art trade-offs between performance and FLOPs for three dense prediction tasks and an image classification task, given only small computational budgets. For example, HR-NAS surpasses SqueezeNAS that is specially designed for semantic segmentation while improving efficiency by 45.9%. Code is available at https://github.com/dingmyu/HR-NAS



### GANs N' Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!)
- **Arxiv ID**: http://arxiv.org/abs/2106.06561v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.06561v1)
- **Published**: 2021-06-11 18:23:00+00:00
- **Updated**: 2021-06-11 18:23:00+00:00
- **Authors**: Min Jin Chong, David Forsyth
- **Comment**: code is here https://github.com/mchong6/GANsNRoses
- **Journal**: None
- **Summary**: We show how to learn a map that takes a content code, derived from a face image, and a randomly chosen style code to an anime image. We derive an adversarial loss from our simple and effective definitions of style and content. This adversarial loss guarantees the map is diverse -- a very wide range of anime can be produced from a single content code. Under plausible assumptions, the map is not just diverse, but also correctly represents the probability of an anime, conditioned on an input face. In contrast, current multimodal generation procedures cannot capture the complex styles that appear in anime. Extensive quantitative experiments support the idea the map is correct. Extensive qualitative results show that the method can generate a much more diverse range of styles than SOTA comparisons. Finally, we show that our formalization of content and style allows us to perform video to video translation without ever training on videos.



### Federated Learning with Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.06579v1
- **DOI**: 10.1109/TSP.2021.3121632
- **Categories**: **cs.LG**, cs.CV, cs.DC, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2106.06579v1)
- **Published**: 2021-06-11 19:00:58+00:00
- **Updated**: 2021-06-11 19:00:58+00:00
- **Authors**: Yeshwanth Venkatesha, Youngeun Kim, Leandros Tassiulas, Priyadarshini Panda
- **Comment**: None
- **Journal**: None
- **Summary**: As neural networks get widespread adoption in resource-constrained embedded devices, there is a growing need for low-power neural systems. Spiking Neural Networks (SNNs)are emerging to be an energy-efficient alternative to the traditional Artificial Neural Networks (ANNs) which are known to be computationally intensive. From an application perspective, as federated learning involves multiple energy-constrained devices, there is a huge scope to leverage energy efficiency provided by SNNs. Despite its importance, there has been little attention on training SNNs on a large-scale distributed system like federated learning. In this paper, we bring SNNs to a more realistic federated learning scenario. Specifically, we propose a federated learning framework for decentralized and privacy-preserving training of SNNs. To validate the proposed federated learning framework, we experimentally evaluate the advantages of SNNs on various aspects of federated learning with CIFAR10 and CIFAR100 benchmarks. We observe that SNNs outperform ANNs in terms of overall accuracy by over 15% when the data is distributed across a large number of clients in the federation while providing up to5.3x energy efficiency. In addition to efficiency, we also analyze the sensitivity of the proposed federated SNN framework to data distribution among the clients, stragglers, and gradient noise and perform a comprehensive comparison with ANNs.



### Deception Detection and Remote Physiological Monitoring: A Dataset and Baseline Experimental Results
- **Arxiv ID**: http://arxiv.org/abs/2106.06583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06583v1)
- **Published**: 2021-06-11 19:08:23+00:00
- **Updated**: 2021-06-11 19:08:23+00:00
- **Authors**: Jeremy Speth, Nathan Vance, Adam Czajka, Kevin W. Bowyer, Diane Wright, Patrick Flynn
- **Comment**: The dataset will be available for download at
  https://cvrl.nd.edu/projects/data/#deception-detection-and-physiological-monitoringddpm
- **Journal**: None
- **Summary**: We present the Deception Detection and Physiological Monitoring (DDPM) dataset and initial baseline results on this dataset. Our application context is an interview scenario in which the interviewee attempts to deceive the interviewer on selected responses. The interviewee is recorded in RGB, near-infrared, and long-wave infrared, along with cardiac pulse, blood oxygenation, and audio. After collection, data were annotated for interviewer/interviewee, curated, ground-truthed, and organized into train / test parts for a set of canonical deception detection experiments. Baseline experiments found random accuracy for micro-expressions as an indicator of deception, but that saccades can give a statistically significant response. We also estimated subject heart rates from face videos (remotely) with a mean absolute error as low as 3.16 bpm. The database contains almost 13 hours of recordings of 70 subjects, and over 8 million visible-light, near-infrared, and thermal video frames, along with appropriate meta, audio and pulse oximeter data. To our knowledge, this is the only collection offering recordings of five modalities in an interview scenario that can be used in both deception detection and remote photoplethysmography research.



### Diseño y desarrollo de aplicación móvil para la clasificación de flora nativa chilena utilizando redes neuronales convolucionales
- **Arxiv ID**: http://arxiv.org/abs/2106.06592v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06592v2)
- **Published**: 2021-06-11 19:43:47+00:00
- **Updated**: 2021-12-10 13:34:22+00:00
- **Authors**: Ignacio Muñoz, Alfredo Bolt
- **Comment**: in Spanish
- **Journal**: None
- **Summary**: Introduction: Mobile apps, through artificial vision, are capable of recognizing vegetable species in real time. However, the existing species recognition apps do not take in consideration the wide variety of endemic and native (Chilean) species, which leads to wrong species predictions. This study introduces the development of a chilean species dataset and an optimized classification model implemented to a mobile app. Method: the data set was built by putting together pictures of several species captured on the field and by selecting some pictures available from other datasets available online. Convolutional neural networks were used in order to develop the images prediction models. The networks were trained by performing a sensitivity analysis, validating with k-fold cross validation and performing tests with different hyper-parameters, optimizers, convolutional layers, and learning rates in order to identify and choose the best models and then put them together in one classification model. Results: The final data set was compounded by 46 species, including native species, endemic and exotic from Chile, with 6120 training pictures and 655 testing pictures. The best models were implemented on a mobile app, obtaining a 95% correct prediction rate with respect to the set of tests. Conclusion: The app developed in this study is capable of classifying species with a high level of accuracy, depending on the state of the art of the artificial vision and it can also show relevant information related to the classified species.



### Evaluating Deep Neural Networks for Image Document Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2106.15286v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.4.3; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2106.15286v1)
- **Published**: 2021-06-11 19:48:28+00:00
- **Updated**: 2021-06-11 19:48:28+00:00
- **Authors**: Lucas N. Kirsten, Ricardo Piccoli, Ricardo Ribani
- **Comment**: 12 pages, 6 figures, 2 tables, CBDAR conference
- **Journal**: None
- **Summary**: This work evaluates six state-of-the-art deep neural network (DNN) architectures applied to the problem of enhancing camera-captured document images. The results from each network were evaluated both qualitatively and quantitatively using Image Quality Assessment (IQA) metrics, and also compared with an existing approach based on traditional computer vision techniques. The best performing architectures generally produced good enhancement compared to the existing algorithm, showing that it is possible to use DNNs for document image enhancement. Furthermore, the best performing architectures could work as a baseline for future investigations on document enhancement using deep learning techniques. The main contributions of this paper are: a baseline of deep learning techniques that can be further improved to provide better results, and a evaluation methodology using IQA metrics for quantitatively comparing the produced images from the neural networks to a ground truth.



### Toward Accurate and Realistic Outfits Visualization with Attention to Details
- **Arxiv ID**: http://arxiv.org/abs/2106.06593v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.06593v1)
- **Published**: 2021-06-11 19:53:34+00:00
- **Updated**: 2021-06-11 19:53:34+00:00
- **Authors**: Kedan Li, Min jin Chong, Jeffrey Zhang, Jingen Liu
- **Comment**: Accepted to CVPR2021. Live demo here https://revery.ai/demo.html
- **Journal**: None
- **Summary**: Virtual try-on methods aim to generate images of fashion models wearing arbitrary combinations of garments. This is a challenging task because the generated image must appear realistic and accurately display the interaction between garments. Prior works produce images that are filled with artifacts and fail to capture important visual details necessary for commercial applications. We propose Outfit Visualization Net (OVNet) to capture these important details (e.g. buttons, shading, textures, realistic hemlines, and interactions between garments) and produce high quality multiple-garment virtual try-on images. OVNet consists of 1) a semantic layout generator and 2) an image generation pipeline using multiple coordinated warps. We train the warper to output multiple warps using a cascade loss, which refines each successive warp to focus on poorly generated regions of a previous warp and yields consistent improvements in detail. In addition, we introduce a method for matching outfits with the most suitable model and produce significant improvements for both our and other previous try-on methods. Through quantitative and qualitative analysis, we demonstrate our method generates substantially higher-quality studio images compared to prior works for multi-garment outfits. An interactive interface powered by this method has been deployed on fashion e-commerce websites and received overwhelmingly positive feedback.



### Robust Representation Learning via Perceptual Similarity Metrics
- **Arxiv ID**: http://arxiv.org/abs/2106.06620v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.06620v1)
- **Published**: 2021-06-11 21:45:44+00:00
- **Updated**: 2021-06-11 21:45:44+00:00
- **Authors**: Saeid Asgari Taghanaki, Kristy Choi, Amir Khasahmadi, Anirudh Goyal
- **Comment**: Accepted to ICML 2021
- **Journal**: None
- **Summary**: A fundamental challenge in artificial intelligence is learning useful representations of data that yield good performance on a downstream task, without overfitting to spurious input features. Extracting such task-relevant predictive information is particularly difficult for real-world datasets. In this work, we propose Contrastive Input Morphing (CIM), a representation learning framework that learns input-space transformations of the data to mitigate the effect of irrelevant input features on downstream performance. Our method leverages a perceptual similarity metric via a triplet loss to ensure that the transformation preserves task-relevant information.Empirically, we demonstrate the efficacy of our approach on tasks which typically suffer from the presence of spurious correlations: classification with nuisance information, out-of-distribution generalization, and preservation of subgroup accuracies. We additionally show that CIM is complementary to other mutual information-based representation learning techniques, and demonstrate that it improves the performance of variational information bottleneck (VIB) when used together.



### Pay Attention with Focus: A Novel Learning Scheme for Classification of Whole Slide Images
- **Arxiv ID**: http://arxiv.org/abs/2106.06623v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.06623v1)
- **Published**: 2021-06-11 21:59:02+00:00
- **Updated**: 2021-06-11 21:59:02+00:00
- **Authors**: Shivam Kalra, Mohammed Adnan, Sobhan Hemati, Taher Dehkharghanian, Shahryar Rahnamayan, Hamid Tizhoosh
- **Comment**: Accepted in MICCAI, 2021
- **Journal**: None
- **Summary**: Deep learning methods such as convolutional neural networks (CNNs) are difficult to directly utilize to analyze whole slide images (WSIs) due to the large image dimensions. We overcome this limitation by proposing a novel two-stage approach. First, we extract a set of representative patches (called mosaic) from a WSI. Each patch of a mosaic is encoded to a feature vector using a deep network. The feature extractor model is fine-tuned using hierarchical target labels of WSIs, i.e., anatomic site and primary diagnosis. In the second stage, a set of encoded patch-level features from a WSI is used to compute the primary diagnosis probability through the proposed Pay Attention with Focus scheme, an attention-weighted averaging of predicted probabilities for all patches of a mosaic modulated by a trainable focal factor. Experimental results show that the proposed model can be robust, and effective for the classification of WSIs.



### Mirror3D: Depth Refinement for Mirror Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2106.06629v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.06629v1)
- **Published**: 2021-06-11 22:30:12+00:00
- **Updated**: 2021-06-11 22:30:12+00:00
- **Authors**: Jiaqi Tan, Weijie Lin, Angel X. Chang, Manolis Savva
- **Comment**: Paper presented at CVPR 2021. For code, data and pretrained models,
  see https://3dlg-hcvc.github.io/mirror3d/
- **Journal**: None
- **Summary**: Despite recent progress in depth sensing and 3D reconstruction, mirror surfaces are a significant source of errors. To address this problem, we create the Mirror3D dataset: a 3D mirror plane dataset based on three RGBD datasets (Matterport3D, NYUv2 and ScanNet) containing 7,011 mirror instance masks and 3D planes. We then develop Mirror3DNet: a module that refines raw sensor depth or estimated depth to correct errors on mirror surfaces. Our key idea is to estimate the 3D mirror plane based on RGB input and surrounding depth context, and use this estimate to directly regress mirror surface depth. Our experiments show that Mirror3DNet significantly mitigates errors from a variety of input depth data, including raw sensor depth and depth estimation or completion methods.



### CAR-Net: Unsupervised Co-Attention Guided Registration Network for Joint Registration and Structure Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.06637v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.06637v1)
- **Published**: 2021-06-11 23:25:49+00:00
- **Updated**: 2021-06-11 23:25:49+00:00
- **Authors**: Xiang Chen, Yan Xia, Nishant Ravikumar, Alejandro F Frangi
- **Comment**: None
- **Journal**: None
- **Summary**: Image registration is a fundamental building block for various applications in medical image analysis. To better explore the correlation between the fixed and moving images and improve registration performance, we propose a novel deep learning network, Co-Attention guided Registration Network (CAR-Net). CAR-Net employs a co-attention block to learn a new representation of the inputs, which drives the registration of the fixed and moving images. Experiments on UK Biobank cardiac cine-magnetic resonance image data demonstrate that CAR-Net obtains higher registration accuracy and smoother deformation fields than state-of-the-art unsupervised registration methods, while achieving comparable or better registration performance than corresponding weakly-supervised variants. In addition, our approach can provide critical structural information of the input fixed and moving images simultaneously in a completely unsupervised manner.



