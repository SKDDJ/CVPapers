# Arxiv Papers in cs.CV on 2021-06-06
### Self-Damaging Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.02990v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.02990v1)
- **Published**: 2021-06-06 00:04:49+00:00
- **Updated**: 2021-06-06 00:04:49+00:00
- **Authors**: Ziyu Jiang, Tianlong Chen, Bobak Mortazavi, Zhangyang Wang
- **Comment**: ICML 2021
- **Journal**: None
- **Summary**: The recent breakthrough achieved by contrastive learning accelerates the pace for deploying unsupervised training on real-world data applications. However, unlabeled data in reality is commonly imbalanced and shows a long-tail distribution, and it is unclear how robustly the latest contrastive learning methods could perform in the practical scenario. This paper proposes to explicitly tackle this challenge, via a principled framework called Self-Damaging Contrastive Learning (SDCLR), to automatically balance the representation learning without knowing the classes. Our main inspiration is drawn from the recent finding that deep models have difficult-to-memorize samples, and those may be exposed through network pruning. It is further natural to hypothesize that long-tail samples are also tougher for the model to learn well due to insufficient examples. Hence, the key innovation in SDCLR is to create a dynamic self-competitor model to contrast with the target model, which is a pruned version of the latter. During training, contrasting the two models will lead to adaptive online mining of the most easily forgotten samples for the current target model, and implicitly emphasize them more in the contrastive loss. Extensive experiments across multiple datasets and imbalance settings show that SDCLR significantly improves not only overall accuracies but also balancedness, in terms of linear evaluation on the full-shot and few-shot settings. Our code is available at: https://github.com/VITA-Group/SDCLR.



### Learning Topology from Synthetic Data for Unsupervised Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/2106.02994v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.02994v3)
- **Published**: 2021-06-06 00:21:12+00:00
- **Updated**: 2021-08-24 07:35:30+00:00
- **Authors**: Alex Wong, Safa Cicek, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for inferring dense depth maps from images and sparse depth measurements by leveraging synthetic data to learn the association of sparse point clouds with dense natural shapes, and using the image as evidence to validate the predicted depth map. Our learned prior for natural shapes uses only sparse depth as input, not images, so the method is not affected by the covariate shift when attempting to transfer learned models from synthetic data to real ones. This allows us to use abundant synthetic data with ground truth to learn the most difficult component of the reconstruction process, which is topology estimation, and use the image to refine the prediction based on photometric evidence. Our approach uses fewer parameters than previous methods, yet, achieves the state of the art on both indoor and outdoor benchmark datasets. Code available at: https://github.com/alexklwong/learning-topology-synthetic-data.



### An Adaptive Framework for Learning Unsupervised Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/2106.03010v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.03010v2)
- **Published**: 2021-06-06 02:27:55+00:00
- **Updated**: 2021-08-24 07:44:15+00:00
- **Authors**: Alex Wong, Xiaohan Fei, Byung-Woo Hong, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method to infer a dense depth map from a color image and associated sparse depth measurements. Our main contribution lies in the design of an annealing process for determining co-visibility (occlusions, disocclusions) and the degree of regularization to impose on the model. We show that regularization and co-visibility are related via the fitness (residual) of model to data and both can be unified into a single framework to improve the learning process. Our method is an adaptive weighting scheme that guides optimization by measuring the residual at each pixel location over each training step for (i) estimating a soft visibility mask and (ii) determining the amount of regularization. We demonstrate the effectiveness our method by applying it to several recent unsupervised depth completion methods and improving their performance on public benchmark datasets, without incurring additional trainable parameters or increase in inference time. Code available at: https://github.com/alexklwong/adaframe-depth-completion.



### SADRNet: Self-Aligned Dual Face Regression Networks for Robust 3D Dense Face Alignment and Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2106.03021v1
- **DOI**: 10.1109/TIP.2021.3087397
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03021v1)
- **Published**: 2021-06-06 03:31:24+00:00
- **Updated**: 2021-06-06 03:31:24+00:00
- **Authors**: Zeyu Ruan, Changqing Zou, Longhai Wu, Gangshan Wu, Limin Wang
- **Comment**: To appear in IEEE Transactions on Image Processing. Code and model is
  available at https://github.com/MCG-NJU/SADRNet
- **Journal**: None
- **Summary**: Three-dimensional face dense alignment and reconstruction in the wild is a challenging problem as partial facial information is commonly missing in occluded and large pose face images. Large head pose variations also increase the solution space and make the modeling more difficult. Our key idea is to model occlusion and pose to decompose this challenging task into several relatively more manageable subtasks. To this end, we propose an end-to-end framework, termed as Self-aligned Dual face Regression Network (SADRNet), which predicts a pose-dependent face, a pose-independent face. They are combined by an occlusion-aware self-alignment to generate the final 3D face. Extensive experiments on two popular benchmarks, AFLW2000-3D and Florence, demonstrate that the proposed method achieves significant superior performance over existing state-of-the-art methods.



### DAMSL: Domain Agnostic Meta Score-based Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.03041v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03041v1)
- **Published**: 2021-06-06 06:08:05+00:00
- **Updated**: 2021-06-06 06:08:05+00:00
- **Authors**: John Cai, Bill Cai, Shengmei Shen
- **Comment**: Accepted to CVPR 2021 L2ID Workshop
- **Journal**: None
- **Summary**: In this paper, we propose Domain Agnostic Meta Score-based Learning (DAMSL), a novel, versatile and highly effective solution that delivers significant out-performance over state-of-the-art methods for cross-domain few-shot learning. We identify key problems in previous meta-learning methods over-fitting to the source domain, and previous transfer-learning methods under-utilizing the structure of the support set. The core idea behind our method is that instead of directly using the scores from a fine-tuned feature encoder, we use these scores to create input coordinates for a domain agnostic metric space. A graph neural network is applied to learn an embedding and relation function over these coordinates to process all information contained in the score distribution of the support set. We test our model on both established CD-FSL benchmarks and new domains and show that our method overcomes the limitations of previous meta-learning and transfer-learning methods to deliver substantial improvements in accuracy across both smaller and larger domain shifts.



### Occlusion-aware Unsupervised Learning of Depth from 4-D Light Fields
- **Arxiv ID**: http://arxiv.org/abs/2106.03043v2
- **DOI**: 10.1109/TIP.2022.3154288
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03043v2)
- **Published**: 2021-06-06 06:19:50+00:00
- **Updated**: 2022-01-24 03:17:33+00:00
- **Authors**: Jing Jin, Junhui Hou
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: Depth estimation is a fundamental issue in 4-D light field processing and analysis. Although recent supervised learning-based light field depth estimation methods have significantly improved the accuracy and efficiency of traditional optimization-based ones, these methods rely on the training over light field data with ground-truth depth maps which are challenging to obtain or even unavailable for real-world light field data. Besides, due to the inevitable gap (or domain difference) between real-world and synthetic data, they may suffer from serious performance degradation when generalizing the models trained with synthetic data to real-world data. By contrast, we propose an unsupervised learning-based method, which does not require ground-truth depth as supervision during training. Specifically, based on the basic knowledge of the unique geometry structure of light field data, we present an occlusion-aware strategy to improve the accuracy on occlusion areas, in which we explore the angular coherence among subsets of the light field views to estimate initial depth maps, and utilize a constrained unsupervised loss to learn their corresponding reliability for final depth prediction. Additionally, we adopt a multi-scale network with a weighted smoothness loss to handle the textureless areas. Experimental results on synthetic data show that our method can significantly shrink the performance gap between the previous unsupervised method and supervised ones, and produce depth maps with comparable accuracy to traditional methods with obviously reduced computational cost. Moreover, experiments on real-world datasets show that our method can avoid the domain shift problem presented in supervised methods, demonstrating the great potential of our method.



### Brain Age Estimation From MRI Using Cascade Networks with Ranking Loss
- **Arxiv ID**: http://arxiv.org/abs/2106.03052v1
- **DOI**: 10.1109/TMI.2021.3085948
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03052v1)
- **Published**: 2021-06-06 07:11:25+00:00
- **Updated**: 2021-06-06 07:11:25+00:00
- **Authors**: Jian Cheng, Ziyang Liu, Hao Guan, Zhenzhou Wu, Haogang Zhu, Jiyang Jiang, Wei Wen, Dacheng Tao, Tao Liu
- **Comment**: Accepted by IEEE transactions on Medical Imaging, 13 pages, 6 figures
- **Journal**: None
- **Summary**: Chronological age of healthy people is able to be predicted accurately using deep neural networks from neuroimaging data, and the predicted brain age could serve as a biomarker for detecting aging-related diseases. In this paper, a novel 3D convolutional network, called two-stage-age-network (TSAN), is proposed to estimate brain age from T1-weighted MRI data. Compared with existing methods, TSAN has the following improvements. First, TSAN uses a two-stage cascade network architecture, where the first-stage network estimates a rough brain age, then the second-stage network estimates the brain age more accurately from the discretized brain age by the first-stage network. Second, to our knowledge, TSAN is the first work to apply novel ranking losses in brain age estimation, together with the traditional mean square error (MSE) loss. Third, densely connected paths are used to combine feature maps with different scales. The experiments with $6586$ MRIs showed that TSAN could provide accurate brain age estimation, yielding mean absolute error (MAE) of $2.428$ and Pearson's correlation coefficient (PCC) of $0.985$, between the estimated and chronological ages. Furthermore, using the brain age gap between brain age and chronological age as a biomarker, Alzheimer's disease (AD) and Mild Cognitive Impairment (MCI) can be distinguished from healthy control (HC) subjects by support vector machine (SVM). Classification AUC in AD/HC and MCI/HC was $0.904$ and $0.823$, respectively. It showed that brain age gap is an effective biomarker associated with risk of dementia, and has potential for early-stage dementia risk screening. The codes and trained models have been released on GitHub: https://github.com/Milan-BUAA/TSAN-brain-age-estimation.



### Noise Conditional Flow Model for Learning the Super-Resolution Space
- **Arxiv ID**: http://arxiv.org/abs/2106.04428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.04428v1)
- **Published**: 2021-06-06 07:43:52+00:00
- **Updated**: 2021-06-06 07:43:52+00:00
- **Authors**: Younggeun Kim, Donghee Son
- **Comment**: Final CVPR2021 workshop version
- **Journal**: None
- **Summary**: Fundamentally, super-resolution is ill-posed problem because a low-resolution image can be obtained from many high-resolution images. Recent studies for super-resolution cannot create diverse super-resolution images. Although SRFlow tried to account for ill-posed nature of the super-resolution by predicting multiple high-resolution images given a low-resolution image, there is room to improve the diversity and visual quality. In this paper, we propose Noise Conditional flow model for Super-Resolution, NCSR, which increases the visual quality and diversity of images through noise conditional layer. To learn more diverse data distribution, we add noise to training data. However, low-quality images are resulted from adding noise. We propose the noise conditional layer to overcome this phenomenon. The noise conditional layer makes our model generate more diverse images with higher visual quality than other works. Furthermore, we show that this layer can overcome data distribution mismatch, a problem that arises in normalizing flow models. With these benefits, NCSR outperforms baseline in diversity and visual quality and achieves better visual quality than traditional GAN-based models. We also get outperformed scores at NTIRE 2021 challenge.



### Using GANs to Augment Data for Cloud Image Segmentation Task
- **Arxiv ID**: http://arxiv.org/abs/2106.03064v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03064v1)
- **Published**: 2021-06-06 09:01:43+00:00
- **Updated**: 2021-06-06 09:01:43+00:00
- **Authors**: Mayank Jain, Conor Meegan, Soumyabrata Dev
- **Comment**: Published in IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS), 2021
- **Journal**: None
- **Summary**: While cloud/sky image segmentation has extensive real-world applications, a large amount of labelled data is needed to train a highly accurate models to perform the task. Scarcity of such volumes of cloud/sky images with corresponding ground-truth binary maps makes it highly difficult to train such complex image segmentation models. In this paper, we demonstrate the effectiveness of using Generative Adversarial Networks (GANs) to generate data to augment the training set in order to increase the prediction accuracy of image segmentation model. We further present a way to estimate ground-truth binary maps for the GAN-generated images to facilitate their effective use as augmented images. Finally, we validate our work with different statistical techniques.



### Multi-Level Graph Encoding with Structural-Collaborative Relation Learning for Skeleton-Based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2106.03069v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03069v1)
- **Published**: 2021-06-06 09:09:57+00:00
- **Updated**: 2021-06-06 09:09:57+00:00
- **Authors**: Haocong Rao, Shihao Xu, Xiping Hu, Jun Cheng, Bin Hu
- **Comment**: Accepted at IJCAI 2021 Main Track. Sole copyright holder is IJCAI.
  Codes are available at https://github.com/Kali-Hac/MG-SCR
- **Journal**: In IJCAI, 2021
- **Summary**: Skeleton-based person re-identification (Re-ID) is an emerging open topic providing great value for safety-critical applications. Existing methods typically extract hand-crafted features or model skeleton dynamics from the trajectory of body joints, while they rarely explore valuable relation information contained in body structure or motion. To fully explore body relations, we construct graphs to model human skeletons from different levels, and for the first time propose a Multi-level Graph encoding approach with Structural-Collaborative Relation learning (MG-SCR) to encode discriminative graph features for person Re-ID. Specifically, considering that structurally-connected body components are highly correlated in a skeleton, we first propose a multi-head structural relation layer to learn different relations of neighbor body-component nodes in graphs, which helps aggregate key correlative features for effective node representations. Second, inspired by the fact that body-component collaboration in walking usually carries recognizable patterns, we propose a cross-level collaborative relation layer to infer collaboration between different level components, so as to capture more discriminative skeleton graph features. Finally, to enhance graph dynamics encoding, we propose a novel self-supervised sparse sequential prediction task for model pre-training, which facilitates encoding high-level graph semantics for person Re-ID. MG-SCR outperforms state-of-the-art skeleton-based methods, and it achieves superior performance to many multi-modal methods that utilize extra RGB or depth features. Our codes are available at https://github.com/Kali-Hac/MG-SCR.



### Neural Implicit 3D Shapes from Single Images with Spatial Patterns
- **Arxiv ID**: http://arxiv.org/abs/2106.03087v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03087v3)
- **Published**: 2021-06-06 10:35:31+00:00
- **Updated**: 2022-01-31 12:33:36+00:00
- **Authors**: Yixin Zhuang, Yunzhe Liu, Yujie Wang, Baoquan Chen
- **Comment**: 8 pages, 7Mb
- **Journal**: None
- **Summary**: Neural implicit functions have achieved impressive results for reconstructing 3D shapes from single images. However, the image features for describing 3D point samplings of implicit functions are less effective when significant variations of occlusions, views, and appearances exist from the image. To better encode image features, we study a geometry-aware convolutional kernel to leverage geometric relationships of point samplings by the proposed \emph{spatial pattern}, i.e., a structured point set. Specifically, the kernel operates at 2D projections of 3D points from the spatial pattern. Supported by the spatial pattern, the 2D kernel encodes geometric information that is crucial for 3D reconstruction tasks, while traditional ones mainly consider appearance information. Furthermore, to enable the network to discover more adaptive spatial patterns for further capturing non-local contextual information, the kernel is devised to be deformable manipulated by a spatial pattern generator. Experimental results on both synthetic and real datasets demonstrate the superiority of the proposed method. Pre-trained models, codes, and data are available at https://github.com/yixin26/SVR-SP.



### Reducing the feature divergence of RGB and near-infrared images using Switchable Normalization
- **Arxiv ID**: http://arxiv.org/abs/2106.03088v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03088v1)
- **Published**: 2021-06-06 10:48:59+00:00
- **Updated**: 2021-06-06 10:48:59+00:00
- **Authors**: Siwei Yang, Shaozuo Yu, Bingchen Zhao, Yin Wang
- **Comment**: CVPR2020 AgriVision workshop
- **Journal**: None
- **Summary**: Visual pattern recognition over agricultural areas is an important application of aerial image processing. In this paper, we consider the multi-modality nature of agricultural aerial images and show that naively combining different modalities together without taking the feature divergence into account can lead to sub-optimal results. Thus, we apply a Switchable Normalization block to our DeepLabV3 segmentation model to alleviate the feature divergence. Using the popular symmetric Kullback Leibler divergence measure, we show that our model can greatly reduce the divergence between RGB and near-infrared channels. Together with a hybrid loss function, our model achieves nearly 10\% improvements in mean IoU over previously published baseline.



### Referring Transformer: A One-step Approach to Multi-task Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/2106.03089v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03089v2)
- **Published**: 2021-06-06 10:53:39+00:00
- **Updated**: 2021-07-14 12:22:08+00:00
- **Authors**: Muchen Li, Leonid Sigal
- **Comment**: None
- **Journal**: None
- **Summary**: As an important step towards visual reasoning, visual grounding (e.g., phrase localization, referring expression comprehension/segmentation) has been widely explored Previous approaches to referring expression comprehension (REC) or segmentation (RES) either suffer from limited performance, due to a two-stage setup, or require the designing of complex task-specific one-stage architectures. In this paper, we propose a simple one-stage multi-task framework for visual grounding tasks. Specifically, we leverage a transformer architecture, where two modalities are fused in a visual-lingual encoder. In the decoder, the model learns to generate contextualized lingual queries which are then decoded and used to directly regress the bounding box and produce a segmentation mask for the corresponding referred regions. With this simple but highly contextualized model, we outperform state-of-the-arts methods by a large margin on both REC and RES tasks. We also show that a simple pre-training schedule (on an external dataset) further improves the performance. Extensive experiments and ablations illustrate that our model benefits greatly from contextualized information and multi-task training.



### Deep Matching Prior: Test-Time Optimization for Dense Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2106.03090v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03090v4)
- **Published**: 2021-06-06 10:56:01+00:00
- **Updated**: 2021-12-15 09:36:03+00:00
- **Authors**: Sunghwan Hong, Seungryong Kim
- **Comment**: Accepted to ICCV 2021, Camera-ready version. Project page :
  https://sunghwanhong.github.io/DMP/
- **Journal**: 2021 IEEE/CVF International Conference on Computer Vision (ICCV)
- **Summary**: Conventional techniques to establish dense correspondences across visually or semantically similar images focused on designing a task-specific matching prior, which is difficult to model. To overcome this, recent learning-based methods have attempted to learn a good matching prior within a model itself on large training data. The performance improvement was apparent, but the need for sufficient training data and intensive learning hinders their applicability. Moreover, using the fixed model at test time does not account for the fact that a pair of images may require their own prior, thus providing limited performance and poor generalization to unseen images. In this paper, we show that an image pair-specific prior can be captured by solely optimizing the untrained matching networks on an input pair of images. Tailored for such test-time optimization for dense correspondence, we present a residual matching network and a confidence-aware contrastive loss to guarantee a meaningful convergence. Experiments demonstrate that our framework, dubbed Deep Matching Prior (DMP), is competitive, or even outperforms, against the latest learning-based methods on several benchmarks for geometric matching and semantic matching, even though it requires neither large training data nor intensive learning. With the networks pre-trained, DMP attains state-of-the-art performance on all benchmarks.



### Preservation of the Global Knowledge by Not-True Distillation in Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.03097v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03097v5)
- **Published**: 2021-06-06 11:51:47+00:00
- **Updated**: 2022-11-29 15:06:41+00:00
- **Authors**: Gihun Lee, Minchan Jeong, Yongjin Shin, Sangmin Bae, Se-Young Yun
- **Comment**: 36th Conference on Neural Information Processing Systems (NeurIPS
  2022)
- **Journal**: None
- **Summary**: In federated learning, a strong global model is collaboratively learned by aggregating clients' locally trained models. Although this precludes the need to access clients' data directly, the global model's convergence often suffers from data heterogeneity. This study starts from an analogy to continual learning and suggests that forgetting could be the bottleneck of federated learning. We observe that the global model forgets the knowledge from previous rounds, and the local training induces forgetting the knowledge outside of the local distribution. Based on our findings, we hypothesize that tackling down forgetting will relieve the data heterogeneity problem. To this end, we propose a novel and effective algorithm, Federated Not-True Distillation (FedNTD), which preserves the global perspective on locally available data only for the not-true classes. In the experiments, FedNTD shows state-of-the-art performance on various setups without compromising data privacy or incurring additional communication costs.



### Uformer: A General U-Shaped Transformer for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2106.03106v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03106v2)
- **Published**: 2021-06-06 12:33:22+00:00
- **Updated**: 2021-11-25 10:19:05+00:00
- **Authors**: Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, Houqiang Li
- **Comment**: 17 pages, 13 figures
- **Journal**: None
- **Summary**: In this paper, we present Uformer, an effective and efficient Transformer-based architecture for image restoration, in which we build a hierarchical encoder-decoder network using the Transformer block. In Uformer, there are two core designs. First, we introduce a novel locally-enhanced window (LeWin) Transformer block, which performs nonoverlapping window-based self-attention instead of global self-attention. It significantly reduces the computational complexity on high resolution feature map while capturing local context. Second, we propose a learnable multi-scale restoration modulator in the form of a multi-scale spatial bias to adjust features in multiple layers of the Uformer decoder. Our modulator demonstrates superior capability for restoring details for various image restoration tasks while introducing marginal extra parameters and computational cost. Powered by these two designs, Uformer enjoys a high capability for capturing both local and global dependencies for image restoration. To evaluate our approach, extensive experiments are conducted on several image restoration tasks, including image denoising, motion deblurring, defocus deblurring and deraining. Without bells and whistles, our Uformer achieves superior or comparable performance compared with the state-of-the-art algorithms. The code and models are available at https://github.com/ZhendongWang6/Uformer.



### Asymmetric Loss Functions for Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2106.03110v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03110v1)
- **Published**: 2021-06-06 12:52:48+00:00
- **Updated**: 2021-06-06 12:52:48+00:00
- **Authors**: Xiong Zhou, Xianming Liu, Junjun Jiang, Xin Gao, Xiangyang Ji
- **Comment**: ICML 2021
- **Journal**: None
- **Summary**: Robust loss functions are essential for training deep neural networks with better generalization power in the presence of noisy labels. Symmetric loss functions are confirmed to be robust to label noise. However, the symmetric condition is overly restrictive. In this work, we propose a new class of loss functions, namely \textit{asymmetric loss functions}, which are robust to learning with noisy labels for various types of noise. We investigate general theoretical properties of asymmetric loss functions, including classification calibration, excess risk bound, and noise tolerance. Meanwhile, we introduce the asymmetry ratio to measure the asymmetry of a loss function. The empirical results show that a higher ratio would provide better noise tolerance. Moreover, we modify several commonly-used loss functions and establish the necessary and sufficient conditions for them to be asymmetric. Experimental results on benchmark datasets demonstrate that asymmetric loss functions can outperform state-of-the-art methods. The code is available at \href{https://github.com/hitcszx/ALFs}{https://github.com/hitcszx/ALFs}



### Rethinking Training from Scratch for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.03112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03112v1)
- **Published**: 2021-06-06 13:05:57+00:00
- **Updated**: 2021-06-06 13:05:57+00:00
- **Authors**: Yang Li, Hong Zhang, Yu Zhang
- **Comment**: tech reports
- **Journal**: None
- **Summary**: The ImageNet pre-training initialization is the de-facto standard for object detection. He et al. found it is possible to train detector from scratch(random initialization) while needing a longer training schedule with proper normalization technique. In this paper, we explore to directly pre-training on target dataset for object detection. Under this situation, we discover that the widely adopted large resizing strategy e.g. resize image to (1333, 800) is important for fine-tuning but it's not necessary for pre-training. Specifically, we propose a new training pipeline for object detection that follows `pre-training and fine-tuning', utilizing low resolution images within target dataset to pre-training detector then load it to fine-tuning with high resolution images. With this strategy, we can use batch normalization(BN) with large bath size during pre-training, it's also memory efficient that we can apply it on machine with very limited GPU memory(11G). We call it direct detection pre-training, and also use direct pre-training for short. Experiment results show that direct pre-training accelerates the pre-training phase by more than 11x on COCO dataset while with even +1.8mAP compared to ImageNet pre-training. Besides, we found direct pre-training is also applicable to transformer based backbones e.g. Swin Transformer. Code will be available.



### End-to-End Neuro-Symbolic Architecture for Image-to-Image Reasoning Tasks
- **Arxiv ID**: http://arxiv.org/abs/2106.03121v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.03121v1)
- **Published**: 2021-06-06 13:27:33+00:00
- **Updated**: 2021-06-06 13:27:33+00:00
- **Authors**: Ananye Agarwal, Pradeep Shenoy, Mausam
- **Comment**: None
- **Journal**: None
- **Summary**: Neural models and symbolic algorithms have recently been combined for tasks requiring both perception and reasoning. Neural models ground perceptual input into a conceptual vocabulary, on which a classical reasoning algorithm is applied to generate output. A key limitation is that such neural-to-symbolic models can only be trained end-to-end for tasks where the output space is symbolic. In this paper, we study neural-symbolic-neural models for reasoning tasks that require a conversion from an image input (e.g., a partially filled sudoku) to an image output (e.g., the image of the completed sudoku). While designing such a three-step hybrid architecture may be straightforward, the key technical challenge is end-to-end training -- how to backpropagate without intermediate supervision through the symbolic component. We propose NSNnet, an architecture that combines an image reconstruction loss with a novel output encoder to generate a supervisory signal, develops update algorithms that leverage policy gradient methods for supervision, and optimizes loss using a novel subsampling heuristic. We experiment on problem settings where symbolic algorithms are easily specified: a visual maze solving task and a visual Sudoku solver where the supervision is in image form. Experiments show high accuracy with significantly less data compared to purely neural approaches.



### MOC-GAN: Mixing Objects and Captions to Generate Realistic Images
- **Arxiv ID**: http://arxiv.org/abs/2106.03128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03128v1)
- **Published**: 2021-06-06 14:04:07+00:00
- **Updated**: 2021-06-06 14:04:07+00:00
- **Authors**: Tao Ma, Yikang Li
- **Comment**: 9 pages, 3 figures, submitted to NeurIPS 2021
- **Journal**: None
- **Summary**: Generating images with conditional descriptions gains increasing interests in recent years. However, existing conditional inputs are suffering from either unstructured forms (captions) or limited information and expensive labeling (scene graphs). For a targeted scene, the core items, objects, are usually definite while their interactions are flexible and hard to clearly define. Thus, we introduce a more rational setting, generating a realistic image from the objects and captions. Under this setting, objects explicitly define the critical roles in the targeted images and captions implicitly describe their rich attributes and connections. Correspondingly, a MOC-GAN is proposed to mix the inputs of two modalities to generate realistic images. It firstly infers the implicit relations between object pairs from the captions to build a hidden-state scene graph. So a multi-layer representation containing objects, relations and captions is constructed, where the scene graph provides the structures of the scene and the caption provides the image-level guidance. Then a cascaded attentive generative network is designed to coarse-to-fine generate phrase patch by paying attention to the most relevant words in the caption. In addition, a phrase-wise DAMSM is proposed to better supervise the fine-grained phrase-patch consistency. On COCO dataset, our method outperforms the state-of-the-art methods on both Inception Score and FID while maintaining high visual quality. Extensive experiments demonstrate the unique features of our proposed method.



### Go with the Flows: Mixtures of Normalizing Flows for Point Cloud Generation and Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2106.03135v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03135v3)
- **Published**: 2021-06-06 14:25:45+00:00
- **Updated**: 2021-11-29 19:58:12+00:00
- **Authors**: Janis Postels, Mengya Liu, Riccardo Spezialetti, Luc Van Gool, Federico Tombari
- **Comment**: None
- **Journal**: International Conference on 3D Vision 2021
- **Summary**: Recently normalizing flows (NFs) have demonstrated state-of-the-art performance on modeling 3D point clouds while allowing sampling with arbitrary resolution at inference time. However, these flow-based models still require long training times and large models for representing complicated geometries. This work enhances their representational power by applying mixtures of NFs to point clouds. We show that in this more general framework each component learns to specialize in a particular subregion of an object in a completely unsupervised fashion. By instantiating each mixture component with a comparatively small NF we generate point clouds with improved details compared to single-flow-based models while using fewer parameters and considerably reducing the inference runtime. We further demonstrate that by adding data augmentation, individual mixture components can learn to specialize in a semantically meaningful manner. We evaluate mixtures of NFs on generation, autoencoding and single-view reconstruction based on the ShapeNet dataset.



### 3D Convolution Neural Network based Person Identification using Gait cycles
- **Arxiv ID**: http://arxiv.org/abs/2106.03136v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, This paper tells us how human can be identified by their Gait cycle
  using any simple camera
- **Links**: [PDF](http://arxiv.org/pdf/2106.03136v1)
- **Published**: 2021-06-06 14:27:06+00:00
- **Updated**: 2021-06-06 14:27:06+00:00
- **Authors**: Ravi Shekhar Tiwari, Supraja P, Rijo Jackson Tom
- **Comment**: None
- **Journal**: None
- **Summary**: Human identification plays a prominent role in terms of security. In modern times security is becoming the key term for an individual or a country, especially for countries which are facing internal or external threats. Gait analysis is interpreted as the systematic study of the locomotive in humans. It can be used to extract the exact walking features of individuals. Walking features depends on biological as well as the physical feature of the object; hence, it is unique to every individual. In this work, gait features are used to identify an individual. The steps involve object detection, background subtraction, silhouettes extraction, skeletonization, and training 3D Convolution Neural Network on these gait features. The model is trained and evaluated on the dataset acquired by CASIA B Gait, which consists of 15000 videos of 124 subjects walking pattern captured from 11 different angles carrying objects such as bag and coat. The proposed method focuses more on the lower body part to extract features such as the angle between knee and thighs, hip angle, angle of contact, and many other features. The experimental results are compared with amongst accuracies of silhouettes as datasets for training and skeletonized image as training data. The results show that extracting the information from skeletonized data yields improved accuracy.



### CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2106.03143v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03143v3)
- **Published**: 2021-06-06 14:54:55+00:00
- **Updated**: 2021-11-09 03:03:27+00:00
- **Authors**: Tatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, Alex Rogozhnikov
- **Comment**: None
- **Journal**: None
- **Summary**: Without positional information, attention-based Transformer neural networks are permutation-invariant. Absolute or relative positional embeddings are the most popular ways to feed Transformer models with positional information. Absolute positional embeddings are simple to implement, but suffer from generalization issues when evaluating on sequences longer than seen at training time. Relative positions are more robust to input length change, but are more complex to implement and yield inferior model throughput due to extra computational and memory costs. In this paper, we propose an augmentation-based approach (CAPE) for absolute positional embeddings, which keeps the advantages of both absolute (simplicity and speed) and relative positional embeddings (better generalization). In addition, our empirical evaluation on state-of-the-art models in machine translation, image and speech recognition demonstrates that CAPE leads to better generalization performance as well as increased stability with respect to training hyper-parameters.



### Oriented Object Detection with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2106.03146v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03146v1)
- **Published**: 2021-06-06 14:57:17+00:00
- **Updated**: 2021-06-06 14:57:17+00:00
- **Authors**: Teli Ma, Mingyuan Mao, Honghui Zheng, Peng Gao, Xiaodi Wang, Shumin Han, Errui Ding, Baochang Zhang, David Doermann
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection with Transformers (DETR) has achieved a competitive performance over traditional detectors, such as Faster R-CNN. However, the potential of DETR remains largely unexplored for the more challenging task of arbitrary-oriented object detection problem. We provide the first attempt and implement Oriented Object DEtection with TRansformer ($\bf O^2DETR$) based on an end-to-end network. The contributions of $\rm O^2DETR$ include: 1) we provide a new insight into oriented object detection, by applying Transformer to directly and efficiently localize objects without a tedious process of rotated anchors as in conventional detectors; 2) we design a simple but highly efficient encoder for Transformer by replacing the attention mechanism with depthwise separable convolution, which can significantly reduce the memory and computational cost of using multi-scale features in the original Transformer; 3) our $\rm O^2DETR$ can be another new benchmark in the field of oriented object detection, which achieves up to 3.85 mAP improvement over Faster R-CNN and RetinaNet. We simply fine-tune the head mounted on $\rm O^2DETR$ in a cascaded architecture and achieve a competitive performance over SOTA in the DOTA dataset.



### Large-scale Unsupervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.03149v3
- **DOI**: 10.1109/TPAMI.2022.3218275
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03149v3)
- **Published**: 2021-06-06 15:02:11+00:00
- **Updated**: 2022-11-03 12:31:02+00:00
- **Authors**: Shanghua Gao, Zhong-Yu Li, Ming-Hsuan Yang, Ming-Ming Cheng, Junwei Han, Philip Torr
- **Comment**: Benchmark and Source Code: https://github.com/LUSSeg
- **Journal**: IEEE TPAMI 2022
- **Summary**: Empowered by large datasets, e.g., ImageNet, unsupervised learning on large-scale data has enabled significant advances for classification tasks. However, whether the large-scale unsupervised semantic segmentation can be achieved remains unknown. There are two major challenges: i) we need a large-scale benchmark for assessing algorithms; ii) we need to develop methods to simultaneously learn category and shape representation in an unsupervised manner. In this work, we propose a new problem of large-scale unsupervised semantic segmentation (LUSS) with a newly created benchmark dataset to help the research progress. Building on the ImageNet dataset, we propose the ImageNet-S dataset with 1.2 million training images and 50k high-quality semantic segmentation annotations for evaluation. Our benchmark has a high data diversity and a clear task objective. We also present a simple yet effective method that works surprisingly well for LUSS. In addition, we benchmark related un/weakly/fully supervised methods accordingly, identifying the challenges and possible directions of LUSS. The benchmark and source code is publicly available at https://github.com/LUSSeg.



### Technical Report: Temporal Aggregate Representations
- **Arxiv ID**: http://arxiv.org/abs/2106.03152v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03152v2)
- **Published**: 2021-06-06 15:27:47+00:00
- **Updated**: 2021-06-15 07:11:24+00:00
- **Authors**: Fadime Sener, Dibyadip Chatterjee, Angela Yao
- **Comment**: None
- **Journal**: None
- **Summary**: This technical report extends our work presented in [9] with more experiments. In [9], we tackle long-term video understanding, which requires reasoning from current and past or future observations and raises several fundamental questions. How should temporal or sequential relationships be modelled? What temporal extent of information and context needs to be processed? At what temporal scale should they be derived? [9] addresses these questions with a flexible multi-granular temporal aggregation framework. In this report, we conduct further experiments with this framework on different tasks and a new dataset, EPIC-KITCHENS-100.



### Transferring Knowledge from Text to Video: Zero-Shot Anticipation for Procedural Actions
- **Arxiv ID**: http://arxiv.org/abs/2106.03158v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03158v2)
- **Published**: 2021-06-06 15:43:39+00:00
- **Updated**: 2022-11-05 17:45:25+00:00
- **Authors**: Fadime Sener, Rishabh Saraf, Angela Yao
- **Comment**: TPAMI 2022. arXiv admin note: text overlap with arXiv:1812.02501
- **Journal**: None
- **Summary**: Can we teach a robot to recognize and make predictions for activities that it has never seen before? We tackle this problem by learning models for video from text. This paper presents a hierarchical model that generalizes instructional knowledge from large-scale text corpora and transfers the knowledge to video. Given a portion of an instructional video, our model recognizes and predicts coherent and plausible actions multiple steps into the future, all in rich natural language. To demonstrate the capabilities of our model, we introduce the \emph{Tasty Videos Dataset V2}, a collection of 4022 recipes for zero-shot learning, recognition and anticipation. Extensive experiments with various evaluation metrics demonstrate the potential of our method for generalization, given limited video data for training models.



### Transformed ROIs for Capturing Visual Transformations in Videos
- **Arxiv ID**: http://arxiv.org/abs/2106.03162v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03162v2)
- **Published**: 2021-06-06 15:59:53+00:00
- **Updated**: 2022-11-05 17:57:37+00:00
- **Authors**: Abhinav Rai, Fadime Sener, Angela Yao
- **Comment**: CVIU 2022 - Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: Modeling the visual changes that an action brings to a scene is critical for video understanding. Currently, CNNs process one local neighbourhood at a time, thus contextual relationships over longer ranges, while still learnable, are indirect. We present TROI, a plug-and-play module for CNNs to reason between mid-level feature representations that are otherwise separated in space and time. The module relates localized visual entities such as hands and interacting objects and transforms their corresponding regions of interest directly in the feature maps of convolutional layers. With TROI, we achieve state-of-the-art action recognition results on the large-scale datasets Something-Something-V2 and EPIC-Kitchens-100.



### Feature-based Style Randomization for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2106.03171v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03171v2)
- **Published**: 2021-06-06 16:34:44+00:00
- **Updated**: 2022-02-16 13:54:26+00:00
- **Authors**: Yue Wang, Lei Qi, Yinghuan Shi, Yang Gao
- **Comment**: To appear in IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT)
- **Journal**: None
- **Summary**: As a recent noticeable topic, domain generalization (DG) aims to first learn a generic model on multiple source domains and then directly generalize to an arbitrary unseen target domain without any additional adaption. In previous DG models, by generating virtual data to supplement observed source domains, the data augmentation based methods have shown its effectiveness. To simulate the possible unseen domains, most of them enrich the diversity of original data via image-level style transformation. However, we argue that the potential styles are hard to be exhaustively illustrated and fully augmented due to the limited referred styles, leading the diversity could not be always guaranteed. Unlike image-level augmentation, we in this paper develop a simple yet effective feature-based style randomization module to achieve feature-level augmentation, which can produce random styles via integrating random noise into the original style. Compared with existing image-level augmentation, our feature-level augmentation favors a more goal-oriented and sample-diverse way. Furthermore, to sufficiently explore the efficacy of the proposed module, we design a novel progressive training strategy to enable all parameters of the network to be fully trained. Extensive experiments on three standard benchmark datasets, i.e., PACS, VLCS and Office-Home, highlight the superiority of our method compared to the state-of-the-art methods.



### Vision Transformers with Hierarchical Attention
- **Arxiv ID**: http://arxiv.org/abs/2106.03180v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03180v3)
- **Published**: 2021-06-06 17:01:13+00:00
- **Updated**: 2022-06-15 15:15:28+00:00
- **Authors**: Yun Liu, Yu-Huan Wu, Guolei Sun, Le Zhang, Ajad Chhatkuli, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: This paper tackles the low-efficiency flaw of the vision transformer caused by the high computational/space complexity in Multi-Head Self-Attention (MHSA). To this end, we propose the Hierarchical MHSA (H-MHSA), whose representation is computed in a hierarchical manner. Specifically, we first divide the input image into patches as commonly done, and each patch is viewed as a token. Then, the proposed H-MHSA learns token relationships within local patches, serving as local relationship modeling. Then, the small patches are merged into larger ones, and H-MHSA models the global dependencies for the small number of the merged tokens. At last, the local and global attentive features are aggregated to obtain features with powerful representation capacity. Since we only calculate attention for a limited number of tokens at each step, the computational load is reduced dramatically. Hence, H-MHSA can efficiently model global relationships among tokens without sacrificing fine-grained information. With the H-MHSA module incorporated, we build a family of Hierarchical-Attention-based Transformer Networks, namely HAT-Net. To demonstrate the superiority of HAT-Net in scene understanding, we conduct extensive experiments on fundamental vision tasks, including image classification, semantic segmentation, object detection, and instance segmentation. Therefore, HAT-Net provides a new perspective for the vision transformer. Code and pretrained models are available at https://github.com/yun-liu/HAT-Net.



### Combinatorial Optimization for Panoptic Segmentation: A Fully Differentiable Approach
- **Arxiv ID**: http://arxiv.org/abs/2106.03188v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03188v3)
- **Published**: 2021-06-06 17:39:13+00:00
- **Updated**: 2021-10-25 10:12:09+00:00
- **Authors**: Ahmed Abbas, Paul Swoboda
- **Comment**: To be presented at NeurIPS 2021
- **Journal**: None
- **Summary**: We propose a fully differentiable architecture for simultaneous semantic and instance segmentation (a.k.a. panoptic segmentation) consisting of a convolutional neural network and an asymmetric multiway cut problem solver. The latter solves a combinatorial optimization problem that elegantly incorporates semantic and boundary predictions to produce a panoptic labeling. Our formulation allows to directly maximize a smooth surrogate of the panoptic quality metric by backpropagating the gradient through the optimization problem. Experimental evaluation shows improvement by backpropagating through the optimization problem w.r.t. comparable approaches on Cityscapes and COCO datasets. Overall, our approach shows the utility of using combinatorial optimization in tandem with deep learning in a challenging large scale real-world problem and showcases benefits and insights into training such an architecture.



### Alpha Matte Generation from Single Input for Portrait Matting
- **Arxiv ID**: http://arxiv.org/abs/2106.03210v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03210v3)
- **Published**: 2021-06-06 18:53:42+00:00
- **Updated**: 2022-04-25 17:11:51+00:00
- **Authors**: Dogucan Yaman, Hazım Kemal Ekenel, Alexander Waibel
- **Comment**: Accepted for CVPR 2022 NTIRE Workshop
- **Journal**: None
- **Summary**: In the portrait matting, the goal is to predict an alpha matte that identifies the effect of each pixel on the foreground subject. Traditional approaches and most of the existing works utilized an additional input, e.g., trimap, background image, to predict alpha matte. However, (1) providing additional input is not always practical, and (2) models are too sensitive to these additional inputs. To address these points, in this paper, we introduce an additional input-free approach to perform portrait matting. We divide the task into two subtasks, segmentation and alpha matte prediction. We first generate a coarse segmentation map from the input image and then predict the alpha matte by utilizing the image and segmentation map. Besides, we present a segmentation encoding block to downsample the coarse segmentation map and provide useful feature representation to the residual block, since using a single encoder causes the vanishing of the segmentation information. We tested our model on four different benchmark datasets. The proposed method outperformed the MODNet and MGMatting methods that also take a single input. Besides, we obtained comparable results with BGM-V2 and FBA methods that require additional input.



### Meta-learning with implicit gradients in a few-shot setting for medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.03223v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.03223v2)
- **Published**: 2021-06-06 19:52:06+00:00
- **Updated**: 2022-01-31 03:42:44+00:00
- **Authors**: Rabindra Khadga, Debesh Jha, Steven Hicks, Vajira Thambawita, Michael A. Riegler, Sharib Ali, Pål Halvorsen
- **Comment**: None
- **Journal**: Computers in Biology and Medicine, 2022
- **Summary**: Widely used traditional supervised deep learning methods require a large number of training samples but often fail to generalize on unseen datasets. Therefore, a more general application of any trained model is quite limited for medical imaging for clinical practice. Using separately trained models for each unique lesion category or a unique patient population will require sufficiently large curated datasets, which is not practical to use in a real-world clinical set-up. Few-shot learning approaches can not only minimize the need for an enormous number of reliable ground truth labels that are labour-intensive and expensive but can also be used to model on a dataset coming from a new population. To this end, we propose to exploit an optimization-based implicit model agnostic meta-learning (iMAML) algorithm under few-shot settings for medical image segmentation. Our approach can leverage the learned weights from diverse but small training samples to perform analysis on unseen datasets with high accuracy. We show that, unlike classical few-shot learning approaches, our method improves generalization capability. To our knowledge, this is the first work that exploits iMAML for medical image segmentation and explores the strength of the model on scenarios such as meta-training on unique and mixed instances of lesion datasets. Our quantitative results on publicly available skin and polyp datasets show that the proposed method outperforms the naive supervised baseline model and two recent few-shot segmentation approaches by large margins. In addition, our iMAML approach shows an improvement of 2%-4% in dice score compared to its counterpart MAML for most experiments.



### Efficient Lottery Ticket Finding: Less Data is More
- **Arxiv ID**: http://arxiv.org/abs/2106.03225v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03225v1)
- **Published**: 2021-06-06 19:58:17+00:00
- **Updated**: 2021-06-06 19:58:17+00:00
- **Authors**: Zhenyu Zhang, Xuxi Chen, Tianlong Chen, Zhangyang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The lottery ticket hypothesis (LTH) reveals the existence of winning tickets (sparse but critical subnetworks) for dense networks, that can be trained in isolation from random initialization to match the latter's accuracies. However, finding winning tickets requires burdensome computations in the train-prune-retrain process, especially on large-scale datasets (e.g., ImageNet), restricting their practical benefits. This paper explores a new perspective on finding lottery tickets more efficiently, by doing so only with a specially selected subset of data, called Pruning-Aware Critical set (PrAC set), rather than using the full training set. The concept of PrAC set was inspired by the recent observation, that deep networks have samples that are either hard to memorize during training, or easy to forget during pruning. A PrAC set is thus hypothesized to capture those most challenging and informative examples for the dense model. We observe that a high-quality winning ticket can be found with training and pruning the dense network on the very compact PrAC set, which can substantially save training iterations for the ticket finding process. Extensive experiments validate our proposal across diverse datasets and network architectures. Specifically, on CIFAR-10, CIFAR-100, and Tiny ImageNet, we locate effective PrAC sets at 35.32%~78.19% of their training set sizes. On top of them, we can obtain the same competitive winning tickets for the corresponding dense networks, yet saving up to 82.85%~92.77%, 63.54%~74.92%, and 76.14%~86.56% training iterations, respectively. Crucially, we show that a PrAC set found is reusable across different network architectures, which can amortize the extra cost of finding PrAC sets, yielding a practical regime for efficient lottery ticket finding.



### Highlighting the Importance of Reducing Research Bias and Carbon Emissions in CNNs
- **Arxiv ID**: http://arxiv.org/abs/2106.03242v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.03242v1)
- **Published**: 2021-06-06 20:42:00+00:00
- **Updated**: 2021-06-06 20:42:00+00:00
- **Authors**: Ahmed Badar, Arnav Varma, Adrian Staniec, Mahmoud Gamal, Omar Magdy, Haris Iqbal, Elahe Arani, Bahram Zonooz
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have become commonplace in addressing major challenges in computer vision. Researchers are not only coming up with new CNN architectures but are also researching different techniques to improve the performance of existing architectures. However, there is a tendency to over-emphasize performance improvement while neglecting certain important variables such as simplicity, versatility, the fairness of comparisons, and energy efficiency. Overlooking these variables in architectural design and evaluation has led to research bias and a significantly negative environmental impact. Furthermore, this can undermine the positive impact of research in using deep learning models to tackle climate change. Here, we perform an extensive and fair empirical study of a number of proposed techniques to gauge the utility of each technique for segmentation and classification. Our findings restate the importance of favoring simplicity over complexity in model design (Occam's Razor). Furthermore, our results indicate that simple standardized practices can lead to a significant reduction in environmental impact with little drop in performance. We highlight that there is a need to rethink the design and evaluation of CNNs to alleviate the issue of research bias and carbon emissions.



### Understand and Improve Contrastive Learning Methods for Visual Representation: A Review
- **Arxiv ID**: http://arxiv.org/abs/2106.03259v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.03259v1)
- **Published**: 2021-06-06 21:59:49+00:00
- **Updated**: 2021-06-06 21:59:49+00:00
- **Authors**: Ran Liu
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Traditional supervised learning methods are hitting a bottleneck because of their dependency on expensive manually labeled data and their weaknesses such as limited generalization ability and vulnerability to adversarial attacks. A promising alternative, self-supervised learning, as a type of unsupervised learning, has gained popularity because of its potential to learn effective data representations without manual labeling. Among self-supervised learning algorithms, contrastive learning has achieved state-of-the-art performance in several fields of research. This literature review aims to provide an up-to-date analysis of the efforts of researchers to understand the key components and the limitations of self-supervised learning.



### Towards Fast and Accurate Multi-Person Pose Estimation on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/2106.15304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15304v1)
- **Published**: 2021-06-06 22:39:40+00:00
- **Updated**: 2021-06-06 22:39:40+00:00
- **Authors**: Xuan Shen, Geng Yuan, Wei Niu, Xiaolong Ma, Jiexiong Guan, Zhengang Li, Bin Ren, Yanzhi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The rapid development of autonomous driving, abnormal behavior detection, and behavior recognition makes an increasing demand for multi-person pose estimation-based applications, especially on mobile platforms. However, to achieve high accuracy, state-of-the-art methods tend to have a large model size and complex post-processing algorithm, which costs intense computation and long end-to-end latency. To solve this problem, we propose an architecture optimization and weight pruning framework to accelerate inference of multi-person pose estimation on mobile devices. With our optimization framework, we achieve up to 2.51x faster model inference speed with higher accuracy compared to representative lightweight multi-person pose estimator.



