# Arxiv Papers in cs.CV on 2021-06-01
### Integrative Use of Computer Vision and Unmanned Aircraft Technologies in Public Inspection: Foreign Object Debris Image Collection
- **Arxiv ID**: http://arxiv.org/abs/2106.00161v1
- **DOI**: 10.1145/3463677.3463743
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00161v1)
- **Published**: 2021-06-01 00:45:32+00:00
- **Updated**: 2021-06-01 00:45:32+00:00
- **Authors**: Travis J. E. Munyer, Daniel Brinkman, Chenyu Huang, Xin Zhong
- **Comment**: This paper has been accepted for publication by the 22nd Annual
  International Conference on Digital Government Research. 7 pages, 1 figure, 1
  table
- **Journal**: None
- **Summary**: Unmanned Aircraft Systems (UAS) have become an important resource for public service providers and smart cities. The purpose of this study is to expand this research area by integrating computer vision and UAS technology to automate public inspection. As an initial case study for this work, a dataset of common foreign object debris (FOD) is developed to assess the potential of light-weight automated detection. This paper presents the rationale and creation of this dataset. Future iterations of our work will include further technical details analyzing experimental implementation. At a local airport, UAS and portable cameras are used to collect the data contained in the initial version of this dataset. After collecting these videos of FOD, they were split into individual frames and stored as several thousand images. These frames are then annotated following standard computer vision format and stored in a folder-structure that reflects our creation method. The dataset annotations are validated using a custom tool that could be abstracted to fit future applications. Initial detection models were successfully created using the famous You Only Look Once algorithm, which indicates the practicality of the proposed data. Finally, several potential scenarios that could utilize either this dataset or similar methods for other public service are presented.



### Rethinking Pseudo Labels for Semi-Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.00168v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00168v2)
- **Published**: 2021-06-01 01:32:03+00:00
- **Updated**: 2021-12-30 02:33:52+00:00
- **Authors**: Hengduo Li, Zuxuan Wu, Abhinav Shrivastava, Larry S. Davis
- **Comment**: AAAI 2022
- **Journal**: None
- **Summary**: Recent advances in semi-supervised object detection (SSOD) are largely driven by consistency-based pseudo-labeling methods for image classification tasks, producing pseudo labels as supervisory signals. However, when using pseudo labels, there is a lack of consideration in localization precision and amplified class imbalance, both of which are critical for detection tasks. In this paper, we introduce certainty-aware pseudo labels tailored for object detection, which can effectively estimate the classification and localization quality of derived pseudo labels. This is achieved by converting conventional localization as a classification task followed by refinement. Conditioned on classification and localization quality scores, we dynamically adjust the thresholds used to generate pseudo labels and reweight loss functions for each category to alleviate the class imbalance problem. Extensive experiments demonstrate that our method improves state-of-the-art SSOD performance by 1-2% AP on COCO and PASCAL VOC while being orthogonal and complementary to most existing methods. In the limited-annotation regime, our approach improves supervised baselines by up to 10% AP using only 1-10% labeled data from COCO.



### Language-Driven Image Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2106.00178v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00178v3)
- **Published**: 2021-06-01 01:58:50+00:00
- **Updated**: 2022-07-17 22:53:37+00:00
- **Authors**: Tsu-Jui Fu, Xin Eric Wang, William Yang Wang
- **Comment**: ECCV'22
- **Journal**: None
- **Summary**: Despite having promising results, style transfer, which requires preparing style images in advance, may result in lack of creativity and accessibility. Following human instruction, on the other hand, is the most natural way to perform artistic style transfer that can significantly improve controllability for visual effect applications. We introduce a new task, language-driven artistic style transfer (LDAST), to manipulate the style of a content image, guided by a text. We propose contrastive language visual artist (CLVA) that learns to extract visual semantics from style instructions and accomplish LDAST by the patch-wise style discriminator. The discriminator considers the correlation between language and patches of style images or transferred results to jointly embed style instructions. CLVA further compares contrastive pairs of content images and style instructions to improve the mutual relativeness. The results from the same content image can preserve consistent content structures. Besides, they should present analogous style patterns from style instructions that contain similar visual semantics. The experiments show that our CLVA is effective and achieves superb transferred results on LDAST.



### Dual Normalization Multitasking for Audio-Visual Sounding Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2106.00180v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2106.00180v1)
- **Published**: 2021-06-01 02:02:52+00:00
- **Updated**: 2021-06-01 02:02:52+00:00
- **Authors**: Tokuhiro Nishikawa, Daiki Shimada, Jerry Jun Yokono
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Although several research works have been reported on audio-visual sound source localization in unconstrained videos, no datasets and metrics have been proposed in the literature to quantitatively evaluate its performance. Defining the ground truth for sound source localization is difficult, because the location where the sound is produced is not limited to the range of the source object, but the vibrations propagate and spread through the surrounding objects. Therefore we propose a new concept, Sounding Object, to reduce the ambiguity of the visual location of sound, making it possible to annotate the location of the wide range of sound sources. With newly proposed metrics for quantitative evaluation, we formulate the problem of Audio-Visual Sounding Object Localization (AVSOL). We also created the evaluation dataset (AVSOL-E dataset) by manually annotating the test set of well-known Audio-Visual Event (AVE) dataset. To tackle this new AVSOL problem, we propose a novel multitask training strategy and architecture called Dual Normalization Multitasking (DNM), which aggregates the Audio-Visual Correspondence (AVC) task and the classification task for video events into a single audio-visual similarity map. By efficiently utilize both supervisions by DNM, our proposed architecture significantly outperforms the baseline methods.



### Quantification of Carbon Sequestration in Urban Forests
- **Arxiv ID**: http://arxiv.org/abs/2106.00182v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00182v2)
- **Published**: 2021-06-01 02:15:20+00:00
- **Updated**: 2021-07-20 13:53:34+00:00
- **Authors**: Levente J. Klein, Wang Zhou, Conrad M. Albrecht
- **Comment**: None
- **Journal**: International Conference on Machine Learning (ICML 2021) Workshop
- **Summary**: Vegetation, trees in particular, sequester carbon by absorbing carbon dioxide from the atmosphere. However, the lack of efficient quantification methods of carbon stored in trees renders it difficult to track the process. We present an approach to estimate the carbon storage in trees based on fusing multi-spectral aerial imagery and LiDAR data to identify tree coverage, geometric shape, and tree species -- key attributes to carbon storage quantification. We demonstrate that tree species information and their three-dimensional geometric shapes can be estimated from aerial imagery in order to determine the tree's biomass. Specifically, we estimate a total of $52,000$ tons of carbon sequestered in trees for New York City's borough Manhattan.



### Anti-aliasing Semantic Reconstruction for Few-Shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.00184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00184v1)
- **Published**: 2021-06-01 02:17:36+00:00
- **Updated**: 2021-06-01 02:17:36+00:00
- **Authors**: Binghao Liu, Yao Ding, Jianbin Jiao, Xiangyang Ji, Qixiang Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Encouraging progress in few-shot semantic segmentation has been made by leveraging features learned upon base classes with sufficient training data to represent novel classes with few-shot examples. However, this feature sharing mechanism inevitably causes semantic aliasing between novel classes when they have similar compositions of semantic concepts. In this paper, we reformulate few-shot segmentation as a semantic reconstruction problem, and convert base class features into a series of basis vectors which span a class-level semantic space for novel class reconstruction. By introducing contrastive loss, we maximize the orthogonality of basis vectors while minimizing semantic aliasing between classes. Within the reconstructed representation space, we further suppress interference from other classes by projecting query features to the support vector for precise semantic activation. Our proposed approach, referred to as anti-aliasing semantic reconstruction (ASR), provides a systematic yet interpretable solution for few-shot learning problems. Extensive experiments on PASCAL VOC and MS COCO datasets show that ASR achieves strong results compared with the prior works.



### Towards Light-weight and Real-time Line Segment Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.00186v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.00186v3)
- **Published**: 2021-06-01 02:28:08+00:00
- **Updated**: 2022-04-26 04:43:45+00:00
- **Authors**: Geonmo Gu, Byungsoo Ko, SeoungHyun Go, Sung-Hyun Lee, Jingeun Lee, Minchul Shin
- **Comment**: Accepted by AAAI2022
- **Journal**: None
- **Summary**: Previous deep learning-based line segment detection (LSD) suffers from the immense model size and high computational cost for line prediction. This constrains them from real-time inference on computationally restricted environments. In this paper, we propose a real-time and light-weight line segment detector for resource-constrained environments named Mobile LSD (M-LSD). We design an extremely efficient LSD architecture by minimizing the backbone network and removing the typical multi-module process for line prediction found in previous methods. To maintain competitive performance with a light-weight network, we present novel training schemes: Segments of Line segment (SoL) augmentation, matching and geometric loss. SoL augmentation splits a line segment into multiple subparts, which are used to provide auxiliary line data during the training process. Moreover, the matching and geometric loss allow a model to capture additional geometric cues. Compared with TP-LSD-Lite, previously the best real-time LSD method, our model (M-LSD-tiny) achieves competitive performance with 2.5% of model size and an increase of 130.5% in inference speed on GPU. Furthermore, our model runs at 56.8 FPS and 48.6 FPS on the latest Android and iPhone mobile devices, respectively. To the best of our knowledge, this is the first real-time deep LSD available on mobile devices. Our code is available.



### Rethinking Re-Sampling in Imbalanced Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.00209v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00209v2)
- **Published**: 2021-06-01 03:58:18+00:00
- **Updated**: 2021-12-10 20:28:44+00:00
- **Authors**: Ju He, Adam Kortylewski, Shaokang Yang, Shuai Liu, Cheng Yang, Changhu Wang, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-Supervised Learning (SSL) has shown its strong ability in utilizing unlabeled data when labeled data is scarce. However, most SSL algorithms work under the assumption that the class distributions are balanced in both training and test sets. In this work, we consider the problem of SSL on class-imbalanced data, which better reflects real-world situations. In particular, we decouple the training of the representation and the classifier, and systematically investigate the effects of different data re-sampling techniques when training the whole network including a classifier as well as fine-tuning the feature extractor only. We find that data re-sampling is of critical importance to learn a good classifier as it increases the accuracy of the pseudo-labels, in particular for the minority classes in the unlabeled data. Interestingly, we find that accurate pseudo-labels do not help when training the feature extractor, rather contrariwise, data re-sampling harms the training of the feature extractor. This finding is against the general intuition that wrong pseudo-labels always harm the model performance in SSL. Based on these findings, we suggest to re-think the current paradigm of having a single data re-sampling strategy and develop a simple yet highly effective Bi-Sampling (BiS) strategy for SSL on class-imbalanced data. BiS implements two different re-sampling strategies for training the feature extractor and the classifier and integrates this decoupled training into an end-to-end framework. In particular, BiS progressively changes the data distribution during training such that in the beginning the feature extractor is trained effectively, while towards the end of the training the data is re-balanced such that the classifier is trained reliably. We benchmark our proposed bi-sampling strategy extensively on popular datasets and achieve state-of-the-art performances.



### A Voxel Graph CNN for Object Classification with Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/2106.00216v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00216v3)
- **Published**: 2021-06-01 04:07:03+00:00
- **Updated**: 2022-04-08 05:06:06+00:00
- **Authors**: Yongjian Deng, Hao Chen, Hai Liu, Youfu Li
- **Comment**: 10 pages, 5 Figures
- **Journal**: None
- **Summary**: Event cameras attract researchers' attention due to their low power consumption, high dynamic range, and extremely high temporal resolution. Learning models on event-based object classification have recently achieved massive success by accumulating sparse events into dense frames to apply traditional 2D learning methods. Yet, these approaches necessitate heavy-weight models and are with high computational complexity due to the redundant information introduced by the sparse-to-dense conversion, limiting the potential of event cameras on real-life applications. This study aims to address the core problem of balancing accuracy and model complexity for event-based classification models. To this end, we introduce a novel graph representation for event data to exploit their sparsity better and customize a lightweight voxel graph convolutional neural network (\textit{EV-VGCNN}) for event-based classification. Specifically, (1) using voxel-wise vertices rather than previous point-wise inputs to explicitly exploit regional 2D semantics of event streams while keeping the sparsity;(2) proposing a multi-scale feature relational layer (\textit{MFRL}) to extract spatial and motion cues from each vertex discriminatively concerning its distances to neighbors. Comprehensive experiments show that our model can advance state-of-the-art classification accuracy with extremely low model complexity (merely 0.84M parameters).



### VA-GCN: A Vector Attention Graph Convolution Network for learning on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2106.00227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00227v1)
- **Published**: 2021-06-01 04:49:25+00:00
- **Updated**: 2021-06-01 04:49:25+00:00
- **Authors**: Haotian Hu, Fanyi Wang, Huixiao Le
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Owing to the development of research on local aggregation operators, dramatic breakthrough has been made in point cloud analysis models. However, existing local aggregation operators in the current literature fail to attach decent importance to the local information of the point cloud, which limits the power of the models. To fit this gap, we propose an efficient Vector Attention Convolution module (VAConv), which utilizes K-Nearest Neighbor (KNN) to extract the neighbor points of each input point, and then uses the elevation and azimuth relationship of the vectors between the center point and its neighbors to construct an attention weight matrix for edge features. Afterwards, the VAConv adopts a dual-channel structure to fuse weighted edge features and global features. To verify the efficiency of the VAConv, we connect the VAConvs with different receptive fields in parallel to obtain a Multi-scale graph convolutional network, VA-GCN. The proposed VA-GCN achieves state-of-the-art performance on standard benchmarks including ModelNet40, S3DIS and ShapeNet. Remarkably, on the ModelNet40 dataset for 3D classification, VA-GCN increased by 2.4% compared to the baseline.



### Volta at SemEval-2021 Task 6: Towards Detecting Persuasive Texts and Images using Textual and Multimodal Ensemble
- **Arxiv ID**: http://arxiv.org/abs/2106.00240v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00240v1)
- **Published**: 2021-06-01 05:41:03+00:00
- **Updated**: 2021-06-01 05:41:03+00:00
- **Authors**: Kshitij Gupta, Devansh Gautam, Radhika Mamidi
- **Comment**: 7 pages, accepted at SemEval-2021 co-located with ACL-IJCNLP 2021
- **Journal**: None
- **Summary**: Memes are one of the most popular types of content used to spread information online. They can influence a large number of people through rhetorical and psychological techniques. The task, Detection of Persuasion Techniques in Texts and Images, is to detect these persuasive techniques in memes. It consists of three subtasks: (A) Multi-label classification using textual content, (B) Multi-label classification and span identification using textual content, and (C) Multi-label classification using visual and textual content. In this paper, we propose a transfer learning approach to fine-tune BERT-based models in different modalities. We also explore the effectiveness of ensembles of models trained in different modalities. We achieve an F1-score of 57.0, 48.2, and 52.1 in the corresponding subtasks.



### Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models
- **Arxiv ID**: http://arxiv.org/abs/2106.00245v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2106.00245v2)
- **Published**: 2021-06-01 05:54:41+00:00
- **Updated**: 2021-08-13 07:01:48+00:00
- **Authors**: Linjie Li, Jie Lei, Zhe Gan, Jingjing Liu
- **Comment**: To appear in ICCV 2021; Website: https://adversarialvqa.github.io/
- **Journal**: None
- **Summary**: Benefiting from large-scale pre-training, we have witnessed significant performance boost on the popular Visual Question Answering (VQA) task. Despite rapid progress, it remains unclear whether these state-of-the-art (SOTA) models are robust when encountering examples in the wild. To study this, we introduce Adversarial VQA, a new large-scale VQA benchmark, collected iteratively via an adversarial human-and-model-in-the-loop procedure. Through this new benchmark, we discover several interesting findings. (i) Surprisingly, we find that during dataset collection, non-expert annotators can easily attack SOTA VQA models successfully. (ii) Both large-scale pre-trained models and adversarial training methods achieve far worse performance on the new benchmark than over standard VQA v2 dataset, revealing the fragility of these models while demonstrating the effectiveness of our adversarial dataset. (iii) When used for data augmentation, our dataset can effectively boost model performance on other robust VQA benchmarks. We hope our Adversarial VQA dataset can shed new light on robustness study in the community and serve as a valuable benchmark for future work.



### ViTA: Visual-Linguistic Translation by Aligning Object Tags
- **Arxiv ID**: http://arxiv.org/abs/2106.00250v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00250v3)
- **Published**: 2021-06-01 06:19:29+00:00
- **Updated**: 2021-06-28 10:44:44+00:00
- **Authors**: Kshitij Gupta, Devansh Gautam, Radhika Mamidi
- **Comment**: 7 pages, accepted at WAT-2021 co-located with ACL-IJCNLP 2021
- **Journal**: None
- **Summary**: Multimodal Machine Translation (MMT) enriches the source text with visual information for translation. It has gained popularity in recent years, and several pipelines have been proposed in the same direction. Yet, the task lacks quality datasets to illustrate the contribution of visual modality in the translation systems. In this paper, we propose our system under the team name Volta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We also participate in the textual-only subtask of the same language pair for which we use mBART, a pretrained multilingual sequence-to-sequence model. For multimodal translation, we propose to enhance the textual input by bringing the visual information to a textual domain by extracting object tags from the image. We also explore the robustness of our system by systematically degrading the source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test set and challenge set of the multimodal task.



### Reconciliation of Statistical and Spatial Sparsity For Robust Image and Image-Set Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.00256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00256v1)
- **Published**: 2021-06-01 06:33:24+00:00
- **Updated**: 2021-06-01 06:33:24+00:00
- **Authors**: Hao Cheng, Kim-Hui Yap, Bihan Wen
- **Comment**: Submitted to IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Recent image classification algorithms, by learning deep features from large-scale datasets, have achieved significantly better results comparing to the classic feature-based approaches. However, there are still various challenges of image classifications in practice, such as classifying noisy image or image-set queries and training deep image classification models over the limited-scale dataset. Instead of applying generic deep features, the model-based approaches can be more effective and data-efficient for robust image and image-set classification tasks, as various image priors are exploited for modeling the inter- and intra-set data variations while preventing over-fitting. In this work, we propose a novel Joint Statistical and Spatial Sparse representation, dubbed \textit{J3S}, to model the image or image-set data for classification, by reconciling both their local patch structures and global Gaussian distribution mapped into Riemannian manifold. To the best of our knowledge, no work to date utilized both global statistics and local patch structures jointly via joint sparse representation. We propose to solve the joint sparse coding problem based on the J3S model, by coupling the local and global image representations using joint sparsity. The learned J3S models are used for robust image and image-set classification. Experiments show that the proposed J3S-based image classification scheme outperforms the popular or state-of-the-art competing methods over FMD, UIUC, ETH-80 and YTC databases.



### Neuron segmentation using 3D wavelet integrated encoder-decoder network
- **Arxiv ID**: http://arxiv.org/abs/2106.00259v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00259v2)
- **Published**: 2021-06-01 06:46:50+00:00
- **Updated**: 2021-11-08 07:05:26+00:00
- **Authors**: Qiufu Li, Linlin Shen
- **Comment**: Bioinformatics accepted paper
- **Journal**: None
- **Summary**: Motivation: 3D neuron segmentation is a key step for the neuron digital reconstruction, which is essential for exploring brain circuits and understanding brain functions. However, the fine line-shaped nerve fibers of neuron could spread in a large region, which brings great computational cost to the neuron segmentation. Meanwhile, the strong noises and disconnected nerve fibers bring great challenges to the task. Results: In this paper, we propose a 3D wavelet and deep learning based 3D neuron segmentation method. The neuronal image is first partitioned into neuronal cubes to simplify the segmentation task. Then, we design 3D WaveUNet, the first 3D wavelet integrated encoder-decoder network, to segment the nerve fibers in the cubes; the wavelets could assist the deep networks in suppressing data noises and connecting the broken fibers. We also produce a Neuronal Cube Dataset (NeuCuDa) using the biggest available annotated neuronal image dataset, BigNeuron, to train 3D WaveUNet. Finally, the nerve fibers segmented in cubes are assembled to generate the complete neuron, which is digitally reconstructed using an available automatic tracing algorithm. The experimental results show that our neuron segmentation method could completely extract the target neuron in noisy neuronal images. The integrated 3D wavelets can efficiently improve the performance of 3D neuron segmentation and reconstruction. Availability: The data and codes for this work are available at https://github.com/LiQiufu/3D-WaveUNet.



### Hardness Sampling for Self-Training Based Transductive Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.00264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00264v1)
- **Published**: 2021-06-01 06:55:19+00:00
- **Updated**: 2021-06-01 06:55:19+00:00
- **Authors**: Liu Bo, Qiulei Dong, Zhanyi Hu
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Transductive zero-shot learning (T-ZSL) which could alleviate the domain shift problem in existing ZSL works, has received much attention recently. However, an open problem in T-ZSL: how to effectively make use of unseen-class samples for training, still remains. Addressing this problem, we first empirically analyze the roles of unseen-class samples with different degrees of hardness in the training process based on the uneven prediction phenomenon found in many ZSL methods, resulting in three observations. Then, we propose two hardness sampling approaches for selecting a subset of diverse and hard samples from a given unseen-class dataset according to these observations. The first one identifies the samples based on the class-level frequency of the model predictions while the second enhances the former by normalizing the class frequency via an approximate class prior estimated by an explored prior estimation algorithm. Finally, we design a new Self-Training framework with Hardness Sampling for T-ZSL, called STHS, where an arbitrary inductive ZSL method could be seamlessly embedded and it is iteratively trained with unseen-class samples selected by the hardness sampling approach. We introduce two typical ZSL methods into the STHS framework and extensive experiments demonstrate that the derived T-ZSL methods outperform many state-of-the-art methods on three public benchmarks. Besides, we note that the unseen-class dataset is separately used for training in some existing transductive generalized ZSL (T-GZSL) methods, which is not strict for a GZSL task. Hence, we suggest a more strict T-GZSL data setting and establish a competitive baseline on this setting by introducing the proposed STHS framework to T-GZSL.



### Analysis of classifiers robust to noisy labels
- **Arxiv ID**: http://arxiv.org/abs/2106.00274v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/2106.00274v1)
- **Published**: 2021-06-01 07:14:51+00:00
- **Updated**: 2021-06-01 07:14:51+00:00
- **Authors**: Alex Díaz, Damian Steele
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: We explore contemporary robust classification algorithms for overcoming class-dependant labelling noise: Forward, Importance Re-weighting and T-revision. The classifiers are trained and evaluated on class-conditional random label noise data while the final test data is clean. We demonstrate methods for estimating the transition matrix in order to obtain better classifier performance when working with noisy data. We apply deep learning to three data-sets and derive an end-to-end analysis with unknown noise on the CIFAR data-set from scratch. The effectiveness and robustness of the classifiers are analysed, and we compare and contrast the results of each experiment are using top-1 accuracy as our criterion.



### Independent Prototype Propagation for Zero-Shot Compositionality
- **Arxiv ID**: http://arxiv.org/abs/2106.00305v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.00305v2)
- **Published**: 2021-06-01 08:24:09+00:00
- **Updated**: 2021-06-12 15:26:47+00:00
- **Authors**: Frank Ruis, Gertjan Burghouts, Doina Bucur
- **Comment**: None
- **Journal**: None
- **Summary**: Humans are good at compositional zero-shot reasoning; someone who has never seen a zebra before could nevertheless recognize one when we tell them it looks like a horse with black and white stripes. Machine learning systems, on the other hand, usually leverage spurious correlations in the training data, and while such correlations can help recognize objects in context, they hurt generalization. To be able to deal with underspecified datasets while still leveraging contextual clues during classification, we propose ProtoProp, a novel prototype propagation graph method. First we learn prototypical representations of objects (e.g., zebra) that are conditionally independent w.r.t. their attribute labels (e.g., stripes) and vice versa. Next we propagate the independent prototypes through a compositional graph, to learn compositional prototypes of novel attribute-object combinations that reflect the dependencies of the target distribution. The method does not rely on any external data, such as class hierarchy graphs or pretrained word embeddings. We evaluate our approach on AO-Clever, a synthetic and strongly visual dataset with clean labels, and UT-Zappos, a noisy real-world dataset of fine-grained shoe types. We show that in the generalized compositional zero-shot setting we outperform state-of-the-art results, and through ablations we show the importance of each part of the method and their contribution to the final results.



### Separated-Spectral-Distribution Estimation Based on Bayesian Inference with Single RGB Camera
- **Arxiv ID**: http://arxiv.org/abs/2106.01861v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.01861v1)
- **Published**: 2021-06-01 08:31:47+00:00
- **Updated**: 2021-06-01 08:31:47+00:00
- **Authors**: Yuma Kinoshita, Hitoshi Kiya
- **Comment**: to appear in IEEE ICIP 2021
- **Journal**: None
- **Summary**: In this paper, we propose a novel method for separately estimating spectral distributions from images captured by a typical RGB camera. The proposed method allows us to separately estimate a spectral distribution of illumination, reflectance, or camera sensitivity, while recent hyperspectral cameras are limited to capturing a joint spectral distribution from a scene. In addition, the use of Bayesian inference makes it possible to take into account prior information of both spectral distributions and image noise as probability distributions. As a result, the proposed method can estimate spectral distributions in a unified way, and it can enhance the robustness of the estimation against noise, which conventional spectral-distribution estimation methods cannot. The use of Bayesian inference also enables us to obtain the confidence of estimation results. In an experiment, the proposed method is shown not only to outperform conventional estimation methods in terms of RMSE but also to be robust against noise.



### Semi-Supervised Disparity Estimation with Deep Feature Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2106.00318v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00318v1)
- **Published**: 2021-06-01 08:48:38+00:00
- **Updated**: 2021-06-01 08:48:38+00:00
- **Authors**: Julia Guerrero-Viu, Sergio Izquierdo, Philipp Schröppel, Thomas Brox
- **Comment**: Women in Computer Vision workshop CVPR 2021
- **Journal**: None
- **Summary**: Despite the success of deep learning in disparity estimation, the domain generalization gap remains an issue. We propose a semi-supervised pipeline that successfully adapts DispNet to a real-world domain by joint supervised training on labeled synthetic data and self-supervised training on unlabeled real data. Furthermore, accounting for the limitations of the widely-used photometric loss, we analyze the impact of deep feature reconstruction as a promising supervisory signal for disparity estimation.



### Consistent Two-Flow Network for Tele-Registration of Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2106.00329v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00329v3)
- **Published**: 2021-06-01 09:03:21+00:00
- **Updated**: 2021-10-11 02:25:04+00:00
- **Authors**: Zihao Yan, Zimu Yi, Ruizhen Hu, Niloy J. Mitra, Daniel Cohen-Or, Hui Huang
- **Comment**: Accepted to IEEE TVCG 2021, project page at
  https://vcc.tech/research/2021/CTFNet
- **Journal**: None
- **Summary**: Rigid registration of partial observations is a fundamental problem in various applied fields. In computer graphics, special attention has been given to the registration between two partial point clouds generated by scanning devices. State-of-the-art registration techniques still struggle when the overlap region between the two point clouds is small, and completely fail if there is no overlap between the scan pairs. In this paper, we present a learning-based technique that alleviates this problem, and allows registration between point clouds, presented in arbitrary poses, and having little or even no overlap, a setting that has been referred to as tele-registration. Our technique is based on a novel neural network design that learns a prior of a class of shapes and can complete a partial shape. The key idea is combining the registration and completion tasks in a way that reinforces each other. In particular, we simultaneously train the registration network and completion network using two coupled flows, one that register-and-complete, and one that complete-and-register, and encourage the two flows to produce a consistent result. We show that, compared with each separate flow, this two-flow training leads to robust and reliable tele-registration, and hence to a better point cloud prediction that completes the registered scans. It is also worth mentioning that each of the components in our neural network outperforms state-of-the-art methods in both completion and registration. We further analyze our network with several ablation studies and demonstrate its performance on a large number of partial point clouds, both synthetic and real-world, that have only small or no overlap.



### Towards Efficient Cross-Modal Visual Textual Retrieval using Transformer-Encoder Deep Features
- **Arxiv ID**: http://arxiv.org/abs/2106.00358v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00358v1)
- **Published**: 2021-06-01 10:11:46+00:00
- **Updated**: 2021-06-01 10:11:46+00:00
- **Authors**: Nicola Messina, Giuseppe Amato, Fabrizio Falchi, Claudio Gennaro, Stéphane Marchand-Maillet
- **Comment**: Accepted at CBMI 2021
- **Journal**: None
- **Summary**: Cross-modal retrieval is an important functionality in modern search engines, as it increases the user experience by allowing queries and retrieved objects to pertain to different modalities. In this paper, we focus on the image-sentence retrieval task, where the objective is to efficiently find relevant images for a given sentence (image-retrieval) or the relevant sentences for a given image (sentence-retrieval). Computer vision literature reports the best results on the image-sentence matching task using deep neural networks equipped with attention and self-attention mechanisms. They evaluate the matching performance on the retrieval task by performing sequential scans of the whole dataset. This method does not scale well with an increasing amount of images or captions. In this work, we explore different preprocessing techniques to produce sparsified deep multi-modal features extracting them from state-of-the-art deep-learning architectures for image-text matching. Our main objective is to lay down the paths for efficient indexing of complex multi-modal descriptions. We use the recently introduced TERN architecture as an image-sentence features extractor. It is designed for producing fixed-size 1024-d vectors describing whole images and sentences, as well as variable-length sets of 1024-d vectors describing the various building components of the two modalities (image regions and sentence words respectively). All these vectors are enforced by the TERN design to lie into the same common space. Our experiments show interesting preliminary results on the explored methods and suggest further experimentation in this important research direction.



### Learning Football Body-Orientation as a Matter of Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.00359v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00359v1)
- **Published**: 2021-06-01 10:12:32+00:00
- **Updated**: 2021-06-01 10:12:32+00:00
- **Authors**: Adrià Arbués-Sangüesa, Adrián Martín, Paulino Granero, Coloma Ballester, Gloria Haro
- **Comment**: Accepted in the AI for Sports Analytics Workshop at ICJAI 2021
- **Journal**: None
- **Summary**: Orientation is a crucial skill for football players that becomes a differential factor in a large set of events, especially the ones involving passes. However, existing orientation estimation methods, which are based on computer-vision techniques, still have a lot of room for improvement. To the best of our knowledge, this article presents the first deep learning model for estimating orientation directly from video footage. By approaching this challenge as a classification problem where classes correspond to orientation bins, and by introducing a cyclic loss function, a well-known convolutional network is refined to provide player orientation data. The model is trained by using ground-truth orientation data obtained from wearable EPTS devices, which are individually compensated with respect to the perceived orientation in the current frame. The obtained results outperform previous methods; in particular, the absolute median error is less than 12 degrees per player. An ablation study is included in order to show the potential generalization to any kind of football video footage.



### Natural Statistics of Network Activations and Implications for Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2106.00368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00368v1)
- **Published**: 2021-06-01 10:18:30+00:00
- **Updated**: 2021-06-01 10:18:30+00:00
- **Authors**: Michael Rotman, Lior Wolf
- **Comment**: Accepted to ICIP 2021
- **Journal**: None
- **Summary**: In a matter that is analog to the study of natural image statistics, we study the natural statistics of the deep neural network activations at various layers. As we show, these statistics, similar to image statistics, follow a power law. We also show, both analytically and empirically, that with depth the exponent of this power law increases at a linear rate.   As a direct implication of our discoveries, we present a method for performing Knowledge Distillation (KD). While classical KD methods consider the logits of the teacher network, more recent methods obtain a leap in performance by considering the activation maps. This, however, uses metrics that are suitable for comparing images. We propose to employ two additional loss terms that are based on the spectral properties of the intermediate activation maps. The proposed method obtains state of the art results on multiple image recognition KD benchmarks.



### Markov Localisation using Heatmap Regression and Deep Convolutional Odometry
- **Arxiv ID**: http://arxiv.org/abs/2106.00371v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.00371v1)
- **Published**: 2021-06-01 10:28:49+00:00
- **Updated**: 2021-06-01 10:28:49+00:00
- **Authors**: Oscar Mendez, Simon Hadfield, Richard Bowden
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA) 2021
- **Journal**: None
- **Summary**: In the context of self-driving vehicles there is strong competition between approaches based on visual localisation and LiDAR. While LiDAR provides important depth information, it is sparse in resolution and expensive. On the other hand, cameras are low-cost and recent developments in deep learning mean they can provide high localisation performance. However, several fundamental problems remain, particularly in the domain of uncertainty, where learning based approaches can be notoriously over-confident.   Markov, or grid-based, localisation was an early solution to the localisation problem but fell out of favour due to its computational complexity. Representing the likelihood field as a grid (or volume) means there is a trade off between accuracy and memory size. Furthermore, it is necessary to perform expensive convolutions across the entire likelihood volume. Despite the benefit of simultaneously maintaining a likelihood for all possible locations, grid based approaches were superseded by more efficient particle filters and Monte Carlo Localisation (MCL). However, MCL introduces its own problems e.g. particle deprivation.   Recent advances in deep learning hardware allow large likelihood volumes to be stored directly on the GPU, along with the hardware necessary to efficiently perform GPU-bound 3D convolutions and this obviates many of the disadvantages of grid based methods. In this work, we present a novel CNN-based localisation approach that can leverage modern deep learning hardware. By implementing a grid-based Markov localisation approach directly on the GPU, we create a hybrid CNN that can perform image-based localisation and odometry-based likelihood propagation within a single neural network. The resulting approach is capable of outperforming direct pose regression methods as well as state-of-the-art localisation systems.



### Hybrid Deep Neural Network for Brachial Plexus Nerve Segmentation in Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2106.00373v1
- **DOI**: 10.23919/EUSIPCO54536.2021.9616329
- **Categories**: **eess.IV**, cs.CV, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2106.00373v1)
- **Published**: 2021-06-01 10:31:47+00:00
- **Updated**: 2021-06-01 10:31:47+00:00
- **Authors**: Juul P. A. van Boxtel, Vincent R. J. Vousten, Josien Pluim, Nastaran Mohammadian Rad
- **Comment**: The first two authors contributed equally
- **Journal**: None
- **Summary**: Ultrasound-guided regional anesthesia (UGRA) can replace general anesthesia (GA), improving pain control and recovery time. This method can be applied on the brachial plexus (BP) after clavicular surgeries. However, identification of the BP from ultrasound (US) images is difficult, even for trained professionals. To address this problem, convolutional neural networks (CNNs) and more advanced deep neural networks (DNNs) can be used for identification and segmentation of the BP nerve region. In this paper, we propose a hybrid model consisting of a classification model followed by a segmentation model to segment BP nerve regions in ultrasound images. A CNN model is employed as a classifier to precisely select the images with the BP region. Then, a U-net or M-net model is used for the segmentation. Our experimental results indicate that the proposed hybrid model significantly improves the segmentation performance over a single segmentation model.



### DLA-Net: Learning Dual Local Attention Features for Semantic Segmentation of Large-Scale Building Facade Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2106.00376v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00376v1)
- **Published**: 2021-06-01 10:39:11+00:00
- **Updated**: 2021-06-01 10:39:11+00:00
- **Authors**: Yanfei Su, Weiquan Liu, Zhimin Yuan, Ming Cheng, Zhihong Zhang, Xuelun Shen, Cheng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation of building facade is significant in various applications, such as urban building reconstruction and damage assessment. As there is a lack of 3D point clouds datasets related to the fine-grained building facade, we construct the first large-scale building facade point clouds benchmark dataset for semantic segmentation. The existing methods of semantic segmentation cannot fully mine the local neighborhood information of point clouds. Addressing this problem, we propose a learnable attention module that learns Dual Local Attention features, called DLA in this paper. The proposed DLA module consists of two blocks, including the self-attention block and attentive pooling block, which both embed an enhanced position encoding block. The DLA module could be easily embedded into various network architectures for point cloud segmentation, naturally resulting in a new 3D semantic segmentation network with an encoder-decoder architecture, called DLA-Net in this work. Extensive experimental results on our constructed building facade dataset demonstrate that the proposed DLA-Net achieves better performance than the state-of-the-art methods for semantic segmentation.



### Analysis of Vision-based Abnormal Red Blood Cell Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.00389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00389v1)
- **Published**: 2021-06-01 10:52:41+00:00
- **Updated**: 2021-06-01 10:52:41+00:00
- **Authors**: Annika Wong, Nantheera Anantrasirichai, Thanarat H. Chalidabhongse, Duangdao Palasuwan, Attakorn Palasuwan, David Bull
- **Comment**: None
- **Journal**: None
- **Summary**: Identification of abnormalities in red blood cells (RBC) is key to diagnosing a range of medical conditions from anaemia to liver disease. Currently this is done manually, a time-consuming and subjective process. This paper presents an automated process utilising the advantages of machine learning to increase capacity and standardisation of cell abnormality detection, and its performance is analysed. Three different machine learning technologies were used: a Support Vector Machine (SVM), a classical machine learning technology; TabNet, a deep learning architecture for tabular data; U-Net, a semantic segmentation network designed for medical image segmentation. A critical issue was the highly imbalanced nature of the dataset which impacts the efficacy of machine learning. To address this, synthesising minority class samples in feature space was investigated via Synthetic Minority Over-sampling Technique (SMOTE) and cost-sensitive learning. A combination of these two methods is investigated to improve the overall performance. These strategies were found to increase sensitivity to minority classes. The impact of unknown cells on semantic segmentation is demonstrated, with some evidence of the model applying learning of labelled cells to these anonymous cells. These findings indicate both classical models and new deep learning networks as promising methods in automating RBC abnormality detection.



### Neural Network Structure Design based on N-Gauss Activation Function
- **Arxiv ID**: http://arxiv.org/abs/2106.07562v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07562v1)
- **Published**: 2021-06-01 11:16:37+00:00
- **Updated**: 2021-06-01 11:16:37+00:00
- **Authors**: Xiangri Lu, Hongbin Ma, Jingcheng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work has shown that the activation function of the convolutional neural network can meet the Lipschitz condition, then the corresponding convolutional neural network structure can be constructed according to the scale of the data set, and the data set can be trained more deeply, more accurately and more effectively. In this article, we have accepted the experimental results and introduced the core block N-Gauss, N-Gauss, and Swish (Conv1, Conv2, FC1) neural network structure design to train MNIST, CIFAR10, and CIFAR100 respectively. Experiments show that N-Gauss gives full play to the main role of nonlinear modeling of activation functions, so that deep convolutional neural networks have hierarchical nonlinear mapping learning capabilities. At the same time, the training ability of N-Gauss on simple one-dimensional channel small data sets is equivalent to the performance of ReLU and Swish.



### Deep Clustering Activation Maps for Emphysema Subtyping
- **Arxiv ID**: http://arxiv.org/abs/2106.01351v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.01351v1)
- **Published**: 2021-06-01 11:24:48+00:00
- **Updated**: 2021-06-01 11:24:48+00:00
- **Authors**: Weiyi Xie, Colin Jacobs, Bram van Ginneken
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deep learning clustering method that exploits dense features from a segmentation network for emphysema subtyping from computed tomography (CT) scans. Using dense features enables high-resolution visualization of image regions corresponding to the cluster assignment via dense clustering activation maps (dCAMs). This approach provides model interpretability. We evaluated clustering results on 500 subjects from the COPDGenestudy, where radiologists manually annotated emphysema sub-types according to their visual CT assessment. We achieved a 43% unsupervised clustering accuracy, outperforming our baseline at 41% and yielding results comparable to supervised classification at 45%. The proposed method also offers a better cluster formation than the baseline, achieving0.54 in silhouette coefficient and 0.55 in David-Bouldin scores.



### Asymmetrical Bi-RNN for pedestrian trajectory encoding
- **Arxiv ID**: http://arxiv.org/abs/2106.04419v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T45, I.2.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2106.04419v2)
- **Published**: 2021-06-01 12:05:15+00:00
- **Updated**: 2021-06-19 12:33:47+00:00
- **Authors**: Raphaël Rozenberg, Joseph Gesnouin, Fabien Moutarde
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Pedestrian motion behavior involves a combination of individual goals and social interactions with other agents. In this article, we present an asymmetrical bidirectional recurrent neural network architecture called U-RNN to encode pedestrian trajectories and evaluate its relevance to replace LSTMs for various forecasting models. Experimental results on the Trajnet++ benchmark show that the U-LSTM variant yields better results regarding every available metrics (ADE, FDE, Collision rate) than common trajectory encoders for a variety of approaches and interaction modules, suggesting that the proposed approach is a viable alternative to the de facto sequence encoding RNNs.   Our implementation of the asymmetrical Bi-RNNs for the Trajnet++ benchmark is available at: github.com/JosephGesnouin/Asymmetrical-Bi-RNNs-to-encode-pedestrian-trajectories



### COV-ECGNET: COVID-19 detection using ECG trace images with deep convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2106.00436v1
- **DOI**: 10.1007/s13755-021-00169-1
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.00436v1)
- **Published**: 2021-06-01 12:33:08+00:00
- **Updated**: 2021-06-01 12:33:08+00:00
- **Authors**: Tawsifur Rahman, Alex Akinbi, Muhammad E. H. Chowdhury, Tarik A. Rashid, Abdulkadir Şengür, Amith Khandakar, Khandaker Reajul Islam, Aras M. Ismael
- **Comment**: 24 pages
- **Journal**: Health Information Science and Systems (2022) 10:1
- **Summary**: The reliable and rapid identification of the COVID-19 has become crucial to prevent the rapid spread of the disease, ease lockdown restrictions and reduce pressure on public health infrastructures. Recently, several methods and techniques have been proposed to detect the SARS-CoV-2 virus using different images and data. However, this is the first study that will explore the possibility of using deep convolutional neural network (CNN) models to detect COVID-19 from electrocardiogram (ECG) trace images. In this work, COVID-19 and other cardiovascular diseases (CVDs) were detected using deep-learning techniques. A public dataset of ECG images consists of 1937 images from five distinct categories, such as Normal, COVID-19, myocardial infarction (MI), abnormal heartbeat (AHB), and recovered myocardial infarction (RMI) were used in this study. Six different deep CNN models (ResNet18, ResNet50, ResNet101, InceptionV3, DenseNet201, and MobileNetv2) were used to investigate three different classification schemes: two-class classification (Normal vs COVID-19); three-class classification (Normal, COVID-19, and Other CVDs), and finally, five-class classification (Normal, COVID-19, MI, AHB, and RMI). For two-class and three-class classification, Densenet201 outperforms other networks with an accuracy of 99.1%, and 97.36%, respectively; while for the five-class classification, InceptionV3 outperforms others with an accuracy of 97.83%. ScoreCAM visualization confirms that the networks are learning from the relevant area of the trace images. Since the proposed method uses ECG trace images which can be captured by smartphones and are readily available facilities in low-resources countries, this study will help in faster computer-aided diagnosis of COVID-19 and other cardiac abnormalities.



### PanoDR: Spherical Panorama Diminished Reality for Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/2106.00446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00446v1)
- **Published**: 2021-06-01 12:56:53+00:00
- **Updated**: 2021-06-01 12:56:53+00:00
- **Authors**: V. Gkitsas, V. Sterzentsenko, N. Zioulis, G. Albanis, D. Zarpalas
- **Comment**: Accepted at CVPR, OmniCV Workshop. Code and models are available at
  https://vcl3d.github.io/PanoDR/
- **Journal**: None
- **Summary**: The rising availability of commercial $360^\circ$ cameras that democratize indoor scanning, has increased the interest for novel applications, such as interior space re-design. Diminished Reality (DR) fulfills the requirement of such applications, to remove existing objects in the scene, essentially translating this to a counterfactual inpainting task. While recent advances in data-driven inpainting have shown significant progress in generating realistic samples, they are not constrained to produce results with reality mapped structures. To preserve the `reality' in indoor (re-)planning applications, the scene's structure preservation is crucial. To ensure structure-aware counterfactual inpainting, we propose a model that initially predicts the structure of an indoor scene and then uses it to guide the reconstruction of an empty -- background only -- representation of the same scene. We train and compare against other state-of-the-art methods on a version of the Structured3D dataset modified for DR, showing superior results in both quantitative metrics and qualitative results, but more interestingly, our approach exhibits a much faster convergence rate. Code and models are available at https://vcl3d.github.io/PanoDR/ .



### Detecting Anomalies in Semantic Segmentation with Prototypes
- **Arxiv ID**: http://arxiv.org/abs/2106.00472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00472v1)
- **Published**: 2021-06-01 13:22:33+00:00
- **Updated**: 2021-06-01 13:22:33+00:00
- **Authors**: Dario Fontanel, Fabio Cermelli, Massimiliano Mancini, Barbara Caputo
- **Comment**: SAIAD CVPR21 Workshop
- **Journal**: None
- **Summary**: Traditional semantic segmentation methods can recognize at test time only the classes that are present in the training set. This is a significant limitation, especially for semantic segmentation algorithms mounted on intelligent autonomous systems, deployed in realistic settings. Regardless of how many classes the system has seen at training time, it is inevitable that unexpected, unknown objects will appear at test time. The failure in identifying such anomalies may lead to incorrect, even dangerous behaviors of the autonomous agent equipped with such segmentation model when deployed in the real world. Current state of the art of anomaly segmentation uses generative models, exploiting their incapability to reconstruct patterns unseen during training. However, training these models is expensive, and their generated artifacts may create false anomalies. In this paper we take a different route and we propose to address anomaly segmentation through prototype learning. Our intuition is that anomalous pixels are those that are dissimilar to all class prototypes known by the model. We extract class prototypes from the training data in a lightweight manner using a cosine similarity-based classifier. Experiments on StreetHazards show that our approach achieves the new state of the art, with a significant margin over previous works, despite the reduced computational overhead. Code is available at https://github.com/DarioFontanel/PAnS.



### Dense Nested Attention Network for Infrared Small Target Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.00487v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00487v3)
- **Published**: 2021-06-01 13:45:35+00:00
- **Updated**: 2022-08-16 02:45:46+00:00
- **Authors**: Boyang Li, Chao Xiao, Longguang Wang, Yingqian Wang, Zaiping Lin, Miao Li, Wei An, Yulan Guo
- **Comment**: Accepted by IEEE Transactions on Image Processing (TIP)
- **Journal**: None
- **Summary**: Single-frame infrared small target (SIRST) detection aims at separating small targets from clutter backgrounds. With the advances of deep learning, CNN-based methods have yielded promising results in generic object detection due to their powerful modeling capability. However, existing CNN-based methods cannot be directly applied for infrared small targets since pooling layers in their networks could lead to the loss of targets in deep layers. To handle this problem, we propose a dense nested attention network (DNANet) in this paper. Specifically, we design a dense nested interactive module (DNIM) to achieve progressive interaction among high-level and low-level features. With the repeated interaction in DNIM, infrared small targets in deep layers can be maintained. Based on DNIM, we further propose a cascaded channel and spatial attention module (CSAM) to adaptively enhance multi-level features. With our DNANet, contextual information of small targets can be well incorporated and fully exploited by repeated fusion and enhancement. Moreover, we develop an infrared small target dataset (namely, NUDT-SIRST) and propose a set of evaluation metrics to conduct comprehensive performance evaluation. Experiments on both public and our self-developed datasets demonstrate the effectiveness of our method. Compared to other state-of-the-art methods, our method achieves better performance in terms of probability of detection (Pd), false-alarm rate (Fa), and intersection of union (IoU).



### RAI-Net: Range-Adaptive LiDAR Point Cloud Frame Interpolation Network
- **Arxiv ID**: http://arxiv.org/abs/2106.00496v1
- **DOI**: 10.1109/BMSB53066.2021.9547131
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00496v1)
- **Published**: 2021-06-01 13:59:08+00:00
- **Updated**: 2021-06-01 13:59:08+00:00
- **Authors**: Lili Zhao, Zezhi Zhu, Xuhu Lin, Xuezhou Guo, Qian Yin, Wenyi Wang, Jianwen Chen
- **Comment**: Accepted by the IEEE International Symposium on Broadband Multimedia
  Systems and Broadcasting 2021
- **Journal**: None
- **Summary**: LiDAR point cloud frame interpolation, which synthesizes the intermediate frame between the captured frames, has emerged as an important issue for many applications. Especially for reducing the amounts of point cloud transmission, it is by predicting the intermediate frame based on the reference frames to upsample data to high frame rate ones. However, due to high-dimensional and sparse characteristics of point clouds, it is more difficult to predict the intermediate frame for LiDAR point clouds than videos. In this paper, we propose a novel LiDAR point cloud frame interpolation method, which exploits range images (RIs) as an intermediate representation with CNNs to conduct the frame interpolation process. Considering the inherited characteristics of RIs differ from that of color images, we introduce spatially adaptive convolutions to extract range features adaptively, while a high-efficient flow estimation method is presented to generate optical flows. The proposed model then warps the input frames and range features, based on the optical flows to synthesize the interpolated frame. Extensive experiments on the KITTI dataset have clearly demonstrated that our method consistently achieves superior frame interpolation results with better perceptual quality to that of using state-of-the-art video frame interpolation methods. The proposed method could be integrated into any LiDAR point cloud compression systems for inter prediction.



### A Novel Graph-Theoretic Deep Representation Learning Method for Multi-Label Remote Sensing Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2106.00506v1
- **DOI**: 10.1109/IGARSS47720.2021.9554466
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00506v1)
- **Published**: 2021-06-01 14:11:08+00:00
- **Updated**: 2021-06-01 14:11:08+00:00
- **Authors**: Gencer Sumbul, Begüm Demir
- **Comment**: Accepted at IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS) 2021. Our code is available at
  https://git.tu-berlin.de/rsim/GT-DRL-CBIR
- **Journal**: None
- **Summary**: This paper presents a novel graph-theoretic deep representation learning method in the framework of multi-label remote sensing (RS) image retrieval problems. The proposed method aims to extract and exploit multi-label co-occurrence relationships associated to each RS image in the archive. To this end, each training image is initially represented with a graph structure that provides region-based image representation combining both local information and the related spatial organization. Unlike the other graph-based methods, the proposed method contains a novel learning strategy to train a deep neural network for automatically predicting a graph structure of each RS image in the archive. This strategy employs a region representation learning loss function to characterize the image content based on its multi-label co-occurrence relationship. Experimental results show the effectiveness of the proposed method for retrieval problems in RS compared to state-of-the-art deep representation learning methods. The code of the proposed method is publicly available at https://git.tu-berlin.de/rsim/GT-DRL-CBIR .



### Exploring the Diversity and Invariance in Yourself for Visual Pre-Training Task
- **Arxiv ID**: http://arxiv.org/abs/2106.00537v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00537v1)
- **Published**: 2021-06-01 14:52:36+00:00
- **Updated**: 2021-06-01 14:52:36+00:00
- **Authors**: Longhui Wei, Lingxi Xie, Wengang Zhou, Houqiang Li, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, self-supervised learning methods have achieved remarkable success in visual pre-training task. By simply pulling the different augmented views of each image together or other novel mechanisms, they can learn much unsupervised knowledge and significantly improve the transfer performance of pre-training models. However, these works still cannot avoid the representation collapse problem, i.e., they only focus on limited regions or the extracted features on totally different regions inside each image are nearly the same. Generally, this problem makes the pre-training models cannot sufficiently describe the multi-grained information inside images, which further limits the upper bound of their transfer performance. To alleviate this issue, this paper introduces a simple but effective mechanism, called Exploring the Diversity and Invariance in Yourself E-DIY. By simply pushing the most different regions inside each augmented view away, E-DIY can preserve the diversity of extracted region-level features. By pulling the most similar regions from different augmented views of the same image together, E-DIY can ensure the robustness of region-level features. Benefited from the above diversity and invariance exploring mechanism, E-DIY maximally extracts the multi-grained visual information inside each image. Extensive experiments on downstream tasks demonstrate the superiority of our proposed approach, e.g., there are 2.1% improvements compared with the strong baseline BYOL on COCO while fine-tuning Mask R-CNN with the R50-C4 backbone and 1X learning schedule.



### Predicting Vehicles Trajectories in Urban Scenarios with Transformer Networks and Augmented Information
- **Arxiv ID**: http://arxiv.org/abs/2106.00559v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00559v2)
- **Published**: 2021-06-01 15:18:55+00:00
- **Updated**: 2021-06-07 23:38:30+00:00
- **Authors**: A. Quintanar, D. Fernández-Llorca, I. Parra, R. Izquierdo, M. A. Sotelo
- **Comment**: This work has been accepted for publication at IEEE Intelligent
  Vehicles Symposium 2021
- **Journal**: None
- **Summary**: Understanding the behavior of road users is of vital importance for the development of trajectory prediction systems. In this context, the latest advances have focused on recurrent structures, establishing the social interaction between the agents involved in the scene. More recently, simpler structures have also been introduced for predicting pedestrian trajectories, based on Transformer Networks, and using positional information. They allow the individual modelling of each agent's trajectory separately without any complex interaction terms. Our model exploits these simple structures by adding augmented data (position and heading), and adapting their use to the problem of vehicle trajectory prediction in urban scenarios in prediction horizons up to 5 seconds. In addition, a cross-performance analysis is performed between different types of scenarios, including highways, intersections and roundabouts, using recent datasets (inD, rounD, highD and INTERACTION). Our model achieves state-of-the-art results and proves to be flexible and adaptable to different types of urban contexts.



### Full-Resolution Encoder-Decoder Networks with Multi-Scale Feature Fusion for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2106.00566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00566v1)
- **Published**: 2021-06-01 15:30:09+00:00
- **Updated**: 2021-06-01 15:30:09+00:00
- **Authors**: Jie Ou, Mingjian Chen, Hong Wu
- **Comment**: None
- **Journal**: None
- **Summary**: To achieve more accurate 2D human pose estimation, we extend the successful encoder-decoder network, simple baseline network (SBN), in three ways. To reduce the quantization errors caused by the large output stride size, two more decoder modules are appended to the end of the simple baseline network to get full output resolution. Then, the global context blocks (GCBs) are added to the encoder and decoder modules to enhance them with global context features. Furthermore, we propose a novel spatial-attention-based multi-scale feature collection and distribution module (SA-MFCD) to fuse and distribute multi-scale features to boost the pose estimation. Experimental results on the MS COCO dataset indicate that our network can remarkably improve the accuracy of human pose estimation over SBN, our network using ResNet34 as the backbone network can even achieve the same accuracy as SBN with ResNet152, and our networks can achieve superior results with big backbone networks.



### Prior-Enhanced Few-Shot Segmentation with Meta-Prototypes
- **Arxiv ID**: http://arxiv.org/abs/2106.00572v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00572v1)
- **Published**: 2021-06-01 15:34:30+00:00
- **Updated**: 2021-06-01 15:34:30+00:00
- **Authors**: Jian-Wei Zhang, Lei Lv, Yawei Luo, Hao-Zhe Feng, Yi Yang, Wei Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot segmentation~(FSS) performance has been extensively promoted by introducing episodic training and class-wise prototypes. However, the FSS problem remains challenging due to three limitations: (1) Models are distracted by task-unrelated information; (2) The representation ability of a single prototype is limited; (3) Class-related prototypes ignore the prior knowledge of base classes. We propose the Prior-Enhanced network with Meta-Prototypes to tackle these limitations. The prior-enhanced network leverages the support and query (pseudo-) labels in feature extraction, which guides the model to focus on the task-related features of the foreground objects, and suppress much noise due to the lack of supervised knowledge. Moreover, we introduce multiple meta-prototypes to encode hierarchical features and learn class-agnostic structural information. The hierarchical features help the model highlight the decision boundary and focus on hard pixels, and the structural information learned from base classes is treated as the prior knowledge for novel classes. Experiments show that our method achieves the mean-IoU scores of 60.79% and 41.16% on PASCAL-$5^i$ and COCO-$20^i$, outperforming the state-of-the-art method by 3.49% and 5.64% in the 5-shot setting. Moreover, comparing with 1-shot results, our method promotes 5-shot accuracy by 3.73% and 10.32% on the above two benchmarks. The source code of our method is available at https://github.com/Jarvis73/PEMP.



### Exposing Previously Undetectable Faults in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.00576v1
- **DOI**: 10.1145/3460319.3464801
- **Categories**: **cs.LG**, cs.CV, cs.SE, I.2.6; D.2.5
- **Links**: [PDF](http://arxiv.org/pdf/2106.00576v1)
- **Published**: 2021-06-01 15:37:30+00:00
- **Updated**: 2021-06-01 15:37:30+00:00
- **Authors**: Isaac Dunn, Hadrien Pouget, Daniel Kroening, Tom Melham
- **Comment**: Accepted to the ACM SIGSOFT International Symposium on Software
  Testing and Analysis (ISSTA 2021)
- **Journal**: None
- **Summary**: Existing methods for testing DNNs solve the oracle problem by constraining the raw features (e.g. image pixel values) to be within a small distance of a dataset example for which the desired DNN output is known. But this limits the kinds of faults these approaches are able to detect. In this paper, we introduce a novel DNN testing method that is able to find faults in DNNs that other methods cannot. The crux is that, by leveraging generative machine learning, we can generate fresh test inputs that vary in their high-level features (for images, these include object shape, location, texture, and colour). We demonstrate that our approach is capable of detecting deliberately injected faults as well as new faults in state-of-the-art DNNs, and that in both cases, existing methods are unable to find these faults.



### TransVOS: Video Object Segmentation with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2106.00588v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.00588v2)
- **Published**: 2021-06-01 15:56:10+00:00
- **Updated**: 2021-09-18 01:06:17+00:00
- **Authors**: Jianbiao Mei, Mengmeng Wang, Yeneng Lin, Yi Yuan, Yong Liu
- **Comment**: 9 pages, 2 figures
- **Journal**: None
- **Summary**: Recently, Space-Time Memory Network (STM) based methods have achieved state-of-the-art performance in semi-supervised video object segmentation (VOS). A crucial problem in this task is how to model the dependency both among different frames and inside every frame. However, most of these methods neglect the spatial relationships (inside each frame) and do not make full use of the temporal relationships (among different frames). In this paper, we propose a new transformer-based framework, termed TransVOS, introducing a vision transformer to fully exploit and model both the temporal and spatial relationships. Moreover, most STM-based approaches employ two separate encoders to extract features of two significant inputs, i.e., reference sets (history frames with predicted masks) and query frame (current frame), respectively, increasing the models' parameters and complexity. To slim the popular two-encoder pipeline while keeping the effectiveness, we design a single two-path feature extractor to encode the above two inputs in a unified way. Extensive experiments demonstrate the superiority of our TransVOS over state-of-the-art methods on both DAVIS and YouTube-VOS datasets.



### Semi-Supervised Domain Generalization with Stochastic StyleMatch
- **Arxiv ID**: http://arxiv.org/abs/2106.00592v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.00592v2)
- **Published**: 2021-06-01 16:00:08+00:00
- **Updated**: 2021-12-16 01:05:48+00:00
- **Authors**: Kaiyang Zhou, Chen Change Loy, Ziwei Liu
- **Comment**: Tech report. Code available at
  https://github.com/KaiyangZhou/ssdg-benchmark
- **Journal**: None
- **Summary**: Ideally, visual learning algorithms should be generalizable, for dealing with any unseen domain shift when deployed in a new target environment; and data-efficient, for reducing development costs by using as little labels as possible. To this end, we study semi-supervised domain generalization (SSDG), which aims to learn a domain-generalizable model using multi-source, partially-labeled training data. We design two benchmarks that cover state-of-the-art methods developed in two related fields, i.e., domain generalization (DG) and semi-supervised learning (SSL). We find that the DG methods, which by design are unable to handle unlabeled data, perform poorly with limited labels in SSDG; the SSL methods, especially FixMatch, obtain much better results but are still far away from the basic vanilla model trained using full labels. We propose StyleMatch, a simple approach that extends FixMatch with a couple of new ingredients tailored for SSDG: 1) stochastic modeling for reducing overfitting in scarce labels, and 2) multi-view consistency learning for enhancing domain generalization. Despite the concise designs, StyleMatch achieves significant improvements in SSDG. We hope our approach and the comprehensive benchmarks can pave the way for future research on generalizable and data-efficient learning systems. The source code is released at \url{https://github.com/KaiyangZhou/ssdg-benchmark}.



### Look Wide and Interpret Twice: Improving Performance on Interactive Instruction-following Tasks
- **Arxiv ID**: http://arxiv.org/abs/2106.00596v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00596v2)
- **Published**: 2021-06-01 16:06:09+00:00
- **Updated**: 2021-06-06 14:38:04+00:00
- **Authors**: Van-Quang Nguyen, Masanori Suganuma, Takayuki Okatani
- **Comment**: To appear in IJCAI2021. 8-page main paper and Appendix following.
  Appendix E for details of entry submission to EAI 2021. Github:
  https://github.com/davidnvq/lwit-alfred
- **Journal**: None
- **Summary**: There is a growing interest in the community in making an embodied AI agent perform a complicated task while interacting with an environment following natural language directives. Recent studies have tackled the problem using ALFRED, a well-designed dataset for the task, but achieved only very low accuracy. This paper proposes a new method, which outperforms the previous methods by a large margin. It is based on a combination of several new ideas. One is a two-stage interpretation of the provided instructions. The method first selects and interprets an instruction without using visual information, yielding a tentative action sequence prediction. It then integrates the prediction with the visual information etc., yielding the final prediction of an action and an object. As the object's class to interact is identified in the first stage, it can accurately select the correct object from the input image. Moreover, our method considers multiple egocentric views of the environment and extracts essential information by applying hierarchical attention conditioned on the current instruction. This contributes to the accurate prediction of actions for navigation. A preliminary version of the method won the ALFRED Challenge 2020. The current version achieves the unseen environment's success rate of 4.45% with a single view, which is further improved to 8.37% with multiple views.



### Robust Mutual Learning for Semi-supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.00609v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00609v2)
- **Published**: 2021-06-01 16:22:01+00:00
- **Updated**: 2021-12-27 03:37:20+00:00
- **Authors**: Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Fang Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Recent semi-supervised learning (SSL) methods are commonly based on pseudo labeling. Since the SSL performance is greatly influenced by the quality of pseudo labels, mutual learning has been proposed to effectively suppress the noises in the pseudo supervision. In this work, we propose robust mutual learning that improves the prior approach in two aspects. First, the vanilla mutual learners suffer from the coupling issue that models may converge to learn homogeneous knowledge. We resolve this issue by introducing mean teachers to generate mutual supervisions so that there is no direct interaction between the two students. We also show that strong data augmentations, model noises and heterogeneous network architectures are essential to alleviate the model coupling. Second, we notice that mutual learning fails to leverage the network's own ability for pseudo label refinement. Therefore, we introduce self-rectification that leverages the internal knowledge and explicitly rectifies the pseudo labels before the mutual teaching. Such self-rectification and mutual teaching collaboratively improve the pseudo label accuracy throughout the learning. The proposed robust mutual learning demonstrates state-of-the-art performance on semantic segmentation in low-data regime.



### Raman spectral analysis of mixtures with one-dimensional convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2106.05316v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2106.05316v1)
- **Published**: 2021-06-01 16:23:30+00:00
- **Updated**: 2021-06-01 16:23:30+00:00
- **Authors**: M. Hamed Mozaffari, Li-Lin Tay
- **Comment**: 9 pages, 5 tables, 3 figures
- **Journal**: None
- **Summary**: Recently, the combination of robust one-dimensional convolutional neural networks (1-D CNNs) and Raman spectroscopy has shown great promise in rapid identification of unknown substances with good accuracy. Using this technique, researchers can recognize a pure compound and distinguish it from unknown substances in a mixture. The novelty of this approach is that the trained neural network operates automatically without any pre- or post-processing of data. Some studies have attempted to extend this technique to the classification of pure compounds in an unknown mixture. However, the application of 1-D CNNs has typically been restricted to binary classifications of pure compounds. Here we will highlight a new approach in spectral recognition and quantification of chemical components in a multicomponent mixture. Two 1-D CNN models, RaMixNet I and II, have been developed for this purpose. The former is for rapid classification of components in a mixture while the latter is for quantitative determination of those constituents. In the proposed method, there is no limit to the number of compounds in a mixture. A data augmentation method is also introduced by adding random baselines to the Raman spectra. The experimental results revealed that the classification accuracy of RaMixNet I and II is 100% for analysis of unknown test mixtures; at the same time, the RaMixNet II model may achieve a regression accuracy of 88% for the quantification of each component.



### Decoupling Shape and Density for Liver Lesion Synthesis Using Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.00629v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00629v1)
- **Published**: 2021-06-01 16:45:19+00:00
- **Updated**: 2021-06-01 16:45:19+00:00
- **Authors**: Dario Augusto Borges Oliveira
- **Comment**: None
- **Journal**: None
- **Summary**: Lesion synthesis received much attention with the rise of efficient generative models for augmenting training data, drawing lesion evolution scenarios, or aiding expert training. The quality and diversity of synthesized data are highly dependent on the annotated data used to train the models, which not rarely struggle to derive very different yet realistic samples from the training ones. That adds an inherent bias to lesion segmentation algorithms and limits synthesizing lesion evolution scenarios efficiently. This paper presents a method for decoupling shape and density for liver lesion synthesis, creating a framework that allows straight-forwardly driving the synthesis. We offer qualitative results that show the synthesis control by modifying shape and density individually, and quantitative results that demonstrate that embedding the density information in the generator model helps to increase lesion segmentation performance compared to using the shape solely.



### Quantifying Predictive Uncertainty in Medical Image Analysis with Deep Kernel Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.00638v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00638v1)
- **Published**: 2021-06-01 17:09:47+00:00
- **Updated**: 2021-06-01 17:09:47+00:00
- **Authors**: Zhiliang Wu, Yinchong Yang, Jindong Gu, Volker Tresp
- **Comment**: None
- **Journal**: 2021 IEEE International Conference on Healthcare Informatics
  (ICHI)
- **Summary**: Deep neural networks are increasingly being used for the analysis of medical images. However, most works neglect the uncertainty in the model's prediction. We propose an uncertainty-aware deep kernel learning model which permits the estimation of the uncertainty in the prediction by a pipeline of a Convolutional Neural Network and a sparse Gaussian Process. Furthermore, we adapt different pre-training methods to investigate their impacts on the proposed model. We apply our approach to Bone Age Prediction and Lesion Localization. In most cases, the proposed model shows better performance compared to common architectures. More importantly, our model expresses systematically higher confidence in more accurate predictions and less confidence in less accurate ones. Our model can also be used to detect challenging and controversial test samples. Compared to related methods such as Monte-Carlo Dropout, our approach derives the uncertainty information in a purely analytical fashion and is thus computationally more efficient.



### Hyperspectral Band Selection for Multispectral Image Classification with Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.00645v1
- **DOI**: 10.1109/IJCNN52387.2021.9533700
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00645v1)
- **Published**: 2021-06-01 17:24:35+00:00
- **Updated**: 2021-06-01 17:24:35+00:00
- **Authors**: Giorgio Morales, John Sheppard, Riley Logan, Joseph Shaw
- **Comment**: Accepted to appear in the International Joint Conference on Neural
  Networks 2021
- **Journal**: None
- **Summary**: In recent years, Hyperspectral Imaging (HSI) has become a powerful source for reliable data in applications such as remote sensing, agriculture, and biomedicine. However, hyperspectral images are highly data-dense and often benefit from methods to reduce the number of spectral bands while retaining the most useful information for a specific application. We propose a novel band selection method to select a reduced set of wavelengths, obtained from an HSI system in the context of image classification. Our approach consists of two main steps: the first utilizes a filter-based approach to find relevant spectral bands based on a collinearity analysis between a band and its neighbors. This analysis helps to remove redundant bands and dramatically reduces the search space. The second step applies a wrapper-based approach to select bands from the reduced set based on their information entropy values, and trains a compact Convolutional Neural Network (CNN) to evaluate the performance of the current selection. We present classification results obtained from our method and compare them to other feature selection methods on two hyperspectral image datasets. Additionally, we use the original hyperspectral data cube to simulate the process of using actual filters in a multispectral imager. We show that our method produces more suitable results for a multispectral sensor design.



### Comprehensive Validation of Automated Whole Body Skeletal Muscle, Adipose Tissue, and Bone Segmentation from 3D CT images for Body Composition Analysis: Towards Extended Body Composition
- **Arxiv ID**: http://arxiv.org/abs/2106.00652v3
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/2106.00652v3)
- **Published**: 2021-06-01 17:30:45+00:00
- **Updated**: 2021-07-27 01:13:06+00:00
- **Authors**: Da Ma, Vincent Chow, Karteek Popuri, Mirza Faisal Beg
- **Comment**: This paper is based on concepts presented at the NIH Body Composition
  and Cancer Outcomes Research Webinar Series on December 17th, 2020 by Mirza
  Faisal Beg titled "Automating Body Composition from Routinely Acquired CT
  images - towards 3D measurements". The talk is archived
  [here](https://epi.grants.cancer.gov/events/body-composition/#past)
- **Journal**: None
- **Summary**: The latest advances in computer-assisted precision medicine are making it feasible to move from population-wide models that are useful to discover aggregate patterns that hold for group-based analysis to patient-specific models that can drive patient-specific decisions with regard to treatment choices, and predictions of outcomes of treatment. Body Composition is recognized as an important driver and risk factor for a wide variety of diseases, as well as a predictor of individual patient-specific clinical outcomes to treatment choices or surgical interventions. 3D CT images are routinely acquired in the oncological worklows and deliver accurate rendering of internal anatomy and therefore can be used opportunistically to assess the amount of skeletal muscle and adipose tissue compartments. Powerful tools of artificial intelligence such as deep learning are making it feasible now to segment the entire 3D image and generate accurate measurements of all internal anatomy. These will enable the overcoming of the severe bottleneck that existed previously, namely, the need for manual segmentation, which was prohibitive to scale to the hundreds of 2D axial slices that made up a 3D volumetric image. Automated tools such as presented here will now enable harvesting whole-body measurements from 3D CT or MRI images, leading to a new era of discovery of the drivers of various diseases based on individual tissue, organ volume, shape, and functional status. These measurements were hitherto unavailable thereby limiting the field to a very small and limited subset. These discoveries and the potential to perform individual image segmentation with high speed and accuracy are likely to lead to the incorporation of these 3D measures into individual specific treatment planning models related to nutrition, aging, chemotoxicity, surgery and survival after the onset of a major disease such as cancer.



### Markpainting: Adversarial Machine Learning meets Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2106.00660v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2106.00660v1)
- **Published**: 2021-06-01 17:45:52+00:00
- **Updated**: 2021-06-01 17:45:52+00:00
- **Authors**: David Khachaturov, Ilia Shumailov, Yiren Zhao, Nicolas Papernot, Ross Anderson
- **Comment**: Proceedings of the 38th International Conference on Machine Learning
  (ICML 2021)
- **Journal**: None
- **Summary**: Inpainting is a learned interpolation technique that is based on generative modeling and used to populate masked or missing pieces in an image; it has wide applications in picture editing and retouching. Recently, inpainting started being used for watermark removal, raising concerns. In this paper we study how to manipulate it using our markpainting technique. First, we show how an image owner with access to an inpainting model can augment their image in such a way that any attempt to edit it using that model will add arbitrary visible information. We find that we can target multiple different models simultaneously with our technique. This can be designed to reconstitute a watermark if the editor had been trying to remove it. Second, we show that our markpainting technique is transferable to models that have different architectures or were trained on different datasets, so watermarks created using it are difficult for adversaries to remove. Markpainting is novel and can be used as a manipulation alarm that becomes visible in the event of inpainting.



### You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.00666v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.00666v3)
- **Published**: 2021-06-01 17:54:09+00:00
- **Updated**: 2021-10-27 02:14:12+00:00
- **Authors**: Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu
- **Comment**: NeurIPS 2021 Camera Ready
- **Journal**: None
- **Summary**: Can Transformer perform 2D object- and region-level recognition from a pure sequence-to-sequence perspective with minimal knowledge about the 2D spatial structure? To answer this question, we present You Only Look at One Sequence (YOLOS), a series of object detection models based on the vanilla Vision Transformer with the fewest possible modifications, region priors, as well as inductive biases of the target task. We find that YOLOS pre-trained on the mid-sized ImageNet-1k dataset only can already achieve quite competitive performance on the challenging COCO object detection benchmark, e.g., YOLOS-Base directly adopted from BERT-Base architecture can obtain 42.0 box AP on COCO val. We also discuss the impacts as well as limitations of current pre-train schemes and model scaling strategies for Transformer in vision through YOLOS. Code and pre-trained models are available at https://github.com/hustvl/YOLOS.



### What Can I Do Here? Learning New Skills by Imagining Visual Affordances
- **Arxiv ID**: http://arxiv.org/abs/2106.00671v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.00671v2)
- **Published**: 2021-06-01 17:58:02+00:00
- **Updated**: 2021-06-13 01:30:53+00:00
- **Authors**: Alexander Khazatsky, Ashvin Nair, Daniel Jing, Sergey Levine
- **Comment**: 10 pages, 10 figures. Presented at ICRA 2021. Project website:
  https://sites.google.com/view/val-rl
- **Journal**: None
- **Summary**: A generalist robot equipped with learned skills must be able to perform many tasks in many different environments. However, zero-shot generalization to new settings is not always possible. When the robot encounters a new environment or object, it may need to finetune some of its previously learned skills to accommodate this change. But crucially, previously learned behaviors and models should still be suitable to accelerate this relearning. In this paper, we aim to study how generative models of possible outcomes can allow a robot to learn visual representations of affordances, so that the robot can sample potentially possible outcomes in new situations, and then further train its policy to achieve those outcomes. In effect, prior data is used to learn what kinds of outcomes may be possible, such that when the robot encounters an unfamiliar setting, it can sample potential outcomes from its model, attempt to reach them, and thereby update both its skills and its outcome model. This approach, visuomotor affordance learning (VAL), can be used to train goal-conditioned policies that operate on raw image inputs, and can rapidly learn to manipulate new objects via our proposed affordance-directed exploration scheme. We show that VAL can utilize prior data to solve real-world tasks such drawer opening, grasping, and placing objects in new scenes with only five minutes of online experience in the new scene.



### Fidelity Estimation Improves Noisy-Image Classification With Pretrained Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.00673v2
- **DOI**: 10.1109/LSP.2021.3104769
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00673v2)
- **Published**: 2021-06-01 17:58:32+00:00
- **Updated**: 2021-10-04 19:06:23+00:00
- **Authors**: Xiaoyu Lin, Deblina Bhattacharjee, Majed El Helou, Sabine Süsstrunk
- **Comment**: Published in IEEE Signal Processing Letters
- **Journal**: IEEE Signal Processing Letters 28 (2021) 1719 - 1723
- **Summary**: Image classification has significantly improved using deep learning. This is mainly due to convolutional neural networks (CNNs) that are capable of learning rich feature extractors from large datasets. However, most deep learning classification methods are trained on clean images and are not robust when handling noisy ones, even if a restoration preprocessing step is applied. While novel methods address this problem, they rely on modified feature extractors and thus necessitate retraining. We instead propose a method that can be applied on a $pretrained$ classifier. Our method exploits a fidelity map estimate that is fused into the internal representations of the feature extractor, thereby guiding the attention of the network and making it more robust to noisy data. We improve the noisy-image classification (NIC) results by significantly large margins, especially at high noise levels, and come close to the fully retrained approaches. Furthermore, as proof of concept, we show that when using our oracle fidelity map we even outperform the fully retrained methods, whether trained on noisy or restored images.



### VILA: Improving Structured Content Extraction from Scientific PDFs Using Visual Layout Groups
- **Arxiv ID**: http://arxiv.org/abs/2106.00676v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00676v3)
- **Published**: 2021-06-01 17:59:00+00:00
- **Updated**: 2022-01-05 15:59:32+00:00
- **Authors**: Zejiang Shen, Kyle Lo, Lucy Lu Wang, Bailey Kuehl, Daniel S. Weld, Doug Downey
- **Comment**: To appear in TACL 2022. The arXiv version is a pre-MIT Press
  publication version. (17 pages, 5 figures, 9 tables)
- **Journal**: None
- **Summary**: Accurately extracting structured content from PDFs is a critical first step for NLP over scientific papers. Recent work has improved extraction accuracy by incorporating elementary layout information, e.g., each token's 2D position on the page, into language model pretraining. We introduce new methods that explicitly model VIsual LAyout (VILA) groups, i.e., text lines or text blocks, to further improve performance. In our I-VILA approach, we show that simply inserting special tokens denoting layout group boundaries into model inputs can lead to a 1.9% Macro F1 improvement in token classification. In the H-VILA approach, we show that hierarchical encoding of layout-groups can result in up-to 47% inference time reduction with less than 0.8% Macro F1 loss. Unlike prior layout-aware approaches, our methods do not require expensive additional pretraining, only fine-tuning, which we show can reduce training cost by up to 95%. Experiments are conducted on a newly curated evaluation suite, S2-VLUE, that unifies existing automatically-labeled datasets and includes a new dataset of manual annotations covering diverse papers from 19 scientific disciplines. Pre-trained weights, benchmark datasets, and source code are available at https://github.com/allenai/VILA.



### Bootstrap Your Own Correspondences
- **Arxiv ID**: http://arxiv.org/abs/2106.00677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00677v1)
- **Published**: 2021-06-01 17:59:08+00:00
- **Updated**: 2021-06-01 17:59:08+00:00
- **Authors**: Mohamed El Banani, Justin Johnson
- **Comment**: Preprint. 10 pages, 4 figures
- **Journal**: None
- **Summary**: Geometric feature extraction is a crucial component of point cloud registration pipelines. Recent work has demonstrated how supervised learning can be leveraged to learn better and more compact 3D features. However, those approaches' reliance on ground-truth annotation limits their scalability. We propose BYOC: a self-supervised approach that learns visual and geometric features from RGB-D video without relying on ground-truth pose or correspondence. Our key observation is that randomly-initialized CNNs readily provide us with good correspondences; allowing us to bootstrap the learning of both visual and geometric features. Our approach combines classic ideas from point cloud registration with more recent representation learning approaches. We evaluate our approach on indoor scene datasets and find that our method outperforms traditional and learned descriptors, while being competitive with current state-of-the-art supervised approaches.



### Evaluating Recipes Generated from Functional Object-Oriented Network
- **Arxiv ID**: http://arxiv.org/abs/2106.00728v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00728v1)
- **Published**: 2021-06-01 19:00:52+00:00
- **Updated**: 2021-06-01 19:00:52+00:00
- **Authors**: Md Sadman Sakib, Hailey Baez, David Paulius, Yu Sun
- **Comment**: This manuscript has been accepted at Ubiquitous Robots 2021
- **Journal**: None
- **Summary**: The functional object-oriented network (FOON) has been introduced as a knowledge representation, which takes the form of a graph, for symbolic task planning. To get a sequential plan for a manipulation task, a robot can obtain a task tree through a knowledge retrieval process from the FOON. To evaluate the quality of an acquired task tree, we compare it with a conventional form of task knowledge, such as recipes or manuals. We first automatically convert task trees to recipes, and we then compare them with the human-created recipes in the Recipe1M+ dataset via a survey. Our preliminary study finds no significant difference between the recipes in Recipe1M+ and the recipes generated from FOON task trees in terms of correctness, completeness, and clarity.



### ICDAR 2021 Competition on On-Line Signature Verification
- **Arxiv ID**: http://arxiv.org/abs/2106.00739v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2106.00739v1)
- **Published**: 2021-06-01 19:33:46+00:00
- **Updated**: 2021-06-01 19:33:46+00:00
- **Authors**: Ruben Tolosana, Ruben Vera-Rodriguez, Carlos Gonzalez-Garcia, Julian Fierrez, Santiago Rengifo, Aythami Morales, Javier Ortega-Garcia, Juan Carlos Ruiz-Garcia, Sergio Romero-Tapiador, Jiajia Jiang, Songxuan Lai, Lianwen Jin, Yecheng Zhu, Javier Galbally, Moises Diaz, Miguel Angel Ferrer, Marta Gomez-Barrero, Ilya Hodashinsky, Konstantin Sarin, Artem Slezkin, Marina Bardamova, Mikhail Svetlakov, Mohammad Saleem, Cintia Lia Szücs, Bence Kovari, Falk Pulsmeyer, Mohamad Wehbi, Dario Zanca, Sumaiya Ahmad, Sarthak Mishra, Suraiya Jabin
- **Comment**: None
- **Journal**: Proc. International Conference on Document Analysis and
  Recognition 2021
- **Summary**: This paper describes the experimental framework and results of the ICDAR 2021 Competition on On-Line Signature Verification (SVC 2021). The goal of SVC 2021 is to evaluate the limits of on-line signature verification systems on popular scenarios (office/mobile) and writing inputs (stylus/finger) through large-scale public databases. Three different tasks are considered in the competition, simulating realistic scenarios as both random and skilled forgeries are simultaneously considered on each task. The results obtained in SVC 2021 prove the high potential of deep learning methods. In particular, the best on-line signature verification system of SVC 2021 obtained Equal Error Rate (EER) values of 3.33% (Task 1), 7.41% (Task 2), and 6.04% (Task 3).   SVC 2021 will be established as an on-going competition, where researchers can easily benchmark their systems against the state of the art in an open common platform using large-scale public databases such as DeepSignDB and SVC2021_EvalDB, and standard experimental protocols.



### Fourier Space Losses for Efficient Perceptual Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2106.00783v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00783v1)
- **Published**: 2021-06-01 20:34:52+00:00
- **Updated**: 2021-06-01 20:34:52+00:00
- **Authors**: Dario Fuoli, Luc Van Gool, Radu Timofte
- **Comment**: None
- **Journal**: None
- **Summary**: Many super-resolution (SR) models are optimized for high performance only and therefore lack efficiency due to large model complexity. As large models are often not practical in real-world applications, we investigate and propose novel loss functions, to enable SR with high perceptual quality from much more efficient models. The representative power for a given low-complexity generator network can only be fully leveraged by strong guidance towards the optimal set of parameters. We show that it is possible to improve the performance of a recently introduced efficient generator architecture solely with the application of our proposed loss functions. In particular, we use a Fourier space supervision loss for improved restoration of missing high-frequency (HF) content from the ground truth image and design a discriminator architecture working directly in the Fourier domain to better match the target HF distribution. We show that our losses' direct emphasis on the frequencies in Fourier-space significantly boosts the perceptual image quality, while at the same time retaining high restoration quality in comparison to previously proposed loss functions for this task. The performance is further improved by utilizing a combination of spatial and frequency domain losses, as both representations provide complementary information during training. On top of that, the trained generator achieves comparable results with and is 2.4x and 48x faster than state-of-the-art perceptual SR methods RankSRGAN and SRFlow respectively.



### Multi-task fully convolutional network for tree species mapping in dense forests using small training hyperspectral data
- **Arxiv ID**: http://arxiv.org/abs/2106.00799v2
- **DOI**: 10.1016/j.isprsjprs.2021.07.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00799v2)
- **Published**: 2021-06-01 21:10:10+00:00
- **Updated**: 2021-09-06 13:44:35+00:00
- **Authors**: Laura Elena Cué La Rosa, Camile Sothe, Raul Queiroz Feitosa, Cláudia Maria de Almeida, Marcos Benedito Schimalski, Dario Augusto Borges Oliveira
- **Comment**: Full version of preprint accepted at ISPRS Journal of Photogrammetry
  and Remote Sensing
- **Journal**: None
- **Summary**: This work proposes a multi-task fully convolutional architecture for tree species mapping in dense forests from sparse and scarce polygon-level annotations using hyperspectral UAV-borne data. Our model implements a partial loss function that enables dense tree semantic labeling outcomes from non-dense training samples, and a distance regression complementary task that enforces tree crown boundary constraints and substantially improves the model performance. Our multi-task architecture uses a shared backbone network that learns common representations for both tasks and two task-specific decoders, one for the semantic segmentation output and one for the distance map regression. We report that introducing the complementary task boosts the semantic segmentation performance compared to the single-task counterpart in up to 11% reaching an average user's accuracy of 88.63% and an average producer's accuracy of 88.59%, achieving state-of-art performance for tree species classification in tropical forests.



### Cleaning and Structuring the Label Space of the iMet Collection 2020
- **Arxiv ID**: http://arxiv.org/abs/2106.00815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00815v1)
- **Published**: 2021-06-01 21:36:26+00:00
- **Updated**: 2021-06-01 21:36:26+00:00
- **Authors**: Vivien Nguyen, Sunnie S. Y. Kim
- **Comment**: A shorter version of this work was accepted to the CVPR 2021 FGVC
  Workshop
- **Journal**: None
- **Summary**: The iMet 2020 dataset is a valuable resource in the space of fine-grained art attribution recognition, but we believe it has yet to reach its true potential. We document the unique properties of the dataset and observe that many of the attribute labels are noisy, more than is implied by the dataset description. Oftentimes, there are also semantic relationships between the labels (e.g., identical, mutual exclusion, subsumption, overlap with uncertainty) which we believe are underutilized. We propose an approach to cleaning and structuring the iMet 2020 labels, and discuss the implications and value of doing so. Further, we demonstrate the benefits of our proposed approach through several experiments. Our code and cleaned labels are available at https://github.com/sunniesuhyoung/iMet2020cleaned.



### nnDetection: A Self-configuring Method for Medical Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.00817v2
- **DOI**: 10.1007/978-3-030-87240-3_51
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.00817v2)
- **Published**: 2021-06-01 21:55:03+00:00
- **Updated**: 2022-01-11 11:11:46+00:00
- **Authors**: Michael Baumgartner, Paul F. Jaeger, Fabian Isensee, Klaus H. Maier-Hein
- **Comment**: MICCAI 2021 (splitted LN data set for camera-ready version); *Michael
  Baumgartner and Paul F. J\"ager contributed equally
- **Journal**: None
- **Summary**: Simultaneous localisation and categorization of objects in medical images, also referred to as medical object detection, is of high clinical relevance because diagnostic decisions often depend on rating of objects rather than e.g. pixels. For this task, the cumbersome and iterative process of method configuration constitutes a major research bottleneck. Recently, nnU-Net has tackled this challenge for the task of image segmentation with great success. Following nnU-Net's agenda, in this work we systematize and automate the configuration process for medical object detection. The resulting self-configuring method, nnDetection, adapts itself without any manual intervention to arbitrary medical detection problems while achieving results en par with or superior to the state-of-the-art. We demonstrate the effectiveness of nnDetection on two public benchmarks, ADAM and LUNA16, and propose 11 further medical object detection tasks on public data sets for comprehensive method evaluation. Code is at https://github.com/MIC-DKFZ/nnDetection .



### Refining the bounding volumes for lossless compression of voxelized point clouds geometry
- **Arxiv ID**: http://arxiv.org/abs/2106.00828v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.00828v1)
- **Published**: 2021-06-01 22:16:06+00:00
- **Updated**: 2021-06-01 22:16:06+00:00
- **Authors**: Emre Can Kaya, Sebastian Schwarz, Ioan Tabus
- **Comment**: ICIP \c{opyright} 2021 IEEE. Personal use of this material is
  permitted. Permission from IEEE must be obtained for all other uses, in any
  current or future media, including reprinting/republishing this material for
  advertising or promotional purposes, creating new collective works, for
  resale or redistribution to servers or lists, or reuse of any copyrighted
  component of this work in other works
- **Journal**: None
- **Summary**: This paper describes a novel lossless compression method for point cloud geometry, building on a recent lossy compression method that aimed at reconstructing only the bounding volume of a point cloud. The proposed scheme starts by partially reconstructing the geometry from the two depthmaps associated to a single projection direction. The partial reconstruction obtained from the depthmaps is completed to a full reconstruction of the point cloud by sweeping section by section along one direction and encoding the points which were not contained in the two depthmaps. The main ingredient is a list-based encoding of the inner points (situated inside the feasible regions) by a novel arithmetic three dimensional context coding procedure that efficiently utilizes rotational invariances present in the input data. State-of-the-art bits-per-voxel results are obtained on benchmark datasets.



### Dataset for eye-tracking tasks
- **Arxiv ID**: http://arxiv.org/abs/2106.07554v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.07554v1)
- **Published**: 2021-06-01 23:54:23+00:00
- **Updated**: 2021-06-01 23:54:23+00:00
- **Authors**: R. Ildar
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years many different deep neural networks were developed, but due to a large number of layers in deep networks, their training requires a long time and a large number of datasets. Today is popular to use trained deep neural networks for various tasks, even for simple ones in which such deep networks are not required. The well-known deep networks such as YoloV3, SSD, etc. are intended for tracking and monitoring various objects, therefore their weights are heavy and the overall accuracy for a specific task is low. Eye-tracking tasks need to detect only one object - an iris in a given area. Therefore, it is logical to use a neural network only for this task. But the problem is the lack of suitable datasets for training the model. In the manuscript, we presented a dataset that is suitable for training custom models of convolutional neural networks for eye-tracking tasks. Using data set data, each user can independently pre-train the convolutional neural network models for eye-tracking tasks. This dataset contains annotated 10,000 eye images in an extension of 416 by 416 pixels. The table with annotation information shows the coordinates and radius of the eye for each image. This manuscript can be considered as a guide for the preparation of datasets for eye-tracking devices



### Adversarially Adaptive Normalization for Single Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2106.01899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.01899v1)
- **Published**: 2021-06-01 23:58:23+00:00
- **Updated**: 2021-06-01 23:58:23+00:00
- **Authors**: Xinjie Fan, Qifei Wang, Junjie Ke, Feng Yang, Boqing Gong, Mingyuan Zhou
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Single domain generalization aims to learn a model that performs well on many unseen domains with only one domain data for training. Existing works focus on studying the adversarial domain augmentation (ADA) to improve the model's generalization capability. The impact on domain generalization of the statistics of normalization layers is still underinvestigated. In this paper, we propose a generic normalization approach, adaptive standardization and rescaling normalization (ASR-Norm), to complement the missing part in previous works. ASR-Norm learns both the standardization and rescaling statistics via neural networks. This new form of normalization can be viewed as a generic form of the traditional normalizations. When trained with ADA, the statistics in ASR-Norm are learned to be adaptive to the data coming from different domains, and hence improves the model generalization performance across domains, especially on the target domain with large discrepancy from the source domain. The experimental results show that ASR-Norm can bring consistent improvement to the state-of-the-art ADA approaches by 1.6%, 2.7%, and 6.3% averagely on the Digits, CIFAR-10-C, and PACS benchmarks, respectively. As a generic tool, the improvement introduced by ASR-Norm is agnostic to the choice of ADA methods.



