# Arxiv Papers in cs.CV on 2021-06-30
### 10-mega pixel snapshot compressive imaging with a hybrid coded aperture
- **Arxiv ID**: http://arxiv.org/abs/2106.15765v2
- **DOI**: 10.1364/PRJ.435256
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2106.15765v2)
- **Published**: 2021-06-30 01:09:24+00:00
- **Updated**: 2021-08-15 08:37:42+00:00
- **Authors**: Zhihong Zhang, Chao Deng, Yang Liu, Xin Yuan, Jinli Suo, Qionghai Dai
- **Comment**: 11 pages, 8 figures, accepted by Photonics Research
- **Journal**: None
- **Summary**: High resolution images are widely used in our daily life, whereas high-speed video capture is challenging due to the low frame rate of cameras working at the high resolution mode. Digging deeper, the main bottleneck lies in the low throughput of existing imaging systems. Towards this end, snapshot compressive imaging (SCI) was proposed as a promising solution to improve the throughput of imaging systems by compressive sampling and computational reconstruction. During acquisition, multiple high-speed images are encoded and collapsed to a single measurement. After this, algorithms are employed to retrieve the video frames from the coded snapshot. Recently developed Plug-and-Play (PnP) algorithms make it possible for SCI reconstruction in large-scale problems. However, the lack of high-resolution encoding systems still precludes SCI's wide application. In this paper, we build a novel hybrid coded aperture snapshot compressive imaging (HCA-SCI) system by incorporating a dynamic liquid crystal on silicon and a high-resolution lithography mask. We further implement a PnP reconstruction algorithm with cascaded denoisers for high quality reconstruction. Based on the proposed HCA-SCI system and algorithm, we achieve a 10-mega pixel SCI system to capture high-speed scenes, leading to a high throughput of 4.6G voxels per second. Both simulation and real data experiments verify the feasibility and performance of our proposed HCA-SCI scheme.



### Dense Graph Convolutional Neural Networks on 3D Meshes for 3D Object Segmentation and Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.15778v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2106.15778v1)
- **Published**: 2021-06-30 02:17:16+00:00
- **Updated**: 2021-06-30 02:17:16+00:00
- **Authors**: Wenming Tang Guoping Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents new designs of graph convolutional neural networks (GCNs) on 3D meshes for 3D object segmentation and classification. We use the faces of the mesh as basic processing units and represent a 3D mesh as a graph where each node corresponds to a face. To enhance the descriptive power of the graph, we introduce a 1-ring face neighbourhood structure to derive novel multi-dimensional spatial and structure features to represent the graph nodes. Based on this new graph representation, we then design a densely connected graph convolutional block which aggregates local and regional features as the key construction component to build effective and efficient practical GCN models for 3D object classification and segmentation. We will present experimental results to show that our new technique outperforms state of the art where our models are shown to have the smallest number of parameters and consietently achieve the highest accuracies across a number of benchmark datasets. We will also present ablation studies to demonstrate the soundness of our design principles and the effectiveness of our practical models.



### Long-Short Temporal Modeling for Efficient Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.15787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15787v1)
- **Published**: 2021-06-30 02:54:13+00:00
- **Updated**: 2021-06-30 02:54:13+00:00
- **Authors**: Liyu Wu, Yuexian Zou, Can Zhang
- **Comment**: Accepted by ICASSP 2021
- **Journal**: None
- **Summary**: Efficient long-short temporal modeling is key for enhancing the performance of action recognition task. In this paper, we propose a new two-stream action recognition network, termed as MENet, consisting of a Motion Enhancement (ME) module and a Video-level Aggregation (VLA) module to achieve long-short temporal modeling. Specifically, motion representations have been proved effective in capturing short-term and high-frequency action. However, current motion representations are calculated from adjacent frames, which may have poor interpretation and bring useless information (noisy or blank). Thus, for short-term motions, we design an efficient ME module to enhance the short-term motions by mingling the motion saliency among neighboring segments. As for long-term aggregations, VLA is adopted at the top of the appearance branch to integrate the long-term dependencies across all segments. The two components of MENet are complementary in temporal modeling. Extensive experiments are conducted on UCF101 and HMDB51 benchmarks, which verify the effectiveness and efficiency of our proposed MENet.



### Exploring Localization for Self-supervised Fine-grained Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.15788v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15788v4)
- **Published**: 2021-06-30 02:56:26+00:00
- **Updated**: 2022-10-11 06:31:41+00:00
- **Authors**: Di Wu, Siyuan Li, Zelin Zang, Stan Z. Li
- **Comment**: BMVC 2022 camera-ready. 15 pages (main) with 5 pages appendix
- **Journal**: None
- **Summary**: Self-supervised contrastive learning has demonstrated great potential in learning visual representations. Despite their success in various downstream tasks such as image classification and object detection, self-supervised pre-training for fine-grained scenarios is not fully explored. We point out that current contrastive methods are prone to memorizing background/foreground texture and therefore have a limitation in localizing the foreground object. Analysis suggests that learning to extract discriminative texture information and localization are equally crucial for fine-grained self-supervised pre-training. Based on our findings, we introduce cross-view saliency alignment (CVSA), a contrastive learning framework that first crops and swaps saliency regions of images as a novel view generation and then guides the model to localize on foreground objects via a cross-view alignment loss. Extensive experiments on both small- and large-scale fine-grained classification benchmarks show that CVSA significantly improves the learned representation.



### Multi-Source Domain Adaptation for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.15793v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15793v1)
- **Published**: 2021-06-30 03:17:20+00:00
- **Updated**: 2021-06-30 03:17:20+00:00
- **Authors**: Xingxu Yao, Sicheng Zhao, Pengfei Xu, Jufeng Yang
- **Comment**: None
- **Journal**: None
- **Summary**: To reduce annotation labor associated with object detection, an increasing number of studies focus on transferring the learned knowledge from a labeled source domain to another unlabeled target domain. However, existing methods assume that the labeled data are sampled from a single source domain, which ignores a more generalized scenario, where labeled data are from multiple source domains. For the more challenging task, we propose a unified Faster R-CNN based framework, termed Divide-and-Merge Spindle Network (DMSN), which can simultaneously enhance domain invariance and preserve discriminative power. Specifically, the framework contains multiple source subnets and a pseudo target subnet. First, we propose a hierarchical feature alignment strategy to conduct strong and weak alignments for low- and high-level features, respectively, considering their different effects for object detection. Second, we develop a novel pseudo subnet learning algorithm to approximate optimal parameters of pseudo target subset by weighted combination of parameters in different source subnets. Finally, a consistency regularization for region proposal network is proposed to facilitate each subnet to learn more abstract invariances. Extensive experiments on different adaptation scenarios demonstrate the effectiveness of the proposed model.



### Monocular 3D Object Detection: An Extrinsic Parameter Free Approach
- **Arxiv ID**: http://arxiv.org/abs/2106.15796v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15796v2)
- **Published**: 2021-06-30 03:35:51+00:00
- **Updated**: 2021-12-08 06:54:04+00:00
- **Authors**: Yunsong Zhou, Yuan He, Hongzi Zhu, Cheng Wang, Hongyang Li, Qinhong Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular 3D object detection is an important task in autonomous driving. It can be easily intractable where there exists ego-car pose change w.r.t. ground plane. This is common due to the slight fluctuation of road smoothness and slope. Due to the lack of insight in industrial application, existing methods on open datasets neglect the camera pose information, which inevitably results in the detector being susceptible to camera extrinsic parameters. The perturbation of objects is very popular in most autonomous driving cases for industrial products. To this end, we propose a novel method to capture camera pose to formulate the detector free from extrinsic perturbation. Specifically, the proposed framework predicts camera extrinsic parameters by detecting vanishing point and horizon change. A converter is designed to rectify perturbative features in the latent space. By doing so, our 3D detector works independent of the extrinsic parameter variations and produces accurate results in realistic cases, e.g., potholed and uneven roads, where almost all existing monocular detectors fail to handle. Experiments demonstrate our method yields the best performance compared with the other state-of-the-arts by a large margin on both KITTI 3D and nuScenes datasets.



### Content-Aware Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.15797v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15797v2)
- **Published**: 2021-06-30 03:54:35+00:00
- **Updated**: 2021-07-23 07:32:54+00:00
- **Authors**: Yong Guo, Yaofo Chen, Mingkui Tan, Kui Jia, Jian Chen, Jingdong Wang
- **Comment**: Accepted by Neural Networks
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have achieved great success due to the powerful feature learning ability of convolution layers. Specifically, the standard convolution traverses the input images/features using a sliding window scheme to extract features. However, not all the windows contribute equally to the prediction results of CNNs. In practice, the convolutional operation on some of the windows (e.g., smooth windows that contain very similar pixels) can be very redundant and may introduce noises into the computation. Such redundancy may not only deteriorate the performance but also incur the unnecessary computational cost. Thus, it is important to reduce the computational redundancy of convolution to improve the performance. To this end, we propose a Content-aware Convolution (CAC) that automatically detects the smooth windows and applies a 1x1 convolutional kernel to replace the original large kernel. In this sense, we are able to effectively avoid the redundant computation on similar pixels. By replacing the standard convolution in CNNs with our CAC, the resultant models yield significantly better performance and lower computational cost than the baseline models with the standard convolution. More critically, we are able to dynamically allocate suitable computation resources according to the data smoothness of different images, making it possible for content-aware computation. Extensive experiments on various computer vision tasks demonstrate the superiority of our method over existing methods.



### When Video Classification Meets Incremental Classes
- **Arxiv ID**: http://arxiv.org/abs/2106.15827v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15827v2)
- **Published**: 2021-06-30 06:12:33+00:00
- **Updated**: 2021-09-01 02:23:10+00:00
- **Authors**: Hanbin Zhao, Xin Qin, Shihao Su, Yongjian Fu, Zibo Lin, Xi Li
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid development of social media, tremendous videos with new classes are generated daily, which raise an urgent demand for video classification methods that can continuously update new classes while maintaining the knowledge of old videos with limited storage and computing resources. In this paper, we summarize this task as Class-Incremental Video Classification (CIVC) and propose a novel framework to address it. As a subarea of incremental learning tasks, the challenge of catastrophic forgetting is unavoidable in CIVC. To better alleviate it, we utilize some characteristics of videos. First, we decompose the spatio-temporal knowledge before distillation rather than treating it as a whole in the knowledge transfer process; trajectory is also used to refine the decomposition. Second, we propose a dual granularity exemplar selection method to select and store representative video instances of old classes and key-frames inside videos under a tight storage budget. We benchmark our method and previous SOTA class-incremental learning methods on Something-Something V2 and Kinetics datasets, and our method outperforms previous methods significantly.



### Semantic Segmentation of Periocular Near-Infra-Red Eye Images Under Alcohol Effects
- **Arxiv ID**: http://arxiv.org/abs/2106.15828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15828v1)
- **Published**: 2021-06-30 06:15:17+00:00
- **Updated**: 2021-06-30 06:15:17+00:00
- **Authors**: Juan Tapia, Enrique Lopez Droguett, Andres Valenzuela, Daniel Benalcazar, Leonardo Causa, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a new framework to detect, segment, and estimate the localization of the eyes from a periocular Near-Infra-Red iris image under alcohol consumption. The purpose of the system is to measure the fitness for duty. Fitness systems allow us to determine whether a person is physically or psychologically able to perform their tasks. Our framework is based on an object detector trained from scratch to detect both eyes from a single image. Then, two efficient networks were used for semantic segmentation; a Criss-Cross attention network and DenseNet10, with only 122,514 and 210,732 parameters, respectively. These networks can find the pupil, iris, and sclera. In the end, the binary output eye mask is used for pupil and iris diameter estimation with high precision. Five state-of-the-art algorithms were used for this purpose. A mixed proposal reached the best results. A second contribution is establishing an alcohol behavior curve to detect the alcohol presence utilizing a stream of images captured from an iris instance. Also, a manually labeled database with more than 20k images was created. Our best method obtains a mean Intersection-over-Union of 94.54% with DenseNet10 with only 210,732 parameters and an error of only 1-pixel on average.



### The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning
- **Arxiv ID**: http://arxiv.org/abs/2106.15831v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15831v1)
- **Published**: 2021-06-30 06:21:42+00:00
- **Updated**: 2021-06-30 06:21:42+00:00
- **Authors**: Anders Andreassen, Yasaman Bahri, Behnam Neyshabur, Rebecca Roelofs
- **Comment**: 27 pages, 25 figures
- **Journal**: None
- **Summary**: Although machine learning models typically experience a drop in performance on out-of-distribution data, accuracies on in- versus out-of-distribution data are widely observed to follow a single linear trend when evaluated across a testbed of models. Models that are more accurate on the out-of-distribution data relative to this baseline exhibit "effective robustness" and are exceedingly rare. Identifying such models, and understanding their properties, is key to improving out-of-distribution performance. We conduct a thorough empirical investigation of effective robustness during fine-tuning and surprisingly find that models pre-trained on larger datasets exhibit effective robustness during training that vanishes at convergence. We study how properties of the data influence effective robustness, and we show that it increases with the larger size, more diversity, and higher example difficulty of the dataset. We also find that models that display effective robustness are able to correctly classify 10% of the examples that no other current testbed model gets correct. Finally, we discuss several strategies for scaling effective robustness to the high-accuracy regime to improve the out-of-distribution accuracy of state-of-the-art models.



### A Structured Analysis of the Video Degradation Effects on the Performance of a Machine Learning-enabled Pedestrian Detector
- **Arxiv ID**: http://arxiv.org/abs/2106.15889v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15889v1)
- **Published**: 2021-06-30 08:30:12+00:00
- **Updated**: 2021-06-30 08:30:12+00:00
- **Authors**: Christian Berger
- **Comment**: None
- **Journal**: None
- **Summary**: ML-enabled software systems have been incorporated in many public demonstrations for automated driving (AD) systems. Such solutions have also been considered as a crucial approach to aim at SAE Level 5 systems, where the passengers in such vehicles do not have to interact with the system at all anymore. Already in 2016, Nvidia demonstrated a complete end-to-end approach for training the complete software stack covering perception, planning and decision making, and the actual vehicle control. While such approaches show the great potential of such ML-enabled systems, there have also been demonstrations where already changes to single pixels in a video frame can potentially lead to completely different decisions with dangerous consequences. In this paper, a structured analysis has been conducted to explore video degradation effects on the performance of an ML-enabled pedestrian detector. Firstly, a baseline of applying YOLO to 1,026 frames with pedestrian annotations in the KITTI Vision Benchmark Suite has been established. Next, video degradation candidates for each of these frames were generated using the leading video codecs libx264, libx265, Nvidia HEVC, and AV1: 52 frames for the various compression presets for color and gray-scale frames resulting in 104 degradation candidates per original KITTI frame and 426,816 images in total. YOLO was applied to each image to compute the intersection-over-union (IoU) metric to compare the performance with the original baseline. While aggressively lossy compression settings result in significant performance drops as expected, it was also observed that some configurations actually result in slightly better IoU results compared to the baseline. The findings show that carefully chosen lossy video configurations preserve a decent performance of particular ML-enabled systems while allowing for substantial savings when storing or transmitting data.



### Fast whole-slide cartography in colon cancer histology using superpixels and CNN classification
- **Arxiv ID**: http://arxiv.org/abs/2106.15893v3
- **DOI**: 10.1117/1.JMI.9.2.027501
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15893v3)
- **Published**: 2021-06-30 08:34:06+00:00
- **Updated**: 2022-03-15 18:27:04+00:00
- **Authors**: Frauke Wilm, Michaela Benz, Volker Bruns, Serop Baghdadlian, Jakob Dexl, David Hartmann, Petr Kuritcyn, Martin Weidenfeller, Thomas Wittenberg, Susanne Merkel, Arndt Hartmann, Markus Eckstein, Carol I. Geppert
- **Comment**: 28 pages, 17 figures, 5 tables, published in SPIE Journal of Medical
  Imaging
- **Journal**: J. Med. Imag. 9(2), 027501 (2022)
- **Summary**: Automatic outlining of different tissue types in digitized histological specimen provides a basis for follow-up analyses and can potentially guide subsequent medical decisions. The immense size of whole-slide-images (WSI), however, poses a challenge in terms of computation time. In this regard, the analysis of non-overlapping patches outperforms pixelwise segmentation approaches, but still leaves room for optimization. Furthermore, the division into patches, regardless of the biological structures they contain, is a drawback due to the loss of local dependencies. We propose to subdivide the WSI into coherent regions prior to classification by grouping visually similar adjacent pixels into superpixels. Afterwards, only a random subset of patches per superpixel is classified and patch labels are combined into a superpixel label. We propose a metric for identifying superpixels with an uncertain classification and evaluate two medical applications, namely tumor area and invasive margin estimation and tumor composition analysis. The algorithm has been developed on 159 hand-annotated WSIs of colon resections and its performance is compared to an analysis without prior segmentation. The algorithm shows an average speed-up of 41% and an increase in accuracy from 93.8% to 95.7%. By assigning a rejection label to uncertain superpixels, we further increase the accuracy by 0.4%. Whilst tumor area estimation shows high concordance to the annotated area, the analysis of tumor composition highlights limitations of our approach. By combining superpixel segmentation and patch classification, we designed a fast and accurate framework for whole-slide cartography that is AI-model agnostic and provides the basis for various medical endpoints.



### Non-isomorphic Inter-modality Graph Alignment and Synthesis for Holistic Brain Mapping
- **Arxiv ID**: http://arxiv.org/abs/2107.06281v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.06281v1)
- **Published**: 2021-06-30 08:59:55+00:00
- **Updated**: 2021-06-30 08:59:55+00:00
- **Authors**: Islem Mhiri, Ahmed Nebli, Mohamed Ali Mahjoub, Islem Rekik
- **Comment**: None
- **Journal**: None
- **Summary**: Brain graph synthesis marked a new era for predicting a target brain graph from a source one without incurring the high acquisition cost and processing time of neuroimaging data. However, existing multi-modal graph synthesis frameworks have several limitations. First, they mainly focus on generating graphs from the same domain (intra-modality), overlooking the rich multimodal representations of brain connectivity (inter-modality). Second, they can only handle isomorphic graph generation tasks, limiting their generalizability to synthesizing target graphs with a different node size and topological structure from those of the source one. More importantly, both target and source domains might have different distributions, which causes a domain fracture between them (i.e., distribution misalignment). To address such challenges, we propose an inter-modality aligner of non-isomorphic graphs (IMANGraphNet) framework to infer a target graph modality based on a given modality. Our three core contributions lie in (i) predicting a target graph (e.g., functional) from a source graph (e.g., morphological) based on a novel graph generative adversarial network (gGAN); (ii) using non-isomorphic graphs for both source and target domains with a different number of nodes, edges and structure; and (iii) enforcing the predicted target distribution to match that of the ground truth graphs using a graph autoencoder to relax the designed loss oprimization. To handle the unstable behavior of gGAN, we design a new Ground Truth-Preserving (GT-P) loss function to guide the generator in learning the topological structure of ground truth brain graphs. Our comprehensive experiments on predicting functional from morphological graphs demonstrate the outperformance of IMANGraphNet in comparison with its variants. This can be further leveraged for integrative and holistic brain mapping in health and disease.



### Positive-unlabeled Learning for Cell Detection in Histopathology Images with Incomplete Annotations
- **Arxiv ID**: http://arxiv.org/abs/2106.15918v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15918v1)
- **Published**: 2021-06-30 09:20:25+00:00
- **Updated**: 2021-06-30 09:20:25+00:00
- **Authors**: Zipei Zhao, Fengqian Pang, Zhiwen Liu, Chuyang Ye
- **Comment**: Accepted by MICCAI 2021
- **Journal**: None
- **Summary**: Cell detection in histopathology images is of great value in clinical practice. \textit{Convolutional neural networks} (CNNs) have been applied to cell detection to improve the detection accuracy, where cell annotations are required for network training. However, due to the variety and large number of cells, complete annotations that include every cell of interest in the training images can be challenging. Usually, incomplete annotations can be achieved, where positive labeling results are carefully examined to ensure their reliability but there can be other positive instances, i.e., cells of interest, that are not included in the annotations. This annotation strategy leads to a lack of knowledge about true negative samples. Most existing methods simply treat instances that are not labeled as positive as truly negative during network training, which can adversely affect the network performance. In this work, to address the problem of incomplete annotations, we formulate the training of detection networks as a positive-unlabeled learning problem. Specifically, the classification loss in network training is revised to take into account incomplete annotations, where the terms corresponding to negative samples are approximated with the true positive samples and the other samples of which the labels are unknown. To evaluate the proposed method, experiments were performed on a publicly available dataset for mitosis detection in breast cancer cells, and the experimental results show that our method improves the performance of cell detection given incomplete annotations for training.



### Augmented Shortcuts for Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2106.15941v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15941v1)
- **Published**: 2021-06-30 09:48:30+00:00
- **Updated**: 2021-06-30 09:48:30+00:00
- **Authors**: Yehui Tang, Kai Han, Chang Xu, An Xiao, Yiping Deng, Chao Xu, Yunhe Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Transformer models have achieved great progress on computer vision tasks recently. The rapid development of vision transformers is mainly contributed by their high representation ability for extracting informative features from input images. However, the mainstream transformer models are designed with deep architectures, and the feature diversity will be continuously reduced as the depth increases, i.e., feature collapse. In this paper, we theoretically analyze the feature collapse phenomenon and study the relationship between shortcuts and feature diversity in these transformer models. Then, we present an augmented shortcut scheme, which inserts additional paths with learnable parameters in parallel on the original shortcuts. To save the computational costs, we further explore an efficient approach that uses the block-circulant projection to implement augmented shortcuts. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of the proposed method, which brings about 1% accuracy increase of the state-of-the-art visual transformers without obviously increasing their parameters and FLOPs.



### A survey on computational spectral reconstruction methods from RGB to hyperspectral imaging
- **Arxiv ID**: http://arxiv.org/abs/2106.15944v2
- **DOI**: 10.1038/s41598-022-16223-1
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15944v2)
- **Published**: 2021-06-30 09:52:41+00:00
- **Updated**: 2022-07-13 14:01:13+00:00
- **Authors**: Jingang Zhang, Runmu Su, Wenqi Ren, Qiang Fu, Felix Heide, Yunfeng Nie
- **Comment**: None
- **Journal**: Scientific Reports | (2022) 12:11905
- **Summary**: Hyperspectral imaging enables versatile applications due to its competence in capturing abundant spatial and spectral information, which are crucial for identifying substances. However, the devices for acquiring hyperspectral images are expensive and complicated. Therefore, many alternative spectral imaging methods have been proposed by directly reconstructing the hyperspectral information from lower-cost, more available RGB images. We present a thorough investigation of these state-of-the-art spectral reconstruction methods from the widespread RGB images. A systematic study and comparison of more than 25 methods has revealed that most of the data-driven deep learning methods are superior to prior-based methods in terms of reconstruction accuracy and quality despite lower speeds. This comprehensive review can serve as a fruitful reference source for peer researchers, thus further inspiring future development directions in related domains.



### SOLO: A Simple Framework for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.15947v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15947v1)
- **Published**: 2021-06-30 09:56:54+00:00
- **Updated**: 2021-06-30 09:56:54+00:00
- **Authors**: Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, Lei Li
- **Comment**: 20 pages. arXiv admin note: substantial text overlap with
  arXiv:1912.04488, arXiv:2003.10152
- **Journal**: None
- **Summary**: Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that has made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the 'detect-then-segment' strategy (e.g., Mask R-CNN), or predict embedding vectors first then cluster pixels into individual instances. In this paper, we view the task of instance segmentation from a completely new perspective by introducing the notion of "instance categories", which assigns categories to each pixel within an instance according to the instance's location. With this notion, we propose segmenting objects by locations (SOLO), a simple, direct, and fast framework for instance segmentation with strong performance. We derive a few SOLO variants (e.g., Vanilla SOLO, Decoupled SOLO, Dynamic SOLO) following the basic principle. Our method directly maps a raw input image to the desired object categories and instance masks, eliminating the need for the grouping post-processing or the bounding box detection. Our approach achieves state-of-the-art results for instance segmentation in terms of both speed and accuracy, while being considerably simpler than the existing methods. Besides instance segmentation, our method yields state-of-the-art results in object detection (from our mask byproduct) and panoptic segmentation. We further demonstrate the flexibility and high-quality segmentation of SOLO by extending it to perform one-stage instance-level image matting. Code is available at: https://git.io/AdelaiDet



### BLNet: A Fast Deep Learning Framework for Low-Light Image Enhancement with Noise Removal and Color Restoration
- **Arxiv ID**: http://arxiv.org/abs/2106.15953v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2106.15953v1)
- **Published**: 2021-06-30 10:06:16+00:00
- **Updated**: 2021-06-30 10:06:16+00:00
- **Authors**: Xinxu Wei, Xianshi Zhang, Shisen Wang, Cheng Cheng, Yanlin Huang, Kaifu Yang, Yongjie Li
- **Comment**: 13 pages, 12 figures, journal
- **Journal**: None
- **Summary**: Images obtained in real-world low-light conditions are not only low in brightness, but they also suffer from many other types of degradation, such as color bias, unknown noise, detail loss and halo artifacts. In this paper, we propose a very fast deep learning framework called Bringing the Lightness (denoted as BLNet) that consists of two U-Nets with a series of well-designed loss functions to tackle all of the above degradations. Based on Retinex Theory, the decomposition net in our model can decompose low-light images into reflectance and illumination and remove noise in the reflectance during the decomposition phase. We propose a Noise and Color Bias Control module (NCBC Module) that contains a convolutional neural network and two loss functions (noise loss and color loss). This module is only used to calculate the loss functions during the training phase, so our method is very fast during the test phase. This module can smooth the reflectance to achieve the purpose of noise removal while preserving details and edge information and controlling color bias. We propose a network that can be trained to learn the mapping between low-light and normal-light illumination and enhance the brightness of images taken in low-light illumination. We train and evaluate the performance of our proposed model over the real-world Low-Light (LOL) dataset), and we also test our model over several other frequently used datasets (LIME, DICM and MEF datasets). We conduct extensive experiments to demonstrate that our approach achieves a promising effect with good rubustness and generalization and outperforms many other state-of-the-art methods qualitatively and quantitatively. Our method achieves high speed because we use loss functions instead of introducing additional denoisers for noise removal and color correction. The code and model are available at https://github.com/weixinxu666/BLNet.



### Word-level Sign Language Recognition with Multi-stream Neural Networks Focusing on Local Regions
- **Arxiv ID**: http://arxiv.org/abs/2106.15989v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.15989v1)
- **Published**: 2021-06-30 11:30:06+00:00
- **Updated**: 2021-06-30 11:30:06+00:00
- **Authors**: Mizuki Maruyama, Shuvozit Ghose, Katsufumi Inoue, Partha Pratim Roy, Masakazu Iwamura, Michifumi Yoshioka
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, Word-level Sign Language Recognition (WSLR) research has gained popularity in the computer vision community, and thus various approaches have been proposed. Among these approaches, the method using I3D network achieves the highest recognition accuracy on large public datasets for WSLR. However, the method with I3D only utilizes appearance information of the upper body of the signers to recognize sign language words. On the other hand, in WSLR, the information of local regions, such as the hand shape and facial expression, and the positional relationship among the body and both hands are important. Thus in this work, we utilized local region images of both hands and face, along with skeletal information to capture local information and the positions of both hands relative to the body, respectively. In other words, we propose a novel multi-stream WSLR framework, in which a stream with local region images and a stream with skeletal information are introduced by extending I3D network to improve the recognition accuracy of WSLR. From the experimental results on WLASL dataset, it is evident that the proposed method has achieved about 15% improvement in the Top-1 accuracy than the existing conventional methods.



### Cyclist Trajectory Forecasts by Incorporation of Multi-View Video Information
- **Arxiv ID**: http://arxiv.org/abs/2106.15991v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15991v1)
- **Published**: 2021-06-30 11:34:43+00:00
- **Updated**: 2021-06-30 11:34:43+00:00
- **Authors**: Stefan Zernetsch, Oliver Trupp, Viktor Kress, Konrad Doll, Bernhard Sick
- **Comment**: None
- **Journal**: None
- **Summary**: This article presents a novel approach to incorporate visual cues from video-data from a wide-angle stereo camera system mounted at an urban intersection into the forecast of cyclist trajectories. We extract features from image and optical flow (OF) sequences using 3D convolutional neural networks (3D-ConvNet) and combine them with features extracted from the cyclist's past trajectory to forecast future cyclist positions. By the use of additional information, we are able to improve positional accuracy by about 7.5 % for our test dataset and by up to 22 % for specific motion types compared to a method solely based on past trajectories. Furthermore, we compare the use of image sequences to the use of OF sequences as additional information, showing that OF alone leads to significant improvements in positional accuracy. By training and testing our methods using a real-world dataset recorded at a heavily frequented public intersection and evaluating the methods' runtimes, we demonstrate the applicability in real traffic scenarios. Our code and parts of our dataset are made publicly available.



### Single-Step Adversarial Training for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.15998v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15998v1)
- **Published**: 2021-06-30 11:41:09+00:00
- **Updated**: 2021-06-30 11:41:09+00:00
- **Authors**: Daniel Wiens, Barbara Hammer
- **Comment**: None
- **Journal**: None
- **Summary**: Even though deep neural networks succeed on many different tasks including semantic segmentation, they lack on robustness against adversarial examples. To counteract this exploit, often adversarial training is used. However, it is known that adversarial training with weak adversarial attacks (e.g. using the Fast Gradient Method) does not improve the robustness against stronger attacks. Recent research shows that it is possible to increase the robustness of such single-step methods by choosing an appropriate step size during the training. Finding such a step size, without increasing the computational effort of single-step adversarial training, is still an open challenge. In this work we address the computationally particularly demanding task of semantic segmentation and propose a new step size control algorithm that increases the robustness of single-step adversarial training. The proposed algorithm does not increase the computational effort of single-step adversarial training considerably and also simplifies training, because it is free of meta-parameter. We show that the robustness of our approach can compete with multi-step adversarial training on two popular benchmarks for semantic segmentation.



### Mutual-GAN: Towards Unsupervised Cross-Weather Adaptation with Mutual Information Constraint
- **Arxiv ID**: http://arxiv.org/abs/2106.16000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.16000v1)
- **Published**: 2021-06-30 11:44:22+00:00
- **Updated**: 2021-06-30 11:44:22+00:00
- **Authors**: Jiawei Chen, Yuexiang Li, Kai Ma, Yefeng Zheng
- **Comment**: An extension of our MICCAI paper
- **Journal**: None
- **Summary**: Convolutional neural network (CNN) have proven its success for semantic segmentation, which is a core task of emerging industrial applications such as autonomous driving. However, most progress in semantic segmentation of urban scenes is reported on standard scenarios, i.e., daytime scenes with favorable illumination conditions. In practical applications, the outdoor weather and illumination are changeable, e.g., cloudy and nighttime, which results in a significant drop of semantic segmentation accuracy of CNN only trained with daytime data. In this paper, we propose a novel generative adversarial network (namely Mutual-GAN) to alleviate the accuracy decline when daytime-trained neural network is applied to videos captured under adverse weather conditions. The proposed Mutual-GAN adopts mutual information constraint to preserve image-objects during cross-weather adaptation, which is an unsolved problem for most unsupervised image-to-image translation approaches (e.g., CycleGAN). The proposed Mutual-GAN is evaluated on two publicly available driving video datasets (i.e., CamVid and SYNTHIA). The experimental results demonstrate that our Mutual-GAN can yield visually plausible translated images and significantly improve the semantic segmentation accuracy of daytime-trained deep learning network while processing videos under challenging weathers.



### Extraction of Key-frames of Endoscopic Videos by using Depth Information
- **Arxiv ID**: http://arxiv.org/abs/2107.00005v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00005v1)
- **Published**: 2021-06-30 11:48:23+00:00
- **Updated**: 2021-06-30 11:48:23+00:00
- **Authors**: Pradipta Sasmal, Avinash Paul, M. K. Bhuyan, Yuji Iwahori
- **Comment**: None
- **Journal**: The paper is under consideration at Pattern Recognition Letters,
  Elsevier, 2021
- **Summary**: A deep learning-based monocular depth estimation (MDE) technique is proposed for selection of most informative frames (key frames) of an endoscopic video. In most of the cases, ground truth depth maps of polyps are not readily available and that is why the transfer learning approach is adopted in our method. An endoscopic modalities generally capture thousands of frames. In this scenario, it is quite important to discard low-quality and clinically irrelevant frames of an endoscopic video while the most informative frames should be retained for clinical diagnosis. In this view, a key-frame selection strategy is proposed by utilizing the depth information of polyps. In our method, image moment, edge magnitude, and key-points are considered for adaptively selecting the key frames. One important application of our proposed method could be the 3D reconstruction of polyps with the help of extracted key frames. Also, polyps are localized with the help of extracted depth maps.



### Improving the Efficiency of Transformers for Resource-Constrained Devices
- **Arxiv ID**: http://arxiv.org/abs/2106.16006v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.16006v1)
- **Published**: 2021-06-30 12:10:48+00:00
- **Updated**: 2021-06-30 12:10:48+00:00
- **Authors**: Hamid Tabani, Ajay Balasubramaniam, Shabbir Marzban, Elahe Arani, Bahram Zonooz
- **Comment**: This paper is accepted as a full paper at 24th Euromicro Conference
  on Digital System Design (DSD)
- **Journal**: None
- **Summary**: Transformers provide promising accuracy and have become popular and used in various domains such as natural language processing and computer vision. However, due to their massive number of model parameters, memory and computation requirements, they are not suitable for resource-constrained low-power devices. Even with high-performance and specialized devices, the memory bandwidth can become a performance-limiting bottleneck. In this paper, we present a performance analysis of state-of-the-art vision transformers on several devices. We propose to reduce the overall memory footprint and memory transfers by clustering the model parameters. We show that by using only 64 clusters to represent model parameters, it is possible to reduce the data transfer from the main memory by more than 4x, achieve up to 22% speedup and 39% energy savings on mobile devices with less than 0.1% accuracy loss.



### MissFormer: (In-)attention-based handling of missing observations for trajectory filtering and prediction
- **Arxiv ID**: http://arxiv.org/abs/2106.16009v4
- **DOI**: 10.1007/978-3-030-90439-5_41
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.16009v4)
- **Published**: 2021-06-30 12:12:52+00:00
- **Updated**: 2021-12-02 16:59:10+00:00
- **Authors**: Stefan Becker, Ronny Hug, Wolfgang Hübner, Michael Arens, Brendan T. Morris
- **Comment**: Accepted at the International Symposium on Visual Computing (ISVC)
  2021
- **Journal**: None
- **Summary**: In applications such as object tracking, time-series data inevitably carry missing observations. Following the success of deep learning-based models for various sequence learning tasks, these models increasingly replace classic approaches in object tracking applications for inferring the objects' motion states. While traditional tracking approaches can deal with missing observations, most of their deep counterparts are, by default, not suited for this.   Towards this end, this paper introduces a transformer-based approach for handling missing observations in variable input length trajectory data. The model is formed indirectly by successively increasing the complexity of the demanded inference tasks. Starting from reproducing noise-free trajectories, the model then learns to infer trajectories from noisy inputs. By providing missing tokens, binary-encoded missing events, the model learns to in-attend to missing data and infers a complete trajectory conditioned on the remaining inputs. In the case of a sequence of successive missing events, the model then acts as a pure prediction model. The abilities of the approach are demonstrated on synthetic data and real-world data reflecting prototypical object tracking scenarios.



### Real-world Video Deblurring: A Benchmark Dataset and An Efficient Recurrent Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2106.16028v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.16028v2)
- **Published**: 2021-06-30 12:53:02+00:00
- **Updated**: 2022-10-15 08:56:06+00:00
- **Authors**: Zhihang Zhong, Ye Gao, Yinqiang Zheng, Bo Zheng, Imari Sato
- **Comment**: Accepted by IJCV (extended version of ECCV2020)
- **Journal**: None
- **Summary**: Real-world video deblurring in real time still remains a challenging task due to the complexity of spatially and temporally varying blur itself and the requirement of low computational cost. To improve the network efficiency, we adopt residual dense blocks into RNN cells, so as to efficiently extract the spatial features of the current frame. Furthermore, a global spatio-temporal attention module is proposed to fuse the effective hierarchical features from past and future frames to help better deblur the current frame. Another issue that needs to be addressed urgently is the lack of a real-world benchmark dataset. Thus, we contribute a novel dataset (BSD) to the community, by collecting paired blurry/sharp video clips using a co-axis beam splitter acquisition system. Experimental results show that the proposed method (ESTRNN) can achieve better deblurring performance both quantitatively and qualitatively with less computational cost against state-of-the-art video deblurring methods. In addition, cross-validation experiments between datasets illustrate the high generality of BSD over the synthetic datasets. The code and dataset are released at https://github.com/zzh-tech/ESTRNN.



### ResViT: Residual vision transformers for multi-modal medical image synthesis
- **Arxiv ID**: http://arxiv.org/abs/2106.16031v3
- **DOI**: 10.1109/TMI.2022.3167808
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.16031v3)
- **Published**: 2021-06-30 12:57:37+00:00
- **Updated**: 2022-03-06 11:07:38+00:00
- **Authors**: Onat Dalmaz, Mahmut Yurt, Tolga Çukur
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial models with convolutional neural network (CNN) backbones have recently been established as state-of-the-art in numerous medical image synthesis tasks. However, CNNs are designed to perform local processing with compact filters, and this inductive bias compromises learning of contextual features. Here, we propose a novel generative adversarial approach for medical image synthesis, ResViT, that leverages the contextual sensitivity of vision transformers along with the precision of convolution operators and realism of adversarial learning.} ResViT's generator employs a central bottleneck comprising novel aggregated residual transformer (ART) blocks that synergistically combine residual convolutional and transformer modules. Residual connections in ART blocks promote diversity in captured representations, while a channel compression module distills task-relevant information. A weight sharing strategy is introduced among ART blocks to mitigate computational burden. A unified implementation is introduced to avoid the need to rebuild separate synthesis models for varying source-target modality configurations. Comprehensive demonstrations are performed for synthesizing missing sequences in multi-contrast MRI, and CT images from MRI. Our results indicate superiority of ResViT against competing CNN- and transformer-based methods in terms of qualitative observations and quantitative metrics.



### A Survey on Adversarial Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2106.16056v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.16056v2)
- **Published**: 2021-06-30 13:31:48+00:00
- **Updated**: 2021-07-16 13:38:47+00:00
- **Authors**: William Roy, Glen Kelly, Robert Leer, Frederick Ricardo
- **Comment**: arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text overlap with external source
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have been extremely successful in various application domains. Adversarial image synthesis has drawn increasing attention and made tremendous progress in recent years because of its wide range of applications in many computer vision and image processing problems. Among the many applications of GAN, image synthesis is the most well-studied one, and research in this area has already demonstrated the great potential of using GAN in image synthesis. In this paper, we provide a taxonomy of methods used in image synthesis, review different models for text-to-image synthesis and image-to-image translation, and discuss some evaluation metrics as well as possible future research directions in image synthesis with GAN.



### Leveraging Hidden Structure in Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.16060v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.16060v1)
- **Published**: 2021-06-30 13:35:36+00:00
- **Updated**: 2021-06-30 13:35:36+00:00
- **Authors**: Emanuele Sansone
- **Comment**: None
- **Journal**: None
- **Summary**: This work considers the problem of learning structured representations from raw images using self-supervised learning. We propose a principled framework based on a mutual information objective, which integrates self-supervised and structure learning. Furthermore, we devise a post-hoc procedure to interpret the meaning of the learnt representations. Preliminary experiments on CIFAR-10 show that the proposed framework achieves higher generalization performance in downstream classification tasks and provides more interpretable representations compared to the ones learnt through traditional self-supervised learning.



### Exploring the Latent Space of Autoencoders with Interventional Assays
- **Arxiv ID**: http://arxiv.org/abs/2106.16091v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.16091v4)
- **Published**: 2021-06-30 14:31:08+00:00
- **Updated**: 2023-01-11 13:24:54+00:00
- **Authors**: Felix Leeb, Stefan Bauer, Michel Besserve, Bernhard Schölkopf
- **Comment**: Published in NeurIPS 2022 Conference Proceedings
- **Journal**: None
- **Summary**: Autoencoders exhibit impressive abilities to embed the data manifold into a low-dimensional latent space, making them a staple of representation learning methods. However, without explicit supervision, which is often unavailable, the representation is usually uninterpretable, making analysis and principled progress challenging. We propose a framework, called latent responses, which exploits the locally contractive behavior exhibited by variational autoencoders to explore the learned manifold. More specifically, we develop tools to probe the representation using interventions in the latent space to quantify the relationships between latent variables. We extend the notion of disentanglement to take the learned generative process into account and consequently avoid the limitations of existing metrics that may rely on spurious correlations. Our analyses underscore the importance of studying the causal structure of the representation to improve performance on downstream tasks such as generation, interpolation, and inference of the factors of variation.



### Multi-Source domain adaptation via supervised contrastive learning and confident consistency regularization
- **Arxiv ID**: http://arxiv.org/abs/2106.16093v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.16093v3)
- **Published**: 2021-06-30 14:39:15+00:00
- **Updated**: 2021-09-27 13:51:09+00:00
- **Authors**: Marin Scalbert, Maria Vakalopoulou, Florent Couzinié-Devy
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-Source Unsupervised Domain Adaptation (multi-source UDA) aims to learn a model from several labeled source domains while performing well on a different target domain where only unlabeled data are available at training time. To align source and target features distributions, several recent works use source and target explicit statistics matching such as features moments or class centroids. Yet, these approaches do not guarantee class conditional distributions alignment across domains. In this work, we propose a new framework called Contrastive Multi-Source Domain Adaptation (CMSDA) for multi-source UDA that addresses this limitation. Discriminative features are learned from interpolated source examples via cross entropy minimization and from target examples via consistency regularization and hard pseudo-labeling. Simultaneously, interpolated source examples are leveraged to align source class conditional distributions through an interpolated version of the supervised contrastive loss. This alignment leads to more general and transferable features which further improves the generalization on the target domain. Extensive experiments have been carried out on three standard multi-source UDA datasets where our method reports state-of-the-art results.



### Synthetic Data Are as Good as the Real for Association Knowledge Learning in Multi-object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2106.16100v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.16100v3)
- **Published**: 2021-06-30 14:46:36+00:00
- **Updated**: 2021-10-25 04:53:42+00:00
- **Authors**: Yuchi Liu, Zhongdao Wang, Xiangxin Zhou, Liang Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Association, aiming to link bounding boxes of the same identity in a video sequence, is a central component in multi-object tracking (MOT). To train association modules, e.g., parametric networks, real video data are usually used. However, annotating person tracks in consecutive video frames is expensive, and such real data, due to its inflexibility, offer us limited opportunities to evaluate the system performance w.r.t changing tracking scenarios. In this paper, we study whether 3D synthetic data can replace real-world videos for association training. Specifically, we introduce a large-scale synthetic data engine named MOTX, where the motion characteristics of cameras and objects are manually configured to be similar to those in real-world datasets. We show that compared with real data, association knowledge obtained from synthetic data can achieve very similar performance on real-world test sets without domain adaption techniques. Our intriguing observation is credited to two factors. First and foremost, 3D engines can well simulate motion factors such as camera movement, camera view and object movement, so that the simulated videos can provide association modules with effective motion features. Second, experimental results show that the appearance domain gap hardly harms the learning of association knowledge. In addition, the strong customization ability of MOTX allows us to quantitatively assess the impact of motion factors on MOT, which brings new insights to the community.



### Zero-shot Learning with Class Description Regularization
- **Arxiv ID**: http://arxiv.org/abs/2106.16108v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.16108v1)
- **Published**: 2021-06-30 14:56:15+00:00
- **Updated**: 2021-06-30 14:56:15+00:00
- **Authors**: Shayan Kousha, Marcus A. Brubaker
- **Comment**: None
- **Journal**: None
- **Summary**: The purpose of generative Zero-shot learning (ZSL) is to learning from seen classes, transfer the learned knowledge, and create samples of unseen classes from the description of these unseen categories. To achieve better ZSL accuracies, models need to better understand the descriptions of unseen classes. We introduce a novel form of regularization that encourages generative ZSL models to pay more attention to the description of each category. Our empirical results demonstrate improvements over the performance of multiple state-of-the-art models on the task of generalized zero-shot recognition and classification when trained on textual description-based datasets like CUB and NABirds and attribute-based datasets like AWA2, aPY and SUN.



### SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo
- **Arxiv ID**: http://arxiv.org/abs/2106.16118v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.16118v1)
- **Published**: 2021-06-30 15:18:14+00:00
- **Updated**: 2021-06-30 15:18:14+00:00
- **Authors**: Thomas Kollar, Michael Laskey, Kevin Stone, Brijen Thananjeyan, Mark Tjersland
- **Comment**: 19 pages, 13 figures
- **Journal**: None
- **Summary**: Robot manipulation of unknown objects in unstructured environments is a challenging problem due to the variety of shapes, materials, arrangements and lighting conditions. Even with large-scale real-world data collection, robust perception and manipulation of transparent and reflective objects across various lighting conditions remain challenging. To address these challenges we propose an approach to performing sim-to-real transfer of robotic perception. The underlying model, SimNet, is trained as a single multi-headed neural network using simulated stereo data as input and simulated object segmentation masks, 3D oriented bounding boxes (OBBs), object keypoints, and disparity as output. A key component of SimNet is the incorporation of a learned stereo sub-network that predicts disparity. SimNet is evaluated on 2D car detection, unknown object detection, and deformable object keypoint detection and significantly outperforms a baseline that uses a structured light RGB-D sensor. By inferring grasp positions using the OBB and keypoint predictions, SimNet can be used to perform end-to-end manipulation of unknown objects in both easy and hard scenarios using our fleet of Toyota HSR robots in four home environments. In unknown object grasping experiments, the predictions from the baseline RGB-D network and SimNet enable successful grasps of most of the easy objects. However, the RGB-D baseline only grasps 35% of the hard (e.g., transparent) objects, while SimNet grasps 95%, suggesting that SimNet can enable robust manipulation of unknown objects, including transparent objects, in unknown environments.



### Affective Image Content Analysis: Two Decades Review and New Perspectives
- **Arxiv ID**: http://arxiv.org/abs/2106.16125v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.16125v1)
- **Published**: 2021-06-30 15:20:56+00:00
- **Updated**: 2021-06-30 15:20:56+00:00
- **Authors**: Sicheng Zhao, Xingxu Yao, Jufeng Yang, Guoli Jia, Guiguang Ding, Tat-Seng Chua, Björn W. Schuller, Kurt Keutzer
- **Comment**: Accepted by IEEE TPAMI
- **Journal**: None
- **Summary**: Images can convey rich semantics and induce various emotions in viewers. Recently, with the rapid advancement of emotional intelligence and the explosive growth of visual data, extensive research efforts have been dedicated to affective image content analysis (AICA). In this survey, we will comprehensively review the development of AICA in the recent two decades, especially focusing on the state-of-the-art methods with respect to three main challenges -- the affective gap, perception subjectivity, and label noise and absence. We begin with an introduction to the key emotion representation models that have been widely employed in AICA and description of available datasets for performing evaluation with quantitative comparison of label noise and dataset bias. We then summarize and compare the representative approaches on (1) emotion feature extraction, including both handcrafted and deep features, (2) learning methods on dominant emotion recognition, personalized emotion prediction, emotion distribution learning, and learning from noisy data or few labels, and (3) AICA based applications. Finally, we discuss some challenges and promising research directions in the future, such as image content and context understanding, group emotion clustering, and viewer-image interaction.



### Recognizing Facial Expressions in the Wild using Multi-Architectural Representations based Ensemble Learning with Distillation
- **Arxiv ID**: http://arxiv.org/abs/2106.16126v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.16126v3)
- **Published**: 2021-06-30 15:22:24+00:00
- **Updated**: 2021-08-20 09:08:47+00:00
- **Authors**: Rauf Momin, Ali Shan Momin, Khalid Rasheed, Muhammad Saqib
- **Comment**: 6 pages, 3 figures, 4 tables
- **Journal**: None
- **Summary**: Facial expressions are the most common universal forms of body language. In the past few years, automatic facial expression recognition (FER) has been an active field of research. However, it is still a challenging task due to different uncertainties and complications. Nevertheless, efficiency and performance are yet essential aspects for building robust systems. We proposed two models, EmoXNet which is an ensemble learning technique for learning convoluted facial representations, and EmoXNetLite which is a distillation technique that is useful for transferring the knowledge from our ensemble model to an efficient deep neural network using label-smoothen soft labels for able to effectively detect expressions in real-time. Both of the techniques performed quite well, where the ensemble model (EmoXNet) helped to achieve 85.07% test accuracy on FER2013 with FER+ annotations and 86.25% test accuracy on RAF-DB. Moreover, the distilled model (EmoXNetLite) showed 82.07% test accuracy on FER2013 with FER+ annotations and 81.78% test accuracy on RAF-DB. Results show that our models seem to generalize well on new data and are learned to focus on relevant facial representations for expressions recognition.



### Dual Reweighting Domain Generalization for Face Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.16128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.16128v1)
- **Published**: 2021-06-30 15:24:34+00:00
- **Updated**: 2021-06-30 15:24:34+00:00
- **Authors**: Shubao Liu, Ke-Yue Zhang, Taiping Yao, Kekai Sheng, Shouhong Ding, Ying Tai, Jilin Li, Yuan Xie, Lizhuang Ma
- **Comment**: accepted on IJCAI 2021
- **Journal**: None
- **Summary**: Face anti-spoofing approaches based on domain generalization (DG) have drawn growing attention due to their robustness for unseen scenarios. Previous methods treat each sample from multiple domains indiscriminately during the training process, and endeavor to extract a common feature space to improve the generalization. However, due to complex and biased data distribution, directly treating them equally will corrupt the generalization ability. To settle the issue, we propose a novel Dual Reweighting Domain Generalization (DRDG) framework which iteratively reweights the relative importance between samples to further improve the generalization. Concretely, Sample Reweighting Module is first proposed to identify samples with relatively large domain bias, and reduce their impact on the overall optimization. Afterwards, Feature Reweighting Module is introduced to focus on these samples and extract more domain-irrelevant features via a self-distilling mechanism. Combined with the domain discriminator, the iteration of the two modules promotes the extraction of generalized features. Extensive experiments and visualizations are presented to demonstrate the effectiveness and interpretability of our method against the state-of-the-art competitors.



### Recurrently Estimating Reflective Symmetry Planes from Partial Pointclouds
- **Arxiv ID**: http://arxiv.org/abs/2106.16129v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.16129v1)
- **Published**: 2021-06-30 15:26:15+00:00
- **Updated**: 2021-06-30 15:26:15+00:00
- **Authors**: Mihaela Cătălina Stoian, Tommaso Cavallari
- **Comment**: Presented at the CVPR 2021 Workshop on 3D Vision and Robotics
- **Journal**: None
- **Summary**: Many man-made objects are characterised by a shape that is symmetric along one or more planar directions. Estimating the location and orientation of such symmetry planes can aid many tasks such as estimating the overall orientation of an object of interest or performing shape completion, where a partial scan of an object is reflected across the estimated symmetry plane in order to obtain a more detailed shape. Many methods processing 3D data rely on expensive 3D convolutions. In this paper we present an alternative novel encoding that instead slices the data along the height dimension and passes it sequentially to a 2D convolutional recurrent regression scheme. The method also comprises a differentiable least squares step, allowing for end-to-end accurate and fast processing of both full and partial scans of symmetric objects. We use this approach to efficiently handle 3D inputs to design a method to estimate planar reflective symmetries. We show that our approach has an accuracy comparable to state-of-the-art techniques on the task of planar reflective symmetry estimation on full synthetic objects. Additionally, we show that it can be deployed on partial scans of objects in a real-world pipeline to improve the outputs of a 3D object detector.



### Weakly Supervised Temporal Adjacent Network for Language Grounding
- **Arxiv ID**: http://arxiv.org/abs/2106.16136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.16136v1)
- **Published**: 2021-06-30 15:42:08+00:00
- **Updated**: 2021-06-30 15:42:08+00:00
- **Authors**: Yuechen Wang, Jiajun Deng, Wengang Zhou, Houqiang Li
- **Comment**: Accepted by IEEE Transactions on Multimedia, 2021
- **Journal**: None
- **Summary**: Temporal language grounding (TLG) is a fundamental and challenging problem for vision and language understanding. Existing methods mainly focus on fully supervised setting with temporal boundary labels for training, which, however, suffers expensive cost of annotation. In this work, we are dedicated to weakly supervised TLG, where multiple description sentences are given to an untrimmed video without temporal boundary labels. In this task, it is critical to learn a strong cross-modal semantic alignment between sentence semantics and visual content. To this end, we introduce a novel weakly supervised temporal adjacent network (WSTAN) for temporal language grounding. Specifically, WSTAN learns cross-modal semantic alignment by exploiting temporal adjacent network in a multiple instance learning (MIL) paradigm, with a whole description paragraph as input. Moreover, we integrate a complementary branch into the framework, which explicitly refines the predictions with pseudo supervision from the MIL stage. An additional self-discriminating loss is devised on both the MIL branch and the complementary branch, aiming to enhance semantic discrimination by self-supervising. Extensive experiments are conducted on three widely used benchmark datasets, \emph{i.e.}, ActivityNet-Captions, Charades-STA, and DiDeMo, and the results demonstrate the effectiveness of our approach.



### Deep Convolutional Neural Networks for Onychomycosis Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.16139v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.16139v3)
- **Published**: 2021-06-30 15:45:47+00:00
- **Updated**: 2022-01-05 07:57:35+00:00
- **Authors**: Abdurrahim Yilmaz, Fatih Goktay, Rahmetullah Varol, Gulsum Gencoglan, Huseyin Uvet
- **Comment**: None
- **Journal**: None
- **Summary**: The diagnosis of superficial fungal infections in dermatology is still mostly based on manual direct microscopic examination with Potassium Hydroxide (KOH) solution. However, this method can be time consuming and its diagnostic accuracy rates vary widely depending on the clinician's experience. With the increase of neural network applications in the field of clinical microscopy, it is now possible to automate such manual processes increasing both efficiency and accuracy. This study presents a deep neural network structure that enables the rapid solutions for these problems and can perform automatic fungi detection in grayscale images without dyes. 160 microscopic field photographs containing the fungal element, obtained from patients with onychomycosis, and 297 microscopic field photographs containing dissolved keratin obtained from normal nails were collected. Smaller patches containing 4234 fungi and 4981 keratin were extracted from these images. In order to detect fungus and keratin, VGG16 and InceptionV3 models were developed. The VGG16 model had 95.98% accuracy, and the area under the curve (AUC) value of 0.9930, while the InceptionV3 model had 95.90% accuracy and the AUC value of 0.9917. However, average accuracy and AUC value of clinicians is 72.8% and 0.87, respectively. This deep learning model allows the development of an automated system that can detect fungi within microscopic images.



### Learning More for Free - A Multi Task Learning Approach for Improved Pathology Classification in Capsule Endoscopy
- **Arxiv ID**: http://arxiv.org/abs/2106.16162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.16162v1)
- **Published**: 2021-06-30 15:55:17+00:00
- **Updated**: 2021-06-30 15:55:17+00:00
- **Authors**: Anuja Vats, Marius Pedersen, Ahmed Mohammed, Øistein Hovde
- **Comment**: MICCAI 2021 (Provisional accept)
- **Journal**: None
- **Summary**: The progress in Computer Aided Diagnosis (CADx) of Wireless Capsule Endoscopy (WCE) is thwarted by the lack of data. The inadequacy in richly representative healthy and abnormal conditions results in isolated analyses of pathologies, that can not handle realistic multi-pathology scenarios. In this work, we explore how to learn more for free, from limited data through solving a WCE multicentric, multi-pathology classification problem. Learning more implies to learning more than full supervision would allow with the same data. This is done by combining self supervision with full supervision, under multi task learning. Additionally, we draw inspiration from the Human Visual System (HVS) in designing self supervision tasks and investigate if seemingly ineffectual signals within the data itself can be exploited to gain performance, if so, which signals would be better than others. Further, we present our analysis of the high level features as a stepping stone towards more robust multi-pathology CADx in WCE.



### Hierarchical Phenotyping and Graph Modeling of Spatial Architecture in Lymphoid Neoplasms
- **Arxiv ID**: http://arxiv.org/abs/2106.16174v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV, 68T01 (Primary), I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2106.16174v2)
- **Published**: 2021-06-30 16:09:32+00:00
- **Updated**: 2021-09-20 02:23:31+00:00
- **Authors**: Pingjun Chen, Muhammad Aminu, Siba El Hussein, Joseph D. Khoury, Jia Wu
- **Comment**: Accepted by MICCAI2021
- **Journal**: None
- **Summary**: The cells and their spatial patterns in the tumor microenvironment (TME) play a key role in tumor evolution, and yet the latter remains an understudied topic in computational pathology. This study, to the best of our knowledge, is among the first to hybridize local and global graph methods to profile orchestration and interaction of cellular components. To address the challenge in hematolymphoid cancers, where the cell classes in TME may be unclear, we first implemented cell-level unsupervised learning and identified two new cell subtypes. Local cell graphs or supercells were built for each image by considering the individual cell's geospatial location and classes. Then, we applied supercell level clustering and identified two new cell communities. In the end, we built global graphs to abstract spatial interaction patterns and extract features for disease diagnosis. We evaluate the proposed algorithm on H&E slides of 60 hematolymphoid neoplasms and further compared it with three cell level graph-based algorithms, including the global cell graph, cluster cell graph, and FLocK. The proposed algorithm achieved a mean diagnosis accuracy of 0.703 with the repeated 5-fold cross-validation scheme. In conclusion, our algorithm shows superior performance over the existing methods and can be potentially applied to other cancer types.



### Adversarial examples within the training distribution: A widespread challenge
- **Arxiv ID**: http://arxiv.org/abs/2106.16198v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.16198v2)
- **Published**: 2021-06-30 16:49:19+00:00
- **Updated**: 2023-02-17 20:22:40+00:00
- **Authors**: Spandan Madan, Tomotake Sasaki, Hanspeter Pfister, Tzu-Mao Li, Xavier Boix
- **Comment**: None
- **Journal**: None
- **Summary**: Despite a plethora of proposed theories, understanding why deep neural networks are susceptible to adversarial attacks remains an open question. A promising recent strand of research investigates adversarial attacks within the training data distribution, providing a more stringent and worrisome definition for these attacks. These theories posit that the key issue is that in high dimensional datasets, most data points are close to the ground-truth class boundaries. This has been shown in theory for some simple data distributions, but it is unclear if this theory is relevant in practice. Here, we demonstrate the existence of in-distribution adversarial examples for object recognition. This result provides evidence supporting theories attributing adversarial examples to the proximity of data to ground-truth class boundaries, and calls into question other theories which do not account for this more stringent definition of adversarial attacks. These experiments are enabled by our novel gradient-free, evolutionary strategies (ES) based approach for finding in-distribution adversarial examples in 3D rendered objects, which we call CMA-Search.



### A data-centric approach for improving ambiguous labels with combined semi-supervised classification and clustering
- **Arxiv ID**: http://arxiv.org/abs/2106.16209v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.16209v4)
- **Published**: 2021-06-30 17:00:47+00:00
- **Updated**: 2022-10-06 08:57:06+00:00
- **Authors**: Lars Schmarje, Monty Santarossa, Simon-Martin Schröder, Claudius Zelenka, Rainer Kiko, Jenny Stracke, Nina Volkmann, Reinhard Koch
- **Comment**: Source code is available at https://github.com/Emprime/dc3, Datasets
  available at https://doi.org/10.5281/zenodo.5550916
- **Journal**: Proceedings of the European Conference on Computer Vision (ECCV
  2022)
- **Summary**: Consistently high data quality is essential for the development of novel loss functions and architectures in the field of deep learning. The existence of such data and labels is usually presumed, while acquiring high-quality datasets is still a major issue in many cases. In real-world datasets we often encounter ambiguous labels due to subjective annotations by annotators. In our data-centric approach, we propose a method to relabel such ambiguous labels instead of implementing the handling of this issue in a neural network. A hard classification is by definition not enough to capture the real-world ambiguity of the data. Therefore, we propose our method "Data-Centric Classification & Clustering (DC3)" which combines semi-supervised classification and clustering. It automatically estimates the ambiguity of an image and performs a classification or clustering depending on that ambiguity. DC3 is general in nature so that it can be used in addition to many Semi-Supervised Learning (SSL) algorithms. On average, this results in a 7.6% better F1-Score for classifications and 7.9% lower inner distance of clusters across multiple evaluated SSL algorithms and datasets. Most importantly, we give a proof-of-concept that the classifications and clusterings from DC3 are beneficial as proposals for the manual refinement of such ambiguous labels. Overall, a combination of SSL with our method DC3 can lead to better handling of ambiguous labels during the annotation process.



### Multimodal Shape Completion via IMLE
- **Arxiv ID**: http://arxiv.org/abs/2106.16237v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.16237v2)
- **Published**: 2021-06-30 17:45:10+00:00
- **Updated**: 2021-07-07 17:41:55+00:00
- **Authors**: Himanshu Arora, Saurabh Mishra, Shichong Peng, Ke Li, Ali Mahdavi-Amiri
- **Comment**: Project Website:
  https://sites.google.com/site/alimahdaviamiri/projects/shape-completion
- **Journal**: None
- **Summary**: Shape completion is the problem of completing partial input shapes such as partial scans. This problem finds important applications in computer vision and robotics due to issues such as occlusion or sparsity in real-world data. However, most of the existing research related to shape completion has been focused on completing shapes by learning a one-to-one mapping which limits the diversity and creativity of the produced results. We propose a novel multimodal shape completion technique that is effectively able to learn a one-to-many mapping and generates diverse complete shapes. Our approach is based on the conditional Implicit MaximumLikelihood Estimation (IMLE) technique wherein we condition our inputs on partial 3D point clouds. We extensively evaluate our approach by comparing it to various baselines both quantitatively and qualitatively. We show that our method is superior to alternatives in terms of completeness and diversity of shapes.



### How to Train Your MAML to Excel in Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.16245v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.16245v3)
- **Published**: 2021-06-30 17:56:15+00:00
- **Updated**: 2022-07-11 01:59:05+00:00
- **Authors**: Han-Jia Ye, Wei-Lun Chao
- **Comment**: Accepted to International Conference on Learning Representations 2022
  (ICLR 2022)
- **Journal**: None
- **Summary**: Model-agnostic meta-learning (MAML) is arguably one of the most popular meta-learning algorithms nowadays. Nevertheless, its performance on few-shot classification is far behind many recent algorithms dedicated to the problem. In this paper, we point out several key facets of how to train MAML to excel in few-shot classification. First, we find that MAML needs a large number of gradient steps in its inner loop update, which contradicts its common usage in few-shot classification. Second, we find that MAML is sensitive to the class label assignments during meta-testing. Concretely, MAML meta-trains the initialization of an $N$-way classifier. These $N$ ways, during meta-testing, then have "$N!$" different permutations to be paired with a few-shot task of $N$ novel classes. We find that these permutations lead to a huge variance of accuracy, making MAML unstable in few-shot classification. Third, we investigate several approaches to make MAML permutation-invariant, among which meta-training a single vector to initialize all the $N$ weight vectors in the classification head performs the best. On benchmark datasets like MiniImageNet and TieredImageNet, our approach, which we name UNICORN-MAML, performs on a par with or even outperforms many recent few-shot classification algorithms, without sacrificing MAML's simplicity.



### Simple Training Strategies and Model Scaling for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.00057v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.00057v1)
- **Published**: 2021-06-30 18:41:47+00:00
- **Updated**: 2021-06-30 18:41:47+00:00
- **Authors**: Xianzhi Du, Barret Zoph, Wei-Chih Hung, Tsung-Yi Lin
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: The speed-accuracy Pareto curve of object detection systems have advanced through a combination of better model architectures, training and inference methods. In this paper, we methodically evaluate a variety of these techniques to understand where most of the improvements in modern detection systems come from. We benchmark these improvements on the vanilla ResNet-FPN backbone with RetinaNet and RCNN detectors. The vanilla detectors are improved by 7.7% in accuracy while being 30% faster in speed. We further provide simple scaling strategies to generate family of models that form two Pareto curves, named RetinaNet-RS and Cascade RCNN-RS. These simple rescaled detectors explore the speed-accuracy trade-off between the one-stage RetinaNet detectors and two-stage RCNN detectors. Our largest Cascade RCNN-RS models achieve 52.9% AP with a ResNet152-FPN backbone and 53.6% with a SpineNet143L backbone. Finally, we show the ResNet architecture, with three minor architectural changes, outperforms EfficientNet as the backbone for object detection and instance segmentation systems.



### Fair Visual Recognition in Limited Data Regime using Self-Supervision and Self-Distillation
- **Arxiv ID**: http://arxiv.org/abs/2107.00067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00067v1)
- **Published**: 2021-06-30 19:22:46+00:00
- **Updated**: 2021-06-30 19:22:46+00:00
- **Authors**: Pratik Mazumder, Pravendra Singh, Vinay P. Namboodiri
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Deep learning models generally learn the biases present in the training data. Researchers have proposed several approaches to mitigate such biases and make the model fair. Bias mitigation techniques assume that a sufficiently large number of training examples are present. However, we observe that if the training data is limited, then the effectiveness of bias mitigation methods is severely degraded. In this paper, we propose a novel approach to address this problem. Specifically, we adapt self-supervision and self-distillation to reduce the impact of biases on the model in this setting. Self-supervision and self-distillation are not used for bias mitigation. However, through this work, we demonstrate for the first time that these techniques are very effective in bias mitigation. We empirically show that our approach can significantly reduce the biases learned by the model. Further, we experimentally demonstrate that our approach is complementary to other bias mitigation strategies. Our approach significantly improves their performance and further reduces the model biases in the limited data regime. Specifically, on the L-CIFAR-10S skewed dataset, our approach significantly reduces the bias score of the baseline model by 78.22% and outperforms it in terms of accuracy by a significant absolute margin of 8.89%. It also significantly reduces the bias score for the state-of-the-art domain independent bias mitigation method by 59.26% and improves its performance by a significant absolute margin of 7.08%.



### Dep-$L_0$: Improving $L_0$-based Network Sparsification via Dependency Modeling
- **Arxiv ID**: http://arxiv.org/abs/2107.00070v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00070v1)
- **Published**: 2021-06-30 19:33:35+00:00
- **Updated**: 2021-06-30 19:33:35+00:00
- **Authors**: Yang Li, Shihao Ji
- **Comment**: Published as a conference paper at ECML 2021
- **Journal**: None
- **Summary**: Training deep neural networks with an $L_0$ regularization is one of the prominent approaches for network pruning or sparsification. The method prunes the network during training by encouraging weights to become exactly zero. However, recent work of Gale et al. reveals that although this method yields high compression rates on smaller datasets, it performs inconsistently on large-scale learning tasks, such as ResNet50 on ImageNet. We analyze this phenomenon through the lens of variational inference and find that it is likely due to the independent modeling of binary gates, the mean-field approximation, which is known in Bayesian statistics for its poor performance due to the crude approximation. To mitigate this deficiency, we propose a dependency modeling of binary gates, which can be modeled effectively as a multi-layer perceptron (MLP). We term our algorithm Dep-$L_0$ as it prunes networks via a dependency-enabled $L_0$ regularization. Extensive experiments on CIFAR10, CIFAR100 and ImageNet with VGG16, ResNet50, ResNet56 show that our Dep-$L_0$ outperforms the original $L_0$-HC algorithm of Louizos et al. by a significant margin, especially on ImageNet. Compared with the state-of-the-arts network sparsification algorithms, our dependency modeling makes the $L_0$-based sparsification once again very competitive on large-scale learning tasks. Our source code is available at https://github.com/leo-yangli/dep-l0.



### CLDA: Contrastive Learning for Semi-Supervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2107.00085v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00085v2)
- **Published**: 2021-06-30 20:23:19+00:00
- **Updated**: 2021-11-10 04:33:07+00:00
- **Authors**: Ankit Singh
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) aims to align the labeled source distribution with the unlabeled target distribution to obtain domain invariant predictive models. However, the application of well-known UDA approaches does not generalize well in Semi-Supervised Domain Adaptation (SSDA) scenarios where few labeled samples from the target domain are available. In this paper, we propose a simple Contrastive Learning framework for semi-supervised Domain Adaptation (CLDA) that attempts to bridge the intra-domain gap between the labeled and unlabeled target distributions and inter-domain gap between source and unlabeled target distribution in SSDA. We suggest employing class-wise contrastive learning to reduce the inter-domain gap and instance-level contrastive alignment between the original (input image) and strongly augmented unlabeled target images to minimize the intra-domain discrepancy. We have shown empirically that both of these modules complement each other to achieve superior performance. Experiments on three well-known domain adaptation benchmark datasets namely DomainNet, Office-Home, and Office31 demonstrate the effectiveness of our approach. CLDA achieves state-of-the-art results on all the above datasets.



### Automated Detection and Diagnosis of Diabetic Retinopathy: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/2107.00115v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, J.3, I.4, I.2
- **Links**: [PDF](http://arxiv.org/pdf/2107.00115v1)
- **Published**: 2021-06-30 21:45:15+00:00
- **Updated**: 2021-06-30 21:45:15+00:00
- **Authors**: Vasudevan Lakshminarayanan, Hoda Kherdfallah, Arya Sarkar, J. Jothi Balaji
- **Comment**: Submitted to MDPI Journal of Imaging special issue "Frontiers In
  Retinal Image Processing"2021
- **Journal**: None
- **Summary**: Diabetic Retinopathy (DR) is a leading cause of vision loss in the world,. In the past few Diabetic Retinopathy (DR) is a leading cause of vision loss in the world. In the past few years, Artificial Intelligence (AI) based approaches have been used to detect and grade DR. Early detection enables appropriate treatment and thus prevents vision loss, Both fundus and optical coherence tomography (OCT) images are used to image the retina. With deep learning/machine learning apprroaches it is possible to extract features from the images and detect the presence of DR. Multiple strategies are implemented to detect and grade the presence of DR using classification, segmentation, and hybrid techniques. This review covers the literature dealing with AI approaches to DR that have been published in the open literature over a five year span (2016-2021). In addition a comprehensive list of available DR datasets is reported. Both the PICO (P-patient, I-intervention, C-control O-outcome) and Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA)2009 search strategies were employed. We summarize a total of 114 published articles which conformed to the scope of the review. In addition a list of 43 major datasets is presented.



### Attention Bottlenecks for Multimodal Fusion
- **Arxiv ID**: http://arxiv.org/abs/2107.00135v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00135v3)
- **Published**: 2021-06-30 22:44:12+00:00
- **Updated**: 2022-11-30 23:34:16+00:00
- **Authors**: Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, Chen Sun
- **Comment**: Published at NeurIPS 2021. Note this version updates numbers due to a
  bug in the AudioSet mAP calculation in Table 1 (last row)
- **Journal**: None
- **Summary**: Humans perceive the world by concurrently processing and fusing high-dimensional inputs from multiple modalities such as vision and audio. Machine perception models, in stark contrast, are typically modality-specific and optimised for unimodal benchmarks, and hence late-stage fusion of final representations or predictions from each modality (`late-fusion') is still a dominant paradigm for multimodal video classification. Instead, we introduce a novel transformer based architecture that uses `fusion bottlenecks' for modality fusion at multiple layers. Compared to traditional pairwise self-attention, our model forces information between different modalities to pass through a small number of bottleneck latents, requiring the model to collate and condense the most relevant information in each modality and only share what is necessary. We find that such a strategy improves fusion performance, at the same time reducing computational cost. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple audio-visual classification benchmarks including Audioset, Epic-Kitchens and VGGSound. All code and models will be released.



### One-class Steel Detector Using Patch GAN Discriminator for Visualising Anomalous Feature Map
- **Arxiv ID**: http://arxiv.org/abs/2107.00143v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.5.4; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2107.00143v1)
- **Published**: 2021-06-30 23:01:09+00:00
- **Updated**: 2021-06-30 23:01:09+00:00
- **Authors**: Takato Yasuno, Junichiro Fujii, Sakura Fukami
- **Comment**: 14 pages, 8 figures, 7 tables
- **Journal**: None
- **Summary**: For steel product manufacturing in indoor factories, steel defect detection is important for quality control. For example, a steel sheet is extremely delicate, and must be accurately inspected. However, to maintain the painted steel parts of the infrastructure around a severe outdoor environment, corrosion detection is critical for predictive maintenance. In this paper, we propose a general-purpose application for steel anomaly detection that consists of the following four components. The first, a learner, is a unit image classification network to determine whether the region of interest or background has been recognised, after dividing the original large sized image into 256 square unit images. The second, an extractor, is a discriminator feature encoder based on a pre-trained steel generator with a patch generative adversarial network discriminator(GAN). The third, an anomaly detector, is a one-class support vector machine(SVM) to predict the anomaly score using the discriminator feature. The fourth, an indicator, is an anomalous probability map used to visually explain the anomalous features. Furthermore, we demonstrated our method through the inspection of steel sheet defects with 13,774 unit images using high-speed cameras, and painted steel corrosion with 19,766 unit images based on an eye inspection of the photographs. Finally, we visualise anomalous feature maps of steel using a strip and painted steel inspection dataset



