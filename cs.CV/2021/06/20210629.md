# Arxiv Papers in cs.CV on 2021-06-29
### EVPropNet: Detecting Drones By Finding Propellers For Mid-Air Landing And Following
- **Arxiv ID**: http://arxiv.org/abs/2106.15045v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.15045v1)
- **Published**: 2021-06-29 01:16:01+00:00
- **Updated**: 2021-06-29 01:16:01+00:00
- **Authors**: Nitin J. Sanket, Chahat Deep Singh, Chethan M. Parameshwara, Cornelia Ferm√ºller, Guido C. H. E. de Croon, Yiannis Aloimonos
- **Comment**: 11 pages, 10 figures, 6 tables. Accepted in Robotics: Science and
  Systems (RSS) 2021
- **Journal**: None
- **Summary**: The rapid rise of accessibility of unmanned aerial vehicles or drones pose a threat to general security and confidentiality. Most of the commercially available or custom-built drones are multi-rotors and are comprised of multiple propellers. Since these propellers rotate at a high-speed, they are generally the fastest moving parts of an image and cannot be directly "seen" by a classical camera without severe motion blur. We utilize a class of sensors that are particularly suitable for such scenarios called event cameras, which have a high temporal resolution, low-latency, and high dynamic range.   In this paper, we model the geometry of a propeller and use it to generate simulated events which are used to train a deep neural network called EVPropNet to detect propellers from the data of an event camera. EVPropNet directly transfers to the real world without any fine-tuning or retraining. We present two applications of our network: (a) tracking and following an unmarked drone and (b) landing on a near-hover drone. We successfully evaluate and demonstrate the proposed approach in many real-world experiments with different propeller shapes and sizes. Our network can detect propellers at a rate of 85.1% even when 60% of the propeller is occluded and can run at upto 35Hz on a 2W power budget. To our knowledge, this is the first deep learning-based solution for detecting propellers (to detect drones). Finally, our applications also show an impressive success rate of 92% and 90% for the tracking and landing tasks respectively.



### Efficient Fourier single-pixel imaging with Gaussian random sampling
- **Arxiv ID**: http://arxiv.org/abs/2108.02317v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2108.02317v1)
- **Published**: 2021-06-29 01:23:33+00:00
- **Updated**: 2021-06-29 01:23:33+00:00
- **Authors**: Ziheng Qiu, Xinyi Guo, Tianao Lu, Pan Qi, Zibang Zhang, Jingang Zhong
- **Comment**: None
- **Journal**: None
- **Summary**: Fourier single-pixel imaging (FSI) is a branch of single-pixel imaging techniques. It uses Fourier basis patterns as structured patterns for spatial information acquisition in the Fourier domain. However, the spatial resolution of the image reconstructed by FSI mainly depends on the number of Fourier coefficients sampled. The reconstruction of a high-resolution image typically requires a number of Fourier coefficients to be sampled, and therefore takes a long data acquisition time. Here we propose a new sampling strategy for FSI. It allows FSI to reconstruct a clear and sharp image with a reduced number of measurements. The core of the proposed sampling strategy is to perform a variable density sampling in the Fourier space and, more importantly, the density with respect to the importance of Fourier coefficients is subject to a one-dimensional Gaussian function. Combined with compressive sensing, the proposed sampling strategy enables better reconstruction quality than conventional sampling strategies, especially when the sampling ratio is low. We experimentally demonstrate compressive FSI combined with the proposed sampling strategy is able to reconstruct a sharp and clear image of 256-by-256 pixels with a sampling ratio of 10%. The proposed method enables fast single-pixel imaging and provides a new approach for efficient spatial information acquisition.



### Improving Transferability of Adversarial Patches on Face Recognition with Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2106.15058v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15058v2)
- **Published**: 2021-06-29 02:13:05+00:00
- **Updated**: 2021-08-30 03:59:50+00:00
- **Authors**: Zihao Xiao, Xianfeng Gao, Chilin Fu, Yinpeng Dong, Wei Gao, Xiaolu Zhang, Jun Zhou, Jun Zhu
- **Comment**: Accpeted by CVPR 2021. Based on the camera ready version, some typos
  are fixed
- **Journal**: None
- **Summary**: Face recognition is greatly improved by deep convolutional neural networks (CNNs). Recently, these face recognition models have been used for identity authentication in security sensitive applications. However, deep CNNs are vulnerable to adversarial patches, which are physically realizable and stealthy, raising new security concerns on the real-world applications of these models. In this paper, we evaluate the robustness of face recognition models using adversarial patches based on transferability, where the attacker has limited accessibility to the target models. First, we extend the existing transfer-based attack techniques to generate transferable adversarial patches. However, we observe that the transferability is sensitive to initialization and degrades when the perturbation magnitude is large, indicating the overfitting to the substitute models. Second, we propose to regularize the adversarial patches on the low dimensional data manifold. The manifold is represented by generative models pre-trained on legitimate human face images. Using face-like features as adversarial perturbations through optimization on the manifold, we show that the gaps between the responses of substitute models and the target models dramatically decrease, exhibiting a better transferability. Extensive digital world experiments are conducted to demonstrate the superiority of the proposed method in the black-box setting. We apply the proposed method in the physical world as well.



### GuidedMix-Net: Learning to Improve Pseudo Masks Using Labeled Images as Reference
- **Arxiv ID**: http://arxiv.org/abs/2106.15064v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15064v2)
- **Published**: 2021-06-29 02:48:45+00:00
- **Updated**: 2021-06-30 05:25:21+00:00
- **Authors**: Peng Tu, Yawen Huang, Rongrong Ji, Feng Zheng, Ling Shao
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Semi-supervised learning is a challenging problem which aims to construct a model by learning from a limited number of labeled examples. Numerous methods have been proposed to tackle this problem, with most focusing on utilizing the predictions of unlabeled instances consistency alone to regularize networks. However, treating labeled and unlabeled data separately often leads to the discarding of mass prior knowledge learned from the labeled examples, and failure to mine the feature interaction between the labeled and unlabeled image pairs. In this paper, we propose a novel method for semi-supervised semantic segmentation named GuidedMix-Net, by leveraging labeled information to guide the learning of unlabeled instances. Specifically, we first introduce a feature alignment objective between labeled and unlabeled data to capture potentially similar image pairs and then generate mixed inputs from them. The proposed mutual information transfer (MITrans), based on the cluster assumption, is shown to be a powerful knowledge module for further progressive refining features of unlabeled data in the mixed data space. To take advantage of the labeled examples and guide unlabeled data learning, we further propose a mask generation module to generate high-quality pseudo masks for the unlabeled data. Along with supervised learning for labeled data, the prediction of unlabeled data is jointly learned with the generated pseudo masks from the mixed data. Extensive experiments on PASCAL VOC 2012, PASCAL-Context and Cityscapes demonstrate the effectiveness of our GuidedMix-Net, which achieves competitive segmentation accuracy and significantly improves the mIoU by +7$\%$ compared to previous state-of-the-art approaches.



### Towards Understanding the Effectiveness of Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2106.15067v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15067v1)
- **Published**: 2021-06-29 02:58:59+00:00
- **Updated**: 2021-06-29 02:58:59+00:00
- **Authors**: Xiang Ye, Zihang He, Heng Wang, Yong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Attention Mechanism is a widely used method for improving the performance of convolutional neural networks (CNNs) on computer vision tasks. Despite its pervasiveness, we have a poor understanding of what its effectiveness stems from. It is popularly believed that its effectiveness stems from the visual attention explanation, advocating focusing on the important part of input data rather than ingesting the entire input. In this paper, we find that there is only a weak consistency between the attention weights of features and their importance. Instead, we verify the crucial role of feature map multiplication in attention mechanism and uncover a fundamental impact of feature map multiplication on the learned landscapes of CNNs: with the high order non-linearity brought by the feature map multiplication, it played a regularization role on CNNs, which made them learn smoother and more stable landscapes near real samples compared to vanilla CNNs. This smoothness and stability induce a more predictive and stable behavior in-between real samples, and make CNNs generate better. Moreover, motivated by the proposed effectiveness of feature map multiplication, we design feature map multiplication network (FMMNet) by simply replacing the feature map addition in ResNet with feature map multiplication. FMMNet outperforms ResNet on various datasets, and this indicates that feature map multiplication plays a vital role in improving the performance even without finely designed attention mechanism in existing methods.



### An End-to-End Autofocus Camera for Iris on the Move
- **Arxiv ID**: http://arxiv.org/abs/2106.15069v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15069v1)
- **Published**: 2021-06-29 03:00:39+00:00
- **Updated**: 2021-06-29 03:00:39+00:00
- **Authors**: Leyuan Wang, Kunbo Zhang, Yunlong Wang, Zhenan Sun
- **Comment**: 8 pages, 7 figures, International Joint Conference on Biometrics 2021
- **Journal**: None
- **Summary**: For distant iris recognition, a long focal length lens is generally used to ensure the resolution ofiris images, which reduces the depth of field and leads to potential defocus blur. To accommodate users at different distances, it is necessary to control focus quickly and accurately. While for users in motion, it is expected to maintain the correct focus on the iris area continuously. In this paper, we introduced a novel rapid autofocus camera for active refocusing ofthe iris area ofthe moving objects using a focus-tunable lens. Our end-to-end computational algorithm can predict the best focus position from one single blurred image and generate a lens diopter control signal automatically. This scene-based active manipulation method enables real-time focus tracking of the iris area ofa moving object. We built a testing bench to collect real-world focal stacks for evaluation of the autofocus methods. Our camera has reached an autofocus speed ofover 50 fps. The results demonstrate the advantages of our proposed camera for biometric perception in static and dynamic scenes. The code is available at https://github.com/Debatrix/AquulaCam.



### ElephantBook: A Semi-Automated Human-in-the-Loop System for Elephant Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2106.15083v2
- **DOI**: 10.1145/3460112.3471947
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15083v2)
- **Published**: 2021-06-29 04:18:22+00:00
- **Updated**: 2021-06-30 01:46:20+00:00
- **Authors**: Peter Kulits, Jake Wall, Anka Bedetti, Michelle Henley, Sara Beery
- **Comment**: None
- **Journal**: None
- **Summary**: African elephants are vital to their ecosystems, but their populations are threatened by a rise in human-elephant conflict and poaching. Monitoring population dynamics is essential in conservation efforts; however, tracking elephants is a difficult task, usually relying on the invasive and sometimes dangerous placement of GPS collars. Although there have been many recent successes in the use of computer vision techniques for automated identification of other species, identification of elephants is extremely difficult and typically requires expertise as well as familiarity with elephants in the population. We have built and deployed a web-based platform and database for human-in-the-loop re-identification of elephants combining manual attribute labeling and state-of-the-art computer vision algorithms, known as ElephantBook. Our system is currently in use at the Mara Elephant Project, helping monitor the protected and at-risk population of elephants in the Greater Maasai Mara ecosystem. ElephantBook makes elephant re-identification usable by non-experts and scalable for use by multiple conservation NGOs.



### O2O-Afford: Annotation-Free Large-Scale Object-Object Affordance Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.15087v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.15087v2)
- **Published**: 2021-06-29 04:38:12+00:00
- **Updated**: 2021-10-25 21:26:44+00:00
- **Authors**: Kaichun Mo, Yuzhe Qin, Fanbo Xiang, Hao Su, Leonidas Guibas
- **Comment**: to appear in CoRL 2021
- **Journal**: None
- **Summary**: Contrary to the vast literature in modeling, perceiving, and understanding agent-object (e.g., human-object, hand-object, robot-object) interaction in computer vision and robotics, very few past works have studied the task of object-object interaction, which also plays an important role in robotic manipulation and planning tasks. There is a rich space of object-object interaction scenarios in our daily life, such as placing an object on a messy tabletop, fitting an object inside a drawer, pushing an object using a tool, etc. In this paper, we propose a unified affordance learning framework to learn object-object interaction for various tasks. By constructing four object-object interaction task environments using physical simulation (SAPIEN) and thousands of ShapeNet models with rich geometric diversity, we are able to conduct large-scale object-object affordance learning without the need for human annotations or demonstrations. At the core of technical contribution, we propose an object-kernel point convolution network to reason about detailed interaction between two objects. Experiments on large-scale synthetic data and real-world data prove the effectiveness of the proposed approach. Please refer to the project webpage for code, data, video, and more materials: https://cs.stanford.edu/~kaichun/o2oafford



### IREM: High-Resolution Magnetic Resonance (MR) Image Reconstruction via Implicit Neural Representation
- **Arxiv ID**: http://arxiv.org/abs/2106.15097v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15097v1)
- **Published**: 2021-06-29 05:25:43+00:00
- **Updated**: 2021-06-29 05:25:43+00:00
- **Authors**: Qing Wu, Yuwei Li, Lan Xu, Ruiming Feng, Hongjiang Wei, Qing Yang, Boliang Yu, Xiaozhao Liu, Jingyi Yu, Yuyao Zhang
- **Comment**: 8 pages, 6 figures, conference
- **Journal**: None
- **Summary**: For collecting high-quality high-resolution (HR) MR image, we propose a novel image reconstruction network named IREM, which is trained on multiple low-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HR image reconstruction. In this work, we suppose the desired HR image as an implicit continuous function of the 3D image spatial coordinate and the thick-slice LR images as several sparse discrete samplings of this function. Then the super-resolution (SR) task is to learn the continuous volumetric function from a limited observations using an fully-connected neural network combined with Fourier feature positional encoding. By simply minimizing the error between the network prediction and the acquired LR image intensity across each imaging plane, IREM is trained to represent a continuous model of the observed tissue anatomy. Experimental results indicate that IREM succeeds in representing high frequency image feature, and in real scene data collection, IREM reduces scan time and achieves high-quality high-resolution MR imaging in terms of SNR and local image detail.



### OpenCoS: Contrastive Semi-supervised Learning for Handling Open-set Unlabeled Data
- **Arxiv ID**: http://arxiv.org/abs/2107.08943v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.08943v3)
- **Published**: 2021-06-29 06:10:05+00:00
- **Updated**: 2022-08-19 12:44:21+00:00
- **Authors**: Jongjin Park, Sukmin Yun, Jongheon Jeong, Jinwoo Shin
- **Comment**: ECCV Workshop on Learning from Limited and Imperfect Data, 2022. Code
  is available at https://github.com/alinlab/OpenCoS
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) has been a powerful strategy to incorporate few labels in learning better representations. In this paper, we focus on a practical scenario that one aims to apply SSL when unlabeled data may contain out-of-class samples - those that cannot have one-hot encoded labels from a closed-set of classes in label data, i.e., the unlabeled data is an open-set. Specifically, we introduce OpenCoS, a simple framework for handling this realistic semi-supervised learning scenario based upon a recent framework of self-supervised visual representation learning. We first observe that the out-of-class samples in the open-set unlabeled dataset can be identified effectively via self-supervised contrastive learning. Then, OpenCoS utilizes this information to overcome the failure modes in the existing state-of-the-art semi-supervised methods, by utilizing one-hot pseudo-labels and soft-labels for the identified in- and out-of-class unlabeled data, respectively. Our extensive experimental results show the effectiveness of OpenCoS under the presence of out-of-class samples, fixing up the state-of-the-art semi-supervised methods to be suitable for diverse scenarios involving open-set unlabeled data.



### An Efficient Cervical Whole Slide Image Analysis Framework Based on Multi-scale Semantic and Location Deep Features
- **Arxiv ID**: http://arxiv.org/abs/2106.15113v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.15113v3)
- **Published**: 2021-06-29 06:24:55+00:00
- **Updated**: 2022-06-03 03:53:59+00:00
- **Authors**: Ziquan Wei, Shenghua Cheng, Junbo Hu, Li Chen, Shaoqun Zeng, Xiuli Liu
- **Comment**: 15 pages, 8 figures, under review to Medical Image Analysis (2nd
  round)
- **Journal**: None
- **Summary**: Digital gigapixel whole slide image (WSI) is widely used in clinical diagnosis, and automated WSI analysis is key for computer-aided diagnosis. Currently, analyzing the integrated descriptor of probabilities or feature maps from massive local patches encoded by ResNet classifier is the main manner for WSI-level prediction. Feature representations of the sparse and tiny lesion cells in cervical slides, however, are still challenging, while the unused location representations are available to supply the semantics classification. This study designs a novel and efficient framework with a new module InCNet constructed lightweight model YOLCO (You Only Look Cytology Once). It directly extracts feature inside the single cell (cluster) instead of the traditional way that from image tile with a fixed size. The InCNet (Inline Connection Network) enriches the multi-scale connectivity without efficiency loss. The proposal allows the input size enlarged to megapixel that can stitch the WSI by the average repeats decreased from $10^3\sim10^4$ to $10^1\sim10^2$ for collecting features and predictions at two scales. Based on Transformer for classifying the integrated multi-scale multi-task WSI features, the experimental results appear $0.872$ AUC score better than the best conventional model on our dataset ($n$=2,019) from four scanners. The code is available at https://github.com/Chrisa142857/You-Only-Look-Cytopathology-Once , where the deployment version has the speed $\sim$70 s/WSI.



### SDL: New data generation tools for full-level annotated document layout
- **Arxiv ID**: http://arxiv.org/abs/2106.15117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15117v1)
- **Published**: 2021-06-29 06:32:31+00:00
- **Updated**: 2021-06-29 06:32:31+00:00
- **Authors**: Son Nguyen Truong
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel data generation tool for document processing. The tool focuses on providing a maximal level of visual information in a normal type document, ranging from character position to paragraph-level position. It also enables working with a large dataset on low-resource languages as well as providing a mean of processing thorough full-level information of the documented text. The data generation tools come with a dataset of 320000 Vietnamese synthetic document images and an instruction to generate a dataset of similar size in other languages. The repository can be found at: https://github.com/tson1997/SDL-Document-Image-Generation



### Face Sketch Synthesis via Semantic-Driven Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2106.15121v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15121v1)
- **Published**: 2021-06-29 07:03:56+00:00
- **Updated**: 2021-06-29 07:03:56+00:00
- **Authors**: Xingqun Qi, Muyi Sun, Weining Wang, Xiaoxiao Dong, Qi Li, Caifeng Shan
- **Comment**: None
- **Journal**: None
- **Summary**: Face sketch synthesis has made significant progress with the development of deep neural networks in these years. The delicate depiction of sketch portraits facilitates a wide range of applications like digital entertainment and law enforcement. However, accurate and realistic face sketch generation is still a challenging task due to the illumination variations and complex backgrounds in the real scenes. To tackle these challenges, we propose a novel Semantic-Driven Generative Adversarial Network (SDGAN) which embeds global structure-level style injection and local class-level knowledge re-weighting. Specifically, we conduct facial saliency detection on the input face photos to provide overall facial texture structure, which could be used as a global type of prior information. In addition, we exploit face parsing layouts as the semantic-level spatial prior to enforce globally structural style injection in the generator of SDGAN. Furthermore, to enhance the realistic effect of the details, we propose a novel Adaptive Re-weighting Loss (ARLoss) which dedicates to balance the contributions of different semantic classes. Experimentally, our extensive experiments on CUFS and CUFSF datasets show that our proposed algorithm achieves state-of-the-art performance.



### Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.15125v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15125v2)
- **Published**: 2021-06-29 07:09:11+00:00
- **Updated**: 2022-03-03 11:03:52+00:00
- **Authors**: Yi-Fan Song, Zhang Zhang, Caifeng Shan, Liang Wang
- **Comment**: 15 pages, 12 tables, 10 figures, Accepted by IEEE T-PAMI. arXiv admin
  note: text overlap with arXiv:2010.09978
- **Journal**: None
- **Summary**: One essential problem in skeleton-based action recognition is how to extract discriminative features over all skeleton joints. However, the complexity of the recent State-Of-The-Art (SOTA) models for this task tends to be exceedingly sophisticated and over-parameterized. The low efficiency in model training and inference has increased the validation costs of model architectures in large-scale datasets. To address the above issue, recent advanced separable convolutional layers are embedded into an early fused Multiple Input Branches (MIB) network, constructing an efficient Graph Convolutional Network (GCN) baseline for skeleton-based action recognition. In addition, based on such the baseline, we design a compound scaling strategy to expand the model's width and depth synchronously, and eventually obtain a family of efficient GCN baselines with high accuracies and small amounts of trainable parameters, termed EfficientGCN-Bx, where "x" denotes the scaling coefficient. On two large-scale datasets, i.e., NTU RGB+D 60 and 120, the proposed EfficientGCN-B4 baseline outperforms other SOTA methods, e.g., achieving 91.7% accuracy on the cross-subject benchmark of NTU 60 dataset, while being 3.15x smaller and 3.21x faster than MS-G3D, which is one of the best SOTA methods. The source code in PyTorch version and the pretrained models are available at https://github.com/yfsong0709/EfficientGCNv1.



### Do Not Deceive Your Employer with a Virtual Background: A Video Conferencing Manipulation-Detection System
- **Arxiv ID**: http://arxiv.org/abs/2106.15130v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.15130v1)
- **Published**: 2021-06-29 07:31:21+00:00
- **Updated**: 2021-06-29 07:31:21+00:00
- **Authors**: Mauro Conti, Simone Milani, Ehsan Nowroozi, Gabriele Orazi
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: The last-generation video conferencing software allows users to utilize a virtual background to conceal their personal environment due to privacy concerns, especially in official meetings with other employers. On the other hand, users maybe want to fool people in the meeting by considering the virtual background to conceal where they are. In this case, developing tools to understand the virtual background utilize for fooling people in meeting plays an important role. Besides, such detectors must prove robust against different kinds of attacks since a malicious user can fool the detector by applying a set of adversarial editing steps on the video to conceal any revealing footprint. In this paper, we study the feasibility of an efficient tool to detect whether a videoconferencing user background is real. In particular, we provide the first tool which computes pixel co-occurrences matrices and uses them to search for inconsistencies among spectral and spatial bands. Our experiments confirm that cross co-occurrences matrices improve the robustness of the detector against different kinds of attacks. This work's performance is especially noteworthy with regard to color SPAM features. Moreover, the performance especially is significant with regard to robustness versus post-processing, like geometric transformations, filtering, contrast enhancement, and JPEG compression with different quality factors.



### Spatio-Temporal Context for Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.15171v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15171v1)
- **Published**: 2021-06-29 08:33:48+00:00
- **Updated**: 2021-06-29 08:33:48+00:00
- **Authors**: Manuel Sarmiento Calder√≥, David Varas, Elisenda Bou-Balust
- **Comment**: Computer Vision and Pattern Recognition Workshop
- **Journal**: None
- **Summary**: Research in action detection has grown in the recentyears, as it plays a key role in video understanding. Modelling the interactions (either spatial or temporal) between actors and their context has proven to be essential for this task. While recent works use spatial features with aggregated temporal information, this work proposes to use non-aggregated temporal information. This is done by adding an attention based method that leverages spatio-temporal interactions between elements in the scene along the clip.The main contribution of this work is the introduction of two cross attention blocks to effectively model the spatial relations and capture short range temporal interactions.Experiments on the AVA dataset show the advantages of the proposed approach that models spatio-temporal relations between relevant elements in the scene, outperforming other methods that model actor interactions with their context by +0.31 mAP.



### TUCaN: Progressively Teaching Colourisation to Capsules
- **Arxiv ID**: http://arxiv.org/abs/2106.15176v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15176v1)
- **Published**: 2021-06-29 08:44:15+00:00
- **Updated**: 2021-06-29 08:44:15+00:00
- **Authors**: Rita Pucci, Niki Martinel
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic image colourisation is the computer vision research path that studies how to colourise greyscale images (for restoration). Deep learning techniques improved image colourisation yielding astonishing results. These differ by various factors, such as structural differences, input types, user assistance, etc. Most of them, base the architectural structure on convolutional layers with no emphasis on layers specialised in object features extraction. We introduce a novel downsampling upsampling architecture named TUCaN (Tiny UCapsNet) that exploits the collaboration of convolutional layers and capsule layers to obtain a neat colourisation of entities present in every single image. This is obtained by enforcing collaboration among such layers by skip and residual connections. We pose the problem as a per pixel colour classification task that identifies colours as a bin in a quantized space. To train the network, in contrast with the standard end to end learning method, we propose the progressive learning scheme to extract the context of objects by only manipulating the learning process without changing the model. In this scheme, the upsampling starts from the reconstruction of low resolution images and progressively grows to high resolution images throughout the training phase. Experimental results on three benchmark datasets show that our approach with ImageNet10k dataset outperforms existing methods on standard quality metrics and achieves state of the art performances on image colourisation. We performed a user study to quantify the perceptual realism of the colourisation results demonstrating: that progressive learning let the TUCaN achieve better colours than the end to end scheme; and pointing out the limitations of the existing evaluation metrics.



### Wrong Colored Vermeer: Color-Symmetric Image Distortion
- **Arxiv ID**: http://arxiv.org/abs/2106.15179v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2106.15179v1)
- **Published**: 2021-06-29 08:51:23+00:00
- **Updated**: 2021-06-29 08:51:23+00:00
- **Authors**: Hendrik Richter
- **Comment**: None
- **Journal**: None
- **Summary**: Color symmetry implies that the colors of geometrical objects are assigned according to their symmetry properties. It is defined by associating the elements of the symmetry group with a color permutation. I use this concept for generative art and apply symmetry-consistent color distortions to images of paintings by Johannes Vermeer. The color permutations are realized as mappings of the HSV color space onto itself.



### SALYPATH: A Deep-Based Architecture for visual attention prediction
- **Arxiv ID**: http://arxiv.org/abs/2107.00559v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00559v1)
- **Published**: 2021-06-29 08:53:51+00:00
- **Updated**: 2021-06-29 08:53:51+00:00
- **Authors**: Mohamed Amine Kerkouri, Marouane Tliba, Aladine Chetouani, Rachid Harba
- **Comment**: Accepted at ICIP, 5 pages, 2 figures and 3 tables
- **Journal**: None
- **Summary**: Human vision is naturally more attracted by some regions within their field of view than others. This intrinsic selectivity mechanism, so-called visual attention, is influenced by both high- and low-level factors; such as the global environment (illumination, background texture, etc.), stimulus characteristics (color, intensity, orientation, etc.), and some prior visual information. Visual attention is useful for many computer vision applications such as image compression, recognition, and captioning. In this paper, we propose an end-to-end deep-based method, so-called SALYPATH (SALiencY and scanPATH), that efficiently predicts the scanpath of an image through features of a saliency model. The idea is predict the scanpath by exploiting the capacity of a deep-based model to predict the saliency. The proposed method was evaluated through 2 well-known datasets. The results obtained showed the relevance of the proposed framework comparing to state-of-the-art models.



### Multi-Exit Vision Transformer for Dynamic Inference
- **Arxiv ID**: http://arxiv.org/abs/2106.15183v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15183v3)
- **Published**: 2021-06-29 09:01:13+00:00
- **Updated**: 2021-10-22 12:54:28+00:00
- **Authors**: Arian Bakhtiarnia, Qi Zhang, Alexandros Iosifidis
- **Comment**: Accepted by the 2021 British Machine Vision Conference (BMVC 2021)
- **Journal**: None
- **Summary**: Deep neural networks can be converted to multi-exit architectures by inserting early exit branches after some of their intermediate layers. This allows their inference process to become dynamic, which is useful for time critical IoT applications with stringent latency requirements, but with time-variant communication and computation resources. In particular, in edge computing systems and IoT networks where the exact computation time budget is variable and not known beforehand. Vision Transformer is a recently proposed architecture which has since found many applications across various domains of computer vision. In this work, we propose seven different architectures for early exit branches that can be used for dynamic inference in Vision Transformer backbones. Through extensive experiments involving both classification and regression problems, we show that each one of our proposed architectures could prove useful in the trade-off between accuracy and speed.



### Inconspicuous Adversarial Patches for Fooling Image Recognition Systems on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/2106.15202v2
- **DOI**: 10.1109/JIOT.2021.3124815 10.1109/JIOT.2021.3124815
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.15202v2)
- **Published**: 2021-06-29 09:39:34+00:00
- **Updated**: 2021-11-21 08:43:15+00:00
- **Authors**: Tao Bai, Jinqi Luo, Jun Zhao
- **Comment**: accpeted by iotj. arXiv admin note: substantial text overlap with
  arXiv:2009.09774
- **Journal**: None
- **Summary**: Deep learning based image recognition systems have been widely deployed on mobile devices in today's world. In recent studies, however, deep learning models are shown vulnerable to adversarial examples. One variant of adversarial examples, called adversarial patch, draws researchers' attention due to its strong attack abilities. Though adversarial patches achieve high attack success rates, they are easily being detected because of the visual inconsistency between the patches and the original images. Besides, it usually requires a large amount of data for adversarial patch generation in the literature, which is computationally expensive and time-consuming. To tackle these challenges, we propose an approach to generate inconspicuous adversarial patches with one single image. In our approach, we first decide the patch locations basing on the perceptual sensitivity of victim models, then produce adversarial patches in a coarse-to-fine way by utilizing multiple-scale generators and discriminators. The patches are encouraged to be consistent with the background images with adversarial training while preserving strong attack abilities. Our approach shows the strong attack abilities in white-box settings and the excellent transferability in black-box settings through extensive experiments on various models with different architectures and training methods. Compared to other adversarial patches, our adversarial patches hold the most negligible risks to be detected and can evade human observations, which is supported by the illustrations of saliency maps and results of user evaluations. Lastly, we show that our adversarial patches can be applied in the physical world.



### Domain-Class Correlation Decomposition for Generalizable Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2106.15206v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15206v1)
- **Published**: 2021-06-29 09:45:03+00:00
- **Updated**: 2021-06-29 09:45:03+00:00
- **Authors**: Kaiwen Yang, Xinmei Tian
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Domain generalization in person re-identification is a highly important meaningful and practical task in which a model trained with data from several source domains is expected to generalize well to unseen target domains. Domain adversarial learning is a promising domain generalization method that aims to remove domain information in the latent representation through adversarial training. However, in person re-identification, the domain and class are correlated, and we theoretically show that domain adversarial learning will lose certain information about class due to this domain-class correlation. Inspired by casual inference, we propose to perform interventions to the domain factor $d$, aiming to decompose the domain-class correlation. To achieve this goal, we proposed estimating the resulting representation $z^{*}$ caused by the intervention through first- and second-order statistical characteristic matching. Specifically, we build a memory bank to restore the statistical characteristics of each domain. Then, we use the newly generated samples $\{z^{*},y,d^{*}\}$ to compute the loss function. These samples are domain-class correlation decomposed; thus, we can learn a domain-invariant representation that can capture more class-related features. Extensive experiments show that our model outperforms the state-of-the-art methods on the large-scale domain generalization Re-ID benchmark.



### Using Robust Regression to Find Font Usage Trends
- **Arxiv ID**: http://arxiv.org/abs/2106.15232v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15232v3)
- **Published**: 2021-06-29 10:29:00+00:00
- **Updated**: 2021-07-05 13:01:44+00:00
- **Authors**: Kaigen Tsuji, Seiichi Uchida, Brian Kenji Iwana
- **Comment**: 16 pages with 10 figures. Accepted at ICDAR 2021 Workshop on Machine
  Learning(ICDAR-WML2021)
- **Journal**: None
- **Summary**: Fonts have had trends throughout their history, not only in when they were invented but also in their usage and popularity. In this paper, we attempt to specifically find the trends in font usage using robust regression on a large collection of text images. We utilize movie posters as the source of fonts for this task because movie posters can represent time periods by using their release date. In addition, movie posters are documents that are carefully designed and represent a wide range of fonts. To understand the relationship between the fonts of movie posters and time, we use a regression Convolutional Neural Network (CNN) to estimate the release year of a movie using an isolated title text image. Due to the difficulty of the task, we propose to use of a hybrid training regimen that uses a combination of Mean Squared Error (MSE) and Tukey's biweight loss. Furthermore, we perform a thorough analysis on the trends of fonts through time.



### AutoNovel: Automatically Discovering and Learning Novel Visual Categories
- **Arxiv ID**: http://arxiv.org/abs/2106.15252v1
- **DOI**: 10.1109/TPAMI.2021.3091944
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15252v1)
- **Published**: 2021-06-29 11:12:16+00:00
- **Updated**: 2021-06-29 11:12:16+00:00
- **Authors**: Kai Han, Sylvestre-Alvise Rebuffi, S√©bastien Ehrhardt, Andrea Vedaldi, Andrew Zisserman
- **Comment**: TPAMI 2021, code:
  http://www.robots.ox.ac.uk/~vgg/research/auto_novel/. arXiv admin note:
  substantial text overlap with arXiv:2002.05714
- **Journal**: None
- **Summary**: We tackle the problem of discovering novel classes in an image collection given labelled examples of other classes. We present a new approach called AutoNovel to address this problem by combining three ideas: (1) we suggest that the common approach of bootstrapping an image representation using the labelled data only introduces an unwanted bias, and that this can be avoided by using self-supervised learning to train the representation from scratch on the union of labelled and unlabelled data; (2) we use ranking statistics to transfer the model's knowledge of the labelled classes to the problem of clustering the unlabelled images; and, (3) we train the data representation by optimizing a joint objective function on the labelled and unlabelled subsets of the data, improving both the supervised classification of the labelled data, and the clustering of the unlabelled data. Moreover, we propose a method to estimate the number of classes for the case where the number of new categories is not known a priori. We evaluate AutoNovel on standard classification benchmarks and substantially outperform current methods for novel category discovery. In addition, we also show that AutoNovel can be used for fully unsupervised image clustering, achieving promising results.



### SRF-Net: Selective Receptive Field Network for Anchor-Free Temporal Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.15258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15258v1)
- **Published**: 2021-06-29 11:29:16+00:00
- **Updated**: 2021-06-29 11:29:16+00:00
- **Authors**: Ranyu Ning, Can Zhang, Yuexian Zou
- **Comment**: Accepted by ICASSP 2021
- **Journal**: None
- **Summary**: Temporal action detection (TAD) is a challenging task which aims to temporally localize and recognize the human action in untrimmed videos. Current mainstream one-stage TAD approaches localize and classify action proposals relying on pre-defined anchors, where the location and scale for action instances are set by designers. Obviously, such an anchor-based TAD method limits its generalization capability and will lead to performance degradation when videos contain rich action variation. In this study, we explore to remove the requirement of pre-defined anchors for TAD methods. A novel TAD model termed as Selective Receptive Field Network (SRF-Net) is developed, in which the location offsets and classification scores at each temporal location can be directly estimated in the feature map and SRF-Net is trained in an end-to-end manner. Innovatively, a building block called Selective Receptive Field Convolution (SRFC) is dedicatedly designed which is able to adaptively adjust its receptive field size according to multiple scales of input information at each temporal location in the feature map. Extensive experiments are conducted on the THUMOS14 dataset, and superior results are reported comparing to state-of-the-art TAD approaches.



### Open-Set Representation Learning through Combinatorial Embedding
- **Arxiv ID**: http://arxiv.org/abs/2106.15278v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15278v3)
- **Published**: 2021-06-29 11:51:57+00:00
- **Updated**: 2023-03-15 20:53:33+00:00
- **Authors**: Geeho Kim, Junoh Kang, Bohyung Han
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: Visual recognition tasks are often limited to dealing with a small subset of classes simply because the labels for the remaining classes are unavailable. We are interested in identifying novel concepts in a dataset through representation learning based on both labeled and unlabeled examples, and extending the horizon of recognition to both known and novel classes. To address this challenging task, we propose a combinatorial learning approach, which naturally clusters the examples in unseen classes using the compositional knowledge given by multiple supervised meta-classifiers on heterogeneous label spaces. The representations given by the combinatorial embedding are made more robust by unsupervised pairwise relation learning. The proposed algorithm discovers novel concepts via a joint optimization for enhancing the discrimitiveness of unseen classes as well as learning the representations of known classes generalizable to novel ones. Our extensive experiments demonstrate remarkable performance gains by the proposed approach on public datasets for image retrieval and image categorization with novel class discovery.



### On Board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery
- **Arxiv ID**: http://arxiv.org/abs/2106.15281v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15281v2)
- **Published**: 2021-06-29 11:52:43+00:00
- **Updated**: 2021-07-28 10:20:57+00:00
- **Authors**: Maria Pia Del Rosso, Alessandro Sebastianelli, Dario Spiller, Pierre Philippe Mathieu, Silvia Liberata Ullo
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the growth of Machine Learning (ML) algorithms has raised the number of studies including their applicability in a variety of different scenarios. Among all, one of the hardest ones is the aerospace, due to its peculiar physical requirements. In this context, a feasibility study and a first prototype for an Artificial Intelligence (AI) model to be deployed on board satellites are presented in this work. As a case study, the detection of volcanic eruptions has been investigated as a method to swiftly produce alerts and allow immediate interventions. Two Convolutional Neural Networks (CNNs) have been proposed and designed, showing how to efficiently implement them for identifying the eruptions and at the same time adapting their complexity in order to fit on board requirements.



### Tackling Catastrophic Forgetting and Background Shift in Continual Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.15287v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15287v1)
- **Published**: 2021-06-29 11:57:21+00:00
- **Updated**: 2021-06-29 11:57:21+00:00
- **Authors**: Arthur Douillard, Yifu Chen, Arnaud Dapogny, Matthieu Cord
- **Comment**: Under review at IEEE TPAMI, journal extension of arXiv:2011.11390
- **Journal**: None
- **Summary**: Deep learning approaches are nowadays ubiquitously used to tackle computer vision tasks such as semantic segmentation, requiring large datasets and substantial computational power. Continual learning for semantic segmentation (CSS) is an emerging trend that consists in updating an old model by sequentially adding new classes. However, continual learning methods are usually prone to catastrophic forgetting. This issue is further aggravated in CSS where, at each step, old classes from previous iterations are collapsed into the background. In this paper, we propose Local POD, a multi-scale pooling distillation scheme that preserves long- and short-range spatial relationships at feature level. Furthermore, we design an entropy-based pseudo-labelling of the background w.r.t. classes predicted by the old model to deal with background shift and avoid catastrophic forgetting of the old classes. Finally, we introduce a novel rehearsal method that is particularly suited for segmentation. Our approach, called PLOP, significantly outperforms state-of-the-art methods in existing CSS scenarios, as well as in newly proposed challenging benchmarks.



### MFR 2021: Masked Face Recognition Competition
- **Arxiv ID**: http://arxiv.org/abs/2106.15288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15288v1)
- **Published**: 2021-06-29 11:59:56+00:00
- **Updated**: 2021-06-29 11:59:56+00:00
- **Authors**: Fadi Boutros, Naser Damer, Jan Niklas Kolf, Kiran Raja, Florian Kirchbuchner, Raghavendra Ramachandra, Arjan Kuijper, Pengcheng Fang, Chao Zhang, Fei Wang, David Montero, Naiara Aginako, Basilio Sierra, Marcos Nieto, Mustafa Ekrem Erakin, Ugur Demir, Hazim Kemal, Ekenel, Asaki Kataoka, Kohei Ichikawa, Shizuma Kubo, Jie Zhang, Mingjie He, Dan Han, Shiguang Shan, Klemen Grm, Vitomir ≈†truc, Sachith Seneviratne, Nuran Kasthuriarachchi, Sanka Rasnayaka, Pedro C. Neto, Ana F. Sequeira, Joao Ribeiro Pinto, Mohsen Saffari, Jaime S. Cardoso
- **Comment**: Accepted at International Join Conference on Biometrics (IJCB 2021)
- **Journal**: None
- **Summary**: This paper presents a summary of the Masked Face Recognition Competitions (MFR) held within the 2021 International Joint Conference on Biometrics (IJCB 2021). The competition attracted a total of 10 participating teams with valid submissions. The affiliations of these teams are diverse and associated with academia and industry in nine different countries. These teams successfully submitted 18 valid solutions. The competition is designed to motivate solutions aiming at enhancing the face recognition accuracy of masked faces. Moreover, the competition considered the deployability of the proposed solutions by taking the compactness of the face recognition models into account. A private dataset representing a collaborative, multi-session, real masked, capture scenario is used to evaluate the submitted solutions. In comparison to one of the top-performing academic face recognition solutions, 10 out of the 18 submitted solutions did score higher masked face verification accuracy.



### Adaptive Sample Selection for Robust Learning under Label Noise
- **Arxiv ID**: http://arxiv.org/abs/2106.15292v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15292v3)
- **Published**: 2021-06-29 12:10:58+00:00
- **Updated**: 2022-12-05 07:05:20+00:00
- **Authors**: Deep Patel, P. S. Sastry
- **Comment**: Accepted at WACV 2023
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have been shown to be susceptible to memorization or overfitting in the presence of noisily-labelled data. For the problem of robust learning under such noisy data, several algorithms have been proposed. A prominent class of algorithms rely on sample selection strategies wherein, essentially, a fraction of samples with loss values below a certain threshold are selected for training. These algorithms are sensitive to such thresholds, and it is difficult to fix or learn these thresholds. Often, these algorithms also require information such as label noise rates which are typically unavailable in practice. In this paper, we propose an adaptive sample selection strategy that relies only on batch statistics of a given mini-batch to provide robustness against label noise. The algorithm does not have any additional hyperparameters for sample selection, does not need any information on noise rates and does not need access to separate data with clean labels. We empirically demonstrate the effectiveness of our algorithm on benchmark datasets.



### Convolutional Sparse Coding Fast Approximation with Application to Seismic Reflectivity Estimation
- **Arxiv ID**: http://arxiv.org/abs/2106.15296v1
- **DOI**: 10.1109/TGRS.2021.3105300
- **Categories**: **cs.LG**, cs.CE, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15296v1)
- **Published**: 2021-06-29 12:19:07+00:00
- **Updated**: 2021-06-29 12:19:07+00:00
- **Authors**: Deborah Pereg, Israel Cohen, Anthony A. Vassiliou
- **Comment**: None
- **Journal**: None
- **Summary**: In sparse coding, we attempt to extract features of input vectors, assuming that the data is inherently structured as a sparse superposition of basic building blocks. Similarly, neural networks perform a given task by learning features of the training data set. Recently both data-driven and model-driven feature extracting methods have become extremely popular and have achieved remarkable results. Nevertheless, practical implementations are often too slow to be employed in real-life scenarios, especially for real-time applications. We propose a speed-up upgraded version of the classic iterative thresholding algorithm, that produces a good approximation of the convolutional sparse code within 2-5 iterations. The speed advantage is gained mostly from the observation that most solvers are slowed down by inefficient global thresholding. The main idea is to normalize each data point by the local receptive field energy, before applying a threshold. This way, the natural inclination towards strong feature expressions is suppressed, so that one can rely on a global threshold that can be easily approximated, or learned during training. The proposed algorithm can be employed with a known predetermined dictionary, or with a trained dictionary. The trained version is implemented as a neural net designed as the unfolding of the proposed solver. The performance of the proposed solution is demonstrated via the seismic inversion problem in both synthetic and real data scenarios. We also provide theoretical guarantees for a stable support recovery. Namely, we prove that under certain conditions the true support is perfectly recovered within the first iteration.



### Cells are Actors: Social Network Analysis with Classical ML for SOTA Histology Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.15299v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2106.15299v2)
- **Published**: 2021-06-29 12:22:10+00:00
- **Updated**: 2021-07-10 11:57:37+00:00
- **Authors**: Neda Zamanitajeddin, Mostafa Jahanifar, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Digitization of histology images and the advent of new computational methods, like deep learning, have helped the automatic grading of colorectal adenocarcinoma cancer (CRA). Present automated CRA grading methods, however, usually use tiny image patches and thus fail to integrate the entire tissue micro-architecture for grading purposes. To tackle these challenges, we propose to use a statistical network analysis method to describe the complex structure of the tissue micro-environment by modelling nuclei and their connections as a network. We show that by analyzing only the interactions between the cells in a network, we can extract highly discriminative statistical features for CRA grading. Unlike other deep learning or convolutional graph-based approaches, our method is highly scalable (can be used for cell networks consist of millions of nodes), completely explainable, and computationally inexpensive. We create cell networks on a broad CRC histology image dataset, experiment with our method, and report state-of-the-art performance for the prediction of three-class CRA grading.



### Contrastive Semantic Similarity Learning for Image Captioning Evaluation with Intrinsic Auto-encoder
- **Arxiv ID**: http://arxiv.org/abs/2106.15312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15312v1)
- **Published**: 2021-06-29 12:27:05+00:00
- **Updated**: 2021-06-29 12:27:05+00:00
- **Authors**: Chao Zeng, Tiesong Zhao, Sam Kwong
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically evaluating the quality of image captions can be very challenging since human language is quite flexible that there can be various expressions for the same meaning. Most of the current captioning metrics rely on token level matching between candidate caption and the ground truth label sentences. It usually neglects the sentence-level information. Motivated by the auto-encoder mechanism and contrastive representation learning advances, we propose a learning-based metric for image captioning, which we call Intrinsic Image Captioning Evaluation($I^2CE$). We develop three progressive model structures to learn the sentence level representations--single branch model, dual branches model, and triple branches model. Our empirical tests show that $I^2CE$ trained with dual branches structure achieves better consistency with human judgments to contemporary image captioning evaluation metrics. Furthermore, We select several state-of-the-art image captioning models and test their performances on the MS COCO dataset concerning both contemporary metrics and the proposed $I^2CE$. Experiment results show that our proposed method can align well with the scores generated from other contemporary metrics. On this concern, the proposed metric could serve as a novel indicator of the intrinsic information between captions, which may be complementary to the existing ones.



### Quantifying urban streetscapes with deep learning: focus on aesthetic evaluation
- **Arxiv ID**: http://arxiv.org/abs/2106.15361v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2106.15361v1)
- **Published**: 2021-06-29 12:51:00+00:00
- **Updated**: 2021-06-29 12:51:00+00:00
- **Authors**: Yusuke Kumakoshi, Shigeaki Onoda, Tetsuya Takahashi, Yuji Yoshimura
- **Comment**: 4pages, 3 figures
- **Journal**: None
- **Summary**: The disorder of urban streetscapes would negatively affect people's perception of their aesthetic quality. The presence of billboards on building facades has been regarded as an important factor of the disorder, but its quantification methodology has not yet been developed in a scalable manner. To fill the gap, this paper reports the performance of our deep learning model on a unique data set prepared in Tokyo to recognize the areas covered by facades and billboards in streetscapes, respectively. The model achieved 63.17 % of accuracy, measured by Intersection-over-Union (IoU), thus enabling researchers and practitioners to obtain insights on urban streetscape design by combining data of people's preferences.



### Text Prior Guided Scene Text Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2106.15368v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15368v2)
- **Published**: 2021-06-29 12:52:33+00:00
- **Updated**: 2021-06-30 14:14:56+00:00
- **Authors**: Jianqi Ma, Shi Guo, Lei Zhang
- **Comment**: Code has been released on https://github.com/mjq11302010044/TPGSR
- **Journal**: None
- **Summary**: Scene text image super-resolution (STISR) aims to improve the resolution and visual quality of low-resolution (LR) scene text images, and consequently boost the performance of text recognition. However, most of existing STISR methods regard text images as natural scene images, ignoring the categorical information of text. In this paper, we make an inspiring attempt to embed categorical text prior into STISR model training. Specifically, we adopt the character probability sequence as the text prior, which can be obtained conveniently from a text recognition model. The text prior provides categorical guidance to recover high-resolution (HR) text images. On the other hand, the reconstructed HR image can refine the text prior in return. Finally, we present a multi-stage text prior guided super-resolution (TPGSR) framework for STISR. Our experiments on the benchmark TextZoom dataset show that TPGSR can not only effectively improve the visual quality of scene text images, but also significantly improve the text recognition accuracy over existing STISR methods. Our model trained on TextZoom also demonstrates certain generalization capability to the LR images in other datasets.



### Unified Framework for Spectral Dimensionality Reduction, Maximum Variance Unfolding, and Kernel Learning By Semidefinite Programming: Tutorial and Survey
- **Arxiv ID**: http://arxiv.org/abs/2106.15379v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15379v2)
- **Published**: 2021-06-29 13:09:40+00:00
- **Updated**: 2022-08-03 04:46:44+00:00
- **Authors**: Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley
- **Comment**: To appear as a part of an upcoming textbook on dimensionality
  reduction and manifold learning. v2: corrected some typos
- **Journal**: None
- **Summary**: This is a tutorial and survey paper on unification of spectral dimensionality reduction methods, kernel learning by Semidefinite Programming (SDP), Maximum Variance Unfolding (MVU) or Semidefinite Embedding (SDE), and its variants. We first explain how the spectral dimensionality reduction methods can be unified as kernel Principal Component Analysis (PCA) with different kernels. This unification can be interpreted as eigenfunction learning or representation of kernel in terms of distance matrix. Then, since the spectral methods are unified as kernel PCA, we say let us learn the best kernel for unfolding the manifold of data to its maximum variance. We first briefly introduce kernel learning by SDP for the transduction task. Then, we explain MVU in detail. Various versions of supervised MVU using nearest neighbors graph, by class-wise unfolding, by Fisher criterion, and by colored MVU are explained. We also explain out-of-sample extension of MVU using eigenfunctions and kernel mapping. Finally, we introduce other variants of MVU including action respecting embedding, relaxed MVU, and landmark MVU for big data.



### Multiple Graph Learning for Scalable Multi-view Clustering
- **Arxiv ID**: http://arxiv.org/abs/2106.15382v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15382v2)
- **Published**: 2021-06-29 13:10:56+00:00
- **Updated**: 2021-08-15 13:22:32+00:00
- **Authors**: Tianyu Jiang, Quanxue Gao, Xinbo Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Graph-based multi-view clustering has become an active topic due to the efficiency in characterizing both the complex structure and relationship between multimedia data. However, existing methods have the following shortcomings: (1) They are inefficient or even fail for graph learning in large scale due to the graph construction and eigen-decomposition. (2) They cannot well exploit both the complementary information and spatial structure embedded in graphs of different views. To well exploit complementary information and tackle the scalability issue plaguing graph-based multi-view clustering, we propose an efficient multiple graph learning model via a small number of anchor points and tensor Schatten p-norm minimization. Specifically, we construct a hidden and tractable large graph by anchor graph for each view and well exploit complementary information embedded in anchor graphs of different views by tensor Schatten p-norm regularizer. Finally, we develop an efficient algorithm, which scales linearly with the data size, to solve our proposed model. Extensive experimental results on several datasets indicate that our proposed method outperforms some state-of-the-art multi-view clustering algorithms.



### Two-Stage Self-Supervised Cycle-Consistency Network for Reconstruction of Thin-Slice MR Images
- **Arxiv ID**: http://arxiv.org/abs/2106.15395v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15395v1)
- **Published**: 2021-06-29 13:29:18+00:00
- **Updated**: 2021-06-29 13:29:18+00:00
- **Authors**: Zhiyang Lu, Zheng Li, Jun Wang, Jun shi, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: The thick-slice magnetic resonance (MR) images are often structurally blurred in coronal and sagittal views, which causes harm to diagnosis and image post-processing. Deep learning (DL) has shown great potential to re-construct the high-resolution (HR) thin-slice MR images from those low-resolution (LR) cases, which we refer to as the slice interpolation task in this work. However, since it is generally difficult to sample abundant paired LR-HR MR images, the classical fully supervised DL-based models cannot be effectively trained to get robust performance. To this end, we propose a novel Two-stage Self-supervised Cycle-consistency Network (TSCNet) for MR slice interpolation, in which a two-stage self-supervised learning (SSL) strategy is developed for unsupervised DL network training. The paired LR-HR images are synthesized along the sagittal and coronal directions of input LR images for network pretraining in the first-stage SSL, and then a cyclic in-terpolation procedure based on triplet axial slices is designed in the second-stage SSL for further refinement. More training samples with rich contexts along all directions are exploited as guidance to guarantee the improved in-terpolation performance. Moreover, a new cycle-consistency constraint is proposed to supervise this cyclic procedure, which encourages the network to reconstruct more realistic HR images. The experimental results on a real MRI dataset indicate that TSCNet achieves superior performance over the conventional and other SSL-based algorithms, and obtains competitive quali-tative and quantitative results compared with the fully supervised algorithm.



### IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2106.15413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15413v1)
- **Published**: 2021-06-29 13:34:20+00:00
- **Updated**: 2021-06-29 13:34:20+00:00
- **Authors**: Jie Li, Laiyan Ding, Rui Huang
- **Comment**: Accepted by IJCAI 2021
- **Journal**: None
- **Summary**: 3D semantic scene completion and 2D semantic segmentation are two tightly correlated tasks that are both essential for indoor scene understanding, because they predict the same semantic classes, using positively correlated high-level features. Current methods use 2D features extracted from early-fused RGB-D images for 2D segmentation to improve 3D scene completion. We argue that this sequential scheme does not ensure these two tasks fully benefit each other, and present an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage. Specifically, two refinement modules are developed under a unified framework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP) module, which receives the projection from the current 3D predictions to refine the 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is proposed to leverage the reprojected results from 2D predictions to update the coarse 3D predictions. This iterative fusion happens to the stable high-level features of both tasks at a late stage. Extensive experiments on NYU and NYUCAD datasets verify the effectiveness of the proposed iterative late fusion scheme, and our approach outperforms the state of the art on both 3D semantic scene completion and 2D semantic segmentation.



### Spiking-GAN: A Spiking Generative Adversarial Network Using Time-To-First-Spike Coding
- **Arxiv ID**: http://arxiv.org/abs/2106.15420v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2106.15420v1)
- **Published**: 2021-06-29 13:43:07+00:00
- **Updated**: 2021-06-29 13:43:07+00:00
- **Authors**: Vineet Kotariya, Udayan Ganguly
- **Comment**: None
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) have shown great potential in solving deep learning problems in an energy-efficient manner. However, they are still limited to simple classification tasks. In this paper, we propose Spiking-GAN, the first spike-based Generative Adversarial Network (GAN). It employs a kind of temporal coding scheme called time-to-first-spike coding. We train it using approximate backpropagation in the temporal domain. We use simple integrate-and-fire (IF) neurons with very high refractory period for our network which ensures a maximum of one spike per neuron. This makes the model much sparser than a spike rate-based system. Our modified temporal loss function called 'Aggressive TTFS' improves the inference time of the network by over 33% and reduces the number of spikes in the network by more than 11% compared to previous works. Our experiments show that on training the network on the MNIST dataset using this approach, we can generate high quality samples. Thereby demonstrating the potential of this framework for solving such problems in the spiking domain.



### Detecting Cattle and Elk in the Wild from Space
- **Arxiv ID**: http://arxiv.org/abs/2106.15448v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15448v1)
- **Published**: 2021-06-29 14:35:23+00:00
- **Updated**: 2021-06-29 14:35:23+00:00
- **Authors**: Caleb Robinson, Anthony Ortiz, Lacey Hughey, Jared A. Stabach, Juan M. Lavista Ferres
- **Comment**: Presented at the KDD 2021 Fragile Earth Workshop
- **Journal**: None
- **Summary**: Localizing and counting large ungulates -- hoofed mammals like cows and elk -- in very high-resolution satellite imagery is an important task for supporting ecological studies. Prior work has shown that this is feasible with deep learning based methods and sub-meter multi-spectral satellite imagery. We extend this line of work by proposing a baseline method, CowNet, that simultaneously estimates the number of animals in an image (counts), as well as predicts their location at a pixel level (localizes). We also propose an methodology for evaluating such models on counting and localization tasks across large scenes that takes the uncertainty of noisy labels and the information needed by stakeholders in ecological monitoring tasks into account. Finally, we benchmark our baseline method with state of the art vision methods for counting objects in scenes. We specifically test the temporal generalization of the resulting models over a large landscape in Point Reyes Seashore, CA. We find that the LC-FCN model performs the best and achieves an average precision between 0.56 and 0.61 and an average recall between 0.78 and 0.92 over three held out test scenes.



### Critically examining the Domain Generalizability of Facial Expression Recognition models
- **Arxiv ID**: http://arxiv.org/abs/2106.15453v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.15453v2)
- **Published**: 2021-06-29 14:41:19+00:00
- **Updated**: 2023-03-08 20:54:12+00:00
- **Authors**: Varsha Suresh, Gerard Yeo, Desmond C. Ong
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Facial Expression Recognition is a commercially-important application, but one under-appreciated limitation is that such applications require making predictions on out-of-sample distributions, where target images have different properties from the images the model was trained on. How well -- or how badly -- do facial expression recognition models do on unseen target domains? We provide a systematic and critical evaluation of transfer learning -- specifically, domain generalization -- in facial expression recognition. Using a state-of-the-art model with twelve datasets (six collected in-lab and six ``in-the-wild"), we conduct extensive round-robin-style experiments to evaluate classification accuracies when given new data from an unseen dataset. We also perform multi-source experiments to examine a model's ability to generalize from multiple source datasets, including (i) within-setting (e.g., lab to lab), (ii) cross-setting (e.g., in-the-wild to lab), and (iii) leave-one-out settings. Finally, we compare our results with three commercially-available software. We find sobering results: the accuracy of single- and multi-source domain generalization is only modest. Even for the best-performing multi-source settings, we observe average classification accuracies of 65.6% (range: 34.6%-88.6%; chance: 14.3%), corresponding to an average drop of 10.8 percentage points from the within-corpus classification performance (mean: 76.4%). We discuss the need for regular, systematic investigations into the generalizability of affective computing models and applications.



### How Does Heterogeneous Label Noise Impact Generalization in Neural Nets?
- **Arxiv ID**: http://arxiv.org/abs/2106.15475v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15475v3)
- **Published**: 2021-06-29 14:58:46+00:00
- **Updated**: 2021-09-26 18:18:18+00:00
- **Authors**: Bidur Khanal, Christopher Kanan
- **Comment**: None
- **Journal**: None
- **Summary**: Incorrectly labeled examples, or label noise, is common in real-world computer vision datasets. While the impact of label noise on learning in deep neural networks has been studied in prior work, these studies have exclusively focused on homogeneous label noise, i.e., the degree of label noise is the same across all categories. However, in the real-world, label noise is often heterogeneous, with some categories being affected to a greater extent than others. Here, we address this gap in the literature. We hypothesized that heterogeneous label noise would only affect the classes that had label noise unless there was transfer from those classes to the classes without label noise. To test this hypothesis, we designed a series of computer vision studies using MNIST, CIFAR-10, CIFAR-100, and MS-COCO where we imposed heterogeneous label noise during the training of multi-class, multi-task, and multi-label systems. Our results provide evidence in support of our hypothesis: label noise only affects the class affected by it unless there is transfer.



### Fast and Accurate Road Crack Detection Based on Adaptive Cost-Sensitive Loss Function
- **Arxiv ID**: http://arxiv.org/abs/2106.15510v2
- **DOI**: 10.1109/TCYB.2021.3103885
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15510v2)
- **Published**: 2021-06-29 15:39:37+00:00
- **Updated**: 2021-09-14 02:13:08+00:00
- **Authors**: Kai Li, Bo Wang, Yingjie Tian, Zhiquan Qi
- **Comment**: 14 pages,12 figures, 8 tables
- **Journal**: None
- **Summary**: Numerous detection problems in computer vision, including road crack detection, suffer from exceedingly foreground-background imbalance. Fortunately, modification of loss function appears to solve this puzzle once and for all. In this paper, we propose a pixel-based adaptive weighted cross-entropy loss in conjunction with Jaccard distance to facilitate high-quality pixel-level road crack detection. Our work profoundly demonstrates the influence of loss functions on detection outcomes, and sheds light on the sophisticated consecutive improvements in the realm of crack detection. Specifically, to verify the effectiveness of the proposed loss, we conduct extensive experiments on four public databases, i.e., CrackForest, AigleRN, Crack360, and BJN260. Compared with the vanilla weighted cross-entropy, the proposed loss significantly speeds up the training process while retaining the test accuracy.



### Hate speech detection using static BERT embeddings
- **Arxiv ID**: http://arxiv.org/abs/2106.15537v1
- **DOI**: 10.1007/978-3-030-93620-4_6
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15537v1)
- **Published**: 2021-06-29 16:17:10+00:00
- **Updated**: 2021-06-29 16:17:10+00:00
- **Authors**: Gaurav Rajput, Narinder Singh punn, Sanjay Kumar Sonbhadra, Sonali Agarwal
- **Comment**: None
- **Journal**: None
- **Summary**: With increasing popularity of social media platforms hate speech is emerging as a major concern, where it expresses abusive speech that targets specific group characteristics, such as gender, religion or ethnicity to spread violence. Earlier people use to verbally deliver hate speeches but now with the expansion of technology, some people are deliberately using social media platforms to spread hate by posting, sharing, commenting, etc. Whether it is Christchurch mosque shootings or hate crimes against Asians in west, it has been observed that the convicts are very much influenced from hate text present online. Even though AI systems are in place to flag such text but one of the key challenges is to reduce the false positive rate (marking non hate as hate), so that these systems can detect hate speech without undermining the freedom of expression. In this paper, we use ETHOS hate speech detection dataset and analyze the performance of hate speech detection classifier by replacing or integrating the word embeddings (fastText (FT), GloVe (GV) or FT + GV) with static BERT embeddings (BE). With the extensive experimental trails it is observed that the neural network performed better with static BE compared to using FT, GV or FT + GV as word embeddings. In comparison to fine-tuned BERT, one metric that significantly improved is specificity.



### Uncertainty-Guided Progressive GANs for Medical Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2106.15542v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15542v2)
- **Published**: 2021-06-29 16:26:12+00:00
- **Updated**: 2021-07-02 05:13:09+00:00
- **Authors**: Uddeshya Upadhyay, Yanbei Chen, Tobias Hepp, Sergios Gatidis, Zeynep Akata
- **Comment**: accepted at MICCAI 2021, code is released here:
  https://github.com/ExplainableML/UncerGuidedI2I
- **Journal**: None
- **Summary**: Image-to-image translation plays a vital role in tackling various medical imaging tasks such as attenuation correction, motion correction, undersampled reconstruction, and denoising. Generative adversarial networks have been shown to achieve the state-of-the-art in generating high fidelity images for these tasks. However, the state-of-the-art GAN-based frameworks do not estimate the uncertainty in the predictions made by the network that is essential for making informed medical decisions and subsequent revision by medical experts and has recently been shown to improve the performance and interpretability of the model. In this work, we propose an uncertainty-guided progressive learning scheme for image-to-image translation. By incorporating aleatoric uncertainty as attention maps for GANs trained in a progressive manner, we generate images of increasing fidelity progressively. We demonstrate the efficacy of our model on three challenging medical image translation tasks, including PET to CT translation, undersampled MRI reconstruction, and MRI motion artefact correction. Our model generalizes well in three different tasks and improves performance over state of the art under full-supervision and weak-supervision with limited data. Code is released here: https://github.com/ExplainableML/UncerGuidedI2I



### Unified Questioner Transformer for Descriptive Question Generation in Goal-Oriented Visual Dialogue
- **Arxiv ID**: http://arxiv.org/abs/2106.15550v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15550v1)
- **Published**: 2021-06-29 16:36:34+00:00
- **Updated**: 2021-06-29 16:36:34+00:00
- **Authors**: Shoya Matsumori, Kosuke Shingyouchi, Yuki Abe, Yosuke Fukuchi, Komei Sugiura, Michita Imai
- **Comment**: None
- **Journal**: None
- **Summary**: Building an interactive artificial intelligence that can ask questions about the real world is one of the biggest challenges for vision and language problems. In particular, goal-oriented visual dialogue, where the aim of the agent is to seek information by asking questions during a turn-taking dialogue, has been gaining scholarly attention recently. While several existing models based on the GuessWhat?! dataset have been proposed, the Questioner typically asks simple category-based questions or absolute spatial questions. This might be problematic for complex scenes where the objects share attributes or in cases where descriptive questions are required to distinguish objects. In this paper, we propose a novel Questioner architecture, called Unified Questioner Transformer (UniQer), for descriptive question generation with referring expressions. In addition, we build a goal-oriented visual dialogue task called CLEVR Ask. It synthesizes complex scenes that require the Questioner to generate descriptive questions. We train our model with two variants of CLEVR Ask datasets. The results of the quantitative and qualitative evaluations show that UniQer outperforms the baseline.



### Evaluation of Automated Image Descriptions for Visually Impaired Students
- **Arxiv ID**: http://arxiv.org/abs/2106.15553v1
- **DOI**: 10.1007/978-3-030-78270-2_35
- **Categories**: **cs.HC**, cs.CV, cs.CY, H.5.2; I.4; K.3.1
- **Links**: [PDF](http://arxiv.org/pdf/2106.15553v1)
- **Published**: 2021-06-29 16:40:04+00:00
- **Updated**: 2021-06-29 16:40:04+00:00
- **Authors**: Anett Hoppe, David Morris, Ralph Ewerth
- **Comment**: 6 pages, 12 references. Accepted for publication at the 22nd
  International Conference on Artificial Intelligence in Education (AIED 2021),
  June 14-16 2021, Utrecht, The Netherlands
- **Journal**: Hoppe A., Morris D., Ewerth R. (2021) Evaluation of Automated
  Image Descriptions for Visually Impaired Students. In: Roll I., McNamara D.,
  Sosnovsky S., Luckin R., Dimitrova V. (eds) AIED 2021. LNCS vol 12749.
  Springer, Cham
- **Summary**: Illustrations are widely used in education, and sometimes, alternatives are not available for visually impaired students. Therefore, those students would benefit greatly from an automatic illustration description system, but only if those descriptions were complete, correct, and easily understandable using a screenreader. In this paper, we report on a study for the assessment of automated image descriptions. We interviewed experts to establish evaluation criteria, which we then used to create an evaluation questionnaire for sighted non-expert raters, and description templates. We used this questionnaire to evaluate the quality of descriptions which could be generated with a template-based automatic image describer. We present evidence that these templates have the potential to generate useful descriptions, and that the questionnaire identifies problems with description templates.



### A Mixed-Supervision Multilevel GAN Framework for Image Quality Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2106.15575v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15575v1)
- **Published**: 2021-06-29 17:10:41+00:00
- **Updated**: 2021-06-29 17:10:41+00:00
- **Authors**: Uddeshya Upadhyay, Suyash Awate
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: Deep neural networks for image quality enhancement typically need large quantities of highly-curated training data comprising pairs of low-quality images and their corresponding high-quality images. While high-quality image acquisition is typically expensive and time-consuming, medium-quality images are faster to acquire, at lower equipment costs, and available in larger quantities. Thus, we propose a novel generative adversarial network (GAN) that can leverage training data at multiple levels of quality (e.g., high and medium quality) to improve performance while limiting costs of data curation. We apply our mixed-supervision GAN to (i) super-resolve histopathology images and (ii) enhance laparoscopy images by combining super-resolution and surgical smoke removal. Results on large clinical and pre-clinical datasets show the benefits of our mixed-supervision GAN over the state of the art.



### Segmentation with Multiple Acceptable Annotations: A Case Study of Myocardial Segmentation in Contrast Echocardiography
- **Arxiv ID**: http://arxiv.org/abs/2106.15597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15597v1)
- **Published**: 2021-06-29 17:32:24+00:00
- **Updated**: 2021-06-29 17:32:24+00:00
- **Authors**: Dewen Zeng, Mingqi Li, Yukun Ding, Xiaowei Xu, Qiu Xie, Ruixue Xu, Hongwen Fei, Meiping Huang, Jian Zhuang, Yiyu Shi
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Most existing deep learning-based frameworks for image segmentation assume that a unique ground truth is known and can be used for performance evaluation. This is true for many applications, but not all. Myocardial segmentation of Myocardial Contrast Echocardiography (MCE), a critical task in automatic myocardial perfusion analysis, is an example. Due to the low resolution and serious artifacts in MCE data, annotations from different cardiologists can vary significantly, and it is hard to tell which one is the best. In this case, how can we find a good way to evaluate segmentation performance and how do we train the neural network? In this paper, we address the first problem by proposing a new extended Dice to effectively evaluate the segmentation performance when multiple accepted ground truth is available. Then based on our proposed metric, we solve the second problem by further incorporating the new metric into a loss function that enables neural networks to flexibly learn general features of myocardium. Experiment results on our clinical MCE data set demonstrate that the neural network trained with the proposed loss function outperforms those existing ones that try to obtain a unique ground truth from multiple annotations, both quantitatively and qualitatively. Finally, our grading study shows that using extended Dice as an evaluation metric can better identify segmentation results that need manual correction compared with using Dice.



### Framework for an Intelligent Affect Aware Smart Home Environment for Elderly People
- **Arxiv ID**: http://arxiv.org/abs/2106.15599v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV, cs.ET, cs.LG, H.5; I.2; I.5; E.0; D.0
- **Links**: [PDF](http://arxiv.org/pdf/2106.15599v1)
- **Published**: 2021-06-29 17:34:16+00:00
- **Updated**: 2021-06-29 17:34:16+00:00
- **Authors**: Nirmalya Thakur, Chia Y. Han
- **Comment**: None
- **Journal**: International Journal of Recent Trends in Human Computer
  Interaction (IJHCI), Volume - 9, Issue 1, 2019, pp. 23-43
- **Summary**: The population of elderly people has been increasing at a rapid rate over the last few decades and their population is expected to further increase in the upcoming future. Their increasing population is associated with their increasing needs due to problems like physical disabilities, cognitive issues, weakened memory and disorganized behavior, that elderly people face with increasing age. To reduce their financial burden on the world economy and to enhance their quality of life, it is essential to develop technology-based solutions that are adaptive, assistive and intelligent in nature. Intelligent Affect Aware Systems that can not only analyze but also predict the behavior of elderly people in the context of their day to day interactions with technology in an IoT-based environment, holds immense potential for serving as a long-term solution for improving the user experience of elderly in smart homes. This work therefore proposes the framework for an Intelligent Affect Aware environment for elderly people that can not only analyze the affective components of their interactions but also predict their likely user experience even before they start engaging in any activity in the given smart home environment. This forecasting of user experience would provide scope for enhancing the same, thereby increasing the assistive and adaptive nature of such intelligent systems. To uphold the efficacy of this proposed framework for improving the quality of life of elderly people in smart homes, it has been tested on three datasets and the results are presented and discussed.



### Framework for A Personalized Intelligent Assistant to Elderly People for Activities of Daily Living
- **Arxiv ID**: http://arxiv.org/abs/2107.07344v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV, cs.LG, H.5; I.2; I.5; E.0; D.0
- **Links**: [PDF](http://arxiv.org/pdf/2107.07344v1)
- **Published**: 2021-06-29 17:36:07+00:00
- **Updated**: 2021-06-29 17:36:07+00:00
- **Authors**: Nirmalya Thakur, Chia Y. Han
- **Comment**: arXiv admin note: text overlap with arXiv:2106.15599
- **Journal**: International Journal of Recent Trends in Human Computer
  Interaction (IJHCI), Volume 9, Issue 1, 2019, pp. 1-22
- **Summary**: The increasing population of elderly people is associated with the need to meet their increasing requirements and to provide solutions that can improve their quality of life in a smart home. In addition to fear and anxiety towards interfacing with systems; cognitive disabilities, weakened memory, disorganized behavior and even physical limitations are some of the problems that elderly people tend to face with increasing age. The essence of providing technology-based solutions to address these needs of elderly people and to create smart and assisted living spaces for the elderly; lies in developing systems that can adapt by addressing their diversity and can augment their performances in the context of their day to day goals. Therefore, this work proposes a framework for development of a Personalized Intelligent Assistant to help elderly people perform Activities of Daily Living (ADLs) in a smart and connected Internet of Things (IoT) based environment. This Personalized Intelligent Assistant can analyze different tasks performed by the user and recommend activities by considering their daily routine, current affective state and the underlining user experience. To uphold the efficacy of this proposed framework, it has been tested on a couple of datasets for modelling an average user and a specific user respectively. The results presented show that the model achieves a performance accuracy of 73.12% when modelling a specific user, which is considerably higher than its performance while modelling an average user, this upholds the relevance for development and implementation of this proposed framework.



### An Image is Worth More Than a Thousand Words: Towards Disentanglement in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2106.15610v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15610v2)
- **Published**: 2021-06-29 17:54:24+00:00
- **Updated**: 2021-10-25 17:43:12+00:00
- **Authors**: Aviv Gabbay, Niv Cohen, Yedid Hoshen
- **Comment**: NeurIPS 2021. Project page: http://www.vision.huji.ac.il/zerodim
- **Journal**: None
- **Summary**: Unsupervised disentanglement has been shown to be theoretically impossible without inductive biases on the models and the data. As an alternative approach, recent methods rely on limited supervision to disentangle the factors of variation and allow their identifiability. While annotating the true generative factors is only required for a limited number of observations, we argue that it is infeasible to enumerate all the factors of variation that describe a real-world image distribution. To this end, we propose a method for disentangling a set of factors which are only partially labeled, as well as separating the complementary set of residual factors that are never explicitly specified. Our success in this challenging setting, demonstrated on synthetic benchmarks, gives rise to leveraging off-the-shelf image descriptors to partially annotate a subset of attributes in real image domains (e.g. of human faces) with minimal manual effort. Specifically, we use a recent language-image embedding model (CLIP) to annotate a set of attributes of interest in a zero-shot manner and demonstrate state-of-the-art disentangled image manipulation results.



### Learning to Map for Active Semantic Goal Navigation
- **Arxiv ID**: http://arxiv.org/abs/2106.15648v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.15648v2)
- **Published**: 2021-06-29 18:01:30+00:00
- **Updated**: 2022-03-08 21:02:12+00:00
- **Authors**: Georgios Georgakis, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Kostas Daniilidis
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of object goal navigation in unseen environments. Solving this problem requires learning of contextual semantic priors, a challenging endeavour given the spatial and semantic variability of indoor environments. Current methods learn to implicitly encode these priors through goal-oriented navigation policy functions operating on spatial representations that are limited to the agent's observable areas. In this work, we propose a novel framework that actively learns to generate semantic maps outside the field of view of the agent and leverages the uncertainty over the semantic classes in the unobserved areas to decide on long term goals. We demonstrate that through this spatial prediction strategy, we are able to learn semantic priors in scenes that can be leveraged in unknown environments. Additionally, we show how different objectives can be defined by balancing exploration with exploitation during searching for semantic targets. Our method is validated in the visually realistic environments of the Matterport3D dataset and show improved results on object goal navigation over competitive baselines.



### SIMPL: Generating Synthetic Overhead Imagery to Address Zero-shot and Few-Shot Detection Problems
- **Arxiv ID**: http://arxiv.org/abs/2106.15681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15681v1)
- **Published**: 2021-06-29 19:06:05+00:00
- **Updated**: 2021-06-29 19:06:05+00:00
- **Authors**: Yang Xu, Bohao Huang, Xiong Luo, Kyle Bradbury, Jordan M. Malof
- **Comment**: None
- **Journal**: None
- **Summary**: Recently deep neural networks (DNNs) have achieved tremendous success for object detection in overhead (e.g., satellite) imagery. One ongoing challenge however is the acquisition of training data, due to high costs of obtaining satellite imagery and annotating objects in it. In this work we present a simple approach - termed Synthetic object IMPLantation (SIMPL) - to easily and rapidly generate large quantities of synthetic overhead training data for custom target objects. We demonstrate the effectiveness of using SIMPL synthetic imagery for training DNNs in zero-shot scenarios where no real imagery is available; and few-shot learning scenarios, where limited real-world imagery is available. We also conduct experiments to study the sensitivity of SIMPL's effectiveness to some key design parameters, providing users for insights when designing synthetic imagery for custom objects. We release a software implementation of our SIMPL approach so that others can build upon it, or use it for their own custom problems.



### Attention Aware Wavelet-based Detection of Morphed Face Images
- **Arxiv ID**: http://arxiv.org/abs/2106.15686v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15686v2)
- **Published**: 2021-06-29 19:29:19+00:00
- **Updated**: 2021-07-22 19:46:56+00:00
- **Authors**: Poorya Aghdaie, Baaria Chaudhary, Sobhan Soleymani, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: IJCB 2021
- **Journal**: None
- **Summary**: Morphed images have exploited loopholes in the face recognition checkpoints, e.g., Credential Authentication Technology (CAT), used by Transportation Security Administration (TSA), which is a non-trivial security concern. To overcome the risks incurred due to morphed presentations, we propose a wavelet-based morph detection methodology which adopts an end-to-end trainable soft attention mechanism . Our attention-based deep neural network (DNN) focuses on the salient Regions of Interest (ROI) which have the most spatial support for morph detector decision function, i.e, morph class binary softmax output. A retrospective of morph synthesizing procedure aids us to speculate the ROI as regions around facial landmarks , particularly for the case of landmark-based morphing techniques. Moreover, our attention-based DNN is adapted to the wavelet space, where inputs of the network are coarse-to-fine spectral representations, 48 stacked wavelet sub-bands to be exact. We evaluate performance of the proposed framework using three datasets, VISAPP17, LMA, and MorGAN. In addition, as attention maps can be a robust indicator whether a probe image under investigation is genuine or counterfeit, we analyze the estimated attention maps for both a bona fide image and its corresponding morphed image. Finally, we present an ablation study on the efficacy of utilizing attention mechanism for the sake of morph detection.



### SinGAN-Seg: Synthetic training data generation for medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.00471v2
- **DOI**: 10.1371/journal.pone.0267976
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00471v2)
- **Published**: 2021-06-29 19:34:34+00:00
- **Updated**: 2022-04-25 14:52:46+00:00
- **Authors**: Vajira Thambawita, Pegah Salehi, Sajad Amouei Sheshkal, Steven A. Hicks, Hugo L. Hammer, Sravanthi Parasa, Thomas de Lange, P√•l Halvorsen, Michael A. Riegler
- **Comment**: None
- **Journal**: None
- **Summary**: Analyzing medical data to find abnormalities is a time-consuming and costly task, particularly for rare abnormalities, requiring tremendous efforts from medical experts. Artificial intelligence has become a popular tool for the automatic processing of medical data, acting as a supportive tool for doctors. However, the machine learning models used to build these tools are highly dependent on the data used to train them. Large amounts of data can be difficult to obtain in medicine due to privacy, expensive and time-consuming annotations, and a general lack of data samples for infrequent lesions. Here, we present a novel synthetic data generation pipeline, called SinGAN-Seg, to produce synthetic medical images with corresponding masks using a single training image. Our method is different from the traditional GANs because our model needs only a single image and the corresponding ground truth to train. Our method produces alternative artificial segmentation datasets with ground truth masks when real datasets are not allowed to share. The pipeline is evaluated using qualitative and quantitative comparisons between real and synthetic data to show that the style transfer technique used in our pipeline significantly improves the quality of the generated data and our method is better than other state-of-the-art GANs to prepare synthetic images when the size of training datasets are limited. By training UNet++ using both real and the synthetic data generated from the SinGAN-Seg pipeline, we show that models trained with synthetic data have very close performances to those trained on real data when the datasets have a considerable amount of data. In contrast, Synthetic data generated from the SinGAN-Seg pipeline can improve the performance of segmentation models when training datasets do not have a considerable amount of data. The code is available on GitHub.



### Domain adaptation for person re-identification on new unlabeled data using AlignedReID++
- **Arxiv ID**: http://arxiv.org/abs/2106.15693v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45 (Primary) 68T10, 68T07 (Secondary), I.4.9; I.5.4; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2106.15693v1)
- **Published**: 2021-06-29 19:58:04+00:00
- **Updated**: 2021-06-29 19:58:04+00:00
- **Authors**: Tiago de C. G. Pereira, Teofilo E. de Campos
- **Comment**: 9 pages; 4 figues; built upon work published in VISAPP 2020 (best
  student paper award)
- **Journal**: None
- **Summary**: In the world where big data reigns and there is plenty of hardware prepared to gather a huge amount of non structured data, data acquisition is no longer a problem. Surveillance cameras are ubiquitous and they capture huge numbers of people walking across different scenes. However, extracting value from this data is challenging, specially for tasks that involve human images, such as face recognition and person re-identification. Annotation of this kind of data is a challenging and expensive task. In this work we propose a domain adaptation workflow to allow CNNs that were trained in one domain to be applied to another domain without the need for new annotation of the target data. Our method uses AlignedReID++ as the baseline, trained using a Triplet loss with batch hard. Domain adaptation is done by using pseudo-labels generated using an unsupervised learning strategy. Our results show that domain adaptation techniques really improve the performance of the CNN when applied in the target domain.



### RICE: Refining Instance Masks in Cluttered Environments with Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.15711v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.15711v1)
- **Published**: 2021-06-29 20:29:29+00:00
- **Updated**: 2021-06-29 20:29:29+00:00
- **Authors**: Christopher Xie, Arsalan Mousavian, Yu Xiang, Dieter Fox
- **Comment**: None
- **Journal**: None
- **Summary**: Segmenting unseen object instances in cluttered environments is an important capability that robots need when functioning in unstructured environments. While previous methods have exhibited promising results, they still tend to provide incorrect results in highly cluttered scenes. We postulate that a network architecture that encodes relations between objects at a high-level can be beneficial. Thus, in this work, we propose a novel framework that refines the output of such methods by utilizing a graph-based representation of instance masks. We train deep networks capable of sampling smart perturbations to the segmentations, and a graph neural network, which can encode relations between objects, to evaluate the perturbed segmentations. Our proposed method is orthogonal to previous works and achieves state-of-the-art performance when combined with them. We demonstrate an application that uses uncertainty estimates generated by our method to guide a manipulator, leading to efficient understanding of cluttered scenes. Code, models, and video can be found at https://github.com/chrisdxie/rice .



### Diff2Dist: Learning Spectrally Distinct Edge Functions, with Applications to Cell Morphology Analysis
- **Arxiv ID**: http://arxiv.org/abs/2106.15716v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.MG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15716v1)
- **Published**: 2021-06-29 20:40:22+00:00
- **Updated**: 2021-06-29 20:40:22+00:00
- **Authors**: Cory Braker Scott, Eric Mjolsness, Diane Oyen, Chie Kodera, David Bouchez, Magalie Uyttewaal
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for learning "spectrally descriptive" edge weights for graphs. We generalize a previously known distance measure on graphs (Graph Diffusion Distance), thereby allowing it to be tuned to minimize an arbitrary loss function. Because all steps involved in calculating this modified GDD are differentiable, we demonstrate that it is possible for a small neural network model to learn edge weights which minimize loss. GDD alone does not effectively discriminate between graphs constructed from shoot apical meristem images of wild-type vs. mutant \emph{Arabidopsis thaliana} specimens. However, training edge weights and kernel parameters with contrastive loss produces a learned distance metric with large margins between these graph categories. We demonstrate this by showing improved performance of a simple k-nearest-neighbors classifier on the learned distance matrix. We also demonstrate a further application of this method to biological image analysis: once trained, we use our model to compute the distance between the biological graphs and a set of graphs output by a cell division simulator. This allows us to identify simulation parameter regimes which are similar to each class of graph in our original dataset.



### RCNN-SliceNet: A Slice and Cluster Approach for Nuclei Centroid Detection in Three-Dimensional Fluorescence Microscopy Images
- **Arxiv ID**: http://arxiv.org/abs/2106.15753v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.15753v3)
- **Published**: 2021-06-29 23:38:29+00:00
- **Updated**: 2021-11-04 15:59:13+00:00
- **Authors**: Liming Wu, Shuo Han, Alain Chen, Paul Salama, Kenneth W. Dunn, Edward J. Delp
- **Comment**: None
- **Journal**: None
- **Summary**: Robust and accurate nuclei centroid detection is important for the understanding of biological structures in fluorescence microscopy images. Existing automated nuclei localization methods face three main challenges: (1) Most of object detection methods work only on 2D images and are difficult to extend to 3D volumes; (2) Segmentation-based models can be used on 3D volumes but it is computational expensive for large microscopy volumes and they have difficulty distinguishing different instances of objects; (3) Hand annotated ground truth is limited for 3D microscopy volumes. To address these issues, we present a scalable approach for nuclei centroid detection of 3D microscopy volumes. We describe the RCNN-SliceNet to detect 2D nuclei centroids for each slice of the volume from different directions and 3D agglomerative hierarchical clustering (AHC) is used to estimate the 3D centroids of nuclei in a volume. The model was trained with the synthetic microscopy data generated using Spatially Constrained Cycle-Consistent Adversarial Networks (SpCycleGAN) and tested on different types of real 3D microscopy data. Extensive experimental results demonstrate that our proposed method can accurately count and detect the nuclei centroids in a 3D microscopy volume.



### Looking Outside the Window: Wide-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2106.15754v6
- **DOI**: 10.1109/TGRS.2022.3168697
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15754v6)
- **Published**: 2021-06-29 23:41:54+00:00
- **Updated**: 2022-04-20 03:35:22+00:00
- **Authors**: Lei Ding, Dong Lin, Shaofu Lin, Jing Zhang, Xiaojie Cui, Yuebin Wang, Hao Tang, Lorenzo Bruzzone
- **Comment**: None
- **Journal**: None
- **Summary**: Long-range contextual information is crucial for the semantic segmentation of High-Resolution (HR) Remote Sensing Images (RSIs). However, image cropping operations, commonly used for training neural networks, limit the perception of long-range contexts in large RSIs. To overcome this limitation, we propose a Wide-Context Network (WiCoNet) for the semantic segmentation of HR RSIs. Apart from extracting local features with a conventional CNN, the WiCoNet has an extra context branch to aggregate information from a larger image area. Moreover, we introduce a Context Transformer to embed contextual information from the context branch and selectively project it onto the local features. The Context Transformer extends the Vision Transformer, an emerging kind of neural network, to model the dual-branch semantic correlations. It overcomes the locality limitation of CNNs and enables the WiCoNet to see the bigger picture before segmenting the land-cover/land-use (LCLU) classes. Ablation studies and comparative experiments conducted on several benchmark datasets demonstrate the effectiveness of the proposed method. In addition, we present a new Beijing Land-Use (BLU) dataset. This is a large-scale HR satellite dataset with high-quality and fine-grained reference labels, which can facilitate future studies in this field.



