# Arxiv Papers in cs.CV on 2021-06-14
### Understanding Latent Correlation-Based Multiview Learning and Self-Supervision: An Identifiability Perspective
- **Arxiv ID**: http://arxiv.org/abs/2106.07115v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2106.07115v3)
- **Published**: 2021-06-14 00:12:36+00:00
- **Updated**: 2022-04-08 19:37:10+00:00
- **Authors**: Qi Lyu, Xiao Fu, Weiran Wang, Songtao Lu
- **Comment**: Accepted to ICLR 2022 Spotlight, 37 pages, 11 figures
- **Journal**: None
- **Summary**: Multiple views of data, both naturally acquired (e.g., image and audio) and artificially produced (e.g., via adding different noise to data samples), have proven useful in enhancing representation learning. Natural views are often handled by multiview analysis tools, e.g., (deep) canonical correlation analysis [(D)CCA], while the artificial ones are frequently used in self-supervised learning (SSL) paradigms, e.g., BYOL and Barlow Twins. Both types of approaches often involve learning neural feature extractors such that the embeddings of data exhibit high cross-view correlations. Although intuitive, the effectiveness of correlation-based neural embedding is mostly empirically validated.   This work aims to understand latent correlation maximization-based deep multiview learning from a latent component identification viewpoint. An intuitive generative model of multiview data is adopted, where the views are different nonlinear mixtures of shared and private components. Since the shared components are view/distortion-invariant, representing the data using such components is believed to reveal the identity of the samples effectively and robustly. Under this model, latent correlation maximization is shown to guarantee the extraction of the shared components across views (up to certain ambiguities). In addition, it is further shown that the private information in each view can be provably disentangled from the shared using proper regularization design. A finite sample analysis, which has been rare in nonlinear mixture identifiability study, is also presented. The theoretical results and newly designed regularization are tested on a series of tasks.



### Discerning the painter's hand: machine learning on surface topography
- **Arxiv ID**: http://arxiv.org/abs/2106.07134v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07134v1)
- **Published**: 2021-06-14 02:17:43+00:00
- **Updated**: 2021-06-14 02:17:43+00:00
- **Authors**: F. Ji, M. S. McMaster, S. Schwab, G. Singh, L. N. Smith, S. Adhikari, M. O'Dwyer, F. Sayed, A. Ingrisano, D. Yoder, E. S. Bolman, I. T. Martin, M. Hinczewski, K. D. Singer
- **Comment**: main text: 24 pages, 6 figures; SI: 6 pages, 4 figures
- **Journal**: None
- **Summary**: Attribution of paintings is a critical problem in art history. This study extends machine learning analysis to surface topography of painted works. A controlled study of positive attribution was designed with paintings produced by a class of art students. The paintings were scanned using a confocal optical profilometer to produce surface data. The surface data were divided into virtual patches and used to train an ensemble of convolutional neural networks (CNNs) for attribution. Over a range of patch sizes from 0.5 to 60 mm, the resulting attribution was found to be 60 to 96% accurate, and, when comparing regions of different color, was nearly twice as accurate as CNNs using color images of the paintings. Remarkably, short length scales, as small as twice a bristle diameter, were the key to reliably distinguishing among artists. These results show promise for real-world attribution, particularly in the case of workshop practice.



### Bayesian dense inverse searching algorithm for real-time stereo matching in minimally invasive surgery
- **Arxiv ID**: http://arxiv.org/abs/2106.07136v2
- **DOI**: 10.1007/978-3-031-16449-1_32
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07136v2)
- **Published**: 2021-06-14 02:26:27+00:00
- **Updated**: 2022-09-16 10:22:52+00:00
- **Authors**: Jingwei Song, Qiuchen Zhu, Jianyu Lin, Maani Ghaffari
- **Comment**: Accepted by '25th International Conference on Medical Image Computing
  and Computer Assisted Intervention, MICCAI 2022' as oral presentation
- **Journal**: Medical Image Computing and Computer Assisted Intervention --
  MICCAI 2022
- **Summary**: This paper reports a CPU-level real-time stereo matching method for surgical images (10 Hz on 640 * 480 image with a single core of i5-9400). The proposed method is built on the fast ''dense inverse searching'' algorithm, which estimates the disparity of the stereo images. The overlapping image patches (arbitrary squared image segment) from the images at different scales are aligned based on the photometric consistency presumption. We propose a Bayesian framework to evaluate the probability of the optimized patch disparity at different scales. Moreover, we introduce a spatial Gaussian mixed probability distribution to address the pixel-wise probability within the patch. In-vivo and synthetic experiments show that our method can handle ambiguities resulted from the textureless surfaces and the photometric inconsistency caused by the Lambertian reflectance. Our Bayesian method correctly balances the probability of the patch for stereo images at different scales. Experiments indicate that the estimated depth has higher accuracy and fewer outliers than the baseline methods in the surgical scenario.



### SinIR: Efficient General Image Manipulation with Single Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2106.07140v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07140v1)
- **Published**: 2021-06-14 02:41:26+00:00
- **Updated**: 2021-06-14 02:41:26+00:00
- **Authors**: Jihyeong Yoo, Qifeng Chen
- **Comment**: Accepted to ICML 2021
- **Journal**: None
- **Summary**: We propose SinIR, an efficient reconstruction-based framework trained on a single natural image for general image manipulation, including super-resolution, editing, harmonization, paint-to-image, photo-realistic style transfer, and artistic style transfer. We train our model on a single image with cascaded multi-scale learning, where each network at each scale is responsible for image reconstruction. This reconstruction objective greatly reduces the complexity and running time of training, compared to the GAN objective. However, the reconstruction objective also exacerbates the output quality. Therefore, to solve this problem, we further utilize simple random pixel shuffling, which also gives control over manipulation, inspired by the Denoising Autoencoder. With quantitative evaluation, we show that SinIR has competitive performance on various image manipulation tasks. Moreover, with a much simpler training objective (i.e., reconstruction), SinIR is trained 33.5 times faster than SinGAN (for 500 X 500 images) that solves similar tasks. Our code is publicly available at github.com/YooJiHyeong/SinIR.



### Selection of Source Images Heavily Influences the Effectiveness of Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2106.07141v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07141v3)
- **Published**: 2021-06-14 02:45:45+00:00
- **Updated**: 2021-11-22 08:17:28+00:00
- **Authors**: Utku Ozbulak, Esla Timothy Anzaku, Wesley De Neve, Arnout Van Messem
- **Comment**: Accepted for publication in the 32nd British Machine Vision
  Conference (BMVC)
- **Journal**: None
- **Summary**: Although the adoption rate of deep neural networks (DNNs) has tremendously increased in recent years, a solution for their vulnerability against adversarial examples has not yet been found. As a result, substantial research efforts are dedicated to fix this weakness, with many studies typically using a subset of source images to generate adversarial examples, treating every image in this subset as equal. We demonstrate that, in fact, not every source image is equally suited for this kind of assessment. To do so, we devise a large-scale model-to-model transferability scenario for which we meticulously analyze the properties of adversarial examples, generated from every suitable source image in ImageNet by making use of three of the most frequently deployed attacks. In this transferability scenario, which involves seven distinct DNN models, including the recently proposed vision transformers, we reveal that it is possible to have a difference of up to $12.5\%$ in model-to-model transferability success, $1.01$ in average $L_2$ perturbation, and $0.03$ ($8/225$) in average $L_{\infty}$ perturbation when $1,000$ source images are sampled randomly among all suitable candidates. We then take one of the first steps in evaluating the robustness of images used to create adversarial examples, proposing a number of simple but effective methods to identify unsuitable source images, thus making it possible to mitigate extreme cases in experimentation and support high-quality benchmarking.



### Object-Guided Instance Segmentation With Auxiliary Feature Refinement for Biological Images
- **Arxiv ID**: http://arxiv.org/abs/2106.07159v1
- **DOI**: 10.1109/TMI.2021.3077285
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07159v1)
- **Published**: 2021-06-14 04:35:36+00:00
- **Updated**: 2021-06-14 04:35:36+00:00
- **Authors**: Jingru Yi, Pengxiang Wu, Hui Tang, Bo Liu, Qiaoying Huang, Hui Qu, Lianyi Han, Wei Fan, Daniel J. Hoeppner, Dimitris N. Metaxas
- **Comment**: Accepted in TMI
- **Journal**: None
- **Summary**: Instance segmentation is of great importance for many biological applications, such as study of neural cell interactions, plant phenotyping, and quantitatively measuring how cells react to drug treatment. In this paper, we propose a novel box-based instance segmentation method. Box-based instance segmentation methods capture objects via bounding boxes and then perform individual segmentation within each bounding box region. However, existing methods can hardly differentiate the target from its neighboring objects within the same bounding box region due to their similar textures and low-contrast boundaries. To deal with this problem, in this paper, we propose an object-guided instance segmentation method. Our method first detects the center points of the objects, from which the bounding box parameters are then predicted. To perform segmentation, an object-guided coarse-to-fine segmentation branch is built along with the detection branch. The segmentation branch reuses the object features as guidance to separate target object from the neighboring ones within the same bounding box region. To further improve the segmentation quality, we design an auxiliary feature refinement module that densely samples and refines point-wise features in the boundary regions. Experimental results on three biological image datasets demonstrate the advantages of our method. The code will be available at https://github.com/yijingru/ObjGuided-Instance-Segmentation.



### Self-training Guided Adversarial Domain Adaptation For Thermal Imagery
- **Arxiv ID**: http://arxiv.org/abs/2106.07165v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07165v1)
- **Published**: 2021-06-14 05:17:21+00:00
- **Updated**: 2021-06-14 05:17:21+00:00
- **Authors**: Ibrahim Batuhan Akkaya, Fazil Altinel, Ugur Halici
- **Comment**: Accepted to CVPR 2021 Perception Beyond the Visible Spectrum (PBVS)
  workshop
- **Journal**: None
- **Summary**: Deep models trained on large-scale RGB image datasets have shown tremendous success. It is important to apply such deep models to real-world problems. However, these models suffer from a performance bottleneck under illumination changes. Thermal IR cameras are more robust against such changes, and thus can be very useful for the real-world problems. In order to investigate efficacy of combining feature-rich visible spectrum and thermal image modalities, we propose an unsupervised domain adaptation method which does not require RGB-to-thermal image pairs. We employ large-scale RGB dataset MS-COCO as source domain and thermal dataset FLIR ADAS as target domain to demonstrate results of our method. Although adversarial domain adaptation methods aim to align the distributions of source and target domains, simply aligning the distributions cannot guarantee perfect generalization to the target domain. To this end, we propose a self-training guided adversarial domain adaptation method to promote generalization capabilities of adversarial domain adaptation methods. To perform self-training, pseudo labels are assigned to the samples on the target thermal domain to learn more generalized representations for the target domain. Extensive experimental analyses show that our proposed method achieves better results than the state-of-the-art adversarial domain adaptation methods. The code and models are publicly available.



### 2rd Place Solutions in the HC-STVG track of Person in Context Challenge 2021
- **Arxiv ID**: http://arxiv.org/abs/2106.07166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07166v1)
- **Published**: 2021-06-14 05:18:34+00:00
- **Updated**: 2021-06-14 05:18:34+00:00
- **Authors**: YiYu, XinyingWang, WeiHu, XunLuo, ChengLi
- **Comment**: None
- **Journal**: None
- **Summary**: In this technical report, we present our solution to localize a spatio-temporal person in an untrimmed video based on a sentence. We achieve the second vIOU(0.30025) in the HC-STVG track of the 3rd Person in Context(PIC) Challenge. Our solution contains three parts: 1) human attributes information is extracted from the sentence, it is helpful to filter out tube proposals in the testing phase and supervise our classifier to learn appearance information in the training phase. 2) we detect humans with YoloV5 and track humans based on the DeepSort framework but replace the original ReID network with FastReID. 3) a visual transformer is used to extract cross-modal representations for localizing a spatio-temporal tube of the target person.



### Sejong Face Database: A Multi-Modal Disguise Face Database
- **Arxiv ID**: http://arxiv.org/abs/2106.07186v1
- **DOI**: 10.1016/j.cviu.2021.103218
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07186v1)
- **Published**: 2021-06-14 06:29:41+00:00
- **Updated**: 2021-06-14 06:29:41+00:00
- **Authors**: Usman Cheema, Seungbin Moon
- **Comment**: Database Access Link:
  https://github.com/usmancheema89/SejongFaceDatabase
- **Journal**: Computer Vision and Image Understanding, Volumes 208-209, 2021
- **Summary**: Commercial application of facial recognition demands robustness to a variety of challenges such as illumination, occlusion, spoofing, disguise, etc. Disguised face recognition is one of the emerging issues for access control systems, such as security checkpoints at the borders. However, the lack of availability of face databases with a variety of disguise addons limits the development of academic research in the area. In this paper, we present a multimodal disguised face dataset to facilitate the disguised face recognition research. The presented database contains 8 facial add-ons and 7 additional combinations of these add-ons to create a variety of disguised face images. Each facial image is captured in visible, visible plus infrared, infrared, and thermal spectra. Specifically, the database contains 100 subjects divided into subset-A (30 subjects, 1 image per modality) and subset-B (70 subjects, 5 plus images per modality). We also present baseline face detection results performed on the proposed database to provide reference results and compare the performance in different modalities. Qualitative and quantitative analysis is performed to evaluate the challenging nature of disguise addons. The dataset will be publicly available with the acceptance of the research article. The database is available at: https://github.com/usmancheema89/SejongFaceDatabase.



### Group-based Bi-Directional Recurrent Wavelet Neural Networks for Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2106.07190v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.07190v1)
- **Published**: 2021-06-14 06:36:13+00:00
- **Updated**: 2021-06-14 06:36:13+00:00
- **Authors**: Young-Ju Choi, Young-Woon Lee, Byung-Gyu Kim
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Video super-resolution (VSR) aims to estimate a high-resolution (HR) frame from a low-resolution (LR) frames. The key challenge for VSR lies in the effective exploitation of spatial correlation in an intra-frame and temporal dependency between consecutive frames. However, most of the previous methods treat different types of the spatial features identically and extract spatial and temporal features from the separated modules. It leads to lack of obtaining meaningful information and enhancing the fine details. In VSR, there are three types of temporal modeling frameworks: 2D convolutional neural networks (CNN), 3D CNN, and recurrent neural networks (RNN). Among them, the RNN-based approach is suitable for sequential data. Thus the SR performance can be greatly improved by using the hidden states of adjacent frames. However, at each of time step in a recurrent structure, the RNN-based previous works utilize the neighboring features restrictively. Since the range of accessible motion per time step is narrow, there are still limitations to restore the missing details for dynamic or large motion. In this paper, we propose a group-based bi-directional recurrent wavelet neural networks (GBR-WNN) to exploit the sequential data and spatio-temporal information effectively for VSR. The proposed group-based bi-directional RNN (GBR) temporal modeling framework is built on the well-structured process with the group of pictures (GOP). We propose a temporal wavelet attention (TWA) module, in which attention is adopted for both spatial and temporal features. Experimental results demonstrate that the proposed method achieves superior performance compared with state-of-the-art methods in both of quantitative and qualitative evaluations.



### Hard Samples Rectification for Unsupervised Cross-domain Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2106.07204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07204v1)
- **Published**: 2021-06-14 07:38:42+00:00
- **Updated**: 2021-06-14 07:38:42+00:00
- **Authors**: Chih-Ting Liu, Man-Yu Lee, Tsai-Shien Chen, Shao-Yi Chien
- **Comment**: This paper was accepted by IEEE International Conference on Image
  Processing (ICIP) 2021
- **Journal**: None
- **Summary**: Person re-identification (re-ID) has received great success with the supervised learning methods. However, the task of unsupervised cross-domain re-ID is still challenging. In this paper, we propose a Hard Samples Rectification (HSR) learning scheme which resolves the weakness of original clustering-based methods being vulnerable to the hard positive and negative samples in the target unlabelled dataset. Our HSR contains two parts, an inter-camera mining method that helps recognize a person under different views (hard positive) and a part-based homogeneity technique that makes the model discriminate different persons but with similar appearance (hard negative). By rectifying those two hard cases, the re-ID model can learn effectively and achieve promising results on two large-scale benchmarks.



### Influential Rank: A New Perspective of Post-training for Robust Model against Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2106.07217v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.07217v4)
- **Published**: 2021-06-14 08:04:18+00:00
- **Updated**: 2023-04-19 05:33:58+00:00
- **Authors**: Seulki Park, Hwanjun Song, Daeho Um, Dae Ung Jo, Sangdoo Yun, Jin Young Choi
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Deep neural network can easily overfit to even noisy labels due to its high capacity, which degrades the generalization performance of a model. To overcome this issue, we propose a new approach for learning from noisy labels (LNL) via post-training, which can significantly improve the generalization performance of any pre-trained model on noisy label data. To this end, we rather exploit the overfitting property of a trained model to identify mislabeled samples. Specifically, our post-training approach gradually removes samples with high influence on the decision boundary and refines the decision boundary to improve generalization performance. Our post-training approach creates great synergies when combined with the existing LNL methods. Experimental results on various real-world and synthetic benchmark datasets demonstrate the validity of our approach in diverse realistic scenarios.



### Physics-Aware Downsampling with Deep Learning for Scalable Flood Modeling
- **Arxiv ID**: http://arxiv.org/abs/2106.07218v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.07218v2)
- **Published**: 2021-06-14 08:05:14+00:00
- **Updated**: 2021-10-31 12:29:05+00:00
- **Authors**: Niv Giladi, Zvika Ben-Haim, Sella Nevo, Yossi Matias, Daniel Soudry
- **Comment**: None
- **Journal**: 35th Conference on Neural Information Processing Systems (NeurIPS
  2021)
- **Summary**: Background: Floods are the most common natural disaster in the world, affecting the lives of hundreds of millions. Flood forecasting is therefore a vitally important endeavor, typically achieved using physical water flow simulations, which rely on accurate terrain elevation maps. However, such simulations, based on solving partial differential equations, are computationally prohibitive on a large scale. This scalability issue is commonly alleviated using a coarse grid representation of the elevation map, though this representation may distort crucial terrain details, leading to significant inaccuracies in the simulation. Contributions: We train a deep neural network to perform physics-informed downsampling of the terrain map: we optimize the coarse grid representation of the terrain maps, so that the flood prediction will match the fine grid solution. For the learning process to succeed, we configure a dataset specifically for this task. We demonstrate that with this method, it is possible to achieve a significant reduction in computational cost, while maintaining an accurate solution. A reference implementation accompanies the paper as well as documentation and code for dataset reproduction.



### Context-Aware Image Inpainting with Learned Semantic Priors
- **Arxiv ID**: http://arxiv.org/abs/2106.07220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07220v1)
- **Published**: 2021-06-14 08:09:43+00:00
- **Updated**: 2021-06-14 08:09:43+00:00
- **Authors**: Wendong Zhang, Junwei Zhu, Ying Tai, Yunbo Wang, Wenqing Chu, Bingbing Ni, Chengjie Wang, Xiaokang Yang
- **Comment**: Accepted by IJCAI 2021
- **Journal**: None
- **Summary**: Recent advances in image inpainting have shown impressive results for generating plausible visual details on rather simple backgrounds. However, for complex scenes, it is still challenging to restore reasonable contents as the contextual information within the missing regions tends to be ambiguous. To tackle this problem, we introduce pretext tasks that are semantically meaningful to estimating the missing contents. In particular, we perform knowledge distillation on pretext models and adapt the features to image inpainting. The learned semantic priors ought to be partially invariant between the high-level pretext task and low-level image inpainting, which not only help to understand the global context but also provide structural guidance for the restoration of local textures. Based on the semantic priors, we further propose a context-aware image inpainting model, which adaptively integrates global semantics and local features in a unified image generator. The semantic learner and the image generator are trained in an end-to-end manner. We name the model SPL to highlight its ability to learn and leverage semantic priors. It achieves the state of the art on Places2, CelebA, and Paris StreetView datasets.



### SGE net: Video object detection with squeezed GRU and information entropy map
- **Arxiv ID**: http://arxiv.org/abs/2106.07224v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07224v1)
- **Published**: 2021-06-14 08:26:17+00:00
- **Updated**: 2021-06-14 08:26:17+00:00
- **Authors**: Rui Su, Wenjing Huang, Haoyu Ma, Xiaowei Song, Jinglu Hu
- **Comment**: ICIP 2021
- **Journal**: None
- **Summary**: Recently, deep learning based video object detection has attracted more and more attention. Compared with object detection of static images, video object detection is more challenging due to the motion of objects, while providing rich temporal information. The RNN-based algorithm is an effective way to enhance detection performance in videos with temporal information. However, most studies in this area only focus on accuracy while ignoring the calculation cost and the number of parameters.   In this paper, we propose an efficient method that combines channel-reduced convolutional GRU (Squeezed GRU), and Information Entropy map for video object detection (SGE-Net). The experimental results validate the accuracy improvement, computational savings of the Squeezed GRU, and superiority of the information entropy attention mechanism on the classification performance. The mAP has increased by 3.7 contrasted with the baseline, and the number of parameters has decreased from 6.33 million to 0.67 million compared with the standard GRU.



### More Real than Real: A Study on Human Visual Perception of Synthetic Faces
- **Arxiv ID**: http://arxiv.org/abs/2106.07226v2
- **DOI**: 10.1109/MSP.2021.3120982
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2106.07226v2)
- **Published**: 2021-06-14 08:27:25+00:00
- **Updated**: 2021-10-20 08:56:34+00:00
- **Authors**: Federica Lago, Cecilia Pasquini, Rainer Böhme, Hélène Dumont, Valérie Goffaux, Giulia Boato
- **Comment**: None
- **Journal**: None
- **Summary**: Deep fakes became extremely popular in the last years, also thanks to their increasing realism. Therefore, there is the need to measures human's ability to distinguish between real and synthetic face images when confronted with cutting-edge creation technologies. We describe the design and results of a perceptual experiment we have conducted, where a wide and diverse group of volunteers has been exposed to synthetic face images produced by state-of-the-art Generative Adversarial Networks (namely, PG-GAN, StyleGAN, StyleGAN2). The experiment outcomes reveal how strongly we should call into question our human ability to discriminate real faces from synthetic ones generated through modern AI.



### Automated Parking Space Detection Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.07228v1
- **DOI**: 10.1109/RoboMech.2017.8261114
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07228v1)
- **Published**: 2021-06-14 08:30:38+00:00
- **Updated**: 2021-06-14 08:30:38+00:00
- **Authors**: Julien Nyambal, Richard Klein
- **Comment**: None
- **Journal**: None
- **Summary**: Finding a parking space nowadays becomes an issue that is not to be neglected, it consumes time and energy. We have used computer vision techniques to infer the state of the parking lot given the data collected from the University of The Witwatersrand. This paper presents an approach for a real-time parking space classification based on Convolutional Neural Networks (CNN) using Caffe and Nvidia DiGITS framework. The training process has been done using DiGITS and the output is a caffemodel used for predictions to detect vacant and occupied parking spots. The system checks a defined area whether a parking spot (bounding boxes defined at initialization of the system) is containing a car or not (occupied or vacant). Those bounding box coordinates are saved from a frame of the video of the parking lot in a JSON format, to be later used by the system for sequential prediction on each parking spot. The system has been trained using the LeNet network with the Nesterov Accelerated Gradient as solver and the AlexNet network with the Stochastic Gradient Descent as solver. We were able to get an accuracy on the validation set of 99\% for both networks. The accuracy on a foreign dataset(PKLot) returned as well 99\%. Those are experimental results based on the training set shows how robust the system can be when the prediction has to take place in a different parking space.



### Deterministic Guided LiDAR Depth Map Completion
- **Arxiv ID**: http://arxiv.org/abs/2106.07256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07256v1)
- **Published**: 2021-06-14 09:19:47+00:00
- **Updated**: 2021-06-14 09:19:47+00:00
- **Authors**: Bryan Krauss, Gregory Schroeder, Marko Gustke, Ahmed Hussein
- **Comment**: Submitted to 2021 IEEE Intelligent Vehicles Symposium (IV21). This
  work has been submitted to the IEEE for possible publication. Copyright may
  be transferred without notice, after which this version may no longer be
  accessible
- **Journal**: None
- **Summary**: Accurate dense depth estimation is crucial for autonomous vehicles to analyze their environment. This paper presents a non-deep learning-based approach to densify a sparse LiDAR-based depth map using a guidance RGB image. To achieve this goal the RGB image is at first cleared from most of the camera-LiDAR misalignment artifacts. Afterward, it is over segmented and a plane for each superpixel is approximated. In the case a superpixel is not well represented by a plane, a plane is approximated for a convex hull of the most inlier. Finally, the pinhole camera model is used for the interpolation process and the remaining areas are interpolated. The evaluation of this work is executed using the KITTI depth completion benchmark, which validates the proposed work and shows that it outperforms the state-of-the-art non-deep learning-based methods, in addition to several deep learning-based methods.



### Attention-based Domain Adaptation for Single Stage Detectors
- **Arxiv ID**: http://arxiv.org/abs/2106.07283v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07283v2)
- **Published**: 2021-06-14 10:30:44+00:00
- **Updated**: 2021-08-20 11:51:17+00:00
- **Authors**: Vidit Vidit, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: While domain adaptation has been used to improve the performance of object detectors when the training and test data follow different distributions, previous work has mostly focused on two-stage detectors. This is because their use of region proposals makes it possible to perform local adaptation, which has been shown to significantly improve the adaptation effectiveness. Here, by contrast, we target single-stage architectures, which are better suited to resource-constrained detection than two-stage ones but do not provide region proposals. To nonetheless benefit from the strength of local adaptation, we introduce an attention mechanism that lets us identify the important regions on which adaptation should focus. Our method gradually adapts the features from global, image-level to local, instance-level. Our approach is generic and can be integrated into any single-stage detector. We demonstrate this on standard benchmark datasets by applying it to both SSD and YOLOv5. Furthermore, for equivalent single-stage architectures, our method outperforms the state-of-the-art domain adaptation techniques even though they were designed for specific detectors.



### TimeLens: Event-based Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2106.07286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07286v1)
- **Published**: 2021-06-14 10:33:47+00:00
- **Updated**: 2021-06-14 10:33:47+00:00
- **Authors**: Stepan Tulyakov, Daniel Gehrig, Stamatios Georgoulis, Julius Erbach, Mathias Gehrig, Yuanyou Li, Davide Scaramuzza
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2021
- **Summary**: State-of-the-art frame interpolation methods generate intermediate frames by inferring object motions in the image from consecutive key-frames. In the absence of additional information, first-order approximations, i.e. optical flow, must be used, but this choice restricts the types of motions that can be modeled, leading to errors in highly dynamic scenarios. Event cameras are novel sensors that address this limitation by providing auxiliary visual information in the blind-time between frames. They asynchronously measure per-pixel brightness changes and do this with high temporal resolution and low latency. Event-based frame interpolation methods typically adopt a synthesis-based approach, where predicted frame residuals are directly applied to the key-frames. However, while these approaches can capture non-linear motions they suffer from ghosting and perform poorly in low-texture regions with few events. Thus, synthesis-based and flow-based approaches are complementary. In this work, we introduce Time Lens, a novel indicates equal contribution method that leverages the advantages of both. We extensively evaluate our method on three synthetic and two real benchmarks where we show an up to 5.21 dB improvement in terms of PSNR over state-of-the-art frame-based and event-based methods. Finally, we release a new large-scale dataset in highly dynamic scenarios, aimed at pushing the limits of existing methods.



### Pixel Sampling for Style Preserving Face Pose Editing
- **Arxiv ID**: http://arxiv.org/abs/2106.07310v1
- **DOI**: 10.1109/IJCB48548.2020.9304867
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07310v1)
- **Published**: 2021-06-14 11:29:29+00:00
- **Updated**: 2021-06-14 11:29:29+00:00
- **Authors**: Xiangnan Yin, Di Huang, Hongyu Yang, Zehua Fu, Yunhong Wang, Liming Chen
- **Comment**: None
- **Journal**: IJCB,2020,pp. 1-10
- **Summary**: The existing auto-encoder based face pose editing methods primarily focus on modeling the identity preserving ability during pose synthesis, but are less able to preserve the image style properly, which refers to the color, brightness, saturation, etc. In this paper, we take advantage of the well-known frontal/profile optical illusion and present a novel two-stage approach to solve the aforementioned dilemma, where the task of face pose manipulation is cast into face inpainting. By selectively sampling pixels from the input face and slightly adjust their relative locations with the proposed ``Pixel Attention Sampling" module, the face editing result faithfully keeps the identity information as well as the image style unchanged. By leveraging high-dimensional embedding at the inpainting stage, finer details are generated. Further, with the 3D facial landmarks as guidance, our method is able to manipulate face pose in three degrees of freedom, i.e., yaw, pitch, and roll, resulting in more flexible face pose editing than merely controlling the yaw angle as usually achieved by the current state-of-the-art. Both the qualitative and quantitative evaluations validate the superiority of the proposed approach.



### Analysing Affective Behavior in the second ABAW2 Competition
- **Arxiv ID**: http://arxiv.org/abs/2106.15318v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15318v2)
- **Published**: 2021-06-14 11:30:19+00:00
- **Updated**: 2021-07-03 19:33:22+00:00
- **Authors**: Dimitrios Kollias, Irene Kotsia, Elnar Hajiyev, Stefanos Zafeiriou
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2001.11409
- **Journal**: None
- **Summary**: The Affective Behavior Analysis in-the-wild (ABAW2) 2021 Competition is the second -- following the first very successful ABAW Competition held in conjunction with IEEE FG 2020- Competition that aims at automatically analyzing affect. ABAW2 is split into three Challenges, each one addressing one of the three main behavior tasks of valence-arousal estimation, basic expression classification and action unit detection. All three Challenges are based on a common benchmark database, Aff-Wild2, which is a large scale in-the-wild database and the first one to be annotated for all these three tasks. In this paper, we describe this Competition, to be held in conjunction with ICCV 2021. We present the three Challenges, with the utilized Competition corpora. We outline the evaluation metrics and present the baseline system with its results. More information regarding the Competition is provided in the Competition site: https://ibug.doc.ic.ac.uk/resources/iccv-2021-2nd-abaw.



### Computer Vision Tool for Detection, Mapping and Fault Classification of PV Modules in Aerial IR Videos
- **Arxiv ID**: http://arxiv.org/abs/2106.07314v1
- **DOI**: 10.1002/pip.3448
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07314v1)
- **Published**: 2021-06-14 11:38:13+00:00
- **Updated**: 2021-06-14 11:38:13+00:00
- **Authors**: Lukas Bommes, Tobias Pickel, Claudia Buerhop-Lutz, Jens Hauch, Christoph Brabec, Ian Marius Peters
- **Comment**: None
- **Journal**: None
- **Summary**: Increasing deployment of photovoltaics (PV) plants demands for cheap and fast inspection. A viable tool for this task is thermographic imaging by unmanned aerial vehicles (UAV). In this work, we develop a computer vision tool for the semi-automatic extraction of PV modules from thermographic UAV videos. We use it to curate a dataset containing 4.3 million IR images of 107842 PV modules from thermographic videos of seven different PV plants. To demonstrate its use for automated PV plant inspection, we train a ResNet-50 to classify ten common module anomalies with more than 90 % test accuracy. Experiments show that our tool generalizes well to different PV plants. It successfully extracts PV modules from 512 out of 561 plant rows. Failures are mostly due to an inappropriate UAV trajectory and erroneous module segmentation. Including all manual steps our tool enables inspection of 3.5 MW p to 9 MW p of PV installations per day, potentially scaling to multi-gigawatt plants due to its parallel nature. While we present an effective method for automated PV plant inspection, we are also confident that our approach helps to meet the growing demand for large thermographic datasets for machine learning tasks, such as power prediction or unsupervised defect identification.



### Variational Quanvolutional Neural Networks with enhanced image encoding
- **Arxiv ID**: http://arxiv.org/abs/2106.07327v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07327v2)
- **Published**: 2021-06-14 12:08:30+00:00
- **Updated**: 2021-06-23 12:46:28+00:00
- **Authors**: Denny Mattern, Darya Martyniuk, Henri Willems, Fabian Bergmann, Adrian Paschke
- **Comment**: None
- **Journal**: None
- **Summary**: Image classification is an important task in various machine learning applications. In recent years, a number of classification methods based on quantum machine learning and different quantum image encoding techniques have been proposed. In this paper, we study the effect of three different quantum image encoding approaches on the performance of a convolution-inspired hybrid quantum-classical image classification algorithm called quanvolutional neural network (QNN). We furthermore examine the effect of variational - i.e. trainable - quantum circuits on the classification results. Our experiments indicate that some image encodings are better suited for variational circuits. However, our experiments show as well that there is not one best image encoding, but that the choice of the encoding depends on the specific constraints of the application.



### Deep Transfer Learning for Brain Magnetic Resonance Image Multi-class Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.07333v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07333v2)
- **Published**: 2021-06-14 12:19:27+00:00
- **Updated**: 2021-06-15 16:01:46+00:00
- **Authors**: Yusuf Brima, Mossadek Hossain Kamal Tushar, Upama Kabir, Tariqul Islam
- **Comment**: This work was carried out as a collaboration between the Department
  of Computer Science and Engineering -- the University of Dhaka and the
  National Institute of Neuroscience (NINS), Bangladesh. We created a novel
  neurological discord dataset of 37 disease categories
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) is a principal diagnostic approach used in the field of radiology to create images of the anatomical and physiological structure of patients. MRI is the prevalent medical imaging practice to find abnormalities in soft tissues. Traditionally they are analyzed by a radiologist to detect abnormalities in soft tissues, especially the brain. The process of interpreting a massive volume of patient's MRI is laborious. Hence, the use of Machine Learning methodologies can aid in detecting abnormalities in soft tissues with considerable accuracy. In this research, we have curated a novel dataset and developed a framework that uses Deep Transfer Learning to perform a multi-classification of tumors in the brain MRI images. In this paper, we adopted the Deep Residual Convolutional Neural Network (ResNet50) architecture for the experiments along with discriminative learning techniques to train the model. Using the novel dataset and two publicly available MRI brain datasets, this proposed approach attained a classification accuracy of 86.40% on the curated dataset, 93.80% on the Harvard Whole Brain Atlas dataset, and 97.05% accuracy on the School of Biomedical Engineering dataset. Results of our experiments significantly demonstrate our proposed framework for transfer learning is a potential and effective method for brain tumor multi-classification tasks.



### Weakly-Supervised Photo-realistic Texture Generation for 3D Face Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2106.08148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.08148v1)
- **Published**: 2021-06-14 12:34:35+00:00
- **Updated**: 2021-06-14 12:34:35+00:00
- **Authors**: Xiangnan Yin, Di Huang, Zehua Fu, Yunhong Wang, Liming Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Although much progress has been made recently in 3D face reconstruction, most previous work has been devoted to predicting accurate and fine-grained 3D shapes. In contrast, relatively little work has focused on generating high-fidelity face textures. Compared with the prosperity of photo-realistic 2D face image generation, high-fidelity 3D face texture generation has yet to be studied. In this paper, we proposed a novel UV map generation model that predicts the UV map from a single face image. The model consists of a UV sampler and a UV generator. By selectively sampling the input face image's pixels and adjusting their relative locations, the UV sampler generates an incomplete UV map that could faithfully reconstruct the original face. Missing textures in the incomplete UV map are further full-filled by the UV generator. The training is based on pseudo ground truth blended by the 3DMM texture and the input face texture, thus weakly supervised. To deal with the artifacts in the imperfect pseudo UV map, multiple partial UV map discriminators are leveraged.



### Quality-Aware Network for Face Parsing
- **Arxiv ID**: http://arxiv.org/abs/2106.07368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07368v1)
- **Published**: 2021-06-14 12:40:46+00:00
- **Updated**: 2021-06-14 12:40:46+00:00
- **Authors**: Lu Yang, Qing Song, Xueshi Xin, Zhiwei Liu
- **Comment**: 2nd place in Short-video Face Parsing Track of The 3rd Person in
  Context (PIC) Workshop and Challenge at CVPR 2021
- **Journal**: None
- **Summary**: This is a very short technical report, which introduces the solution of the Team BUPT-CASIA for Short-video Face Parsing Track of The 3rd Person in Context (PIC) Workshop and Challenge at CVPR 2021.   Face parsing has recently attracted increasing interest due to its numerous application potentials. Generally speaking, it has a lot in common with human parsing, such as task setting, data characteristics, number of categories and so on. Therefore, this work applies state-of-the-art human parsing method to face parsing task to explore the similarities and differences between them. Our submission achieves 86.84% score and wins the 2nd place in the challenge.



### Dilated filters for edge detection algorithms
- **Arxiv ID**: http://arxiv.org/abs/2106.07395v1
- **DOI**: 10.3390/app112210716
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07395v1)
- **Published**: 2021-06-14 12:52:17+00:00
- **Updated**: 2021-06-14 12:52:17+00:00
- **Authors**: Ciprian Orhei, Victor Bogdan, Cosmin Bonchis
- **Comment**: None
- **Journal**: None
- **Summary**: Edges are a basic and fundamental feature in image processing, that are used directly or indirectly in huge amount of applications. Inspired by the expansion of image resolution and processing power dilated convolution techniques appeared. Dilated convolution have impressive results in machine learning, we discuss here the idea of dilating the standard filters which are used in edge detection algorithms. In this work we try to put together all our previous and current results by using instead of the classical convolution filters a dilated one. We compare the results of the edge detection algorithms using the proposed dilation filters with original filters or custom variants. Experimental results confirm our statement that dilation of filters have positive impact for edge detection algorithms form simple to rather complex algorithms.



### 3rd Place Solution for Short-video Face Parsing Challenge
- **Arxiv ID**: http://arxiv.org/abs/2106.07409v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07409v3)
- **Published**: 2021-06-14 13:22:19+00:00
- **Updated**: 2021-07-14 11:06:17+00:00
- **Authors**: Xiao Liu, Xiaofei Si, Jiangtao Xie
- **Comment**: None
- **Journal**: None
- **Summary**: This is a short technical report introducing the solution of Team Rat for Short-video Parsing Face Parsing Track of The 3rd Person in Context (PIC) Workshop and Challenge at CVPR 2021.   In this report, we propose an Edge-Aware Network (EANet) that uses edge information to refine the segmentation edge. To further obtain the finer edge results, we introduce edge attention loss that only compute cross entropy on the edges, it can effectively reduce the classification error around edge and get more smooth boundary. Benefiting from the edge information and edge attention loss, the proposed EANet achieves 86.16\% accuracy in the Short-video Face Parsing track of the 3rd Person in Context (PIC) Workshop and Challenge, ranked the third place.



### Partial success in closing the gap between human and machine vision
- **Arxiv ID**: http://arxiv.org/abs/2106.07411v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2106.07411v2)
- **Published**: 2021-06-14 13:23:35+00:00
- **Updated**: 2021-10-25 09:44:25+00:00
- **Authors**: Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer, Matthias Bethge, Felix A. Wichmann, Wieland Brendel
- **Comment**: NeurIPS 2021 Oral, camera ready version. A preliminary version of
  this work was presented as Oral at the 2020 NeurIPS workshop on "Shared
  Visual Representations in Human & Machine Intelligence" (arXiv:2010.08377)
- **Journal**: None
- **Summary**: A few years ago, the first CNN surpassed human performance on ImageNet. However, it soon became clear that machines lack robustness on more challenging test cases, a major obstacle towards deploying machines "in the wild" and towards obtaining better computational models of human visual perception. Here we ask: Are we making progress in closing the gap between human and machine vision? To answer this question, we tested human observers on a broad range of out-of-distribution (OOD) datasets, recording 85,120 psychophysical trials across 90 participants. We then investigated a range of promising machine learning developments that crucially deviate from standard supervised CNNs along three axes: objective function (self-supervised, adversarially trained, CLIP language-image training), architecture (e.g. vision transformers), and dataset size (ranging from 1M to 1B).   Our findings are threefold. (1.) The longstanding distortion robustness gap between humans and CNNs is closing, with the best models now exceeding human feedforward performance on most of the investigated OOD datasets. (2.) There is still a substantial image-level consistency gap, meaning that humans make different errors than models. In contrast, most models systematically agree in their categorisation errors, even substantially different ones like contrastive self-supervised vs. standard supervised models. (3.) In many cases, human-to-model consistency improves when training dataset size is increased by one to three orders of magnitude. Our results give reason for cautious optimism: While there is still much room for improvement, the behavioural difference between human and machine vision is narrowing. In order to measure future progress, 17 OOD datasets with image-level human behavioural data and evaluation code are provided as a toolbox and benchmark at: https://github.com/bethgelab/model-vs-human/



### Automatically eliminating seam lines with Poisson editing in complex relative radiometric normalization mosaicking scenarios
- **Arxiv ID**: http://arxiv.org/abs/2106.07441v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07441v1)
- **Published**: 2021-06-14 14:06:20+00:00
- **Updated**: 2021-06-14 14:06:20+00:00
- **Authors**: Shiqi Liu, Jie Lian, Xuchen Zhan, Cong Liu, Yuze Tian, Hongwei Duan
- **Comment**: None
- **Journal**: None
- **Summary**: Relative radiometric normalization (RRN) mosaicking among multiple remote sensing images is crucial for the downstream tasks, including map-making, image recognition, semantic segmentation, and change detection. However, there are often seam lines on the mosaic boundary and radiometric contrast left, especially in complex scenarios, making the appearance of mosaic images unsightly and reducing the accuracy of the latter classification/recognition algorithms. This paper renders a novel automatical approach to eliminate seam lines in complex RRN mosaicking scenarios. It utilizes the histogram matching on the overlap area to alleviate radiometric contrast, Poisson editing to remove the seam lines, and merging procedure to determine the normalization transfer order. Our method can handle the mosaicking seam lines with arbitrary shapes and images with extreme topological relationships (with a small intersection area). These conditions make the main feathering or blending methods, e.g., linear weighted blending and Laplacian pyramid blending, unavailable. In the experiment, our approach visually surpasses the automatic methods without Poisson editing and the manual blurring and feathering method using GIMP software.



### PopSkipJump: Decision-Based Attack for Probabilistic Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2106.07445v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2106.07445v1)
- **Published**: 2021-06-14 14:13:12+00:00
- **Updated**: 2021-06-14 14:13:12+00:00
- **Authors**: Carl-Johann Simon-Gabriel, Noman Ahmed Sheikh, Andreas Krause
- **Comment**: ICML'21. Code available at https://github.com/cjsg/PopSkipJump . 9
  pages & 7 figures in main part, 14 pages & 10 figures in appendix
- **Journal**: None
- **Summary**: Most current classifiers are vulnerable to adversarial examples, small input perturbations that change the classification output. Many existing attack algorithms cover various settings, from white-box to black-box classifiers, but typically assume that the answers are deterministic and often fail when they are not. We therefore propose a new adversarial decision-based attack specifically designed for classifiers with probabilistic outputs. It is based on the HopSkipJump attack by Chen et al. (2019, arXiv:1904.02144v5 ), a strong and query efficient decision-based attack originally designed for deterministic classifiers. Our P(robabilisticH)opSkipJump attack adapts its amount of queries to maintain HopSkipJump's original output quality across various noise levels, while converging to its query efficiency as the noise level decreases. We test our attack on various noise models, including state-of-the-art off-the-shelf randomized defenses, and show that they offer almost no extra robustness to decision-based attacks. Code is available at https://github.com/cjsg/PopSkipJump .



### A Novel mapping for visual to auditory sensory substitution
- **Arxiv ID**: http://arxiv.org/abs/2106.07448v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.AI, cs.CV, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.07448v1)
- **Published**: 2021-06-14 14:14:50+00:00
- **Updated**: 2021-06-14 14:14:50+00:00
- **Authors**: Ezsan Mehrbani, Sezedeh Fatemeh Mirhoseini, Noushin Riahi
- **Comment**: None
- **Journal**: None
- **Summary**: visual information can be converted into audio stream via sensory substitution devices in order to give visually impaired people the chance of perception of their surrounding easily and simultaneous to performing everyday tasks. In this study, visual environmental features namely, coordinate, type of objects and their size are assigned to audio features related to music tones such as frequency, time duration and note permutations. Results demonstrated that this new method has more training time efficiency in comparison with our previous method named VBTones which sinusoidal tones were applied. Moreover, results in blind object recognition for real objects was achieved 88.05 on average.



### Comparing vector fields across surfaces: interest for characterizing the orientations of cortical folds
- **Arxiv ID**: http://arxiv.org/abs/2106.07470v1
- **DOI**: None
- **Categories**: **cs.CV**, math.DG, physics.bio-ph, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2106.07470v1)
- **Published**: 2021-06-14 14:56:44+00:00
- **Updated**: 2021-06-14 14:56:44+00:00
- **Authors**: Amine Bohi, Guillaume Auzias, Julien Lefèvre
- **Comment**: None
- **Journal**: None
- **Summary**: Vectors fields defined on surfaces constitute relevant and useful representations but are rarely used. One reason might be that comparing vector fields across two surfaces of the same genus is not trivial: it requires to transport the vector fields from the original surfaces onto a common domain. In this paper, we propose a framework to achieve this task by mapping the vector fields onto a common space, using some notions of differential geometry. The proposed framework enables the computation of statistics on vector fields. We demonstrate its interest in practice with an application on real data with a quantitative assessment of the reproducibility of curvature directions that describe the complex geometry of cortical folding patterns. The proposed framework is general and can be applied to different types of vector fields and surfaces, allowing for a large number of high potential applications in medical imaging.



### S$^2$-MLP: Spatial-Shift MLP Architecture for Vision
- **Arxiv ID**: http://arxiv.org/abs/2106.07477v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07477v2)
- **Published**: 2021-06-14 15:05:11+00:00
- **Updated**: 2021-06-23 17:58:04+00:00
- **Authors**: Tan Yu, Xu Li, Yunfeng Cai, Mingming Sun, Ping Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, visual Transformer (ViT) and its following works abandon the convolution and exploit the self-attention operation, attaining a comparable or even higher accuracy than CNNs. More recently, MLP-Mixer abandons both the convolution and the self-attention operation, proposing an architecture containing only MLP layers. To achieve cross-patch communications, it devises an additional token-mixing MLP besides the channel-mixing MLP. It achieves promising results when training on an extremely large-scale dataset. But it cannot achieve as outstanding performance as its CNN and ViT counterparts when training on medium-scale datasets such as ImageNet1K and ImageNet21K. The performance drop of MLP-Mixer motivates us to rethink the token-mixing MLP. We discover that the token-mixing MLP is a variant of the depthwise convolution with a global reception field and spatial-specific configuration. But the global reception field and the spatial-specific property make token-mixing MLP prone to over-fitting. In this paper, we propose a novel pure MLP architecture, spatial-shift MLP (S$^2$-MLP). Different from MLP-Mixer, our S$^2$-MLP only contains channel-mixing MLP. We utilize a spatial-shift operation for communications between patches. It has a local reception field and is spatial-agnostic. It is parameter-free and efficient for computation. The proposed S$^2$-MLP attains higher recognition accuracy than MLP-Mixer when training on ImageNet-1K dataset. Meanwhile, S$^2$-MLP accomplishes as excellent performance as ViT on ImageNet-1K dataset with considerably simpler architecture and fewer FLOPs and parameters.



### User-Guided Personalized Image Aesthetic Assessment based on Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.07488v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, 94, H.5; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2106.07488v1)
- **Published**: 2021-06-14 15:19:48+00:00
- **Updated**: 2021-06-14 15:19:48+00:00
- **Authors**: Pei Lv, Jianqi Fan, Xixi Nie, Weiming Dong, Xiaoheng Jiang, Bing Zhou, Mingliang Xu, Changsheng Xu
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: Personalized image aesthetic assessment (PIAA) has recently become a hot topic due to its usefulness in a wide variety of applications such as photography, film and television, e-commerce, fashion design and so on. This task is more seriously affected by subjective factors and samples provided by users. In order to acquire precise personalized aesthetic distribution by small amount of samples, we propose a novel user-guided personalized image aesthetic assessment framework. This framework leverages user interactions to retouch and rank images for aesthetic assessment based on deep reinforcement learning (DRL), and generates personalized aesthetic distribution that is more in line with the aesthetic preferences of different users. It mainly consists of two stages. In the first stage, personalized aesthetic ranking is generated by interactive image enhancement and manual ranking, meanwhile two policy networks will be trained. The images will be pushed to the user for manual retouching and simultaneously to the enhancement policy network. The enhancement network utilizes the manual retouching results as the optimization goals of DRL. After that, the ranking process performs the similar operations like the retouching mentioned before. These two networks will be trained iteratively and alternatively to help to complete the final personalized aesthetic assessment automatically. In the second stage, these modified images are labeled with aesthetic attributes by one style-specific classifier, and then the personalized aesthetic distribution is generated based on the multiple aesthetic attributes of these images, which conforms to the aesthetic preference of users better.



### EuroCrops: A Pan-European Dataset for Time Series Crop Type Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.08151v1
- **DOI**: 10.2760/125905
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.08151v1)
- **Published**: 2021-06-14 15:21:50+00:00
- **Updated**: 2021-06-14 15:21:50+00:00
- **Authors**: Maja Schneider, Amelie Broszeit, Marco Körner
- **Comment**: 4 pages, website: https://www.eurocrops.tum.de/
- **Journal**: Proc. of the 2021 conference on Big Data from Space (BiDS21),
  2021, 5, 125-128
- **Summary**: We present EuroCrops, a dataset based on self-declared field annotations for training and evaluating methods for crop type classification and mapping, together with its process of acquisition and harmonisation. By this, we aim to enrich the research efforts and discussion for data-driven land cover classification via Earth observation and remote sensing. Additionally, through inclusion of self-declarations gathered in the scope of subsidy control from all countries of the European Union (EU), this dataset highlights the difficulties and pitfalls one comes across when operating on a transnational level. We, therefore, also introduce a new taxonomy scheme, HCAT-ID, that aspires to capture all the aspects of reference data originating from administrative and agency databases. To address researchers from both the remote sensing and the computer vision and machine learning communities, we publish the dataset in different formats and processing levels.



### MIA-COV19D: COVID-19 Detection through 3-D Chest CT Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2106.07524v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07524v2)
- **Published**: 2021-06-14 15:48:14+00:00
- **Updated**: 2021-06-21 16:00:36+00:00
- **Authors**: Dimitrios Kollias, Anastasios Arsenos, Levon Soukissian, Stefanos Kollias
- **Comment**: None
- **Journal**: None
- **Summary**: Early and reliable COVID-19 diagnosis based on chest 3-D CT scans can assist medical specialists in vital circumstances. Deep learning methodologies constitute a main approach for chest CT scan analysis and disease prediction. However, large annotated databases are necessary for developing deep learning models that are able to provide COVID-19 diagnosis across various medical environments in different countries. Due to privacy issues, publicly available COVID-19 CT datasets are highly difficult to obtain, which hinders the research and development of AI-enabled diagnosis methods of COVID-19 based on CT scans. In this paper we present the COV19-CT-DB database which is annotated for COVID-19, consisting of about 5,000 3-D CT scans, We have split the database in training, validation and test datasets. The former two datasets can be used for training and validation of machine learning models, while the latter will be used for evaluation of the developed models. We also present a deep learning approach, based on a CNN-RNN network and report its performance on the COVID19-CT-DB database.



### PolarStream: Streaming Lidar Object Detection and Segmentation with Polar Pillars
- **Arxiv ID**: http://arxiv.org/abs/2106.07545v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.07545v2)
- **Published**: 2021-06-14 16:11:28+00:00
- **Updated**: 2022-03-24 01:33:38+00:00
- **Authors**: Qi Chen, Sourabh Vora, Oscar Beijbom
- **Comment**: NeurIPS 2021; code and pretrained models available at
  https://github.com/motional/polarstream
- **Journal**: None
- **Summary**: Recent works recognized lidars as an inherently streaming data source and showed that the end-to-end latency of lidar perception models can be reduced significantly by operating on wedge-shaped point cloud sectors rather then the full point cloud. However, due to use of cartesian coordinate systems these methods represent the sectors as rectangular regions, wasting memory and compute. In this work we propose using a polar coordinate system and make two key improvements on this design. First, we increase the spatial context by using multi-scale padding from neighboring sectors: preceding sector from the current scan and/or the following sector from the past scan. Second, we improve the core polar convolutional architecture by introducing feature undistortion and range stratified convolutions. Experimental results on the nuScenes dataset show significant improvements over other streaming based methods. We also achieve comparable results to existing non-streaming methods but with lower latencies. The code and pretrained models are available at \url{https://github.com/motional/polarstream}.



### Full interpretable machine learning in 2D with inline coordinates
- **Arxiv ID**: http://arxiv.org/abs/2106.07568v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.07568v2)
- **Published**: 2021-06-14 16:21:06+00:00
- **Updated**: 2021-07-03 21:19:21+00:00
- **Authors**: Boris Kovalerchuk, Hoang Phan
- **Comment**: 8 pages, 20 figures
- **Journal**: None
- **Summary**: This paper proposed a new methodology for machine learning in 2-dimensional space (2-D ML) in inline coordinates. It is a full machine learning approach that does not require to deal with n-dimensional data in n-dimensional space. It allows discovering n-D patterns in 2-D space without loss of n-D information using graph representations of n-D data in 2-D. Specifically, it can be done with the inline based coordinates in different modifications, including static and dynamic ones. The classification and regression algorithms based on these inline coordinates were introduced. A successful case study based on a benchmark data demonstrated the feasibility of the approach. This approach helps to consolidate further a whole new area of full 2-D machine learning as a promising ML methodology. It has advantages of abilities to involve actively the end-users into the discovering of models and their justification. Another advantage is providing interpretable ML models.



### Non Gaussian Denoising Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2106.07582v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2106.07582v1)
- **Published**: 2021-06-14 16:42:43+00:00
- **Updated**: 2021-06-14 16:42:43+00:00
- **Authors**: Eliya Nachmani, Robin San Roman, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: Generative diffusion processes are an emerging and effective tool for image and speech generation. In the existing methods, the underline noise distribution of the diffusion process is Gaussian noise. However, fitting distributions with more degrees of freedom, could help the performance of such generative models. In this work, we investigate other types of noise distribution for the diffusion process. Specifically, we show that noise from Gamma distribution provides improved results for image and speech generation. Moreover, we show that using a mixture of Gaussian noise variables in the diffusion process improves the performance over a diffusion process that is based on a single distribution. Our approach preserves the ability to efficiently sample state in the training diffusion process while using Gamma noise and a mixture of noise.



### Recursive Refinement Network for Deformable Lung Registration between Exhale and Inhale CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2106.07608v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07608v1)
- **Published**: 2021-06-14 17:14:17+00:00
- **Updated**: 2021-06-14 17:14:17+00:00
- **Authors**: Xinzi He, Jia Guo, Xuzhe Zhang, Hanwen Bi, Sarah Gerard, David Kaczka, Amin Motahari, Eric Hoffman, Joseph Reinhardt, R. Graham Barr, Elsa Angelini, Andrew Laine
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised learning-based medical image registration approaches have witnessed rapid development in recent years. We propose to revisit a commonly ignored while simple and well-established principle: recursive refinement of deformation vector fields across scales. We introduce a recursive refinement network (RRN) for unsupervised medical image registration, to extract multi-scale features, construct normalized local cost correlation volume and recursively refine volumetric deformation vector fields. RRN achieves state of the art performance for 3D registration of expiratory-inspiratory pairs of CT lung scans. On DirLab COPDGene dataset, RRN returns an average Target Registration Error (TRE) of 0.83 mm, which corresponds to a 13% error reduction from the best result presented in the leaderboard. In addition to comparison with conventional methods, RRN leads to 89% error reduction compared to deep-learning-based peer approaches.



### Magic Layouts: Structural Prior for Component Detection in User Interface Designs
- **Arxiv ID**: http://arxiv.org/abs/2106.07615v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07615v1)
- **Published**: 2021-06-14 17:20:36+00:00
- **Updated**: 2021-06-14 17:20:36+00:00
- **Authors**: Dipu Manandhar, Hailin Jin, John Collomosse
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: We present Magic Layouts; a method for parsing screenshots or hand-drawn sketches of user interface (UI) layouts. Our core contribution is to extend existing detectors to exploit a learned structural prior for UI designs, enabling robust detection of UI components; buttons, text boxes and similar. Specifically we learn a prior over mobile UI layouts, encoding common spatial co-occurrence relationships between different UI components. Conditioning region proposals using this prior leads to performance gains on UI layout parsing for both hand-drawn UIs and app screenshots, which we demonstrate within the context an interactive application for rapidly acquiring digital prototypes of user experience (UX) designs.



### Delving Deep into the Generalization of Vision Transformers under Distribution Shifts
- **Arxiv ID**: http://arxiv.org/abs/2106.07617v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07617v4)
- **Published**: 2021-06-14 17:21:41+00:00
- **Updated**: 2022-03-08 03:09:40+00:00
- **Authors**: Chongzhi Zhang, Mingyuan Zhang, Shanghang Zhang, Daisheng Jin, Qiang Zhou, Zhongang Cai, Haiyu Zhao, Xianglong Liu, Ziwei Liu
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Vision Transformers (ViTs) have achieved impressive performance on various vision tasks, yet their generalization under distribution shifts (DS) is rarely understood. In this work, we comprehensively study the out-of-distribution (OOD) generalization of ViTs. For systematic investigation, we first present a taxonomy of DS. We then perform extensive evaluations of ViT variants under different DS and compare their generalization with Convolutional Neural Network (CNN) models. Important observations are obtained: 1) ViTs learn weaker biases on backgrounds and textures, while they are equipped with stronger inductive biases towards shapes and structures, which is more consistent with human cognitive traits. Therefore, ViTs generalize better than CNNs under DS. With the same or less amount of parameters, ViTs are ahead of corresponding CNNs by more than 5% in top-1 accuracy under most types of DS. 2) As the model scale increases, ViTs strengthen these biases and thus gradually narrow the in-distribution and OOD performance gap. To further improve the generalization of ViTs, we design the Generalization-Enhanced ViTs (GE-ViTs) from the perspectives of adversarial learning, information theory, and self-supervised learning. By comprehensively investigating these GE-ViTs and comparing with their corresponding CNN models, we observe: 1) For the enhanced model, larger ViTs still benefit more for the OOD generalization. 2) GE-ViTs are more sensitive to the hyper-parameters than their corresponding CNN models. We design a smoother learning strategy to achieve a stable training process and obtain performance improvements on OOD data by 4% from vanilla ViTs. We hope our comprehensive study could shed light on the design of more generalizable learning architectures.



### Toward Automatic Interpretation of 3D Plots
- **Arxiv ID**: http://arxiv.org/abs/2106.07627v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07627v1)
- **Published**: 2021-06-14 17:32:53+00:00
- **Updated**: 2021-06-14 17:32:53+00:00
- **Authors**: Laura E. Brandt, William T. Freeman
- **Comment**: 16 pages, 12 figures, accepted to the 16th International Conference
  on Document Analysis and Recognition (ICDAR21)
- **Journal**: None
- **Summary**: This paper explores the challenge of teaching a machine how to reverse-engineer the grid-marked surfaces used to represent data in 3D surface plots of two-variable functions. These are common in scientific and economic publications; and humans can often interpret them with ease, quickly gleaning general shape and curvature information from the simple collection of curves. While machines have no such visual intuition, they do have the potential to accurately extract the more detailed quantitative data that guided the surface's construction. We approach this problem by synthesizing a new dataset of 3D grid-marked surfaces (SurfaceGrid) and training a deep neural net to estimate their shape. Our algorithm successfully recovers shape information from synthetic 3D surface plots that have had axes and shading information removed, been rendered with a variety of grid types, and viewed from a range of viewpoints.



### Improved Transformer for High-Resolution GANs
- **Arxiv ID**: http://arxiv.org/abs/2106.07631v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07631v3)
- **Published**: 2021-06-14 17:39:49+00:00
- **Updated**: 2021-12-24 01:11:20+00:00
- **Authors**: Long Zhao, Zizhao Zhang, Ting Chen, Dimitris N. Metaxas, Han Zhang
- **Comment**: Accepted to NeurIPS 2021 (with updated ImageNet results). Code is
  available at https://github.com/google-research/hit-gan
- **Journal**: None
- **Summary**: Attention-based models, exemplified by the Transformer, can effectively model long range dependency, but suffer from the quadratic complexity of self-attention operation, making them difficult to be adopted for high-resolution image generation based on Generative Adversarial Networks (GANs). In this paper, we introduce two key ingredients to Transformer to address this challenge. First, in low-resolution stages of the generative process, standard global self-attention is replaced with the proposed multi-axis blocked self-attention which allows efficient mixing of local and global attention. Second, in high-resolution stages, we drop self-attention while only keeping multi-layer perceptrons reminiscent of the implicit neural function. To further improve the performance, we introduce an additional self-modulation component based on cross-attention. The resulting model, denoted as HiT, has a nearly linear computational complexity with respect to the image size and thus directly scales to synthesizing high definition images. We show in the experiments that the proposed HiT achieves state-of-the-art FID scores of 30.83 and 2.95 on unconditional ImageNet $128 \times 128$ and FFHQ $256 \times 256$, respectively, with a reasonable throughput. We believe the proposed HiT is an important milestone for generators in GANs which are completely free of convolutions. Our code is made publicly available at https://github.com/google-research/hit-gan



### Unsupervised Learning of Visual 3D Keypoints for Control
- **Arxiv ID**: http://arxiv.org/abs/2106.07643v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.07643v1)
- **Published**: 2021-06-14 17:59:59+00:00
- **Updated**: 2021-06-14 17:59:59+00:00
- **Authors**: Boyuan Chen, Pieter Abbeel, Deepak Pathak
- **Comment**: Accepted at ICML 2021. Videos and code at
  https://buoyancy99.github.io/unsup-3d-keypoints/
- **Journal**: None
- **Summary**: Learning sensorimotor control policies from high-dimensional images crucially relies on the quality of the underlying visual representations. Prior works show that structured latent space such as visual keypoints often outperforms unstructured representations for robotic control. However, most of these representations, whether structured or unstructured are learned in a 2D space even though the control tasks are usually performed in a 3D environment. In this work, we propose a framework to learn such a 3D geometric structure directly from images in an end-to-end unsupervised manner. The input images are embedded into latent 3D keypoints via a differentiable encoder which is trained to optimize both a multi-view consistency loss and downstream task objective. These discovered 3D keypoints tend to meaningfully capture robot joints as well as object movements in a consistent manner across both time and 3D space. The proposed approach outperforms prior state-of-art methods across a variety of reinforcement learning benchmarks. Code and videos at https://buoyancy99.github.io/unsup-3d-keypoints/



### Face Age Progression With Attribute Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2106.07696v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.07696v1)
- **Published**: 2021-06-14 18:26:48+00:00
- **Updated**: 2021-06-14 18:26:48+00:00
- **Authors**: Sinzith Tatikonda, Athira Nambiar, Anurag Mittal
- **Comment**: -
- **Journal**: None
- **Summary**: Face is one of the predominant means of person recognition. In the process of ageing, human face is prone to many factors such as time, attributes, weather and other subject specific variations. The impact of these factors were not well studied in the literature of face aging. In this paper, we propose a novel holistic model in this regard viz., ``Face Age progression With Attribute Manipulation (FAWAM)", i.e. generating face images at different ages while simultaneously varying attributes and other subject specific characteristics. We address the task in a bottom-up manner, as two submodules i.e. face age progression and face attribute manipulation. For face aging, we use an attribute-conscious face aging model with a pyramidal generative adversarial network that can model age-specific facial changes while maintaining intrinsic subject specific characteristics. For facial attribute manipulation, the age processed facial image is manipulated with desired attributes while preserving other details unchanged, leveraging an attribute generative adversarial network architecture. We conduct extensive analysis in standard large scale datasets and our model achieves significant performance both quantitatively and qualitatively.



### CathAI: Fully Automated Interpretation of Coronary Angiograms Using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.07708v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, eess.IV, I.4.9; I.2.10; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2106.07708v1)
- **Published**: 2021-06-14 18:58:09+00:00
- **Updated**: 2021-06-14 18:58:09+00:00
- **Authors**: Robert Avram, Jeffrey E. Olgin, Alvin Wan, Zeeshan Ahmed, Louis Verreault-Julien, Sean Abreau, Derek Wan, Joseph E. Gonzalez, Derek Y. So, Krishan Soni, Geoffrey H. Tison
- **Comment**: 62 pages, 3 main figures, 2 main tables
- **Journal**: None
- **Summary**: Coronary heart disease (CHD) is the leading cause of adult death in the United States and worldwide, and for which the coronary angiography procedure is the primary gateway for diagnosis and clinical management decisions. The standard-of-care for interpretation of coronary angiograms depends upon ad-hoc visual assessment by the physician operator. However, ad-hoc visual interpretation of angiograms is poorly reproducible, highly variable and bias prone. Here we show for the first time that fully-automated angiogram interpretation to estimate coronary artery stenosis is possible using a sequence of deep neural network algorithms. The algorithmic pipeline we developed--called CathAI--achieves state-of-the art performance across the sequence of tasks required to accomplish automated interpretation of unselected, real-world angiograms. CathAI (Algorithms 1-2) demonstrated positive predictive value, sensitivity and F1 score of >=90% to identify the projection angle overall and >=93% for left or right coronary artery angiogram detection, the primary anatomic structures of interest. To predict obstructive coronary artery stenosis (>=70% stenosis), CathAI (Algorithm 4) exhibited an area under the receiver operating characteristic curve (AUC) of 0.862 (95% CI: 0.843-0.880). When externally validated in a healthcare system in another country, CathAI AUC was 0.869 (95% CI: 0.830-0.907) to predict obstructive coronary artery stenosis. Our results demonstrate that multiple purpose-built neural networks can function in sequence to accomplish the complex series of tasks required for automated analysis of real-world angiograms. Deployment of CathAI may serve to increase standardization and reproducibility in coronary stenosis assessment, while providing a robust foundation to accomplish future tasks for algorithmic angiographic interpretation.



### Learning Deep Morphological Networks with Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2106.07714v2
- **DOI**: 10.1016/j.patcog.2022.108893
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07714v2)
- **Published**: 2021-06-14 19:19:48+00:00
- **Updated**: 2022-07-27 18:06:22+00:00
- **Authors**: Yufei Hu, Nacim Belkhir, Jesus Angulo, Angela Yao, Gianni Franchi
- **Comment**: 18 pages
- **Journal**: Pattern Recognition, 108893 (2022)
- **Summary**: Deep Neural Networks (DNNs) are generated by sequentially performing linear and non-linear processes. Using a combination of linear and non-linear procedures is critical for generating a sufficiently deep feature space. The majority of non-linear operators are derivations of activation functions or pooling functions. Mathematical morphology is a branch of mathematics that provides non-linear operators for a variety of image processing problems. We investigate the utility of integrating these operations in an end-to-end deep learning framework in this paper. DNNs are designed to acquire a realistic representation for a particular job. Morphological operators give topological descriptors that convey salient information about the shapes of objects depicted in images. We propose a method based on meta-learning to incorporate morphological operators into DNNs. The learned architecture demonstrates how our novel morphological operations significantly increase DNN performance on various tasks, including picture classification and edge detection.



### Learning Audio-Visual Dereverberation
- **Arxiv ID**: http://arxiv.org/abs/2106.07732v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2106.07732v2)
- **Published**: 2021-06-14 20:01:24+00:00
- **Updated**: 2023-03-13 21:34:57+00:00
- **Authors**: Changan Chen, Wei Sun, David Harwath, Kristen Grauman
- **Comment**: Accepted at ICASSP 2023. This is the longer version of the five-page
  camera-ready paper. Project page:
  https://vision.cs.utexas.edu/projects/learning-audio-visual-dereverberation
- **Journal**: None
- **Summary**: Reverberation not only degrades the quality of speech for human perception, but also severely impacts the accuracy of automatic speech recognition. Prior work attempts to remove reverberation based on the audio modality only. Our idea is to learn to dereverberate speech from audio-visual observations. The visual environment surrounding a human speaker reveals important cues about the room geometry, materials, and speaker location, all of which influence the precise reverberation effects. We introduce Visually-Informed Dereverberation of Audio (VIDA), an end-to-end approach that learns to remove reverberation based on both the observed monaural sound and visual scene. In support of this new task, we develop a large-scale dataset SoundSpaces-Speech that uses realistic acoustic renderings of speech in real-world 3D scans of homes offering a variety of room acoustics. Demonstrating our approach on both simulated and real imagery for speech enhancement, speech recognition, and speaker identification, we show it achieves state-of-the-art performance and substantially improves over audio-only methods.



### Potato Crop Stress Identification in Aerial Images using Deep Learning-based Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.07770v3
- **DOI**: 10.1002/agj2.20841
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07770v3)
- **Published**: 2021-06-14 21:57:40+00:00
- **Updated**: 2021-10-28 21:32:37+00:00
- **Authors**: Sujata Butte, Aleksandar Vakanski, Kasia Duellman, Haotian Wang, Amin Mirkouei
- **Comment**: 15 pages, 4 figures
- **Journal**: Agronomy Journal, vol. 113, no. 5, pp. 3991-4002, 2021
- **Summary**: Recent research on the application of remote sensing and deep learning-based analysis in precision agriculture demonstrated a potential for improved crop management and reduced environmental impacts of agricultural production. Despite the promising results, the practical relevance of these technologies for field deployment requires novel algorithms that are customized for analysis of agricultural images and robust to implementation on natural field imagery. The paper presents an approach for analyzing aerial images of a potato (Solanum tuberosum L.) crop using deep neural networks. The main objective is to demonstrate automated spatial recognition of healthy vs. stressed crop at a plant level. Specifically, we examine premature plant senescence resulting in drought stress on Russet Burbank potato plants. We propose a novel deep learning (DL) model for detecting crop stress, named Retina-UNet-Ag. The proposed architecture is a variant of Retina-UNet and includes connections from low-level semantic representation maps to the feature pyramid network. The paper also introduces a dataset of aerial field images acquired with a Parrot Sequoia camera. The dataset includes manually annotated bounding boxes of healthy and stressed plant regions. Experimental validation demonstrated the ability for distinguishing healthy and stressed plants in field images, achieving an average dice score coefficient (DSC) of 0.74. A comparison to related state-of-the-art DL models for object detection revealed that the presented approach is effective for this task. The proposed method is conducive toward the assessment and recognition of potato crop stress in aerial field images collected under natural conditions.



### Flow Guided Transformable Bottleneck Networks for Motion Retargeting
- **Arxiv ID**: http://arxiv.org/abs/2106.07771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07771v1)
- **Published**: 2021-06-14 21:58:30+00:00
- **Updated**: 2021-06-14 21:58:30+00:00
- **Authors**: Jian Ren, Menglei Chai, Oliver J. Woodford, Kyle Olszewski, Sergey Tulyakov
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Human motion retargeting aims to transfer the motion of one person in a "driving" video or set of images to another person. Existing efforts leverage a long training video from each target person to train a subject-specific motion transfer model. However, the scalability of such methods is limited, as each model can only generate videos for the given target subject, and such training videos are labor-intensive to acquire and process. Few-shot motion transfer techniques, which only require one or a few images from a target, have recently drawn considerable attention. Methods addressing this task generally use either 2D or explicit 3D representations to transfer motion, and in doing so, sacrifice either accurate geometric modeling or the flexibility of an end-to-end learned representation. Inspired by the Transformable Bottleneck Network, which renders novel views and manipulations of rigid objects, we propose an approach based on an implicit volumetric representation of the image content, which can then be spatially manipulated using volumetric flow fields. We address the challenging question of how to aggregate information across different body poses, learning flow fields that allow for combining content from the appropriate regions of input images of highly non-rigid human subjects performing complex motions into a single implicit volumetric representation. This allows us to learn our 3D representation solely from videos of moving people. Armed with both 3D object understanding and end-to-end learned rendering, this categorically novel representation delivers state-of-the-art image generation quality, as shown by our quantitative and qualitative evaluations.



### DFM: A Performance Baseline for Deep Feature Matching
- **Arxiv ID**: http://arxiv.org/abs/2106.07791v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.0; I.4.7
- **Links**: [PDF](http://arxiv.org/pdf/2106.07791v1)
- **Published**: 2021-06-14 22:55:06+00:00
- **Updated**: 2021-06-14 22:55:06+00:00
- **Authors**: Ufuk Efe, Kutalmis Gokalp Ince, A. Aydin Alatan
- **Comment**: CVPR 2021 Image Matching Workshop Camera Ready Version
- **Journal**: None
- **Summary**: A novel image matching method is proposed that utilizes learned features extracted by an off-the-shelf deep neural network to obtain a promising performance. The proposed method uses pre-trained VGG architecture as a feature extractor and does not require any additional training specific to improve matching. Inspired by well-established concepts in the psychology area, such as the Mental Rotation paradigm, an initial warping is performed as a result of a preliminary geometric transformation estimate. These estimates are simply based on dense matching of nearest neighbors at the terminal layer of VGG network outputs of the images to be matched. After this initial alignment, the same approach is repeated again between reference and aligned images in a hierarchical manner to reach a good localization and matching performance. Our algorithm achieves 0.57 and 0.80 overall scores in terms of Mean Matching Accuracy (MMA) for 1 pixel and 2 pixels thresholds respectively on Hpatches dataset, which indicates a better performance than the state-of-the-art.



### Highdicom: A Python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology
- **Arxiv ID**: http://arxiv.org/abs/2106.07806v3
- **DOI**: 10.1007/s10278-022-00683-y
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07806v3)
- **Published**: 2021-06-14 23:42:48+00:00
- **Updated**: 2022-05-08 16:41:22+00:00
- **Authors**: Christopher P. Bridge, Chris Gorman, Steven Pieper, Sean W. Doyle, Jochen K. Lennerz, Jayashree Kalpathy-Cramer, David A. Clunie, Andriy Y. Fedorov, Markus D. Herrmann
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning is revolutionizing image-based diagnostics in pathology and radiology. ML models have shown promising results in research settings, but their lack of interoperability has been a major barrier for clinical integration and evaluation. The DICOM a standard specifies Information Object Definitions and Services for the representation and communication of digital images and related information, including image-derived annotations and analysis results. However, the complexity of the standard represents an obstacle for its adoption in the ML community and creates a need for software libraries and tools that simplify working with data sets in DICOM format. Here we present the highdicom library, which provides a high-level application programming interface for the Python programming language that abstracts low-level details of the standard and enables encoding and decoding of image-derived information in DICOM format in a few lines of Python code. The highdicom library ties into the extensive Python ecosystem for image processing and machine learning. Simultaneously, by simplifying creation and parsing of DICOM-compliant files, highdicom achieves interoperability with the medical imaging systems that hold the data used to train and run ML models, and ultimately communicate and store model outputs for clinical use. We demonstrate through experiments with slide microscopy and computed tomography imaging, that, by bridging these two ecosystems, highdicom enables developers to train and evaluate state-of-the-art ML models in pathology and radiology while remaining compliant with the DICOM standard and interoperable with clinical systems at all stages. To promote standardization of ML research and streamline the ML model development and deployment process, we made the library available free and open-source.



### Dynamic Distillation Network for Cross-Domain Few-Shot Recognition with Unlabeled Data
- **Arxiv ID**: http://arxiv.org/abs/2106.07807v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07807v3)
- **Published**: 2021-06-14 23:44:34+00:00
- **Updated**: 2021-11-01 04:28:04+00:00
- **Authors**: Ashraful Islam, Chun-Fu Chen, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Richard J. Radke
- **Comment**: Accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: Most existing works in few-shot learning rely on meta-learning the network on a large base dataset which is typically from the same domain as the target dataset. We tackle the problem of cross-domain few-shot learning where there is a large shift between the base and target domain. The problem of cross-domain few-shot recognition with unlabeled target data is largely unaddressed in the literature. STARTUP was the first method that tackles this problem using self-training. However, it uses a fixed teacher pretrained on a labeled base dataset to create soft labels for the unlabeled target samples. As the base dataset and unlabeled dataset are from different domains, projecting the target images in the class-domain of the base dataset with a fixed pretrained model might be sub-optimal. We propose a simple dynamic distillation-based approach to facilitate unlabeled images from the novel/base dataset. We impose consistency regularization by calculating predictions from the weakly-augmented versions of the unlabeled images from a teacher network and matching it with the strongly augmented versions of the same images from a student network. The parameters of the teacher network are updated as exponential moving average of the parameters of the student network. We show that the proposed network learns representation that can be easily adapted to the target domain even though it has not been trained with target-specific classes during the pretraining phase. Our model outperforms the current state-of-the art method by 4.4% for 1-shot and 3.6% for 5-shot classification in the BSCD-FSL benchmark, and also shows competitive performance on traditional in-domain few-shot learning task.



