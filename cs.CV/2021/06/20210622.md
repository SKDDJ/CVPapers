# Arxiv Papers in cs.CV on 2021-06-22
### Gait analysis with curvature maps: A simulation study
- **Arxiv ID**: http://arxiv.org/abs/2106.11466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11466v1)
- **Published**: 2021-06-22 00:59:17+00:00
- **Updated**: 2021-06-22 00:59:17+00:00
- **Authors**: Khac Chinh Tran, Marc Daniel, Jean Meunier
- **Comment**: 4 pages, 5 figures
- **Journal**: None
- **Summary**: Gait analysis is an important aspect of clinical investigation for detecting neurological and musculoskeletal disorders and assessing the global health of a patient. In this paper we propose to focus our attention on extracting relevant curvature information from the body surface provided by a depth camera. We assumed that the 3D mesh was made available in a previous step and demonstrated how curvature maps could be useful to assess asymmetric anomalies with two simple simulated abnormal gaits compared with a normal one. This research set the grounds for the future development of a curvature-based gait analysis system for healthcare professionals.



### Multimodal trajectory forecasting based on discrete heat map
- **Arxiv ID**: http://arxiv.org/abs/2106.11467v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11467v1)
- **Published**: 2021-06-22 01:02:52+00:00
- **Updated**: 2021-06-22 01:02:52+00:00
- **Authors**: Jingni Yuan, Jianyun Xu, Yushi Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In Argoverse motion forecasting competition, the task is to predict the probabilistic future trajectory distribution for the interested targets in the traffic scene. We use vectorized lane map and 2 s targets' history trajectories as input. Then the model outputs 6 forecasted trajectories with probability for each target.



### Fourier Transform Approximation as an Auxiliary Task for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.11478v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11478v3)
- **Published**: 2021-06-22 01:47:18+00:00
- **Updated**: 2021-07-01 22:25:39+00:00
- **Authors**: Chen Liu
- **Comment**: Work in progress. It will be very much appreciated if you can give
  suggestions on additional experiments and analyses that may improve this
  manuscript
- **Journal**: None
- **Summary**: Image reconstruction is likely the most predominant auxiliary task for image classification, but we would like to think twice about this convention. In this paper, we investigated "approximating the Fourier Transform of the input image" as a potential alternative, in the hope that it may further boost the performances on the primary task or introduce novel constraints not well covered by image reconstruction. We experimented with five popular classification architectures on the CIFAR-10 dataset, and the empirical results indicated that our proposed auxiliary task generally improves the classification accuracy. More notably, the results showed that in certain cases our proposed auxiliary task may enhance the classifiers' resistance to adversarial attacks generated using the fast gradient sign method.



### VoxelEmbed: 3D Instance Segmentation and Tracking with Voxel Embedding based Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.11480v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11480v1)
- **Published**: 2021-06-22 02:03:26+00:00
- **Updated**: 2021-06-22 02:03:26+00:00
- **Authors**: Mengyang Zhao, Quan Liu, Aadarsh Jha, Ruining Deng, Tianyuan Yao, Anita Mahadevan-Jansen, Matthew J. Tyska, Bryan A. Millis, Yuankai Huo
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in bioimaging have provided scientists a superior high spatial-temporal resolution to observe dynamics of living cells as 3D volumetric videos. Unfortunately, the 3D biomedical video analysis is lagging, impeded by resource insensitive human curation using off-the-shelf 3D analytic tools. Herein, biologists often need to discard a considerable amount of rich 3D spatial information by compromising on 2D analysis via maximum intensity projection. Recently, pixel embedding-based cell instance segmentation and tracking provided a neat and generalizable computing paradigm for understanding cellular dynamics. In this work, we propose a novel spatial-temporal voxel-embedding (VoxelEmbed) based learning method to perform simultaneous cell instance segmenting and tracking on 3D volumetric video sequences. Our contribution is in four-fold: (1) The proposed voxel embedding generalizes the pixel embedding with 3D context information; (2) Present a simple multi-stream learning approach that allows effective spatial-temporal embedding; (3) Accomplished an end-to-end framework for one-stage 3D cell instance segmentation and tracking without heavy parameter tuning; (4) The proposed 3D quantification is memory efficient via a single GPU with 12 GB memory. We evaluate our VoxelEmbed method on four 3D datasets (with different cell types) from the ISBI Cell Tracking Challenge. The proposed VoxelEmbed method achieved consistent superior overall performance (OP) on two densely annotated datasets. The performance is also competitive on two sparsely annotated cohorts with 20.6% and 2% of data-set having segmentation annotations. The results demonstrate that the VoxelEmbed method is a generalizable and memory-efficient solution.



### SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for Day-Night Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.11481v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.11481v1)
- **Published**: 2021-06-22 02:05:32+00:00
- **Updated**: 2021-06-22 02:05:32+00:00
- **Authors**: Sourav Garg, Michael Milford
- **Comment**: Accepted to CVPR 2021 Workshop on 3D Vision and Robotics (3DVR).
  https://sites.google.com/view/cvpr2021-3d-vision-robotics/
- **Journal**: None
- **Summary**: Place Recognition is a crucial capability for mobile robot localization and navigation. Image-based or Visual Place Recognition (VPR) is a challenging problem as scene appearance and camera viewpoint can change significantly when places are revisited. Recent VPR methods based on ``sequential representations'' have shown promising results as compared to traditional sequence score aggregation or single image based techniques. In parallel to these endeavors, 3D point clouds based place recognition is also being explored following the advances in deep learning based point cloud processing. However, a key question remains: is an explicit 3D structure based place representation always superior to an implicit ``spatial'' representation based on sequence of RGB images which can inherently learn scene structure. In this extended abstract, we attempt to compare these two types of methods by considering a similar ``metric span'' to represent places. We compare a 3D point cloud based method (PointNetVLAD) with image sequence based methods (SeqNet and others) and showcase that image sequence based techniques approach, and can even surpass, the performance achieved by point cloud based methods for a given metric span. These performance variations can be attributed to differences in data richness of input sensors as well as data accumulation strategies for a mobile robot. While a perfect apple-to-apple comparison may not be feasible for these two different modalities, the presented comparison takes a step in the direction of answering deeper questions regarding spatial representations, relevant to several applications like Autonomous Driving and Augmented/Virtual Reality. Source code available publicly https://github.com/oravus/seqNet.



### Wallpaper Texture Generation and Style Transfer Based on Multi-label Semantics
- **Arxiv ID**: http://arxiv.org/abs/2106.11482v1
- **DOI**: 10.1109/TCSVT.2021.3078560.
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11482v1)
- **Published**: 2021-06-22 02:09:25+00:00
- **Updated**: 2021-06-22 02:09:25+00:00
- **Authors**: Ying Gao, Xiaohan Feng, Tiange Zhang, Eric Rigall, Huiyu Zhou, Lin Qi, Junyu Dong
- **Comment**: IEEE Transactions on Circuits and Systems for Video Technology
- **Journal**: None
- **Summary**: Textures contain a wealth of image information and are widely used in various fields such as computer graphics and computer vision. With the development of machine learning, the texture synthesis and generation have been greatly improved. As a very common element in everyday life, wallpapers contain a wealth of texture information, making it difficult to annotate with a simple single label. Moreover, wallpaper designers spend significant time to create different styles of wallpaper. For this purpose, this paper proposes to describe wallpaper texture images by using multi-label semantics. Based on these labels and generative adversarial networks, we present a framework for perception driven wallpaper texture generation and style transfer. In this framework, a perceptual model is trained to recognize whether the wallpapers produced by the generator network are sufficiently realistic and have the attribute designated by given perceptual description; these multi-label semantic attributes are treated as condition variables to generate wallpaper images. The generated wallpaper images can be converted to those with well-known artist styles using CycleGAN. Finally, using the aesthetic evaluation method, the generated wallpaper images are quantitatively measured. The experimental results demonstrate that the proposed method can generate wallpaper textures conforming to human aesthetics and have artistic characteristics.



### Spatial-Temporal Super-Resolution of Satellite Imagery via Conditional Pixel Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2106.11485v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.11485v3)
- **Published**: 2021-06-22 02:16:24+00:00
- **Updated**: 2022-04-04 16:39:41+00:00
- **Authors**: Yutong He, Dingjie Wang, Nicholas Lai, William Zhang, Chenlin Meng, Marshall Burke, David B. Lobell, Stefano Ermon
- **Comment**: None
- **Journal**: Advances in Neural Information Processing Systems 35 (2021)
  27903-27915
- **Summary**: High-resolution satellite imagery has proven useful for a broad range of tasks, including measurement of global human population, local economic livelihoods, and biodiversity, among many others. Unfortunately, high-resolution imagery is both infrequently collected and expensive to purchase, making it hard to efficiently and effectively scale these downstream tasks over both time and space. We propose a new conditional pixel synthesis model that uses abundant, low-cost, low-resolution imagery to generate accurate high-resolution imagery at locations and times in which it is unavailable. We show that our model attains photo-realistic sample quality and outperforms competing baselines on a key downstream task -- object counting -- particularly in geographic locations where conditions on the ground are changing rapidly.



### Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction for Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.11486v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11486v1)
- **Published**: 2021-06-22 02:25:01+00:00
- **Updated**: 2021-06-22 02:25:01+00:00
- **Authors**: Dong Hoon Lee, Sae-Young Chung
- **Comment**: Accepted to ICML 2021
- **Journal**: None
- **Summary**: We propose unsupervised embedding adaptation for the downstream few-shot classification task. Based on findings that deep neural networks learn to generalize before memorizing, we develop Early-Stage Feature Reconstruction (ESFR) -- a novel adaptation scheme with feature reconstruction and dimensionality-driven early stopping that finds generalizable features. Incorporating ESFR consistently improves the performance of baseline methods on all standard settings, including the recently proposed transductive method. ESFR used in conjunction with the transductive method further achieves state-of-the-art performance on mini-ImageNet, tiered-ImageNet, and CUB; especially with 1.2%~2.0% improvements in accuracy over the previous best performing method on 1-shot setting.



### SA-LOAM: Semantic-aided LiDAR SLAM with Loop Closure
- **Arxiv ID**: http://arxiv.org/abs/2106.11516v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11516v2)
- **Published**: 2021-06-22 03:14:20+00:00
- **Updated**: 2021-07-01 06:56:19+00:00
- **Authors**: Lin Li, Xin Kong, Xiangrui Zhao, Wanlong Li, Feng Wen, Hongbo Zhang, Yong Liu
- **Comment**: 8 pages. Accepted by ICRA-2021
- **Journal**: None
- **Summary**: LiDAR-based SLAM system is admittedly more accurate and stable than others, while its loop closure detection is still an open issue. With the development of 3D semantic segmentation for point cloud, semantic information can be obtained conveniently and steadily, essential for high-level intelligence and conductive to SLAM. In this paper, we present a novel semantic-aided LiDAR SLAM with loop closure based on LOAM, named SA-LOAM, which leverages semantics in odometry as well as loop closure detection. Specifically, we propose a semantic-assisted ICP, including semantically matching, downsampling and plane constraint, and integrates a semantic graph-based place recognition method in our loop closure detection module. Benefitting from semantics, we can improve the localization accuracy, detect loop closures effectively, and construct a global consistent semantic map even in large-scale scenes. Extensive experiments on KITTI and Ford Campus dataset show that our system significantly improves baseline performance, has generalization ability to unseen data and achieves competitive results compared with state-of-the-art methods.



### Recent Deep Semi-supervised Learning Approaches and Related Works
- **Arxiv ID**: http://arxiv.org/abs/2106.11528v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11528v2)
- **Published**: 2021-06-22 03:44:03+00:00
- **Updated**: 2022-07-05 08:38:41+00:00
- **Authors**: Gyeongho Kim
- **Comment**: None
- **Journal**: None
- **Summary**: The author of this work proposes an overview of the recent semi-supervised learning approaches and related works. Despite the remarkable success of neural networks in various applications, there exist few formidable constraints including the need for a large amount of labeled data. Therefore, semi-supervised learning, which is a learning scheme in which the scarce labels and a larger amount of unlabeled data are utilized to train models (e.g., deep neural networks) is getting more important. Based on the key assumptions of semi-supervised learning, which are the manifold assumption, cluster assumption, and continuity assumption, the work reviews the recent semi-supervised learning approaches. In particular, the methods in regard to using deep neural networks in a semi-supervised learning setting are primarily discussed. In addition, the existing works are first classified based on the underlying idea and explained, and then the holistic approaches that unify the aforementioned ideas are detailed.



### Serial-EMD: Fast Empirical Mode Decomposition Method for Multi-dimensional Signals Based on Serialization
- **Arxiv ID**: http://arxiv.org/abs/2106.15319v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.15319v1)
- **Published**: 2021-06-22 03:56:08+00:00
- **Updated**: 2021-06-22 03:56:08+00:00
- **Authors**: Jin Zhang, Fan Feng, Pere Marti-Puig, Cesar F. Caiafa, Zhe Sun, Feng Duan, Jordi Solé-Casals
- **Comment**: 19 pages, 17 figures
- **Journal**: None
- **Summary**: Empirical mode decomposition (EMD) has developed into a prominent tool for adaptive, scale-based signal analysis in various fields like robotics, security and biomedical engineering. Since the dramatic increase in amount of data puts forward higher requirements for the capability of real-time signal analysis, it is difficult for existing EMD and its variants to trade off the growth of data dimension and the speed of signal analysis. In order to decompose multi-dimensional signals at a faster speed, we present a novel signal-serialization method (serial-EMD), which concatenates multi-variate or multi-dimensional signals into a one-dimensional signal and uses various one-dimensional EMD algorithms to decompose it. To verify the effects of the proposed method, synthetic multi-variate time series, artificial 2D images with various textures and real-world facial images are tested. Compared with existing multi-EMD algorithms, the decomposition time becomes significantly reduced. In addition, the results of facial recognition with Intrinsic Mode Functions (IMFs) extracted using our method can achieve a higher accuracy than those obtained by existing multi-EMD algorithms, which demonstrates the superior performance of our method in terms of the quality of IMFs. Furthermore, this method can provide a new perspective to optimize the existing EMD algorithms, that is, transforming the structure of the input signal rather than being constrained by developing envelope computation techniques or signal decomposition methods. In summary, the study suggests that the serial-EMD technique is a highly competitive and fast alternative for multi-dimensional signal analysis.



### Deep3DPose: Realtime Reconstruction of Arbitrarily Posed Human Bodies from Single RGB Images
- **Arxiv ID**: http://arxiv.org/abs/2106.11536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11536v1)
- **Published**: 2021-06-22 04:26:11+00:00
- **Updated**: 2021-06-22 04:26:11+00:00
- **Authors**: Liguo Jiang, Miaopeng Li, Jianjie Zhang, Congyi Wang, Juntao Ye, Xinguo Liu, Jinxiang Chai
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce an approach that accurately reconstructs 3D human poses and detailed 3D full-body geometric models from single images in realtime. The key idea of our approach is a novel end-to-end multi-task deep learning framework that uses single images to predict five outputs simultaneously: foreground segmentation mask, 2D joints positions, semantic body partitions, 3D part orientations and uv coordinates (uv map). The multi-task network architecture not only generates more visual cues for reconstruction, but also makes each individual prediction more accurate. The CNN regressor is further combined with an optimization based algorithm for accurate kinematic pose reconstruction and full-body shape modeling. We show that the realtime reconstruction reaches accurate fitting that has not been seen before, especially for wild images. We demonstrate the results of our realtime 3D pose and human body reconstruction system on various challenging in-the-wild videos. We show the system advances the frontier of 3D human body and pose reconstruction from single images by quantitative evaluations and comparisons with state-of-the-art methods.



### DocFormer: End-to-End Transformer for Document Understanding
- **Arxiv ID**: http://arxiv.org/abs/2106.11539v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11539v2)
- **Published**: 2021-06-22 04:28:07+00:00
- **Updated**: 2021-09-20 06:12:57+00:00
- **Authors**: Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, R. Manmatha
- **Comment**: Accepted to ICCV 2021 main conference
- **Journal**: None
- **Summary**: We present DocFormer -- a multi-modal transformer based architecture for the task of Visual Document Understanding (VDU). VDU is a challenging problem which aims to understand documents in their varied formats (forms, receipts etc.) and layouts. In addition, DocFormer is pre-trained in an unsupervised fashion using carefully designed tasks which encourage multi-modal interaction. DocFormer uses text, vision and spatial features and combines them using a novel multi-modal self-attention layer. DocFormer also shares learned spatial embeddings across modalities which makes it easy for the model to correlate text to visual tokens and vice versa. DocFormer is evaluated on 4 different datasets each with strong baselines. DocFormer achieves state-of-the-art results on all of them, sometimes beating models 4x its size (in no. of parameters).



### Kernel Clustering with Sigmoid-based Regularization for Efficient Segmentation of Sequential Data
- **Arxiv ID**: http://arxiv.org/abs/2106.11541v2
- **DOI**: 10.1109/ACCESS.2022.3182345
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11541v2)
- **Published**: 2021-06-22 04:32:21+00:00
- **Updated**: 2022-06-22 04:14:15+00:00
- **Authors**: Tung Doan, Atsuhiro Takasu
- **Comment**: None
- **Journal**: None
- **Summary**: Kernel segmentation aims at partitioning a data sequence into several non-overlapping segments that may have nonlinear and complex structures. In general, it is formulated as a discrete optimization problem with combinatorial constraints. A popular algorithm for optimally solving this problem is dynamic programming (DP), which has quadratic computation and memory requirements. Given that sequences in practice are too long, this algorithm is not a practical approach. Although many heuristic algorithms have been proposed to approximate the optimal segmentation, they have no guarantee on the quality of their solutions. In this paper, we take a differentiable approach to alleviate the aforementioned issues. First, we introduce a novel sigmoid-based regularization to smoothly approximate the combinatorial constraints. Combining it with objective of the balanced kernel clustering, we formulate a differentiable model termed Kernel clustering with sigmoid-based regularization (KCSR), where the gradient-based algorithm can be exploited to obtain the optimal segmentation. Second, we develop a stochastic variant of the proposed model. By using the stochastic gradient descent algorithm, which has much lower time and space complexities, for optimization, the second model can perform segmentation on overlong data sequences. Finally, for simultaneously segmenting multiple data sequences, we slightly modify the sigmoid-based regularization to further introduce an extended variant of the proposed model. Through extensive experiments on various types of data sequences performances of our models are evaluated and compared with those of the existing methods. The experimental results validate advantages of the proposed models. Our Matlab source code is available on github.



### Connection Sensitivity Matters for Training-free DARTS: From Architecture-Level Scoring to Operation-Level Sensitivity Analysis
- **Arxiv ID**: http://arxiv.org/abs/2106.11542v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11542v4)
- **Published**: 2021-06-22 04:40:34+00:00
- **Updated**: 2023-05-12 13:17:29+00:00
- **Authors**: Miao Zhang, Wei Huang, Li Wang
- **Comment**: A revised version with more theoretical analysis
- **Journal**: None
- **Summary**: The recently proposed training-free NAS methods abandon the training phase and design various zero-cost proxies as scores to identify excellent architectures, arousing extreme computational efficiency for neural architecture search. In this paper, we raise an interesting problem: can we properly measure the operation importance in DARTS through a training-free way, with avoiding the parameter-intensive bias? We investigate this question through the lens of edge connectivity, and provide an affirmative answer by defining a connectivity concept, ZERo-cost Operation Sensitivity (ZEROS), to score the importance of candidate operations in DARTS at initialization. By devising an iterative and data-agnostic manner in utilizing ZEROS for NAS, our novel trial leads to a framework called training free differentiable architecture search (FreeDARTS). Based on the theory of Neural Tangent Kernel (NTK), we show the proposed connectivity score provably negatively correlated with the generalization bound of DARTS supernet after convergence under gradient descent training. In addition, we theoretically explain how ZEROS implicitly avoids parameter-intensive bias in selecting architectures, and empirically show the searched architectures by FreeDARTS are of comparable size. Extensive experiments have been conducted on a series of search spaces, and results have demonstrated that FreeDARTS is a reliable and efficient baseline for neural architecture search.



### Winning the CVPR'2021 Kinetics-GEBD Challenge: Contrastive Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2106.11549v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11549v1)
- **Published**: 2021-06-22 05:21:59+00:00
- **Updated**: 2021-06-22 05:21:59+00:00
- **Authors**: Hyolim Kang, Jinwoo Kim, Kyungmin Kim, Taehyun Kim, Seon Joo Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Generic Event Boundary Detection (GEBD) is a newly introduced task that aims to detect "general" event boundaries that correspond to natural human perception. In this paper, we introduce a novel contrastive learning based approach to deal with the GEBD. Our intuition is that the feature similarity of the video snippet would significantly vary near the event boundaries, while remaining relatively the same in the remaining part of the video. In our model, Temporal Self-similarity Matrix (TSM) is utilized as an intermediate representation which takes on a role as an information bottleneck. With our model, we achieved significant performance boost compared to the given baselines. Our code is available at https://github.com/hello-jinwoo/LOVEU-CVPR2021.



### Learning-Based Practical Light Field Image Compression Using A Disparity-Aware Model
- **Arxiv ID**: http://arxiv.org/abs/2106.11558v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11558v2)
- **Published**: 2021-06-22 06:30:25+00:00
- **Updated**: 2021-06-23 04:45:09+00:00
- **Authors**: Mohana Singh, Renu M. Rameshan
- **Comment**: accepted to Picture Coding Symposium 2021, corrected typo in link to
  source code
- **Journal**: None
- **Summary**: Light field technology has increasingly attracted the attention of the research community with its many possible applications. The lenslet array in commercial plenoptic cameras helps capture both the spatial and angular information of light rays in a single exposure. While the resulting high dimensionality of light field data enables its superior capabilities, it also impedes its extensive adoption. Hence, there is a compelling need for efficient compression of light field images. Existing solutions are commonly composed of several separate modules, some of which may not have been designed for the specific structure and quality of light field data. This increases the complexity of the codec and results in impractical decoding runtimes. We propose a new learning-based, disparity-aided model for compression of 4D light field images capable of parallel decoding. The model is end-to-end trainable, eliminating the need for hand-tuning separate modules and allowing joint learning of rate and distortion. The disparity-aided approach ensures the structural integrity of the reconstructed light fields. Comparisons with the state of the art show encouraging performance in terms of PSNR and MS-SSIM metrics. Also, there is a notable gain in the encoding and decoding runtimes. Source code is available at https://moha23.github.io/LF-DAAE.



### Hand-Drawn Electrical Circuit Recognition using Object Detection and Node Recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.11559v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11559v2)
- **Published**: 2021-06-22 06:30:50+00:00
- **Updated**: 2021-11-27 14:36:12+00:00
- **Authors**: Rachala Rohith Reddy, Mahesh Raveendranatha Panicker
- **Comment**: 10 pages. 8 figures, under review in springer
- **Journal**: None
- **Summary**: With the recent developments in neural networks, there has been a resurgence in algorithms for the automatic generation of simulation ready electronic circuits from hand-drawn circuits. However, most of the approaches in literature were confined to classify different types of electrical components and only a few of those methods have shown a way to rebuild the circuit schematic from the scanned image, which is extremely important for further automation of netlist generation. This paper proposes a real-time algorithm for the automatic recognition of hand-drawn electrical circuits based on object detection and circuit node recognition. The proposed approach employs You Only Look Once version 5 (YOLOv5) for detection of circuit components and a novel Hough transform based approach for node recognition. Using YOLOv5 object detection algorithm, a mean average precision (mAP0.5) of 98.2% is achieved in detecting the components. The proposed method is also able to rebuild the circuit schematic with 80% accuracy with a near-real time performance of 0.33s per schematic generation.



### Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks
- **Arxiv ID**: http://arxiv.org/abs/2106.12900v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.12900v3)
- **Published**: 2021-06-22 06:31:16+00:00
- **Updated**: 2021-07-01 09:41:16+00:00
- **Authors**: Fan Liu, Shuyu Zhao, Xuelong Dai, Bin Xiao
- **Comment**: Accepted by the ICML 2021 Workshop on A Blessing in Disguise: The
  Prospects and Perils of Adversarial Machine
  Learning(https://openreview.net/group?id=ICML.cc/2021/Workshop/AML)
- **Journal**: None
- **Summary**: Meta-learning model can quickly adapt to new tasks using few-shot labeled data. However, despite achieving good generalization on few-shot classification tasks, it is still challenging to improve the adversarial robustness of the meta-learning model in few-shot learning. Although adversarial training (AT) methods such as Adversarial Query (AQ) can improve the adversarially robust performance of meta-learning models, AT is still computationally expensive training. On the other hand, meta-learning models trained with AT will drop significant accuracy on the original clean images. This paper proposed a meta-learning method on the adversarially robust neural network called Long-term Cross Adversarial Training (LCAT). LCAT will update meta-learning model parameters cross along the natural and adversarial sample distribution direction with long-term to improve both adversarial and clean few-shot classification accuracy. Due to cross-adversarial training, LCAT only needs half of the adversarial training epoch than AQ, resulting in a low adversarial training computation. Experiment results show that LCAT achieves superior performance both on the clean and adversarial few-shot classification accuracy than SOTA adversarial training methods for meta-learning models.



### SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.11562v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11562v3)
- **Published**: 2021-06-22 06:40:26+00:00
- **Updated**: 2021-11-19 19:52:19+00:00
- **Authors**: Sungmin Cha, Beomyoung Kim, Youngjoon Yoo, Taesup Moon
- **Comment**: NeurIPS 2021 camera ready version
- **Journal**: None
- **Summary**: This paper introduces a solid state-of-the-art baseline for a class-incremental semantic segmentation (CISS) problem. While the recent CISS algorithms utilize variants of the knowledge distillation (KD) technique to tackle the problem, they failed to fully address the critical challenges in CISS causing the catastrophic forgetting; the semantic drift of the background class and the multi-label prediction issue. To better address these challenges, we propose a new method, dubbed SSUL-M (Semantic Segmentation with Unknown Label with Memory), by carefully combining techniques tailored for semantic segmentation. Specifically, we claim three main contributions. (1) defining unknown classes within the background class to help to learn future classes (help plasticity), (2) freezing backbone network and past classifiers with binary cross-entropy loss and pseudo-labeling to overcome catastrophic forgetting (help stability), and (3) utilizing tiny exemplar memory for the first time in CISS to improve both plasticity and stability. The extensively conducted experiments show the effectiveness of our method, achieving significantly better performance than the recent state-of-the-art baselines on the standard benchmark datasets. Furthermore, we justify our contributions with thorough ablation analyses and discuss different natures of the CISS problem compared to the traditional class-incremental learning targeting classification. The official code is available at https://github.com/clovaai/SSUL.



### Creating A New Color Space utilizing PSO and FCM to Perform Skin Detection by using Neural Network and ANFIS
- **Arxiv ID**: http://arxiv.org/abs/2106.11563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11563v1)
- **Published**: 2021-06-22 06:41:33+00:00
- **Updated**: 2021-06-22 06:41:33+00:00
- **Authors**: Kobra Nazari, Samaneh Mazaheri, Bahram Sadeghi Bigham
- **Comment**: None
- **Journal**: None
- **Summary**: Skin color detection is an essential required step in various applications related to computer vision. These applications will include face detection, finding pornographic images in movies and photos, finding ethnicity, age, diagnosis, and so on. Therefore, proposing a proper skin detection method can provide solution to several problems. In this study, first a new color space is created using FCM and PSO algorithms. Then, skin classification has been performed in the new color space utilizing linear and nonlinear modes. Additionally, it has been done in RGB and LAB color spaces by using ANFIS and neural network. Skin detection in RBG color space has been performed using Mahalanobis distance and Euclidean distance algorithms. In comparison, this method has 18.38% higher accuracy than the most accurate method on the same database. Additionally, this method has achieved 90.05% in equal error rate (1-EER) in testing COMPAQ dataset and 92.93% accuracy in testing Pratheepan dataset, which compared to the previous method on COMPAQ database, 1-EER has increased by %0.87.



### Universal Domain Adaptation in Ordinal Regression
- **Arxiv ID**: http://arxiv.org/abs/2106.11576v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.11576v2)
- **Published**: 2021-06-22 07:23:39+00:00
- **Updated**: 2021-08-25 08:38:25+00:00
- **Authors**: Boris Chidlovskii, Assem Sadek, Christian Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of universal domain adaptation (UDA) in ordinal regression (OR), which attempts to solve classification problems in which labels are not independent, but follow a natural order. We show that the UDA techniques developed for classification and based on the clustering assumption, under-perform in OR settings. We propose a method that complements the OR classifier with an auxiliary task of order learning, which plays the double role of discriminating between common and private instances, and expanding class labels to the private target images via ranking. Combined with adversarial domain discrimination, our model is able to address the closed set, partial and open set configurations. We evaluate our method on three face age estimation datasets, and show that it outperforms the baseline methods.



### A Comparison for Patch-level Classification of Deep Learning Methods on Transparent Environmental Microorganism Images: from Convolutional Neural Networks to Visual Transformers
- **Arxiv ID**: http://arxiv.org/abs/2106.11582v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11582v2)
- **Published**: 2021-06-22 07:30:45+00:00
- **Updated**: 2021-07-21 01:37:40+00:00
- **Authors**: Hechen Yang, Chen Li, Jinghua Zhang, Peng Zhao, Ao Chen, Xin Zhao, Tao Jiang, Marcin Grzegorzek
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, analysis of Transparent Environmental Microorganism Images (T-EM images) in the field of computer vision has gradually become a new and interesting spot. This paper compares different deep learning classification performance for the problem that T-EM images are challenging to analyze. We crop the T-EM images into 8 * 8 and 224 * 224 pixel patches in the same proportion and then divide the two different pixel patches into foreground and background according to ground truth. We also use four convolutional neural networks and a novel ViT network model to compare the foreground and background classification experiments. We conclude that ViT performs the worst in classifying 8 * 8 pixel patches, but it outperforms most convolutional neural networks in classifying 224 * 224 pixel patches.



### Part-Aware Measurement for Robust Multi-View Multi-Human 3D Pose Estimation and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2106.11589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11589v1)
- **Published**: 2021-06-22 07:50:09+00:00
- **Updated**: 2021-06-22 07:50:09+00:00
- **Authors**: Hau Chu, Jia-Hong Lee, Yao-Chih Lee, Ching-Hsien Hsu, Jia-Da Li, Chu-Song Chen
- **Comment**: 12 pages with supplementary material; accepted to CVPR 2021 B-AMFG
  Workshop
- **Journal**: None
- **Summary**: This paper introduces an approach for multi-human 3D pose estimation and tracking based on calibrated multi-view. The main challenge lies in finding the cross-view and temporal correspondences correctly even when several human pose estimations are noisy. Compare to previous solutions that construct 3D poses from multiple views, our approach takes advantage of temporal consistency to match the 2D poses estimated with previously constructed 3D skeletons in every view. Therefore cross-view and temporal associations are accomplished simultaneously. Since the performance suffers from mistaken association and noisy predictions, we design two strategies for aiming better correspondences and 3D reconstruction. Specifically, we propose a part-aware measurement for 2D-3D association and a filter that can cope with 2D outliers during reconstruction. Our approach is efficient and effective comparing to state-of-the-art methods; it achieves competitive results on two benchmarks: 96.8% on Campus and 97.4% on Shelf. Moreover, we extends the length of Campus evaluation frames to be more challenging and our proposal also reach well-performed result.



### Multi-layered Semantic Representation Network for Multi-label Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2106.11596v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.11596v1)
- **Published**: 2021-06-22 08:04:22+00:00
- **Updated**: 2021-06-22 08:04:22+00:00
- **Authors**: Xiwen Qu, Hao Che, Jun Huang, Linchuan Xu, Xiao Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-label image classification (MLIC) is a fundamental and practical task, which aims to assign multiple possible labels to an image. In recent years, many deep convolutional neural network (CNN) based approaches have been proposed which model label correlations to discover semantics of labels and learn semantic representations of images. This paper advances this research direction by improving both the modeling of label correlations and the learning of semantic representations. On the one hand, besides the local semantics of each label, we propose to further explore global semantics shared by multiple labels. On the other hand, existing approaches mainly learn the semantic representations at the last convolutional layer of a CNN. But it has been noted that the image representations of different layers of CNN capture different levels or scales of features and have different discriminative abilities. We thus propose to learn semantic representations at multiple convolutional layers. To this end, this paper designs a Multi-layered Semantic Representation Network (MSRN) which discovers both local and global semantics of labels through modeling label correlations and utilizes the label semantics to guide the semantic representations learning at multiple layers through an attention mechanism. Extensive experiments on four benchmark datasets including VOC 2007, COCO, NUS-WIDE, and Apparel show a competitive performance of the proposed MSRN against state-of-the-art models.



### Zero-Shot Chinese Character Recognition with Stroke-Level Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2106.11613v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.11613v1)
- **Published**: 2021-06-22 08:49:03+00:00
- **Updated**: 2021-06-22 08:49:03+00:00
- **Authors**: Jingye Chen, Bin Li, Xiangyang Xue
- **Comment**: 7 pages, 7 figures
- **Journal**: None
- **Summary**: Chinese character recognition has attracted much research interest due to its wide applications. Although it has been studied for many years, some issues in this field have not been completely resolved yet, e.g. the zero-shot problem. Previous character-based and radical-based methods have not fundamentally addressed the zero-shot problem since some characters or radicals in test sets may not appear in training sets under a data-hungry condition. Inspired by the fact that humans can generalize to know how to write characters unseen before if they have learned stroke orders of some characters, we propose a stroke-based method by decomposing each character into a sequence of strokes, which are the most basic units of Chinese characters. However, we observe that there is a one-to-many relationship between stroke sequences and Chinese characters. To tackle this challenge, we employ a matching-based strategy to transform the predicted stroke sequence to a specific character. We evaluate the proposed method on handwritten characters, printed artistic characters, and scene characters. The experimental results validate that the proposed method outperforms existing methods on both character zero-shot and radical zero-shot tasks. Moreover, the proposed method can be easily generalized to other languages whose characters can be decomposed into strokes.



### Confidence-Aware Learning for Camouflaged Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.11641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11641v1)
- **Published**: 2021-06-22 09:49:23+00:00
- **Updated**: 2021-06-22 09:49:23+00:00
- **Authors**: Jiawei Liu, Jing Zhang, Nick Barnes
- **Comment**: None
- **Journal**: None
- **Summary**: Confidence-aware learning is proven as an effective solution to prevent networks becoming overconfident. We present a confidence-aware camouflaged object detection framework using dynamic supervision to produce both accurate camouflage map and meaningful "confidence" representing model awareness about the current prediction. A camouflaged object detection network is designed to produce our camouflage prediction. Then, we concatenate it with the input image and feed it to the confidence estimation network to produce an one channel confidence map.We generate dynamic supervision for the confidence estimation network, representing the agreement of camouflage prediction with the ground truth camouflage map. With the produced confidence map, we introduce confidence-aware learning with the confidence map as guidance to pay more attention to the hard/low-confidence pixels in the loss function. We claim that, once trained, our confidence estimation network can evaluate pixel-wise accuracy of the prediction without relying on the ground truth camouflage map. Extensive results on four camouflaged object detection testing datasets illustrate the superior performance of the proposed model in explaining the camouflage prediction.



### NCIS: Neural Contextual Iterative Smoothing for Purifying Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2106.11644v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11644v2)
- **Published**: 2021-06-22 09:51:51+00:00
- **Updated**: 2021-12-30 11:48:43+00:00
- **Authors**: Sungmin Cha, Naeun Ko, Youngjoon Yoo, Taesup Moon
- **Comment**: Preprint version
- **Journal**: None
- **Summary**: We propose a novel and effective purification based adversarial defense method against pre-processor blind white- and black-box attacks. Our method is computationally efficient and trained only with self-supervised learning on general images, without requiring any adversarial training or retraining of the classification model. We first show an empirical analysis on the adversarial noise, defined to be the residual between an original image and its adversarial example, has almost zero mean, symmetric distribution. Based on this observation, we propose a very simple iterative Gaussian Smoothing (GS) which can effectively smooth out adversarial noise and achieve substantially high robust accuracy. To further improve it, we propose Neural Contextual Iterative Smoothing (NCIS), which trains a blind-spot network (BSN) in a self-supervised manner to reconstruct the discriminative features of the original image that is also smoothed out by GS. From our extensive experiments on the large-scale ImageNet using four classification models, we show that our method achieves both competitive standard accuracy and state-of-the-art robust accuracy against most strong purifier-blind white- and black-box attacks. Also, we propose a new benchmark for evaluating a purification method based on commercial image classification APIs, such as AWS, Azure, Clarifai and Google. We generate adversarial examples by ensemble transfer-based black-box attack, which can induce complete misclassification of APIs, and demonstrate that our method can be used to increase adversarial robustness of APIs.



### A Survey on Human-aware Robot Navigation
- **Arxiv ID**: http://arxiv.org/abs/2106.11650v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11650v1)
- **Published**: 2021-06-22 10:09:25+00:00
- **Updated**: 2021-06-22 10:09:25+00:00
- **Authors**: Ronja Möller, Antonino Furnari, Sebastiano Battiato, Aki Härmä, Giovanni Maria Farinella
- **Comment**: Robotics and Autonomous Systems, 2021
- **Journal**: None
- **Summary**: Intelligent systems are increasingly part of our everyday lives and have been integrated seamlessly to the point where it is difficult to imagine a world without them. Physical manifestations of those systems on the other hand, in the form of embodied agents or robots, have so far been used only for specific applications and are often limited to functional roles (e.g. in the industry, entertainment and military fields). Given the current growth and innovation in the research communities concerned with the topics of robot navigation, human-robot-interaction and human activity recognition, it seems like this might soon change. Robots are increasingly easy to obtain and use and the acceptance of them in general is growing. However, the design of a socially compliant robot that can function as a companion needs to take various areas of research into account. This paper is concerned with the navigation aspect of a socially-compliant robot and provides a survey of existing solutions for the relevant areas of research as well as an outlook on possible future directions.



### Source Data-Free Cross-Domain Semantic Segmentation: Align, Teach and Propagate
- **Arxiv ID**: http://arxiv.org/abs/2106.11653v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11653v4)
- **Published**: 2021-06-22 10:21:39+00:00
- **Updated**: 2022-06-03 07:07:17+00:00
- **Authors**: Yuxi Wang, Jian Liang, Zhaoxiang Zhang
- **Comment**: 15 pages, 6 figures
- **Journal**: None
- **Summary**: Benefiting from considerable pixel-level annotations collected from a specific situation (source), the trained semantic segmentation model performs quite well but fails in a new situation (target). To mitigate the domain gap, previous cross-domain semantic segmentation methods always assume the co-existence of source data and target data during domain alignment. However, accessing source data in the real scenario may raise privacy concerns and violate intellectual property. To tackle this problem, we focus on an interesting and challenging cross-domain semantic segmentation task where only the trained source model is provided to the target domain. Specifically, we propose a unified framework called \textbf{ATP}, which consists of three schemes, i.e., feature \textbf{A}lignment, bidirectional \textbf{T}eaching, and information \textbf{P}ropagation. First, considering explicit alignment is infeasible due to no source data, we devise a curriculum-style entropy minimization objective to implicitly align the target features with unseen source features via the provided source model. Second, besides positive pseudo labels in vanilla self-training, we introduce negative pseudo labels to this field and develop a bidirectional self-training strategy to enhance the representation learning in the target domain. It is the first work to use negative pseudo labels during self-training for domain adaptation. Finally, the information propagation scheme is employed to further reduce the intra-domain discrepancy within the target domain via pseudo-semi-supervised learning, which is the first step by providing a simple and effective post-process for the domain adaptation field. Furthermore, we also extend the proposed to the more challenging black-box source-model scenario where only the source model's prediction is available.



### The Hitchhiker's Guide to Prior-Shift Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2106.11695v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11695v2)
- **Published**: 2021-06-22 11:55:51+00:00
- **Updated**: 2021-12-03 08:54:37+00:00
- **Authors**: Tomas Sipka, Milan Sulc, Jiri Matas
- **Comment**: WACV 2022 16 pages, 7 figures
- **Journal**: None
- **Summary**: In many computer vision classification tasks, class priors at test time often differ from priors on the training set. In the case of such prior shift, classifiers must be adapted correspondingly to maintain close to optimal performance. This paper analyzes methods for adaptation of probabilistic classifiers to new priors and for estimating new priors on an unlabeled test set. We propose a novel method to address a known issue of prior estimation methods based on confusion matrices, where inconsistent estimates of decision probabilities and confusion matrices lead to negative values in the estimated priors. Experiments on fine-grained image classification datasets provide insight into the best practice of prior shift estimation and classifier adaptation, and show that the proposed method achieves state-of-the-art results in prior adaptation. Applying the best practice to two tasks with naturally imbalanced priors, learning from web-crawled images and plant species classification, increased the recognition accuracy by 1.1% and 3.4% respectively.



### RGB2Hands: Real-Time Tracking of 3D Hand Interactions from Monocular RGB Video
- **Arxiv ID**: http://arxiv.org/abs/2106.11725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11725v1)
- **Published**: 2021-06-22 12:53:56+00:00
- **Updated**: 2021-06-22 12:53:56+00:00
- **Authors**: Jiayi Wang, Franziska Mueller, Florian Bernard, Suzanne Sorli, Oleksandr Sotnychenko, Neng Qian, Miguel A. Otaduy, Dan Casas, Christian Theobalt
- **Comment**: SIGGRAPH Asia 2020
- **Journal**: ACM Transactions on Graphics (TOG) 39 (6), 1-16, 2020
- **Summary**: Tracking and reconstructing the 3D pose and geometry of two hands in interaction is a challenging problem that has a high relevance for several human-computer interaction applications, including AR/VR, robotics, or sign language recognition. Existing works are either limited to simpler tracking settings (e.g., considering only a single hand or two spatially separated hands), or rely on less ubiquitous sensors, such as depth cameras. In contrast, in this work we present the first real-time method for motion capture of skeletal pose and 3D surface geometry of hands from a single RGB camera that explicitly considers close interactions. In order to address the inherent depth ambiguities in RGB data, we propose a novel multi-task CNN that regresses multiple complementary pieces of information, including segmentation, dense matchings to a 3D hand model, and 2D keypoint positions, together with newly proposed intra-hand relative depth and inter-hand distance maps. These predictions are subsequently used in a generative model fitting framework in order to estimate pose and shape parameters of a 3D hand model for both hands. We experimentally verify the individual components of our RGB two-hand tracking and 3D reconstruction pipeline through an extensive ablation study. Moreover, we demonstrate that our approach offers previously unseen two-hand tracking performance from RGB, and quantitatively and qualitatively outperforms existing RGB-based methods that were not explicitly designed for two-hand interactions. Moreover, our method even performs on-par with depth-based real-time methods.



### MIMIR: Deep Regression for Automated Analysis of UK Biobank Body MRI
- **Arxiv ID**: http://arxiv.org/abs/2106.11731v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11731v2)
- **Published**: 2021-06-22 13:09:40+00:00
- **Updated**: 2021-09-14 11:40:45+00:00
- **Authors**: Taro Langner, Andrés Martínez Mora, Robin Strand, Håkan Ahlström, Joel Kullberg
- **Comment**: None
- **Journal**: None
- **Summary**: UK Biobank (UKB) conducts large-scale examinations of more than half a million volunteers, collecting health-related information on genetics, lifestyle, blood biochemistry, and more. Medical imaging of 100,000 subjects, with 70,000 follow-up sessions, enables measurements of organs, muscle, and body composition. With up to 170,000 mounting MR images, various methodologies are accordingly engaged in large-scale image analysis. This work presents an experimental inference engine that can automatically predict a comprehensive profile of subject metadata from UKB neck-to-knee body MRI. It was evaluated in cross-validation for baseline characteristics such as age, height, weight, and sex, but also measurements of body composition, organ volumes, and abstract properties like grip strength, pulse rate, and type 2 diabetic status. It predicted subsequently released test data covering twelve body composition metrics with a 3% median error. The proposed system can automatically analyze one thousand subjects within ten minutes, providing individual confidence intervals. The underlying methodology utilizes convolutional neural networks for image-based mean-variance regression on two-dimensional representations of the MRI data. This work aims to make the proposed system available for free to researchers, who can use it to obtain fast and fully-automated estimates of 72 different measurements immediately upon release of new UKB image data.



### Fine-Tuning StyleGAN2 For Cartoon Face Generation
- **Arxiv ID**: http://arxiv.org/abs/2106.12445v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.12445v1)
- **Published**: 2021-06-22 14:00:10+00:00
- **Updated**: 2021-06-22 14:00:10+00:00
- **Authors**: Jihye Back
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Recent studies have shown remarkable success in the unsupervised image to image (I2I) translation. However, due to the imbalance in the data, learning joint distribution for various domains is still very challenging. Although existing models can generate realistic target images, it's difficult to maintain the structure of the source image. In addition, training a generative model on large data in multiple domains requires a lot of time and computer resources. To address these limitations, we propose a novel image-to-image translation method that generates images of the target domain by finetuning a stylegan2 pretrained model. The stylegan2 model is suitable for unsupervised I2I translation on unbalanced datasets; it is highly stable, produces realistic images, and even learns properly from limited data when applied with simple fine-tuning techniques. Thus, in this paper, we propose new methods to preserve the structure of the source images and generate realistic images in the target domain. The code and results are available at https://github.com/happy-jihye/Cartoon-StyleGan2



### Evaluation of a Region Proposal Architecture for Multi-task Document Layout Analysis
- **Arxiv ID**: http://arxiv.org/abs/2106.11797v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11797v1)
- **Published**: 2021-06-22 14:07:27+00:00
- **Updated**: 2021-06-22 14:07:27+00:00
- **Authors**: Lorenzo Quirós, Enrique Vidal
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically recognizing the layout of handwritten documents is an important step towards useful extraction of information from those documents. The most common application is to feed downstream applications such as automatic text recognition and keyword spotting; however, the recognition of the layout also helps to establish relationships between elements in the document which allows to enrich the information that can be extracted. Most of the modern document layout analysis systems are designed to address only one part of the document layout problem, namely: baseline detection or region segmentation. In contrast, we evaluate the effectiveness of the Mask-RCNN architecture to address the problem of baseline detection and region segmentation in an integrated manner. We present experimental results on two handwritten text datasets and one handwritten music dataset. The analyzed architecture yields promising results, outperforming state-of-the-art techniques in all three datasets.



### NuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles
- **Arxiv ID**: http://arxiv.org/abs/2106.11810v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11810v4)
- **Published**: 2021-06-22 14:24:55+00:00
- **Updated**: 2022-02-04 02:50:02+00:00
- **Authors**: Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom, Sammy Omari
- **Comment**: Minor updates to Related Work
- **Journal**: None
- **Summary**: In this work, we propose the world's first closed-loop ML-based planning benchmark for autonomous driving. While there is a growing body of ML-based motion planners, the lack of established datasets and metrics has limited the progress in this area. Existing benchmarks for autonomous vehicle motion prediction have focused on short-term motion forecasting, rather than long-term planning. This has led previous works to use open-loop evaluation with L2-based metrics, which are not suitable for fairly evaluating long-term planning. Our benchmark overcomes these limitations by introducing a large-scale driving dataset, lightweight closed-loop simulator, and motion-planning-specific metrics. We provide a high-quality dataset with 1500h of human driving data from 4 cities across the US and Asia with widely varying traffic patterns (Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop simulation framework with reactive agents and provide a large set of both general and scenario-specific planning metrics. We plan to release the dataset at NeurIPS 2021 and organize benchmark challenges starting in early 2022.



### Data Augmentation for Opcode Sequence Based Malware Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.11821v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11821v2)
- **Published**: 2021-06-22 14:36:35+00:00
- **Updated**: 2022-03-31 13:24:06+00:00
- **Authors**: Niall McLaughlin, Jesus Martinez del Rincon
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: In this paper we study data augmentation for opcode sequence based Android malware detection. Data augmentation has been successfully used in many areas of deep-learning to significantly improve model performance. Typically, data augmentation simulates realistic variations in data to increase the apparent diversity of the training-set. However, for opcode-based malware analysis it is not immediately clear how to apply data augmentation. Hence we first study the use of fixed transformations, then progress to adaptive methods. We propose a novel data augmentation method -- Self-Embedding Language Model Augmentation -- that uses a malware detection network's own opcode embedding layer to measure opcode similarity for adaptive augmentation. To the best of our knowledge this is the first paper to carry out a systematic study of different augmentation methods for opcode sequence based Android malware classification.



### Domain-Smoothing Network for Zero-Shot Sketch-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2106.11841v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11841v1)
- **Published**: 2021-06-22 14:58:08+00:00
- **Updated**: 2021-06-22 14:58:08+00:00
- **Authors**: Zhipeng Wang, Hao Wang, Jiexi Yan, Aming Wu, Cheng Deng
- **Comment**: Accepted to IJCAI 2021
- **Journal**: None
- **Summary**: Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) is a novel cross-modal retrieval task, where abstract sketches are used as queries to retrieve natural images under zero-shot scenario. Most existing methods regard ZS-SBIR as a traditional classification problem and employ a cross-entropy or triplet-based loss to achieve retrieval, which neglect the problems of the domain gap between sketches and natural images and the large intra-class diversity in sketches. Toward this end, we propose a novel Domain-Smoothing Network (DSN) for ZS-SBIR. Specifically, a cross-modal contrastive method is proposed to learn generalized representations to smooth the domain gap by mining relations with additional augmented samples. Furthermore, a category-specific memory bank with sketch features is explored to reduce intra-class diversity in the sketch domain. Extensive experiments demonstrate that our approach notably outperforms the state-of-the-art methods in both Sketchy and TU-Berlin datasets. Our source code is publicly available at https://github.com/haowang1992/DSN.



### HybVIO: Pushing the Limits of Real-time Visual-inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/2106.11857v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11857v3)
- **Published**: 2021-06-22 15:21:33+00:00
- **Updated**: 2021-11-25 13:33:44+00:00
- **Authors**: Otto Seiskari, Pekka Rantalankila, Juho Kannala, Jerry Ylilammi, Esa Rahtu, Arno Solin
- **Comment**: 2022 IEEE Winter Conference on Applications of Computer Vision (WACV)
- **Journal**: None
- **Summary**: We present HybVIO, a novel hybrid approach for combining filtering-based visual-inertial odometry (VIO) with optimization-based SLAM. The core of our method is highly robust, independent VIO with improved IMU bias modeling, outlier rejection, stationarity detection, and feature track selection, which is adjustable to run on embedded hardware. Long-term consistency is achieved with a loosely-coupled SLAM module. In academic benchmarks, our solution yields excellent performance in all categories, especially in the real-time use case, where we outperform the current state-of-the-art. We also demonstrate the feasibility of VIO for vehicular tracking on consumer-grade hardware using a custom dataset, and show good performance in comparison to current commercial VISLAM alternatives. An open-source implementation of the HybVIO method is available at https://github.com/SpectacularAI/HybVIO



### MEAL: Manifold Embedding-based Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.11858v3
- **DOI**: 10.1109/ICCVW54120.2021.00120
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11858v3)
- **Published**: 2021-06-22 15:22:56+00:00
- **Updated**: 2021-09-10 07:11:19+00:00
- **Authors**: Deepthi Sreenivasaiah, Johannes Otterbach, Thomas Wollmann
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV) Workshops 2021
- **Summary**: Image segmentation is a common and challenging task in autonomous driving. Availability of sufficient pixel-level annotations for the training data is a hurdle. Active learning helps learning from small amounts of data by suggesting the most promising samples for labeling. In this work, we propose a new pool-based method for active learning, which proposes promising patches extracted from full image, in each acquisition step. The problem is framed in an exploration-exploitation framework by combining an embedding based on Uniform Manifold Approximation to model representativeness with entropy as uncertainty measure to model informativeness. We applied our proposed method to the autonomous driving datasets CamVid and Cityscapes and performed a quantitative comparison with state-of-the-art baselines. We find that our active learning method achieves better performance compared to previous methods.



### Euro-PVI: Pedestrian Vehicle Interactions in Dense Urban Centers
- **Arxiv ID**: http://arxiv.org/abs/2106.12442v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2106.12442v1)
- **Published**: 2021-06-22 15:40:21+00:00
- **Updated**: 2021-06-22 15:40:21+00:00
- **Authors**: Apratim Bhattacharyya, Daniel Olmeda Reino, Mario Fritz, Bernt Schiele
- **Comment**: To appear at CVPR 2021
- **Journal**: None
- **Summary**: Accurate prediction of pedestrian and bicyclist paths is integral to the development of reliable autonomous vehicles in dense urban environments. The interactions between vehicle and pedestrian or bicyclist have a significant impact on the trajectories of traffic participants e.g. stopping or turning to avoid collisions. Although recent datasets and trajectory prediction approaches have fostered the development of autonomous vehicles yet the amount of vehicle-pedestrian (bicyclist) interactions modeled are sparse. In this work, we propose Euro-PVI, a dataset of pedestrian and bicyclist trajectories. In particular, our dataset caters more diverse and complex interactions in dense urban scenarios compared to the existing datasets. To address the challenges in predicting future trajectories with dense interactions, we develop a joint inference model that learns an expressive multi-modal shared latent space across agents in the urban scene. This enables our Joint-$\beta$-cVAE approach to better model the distribution of future trajectories. We achieve state of the art results on the nuScenes and Euro-PVI datasets demonstrating the importance of capturing interactions between ego-vehicle and pedestrians (bicyclists) for accurate predictions.



### A Latent Transformer for Disentangled Face Editing in Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/2106.11895v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11895v2)
- **Published**: 2021-06-22 16:04:30+00:00
- **Updated**: 2021-08-17 14:53:36+00:00
- **Authors**: Xu Yao, Alasdair Newson, Yann Gousseau, Pierre Hellier
- **Comment**: Accepted by ICCV 2021. Source codes are available at
  https://github.com/InterDigitalInc/latent-transformer
- **Journal**: None
- **Summary**: High quality facial image editing is a challenging problem in the movie post-production industry, requiring a high degree of control and identity preservation. Previous works that attempt to tackle this problem may suffer from the entanglement of facial attributes and the loss of the person's identity. Furthermore, many algorithms are limited to a certain task. To tackle these limitations, we propose to edit facial attributes via the latent space of a StyleGAN generator, by training a dedicated latent transformation network and incorporating explicit disentanglement and identity preservation terms in the loss function. We further introduce a pipeline to generalize our face editing to videos. Our model achieves a disentangled, controllable, and identity-preserving facial attribute editing, even in the challenging case of real (i.e., non-synthetic) images and videos. We conduct extensive experiments on image and video datasets and show that our model outperforms other state-of-the-art methods in visual quality and quantitative evaluation. Source codes are available at https://github.com/InterDigitalInc/latent-transformer.



### PALMAR: Towards Adaptive Multi-inhabitant Activity Recognition in Point-Cloud Technology
- **Arxiv ID**: http://arxiv.org/abs/2106.11902v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2106.11902v2)
- **Published**: 2021-06-22 16:17:50+00:00
- **Updated**: 2022-12-03 02:22:05+00:00
- **Authors**: Mohammad Arif Ul Alam, Md Mahmudur Rahman, Jared Q Widberg
- **Comment**: Accepted in IEEE International Conference on Computer Communications
  2021
- **Journal**: None
- **Summary**: With the advancement of deep neural networks and computer vision-based Human Activity Recognition, employment of Point-Cloud Data technologies (LiDAR, mmWave) has seen a lot interests due to its privacy preserving nature. Given the high promise of accurate PCD technologies, we develop, PALMAR, a multiple-inhabitant activity recognition system by employing efficient signal processing and novel machine learning techniques to track individual person towards developing an adaptive multi-inhabitant tracking and HAR system. More specifically, we propose (i) a voxelized feature representation-based real-time PCD fine-tuning method, (ii) efficient clustering (DBSCAN and BIRCH), Adaptive Order Hidden Markov Model based multi-person tracking and crossover ambiguity reduction techniques and (iii) novel adaptive deep learning-based domain adaptation technique to improve the accuracy of HAR in presence of data scarcity and diversity (device, location and population diversity). We experimentally evaluate our framework and systems using (i) a real-time PCD collected by three devices (3D LiDAR and 79 GHz mmWave) from 6 participants, (ii) one publicly available 3D LiDAR activity data (28 participants) and (iii) an embedded hardware prototype system which provided promising HAR performances in multi-inhabitants (96%) scenario with a 63% improvement of multi-person tracking than state-of-art framework without losing significant system performances in the edge computing device.



### Residual Networks as Flows of Velocity Fields for Diffeomorphic Time Series Alignment
- **Arxiv ID**: http://arxiv.org/abs/2106.11911v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11911v1)
- **Published**: 2021-06-22 16:38:48+00:00
- **Updated**: 2021-06-22 16:38:48+00:00
- **Authors**: Hao Huang, Boulbaba Ben Amor, Xichan Lin, Fan Zhu, Yi Fang
- **Comment**: 19 pages
- **Journal**: None
- **Summary**: Non-linear (large) time warping is a challenging source of nuisance in time-series analysis. In this paper, we propose a novel diffeomorphic temporal transformer network for both pairwise and joint time-series alignment. Our ResNet-TW (Deep Residual Network for Time Warping) tackles the alignment problem by compositing a flow of incremental diffeomorphic mappings. Governed by the flow equation, our Residual Network (ResNet) builds smooth, fluid and regular flows of velocity fields and consequently generates smooth and invertible transformations (i.e. diffeomorphic warping functions). Inspired by the elegant Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework, the final transformation is built by the flow of time-dependent vector fields which are none other than the building blocks of our Residual Network. The latter is naturally viewed as an Eulerian discretization schema of the flow equation (an ODE). Once trained, our ResNet-TW aligns unseen data by a single inexpensive forward pass. As we show in experiments on both univariate (84 datasets from UCR archive) and multivariate time-series (MSR Action-3D, Florence-3D and MSR Daily Activity), ResNet-TW achieves competitive performance in joint alignment and classification.



### Enhanced Separable Disentanglement for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2106.11915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11915v1)
- **Published**: 2021-06-22 16:50:53+00:00
- **Updated**: 2021-06-22 16:50:53+00:00
- **Authors**: Youshan Zhang, Brian D. Davison
- **Comment**: ICIP 2021
- **Journal**: None
- **Summary**: Domain adaptation aims to mitigate the domain gap when transferring knowledge from an existing labeled domain to a new domain. However, existing disentanglement-based methods do not fully consider separation between domain-invariant and domain-specific features, which means the domain-invariant features are not discriminative. The reconstructed features are also not sufficiently used during training. In this paper, we propose a novel enhanced separable disentanglement (ESD) model. We first employ a disentangler to distill domain-invariant and domain-specific features. Then, we apply feature separation enhancement processes to minimize contamination between domain-invariant and domain-specific features. Finally, our model reconstructs complete feature vectors, which are used for further disentanglement during the training phase. Extensive experiments from three benchmark datasets outperform state-of-the-art methods, especially on challenging cross-domain tasks.



### G-VAE, a Geometric Convolutional VAE for ProteinStructure Generation
- **Arxiv ID**: http://arxiv.org/abs/2106.11920v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11920v1)
- **Published**: 2021-06-22 16:52:48+00:00
- **Updated**: 2021-06-22 16:52:48+00:00
- **Authors**: Hao Huang, Boulbaba Ben Amor, Xichan Lin, Fan Zhu, Yi Fang
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Analyzing the structure of proteins is a key part of understanding their functions and thus their role in biology at the molecular level. In addition, design new proteins in a methodical way is a major engineering challenge. In this work, we introduce a joint geometric-neural networks approach for comparing, deforming and generating 3D protein structures. Viewing protein structures as 3D open curves, we adopt the Square Root Velocity Function (SRVF) representation and leverage its suitable geometric properties along with Deep Residual Networks (ResNets) for a joint registration and comparison. Our ResNets handle better large protein deformations while being more computationally efficient. On top of the mathematical framework, we further design a Geometric Variational Auto-Encoder (G-VAE), that once trained, maps original, previously unseen structures, into a low-dimensional (latent) hyper-sphere. Motivated by the spherical structure of the pre-shape space, we naturally adopt the von Mises-Fisher (vMF) distribution to model our hidden variables. We test the effectiveness of our models by generating novel protein structures and predicting completions of corrupted protein structures. Experimental results show that our method is able to generate plausible structures, different from the structures in the training data.



### Not All Labels Are Equal: Rationalizing The Labeling Costs for Training Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2106.11921v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11921v2)
- **Published**: 2021-06-22 16:53:09+00:00
- **Updated**: 2021-11-29 16:51:31+00:00
- **Authors**: Ismail Elezi, Zhiding Yu, Anima Anandkumar, Laura Leal-Taixe, Jose M. Alvarez
- **Comment**: Includes supplementary material
- **Journal**: None
- **Summary**: Deep neural networks have reached high accuracy on object detection but their success hinges on large amounts of labeled data. To reduce the labels dependency, various active learning strategies have been proposed, typically based on the confidence of the detector. However, these methods are biased towards high-performing classes and can lead to acquired datasets that are not good representatives of the testing set data. In this work, we propose a unified framework for active learning, that considers both the uncertainty and the robustness of the detector, ensuring that the network performs well in all classes. Furthermore, our method leverages auto-labeling to suppress a potential distribution drift while boosting the performance of the model. Experiments on PASCAL VOC07+12 and MS-COCO show that our method consistently outperforms a wide range of active learning methods, yielding up to a 7.7% improvement in mAP, or up to 82% reduction in labeling cost. Code will be released upon acceptance of the paper.



### On the importance of cross-task features for class-incremental learning
- **Arxiv ID**: http://arxiv.org/abs/2106.11930v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.11930v3)
- **Published**: 2021-06-22 17:03:15+00:00
- **Updated**: 2021-09-29 07:28:32+00:00
- **Authors**: Albin Soutif--Cormerais, Marc Masana, Joost Van de Weijer, Bartłomiej Twardowski
- **Comment**: Accepted at the Theory and Foundation of Continual Learning Workshop,
  International Conference on Machine Learning (ICML) 2021. Supplementary
  material included
- **Journal**: None
- **Summary**: In class-incremental learning, an agent with limited resources needs to learn a sequence of classification tasks, forming an ever growing classification problem, with the constraint of not being able to access data from previous tasks. The main difference with task-incremental learning, where a task-ID is available at inference time, is that the learner also needs to perform cross-task discrimination, i.e. distinguish between classes that have not been seen together. Approaches to tackle this problem are numerous and mostly make use of an external memory (buffer) of non-negligible size. In this paper, we ablate the learning of cross-task features and study its influence on the performance of basic replay strategies used for class-IL. We also define a new forgetting measure for class-incremental learning, and see that forgetting is not the principal cause of low performance. Our experimental results show that future algorithms for class-incremental learning should not only prevent forgetting, but also aim to improve the quality of the cross-task features, and the knowledge transfer between tasks. This is especially important when tasks contain limited amount of data.



### RootPainter3D: Interactive-machine-learning enables rapid and accurate contouring for radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/2106.11942v1
- **DOI**: 10.1002/mp.15353
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.11942v1)
- **Published**: 2021-06-22 17:26:58+00:00
- **Updated**: 2021-06-22 17:26:58+00:00
- **Authors**: Abraham George Smith, Jens Petersen, Cynthia Terrones-Campos, Anne Kiil Berthelsen, Nora Jarrett Forbes, Sune Darkner, Lena Specht, Ivan Richter Vogelius
- **Comment**: None
- **Journal**: None
- **Summary**: Organ-at-risk contouring is still a bottleneck in radiotherapy, with many deep learning methods falling short of promised results when evaluated on clinical data. We investigate the accuracy and time-savings resulting from the use of an interactive-machine-learning method for an organ-at-risk contouring task. We compare the method to the Eclipse contouring software and find strong agreement with manual delineations, with a dice score of 0.95. The annotations created using corrective-annotation also take less time to create as more images are annotated, resulting in substantial time savings compared to manual methods, with hearts that take 2 minutes and 2 seconds to delineate on average, after 923 images have been delineated, compared to 7 minutes and 1 seconds when delineating manually. Our experiment demonstrates that interactive-machine-learning with corrective-annotation provides a fast and accessible way for non computer-scientists to train deep-learning models to segment their own structures of interest as part of routine clinical workflows.   Source code is available at \href{https://github.com/Abe404/RootPainter3D}{this HTTPS URL}.



### MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images
- **Arxiv ID**: http://arxiv.org/abs/2106.11944v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11944v2)
- **Published**: 2021-06-22 17:30:12+00:00
- **Updated**: 2022-01-20 13:32:44+00:00
- **Authors**: Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas Geiger, Siyu Tang
- **Comment**: NeurIPS 2021 final camera-ready revision. Project page:
  https://neuralbodies.github.io/metavatar/
- **Journal**: None
- **Summary**: In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune, compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames.



### Diabetic Retinopathy Detection using Ensemble Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2106.12545v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.12545v1)
- **Published**: 2021-06-22 17:36:08+00:00
- **Updated**: 2021-06-22 17:36:08+00:00
- **Authors**: Israa Odeh, Mouhammd Alkasassbeh, Mohammad Alauthman
- **Comment**: None
- **Journal**: None
- **Summary**: Diabetic Retinopathy (DR) is among the worlds leading vision loss causes in diabetic patients. DR is a microvascular disease that affects the eye retina, which causes vessel blockage and therefore cuts the main source of nutrition for the retina tissues. Treatment for this visual disorder is most effective when it is detected in its earliest stages, as severe DR can result in irreversible blindness. Nonetheless, DR identification requires the expertise of Ophthalmologists which is often expensive and time-consuming. Therefore, automatic detection systems were introduced aiming to facilitate the identification process, making it available globally in a time and cost-efficient manner. However, due to the limited reliable datasets and medical records for this particular eye disease, the obtained predictions accuracies were relatively unsatisfying for eye specialists to rely on them as diagnostic systems. Thus, we explored an ensemble-based learning strategy, merging a substantial selection of well-known classification algorithms in one sophisticated diagnostic model. The proposed framework achieved the highest accuracy rates among all other common classification algorithms in the area. 4 subdatasets were generated to contain the top 5 and top 10 features of the Messidor dataset, selected by InfoGainEval. and WrapperSubsetEval., accuracies of 70.7% and 75.1% were achieved on the InfoGainEval. top 5 and original dataset respectively. The results imply the impressive performance of the subdataset, which significantly conduces to a less complex classification process



### Differentiable Programming of Reaction-Diffusion Patterns
- **Arxiv ID**: http://arxiv.org/abs/2107.06862v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.06862v1)
- **Published**: 2021-06-22 17:38:34+00:00
- **Updated**: 2021-06-22 17:38:34+00:00
- **Authors**: Alexander Mordvintsev, Ettore Randazzo, Eyvind Niklasson
- **Comment**: ALIFE 2021
- **Journal**: None
- **Summary**: Reaction-Diffusion (RD) systems provide a computational framework that governs many pattern formation processes in nature. Current RD system design practices boil down to trial-and-error parameter search. We propose a differentiable optimization method for learning the RD system parameters to perform example-based texture synthesis on a 2D plane. We do this by representing the RD system as a variant of Neural Cellular Automata and using task-specific differentiable loss functions. RD systems generated by our method exhibit robust, non-trivial 'life-like' behavior.



### Unsupervised Object-Level Representation Learning from Scene Images
- **Arxiv ID**: http://arxiv.org/abs/2106.11952v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11952v2)
- **Published**: 2021-06-22 17:51:24+00:00
- **Updated**: 2021-12-03 13:51:38+00:00
- **Authors**: Jiahao Xie, Xiaohang Zhan, Ziwei Liu, Yew Soon Ong, Chen Change Loy
- **Comment**: NeurIPS 2021. Project page: https://www.mmlab-ntu.com/project/orl/
  Code: https://github.com/Jiahao000/ORL
- **Journal**: None
- **Summary**: Contrastive self-supervised learning has largely narrowed the gap to supervised pre-training on ImageNet. However, its success highly relies on the object-centric priors of ImageNet, i.e., different augmented views of the same image correspond to the same object. Such a heavily curated constraint becomes immediately infeasible when pre-trained on more complex scene images with many objects. To overcome this limitation, we introduce Object-level Representation Learning (ORL), a new self-supervised learning framework towards scene images. Our key insight is to leverage image-level self-supervised pre-training as the prior to discover object-level semantic correspondence, thus realizing object-level representation learning from scene images. Extensive experiments on COCO show that ORL significantly improves the performance of self-supervised learning on scene images, even surpassing supervised ImageNet pre-training on several downstream tasks. Furthermore, ORL improves the downstream performance when more unlabeled scene images are available, demonstrating its great potential of harnessing unlabeled data in the wild. We hope our approach can motivate future research on more general-purpose unsupervised representation learning from scene data.



### Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.11958v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.11958v2)
- **Published**: 2021-06-22 17:57:24+00:00
- **Updated**: 2021-11-30 22:29:39+00:00
- **Authors**: Lei Ke, Xia Li, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu
- **Comment**: NeurIPS 2021, Spotlight; Our code and video resources are available
  at http://vis.xyz/pub/pcan
- **Journal**: None
- **Summary**: Multiple object tracking and segmentation requires detecting, tracking, and segmenting objects belonging to a set of given classes. Most approaches only exploit the temporal dimension to address the association problem, while relying on single frame predictions for the segmentation mask itself. We propose Prototypical Cross-Attention Network (PCAN), capable of leveraging rich spatio-temporal information for online multiple object tracking and segmentation. PCAN first distills a space-time memory into a set of prototypes and then employs cross-attention to retrieve rich information from the past frames. To segment each object, PCAN adopts a prototypical appearance module to learn a set of contrastive foreground and background prototypes, which are then propagated over time. Extensive experiments demonstrate that PCAN outperforms current video instance tracking and segmentation competition winners on both Youtube-VIS and BDD100K datasets, and shows efficacy to both one-stage and two-stage segmentation frameworks. Code and video resources are available at http://vis.xyz/pub/pcan.



### Tracking Instances as Queries
- **Arxiv ID**: http://arxiv.org/abs/2106.11963v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2106.11963v2)
- **Published**: 2021-06-22 17:59:12+00:00
- **Updated**: 2021-06-23 15:02:24+00:00
- **Authors**: Shusheng Yang, Yuxin Fang, Xinggang Wang, Yu Li, Ying Shan, Bin Feng, Wenyu Liu
- **Comment**: Preprint. Work in progress
- **Journal**: CVPR 2021 Workshop. 2nd Place Solution for YouTube-VOS Challenge
  2021: Video Instance Segmentation
- **Summary**: Recently, query based deep networks catch lots of attention owing to their end-to-end pipeline and competitive results on several fundamental computer vision tasks, such as object detection, semantic segmentation, and instance segmentation. However, how to establish a query based video instance segmentation (VIS) framework with elegant architecture and strong performance remains to be settled. In this paper, we present \textbf{QueryTrack} (i.e., tracking instances as queries), a unified query based VIS framework fully leveraging the intrinsic one-to-one correspondence between instances and queries in QueryInst. The proposed method obtains 52.7 / 52.3 AP on YouTube-VIS-2019 / 2021 datasets, which wins the 2-nd place in the YouTube-VIS Challenge at CVPR 2021 \textbf{with a single online end-to-end model, single scale testing \& modest amount of training data}. We also provide QueryTrack-ResNet-50 baseline results on YouTube-VIS-2021 val set as references for the VIS community.



### P2T: Pyramid Pooling Transformer for Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2106.12011v6
- **DOI**: 10.1109/TPAMI.2022.3202765
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.12011v6)
- **Published**: 2021-06-22 18:28:52+00:00
- **Updated**: 2022-08-31 07:11:00+00:00
- **Authors**: Yu-Huan Wu, Yun Liu, Xin Zhan, Ming-Ming Cheng
- **Comment**: Accepted by IEEE TPAMI
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2022
- **Summary**: Recently, the vision transformer has achieved great success by pushing the state-of-the-art of various vision tasks. One of the most challenging problems in the vision transformer is that the large sequence length of image tokens leads to high computational cost (quadratic complexity). A popular solution to this problem is to use a single pooling operation to reduce the sequence length. This paper considers how to improve existing vision transformers, where the pooled feature extracted by a single pooling operation seems less powerful. To this end, we note that pyramid pooling has been demonstrated to be effective in various vision tasks owing to its powerful ability in context abstraction. However, pyramid pooling has not been explored in backbone network design. To bridge this gap, we propose to adapt pyramid pooling to Multi-Head Self-Attention (MHSA) in the vision transformer, simultaneously reducing the sequence length and capturing powerful contextual features. Plugged with our pooling-based MHSA, we build a universal vision transformer backbone, dubbed Pyramid Pooling Transformer (P2T). Extensive experiments demonstrate that, when applied P2T as the backbone network, it shows substantial superiority in various vision tasks such as image classification, semantic segmentation, object detection, and instance segmentation, compared to previous CNN- and transformer-based networks. The code will be released at https://github.com/yuhuan-wu/P2T.



### Transfer Learning of Deep Spatiotemporal Networks to Model Arbitrarily Long Videos of Seizures
- **Arxiv ID**: http://arxiv.org/abs/2106.12014v2
- **DOI**: 10.1007/978-3-030-87240-3_32
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.12014v2)
- **Published**: 2021-06-22 18:40:31+00:00
- **Updated**: 2021-08-05 11:01:53+00:00
- **Authors**: Fernando Pérez-García, Catherine Scott, Rachel Sparks, Beate Diehl, Sébastien Ourselin
- **Comment**: Accepted at the 24th International Conference on Medical Image
  Computing and Computer Assisted Intervention (MICCAI 2021)
- **Journal**: Medical Image Computing and Computer Assisted Intervention -
  MICCAI 2021. Lecture Notes in Computer Science. Springer, Cham
- **Summary**: Detailed analysis of seizure semiology, the symptoms and signs which occur during a seizure, is critical for management of epilepsy patients. Inter-rater reliability using qualitative visual analysis is often poor for semiological features. Therefore, automatic and quantitative analysis of video-recorded seizures is needed for objective assessment.   We present GESTURES, a novel architecture combining convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to learn deep representations of arbitrarily long videos of epileptic seizures.   We use a spatiotemporal CNN (STCNN) pre-trained on large human action recognition (HAR) datasets to extract features from short snippets (approx. 0.5 s) sampled from seizure videos. We then train an RNN to learn seizure-level representations from the sequence of features.   We curated a dataset of seizure videos from 68 patients and evaluated GESTURES on its ability to classify seizures into focal onset seizures (FOSs) (N = 106) vs. focal to bilateral tonic-clonic seizures (TCSs) (N = 77), obtaining an accuracy of 98.9% using bidirectional long short-term memory (BLSTM) units.   We demonstrate that an STCNN trained on a HAR dataset can be used in combination with an RNN to accurately represent arbitrarily long videos of seizures. GESTURES can provide accurate seizure classification by modeling sequences of semiologies.



### On Matrix Factorizations in Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/2106.12016v1
- **DOI**: None
- **Categories**: **cs.CV**, 68P99, 68T10, 62H30
- **Links**: [PDF](http://arxiv.org/pdf/2106.12016v1)
- **Published**: 2021-06-22 18:42:44+00:00
- **Updated**: 2021-06-22 18:42:44+00:00
- **Authors**: Reeshad Arian, Keaton Hamm
- **Comment**: 13 pages plus 4 pages of tables
- **Journal**: None
- **Summary**: This article explores subspace clustering algorithms using CUR decompositions, and examines the effect of various hyperparameters in these algorithms on clustering performance on two real-world benchmark datasets, the Hopkins155 motion segmentation dataset and the Yale face dataset. Extensive experiments are done for a variety of sampling methods and oversampling parameters for these datasets, and some guidelines for parameter choices are given for practical applications.



### Team PyKale (xy9) Submission to the EPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2106.12023v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.12023v2)
- **Published**: 2021-06-22 19:17:03+00:00
- **Updated**: 2021-08-09 16:06:11+00:00
- **Authors**: Xianyuan Liu, Raivo Koot, Shuo Zhou, Tao Lei, Haiping Lu
- **Comment**: This paper is not good enough for publication--no need to occupy
  resources here
- **Journal**: None
- **Summary**: This report describes the technical details of our submission to the EPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action Recognition. The EPIC-Kitchens dataset is more difficult than other video domain adaptation datasets due to multi-tasks with more modalities. Firstly, to participate in the challenge, we employ a transformer to capture the spatial information from each modality. Secondly, we employ a temporal attention module to model temporal-wise inter-dependency. Thirdly, we employ the adversarial domain adaptation network to learn the general features between labeled source and unlabeled target domain. Finally, we incorporate multiple modalities to improve the performance by a three-stream network with late fusion. Our network achieves the comparable performance with the state-of-the-art baseline T$A^3$N and outperforms the baseline on top-1 accuracy for verb class and top-5 accuracies for all three tasks which are verb, noun and action. Under the team name xy9, our submission achieved 5th place in terms of top-1 accuracy for verb class and all top-5 accuracies.



### The Neurally-Guided Shape Parser: Grammar-based Labeling of 3D Shape Regions with Approximate Inference
- **Arxiv ID**: http://arxiv.org/abs/2106.12026v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.12026v3)
- **Published**: 2021-06-22 19:26:01+00:00
- **Updated**: 2022-03-22 19:27:22+00:00
- **Authors**: R. Kenny Jones, Aalia Habib, Rana Hanocka, Daniel Ritchie
- **Comment**: CVPR 2022; https://github.com/rkjones4/NGSP
- **Journal**: None
- **Summary**: We propose the Neurally-Guided Shape Parser (NGSP), a method that learns how to assign fine-grained semantic labels to regions of a 3D shape. NGSP solves this problem via MAP inference, modeling the posterior probability of a label assignment conditioned on an input shape with a learned likelihood function. To make this search tractable, NGSP employs a neural guide network that learns to approximate the posterior. NGSP finds high-probability label assignments by first sampling proposals with the guide network and then evaluating each proposal under the full likelihood. We evaluate NGSP on the task of fine-grained semantic segmentation of manufactured 3D shapes from PartNet, where shapes have been decomposed into regions that correspond to part instance over-segmentations. We find that NGSP delivers significant performance improvements over comparison methods that (i) use regions to group per-point predictions, (ii) use regions as a self-supervisory signal or (iii) assign labels to regions under alternative formulations. Further, we show that NGSP maintains strong performance even with limited labeled data or noisy input shape regions. Finally, we demonstrate that NGSP can be directly applied to CAD shapes found in online repositories and validate its effectiveness with a perceptual study.



### Joint Learning of Portrait Intrinsic Decomposition and Relighting
- **Arxiv ID**: http://arxiv.org/abs/2106.15305v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15305v1)
- **Published**: 2021-06-22 19:40:22+00:00
- **Updated**: 2021-06-22 19:40:22+00:00
- **Authors**: Mona Zehni, Shaona Ghosh, Krishna Sridhar, Sethu Raman
- **Comment**: None
- **Journal**: None
- **Summary**: Inverse rendering is the problem of decomposing an image into its intrinsic components, i.e. albedo, normal and lighting. To solve this ill-posed problem from single image, state-of-the-art methods in shape from shading mostly resort to supervised training on all the components on either synthetic or real datasets. Here, we propose a new self-supervised training paradigm that 1) reduces the need for full supervision on the decomposition task and 2) takes into account the relighting task. We introduce new self-supervised loss terms that leverage the consistencies between multi-lit images (images of the same scene under different illuminations). Our approach is applicable to multi-lit datasets. We apply our training approach in two settings: 1) train on a mixture of synthetic and real data, 2) train on real datasets with limited supervision. We show-case the effectiveness of our training paradigm on both intrinsic decomposition and relighting and demonstrate how the model struggles in both tasks without the self-supervised loss terms in limited supervision settings. We provide results of comprehensive experiments on SfSNet, CelebA and Photoface datasets and verify the performance of our approach on images in the wild.



### Volume Rendering of Neural Implicit Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2106.12052v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.12052v2)
- **Published**: 2021-06-22 20:23:16+00:00
- **Updated**: 2021-12-01 18:48:10+00:00
- **Authors**: Lior Yariv, Jiatao Gu, Yoni Kasten, Yaron Lipman
- **Comment**: None
- **Journal**: None
- **Summary**: Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace's cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two.



### Automatic Head Overcoat Thickness Measure with NASNet-Large-Decoder Net
- **Arxiv ID**: http://arxiv.org/abs/2106.12054v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.12054v1)
- **Published**: 2021-06-22 20:53:58+00:00
- **Updated**: 2021-06-22 20:53:58+00:00
- **Authors**: Youshan Zhang, Brian D. Davison, Vivien W. Talghader, Zhiyu Chen, Zhiyong Xiao, Gary J. Kunkel
- **Comment**: None
- **Journal**: None
- **Summary**: Transmission electron microscopy (TEM) is one of the primary tools to show microstructural characterization of materials as well as film thickness. However, manual determination of film thickness from TEM images is time-consuming as well as subjective, especially when the films in question are very thin and the need for measurement precision is very high. Such is the case for head overcoat (HOC) thickness measurements in the magnetic hard disk drive industry. It is therefore necessary to develop software to automatically measure HOC thickness. In this paper, for the first time, we propose a HOC layer segmentation method using NASNet-Large as an encoder and then followed by a decoder architecture, which is one of the most commonly used architectures in deep learning for image segmentation. To further improve segmentation results, we are the first to propose a post-processing layer to remove irrelevant portions in the segmentation result. To measure the thickness of the segmented HOC layer, we propose a regressive convolutional neural network (RCNN) model as well as orthogonal thickness calculation methods. Experimental results demonstrate a higher dice score for our model which has lower mean squared error and outperforms current state-of-the-art manual measurement.



### Towards Consistent Predictive Confidence through Fitted Ensembles
- **Arxiv ID**: http://arxiv.org/abs/2106.12070v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2106.12070v1)
- **Published**: 2021-06-22 21:32:31+00:00
- **Updated**: 2021-06-22 21:32:31+00:00
- **Authors**: Navid Kardan, Ankit Sharma, Kenneth O. Stanley
- **Comment**: IJCNN 2021
- **Journal**: None
- **Summary**: Deep neural networks are behind many of the recent successes in machine learning applications. However, these models can produce overconfident decisions while encountering out-of-distribution (OOD) examples or making a wrong prediction. This inconsistent predictive confidence limits the integration of independently-trained learning models into a larger system. This paper introduces separable concept learning framework to realistically measure the performance of classifiers in presence of OOD examples. In this setup, several instances of a classifier are trained on different parts of a partition of the set of classes. Later, the performance of the combination of these models is evaluated on a separate test set. Unlike current OOD detection techniques, this framework does not require auxiliary OOD datasets and does not separate classification from detection performance. Furthermore, we present a new strong baseline for more consistent predictive confidence in deep models, called fitted ensembles, where overconfident predictions are rectified by transformed versions of the original classification task. Fitted ensembles can naturally detect OOD examples without requiring auxiliary data by observing contradicting predictions among its components. Experiments on MNIST, SVHN, CIFAR-10/100, and ImageNet show fitted ensemble significantly outperform conventional ensembles on OOD examples and are possible to scale.



### Reachability Analysis of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2106.12074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.12074v1)
- **Published**: 2021-06-22 21:42:00+00:00
- **Updated**: 2021-06-22 21:42:00+00:00
- **Authors**: Xiaodong Yang, Tomoya Yamaguchi, Hoang-Dung Tran, Bardh Hoxha, Taylor T Johnson, Danil Prokhorov
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks have been widely employed as an effective technique to handle complex and practical problems. However, one of the fundamental problems is the lack of formal methods to analyze their behavior. To address this challenge, we propose an approach to compute the exact reachable sets of a network given an input domain, where the reachable set is represented by the face lattice structure. Besides the computation of reachable sets, our approach is also capable of backtracking to the input domain given an output reachable set. Therefore, a full analysis of a network's behavior can be realized. In addition, an approach for fast analysis is also introduced, which conducts fast computation of reachable sets by considering selected sensitive neurons in each layer. The exact pixel-level reachability analysis method is evaluated on a CNN for the CIFAR10 dataset and compared to related works. The fast analysis method is evaluated over a CNN CIFAR10 dataset and VGG16 architecture for the ImageNet dataset.



### Face Identification Proficiency Test Designed Using Item Response Theory
- **Arxiv ID**: http://arxiv.org/abs/2106.15323v3
- **DOI**: 10.3758/s13428-023-02092-7
- **Categories**: **cs.CV**, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2106.15323v3)
- **Published**: 2021-06-22 22:37:32+00:00
- **Updated**: 2022-08-09 22:03:01+00:00
- **Authors**: Géraldine Jeckeln, Ying Hu, Jacqueline G. Cavazos, Amy N. Yates, Carina A. Hahn, Larry Tang, P. Jonathon Phillips, Alice J. O'Toole
- **Comment**: 20 pages (including references), 10 figures
- **Journal**: None
- **Summary**: Measures of face-identification proficiency are essential to ensure accurate and consistent performance by professional forensic face examiners and others who perform face-identification tasks in applied scenarios. Current proficiency tests rely on static sets of stimulus items, and so, cannot be administered validly to the same individual multiple times. To create a proficiency test, a large number of items of "known" difficulty must be assembled. Multiple tests of equal difficulty can be constructed then using subsets of items. We introduce the Triad Identity Matching (TIM) test and evaluate it using Item Response Theory (IRT). Participants view face-image "triads" (N=225) (two images of one identity, one image of a different identity) and select the different identity. In Experiment 1, university students (N=197) showed wide-ranging accuracy on the TIM test, and IRT modeling demonstrated that the TIM items span various difficulty levels. In Experiment 2, we used IRT-based item metrics to partition the test into subsets of specific difficulties. Simulations showed that subsets of the TIM items yielded reliable estimates of subject ability. In Experiments 3a and 3b, we found that the student-derived IRT model reliably evaluated the ability of non-student participants and that ability generalized across different test sessions. In Experiment 3c, we show that TIM test performance correlates with other common face-recognition tests. In summary, the TIM test provides a starting point for developing a framework that is flexible and calibrated to measure proficiency across various ability levels (e.g., professionals or populations with face-processing deficits).



