# Arxiv Papers in cs.CV on 2021-04-27
### Reconstruction of Convex Polytope Compositions from 3D Point-clouds
- **Arxiv ID**: http://arxiv.org/abs/2105.02956v1
- **DOI**: 10.5220/0010297100750084
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2105.02956v1)
- **Published**: 2021-04-27 00:14:55+00:00
- **Updated**: 2021-04-27 00:14:55+00:00
- **Authors**: Markus Friedrich, Pierre-Alain Fayolle
- **Comment**: In Proceedings of the 16th International Joint Conference on Computer
  Vision, Imaging and Computer Graphics Theory and Applications - Volume 1:
  GRAPP, ISBN 978-989-758-488-6 ISSN 2184-4321, pages 75-84
- **Journal**: None
- **Summary**: Reconstructing a composition (union) of convex polytopes that perfectly fits the corresponding input point-cloud is a hard optimization problem with interesting applications in reverse engineering and rigid body dynamics simulations. We propose a pipeline that first extracts a set of planes, then partitions the input point-cloud into weakly convex clusters and finally generates a set of convex polytopes as the intersection of fitted planes for each partition. Finding the best-fitting convex polytopes is formulated as a combinatorial optimization problem over the set of fitted planes and is solved using an Evolutionary Algorithm. For convex clustering, we employ two different methods and detail their strengths and weaknesses in a thorough evaluation based on multiple input data-sets.



### Detecting Personality and Emotion Traits in Crowds from Video Sequences
- **Arxiv ID**: http://arxiv.org/abs/2104.12927v1
- **DOI**: 10.1007/s00138-018-0979-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12927v1)
- **Published**: 2021-04-27 01:00:16+00:00
- **Updated**: 2021-04-27 01:00:16+00:00
- **Authors**: Rodolfo Migon Favaretto, Paulo Knob, Soraia Raupp Musse, Felipe Vilanova, Ângelo Brandelli Costa
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a methodology to detect personality and basic emotion characteristics of crowds in video sequences. Firstly, individuals are detected and tracked, then groups are recognized and characterized. Such information is then mapped to OCEAN dimensions, used to find out personality and emotion in videos, based on OCC emotion models. Although it is a clear challenge to validate our results with real life experiments, we evaluate our method with the available literature information regarding OCEAN values of different Countries and also emergent Personal distance among people. Hence, such analysis refer to cultural differences of each country too. Our results indicate that this model generates coherent information when compared to data provided in available literature, as shown in qualitative and quantitative results.



### If your data distribution shifts, use self-learning
- **Arxiv ID**: http://arxiv.org/abs/2104.12928v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.12928v3)
- **Published**: 2021-04-27 01:02:15+00:00
- **Updated**: 2022-11-29 16:48:48+00:00
- **Authors**: Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Gehler, Oliver Bringmann, Wieland Brendel, Matthias Bethge
- **Comment**: Web: https://domainadaptation.org/selflearning
- **Journal**: None
- **Summary**: We demonstrate that self-learning techniques like entropy minimization and pseudo-labeling are simple and effective at improving performance of a deployed computer vision model under systematic domain shifts. We conduct a wide range of large-scale experiments and show consistent improvements irrespective of the model architecture, the pre-training technique or the type of distribution shift. At the same time, self-learning is simple to use in practice because it does not require knowledge or access to the original training data or scheme, is robust to hyperparameter choices, is straight-forward to implement and requires only a few adaptation epochs. This makes self-learning techniques highly attractive for any practitioner who applies machine learning algorithms in the real world. We present state-of-the-art adaptation results on CIFAR10-C (8.5% error), ImageNet-C (22.0% mCE), ImageNet-R (17.4% error) and ImageNet-A (14.8% error), theoretically study the dynamics of self-supervised adaptation methods and propose a new classification dataset (ImageNet-D) which is challenging even with adaptation.



### Provably Convergent Learned Inexact Descent Algorithm for Low-Dose CT Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2104.12939v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.12939v1)
- **Published**: 2021-04-27 02:02:41+00:00
- **Updated**: 2021-04-27 02:02:41+00:00
- **Authors**: Qingchao Zhang, Mehrdad Alvandipour, Wenjun Xia, Yi Zhang, Xiaojing Ye, Yunmei Chen
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a provably convergent method, called Efficient Learned Descent Algorithm (ELDA), for low-dose CT (LDCT) reconstruction. ELDA is a highly interpretable neural network architecture with learned parameters and meanwhile retains convergence guarantee as classical optimization algorithms. To improve reconstruction quality, the proposed ELDA also employs a new non-local feature mapping and an associated regularizer. We compare ELDA with several state-of-the-art deep image methods, such as RED-CNN and Learned Primal-Dual, on a set of LDCT reconstruction problems. Numerical experiments demonstrate improvement of reconstruction quality using ELDA with merely 19 layers, suggesting the promising performance of ELDA in solution accuracy and parameter efficiency.



### Unsupervised Multi-Source Domain Adaptation for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2104.12961v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12961v1)
- **Published**: 2021-04-27 03:33:35+00:00
- **Updated**: 2021-04-27 03:33:35+00:00
- **Authors**: Zechen Bai, Zhigang Wang, Jian Wang, Di Hu, Errui Ding
- **Comment**: CVPR 2021 Oral
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) methods for person re-identification (re-ID) aim at transferring re-ID knowledge from labeled source data to unlabeled target data. Although achieving great success, most of them only use limited data from a single-source domain for model pre-training, making the rich labeled data insufficiently exploited. To make full use of the valuable labeled data, we introduce the multi-source concept into UDA person re-ID field, where multiple source datasets are used during training. However, because of domain gaps, simply combining different datasets only brings limited improvement. In this paper, we try to address this problem from two perspectives, \ie{} domain-specific view and domain-fusion view. Two constructive modules are proposed, and they are compatible with each other. First, a rectification domain-specific batch normalization (RDSBN) module is explored to simultaneously reduce domain-specific characteristics and increase the distinctiveness of person features. Second, a graph convolutional network (GCN) based multi-domain information fusion (MDIF) module is developed, which minimizes domain distances by fusing features of different domains. The proposed method outperforms state-of-the-art UDA person re-ID methods by a large margin, and even achieves comparable performance to the supervised approaches without any post-processing techniques.



### Multi-view Deep One-class Classification: A Systematic Exploration
- **Arxiv ID**: http://arxiv.org/abs/2104.13000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.13000v1)
- **Published**: 2021-04-27 06:44:07+00:00
- **Updated**: 2021-04-27 06:44:07+00:00
- **Authors**: Siqi Wang, Jiyuan Liu, Guang Yu, Xinwang Liu, Sihang Zhou, En Zhu, Yuexiang Yang, Jianping Yin
- **Comment**: None
- **Journal**: None
- **Summary**: One-class classification (OCC), which models one single positive class and distinguishes it from the negative class, has been a long-standing topic with pivotal application to realms like anomaly detection. As modern society often deals with massive high-dimensional complex data spawned by multiple sources, it is natural to consider OCC from the perspective of multi-view deep learning. However, it has not been discussed by the literature and remains an unexplored topic. Motivated by this blank, this paper makes four-fold contributions: First, to our best knowledge, this is the first work that formally identifies and formulates the multi-view deep OCC problem. Second, we take recent advances in relevant areas into account and systematically devise eleven different baseline solutions for multi-view deep OCC, which lays the foundation for research on multi-view deep OCC. Third, to remedy the problem that limited benchmark datasets are available for multi-view deep OCC, we extensively collect existing public data and process them into more than 30 new multi-view benchmark datasets via multiple means, so as to provide a publicly available evaluation platform for multi-view deep OCC. Finally, by comprehensively evaluating the devised solutions on benchmark datasets, we conduct a thorough analysis on the effectiveness of the designed baselines, and hopefully provide other researchers with beneficial guidance and insight to multi-view deep OCC. Our data and codes are opened at https://github.com/liujiyuan13/MvDOCC-datasets and https://github.com/liujiyuan13/MvDOCC-code respectively to facilitate future research.



### Underwater Image Enhancement via Medium Transmission-Guided Multi-Color Space Embedding
- **Arxiv ID**: http://arxiv.org/abs/2104.13015v1
- **DOI**: 10.1109/TIP.2021.3076367
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13015v1)
- **Published**: 2021-04-27 07:35:30+00:00
- **Updated**: 2021-04-27 07:35:30+00:00
- **Authors**: Chongyi Li, Saeed Anwar, Junhui Hou, Runmin Cong, Chunle Guo, Wenqi Ren
- **Comment**: Accepted by IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Underwater images suffer from color casts and low contrast due to wavelength- and distance-dependent attenuation and scattering. To solve these two degradation issues, we present an underwater image enhancement network via medium transmission-guided multi-color space embedding, called Ucolor. Concretely, we first propose a multi-color space encoder network, which enriches the diversity of feature representations by incorporating the characteristics of different color spaces into a unified structure. Coupled with an attention mechanism, the most discriminative features extracted from multiple color spaces are adaptively integrated and highlighted. Inspired by underwater imaging physical models, we design a medium transmission (indicating the percentage of the scene radiance reaching the camera)-guided decoder network to enhance the response of the network towards quality-degraded regions. As a result, our network can effectively improve the visual quality of underwater images by exploiting multiple color spaces embedding and the advantages of both physical model-based and learning-based methods. Extensive experiments demonstrate that our Ucolor achieves superior performance against state-of-the-art methods in terms of both visual quality and quantitative metrics.



### Attention and Prediction Guided Motion Detection for Low-Contrast Small Moving Targets
- **Arxiv ID**: http://arxiv.org/abs/2104.13018v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13018v7)
- **Published**: 2021-04-27 07:42:31+00:00
- **Updated**: 2022-04-23 01:59:01+00:00
- **Authors**: Hongxin Wang, Jiannan Zhao, Huatian Wang, Cheng Hu, Jigen Peng, Shigang Yue
- **Comment**: 13 pages, 21 figures
- **Journal**: None
- **Summary**: Small target motion detection within complex natural environments is an extremely challenging task for autonomous robots. Surprisingly, the visual systems of insects have evolved to be highly efficient in detecting mates and tracking prey, even though targets occupy as small as a few degrees of their visual fields. The excellent sensitivity to small target motion relies on a class of specialized neurons called small target motion detectors (STMDs). However, existing STMD-based models are heavily dependent on visual contrast and perform poorly in complex natural environments where small targets generally exhibit extremely low contrast against neighbouring backgrounds. In this paper, we develop an attention and prediction guided visual system to overcome this limitation. The developed visual system comprises three main subsystems, namely, an attention module, an STMD-based neural network, and a prediction module. The attention module searches for potential small targets in the predicted areas of the input image and enhances their contrast against complex background. The STMD-based neural network receives the contrast-enhanced image and discriminates small moving targets from background false positives. The prediction module foresees future positions of the detected targets and generates a prediction map for the attention module. The three subsystems are connected in a recurrent architecture allowing information to be processed sequentially to activate specific areas for small target detection. Extensive experiments on synthetic and real-world datasets demonstrate the effectiveness and superiority of the proposed visual system for detecting small, low-contrast moving targets against complex natural environments.



### AT-ST: Self-Training Adaptation Strategy for OCR in Domains with Limited Transcriptions
- **Arxiv ID**: http://arxiv.org/abs/2104.13037v1
- **DOI**: 10.1007/978-3-030-86337-1_31
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13037v1)
- **Published**: 2021-04-27 08:20:46+00:00
- **Updated**: 2021-04-27 08:20:46+00:00
- **Authors**: Martin Kišš, Karel Beneš, Michal Hradiš
- **Comment**: 15 pages, 6 figures, 5 tables
- **Journal**: None
- **Summary**: This paper addresses text recognition for domains with limited manual annotations by a simple self-training strategy. Our approach should reduce human annotation effort when target domain data is plentiful, such as when transcribing a collection of single person's correspondence or a large manuscript. We propose to train a seed system on large scale data from related domains mixed with available annotated data from the target domain. The seed system transcribes the unannotated data from the target domain which is then used to train a better system. We study several confidence measures and eventually decide to use the posterior probability of a transcription for data selection. Additionally, we propose to augment the data using an aggressive masking scheme. By self-training, we achieve up to 55 % reduction in character error rate for handwritten data and up to 38 % on printed data. The masking augmentation itself reduces the error rate by about 10 % and its effect is better pronounced in case of difficult handwritten data.



### Dual Transformer for Point Cloud Analysis
- **Arxiv ID**: http://arxiv.org/abs/2104.13044v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.13044v1)
- **Published**: 2021-04-27 08:41:02+00:00
- **Updated**: 2021-04-27 08:41:02+00:00
- **Authors**: Xian-Feng Han, Yi-Fei Jin, Hui-Xian Cheng, Guo-Qiang Xiao
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Following the tremendous success of transformer in natural language processing and image understanding tasks, in this paper, we present a novel point cloud representation learning architecture, named Dual Transformer Network (DTNet), which mainly consists of Dual Point Cloud Transformer (DPCT) module. Specifically, by aggregating the well-designed point-wise and channel-wise multi-head self-attention models simultaneously, DPCT module can capture much richer contextual dependencies semantically from the perspective of position and channel. With the DPCT module as a fundamental component, we construct the DTNet for performing point cloud analysis in an end-to-end manner. Extensive quantitative and qualitative experiments on publicly available benchmarks demonstrate the effectiveness of our proposed transformer framework for the tasks of 3D point cloud classification and segmentation, achieving highly competitive performance in comparison with the state-of-the-art approaches.



### Three-stream network for enriched Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.13051v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13051v2)
- **Published**: 2021-04-27 08:56:11+00:00
- **Updated**: 2021-06-23 23:11:00+00:00
- **Authors**: Ivaxi Sheth
- **Comment**: CVPR 2021 workshop
- **Journal**: None
- **Summary**: Understanding accurate information on human behaviours is one of the most important tasks in machine intelligence. Human Activity Recognition that aims to understand human activities from a video is a challenging task due to various problems including background, camera motion and dataset variations. This paper proposes two CNN based architectures with three streams which allow the model to exploit the dataset under different settings. The three pathways are differentiated in frame rates. The single pathway, operates at a single frame rate captures spatial information, the slow pathway operates at low frame rates captures the spatial information and the fast pathway operates at high frame rates that capture fine temporal information. Post CNN encoders, we add bidirectional LSTM and attention heads respectively to capture the context and temporal features. By experimenting with various algorithms on UCF-101, Kinetics-600 and AVA dataset, we observe that the proposed models achieve state-of-art performance for human action recognition task.



### Cross-Level Cross-Scale Cross-Attention Network for Point Cloud Representation
- **Arxiv ID**: http://arxiv.org/abs/2104.13053v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.13053v1)
- **Published**: 2021-04-27 09:01:14+00:00
- **Updated**: 2021-04-27 09:01:14+00:00
- **Authors**: Xian-Feng Han, Zhang-Yue He, Jia Chen, Guo-Qiang Xiao
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Self-attention mechanism recently achieves impressive advancement in Natural Language Processing (NLP) and Image Processing domains. And its permutation invariance property makes it ideally suitable for point cloud processing. Inspired by this remarkable success, we propose an end-to-end architecture, dubbed Cross-Level Cross-Scale Cross-Attention Network (CLCSCANet), for point cloud representation learning. First, a point-wise feature pyramid module is introduced to hierarchically extract features from different scales or resolutions. Then a cross-level cross-attention is designed to model long-range inter-level and intra-level dependencies. Finally, we develop a cross-scale cross-attention module to capture interactions between-and-within scales for representation enhancement. Compared with state-of-the-art approaches, our network can obtain competitive performance on challenging 3D object classification, point cloud segmentation tasks via comprehensive experimental evaluation.



### Graphical Modeling for Multi-Source Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2104.13057v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13057v2)
- **Published**: 2021-04-27 09:04:22+00:00
- **Updated**: 2022-06-15 09:07:28+00:00
- **Authors**: Minghao Xu, Hang Wang, Bingbing Ni
- **Comment**: Accepted by TPAMI. Code is available at
  https://github.com/Francis0625/Graphical-Modeling-for-Multi-Source-Domain-Adaptation
- **Journal**: None
- **Summary**: Multi-Source Domain Adaptation (MSDA) focuses on transferring the knowledge from multiple source domains to the target domain, which is a more practical and challenging problem compared to the conventional single-source domain adaptation. In this problem, it is essential to model multiple source domains and target domain jointly, and an effective domain combination scheme is also highly required. The graphical structure among different domains is useful to tackle these challenges, in which the interdependency among various instances/categories can be effectively modeled. In this work, we propose two types of graphical models, i.e. Conditional Random Field for MSDA (CRF-MSDA) and Markov Random Field for MSDA (MRF-MSDA), for cross-domain joint modeling and learnable domain combination. In a nutshell, given an observation set composed of a query sample and the semantic prototypes (i.e. representative category embeddings) on various domains, the CRF-MSDA model seeks to learn the joint distribution of labels conditioned on the observations. We attain this goal by constructing a relational graph over all observations and conducting local message passing on it. By comparison, MRF-MSDA aims to model the joint distribution of observations over different Markov networks via an energy-based formulation, and it can naturally perform label prediction by summing the joint likelihoods over several specific networks. Compared to the CRF-MSDA counterpart, the MRF-MSDA model is more expressive and possesses lower computational cost. We evaluate these two models on four standard benchmark data sets of MSDA with distinct domain shift and data complexity, and both models achieve superior performance over existing methods on all benchmarks. In addition, the analytical studies illustrate the effect of different model components and provide insights about how the cross-domain joint modeling performs.



### Weakly Supervised Volumetric Segmentation via Self-taught Shape Denoising Model
- **Arxiv ID**: http://arxiv.org/abs/2104.13082v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13082v2)
- **Published**: 2021-04-27 10:03:45+00:00
- **Updated**: 2021-05-06 14:14:28+00:00
- **Authors**: Qian He, Shuailin Li, Xuming He
- **Comment**: To appear in MIDL 2021
- **Journal**: None
- **Summary**: Weakly supervised segmentation is an important problem in medical image analysis due to the high cost of pixelwise annotation. Prior methods, while often focusing on weak labels of 2D images, exploit few structural cues of volumetric medical images. To address this, we propose a novel weakly-supervised segmentation strategy capable of better capturing 3D shape prior in both model prediction and learning. Our main idea is to extract a self-taught shape representation by leveraging weak labels, and then integrate this representation into segmentation prediction for shape refinement. To this end, we design a deep network consisting of a segmentation module and a shape denoising module, which are trained by an iterative learning strategy. Moreover, we introduce a weak annotation scheme with a hybrid label design for volumetric images, which improves model learning without increasing the overall annotation cost. The empirical experiments show that our approach outperforms existing SOTA strategies on three organ segmentation benchmarks with distinctive shape properties. Notably, we can achieve strong performance with even 10\% labeled slices, which is significantly superior to other methods.



### VID-WIN: Fast Video Event Matching with Query-Aware Windowing at the Edge for the Internet of Multimedia Things
- **Arxiv ID**: http://arxiv.org/abs/2105.02957v1
- **DOI**: 10.1109/JIOT.2021.3075336
- **Categories**: **cs.CV**, cs.DC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2105.02957v1)
- **Published**: 2021-04-27 10:08:40+00:00
- **Updated**: 2021-04-27 10:08:40+00:00
- **Authors**: Piyush Yadav, Dhaval Salwala, Edward Curry
- **Comment**: 22 pages, 24 figures, 9 tables, Journal accepted in IEEE Internet of
  Things Journal
- **Journal**: None
- **Summary**: Efficient video processing is a critical component in many IoMT applications to detect events of interest. Presently, many window optimization techniques have been proposed in event processing with an underlying assumption that the incoming stream has a structured data model. Videos are highly complex due to the lack of any underlying structured data model. Video stream sources such as CCTV cameras and smartphones are resource-constrained edge nodes. At the same time, video content extraction is expensive and requires computationally intensive Deep Neural Network (DNN) models that are primarily deployed at high-end (or cloud) nodes. This paper presents VID-WIN, an adaptive 2-stage allied windowing approach to accelerate video event analytics in an edge-cloud paradigm. VID-WIN runs parallelly across edge and cloud nodes and performs the query and resource-aware optimization for state-based complex event matching. VID-WIN exploits the video content and DNN input knobs to accelerate the video inference process across nodes. The paper proposes a novel content-driven micro-batch resizing, queryaware caching and micro-batch based utility filtering strategy of video frames under resource-constrained edge nodes to improve the overall system throughput, latency, and network usage. Extensive evaluations are performed over five real-world datasets. The experimental results show that VID-WIN video event matching achieves ~2.3X higher throughput with minimal latency and ~99% bandwidth reduction compared to other baselines while maintaining query-level accuracy and resource bounds.



### Fisheye Lens Camera based Autonomous Valet Parking System
- **Arxiv ID**: http://arxiv.org/abs/2104.13119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13119v1)
- **Published**: 2021-04-27 11:36:03+00:00
- **Updated**: 2021-04-27 11:36:03+00:00
- **Authors**: Young Gon Jo, Seok Hyeon Hong, Sung Soo Hwang, Jeong Mok Ha
- **Comment**: 8 pages, 17 figures, 4 tables
- **Journal**: None
- **Summary**: This paper proposes an efficient autonomous valet parking system utilizing only cameras which are the most widely used sensor. To capture more information instantaneously and respond rapidly to changes in the surrounding environment, fisheye cameras which have a wider angle of view compared to pinhole cameras are used. Accordingly, visual simultaneous localization and mapping is used to identify the layout of the parking lot and track the location of the vehicle. In addition, the input image frames are converted into around view monitor images to resolve the distortion of fisheye lens because the algorithm to detect edges are supposed to be applied to images taken with pinhole cameras. The proposed system adopts a look up table for real time operation by minimizing the computational complexity encountered when processing AVM images. The detection rate of each process and the success rate of autonomous parking were measured to evaluate performance. The experimental results confirm that autonomous parking can be achieved using only visual sensors.



### LUCES: A Dataset for Near-Field Point Light Source Photometric Stereo
- **Arxiv ID**: http://arxiv.org/abs/2104.13135v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13135v2)
- **Published**: 2021-04-27 12:30:42+00:00
- **Updated**: 2021-10-12 16:32:29+00:00
- **Authors**: Roberto Mecca, Fotios Logothetis, Ignas Budvytis, Roberto Cipolla
- **Comment**: None
- **Journal**: None
- **Summary**: Three-dimensional reconstruction of objects from shading information is a challenging task in computer vision. As most of the approaches facing the Photometric Stereo problem use simplified far-field assumptions, real-world scenarios have essentially more complex physical effects that need to be handled for accurately reconstructing the 3D shape. An increasing number of methods have been proposed to address the problem when point light sources are assumed to be nearby the target object. The proximity of the light sources complicates the modeling of the image formation as the light behaviour requires non-linear parameterisation to describe its propagation and attenuation.   To understand the capability of the approaches dealing with this near-field scenario, the literature till now has used synthetically rendered photometric images or minimal and very customised real-world data. In order to fill the gap in evaluating near-field photometric stereo methods, we introduce LUCES the first real-world 'dataset for near-fieLd point light soUrCe photomEtric Stereo' of 14 objects of a varying of materials. A device counting 52 LEDs has been designed to lit each object positioned 10 to 30 centimeters away from the camera. Together with the raw images, in order to evaluate the 3D reconstructions, the dataset includes both normal and depth maps for comparing different features of the retrieved 3D geometry. Furthermore, we evaluate the performance of the latest near-field Photometric Stereo algorithms on the proposed dataset to assess the SOTA method with respect to actual close range effects and object materials.



### Rethinking BiSeNet For Real-time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.13188v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13188v1)
- **Published**: 2021-04-27 13:49:47+00:00
- **Updated**: 2021-04-27 13:49:47+00:00
- **Authors**: Mingyuan Fan, Shenqi Lai, Junshi Huang, Xiaoming Wei, Zhenhua Chai, Junfeng Luo, Xiaolin Wei
- **Comment**: None
- **Journal**: None
- **Summary**: BiSeNet has been proved to be a popular two-stream network for real-time segmentation. However, its principle of adding an extra path to encode spatial information is time-consuming, and the backbones borrowed from pretrained tasks, e.g., image classification, may be inefficient for image segmentation due to the deficiency of task-specific design. To handle these problems, we propose a novel and efficient structure named Short-Term Dense Concatenate network (STDC network) by removing structure redundancy. Specifically, we gradually reduce the dimension of feature maps and use the aggregation of them for image representation, which forms the basic module of STDC network. In the decoder, we propose a Detail Aggregation module by integrating the learning of spatial information into low-level layers in single-stream manner. Finally, the low-level features and deep features are fused to predict the final segmentation results. Extensive experiments on Cityscapes and CamVid dataset demonstrate the effectiveness of our method by achieving promising trade-off between segmentation accuracy and inference speed. On Cityscapes, we achieve 71.9% mIoU on the test set with a speed of 250.4 FPS on NVIDIA GTX 1080Ti, which is 45.2% faster than the latest methods, and achieve 76.8% mIoU with 97.0 FPS while inferring on higher resolution images.



### LasHeR: A Large-scale High-diversity Benchmark for RGBT Tracking
- **Arxiv ID**: http://arxiv.org/abs/2104.13202v2
- **DOI**: 10.1109/TIP.2021.3130533
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13202v2)
- **Published**: 2021-04-27 14:04:23+00:00
- **Updated**: 2021-11-26 08:01:48+00:00
- **Authors**: Chenglong Li, Wanlin Xue, Yaqing Jia, Zhichen Qu, Bin Luo, Jin Tang, Dengdi Sun
- **Comment**: IEEE TRANSACTIONS ON IMAGE PROCESSING
- **Journal**: None
- **Summary**: RGBT tracking receives a surge of interest in the computer vision community, but this research field lacks a large-scale and high-diversity benchmark dataset, which is essential for both the training of deep RGBT trackers and the comprehensive evaluation of RGBT tracking methods. To this end, we present a Large-scale High-diversity benchmark for RGBT tracking (LasHeR) in this work. LasHeR consists of 1224 visible and thermal infrared video pairs with more than 730K frame pairs in total. Each frame pair is spatially aligned and manually annotated with a bounding box, making the dataset well and densely annotated. LasHeR is highly diverse capturing from a broad range of object categories, camera viewpoints, scene complexities and environmental factors across seasons, weathers, day and night. We conduct a comprehensive performance evaluation of 12 RGBT tracking algorithms on the LasHeR dataset and present detailed analysis to clarify the research room in RGBT tracking. In addition, we release the unaligned version of LasHeR to attract the research interest for alignment-free RGBT tracking, which is a more practical task in real-world applications. The datasets and evaluation protocols are available at: https://github.com/BUGPLEASEOUT/LasHeR.



### Every Annotation Counts: Multi-label Deep Supervision for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.13243v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6; I.2.6; I.5.4; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2104.13243v1)
- **Published**: 2021-04-27 14:51:19+00:00
- **Updated**: 2021-04-27 14:51:19+00:00
- **Authors**: Simon Reiß, Constantin Seibold, Alexander Freytag, Erik Rodner, Rainer Stiefelhagen
- **Comment**: Accepted at CVPR 2021
- **Journal**: None
- **Summary**: Pixel-wise segmentation is one of the most data and annotation hungry tasks in our field. Providing representative and accurate annotations is often mission-critical especially for challenging medical applications. In this paper, we propose a semi-weakly supervised segmentation algorithm to overcome this barrier. Our approach is based on a new formulation of deep supervision and student-teacher model and allows for easy integration of different supervision signals. In contrast to previous work, we show that care has to be taken how deep supervision is integrated in lower layers and we present multi-label deep supervision as the most important secret ingredient for success. With our novel training regime for segmentation that flexibly makes use of images that are either fully labeled, marked with bounding boxes, just global labels, or not at all, we are able to cut the requirement for expensive labels by 94.22% - narrowing the gap to the best fully supervised baseline to only 5% mean IoU. Our approach is validated by extensive experiments on retinal fluid segmentation and we provide an in-depth analysis of the anticipated effect each annotation type can have in boosting segmentation performance.



### Semi-supervised Superpixel-based Multi-Feature Graph Learning for Hyperspectral Image Data
- **Arxiv ID**: http://arxiv.org/abs/2104.13268v1
- **DOI**: 10.1109/TGRS.2021.3112298
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13268v1)
- **Published**: 2021-04-27 15:36:26+00:00
- **Updated**: 2021-04-27 15:36:26+00:00
- **Authors**: Madeleine Kotzagiannidis, Carola-Bibiane Schönlieb
- **Comment**: None
- **Journal**: None
- **Summary**: Graphs naturally lend themselves to model the complexities of Hyperspectral Image (HSI) data as well as to serve as semi-supervised classifiers by propagating given labels among nearest neighbours. In this work, we present a novel framework for the classification of HSI data in light of a very limited amount of labelled data, inspired by multi-view graph learning and graph signal processing. Given an a priori superpixel-segmented hyperspectral image, we seek a robust and efficient graph construction and label propagation method to conduct semi-supervised learning (SSL). Since the graph is paramount to the success of the subsequent classification task, particularly in light of the intrinsic complexity of HSI data, we consider the problem of finding the optimal graph to model such data. Our contribution is two-fold: firstly, we propose a multi-stage edge-efficient semi-supervised graph learning framework for HSI data which exploits given label information through pseudo-label features embedded in the graph construction. Secondly, we examine and enhance the contribution of multiple superpixel features embedded in the graph on the basis of pseudo-labels in an extension of the previous framework, which is less reliant on excessive parameter tuning. Ultimately, we demonstrate the superiority of our approaches in comparison with state-of-the-art methods through extensive numerical experiments.



### Evidential segmentation of 3D PET/CT images
- **Arxiv ID**: http://arxiv.org/abs/2104.13293v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.13293v1)
- **Published**: 2021-04-27 16:06:27+00:00
- **Updated**: 2021-04-27 16:06:27+00:00
- **Authors**: Ling Huang, Su Ruan, Pierre Decazes, Thierry Denoeux
- **Comment**: Belief2021
- **Journal**: None
- **Summary**: PET and CT are two modalities widely used in medical image analysis. Accurately detecting and segmenting lymphomas from these two imaging modalities are critical tasks for cancer staging and radiotherapy planning. However, this task is still challenging due to the complexity of PET/CT images, and the computation cost to process 3D data. In this paper, a segmentation method based on belief functions is proposed to segment lymphomas in 3D PET/CT images. The architecture is composed of a feature extraction module and an evidential segmentation (ES) module. The ES module outputs not only segmentation results (binary maps indicating the presence or absence of lymphoma in each voxel) but also uncertainty maps quantifying the classification uncertainty. The whole model is optimized by minimizing Dice and uncertainty loss functions to increase segmentation accuracy. The method was evaluated on a database of 173 patients with diffuse large b-cell lymphoma. Quantitative and qualitative results show that our method outperforms the state-of-the-art methods.



### Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification
- **Arxiv ID**: http://arxiv.org/abs/2104.13298v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13298v2)
- **Published**: 2021-04-27 16:11:45+00:00
- **Updated**: 2021-11-20 09:22:24+00:00
- **Authors**: Yixiao Ge, Xiao Zhang, Ching Lam Choi, Ka Chun Cheung, Peipei Zhao, Feng Zhu, Xiaogang Wang, Rui Zhao, Hongsheng Li
- **Comment**: Project Page: https://geyixiao.com/projects/bake
- **Journal**: None
- **Summary**: The recent studies of knowledge distillation have discovered that ensembling the "dark knowledge" from multiple teachers or students contributes to creating better soft targets for training, but at the cost of significantly more computations and/or parameters. In this work, we present BAtch Knowledge Ensembling (BAKE) to produce refined soft targets for anchor images by propagating and ensembling the knowledge of the other samples in the same mini-batch. Specifically, for each sample of interest, the propagation of knowledge is weighted in accordance with the inter-sample affinities, which are estimated on-the-fly with the current network. The propagated knowledge can then be ensembled to form a better soft target for distillation. In this way, our BAKE framework achieves online knowledge ensembling across multiple samples with only a single network. It requires minimal computational and memory overhead compared to existing knowledge ensembling methods. Extensive experiments demonstrate that the lightweight yet effective BAKE consistently boosts the classification performance of various architectures on multiple datasets, e.g., a significant +0.7% gain of Swin-T on ImageNet with only +1.5% computational overhead and zero additional parameters. BAKE does not only improve the vanilla baselines, but also surpasses the single-network state-of-the-arts on all the benchmarks.



### MVS2D: Efficient Multi-view Stereo via Attention-Driven 2D Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2104.13325v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13325v2)
- **Published**: 2021-04-27 16:56:05+00:00
- **Updated**: 2021-12-11 23:49:15+00:00
- **Authors**: Zhenpei Yang, Zhile Ren, Qi Shan, Qixing Huang
- **Comment**: Our code is released at https://github.com/zhenpeiyang/MVS2D
- **Journal**: None
- **Summary**: Deep learning has made significant impacts on multi-view stereo systems. State-of-the-art approaches typically involve building a cost volume, followed by multiple 3D convolution operations to recover the input image's pixel-wise depth. While such end-to-end learning of plane-sweeping stereo advances public benchmarks' accuracy, they are typically very slow to compute. We present \ouralg, a highly efficient multi-view stereo algorithm that seamlessly integrates multi-view constraints into single-view networks via an attention mechanism. Since \ouralg only builds on 2D convolutions, it is at least $2\times$ faster than all the notable counterparts. Moreover, our algorithm produces precise depth estimations and 3D reconstructions, achieving state-of-the-art results on challenging benchmarks ScanNet, SUN3D, RGBD, and the classical DTU dataset. our algorithm also out-performs all other algorithms in the setting of inexact camera poses. Our code is released at \url{https://github.com/zhenpeiyang/MVS2D}



### End-to-End Video-To-Speech Synthesis using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.13332v3
- **DOI**: 10.1109/TCYB.2022.3162495
- **Categories**: **cs.LG**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2104.13332v3)
- **Published**: 2021-04-27 17:12:30+00:00
- **Updated**: 2022-08-15 19:00:19+00:00
- **Authors**: Rodrigo Mira, Konstantinos Vougioukas, Pingchuan Ma, Stavros Petridis, Björn W. Schuller, Maja Pantic
- **Comment**: Published in IEEE Transactions on Cybernetics (April 2022)
- **Journal**: None
- **Summary**: Video-to-speech is the process of reconstructing the audio speech from a video of a spoken utterance. Previous approaches to this task have relied on a two-step process where an intermediate representation is inferred from the video, and is then decoded into waveform audio using a vocoder or a waveform reconstruction algorithm. In this work, we propose a new end-to-end video-to-speech model based on Generative Adversarial Networks (GANs) which translates spoken video to waveform end-to-end without using any intermediate representation or separate waveform synthesis algorithm. Our model consists of an encoder-decoder architecture that receives raw video as input and generates speech, which is then fed to a waveform critic and a power critic. The use of an adversarial loss based on these two critics enables the direct synthesis of raw audio waveform and ensures its realism. In addition, the use of our three comparative losses helps establish direct correspondence between the generated audio and the input video. We show that this model is able to reconstruct speech with remarkable realism for constrained datasets such as GRID, and that it is the first end-to-end model to produce intelligible speech for LRW (Lip Reading in the Wild), featuring hundreds of speakers recorded entirely `in the wild'. We evaluate the generated samples in two different scenarios -- seen and unseen speakers -- using four objective metrics which measure the quality and intelligibility of artificial speech. We demonstrate that the proposed approach outperforms all previous works in most metrics on GRID and LRW.



### Sifting out the features by pruning: Are convolutional networks the winning lottery ticket of fully connected ones?
- **Arxiv ID**: http://arxiv.org/abs/2104.13343v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.13343v2)
- **Published**: 2021-04-27 17:25:54+00:00
- **Updated**: 2021-05-14 10:52:49+00:00
- **Authors**: Franco Pellegrini, Giulio Biroli
- **Comment**: 25 pages, 18 figures; typos corrected, references added
- **Journal**: None
- **Summary**: Pruning methods can considerably reduce the size of artificial neural networks without harming their performance. In some cases, they can even uncover sub-networks that, when trained in isolation, match or surpass the test accuracy of their dense counterparts. Here we study the inductive bias that pruning imprints in such "winning lottery tickets". Focusing on visual tasks, we analyze the architecture resulting from iterative magnitude pruning of a simple fully connected network (FCN). We show that the surviving node connectivity is local in input space, and organized in patterns reminiscent of the ones found in convolutional networks (CNN). We investigate the role played by data and tasks in shaping the architecture of pruned sub-networks. Our results show that the winning lottery tickets of FCNs display the key features of CNNs. The ability of such automatic network-simplifying procedure to recover the key features "hand-crafted" in the design of CNNs suggests interesting applications to other datasets and tasks, in order to discover new and efficient architectural inductive biases.



### NTIRE 2021 Depth Guided Image Relighting Challenge
- **Arxiv ID**: http://arxiv.org/abs/2104.13365v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.13365v1)
- **Published**: 2021-04-27 17:53:32+00:00
- **Updated**: 2021-04-27 17:53:32+00:00
- **Authors**: Majed El Helou, Ruofan Zhou, Sabine Susstrunk, Radu Timofte
- **Comment**: Code and data available on https://github.com/majedelhelou/VIDIT
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition
  Workshops 2021
- **Summary**: Image relighting is attracting increasing interest due to its various applications. From a research perspective, image relighting can be exploited to conduct both image normalization for domain adaptation, and also for data augmentation. It also has multiple direct uses for photo montage and aesthetic enhancement. In this paper, we review the NTIRE 2021 depth guided image relighting challenge.   We rely on the VIDIT dataset for each of our two challenge tracks, including depth information. The first track is on one-to-one relighting where the goal is to transform the illumination setup of an input image (color temperature and light source position) to the target illumination setup. In the second track, the any-to-any relighting challenge, the objective is to transform the illumination settings of the input image to match those of another guide image, similar to style transfer. In both tracks, participants were given depth information about the captured scenes. We had nearly 250 registered participants, leading to 18 confirmed team submissions in the final competition stage. The competitions, methods, and final results are presented in this paper.



### Unsupervised 3D Shape Completion through GAN Inversion
- **Arxiv ID**: http://arxiv.org/abs/2104.13366v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13366v2)
- **Published**: 2021-04-27 17:53:46+00:00
- **Updated**: 2021-04-29 13:09:32+00:00
- **Authors**: Junzhe Zhang, Xinyi Chen, Zhongang Cai, Liang Pan, Haiyu Zhao, Shuai Yi, Chai Kiat Yeo, Bo Dai, Chen Change Loy
- **Comment**: Accepted in CVPR 2021, project webpage:
  https://junzhezhang.github.io/projects/ShapeInversion/
- **Journal**: None
- **Summary**: Most 3D shape completion approaches rely heavily on partial-complete shape pairs and learn in a fully supervised manner. Despite their impressive performances on in-domain data, when generalizing to partial shapes in other forms or real-world partial scans, they often obtain unsatisfactory results due to domain gaps. In contrast to previous fully supervised approaches, in this paper we present ShapeInversion, which introduces Generative Adversarial Network (GAN) inversion to shape completion for the first time. ShapeInversion uses a GAN pre-trained on complete shapes by searching for a latent code that gives a complete shape that best reconstructs the given partial input. In this way, ShapeInversion no longer needs paired training data, and is capable of incorporating the rich prior captured in a well-trained generative model. On the ShapeNet benchmark, the proposed ShapeInversion outperforms the SOTA unsupervised method, and is comparable with supervised methods that are learned using paired data. It also demonstrates remarkable generalization ability, giving robust results for real-world scans and partial inputs of various forms and incompleteness levels. Importantly, ShapeInversion naturally enables a series of additional abilities thanks to the involvement of a pre-trained GAN, such as producing multiple valid complete shapes for an ambiguous partial input, as well as shape manipulation and interpolation.



### Explaining in Style: Training a GAN to explain a classifier in StyleSpace
- **Arxiv ID**: http://arxiv.org/abs/2104.13369v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.13369v2)
- **Published**: 2021-04-27 17:57:19+00:00
- **Updated**: 2021-09-01 08:04:54+00:00
- **Authors**: Oran Lang, Yossi Gandelsman, Michal Yarom, Yoav Wald, Gal Elidan, Avinatan Hassidim, William T. Freeman, Phillip Isola, Amir Globerson, Michal Irani, Inbar Mosseri
- **Comment**: Accepted to ICCV 2021. Project page:
  https://explaining-in-style.github.io/, Code:
  https://github.com/google/explaining-in-style
- **Journal**: None
- **Summary**: Image classification models can depend on multiple different semantic attributes of the image. An explanation of the decision of the classifier needs to both discover and visualize these properties. Here we present StylEx, a method for doing this, by training a generative model to specifically explain multiple attributes that underlie classifier decisions. A natural source for such attributes is the StyleSpace of StyleGAN, which is known to generate semantically meaningful dimensions in the image. However, because standard GAN training is not dependent on the classifier, it may not represent these attributes which are important for the classifier decision, and the dimensions of StyleSpace may represent irrelevant attributes. To overcome this, we propose a training procedure for a StyleGAN, which incorporates the classifier model, in order to learn a classifier-specific StyleSpace. Explanatory attributes are then selected from this space. These can be used to visualize the effect of changing multiple attributes per image, thus providing image-specific explanations. We apply StylEx to multiple domains, including animals, leaves, faces and retinal images. For these, we show how an image can be modified in different ways to change its classifier output. Our results show that the method finds attributes that align well with semantic ones, generate meaningful image-specific explanations, and are human-interpretable as measured in user-studies.



### BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment
- **Arxiv ID**: http://arxiv.org/abs/2104.13371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13371v1)
- **Published**: 2021-04-27 17:58:31+00:00
- **Updated**: 2021-04-27 17:58:31+00:00
- **Authors**: Kelvin C. K. Chan, Shangchen Zhou, Xiangyu Xu, Chen Change Loy
- **Comment**: 3 champions and 1 runner-up in NTIRE 2021
- **Journal**: None
- **Summary**: A recurrent structure is a popular framework choice for the task of video super-resolution. The state-of-the-art method BasicVSR adopts bidirectional propagation with feature alignment to effectively exploit information from the entire input video. In this study, we redesign BasicVSR by proposing second-order grid propagation and flow-guided deformable alignment. We show that by empowering the recurrent framework with the enhanced propagation and alignment, one can exploit spatiotemporal information across misaligned video frames more effectively. The new components lead to an improved performance under a similar computational constraint. In particular, our model BasicVSR++ surpasses BasicVSR by 0.82 dB in PSNR with similar number of parameters. In addition to video super-resolution, BasicVSR++ generalizes well to other video restoration tasks such as compressed video enhancement. In NTIRE 2021, BasicVSR++ obtains three champions and one runner-up in the Video Super-Resolution and Compressed Video Enhancement Challenges. Codes and models will be released to MMEditing.



### ACDC: The Adverse Conditions Dataset with Correspondences for Semantic Driving Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2104.13395v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13395v3)
- **Published**: 2021-04-27 18:00:05+00:00
- **Updated**: 2021-09-01 13:53:00+00:00
- **Authors**: Christos Sakaridis, Dengxin Dai, Luc Van Gool
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Level 5 autonomy for self-driving cars requires a robust visual perception system that can parse input images under any visual condition. However, existing semantic segmentation datasets are either dominated by images captured under normal conditions or are small in scale. To address this, we introduce ACDC, the Adverse Conditions Dataset with Correspondences for training and testing semantic segmentation methods on adverse visual conditions. ACDC consists of a large set of 4006 images which are equally distributed between four common adverse conditions: fog, nighttime, rain, and snow. Each adverse-condition image comes with a high-quality fine pixel-level semantic annotation, a corresponding image of the same scene taken under normal conditions, and a binary mask that distinguishes between intra-image regions of clear and uncertain semantic content. Thus, ACDC supports both standard semantic segmentation and the newly introduced uncertainty-aware semantic segmentation. A detailed empirical study demonstrates the challenges that the adverse domains of ACDC pose to state-of-the-art supervised and unsupervised approaches and indicates the value of our dataset in steering future progress in the field. Our dataset and benchmark are publicly available.



### FrameExit: Conditional Early Exiting for Efficient Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.13400v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13400v1)
- **Published**: 2021-04-27 18:01:05+00:00
- **Updated**: 2021-04-27 18:01:05+00:00
- **Authors**: Amir Ghodrati, Babak Ehteshami Bejnordi, Amirhossein Habibian
- **Comment**: CVPR 2021 | Oral paper
- **Journal**: None
- **Summary**: In this paper, we propose a conditional early exiting framework for efficient video recognition. While existing works focus on selecting a subset of salient frames to reduce the computation costs, we propose to use a simple sampling strategy combined with conditional early exiting to enable efficient recognition. Our model automatically learns to process fewer frames for simpler videos and more frames for complex ones. To achieve this, we employ a cascade of gating modules to automatically determine the earliest point in processing where an inference is sufficiently reliable. We generate on-the-fly supervision signals to the gates to provide a dynamic trade-off between accuracy and computational cost. Our proposed model outperforms competing methods on three large-scale video benchmarks. In particular, on ActivityNet1.3 and mini-kinetics, we outperform the state-of-the-art efficient video recognition methods with 1.3$\times$ and 2.1$\times$ less GFLOPs, respectively. Additionally, our method sets a new state of the art for efficient video understanding on the HVU benchmark.



### Semi-Supervised Semantic Segmentation with Pixel-Level Contrastive Learning from a Class-wise Memory Bank
- **Arxiv ID**: http://arxiv.org/abs/2104.13415v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13415v3)
- **Published**: 2021-04-27 18:19:33+00:00
- **Updated**: 2021-08-06 08:07:18+00:00
- **Authors**: Inigo Alonso, Alberto Sabater, David Ferstl, Luis Montesano, Ana C. Murillo
- **Comment**: None
- **Journal**: IEEE International Conference on Computer Vision 2021
- **Summary**: This work presents a novel approach for semi-supervised semantic segmentation. The key element of this approach is our contrastive learning module that enforces the segmentation network to yield similar pixel-level feature representations for same-class samples across the whole dataset. To achieve this, we maintain a memory bank continuously updated with relevant and high-quality feature vectors from labeled data. In an end-to-end training, the features from both labeled and unlabeled data are optimized to be similar to same-class samples from the memory bank. Our approach outperforms the current state-of-the-art for semi-supervised semantic segmentation and semi-supervised domain adaptation on well-known public benchmarks, with larger improvements on the most challenging scenarios, i.e., less available labeled data. https://github.com/Shathe/SemiSeg-Contrastive



### Towards Fair Federated Learning with Zero-Shot Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.13417v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.13417v1)
- **Published**: 2021-04-27 18:23:54+00:00
- **Updated**: 2021-04-27 18:23:54+00:00
- **Authors**: Weituo Hao, Mostafa El-Khamy, Jungwon Lee, Jianyi Zhang, Kevin J Liang, Changyou Chen, Lawrence Carin
- **Comment**: Accepted by IEEE CVPR Workshop on Fair, Data Efficient And Trusted
  Computer Vision
- **Journal**: None
- **Summary**: Federated learning has emerged as an important distributed learning paradigm, where a server aggregates a global model from many client-trained models while having no access to the client data. Although it is recognized that statistical heterogeneity of the client local data yields slower global model convergence, it is less commonly recognized that it also yields a biased federated global model with a high variance of accuracy across clients. In this work, we aim to provide federated learning schemes with improved fairness. To tackle this challenge, we propose a novel federated learning system that employs zero-shot data augmentation on under-represented data to mitigate statistical heterogeneity and encourage more uniform accuracy performance across clients in federated networks. We study two variants of this scheme, Fed-ZDAC (federated learning with zero-shot data augmentation at the clients) and Fed-ZDAS (federated learning with zero-shot data augmentation at the server). Empirical results on a suite of datasets demonstrate the effectiveness of our methods on simultaneously improving the test accuracy and fairness.



### Self-supervised Spatial Reasoning on Multi-View Line Drawings
- **Arxiv ID**: http://arxiv.org/abs/2104.13433v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13433v2)
- **Published**: 2021-04-27 19:05:27+00:00
- **Updated**: 2022-05-16 02:47:32+00:00
- **Authors**: Siyuan Xiang, Anbang Yang, Yanfei Xue, Yaoqing Yang, Chen Feng
- **Comment**: The first two authors contributed equally. Chen Feng is the
  corresponding author
- **Journal**: None
- **Summary**: Spatial reasoning on multi-view line drawings by state-of-the-art supervised deep networks is recently shown with puzzling low performances on the SPARE3D dataset. Based on the fact that self-supervised learning is helpful when a large number of data are available, we propose two self-supervised learning approaches to improve the baseline performance for view consistency reasoning and camera pose reasoning tasks on the SPARE3D dataset. For the first task, we use a self-supervised binary classification network to contrast the line drawing differences between various views of any two similar 3D objects, enabling the trained networks to effectively learn detail-sensitive yet view-invariant line drawing representations of 3D objects. For the second type of task, we propose a self-supervised multi-class classification framework to train a model to select the correct corresponding view from which a line drawing is rendered. Our method is even helpful for the downstream tasks with unseen camera poses. Experiments show that our method could significantly increase the baseline performance in SPARE3D, while some popular self-supervised learning methods cannot.



### Incident Detection on Junctions Using Image Processing
- **Arxiv ID**: http://arxiv.org/abs/2104.13437v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13437v1)
- **Published**: 2021-04-27 19:18:05+00:00
- **Updated**: 2021-04-27 19:18:05+00:00
- **Authors**: Murat Tulgaç, Enes Yüncü, Mohamad-Alhaddad, Ceylan Yozgatlıgil
- **Comment**: None
- **Journal**: None
- **Summary**: In traffic management, it is a very important issue to shorten the response time by detecting the incidents (accident, vehicle breakdown, an object falling on the road, etc.) and informing the corresponding personnel. In this study, an anomaly detection framework for road junctions is proposed. The final judgment is based on the trajectories followed by the vehicles. Trajectory information is provided by vehicle detection and tracking algorithms on visual data streamed from a fisheye camera. Deep learning algorithms are used for vehicle detection, and Kalman Filter is used for tracking. To observe the trajectories more accurately, the detected vehicle coordinates are transferred to the bird's eye view coordinates using the lens distortion model prediction algorithm. The system determines whether there is an abnormality in trajectories by comparing historical trajectory data and instantaneous incoming data. The proposed system has achieved 84.6% success in vehicle detection and 96.8% success in abnormality detection on synthetic data. The system also works with a 97.3% success rate in detecting abnormalities on real data.



### Morphological classification of astronomical images with limited labelling
- **Arxiv ID**: http://arxiv.org/abs/2105.02958v1
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.GA, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.02958v1)
- **Published**: 2021-04-27 19:26:27+00:00
- **Updated**: 2021-04-27 19:26:27+00:00
- **Authors**: Andrey Soroka, Alex Meshcheryakov, Sergey Gerasimov
- **Comment**: None
- **Journal**: None
- **Summary**: The task of morphological classification is complex for simple parameterization, but important for research in the galaxy evolution field. Future galaxy surveys (e.g. EUCLID) will collect data about more than a $10^9$ galaxies. To obtain morphological information one needs to involve people to mark up galaxy images, which requires either a considerable amount of money or a huge number of volunteers. We propose an effective semi-supervised approach for galaxy morphology classification task, based on active learning of adversarial autoencoder (AAE) model. For a binary classification problem (top level question of Galaxy Zoo 2 decision tree) we achieved accuracy 93.1% on the test part with only 0.86 millions markup actions, this model can easily scale up on any number of images. Our best model with additional markup achieves accuracy of 95.5%. To the best of our knowledge it is a first time AAE semi-supervised learning model used in astronomy.



### SrvfNet: A Generative Network for Unsupervised Multiple Diffeomorphic Shape Alignment
- **Arxiv ID**: http://arxiv.org/abs/2104.13449v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.DG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13449v1)
- **Published**: 2021-04-27 19:49:46+00:00
- **Updated**: 2021-04-27 19:49:46+00:00
- **Authors**: Elvis Nunez, Andrew Lizarraga, Shantanu H. Joshi
- **Comment**: None
- **Journal**: None
- **Summary**: We present SrvfNet, a generative deep learning framework for the joint multiple alignment of large collections of functional data comprising square-root velocity functions (SRVF) to their templates. Our proposed framework is fully unsupervised and is capable of aligning to a predefined template as well as jointly predicting an optimal template from data while simultaneously achieving alignment. Our network is constructed as a generative encoder-decoder architecture comprising fully-connected layers capable of producing a distribution space of the warping functions. We demonstrate the strength of our framework by validating it on synthetic data as well as diffusion profiles from magnetic resonance imaging (MRI) data.



### Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings
- **Arxiv ID**: http://arxiv.org/abs/2104.13450v8
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.13450v8)
- **Published**: 2021-04-27 19:51:39+00:00
- **Updated**: 2022-03-29 04:36:06+00:00
- **Authors**: Innfarn Yoo, Huiwen Chang, Xiyang Luo, Ondrej Stava, Ce Liu, Peyman Milanfar, Feng Yang
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Digital watermarking is widely used for copyright protection. Traditional 3D watermarking approaches or commercial software are typically designed to embed messages into 3D meshes, and later retrieve the messages directly from distorted/undistorted watermarked 3D meshes. However, in many cases, users only have access to rendered 2D images instead of 3D meshes. Unfortunately, retrieving messages from 2D renderings of 3D meshes is still challenging and underexplored. We introduce a novel end-to-end learning framework to solve this problem through: 1) an encoder to covertly embed messages in both mesh geometry and textures; 2) a differentiable renderer to render watermarked 3D objects from different camera angles and under varied lighting conditions; 3) a decoder to recover the messages from 2D rendered images. From our experiments, we show that our model can learn to embed information visually imperceptible to humans, and to retrieve the embedded information from 2D renderings that undergo 3D distortions. In addition, we demonstrate that our method can also work with other renderers, such as ray tracers and real-time renderers with and without fine-tuning.



### Estimating Egocentric 3D Human Pose in Global Space
- **Arxiv ID**: http://arxiv.org/abs/2104.13454v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13454v3)
- **Published**: 2021-04-27 20:01:57+00:00
- **Updated**: 2021-08-24 13:43:38+00:00
- **Authors**: Jian Wang, Lingjie Liu, Weipeng Xu, Kripasindhu Sarkar, Christian Theobalt
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Egocentric 3D human pose estimation using a single fisheye camera has become popular recently as it allows capturing a wide range of daily activities in unconstrained environments, which is difficult for traditional outside-in motion capture with external cameras. However, existing methods have several limitations. A prominent problem is that the estimated poses lie in the local coordinate system of the fisheye camera, rather than in the world coordinate system, which is restrictive for many applications. Furthermore, these methods suffer from limited accuracy and temporal instability due to ambiguities caused by the monocular setup and the severe occlusion in a strongly distorted egocentric perspective. To tackle these limitations, we present a new method for egocentric global 3D body pose estimation using a single head-mounted fisheye camera. To achieve accurate and temporally stable global poses, a spatio-temporal optimization is performed over a sequence of frames by minimizing heatmap reprojection errors and enforcing local and global body motion priors learned from a mocap dataset. Experimental results show that our approach outperforms state-of-the-art methods both quantitatively and qualitatively.



### Deep Two-Stage High-Resolution Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2104.13464v1
- **DOI**: 10.51130/graphicon-2020-2-4-18
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.13464v1)
- **Published**: 2021-04-27 20:32:21+00:00
- **Updated**: 2021-04-27 20:32:21+00:00
- **Authors**: Andrey Moskalenko, Mikhail Erofeev, Dmitriy Vatolin
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the field of image inpainting has developed rapidly, learning based approaches show impressive results in the task of filling missing parts in an image. But most deep methods are strongly tied to the resolution of the images on which they were trained. A slight resolution increase leads to serious artifacts and unsatisfactory filling quality. These methods are therefore unsuitable for interactive image processing. In this article, we propose a method that solves the problem of inpainting arbitrary-size images. We also describe a way to better restore texture fragments in the filled area. For this, we propose to use information from neighboring pixels by shifting the original image in four directions. Moreover, this approach can work with existing inpainting models, making them almost resolution independent without the need for retraining. We also created a GIMP plugin that implements our technique. The plugin, code, and model weights are available at https://github.com/a-mos/High_Resolution_Image_Inpainting.



### Do Feature Attribution Methods Correctly Attribute Features?
- **Arxiv ID**: http://arxiv.org/abs/2104.14403v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.14403v2)
- **Published**: 2021-04-27 20:35:30+00:00
- **Updated**: 2021-12-15 16:30:39+00:00
- **Authors**: Yilun Zhou, Serena Booth, Marco Tulio Ribeiro, Julie Shah
- **Comment**: AAAI 2022. Video summary at
  https://www.youtube.com/watch?v=kAodFw6jvvo
- **Journal**: None
- **Summary**: Feature attribution methods are popular in interpretable machine learning. These methods compute the attribution of each input feature to represent its importance, but there is no consensus on the definition of "attribution", leading to many competing methods with little systematic evaluation, complicated in particular by the lack of ground truth attribution. To address this, we propose a dataset modification procedure to induce such ground truth. Using this procedure, we evaluate three common methods: saliency maps, rationales, and attentions. We identify several deficiencies and add new perspectives to the growing body of evidence questioning the correctness and reliability of these methods applied on datasets in the wild. We further discuss possible avenues for remedy and recommend new attribution methods to be tested against ground truth before deployment. The code is available at https://github.com/YilunZhou/feature-attribution-evaluation



### TRECVID 2020: A comprehensive campaign for evaluating video retrieval tasks across multiple application domains
- **Arxiv ID**: http://arxiv.org/abs/2104.13473v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.13473v1)
- **Published**: 2021-04-27 20:59:27+00:00
- **Updated**: 2021-04-27 20:59:27+00:00
- **Authors**: George Awad, Asad A. Butt, Keith Curtis, Jonathan Fiscus, Afzal Godil, Yooyoung Lee, Andrew Delgado, Jesse Zhang, Eliot Godard, Baptiste Chocot, Lukas Diduch, Jeffrey Liu, Alan F. Smeaton, Yvette Graham, Gareth J. F. Jones, Wessel Kraaij, Georges Quenot
- **Comment**: TRECVID 2020 Workshop Overview Paper. arXiv admin note: substantial
  text overlap with arXiv:2009.09984
- **Journal**: None
- **Summary**: The TREC Video Retrieval Evaluation (TRECVID) is a TREC-style video analysis and retrieval evaluation with the goal of promoting progress in research and development of content-based exploitation and retrieval of information from digital video via open, metrics-based evaluation. Over the last twenty years this effort has yielded a better understanding of how systems can effectively accomplish such processing and how one can reliably benchmark their performance. TRECVID has been funded by NIST (National Institute of Standards and Technology) and other US government agencies. In addition, many organizations and individuals worldwide contribute significant time and effort. TRECVID 2020 represented a continuation of four tasks and the addition of two new tasks. In total, 29 teams from various research organizations worldwide completed one or more of the following six tasks: 1. Ad-hoc Video Search (AVS), 2. Instance Search (INS), 3. Disaster Scene Description and Indexing (DSDI), 4. Video to Text Description (VTT), 5. Activities in Extended Video (ActEV), 6. Video Summarization (VSUM). This paper is an introduction to the evaluation framework, tasks, data, and measures used in the evaluation campaign.



### Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges
- **Arxiv ID**: http://arxiv.org/abs/2104.13478v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CG, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.13478v2)
- **Published**: 2021-04-27 21:09:51+00:00
- **Updated**: 2021-05-02 16:16:03+00:00
- **Authors**: Michael M. Bronstein, Joan Bruna, Taco Cohen, Petar Veličković
- **Comment**: 156 pages. Work in progress -- comments welcome!
- **Journal**: None
- **Summary**: The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation.   While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications.   Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.



### Stochastic Neural Networks for Automatic Cell Tracking in Microscopy Image Sequences of Bacterial Colonies
- **Arxiv ID**: http://arxiv.org/abs/2104.13482v2
- **DOI**: 10.3390/mca27020022
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13482v2)
- **Published**: 2021-04-27 21:24:32+00:00
- **Updated**: 2022-07-08 21:46:13+00:00
- **Authors**: Sorena Sarmadi, James J. Winkle, Razan N. Alnahhas, Matthew R. Bennett, Krešimir Josić, Andreas Mang, Robert Azencott
- **Comment**: 35 pages, 8 figures, 7 tables
- **Journal**: Math. Comput. Appl. 2022, 27(2), 22
- **Summary**: Our work targets automated analysis to quantify the growth dynamics of a population of bacilliform bacteria. We propose an innovative approach to frame-sequence tracking of deformable-cell motion by the automated minimization of a new, specific cost functional. This minimization is implemented by dedicated Boltzmann machines (stochastic recurrent neural networks). Automated detection of cell divisions is handled similarly by successive minimizations of two cost functions, alternating the identification of children pairs and parent identification. We validate the proposed automatic cell tracking algorithm using (i) recordings of simulated cell colonies that closely mimic the growth dynamics of E. coli in microfluidic traps and (ii) real data. On a batch of 1100 simulated image frames, cell registration accuracies per frame ranged from 94.5% to 100%, with a high average. Our initial tests using experimental image sequences (i.e., real data) of E. coli colonies also yield convincing results, with a registration accuracy ranging from 90% to 100%.



### Efficient Pre-trained Features and Recurrent Pseudo-Labeling in Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2104.13486v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13486v2)
- **Published**: 2021-04-27 21:35:28+00:00
- **Updated**: 2021-05-01 15:46:31+00:00
- **Authors**: Youshan Zhang, Brian D. Davison
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation (DA) mitigates the domain shift problem when transferring knowledge from one annotated domain to another similar but different unlabeled domain. However, existing models often utilize one of the ImageNet models as the backbone without exploring others, and fine-tuning or retraining the backbone ImageNet model is also time-consuming. Moreover, pseudo-labeling has been used to improve the performance in the target domain, while how to generate confident pseudo labels and explicitly align domain distributions has not been well addressed. In this paper, we show how to efficiently opt for the best pre-trained features from seventeen well-known ImageNet models in unsupervised DA problems. In addition, we propose a recurrent pseudo-labeling model using the best pre-trained features (termed PRPL) to improve classification performance. To show the effectiveness of PRPL, we evaluate it on three benchmark datasets, Office+Caltech-10, Office-31, and Office-Home. Extensive experiments show that our model reduces computation time and boosts the mean accuracy to 98.1%, 92.4%, and 81.2%, respectively, substantially outperforming the state of the art.



### ConTNet: Why not use convolution and transformer at the same time?
- **Arxiv ID**: http://arxiv.org/abs/2104.13497v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13497v3)
- **Published**: 2021-04-27 22:29:55+00:00
- **Updated**: 2021-05-10 18:39:48+00:00
- **Authors**: Haotian Yan, Zhe Li, Weijian Li, Changhu Wang, Ming Wu, Chuang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Although convolutional networks (ConvNets) have enjoyed great success in computer vision (CV), it suffers from capturing global information crucial to dense prediction tasks such as object detection and segmentation. In this work, we innovatively propose ConTNet (ConvolutionTransformer Network), combining transformer with ConvNet architectures to provide large receptive fields. Unlike the recently-proposed transformer-based models (e.g., ViT, DeiT) that are sensitive to hyper-parameters and extremely dependent on a pile of data augmentations when trained from scratch on a midsize dataset (e.g., ImageNet1k), ConTNet can be optimized like normal ConvNets (e.g., ResNet) and preserve an outstanding robustness. It is also worth pointing that, given identical strong data augmentations, the performance improvement of ConTNet is more remarkable than that of ResNet. We present its superiority and effectiveness on image classification and downstream tasks. For example, our ConTNet achieves 81.8% top-1 accuracy on ImageNet which is the same as DeiT-B with less than 40% computational complexity. ConTNet-M also outperforms ResNet50 as the backbone of both Faster-RCNN (by 2.6%) and Mask-RCNN (by 3.2%) on COCO2017 dataset. We hope that ConTNet could serve as a useful backbone for CV tasks and bring new ideas for model design



### KAMA: 3D Keypoint Aware Body Mesh Articulation
- **Arxiv ID**: http://arxiv.org/abs/2104.13502v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13502v1)
- **Published**: 2021-04-27 23:01:03+00:00
- **Updated**: 2021-04-27 23:01:03+00:00
- **Authors**: Umar Iqbal, Kevin Xie, Yunrong Guo, Jan Kautz, Pavlo Molchanov
- **Comment**: "Additional qualitative results: https://youtu.be/mPikZEIpUE0"
- **Journal**: None
- **Summary**: We present KAMA, a 3D Keypoint Aware Mesh Articulation approach that allows us to estimate a human body mesh from the positions of 3D body keypoints. To this end, we learn to estimate 3D positions of 26 body keypoints and propose an analytical solution to articulate a parametric body model, SMPL, via a set of straightforward geometric transformations. Since keypoint estimation directly relies on image clues, our approach offers significantly better alignment to image content when compared to state-of-the-art approaches. Our proposed approach does not require any paired mesh annotations and is able to achieve state-of-the-art mesh fittings through 3D keypoint regression only. Results on the challenging 3DPW and Human3.6M demonstrate that our approach yields state-of-the-art body mesh fittings.



