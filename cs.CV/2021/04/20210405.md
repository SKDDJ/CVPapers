# Arxiv Papers in cs.CV on 2021-04-05
### Training Deep Neural Networks via Branch-and-Bound
- **Arxiv ID**: http://arxiv.org/abs/2104.01730v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01730v2)
- **Published**: 2021-04-05 00:43:03+00:00
- **Updated**: 2021-10-24 02:15:37+00:00
- **Authors**: Yuanwei Wu, Ziming Zhang, Guanghui Wang
- **Comment**: 29 pages, 17 figures. arXiv admin note: substantial text overlap with
  arXiv:1711.06959
- **Journal**: None
- **Summary**: In this paper, we propose BPGrad, a novel approximate algorithm for deep nueral network training, based on adaptive estimates of feasible region via branch-and-bound. The method is based on the assumption of Lipschitz continuity in objective function, and as a result, it can adaptively determine the step size for the current gradient given the history of previous updates. We prove that, by repeating such a branch-and-pruning procedure, it can achieve the optimal solution within finite iterations. A computationally efficient solver based on BPGrad has been proposed to train the deep neural networks. Empirical results demonstrate that BPGrad solver works well in practice and compares favorably to other stochastic optimization methods in the tasks of object recognition, detection, and segmentation. The code is available at \url{https://github.com/RyanCV/BPGrad}.



### Semantically Stealthy Adversarial Attacks against Segmentation Models
- **Arxiv ID**: http://arxiv.org/abs/2104.01732v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01732v3)
- **Published**: 2021-04-05 00:56:45+00:00
- **Updated**: 2022-01-07 07:29:04+00:00
- **Authors**: Zhenhua Chen, Chuhua Wang, David J. Crandall
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV), 2022, pp. 4080-4089
- **Summary**: Segmentation models have been found to be vulnerable to targeted and non-targeted adversarial attacks. However, the resulting segmentation outputs are often so damaged that it is easy to spot an attack. In this paper, we propose semantically stealthy adversarial attacks which can manipulate targeted labels while preserving non-targeted labels at the same time. One challenge is making semantically meaningful manipulations across datasets and models. Another challenge is avoiding damaging non-targeted labels. To solve these challenges, we consider each input image as prior knowledge to generate perturbations. We also design a special regularizer to help extract features. To evaluate our model's performance, we design three basic attack types, namely `vanishing into the context,' `embedding fake labels,' and `displacing target objects.' Our experiments show that our stealthy adversarial model can attack segmentation models with a relatively high success rate on Cityscapes, Mapillary, and BDD100K. Our framework shows good empirical generalization across datasets and models.



### Opportunistic Screening of Osteoporosis Using Plain Film Chest X-ray
- **Arxiv ID**: http://arxiv.org/abs/2104.01734v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01734v1)
- **Published**: 2021-04-05 01:25:23+00:00
- **Updated**: 2021-04-05 01:25:23+00:00
- **Authors**: Fakai Wang, Kang Zheng, Yirui Wang, Xiaoyun Zhou, Le Lu, Jing Xiao, Min Wu, Chang-Fu Kuo, Shun Miao
- **Comment**: None
- **Journal**: None
- **Summary**: Osteoporosis is a common chronic metabolic bone disease that is often under-diagnosed and under-treated due to the limited access to bone mineral density (BMD) examinations, Dual-energy X-ray Absorptiometry (DXA). In this paper, we propose a method to predict BMD from Chest X-ray (CXR), one of the most common, accessible, and low-cost medical image examinations. Our method first automatically detects Regions of Interest (ROIs) of local and global bone structures from the CXR. Then a multi-ROI model is developed to exploit both local and global information in the chest X-ray image for accurate BMD estimation. Our method is evaluated on 329 CXR cases with ground truth BMD measured by DXA. The model predicted BMD has a strong correlation with the gold standard DXA BMD (Pearson correlation coefficient 0.840). When applied for osteoporosis screening, it achieves a high classification performance (AUC 0.936). As the first effort in the field to use CXR scans to predict the spine BMD, the proposed algorithm holds strong potential in enabling early osteoporosis screening through routine chest X-rays and contributing to the enhancement of public health.



### A Dual-Critic Reinforcement Learning Framework for Frame-level Bit Allocation in HEVC/H.265
- **Arxiv ID**: http://arxiv.org/abs/2104.01735v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01735v1)
- **Published**: 2021-04-05 01:26:52+00:00
- **Updated**: 2021-04-05 01:26:52+00:00
- **Authors**: Yung-Han Ho, Guo-Lun Jin, Yun Liang, Wen-Hsiao Peng, Xiaobo Li
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a dual-critic reinforcement learning (RL) framework to address the problem of frame-level bit allocation in HEVC/H.265. The objective is to minimize the distortion of a group of pictures (GOP) under a rate constraint. Previous RL-based methods tackle such a constrained optimization problem by maximizing a single reward function that often combines a distortion and a rate reward. However, the way how these rewards are combined is usually ad hoc and may not generalize well to various coding conditions and video sequences. To overcome this issue, we adapt the deep deterministic policy gradient (DDPG) reinforcement learning algorithm for use with two critics, with one learning to predict the distortion reward and the other the rate reward. In particular, the distortion critic works to update the agent when the rate constraint is satisfied. By contrast, the rate critic makes the rate constraint a priority when the agent goes over the bit budget. Experimental results on commonly used datasets show that our method outperforms the bit allocation scheme in x265 and the single-critic baseline by a significant margin in terms of rate-distortion performance while offering fairly precise rate control.



### Explainability-aided Domain Generalization for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2104.01742v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01742v1)
- **Published**: 2021-04-05 02:27:01+00:00
- **Updated**: 2021-04-05 02:27:01+00:00
- **Authors**: Robin M. Schmidt
- **Comment**: None
- **Journal**: None
- **Summary**: Traditionally, for most machine learning settings, gaining some degree of explainability that tries to give users more insights into how and why the network arrives at its predictions, restricts the underlying model and hinders performance to a certain degree. For example, decision trees are thought of as being more explainable than deep neural networks but they lack performance on visual tasks. In this work, we empirically demonstrate that applying methods and architectures from the explainability literature can, in fact, achieve state-of-the-art performance for the challenging task of domain generalization while offering a framework for more insights into the prediction and training process. For that, we develop a set of novel algorithms including DivCAM, an approach where the network receives guidance during training via gradient based class activation maps to focus on a diverse set of discriminative features, as well as ProDrop and D-Transformers which apply prototypical networks to the domain generalization task, either with self-challenging or attention alignment. Since these methods offer competitive performance on top of explainability, we argue that the proposed methods can be used as a tool to improve the robustness of deep neural network architectures.



### A Video Is Worth Three Views: Trigeminal Transformers for Video-based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2104.01745v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.01745v1)
- **Published**: 2021-04-05 02:50:16+00:00
- **Updated**: 2021-04-05 02:50:16+00:00
- **Authors**: Xuehu Liu, Pingping Zhang, Chenyang Yu, Huchuan Lu, Xuesheng Qian, Xiaoyun Yang
- **Comment**: This work includes 10 pages, 5 figures and 4 Tables
- **Journal**: None
- **Summary**: Video-based person re-identification (Re-ID) aims to retrieve video sequences of the same person under non-overlapping cameras. Previous methods usually focus on limited views, such as spatial, temporal or spatial-temporal view, which lack of the observations in different feature domains. To capture richer perceptions and extract more comprehensive video representations, in this paper we propose a novel framework named Trigeminal Transformers (TMT) for video-based person Re-ID. More specifically, we design a trigeminal feature extractor to jointly transform raw video data into spatial, temporal and spatial-temporal domain. Besides, inspired by the great success of vision transformer, we introduce the transformer structure for video-based person Re-ID. In our work, three self-view transformers are proposed to exploit the relationships between local features for information enhancement in spatial, temporal and spatial-temporal domains. Moreover, a cross-view transformer is proposed to aggregate the multi-view features for comprehensive video representations. The experimental results indicate that our approach can achieve better performance than other state-of-the-art approaches on public Re-ID benchmarks. We will release the code for model reproduction.



### Perceptual Indistinguishability-Net (PI-Net): Facial Image Obfuscation with Manipulable Semantics
- **Arxiv ID**: http://arxiv.org/abs/2104.01753v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01753v2)
- **Published**: 2021-04-05 03:40:07+00:00
- **Updated**: 2021-04-07 09:06:15+00:00
- **Authors**: Jia-Wei Chen, Li-Ju Chen, Chia-Mu Yu, Chun-Shien Lu
- **Comment**: None
- **Journal**: None
- **Summary**: With the growing use of camera devices, the industry has many image datasets that provide more opportunities for collaboration between the machine learning community and industry. However, the sensitive information in the datasets discourages data owners from releasing these datasets. Despite recent research devoted to removing sensitive information from images, they provide neither meaningful privacy-utility trade-off nor provable privacy guarantees. In this study, with the consideration of the perceptual similarity, we propose perceptual indistinguishability (PI) as a formal privacy notion particularly for images. We also propose PI-Net, a privacy-preserving mechanism that achieves image obfuscation with PI guarantee. Our study shows that PI-Net achieves significantly better privacy utility trade-off through public image data.



### Potential Convolution: Embedding Point Clouds into Potential Fields
- **Arxiv ID**: http://arxiv.org/abs/2104.01754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01754v1)
- **Published**: 2021-04-05 03:46:09+00:00
- **Updated**: 2021-04-05 03:46:09+00:00
- **Authors**: Dengsheng Chen, Haowen Deng, Jun Li, Duo Li, Yao Duan, Kai Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, various convolutions based on continuous or discrete kernels for point cloud processing have been widely studied, and achieve impressive performance in many applications, such as shape classification, scene segmentation and so on. However, they still suffer from some drawbacks. For continuous kernels, the inaccurate estimation of the kernel weights constitutes a bottleneck for further improving the performance; while for discrete ones, the kernels represented as the points located in the 3D space are lack of rich geometry information. In this work, rather than defining a continuous or discrete kernel, we directly embed convolutional kernels into the learnable potential fields, giving rise to potential convolution. It is convenient for us to define various potential functions for potential convolution which can generalize well to a wide range of tasks. Specifically, we provide two simple yet effective potential functions via point-wise convolution operations. Comprehensive experiments demonstrate the effectiveness of our method, which achieves superior performance on the popular 3D shape classification and scene segmentation benchmarks compared with other state-of-the-art point convolution methods.



### 3D Human Body Reshaping with Anthropometric Modeling
- **Arxiv ID**: http://arxiv.org/abs/2104.01762v1
- **DOI**: 10.1007/978-981-10-8530-7_10
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2104.01762v1)
- **Published**: 2021-04-05 04:09:39+00:00
- **Updated**: 2021-04-05 04:09:39+00:00
- **Authors**: Yanhong Zeng, Jianlong Fu, Hongyang Chao
- **Comment**: ICIMCS 2017(oral). The final publication is available at Springer via
  https://doi.org/10.1007/978-981-10-8530-7_10
- **Journal**: In International Conference on Internet Multimedia Computing and
  Service (pp. 96-107). Springer, Singapore (2017)
- **Summary**: Reshaping accurate and realistic 3D human bodies from anthropometric parameters (e.g., height, chest size, etc.) poses a fundamental challenge for person identification, online shopping and virtual reality. Existing approaches for creating such 3D shapes often suffer from complex measurement by range cameras or high-end scanners, which either involve heavy expense cost or result in low quality. However, these high-quality equipments limit existing approaches in real applications, because the equipments are not easily accessible for common users. In this paper, we have designed a 3D human body reshaping system by proposing a novel feature-selection-based local mapping technique, which enables automatic anthropometric parameter modeling for each body facet. Note that the proposed approach can leverage limited anthropometric parameters (i.e., 3-5 measurements) as input, which avoids complex measurement, and thus better user-friendly experience can be achieved in real scenarios. Specifically, the proposed reshaping model consists of three steps. First, we calculate full-body anthropometric parameters from limited user inputs by imputation technique, and thus essential anthropometric parameters for 3D body reshaping can be obtained. Second, we select the most relevant anthropometric parameters for each facet by adopting relevance masks, which are learned offline by the proposed local mapping technique. Third, we generate the 3D body meshes by mapping matrices, which are learned by linear regression from the selected parameters to mesh-based body representation. We conduct experiments by anthropomorphic evaluation and a user study from 68 volunteers. Experiments show the superior results of the proposed system in terms of mean reconstruction error against the state-of-the-art approaches.



### GSECnet: Ground Segmentation of Point Clouds for Edge Computing
- **Arxiv ID**: http://arxiv.org/abs/2104.01766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01766v1)
- **Published**: 2021-04-05 04:29:28+00:00
- **Updated**: 2021-04-05 04:29:28+00:00
- **Authors**: Dong He, Jie Cheng, Jong-Hwan Kim
- **Comment**: 6 pages, 5 figures, code is available at
  https://github.com/SAMMiCA/GSECnet
- **Journal**: None
- **Summary**: Ground segmentation of point clouds remains challenging because of the sparse and unordered data structure. This paper proposes the GSECnet - Ground Segmentation network for Edge Computing, an efficient ground segmentation framework of point clouds specifically designed to be deployable on a low-power edge computing unit. First, raw point clouds are converted into a discretization representation by pillarization. Afterward, features of points within pillars are fed into PointNet to get the corresponding pillars feature map. Then, a depthwise-separable U-Net with the attention module learns the classification from the pillars feature map with an enormously diminished model parameter size. Our proposed framework is evaluated on SemanticKITTI against both point-based and discretization-based state-of-the-art learning approaches, and achieves an excellent balance between high accuracy and low computing complexity. Remarkably, our framework achieves the inference runtime of 135.2 Hz on a desktop platform. Moreover, experiments verify that it is deployable on a low-power edge computing unit powered 10 watts only.



### Procrustean Training for Imbalanced Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.01769v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01769v2)
- **Published**: 2021-04-05 04:44:01+00:00
- **Updated**: 2021-10-10 17:53:22+00:00
- **Authors**: Han-Jia Ye, De-Chuan Zhan, Wei-Lun Chao
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Neural networks trained with class-imbalanced data are known to perform poorly on minor classes of scarce training data. Several recent works attribute this to over-fitting to minor classes. In this paper, we provide a novel explanation of this issue. We found that a neural network tends to first under-fit the minor classes by classifying most of their data into the major classes in early training epochs. To correct these wrong predictions, the neural network then must focus on pushing features of minor class data across the decision boundaries between major and minor classes, leading to much larger gradients for features of minor classes. We argue that such an under-fitting phase over-emphasizes the competition between major and minor classes, hinders the neural network from learning the discriminative knowledge that can be generalized to test data, and eventually results in over-fitting. To address this issue, we propose a novel learning strategy to equalize the training progress across classes. We mix features of the major class data with those of other data in a mini-batch, intentionally weakening their features to prevent a neural network from fitting them first. We show that this strategy can largely balance the training accuracy and feature gradients across classes, effectively mitigating the under-fitting then over-fitting problem for minor class data. On several benchmark datasets, our approach achieves the state-of-the-art accuracy, especially for the challenging step-imbalanced cases.



### FocusNetv2: Imbalanced Large and Small Organ Segmentation with Adversarial Shape Constraint for Head and Neck CT Images
- **Arxiv ID**: http://arxiv.org/abs/2104.01771v1
- **DOI**: 10.1016/j.media.2020.101831
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01771v1)
- **Published**: 2021-04-05 04:45:31+00:00
- **Updated**: 2021-04-05 04:45:31+00:00
- **Authors**: Yunhe Gao, Rui Huang, Yiwei Yang, Jie Zhang, Kainan Shao, Changjuan Tao, Yuanyuan Chen, Dimitris N. Metaxas, Hongsheng Li, Ming Chen
- **Comment**: Accepted by Medical Image Analysis
- **Journal**: None
- **Summary**: Radiotherapy is a treatment where radiation is used to eliminate cancer cells. The delineation of organs-at-risk (OARs) is a vital step in radiotherapy treatment planning to avoid damage to healthy organs. For nasopharyngeal cancer, more than 20 OARs are needed to be precisely segmented in advance. The challenge of this task lies in complex anatomical structure, low-contrast organ contours, and the extremely imbalanced size between large and small organs. Common segmentation methods that treat them equally would generally lead to inaccurate small-organ labeling. We propose a novel two-stage deep neural network, FocusNetv2, to solve this challenging problem by automatically locating, ROI-pooling, and segmenting small organs with specifically designed small-organ localization and segmentation sub-networks while maintaining the accuracy of large organ segmentation. In addition to our original FocusNet, we employ a novel adversarial shape constraint on small organs to ensure the consistency between estimated small-organ shapes and organ shape prior knowledge. Our proposed framework is extensively tested on both self-collected dataset of 1,164 CT scans and the MICCAI Head and Neck Auto Segmentation Challenge 2015 dataset, which shows superior performance compared with state-of-the-art head and neck OAR segmentation methods.



### Convolutional Neural Opacity Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2104.01772v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01772v1)
- **Published**: 2021-04-05 04:46:46+00:00
- **Updated**: 2021-04-05 04:46:46+00:00
- **Authors**: Haimin Luo, Anpei Chen, Qixuan Zhang, Bai Pang, Minye Wu, Lan Xu, Jingyi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Photo-realistic modeling and rendering of fuzzy objects with complex opacity are critical for numerous immersive VR/AR applications, but it suffers from strong view-dependent brightness, color. In this paper, we propose a novel scheme to generate opacity radiance fields with a convolutional neural renderer for fuzzy objects, which is the first to combine both explicit opacity supervision and convolutional mechanism into the neural radiance field framework so as to enable high-quality appearance and global consistent alpha mattes generation in arbitrary novel views. More specifically, we propose an efficient sampling strategy along with both the camera rays and image plane, which enables efficient radiance field sampling and learning in a patch-wise manner, as well as a novel volumetric feature integration scheme that generates per-patch hybrid feature embeddings to reconstruct the view-consistent fine-detailed appearance and opacity output. We further adopt a patch-wise adversarial training scheme to preserve both high-frequency appearance and opacity details in a self-supervised framework. We also introduce an effective multi-view image capture system to capture high-quality color and alpha maps for challenging fuzzy objects. Extensive experiments on existing and our new challenging fuzzy object dataset demonstrate that our method achieves photo-realistic, globally consistent, and fined detailed appearance and opacity free-viewpoint rendering for various fuzzy objects.



### Reducing Racial Bias in Facial Age Prediction using Unsupervised Domain Adaptation in Regression
- **Arxiv ID**: http://arxiv.org/abs/2104.01781v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.01781v1)
- **Published**: 2021-04-05 05:31:12+00:00
- **Updated**: 2021-04-05 05:31:12+00:00
- **Authors**: Apoorva Gokhale, Astuti Sharma, Kaustav Datta, Savyasachi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an approach for unsupervised domain adaptation for the task of estimating someone's age from a given face image. In order to avoid the propagation of racial bias in most publicly available face image datasets into the inefficacy of models trained on them, we perform domain adaptation to motivate the predictor to learn features that are invariant to ethnicity, enhancing the generalization performance across faces of people from different ethnic backgrounds. Exploiting the ordinality of age, we also impose ranking constraints on the prediction of the model and design our model such that it takes as input a pair of images, and outputs both the relative age difference and the rank of the first identity with respect to the other in terms of their ages. Furthermore, we implement Multi-Dimensional Scaling to retrieve absolute ages from the predicted age differences from as few as two labeled images from the domain to be adapted to. We experiment with a publicly available dataset with age labels, dividing it into subsets based on the ethnicity labels, and evaluating the performance of our approach on the data from an ethnicity different from the one that the model is trained on. Additionally, we impose a constraint to preserve the sanity of the predictions with respect to relative and absolute ages, and another to ensure the smoothness of the predictions with respect to the input. We experiment extensively and compare various domain adaptation approaches for the task of regression.



### BTS-Net: Bi-directional Transfer-and-Selection Network For RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.01784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01784v1)
- **Published**: 2021-04-05 05:58:43+00:00
- **Updated**: 2021-04-05 05:58:43+00:00
- **Authors**: Wenbo Zhang, Yao Jiang, Keren Fu, Qijun Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Depth information has been proved beneficial in RGB-D salient object detection (SOD). However, depth maps obtained often suffer from low quality and inaccuracy. Most existing RGB-D SOD models have no cross-modal interactions or only have unidirectional interactions from depth to RGB in their encoder stages, which may lead to inaccurate encoder features when facing low quality depth. To address this limitation, we propose to conduct progressive bi-directional interactions as early in the encoder stage, yielding a novel bi-directional transfer-and-selection network named BTS-Net, which adopts a set of bi-directional transfer-and-selection (BTS) modules to purify features during encoding. Based on the resulting robust encoder features, we also design an effective light-weight group decoder to achieve accurate final saliency prediction. Comprehensive experiments on six widely used datasets demonstrate that BTS-Net surpasses 16 latest state-of-the-art approaches in terms of four key metrics.



### Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks and Defenses
- **Arxiv ID**: http://arxiv.org/abs/2104.01789v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2104.01789v2)
- **Published**: 2021-04-05 06:31:47+00:00
- **Updated**: 2021-04-10 02:28:02+00:00
- **Authors**: Yao Deng, Tiehua Zhang, Guannan Lou, Xi Zheng, Jiong Jin, Qing-Long Han
- **Comment**: None
- **Journal**: None
- **Summary**: The rapid development of artificial intelligence, especially deep learning technology, has advanced autonomous driving systems (ADSs) by providing precise control decisions to counterpart almost any driving event, spanning from anti-fatigue safe driving to intelligent route planning. However, ADSs are still plagued by increasing threats from different attacks, which could be categorized into physical attacks, cyberattacks and learning-based adversarial attacks. Inevitably, the safety and security of deep learning-based autonomous driving are severely challenged by these attacks, from which the countermeasures should be analyzed and studied comprehensively to mitigate all potential risks. This survey provides a thorough analysis of different attacks that may jeopardize ADSs, as well as the corresponding state-of-the-art defense mechanisms. The analysis is unrolled by taking an in-depth overview of each step in the ADS workflow, covering adversarial attacks for various deep learning models and attacks in both physical and cyber context. Furthermore, some promising research directions are suggested in order to improve deep learning-based autonomous driving safety, including model robustness training, model testing and verification, and anomaly detection based on cloud/edge servers.



### Hierarchical Pyramid Representations for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.01792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01792v1)
- **Published**: 2021-04-05 06:39:12+00:00
- **Updated**: 2021-04-05 06:39:12+00:00
- **Authors**: Hiroaki Aizawa, Yukihiro Domae, Kunihito Kato
- **Comment**: Project page: https://aizawan.github.io/hdca/
- **Journal**: None
- **Summary**: Understanding the context of complex and cluttered scenes is a challenging problem for semantic segmentation. However, it is difficult to model the context without prior and additional supervision because the scene's factors, such as the scale, shape, and appearance of objects, vary considerably in these scenes. To solve this, we propose to learn the structures of objects and the hierarchy among objects because context is based on these intrinsic properties. In this study, we design novel hierarchical, contextual, and multiscale pyramidal representations to capture the properties from an input image. Our key idea is the recursive segmentation in different hierarchical regions based on a predefined number of regions and the aggregation of the context in these regions. The aggregated contexts are used to predict the contextual relationship between the regions and partition the regions in the following hierarchical level. Finally, by constructing the pyramid representations from the recursively aggregated context, multiscale and hierarchical properties are attained. In the experiments, we confirmed that our proposed method achieves state-of-the-art performance in PASCAL Context.



### Monocular 3D Multi-Person Pose Estimation by Integrating Top-Down and Bottom-Up Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.01797v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01797v2)
- **Published**: 2021-04-05 07:05:21+00:00
- **Updated**: 2021-04-07 06:22:10+00:00
- **Authors**: Yu Cheng, Bo Wang, Bo Yang, Robby T. Tan
- **Comment**: Accepted to CVPR 2021. Code is available at:
  https://github.com/3dpose/3D-Multi-Person-Pose
- **Journal**: None
- **Summary**: In monocular video 3D multi-person pose estimation, inter-person occlusion and close interactions can cause human detection to be erroneous and human-joints grouping to be unreliable. Existing top-down methods rely on human detection and thus suffer from these problems. Existing bottom-up methods do not use human detection, but they process all persons at once at the same scale, causing them to be sensitive to multiple-persons scale variations. To address these challenges, we propose the integration of top-down and bottom-up approaches to exploit their strengths. Our top-down network estimates human joints from all persons instead of one in an image patch, making it robust to possible erroneous bounding boxes. Our bottom-up network incorporates human-detection based normalized heatmaps, allowing the network to be more robust in handling scale variations. Finally, the estimated 3D poses from the top-down and bottom-up networks are fed into our integration network for final 3D poses. Besides the integration of top-down and bottom-up networks, unlike existing pose discriminators that are designed solely for single person, and consequently cannot assess natural inter-person interactions, we propose a two-person pose discriminator that enforces natural two-person interactions. Lastly, we also apply a semi-supervised method to overcome the 3D ground-truth data scarcity. Our quantitative and qualitative evaluations show the effectiveness of our method compared to the state-of-the-art baselines.



### Task-Independent Knowledge Makes for Transferable Representations for Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.01832v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.01832v1)
- **Published**: 2021-04-05 10:05:48+00:00
- **Updated**: 2021-04-05 10:05:48+00:00
- **Authors**: Chaoqun Wang, Xuejin Chen, Shaobo Min, Xiaoyan Sun, Houqiang Li
- **Comment**: Accepted at AAAI2021
- **Journal**: None
- **Summary**: Generalized Zero-Shot Learning (GZSL) targets recognizing new categories by learning transferable image representations. Existing methods find that, by aligning image representations with corresponding semantic labels, the semantic-aligned representations can be transferred to unseen categories. However, supervised by only seen category labels, the learned semantic knowledge is highly task-specific, which makes image representations biased towards seen categories. In this paper, we propose a novel Dual-Contrastive Embedding Network (DCEN) that simultaneously learns task-specific and task-independent knowledge via semantic alignment and instance discrimination. First, DCEN leverages task labels to cluster representations of the same semantic category by cross-modal contrastive learning and exploring semantic-visual complementarity. Besides task-specific knowledge, DCEN then introduces task-independent knowledge by attracting representations of different views of the same image and repelling representations of different images. Compared to high-level seen category supervision, this instance discrimination supervision encourages DCEN to capture low-level visual knowledge, which is less biased toward seen categories and alleviates the representation bias. Consequently, the task-specific and task-independent knowledge jointly make for transferable representations of DCEN, which obtains averaged 4.1% improvement on four public benchmarks.



### Unsupervised Multi-source Domain Adaptation Without Access to Source Data
- **Arxiv ID**: http://arxiv.org/abs/2104.01845v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01845v1)
- **Published**: 2021-04-05 10:45:12+00:00
- **Updated**: 2021-04-05 10:45:12+00:00
- **Authors**: Sk Miraj Ahmed, Dripta S. Raychaudhuri, Sujoy Paul, Samet Oymak, Amit K. Roy-Chowdhury
- **Comment**: This paper will appear at CVPR 2021
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) aims to learn a predictor model for an unlabeled domain by transferring knowledge from a separate labeled source domain. However, most of these conventional UDA approaches make the strong assumption of having access to the source data during training, which may not be very practical due to privacy, security and storage concerns. A recent line of work addressed this problem and proposed an algorithm that transfers knowledge to the unlabeled target domain from a single source model without requiring access to the source data. However, for adaptation purposes, if there are multiple trained source models available to choose from, this method has to go through adapting each and every model individually, to check for the best source. Thus, we ask the question: can we find the optimal combination of source models, with no source data and without target labels, whose performance is no worse than the single best source? To answer this, we propose a novel and efficient algorithm which automatically combines the source models with suitable weights in such a way that it performs at least as good as the best source model. We provide intuitive theoretical insights to justify our claim. Furthermore, extensive experiments are conducted on several benchmark datasets to show the effectiveness of our algorithm, where in most cases, our method not only reaches best source accuracy but also outperforms it.



### Integrating 2D and 3D Digital Plant Information Towards Automatic Generation of Digital Twins
- **Arxiv ID**: http://arxiv.org/abs/2104.01854v1
- **DOI**: 10.1109/ISIE45063.2020.9152371
- **Categories**: **eess.SY**, cs.AI, cs.CV, cs.IT, cs.SE, cs.SY, math.IT, 93-04, H.0
- **Links**: [PDF](http://arxiv.org/pdf/2104.01854v1)
- **Published**: 2021-04-05 11:07:05+00:00
- **Updated**: 2021-04-05 11:07:05+00:00
- **Authors**: Seppo Sierla, Mohammad Azangoo, Alexander Fay, Valeriy Vyatkin, Nikolaos Papakonstantinou
- **Comment**: 8 pages, 13 figures
- **Journal**: None
- **Summary**: Ongoing standardization in Industry 4.0 supports tool vendor neutral representations of Piping and Instrumentation diagrams as well as 3D pipe routing. However, a complete digital plant model requires combining these two representations. 3D pipe routing information is essential for building any accurate first-principles process simulation model. Piping and instrumentation diagrams are the primary source for control loops. In order to automatically integrate these information sources to a unified digital plant model, it is necessary to develop algorithms for identifying corresponding elements such as tanks and pumps from piping and instrumentation diagrams and 3D CAD models. One approach is to raise these two information sources to a common level of abstraction and to match them at this level of abstraction. Graph matching is a potential technique for this purpose. This article focuses on automatic generation of the graphs as a prerequisite to graph matching. Algorithms for this purpose are proposed and validated with a case study. The paper concludes with a discussion of further research needed to reprocess the generated graphs in order to enable effective matching.



### Lipstick ain't enough: Beyond Color Matching for In-the-Wild Makeup Transfer
- **Arxiv ID**: http://arxiv.org/abs/2104.01867v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01867v1)
- **Published**: 2021-04-05 12:12:56+00:00
- **Updated**: 2021-04-05 12:12:56+00:00
- **Authors**: Thao Nguyen, Anh Tran, Minh Hoai
- **Comment**: Accepted to CVPR'21
- **Journal**: None
- **Summary**: Makeup transfer is the task of applying on a source face the makeup style from a reference image. Real-life makeups are diverse and wild, which cover not only color-changing but also patterns, such as stickers, blushes, and jewelries. However, existing works overlooked the latter components and confined makeup transfer to color manipulation, focusing only on light makeup styles. In this work, we propose a holistic makeup transfer framework that can handle all the mentioned makeup components. It consists of an improved color transfer branch and a novel pattern transfer branch to learn all makeup properties, including color, shape, texture, and location. To train and evaluate such a system, we also introduce new makeup datasets for real and synthetic extreme makeup. Experimental results show that our framework achieves the state of the art performance on both light and extreme makeup styles. Code is available at https://github.com/VinAIResearch/CPM.



### MetaHTR: Towards Writer-Adaptive Handwritten Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.01876v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01876v1)
- **Published**: 2021-04-05 12:35:39+00:00
- **Updated**: 2021-04-05 12:35:39+00:00
- **Authors**: Ayan Kumar Bhunia, Shuvozit Ghose, Amandeep Kumar, Pinaki Nath Chowdhury, Aneeshan Sain, Yi-Zhe Song
- **Comment**: IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021
- **Journal**: None
- **Summary**: Handwritten Text Recognition (HTR) remains a challenging problem to date, largely due to the varying writing styles that exist amongst us. Prior works however generally operate with the assumption that there is a limited number of styles, most of which have already been captured by existing datasets. In this paper, we take a completely different perspective -- we work on the assumption that there is always a new style that is drastically different, and that we will only have very limited data during testing to perform adaptation. This results in a commercially viable solution -- the model has the best shot at adaptation being exposed to the new style, and the few samples nature makes it practical to implement. We achieve this via a novel meta-learning framework which exploits additional new-writer data through a support set, and outputs a writer-adapted model via single gradient step update, all during inference. We discover and leverage on the important insight that there exists few key characters per writer that exhibit relatively larger style discrepancies. For that, we additionally propose to meta-learn instance specific weights for a character-wise cross-entropy loss, which is specifically designed to work with the sequential nature of text data. Our writer-adaptive MetaHTR framework can be easily implemented on the top of most state-of-the-art HTR models. Experiments show an average performance gain of 5-7% can be obtained by observing very few new style data. We further demonstrate via a set of ablative studies the advantage of our meta design when compared with alternative adaption mechanisms.



### Non-Homogeneous Haze Removal via Artificial Scene Prior and Bidimensional Graph Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2104.01888v2
- **DOI**: 10.1109/TIP.2021.3122806
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01888v2)
- **Published**: 2021-04-05 13:04:44+00:00
- **Updated**: 2022-11-15 10:48:24+00:00
- **Authors**: Haoran Wei, Qingbo Wu, Hui Li, King Ngi Ngan, Hongliang Li, Fanman Meng, Linfeng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the lack of natural scene and haze prior information, it is greatly challenging to completely remove the haze from a single image without distorting its visual content. Fortunately, the real-world haze usually presents non-homogeneous distribution, which provides us with many valuable clues in partial well-preserved regions. In this paper, we propose a Non-Homogeneous Haze Removal Network (NHRN) via artificial scene prior and bidimensional graph reasoning. Firstly, we employ the gamma correction iteratively to simulate artificial multiple shots under different exposure conditions, whose haze degrees are different and enrich the underlying scene prior. Secondly, beyond utilizing the local neighboring relationship, we build a bidimensional graph reasoning module to conduct non-local filtering in the spatial and channel dimensions of feature maps, which models their long-range dependency and propagates the natural scene prior between the well-preserved nodes and the nodes contaminated by haze. To the best of our knowledge, this is the first exploration to remove non-homogeneous haze via the graph reasoning based framework. We evaluate our method on different benchmark datasets. The results demonstrate that our method achieves superior performance over many state-of-the-art algorithms for both the single image dehazing and hazy image understanding tasks. The source code of the proposed NHRN is available on https://github.com/whrws/NHRNet.



### Adaptive Gradient Balancing for Undersampled MRI Reconstruction and Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2104.01889v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01889v1)
- **Published**: 2021-04-05 13:05:22+00:00
- **Updated**: 2021-04-05 13:05:22+00:00
- **Authors**: Itzik Malkiel, Sangtae Ahn, Valentina Taviani, Anne Menini, Lior Wolf, Christopher J. Hardy
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1905.00985
- **Journal**: None
- **Summary**: Recent accelerated MRI reconstruction models have used Deep Neural Networks (DNNs) to reconstruct relatively high-quality images from highly undersampled k-space data, enabling much faster MRI scanning. However, these techniques sometimes struggle to reconstruct sharp images that preserve fine detail while maintaining a natural appearance. In this work, we enhance the image quality by using a Conditional Wasserstein Generative Adversarial Network combined with a novel Adaptive Gradient Balancing (AGB) technique that automates the process of combining the adversarial and pixel-wise terms and streamlines hyperparameter tuning. In addition, we introduce a Densely Connected Iterative Network, which is an undersampled MRI reconstruction network that utilizes dense connections. In MRI, our method minimizes artifacts, while maintaining a high-quality reconstruction that produces sharper images than other techniques. To demonstrate the general nature of our method, it is further evaluated on a battery of image-to-image translation experiments, demonstrating an ability to recover from sub-optimal weighting in multi-term adversarial training.



### Adaptive Prototype Learning and Allocation for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.01893v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01893v2)
- **Published**: 2021-04-05 13:10:50+00:00
- **Updated**: 2021-05-16 12:49:14+00:00
- **Authors**: Gen Li, Varun Jampani, Laura Sevilla-Lara, Deqing Sun, Jonghyun Kim, Joongkyu Kim
- **Comment**: Accepted to CVPR2021
- **Journal**: None
- **Summary**: Prototype learning is extensively used for few-shot segmentation. Typically, a single prototype is obtained from the support feature by averaging the global object information. However, using one prototype to represent all the information may lead to ambiguities. In this paper, we propose two novel modules, named superpixel-guided clustering (SGC) and guided prototype allocation (GPA), for multiple prototype extraction and allocation. Specifically, SGC is a parameter-free and training-free approach, which extracts more representative prototypes by aggregating similar feature vectors, while GPA is able to select matched prototypes to provide more accurate guidance. By integrating the SGC and GPA together, we propose the Adaptive Superpixel-guided Network (ASGNet), which is a lightweight model and adapts to object scale and shape variation. In addition, our network can easily generalize to k-shot segmentation with substantial improvement and no additional computational cost. In particular, our evaluations on COCO demonstrate that ASGNet surpasses the state-of-the-art method by 5% in 5-shot segmentation.



### Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2104.01894v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.01894v3)
- **Published**: 2021-04-05 13:11:40+00:00
- **Updated**: 2021-06-15 17:03:38+00:00
- **Authors**: Ramon Sanabria, Austin Waters, Jason Baldridge
- **Comment**: Accepted to INTERSPEECH 2021
- **Journal**: None
- **Summary**: Speech-based image retrieval has been studied as a proxy for joint representation learning, usually without emphasis on retrieval itself. As such, it is unclear how well speech-based retrieval can work in practice -- both in an absolute sense and versus alternative strategies that combine automatic speech recognition (ASR) with strong text encoders. In this work, we extensively study and expand choices of encoder architectures, training methodology (including unimodal and multimodal pretraining), and other factors. Our experiments cover different types of speech in three datasets: Flickr Audio, Places Audio, and Localized Narratives. Our best model configuration achieves large gains over state of the art, e.g., pushing recall-at-one from 21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also show our best speech-based models can match or exceed cascaded ASR-to-text encoding when speech is spontaneous, accented, or otherwise hard to automatically transcribe.



### Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2104.01896v1
- **DOI**: 10.1016/j.media.2021.101989
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.01896v1)
- **Published**: 2021-04-05 13:15:22+00:00
- **Updated**: 2021-04-05 13:15:22+00:00
- **Authors**: Cheng Xue, Lei Zhu, Huazhu Fu, Xiaowei Hu, Xiaomeng Li, Hai Zhang, Pheng Ann Heng
- **Comment**: 16page,10 figures. Accepted by medical image analysis
- **Journal**: None
- **Summary**: Automatic breast lesion segmentation in ultrasound helps to diagnose breast cancer, which is one of the dreadful diseases that affect women globally. Segmenting breast regions accurately from ultrasound image is a challenging task due to the inherent speckle artifacts, blurry breast lesion boundaries, and inhomogeneous intensity distributions inside the breast lesion regions. Recently, convolutional neural networks (CNNs) have demonstrated remarkable results in medical image segmentation tasks. However, the convolutional operations in a CNN often focus on local regions, which suffer from limited capabilities in capturing long-range dependencies of the input ultrasound image, resulting in degraded breast lesion segmentation accuracy. In this paper, we develop a deep convolutional neural network equipped with a global guidance block (GGB) and breast lesion boundary detection (BD) modules for boosting the breast ultrasound lesion segmentation. The GGB utilizes the multi-layer integrated feature map as a guidance information to learn the long-range non-local dependencies from both spatial and channel domains. The BD modules learn additional breast lesion boundary map to enhance the boundary quality of a segmentation result refinement. Experimental results on a public dataset and a collected dataset show that our network outperforms other medical image segmentation methods and the recent semantic segmentation methods on breast ultrasound lesion segmentation. Moreover, we also show the application of our network on the ultrasound prostate segmentation, in which our method better identifies prostate regions than state-of-the-art networks.



### Few-Cost Salient Object Detection with Adversarial-Paced Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.01928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01928v1)
- **Published**: 2021-04-05 14:15:49+00:00
- **Updated**: 2021-04-05 14:15:49+00:00
- **Authors**: Dingwen Zhang, Haibin Tian, Jungong Han
- **Comment**: None
- **Journal**: 34th Conference on Neural Information Processing Systems (NeurIPS
  2020)
- **Summary**: Detecting and segmenting salient objects from given image scenes has received great attention in recent years. A fundamental challenge in training the existing deep saliency detection models is the requirement of large amounts of annotated data. While gathering large quantities of training data becomes cheap and easy, annotating the data is an expensive process in terms of time, labor and human expertise. To address this problem, this paper proposes to learn the effective salient object detection model based on the manual annotation on a few training images only, thus dramatically alleviating human labor in training models. To this end, we name this task as the few-cost salient object detection and propose an adversarial-paced learning (APL)-based framework to facilitate the few-cost learning scenario. Essentially, APL is derived from the self-paced learning (SPL) regime but it infers the robust learning pace through the data-driven adversarial learning mechanism rather than the heuristic design of the learning regularizer. Comprehensive experiments on four widely-used benchmark datasets demonstrate that the proposed method can effectively approach to the existing supervised deep salient object detection models with only 1k human-annotated training images. The project page is available at https://github.com/hb-stone/FC-SOD.



### Robust Trust Region for Weakly Supervised Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.01948v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01948v2)
- **Published**: 2021-04-05 15:11:29+00:00
- **Updated**: 2021-09-01 04:54:16+00:00
- **Authors**: Dmitrii Marin, Yuri Boykov
- **Comment**: Accepted to ICCV 2021
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2021, pp. 6608-6618
- **Summary**: Acquisition of training data for the standard semantic segmentation is expensive if requiring that each pixel is labeled. Yet, current methods significantly deteriorate in weakly supervised settings, e.g. where a fraction of pixels is labeled or when only image-level tags are available. It has been shown that regularized losses - originally developed for unsupervised low-level segmentation and representing geometric priors on pixel labels - can considerably improve the quality of weakly supervised training. However, many common priors require optimization stronger than gradient descent. Thus, such regularizers have limited applicability in deep learning. We propose a new robust trust region approach for regularized losses improving the state-of-the-art results. Our approach can be seen as a higher-order generalization of the classic chain rule. It allows neural network optimization to use strong low-level solvers for the corresponding regularizers, including discrete ones.



### Multi-Atlas Based Pathological Stratification of d-TGA Congenital Heart Disease
- **Arxiv ID**: http://arxiv.org/abs/2104.01960v1
- **DOI**: 10.1109/ISBI.2014.6867821
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01960v1)
- **Published**: 2021-04-05 15:28:39+00:00
- **Updated**: 2021-04-05 15:28:39+00:00
- **Authors**: Maria A. Zuluaga, Alex F. Mendelson, M. Jorge Cardoso, Andrew M. Taylor, Sébastien Ourselin
- **Comment**: In: IEEE International Symposium on Biomedical Imaging 2014
- **Journal**: None
- **Summary**: One of the main sources of error in multi-atlas segmentation propagation approaches comes from the use of atlas databases that are morphologically dissimilar to the target image. In this work, we exploit the segmentation errors associated with poor atlas selection to build a computer aided diagnosis (CAD) system for pathological classification in post-operative dextro-transposition of the great arteries (d-TGA). The proposed approach extracts a set of features, which describe the quality of a segmentation, and introduces them into a logical decision tree that provides the final diagnosis. We have validated our method on a set of 60 whole heart MR images containing healthy cases and two different forms of post-operative d-TGA. The reported overall CAD system accuracy was of 93.33%.



### Cascaded Robust Learning at Imperfect Labels for Chest X-ray Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.01975v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01975v1)
- **Published**: 2021-04-05 15:50:16+00:00
- **Updated**: 2021-04-05 15:50:16+00:00
- **Authors**: Cheng Xue, Qiao Deng, Xiaomeng Li, Qi Dou, Pheng Ann Heng
- **Comment**: 9pages, 4 figures. MICCAI 2020
- **Journal**: None
- **Summary**: The superior performance of CNN on medical image analysis heavily depends on the annotation quality, such as the number of labeled image, the source of image, and the expert experience. The annotation requires great expertise and labour. To deal with the high inter-rater variability, the study of imperfect label has great significance in medical image segmentation tasks. In this paper, we present a novel cascaded robust learning framework for chest X-ray segmentation with imperfect annotation. Our model consists of three independent network, which can effectively learn useful information from the peer networks. The framework includes two stages. In the first stage, we select the clean annotated samples via a model committee setting, the networks are trained by minimizing a segmentation loss using the selected clean samples. In the second stage, we design a joint optimization framework with label correction to gradually correct the wrong annotation and improve the network performance. We conduct experiments on the public chest X-ray image datasets collected by Shenzhen Hospital. The results show that our methods could achieve a significant improvement on the accuracy in segmentation tasks compared to the previous methods.



### HLA-Face: Joint High-Low Adaptation for Low Light Face Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.01984v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01984v1)
- **Published**: 2021-04-05 16:20:57+00:00
- **Updated**: 2021-04-05 16:20:57+00:00
- **Authors**: Wenjing Wang, Wenhan Yang, Jiaying Liu
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Face detection in low light scenarios is challenging but vital to many practical applications, e.g., surveillance video, autonomous driving at night. Most existing face detectors heavily rely on extensive annotations, while collecting data is time-consuming and laborious. To reduce the burden of building new datasets for low light conditions, we make full use of existing normal light data and explore how to adapt face detectors from normal light to low light. The challenge of this task is that the gap between normal and low light is too huge and complex for both pixel-level and object-level. Therefore, most existing low-light enhancement and adaptation methods do not achieve desirable performance. To address the issue, we propose a joint High-Low Adaptation (HLA) framework. Through a bidirectional low-level adaptation and multi-task high-level adaptation scheme, our HLA-Face outperforms state-of-the-art methods even without using dark face labels for training. Our project is publicly available at https://daooshee.github.io/HLA-Face-Website/



### Using spatial-temporal ensembles of convolutional neural networks for lumen segmentation in ureteroscopy
- **Arxiv ID**: http://arxiv.org/abs/2104.01985v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01985v1)
- **Published**: 2021-04-05 16:24:32+00:00
- **Updated**: 2021-04-05 16:24:32+00:00
- **Authors**: Jorge F. Lazo, Aldo Marzullo, Sara Moccia, Michele Catellani, Benoit Rosa, Michel de Mathelin, Elena De Momi
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Ureteroscopy is an efficient endoscopic minimally invasive technique for the diagnosis and treatment of upper tract urothelial carcinoma (UTUC). During ureteroscopy, the automatic segmentation of the hollow lumen is of primary importance, since it indicates the path that the endoscope should follow. In order to obtain an accurate segmentation of the hollow lumen, this paper presents an automatic method based on Convolutional Neural Networks (CNNs).   Methods: The proposed method is based on an ensemble of 4 parallel CNNs to simultaneously process single and multi-frame information. Of these, two architectures are taken as core-models, namely U-Net based in residual blocks($m_1$) and Mask-RCNN($m_2$), which are fed with single still-frames $I(t)$. The other two models ($M_1$, $M_2$) are modifications of the former ones consisting on the addition of a stage which makes use of 3D Convolutions to process temporal information. $M_1$, $M_2$ are fed with triplets of frames ($I(t-1)$, $I(t)$, $I(t+1)$) to produce the segmentation for $I(t)$.   Results: The proposed method was evaluated using a custom dataset of 11 videos (2,673 frames) which were collected and manually annotated from 6 patients. We obtain a Dice similarity coefficient of 0.80, outperforming previous state-of-the-art methods.   Conclusion: The obtained results show that spatial-temporal information can be effectively exploited by the ensemble model to improve hollow lumen segmentation in ureteroscopic images. The method is effective also in presence of poor visibility, occasional bleeding, or specular reflections.



### Can audio-visual integration strengthen robustness under multimodal attacks?
- **Arxiv ID**: http://arxiv.org/abs/2104.02000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2104.02000v1)
- **Published**: 2021-04-05 16:46:45+00:00
- **Updated**: 2021-04-05 16:46:45+00:00
- **Authors**: Yapeng Tian, Chenliang Xu
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: In this paper, we propose to make a systematic study on machines multisensory perception under attacks. We use the audio-visual event recognition task against multimodal adversarial attacks as a proxy to investigate the robustness of audio-visual learning. We attack audio, visual, and both modalities to explore whether audio-visual integration still strengthens perception and how different fusion mechanisms affect the robustness of audio-visual models. For interpreting the multimodal interactions under attacks, we learn a weakly-supervised sound source visual localization model to localize sounding regions in videos. To mitigate multimodal attacks, we propose an audio-visual defense approach based on an audio-visual dissimilarity constraint and external feature memory banks. Extensive experiments demonstrate that audio-visual models are susceptible to multimodal adversarial attacks; audio-visual integration could decrease the model robustness rather than strengthen under multimodal attacks; even a weakly-supervised sound source visual localization model can be successfully fooled; our defense method can improve the invulnerability of audio-visual networks without significantly sacrificing clean model performance.



### Domain Generalization with MixStyle
- **Arxiv ID**: http://arxiv.org/abs/2104.02008v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.02008v1)
- **Published**: 2021-04-05 16:58:09+00:00
- **Updated**: 2021-04-05 16:58:09+00:00
- **Authors**: Kaiyang Zhou, Yongxin Yang, Yu Qiao, Tao Xiang
- **Comment**: ICLR 2021; Code is available at
  https://github.com/KaiyangZhou/mixstyle-release
- **Journal**: None
- **Summary**: Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs.~sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.



### Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation
- **Arxiv ID**: http://arxiv.org/abs/2104.02026v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2104.02026v1)
- **Published**: 2021-04-05 17:30:41+00:00
- **Updated**: 2021-04-05 17:30:41+00:00
- **Authors**: Yapeng Tian, Di Hu, Chenliang Xu
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: There are rich synchronized audio and visual events in our daily life. Inside the events, audio scenes are associated with the corresponding visual objects; meanwhile, sounding objects can indicate and help to separate their individual sounds in the audio track. Based on this observation, in this paper, we propose a cyclic co-learning (CCoL) paradigm that can jointly learn sounding object visual grounding and audio-visual sound separation in a unified framework. Concretely, we can leverage grounded object-sound relations to improve the results of sound separation. Meanwhile, benefiting from discriminative information from separated sounds, we improve training example sampling for sounding object grounding, which builds a co-learning cycle for the two tasks and makes them mutually beneficial. Extensive experiments show that the proposed framework outperforms the compared recent approaches on both tasks, and they can benefit from each other with our cyclic co-learning.



### Automated lung segmentation from CT images of normal and COVID-19 pneumonia patients
- **Arxiv ID**: http://arxiv.org/abs/2104.02042v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2104.02042v1)
- **Published**: 2021-04-05 17:46:12+00:00
- **Updated**: 2021-04-05 17:46:12+00:00
- **Authors**: Faeze Gholamiankhah, Samaneh Mostafapour, Nouraddin Abdi Goushbolagh, Seyedjafar Shojaerazavi, Parvaneh Layegh, Seyyed Mohammad Tabatabaei, Hossein Arabi
- **Comment**: None
- **Journal**: None
- **Summary**: Automated semantic image segmentation is an essential step in quantitative image analysis and disease diagnosis. This study investigates the performance of a deep learning-based model for lung segmentation from CT images for normal and COVID-19 patients. Chest CT images and corresponding lung masks of 1200 confirmed COVID-19 cases were used for training a residual neural network. The reference lung masks were generated through semi-automated/manual segmentation of the CT images. The performance of the model was evaluated on two distinct external test datasets including 120 normal and COVID-19 subjects, and the results of these groups were compared to each other. Different evaluation metrics such as dice coefficient (DSC), mean absolute error (MAE), relative mean HU difference, and relative volume difference were calculated to assess the accuracy of the predicted lung masks. The proposed deep learning method achieved DSC of 0.980 and 0.971 for normal and COVID-19 subjects, respectively, demonstrating significant overlap between predicted and reference lung masks. Moreover, MAEs of 0.037 HU and 0.061 HU, relative mean HU difference of -2.679% and -4.403%, and relative volume difference of 2.405% and 5.928% were obtained for normal and COVID-19 subjects, respectively. The comparable performance in lung segmentation of the normal and COVID-19 patients indicates the accuracy of the model for the identification of the lung tissue in the presence of the COVID-19 induced infections (though slightly better performance was observed for normal patients). The promising results achieved by the proposed deep learning-based model demonstrated its reliability in COVID-19 lung segmentation. This prerequisite step would lead to a more efficient and robust pneumonia lesion analysis.



### Generating Furry Cars: Disentangling Object Shape & Appearance across Multiple Domains
- **Arxiv ID**: http://arxiv.org/abs/2104.02052v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.02052v1)
- **Published**: 2021-04-05 17:59:15+00:00
- **Updated**: 2021-04-05 17:59:15+00:00
- **Authors**: Utkarsh Ojha, Krishna Kumar Singh, Yong Jae Lee
- **Comment**: Camera ready version for ICLR 2021
- **Journal**: None
- **Summary**: We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars). The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively. This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains. Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.



### An Empirical Study of Training Self-Supervised Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.02057v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.02057v4)
- **Published**: 2021-04-05 17:59:40+00:00
- **Updated**: 2021-08-16 17:40:21+00:00
- **Authors**: Xinlei Chen, Saining Xie, Kaiming He
- **Comment**: Camera-ready, ICCV 2021, Oral. Code:
  https://github.com/facebookresearch/moco-v3
- **Journal**: None
- **Summary**: This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: self-supervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other self-supervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.



### Compressing Visual-linguistic Model via Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2104.02096v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.02096v1)
- **Published**: 2021-04-05 18:02:17+00:00
- **Updated**: 2021-04-05 18:02:17+00:00
- **Authors**: Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan Wang, Yezhou Yang, Zicheng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Despite exciting progress in pre-training for visual-linguistic (VL) representations, very few aspire to a small VL model. In this paper, we study knowledge distillation (KD) to effectively compress a transformer-based large VL model into a small VL model. The major challenge arises from the inconsistent regional visual tokens extracted from different detectors of Teacher and Student, resulting in the misalignment of hidden representations and attention distributions. To address the problem, we retrain and adapt the Teacher by using the same region proposals from Student's detector while the features are from Teacher's own object detector. With aligned network inputs, the adapted Teacher is capable of transferring the knowledge through the intermediate representations. Specifically, we use the mean square error loss to mimic the attention distribution inside the transformer block and present a token-wise noise contrastive loss to align the hidden state by contrasting with negative representations stored in a sample queue. To this end, we show that our proposed distillation significantly improves the performance of small VL models on image captioning and visual question answering tasks. It reaches 120.8 in CIDEr score on COCO captioning, an improvement of 5.1 over its non-distilled counterpart; and an accuracy of 69.8 on VQA 2.0, a 0.8 gain from the baseline. Our extensive experiments and ablations confirm the effectiveness of VL distillation in both pre-training and fine-tuning stages.



### Jekyll: Attacking Medical Image Diagnostics using Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2104.02107v1
- **DOI**: 10.1109/EuroSP48549.2020.00017
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.02107v1)
- **Published**: 2021-04-05 18:23:36+00:00
- **Updated**: 2021-04-05 18:23:36+00:00
- **Authors**: Neal Mangaokar, Jiameng Pu, Parantapa Bhattacharya, Chandan K. Reddy, Bimal Viswanath
- **Comment**: Published in proceedings of the 5th European Symposium on Security
  and Privacy (EuroS&P '20)
- **Journal**: None
- **Summary**: Advances in deep neural networks (DNNs) have shown tremendous promise in the medical domain. However, the deep learning tools that are helping the domain, can also be used against it. Given the prevalence of fraud in the healthcare domain, it is important to consider the adversarial use of DNNs in manipulating sensitive data that is crucial to patient healthcare. In this work, we present the design and implementation of a DNN-based image translation attack on biomedical imagery. More specifically, we propose Jekyll, a neural style transfer framework that takes as input a biomedical image of a patient and translates it to a new image that indicates an attacker-chosen disease condition. The potential for fraudulent claims based on such generated 'fake' medical images is significant, and we demonstrate successful attacks on both X-rays and retinal fundus image modalities. We show that these attacks manage to mislead both medical professionals and algorithmic detection schemes. Lastly, we also investigate defensive measures based on machine learning to detect images generated by Jekyll.



### Anchor-Constrained Viterbi for Set-Supervised Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.02113v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.02113v1)
- **Published**: 2021-04-05 18:50:21+00:00
- **Updated**: 2021-04-05 18:50:21+00:00
- **Authors**: Jun Li, Sinisa Todorovic
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: This paper is about action segmentation under weak supervision in training, where the ground truth provides only a set of actions present, but neither their temporal ordering nor when they occur in a training video. We use a Hidden Markov Model (HMM) grounded on a multilayer perceptron (MLP) to label video frames, and thus generate a pseudo-ground truth for the subsequent pseudo-supervised training. In testing, a Monte Carlo sampling of action sets seen in training is used to generate candidate temporal sequences of actions, and select the maximum posterior sequence. Our key contribution is a new anchor-constrained Viterbi algorithm (ACV) for generating the pseudo-ground truth, where anchors are salient action parts estimated for each action from a given ground-truth set. Our evaluation on the tasks of action segmentation and alignment on the benchmark Breakfast, MPII Cooking2, Hollywood Extended datasets demonstrates our superior performance relative to that of prior work.



### Action Shuffle Alternating Learning for Unsupervised Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.02116v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.02116v1)
- **Published**: 2021-04-05 18:58:57+00:00
- **Updated**: 2021-04-05 18:58:57+00:00
- **Authors**: Jun Li, Sinisa Todorovic
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: This paper addresses unsupervised action segmentation. Prior work captures the frame-level temporal structure of videos by a feature embedding that encodes time locations of frames in the video. We advance prior work with a new self-supervised learning (SSL) of a feature embedding that accounts for both frame- and action-level structure of videos. Our SSL trains an RNN to recognize positive and negative action sequences, and the RNN's hidden layer is taken as our new action-level feature embedding. The positive and negative sequences consist of action segments sampled from videos, where in the former the sampled action segments respect their time ordering in the video, and in the latter they are shuffled. As supervision of actions is not available and our SSL requires access to action segments, we specify an HMM that explicitly models action lengths, and infer a MAP action segmentation with the Viterbi algorithm. The resulting action segmentation is used as pseudo-ground truth for estimating our action-level feature embedding and updating the HMM. We alternate the above steps within the Generalized EM framework, which ensures convergence. Our evaluation on the Breakfast, YouTube Instructions, and 50Salads datasets gives superior results to those of the state of the art.



### Automatic Micro-Expression Apex Frame Spotting using Local Binary Pattern from Six Intersection Planes
- **Arxiv ID**: http://arxiv.org/abs/2104.02149v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.02149v1)
- **Published**: 2021-04-05 20:42:57+00:00
- **Updated**: 2021-04-05 20:42:57+00:00
- **Authors**: Vida Esmaeili, Mahmood Mohassel Feghhi, Seyed Omid Shahdi
- **Comment**: 6 pages, 7 figures, Presented at the 11 Iranian and the first
  International Conference on Machine Vision and Image Processing (MVIP), 19-20
  February 2020, https://mvip2020.ut.ac.ir/paper?manu=39079
- **Journal**: None
- **Summary**: Facial expressions are one of the most effective ways for non-verbal communications, which can be expressed as the Micro-Expression (ME) in the high-stake situations. The MEs are involuntary, rapid, and, subtle, and they can reveal real human intentions. However, their feature extraction is very challenging due to their low intensity and very short duration. Although Local Binary Pattern from Three Orthogonal Plane (LBP-TOP) feature extractor is useful for the ME analysis, it does not consider essential information. To address this problem, we propose a new feature extractor called Local Binary Pattern from Six Intersection Planes (LBP-SIPl). This method extracts LBP code on six intersection planes, and then it combines them. Results show that the proposed method has superior performance in apex frame spotting automatically in comparison with the relevant methods on the CASME database. Simulation results show that, using the proposed method, the apex frame has been spotted in 43% of subjects in the CASME database, automatically. Also, the mean absolute error of 1.76 is achieved, using our novel proposed method.



### Adaptive Clustering of Robust Semantic Representations for Adversarial Image Purification
- **Arxiv ID**: http://arxiv.org/abs/2104.02155v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.02155v2)
- **Published**: 2021-04-05 21:07:04+00:00
- **Updated**: 2021-04-07 15:22:42+00:00
- **Authors**: Samuel Henrique Silva, Arun Das, Ian Scarff, Peyman Najafirad
- **Comment**: 11 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: Deep Learning models are highly susceptible to adversarial manipulations that can lead to catastrophic consequences. One of the most effective methods to defend against such disturbances is adversarial training but at the cost of generalization of unseen attacks and transferability across models. In this paper, we propose a robust defense against adversarial attacks, which is model agnostic and generalizable to unseen adversaries. Initially, with a baseline model, we extract the latent representations for each class and adaptively cluster the latent representations that share a semantic similarity. We obtain the distributions for the clustered latent representations and from their originating images, we learn semantic reconstruction dictionaries (SRD). We adversarially train a new model constraining the latent space representation to minimize the distance between the adversarial latent representation and the true cluster distribution. To purify the image, we decompose the input into low and high-frequency components. The high-frequency component is reconstructed based on the most adequate SRD from the clean dataset. In order to evaluate the most adequate SRD, we rely on the distance between robust latent representations and semantic cluster distributions. The output is a purified image with no perturbation. Image purification on CIFAR-10 and ImageNet-10 using our proposed method improved the accuracy by more than 10% compared to state-of-the-art results.



### Unified Detection of Digital and Physical Face Attacks
- **Arxiv ID**: http://arxiv.org/abs/2104.02156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.02156v1)
- **Published**: 2021-04-05 21:08:28+00:00
- **Updated**: 2021-04-05 21:08:28+00:00
- **Authors**: Debayan Deb, Xiaoming Liu, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art defense mechanisms against face attacks achieve near perfect accuracies within one of three attack categories, namely adversarial, digital manipulation, or physical spoofs, however, they fail to generalize well when tested across all three categories. Poor generalization can be attributed to learning incoherent attacks jointly. To overcome this shortcoming, we propose a unified attack detection framework, namely UniFAD, that can automatically cluster 25 coherent attack types belonging to the three categories. Using a multi-task learning framework along with k-means clustering, UniFAD learns joint representations for coherent attacks, while uncorrelated attack types are learned separately. Proposed UniFAD outperforms prevailing defense methods and their fusion with an overall TDR = 94.73% @ 0.2% FDR on a large fake face dataset consisting of 341K bona fide images and 448K attack images of 25 types across all 3 categories. Proposed method can detect an attack within 3 milliseconds on a Nvidia 2080Ti. UniFAD can also identify the attack types and categories with 75.81% and 97.37% accuracies, respectively.



### Learning Optical Flow from a Few Matches
- **Arxiv ID**: http://arxiv.org/abs/2104.02166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.02166v1)
- **Published**: 2021-04-05 21:44:00+00:00
- **Updated**: 2021-04-05 21:44:00+00:00
- **Authors**: Shihao Jiang, Yao Lu, Hongdong Li, Richard Hartley
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: State-of-the-art neural network models for optical flow estimation require a dense correlation volume at high resolutions for representing per-pixel displacement. Although the dense correlation volume is informative for accurate estimation, its heavy computation and memory usage hinders the efficient training and deployment of the models. In this paper, we show that the dense correlation volume representation is redundant and accurate flow estimation can be achieved with only a fraction of elements in it. Based on this observation, we propose an alternative displacement representation, named Sparse Correlation Volume, which is constructed directly by computing the k closest matches in one feature map for each feature vector in the other feature map and stored in a sparse data structure. Experiments show that our method can reduce computational cost and memory use significantly, while maintaining high accuracy compared to previous approaches with dense correlation volumes. Code is available at https://github.com/zacjiang/scv .



### Insight about Detection, Prediction and Weather Impact of Coronavirus (Covid-19) using Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2104.02173v1
- **DOI**: 10.5121/ijaia.2020.11406
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.02173v1)
- **Published**: 2021-04-05 22:18:57+00:00
- **Updated**: 2021-04-05 22:18:57+00:00
- **Authors**: A K M Bahalul Haque, Tahmid Hasan Pranto, Abdulla All Noman, Atik Mahmood
- **Comment**: 15 Pages, 13 Figures and 4 Tables
- **Journal**: International Journal of Artificial Intelligence & Applications
  11(4):67-81, July. 2020
- **Summary**: The world is facing a tough situation due to the catastrophic pandemic caused by novel coronavirus (COVID-19). The number people affected by this virus are increasing exponentially day by day and the number has already crossed 6.4 million. As no vaccine has been discovered yet, the early detection of patients and isolation is the only and most effective way to reduce the spread of the virus. Detecting infected persons from chest X-Ray by using Deep Neural Networks, can be applied as a time and laborsaving solution. In this study, we tried to detect Covid-19 by classification of Covid-19, pneumonia and normal chest X-Rays. We used five different Convolutional Pre-Trained Neural Network models (VGG16, VGG19, Xception, InceptionV3 and Resnet50) and compared their performance. VGG16 and VGG19 shows precise performance in classification. Both models can classify between three kinds of X-Rays with an accuracy over 92%. Another part of our study was to find the impact of weather factors (temperature, humidity, sun hour and wind speed) on this pandemic using Decision Tree Regressor. We found that temperature, humidity and sun-hour jointly hold 85.88% impact on escalation of Covid-19 and 91.89% impact on death due to Covid-19 where humidity has 8.09% impact on death. We also tried to predict the death of an individual based on age, gender, country, and location due to COVID-19 using the LogisticRegression, which can predict death of an individual with a model accuracy of 94.40%.



