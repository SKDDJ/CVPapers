# Arxiv Papers in cs.CV on 2021-04-12
### Event-based Timestamp Image Encoding Network for Human Action Recognition and Anticipation
- **Arxiv ID**: http://arxiv.org/abs/2104.05145v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.05145v2)
- **Published**: 2021-04-12 00:43:31+00:00
- **Updated**: 2021-04-13 00:34:47+00:00
- **Authors**: Chaoxing Huang
- **Comment**: This paper has been accepted by IJCNN 2021,the International Joint
  Conference on Neural Networks. arXiv admin note: substantial text overlap
  with arXiv:2009.13049
- **Journal**: None
- **Summary**: Event camera is an asynchronous, high frequency vision sensor with low power consumption, which is suitable for human action understanding task. It is vital to encode the spatial-temporal information of event data properly and use standard computer vision tool to learn from the data. In this work, we propose a timestamp image encoding 2D network, which takes the encoded spatial-temporal images with polarity information of the event data as input and output the action label. In addition, we propose a future timestamp image generator to generate futureaction information to aid the model to anticipate the human action when the action is not completed. Experiment results show that our method can achieve the same level of performance as those RGB-based benchmarks on real world action recognition,and also achieve the state of the art (SOTA) result on gesture recognition. Our future timestamp image generating model can effectively improve the prediction accuracy when the action is not completed. We also provide insight discussion on the importance of motion and appearance information in action recognition and anticipation.



### Feature Decomposition and Reconstruction Learning for Effective Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.05160v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05160v2)
- **Published**: 2021-04-12 02:22:45+00:00
- **Updated**: 2021-04-21 01:19:47+00:00
- **Authors**: Delian Ruan, Yan Yan, Shenqi Lai, Zhenhua Chai, Chunhua Shen, Hanzi Wang
- **Comment**: accepted to CVPR 2021
- **Journal**: None
- **Summary**: In this paper, we propose a novel Feature Decomposition and Reconstruction Learning (FDRL) method for effective facial expression recognition. We view the expression information as the combination of the shared information (expression similarities) across different expressions and the unique information (expression-specific variations) for each expression. More specifically, FDRL mainly consists of two crucial networks: a Feature Decomposition Network (FDN) and a Feature Reconstruction Network (FRN). In particular, FDN first decomposes the basic features extracted from a backbone network into a set of facial action-aware latent features to model expression similarities. Then, FRN captures the intra-feature and inter-feature relationships for latent features to characterize expression-specific variations, and reconstructs the expression feature. To this end, two modules including an intra-feature relation modeling module and an inter-feature relation modeling module are developed in FRN. Experimental results on both the in-the-lab databases (including CK+, MMI, and Oulu-CASIA) and the in-the-wild databases (including RAF-DB and SFEW) show that the proposed FDRL method consistently achieves higher recognition accuracy than several state-of-the-art methods. This clearly highlights the benefit of feature decomposition and reconstruction for classifying expressions.



### A Learnable Self-supervised Task for Unsupervised Domain Adaptation on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2104.05164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05164v1)
- **Published**: 2021-04-12 02:30:16+00:00
- **Updated**: 2021-04-12 02:30:16+00:00
- **Authors**: Xiaoyuan Luo, Shaolei Liu, Kexue Fu, Manning Wang, Zhijian Song
- **Comment**: 10 pages, 4 figures, 5 tables
- **Journal**: None
- **Summary**: Deep neural networks have achieved promising performance in supervised point cloud applications, but manual annotation is extremely expensive and time-consuming in supervised learning schemes. Unsupervised domain adaptation (UDA) addresses this problem by training a model with only labeled data in the source domain but making the model generalize well in the target domain. Existing studies show that self-supervised learning using both source and target domain data can help improve the adaptability of trained models, but they all rely on hand-crafted designs of the self-supervised tasks. In this paper, we propose a learnable self-supervised task and integrate it into a self-supervision-based point cloud UDA architecture. Specifically, we propose a learnable nonlinear transformation that transforms a part of a point cloud to generate abundant and complicated point clouds while retaining the original semantic information, and the proposed self-supervised task is to reconstruct the original point cloud from the transformed ones. In the UDA architecture, an encoder is shared between the networks for the self-supervised task and the main task of point cloud classification or segmentation, so that the encoder can be trained to extract features suitable for both the source and the target domain data. Experiments on PointDA-10 and PointSegDA datasets show that the proposed method achieves new state-of-the-art performance on both classification and segmentation tasks of point cloud UDA. Code will be made publicly available.



### Object-Centric Representation Learning for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2104.05166v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05166v3)
- **Published**: 2021-04-12 02:37:20+00:00
- **Updated**: 2021-07-09 00:06:59+00:00
- **Authors**: Long Hoang Dang, Thao Minh Le, Vuong Le, Truyen Tran
- **Comment**: Accepted by IJCNN 2021
- **Journal**: None
- **Summary**: Video question answering (Video QA) presents a powerful testbed for human-like intelligent behaviors. The task demands new capabilities to integrate video processing, language understanding, binding abstract linguistic concepts to concrete visual artifacts, and deliberative reasoning over spacetime. Neural networks offer a promising approach to reach this potential through learning from examples rather than handcrafting features and rules. However, neural networks are predominantly feature-based - they map data to unstructured vectorial representation and thus can fall into the trap of exploiting shortcuts through surface statistics instead of true systematic reasoning seen in symbolic systems. To tackle this issue, we advocate for object-centric representation as a basis for constructing spatio-temporal structures from videos, essentially bridging the semantic gap between low-level pattern recognition and high-level symbolic algebra. To this end, we propose a new query-guided representation framework to turn a video into an evolving relational graph of objects, whose features and interactions are dynamically and conditionally inferred. The object lives are then summarized into resumes, lending naturally for deliberative relational reasoning that produces an answer to the query. The framework is evaluated on major Video QA datasets, demonstrating clear benefits of the object-centric approach to video reasoning.



### Egocentric Pose Estimation from Human Vision Span
- **Arxiv ID**: http://arxiv.org/abs/2104.05167v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05167v1)
- **Published**: 2021-04-12 02:38:22+00:00
- **Updated**: 2021-04-12 02:38:22+00:00
- **Authors**: Hao Jiang, Vamsi Krishna Ithapu
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Estimating camera wearer's body pose from an egocentric view (egopose) is a vital task in augmented and virtual reality. Existing approaches either use a narrow field of view front facing camera that barely captures the wearer, or an extruded head-mounted top-down camera for maximal wearer visibility. In this paper, we tackle the egopose estimation from a more natural human vision span, where camera wearer can be seen in the peripheral view and depending on the head pose the wearer may become invisible or has a limited partial view. This is a realistic visual field for user-centric wearable devices like glasses which have front facing wide angle cameras. Existing solutions are not appropriate for this setting, and so, we propose a novel deep learning system taking advantage of both the dynamic features from camera SLAM and the body shape imagery. We compute 3D head pose, 3D body pose, the figure/ground separation, all at the same time while explicitly enforcing a certain geometric consistency across pose attributes. We further show that this system can be trained robustly with lots of existing mocap data so we do not have to collect and annotate large new datasets. Lastly, our system estimates egopose in real time and on the fly while maintaining high accuracy.



### Memory-guided Unsupervised Image-to-image Translation
- **Arxiv ID**: http://arxiv.org/abs/2104.05170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05170v1)
- **Published**: 2021-04-12 03:02:51+00:00
- **Updated**: 2021-04-12 03:02:51+00:00
- **Authors**: Somi Jeong, Youngjung Kim, Eungbean Lee, Kwanghoon Sohn
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: We present a novel unsupervised framework for instance-level image-to-image translation. Although recent advances have been made by incorporating additional object annotations, existing methods often fail to handle images with multiple disparate objects. The main cause is that, during inference, they apply a global style to the whole image and do not consider the large style discrepancy between instance and background, or within instances. To address this problem, we propose a class-aware memory network that explicitly reasons about local style variations. A key-values memory structure, with a set of read/update operations, is introduced to record class-wise style variations and access them without requiring an object detector at the test time. The key stores a domain-agnostic content representation for allocating memory items, while the values encode domain-specific style representations. We also present a feature contrastive loss to boost the discriminative power of memory items. We show that by incorporating our memory, we can transfer class-aware and accurate style representations across domains. Experimental results demonstrate that our model outperforms recent instance-level methods and achieves state-of-the-art performance.



### Deep Recursive Embedding for High-Dimensional Data
- **Arxiv ID**: http://arxiv.org/abs/2104.05171v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05171v2)
- **Published**: 2021-04-12 03:04:38+00:00
- **Updated**: 2022-01-13 11:26:52+00:00
- **Authors**: Zixia Zhou, Yuanyuan Wang, Boudewijn P. F. Lelieveldt, Qian Tao
- **Comment**: Dear arXiv, We would like to withdraw this version, because it is in
  conflict with a later version uploaded and approved (arXiv:2111.00622). This
  version also contains some error: in Section 3.3, Eqn 1, a term p_{ij} is
  missing before log. We sincerely apologize and would like to withdraw to
  avoid any reader confusion. Sincerely, Zixia Zhou, Yuanyuan Wang, Boudewijn
  P.F. Lelieveldt, Qian Tao
- **Journal**: None
- **Summary**: t-distributed stochastic neighbor embedding (t-SNE) is a well-established visualization method for complex high-dimensional data. However, the original t-SNE method is nonparametric, stochastic, and often cannot well prevserve the global structure of data as it emphasizes local neighborhood. With t-SNE as a reference, we propose to combine the deep neural network (DNN) with the mathematical-grounded embedding rules for high-dimensional data embedding. We first introduce a deep embedding network (DEN) framework, which can learn a parametric mapping from high-dimensional space to low-dimensional embedding. DEN has a flexible architecture that can accommodate different input data (vector, image, or tensor) and loss functions. To improve the embedding performance, a recursive training strategy is proposed to make use of the latent representations extracted by DEN. Finally, we propose a two-stage loss function combining the advantages of two popular embedding methods, namely, t-SNE and uniform manifold approximation and projection (UMAP), for optimal visualization effect. We name the proposed method Deep Recursive Embedding (DRE), which optimizes DEN with a recursive training strategy and two-stage losse. Our experiments demonstrated the excellent performance of the proposed DRE method on high-dimensional data embedding, across a variety of public databases. Remarkably, our comparative results suggested that our proposed DRE could lead to improved global structure preservation.



### GarmentNets: Category-Level Pose Estimation for Garments via Canonical Space Shape Completion
- **Arxiv ID**: http://arxiv.org/abs/2104.05177v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, I.2.10; I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2104.05177v2)
- **Published**: 2021-04-12 03:18:00+00:00
- **Updated**: 2021-08-13 19:31:15+00:00
- **Authors**: Cheng Chi, Shuran Song
- **Comment**: None
- **Journal**: None
- **Summary**: This paper tackles the task of category-level pose estimation for garments. With a near infinite degree of freedom, a garment's full configuration (i.e., poses) is often described by the per-vertex 3D locations of its entire 3D surface. However, garments are also commonly subject to extreme cases of self-occlusion, especially when folded or crumpled, making it challenging to perceive their full 3D surface. To address these challenges, we propose GarmentNets, where the key idea is to formulate the deformable object pose estimation problem as a shape completion task in the canonical space. This canonical space is defined across garments instances within a category, therefore, specifies the shared category-level pose. By mapping the observed partial surface to the canonical space and completing it in this space, the output representation describes the garment's full configuration using a complete 3D mesh with the per-vertex canonical coordinate label. To properly handle the thin 3D structure presented on garments, we proposed a novel 3D shape representation using the generalized winding number field. Experiments demonstrate that GarmentNets is able to generalize to unseen garment instances and achieve significantly better performance compared to alternative approaches.



### Advances on image interpolation based on ant colony algorithm
- **Arxiv ID**: http://arxiv.org/abs/2104.12863v1
- **DOI**: 10.1186/s40064-016-2040-9
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.12863v1)
- **Published**: 2021-04-12 04:27:59+00:00
- **Updated**: 2021-04-12 04:27:59+00:00
- **Authors**: Olivier Rukundo, Hanqiang Cao
- **Comment**: 17 pages, 14 figures, 3 tables
- **Journal**: SpringerPlus, 5(1), 403, 2016
- **Summary**: This paper presents an advance on image interpolation based on ant colony algorithm (AACA) for high-resolution image scaling. The difference between the proposed algorithm and the previously proposed optimization of bilinear interpolation based on ant colony algorithm (OBACA) is that AACA uses global weighting, whereas OBACA uses a local weighting scheme. The strength of the proposed global weighting of the AACA algorithm depends on employing solely the pheromone matrix information present on any group of four adjacent pixels to decide which case deserves a maximum global weight value or not. Experimental results are further provided to show the higher performance of the proposed AACA algorithm with reference to the algorithms mentioned in this paper.



### SCPM-Net: An Anchor-free 3D Lung Nodule Detection Network using Sphere Representation and Center Points Matching
- **Arxiv ID**: http://arxiv.org/abs/2104.05215v2
- **DOI**: 10.1016/j.media.2021.102287
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05215v2)
- **Published**: 2021-04-12 05:51:29+00:00
- **Updated**: 2022-01-06 09:13:15+00:00
- **Authors**: Xiangde Luo, Tao Song, Guotai Wang, Jieneng Chen, Yinan Chen, Kang Li, Dimitris N. Metaxas, Shaoting Zhang
- **Comment**: accept to Medical Image Analysis
- **Journal**: None
- **Summary**: Lung nodule detection from 3D Computed Tomography scans plays a vital role in efficient lung cancer screening. Despite the SOTA performance obtained by recent anchor-based detectors using CNNs for this task, they require predetermined anchor parameters such as the size, number, and aspect ratio of anchors, and have limited robustness when dealing with lung nodules with a massive variety of sizes. To overcome these problems, we propose a 3D sphere representation-based center-points matching detection network that is anchor-free and automatically predicts the position, radius, and offset of nodules without the manual design of nodule/anchor parameters. The SCPM-Net consists of two novel components: sphere representation and center points matching. First, to match the nodule annotation in clinical practice, we replace the commonly used bounding box with our proposed bounding sphere to represent nodules with the centroid, radius, and local offset in 3D space. A compatible sphere-based intersection over-union loss function is introduced to train the lung nodule detection network stably and efficiently. Second, we empower the network anchor-free by designing a positive center-points selection and matching process, which naturally discards pre-determined anchor boxes. An online hard example mining and re-focal loss subsequently enable the CPM process to be more robust, resulting in more accurate point assignment and mitigation of class imbalance. In addition, to better capture spatial information and 3D context for the detection, we propose to fuse multi-level spatial coordinate maps with the feature extractor and combine them with 3D squeeze-and-excitation attention modules. Experimental results on the LUNA16 dataset showed that our proposed framework achieves superior performance compared with existing anchor-based and anchor-free methods for lung nodule detection.



### Neural Camera Simulators
- **Arxiv ID**: http://arxiv.org/abs/2104.05237v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05237v2)
- **Published**: 2021-04-12 07:06:27+00:00
- **Updated**: 2021-08-09 09:42:52+00:00
- **Authors**: Hao Ouyang, Zifan Shi, Chenyang Lei, Ka Lung Law, Qifeng Chen
- **Comment**: Accepted to CVPR2021
- **Journal**: None
- **Summary**: We present a controllable camera simulator based on deep neural networks to synthesize raw image data under different camera settings, including exposure time, ISO, and aperture. The proposed simulator includes an exposure module that utilizes the principle of modern lens designs for correcting the luminance level. It also contains a noise module using the noise level function and an aperture module with adaptive attention to simulate the side effects on noise and defocus blur. To facilitate the learning of a simulator model, we collect a dataset of the 10,000 raw images of 450 scenes with different exposure settings. Quantitative experiments and qualitative comparisons show that our approach outperforms relevant baselines in raw data synthesize on multiple cameras. Furthermore, the camera simulator enables various applications, including large-aperture enhancement, HDR, auto exposure, and data augmentation for training local feature detectors. Our work represents the first attempt to simulate a camera sensor's behavior leveraging both the advantage of traditional raw sensor features and the power of data-driven deep learning.



### Look Closer to Segment Better: Boundary Patch Refinement for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.05239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05239v1)
- **Published**: 2021-04-12 07:10:48+00:00
- **Updated**: 2021-04-12 07:10:48+00:00
- **Authors**: Chufeng Tang, Hang Chen, Xiao Li, Jianmin Li, Zhaoxiang Zhang, Xiaolin Hu
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Tremendous efforts have been made on instance segmentation but the mask quality is still not satisfactory. The boundaries of predicted instance masks are usually imprecise due to the low spatial resolution of feature maps and the imbalance problem caused by the extremely low proportion of boundary pixels. To address these issues, we propose a conceptually simple yet effective post-processing refinement framework to improve the boundary quality based on the results of any instance segmentation model, termed BPR. Following the idea of looking closer to segment boundaries better, we extract and refine a series of small boundary patches along the predicted instance boundaries. The refinement is accomplished by a boundary patch refinement network at higher resolution. The proposed BPR framework yields significant improvements over the Mask R-CNN baseline on Cityscapes benchmark, especially on the boundary-aware metrics. Moreover, by applying the BPR framework to the PolyTransform + SegFix baseline, we reached 1st place on the Cityscapes leaderboard.



### All Labels Are Not Created Equal: Enhancing Semi-supervision via Label Grouping and Co-training
- **Arxiv ID**: http://arxiv.org/abs/2104.05248v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.05248v1)
- **Published**: 2021-04-12 07:33:16+00:00
- **Updated**: 2021-04-12 07:33:16+00:00
- **Authors**: Islam Nassar, Samitha Herath, Ehsan Abbasnejad, Wray Buntine, Gholamreza Haffari
- **Comment**: Accepted in CVPR2021
- **Journal**: None
- **Summary**: Pseudo-labeling is a key component in semi-supervised learning (SSL). It relies on iteratively using the model to generate artificial labels for the unlabeled data to train against. A common property among its various methods is that they only rely on the model's prediction to make labeling decisions without considering any prior knowledge about the visual similarity among the classes. In this paper, we demonstrate that this degrades the quality of pseudo-labeling as it poorly represents visually similar classes in the pool of pseudo-labeled data. We propose SemCo, a method which leverages label semantics and co-training to address this problem. We train two classifiers with two different views of the class labels: one classifier uses the one-hot view of the labels and disregards any potential similarity among the classes, while the other uses a distributed view of the labels and groups potentially similar classes together. We then co-train the two classifiers to learn based on their disagreements. We show that our method achieves state-of-the-art performance across various SSL tasks including 5.6% accuracy improvement on Mini-ImageNet dataset with 1000 labeled examples. We also show that our method requires smaller batch size and fewer training iterations to reach its best performance. We make our code available at https://github.com/islam-nassar/semco.



### Improving Online Performance Prediction for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.05255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05255v1)
- **Published**: 2021-04-12 07:44:40+00:00
- **Updated**: 2021-04-12 07:44:40+00:00
- **Authors**: Marvin Klingner, Andreas Bär, Marcel Mross, Tim Fingscheidt
- **Comment**: Accepted to CVPR SAIAD Workshop
- **Journal**: None
- **Summary**: In this work we address the task of observing the performance of a semantic segmentation deep neural network (DNN) during online operation, i.e., during inference, which is of high importance in safety-critical applications such as autonomous driving. Here, many high-level decisions rely on such DNNs, which are usually evaluated offline, while their performance in online operation remains unknown. To solve this problem, we propose an improved online performance prediction scheme, building on a recently proposed concept of predicting the primary semantic segmentation task's performance. This can be achieved by evaluating the auxiliary task of monocular depth estimation with a measurement supplied by a LiDAR sensor and a subsequent regression to the semantic segmentation performance. In particular, we propose (i) sequential training methods for both tasks in a multi-task training setup, (ii) to share the encoder as well as parts of the decoder between both task's networks for improved efficiency, and (iii) a temporal statistics aggregation method, which significantly reduces the performance prediction error at the cost of a small algorithmic latency. Evaluation on the KITTI dataset shows that all three aspects improve the performance prediction compared to previous approaches.



### Robust Classification from Noisy Labels: Integrating Additional Knowledge for Chest Radiography Abnormality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2104.05261v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.05261v3)
- **Published**: 2021-04-12 07:51:07+00:00
- **Updated**: 2021-04-21 09:07:00+00:00
- **Authors**: Sebastian Gündel, Arnaud A. A. Setio, Florin C. Ghesu, Sasa Grbic, Bogdan Georgescu, Andreas Maier, Dorin Comaniciu
- **Comment**: Accepted in Medical Image Analysis (MedIA). arXiv admin note: text
  overlap with arXiv:1905.06362
- **Journal**: None
- **Summary**: Chest radiography is the most common radiographic examination performed in daily clinical practice for the detection of various heart and lung abnormalities. The large amount of data to be read and reported, with more than 100 studies per day for a single radiologist, poses a challenge in consistently maintaining high interpretation accuracy. The introduction of large-scale public datasets has led to a series of novel systems for automated abnormality classification. However, the labels of these datasets were obtained using natural language processed medical reports, yielding a large degree of label noise that can impact the performance. In this study, we propose novel training strategies that handle label noise from such suboptimal data. Prior label probabilities were measured on a subset of training data re-read by 4 board-certified radiologists and were used during training to increase the robustness of the training model to the label noise. Furthermore, we exploit the high comorbidity of abnormalities observed in chest radiography and incorporate this information to further reduce the impact of label noise. Additionally, anatomical knowledge is incorporated by training the system to predict lung and heart segmentation, as well as spatial knowledge labels. To deal with multiple datasets and images derived from various scanners that apply different post-processing techniques, we introduce a novel image normalization strategy. Experiments were performed on an extensive collection of 297,541 chest radiographs from 86,876 patients, leading to a state-of-the-art performance level for 17 abnormalities from 2 datasets. With an average AUC score of 0.880 across all abnormalities, our proposed training strategies can be used to significantly improve performance scores.



### Glance and Gaze: Inferring Action-aware Points for One-Stage Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.05269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05269v1)
- **Published**: 2021-04-12 08:01:04+00:00
- **Updated**: 2021-04-12 08:01:04+00:00
- **Authors**: Xubin Zhong, Xian Qu, Changxing Ding, Dacheng Tao
- **Comment**: Accepted to CVPR2021
- **Journal**: None
- **Summary**: Modern human-object interaction (HOI) detection approaches can be divided into one-stage methods and twostage ones. One-stage models are more efficient due to their straightforward architectures, but the two-stage models are still advantageous in accuracy. Existing one-stage models usually begin by detecting predefined interaction areas or points, and then attend to these areas only for interaction prediction; therefore, they lack reasoning steps that dynamically search for discriminative cues. In this paper, we propose a novel one-stage method, namely Glance and Gaze Network (GGNet), which adaptively models a set of actionaware points (ActPoints) via glance and gaze steps. The glance step quickly determines whether each pixel in the feature maps is an interaction point. The gaze step leverages feature maps produced by the glance step to adaptively infer ActPoints around each pixel in a progressive manner. Features of the refined ActPoints are aggregated for interaction prediction. Moreover, we design an actionaware approach that effectively matches each detected interaction with its associated human-object pair, along with a novel hard negative attentive loss to improve the optimization of GGNet. All the above operations are conducted simultaneously and efficiently for all pixels in the feature maps. Finally, GGNet outperforms state-of-the-art methods by significant margins on both V-COCO and HICODET benchmarks. Code of GGNet is available at https: //github.com/SherlockHolmes221/GGNet.



### Class-Balanced Distillation for Long-Tailed Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.05279v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.05279v2)
- **Published**: 2021-04-12 08:21:03+00:00
- **Updated**: 2022-01-12 21:59:15+00:00
- **Authors**: Ahmet Iscen, André Araujo, Boqing Gong, Cordelia Schmid
- **Comment**: The code is available at
  https://github.com/google-research/google-research/tree/master/class_balanced_distillation
- **Journal**: None
- **Summary**: Real-world imagery is often characterized by a significant imbalance of the number of images per class, leading to long-tailed distributions. An effective and simple approach to long-tailed visual recognition is to learn feature representations and a classifier separately, with instance and class-balanced sampling, respectively. In this work, we introduce a new framework, by making the key observation that a feature representation learned with instance sampling is far from optimal in a long-tailed setting. Our main contribution is a new training method, referred to as Class-Balanced Distillation (CBD), that leverages knowledge distillation to enhance feature representations. CBD allows the feature representation to evolve in the second training stage, guided by the teacher learned in the first stage. The second stage uses class-balanced sampling, in order to focus on under-represented classes. This framework can naturally accommodate the usage of multiple teachers, unlocking the information from an ensemble of models to enhance recognition capabilities. Our experiments show that the proposed technique consistently outperforms the state of the art on long-tailed recognition benchmarks such as ImageNet-LT, iNaturalist17 and iNaturalist18.



### Approach for modeling single branches of meadow orchard trees with 3D point clouds
- **Arxiv ID**: http://arxiv.org/abs/2104.05282v1
- **DOI**: 10.3920/978-90-8686-916-9_88
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.05282v1)
- **Published**: 2021-04-12 08:25:27+00:00
- **Updated**: 2021-04-12 08:25:27+00:00
- **Authors**: Jonas Straub, David Reiser, Hans W. Griepentrog
- **Comment**: None
- **Journal**: None
- **Summary**: The cultivation of orchard meadows provides an ecological benefit for biodiversity, which is significantly higher than in intensively cultivated orchards. The goal of this research is to create a tree model to automatically determine possible pruning points for stand-alone trees within meadows. The algorithm which is presented here is capable of building a skeleton model based on a pre-segmented photogrammetric 3D point cloud. Good results were achieved in assigning the points to their leading branches and building a virtual tree model, reaching an overall accuracy of 95.19 %. This model provided the necessary information about the geometry of the tree for automated pruning.



### Volume and leaf area calculation of cabbage with a neural network-based instance segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.05284v1
- **DOI**: 10.3920/978-90-8686-916-9_86
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.05284v1)
- **Published**: 2021-04-12 08:29:23+00:00
- **Updated**: 2021-04-12 08:29:23+00:00
- **Authors**: Nils Lueling, David Reiser, Hans W. Griepentrog
- **Comment**: None
- **Journal**: None
- **Summary**: Fruit size and leaf area are important indicators for plant health and are of interest for plant nutrient management, plant protection and harvest. In this research, an image-based method for measuring the fruit volume as well as the leaf area for cabbage is presented. For this purpose, a mask region-based convolutional neural network (Mask R-CNN) was trained to segment the cabbage fruit from the leaves and assign it to the corresponding plant. The results indicated that even with a single camera, the developed method can provide a calculation accuracy of fruit size of 92.6% and an accuracy of leaf area of 89.8% on individual plant level.



### StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision
- **Arxiv ID**: http://arxiv.org/abs/2104.05289v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05289v2)
- **Published**: 2021-04-12 08:41:54+00:00
- **Updated**: 2021-04-13 06:47:43+00:00
- **Authors**: Yang Hong, Juyong Zhang, Boyi Jiang, Yudong Guo, Ligang Liu, Hujun Bao
- **Comment**: Accepted by CVPR2021. Project page:
  http://crishy1995.github.io/StereoPIFuProject
- **Journal**: None
- **Summary**: In this paper, we propose StereoPIFu, which integrates the geometric constraints of stereo vision with implicit function representation of PIFu, to recover the 3D shape of the clothed human from a pair of low-cost rectified images. First, we introduce the effective voxel-aligned features from a stereo vision-based network to enable depth-aware reconstruction. Moreover, the novel relative z-offset is employed to associate predicted high-fidelity human depth and occupancy inference, which helps restore fine-level surface details. Second, a network structure that fully utilizes the geometry information from the stereo images is designed to improve the human body reconstruction quality. Consequently, our StereoPIFu can naturally infer the human body's spatial location in camera space and maintain the correct relative position of different parts of the human body, which enables our method to capture human performance. Compared with previous works, our StereoPIFu significantly improves the robustness, completeness, and accuracy of the clothed human reconstruction, which is demonstrated by extensive experimental results.



### Intra-Class Uncertainty Loss Function for Classification
- **Arxiv ID**: http://arxiv.org/abs/2104.05298v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05298v1)
- **Published**: 2021-04-12 09:02:41+00:00
- **Updated**: 2021-04-12 09:02:41+00:00
- **Authors**: He Zhu, Shan Yu
- **Comment**: Accepted to ICME 2021
- **Journal**: None
- **Summary**: Most classification models can be considered as the process of matching templates. However, when intra-class uncertainty/variability is not considered, especially for datasets containing unbalanced classes, this may lead to classification errors. To address this issue, we propose a loss function with intra-class uncertainty following Gaussian distribution. Specifically, in our framework, the features extracted by deep networks of each class are characterized by independent Gaussian distribution. The parameters of distribution are learned with a likelihood regularization along with other network parameters. The means of the Gaussian play a similar role as the center anchor in existing methods, and the variance describes the uncertainty of different classes. In addition, similar to the inter-class margin in traditional loss functions, we introduce a margin to intra-class uncertainty to make each cluster more compact and reduce the imbalance of feature distribution from different categories. Based on MNIST, CIFAR, ImageNet, and Long-tailed CIFAR analyses, the proposed approach shows improved classification performance, through learning a better class representation.



### COVID-19 detection using chest X-rays: is lung segmentation important for generalization?
- **Arxiv ID**: http://arxiv.org/abs/2104.06176v3
- **DOI**: 10.1007/s42600-022-00242-y
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.06176v3)
- **Published**: 2021-04-12 09:06:28+00:00
- **Updated**: 2022-11-02 14:52:02+00:00
- **Authors**: Pedro R. A. S. Bassi, Romis Attux
- **Comment**: Text and figure improvements. Results did not change. Included DOI
  and reference to the published article (Research on Biomedical Engineering,
  Springer). Link for the published paper:
  https://trebuchet.public.springernature.app/get_content/1ab346c8-06ea-49ed-92f3-deaec80f6988
- **Journal**: Research on Biomedical Engineering, Springer (2022)
- **Summary**: Purpose: we evaluated the generalization capability of deep neural networks (DNNs), trained to classify chest X-rays as Covid-19, normal or pneumonia, using a relatively small and mixed dataset. Methods: we proposed a DNN to perform lung segmentation and classification, stacking a segmentation module (U-Net), an original intermediate module and a classification module (DenseNet201). To evaluate generalization, we tested the DNN with an external dataset (from distinct localities) and used Bayesian inference to estimate probability distributions of performance metrics. Results: our DNN achieved 0.917 AUC on the external test dataset, and a DenseNet without segmentation, 0.906. Bayesian inference indicated mean accuracy of 76.1% and [0.695, 0.826] 95% HDI (highest density interval, which concentrates 95% of the metric's probability mass) with segmentation and, without segmentation, 71.7% and [0.646, 0.786]. Conclusion: employing a novel DNN evaluation technique, which uses LRP and Brixia scores, we discovered that areas where radiologists found strong Covid-19 symptoms are the most important for the stacked DNN classification. External validation showed smaller accuracies than internal, indicating difficulty in generalization, which is positively affected by segmentation. Finally, the performance in the external dataset and the analysis with LRP suggest that DNNs can be trained in small and mixed datasets and still successfully detect Covid-19.



### Landmark Regularization: Ranking Guided Super-Net Training in Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2104.05309v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05309v1)
- **Published**: 2021-04-12 09:32:33+00:00
- **Updated**: 2021-04-12 09:32:33+00:00
- **Authors**: Kaicheng Yu, Rene Ranftl, Mathieu Salzmann
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Weight sharing has become a de facto standard in neural architecture search because it enables the search to be done on commodity hardware. However, recent works have empirically shown a ranking disorder between the performance of stand-alone architectures and that of the corresponding shared-weight networks. This violates the main assumption of weight-sharing NAS algorithms, thus limiting their effectiveness. We tackle this issue by proposing a regularization term that aims to maximize the correlation between the performance rankings of the shared-weight network and that of the standalone architectures using a small set of landmark architectures. We incorporate our regularization term into three different NAS algorithms and show that it consistently improves performance across algorithms, search-spaces, and tasks.



### Unsupervised foreign object detection based on dual-energy absorptiometry in the food industry
- **Arxiv ID**: http://arxiv.org/abs/2104.05326v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05326v1)
- **Published**: 2021-04-12 10:15:15+00:00
- **Updated**: 2021-04-12 10:15:15+00:00
- **Authors**: Vladyslav Andriiashen, Robert van Liere, Tristan van Leeuwen, Kees Joost Batenburg
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: X-ray imaging is a widely used technique for non-destructive inspection of agricultural food products. One application of X-ray imaging is the autonomous, in-line detection of foreign objects in food samples. Examples of such inclusions are bone fragments in meat products, plastic and metal debris in fish, fruit infestations. This article presents a processing methodology for unsupervised foreign object detection based on dual-energy X-ray absorptiometry (DEXA). A foreign object is defined as a fragment of material with different X-ray attenuation properties than those belonging to the food product. A novel thickness correction model is introduced as a pre-processing technique for DEXA data. The aim of the model is to homogenize regions in the image that belong to the food product and enhance contrast where the foreign object is present. In this way, the segmentation of the foreign object is more robust to noise and lack of contrast. The proposed methodology was applied to a dataset of 488 samples of meat products. The samples were acquired from a conveyor belt in a food processing factory. Approximately 60\% of the samples contain foreign objects of different types and sizes, while the rest of the samples are void of foreign objects. The results show that samples without foreign objects are correctly identified in 97% of cases, the overall accuracy of foreign object detection reaches 95%.



### MinkLoc++: Lidar and Monocular Image Fusion for Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.05327v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05327v2)
- **Published**: 2021-04-12 10:16:08+00:00
- **Updated**: 2021-04-14 10:02:05+00:00
- **Authors**: Jacek Komorowski, Monika Wysoczanska, Tomasz Trzcinski
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a discriminative multimodal descriptor based on a pair of sensor readings: a point cloud from a LiDAR and an image from an RGB camera. Our descriptor, named MinkLoc++, can be used for place recognition, re-localization and loop closure purposes in robotics or autonomous vehicles applications. We use late fusion approach, where each modality is processed separately and fused in the final part of the processing pipeline. The proposed method achieves state-of-the-art performance on standard place recognition benchmarks. We also identify dominating modality problem when training a multimodal descriptor. The problem manifests itself when the network focuses on a modality with a larger overfit to the training data. This drives the loss down during the training but leads to suboptimal performance on the evaluation set. In this work we describe how to detect and mitigate such risk when using a deep metric learning approach to train a multimodal neural network. Our code is publicly available on the project website: https://github.com/jac99/MinkLocMultimodal.



### RPSRNet: End-to-End Trainable Rigid Point Set Registration Network using Barnes-Hut $2^D$-Tree Representation
- **Arxiv ID**: http://arxiv.org/abs/2104.05328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05328v1)
- **Published**: 2021-04-12 10:16:09+00:00
- **Updated**: 2021-04-12 10:16:09+00:00
- **Authors**: Sk Aziz Ali, Kerem Kahraman, Gerd Reis, Didier Stricker
- **Comment**: Computer Vision and Pattern Recognition (CVPR) 2021, (*Accepted)
- **Journal**: None
- **Summary**: We propose RPSRNet - a novel end-to-end trainable deep neural network for rigid point set registration. For this task, we use a novel $2^D$-tree representation for the input point sets and a hierarchical deep feature embedding in the neural network. An iterative transformation refinement module in our network boosts the feature matching accuracy in the intermediate stages. We achieve an inference speed of 12-15ms to register a pair of input point clouds as large as 250K. Extensive evaluation on (i) KITTI LiDAR odometry and (ii) ModelNet-40 datasets shows that our method outperforms prior state-of-the-art methods - e.g., on the KITTI data set, DCP-v2 by1.3 and 1.5 times, and PointNetLK by 1.8 and 1.9 times better rotational and translational accuracy respectively. Evaluation on ModelNet40 shows that RPSRNet is more robust than other benchmark methods when the samples contain a significant amount of noise and other disturbances. RPSRNet accurately registers point clouds with non-uniform sampling densities, e.g., LiDAR data, which cannot be processed by many existing deep-learning-based registration methods.



### How Sensitive are Meta-Learners to Dataset Imbalance?
- **Arxiv ID**: http://arxiv.org/abs/2104.05344v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05344v1)
- **Published**: 2021-04-12 10:47:42+00:00
- **Updated**: 2021-04-12 10:47:42+00:00
- **Authors**: Mateusz Ochal, Massimiliano Patacchiola, Amos Storkey, Jose Vazquez, Sen Wang
- **Comment**: Published as a workshop paper at the Learning to Learn workshop at
  ICLR 2021. arXiv admin note: text overlap with arXiv:2101.02523
- **Journal**: None
- **Summary**: Meta-Learning (ML) has proven to be a useful tool for training Few-Shot Learning (FSL) algorithms by exposure to batches of tasks sampled from a meta-dataset. However, the standard training procedure overlooks the dynamic nature of the real-world where object classes are likely to occur at different frequencies. While it is generally understood that imbalanced tasks harm the performance of supervised methods, there is no significant research examining the impact of imbalanced meta-datasets on the FSL evaluation task. This study exposes the magnitude and extent of this problem. Our results show that ML methods are more robust against meta-dataset imbalance than imbalance at the task-level with a similar imbalance ratio ($\rho<20$), with the effect holding even in long-tail datasets under a larger imbalance ($\rho=65$). Overall, these results highlight an implicit strength of ML algorithms, capable of learning generalizable features under dataset imbalance and domain-shift. The code to reproduce the experiments is released under an open-source license.



### Dual-Octave Convolution for Accelerated Parallel MR Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2104.05345v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05345v1)
- **Published**: 2021-04-12 10:51:05+00:00
- **Updated**: 2021-04-12 10:51:05+00:00
- **Authors**: Chun-Mei Feng, Zhanyuan Yang, Geng Chen, Yong Xu, Ling Shao
- **Comment**: Proceedings of the 35th AAAI Conference on Artificial Intelligence
  (AAAI) 2021
- **Journal**: Proceedings of the 35th AAAI Conference on Artificial Intelligence
  (AAAI) 2021
- **Summary**: Magnetic resonance (MR) image acquisition is an inherently prolonged process, whose acceleration by obtaining multiple undersampled images simultaneously through parallel imaging has always been the subject of research. In this paper, we propose the Dual-Octave Convolution (Dual-OctConv), which is capable of learning multi-scale spatial-frequency features from both real and imaginary components, for fast parallel MR image reconstruction. By reformulating the complex operations using octave convolutions, our model shows a strong ability to capture richer representations of MR images, while at the same time greatly reducing the spatial redundancy. More specifically, the input feature maps and convolutional kernels are first split into two components (i.e., real and imaginary), which are then divided into four groups according to their spatial frequencies. Then, our Dual-OctConv conducts intra-group information updating and inter-group information exchange to aggregate the contextual information across different groups. Our framework provides two appealing benefits: (i) it encourages interactions between real and imaginary components at various spatial frequencies to achieve richer representational capacity, and (ii) it enlarges the receptive field by learning multiple spatial-frequency features of both the real and imaginary components. We evaluate the performance of the proposed model on the acceleration of multi-coil MR image reconstruction. Extensive experiments are conducted on an {in vivo} knee dataset under different undersampling patterns and acceleration factors. The experimental results demonstrate the superiority of our model in accelerated parallel MR image reconstruction. Our code is available at: github.com/chunmeifeng/Dual-OctConv.



### UNIT-DDPM: UNpaired Image Translation with Denoising Diffusion Probabilistic Models
- **Arxiv ID**: http://arxiv.org/abs/2104.05358v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05358v1)
- **Published**: 2021-04-12 11:22:56+00:00
- **Updated**: 2021-04-12 11:22:56+00:00
- **Authors**: Hiroshi Sasaki, Chris G. Willcocks, Toby P. Breckon
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: We propose a novel unpaired image-to-image translation method that uses denoising diffusion probabilistic models without requiring adversarial training. Our method, UNpaired Image Translation with Denoising Diffusion Probabilistic Models (UNIT-DDPM), trains a generative model to infer the joint distribution of images over both domains as a Markov chain by minimising a denoising score matching objective conditioned on the other domain. In particular, we update both domain translation models simultaneously, and we generate target domain images by a denoising Markov Chain Monte Carlo approach that is conditioned on the input source domain images, based on Langevin dynamics. Our approach provides stable model training for image-to-image translation and generates high-quality image outputs. This enables state-of-the-art Fr\'echet Inception Distance (FID) performance on several public datasets, including both colour and multispectral imagery, significantly outperforming the contemporary adversarial image-to-image translation methods.



### Visiting the Invisible: Layer-by-Layer Completed Scene Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2104.05367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05367v1)
- **Published**: 2021-04-12 11:37:23+00:00
- **Updated**: 2021-04-12 11:37:23+00:00
- **Authors**: Chuanxia Zheng, Duy-Son Dao, Guoxian Song, Tat-Jen Cham, Jianfei Cai
- **Comment**: 20 pages, 16 pages
- **Journal**: None
- **Summary**: Existing scene understanding systems mainly focus on recognizing the visible parts of a scene, ignoring the intact appearance of physical objects in the real-world. Concurrently, image completion has aimed to create plausible appearance for the invisible regions, but requires a manual mask as input. In this work, we propose a higher-level scene understanding system to tackle both visible and invisible parts of objects and backgrounds in a given scene. Particularly, we built a system to decompose a scene into individual objects, infer their underlying occlusion relationships, and even automatically learn which parts of the objects are occluded that need to be completed. In order to disentangle the occluded relationships of all objects in a complex scene, we use the fact that the front object without being occluded is easy to be identified, detected, and segmented. Our system interleaves the two tasks of instance segmentation and scene completion through multiple iterations, solving for objects layer-by-layer. We first provide a thorough experiment using a new realistically rendered dataset with ground-truths for all invisible regions. To bridge the domain gap to real imagery where ground-truths are unavailable, we then train another model with the pseudo-ground-truths generated from our trained synthesis model. We demonstrate results on a wide variety of datasets and show significant improvement over the state-of-the-art.



### Self-supervised Multi-view Stereo via Effective Co-Segmentation and Data-Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.05374v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.05374v1)
- **Published**: 2021-04-12 11:48:54+00:00
- **Updated**: 2021-04-12 11:48:54+00:00
- **Authors**: Hongbin Xu, Zhipeng Zhou, Yu Qiao, Wenxiong Kang, Qiuxia Wu
- **Comment**: This paper is accepted by AAAI-21 with a Distinguished Paper Award
- **Journal**: None
- **Summary**: Recent studies have witnessed that self-supervised methods based on view synthesis obtain clear progress on multi-view stereo (MVS). However, existing methods rely on the assumption that the corresponding points among different views share the same color, which may not always be true in practice. This may lead to unreliable self-supervised signal and harm the final reconstruction performance. To address the issue, we propose a framework integrated with more reliable supervision guided by semantic co-segmentation and data-augmentation. Specially, we excavate mutual semantic from multi-view images to guide the semantic consistency. And we devise effective data-augmentation mechanism which ensures the transformation robustness by treating the prediction of regular samples as pseudo ground truth to regularize the prediction of augmented samples. Experimental results on DTU dataset show that our proposed methods achieve the state-of-the-art performance among unsupervised methods, and even compete on par with supervised methods. Furthermore, extensive experiments on Tanks&Temples dataset demonstrate the effective generalization ability of the proposed method.



### Drafting and Revision: Laplacian Pyramid Network for Fast High-Quality Artistic Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2104.05376v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05376v2)
- **Published**: 2021-04-12 11:53:53+00:00
- **Updated**: 2021-04-18 01:09:09+00:00
- **Authors**: Tianwei Lin, Zhuoqi Ma, Fu Li, Dongliang He, Xin Li, Errui Ding, Nannan Wang, Jie Li, Xinbo Gao
- **Comment**: Accepted by CVPR 2021. Codes will be released soon on
  https://github.com/PaddlePaddle/PaddleGAN/
- **Journal**: None
- **Summary**: Artistic style transfer aims at migrating the style from an example image to a content image. Currently, optimization-based methods have achieved great stylization quality, but expensive time cost restricts their practical applications. Meanwhile, feed-forward methods still fail to synthesize complex style, especially when holistic global and local patterns exist. Inspired by the common painting process of drawing a draft and revising the details, we introduce a novel feed-forward method named Laplacian Pyramid Network (LapStyle). LapStyle first transfers global style patterns in low-resolution via a Drafting Network. It then revises the local details in high-resolution via a Revision Network, which hallucinates a residual image according to the draft and the image textures extracted by Laplacian filtering. Higher resolution details can be easily generated by stacking Revision Networks with multiple Laplacian pyramid levels. The final stylized image is obtained by aggregating outputs of all pyramid levels. %We also introduce a patch discriminator to better learn local patterns adversarially. Experiments demonstrate that our method can synthesize high quality stylized images in real time, where holistic style patterns are properly transferred.



### Dual Discriminator Adversarial Distillation for Data-free Model Compression
- **Arxiv ID**: http://arxiv.org/abs/2104.05382v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05382v2)
- **Published**: 2021-04-12 12:01:45+00:00
- **Updated**: 2021-10-05 01:06:21+00:00
- **Authors**: Haoran Zhao, Xin Sun, Junyu Dong, Hui Yu, Huiyu Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge distillation has been widely used to produce portable and efficient neural networks which can be well applied on edge devices for computer vision tasks. However, almost all top-performing knowledge distillation methods need to access the original training data, which usually has a huge size and is often unavailable. To tackle this problem, we propose a novel data-free approach in this paper, named Dual Discriminator Adversarial Distillation (DDAD) to distill a neural network without any training data or meta-data. To be specific, we use a generator to create samples through dual discriminator adversarial distillation, which mimics the original training data. The generator not only uses the pre-trained teacher's intrinsic statistics in existing batch normalization layers but also obtains the maximum discrepancy from the student model. Then the generated samples are used to train the compact student network under the supervision of the teacher. The proposed method obtains an efficient student network which closely approximates its teacher network, despite using no original training data. Extensive experiments are conducted to to demonstrate the effectiveness of the proposed approach on CIFAR-10, CIFAR-100 and Caltech101 datasets for classification tasks. Moreover, we extend our method to semantic segmentation tasks on several public datasets such as CamVid and NYUv2. All experiments show that our method outperforms all baselines for data-free knowledge distillation.



### Blazer: Laser Scanning Simulation using Physically Based Rendering
- **Arxiv ID**: http://arxiv.org/abs/2104.05430v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05430v1)
- **Published**: 2021-04-12 12:56:52+00:00
- **Updated**: 2021-04-12 12:56:52+00:00
- **Authors**: Sebastian Grans, Lars Tingelstad
- **Comment**: None
- **Journal**: None
- **Summary**: Line laser scanners are a sub-type of structured light 3D scanners that are relatively common devices to find within the industrial setting, typically in the context of assembly, process control, and welding. Despite its extensive use, scanning of some materials remain a difficult or even impossible task without additional pre-processing. For instance, materials which are shiny, or transparent. In this paper, we present a Blazer, a virtual line laser scanner that, combined with physically based rendering, produces synthetic data with a realistic light-matter interaction, and hence realistic appearance. This makes it eligible for the use as a tool in the development of novel algorithms, and in particular as a source of synthetic data for training of machine learning models. Similar systems exist for synthetic RGB-D data generation, but to our knowledge this the first publicly available implementation for synthetic line laser data. We release this implementation under an open-source license to aid further research on line laser scanners.



### Deep learning using Havrda-Charvat entropy for classification of pulmonary endomicroscopy
- **Arxiv ID**: http://arxiv.org/abs/2104.05450v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.05450v3)
- **Published**: 2021-04-12 13:16:04+00:00
- **Updated**: 2021-04-19 09:53:20+00:00
- **Authors**: Thibaud Brochet, Jerome Lapuyade-Lahorgue, Sebastien Bougleux, Mathieu Salaun, Su Ruan
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Pulmonary optical endomicroscopy (POE) is an imaging technology in real time. It allows to examine pulmonary alveoli at a microscopic level. Acquired in clinical settings, a POE image sequence can have as much as 25% of the sequence being uninformative frames (i.e. pure-noise and motion artefacts). For future data analysis, these uninformative frames must be first removed from the sequence. Therefore, the objective of our work is to develop an automatic detection method of uninformative images in endomicroscopy images. We propose to take the detection problem as a classification one. Considering advantages of deep learning methods, a classifier based on CNN (Convolutional Neural Network) is designed with a new loss function based on Havrda-Charvat entropy which is a parametrical generalization of the Shannon entropy. We propose to use this formula to get a better hold on all sorts of data since it provides a model more stable than the Shannon entropy. Our method is tested on one POE dataset including 2947 distinct images, is showing better results than using Shannon entropy and behaves better with regard to the problem of overfitting.   Keywords: Deep Learning, CNN, Shannon entropy, Havrda-Charvat entropy, Pulmonary optical endomicroscopy.



### PGNet: Real-time Arbitrarily-Shaped Text Spotting with Point Gathering Network
- **Arxiv ID**: http://arxiv.org/abs/2104.05458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05458v1)
- **Published**: 2021-04-12 13:27:34+00:00
- **Updated**: 2021-04-12 13:27:34+00:00
- **Authors**: Pengfei Wang, Chengquan Zhang, Fei Qi, Shanshan Liu, Xiaoqiang Zhang, Pengyuan Lyu, Junyu Han, Jingtuo Liu, Errui Ding, Guangming Shi
- **Comment**: 10 pages, 8 figures, AAAI 2021
- **Journal**: None
- **Summary**: The reading of arbitrarily-shaped text has received increasing research attention. However, existing text spotters are mostly built on two-stage frameworks or character-based methods, which suffer from either Non-Maximum Suppression (NMS), Region-of-Interest (RoI) operations, or character-level annotations. In this paper, to address the above problems, we propose a novel fully convolutional Point Gathering Network (PGNet) for reading arbitrarily-shaped text in real-time. The PGNet is a single-shot text spotter, where the pixel-level character classification map is learned with proposed PG-CTC loss avoiding the usage of character-level annotations. With a PG-CTC decoder, we gather high-level character classification vectors from two-dimensional space and decode them into text symbols without NMS and RoI operations involved, which guarantees high efficiency. Additionally, reasoning the relations between each character and its neighbors, a graph refinement module (GRM) is proposed to optimize the coarse recognition and improve the end-to-end performance. Experiments prove that the proposed method achieves competitive accuracy, meanwhile significantly improving the running speed. In particular, in Total-Text, it runs at 46.7 FPS, surpassing the previous spotters with a large margin.



### Learning Chebyshev Basis in Graph Convolutional Networks for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.05482v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05482v2)
- **Published**: 2021-04-12 14:08:58+00:00
- **Updated**: 2021-12-27 16:37:19+00:00
- **Authors**: Hichem Sahbi
- **Comment**: None
- **Journal**: None
- **Summary**: Spectral graph convolutional networks (GCNs) are particular deep models which aim at extending neural networks to arbitrary irregular domains. The principle of these networks consists in projecting graph signals using the eigen-decomposition of their Laplacians, then achieving filtering in the spectral domain prior to back-project the resulting filtered signals onto the input graph domain. However, the success of these operations is highly dependent on the relevance of the used Laplacians which are mostly handcrafted and this makes GCNs clearly sub-optimal. In this paper, we introduce a novel spectral GCN that learns not only the usual convolutional parameters but also the Laplacian operators. The latter are designed "end-to-end" as a part of a recursive Chebyshev decomposition with the particularity of conveying both the differential and the non-differential properties of the learned representations -- with increasing order and discrimination power -- without overparametrizing the trained GCNs. Extensive experiments, conducted on the challenging task of skeleton-based action recognition, show the generalization ability and the outperformance of our proposed Laplacian design w.r.t. different baselines (built upon handcrafted and other learned Laplacians) as well as the related work.



### Predicting Pedestrian Crossing Intention with Feature Fusion and Spatio-Temporal Attention
- **Arxiv ID**: http://arxiv.org/abs/2104.05485v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05485v2)
- **Published**: 2021-04-12 14:10:25+00:00
- **Updated**: 2021-10-13 15:59:39+00:00
- **Authors**: Dongfang Yang, Haolin Zhang, Ekim Yurtsever, Keith Redmill, Ümit Özgüner
- **Comment**: Submitted to IEEE Transactions on Intelligent Vehicles
- **Journal**: None
- **Summary**: Predicting vulnerable road user behavior is an essential prerequisite for deploying Automated Driving Systems (ADS) in the real-world. Pedestrian crossing intention should be recognized in real-time, especially for urban driving. Recent works have shown the potential of using vision-based deep neural network models for this task. However, these models are not robust and certain issues still need to be resolved. First, the global spatio-temproal context that accounts for the interaction between the target pedestrian and the scene has not been properly utilized. Second, the optimum strategy for fusing different sensor data has not been thoroughly investigated. This work addresses the above limitations by introducing a novel neural network architecture to fuse inherently different spatio-temporal features for pedestrian crossing intention prediction. We fuse different phenomena such as sequences of RGB imagery, semantic segmentation masks, and ego-vehicle speed in an optimum way using attention mechanisms and a stack of recurrent neural networks. The optimum architecture was obtained through exhaustive ablation and comparison studies. Extensive comparative experiments on the JAAD pedestrian action prediction benchmark demonstrate the effectiveness of the proposed method, where state-of-the-art performance was achieved. Our code is open-source and publicly available.



### An MRF-UNet Product of Experts for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.05495v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05495v1)
- **Published**: 2021-04-12 14:25:32+00:00
- **Updated**: 2021-04-12 14:25:32+00:00
- **Authors**: Mikael Brudfors, Yaël Balbastre, John Ashburner, Geraint Rees, Parashkev Nachev, Sébastien Ourselin, M. Jorge Cardoso
- **Comment**: Accepted at MIDL 2021
- **Journal**: None
- **Summary**: While convolutional neural networks (CNNs) trained by back-propagation have seen unprecedented success at semantic segmentation tasks, they are known to struggle on out-of-distribution data. Markov random fields (MRFs) on the other hand, encode simpler distributions over labels that, although less flexible than UNets, are less prone to over-fitting. In this paper, we propose to fuse both strategies by computing the product of distributions of a UNet and an MRF. As this product is intractable, we solve for an approximate distribution using an iterative mean-field approach. The resulting MRF-UNet is trained jointly by back-propagation. Compared to other works using conditional random fields (CRFs), the MRF has no dependency on the imaging data, which should allow for less over-fitting. We show on 3D neuroimaging data that this novel network improves generalisation to out-of-distribution samples. Furthermore, it allows the overall number of parameters to be reduced while preserving high accuracy. These results suggest that a classic MRF smoothness prior can allow for less over-fitting when principally integrated into a CNN model. Our implementation is available at https://github.com/balbasty/nitorch.



### Diamond in the rough: Improving image realism by traversing the GAN latent space
- **Arxiv ID**: http://arxiv.org/abs/2104.05518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05518v1)
- **Published**: 2021-04-12 14:45:29+00:00
- **Updated**: 2021-04-12 14:45:29+00:00
- **Authors**: Jeffrey Wen, Fabian Benitez-Quiroz, Qianli Feng, Aleix Martinez
- **Comment**: None
- **Journal**: None
- **Summary**: In just a few years, the photo-realism of images synthesized by Generative Adversarial Networks (GANs) has gone from somewhat reasonable to almost perfect largely by increasing the complexity of the networks, e.g., adding layers, intermediate latent spaces, style-transfer parameters, etc. This trajectory has led many of the state-of-the-art GANs to be inaccessibly large, disengaging many without large computational resources. Recognizing this, we explore a method for squeezing additional performance from existing, low-complexity GANs. Formally, we present an unsupervised method to find a direction in the latent space that aligns with improved photo-realism. Our approach leaves the network unchanged while enhancing the fidelity of the generated image. We use a simple generator inversion to find the direction in the latent space that results in the smallest change in the image space. Leveraging the learned structure of the latent space, we find moving in this direction corrects many image artifacts and brings the image into greater realism. We verify our findings qualitatively and quantitatively, showing an improvement in Frechet Inception Distance (FID) exists along our trajectory which surpasses the original GAN and other approaches including a supervised method. We expand further and provide an optimization method to automatically select latent vectors along the path that balance the variation and realism of samples. We apply our method to several diverse datasets and three architectures of varying complexity to illustrate the generalizability of our approach. By expanding the utility of low-complexity and existing networks, we hope to encourage the democratization of GANs.



### Cloth Interactive Transformer for Virtual Try-On
- **Arxiv ID**: http://arxiv.org/abs/2104.05519v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05519v2)
- **Published**: 2021-04-12 14:45:32+00:00
- **Updated**: 2023-08-20 18:53:52+00:00
- **Authors**: Bin Ren, Hao Tang, Fanyang Meng, Runwei Ding, Philip H. S. Torr, Nicu Sebe
- **Comment**: 16 pages, 7 figures, Accepted by ACM ToMM in 2023
- **Journal**: None
- **Summary**: The 2D image-based virtual try-on has aroused increased interest from the multimedia and computer vision fields due to its enormous commercial value. Nevertheless, most existing image-based virtual try-on approaches directly combine the person-identity representation and the in-shop clothing items without taking their mutual correlations into consideration. Moreover, these methods are commonly established on pure convolutional neural networks (CNNs) architectures which are not simple to capture the long-range correlations among the input pixels. As a result, it generally results in inconsistent results. To alleviate these issues, in this paper, we propose a novel two-stage cloth interactive transformer (CIT) method for the virtual try-on task. During the first stage, we design a CIT matching block, aiming to precisely capture the long-range correlations between the cloth-agnostic person information and the in-shop cloth information. Consequently, it makes the warped in-shop clothing items look more natural in appearance. In the second stage, we put forth a CIT reasoning block for establishing global mutual interactive dependencies among person representation, the warped clothing item, and the corresponding warped cloth mask. The empirical results, based on mutual dependencies, demonstrate that the final try-on results are more realistic. Substantial empirical results on a public fashion dataset illustrate that the suggested CIT attains competitive virtual try-on performance.



### A-FMI: Learning Attributions from Deep Networks via Feature Map Importance
- **Arxiv ID**: http://arxiv.org/abs/2104.05527v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.05527v1)
- **Published**: 2021-04-12 14:54:44+00:00
- **Updated**: 2021-04-12 14:54:44+00:00
- **Authors**: An Zhang, Xiang Wang, Chengfang Fang, Jie Shi, Tat-seng Chua, Zehua Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Gradient-based attribution methods can aid in the understanding of convolutional neural networks (CNNs). However, the redundancy of attribution features and the gradient saturation problem, which weaken the ability to identify significant features and cause an explanation focus shift, are challenges that attribution methods still face. In this work, we propose: 1) an essential characteristic, Strong Relevance, when selecting attribution features; 2) a new concept, feature map importance (FMI), to refine the contribution of each feature map, which is faithful to the CNN model; and 3) a novel attribution method via FMI, termed A-FMI, to address the gradient saturation problem, which couples the target image with a reference image, and assigns the FMI to the difference-from-reference at the granularity of feature map. Through visual inspections and qualitative evaluations on the ImageNet dataset, we show the compelling advantages of A-FMI on its faithfulness, insensitivity to the choice of reference, class discriminability, and superior explanation performance compared with popular attribution methods across varying CNN architectures.



### Efficient Model Monitoring for Quality Control in Cardiac Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.05533v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.05533v1)
- **Published**: 2021-04-12 14:58:58+00:00
- **Updated**: 2021-04-12 14:58:58+00:00
- **Authors**: Francesco Galati, Maria A. Zuluaga
- **Comment**: Accepted to the 11th Biennial Meeting on Functional Imaging and
  Modeling of the Heart (FIMH-2021)
- **Journal**: None
- **Summary**: Deep learning methods have reached state-of-the-art performance in cardiac image segmentation. Currently, the main bottleneck towards their effective translation into clinics requires assuring continuous high model performance and segmentation results. In this work, we present a novel learning framework to monitor the performance of heart segmentation models in the absence of ground truth. Formulated as an anomaly detection problem, the monitoring framework allows deriving surrogate quality measures for a segmentation and allows flagging suspicious results. We propose two different types of quality measures, a global score and a pixel-wise map. We demonstrate their use by reproducing the final rankings of a cardiac segmentation challenge in the absence of ground truth. Results show that our framework is accurate, fast, and scalable, confirming it is a viable option for quality control monitoring in clinical practice and large population studies.



### Learning from Subjective Ratings Using Auto-Decoded Deep Latent Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2104.05570v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.05570v3)
- **Published**: 2021-04-12 15:40:42+00:00
- **Updated**: 2021-10-12 15:06:37+00:00
- **Authors**: Bowen Li, Xinping Ren, Ke Yan, Le Lu, Lingyun Huang, Guotong Xie, Jing Xiao, Dar-In Tai, Adam P. Harrison
- **Comment**: Main body includes 10 pages and 3 figures
- **Journal**: None
- **Summary**: Depending on the application, radiological diagnoses can be associated with high inter- and intra-rater variabilities. Most computer-aided diagnosis (CAD) solutions treat such data as incontrovertible, exposing learning algorithms to considerable and possibly contradictory label noise and biases. Thus, managing subjectivity in labels is a fundamental problem in medical imaging analysis. To address this challenge, we introduce auto-decoded deep latent embeddings (ADDLE), which explicitly models the tendencies of each rater using an auto-decoder framework. After a simple linear transformation, the latent variables can be injected into any backbone at any and multiple points, allowing the model to account for rater-specific effects on the diagnosis. Importantly, ADDLE does not expect multiple raters per image in training, meaning it can readily learn from data mined from hospital archives. Moreover, the complexity of training ADDLE does not increase as more raters are added. During inference each rater can be simulated and a 'mean' or 'greedy' virtual rating can be produced. We test ADDLE on the problem of liver steatosis diagnosis from 2D ultrasound (US) by collecting 46 084 studies along with clinical US diagnoses originating from 65 different raters. We evaluated diagnostic performance using a separate dataset with gold-standard biopsy diagnoses. ADDLE can improve the partial areas under the curve (AUCs) for diagnosing severe steatosis by 10.5% over standard classifiers while outperforming other annotator-noise approaches, including those requiring 65 times the parameters.



### GAttANet: Global attention agreement for convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2104.05575v2
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2104.05575v2)
- **Published**: 2021-04-12 15:45:10+00:00
- **Updated**: 2021-06-30 08:16:07+00:00
- **Authors**: Rufin VanRullen, Andrea Alamia
- **Comment**: Paper accepted to ICANN 2021 - The 30th International Conference on
  Artificial Neural Networks
- **Journal**: None
- **Summary**: Transformer attention architectures, similar to those developed for natural language processing, have recently proved efficient also in vision, either in conjunction with or as a replacement for convolutional layers. Typically, visual attention is inserted in the network architecture as a (series of) feedforward self-attention module(s), with mutual key-query agreement as the main selection and routing operation. However efficient, this strategy is only vaguely compatible with the way that attention is implemented in biological brains: as a separate and unified network of attentional selection regions, receiving inputs from and exerting modulatory influence on the entire hierarchy of visual regions. Here, we report experiments with a simple such attention system that can improve the performance of standard convolutional networks, with relatively few additional parameters. Each spatial position in each layer of the network produces a key-query vector pair; all queries are then pooled into a global attention query. On the next iteration, the match between each key and the global attention query modulates the network's activations -- emphasizing or silencing the locations that agree or disagree (respectively) with the global attention system. We demonstrate the usefulness of this brain-inspired Global Attention Agreement network (GAttANet) for various convolutional backbones (from a simple 5-layer toy model to a standard ResNet50 architecture) and datasets (CIFAR10, CIFAR100, Imagenet-1k). Each time, our global attention system improves accuracy over the corresponding baseline.



### PAC Bayesian Performance Guarantees for Deep (Stochastic) Networks in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2104.05600v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.05600v2)
- **Published**: 2021-04-12 16:21:07+00:00
- **Updated**: 2021-07-08 21:26:29+00:00
- **Authors**: Anthony Sicilia, Xingchen Zhao, Anastasia Sosnovskikh, Seong Jae Hwang
- **Comment**: MICCAI 2021
- **Journal**: None
- **Summary**: Application of deep neural networks to medical imaging tasks has in some sense become commonplace. Still, a "thorn in the side" of the deep learning movement is the argument that deep networks are prone to overfitting and are thus unable to generalize well when datasets are small (as is common in medical imaging tasks). One way to bolster confidence is to provide mathematical guarantees, or bounds, on network performance after training which explicitly quantify the possibility of overfitting. In this work, we explore recent advances using the PAC-Bayesian framework to provide bounds on generalization error for large (stochastic) networks. While previous efforts focus on classification in larger natural image datasets (e.g., MNIST and CIFAR-10), we apply these techniques to both classification and segmentation in a smaller medical imagining dataset: the ISIC 2018 challenge set. We observe the resultant bounds are competitive compared to a simpler baseline, while also being more explainable and alleviating the need for holdout sets.



### Common Limitations of Image Processing Metrics: A Picture Story
- **Arxiv ID**: http://arxiv.org/abs/2104.05642v7
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05642v7)
- **Published**: 2021-04-12 17:03:42+00:00
- **Updated**: 2023-08-15 13:03:12+00:00
- **Authors**: Annika Reinke, Minu D. Tizabi, Carole H. Sudre, Matthias Eisenmann, Tim Rädsch, Michael Baumgartner, Laura Acion, Michela Antonelli, Tal Arbel, Spyridon Bakas, Peter Bankhead, Arriel Benis, Matthew Blaschko, Florian Büttner, M. Jorge Cardoso, Jianxu Chen, Veronika Cheplygina, Evangelia Christodoulou, Beth Cimini, Gary S. Collins, Sandy Engelhardt, Keyvan Farahani, Luciana Ferrer, Adrian Galdran, Bram van Ginneken, Ben Glocker, Patrick Godau, Robert Haase, Fred Hamprecht, Daniel A. Hashimoto, Doreen Heckmann-Nötzel, Peter Hirsch, Michael M. Hoffman, Merel Huisman, Fabian Isensee, Pierre Jannin, Charles E. Kahn, Dagmar Kainmueller, Bernhard Kainz, Alexandros Karargyris, Alan Karthikesalingam, A. Emre Kavur, Hannes Kenngott, Jens Kleesiek, Andreas Kleppe, Sven Kohler, Florian Kofler, Annette Kopp-Schneider, Thijs Kooi, Michal Kozubek, Anna Kreshuk, Tahsin Kurc, Bennett A. Landman, Geert Litjens, Amin Madani, Klaus Maier-Hein, Anne L. Martel, Peter Mattson, Erik Meijering, Bjoern Menze, David Moher, Karel G. M. Moons, Henning Müller, Brennan Nichyporuk, Felix Nickel, M. Alican Noyan, Jens Petersen, Gorkem Polat, Susanne M. Rafelski, Nasir Rajpoot, Mauricio Reyes, Nicola Rieke, Michael Riegler, Hassan Rivaz, Julio Saez-Rodriguez, Clara I. Sánchez, Julien Schroeter, Anindo Saha, M. Alper Selver, Lalith Sharan, Shravya Shetty, Maarten van Smeden, Bram Stieltjes, Ronald M. Summers, Abdel A. Taha, Aleksei Tiulpin, Sotirios A. Tsaftaris, Ben Van Calster, Gaël Varoquaux, Manuel Wiesenfarth, Ziv R. Yaniv, Paul Jäger, Lena Maier-Hein
- **Comment**: Shared first authors: Annika Reinke and Minu D. Tizabi. This is a
  dynamic paper on limitations of commonly used metrics. It discusses metrics
  for image-level classification, semantic and instance segmentation, and
  object detection. For missing use cases, comments or questions, please
  contact a.reinke@dkfz.de. Substantial contributions to this document will be
  acknowledged with a co-authorship
- **Journal**: None
- **Summary**: While the importance of automatic image analysis is continuously increasing, recent meta-research revealed major flaws with respect to algorithm validation. Performance metrics are particularly key for meaningful, objective, and transparent performance assessment and validation of the used automatic algorithms, but relatively little attention has been given to the practical pitfalls when using specific metrics for a given image analysis task. These are typically related to (1) the disregard of inherent metric properties, such as the behaviour in the presence of class imbalance or small target structures, (2) the disregard of inherent data set properties, such as the non-independence of the test cases, and (3) the disregard of the actual biomedical domain interest that the metrics should reflect. This living dynamically document has the purpose to illustrate important limitations of performance metrics commonly applied in the field of image analysis. In this context, it focuses on biomedical image analysis problems that can be phrased as image-level classification, semantic segmentation, instance segmentation, or object detection task. The current version is based on a Delphi process on metrics conducted by an international consortium of image analysis experts from more than 60 institutions worldwide.



### Fruit Quality and Defect Image Classification with Conditional GAN Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.05647v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05647v1)
- **Published**: 2021-04-12 17:13:05+00:00
- **Updated**: 2021-04-12 17:13:05+00:00
- **Authors**: Jordan J. Bird, Chloe M. Barnes, Luis J. Manso, Anikó Ekárt, Diego R. Faria
- **Comment**: 16 pages, 12 figures, 3 tables
- **Journal**: None
- **Summary**: Contemporary Artificial Intelligence technologies allow for the employment of Computer Vision to discern good crops from bad, providing a step in the pipeline of selecting healthy fruit from undesirable fruit, such as those which are mouldy or gangrenous. State-of-the-art works in the field report high accuracy results on small datasets (<1000 images), which are not representative of the population regarding real-world usage. The goals of this study are to further enable real-world usage by improving generalisation with data augmentation as well as to reduce overfitting and energy usage through model pruning. In this work, we suggest a machine learning pipeline that combines the ideas of fine-tuning, transfer learning, and generative model-based training data augmentation towards improving fruit quality image classification. A linear network topology search is performed to tune a VGG16 lemon quality classification model using a publicly-available dataset of 2690 images. We find that appending a 4096 neuron fully connected layer to the convolutional layers leads to an image classification accuracy of 83.77%. We then train a Conditional Generative Adversarial Network on the training data for 2000 epochs, and it learns to generate relatively realistic images. Grad-CAM analysis of the model trained on real photographs shows that the synthetic images can exhibit classifiable characteristics such as shape, mould, and gangrene. A higher image classification accuracy of 88.75% is then attained by augmenting the training with synthetic images, arguing that Conditional Generative Adversarial Networks have the ability to produce new data to alleviate issues of data scarcity. Finally, model pruning is performed via polynomial decay, where we find that the Conditional GAN-augmented classification network can retain 81.16% classification accuracy when compressed to 50% of its original size.



### CAPRI-Net: Learning Compact CAD Shapes with Adaptive Primitive Assembly
- **Arxiv ID**: http://arxiv.org/abs/2104.05652v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.05652v1)
- **Published**: 2021-04-12 17:21:19+00:00
- **Updated**: 2021-04-12 17:21:19+00:00
- **Authors**: Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi, Hooman Shayani, Ali Mahdavi-Amiri, Hao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce CAPRI-Net, a neural network for learning compact and interpretable implicit representations of 3D computer-aided design (CAD) models, in the form of adaptive primitive assemblies. Our network takes an input 3D shape that can be provided as a point cloud or voxel grids, and reconstructs it by a compact assembly of quadric surface primitives via constructive solid geometry (CSG) operations. The network is self-supervised with a reconstruction loss, leading to faithful 3D reconstructions with sharp edges and plausible CSG trees, without any ground-truth shape assemblies. While the parametric nature of CAD models does make them more predictable locally, at the shape level, there is a great deal of structural and topological variations, which present a significant generalizability challenge to state-of-the-art neural models for 3D shapes. Our network addresses this challenge by adaptive training with respect to each test shape, with which we fine-tune the network that was pre-trained on a model collection. We evaluate our learning framework on both ShapeNet and ABC, the largest and most diverse CAD dataset to date, in terms of reconstruction quality, shape edges, compactness, and interpretability, to demonstrate superiority over current alternatives suitable for neural CAD reconstruction.



### View-Guided Point Cloud Completion
- **Arxiv ID**: http://arxiv.org/abs/2104.05666v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05666v2)
- **Published**: 2021-04-12 17:35:45+00:00
- **Updated**: 2021-04-13 04:43:04+00:00
- **Authors**: Xuancheng Zhang, Yutong Feng, Siqi Li, Changqing Zou, Hai Wan, Xibin Zhao, Yandong Guo, Yue Gao
- **Comment**: 10 pages, 8 figures, CVPR2021
- **Journal**: None
- **Summary**: This paper presents a view-guided solution for the task of point cloud completion. Unlike most existing methods directly inferring the missing points using shape priors, we address this task by introducing ViPC (view-guided point cloud completion) that takes the missing crucial global structure information from an extra single-view image. By leveraging a framework that sequentially performs effective cross-modality and cross-level fusions, our method achieves significantly superior results over typical existing solutions on a new large-scale dataset we collect for the view-guided point cloud completion task.



### Learning Robust Visual-semantic Mapping for Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.05668v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05668v1)
- **Published**: 2021-04-12 17:39:38+00:00
- **Updated**: 2021-04-12 17:39:38+00:00
- **Authors**: Jingcai Guo
- **Comment**: thesis, Dec 2020
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims at recognizing unseen class examples (e.g., images) with knowledge transferred from seen classes. This is typically achieved by exploiting a semantic feature space shared by both seen and unseen classes, e.g., attributes or word vectors, as the bridge. In ZSL, the common practice is to train a mapping function between the visual and semantic feature spaces with labeled seen class examples. When inferring, given unseen class examples, the learned mapping function is reused to them and recognizes the class labels on some metrics among their semantic relations. However, the visual and semantic feature spaces are generally independent and exist in entirely different manifolds. Under such a paradigm, the ZSL models may easily suffer from the domain shift problem when constructing and reusing the mapping function, which becomes the major challenge in ZSL. In this thesis, we explore effective ways to mitigate the domain shift problem and learn a robust mapping function between the visual and semantic feature spaces. We focus on fully empowering the semantic feature space, which is one of the key building blocks of ZSL. In summary, this thesis targets fully empowering the semantic feature space and design effective solutions to mitigate the domain shift problem and hence obtain a more robust visual-semantic mapping function for ZSL. Extensive experiments on various datasets demonstrate the effectiveness of our proposed methods.



### Action-Conditioned 3D Human Motion Synthesis with Transformer VAE
- **Arxiv ID**: http://arxiv.org/abs/2104.05670v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05670v2)
- **Published**: 2021-04-12 17:40:27+00:00
- **Updated**: 2021-09-19 12:09:37+00:00
- **Authors**: Mathis Petrovich, Michael J. Black, Gül Varol
- **Comment**: ICCV 2021 Camera ready, 14 pages, 7 figures
- **Journal**: None
- **Summary**: We tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In contrast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encodings, we synthesize variable-length motion sequences conditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We evaluate our approach on the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improving action recognition through adding our synthesized data to training, and motion denoising. Code and models are available on our project page.



### Behavioral Research and Practical Models of Drivers' Attention
- **Arxiv ID**: http://arxiv.org/abs/2104.05677v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.05677v3)
- **Published**: 2021-04-12 17:42:04+00:00
- **Updated**: 2021-12-13 17:48:38+00:00
- **Authors**: Iuliia Kotseruba, John K. Tsotsos
- **Comment**: 78 pages, 21 figures, 9 tables
- **Journal**: None
- **Summary**: Driving is a routine activity for many, but it is far from simple. Drivers deal with multiple concurrent tasks, such as keeping the vehicle in the lane, observing and anticipating the actions of other road users, reacting to hazards, and dealing with distractions inside and outside the vehicle. Failure to notice and respond to the surrounding objects and events can cause accidents. The ongoing improvements of the road infrastructure and vehicle mechanical design have made driving safer overall. Nevertheless, the problem of driver inattention has remained one of the primary causes of accidents. Therefore, understanding where the drivers look and why they do so can help eliminate sources of distractions and identify unsafe attention patterns. Research on driver attention has implications for many practical applications such as policy-making, improving driver education, enhancing road infrastructure and in-vehicle infotainment systems, as well as designing systems for driver monitoring, driver assistance, and automated driving. This report covers the literature on changes in drivers' visual attention distribution due to factors, internal and external to the driver. Aspects of attention during driving have been explored across multiple disciplines, including psychology, human factors, human-computer interaction, intelligent transportation, and computer vision, each offering different perspectives, goals, and explanations for the observed phenomena. We link cross-disciplinary theoretical and behavioral research on driver's attention to practical solutions. Furthermore, limitations and directions for future research are discussed. This report is based on over 175 behavioral studies, nearly 100 practical papers, 20 datasets, and over 70 surveys published since 2010. A curated list of papers used for this report is available at \url{https://github.com/ykotseruba/attention_and_driving}.



### Holistic Image Manipulation Detection using Pixel Co-occurrence Matrices
- **Arxiv ID**: http://arxiv.org/abs/2104.05693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05693v1)
- **Published**: 2021-04-12 17:54:42+00:00
- **Updated**: 2021-04-12 17:54:42+00:00
- **Authors**: Lakshmanan Nataraj, Michael Goebel, Tajuddin Manhar Mohammed, Shivkumar Chandrasekaran, B. S. Manjunath
- **Comment**: None
- **Journal**: None
- **Summary**: Digital image forensics aims to detect images that have been digitally manipulated. Realistic image forgeries involve a combination of splicing, resampling, region removal, smoothing and other manipulation methods. While most detection methods in literature focus on detecting a particular type of manipulation, it is challenging to identify doctored images that involve a host of manipulations. In this paper, we propose a novel approach to holistically detect tampered images using a combination of pixel co-occurrence matrices and deep learning. We extract horizontal and vertical co-occurrence matrices on three color channels in the pixel domain and train a model using a deep convolutional neural network (CNN) framework. Our method is agnostic to the type of manipulation and classifies an image as tampered or untampered. We train and validate our model on a dataset of more than 86,000 images. Experimental results show that our approach is promising and achieves more than 0.99 area under the curve (AUC) evaluation metric on the training and validation subsets. Further, our approach also generalizes well and achieves around 0.81 AUC on an unseen test dataset comprising more than 19,740 images released as part of the Media Forensics Challenge (MFC) 2020. Our score was highest among all other teams that participated in the challenge, at the time of announcement of the challenge results.



### Image-Level or Object-Level? A Tale of Two Resampling Strategies for Long-Tailed Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.05702v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.05702v2)
- **Published**: 2021-04-12 17:58:30+00:00
- **Updated**: 2021-10-18 21:35:10+00:00
- **Authors**: Nadine Chang, Zhiding Yu, Yu-Xiong Wang, Anima Anandkumar, Sanja Fidler, Jose M. Alvarez
- **Comment**: Accepted to ICML 2021
- **Journal**: None
- **Summary**: Training on datasets with long-tailed distributions has been challenging for major recognition tasks such as classification and detection. To deal with this challenge, image resampling is typically introduced as a simple but effective approach. However, we observe that long-tailed detection differs from classification since multiple classes may be present in one image. As a result, image resampling alone is not enough to yield a sufficiently balanced distribution at the object level. We address object-level resampling by introducing an object-centric memory replay strategy based on dynamic, episodic memory banks. Our proposed strategy has two benefits: 1) convenient object-level resampling without significant extra computation, and 2) implicit feature-level augmentation from model updates. We show that image-level and object-level resamplings are both important, and thus unify them with a joint resampling strategy (RIO). Our method outperforms state-of-the-art long-tailed detection and segmentation methods on LVIS v0.5 across various backbones. Code is available at https://github.com/NVlabs/RIO.



### Adversarial Open Domain Adaptation for Sketch-to-Photo Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2104.05703v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.05703v2)
- **Published**: 2021-04-12 17:58:46+00:00
- **Updated**: 2021-12-22 02:51:29+00:00
- **Authors**: Xiaoyu Xiang, Ding Liu, Xiao Yang, Yiheng Zhu, Xiaohui Shen, Jan P. Allebach
- **Comment**: Accepted by WACV 2022
- **Journal**: None
- **Summary**: In this paper, we explore open-domain sketch-to-photo translation, which aims to synthesize a realistic photo from a freehand sketch with its class label, even if the sketches of that class are missing in the training data. It is challenging due to the lack of training supervision and the large geometric distortion between the freehand sketch and photo domains. To synthesize the absent freehand sketches from photos, we propose a framework that jointly learns sketch-to-photo and photo-to-sketch generation. However, the generator trained from fake sketches might lead to unsatisfying results when dealing with sketches of missing classes, due to the domain gap between synthesized sketches and real ones. To alleviate this issue, we further propose a simple yet effective open-domain sampling and optimization strategy to "fool" the generator into treating fake sketches as real ones. Our method takes advantage of the learned sketch-to-photo and photo-to-sketch mapping of in-domain data and generalizes it to the open-domain classes. We validate our method on the Scribble and SketchyCOCO datasets. Compared with the recent competing methods, our approach shows impressive results in synthesizing realistic color, texture, and maintaining the geometric composition for various categories of open-domain sketches. Our code is available at https://github.com/Mukosame/AODA



### Escaping the Big Data Paradigm with Compact Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.05704v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.05704v4)
- **Published**: 2021-04-12 17:58:56+00:00
- **Updated**: 2022-06-07 19:25:30+00:00
- **Authors**: Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu Abuduweili, Jiachen Li, Humphrey Shi
- **Comment**: Added new results on Flowers-102, distillation
- **Journal**: None
- **Summary**: With the rise of Transformers as the standard for language processing, and their advancements in computer vision, there has been a corresponding growth in parameter size and amounts of training data. Many have come to believe that because of this, transformers are not suitable for small sets of data. This trend leads to concerns such as: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we aim to present an approach for small-scale learning by introducing Compact Transformers. We show for the first time that with the right size, convolutional tokenization, transformers can avoid overfitting and outperform state-of-the-art CNNs on small datasets. Our models are flexible in terms of model size, and can have as little as 0.28M parameters while achieving competitive results. Our best model can reach 98% accuracy when training from scratch on CIFAR-10 with only 3.7M parameters, which is a significant improvement in data-efficiency over previous Transformer based models being over 10x smaller than other transformers and is 15% the size of ResNet50 while achieving similar performance. CCT also outperforms many modern CNN based approaches, and even some recent NAS-based approaches. Additionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1 accuracy, and improve upon the existing baseline on ImageNet (82.71% accuracy with 29% as many parameters as ViT), as well as NLP tasks. Our simple and compact design for transformers makes them more feasible to study for those with limited computing resources and/or dealing with small datasets, while extending existing research efforts in data efficient transformers. Our code and pre-trained models are publicly available at https://github.com/SHI-Labs/Compact-Transformers.



### Towards Efficient Graph Convolutional Networks for Point Cloud Handling
- **Arxiv ID**: http://arxiv.org/abs/2104.05706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05706v1)
- **Published**: 2021-04-12 17:59:16+00:00
- **Updated**: 2021-04-12 17:59:16+00:00
- **Authors**: Yawei Li, He Chen, Zhaopeng Cui, Radu Timofte, Marc Pollefeys, Gregory Chirikjian, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we aim at improving the computational efficiency of graph convolutional networks (GCNs) for learning on point clouds. The basic graph convolution that is typically composed of a $K$-nearest neighbor (KNN) search and a multilayer perceptron (MLP) is examined. By mathematically analyzing the operations there, two findings to improve the efficiency of GCNs are obtained. (1) The local geometric structure information of 3D representations propagates smoothly across the GCN that relies on KNN search to gather neighborhood features. This motivates the simplification of multiple KNN searches in GCNs. (2) Shuffling the order of graph feature gathering and an MLP leads to equivalent or similar composite operations. Based on those findings, we optimize the computational procedure in GCNs. A series of experiments show that the optimized networks have reduced computational complexity, decreased memory consumption, and accelerated inference speed while maintaining comparable accuracy for learning on point clouds. Code will be available at \url{https://github.com/ofsoundof/EfficientGCN.git}.



### LocalViT: Bringing Locality to Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.05707v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05707v1)
- **Published**: 2021-04-12 17:59:22+00:00
- **Updated**: 2021-04-12 17:59:22+00:00
- **Authors**: Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: We study how to introduce locality mechanisms into vision transformers. The transformer network originates from machine translation and is particularly good at modelling long-range dependencies within a long sequence. Although the global interaction between the token embeddings could be well modelled by the self-attention mechanism of transformers, what is lacking a locality mechanism for information exchange within a local region. Yet, locality is essential for images since it pertains to structures like lines, edges, shapes, and even objects.   We add locality to vision transformers by introducing depth-wise convolution into the feed-forward network. This seemingly simple solution is inspired by the comparison between feed-forward networks and inverted residual blocks. The importance of locality mechanisms is validated in two ways: 1) A wide range of design choices (activation function, layer placement, expansion ratio) are available for incorporating locality mechanisms and all proper choices can lead to a performance gain over the baseline, and 2) The same locality mechanism is successfully applied to 4 vision transformers, which shows the generalization of the locality concept. In particular, for ImageNet2012 classification, the locality-enhanced transformers outperform the baselines DeiT-T and PVT-T by 2.6\% and 3.1\% with a negligible increase in the number of parameters and computational effort. Code is available at \url{https://github.com/ofsoundof/LocalViT}.



### Towards Extremely Compact RNNs for Video Recognition with Fully Decomposed Hierarchical Tucker Structure
- **Arxiv ID**: http://arxiv.org/abs/2104.05758v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.05758v3)
- **Published**: 2021-04-12 18:40:44+00:00
- **Updated**: 2021-04-20 17:54:22+00:00
- **Authors**: Miao Yin, Siyu Liao, Xiao-Yang Liu, Xiaodong Wang, Bo Yuan
- **Comment**: This paper is a preprint that was accepted in CVPR'21. arXiv admin
  note: substantial text overlap with arXiv:2005.04366
- **Journal**: None
- **Summary**: Recurrent Neural Networks (RNNs) have been widely used in sequence analysis and modeling. However, when processing high-dimensional data, RNNs typically require very large model sizes, thereby bringing a series of deployment challenges. Although various prior works have been proposed to reduce the RNN model sizes, executing RNN models in resource-restricted environments is still a very challenging problem. In this paper, we propose to develop extremely compact RNN models with fully decomposed hierarchical Tucker (FDHT) structure. The HT decomposition does not only provide much higher storage cost reduction than the other tensor decomposition approaches but also brings better accuracy performance improvement for the compact RNN models. Meanwhile, unlike the existing tensor decomposition-based methods that can only decompose the input-to-hidden layer of RNNs, our proposed fully decomposition approach enables the comprehensive compression for the entire RNN models with maintaining very high accuracy. Our experimental results on several popular video recognition datasets show that our proposed fully decomposed hierarchical tucker-based LSTM (FDHT-LSTM) is extremely compact and highly efficient. To the best of our knowledge, FDHT-LSTM, for the first time, consistently achieves very high accuracy with only few thousand parameters (3,132 to 8,808) on different datasets. Compared with the state-of-the-art compressed RNN models, such as TT-LSTM, TR-LSTM and BT-LSTM, our FDHT-LSTM simultaneously enjoys both order-of-magnitude (3,985x to 10,711x) fewer parameters and significant accuracy improvement (0.6% to 12.7%).



### Domain Adaptive Monocular Depth Estimation With Semantic Information
- **Arxiv ID**: http://arxiv.org/abs/2104.05764v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05764v1)
- **Published**: 2021-04-12 18:50:41+00:00
- **Updated**: 2021-04-12 18:50:41+00:00
- **Authors**: Fei Lu, Hyeonwoo Yu, Jean Oh
- **Comment**: 8 pages, 5 figures, code will be released soon
- **Journal**: None
- **Summary**: The advent of deep learning has brought an impressive advance to monocular depth estimation, e.g., supervised monocular depth estimation has been thoroughly investigated. However, the large amount of the RGB-to-depth dataset may not be always available since collecting accurate depth ground truth according to the RGB image is a time-consuming and expensive task. Although the network can be trained on an alternative dataset to overcome the dataset scale problem, the trained model is hard to generalize to the target domain due to the domain discrepancy. Adversarial domain alignment has demonstrated its efficacy to mitigate the domain shift on simple image classification tasks in previous works. However, traditional approaches hardly handle the conditional alignment as they solely consider the feature map of the network. In this paper, we propose an adversarial training model that leverages semantic information to narrow the domain gap. Based on the experiments conducted on the datasets for the monocular depth estimation task including KITTI and Cityscapes, the proposed compact model achieves state-of-the-art performance comparable to complex latest models and shows favorable results on boundaries and objects at far distances.



### Efficient Space-time Video Super Resolution using Low-Resolution Flow and Mask Upsampling
- **Arxiv ID**: http://arxiv.org/abs/2104.05778v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05778v3)
- **Published**: 2021-04-12 19:11:57+00:00
- **Updated**: 2021-06-08 04:17:40+00:00
- **Authors**: Saikat Dutta, Nisarg A. Shah, Anurag Mittal
- **Comment**: Accepted at NTIRE Workshop, CVPR 2021. Code and models:
  https://github.com/saikatdutta/FMU_STSR
- **Journal**: None
- **Summary**: This paper explores an efficient solution for Space-time Super-Resolution, aiming to generate High-resolution Slow-motion videos from Low Resolution and Low Frame rate videos. A simplistic solution is the sequential running of Video Super Resolution and Video Frame interpolation models. However, this type of solutions are memory inefficient, have high inference time, and could not make the proper use of space-time relation property. To this extent, we first interpolate in LR space using quadratic modeling. Input LR frames are super-resolved using a state-of-the-art Video Super-Resolution method. Flowmaps and blending mask which are used to synthesize LR interpolated frame is reused in HR space using bilinear upsampling. This leads to a coarse estimate of HR intermediate frame which often contains artifacts along motion boundaries. We use a refinement network to improve the quality of HR intermediate frame via residual learning. Our model is lightweight and performs better than current state-of-the-art models in REDS STSR Validation set.



### Multi-View Image-to-Image Translation Supervised by 3D Pose
- **Arxiv ID**: http://arxiv.org/abs/2104.05779v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05779v1)
- **Published**: 2021-04-12 19:12:39+00:00
- **Updated**: 2021-04-12 19:12:39+00:00
- **Authors**: Idit Diamant, Oranit Dror, Hai Victor Habi, Arnon Netzer
- **Comment**: *equal contribution
- **Journal**: None
- **Summary**: We address the task of multi-view image-to-image translation for person image generation. The goal is to synthesize photo-realistic multi-view images with pose-consistency across all views. Our proposed end-to-end framework is based on a joint learning of multiple unpaired image-to-image translation models, one per camera viewpoint. The joint learning is imposed by constraints on the shared 3D human pose in order to encourage the 2D pose projections in all views to be consistent. Experimental results on the CMU-Panoptic dataset demonstrate the effectiveness of the suggested framework in generating photo-realistic images of persons with new poses that are more consistent across all views in comparison to a standard Image-to-Image baseline. The code is available at: https://github.com/sony-si/MultiView-Img2Img



### A Recipe for Global Convergence Guarantee in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.05785v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.05785v2)
- **Published**: 2021-04-12 19:25:30+00:00
- **Updated**: 2021-04-15 20:35:03+00:00
- **Authors**: Kenji Kawaguchi, Qingyun Sun
- **Comment**: Published in AAAI 2021
- **Journal**: None
- **Summary**: Existing global convergence guarantees of (stochastic) gradient descent do not apply to practical deep networks in the practical regime of deep learning beyond the neural tangent kernel (NTK) regime. This paper proposes an algorithm, which is ensured to have global convergence guarantees in the practical regime beyond the NTK regime, under a verifiable condition called the expressivity condition. The expressivity condition is defined to be both data-dependent and architecture-dependent, which is the key property that makes our results applicable for practical settings beyond the NTK regime. On the one hand, the expressivity condition is theoretically proven to hold data-independently for fully-connected deep neural networks with narrow hidden layers and a single wide layer. On the other hand, the expressivity condition is numerically shown to hold data-dependently for deep (convolutional) ResNet with batch normalization with various standard image datasets. We also show that the proposed algorithm has generalization performances comparable with those of the heuristic algorithm, with the same hyper-parameters and total number of iterations. Therefore, the proposed algorithm can be viewed as a step towards providing theoretical guarantees for deep learning in the practical regime.



### Understanding Fission Gas Bubble Distribution, Lanthanide Transportation, and Thermal Conductivity Degradation in Neutron-irradiated α-U Using Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.05786v1
- **DOI**: None
- **Categories**: **cond-mat.mtrl-sci**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05786v1)
- **Published**: 2021-04-12 19:29:18+00:00
- **Updated**: 2021-04-12 19:29:18+00:00
- **Authors**: Lu Cai, Fei Xu, Fidelma Dilemma, Daniel J. Murray, Cynthia A. Adkins, Larry K Aagesen Jr, Min Xian, Luca Caprriot, Tiankai Yao
- **Comment**: 19 pages, 12 figures, 2 tables
- **Journal**: None
- **Summary**: UZr based metallic nuclear fuel is the leading candidate for next-generation sodium-cooled fast reactors in the United States. US research reactors have been using and testing this fuel type since the 1960s and accumulated considerable experience and knowledge about the fuel performance. However, most of knowledge remains empirical. The lack of mechanistic understanding of fuel performance is preventing the qualification of UZr fuel for commercial use. This paper proposes a data-driven approach, coupled with advanced post irradiation examination, powered by machine learning algorithms, to facilitate the development of such understandings by providing unpreceded quantified new insights into fission gas bubbles. Specifically, based on the advanced postirradiation examination data collected on a neutron-irradiated U-10Zr annular fuel, we developed a method to automatically detect, classify ~19,000 fission gas bubbles into different categories, and quantitatively link the data to lanthanide transpiration along the radial temperature gradient. The approach is versatile and can be modified to study different coupled irradiation effects, such as secondary phase redistribution and degradation of thermal conductivity, in irradiated nuclear fuel.



### Spatially Varying Label Smoothing: Capturing Uncertainty from Expert Annotations
- **Arxiv ID**: http://arxiv.org/abs/2104.05788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05788v1)
- **Published**: 2021-04-12 19:35:51+00:00
- **Updated**: 2021-04-12 19:35:51+00:00
- **Authors**: Mobarakol Islam, Ben Glocker
- **Comment**: Accepted at IPMI 2021
- **Journal**: None
- **Summary**: The task of image segmentation is inherently noisy due to ambiguities regarding the exact location of boundaries between anatomical structures. We argue that this information can be extracted from the expert annotations at no extra cost, and when integrated into state-of-the-art neural networks, it can lead to improved calibration between soft probabilistic predictions and the underlying uncertainty. We built upon label smoothing (LS) where a network is trained on 'blurred' versions of the ground truth labels which has been shown to be effective for calibrating output predictions. However, LS is not taking the local structure into account and results in overly smoothed predictions with low confidence even for non-ambiguous regions. Here, we propose Spatially Varying Label Smoothing (SVLS), a soft labeling technique that captures the structural uncertainty in semantic segmentation. SVLS also naturally lends itself to incorporate inter-rater uncertainty when multiple labelmaps are available. The proposed approach is extensively validated on four clinical segmentation tasks with different imaging modalities, number of classes and single and multi-rater expert annotations. The results demonstrate that SVLS, despite its simplicity, obtains superior boundary prediction with improved uncertainty and model calibration.



### Detecting Vehicle Type and License Plate Number of different Vehicles on Images
- **Arxiv ID**: http://arxiv.org/abs/2104.09568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09568v1)
- **Published**: 2021-04-12 19:51:17+00:00
- **Updated**: 2021-04-12 19:51:17+00:00
- **Authors**: Aashna Ahuja, Arindam Chaudhuri
- **Comment**: Present Research Work in Progress
- **Journal**: None
- **Summary**: With ever increasing number of vehicles, vehicular tracking is one of the major challenges faced by urban areas. In this paper we try to develop a model that can locate a particular vehicle that the user is looking for depending on two factors 1. the Type of vehicle and the 2. License plate number of the car. The proposed system uses a unique mixture consisting of Mask R-CNN model for vehicle type detection, WpodNet and pytesseract for License Plate detection and Prediction of letters in it.



### Generalizable Multi-Camera 3D Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.05813v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05813v1)
- **Published**: 2021-04-12 20:58:25+00:00
- **Updated**: 2021-04-12 20:58:25+00:00
- **Authors**: João Paulo Lima, Rafael Roberto, Lucas Figueiredo, Francisco Simões, Veronica Teichrieb
- **Comment**: Accepted to CVPRW 2021, LatinX in Computer Vision (LXCV) Workshop
- **Journal**: None
- **Summary**: We present a multi-camera 3D pedestrian detection method that does not need to train using data from the target scene. We estimate pedestrian location on the ground plane using a novel heuristic based on human body poses and person's bounding boxes from an off-the-shelf monocular detector. We then project these locations onto the world ground plane and fuse them with a new formulation of a clique cover problem. We also propose an optional step for exploiting pedestrian appearance during fusion by using a domain-generalizable person re-identification model. We evaluated the proposed approach on the challenging WILDTRACK dataset. It obtained a MODA of 0.569 and an F-score of 0.78, superior to state-of-the-art generalizable detection techniques.



### Localization-Based Tracking
- **Arxiv ID**: http://arxiv.org/abs/2104.05823v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2104.05823v1)
- **Published**: 2021-04-12 21:13:55+00:00
- **Updated**: 2021-04-12 21:13:55+00:00
- **Authors**: Derek Gloudemans, Daniel B. Work
- **Comment**: 10 pages, 5 figures, please email author for supplementary material
- **Journal**: None
- **Summary**: End-to-end production of object tracklets from high resolution video in real-time and with high accuracy remains a challenging problem due to the cost of object detection on each frame. In this work we present Localization-based Tracking (LBT), an extension to any tracker that follows the tracking by detection or joint detection and tracking paradigms. Localization-based Tracking focuses only on regions likely to contain objects to boost detection speed and avoid matching errors. We evaluate LBT as an extension to two example trackers (KIOU and SORT) on the UA-DETRAC and MOT20 datasets. LBT-extended trackers outperform all other reported algorithms in terms of PR-MOTA, PR-MOTP, and mostly tracked objects on the UA-DETRAC benchmark, establishing a new state-of-the art. relative to tracking by detection with KIOU, LBT-extended KIOU achieves a 25% higher frame-rate and is 1.1% more accurate in terms of PR-MOTA on the UA-DETRAC dataset. LBT-extended SORT achieves a 62% speedup and a 3.2% increase in PR-MOTA on the UA-DETRAC dataset. On MOT20, LBT-extended KIOU has a 50% higher frame-rate than tracking by detection and is 0.4% more accurate in terms of MOTA. As of submission time, our LBT-extended KIOU tracker places 10th overall on the MOT20 benchmark.



### Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2104.05833v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.05833v1)
- **Published**: 2021-04-12 21:41:25+00:00
- **Updated**: 2021-04-12 21:41:25+00:00
- **Authors**: Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, Sanja Fidler
- **Comment**: CVPR2021
- **Journal**: None
- **Summary**: Training deep networks with limited labeled data while achieving a strong generalization ability is key in the quest to reduce human annotation efforts. This is the goal of semi-supervised learning, which exploits more widely available unlabeled data to complement small labeled data sets. In this paper, we propose a novel framework for discriminative pixel-level tasks using a generative model of both images and labels. Concretely, we learn a generative adversarial network that captures the joint image-label distribution and is trained efficiently using a large set of unlabeled images supplemented with only few labeled ones. We build our architecture on top of StyleGAN2, augmented with a label synthesis branch. Image labeling at test time is achieved by first embedding the target image into the joint latent space via an encoder network and test-time optimization, and then generating the label from the inferred embedding. We evaluate our approach in two important domains: medical image segmentation and part-based face segmentation. We demonstrate strong in-domain performance compared to several baselines, and are the first to showcase extreme out-of-domain generalization, such as transferring from CT to MRI in medical imaging, and photographs of real faces to paintings, sculptures, and even cartoons and animal faces. Project Page: \url{https://nv-tlabs.github.io/semanticGAN/}



### Visual Goal-Step Inference using wikiHow
- **Arxiv ID**: http://arxiv.org/abs/2104.05845v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.05845v2)
- **Published**: 2021-04-12 22:20:09+00:00
- **Updated**: 2021-09-10 03:10:13+00:00
- **Authors**: Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar, Chris Callison-Burch
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding what sequence of steps are needed to complete a goal can help artificial intelligence systems reason about human activities. Past work in NLP has examined the task of goal-step inference for text. We introduce the visual analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model is given a textual goal and must choose which of four images represents a plausible step towards that goal. With a new dataset harvested from wikiHow consisting of 772,277 images representing human actions, we show that our task is challenging for state-of-the-art multimodal models. Moreover, the multimodal representation learned from our data can be effectively transferred to other datasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task will facilitate multimodal reasoning about procedural events.



### Exploring Geometric Consistency for Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.05858v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05858v2)
- **Published**: 2021-04-12 23:12:48+00:00
- **Updated**: 2022-05-22 01:23:21+00:00
- **Authors**: Qing Lian, Botao Ye, Ruijia Xu, Weilong Yao, Tong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper investigates the geometric consistency for monocular 3D object detection, which suffers from the ill-posed depth estimation. We first conduct a thorough analysis to reveal how existing methods fail to consistently localize objects when different geometric shifts occur. In particular, we design a series of geometric manipulations to diagnose existing detectors and then illustrate their vulnerability to consistently associate the depth with object apparent sizes and positions. To alleviate this issue, we propose four geometry-aware data augmentation approaches to enhance the geometric consistency of the detectors. We first modify some commonly used data augmentation methods for 2D images so that they can maintain geometric consistency in 3D spaces. We demonstrate such modifications are important. In addition, we propose a 3D-specific image perturbation method that employs the camera movement. During the augmentation process, the camera system with the corresponding image is manipulated, while the geometric visual cues for depth recovery are preserved. We show that by using the geometric consistency constraints, the proposed augmentation techniques lead to improvements on the KITTI and nuScenes monocular 3D detection benchmarks with state-of-the-art results. In addition, we demonstrate that the augmentation methods are well suited for semi-supervised training and cross-dataset generalization.



