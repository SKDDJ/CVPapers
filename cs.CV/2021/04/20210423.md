# Arxiv Papers in cs.CV on 2021-04-23
### Eigenbackground Revisited: Can We Model the Background with Eigenvectors?
- **Arxiv ID**: http://arxiv.org/abs/2104.11379v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11379v2)
- **Published**: 2021-04-23 02:21:03+00:00
- **Updated**: 2022-02-04 19:00:53+00:00
- **Authors**: Mahmood Amintoosi, Farzam Farbiz
- **Comment**: None
- **Journal**: None
- **Summary**: Using dominant eigenvectors for background modeling (usually known as Eigenbackground) is a common technique in the literature. However, its results suffer from noticeable artifacts. Thus have been many attempts to reduce the artifacts by making some improvements/enhancement in the Eigenbackground algorithm.   In this paper, we show the main problem of the Eigenbackground is in its own core and in fact, it is not a good idea to use strongest eigenvectors for modeling the background. Instead, we propose an alternative solution by exploiting the weakest eigenvectors (which are usually thrown away and treated as garbage data) for background modeling. MATLAB codes are available at \url{https://github.com/mamintoosi/Eigenbackground-Revisited}



### Intentional Deep Overfit Learning (IDOL): A Novel Deep Learning Strategy for Adaptive Radiation Therapy
- **Arxiv ID**: http://arxiv.org/abs/2104.11401v1
- **DOI**: 10.1002/mp.15352
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.11401v1)
- **Published**: 2021-04-23 03:41:49+00:00
- **Updated**: 2021-04-23 03:41:49+00:00
- **Authors**: Jaehee Chun, Justin C. Park, Sven Olberg, You Zhang, Dan Nguyen, Jing Wang, Jin Sung Kim, Steve Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we propose a tailored DL framework for patient-specific performance that leverages the behavior of a model intentionally overfitted to a patient-specific training dataset augmented from the prior information available in an ART workflow - an approach we term Intentional Deep Overfit Learning (IDOL). Implementing the IDOL framework in any task in radiotherapy consists of two training stages: 1) training a generalized model with a diverse training dataset of N patients, just as in the conventional DL approach, and 2) intentionally overfitting this general model to a small training dataset-specific the patient of interest (N+1) generated through perturbations and augmentations of the available task- and patient-specific prior information to establish a personalized IDOL model. The IDOL framework itself is task-agnostic and is thus widely applicable to many components of the ART workflow, three of which we use as a proof of concept here: the auto-contouring task on re-planning CTs for traditional ART, the MRI super-resolution (SR) task for MRI-guided ART, and the synthetic CT (sCT) reconstruction task for MRI-only ART. In the re-planning CT auto-contouring task, the accuracy measured by the Dice similarity coefficient improves from 0.847 with the general model to 0.935 by adopting the IDOL model. In the case of MRI SR, the mean absolute error (MAE) is improved by 40% using the IDOL framework over the conventional model. Finally, in the sCT reconstruction task, the MAE is reduced from 68 to 22 HU by utilizing the IDOL framework.



### Low Pass Filter for Anti-aliasing in Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2104.11403v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11403v1)
- **Published**: 2021-04-23 03:57:34+00:00
- **Updated**: 2021-04-23 03:57:34+00:00
- **Authors**: Cece Jin, Yuanqi Chen, Ge Li, Tao Zhang, Thomas Li
- **Comment**: None
- **Journal**: None
- **Summary**: In temporal action localization methods, temporal downsampling operations are widely used to extract proposal features, but they often lead to the aliasing problem, due to lacking consideration of sampling rates. This paper aims to verify the existence of aliasing in TAL methods and investigate utilizing low pass filters to solve this problem by inhibiting the high-frequency band. However, the high-frequency band usually contains large amounts of specific information, which is important for model inference. Therefore, it is necessary to make a tradeoff between anti-aliasing and reserving high-frequency information. To acquire optimal performance, this paper learns different cutoff frequencies for different instances dynamically. This design can be plugged into most existing temporal modeling programs requiring only one additional cutoff frequency parameter. Integrating low pass filters to the downsampling operations significantly improves the detection performance and achieves comparable results on THUMOS'14, ActivityNet~1.3, and Charades datasets. Experiments demonstrate that anti-aliasing with low pass filters in TAL is advantageous and efficient.



### Predicting Distant Metastases in Soft-Tissue Sarcomas from PET-CT scans using Constrained Hierarchical Multi-Modality Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.11416v1
- **DOI**: 10.1088/1361-6560/ac3d17
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.11416v1)
- **Published**: 2021-04-23 05:12:02+00:00
- **Updated**: 2021-04-23 05:12:02+00:00
- **Authors**: Yige Peng, Lei Bi, Ashnil Kumar, Michael Fulham, Dagan Feng, Jinman Kim
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Distant metastases (DM) refer to the dissemination of tumors, usually, beyond the organ where the tumor originated. They are the leading cause of death in patients with soft-tissue sarcomas (STSs). Positron emission tomography-computed tomography (PET-CT) is regarded as the imaging modality of choice for the management of STSs. It is difficult to determine from imaging studies which STS patients will develop metastases. 'Radiomics' refers to the extraction and analysis of quantitative features from medical images and it has been employed to help identify such tumors. The state-of-the-art in radiomics is based on convolutional neural networks (CNNs). Most CNNs are designed for single-modality imaging data (CT or PET alone) and do not exploit the information embedded in PET-CT where there is a combination of an anatomical and functional imaging modality. Furthermore, most radiomic methods rely on manual input from imaging specialists for tumor delineation, definition and selection of radiomic features. This approach, however, may not be scalable to tumors with complex boundaries and where there are multiple other sites of disease. We outline a new 3D CNN to help predict DM in STS patients from PET-CT data. The 3D CNN uses a constrained feature learning module and a hierarchical multi-modality feature learning module that leverages the complementary information from the modalities to focus on semantically important regions. Our results on a public PET-CT dataset of STS patients show that multi-modal information improves the ability to identify those patients who develop DM. Further our method outperformed all other related state-of-the-art methods.



### TricubeNet: 2D Kernel-Based Object Representation for Weakly-Occluded Oriented Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.11435v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11435v2)
- **Published**: 2021-04-23 06:50:28+00:00
- **Updated**: 2021-10-05 11:54:49+00:00
- **Authors**: Beomyoung Kim, Janghyeon Lee, Sihaeng Lee, Doyeon Kim, Junmo Kim
- **Comment**: WACV 2022, Accepted
- **Journal**: None
- **Summary**: We present a novel approach for oriented object detection, named TricubeNet, which localizes oriented objects using visual cues ($i.e.,$ heatmap) instead of oriented box offsets regression. We represent each object as a 2D Tricube kernel and extract bounding boxes using simple image-processing algorithms. Our approach is able to (1) obtain well-arranged boxes from visual cues, (2) solve the angle discontinuity problem, and (3) can save computational complexity due to our anchor-free modeling. To further boost the performance, we propose some effective techniques for size-invariant loss, reducing false detections, extracting rotation-invariant features, and heatmap refinement. To demonstrate the effectiveness of our TricubeNet, we experiment on various tasks for weakly-occluded oriented object detection: detection in an aerial image, densely packed object image, and text image. The extensive experimental results show that our TricubeNet is quite effective for oriented object detection. Code is available at https://github.com/qjadud1994/TricubeNet.



### Learning from Ambiguous Labels for Lung Nodule Malignancy Prediction
- **Arxiv ID**: http://arxiv.org/abs/2104.11436v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.11436v1)
- **Published**: 2021-04-23 07:01:10+00:00
- **Updated**: 2021-04-23 07:01:10+00:00
- **Authors**: Zehui Liao, Yutong Xie, Shishuai Hu, Yong Xia
- **Comment**: Submitted to IEEE-TMI
- **Journal**: None
- **Summary**: Lung nodule malignancy prediction is an essential step in the early diagnosis of lung cancer. Besides the difficulties commonly discussed, the challenges of this task also come from the ambiguous labels provided by annotators, since deep learning models may learn, even amplify, the bias embedded in them. In this paper, we propose a multi-view "divide-and-rule" (MV-DAR) model to learn from both reliable and ambiguous annotations for lung nodule malignancy prediction. According to the consistency and reliability of their annotations, we divide nodules into three sets: a consistent and reliable set (CR-Set), an inconsistent set (IC-Set), and a low reliable set (LR-Set). The nodule in IC-Set is annotated by multiple radiologists inconsistently, and the nodule in LR-Set is annotated by only one radiologist. The proposed MV-DAR contains three DAR submodels to characterize a lung nodule from three orthographic views. Each DAR consists of a prediction network (Prd-Net), a counterfactual network (CF-Net), and a low reliable network (LR-Net), learning on CR-Set, IC-Set, and LR-Set, respectively. The image representation ability learned by CF-Net and LR-Net is then transferred to Prd-Net by negative-attention module (NA-Module) and consistent-attention module (CA-Module), aiming to boost the prediction ability of Prd-Net. The MV-DAR model has been evaluated on the LIDC-IDRI dataset and LUNGx dataset. Our results indicate not only the effectiveness of the proposed MV-DAR model in learning from ambiguous labels but also its superiority over present noisy label-learning models in lung nodule malignancy prediction.



### SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos
- **Arxiv ID**: http://arxiv.org/abs/2104.11452v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11452v4)
- **Published**: 2021-04-23 07:52:03+00:00
- **Updated**: 2021-07-16 02:44:10+00:00
- **Authors**: Xin Chen, Anqi Pang, Wei Yang, Yuexin Ma, Lan Xu, Jingyi Yu
- **Comment**: 18 pages, 13 figures
- **Journal**: None
- **Summary**: Markerless motion capture and understanding of professional non-daily human movements is an important yet unsolved task, which suffers from complex motion patterns and severe self-occlusion, especially for the monocular setting. In this paper, we propose SportsCap -- the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. Our approach utilizes the semantic and temporally structured sub-motion prior in the embedding space for motion capture and understanding in a data-driven multi-task manner. To enable robust capture under complex motion patterns, we propose an effective motion embedding module to recover both the implicit motion embedding and explicit 3D motion details via a corresponding mapping function as well as a sub-motion classifier. Based on such hybrid motion information, we introduce a multi-stream spatial-temporal Graph Convolutional Network(ST-GCN) to predict the fine-grained semantic action attributes, and adopt a semantic attribute mapping block to assemble various correlated action attributes into a high-level action label for the overall detailed understanding of the whole sequence, so as to enable various applications like action assessment or motion scoring. Comprehensive experiments on both public and our proposed datasets show that with a challenging monocular sports video input, our novel approach not only significantly improves the accuracy of 3D human motion capture, but also recovers accurate fine-grained semantic action attributes.



### H2O: A Benchmark for Visual Human-human Object Handover Analysis
- **Arxiv ID**: http://arxiv.org/abs/2104.11466v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.11466v1)
- **Published**: 2021-04-23 08:30:54+00:00
- **Updated**: 2021-04-23 08:30:54+00:00
- **Authors**: Ruolin Ye, Wenqiang Xu, Zhendong Xue, Tutian Tang, Yanfeng Wang, Cewu Lu
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV) 2021
- **Summary**: Object handover is a common human collaboration behavior that attracts attention from researchers in Robotics and Cognitive Science. Though visual perception plays an important role in the object handover task, the whole handover process has been specifically explored. In this work, we propose a novel rich-annotated dataset, H2O, for visual analysis of human-human object handovers. The H2O, which contains 18K video clips involving 15 people who hand over 30 objects to each other, is a multi-purpose benchmark. It can support several vision-based tasks, from which, we specifically provide a baseline method, RGPNet, for a less-explored task named Receiver Grasp Prediction. Extensive experiments show that the RGPNet can produce plausible grasps based on the giver's hand-object states in the pre-handover phase. Besides, we also report the hand and object pose errors with existing baselines and show that the dataset can serve as the video demonstrations for robot imitation learning on the handover task. Dataset, model and code will be made public.



### Probabilistic Rainfall Estimation from Automotive Lidar
- **Arxiv ID**: http://arxiv.org/abs/2104.11467v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, cs.RO, I.2.10; I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2104.11467v2)
- **Published**: 2021-04-23 08:35:54+00:00
- **Updated**: 2022-04-25 05:50:39+00:00
- **Authors**: Robin Karlsson, David Robert Wong, Kazunari Kawabata, Simon Thompson, Naoki Sakai
- **Comment**: Accepted for IEEE IV 2022
- **Journal**: None
- **Summary**: Robust sensing and perception in adverse weather conditions remain one of the biggest challenges for realizing reliable autonomous vehicle mobility services. Prior work has established that rainfall rate is a useful measure for the adversity of atmospheric weather conditions. This work presents a probabilistic hierarchical Bayesian model that infers rainfall rate from automotive lidar point cloud sequences with high accuracy and reliability. The model is a hierarchical mixture of experts model, or a probabilistic decision tree, with gating and expert nodes consisting of variational logistic and linear regression models. Experimental data used to train and evaluate the model is collected in a large-scale rainfall experiment facility from both stationary and moving vehicle platforms. The results show prediction accuracy comparable to the measurement resolution of a disdrometer, and the soundness and usefulness of the uncertainty estimation. The model achieves RMSE 2.42\,mm/h after filtering out uncertain predictions. The error is comparable to the mean rainfall rate change of 3.5\,mm/h between measurements. Model parameter studies show how predictive performance changes with tree depth, sampling duration, and crop box dimension. A second experiment demonstrates the predictability of higher rainfall above 300\,mm/h using a different lidar sensor, demonstrating sensor independence.



### Sequential convolutional network for behavioral pattern extraction in gait recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.11473v1
- **DOI**: 10.1016/j.neucom.2021.08.054
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11473v1)
- **Published**: 2021-04-23 08:44:10+00:00
- **Updated**: 2021-04-23 08:44:10+00:00
- **Authors**: Xinnan Ding, Kejun Wang, Chenhui Wang, Tianyi Lan, Liangliang Liu
- **Comment**: None
- **Journal**: Neurocomputing2021
- **Summary**: As a unique and promising biometric, video-based gait recognition has broad applications. The key step of this methodology is to learn the walking pattern of individuals, which, however, often suffers challenges to extract the behavioral feature from a sequence directly. Most existing methods just focus on either the appearance or the motion pattern. To overcome these limitations, we propose a sequential convolutional network (SCN) from a novel perspective, where spatiotemporal features can be learned by a basic convolutional backbone. In SCN, behavioral information extractors (BIE) are constructed to comprehend intermediate feature maps in time series through motion templates where the relationship between frames can be analyzed, thereby distilling the information of the walking pattern. Furthermore, a multi-frame aggregator in SCN performs feature integration on a sequence whose length is uncertain, via a mobile 3D convolutional layer. To demonstrate the effectiveness, experiments have been conducted on two popular public benchmarks, CASIA-B and OU-MVLP, and our approach is demonstrated superior performance, comparing with the state-of-art methods.



### Towards Accurate Text-based Image Captioning with Content Diversity Exploration
- **Arxiv ID**: http://arxiv.org/abs/2105.03236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.03236v1)
- **Published**: 2021-04-23 08:57:47+00:00
- **Updated**: 2021-04-23 08:57:47+00:00
- **Authors**: Guanghui Xu, Shuaicheng Niu, Mingkui Tan, Yucheng Luo, Qing Du, Qi Wu
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Text-based image captioning (TextCap) which aims to read and reason images with texts is crucial for a machine to understand a detailed and complex scene environment, considering that texts are omnipresent in daily life. This task, however, is very challenging because an image often contains complex texts and visual information that is hard to be described comprehensively. Existing methods attempt to extend the traditional image captioning methods to solve this task, which focus on describing the overall scene of images by one global caption. This is infeasible because the complex text and visual information cannot be described well within one caption. To resolve this difficulty, we seek to generate multiple captions that accurately describe different parts of an image in detail. To achieve this purpose, there are three key challenges: 1) it is hard to decide which parts of the texts of images to copy or paraphrase; 2) it is non-trivial to capture the complex relationship between diverse texts in an image; 3) how to generate multiple captions with diverse content is still an open problem. To conquer these, we propose a novel Anchor-Captioner method. Specifically, we first find the important tokens which are supposed to be paid more attention to and consider them as anchors. Then, for each chosen anchor, we group its relevant texts to construct the corresponding anchor-centred graph (ACG). Last, based on different ACGs, we conduct multi-view caption generation to improve the content diversity of generated captions. Experimental results show that our method not only achieves SOTA performance but also generates diverse captions to describe images.



### Skip-Convolutions for Efficient Video Processing
- **Arxiv ID**: http://arxiv.org/abs/2104.11487v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.11487v1)
- **Published**: 2021-04-23 09:10:39+00:00
- **Updated**: 2021-04-23 09:10:39+00:00
- **Authors**: Amirhossein Habibian, Davide Abati, Taco S. Cohen, Babak Ehteshami Bejnordi
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: We propose Skip-Convolutions to leverage the large amount of redundancies in video streams and save computations. Each video is represented as a series of changes across frames and network activations, denoted as residuals. We reformulate standard convolution to be efficiently computed on residual frames: each layer is coupled with a binary gate deciding whether a residual is important to the model prediction,~\eg foreground regions, or it can be safely skipped, e.g. background regions. These gates can either be implemented as an efficient network trained jointly with convolution kernels, or can simply skip the residuals based on their magnitude. Gating functions can also incorporate block-wise sparsity structures, as required for efficient implementation on hardware platforms. By replacing all convolutions with Skip-Convolutions in two state-of-the-art architectures, namely EfficientDet and HRNet, we reduce their computational cost consistently by a factor of 3~4x for two different tasks, without any accuracy drop. Extensive comparisons with existing model compression, as well as image and video efficiency methods demonstrate that Skip-Convolutions set a new state-of-the-art by effectively exploiting the temporal redundancies in videos.



### Autonomous Vehicles that Alert Humans to Take-Over Controls: Modeling with Real-World Data
- **Arxiv ID**: http://arxiv.org/abs/2104.11489v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.11489v3)
- **Published**: 2021-04-23 09:16:53+00:00
- **Updated**: 2021-07-22 23:03:52+00:00
- **Authors**: Akshay Rangesh, Nachiket Deo, Ross Greer, Pujitha Gunaratne, Mohan M. Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: With increasing automation in passenger vehicles, the study of safe and smooth occupant-vehicle interaction and control transitions is key. In this study, we focus on the development of contextual, semantically meaningful representations of the driver state, which can then be used to determine the appropriate timing and conditions for transfer of control between driver and vehicle. To this end, we conduct a large-scale real-world controlled data study where participants are instructed to take-over control from an autonomous agent under different driving conditions while engaged in a variety of distracting activities. These take-over events are captured using multiple driver-facing cameras, which when labelled result in a dataset of control transitions and their corresponding take-over times (TOTs). We then develop and train TOT models that operate sequentially on mid to high-level features produced by computer vision algorithms operating on different driver-facing camera views. The proposed TOT model produces continuous predictions of take-over times without delay, and shows promising qualitative and quantitative results in complex real-world scenarios.



### Stroke-Based Scene Text Erasing Using Synthetic Data for Training
- **Arxiv ID**: http://arxiv.org/abs/2104.11493v3
- **DOI**: 10.1109/TIP.2021.3125260
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11493v3)
- **Published**: 2021-04-23 09:29:41+00:00
- **Updated**: 2021-12-03 06:13:22+00:00
- **Authors**: Zhengmi Tang, Tomo Miyazaki, Yoshihiro Sugaya, Shinichiro Omachi
- **Comment**: None
- **Journal**: IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 30, 2021, 9306-9320
- **Summary**: Scene text erasing, which replaces text regions with reasonable content in natural images, has drawn significant attention in the computer vision community in recent years. There are two potential subtasks in scene text erasing: text detection and image inpainting. Both subtasks require considerable data to achieve better performance; however, the lack of a large-scale real-world scene-text removal dataset does not allow existing methods to realize their potential. To compensate for the lack of pairwise real-world data, we made considerable use of synthetic text after additional enhancement and subsequently trained our model only on the dataset generated by the improved synthetic text engine. Our proposed network contains a stroke mask prediction module and background inpainting module that can extract the text stroke as a relatively small hole from the cropped text image to maintain more background content for better inpainting results. This model can partially erase text instances in a scene image with a bounding box or work with an existing scene-text detector for automatic scene text erasing. The experimental results from the qualitative and quantitative evaluation on the SCUT-Syn, ICDAR2013, and SCUT-EnsText datasets demonstrate that our method significantly outperforms existing state-of-the-art methods even when they are trained on real-world data.



### Learning to Cluster Faces via Transformer
- **Arxiv ID**: http://arxiv.org/abs/2104.11502v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11502v1)
- **Published**: 2021-04-23 09:43:36+00:00
- **Updated**: 2021-04-23 09:43:36+00:00
- **Authors**: Jinxing Ye, Xioajiang Peng, Baigui Sun, Kai Wang, Xiuyu Sun, Hao Li, Hanqing Wu
- **Comment**: Face Transformer
- **Journal**: None
- **Summary**: Face clustering is a useful tool for applications like automatic face annotation and retrieval. The main challenge is that it is difficult to cluster images from the same identity with different face poses, occlusions, and image quality. Traditional clustering methods usually ignore the relationship between individual images and their neighbors which may contain useful context information. In this paper, we repurpose the well-known Transformer and introduce a Face Transformer for supervised face clustering. In Face Transformer, we decompose the face clustering into two steps: relation encoding and linkage predicting. Specifically, given a face image, a \textbf{relation encoder} module aggregates local context information from its neighbors and a \textbf{linkage predictor} module judges whether a pair of images belong to the same cluster or not. In the local linkage graph view, Face Transformer can generate more robust node and edge representations compared to existing methods. Experiments on both MS-Celeb-1M and DeepFashion show that our method achieves state-of-the-art performance, e.g., 91.12\% in pairwise F-score on MS-Celeb-1M.



### DeepfakeUCL: Deepfake Detection via Unsupervised Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.11507v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.11507v1)
- **Published**: 2021-04-23 09:48:10+00:00
- **Updated**: 2021-04-23 09:48:10+00:00
- **Authors**: Sheldon Fung, Xuequan Lu, Chao Zhang, Chang-Tsun Li
- **Comment**: accepted to IJCNN2021
- **Journal**: None
- **Summary**: Face deepfake detection has seen impressive results recently. Nearly all existing deep learning techniques for face deepfake detection are fully supervised and require labels during training. In this paper, we design a novel deepfake detection method via unsupervised contrastive learning. We first generate two different transformed versions of an image and feed them into two sequential sub-networks, i.e., an encoder and a projection head. The unsupervised training is achieved by maximizing the correspondence degree of the outputs of the projection head. To evaluate the detection performance of our unsupervised method, we further use the unsupervised features to train an efficient linear classification network. Extensive experiments show that our unsupervised learning method enables comparable detection performance to state-of-the-art supervised techniques, in both the intra- and inter-dataset settings. We also conduct ablation studies for our method.



### Modeling long-term interactions to enhance action recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.11520v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11520v1)
- **Published**: 2021-04-23 10:08:15+00:00
- **Updated**: 2021-04-23 10:08:15+00:00
- **Authors**: Alejandro Cartas, Petia Radeva, Mariella Dimiccoli
- **Comment**: Accepted to the 25th International Conference on Pattern Recognition
  (ICPR), 2021
- **Journal**: None
- **Summary**: In this paper, we propose a new approach to under-stand actions in egocentric videos that exploits the semantics of object interactions at both frame and temporal levels. At the frame level, we use a region-based approach that takes as input a primary region roughly corresponding to the user hands and a set of secondary regions potentially corresponding to the interacting objects and calculates the action score through a CNN formulation. This information is then fed to a Hierarchical LongShort-Term Memory Network (HLSTM) that captures temporal dependencies between actions within and across shots. Ablation studies thoroughly validate the proposed approach, showing in particular that both levels of the HLSTM architecture contribute to performance improvement. Furthermore, quantitative comparisons show that the proposed approach outperforms the state-of-the-art in terms of action recognition on standard benchmarks,without relying on motion information



### Supervised Video Summarization via Multiple Feature Sets with Parallel Attention
- **Arxiv ID**: http://arxiv.org/abs/2104.11530v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.11530v2)
- **Published**: 2021-04-23 10:46:35+00:00
- **Updated**: 2021-05-13 16:07:41+00:00
- **Authors**: Junaid Ahmed Ghauri, Sherzod Hakimov, Ralph Ewerth
- **Comment**: Accepted in IEEE International Conference on Multimedia and Expo
  (ICME) 2021 (They have copyright to publish camera ready version of this
  work)
- **Journal**: None
- **Summary**: The assignment of importance scores to particular frames or (short) segments in a video is crucial for summarization, but also a difficult task. Previous work utilizes only one source of visual features. In this paper, we suggest a novel model architecture that combines three feature sets for visual content and motion to predict importance scores. The proposed architecture utilizes an attention mechanism before fusing motion features and features representing the (static) visual content, i.e., derived from an image classification model. Comprehensive experimental evaluations are reported for two well-known datasets, SumMe and TVSum. In this context, we identify methodological issues on how previous work used these benchmark datasets, and present a fair evaluation scheme with appropriate data splits that can be used in future work. When using static and motion features with parallel attention mechanism, we improve state-of-the-art results for SumMe, while being on par with the state of the art for the other dataset.



### Recent Advances in Monocular 2D and 3D Human Pose Estimation: A Deep Learning Perspective
- **Arxiv ID**: http://arxiv.org/abs/2104.11536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11536v1)
- **Published**: 2021-04-23 11:07:07+00:00
- **Updated**: 2021-04-23 11:07:07+00:00
- **Authors**: Wu Liu, Qian Bao, Yu Sun, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Estimation of the human pose from a monocular camera has been an emerging research topic in the computer vision community with many applications. Recently, benefited from the deep learning technologies, a significant amount of research efforts have greatly advanced the monocular human pose estimation both in 2D and 3D areas. Although there have been some works to summarize the different approaches, it still remains challenging for researchers to have an in-depth view of how these approaches work. In this paper, we provide a comprehensive and holistic 2D-to-3D perspective to tackle this problem. We categorize the mainstream and milestone approaches since the year 2014 under unified frameworks. By systematically summarizing the differences and connections between these approaches, we further analyze the solutions for challenging cases, such as the lack of data, the inherent ambiguity between 2D and 3D, and the complex multi-person scenarios. We also summarize the pose representation styles, benchmarks, evaluation metrics, and the quantitative performance of popular approaches. Finally, we discuss the challenges and give deep thinking of promising directions for future research. We believe this survey will provide the readers with a deep and insightful understanding of monocular human pose estimation.



### Exploring Modality-shared Appearance Features and Modality-invariant Relation Features for Cross-modality Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2104.11539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11539v1)
- **Published**: 2021-04-23 11:14:07+00:00
- **Updated**: 2021-04-23 11:14:07+00:00
- **Authors**: Nianchang Huang, Jianan Liu, Qiang Zhang, Jungong Han
- **Comment**: 13 pages, 8 figures, submitted to TIP
- **Journal**: None
- **Summary**: Most existing cross-modality person re-identification works rely on discriminative modality-shared features for reducing cross-modality variations and intra-modality variations. Despite some initial success, such modality-shared appearance features cannot capture enough modality-invariant discriminative information due to a massive discrepancy between RGB and infrared images. To address this issue, on the top of appearance features, we further capture the modality-invariant relations among different person parts (referred to as modality-invariant relation features), which are the complement to those modality-shared appearance features and help to identify persons with similar appearances but different body shapes. To this end, a Multi-level Two-streamed Modality-shared Feature Extraction (MTMFE) sub-network is designed, where the modality-shared appearance features and modality-invariant relation features are first extracted in a shared 2D feature space and a shared 3D feature space, respectively. The two features are then fused into the final modality-shared features such that both cross-modality variations and intra-modality variations can be reduced. Besides, a novel cross-modality quadruplet loss is proposed to further reduce the cross-modality variations. Experimental results on several benchmark datasets demonstrate that our proposed method exceeds state-of-the-art algorithms by a noticeable margin.



### Middle-level Fusion for Lightweight RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.11543v3
- **DOI**: 10.1109/TIP.2022.3214092
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11543v3)
- **Published**: 2021-04-23 11:37:15+00:00
- **Updated**: 2021-06-05 09:50:02+00:00
- **Authors**: Nianchang Huang, Qiang Zhang, Jungong Han
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: Most existing lightweight RGB-D salient object detection (SOD) models are based on two-stream structure or single-stream structure. The former one first uses two sub-networks to extract unimodal features from RGB and depth images, respectively, and then fuses them for SOD. While, the latter one directly extracts multi-modal features from the input RGB-D images and then focuses on exploiting cross-level complementary information. However, two-stream structure based models inevitably require more parameters and single-stream structure based ones cannot well exploit the cross-modal complementary information since they ignore the modality difference. To address these issues, we propose to employ the middle-level fusion structure for designing lightweight RGB-D SOD model in this paper, which first employs two sub-networks to extract low- and middle-level unimodal features, respectively, and then fuses those extracted middle-level unimodal features for extracting corresponding high-level multi-modal features in the subsequent sub-network. Different from existing models, this structure can effectively exploit the cross-modal complementary information and significantly reduce the network's parameters, simultaneously. Therefore, a novel lightweight SOD model is designed, which contains a information-aware multi-modal feature fusion (IMFF) module for effectively capturing the cross-modal complementary information and a lightweight feature-level and decision-level feature fusion (LFDF) module for aggregating the feature-level and the decision-level saliency information in different stages with less parameters. Our proposed model has only 3.9M parameters and runs at 33 FPS. The experimental results on several benchmark datasets verify the effectiveness and superiority of the proposed method over some state-of-the-art methods.



### Fine-Grained Texture Identification for Reliable Product Traceability
- **Arxiv ID**: http://arxiv.org/abs/2104.11548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11548v1)
- **Published**: 2021-04-23 11:53:43+00:00
- **Updated**: 2021-04-23 11:53:43+00:00
- **Authors**: Junsong Wang, Yubo Li, Zhiyong Chang, Haitao Yue, Yonghua Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Texture exists in lots of the products, such as wood, beef and compression tea. These abundant and stochastic texture patterns are significantly different between any two products. Unlike the traditional digital ID tracking, in this paper, we propose a novel approach for product traceability, which directly uses the natural texture of the product itself as the unique identifier. A texture identification based traceability system for Pu'er compression tea is developed to demonstrate the feasibility of the proposed solution. With tea-brick images collected from manufactures and individual users, a large-scale dataset has been formed to evaluate the performance of tea-brick texture verification and searching algorithm. The texture similarity approach with local feature extraction and matching achieves the verification accuracy of 99.6% and the top-1 searching accuracy of 98.9%, respectively.



### Research on the Detection Method of Breast Cancer Deep Convolutional Neural Network Based on Computer Aid
- **Arxiv ID**: http://arxiv.org/abs/2104.11551v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.11551v1)
- **Published**: 2021-04-23 12:03:53+00:00
- **Updated**: 2021-04-23 12:03:53+00:00
- **Authors**: Mengfan Li
- **Comment**: \c{opyright}2021 IEEE
- **Journal**: None
- **Summary**: Traditional breast cancer image classification methods require manual extraction of features from medical images, which not only require professional medical knowledge, but also have problems such as time-consuming and labor-intensive and difficulty in extracting high-quality features. Therefore, the paper proposes a computer-based feature fusion Convolutional neural network breast cancer image classification and detection method. The paper pre-trains two convolutional neural networks with different structures, and then uses the convolutional neural network to automatically extract the characteristics of features, fuse the features extracted from the two structures, and finally use the classifier classifies the fused features. The experimental results show that the accuracy of this method in the classification of breast cancer image data sets is 89%, and the classification accuracy of breast cancer images is significantly improved compared with traditional methods.



### Sketch-based Normal Map Generation with Geometric Sampling
- **Arxiv ID**: http://arxiv.org/abs/2104.11554v1
- **DOI**: 10.1117/12.2590760
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2104.11554v1)
- **Published**: 2021-04-23 12:30:22+00:00
- **Updated**: 2021-04-23 12:30:22+00:00
- **Authors**: Yi He, Haoran Xie, Chao Zhang, Xi Yang, Kazunori Miyata
- **Comment**: accepted in International Workshop on Advanced Image Technology 2021,
  5 pages, 2 figures
- **Journal**: None
- **Summary**: Normal map is an important and efficient way to represent complex 3D models. A designer may benefit from the auto-generation of high quality and accurate normal maps from freehand sketches in 3D content creation. This paper proposes a deep generative model for generating normal maps from users sketch with geometric sampling. Our generative model is based on Conditional Generative Adversarial Network with the curvature-sensitive points sampling of conditional masks. This sampling process can help eliminate the ambiguity of generation results as network input. In addition, we adopted a U-Net structure discriminator to help the generator be better trained. It is verified that the proposed framework can generate more accurate normal maps.



### Weakly-supervised Multi-task Learning for Multimodal Affect Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.11560v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.11560v1)
- **Published**: 2021-04-23 12:36:19+00:00
- **Updated**: 2021-04-23 12:36:19+00:00
- **Authors**: Wenliang Dai, Samuel Cahyawijaya, Yejin Bang, Pascale Fung
- **Comment**: 13 pages, 2 figures
- **Journal**: None
- **Summary**: Multimodal affect recognition constitutes an important aspect for enhancing interpersonal relationships in human-computer interaction. However, relevant data is hard to come by and notably costly to annotate, which poses a challenging barrier to build robust multimodal affect recognition systems. Models trained on these relatively small datasets tend to overfit and the improvement gained by using complex state-of-the-art models is marginal compared to simple baselines. Meanwhile, there are many different multimodal affect recognition datasets, though each may be small. In this paper, we propose to leverage these datasets using weakly-supervised multi-task learning to improve the generalization performance on each of them. Specifically, we explore three multimodal affect recognition tasks: 1) emotion recognition; 2) sentiment analysis; and 3) sarcasm recognition. Our experimental results show that multi-tasking can benefit all these tasks, achieving an improvement up to 2.9% accuracy and 3.3% F1-score. Furthermore, our method also helps to improve the stability of model performance. In addition, our analysis suggests that weak supervision can provide a comparable contribution to strong supervision if the tasks are highly correlated.



### The Influence of Audio on Video Memorability with an Audio Gestalt Regulated Video Memorability System
- **Arxiv ID**: http://arxiv.org/abs/2104.11568v1
- **DOI**: 10.1109/CBMI50038.2021.9461903
- **Categories**: **cs.MM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.11568v1)
- **Published**: 2021-04-23 12:53:33+00:00
- **Updated**: 2021-04-23 12:53:33+00:00
- **Authors**: Lorin Sweeney, Graham Healy, Alan F. Smeaton
- **Comment**: 6 pages, 3 figures, 4 tables, paper accepted in CBMI 2021 for
  publication and oral presentation
- **Journal**: None
- **Summary**: Memories are the tethering threads that tie us to the world, and memorability is the measure of their tensile strength. The threads of memory are spun from fibres of many modalities, obscuring the contribution of a single fibre to a thread's overall tensile strength. Unfurling these fibres is the key to understanding the nature of their interaction, and how we can ultimately create more meaningful media content. In this paper, we examine the influence of audio on video recognition memorability, finding evidence to suggest that it can facilitate overall video recognition memorability rich in high-level (gestalt) audio features. We introduce a novel multimodal deep learning-based late-fusion system that uses audio gestalt to estimate the influence of a given video's audio on its overall short-term recognition memorability, and selectively leverages audio features to make a prediction accordingly. We benchmark our audio gestalt based system on the Memento10k short-term video memorability dataset, achieving top-2 state-of-the-art results.



### AttWalk: Attentive Cross-Walks for Deep Mesh Analysis
- **Arxiv ID**: http://arxiv.org/abs/2104.11571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11571v1)
- **Published**: 2021-04-23 13:02:39+00:00
- **Updated**: 2021-04-23 13:02:39+00:00
- **Authors**: Ran Ben Izhak, Alon Lahav, Ayellet Tal
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Mesh representation by random walks has been shown to benefit deep learning. Randomness is indeed a powerful concept. However, it comes with a price: some walks might wander around non-characteristic regions of the mesh, which might be harmful to shape analysis, especially when only a few walks are utilized. We propose a novel walk-attention mechanism that leverages the fact that multiple walks are used. The key idea is that the walks may provide each other with information regarding the meaningful (attentive) features of the mesh. We utilize this mutual information to extract a single descriptor of the mesh. This differs from common attention mechanisms that use attention to improve the representation of each individual descriptor. Our approach achieves SOTA results for two basic 3D shape analysis tasks: classification and retrieval. Even a handful of walks along a mesh suffice for learning.



### CapillaryNet: An Automated System to Quantify Skin Capillary Density and Red Blood Cell Velocity from Handheld Vital Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2104.11574v4
- **DOI**: 10.1016/j.artmed.2022.102287
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.11574v4)
- **Published**: 2021-04-23 13:14:47+00:00
- **Updated**: 2022-01-20 10:33:48+00:00
- **Authors**: Maged Helmy, Anastasiya Dykyy, Tuyen Trung Truong, Paulo Ferreira, Eric Jul
- **Comment**: Improved overall English text and added few more sections
- **Journal**: None
- **Summary**: Capillaries are the smallest vessels in the body responsible for delivering oxygen and nutrients to surrounding cells. Various life-threatening diseases are known to alter the density of healthy capillaries and the flow velocity of erythrocytes within the capillaries. In previous studies, capillary density and flow velocity were manually assessed by trained specialists. However, manual analysis of a standard 20-second microvascular video requires 20 minutes on average and necessitates extensive training. Thus, manual analysis has been reported to hinder the application of microvascular microscopy in a clinical environment. To address this problem, this paper presents a fully automated state-of-the-art system to quantify skin nutritive capillary density and red blood cell velocity captured by handheld-based microscopy videos. The proposed method combines the speed of traditional computer vision algorithms with the accuracy of convolutional neural networks to enable clinical capillary analysis. The results show that the proposed system fully automates capillary detection with an accuracy exceeding that of trained analysts and measures several novel microvascular parameters that had eluded quantification thus far, namely, capillary hematocrit and intracapillary flow velocity heterogeneity. The proposed end-to-end system, named CapillaryNet, can detect capillaries at $\sim$0.9 seconds per frame with $\sim$93\% accuracy. The system is currently being used as a clinical research product in a larger e-health application to analyse capillary data captured from patients suffering from COVID-19, pancreatitis, and acute heart diseases. CapillaryNet narrows the gap between the analysis of microcirculation images in a clinical environment and state-of-the-art systems.



### DeepMix: Online Auto Data Augmentation for Robust Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2104.11585v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11585v2)
- **Published**: 2021-04-23 13:37:47+00:00
- **Updated**: 2021-05-03 03:37:04+00:00
- **Authors**: Ziyi Cheng, Xuhong Ren, Felix Juefei-Xu, Wanli Xue, Qing Guo, Lei Ma, Jianjun Zhao
- **Comment**: 6 pages, 2 figures. This work has been accepted to ICME 2021
- **Journal**: None
- **Summary**: Online updating of the object model via samples from historical frames is of great importance for accurate visual object tracking. Recent works mainly focus on constructing effective and efficient updating methods while neglecting the training samples for learning discriminative object models, which is also a key part of a learning problem. In this paper, we propose the DeepMix that takes historical samples' embeddings as input and generates augmented embeddings online, enhancing the state-of-the-art online learning methods for visual object tracking. More specifically, we first propose the online data augmentation for tracking that online augments the historical samples through object-aware filtering. Then, we propose MixNet which is an offline trained network for performing online data augmentation within one-step, enhancing the tracking accuracy while preserving high speeds of the state-of-the-art online learning methods. The extensive experiments on three different tracking frameworks, i.e., DiMP, DSiam, and SiamRPN++, and three large-scale and challenging datasets, \ie, OTB-2015, LaSOT, and VOT, demonstrate the effectiveness and advantages of the proposed method.



### STRUDEL: Self-Training with Uncertainty Dependent Label Refinement across Domains
- **Arxiv ID**: http://arxiv.org/abs/2104.11596v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11596v2)
- **Published**: 2021-04-23 13:46:26+00:00
- **Updated**: 2021-09-01 09:53:02+00:00
- **Authors**: Fabian Gröger, Anne-Marie Rickmann, Christian Wachinger
- **Comment**: accepted at Machine Learning in Medical Imaging (MLMI 2021)
- **Journal**: None
- **Summary**: We propose an unsupervised domain adaptation (UDA) approach for white matter hyperintensity (WMH) segmentation, which uses Self-Training with Uncertainty DEpendent Label refinement (STRUDEL). Self-training has recently been introduced as a highly effective method for UDA, which is based on self-generated pseudo labels. However, pseudo labels can be very noisy and therefore deteriorate model performance. We propose to predict the uncertainty of pseudo labels and integrate it in the training process with an uncertainty-guided loss function to highlight labels with high certainty. STRUDEL is further improved by incorporating the segmentation output of an existing method in the pseudo label generation that showed high robustness for WMH segmentation. In our experiments, we evaluate STRUDEL with a standard U-Net and a modified network with a higher receptive field. Our results on WMH segmentation across datasets demonstrate the significant improvement of STRUDEL with respect to standard self-training.



### Region-Adaptive Deformable Network for Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2104.11599v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.11599v1)
- **Published**: 2021-04-23 13:47:20+00:00
- **Updated**: 2021-04-23 13:47:20+00:00
- **Authors**: Shuwei Shi, Qingyan Bai, Mingdeng Cao, Weihao Xia, Jiahao Wang, Yifan Chen, Yujiu Yang
- **Comment**: CVPR NTIRE Workshop 2021. The first two authors contribute equally to
  this work. Code is available at https://github.com/IIGROUP/RADN
- **Journal**: None
- **Summary**: Image quality assessment (IQA) aims to assess the perceptual quality of images. The outputs of the IQA algorithms are expected to be consistent with human subjective perception. In image restoration and enhancement tasks, images generated by generative adversarial networks (GAN) can achieve better visual performance than traditional CNN-generated images, although they have spatial shift and texture noise. Unfortunately, the existing IQA methods have unsatisfactory performance on the GAN-based distortion partially because of their low tolerance to spatial misalignment. To this end, we propose the reference-oriented deformable convolution, which can improve the performance of an IQA network on GAN-based distortion by adaptively considering this misalignment. We further propose a patch-level attention module to enhance the interaction among different patch regions, which are processed independently in previous patch-based methods. The modified residual block is also proposed by applying modifications to the classic residual block to construct a patch-region-based baseline called WResNet. Equipping this baseline with the two proposed modules, we further propose Region-Adaptive Deformable Network (RADN). The experiment results on the NTIRE 2021 Perceptual Image Quality Assessment Challenge dataset show the superior performance of RADN, and the ensemble approach won fourth place in the final testing phase of the challenge. Code is available at https://github.com/IIGROUP/RADN.



### A Picture is Worth a Collaboration: Accumulating Design Knowledge for Computer-Vision-based Hybrid Intelligence Systems
- **Arxiv ID**: http://arxiv.org/abs/2104.11600v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.11600v1)
- **Published**: 2021-04-23 13:47:57+00:00
- **Updated**: 2021-04-23 13:47:57+00:00
- **Authors**: Patrick Zschech, Jannis Walk, Kai Heinrich, Michael Vössing, Niklas Kühl
- **Comment**: Preprint accepted for archival and presentation at the 29th European
  Conference on Information Systems (ECIS 2021)
- **Journal**: None
- **Summary**: Computer vision (CV) techniques try to mimic human capabilities of visual perception to support labor-intensive and time-consuming tasks like the recognition and localization of critical objects. Nowadays, CV increasingly relies on artificial intelligence (AI) to automatically extract useful information from images that can be utilized for decision support and business process automation. However, the focus of extant research is often exclusively on technical aspects when designing AI-based CV systems while neglecting socio-technical facets, such as trust, control, and autonomy. For this purpose, we consider the design of such systems from a hybrid intelligence (HI) perspective and aim to derive prescriptive design knowledge for CV-based HI systems. We apply a reflective, practice-inspired design science approach and accumulate design knowledge from six comprehensive CV projects. As a result, we identify four design-related mechanisms (i.e., automation, signaling, modification, and collaboration) that inform our derived meta-requirements and design principles. This can serve as a basis for further socio-technical research on CV-based HI systems.



### Co-training for Deep Object Detection: Comparing Single-modal and Multi-modal Approaches
- **Arxiv ID**: http://arxiv.org/abs/2104.11619v1
- **DOI**: 10.3390/s21093185
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.11619v1)
- **Published**: 2021-04-23 14:13:59+00:00
- **Updated**: 2021-04-23 14:13:59+00:00
- **Authors**: Jose L. Gómez, Gabriel Villalonga, Antonio M. López
- **Comment**: None
- **Journal**: special issue of Sensors (ISSN 1424-8220) "Feature Papers in
  Physical Sensors Section 2020"
- **Summary**: Top-performing computer vision models are powered by convolutional neural networks (CNNs). Training an accurate CNN highly depends on both the raw sensor data and their associated ground truth (GT). Collecting such GT is usually done through human labeling, which is time-consuming and does not scale as we wish. This data labeling bottleneck may be intensified due to domain shifts among image sensors, which could force per-sensor data labeling. In this paper, we focus on the use of co-training, a semi-supervised learning (SSL) method, for obtaining self-labeled object bounding boxes (BBs), i.e., the GT to train deep object detectors. In particular, we assess the goodness of multi-modal co-training by relying on two different views of an image, namely, appearance (RGB) and estimated depth (D). Moreover, we compare appearance-based single-modal co-training with multi-modal. Our results suggest that in a standard SSL setting (no domain shift, a few human-labeled data) and under virtual-to-real domain shift (many virtual-world labeled data, no human-labeled data) multi-modal co-training outperforms single-modal. In the latter case, by performing GAN-based domain translation both co-training modalities are on pair; at least, when using an off-the-shelf depth estimation model not specifically trained on the translated images.



### GuideBP: Guiding Backpropagation Through Weaker Pathways of Parallel Logits
- **Arxiv ID**: http://arxiv.org/abs/2104.11620v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.11620v1)
- **Published**: 2021-04-23 14:14:00+00:00
- **Updated**: 2021-04-23 14:14:00+00:00
- **Authors**: Bodhisatwa Mandal, Swarnendu Ghosh, Teresa Gonçalves, Paulo Quaresma, Mita Nasipuri, Nibaran Das
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks often generate multiple logits and use simple techniques like addition or averaging for loss computation. But this allows gradients to be distributed equally among all paths. The proposed approach guides the gradients of backpropagation along weakest concept representations. A weakness scores defines the class specific performance of individual pathways which is then used to create a logit that would guide gradients along the weakest pathways. The proposed approach has been shown to perform better than traditional column merging techniques and can be used in several application scenarios. Not only can the proposed model be used as an efficient technique for training multiple instances of a model parallely, but also CNNs with multiple output branches have been shown to perform better with the proposed upgrade. Various experiments establish the flexibility of the learning technique which is simple yet effective in various multi-objective scenarios both empirically and statistically.



### Detecting and Matching Related Objects with One Proposal Multiple Predictions
- **Arxiv ID**: http://arxiv.org/abs/2104.12574v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12574v1)
- **Published**: 2021-04-23 14:37:10+00:00
- **Updated**: 2021-04-23 14:37:10+00:00
- **Authors**: Yang Liu, Luiz G. Hafemann, Michael Jamieson, Mehrsan Javan
- **Comment**: CVPR workshop 2021
- **Journal**: None
- **Summary**: Tracking players in sports videos is commonly done in a tracking-by-detection framework, first detecting players in each frame, and then performing association over time. While for some sports tracking players is sufficient for game analysis, sports like hockey, tennis and polo may require additional detections, that include the object the player is holding (e.g. racket, stick). The baseline solution for this problem involves detecting these objects as separate classes, and matching them to player detections based on the intersection over union (IoU). This approach, however, leads to poor matching performance in crowded situations, as it does not model the relationship between players and objects. In this paper, we propose a simple yet efficient way to detect and match players and related objects at once without extra cost, by considering an implicit association for prediction of multiple objects through the same proposal box. We evaluate the method on a dataset of broadcast ice hockey videos, and also a new public dataset we introduce called COCO +Torso. On the ice hockey dataset, the proposed method boosts matching performance from 57.1% to 81.4%, while also improving the meanAP of player+stick detections from 68.4% to 88.3%. On the COCO +Torso dataset, we see matching improving from 47.9% to 65.2%. The COCO +Torso dataset, code and pre-trained models will be released at https://github.com/foreverYoungGitHub/detect-and-match-related-objects.



### Online recognition of unsegmented actions with hierarchical SOM architecture
- **Arxiv ID**: http://arxiv.org/abs/2104.11637v1
- **DOI**: 10.1007/s10339-020-00986-4
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.11637v1)
- **Published**: 2021-04-23 14:41:46+00:00
- **Updated**: 2021-04-23 14:41:46+00:00
- **Authors**: Zahra Gharaee
- **Comment**: None
- **Journal**: Cogn Process 22, 77-91 (2021)
- **Summary**: Automatic recognition of an online series of unsegmented actions requires a method for segmentation that determines when an action starts and when it ends. In this paper, a novel approach for recognizing unsegmented actions in online test experiments is proposed. The method uses self-organizing neural networks to build a three-layer cognitive architecture. The unique features of an action sequence are represented as a series of elicited key activations by the first-layer self-organizing map. An average length of a key activation vector is calculated for all action sequences in a training set and adjusted in learning trials to generate input patterns to the second-layer self-organizing map. The pattern vectors are clustered in the second layer, and the clusters are then labeled by an action identity in the third layer neural network. The experiment results show that although the performance drops slightly in online experiments compared to the offline tests, the ability of the proposed architecture to deal with the unsegmented action sequences as well as the online performance makes the system more plausible and practical in real-case scenarios.



### MULTICAST: MULTI Confirmation-level Alarm SysTem based on CNN and LSTM to mitigate false alarms for handgun detection in video-surveillance
- **Arxiv ID**: http://arxiv.org/abs/2104.11653v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11653v2)
- **Published**: 2021-04-23 15:07:58+00:00
- **Updated**: 2021-05-03 07:14:19+00:00
- **Authors**: Roberto Olmos, Siham Tabik, Francisco Perez-Hernandez, Alberto Lamas, Francisco Herrera
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the constant advances in computer vision, integrating modern single-image detectors in real-time handgun alarm systems in video-surveillance is still debatable. Using such detectors still implies a high number of false alarms and false negatives. In this context, most existent studies select one of the latest single-image detectors and train it on a better dataset or use some pre-processing, post-processing or data-fusion approach to further reduce false alarms. However, none of these works tried to exploit the temporal information present in the videos to mitigate false detections. This paper presents a new system, called MULTI Confirmation-level Alarm SysTem based on Convolutional Neural Networks (CNN) and Long Short Term Memory networks (LSTM) (MULTICAST), that leverages not only the spacial information but also the temporal information existent in the videos for a more reliable handgun detection. MULTICAST consists of three stages, i) a handgun detection stage, ii) a CNN-based spacial confirmation stage and iii) LSTM-based temporal confirmation stage. The temporal confirmation stage uses the positions of the detected handgun in previous instants to predict its trajectory in the next frame. Our experiments show that MULTICAST reduces by 80% the number of false alarms with respect to Faster R-CNN based-single-image detector, which makes it more useful in providing more effective and rapid security responses.



### Favelas 4D: Scalable methods for morphology analysis of informal settlements using terrestrial laser scanning data
- **Arxiv ID**: http://arxiv.org/abs/2105.03235v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.03235v1)
- **Published**: 2021-04-23 15:32:59+00:00
- **Updated**: 2021-04-23 15:32:59+00:00
- **Authors**: Arianna Salazar Miranda, Guangyu Du, Claire Gorman, Fabio Duarte, Washington Fajardo, Carlo Ratti
- **Comment**: None
- **Journal**: None
- **Summary**: One billion people live in informal settlements worldwide. The complex and multilayered spaces that characterize this unplanned form of urbanization pose a challenge to traditional approaches to mapping and morphological analysis. This study proposes a methodology to study the morphological properties of informal settlements based on terrestrial LiDAR (Light Detection and Ranging) data collected in Rocinha, the largest favela in Rio de Janeiro, Brazil. Our analysis operates at two resolutions, including a \emph{global} analysis focused on comparing different streets of the favela to one another, and a \emph{local} analysis unpacking the variation of morphological metrics within streets. We show that our methodology reveals meaningful differences and commonalities both in terms of the global morphological characteristics across streets and their local distributions. Finally, we create morphological maps at high spatial resolution from LiDAR data, which can inform urban planning assessments of concerns related to crowding, structural safety, air quality, and accessibility in the favela. The methods for this study are automated and can be easily scaled to analyze entire informal settlements, leveraging the increasing availability of inexpensive LiDAR scanners on portable devices such as cellphones.



### Skeletor: Skeletal Transformers for Robust Body-Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2104.11712v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11712v1)
- **Published**: 2021-04-23 16:56:42+00:00
- **Updated**: 2021-04-23 16:56:42+00:00
- **Authors**: Tao Jiang, Necati Cihan Camgoz, Richard Bowden
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting 3D human pose from a single monoscopic video can be highly challenging due to factors such as low resolution, motion blur and occlusion, in addition to the fundamental ambiguity in estimating 3D from 2D. Approaches that directly regress the 3D pose from independent images can be particularly susceptible to these factors and result in jitter, noise and/or inconsistencies in skeletal estimation. Much of which can be overcome if the temporal evolution of the scene and skeleton are taken into account. However, rather than tracking body parts and trying to temporally smooth them, we propose a novel transformer based network that can learn a distribution over both pose and motion in an unsupervised fashion. We call our approach Skeletor. Skeletor overcomes inaccuracies in detection and corrects partial or entire skeleton corruption. Skeletor uses strong priors learn from on 25 million frames to correct skeleton sequences smoothly and consistently. Skeletor can achieve this as it implicitly learns the spatio-temporal context of human motion via a transformer based neural network. Extensive experiments show that Skeletor achieves improved performance on 3D human pose estimation and further provides benefits for downstream tasks such as sign language translation.



### Safe Fakes: Evaluating Face Anonymizers for Face Detectors
- **Arxiv ID**: http://arxiv.org/abs/2104.11721v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2104.11721v1)
- **Published**: 2021-04-23 17:16:23+00:00
- **Updated**: 2021-04-23 17:16:23+00:00
- **Authors**: Sander R. Klomp, Matthew van Rijn, Rob G. J. Wijnhoven, Cees G. M. Snoek, Peter H. N. de With
- **Comment**: None
- **Journal**: None
- **Summary**: Since the introduction of the GDPR and CCPA legislation, both public and private facial image datasets are increasingly scrutinized. Several datasets have been taken offline completely and some have been anonymized. However, it is unclear how anonymization impacts face detection performance. To our knowledge, this paper presents the first empirical study on the effect of image anonymization on supervised training of face detectors. We compare conventional face anonymizers with three state-of-the-art Generative Adversarial Network-based (GAN) methods, by training an off-the-shelf face detector on anonymized data. Our experiments investigate the suitability of anonymization methods for maintaining face detector performance, the effect of detectors overtraining on anonymization artefacts, dataset size for training an anonymizer, and the effect of training time of anonymization GANs. A final experiment investigates the correlation between common GAN evaluation metrics and the performance of a trained face detector. Although all tested anonymization methods lower the performance of trained face detectors, faces anonymized using GANs cause far smaller performance degradation than conventional methods. As the most important finding, the best-performing GAN, DeepPrivacy, removes identifiable faces for a face detector trained on anonymized data, resulting in a modest decrease from 91.0 to 88.3 mAP. In the last few years, there have been rapid improvements in realism of GAN-generated faces. We expect that further progression in GAN research will allow the use of Deep Fake technology for privacy-preserving Safe Fakes, without any performance degradation for training face detectors.



### VidTr: Video Transformer Without Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2104.11746v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11746v2)
- **Published**: 2021-04-23 17:59:01+00:00
- **Updated**: 2021-10-15 23:41:28+00:00
- **Authors**: Yanyi Zhang, Xinyu Li, Chunhui Liu, Bing Shuai, Yi Zhu, Biagio Brattoli, Hao Chen, Ivan Marsic, Joseph Tighe
- **Comment**: ICCV 2021 Accepted
- **Journal**: None
- **Summary**: We introduce Video Transformer (VidTr) with separable-attention for video classification. Comparing with commonly used 3D networks, VidTr is able to aggregate spatio-temporal information via stacked attentions and provide better performance with higher efficiency. We first introduce the vanilla video transformer and show that transformer module is able to perform spatio-temporal modeling from raw pixels, but with heavy memory usage. We then present VidTr which reduces the memory cost by 3.3$\times$ while keeping the same performance. To further optimize the model, we propose the standard deviation based topK pooling for attention ($pool_{topK\_std}$), which reduces the computation by dropping non-informative features along temporal dimension. VidTr achieves state-of-the-art performance on five commonly used datasets with lower computational requirement, showing both the efficiency and effectiveness of our design. Finally, error analysis and visualization show that VidTr is especially good at predicting actions that require long-term temporal reasoning.



### Learnable Online Graph Representations for 3D Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2104.11747v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.11747v1)
- **Published**: 2021-04-23 17:59:28+00:00
- **Updated**: 2021-04-23 17:59:28+00:00
- **Authors**: Jan-Nico Zaech, Dengxin Dai, Alexander Liniger, Martin Danelljan, Luc Van Gool
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Tracking of objects in 3D is a fundamental task in computer vision that finds use in a wide range of applications such as autonomous driving, robotics or augmented reality. Most recent approaches for 3D multi object tracking (MOT) from LIDAR use object dynamics together with a set of handcrafted features to match detections of objects. However, manually designing such features and heuristics is cumbersome and often leads to suboptimal performance. In this work, we instead strive towards a unified and learning based approach to the 3D MOT problem. We design a graph structure to jointly process detection and track states in an online manner. To this end, we employ a Neural Message Passing network for data association that is fully trainable. Our approach provides a natural way for track initialization and handling of false positive detections, while significantly improving track stability. We show the merit of the proposed approach on the publicly available nuScenes dataset by achieving state-of-the-art performance of 65.6% AMOTA and 58% fewer ID-switches.



### UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments
- **Arxiv ID**: http://arxiv.org/abs/2104.11776v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.11776v1)
- **Published**: 2021-04-23 18:45:42+00:00
- **Updated**: 2021-04-23 18:45:42+00:00
- **Authors**: Pablo Martinez-Gonzalez, Sergiu Oprea, John Alejandro Castro-Vargas, Alberto Garcia-Garcia, Sergio Orts-Escolano, Jose Garcia-Rodriguez, Markus Vincze
- **Comment**: Accepted at International Joint Conference on Neural Networks (IJCNN)
  2021
- **Journal**: None
- **Summary**: Synthetic data generation has become essential in last years for feeding data-driven algorithms, which surpassed traditional techniques performance in almost every computer vision problem. Gathering and labelling the amount of data needed for these data-hungry models in the real world may become unfeasible and error-prone, while synthetic data give us the possibility of generating huge amounts of data with pixel-perfect annotations. However, most synthetic datasets lack from enough realism in their rendered images. In that context UnrealROX generation tool was presented in 2019, allowing to generate highly realistic data, at high resolutions and framerates, with an efficient pipeline based on Unreal Engine, a cutting-edge videogame engine. UnrealROX enabled robotic vision researchers to generate realistic and visually plausible data with full ground truth for a wide variety of problems such as class and instance semantic segmentation, object detection, depth estimation, visual grasping, and navigation. Nevertheless, its workflow was very tied to generate image sequences from a robotic on-board camera, making hard to generate data for other purposes. In this work, we present UnrealROX+, an improved version of UnrealROX where its decoupled and easy-to-use data acquisition system allows to quickly design and generate data in a much more flexible and customizable way. Moreover, it is packaged as an Unreal plug-in, which makes it more comfortable to use with already existing Unreal projects, and it also includes new features such as generating albedo or a Python API for interacting with the virtual environment from Deep Learning frameworks.



### On the Role of Sensor Fusion for Object Detection in Future Vehicular Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.11785v1
- **DOI**: None
- **Categories**: **cs.NI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.11785v1)
- **Published**: 2021-04-23 18:58:37+00:00
- **Updated**: 2021-04-23 18:58:37+00:00
- **Authors**: Valentina Rossi, Paolo Testolina, Marco Giordani, Michele Zorzi
- **Comment**: This paper has been accepted for presentation at the Joint European
  Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit). 6
  pages, 6 figures, 1 table
- **Journal**: None
- **Summary**: Fully autonomous driving systems require fast detection and recognition of sensitive objects in the environment. In this context, intelligent vehicles should share their sensor data with computing platforms and/or other vehicles, to detect objects beyond their own sensors' fields of view. However, the resulting huge volumes of data to be exchanged can be challenging to handle for standard communication technologies. In this paper, we evaluate how using a combination of different sensors affects the detection of the environment in which the vehicles move and operate. The final objective is to identify the optimal setup that would minimize the amount of data to be distributed over the channel, with negligible degradation in terms of object detection accuracy. To this aim, we extend an already available object detection algorithm so that it can consider, as an input, camera images, LiDAR point clouds, or a combination of the two, and compare the accuracy performance of the different approaches using two realistic datasets. Our results show that, although sensor fusion always achieves more accurate detections, LiDAR only inputs can obtain similar results for large objects while mitigating the burden on the channel.



### Ensembles of GANs for synthetic training data generation
- **Arxiv ID**: http://arxiv.org/abs/2104.11797v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.11797v1)
- **Published**: 2021-04-23 19:38:48+00:00
- **Updated**: 2021-04-23 19:38:48+00:00
- **Authors**: Gabriel Eilertsen, Apostolia Tsirikoglou, Claes Lundström, Jonas Unger
- **Comment**: ICLR 2021 workshop on Synthetic Data Generation: Quality, Privacy,
  Bias
- **Journal**: None
- **Summary**: Insufficient training data is a major bottleneck for most deep learning practices, not least in medical imaging where data is difficult to collect and publicly available datasets are scarce due to ethics and privacy. This work investigates the use of synthetic images, created by generative adversarial networks (GANs), as the only source of training data. We demonstrate that for this application, it is of great importance to make use of multiple GANs to improve the diversity of the generated data, i.e. to sufficiently cover the data distribution. While a single GAN can generate seemingly diverse image content, training on this data in most cases lead to severe over-fitting. We test the impact of ensembled GANs on synthetic 2D data as well as common image datasets (SVHN and CIFAR-10), and using both DCGANs and progressively growing GANs. As a specific use case, we focus on synthesizing digital pathology patches to provide anonymized training data.



### Playing Lottery Tickets with Vision and Language
- **Arxiv ID**: http://arxiv.org/abs/2104.11832v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.11832v2)
- **Published**: 2021-04-23 22:24:33+00:00
- **Updated**: 2021-12-14 23:04:45+00:00
- **Authors**: Zhe Gan, Yen-Chun Chen, Linjie Li, Tianlong Chen, Yu Cheng, Shuohang Wang, Jingjing Liu, Lijuan Wang, Zicheng Liu
- **Comment**: Accepted to AAAI 2022
- **Journal**: None
- **Summary**: Large-scale pre-training has recently revolutionized vision-and-language (VL) research. Models such as LXMERT and UNITER have significantly lifted the state of the art over a wide range of VL tasks. However, the large number of parameters in such models hinders their application in practice. In parallel, work on the lottery ticket hypothesis (LTH) has shown that deep neural networks contain small matching subnetworks that can achieve on par or even better performance than the dense networks when trained in isolation. In this work, we perform the first empirical study to assess whether such trainable subnetworks also exist in pre-trained VL models. We use UNITER as the main testbed (also test on LXMERT and ViLT), and consolidate 7 representative VL tasks for experiments, including visual question answering, visual commonsense reasoning, visual entailment, referring expression comprehension, image-text retrieval, GQA, and NLVR$^2$. Through comprehensive analysis, we summarize our main findings as follows. ($i$) It is difficult to find subnetworks that strictly match the performance of the full model. However, we can find "relaxed" winning tickets at 50%-70% sparsity that maintain 99% of the full accuracy. ($ii$) Subnetworks found by task-specific pruning transfer reasonably well to the other tasks, while those found on the pre-training tasks at 60%/70% sparsity transfer universally, matching 98%/96% of the full accuracy on average over all the tasks. ($iii$) Besides UNITER, other models such as LXMERT and ViLT can also play lottery tickets. However, the highest sparsity we can achieve for ViLT is far lower than LXMERT and UNITER (30% vs. 70%). ($iv$) LTH also remains relevant when using other training methods (e.g., adversarial training).



