# Arxiv Papers in cs.CV on 2021-04-09
### Self-Weighted Ensemble Method to Adjust the Influence of Individual Models based on Reliability
- **Arxiv ID**: http://arxiv.org/abs/2104.04120v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04120v1)
- **Published**: 2021-04-09 00:20:01+00:00
- **Updated**: 2021-04-09 00:20:01+00:00
- **Authors**: YeongHyeon Park, JoonSung Lee, Wonseok Park
- **Comment**: 3 pages, 2 figures, 4 tables A preliminary version of the paper was
  presented at the 33rd Workshop on Image Processing and Image Understanding
- **Journal**: None
- **Summary**: Image classification technology and performance based on Deep Learning have already achieved high standards. Nevertheless, many efforts have conducted to improve the stability of classification via ensembling. However, the existing ensemble method has a limitation in that it requires extra effort including time consumption to find the weight for each model output. In this paper, we propose a simple but improved ensemble method, naming with Self-Weighted Ensemble (SWE), that places the weight of each model via its verification reliability. The proposed ensemble method, SWE, reduces overall efforts for constructing a classification system with varied classifiers. The performance using SWE is 0.033% higher than the conventional ensemble method. Also, the percent of performance superiority to the previous model is up to 73.333% (ratio of 8:22).



### Spatially-Varying Outdoor Lighting Estimation from Intrinsics
- **Arxiv ID**: http://arxiv.org/abs/2104.04160v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04160v2)
- **Published**: 2021-04-09 02:28:54+00:00
- **Updated**: 2021-04-28 05:22:29+00:00
- **Authors**: Yongjie Zhu, Yinda Zhang, Si Li, Boxin Shi
- **Comment**: None
- **Journal**: None
- **Summary**: We present SOLID-Net, a neural network for spatially-varying outdoor lighting estimation from a single outdoor image for any 2D pixel location. Previous work has used a unified sky environment map to represent outdoor lighting. Instead, we generate spatially-varying local lighting environment maps by combining global sky environment map with warped image information according to geometric information estimated from intrinsics. As no outdoor dataset with image and local lighting ground truth is readily available, we introduce the SOLID-Img dataset with physically-based rendered images and their corresponding intrinsic and lighting information. We train a deep neural network to regress intrinsic cues with physically-based constraints and use them to conduct global and local lightings estimation. Experiments on both synthetic and real datasets show that SOLID-Net significantly outperforms previous methods.



### eGAN: Unsupervised approach to class imbalance using transfer learning
- **Arxiv ID**: http://arxiv.org/abs/2104.04162v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.04162v2)
- **Published**: 2021-04-09 02:37:55+00:00
- **Updated**: 2021-08-18 18:39:26+00:00
- **Authors**: Ademola Okerinde, Lior Shamir, William Hsu, Tom Theis, Nasik Nafi
- **Comment**: 2021 The 19th International Conference on Computer Analysis of Images
  and Patterns (CAIP)
- **Journal**: None
- **Summary**: Class imbalance is an inherent problem in many machine learning classification tasks. This often leads to trained models that are unusable for any practical purpose. In this study we explore an unsupervised approach to address these imbalances by leveraging transfer learning from pre-trained image classification models to encoder-based Generative Adversarial Network (eGAN). To the best of our knowledge, this is the first work to tackle this problem using GAN without needing to augment with synthesized fake images.   In the proposed approach we use the discriminator network to output a negative or positive score. We classify as minority, test samples with negative scores and as majority those with positive scores. Our approach eliminates epistemic uncertainty in model predictions, as the P(minority) + P(majority) need not sum up to 1. The impact of transfer learning and combinations of different pre-trained image classification models at the generator and discriminator is also explored. Best result of 0.69 F1-score was obtained on CIFAR-10 classification task with imbalance ratio of 1:2500.   Our approach also provides a mechanism of thresholding the specificity or sensitivity of our machine learning system. Keywords: Class imbalance, Transfer Learning, GAN, nash equilibrium



### Combined Depth Space based Architecture Search For Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2104.04163v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04163v1)
- **Published**: 2021-04-09 02:40:01+00:00
- **Updated**: 2021-04-09 02:40:01+00:00
- **Authors**: Hanjun Li, Gaojie Wu, Wei-Shi Zheng
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Most works on person re-identification (ReID) take advantage of large backbone networks such as ResNet, which are designed for image classification instead of ReID, for feature extraction. However, these backbones may not be computationally efficient or the most suitable architectures for ReID. In this work, we aim to design a lightweight and suitable network for ReID. We propose a novel search space called Combined Depth Space (CDS), based on which we search for an efficient network architecture, which we call CDNet, via a differentiable architecture search algorithm. Through the use of the combined basic building blocks in CDS, CDNet tends to focus on combined pattern information that is typically found in images of pedestrians. We then propose a low-cost search strategy named the Top-k Sample Search strategy to make full use of the search space and avoid trapping in local optimal result. Furthermore, an effective Fine-grained Balance Neck (FBLNeck), which is removable at the inference time, is presented to balance the effects of triplet loss and softmax loss during the training process. Extensive experiments show that our CDNet (~1.8M parameters) has comparable performance with state-of-the-art lightweight networks.



### The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2104.04167v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.04167v2)
- **Published**: 2021-04-09 02:44:39+00:00
- **Updated**: 2021-08-25 08:54:25+00:00
- **Authors**: Yuankai Qi, Zizheng Pan, Yicong Hong, Ming-Hsuan Yang, Anton van den Hengel, Qi Wu
- **Comment**: Original title: Know What and Know Where: An Object-and-Room Informed
  Sequential BERT for Indoor Vision-Language Navigation
- **Journal**: None
- **Summary**: Vision-and-Language Navigation (VLN) requires an agent to find a path to a remote location on the basis of natural-language instructions and a set of photo-realistic panoramas. Most existing methods take the words in the instructions and the discrete views of each panorama as the minimal unit of encoding. However, this requires a model to match different nouns (e.g., TV, table) against the same input view feature. In this work, we propose an object-informed sequential BERT to encode visual perceptions and linguistic instructions at the same fine-grained level, namely objects and words. Our sequential BERT also enables the visual-textual clues to be interpreted in light of the temporal context, which is crucial to multi-round VLN tasks. Additionally, we enable the model to identify the relative direction (e.g., left/right/front/back) of each navigable location and the room type (e.g., bedroom, kitchen) of its current and final navigation goal, as such information is widely mentioned in instructions implying the desired next and final locations. We thus enable the model to know-where the objects lie in the images, and to know-where they stand in the scene. Extensive experiments demonstrate the effectiveness compared against several state-of-the-art methods on three indoor VLN tasks: REVERIE, NDH, and R2R. Project repository: https://github.com/YuankaiQi/ORIST



### Stereo Matching by Self-supervision of Multiscopic Vision
- **Arxiv ID**: http://arxiv.org/abs/2104.04170v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04170v2)
- **Published**: 2021-04-09 02:58:59+00:00
- **Updated**: 2021-08-16 12:54:57+00:00
- **Authors**: Weihao Yuan, Yazhan Zhang, Bingkun Wu, Siyu Zhu, Ping Tan, Michael Yu Wang, Qifeng Chen
- **Comment**: 2021 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)
- **Journal**: None
- **Summary**: Self-supervised learning for depth estimation possesses several advantages over supervised learning. The benefits of no need for ground-truth depth, online fine-tuning, and better generalization with unlimited data attract researchers to seek self-supervised solutions. In this work, we propose a new self-supervised framework for stereo matching utilizing multiple images captured at aligned camera positions. A cross photometric loss, an uncertainty-aware mutual-supervision loss, and a new smoothness loss are introduced to optimize the network in learning disparity maps end-to-end without ground-truth depth information. To train this framework, we build a new multiscopic dataset consisting of synthetic images rendered by 3D engines and real images captured by real cameras. After being trained with only the synthetic images, our network can perform well in unseen outdoor scenes. Our experiment shows that our model obtains better disparity maps than previous unsupervised methods on the KITTI dataset and is comparable to supervised methods when generalized to unseen data. Our source code and dataset are available at https://sites.google.com/view/multiscopic.



### X2CT-FLOW: Maximum a posteriori reconstruction using a progressive flow-based deep generative model for ultra sparse-view computed tomography in ultra low-dose protocols
- **Arxiv ID**: http://arxiv.org/abs/2104.04179v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.04179v2)
- **Published**: 2021-04-09 03:30:27+00:00
- **Updated**: 2021-10-01 02:38:41+00:00
- **Authors**: Hisaichi Shibata, Shouhei Hanaoka, Yukihiro Nomura, Takahiro Nakao, Tomomi Takenaga, Naoto Hayashi, Osamu Abe
- **Comment**: None
- **Journal**: None
- **Summary**: Ultra sparse-view computed tomography (CT) algorithms can reduce radiation exposure of patients, but those algorithms lack an explicit cycle consistency loss minimization and an explicit log-likelihood maximization in testing. Here, we propose X2CT-FLOW for the maximum a posteriori (MAP) reconstruction of a three-dimensional (3D) chest CT image from a single or a few two-dimensional (2D) projection images using a progressive flow-based deep generative model, especially for ultra low-dose protocols. The MAP reconstruction can simultaneously optimize the cycle consistency loss and the log-likelihood. The proposed algorithm is built upon a newly developed progressive flow-based deep generative model, which is featured with exact log-likelihood estimation, efficient sampling, and progressive learning. We applied X2CT-FLOW to reconstruction of 3D chest CT images from biplanar projection images without noise contamination (assuming a standard-dose protocol) and with strong noise contamination (assuming an ultra low-dose protocol). With the standard-dose protocol, our images reconstructed from 2D projected images and 3D ground-truth CT images showed good agreement in terms of structural similarity (SSIM, 0.7675 on average), peak signal-to-noise ratio (PSNR, 25.89 dB on average), mean absolute error (MAE, 0.02364 on average), and normalized root mean square error (NRMSE, 0.05731 on average). Moreover, with the ultra low-dose protocol, our images reconstructed from 2D projected images and the 3D ground-truth CT images also showed good agreement in terms of SSIM (0.7008 on average), PSNR (23.58 dB on average), MAE (0.02991 on average), and NRMSE (0.07349 on average).



### FIBER: Fill-in-the-Blanks as a Challenging Video Understanding Evaluation Framework
- **Arxiv ID**: http://arxiv.org/abs/2104.04182v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04182v3)
- **Published**: 2021-04-09 04:00:10+00:00
- **Updated**: 2022-03-22 21:24:09+00:00
- **Authors**: Santiago Castro, Ruoyao Wang, Pingxuan Huang, Ian Stewart, Oana Ignat, Nan Liu, Jonathan C. Stroud, Rada Mihalcea
- **Comment**: Accepted at ACL 2022 Main conference. Camera-ready version
- **Journal**: None
- **Summary**: We propose fill-in-the-blanks as a video understanding evaluation framework and introduce FIBER -- a novel dataset consisting of 28,000 videos and descriptions in support of this evaluation framework. The fill-in-the-blanks setting tests a model's understanding of a video by requiring it to predict a masked noun phrase in the caption of the video, given the video and the surrounding text. The FIBER benchmark does not share the weaknesses of the current state-of-the-art language-informed video understanding tasks, namely: (1) video question answering using multiple-choice questions, where models perform relatively well because they exploit linguistic biases in the task formulation, thus making our framework challenging for the current state-of-the-art systems to solve; and (2) video captioning, which relies on an open-ended evaluation framework that is often inaccurate because system answers may be perceived as incorrect if they differ in form from the ground truth. The FIBER dataset and our code are available at https://lit.eecs.umich.edu/fiber/.



### Robust Training of Social Media Image Classification Models for Rapid Disaster Response
- **Arxiv ID**: http://arxiv.org/abs/2104.04184v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG, cs.SI, 68T50, I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2104.04184v2)
- **Published**: 2021-04-09 04:30:04+00:00
- **Updated**: 2021-07-19 12:56:08+00:00
- **Authors**: Firoj Alam, Tanvirul Alam, Muhammad Imran, Ferda Ofli
- **Comment**: Social media images, Image Classification, Natural disasters, Crisis
  Informatics, Deep learning. Extended version of arXiv:2011.08916. arXiv admin
  note: substantial text overlap with arXiv:2011.08916
- **Journal**: None
- **Summary**: Images shared on social media help crisis managers gain situational awareness and assess incurred damages, among other response tasks. As the volume and velocity of such content are typically high, real-time image classification has become an urgent need for a faster disaster response. Recent advances in computer vision and deep neural networks have enabled the development of models for real-time image classification for a number of tasks, including detecting crisis incidents, filtering irrelevant images, classifying images into specific humanitarian categories, and assessing the severity of the damage. To develop robust real-time models, it is necessary to understand the capability of the publicly available pre-trained models for these tasks, which remains to be under-explored in the crisis informatics literature. In this study, we address such limitations by investigating ten different network architectures for four different tasks using the largest publicly available datasets for these tasks. We also explore various data augmentation strategies, semi-supervised techniques, and a multitask learning setup. In our extensive experiments, we achieve promising results.



### SI-Score: An image dataset for fine-grained analysis of robustness to object location, rotation and size
- **Arxiv ID**: http://arxiv.org/abs/2104.04191v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.04191v1)
- **Published**: 2021-04-09 05:00:49+00:00
- **Updated**: 2021-04-09 05:00:49+00:00
- **Authors**: Jessica Yung, Rob Romijnders, Alexander Kolesnikov, Lucas Beyer, Josip Djolonga, Neil Houlsby, Sylvain Gelly, Mario Lucic, Xiaohua Zhai
- **Comment**: 4 pages (10 pages including references and appendix), 10 figures.
  Accepted at the ICLR 2021 RobustML Workshop. arXiv admin note: text overlap
  with arXiv:2007.08558
- **Journal**: None
- **Summary**: Before deploying machine learning models it is critical to assess their robustness. In the context of deep neural networks for image understanding, changing the object location, rotation and size may affect the predictions in non-trivial ways. In this work we perform a fine-grained analysis of robustness with respect to these factors of variation using SI-Score, a synthetic dataset. In particular, we investigate ResNets, Vision Transformers and CLIP, and identify interesting qualitative differences between these.



### Reinforced Attention for Few-Shot Learning and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2104.04192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04192v1)
- **Published**: 2021-04-09 05:01:15+00:00
- **Updated**: 2021-04-09 05:01:15+00:00
- **Authors**: Jie Hong, Pengfei Fang, Weihao Li, Tong Zhang, Christian Simon, Mehrtash Harandi, Lars Petersson
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning aims to correctly recognize query samples from unseen classes given a limited number of support samples, often by relying on global embeddings of images. In this paper, we propose to equip the backbone network with an attention agent, which is trained by reinforcement learning. The policy gradient algorithm is employed to train the agent towards adaptively localizing the representative regions on feature maps over time. We further design a reward function based on the prediction of the held-out data, thus helping the attention mechanism to generalize better across the unseen classes. The extensive experiments show, with the help of the reinforced attention, that our embedding network has the capability to progressively generate a more discriminative representation in few-shot learning. Moreover, experiments on the task of image classification also show the effectiveness of the proposed design.



### TaylorMade VDD: Domain-adaptive Visual Defect Detector for High-mix Low-volume Production of Non-convex Cylindrical Metal Objects
- **Arxiv ID**: http://arxiv.org/abs/2104.04203v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04203v1)
- **Published**: 2021-04-09 05:56:27+00:00
- **Updated**: 2021-04-09 05:56:27+00:00
- **Authors**: Kyosuke Tashiro, Koji Takeda, Kanji Tanaka, Tomoe Hiroki
- **Comment**: 6 pages, 6 figures, technical report
- **Journal**: None
- **Summary**: Visual defect detection (VDD) for high-mix low-volume production of non-convex metal objects, such as high-pressure cylindrical piping joint parts (VDD-HPPPs), is challenging because subtle difference in domain (e.g., metal objects, imaging device, viewpoints, lighting) significantly affects the specular reflection characteristics of individual metal object types. In this paper, we address this issue by introducing a tailor-made VDD framework that can be automatically adapted to a new domain. Specifically, we formulate this adaptation task as the problem of network architecture search (NAS) on a deep object-detection network, in which the network architecture is searched via reinforcement learning. We demonstrate the effectiveness of the proposed framework using the VDD-HPPPs task as a factory case study. Experimental results show that the proposed method achieved higher burr detection accuracy compared with the baseline method for data with different training/test domains for the non-convex HPPPs, which are particularly affected by domain shifts.



### Image Segmentation, Compression and Reconstruction from Edge Distribution Estimation with Random Field and Random Cluster Theories
- **Arxiv ID**: http://arxiv.org/abs/2104.10762v14
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 60D05
- **Links**: [PDF](http://arxiv.org/pdf/2104.10762v14)
- **Published**: 2021-04-09 07:09:54+00:00
- **Updated**: 2023-04-26 13:02:50+00:00
- **Authors**: Robert A. Murphy
- **Comment**: None
- **Journal**: None
- **Summary**: Random field and random cluster theory are used to describe certain mathematical results concerning the probability distribution of image pixel intensities characterized as generic $2D$ integer arrays. The size of the smallest bounded region within an image is estimated for segmenting an image, from which, the equilibrium distribution of intensities can be recovered. From the estimated bounded regions, properties of the sub-optimal and equilibrium distributions of intensities are derived, which leads to an image compression methodology whereby only slightly more than half of all pixels are required for a worst-case reconstruction of the original image. A custom deep belief network and heuristic allows for the unsupervised segmentation, detection and localization of objects in an image. An example illustrates the mathematical results.



### Piracy-Resistant DNN Watermarking by Block-Wise Image Transformation with Secret Key
- **Arxiv ID**: http://arxiv.org/abs/2104.04241v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.04241v1)
- **Published**: 2021-04-09 08:21:53+00:00
- **Updated**: 2021-04-09 08:21:53+00:00
- **Authors**: MaungMaung AprilPyone, Hitoshi Kiya
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel DNN watermarking method that utilizes a learnable image transformation method with a secret key. The proposed method embeds a watermark pattern in a model by using learnable transformed images and allows us to remotely verify the ownership of the model. As a result, it is piracy-resistant, so the original watermark cannot be overwritten by a pirated watermark, and adding a new watermark decreases the model accuracy unlike most of the existing DNN watermarking methods. In addition, it does not require a special pre-defined training set or trigger set. We empirically evaluated the proposed method on the CIFAR-10 dataset. The results show that it was resilient against fine-tuning and pruning attacks while maintaining a high watermark-detection accuracy.



### Skeleton-based Hand-Gesture Recognition with Lightweight Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.04255v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04255v2)
- **Published**: 2021-04-09 09:06:53+00:00
- **Updated**: 2021-12-27 16:43:54+00:00
- **Authors**: Hichem Sahbi
- **Comment**: None
- **Journal**: None
- **Summary**: Graph convolutional networks (GCNs) aim at extending deep learning to arbitrary irregular domains, namely graphs. Their success is highly dependent on how the topology of input graphs is defined and most of the existing GCN architectures rely on predefined or handcrafted graph structures. In this paper, we introduce a novel method that learns the topology (or connectivity) of input graphs as a part of GCN design. The main contribution of our method resides in building an orthogonal connectivity basis that optimally aggregates nodes, through their neighborhood, prior to achieve convolution. Our method also considers a stochasticity criterion which acts as a regularizer that makes the learned basis and the underlying GCNs lightweight while still being highly effective. Experiments conducted on the challenging task of skeleton-based hand-gesture recognition show the high effectiveness of the learned GCNs w.r.t. the related work.



### Reversible Watermarking in Deep Convolutional Neural Networks for Integrity Authentication
- **Arxiv ID**: http://arxiv.org/abs/2104.04268v1
- **DOI**: 10.1145/3394171.3413729
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.04268v1)
- **Published**: 2021-04-09 09:32:21+00:00
- **Updated**: 2021-04-09 09:32:21+00:00
- **Authors**: Xiquan Guan, Huamin Feng, Weiming Zhang, Hang Zhou, Jie Zhang, Nenghai Yu
- **Comment**: Accepted to ACM MM 2020
- **Journal**: None
- **Summary**: Deep convolutional neural networks have made outstanding contributions in many fields such as computer vision in the past few years and many researchers published well-trained network for downloading. But recent studies have shown serious concerns about integrity due to model-reuse attacks and backdoor attacks. In order to protect these open-source networks, many algorithms have been proposed such as watermarking. However, these existing algorithms modify the contents of the network permanently and are not suitable for integrity authentication. In this paper, we propose a reversible watermarking algorithm for integrity authentication. Specifically, we present the reversible watermarking problem of deep convolutional neural networks and utilize the pruning theory of model compression technology to construct a host sequence used for embedding watermarking information by histogram shift. As shown in the experiments, the influence of embedding reversible watermarking on the classification performance is less than 0.5% and the parameters of the model can be fully recovered after extracting the watermarking. At the same time, the integrity of the model can be verified by applying the reversible watermarking: if the model is modified illegally, the authentication information generated by original model will be absolutely different from the extracted watermarking information.



### GATSBI: Generative Agent-centric Spatio-temporal Object Interaction
- **Arxiv ID**: http://arxiv.org/abs/2104.04275v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.04275v1)
- **Published**: 2021-04-09 09:45:00+00:00
- **Updated**: 2021-04-09 09:45:00+00:00
- **Authors**: Cheol-Hui Min, Jinseok Bae, Junho Lee, Young Min Kim
- **Comment**: accepted to CVPR'2021 as an oral presentation. Code and video will be
  released soon
- **Journal**: None
- **Summary**: We present GATSBI, a generative model that can transform a sequence of raw observations into a structured latent representation that fully captures the spatio-temporal context of the agent's actions. In vision-based decision-making scenarios, an agent faces complex high-dimensional observations where multiple entities interact with each other. The agent requires a good scene representation of the visual observation that discerns essential components and consistently propagates along the time horizon. Our method, GATSBI, utilizes unsupervised object-centric scene representation learning to separate an active agent, static background, and passive objects. GATSBI then models the interactions reflecting the causal relationships among decomposed entities and predicts physically plausible future states. Our model generalizes to a variety of environments where different types of robots and objects dynamically interact with each other. We show GATSBI achieves superior performance on scene decomposition and video prediction compared to its state-of-the-art counterparts.



### Direct Differentiable Augmentation Search
- **Arxiv ID**: http://arxiv.org/abs/2104.04282v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.04282v2)
- **Published**: 2021-04-09 10:02:24+00:00
- **Updated**: 2021-10-12 12:52:46+00:00
- **Authors**: Aoming Liu, Zehao Huang, Zhiwu Huang, Naiyan Wang
- **Comment**: ICCV2021
- **Journal**: None
- **Summary**: Data augmentation has been an indispensable tool to improve the performance of deep neural networks, however the augmentation can hardly transfer among different tasks and datasets. Consequently, a recent trend is to adopt AutoML technique to learn proper augmentation policy without extensive hand-crafted tuning. In this paper, we propose an efficient differentiable search algorithm called Direct Differentiable Augmentation Search (DDAS). It exploits meta-learning with one-step gradient update and continuous relaxation to the expected training loss for efficient search. Our DDAS can achieve efficient augmentation search without relying on approximations such as Gumbel Softmax or second order gradient approximation. To further reduce the adverse effect of improper augmentations, we organize the search space into a two level hierarchy, in which we first decide whether to apply augmentation, and then determine the specific augmentation policy. On standard image classification benchmarks, our DDAS achieves state-of-the-art performance and efficiency tradeoff while reducing the search cost dramatically, e.g. 0.15 GPU hours for CIFAR-10. In addition, we also use DDAS to search augmentation for object detection task and achieve comparable performance with AutoAugment, while being 1000x faster.



### MLF-SC: Incorporating multi-layer features to sparse coding for anomaly detection
- **Arxiv ID**: http://arxiv.org/abs/2104.04289v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.04289v1)
- **Published**: 2021-04-09 10:20:34+00:00
- **Updated**: 2021-04-09 10:20:34+00:00
- **Authors**: Ryuji Imamura, Kohei Azuma, Atsushi Hanamoto, Atsunori Kanemura
- **Comment**: None
- **Journal**: None
- **Summary**: Anomalies in images occur in various scales from a small hole on a carpet to a large stain. However, anomaly detection based on sparse coding, one of the widely used anomaly detection methods, has an issue in dealing with anomalies that are out of the patch size employed to sparsely represent images. A large anomaly can be considered normal if seen in a small scale, but it is not easy to determine a single scale (patch size) that works well for all images. Then, we propose to incorporate multi-scale features to sparse coding and improve the performance of anomaly detection. The proposed method, multi-layer feature sparse coding (MLF-SC), employs a neural network for feature extraction, and feature maps from intermediate layers of the network are given to sparse coding, whereas the standard sparse-coding-based anomaly detection method directly works on given images. We show that MLF-SC outperforms state-of-the-art anomaly detection methods including those employing deep learning. Our target data are the texture categories of the MVTec Anomaly Detection (MVTec AD) dataset, which is a modern benchmark dataset consisting of images from the real world. Our idea can be a simple and practical option to deal with practical data.



### Brain Surface Reconstruction from MRI Images Based on Segmentation Networks Applying Signed Distance Maps
- **Arxiv ID**: http://arxiv.org/abs/2104.04291v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.04291v1)
- **Published**: 2021-04-09 10:24:27+00:00
- **Updated**: 2021-04-09 10:24:27+00:00
- **Authors**: Heng Fang, Xi Yang, Taichi Kin, Takeo Igarashi
- **Comment**: Accepted by IEEE ISBI 2021 (International Symposium on Biomedical
  Imaging)
- **Journal**: None
- **Summary**: Whole-brain surface extraction is an essential topic in medical imaging systems as it provides neurosurgeons with a broader view of surgical planning and abnormality detection. To solve the problem confronted in current deep learning skull stripping methods lacking prior shape information, we propose a new network architecture that incorporates knowledge of signed distance fields and introduce an additional Laplacian loss to ensure that the prediction results retain shape information. We validated our newly proposed method by conducting experiments on our brain magnetic resonance imaging dataset (111 patients). The evaluation results demonstrate that our approach achieves comparable dice scores and also reduces the Hausdorff distance and average symmetric surface distance, thus producing more stable and smooth brain isosurfaces.



### Trusting small training dataset for supervised change detection
- **Arxiv ID**: http://arxiv.org/abs/2104.05443v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05443v1)
- **Published**: 2021-04-09 10:57:03+00:00
- **Updated**: 2021-04-09 10:57:03+00:00
- **Authors**: Sudipan Saha, Biplab Banerjee, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning (DL) based supervised change detection (CD) models require large labeled training data. Due to the difficulty of collecting labeled multi-temporal data, unsupervised methods are preferred in the CD literature. However, unsupervised methods cannot fully exploit the potentials of data-driven deep learning and thus they are not absolute alternative to the supervised methods. This motivates us to look deeper into the supervised DL methods and investigate how they can be adopted intelligently for CD by minimizing the requirement of labeled training data. Towards this, in this work we show that geographically diverse training dataset can yield significant improvement over less diverse training datasets of the same size. We propose a simple confidence indicator for verifying the trustworthiness/confidence of supervised models trained with small labeled dataset. Moreover, we show that for the test cases where supervised CD model is found to be less confident/trustworthy, unsupervised methods often produce better result than the supervised ones.



### Out-of-distribution detection in satellite image classification
- **Arxiv ID**: http://arxiv.org/abs/2104.05442v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05442v1)
- **Published**: 2021-04-09 11:11:52+00:00
- **Updated**: 2021-04-09 11:11:52+00:00
- **Authors**: Jakob Gawlikowski, Sudipan Saha, Anna Kruspe, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In satellite image analysis, distributional mismatch between the training and test data may arise due to several reasons, including unseen classes in the test data and differences in the geographic area. Deep learning based models may behave in unexpected manner when subjected to test data that has such distributional shifts from the training data, also called out-of-distribution (OOD) examples. Predictive uncertainly analysis is an emerging research topic which has not been explored much in context of satellite image analysis. Towards this, we adopt a Dirichlet Prior Network based model to quantify distributional uncertainty of deep learning models for remote sensing. The approach seeks to maximize the representation gap between the in-domain and OOD examples for a better identification of unknown examples at test time. Experimental results on three exemplary test scenarios show the efficacy of the model in satellite image analysis.



### Context-self contrastive pretraining for crop type semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.04310v2
- **DOI**: 10.1109/TGRS.2022.3198187
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.04310v2)
- **Published**: 2021-04-09 11:29:44+00:00
- **Updated**: 2021-12-18 14:00:09+00:00
- **Authors**: Michail Tarasiou, Riza Alp Guler, Stefanos Zafeiriou
- **Comment**: 15 pages, 17 figures
- **Journal**: None
- **Summary**: In this paper, we propose a fully supervised pre-training scheme based on contrastive learning particularly tailored to dense classification tasks. The proposed Context-Self Contrastive Loss (CSCL) learns an embedding space that makes semantic boundaries pop-up by use of a similarity metric between every location in a training sample and its local context. For crop type semantic segmentation from Satellite Image Time Series (SITS) we find performance at parcel boundaries to be a critical bottleneck and explain how CSCL tackles the underlying cause of that problem, improving the state-of-the-art performance in this task. Additionally, using images from the Sentinel-2 (S2) satellite missions we compile the largest, to our knowledge, SITS dataset densely annotated by crop type and parcel identities, which we make publicly available together with the data generation pipeline. Using that data we find CSCL, even with minimal pre-training, to improve all respective baselines and present a process for semantic segmentation at super-resolution for obtaining crop classes at a more granular level. The code and instructions to download the data can be found in https://github.com/michaeltrs/DeepSatModels.



### CFNet: Cascade and Fused Cost Volume for Robust Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2104.04314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04314v1)
- **Published**: 2021-04-09 11:38:59+00:00
- **Updated**: 2021-04-09 11:38:59+00:00
- **Authors**: Zhelun Shen, Yuchao Dai, Zhibo Rao
- **Comment**: accepted by CVPR2021
- **Journal**: None
- **Summary**: Recently, the ever-increasing capacity of large-scale annotated datasets has led to profound progress in stereo matching. However, most of these successes are limited to a specific dataset and cannot generalize well to other datasets. The main difficulties lie in the large domain differences and unbalanced disparity distribution across a variety of datasets, which greatly limit the real-world applicability of current deep stereo matching models. In this paper, we propose CFNet, a Cascade and Fused cost volume based network to improve the robustness of the stereo matching network. First, we propose a fused cost volume representation to deal with the large domain difference. By fusing multiple low-resolution dense cost volumes to enlarge the receptive field, we can extract robust structural representations for initial disparity estimation. Second, we propose a cascade cost volume representation to alleviate the unbalanced disparity distribution. Specifically, we employ a variance-based uncertainty estimation to adaptively adjust the next stage disparity search space, in this way driving the network progressively prune out the space of unlikely correspondences. By iteratively narrowing down the disparity search space and improving the cost volume resolution, the disparity estimation is gradually refined in a coarse-to-fine manner. When trained on the same training images and evaluated on KITTI, ETH3D, and Middlebury datasets with the fixed model parameters and hyperparameters, our proposed method achieves the state-of-the-art overall performance and obtains the 1st place on the stereo task of Robust Vision Challenge 2020. The code will be available at https://github.com/gallenszl/CFNet.



### Towards Fine-grained Visual Representations by Combining Contrastive Learning with Image Reconstruction and Attention-weighted Pooling
- **Arxiv ID**: http://arxiv.org/abs/2104.04323v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.04323v2)
- **Published**: 2021-04-09 12:12:10+00:00
- **Updated**: 2022-02-22 14:19:30+00:00
- **Authors**: Jonas Dippel, Steffen Vogler, Johannes Höhne
- **Comment**: ICML 2021 Workshop: Self-Supervised Learning for Reasoning and
  Perception
- **Journal**: None
- **Summary**: This paper presents Contrastive Reconstruction, ConRec - a self-supervised learning algorithm that obtains image representations by jointly optimizing a contrastive and a self-reconstruction loss. We showcase that state-of-the-art contrastive learning methods (e.g. SimCLR) have shortcomings to capture fine-grained visual features in their representations. ConRec extends the SimCLR framework by adding (1) a self-reconstruction task and (2) an attention mechanism within the contrastive learning task. This is accomplished by applying a simple encoder-decoder architecture with two heads. We show that both extensions contribute towards an improved vector representation for images with fine-grained visual features. Combining those concepts, ConRec outperforms SimCLR and SimCLR with Attention-Pooling on fine-grained classification datasets.



### Learning Position and Target Consistency for Memory-based Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.04329v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04329v1)
- **Published**: 2021-04-09 12:22:37+00:00
- **Updated**: 2021-04-09 12:22:37+00:00
- **Authors**: Li Hu, Peng Zhang, Bang Zhang, Pan Pan, Yinghui Xu, Rong Jin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies the problem of semi-supervised video object segmentation(VOS). Multiple works have shown that memory-based approaches can be effective for video object segmentation. They are mostly based on pixel-level matching, both spatially and temporally. The main shortcoming of memory-based approaches is that they do not take into account the sequential order among frames and do not exploit object-level knowledge from the target. To address this limitation, we propose to Learn position and target Consistency framework for Memory-based video object segmentation, termed as LCM. It applies the memory mechanism to retrieve pixels globally, and meanwhile learns position consistency for more reliable segmentation. The learned location response promotes a better discrimination between target and distractors. Besides, LCM introduces an object-level relationship from the target to maintain target consistency, making LCM more robust to error drifting. Experiments show that our LCM achieves state-of-the-art performance on both DAVIS and Youtube-VOS benchmark. And we rank the 1st in the DAVIS 2020 challenge semi-supervised VOS task.



### Rock Hunting With Martian Machine Vision
- **Arxiv ID**: http://arxiv.org/abs/2104.04359v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.04359v1)
- **Published**: 2021-04-09 13:44:27+00:00
- **Updated**: 2021-04-09 13:44:27+00:00
- **Authors**: David Noever, Samantha E. Miller Noever
- **Comment**: None
- **Journal**: None
- **Summary**: The Mars Perseverance rover applies computer vision for navigation and hazard avoidance. The challenge to do onboard object recognition highlights the need for low-power, customized training, often including low-contrast backgrounds. We investigate deep learning methods for the classification and detection of Martian rocks. We report greater than 97% accuracy for binary classifications (rock vs. rover). We fine-tune a detector to render geo-located bounding boxes while counting rocks. For these models to run on microcontrollers, we shrink and quantize the neural networks' weights and demonstrate a low-power rock hunter with faster frame rates (1 frame per second) but lower accuracy (37%).



### Multimodal Face Synthesis from Visual Attributes
- **Arxiv ID**: http://arxiv.org/abs/2104.04362v1
- **DOI**: 10.1109/TBIOM.2021.3082038
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04362v1)
- **Published**: 2021-04-09 13:47:23+00:00
- **Updated**: 2021-04-09 13:47:23+00:00
- **Authors**: Xing Di, Vishal M. Patel
- **Comment**: IEEE Transactions on Biometrics, Behavior, and Identity Science
  (T-BIOM) submission
- **Journal**: None
- **Summary**: Synthesis of face images from visual attributes is an important problem in computer vision and biometrics due to its applications in law enforcement and entertainment. Recent advances in deep generative networks have made it possible to synthesize high-quality face images from visual attributes. However, existing methods are specifically designed for generating unimodal images (i.e visible faces) from attributes. In this paper, we propose a novel generative adversarial network that simultaneously synthesizes identity preserving multimodal face images (i.e. visible, sketch, thermal, etc.) from visual attributes without requiring paired data in different domains for training the network. We introduce a novel generator with multimodal stretch-out modules to simultaneously synthesize multimodal face images. Additionally, multimodal stretch-in modules are introduced in the discriminator which discriminates between real and fake images. Extensive experiments and comparisons with several state-of-the-art methods are performed to verify the effectiveness of the proposed attribute-based multimodal synthesis method.



### Video-aided Unsupervised Grammar Induction
- **Arxiv ID**: http://arxiv.org/abs/2104.04369v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2104.04369v2)
- **Published**: 2021-04-09 14:01:36+00:00
- **Updated**: 2021-05-04 00:23:28+00:00
- **Authors**: Songyang Zhang, Linfeng Song, Lifeng Jin, Kun Xu, Dong Yu, Jiebo Luo
- **Comment**: This paper is accepted by NAACL'21
- **Journal**: None
- **Summary**: We investigate video-aided grammar induction, which learns a constituency parser from both unlabeled text and its corresponding video. Existing methods of multi-modal grammar induction focus on learning syntactic grammars from text-image pairs, with promising results showing that the information from static images is useful in induction. However, videos provide even richer information, including not only static objects but also actions and state changes useful for inducing verb phrases. In this paper, we explore rich features (e.g. action, object, scene, audio, face, OCR and speech) from videos, taking the recent Compound PCFG model as the baseline. We further propose a Multi-Modal Compound PCFG model (MMC-PCFG) to effectively aggregate these rich features from different modalities. Our proposed MMC-PCFG is trained end-to-end and outperforms each individual modality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction.



### CondenseNet V2: Sparse Feature Reactivation for Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.04382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04382v1)
- **Published**: 2021-04-09 14:12:43+00:00
- **Updated**: 2021-04-09 14:12:43+00:00
- **Authors**: Le Yang, Haojun Jiang, Ruojin Cai, Yulin Wang, Shiji Song, Gao Huang, Qi Tian
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Reusing features in deep networks through dense connectivity is an effective way to achieve high computational efficiency. The recent proposed CondenseNet has shown that this mechanism can be further improved if redundant features are removed. In this paper, we propose an alternative approach named sparse feature reactivation (SFR), aiming at actively increasing the utility of features for reusing. In the proposed network, named CondenseNetV2, each layer can simultaneously learn to 1) selectively reuse a set of most important features from preceding layers; and 2) actively update a set of preceding features to increase their utility for later layers. Our experiments show that the proposed models achieve promising performance on image classification (ImageNet and CIFAR) and object detection (MS COCO) in terms of both theoretical efficiency and practical speed.



### Look Before You Leap: Learning Landmark Features for One-Stage Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/2104.04386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04386v1)
- **Published**: 2021-04-09 14:20:36+00:00
- **Updated**: 2021-04-09 14:20:36+00:00
- **Authors**: Binbin Huang, Dongze Lian, Weixin Luo, Shenghua Gao
- **Comment**: Code is available at https://github.com/svip-lab/LBYLNet
- **Journal**: None
- **Summary**: An LBYL (`Look Before You Leap') Network is proposed for end-to-end trainable one-stage visual grounding. The idea behind LBYL-Net is intuitive and straightforward: we follow a language's description to localize the target object based on its relative spatial relation to `Landmarks', which is characterized by some spatial positional words and some descriptive words about the object. The core of our LBYL-Net is a landmark feature convolution module that transmits the visual features with the guidance of linguistic description along with different directions. Consequently, such a module encodes the relative spatial positional relations between the current object and its context. Then we combine the contextual information from the landmark feature convolution module with the target's visual features for grounding. To make this landmark feature convolution light-weight, we introduce a dynamic programming algorithm (termed dynamic max pooling) with low complexity to extract the landmark feature. Thanks to the landmark feature convolution module, we mimic the human behavior of `Look Before You Leap' to design an LBYL-Net, which takes full consideration of contextual information. Extensive experiments show our method's effectiveness in four grounding datasets. Specifically, our LBYL-Net outperforms all state-of-the-art two-stage and one-stage methods on ReferitGame. On RefCOCO and RefCOCO+, Our LBYL-Net also achieves comparable results or even better results than existing one-stage methods.



### Flow-based Spatio-Temporal Structured Prediction of Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2104.04391v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04391v2)
- **Published**: 2021-04-09 14:30:35+00:00
- **Updated**: 2022-05-20 10:37:55+00:00
- **Authors**: Mohsen Zand, Ali Etemad, Michael Greenspan
- **Comment**: 11 pages, LaTeX; typos corrected, updated
- **Journal**: None
- **Summary**: Conditional Normalizing Flows (CNFs) are flexible generative models capable of representing complicated distributions with high dimensionality and large interdimensional correlations, making them appealing for structured output learning. Their effectiveness in modelling multivariates spatio-temporal structured data has yet to be completely investigated. We propose MotionFlow as a novel normalizing flows approach that autoregressively conditions the output distributions on the spatio-temporal input features. It combines deterministic and stochastic representations with CNFs to create a probabilistic neural generative approach that can model the variability seen in high-dimensional structured spatio-temporal data. We specifically propose to use conditional priors to factorize the latent space for the time dependent modeling. We also exploit the use of masked convolutions as autoregressive conditionals in CNFs. As a result, our method is able to define arbitrarily expressive output probability distributions under temporal dynamics in multivariate prediction tasks. We apply our method to different tasks, including trajectory prediction, motion prediction, time series forecasting, and binary segmentation, and demonstrate that our model is able to leverage normalizing flows to learn complicated time dependent conditional distributions.



### SVDistNet: Self-Supervised Near-Field Distance Estimation on Surround View Fisheye Cameras
- **Arxiv ID**: http://arxiv.org/abs/2104.04420v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.04420v1)
- **Published**: 2021-04-09 15:20:20+00:00
- **Updated**: 2021-04-09 15:20:20+00:00
- **Authors**: Varun Ravi Kumar, Marvin Klingner, Senthil Yogamani, Markus Bach, Stefan Milz, Tim Fingscheidt, Patrick Mäder
- **Comment**: To be published at IEEE Transactions on Intelligent Transportation
  Systems
- **Journal**: None
- **Summary**: A 360{\deg} perception of scene geometry is essential for automated driving, notably for parking and urban driving scenarios. Typically, it is achieved using surround-view fisheye cameras, focusing on the near-field area around the vehicle. The majority of current depth estimation approaches focus on employing just a single camera, which cannot be straightforwardly generalized to multiple cameras. The depth estimation model must be tested on a variety of cameras equipped to millions of cars with varying camera geometries. Even within a single car, intrinsics vary due to manufacturing tolerances. Deep learning models are sensitive to these changes, and it is practically infeasible to train and test on each camera variant. As a result, we present novel camera-geometry adaptive multi-scale convolutions which utilize the camera parameters as a conditional input, enabling the model to generalize to previously unseen fisheye cameras. Additionally, we improve the distance estimation by pairwise and patchwise vector-based self-attention encoder networks. We evaluate our approach on the Fisheye WoodScape surround-view dataset, significantly improving over previous approaches. We also show a generalization of our approach across different camera viewing angles and perform extensive experiments to support our contributions. To enable comparison with other approaches, we evaluate the front camera data on the KITTI dataset (pinhole camera images) and achieve state-of-the-art performance among self-supervised monocular methods. An overview video with qualitative results is provided at https://youtu.be/bmX0UcU9wtA. Baseline code and dataset will be made public.



### Ice Core Science Meets Computer Vision: Challenges and Perspectives
- **Arxiv ID**: http://arxiv.org/abs/2104.04430v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/2104.04430v1)
- **Published**: 2021-04-09 15:27:44+00:00
- **Updated**: 2021-04-09 15:27:44+00:00
- **Authors**: P. Bohleber, M. Roman, C. Barbante, S. Vascon, K. Siddiqi, M. Pelillo
- **Comment**: 9 pages, 2 figures, submitted to Frontiers in Computer Science,
  section Computer Vision
- **Journal**: None
- **Summary**: Polar ice cores play a central role in studies of the earth's climate system through natural archives. A pressing issue is the analysis of the oldest, highly thinned ice core sections, where the identification of paleoclimate signals is particularly challenging. For this, state-of-the-art imaging by laser-ablation inductively-coupled plasma mass spectrometry (LA-ICP-MS) has the potential to be revolutionary due to its combination of micron-scale 2D chemical information with visual features. However, the quantitative study of record preservation in chemical images raises new questions that call for the expertise of the computer vision community. To illustrate this new inter-disciplinary frontier, we describe a selected set of key questions. One critical task is to assess the paleoclimate significance of single line profiles along the main core axis, which we show is a scale-dependent problem for which advanced image analysis methods are critical. Another important issue is the evaluation of post-depositional layer changes, for which the chemical images provide rich information. Accordingly, the time is ripe to begin an intensified exchange among the two scientific communities of computer vision and ice core science. The collaborative building of a new framework for investigating high-resolution chemical images with automated image analysis techniques will also benefit the already wide-spread application of LA-ICP-MS chemical imaging in the geosciences.



### Benchmarking Scene Text Recognition in Devanagari, Telugu and Malayalam
- **Arxiv ID**: http://arxiv.org/abs/2104.04437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04437v1)
- **Published**: 2021-04-09 15:36:33+00:00
- **Updated**: 2021-04-09 15:36:33+00:00
- **Authors**: Minesh Mathew, Mohit Jain, CV Jawahar
- **Comment**: This work was accepted at MOCR Workshop, ICDAR 2017 Uploading updated
  draft which includes links to download datasets and rendering script
- **Journal**: None
- **Summary**: Inspired by the success of Deep Learning based approaches to English scene text recognition, we pose and benchmark scene text recognition for three Indic scripts - Devanagari, Telugu and Malayalam. Synthetic word images rendered from Unicode fonts are used for training the recognition system. And the performance is bench-marked on a new IIIT-ILST dataset comprising of hundreds of real scene images containing text in the above mentioned scripts. We use a segmentation free, hybrid but end-to-end trainable CNN-RNN deep neural network for transcribing the word images to the corresponding texts. The cropped word images need not be segmented into the sub-word units and the error is calculated and backpropagated for the the given word image at once. The network is trained using CTC loss, which is proven quite effective for sequence-to-sequence transcription tasks. The CNN layers in the network learn to extract robust feature representations from word images. The sequence of features learnt by the convolutional block is transcribed to a sequence of labels by the RNN+CTC block. The transcription is not bound by word length or a lexicon and is ideal for Indian languages which are highly inflectional. IIIT-ILST dataset, synthetic word images dataset and the script used to render synthetic images are available at http://cvit.iiit.ac.in/research/projects/cvit-projects/iiit-ilst



### A Reinforcement-Learning-Based Energy-Efficient Framework for Multi-Task Video Analytics Pipeline
- **Arxiv ID**: http://arxiv.org/abs/2104.04443v2
- **DOI**: 10.1109/TMM.2021.3076612
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04443v2)
- **Published**: 2021-04-09 15:44:06+00:00
- **Updated**: 2021-05-02 11:14:04+00:00
- **Authors**: Yingying Zhao, Mingzhi Dong, Yujiang Wang, Da Feng, Qin Lv, Robert P. Dick, Dongsheng Li, Tun Lu, Ning Gu, Li Shang
- **Comment**: IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Deep-learning-based video processing has yielded transformative results in recent years. However, the video analytics pipeline is energy-intensive due to high data rates and reliance on complex inference algorithms, which limits its adoption in energy-constrained applications. Motivated by the observation of high and variable spatial redundancy and temporal dynamics in video data streams, we design and evaluate an adaptive-resolution optimization framework to minimize the energy use of multi-task video analytics pipelines. Instead of heuristically tuning the input data resolution of individual tasks, our framework utilizes deep reinforcement learning to dynamically govern the input resolution and computation of the entire video analytics pipeline. By monitoring the impact of varying resolution on the quality of high-dimensional video analytics features, hence the accuracy of video analytics results, the proposed end-to-end optimization framework learns the best non-myopic policy for dynamically controlling the resolution of input video streams to globally optimize energy efficiency. Governed by reinforcement learning, optical flow is incorporated into the framework to minimize unnecessary spatio-temporal redundancy that leads to re-computation, while preserving accuracy. The proposed framework is applied to video instance segmentation which is one of the most challenging computer vision tasks, and achieves better energy efficiency than all baseline methods of similar accuracy on the YouTube-VIS dataset.



### Relating Adversarially Robust Generalization to Flat Minima
- **Arxiv ID**: http://arxiv.org/abs/2104.04448v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.04448v2)
- **Published**: 2021-04-09 15:55:01+00:00
- **Updated**: 2021-10-06 17:47:30+00:00
- **Authors**: David Stutz, Matthias Hein, Bernt Schiele
- **Comment**: ICCV'21
- **Journal**: None
- **Summary**: Adversarial training (AT) has become the de-facto standard to obtain models robust against adversarial examples. However, AT exhibits severe robust overfitting: cross-entropy loss on adversarial examples, so-called robust loss, decreases continuously on training examples, while eventually increasing on test examples. In practice, this leads to poor robust generalization, i.e., adversarial robustness does not generalize well to new examples. In this paper, we study the relationship between robust generalization and flatness of the robust loss landscape in weight space, i.e., whether robust loss changes significantly when perturbing weights. To this end, we propose average- and worst-case metrics to measure flatness in the robust loss landscape and show a correlation between good robust generalization and flatness. For example, throughout training, flatness reduces significantly during overfitting such that early stopping effectively finds flatter minima in the robust loss landscape. Similarly, AT variants achieving higher adversarial robustness also correspond to flatter minima. This holds for many popular choices, e.g., AT-AWP, TRADES, MART, AT with self-supervision or additional unlabeled examples, as well as simple regularization techniques, e.g., AutoAugment, weight decay or label noise. For fair comparison across these approaches, our flatness measures are specifically designed to be scale-invariant and we conduct extensive experiments to validate our findings.



### Unsupervised Class-Incremental Learning Through Confusion
- **Arxiv ID**: http://arxiv.org/abs/2104.04450v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.04450v2)
- **Published**: 2021-04-09 15:58:43+00:00
- **Updated**: 2021-12-08 16:31:51+00:00
- **Authors**: Shivam Khare, Kun Cao, James Rehg
- **Comment**: None
- **Journal**: None
- **Summary**: While many works on Continual Learning have shown promising results for mitigating catastrophic forgetting, they have relied on supervised training. To successfully learn in a label-agnostic incremental setting, a model must distinguish between learned and novel classes to properly include samples for training. We introduce a novelty detection method that leverages network confusion caused by training incoming data as a new class. We found that incorporating a class-imbalance during this detection method substantially enhances performance. The effectiveness of our approach is demonstrated across a set of image classification benchmarks: MNIST, SVHN, CIFAR-10, CIFAR-100, and CRIB.



### Bootstrapping Semantic Segmentation with Regional Contrast
- **Arxiv ID**: http://arxiv.org/abs/2104.04465v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.04465v4)
- **Published**: 2021-04-09 16:26:29+00:00
- **Updated**: 2022-01-31 15:31:11+00:00
- **Authors**: Shikun Liu, Shuaifeng Zhi, Edward Johns, Andrew J. Davison
- **Comment**: Published at ICLR 2022. Project Page:
  https://shikun.io/projects/regional-contrast. Code:
  https://github.com/lorenmt/reco
- **Journal**: None
- **Summary**: We present ReCo, a contrastive learning framework designed at a regional level to assist learning in semantic segmentation. ReCo performs semi-supervised or supervised pixel-level contrastive learning on a sparse set of hard negative pixels, with minimal additional memory footprint. ReCo is easy to implement, being built on top of off-the-shelf segmentation networks, and consistently improves performance in both semi-supervised and supervised semantic segmentation methods, achieving smoother segmentation boundaries and faster convergence. The strongest effect is in semi-supervised learning with very few labels. With ReCo, we achieve high-quality semantic segmentation models, requiring only 5 examples of each semantic class. Code is available at https://github.com/lorenmt/reco.



### Improving the Efficiency and Robustness of Deepfakes Detection through Precise Geometric Features
- **Arxiv ID**: http://arxiv.org/abs/2104.04480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04480v1)
- **Published**: 2021-04-09 16:57:55+00:00
- **Updated**: 2021-04-09 16:57:55+00:00
- **Authors**: Zekun Sun, Yujie Han, Zeyu Hua, Na Ruan, Weijia Jia
- **Comment**: IEEE/CVF Conference on Computer Vision and Pattern Recognition 2021
  (CVPR 2021)
- **Journal**: None
- **Summary**: Deepfakes is a branch of malicious techniques that transplant a target face to the original one in videos, resulting in serious problems such as infringement of copyright, confusion of information, or even public panic. Previous efforts for Deepfakes videos detection mainly focused on appearance features, which have a risk of being bypassed by sophisticated manipulation, also resulting in high model complexity and sensitiveness to noise. Besides, how to mine the temporal features of manipulated videos and exploit them is still an open question. We propose an efficient and robust framework named LRNet for detecting Deepfakes videos through temporal modeling on precise geometric features. A novel calibration module is devised to enhance the precision of geometric features, making it more discriminative, and a two-stream Recurrent Neural Network (RNN) is constructed for sufficient exploitation of temporal features. Compared to previous methods, our proposed method is lighter-weighted and easier to train. Moreover, our method has shown robustness in detecting highly compressed or noise corrupted videos. Our model achieved 0.999 AUC on FaceForensics++ dataset. Meanwhile, it has a graceful decline in performance (-0.042 AUC) when faced with highly compressed videos.



### Class-Wise Principal Component Analysis for hyperspectral image feature extraction
- **Arxiv ID**: http://arxiv.org/abs/2104.04496v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04496v1)
- **Published**: 2021-04-09 17:25:11+00:00
- **Updated**: 2021-04-09 17:25:11+00:00
- **Authors**: Dimitra Koumoutsou, Eleni Charou, Georgios Siolas, Giorgos Stamou
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces the Class-wise Principal Component Analysis, a supervised feature extraction method for hyperspectral data. Hyperspectral Imaging (HSI) has appeared in various fields in recent years, including Remote Sensing. Realizing that information extraction tasks for hyperspectral images are burdened by data-specific issues, we identify and address two major problems. Those are the Curse of Dimensionality which occurs due to the high-volume of the data cube and the class imbalance problem which is common in hyperspectral datasets. Dimensionality reduction is an essential preprocessing step to complement a hyperspectral image classification task. Therefore, we propose a feature extraction algorithm for dimensionality reduction, based on Principal Component Analysis (PCA). Evaluations are carried out on the Indian Pines dataset to demonstrate that significant improvements are achieved when using the reduced data in a classification task.



### Chest X-Ray Bone Suppression for Improving Classification of Tuberculosis-Consistent Findings
- **Arxiv ID**: http://arxiv.org/abs/2104.04518v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2104.04518v2)
- **Published**: 2021-04-09 17:58:25+00:00
- **Updated**: 2021-05-07 11:47:05+00:00
- **Authors**: Sivaramakrishnan Rajaraman, Ghada Zamzmi, Les Folio, Philip Alderson, Sameer Antani
- **Comment**: 22 pages, 14 figures, 4 tables
- **Journal**: None
- **Summary**: Chest X-rays are the most commonly performed diagnostic examination to detect cardiopulmonary abnormalities. However, the presence of bony structures such as ribs and clavicles can obscure subtle abnormalities, resulting in diagnostic errors. This study aims to build a deep learning-based bone suppression model that identifies and removes these occluding bony structures in frontal CXRs to assist in reducing errors in radiological interpretation, including DL workflows, related to detecting manifestations consistent with tuberculosis (TB). Several bone suppression models with various deep architectures are trained and optimized using the proposed combined loss function and their performances are evaluated in a cross-institutional test setting. The best-performing model is used to suppress bones in the publicly available Shenzhen and Montgomery TB CXR collections. A VGG-16 model is pretrained on a large collection of publicly available CXRs. The CXR-pretrained model is then fine-tuned individually on the non-bone-suppressed and bone-suppressed CXRs of Shenzhen and Montgomery TB CXR collections to classify them as showing normal lungs or TB manifestations. The performances of these models are compared using several performance metrics, analyzed for statistical significance, and their predictions are qualitatively interpreted through class-selective relevance maps. It is observed that the models trained on bone-suppressed CXRs significantly outperformed (p<0.05) the models trained on the non-bone-suppressed CXRs. Models trained on bone-suppressed CXRs improved detection of TB-consistent findings and resulted in compact clustering of the data points in the feature space signifying that bone suppression improved the model sensitivity toward TB classification.



### Neural RGB-D Surface Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2104.04532v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04532v3)
- **Published**: 2021-04-09 18:00:01+00:00
- **Updated**: 2022-03-14 18:00:00+00:00
- **Authors**: Dejan Azinović, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nießner, Justus Thies
- **Comment**: CVPR'22; Project page:
  https://dazinovic.github.io/neural-rgbd-surface-reconstruction/ Video:
  https://youtu.be/iWuSowPsC3g
- **Journal**: None
- **Summary**: Obtaining high-quality 3D reconstructions of room-scale scenes is of paramount importance for upcoming applications in AR or VR. These range from mixed reality applications for teleconferencing, virtual measuring, virtual room planing, to robotic applications. While current volume-based view synthesis methods that use neural radiance fields (NeRFs) show promising results in reproducing the appearance of an object or scene, they do not reconstruct an actual surface. The volumetric representation of the surface based on densities leads to artifacts when a surface is extracted using Marching Cubes, since during optimization, densities are accumulated along the ray and are not used at a single sample point in isolation. Instead of this volumetric representation of the surface, we propose to represent the surface using an implicit function (truncated signed distance function). We show how to incorporate this representation in the NeRF framework, and extend it to use depth measurements from a commodity RGB-D sensor, such as a Kinect. In addition, we propose a pose and camera refinement technique which improves the overall reconstruction quality. In contrast to concurrent work on integrating depth priors in NeRF which concentrates on novel view synthesis, our approach is able to reconstruct high-quality, metrical 3D reconstructions.



### Uncovering commercial activity in informal cities
- **Arxiv ID**: http://arxiv.org/abs/2104.04545v1
- **DOI**: None
- **Categories**: **econ.GN**, cs.CV, physics.soc-ph, q-fin.EC
- **Links**: [PDF](http://arxiv.org/pdf/2104.04545v1)
- **Published**: 2021-04-09 18:12:52+00:00
- **Updated**: 2021-04-09 18:12:52+00:00
- **Authors**: Daniel Straulino, Juan C. Saldarriaga, Jairo A. Gómez, Juan C. Duque, Neave O'Clery
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge of the spatial organisation of economic activity within a city is key to policy concerns. However, in developing cities with high levels of informality, this information is often unavailable. Recent progress in machine learning together with the availability of street imagery offers an affordable and easily automated solution. Here we propose an algorithm that can detect what we call 'visible firms' using street view imagery. Using Medell\'in, Colombia as a case study, we illustrate how this approach can be used to uncover previously unseen economic activity. Applying spatial analysis to our dataset we detect a polycentric structure with five distinct clusters located in both the established centre and peripheral areas. Comparing the density of visible and registered firms, we find that informal activity concentrates in poor but densely populated areas. Our findings highlight the large gap between what is captured in official data and the reality on the ground.



### RaidaR: A Rich Annotated Image Dataset of Rainy Street Scenes
- **Arxiv ID**: http://arxiv.org/abs/2104.04606v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04606v3)
- **Published**: 2021-04-09 21:15:34+00:00
- **Updated**: 2021-10-26 04:51:57+00:00
- **Authors**: Jiongchao Jin, Arezou Fatemi, Wallace Lira, Fenggen Yu, Biao Leng, Rui Ma, Ali Mahdavi-Amiri, Hao Zhang
- **Comment**: Presented in Second ICCV Workshop on Autonomous Vehicle Vision
  (AVVision), 2021. Website: https://raidar-dataset.com/
- **Journal**: None
- **Summary**: We introduce RaidaR, a rich annotated image dataset of rainy street scenes, to support autonomous driving research. The new dataset contains the largest number of rainy images (58,542) to date, 5,000 of which provide semantic segmentations and 3,658 provide object instance segmentations. The RaidaR images cover a wide range of realistic rain-induced artifacts, including fog, droplets, and road reflections, which can effectively augment existing street scene datasets to improve data-driven machine perception during rainy weather. To facilitate efficient annotation of a large volume of images, we develop a semi-automatic scheme combining manual segmentation and an automated processing akin to cross validation, resulting in 10-20 fold reduction on annotation time. We demonstrate the utility of our new dataset by showing how data augmentation with RaidaR can elevate the accuracy of existing segmentation algorithms. We also present a novel unpaired image-to-image translation algorithm for adding/removing rain artifacts, which directly benefits from RaidaR.



### DexYCB: A Benchmark for Capturing Hand Grasping of Objects
- **Arxiv ID**: http://arxiv.org/abs/2104.04631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04631v1)
- **Published**: 2021-04-09 22:54:21+00:00
- **Updated**: 2021-04-09 22:54:21+00:00
- **Authors**: Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj S. Narang, Karl Van Wyk, Umar Iqbal, Stan Birchfield, Jan Kautz, Dieter Fox
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: We introduce DexYCB, a new dataset for capturing hand grasping of objects. We first compare DexYCB with a related one through cross-dataset evaluation. We then present a thorough benchmark of state-of-the-art approaches on three relevant tasks: 2D object and keypoint detection, 6D object pose estimation, and 3D hand pose estimation. Finally, we evaluate a new robotics-relevant task: generating safe robot grasps in human-to-robot object handover. Dataset and code are available at https://dex-ycb.github.io.



### Pixel Codec Avatars
- **Arxiv ID**: http://arxiv.org/abs/2104.04638v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04638v1)
- **Published**: 2021-04-09 23:17:36+00:00
- **Updated**: 2021-04-09 23:17:36+00:00
- **Authors**: Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang, Yuecheng Li, Fernando De La Torre, Yaser Sheikh
- **Comment**: CVPR 2021 Oral
- **Journal**: None
- **Summary**: Telecommunication with photorealistic avatars in virtual or augmented reality is a promising path for achieving authentic face-to-face communication in 3D over remote physical distances. In this work, we present the Pixel Codec Avatars (PiCA): a deep generative model of 3D human faces that achieves state of the art reconstruction performance while being computationally efficient and adaptive to the rendering conditions during execution. Our model combines two core ideas: (1) a fully convolutional architecture for decoding spatially varying features, and (2) a rendering-adaptive per-pixel decoder. Both techniques are integrated via a dense surface representation that is learned in a weakly-supervised manner from low-topology mesh tracking over training images. We demonstrate that PiCA improves reconstruction over existing techniques across testing expressions and views on persons of different gender and skin tone. Importantly, we show that the PiCA model is much smaller than the state-of-art baseline model, and makes multi-person telecommunicaiton possible: on a single Oculus Quest 2 mobile VR headset, 5 avatars are rendered in realtime in the same scene.



### CodedStereo: Learned Phase Masks for Large Depth-of-field Stereo
- **Arxiv ID**: http://arxiv.org/abs/2104.04641v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2104.04641v1)
- **Published**: 2021-04-09 23:44:52+00:00
- **Updated**: 2021-04-09 23:44:52+00:00
- **Authors**: Shiyu Tan, Yicheng Wu, Shoou-I Yu, Ashok Veeraraghavan
- **Comment**: Accepted to CVPR 2021 as an oral presentation
- **Journal**: None
- **Summary**: Conventional stereo suffers from a fundamental trade-off between imaging volume and signal-to-noise ratio (SNR) -- due to the conflicting impact of aperture size on both these variables. Inspired by the extended depth of field cameras, we propose a novel end-to-end learning-based technique to overcome this limitation, by introducing a phase mask at the aperture plane of the cameras in a stereo imaging system. The phase mask creates a depth-dependent point spread function, allowing us to recover sharp image texture and stereo correspondence over a significantly extended depth of field (EDOF) than conventional stereo. The phase mask pattern, the EDOF image reconstruction, and the stereo disparity estimation are all trained together using an end-to-end learned deep neural network. We perform theoretical analysis and characterization of the proposed approach and show a 6x increase in volume that can be imaged in simulation. We also build an experimental prototype and validate the approach using real-world results acquired using this prototype system.



