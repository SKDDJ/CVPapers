# Arxiv Papers in cs.CV on 2021-04-04
### Learning Neural Representation of Camera Pose with Matrix Representation of Pose Shift via View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2104.01508v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01508v3)
- **Published**: 2021-04-04 00:40:53+00:00
- **Updated**: 2021-05-09 00:04:40+00:00
- **Authors**: Yaxuan Zhu, Ruiqi Gao, Siyuan Huang, Song-Chun Zhu, Ying Nian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: How to effectively represent camera pose is an essential problem in 3D computer vision, especially in tasks such as camera pose regression and novel view synthesis. Traditionally, 3D position of the camera is represented by Cartesian coordinate and the orientation is represented by Euler angle or quaternions. These representations are manually designed, which may not be the most effective representation for downstream tasks. In this work, we propose an approach to learn neural representations of camera poses and 3D scenes, coupled with neural representations of local camera movements. Specifically, the camera pose and 3D scene are represented as vectors and the local camera movement is represented as a matrix operating on the vector of the camera pose. We demonstrate that the camera movement can further be parametrized by a matrix Lie algebra that underlies a rotation system in the neural space. The vector representations are then concatenated and generate the posed 2D image through a decoder network. The model is learned from only posed 2D images and corresponding camera poses, without access to depths or shapes. We conduct extensive experiments on synthetic and real datasets. The results show that compared with other camera pose representations, our learned representation is more robust to noise in novel view synthesis and more effective in camera pose regression.



### Detection of COVID-19 Disease using Deep Neural Networks with Ultrasound Imaging
- **Arxiv ID**: http://arxiv.org/abs/2104.01509v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01509v2)
- **Published**: 2021-04-04 00:53:06+00:00
- **Updated**: 2021-04-11 06:50:25+00:00
- **Authors**: Carlos Rojas-Azabache, Karen Vilca-Janampa, Renzo Guerrero-Huayta, Dennis Núñez-Fernández
- **Comment**: Practical ML for Developing Countries Workshop at ICLR 2021
- **Journal**: None
- **Summary**: The new coronavirus 2019 (COVID-2019) has rapidly become a pandemic and has had a devastating effect on both everyday life, public health and the global economy. It is critical to detect positive cases as early as possible to prevent the further spread of this epidemic and to treat affected patients quickly. The need for auxiliary diagnostic tools has increased as accurate automated tool kits are not available. This paper presents a work in progress that proposes the analysis of images of lung ultrasound scans using a convolutional neural network. The trained model will be used on a Raspberry Pi to predict on new images.



### PDWN: Pyramid Deformable Warping Network for Video Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2104.01517v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01517v1)
- **Published**: 2021-04-04 02:08:57+00:00
- **Updated**: 2021-04-04 02:08:57+00:00
- **Authors**: Zhiqi Chen, Ran Wang, Haojie Liu, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Video interpolation aims to generate a non-existent intermediate frame given the past and future frames. Many state-of-the-art methods achieve promising results by estimating the optical flow between the known frames and then generating the backward flows between the middle frame and the known frames. However, these methods usually suffer from the inaccuracy of estimated optical flows and require additional models or information to compensate for flow estimation errors. Following the recent development in using deformable convolution (DConv) for video interpolation, we propose a light but effective model, called Pyramid Deformable Warping Network (PDWN). PDWN uses a pyramid structure to generate DConv offsets of the unknown middle frame with respect to the known frames through coarse-to-fine successive refinements. Cost volumes between warped features are calculated at every pyramid level to help the offset inference. At the finest scale, the two warped frames are adaptively blended to generate the middle frame. Lastly, a context enhancement network further enhances the contextual detail of the final output. Ablation studies demonstrate the effectiveness of the coarse-to-fine offset refinement, cost volumes, and DConv. Our method achieves better or on-par accuracy compared to state-of-the-art models on multiple datasets while the number of model parameters and the inference time are substantially less than previous models. Moreover, we present an extension of the proposed framework to use four input frames, which can achieve significant improvement over using only two input frames, with only a slight increase in the model size and inference time.



### Generative Locally Linear Embedding
- **Arxiv ID**: http://arxiv.org/abs/2104.01525v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.01525v1)
- **Published**: 2021-04-04 02:59:39+00:00
- **Updated**: 2021-04-04 02:59:39+00:00
- **Authors**: Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley
- **Comment**: None
- **Journal**: None
- **Summary**: Locally Linear Embedding (LLE) is a nonlinear spectral dimensionality reduction and manifold learning method. It has two main steps which are linear reconstruction and linear embedding of points in the input space and embedding space, respectively. In this work, we propose two novel generative versions of LLE, named Generative LLE (GLLE), whose linear reconstruction steps are stochastic rather than deterministic. GLLE assumes that every data point is caused by its linear reconstruction weights as latent factors. The proposed GLLE algorithms can generate various LLE embeddings stochastically while all the generated embeddings relate to the original LLE embedding. We propose two versions for stochastic linear reconstruction, one using expectation maximization and another with direct sampling from a derived distribution by optimization. The proposed GLLE methods are closely related to and inspired by variational inference, factor analysis, and probabilistic principal component analysis. Our simulations show that the proposed GLLE methods work effectively in unfolding and generating submanifolds of data.



### Weakly-supervised Instance Segmentation via Class-agnostic Learning with Salient Images
- **Arxiv ID**: http://arxiv.org/abs/2104.01526v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01526v1)
- **Published**: 2021-04-04 03:01:52+00:00
- **Updated**: 2021-04-04 03:01:52+00:00
- **Authors**: Xinggang Wang, Jiapei Feng, Bin Hu, Qi Ding, Longjin Ran, Xiaoxin Chen, Wenyu Liu
- **Comment**: None
- **Journal**: CVPR 2021
- **Summary**: Humans have a strong class-agnostic object segmentation ability and can outline boundaries of unknown objects precisely, which motivates us to propose a box-supervised class-agnostic object segmentation (BoxCaseg) based solution for weakly-supervised instance segmentation. The BoxCaseg model is jointly trained using box-supervised images and salient images in a multi-task learning manner. The fine-annotated salient images provide class-agnostic and precise object localization guidance for box-supervised images. The object masks predicted by a pretrained BoxCaseg model are refined via a novel merged and dropped strategy as proxy ground truth to train a Mask R-CNN for weakly-supervised instance segmentation. Only using $7991$ salient images, the weakly-supervised Mask R-CNN is on par with fully-supervised Mask R-CNN on PASCAL VOC and significantly outperforms previous state-of-the-art box-supervised instance segmentation methods on COCO. The source code, pretrained models and datasets are available at \url{https://github.com/hustvl/BoxCaseg}.



### SGCN:Sparse Graph Convolution Network for Pedestrian Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2104.01528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01528v1)
- **Published**: 2021-04-04 03:17:42+00:00
- **Updated**: 2021-04-04 03:17:42+00:00
- **Authors**: Liushuai Shi, Le Wang, Chengjiang Long, Sanping Zhou, Mo Zhou, Zhenxing Niu, Gang Hua
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Pedestrian trajectory prediction is a key technology in autopilot, which remains to be very challenging due to complex interactions between pedestrians. However, previous works based on dense undirected interaction suffer from modeling superfluous interactions and neglect of trajectory motion tendency, and thus inevitably result in a considerable deviance from the reality. To cope with these issues, we present a Sparse Graph Convolution Network~(SGCN) for pedestrian trajectory prediction. Specifically, the SGCN explicitly models the sparse directed interaction with a sparse directed spatial graph to capture adaptive interaction pedestrians. Meanwhile, we use a sparse directed temporal graph to model the motion tendency, thus to facilitate the prediction based on the observed direction. Finally, parameters of a bi-Gaussian distribution for trajectory prediction are estimated by fusing the above two sparse graphs. We evaluate our proposed method on the ETH and UCY datasets, and the experimental results show our method outperforms comparative state-of-the-art methods by 9% in Average Displacement Error(ADE) and 13% in Final Displacement Error(FDE). Notably, visualizations indicate that our method can capture adaptive interactions between pedestrians and their effective motion tendencies.



### High-resolution Depth Maps Imaging via Attention-based Hierarchical Multi-modal Fusion
- **Arxiv ID**: http://arxiv.org/abs/2104.01530v3
- **DOI**: 10.1109/TIP.2021.3131041
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01530v3)
- **Published**: 2021-04-04 03:28:33+00:00
- **Updated**: 2021-12-10 03:02:01+00:00
- **Authors**: Zhiwei Zhong, Xianming Liu, Junjun Jiang, Debin Zhao, Zhiwen Chen, Xiangyang Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Depth map records distance between the viewpoint and objects in the scene, which plays a critical role in many real-world applications. However, depth map captured by consumer-grade RGB-D cameras suffers from low spatial resolution. Guided depth map super-resolution (DSR) is a popular approach to address this problem, which attempts to restore a high-resolution (HR) depth map from the input low-resolution (LR) depth and its coupled HR RGB image that serves as the guidance.   The most challenging problems for guided DSR are how to correctly select consistent structures and propagate them, and properly handle inconsistent ones. In this paper, we propose a novel attention-based hierarchical multi-modal fusion (AHMF) network for guided DSR. Specifically, to effectively extract and combine relevant information from LR depth and HR guidance, we propose a multi-modal attention based fusion (MMAF) strategy for hierarchical convolutional layers, including a feature enhance block to select valuable features and a feature recalibration block to unify the similarity metrics of modalities with different appearance characteristics. Furthermore, we propose a bi-directional hierarchical feature collaboration (BHFC) module to fully leverage low-level spatial information and high-level structure information among multi-scale features. Experimental results show that our approach outperforms state-of-the-art methods in terms of reconstruction accuracy, running speed and memory efficiency.



### Hierarchical Image Peeling: A Flexible Scale-space Filtering Framework
- **Arxiv ID**: http://arxiv.org/abs/2104.01534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01534v1)
- **Published**: 2021-04-04 04:08:14+00:00
- **Updated**: 2021-04-04 04:08:14+00:00
- **Authors**: Fu Yuanbin, Guoxiaojie, Hu Qiming, Lin Di, Ma Jiayi, Ling Haibin
- **Comment**: None
- **Journal**: None
- **Summary**: The importance of hierarchical image organization has been witnessed by a wide spectrum of applications in computer vision and graphics. Different from image segmentation with the spatial whole-part consideration, this work designs a modern framework for disassembling an image into a family of derived signals from a scale-space perspective. Specifically, we first offer a formal definition of image disassembly. Then, by concerning desired properties, such as peeling hierarchy and structure preservation, we convert the original complex problem into a series of two-component separation sub-problems, significantly reducing the complexity. The proposed framework is flexible to both supervised and unsupervised settings. A compact recurrent network, namely hierarchical image peeling net, is customized to efficiently and effectively fulfill the task, which is about 3.5Mb in size, and can handle 1080p images in more than 60 fps per recurrence on a GTX 2080Ti GPU, making it attractive for practical use. Both theoretical findings and experimental results are provided to demonstrate the efficacy of the proposed framework, reveal its superiority over other state-of-the-art alternatives, and show its potential to various applicable scenarios. Our code is available at \url{https://github.com/ForawardStar/HIPe}.



### A Modified Convolutional Network for Auto-encoding based on Pattern Theory Growth Function
- **Arxiv ID**: http://arxiv.org/abs/2104.02651v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.02651v1)
- **Published**: 2021-04-04 04:23:36+00:00
- **Updated**: 2021-04-04 04:23:36+00:00
- **Authors**: Erico Tjoa
- **Comment**: None
- **Journal**: None
- **Summary**: This brief paper reports the shortcoming of a variant of convolutional neural network whose components are developed based on the pattern theory framework.



### Performance analysis of facial recognition: A critical review through glass factor
- **Arxiv ID**: http://arxiv.org/abs/2104.01536v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.01536v1)
- **Published**: 2021-04-04 05:02:04+00:00
- **Updated**: 2021-04-04 05:02:04+00:00
- **Authors**: Jiashu He
- **Comment**: None
- **Journal**: None
- **Summary**: COVID-19 pandemic and social distancing urge a reliable human face recognition system in different abnormal situations. However, there is no research which studies the influence of glass factor in facial recognition system. This paper provides a comprehensive review of glass factor. The study contains two steps: data collection and accuracy test. Data collection includes collecting human face images through different situations, such as clear glasses, glass with water and glass with mist. Based on the collected data, an existing state-of-the-art face detection and recognition system built upon MTCNN and Inception V1 deep nets is tested for further analysis. Experimental data supports that 1) the system is robust for classification when comparing real-time images and 2) it fails at determining if two images are of same person by comparing real-time disturbed image with the frontal ones.



### Hypercorrelation Squeeze for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.01538v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01538v3)
- **Published**: 2021-04-04 05:27:13+00:00
- **Updated**: 2021-10-14 18:27:04+00:00
- **Authors**: Juhong Min, Dahyun Kang, Minsu Cho
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Few-shot semantic segmentation aims at learning to segment a target object from a query image using only a few annotated support images of the target class. This challenging task requires to understand diverse levels of visual cues and analyze fine-grained correspondence relations between the query and the support images. To address the problem, we propose Hypercorrelation Squeeze Networks (HSNet) that leverages multi-level feature correlation and efficient 4D convolutions. It extracts diverse features from different levels of intermediate convolutional layers and constructs a collection of 4D correlation tensors, i.e., hypercorrelations. Using efficient center-pivot 4D convolutions in a pyramidal architecture, the method gradually squeezes high-level semantic and low-level geometric cues of the hypercorrelation into precise segmentation masks in coarse-to-fine manner. The significant performance improvements on standard few-shot segmentation benchmarks of PASCAL-5i, COCO-20i, and FSS-1000 verify the efficacy of the proposed method.



### DINE: Domain Adaptation from Single and Multiple Black-box Predictors
- **Arxiv ID**: http://arxiv.org/abs/2104.01539v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.01539v3)
- **Published**: 2021-04-04 05:29:05+00:00
- **Updated**: 2022-03-31 04:56:35+00:00
- **Authors**: Jian Liang, Dapeng Hu, Jiashi Feng, Ran He
- **Comment**: CVPR2022 Camera Ready Version
- **Journal**: None
- **Summary**: To ease the burden of labeling, unsupervised domain adaptation (UDA) aims to transfer knowledge in previous and related labeled datasets (sources) to a new unlabeled dataset (target). Despite impressive progress, prior methods always need to access the raw source data and develop data-dependent alignment approaches to recognize the target samples in a transductive learning manner, which may raise privacy concerns from source individuals. Several recent studies resort to an alternative solution by exploiting the well-trained white-box model from the source domain, yet, it may still leak the raw data through generative adversarial learning. This paper studies a practical and interesting setting for UDA, where only black-box source models (i.e., only network predictions are available) are provided during adaptation in the target domain. To solve this problem, we propose a new two-step knowledge adaptation framework called DIstill and fine-tuNE (DINE). Taking into consideration the target data structure, DINE first distills the knowledge from the source predictor to a customized target model, then fine-tunes the distilled model to further fit the target domain. Besides, neural networks are not required to be identical across domains in DINE, even allowing effective adaptation on a low-resource device. Empirical results on three UDA scenarios (i.e., single-source, multi-source, and partial-set) confirm that DINE achieves highly competitive performance compared to state-of-the-art data-dependent approaches. Code is available at \url{https://github.com/tim-learn/DINE/}.



### Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations
- **Arxiv ID**: http://arxiv.org/abs/2104.01542v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01542v2)
- **Published**: 2021-04-04 05:46:37+00:00
- **Updated**: 2021-07-21 14:39:06+00:00
- **Authors**: Zhenyu Jiang, Yifeng Zhu, Maxwell Svetlik, Kuan Fang, Yuke Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Grasp detection in clutter requires the robot to reason about the 3D scene from incomplete and noisy perception. In this work, we draw insight that 3D reconstruction and grasp learning are two intimately connected tasks, both of which require a fine-grained understanding of local geometry details. We thus propose to utilize the synergies between grasp affordance and 3D reconstruction through multi-task learning of a shared representation. Our model takes advantage of deep implicit functions, a continuous and memory-efficient representation, to enable differentiable training of both tasks. We train the model on self-supervised grasp trials data in simulation. Evaluation is conducted on a clutter removal task, where the robot clears cluttered objects by grasping them one at a time. The experimental results in simulation and on the real robot have demonstrated that the use of implicit neural representations and joint learning of grasp affordance and 3D reconstruction have led to state-of-the-art grasping results. Our method outperforms baselines by over 10% in terms of grasp success rate. Additional results and videos can be found at https://sites.google.com/view/rpl-giga2021



### Graph Sampling Based Deep Metric Learning for Generalizable Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2104.01546v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01546v4)
- **Published**: 2021-04-04 06:44:15+00:00
- **Updated**: 2022-04-06 10:52:30+00:00
- **Authors**: Shengcai Liao, Ling Shao
- **Comment**: This paper has been accepted by CVPR 2022
- **Journal**: None
- **Summary**: Recent studies show that, both explicit deep feature matching as well as large-scale and diverse training data can significantly improve the generalization of person re-identification. However, the efficiency of learning deep matchers on large-scale data has not yet been adequately studied. Though learning with classification parameters or class memory is a popular way, it incurs large memory and computational costs. In contrast, pairwise deep metric learning within mini batches would be a better choice. However, the most popular random sampling method, the well-known PK sampler, is not informative and efficient for deep metric learning. Though online hard example mining has improved the learning efficiency to some extent, the mining in mini batches after random sampling is still limited. This inspires us to explore the use of hard example mining earlier, in the data sampling stage. To do so, in this paper, we propose an efficient mini-batch sampling method, called graph sampling (GS), for large-scale deep metric learning. The basic idea is to build a nearest neighbor relationship graph for all classes at the beginning of each epoch. Then, each mini batch is composed of a randomly selected class and its nearest neighboring classes so as to provide informative and challenging examples for learning. Together with an adapted competitive baseline, we improve the state of the art in generalizable person re-identification significantly, by 25.1% in Rank-1 on MSMT17 when trained on RandPerson. Besides, the proposed method also outperforms the competitive baseline, by 6.8% in Rank-1 on CUHK03-NP when trained on MSMT17. Meanwhile, the training time is significantly reduced, from 25.4 hours to 2 hours when trained on RandPerson with 8,000 identities. Code is available at https://github.com/ShengcaiLiao/QAConv.



### Scene Text Retrieval via Joint Text Detection and Similarity Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.01552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01552v1)
- **Published**: 2021-04-04 07:18:38+00:00
- **Updated**: 2021-04-04 07:18:38+00:00
- **Authors**: Hao Wang, Xiang Bai, Mingkun Yang, Shenggao Zhu, Jing Wang, Wenyu Liu
- **Comment**: Accepted to CVPR 2021. Code is available at:
  https://github.com/lanfeng4659/STR-TDSL
- **Journal**: None
- **Summary**: Scene text retrieval aims to localize and search all text instances from an image gallery, which are the same or similar to a given query text. Such a task is usually realized by matching a query text to the recognized words, outputted by an end-to-end scene text spotter. In this paper, we address this problem by directly learning a cross-modal similarity between a query text and each text instance from natural images. Specifically, we establish an end-to-end trainable network, jointly optimizing the procedures of scene text detection and cross-modal similarity learning. In this way, scene text retrieval can be simply performed by ranking the detected text instances with the learned similarity. Experiments on three benchmark datasets demonstrate our method consistently outperforms the state-of-the-art scene text spotting/retrieval approaches. In particular, the proposed framework of joint detection and similarity learning achieves significantly better performance than separated methods. Code is available at: https://github.com/lanfeng4659/STR-TDSL.



### Synthesizing MR Image Contrast Enhancement Using 3D High-resolution ConvNets
- **Arxiv ID**: http://arxiv.org/abs/2104.01592v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01592v3)
- **Published**: 2021-04-04 11:54:15+00:00
- **Updated**: 2022-07-16 15:28:53+00:00
- **Authors**: Chao Chen, Catalina Raymond, Bill Speier, Xinyu Jin, Timothy F. Cloughesy, Dieter Enzmann, Benjamin M. Ellingson, Corey W. Arnold
- **Comment**: This paper is accpted by IEEE TBME, Code is available at
  \url{https://github.com/chenchao666/Contrast-enhanced-MRI-Synthesis}
- **Journal**: None
- **Summary**: \textit{Objective:} Gadolinium-based contrast agents (GBCAs) have been widely used to better visualize disease in brain magnetic resonance imaging (MRI). However, gadolinium deposition within the brain and body has raised safety concerns about the use of GBCAs. Therefore, the development of novel approaches that can decrease or even eliminate GBCA exposure while providing similar contrast information would be of significant use clinically. \textit{Methods:} In this work, we present a deep learning based approach for contrast-enhanced T1 synthesis on brain tumor patients. A 3D high-resolution fully convolutional network (FCN), which maintains high resolution information through processing and aggregates multi-scale information in parallel, is designed to map pre-contrast MRI sequences to contrast-enhanced MRI sequences. Specifically, three pre-contrast MRI sequences, T1, T2 and apparent diffusion coefficient map (ADC), are utilized as inputs and the post-contrast T1 sequences are utilized as target output. To alleviate the data imbalance problem between normal tissues and the tumor regions, we introduce a local loss to improve the contribution of the tumor regions, which leads to better enhancement results on tumors. \textit{Results:} Extensive quantitative and visual assessments are performed, with our proposed model achieving a PSNR of 28.24dB in the brain and 21.2dB in tumor regions. \textit{Conclusion and Significance:} Our results suggest the potential of substituting GBCAs with synthetic contrast images generated via deep learning. Code is available at \url{https://github.com/chenchao666/Contrast-enhanced-MRI-Synthesis



### Towards Rolling Shutter Correction and Deblurring in Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2104.01601v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01601v1)
- **Published**: 2021-04-04 12:36:48+00:00
- **Updated**: 2021-04-04 12:36:48+00:00
- **Authors**: Zhihang Zhong, Yinqiang Zheng, Imari Sato
- **Comment**: To be published in CVPR 2021
- **Journal**: None
- **Summary**: Joint rolling shutter correction and deblurring (RSCD) techniques are critical for the prevalent CMOS cameras. However, current approaches are still based on conventional energy optimization and are developed for static scenes. To enable learning-based approaches to address real-world RSCD problem, we contribute the first dataset, BS-RSCD, which includes both ego-motion and object-motion in dynamic scenes. Real distorted and blurry videos with corresponding ground truth are recorded simultaneously via a beam-splitter-based acquisition system.   Since direct application of existing individual rolling shutter correction (RSC) or global shutter deblurring (GSD) methods on RSCD leads to undesirable results due to inherent flaws in the network architecture, we further present the first learning-based model (JCD) for RSCD. The key idea is that we adopt bi-directional warping streams for displacement compensation, while also preserving the non-warped deblurring stream for details restoration. The experimental results demonstrate that JCD achieves state-of-the-art performance on the realistic RSCD dataset (BS-RSCD) and the synthetic RSC dataset (Fastec-RS). The dataset and code are available at https://github.com/zzh-tech/RSCD.



### OnTarget: An Electronic Archery Scoring
- **Arxiv ID**: http://arxiv.org/abs/2104.01622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01622v1)
- **Published**: 2021-04-04 15:02:32+00:00
- **Updated**: 2021-04-04 15:02:32+00:00
- **Authors**: Andreea Danielescu
- **Comment**: None
- **Journal**: None
- **Summary**: There are several challenges in creating an electronic archery scoring system using computer vision techniques. Variability of light, reconstruction of the target from several images, variability of target configuration, and filtering noise were significant challenges during the creation of this scoring system. This paper discusses the approach used to determine where an arrow hits a target, for any possible single or set of targets and provides an algorithm that balances the difficulty of robust arrow detection while retaining the required accuracy.



### MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.01633v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01633v1)
- **Published**: 2021-04-04 15:47:14+00:00
- **Updated**: 2021-04-04 15:47:14+00:00
- **Authors**: Jia-Chang Feng, Fa-Ting Hong, Wei-Shi Zheng
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Weakly supervised video anomaly detection (WS-VAD) is to distinguish anomalies from normal events based on discriminative representations. Most existing works are limited in insufficient video representations. In this work, we develop a multiple instance self-training framework (MIST)to efficiently refine task-specific discriminative representations with only video-level annotations. In particular, MIST is composed of 1) a multiple instance pseudo label generator, which adapts a sparse continuous sampling strategy to produce more reliable clip-level pseudo labels, and 2) a self-guided attention boosted feature encoder that aims to automatically focus on anomalous regions in frames while extracting task-specific representations. Moreover, we adopt a self-training scheme to optimize both components and finally obtain a task-specific feature encoder. Extensive experiments on two public datasets demonstrate the efficacy of our method, and our method performs comparably to or even better than existing supervised and weakly supervised methods, specifically obtaining a frame-level AUC 94.83% on ShanghaiTech.



### TATL: Task Agnostic Transfer Learning for Skin Attributes Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.01641v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01641v2)
- **Published**: 2021-04-04 16:24:51+00:00
- **Updated**: 2022-01-28 02:09:28+00:00
- **Authors**: Duy M. H. Nguyen, Thu T. Nguyen, Huong Vu, Quang Pham, Manh-Duy Nguyen, Binh T. Nguyen, Daniel Sonntag
- **Comment**: This version has been accepted at Medical Image Analysis
- **Journal**: None
- **Summary**: Existing skin attributes detection methods usually initialize with a pre-trained Imagenet network and then fine-tune on a medical target task. However, we argue that such approaches are suboptimal because medical datasets are largely different from ImageNet and often contain limited training samples. In this work, we propose \emph{Task Agnostic Transfer Learning (TATL)}, a novel framework motivated by dermatologists' behaviors in the skincare context. TATL learns an attribute-agnostic segmenter that detects lesion skin regions and then transfers this knowledge to a set of attribute-specific classifiers to detect each particular attribute. Since TATL's attribute-agnostic segmenter only detects skin attribute regions, it enjoys ample data from all attributes, allows transferring knowledge among features, and compensates for the lack of training data from rare attributes. We conduct extensive experiments to evaluate the proposed TATL transfer learning mechanism with various neural network architectures on two popular skin attributes detection benchmarks. The empirical results show that TATL not only works well with multiple architectures but also can achieve state-of-the-art performances while enjoying minimal model and computational complexities. We also provide theoretical insights and explanations for why our transfer learning framework performs well in practice.



### Towards Semantic Interpretation of Thoracic Disease and COVID-19 Diagnosis Models
- **Arxiv ID**: http://arxiv.org/abs/2104.02481v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.02481v2)
- **Published**: 2021-04-04 17:35:13+00:00
- **Updated**: 2021-08-04 15:30:28+00:00
- **Authors**: Ashkan Khakzar, Sabrina Musatian, Jonas Buchberger, Icxel Valeriano Quiroz, Nikolaus Pinger, Soroosh Baselizadeh, Seong Tae Kim, Nassir Navab
- **Comment**: Accepted in MICCAI 2021 (Medical Image Computing and Computer
  Assisted Intervention 2021) ---- Project website:
  https://camp-explain-ai.github.io/CheXplain-Dissection/
- **Journal**: None
- **Summary**: Convolutional neural networks are showing promise in the automatic diagnosis of thoracic pathologies on chest x-rays. Their black-box nature has sparked many recent works to explain the prediction via input feature attribution methods (aka saliency methods). However, input feature attribution methods merely identify the importance of input regions for the prediction and lack semantic interpretation of model behavior. In this work, we first identify the semantics associated with internal units (feature maps) of the network. We proceed to investigate the following questions; Does a regression model that is only trained with COVID-19 severity scores implicitly learn visual patterns associated with thoracic pathologies? Does a network that is trained on weakly labeled data (e.g. healthy, unhealthy) implicitly learn pathologies? Moreover, we investigate the effect of pretraining and data imbalance on the interpretability of learned features. In addition to the analysis, we propose semantic attribution to semantically explain each prediction. We present our findings using publicly available chest pathologies (CheXpert, NIH ChestX-ray8) and COVID-19 datasets (BrixIA, and COVID-19 chest X-ray segmentation dataset). The Code is publicly available.



### Faster Convolution Inference Through Using Pre-Calculated Lookup Tables
- **Arxiv ID**: http://arxiv.org/abs/2104.01681v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01681v1)
- **Published**: 2021-04-04 20:09:20+00:00
- **Updated**: 2021-04-04 20:09:20+00:00
- **Authors**: Grigor Gatchev, Valentin Mollov
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Low-cardinality activations permit an algorithm based on fetching the inference values from pre-calculated lookup tables instead of calculating them every time. This algorithm can have extensions, some of which offer abilities beyond those of the currently used algorithms. It also allows for a simpler and more effective CNN-specialized hardware.



### 3D Convolutional Neural Networks for Stalled Brain Capillary Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.01687v2
- **DOI**: 10.1016/j.compbiomed.2021.105089
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01687v2)
- **Published**: 2021-04-04 20:30:14+00:00
- **Updated**: 2022-02-14 14:55:01+00:00
- **Authors**: Roman Solovyev, Alexandr A. Kalinin, Tatiana Gabruseva
- **Comment**: None
- **Journal**: Computers in biology and medicine. 2022
- **Summary**: Adequate blood supply is critical for normal brain function. Brain vasculature dysfunctions such as stalled blood flow in cerebral capillaries are associated with cognitive decline and pathogenesis in Alzheimer's disease. Recent advances in imaging technology enabled generation of high-quality 3D images that can be used to visualize stalled blood vessels. However, localization of stalled vessels in 3D images is often required as the first step for downstream analysis, which can be tedious, time-consuming and error-prone, when done manually. Here, we describe a deep learning-based approach for automatic detection of stalled capillaries in brain images based on 3D convolutional neural networks. Our networks employed custom 3D data augmentations and were used weight transfer from pre-trained 2D models for initialization. We used an ensemble of several 3D models to produce the winning submission to the Clog Loss: Advance Alzheimer's Research with Stall Catchers machine learning competition that challenged the participants with classifying blood vessels in 3D image stacks as stalled or flowing. In this setting, our approach outperformed other methods and demonstrated state-of-the-art results, achieving 0.85 Matthews correlation coefficient, 85% sensitivity, and 99.3% specificity. The source code for our solution is made publicly available.



### FixMyPose: Pose Correctional Captioning and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2104.01703v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01703v1)
- **Published**: 2021-04-04 21:45:44+00:00
- **Updated**: 2021-04-04 21:45:44+00:00
- **Authors**: Hyounghun Kim, Abhay Zala, Graham Burri, Mohit Bansal
- **Comment**: AAAI 2021 (18 pages, 16 figures; webpage:
  https://fixmypose-unc.github.io/)
- **Journal**: None
- **Summary**: Interest in physical therapy and individual exercises such as yoga/dance has increased alongside the well-being trend. However, such exercises are hard to follow without expert guidance (which is impossible to scale for personalized feedback to every trainee remotely). Thus, automated pose correction systems are required more than ever, and we introduce a new captioning dataset named FixMyPose to address this need. We collect descriptions of correcting a "current" pose to look like a "target" pose (in both English and Hindi). The collected descriptions have interesting linguistic properties such as egocentric relations to environment objects, analogous references, etc., requiring an understanding of spatial relations and commonsense knowledge about postures. Further, to avoid ML biases, we maintain a balance across characters with diverse demographics, who perform a variety of movements in several interior environments (e.g., homes, offices). From our dataset, we introduce the pose-correctional-captioning task and its reverse target-pose-retrieval task. During the correctional-captioning task, models must generate descriptions of how to move from the current to target pose image, whereas in the retrieval task, models should select the correct target pose given the initial pose and correctional description. We present strong cross-attention baseline models (uni/multimodal, RL, multilingual) and also show that our baselines are competitive with other models when evaluated on other image-difference datasets. We also propose new task-specific metrics (object-match, body-part-match, direction-match) and conduct human evaluation for more reliable evaluation, and we demonstrate a large human-model performance gap suggesting room for promising future work. To verify the sim-to-real transfer of our FixMyPose dataset, we collect a set of real images and show promising performance on these images.



