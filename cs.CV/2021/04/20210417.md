# Arxiv Papers in cs.CV on 2021-04-17
### FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling
- **Arxiv ID**: http://arxiv.org/abs/2104.08418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08418v1)
- **Published**: 2021-04-17 01:38:54+00:00
- **Updated**: 2021-04-17 01:38:54+00:00
- **Authors**: Christopher Xie, Keunhong Park, Ricardo Martin-Brualla, Matthew Brown
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate the use of Neural Radiance Fields (NeRF) to learn high quality 3D object category models from collections of input images. In contrast to previous work, we are able to do this whilst simultaneously separating foreground objects from their varying backgrounds. We achieve this via a 2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a geometrically constant background and a deformable foreground that represents the object category. We show that this method can learn accurate 3D object category models using only photometric supervision and casually captured images of the objects. Additionally, our 2-part decomposition allows the model to perform accurate and crisp amodal segmentation. We quantitatively evaluate our method with view synthesis and image fidelity metrics, using synthetic, lab-captured, and in-the-wild data. Our results demonstrate convincing 3D object category modelling that exceed the performance of existing methods.



### Fashion-Guided Adversarial Attack on Person Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.08422v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08422v2)
- **Published**: 2021-04-17 02:17:33+00:00
- **Updated**: 2021-04-20 02:16:48+00:00
- **Authors**: Marc Treu, Trung-Nghia Le, Huy H. Nguyen, Junichi Yamagishi, Isao Echizen
- **Comment**: Accepted to Workshop on Media Forensics, CVPR 2021. Project page:
  https://github.com/nii-yamagishilab/fashion_adv
- **Journal**: CVPR Workshops 2021
- **Summary**: This paper presents the first adversarial example based method for attacking human instance segmentation networks, namely person segmentation networks in short, which are harder to fool than classification networks. We propose a novel Fashion-Guided Adversarial Attack (FashionAdv) framework to automatically identify attackable regions in the target image to minimize the effect on image quality. It generates adversarial textures learned from fashion style images and then overlays them on the clothing regions in the original image to make all persons in the image invisible to person segmentation networks. The synthesized adversarial textures are inconspicuous and appear natural to the human eye. The effectiveness of the proposed method is enhanced by robustness training and by jointly attacking multiple components of the target network. Extensive experiments demonstrated the effectiveness of FashionAdv in terms of robustness to image manipulations and storage in cyberspace as well as appearing natural to the human eye. The code and data are publicly released on our project page https://github.com/nii-yamagishilab/fashion_adv



### Towards Efficient Convolutional Network Models with Filter Distribution Templates
- **Arxiv ID**: http://arxiv.org/abs/2104.08446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08446v1)
- **Published**: 2021-04-17 04:51:29+00:00
- **Updated**: 2021-04-17 04:51:29+00:00
- **Authors**: Ramon Izquierdo-Cordova, Walterio Mayol-Cuevas
- **Comment**: None
- **Journal**: None
- **Summary**: Increasing number of filters in deeper layers when feature maps are decreased is a widely adopted pattern in convolutional network design. It can be found in classical CNN architectures and in automatic discovered models. Even CNS methods commonly explore a selection of multipliers derived from this pyramidal pattern. We defy this practice by introducing a small set of templates consisting of easy to implement, intuitive and aggressive variations of the original pyramidal distribution of filters in VGG and ResNet architectures. Experiments on CIFAR, CINIC10 and TinyImagenet datasets show that models produced by our templates, are more efficient in terms of fewer parameters and memory needs.



### Gaze Perception in Humans and CNN-Based Model
- **Arxiv ID**: http://arxiv.org/abs/2104.08447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08447v1)
- **Published**: 2021-04-17 04:52:46+00:00
- **Updated**: 2021-04-17 04:52:46+00:00
- **Authors**: Nicole X. Han, William Yang Wang, Miguel P. Eckstein
- **Comment**: None
- **Journal**: None
- **Summary**: Making accurate inferences about other individuals' locus of attention is essential for human social interactions and will be important for AI to effectively interact with humans. In this study, we compare how a CNN (convolutional neural network) based model of gaze and humans infer the locus of attention in images of real-world scenes with a number of individuals looking at a common location. We show that compared to the model, humans' estimates of the locus of attention are more influenced by the context of the scene, such as the presence of the attended target and the number of individuals in the image.



### A Surface Geometry Model for LiDAR Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/2104.08466v1
- **DOI**: 10.1109/LRA.2021.3068885
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.08466v1)
- **Published**: 2021-04-17 06:48:01+00:00
- **Updated**: 2021-04-17 06:48:01+00:00
- **Authors**: Yiming Zhao, Lin Bai, Ziming Zhang, Xinming Huang
- **Comment**: IEEE Robotics and Automation Letters (2021). Code link:
  https://github.com/placeforyiming/RAL_Non-Learning_DepthCompletion
- **Journal**: None
- **Summary**: LiDAR depth completion is a task that predicts depth values for every pixel on the corresponding camera frame, although only sparse LiDAR points are available. Most of the existing state-of-the-art solutions are based on deep neural networks, which need a large amount of data and heavy computations for training the models. In this letter, a novel non-learning depth completion method is proposed by exploiting the local surface geometry that is enhanced by an outlier removal algorithm. The proposed surface geometry model is inspired by the observation that most pixels with unknown depth have a nearby LiDAR point. Therefore, it is assumed those pixels share the same surface with the nearest LiDAR point, and their respective depth can be estimated as the nearest LiDAR depth value plus a residual error. The residual error is calculated by using a derived equation with several physical parameters as input, including the known camera intrinsic parameters, estimated normal vector, and offset distance on the image plane. The proposed method is further enhanced by an outlier removal algorithm that is designed to remove incorrectly mapped LiDAR points from occluded regions. On KITTI dataset, the proposed solution achieves the best error performance among all existing non-learning methods and is comparable to the best self-supervised learning method and some supervised learning methods. Moreover, since outlier points from occluded regions is a commonly existing problem, the proposed outlier removal algorithm is a general preprocessing step that is applicable to many robotic systems with both camera and LiDAR sensors.



### Semi-Supervised Multi-Modal Multi-Instance Multi-Label Deep Network with Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/2104.08489v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.08489v1)
- **Published**: 2021-04-17 09:18:28+00:00
- **Updated**: 2021-04-17 09:18:28+00:00
- **Authors**: Yang Yang, Zhao-Yang Fu, De-Chuan Zhan, Zhi-Bin Liu, Yuan Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Complex objects are usually with multiple labels, and can be represented by multiple modal representations, e.g., the complex articles contain text and image information as well as multiple annotations. Previous methods assume that the homogeneous multi-modal data are consistent, while in real applications, the raw data are disordered, e.g., the article constitutes with variable number of inconsistent text and image instances. Therefore, Multi-modal Multi-instance Multi-label (M3) learning provides a framework for handling such task and has exhibited excellent performance. However, M3 learning is facing two main challenges: 1) how to effectively utilize label correlation; 2) how to take advantage of multi-modal learning to process unlabeled instances. To solve these problems, we first propose a novel Multi-modal Multi-instance Multi-label Deep Network (M3DN), which considers M3 learning in an end-to-end multi-modal deep network and utilizes consistency principle among different modal bag-level predictions. Based on the M3DN, we learn the latent ground label metric with the optimal transport. Moreover, we introduce the extrinsic unlabeled multi-modal multi-instance data, and propose the M3DNS, which considers the instance-level auto-encoder for single modality and modified bag-level optimal transport to strengthen the consistency among modalities. Thereby M3DNS can better predict label and exploit label correlation simultaneously. Experiments on benchmark datasets and real world WKG Game-Hub dataset validate the effectiveness of the proposed methods.



### Vision Transformer Pruning
- **Arxiv ID**: http://arxiv.org/abs/2104.08500v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08500v4)
- **Published**: 2021-04-17 09:49:24+00:00
- **Updated**: 2021-08-14 06:06:37+00:00
- **Authors**: Mingjian Zhu, Yehui Tang, Kai Han
- **Comment**: Accepted by the KDD 2021 Workshop on Model Mining
- **Journal**: None
- **Summary**: Vision transformer has achieved competitive performance on a variety of computer vision applications. However, their storage, run-time memory, and computational demands are hindering the deployment to mobile devices. Here we present a vision transformer pruning approach, which identifies the impacts of dimensions in each layer of transformer and then executes pruning accordingly. By encouraging dimension-wise sparsity in the transformer, important dimensions automatically emerge. A great number of dimensions with small importance scores can be discarded to achieve a high pruning ratio without significantly compromising accuracy. The pipeline for vision transformer pruning is as follows: 1) training with sparsity regularization; 2) pruning dimensions of linear projections; 3) fine-tuning. The reduced parameters and FLOPs ratios of the proposed algorithm are well evaluated and analyzed on ImageNet dataset to demonstrate the effectiveness of our proposed method.



### Visually Guided Sound Source Separation and Localization using Self-Supervised Motion Representations
- **Arxiv ID**: http://arxiv.org/abs/2104.08506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08506v1)
- **Published**: 2021-04-17 10:09:15+00:00
- **Updated**: 2021-04-17 10:09:15+00:00
- **Authors**: Lingyu Zhu, Esa Rahtu
- **Comment**: 18 pages. main paper: 8 pages; reference: 2 pages; supplementary
  material: 8 pages
- **Journal**: None
- **Summary**: The objective of this paper is to perform audio-visual sound source separation, i.e.~to separate component audios from a mixture based on the videos of sound sources. Moreover, we aim to pinpoint the source location in the input video sequence. Recent works have shown impressive audio-visual separation results when using prior knowledge of the source type (e.g. human playing instrument) and pre-trained motion detectors (e.g. keypoints or optical flows). However, at the same time, the models are limited to a certain application domain. In this paper, we address these limitations and make the following contributions: i) we propose a two-stage architecture, called Appearance and Motion network (AMnet), where the stages specialise to appearance and motion cues, respectively. The entire system is trained in a self-supervised manner; ii) we introduce an Audio-Motion Embedding (AME) framework to explicitly represent the motions that related to sound; iii) we propose an audio-motion transformer architecture for audio and motion feature fusion; iv) we demonstrate state-of-the-art performance on two challenging datasets (MUSIC-21 and AVE) despite the fact that we do not use any pre-trained keypoint detectors or optical flow estimators. Project page: https://ly-zhu.github.io/self-supervised-motion-representations



### Exploring Deep Learning for Joint Audio-Visual Lip Biometrics
- **Arxiv ID**: http://arxiv.org/abs/2104.08510v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2104.08510v1)
- **Published**: 2021-04-17 10:51:55+00:00
- **Updated**: 2021-04-17 10:51:55+00:00
- **Authors**: Meng Liu, Longbiao Wang, Kong Aik Lee, Hanyi Zhang, Chang Zeng, Jianwu Dang
- **Comment**: None
- **Journal**: None
- **Summary**: Audio-visual (AV) lip biometrics is a promising authentication technique that leverages the benefits of both the audio and visual modalities in speech communication. Previous works have demonstrated the usefulness of AV lip biometrics. However, the lack of a sizeable AV database hinders the exploration of deep-learning-based audio-visual lip biometrics. To address this problem, we compile a moderate-size database using existing public databases. Meanwhile, we establish the DeepLip AV lip biometrics system realized with a convolutional neural network (CNN) based video module, a time-delay neural network (TDNN) based audio module, and a multimodal fusion module. Our experiments show that DeepLip outperforms traditional speaker recognition models in context modeling and achieves over 50% relative improvements compared with our best single modality baseline, with an equal error rate of 0.75% and 1.11% on the test datasets, respectively.



### Efficient Screening of Diseased Eyes based on Fundus Autofluorescence Images using Support Vector Machine
- **Arxiv ID**: http://arxiv.org/abs/2104.08519v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.08519v1)
- **Published**: 2021-04-17 11:54:34+00:00
- **Updated**: 2021-04-17 11:54:34+00:00
- **Authors**: Shanmukh Reddy Manne, Kiran Kumar Vupparaboina, Gowtham Chowdary Gudapati, Ram Anudeep Peddoju, Chandra Prakash Konkimalla, Abhilash Goud, Sarforaz Bin Bashar, Jay Chhablani, Soumya Jana
- **Comment**: None
- **Journal**: None
- **Summary**: A variety of vision ailments are associated with geographic atrophy (GA) in the foveal region of the eye. In current clinical practice, the ophthalmologist manually detects potential presence of such GA based on fundus autofluorescence (FAF) images, and hence diagnoses the disease, when relevant. However, in view of the general scarcity of ophthalmologists relative to the large number of subjects seeking eyecare, especially in remote regions, it becomes imperative to develop methods to direct expert time and effort to medically significant cases. Further, subjects from either disadvantaged background or remote localities, who face considerable economic/physical barrier in consulting trained ophthalmologists, tend to seek medical attention only after being reasonably certain that an adverse condition exists. To serve the interest of both the ophthalmologist and the potential patient, we plan a screening step, where healthy and diseased eyes are algorithmically differentiated with limited input from only optometrists who are relatively more abundant in number. Specifically, an early treatment diabetic retinopathy study (ETDRS) grid is placed by an optometrist on each FAF image, based on which sectoral statistics are automatically collected. Using such statistics as features, healthy and diseased eyes are proposed to be classified by training an algorithm using available medical records. In this connection, we demonstrate the efficacy of support vector machines (SVM). Specifically, we consider SVM with linear as well as radial basis function (RBF) kernel, and observe satisfactory performance of both variants. Among those, we recommend the latter in view of its slight superiority in terms of classification accuracy (90.55% at a standard training-to-test ratio of 80:20), and practical class-conditional costs.



### PARE: Part Attention Regressor for 3D Human Body Estimation
- **Arxiv ID**: http://arxiv.org/abs/2104.08527v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08527v2)
- **Published**: 2021-04-17 12:42:56+00:00
- **Updated**: 2021-10-11 18:06:13+00:00
- **Authors**: Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges, Michael J. Black
- **Comment**: None
- **Journal**: None
- **Summary**: Despite significant progress, we show that state of the art 3D human pose and shape estimation methods remain sensitive to partial occlusion and can produce dramatically wrong predictions although much of the body is observable. To address this, we introduce a soft attention mechanism, called the Part Attention REgressor (PARE), that learns to predict body-part-guided attention masks. We observe that state-of-the-art methods rely on global feature representations, making them sensitive to even small occlusions. In contrast, PARE's part-guided attention mechanism overcomes these issues by exploiting information about the visibility of individual body parts while leveraging information from neighboring body-parts to predict occluded parts. We show qualitatively that PARE learns sensible attention masks, and quantitative evaluation confirms that PARE achieves more accurate and robust reconstruction results than existing approaches on both occlusion-specific and standard benchmarks. The code and data are available for research purposes at {\small \url{https://pare.is.tue.mpg.de/}}



### Cycle-free CycleGAN using Invertible Generator for Unsupervised Low-Dose CT Denoising
- **Arxiv ID**: http://arxiv.org/abs/2104.08538v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.08538v1)
- **Published**: 2021-04-17 13:23:36+00:00
- **Updated**: 2021-04-17 13:23:36+00:00
- **Authors**: Taesung Kwon, Jong Chul Ye
- **Comment**: 12 pages, 12 figures
- **Journal**: None
- **Summary**: Recently, CycleGAN was shown to provide high-performance, ultra-fast denoising for low-dose X-ray computed tomography (CT) without the need for a paired training dataset. Although this was possible thanks to cycle consistency, CycleGAN requires two generators and two discriminators to enforce cycle consistency, demanding significant GPU resources and technical skills for training. A recent proposal of tunable CycleGAN with Adaptive Instance Normalization (AdaIN) alleviates the problem in part by using a single generator. However, two discriminators and an additional AdaIN code generator are still required for training. To solve this problem, here we present a novel cycle-free Cycle-GAN architecture, which consists of a single generator and a discriminator but still guarantees cycle consistency. The main innovation comes from the observation that the use of an invertible generator automatically fulfills the cycle consistency condition and eliminates the additional discriminator in the CycleGAN formulation. To make the invertible generator more effective, our network is implemented in the wavelet residual domain. Extensive experiments using various levels of low-dose CT images confirm that our method can significantly improve denoising performance using only 10% of learnable parameters and faster training time compared to the conventional CycleGAN.



### TransVG: End-to-End Visual Grounding with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.08541v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08541v4)
- **Published**: 2021-04-17 13:35:24+00:00
- **Updated**: 2022-01-14 14:46:13+00:00
- **Authors**: Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, Houqiang Li
- **Comment**: This paper has been accepted by ICCV2021
- **Journal**: None
- **Summary**: In this paper, we present a neat yet effective transformer-based framework for visual grounding, namely TransVG, to address the task of grounding a language query to the corresponding region onto an image. The state-of-the-art methods, including two-stage or one-stage ones, rely on a complex module with manually-designed mechanisms to perform the query reasoning and multi-modal fusion. However, the involvement of certain mechanisms in fusion module design, such as query decomposition and image scene graph, makes the models easily overfit to datasets with specific scenarios, and limits the plenitudinous interaction between the visual-linguistic context. To avoid this caveat, we propose to establish the multi-modal correspondence by leveraging transformers, and empirically show that the complex fusion modules e.g., modular attention network, dynamic graph, and multi-modal tree) can be replaced by a simple stack of transformer encoder layers with higher performance. Moreover, we re-formulate the visual grounding as a direct coordinates regression problem and avoid making predictions out of a set of candidates i.e., region proposals or anchor boxes). Extensive experiments are conducted on five widely used datasets, and a series of state-of-the-art records are set by our TransVG. We build the benchmark of transformer-based visual grounding framework and make the code available at \url{https://github.com/djiajunustc/TransVG}.



### Objective-Dependent Uncertainty Driven Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.08554v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08554v1)
- **Published**: 2021-04-17 14:17:09+00:00
- **Updated**: 2021-04-17 14:17:09+00:00
- **Authors**: Suraj Mishra, Danny Z. Chen, X. Sharon Hu
- **Comment**: ISBI 2021
- **Journal**: None
- **Summary**: From diagnosing neovascular diseases to detecting white matter lesions, accurate tiny vessel segmentation in fundus images is critical. Promising results for accurate vessel segmentation have been known. However, their effectiveness in segmenting tiny vessels is still limited. In this paper, we study retinal vessel segmentation by incorporating tiny vessel segmentation into our framework for the overall accurate vessel segmentation. To achieve this, we propose a new deep convolutional neural network (CNN) which divides vessel segmentation into two separate objectives. Specifically, we consider the overall accurate vessel segmentation and tiny vessel segmentation as two individual objectives. Then, by exploiting the objective-dependent (homoscedastic) uncertainty, we enable the network to learn both objectives simultaneously. Further, to improve the individual objectives, we propose: (a) a vessel weight map based auxiliary loss for enhancing tiny vessel connectivity (i.e., improving tiny vessel segmentation), and (b) an enhanced encoder-decoder architecture for improved localization (i.e., for accurate vessel segmentation). Using 3 public retinal vessel segmentation datasets (CHASE_DB1, DRIVE, and STARE), we verify the superiority of our proposed framework in segmenting tiny vessels (8.3% average improvement in sensitivity) while achieving better area under the receiver operating characteristic curve (AUC) compared to state-of-the-art methods.



### Mobile App Tasks with Iterative Feedback (MoTIF): Addressing Task Feasibility in Interactive Visual Environments
- **Arxiv ID**: http://arxiv.org/abs/2104.08560v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.08560v1)
- **Published**: 2021-04-17 14:48:02+00:00
- **Updated**: 2021-04-17 14:48:02+00:00
- **Authors**: Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, Bryan A. Plummer
- **Comment**: Accepted at the workshop on Visually Grounded Interaction and
  Language (ViGIL) at NAACL 2021
- **Journal**: None
- **Summary**: In recent years, vision-language research has shifted to study tasks which require more complex reasoning, such as interactive question answering, visual common sense reasoning, and question-answer plausibility prediction. However, the datasets used for these problems fail to capture the complexity of real inputs and multimodal environments, such as ambiguous natural language requests and diverse digital domains. We introduce Mobile app Tasks with Iterative Feedback (MoTIF), a dataset with natural language commands for the greatest number of interactive environments to date. MoTIF is the first to contain natural language requests for interactive environments that are not satisfiable, and we obtain follow-up questions on this subset to enable research on task uncertainty resolution. We perform initial feasibility classification experiments and only reach an F1 score of 37.3, verifying the need for richer vision-language representations and improved architectures to reason about task feasibility.



### Wide-Baseline Multi-Camera Calibration using Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2104.08568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08568v1)
- **Published**: 2021-04-17 15:09:18+00:00
- **Updated**: 2021-04-17 15:09:18+00:00
- **Authors**: Yan Xu, Yu-Jhe Li, Xinshuo Weng, Kris Kitani
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of estimating the 3D pose of a network of cameras for large-environment wide-baseline scenarios, e.g., cameras for construction sites, sports stadiums, and public spaces. This task is challenging since detecting and matching the same 3D keypoint observed from two very different camera views is difficult, making standard structure-from-motion (SfM) pipelines inapplicable. In such circumstances, treating people in the scene as "keypoints" and associating them across different camera views can be an alternative method for obtaining correspondences. Based on this intuition, we propose a method that uses ideas from person re-identification (re-ID) for wide-baseline camera calibration. Our method first employs a re-ID method to associate human bounding boxes across cameras, then converts bounding box correspondences to point correspondences, and finally solves for camera pose using multi-view geometry and bundle adjustment. Since our method does not require specialized calibration targets except for visible people, it applies to situations where frequent calibration updates are required. We perform extensive experiments on datasets captured from scenes of different sizes, camera settings (indoor and outdoor), and human activities (walking, playing basketball, construction). Experiment results show that our method achieves similar performance to standard SfM methods relying on manually labeled point correspondences.



### RefineMask: Towards High-Quality Instance Segmentation with Fine-Grained Features
- **Arxiv ID**: http://arxiv.org/abs/2104.08569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08569v1)
- **Published**: 2021-04-17 15:09:20+00:00
- **Updated**: 2021-04-17 15:09:20+00:00
- **Authors**: Gang Zhang, Xin Lu, Jingru Tan, Jianmin Li, Zhaoxiang Zhang, Quanquan Li, Xiaolin Hu
- **Comment**: Accepted by CVPR 2021. Code is available at
  https://github.com/zhanggang001/RefineMask
- **Journal**: None
- **Summary**: The two-stage methods for instance segmentation, e.g. Mask R-CNN, have achieved excellent performance recently. However, the segmented masks are still very coarse due to the downsampling operations in both the feature pyramid and the instance-wise pooling process, especially for large objects. In this work, we propose a new method called RefineMask for high-quality instance segmentation of objects and scenes, which incorporates fine-grained features during the instance-wise segmenting process in a multi-stage manner. Through fusing more detailed information stage by stage, RefineMask is able to refine high-quality masks consistently. RefineMask succeeds in segmenting hard cases such as bent parts of objects that are over-smoothed by most previous methods and outputs accurate boundaries. Without bells and whistles, RefineMask yields significant gains of 2.6, 3.4, 3.8 AP over Mask R-CNN on COCO, LVIS, and Cityscapes benchmarks respectively at a small amount of additional computational cost. Furthermore, our single-model result outperforms the winner of the LVIS Challenge 2020 by 1.3 points on the LVIS test-dev set and establishes a new state-of-the-art. Code will be available at https://github.com/zhanggang001/RefineMask.



### On Learning the Geodesic Path for Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.08572v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.08572v1)
- **Published**: 2021-04-17 15:26:34+00:00
- **Updated**: 2021-04-17 15:26:34+00:00
- **Authors**: Christian Simon, Piotr Koniusz, Mehrtash Harandi
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Neural networks notoriously suffer from the problem of catastrophic forgetting, the phenomenon of forgetting the past knowledge when acquiring new knowledge. Overcoming catastrophic forgetting is of significant importance to emulate the process of "incremental learning", where the model is capable of learning from sequential experience in an efficient and robust way. State-of-the-art techniques for incremental learning make use of knowledge distillation towards preventing catastrophic forgetting. Therein, one updates the network while ensuring that the network's responses to previously seen concepts remain stable throughout updates. This in practice is done by minimizing the dissimilarity between current and previous responses of the network one way or another. Our work contributes a novel method to the arsenal of distillation techniques. In contrast to the previous state of the art, we propose to firstly construct low-dimensional manifolds for previous and current responses and minimize the dissimilarity between the responses along the geodesic connecting the manifolds. This induces a more formidable knowledge distillation with smooth properties which preserves the past knowledge more efficiently as observed by our comprehensive empirical study.



### VSpSR: Explorable Super-Resolution via Variational Sparse Representation
- **Arxiv ID**: http://arxiv.org/abs/2104.08575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08575v1)
- **Published**: 2021-04-17 15:36:24+00:00
- **Updated**: 2021-04-17 15:36:24+00:00
- **Authors**: Hangqi Zhou, Chao Huang, Shangqi Gao, Xiahai Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: Super-resolution (SR) is an ill-posed problem, which means that infinitely many high-resolution (HR) images can be degraded to the same low-resolution (LR) image. To study the one-to-many stochastic SR mapping, we implicitly represent the non-local self-similarity of natural images and develop a Variational Sparse framework for Super-Resolution (VSpSR) via neural networks. Since every small patch of a HR image can be well approximated by the sparse representation of atoms in an over-complete dictionary, we design a two-branch module, i.e., VSpM, to explore the SR space. Concretely, one branch of VSpM extracts patch-level basis from the LR input, and the other branch infers pixel-wise variational distributions with respect to the sparse coefficients. By repeatedly sampling coefficients, we could obtain infinite sparse representations, and thus generate diverse HR images. According to the preliminary results of NTIRE 2021 challenge on learning SR space, our team (FudanZmic21) ranks 7-th in terms of released scores. The implementation of VSpSR is released at https://zmiclab.github.io/.



### Color Variants Identification in Fashion e-commerce via Contrastive Self-Supervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.08581v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.08581v2)
- **Published**: 2021-04-17 15:51:56+00:00
- **Updated**: 2021-06-30 22:07:44+00:00
- **Authors**: Ujjal Kr Dutta, Sandeep Repakula, Maulik Parmar, Abhinav Ravi
- **Comment**: Accepted In IJCAI-21 Weakly Supervised Representation Learning (WSRL)
  workshop
- **Journal**: None
- **Summary**: In this paper, we utilize deep visual Representation Learning to address an important problem in fashion e-commerce: color variants identification, i.e., identifying fashion products that match exactly in their design (or style), but only to differ in their color. At first we attempt to tackle the problem by obtaining manual annotations (depicting whether two products are color variants), and train a supervised triplet loss based neural network model to learn representations of fashion products. However, for large scale real-world industrial datasets such as addressed in our paper, it is infeasible to obtain annotations for the entire dataset, while capturing all the difficult corner cases. Interestingly, we observed that color variants are essentially manifestations of color jitter based augmentations. Thus, we instead explore Self-Supervised Learning (SSL) to solve this problem. We observed that existing state-of-the-art SSL methods perform poor, for our problem. To address this, we propose a novel SSL based color variants model that simultaneously focuses on different parts of an apparel. Quantitative and qualitative evaluation shows that our method outperforms existing SSL methods, and at times, the supervised model.



### Age Range Estimation using MTCNN and VGG-Face Model
- **Arxiv ID**: http://arxiv.org/abs/2104.08585v1
- **DOI**: 10.1109/ICCCNT49239.2020.9225443
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08585v1)
- **Published**: 2021-04-17 15:54:14+00:00
- **Updated**: 2021-04-17 15:54:14+00:00
- **Authors**: Dipesh Gyawali, Prashanga Pokharel, Ashutosh Chauhan, Subodh Chandra Shakya
- **Comment**: 6 pages, 10 figures
- **Journal**: 11th IEEE International Conference on Computing, Communication and
  Networking Technologies (ICCCNT), 2020
- **Summary**: The Convolutional Neural Network has amazed us with its usage on several applications. Age range estimation using CNN is emerging due to its application in myriad of areas which makes it a state-of-the-art area for research and improve the estimation accuracy. A deep CNN model is used for identification of people's age range in our proposed work. At first, we extracted only face images from image dataset using MTCNN to remove unnecessary features other than face from the image. Secondly, we used random crop technique for data augmentation to improve the model performance. We have used the concept of transfer learning in our research. A pretrained face recognition model i.e VGG-Face is used to build our model for identification of age range whose performance is evaluated on Adience Benchmark for confirming the efficacy of our work. The performance in test set outperformed existing state-of-the-art by substantial margins.



### Learning Fuzzy Clustering for SPECT/CT Segmentation via Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.08623v3
- **DOI**: 10.1002/mp.14903
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08623v3)
- **Published**: 2021-04-17 19:03:52+00:00
- **Updated**: 2021-05-28 14:43:16+00:00
- **Authors**: Junyu Chen, Ye Li, Licia P. Luna, Hyun Woo Chung, Steven P. Rowe, Yong Du, Lilja B. Solnes, Eric C. Frey
- **Comment**: This manuscript has been published by Medical Physics (2021)
- **Journal**: None
- **Summary**: Quantitative bone single-photon emission computed tomography (QBSPECT) has the potential to provide a better quantitative assessment of bone metastasis than planar bone scintigraphy due to its ability to better quantify activity in overlapping structures. An important element of assessing response of bone metastasis is accurate image segmentation. However, limited by the properties of QBSPECT images, the segmentation of anatomical regions-of-interests (ROIs) still relies heavily on the manual delineation by experts. This work proposes a fast and robust automated segmentation method for partitioning a QBSPECT image into lesion, bone, and background. We present a new unsupervised segmentation loss function and its semi- and supervised variants for training a convolutional neural network (ConvNet). The loss functions were developed based on the objective function of the classical Fuzzy C-means (FCM) algorithm. We conducted a comprehensive study to compare our proposed methods with ConvNets trained using supervised loss functions and conventional clustering methods. The Dice similarity coefficient (DSC) and several other metrics were used as figures of merit as applied to the task of delineating lesion and bone in both simulated and clinical SPECT/CT images. We experimentally demonstrated that the proposed methods yielded good segmentation results on a clinical dataset even though the training was done using realistic simulated images. A ConvNet-based image segmentation method that uses novel loss functions was developed and evaluated. The method can operate in unsupervised, semi-supervised, or fully-supervised modes depending on the availability of annotated training data. The results demonstrated that the proposed method provides fast and robust lesion and bone segmentation for QBSPECT/CT. The method can potentially be applied to other medical image segmentation applications.



### Automated Mathematical Equation Structure Discovery for Visual Analysis
- **Arxiv ID**: http://arxiv.org/abs/2104.08633v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.08633v1)
- **Published**: 2021-04-17 19:42:06+00:00
- **Updated**: 2021-04-17 19:42:06+00:00
- **Authors**: Caroline Pacheco do Espírito Silva, José A. M. Felippe De Souza, Antoine Vacavant, Thierry Bouwmans, Andrews Cordolino Sobral
- **Comment**: 25 pages, 8 figures, submitted to JMLR
- **Journal**: None
- **Summary**: Finding the best mathematical equation to deal with the different challenges found in complex scenarios requires a thorough understanding of the scenario and a trial and error process carried out by experts. In recent years, most state-of-the-art equation discovery methods have been widely applied in modeling and identification systems. However, equation discovery approaches can be very useful in computer vision, particularly in the field of feature extraction. In this paper, we focus on recent AI advances to present a novel framework for automatically discovering equations from scratch with little human intervention to deal with the different challenges encountered in real-world scenarios. In addition, our proposal can reduce human bias by proposing a search space design through generative network instead of hand-designed. As a proof of concept, the equations discovered by our framework are used to distinguish moving objects from the background in video sequences. Experimental results show the potential of the proposed approach and its effectiveness in discovering the best equation in video sequences. The code and data are available at: https://github.com/carolinepacheco/equation-discovery-scene-analysis



### IUPUI Driving Videos and Images in All Weather and Illumination Conditions
- **Arxiv ID**: http://arxiv.org/abs/2104.08657v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.08657v1)
- **Published**: 2021-04-17 22:59:15+00:00
- **Updated**: 2021-04-17 22:59:15+00:00
- **Authors**: Jiang Yu Zheng
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: This document describes an image and video dataset of driving views captured in all weather and illumination conditions. The data set has been submitted to CDVL.



### Higher Order Recurrent Space-Time Transformer for Video Action Prediction
- **Arxiv ID**: http://arxiv.org/abs/2104.08665v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08665v3)
- **Published**: 2021-04-17 23:51:05+00:00
- **Updated**: 2021-09-21 05:25:42+00:00
- **Authors**: Tsung-Ming Tai, Giuseppe Fiameni, Cheng-Kuang Lee, Oswald Lanz
- **Comment**: None
- **Journal**: None
- **Summary**: Endowing visual agents with predictive capability is a key step towards video intelligence at scale. The predominant modeling paradigm for this is sequence learning, mostly implemented through LSTMs. Feed-forward Transformer architectures have replaced recurrent model designs in ML applications of language processing and also partly in computer vision. In this paper we investigate on the competitiveness of Transformer-style architectures for video predictive tasks. To do so we propose HORST, a novel higher order recurrent layer design whose core element is a spatial-temporal decomposition of self-attention for video. HORST achieves state of the art competitive performance on Something-Something early action recognition and EPIC-Kitchens action anticipation, showing evidence of predictive capability that we attribute to our recurrent higher order design of self-attention.



