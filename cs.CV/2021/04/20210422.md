# Arxiv Papers in cs.CV on 2021-04-22
### Colonoscopy Polyp Detection and Classification: Dataset Creation and Comparative Evaluations
- **Arxiv ID**: http://arxiv.org/abs/2104.10824v2
- **DOI**: 10.1371/journal.pone.0255809
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.10824v2)
- **Published**: 2021-04-22 01:57:35+00:00
- **Updated**: 2021-08-05 18:35:17+00:00
- **Authors**: Kaidong Li, Mohammad I. Fathan, Krushi Patel, Tianxiao Zhang, Cuncong Zhong, Ajay Bansal, Amit Rastogi, Jean S. Wang, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Colorectal cancer (CRC) is one of the most common types of cancer with a high mortality rate. Colonoscopy is the preferred procedure for CRC screening and has proven to be effective in reducing CRC mortality. Thus, a reliable computer-aided polyp detection and classification system can significantly increase the effectiveness of colonoscopy. In this paper, we create an endoscopic dataset collected from various sources and annotate the ground truth of polyp location and classification results with the help of experienced gastroenterologists. The dataset can serve as a benchmark platform to train and evaluate the machine learning models for polyp classification. We have also compared the performance of eight state-of-the-art deep learning-based object detection models. The results demonstrate that deep CNN models are promising in CRC screening. This work can serve as a baseline for future research in polyp detection and classification.



### Self-optimizing loop sifting and majorization for 3D reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2104.10826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10826v1)
- **Published**: 2021-04-22 01:59:19+00:00
- **Updated**: 2021-04-22 01:59:19+00:00
- **Authors**: Guoxiang Zhang, YangQuan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Visual simultaneous localization and mapping (vSLAM) and 3D reconstruction methods have gone through impressive progress. These methods are very promising for autonomous vehicle and consumer robot applications because they can map large-scale environments such as cities and indoor environments without the need for much human effort. However, when it comes to loop detection and optimization, there is still room for improvement. vSLAM systems tend to add the loops very conservatively to reduce the severe influence of the false loops. These conservative checks usually lead to correct loops rejected, thus decrease performance. In this paper, an algorithm that can sift and majorize loop detections is proposed. Our proposed algorithm can compare the usefulness and effectiveness of different loops with the dense map posterior (DMP) metric. The algorithm tests and decides the acceptance of each loop without a single user-defined threshold. Thus it is adaptive to different data conditions. The proposed method is general and agnostic to sensor type (as long as depth or LiDAR reading presents), loop detection, and optimization methods. Neither does it require a specific type of SLAM system. Thus it has great potential to be applied to various application scenarios. Experiments are conducted on public datasets. Results show that the proposed method outperforms state-of-the-art methods.



### DANNet: A One-Stage Domain Adaptation Network for Unsupervised Nighttime Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.10834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10834v1)
- **Published**: 2021-04-22 02:49:28+00:00
- **Updated**: 2021-04-22 02:49:28+00:00
- **Authors**: Xinyi Wu, Zhenyao Wu, Hao Guo, Lili Ju, Song Wang
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Semantic segmentation of nighttime images plays an equally important role as that of daytime images in autonomous driving, but the former is much more challenging due to poor illuminations and arduous human annotations. In this paper, we propose a novel domain adaptation network (DANNet) for nighttime semantic segmentation without using labeled nighttime image data. It employs an adversarial training with a labeled daytime dataset and an unlabeled dataset that contains coarsely aligned day-night image pairs. Specifically, for the unlabeled day-night image pairs, we use the pixel-level predictions of static object categories on a daytime image as a pseudo supervision to segment its counterpart nighttime image. We further design a re-weighting strategy to handle the inaccuracy caused by misalignment between day-night image pairs and wrong predictions of daytime images, as well as boost the prediction accuracy of small objects. The proposed DANNet is the first one stage adaptation framework for nighttime semantic segmentation, which does not train additional day-night image transfer models as a separate pre-processing stage. Extensive experiments on Dark Zurich and Nighttime Driving datasets show that our method achieves state-of-the-art performance for nighttime semantic segmentation.



### Localization of Ice-Rink for Broadcast Hockey Videos
- **Arxiv ID**: http://arxiv.org/abs/2104.10847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10847v1)
- **Published**: 2021-04-22 03:39:43+00:00
- **Updated**: 2021-04-22 03:39:43+00:00
- **Authors**: Mehrnaz Fani, Pascale Berunelle Walters, David A. Clausi, John Zelek, Alexander Wong
- **Comment**: 6 pages, 2 figures, Poster in WiCV2021
- **Journal**: None
- **Summary**: In this work, an automatic and simple framework for hockey ice-rink localization from broadcast videos is introduced. First, video is broken into video-shots by a hierarchical partitioning of the video frames, and thresholding based on their histograms. To localize the frames on the ice-rink model, a ResNet18-based regressor is implemented and trained, which regresses to four control points on the model in a frame-by-frame fashion. This leads to the projection jittering problem in the video. To overcome this, in the inference phase, the trajectory of the control points on the ice-rink model are smoothed, for all the consecutive frames of a given video-shot, by convolving a Hann window with the achieved coordinates. Finally, the smoothed homography matrix is computed by using the direct linear transform on the four pairs of corresponding points. A hockey dataset for training and testing the regressor is gathered. The results show success of this simple and comprehensive procedure for localizing the hockey ice-rink and addressing the problem of jittering without affecting the accuracy of homography estimation.



### Mini-batch graphs for robust image classification
- **Arxiv ID**: http://arxiv.org/abs/2105.03237v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.03237v1)
- **Published**: 2021-04-22 03:43:32+00:00
- **Updated**: 2021-04-22 03:43:32+00:00
- **Authors**: Arnab Kumar Mondal, Vineet Jain, Kaleem Siddiqi
- **Comment**: None
- **Journal**: None
- **Summary**: Current deep learning models for classification tasks in computer vision are trained using mini-batches. In the present article, we take advantage of the relationships between samples in a mini-batch, using graph neural networks to aggregate information from similar images. This helps mitigate the adverse effects of alterations to the input images on classification performance. Diverse experiments on image-based object and scene classification show that this approach not only improves a classifier's performance but also increases its robustness to image perturbations and adversarial attacks. Further, we also show that mini-batch graph neural networks can help to alleviate the problem of mode collapse in Generative Adversarial Networks.



### A Strong Baseline for Vehicle Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2104.10850v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10850v1)
- **Published**: 2021-04-22 03:54:55+00:00
- **Updated**: 2021-04-22 03:54:55+00:00
- **Authors**: Su V. Huynh, Nam H. Nguyen, Ngoc T. Nguyen, Vinh TQ. Nguyen, Chau Huynh, Chuong Nguyen
- **Comment**: Accepted to CVPR Workshop 2021, 5th AI City Challenge
- **Journal**: None
- **Summary**: Vehicle Re-Identification (Re-ID) aims to identify the same vehicle across different cameras, hence plays an important role in modern traffic management systems. The technical challenges require the algorithms must be robust in different views, resolution, occlusion and illumination conditions. In this paper, we first analyze the main factors hindering the Vehicle Re-ID performance. We then present our solutions, specifically targeting the dataset Track 2 of the 5th AI City Challenge, including (1) reducing the domain gap between real and synthetic data, (2) network modification by stacking multi heads with attention mechanism, (3) adaptive loss weight adjustment. Our method achieves 61.34% mAP on the private CityFlow testset without using external dataset or pseudo labeling, and outperforms all previous works at 87.1% mAP on the Veri benchmark. The code is available at https://github.com/cybercore-co-ltd/track2_aicity_2021.



### Continuous Learning and Adaptation with Membrane Potential and Activation Threshold Homeostasis
- **Arxiv ID**: http://arxiv.org/abs/2104.10851v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.10851v3)
- **Published**: 2021-04-22 04:01:32+00:00
- **Updated**: 2021-06-09 01:24:43+00:00
- **Authors**: Alexander Hadjiivanov
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: Most classical (non-spiking) neural network models disregard internal neuron dynamics and treat neurons as simple input integrators. However, biological neurons have an internal state governed by complex dynamics that plays a crucial role in learning, adaptation and the overall network activity and behaviour. This paper presents the Membrane Potential and Activation Threshold Homeostasis (MPATH) neuron model, which combines several biologically inspired mechanisms to efficiently simulate internal neuron dynamics with a single parameter analogous to the membrane time constant in biological neurons. The model allows neurons to maintain a form of dynamic equilibrium by automatically regulating their activity when presented with fluctuating input. One consequence of the MPATH model is that it imbues neurons with a sense of time without recurrent connections, paving the way for modelling processes that depend on temporal aspects of neuron activity. Experiments demonstrate the model's ability to adapt to and continually learn from its input.



### Deep Lucas-Kanade Homography for Multimodal Image Alignment
- **Arxiv ID**: http://arxiv.org/abs/2104.11693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11693v1)
- **Published**: 2021-04-22 04:11:29+00:00
- **Updated**: 2021-04-22 04:11:29+00:00
- **Authors**: Yiming Zhao, Xinming Huang, Ziming Zhang
- **Comment**: Accepted by CVPR2021, codelink:
  https://github.com/placeforyiming/CVPR21-Deep-Lucas-Kanade-Homography
- **Journal**: None
- **Summary**: Estimating homography to align image pairs captured by different sensors or image pairs with large appearance changes is an important and general challenge for many computer vision applications. In contrast to others, we propose a generic solution to pixel-wise align multimodal image pairs by extending the traditional Lucas-Kanade algorithm with networks. The key contribution in our method is how we construct feature maps, named as deep Lucas-Kanade feature map (DLKFM). The learned DLKFM can spontaneously recognize invariant features under various appearance-changing conditions. It also has two nice properties for the Lucas-Kanade algorithm: (1) The template feature map keeps brightness consistency with the input feature map, thus the color difference is very small while they are well-aligned. (2) The Lucas-Kanade objective function built on DLKFM has a smooth landscape around ground truth homography parameters, so the iterative solution of the Lucas-Kanade can easily converge to the ground truth. With those properties, directly updating the Lucas-Kanade algorithm on our feature maps will precisely align image pairs with large appearance changes. We share the datasets, code, and demo video online.



### Frequency Domain Loss Function for Deep Exposure Correction of Dark Images
- **Arxiv ID**: http://arxiv.org/abs/2104.10856v2
- **DOI**: 10.1007/s11760-021-01915-4
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.10856v2)
- **Published**: 2021-04-22 04:13:40+00:00
- **Updated**: 2021-05-21 12:41:57+00:00
- **Authors**: Ojasvi Yadav, Koustav Ghosal, Sebastian Lutz, Aljosa Smolic
- **Comment**: Published version 8 pages. SIViP (2021)
- **Journal**: None
- **Summary**: We address the problem of exposure correction of dark, blurry and noisy images captured in low-light conditions in the wild. Classical image-denoising filters work well in the frequency space but are constrained by several factors such as the correct choice of thresholds, frequency estimates etc. On the other hand, traditional deep networks are trained end-to-end in the RGB space by formulating this task as an image-translation problem. However, that is done without any explicit constraints on the inherent noise of the dark images and thus produce noisy and blurry outputs. To this end we propose a DCT/FFT based multi-scale loss function, which when combined with traditional losses, trains a network to translate the important features for visually pleasing output. Our loss function is end-to-end differentiable, scale-agnostic, and generic; i.e., it can be applied to both RAW and JPEG images in most existing frameworks without additional overhead. Using this loss function, we report significant improvements over the state-of-the-art using quantitative metrics and subjective tests.



### All Tokens Matter: Token Labeling for Training Better Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.10858v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10858v3)
- **Published**: 2021-04-22 04:43:06+00:00
- **Updated**: 2021-06-09 15:27:26+00:00
- **Authors**: Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran Wang, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present token labeling -- a new training objective for training high-performance vision transformers (ViTs). Different from the standard training objective of ViTs that computes the classification loss on an additional trainable class token, our proposed one takes advantage of all the image patch tokens to compute the training loss in a dense manner. Specifically, token labeling reformulates the image classification problem into multiple token-level recognition problems and assigns each patch token with an individual location-specific supervision generated by a machine annotator. Experiments show that token labeling can clearly and consistently improve the performance of various ViT models across a wide spectrum. For a vision transformer with 26M learnable parameters serving as an example, with token labeling, the model can achieve 84.4% Top-1 accuracy on ImageNet. The result can be further increased to 86.4% by slightly scaling the model size up to 150M, delivering the minimal-sized model among previous models (250M+) reaching 86%. We also show that token labeling can clearly improve the generalization capability of the pre-trained models on downstream tasks with dense prediction, such as semantic segmentation. Our code and all the training details will be made publicly available at https://github.com/zihangJiang/TokenLabeling.



### Detect caterpillar, grasshopper, aphid and simulation program for neutralizing them by laser
- **Arxiv ID**: http://arxiv.org/abs/2105.02955v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.02955v1)
- **Published**: 2021-04-22 05:02:27+00:00
- **Updated**: 2021-04-22 05:02:27+00:00
- **Authors**: Rakhmatulin Ildar
- **Comment**: None
- **Journal**: None
- **Summary**: The protection of crops from pests is relevant for any cultivated crop. But modern methods of pest control by pesticides carry many dangers for humans. Therefore, research into the development of safe and effective pest control methods is promising. This manuscript presents a new method of pest control. We used neural networks for pest detection and developed a powerful laser device (5 W) for their neutralization. In the manuscript methods of processing images with pests to extract the most useful feature are described in detail. Using the following pets as an example: aphids, grasshopper, cabbage caterpillar, we analyzed various neural network models and selected the optimal models and characteristics for each insect. In the paper the principle of operation of the developed laser device is described in detail. We created the program to search a pest in the video stream calculation of their coordinates and transmission data with coordinates to the device with the laser.



### Towards Adversarial Patch Analysis and Certified Defense against Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2104.10868v3
- **DOI**: 10.1145/3474085.3475378
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10868v3)
- **Published**: 2021-04-22 05:10:55+00:00
- **Updated**: 2021-07-28 03:43:42+00:00
- **Authors**: Qiming Wu, Zhikang Zou, Pan Zhou, Xiaoqing Ye, Binghui Wang, Ang Li
- **Comment**: Accepted by ACM Multimedia 2021
- **Journal**: None
- **Summary**: Crowd counting has drawn much attention due to its importance in safety-critical surveillance systems. Especially, deep neural network (DNN) methods have significantly reduced estimation errors for crowd counting missions. Recent studies have demonstrated that DNNs are vulnerable to adversarial attacks, i.e., normal images with human-imperceptible perturbations could mislead DNNs to make false predictions. In this work, we propose a robust attack strategy called Adversarial Patch Attack with Momentum (APAM) to systematically evaluate the robustness of crowd counting models, where the attacker's goal is to create an adversarial perturbation that severely degrades their performances, thus leading to public safety accidents (e.g., stampede accidents). Especially, the proposed attack leverages the extreme-density background information of input images to generate robust adversarial patches via a series of transformations (e.g., interpolation, rotation, etc.). We observe that by perturbing less than 6\% of image pixels, our attacks severely degrade the performance of crowd counting systems, both digitally and physically. To better enhance the adversarial robustness of crowd counting models, we propose the first regression model-based Randomized Ablation (RA), which is more sufficient than Adversarial Training (ADT) (Mean Absolute Error of RA is 5 lower than ADT on clean samples and 30 lower than ADT on adversarial examples). Extensive experiments on five crowd counting models demonstrate the effectiveness and generality of the proposed method. The supplementary materials and certificate retrained models are available at \url{https://www.dropbox.com/s/hc4fdx133vht0qb/ACM_MM2021_Supp.pdf?dl=0}



### Focusing on Shadows for Predicting Heightmaps from Single Remotely Sensed RGB Images with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.10874v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.10874v1)
- **Published**: 2021-04-22 05:31:13+00:00
- **Updated**: 2021-04-22 05:31:13+00:00
- **Authors**: Savvas Karatsiolis, Andreas Kamilaris
- **Comment**: 30 pages, 13 figures
- **Journal**: None
- **Summary**: Estimating the heightmaps of buildings and vegetation in single remotely sensed images is a challenging problem. Effective solutions to this problem can comprise the stepping stone for solving complex and demanding problems that require 3D information of aerial imagery in the remote sensing discipline, which might be expensive or not feasible to require. We propose a task-focused Deep Learning (DL) model that takes advantage of the shadow map of a remotely sensed image to calculate its heightmap. The shadow is computed efficiently and does not add significant computation complexity. The model is trained with aerial images and their Lidar measurements, achieving superior performance on the task. We validate the model with a dataset covering a large area of Manchester, UK, as well as the 2018 IEEE GRSS Data Fusion Contest Lidar dataset. Our work suggests that the proposed DL architecture and the technique of injecting shadows information into the model are valuable for improving the heightmap estimation task for single remotely sensed imagery.



### Patch Shortcuts: Interpretable Proxy Models Efficiently Find Black-Box Vulnerabilities
- **Arxiv ID**: http://arxiv.org/abs/2104.11691v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.11691v1)
- **Published**: 2021-04-22 05:44:40+00:00
- **Updated**: 2021-04-22 05:44:40+00:00
- **Authors**: Julia Rosenzweig, Joachim Sicking, Sebastian Houben, Michael Mock, Maram Akila
- **Comment**: Under IEEE Copyright; accepted at the SAIAD (Safe Artificial
  Intelligence for Automated Driving) Workshop at CVPR 2021
- **Journal**: None
- **Summary**: An important pillar for safe machine learning (ML) is the systematic mitigation of weaknesses in neural networks to afford their deployment in critical applications. An ubiquitous class of safety risks are learned shortcuts, i.e. spurious correlations a network exploits for its decisions that have no semantic connection to the actual task. Networks relying on such shortcuts bear the risk of not generalizing well to unseen inputs. Explainability methods help to uncover such network vulnerabilities. However, many of these techniques are not directly applicable if access to the network is constrained, in so-called black-box setups. These setups are prevalent when using third-party ML components. To address this constraint, we present an approach to detect learned shortcuts using an interpretable-by-design network as a proxy to the black-box model of interest. Leveraging the proxy's guarantees on introspection we automatically extract candidates for learned shortcuts. Their transferability to the black box is validated in a systematic fashion. Concretely, as proxy model we choose a BagNet, which bases its decisions purely on local image patches. We demonstrate on the autonomous driving dataset A2D2 that extracted patch shortcuts significantly influence the black box model. By efficiently identifying such patch-based vulnerabilities, we contribute to safer ML models.



### Efficient LiDAR Odometry for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2104.10879v1
- **DOI**: 10.1109/LRA.2021.3110372
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.10879v1)
- **Published**: 2021-04-22 06:05:09+00:00
- **Updated**: 2021-04-22 06:05:09+00:00
- **Authors**: Xin Zheng, Jianke Zhu
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters 2021
- **Summary**: LiDAR odometry plays an important role in self-localization and mapping for autonomous navigation, which is usually treated as a scan registration problem. Although having achieved promising performance on KITTI odometry benchmark, the conventional searching tree-based approach still has the difficulty in dealing with the large scale point cloud efficiently. The recent spherical range image-based method enjoys the merits of fast nearest neighbor search by spherical mapping. However, it is not very effective to deal with the ground points nearly parallel to LiDAR beams. To address these issues, we propose a novel efficient LiDAR odometry approach by taking advantage of both non-ground spherical range image and bird's-eye-view map for ground points. Moreover, a range adaptive method is introduced to robustly estimate the local surface normal. Additionally, a very fast and memory-efficient model update scheme is proposed to fuse the points and their corresponding normals at different time-stamps. We have conducted extensive experiments on KITTI odometry benchmark, whose promising results demonstrate that our proposed approach is effective.



### Computer Vision-based Social Distancing Surveillance Solution with Optional Automated Camera Calibration for Large Scale Deployment
- **Arxiv ID**: http://arxiv.org/abs/2104.10891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10891v1)
- **Published**: 2021-04-22 06:43:02+00:00
- **Updated**: 2021-04-22 06:43:02+00:00
- **Authors**: Sreetama Das, Anirban Nag, Dhruba Adhikary, Ramswaroop Jeevan Ram, Aravind BR, Sujit Kumar Ojha, Guruprasad M Hegde
- **Comment**: 8 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: Social distancing has been suggested as one of the most effective measures to break the chain of viral transmission in the current COVID-19 pandemic. We herein describe a computer vision-based AI-assisted solution to aid compliance with social distancing norms. The solution consists of modules to detect and track people and to identify distance violations. It provides the flexibility to choose between a tool-based mode or an automated mode of camera calibration, making the latter suitable for large-scale deployments. In this paper, we discuss different metrics to assess the risk associated with social distancing violations and how we can differentiate between transient or persistent violations. Our proposed solution performs satisfactorily under different test scenarios, processes video feed at real-time speed as well as addresses data privacy regulations by blurring faces of detected people, making it ideal for deployments.



### VeriMedi: Pill Identification using Proxy-based Deep Metric Learning and Exact Solution
- **Arxiv ID**: http://arxiv.org/abs/2104.11231v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2104.11231v1)
- **Published**: 2021-04-22 06:52:30+00:00
- **Updated**: 2021-04-22 06:52:30+00:00
- **Authors**: Tekin Evrim Ozmermer, Viktors Roze, Stanislavs Hilcuks, Alina Nescerecka
- **Comment**: 31 pages, 21 figures, 10 tables
- **Journal**: None
- **Summary**: We present the system that we have developed for the identification and verification of pills using images that are taken by the VeriMedi device. The VeriMedi device is an Internet of Things device that takes pictures of a filled pill vial from the bottom of the vial and uses the solution that is presented in this research to identify the pills in the vials. The solution has two serially connected deep learning solutions which do segmentation and identification. The segmentation solution creates the masks for each pill in the vial image by using the Mask R-CNN model, then segments and crops the pills and blurs the background. After that, the segmented pill images are sent to the identification solution where a Deep Metric Learning model that is trained with Proxy Anchor Loss (PAL) function generates embedding vectors for each pill image. The generated embedding vectors are fed into a one-layer fully connected network that is trained with the exact solution to predict each single pill image. Then, the aggregation/verification function aggregates the multiple predictions coming from multiple single pill images and verifies the correctness of the final prediction with respect to predefined rules. Besides, we enhanced the PAL with a better proxy initialization that increased the performance of the models and let the model learn the new classes of images continually without retraining the model with the whole dataset. When the model that is trained with initial classes is retrained only with new classes, the accuracy of the model increases for both old and new classes. The identification solution that we have presented in this research can also be reused for other problem domains which require continual learning and/or Fine-Grained Visual Categorization.



### Robust 360-8PA: Redesigning The Normalized 8-point Algorithm for 360-FoV Images
- **Arxiv ID**: http://arxiv.org/abs/2104.10900v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.10900v1)
- **Published**: 2021-04-22 07:23:11+00:00
- **Updated**: 2021-04-22 07:23:11+00:00
- **Authors**: Bolivar Solarte, Chin-Hsuan Wu, Kuan-Wei Lu, Min Sun, Wei-Chen Chiu, Yi-Hsuan Tsai
- **Comment**: Accepted to ICRA 2021
- **Journal**: None
- **Summary**: This paper presents a novel preconditioning strategy for the classic 8-point algorithm (8-PA) for estimating an essential matrix from 360-FoV images (i.e., equirectangular images) in spherical projection. To alleviate the effect of uneven key-feature distributions and outlier correspondences, which can potentially decrease the accuracy of an essential matrix, our method optimizes a non-rigid transformation to deform a spherical camera into a new spatial domain, defining a new constraint and a more robust and accurate solution for an essential matrix. Through several experiments using random synthetic points, 360-FoV, and fish-eye images, we demonstrate that our normalization can increase the camera pose accuracy by about 20% without significantly overhead the computation time. In addition, we present further benefits of our method through both a constant weighted least-square optimization that improves further the well known Gold Standard Method (GSM) (i.e., the non-linear optimization by using epipolar errors); and a relaxation of the number of RANSAC iterations, both showing that our normalization outcomes a more reliable, robust, and accurate solution.



### Self-Supervised Learning from Semantically Imprecise Data
- **Arxiv ID**: http://arxiv.org/abs/2104.10901v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10901v2)
- **Published**: 2021-04-22 07:26:14+00:00
- **Updated**: 2022-01-27 15:09:16+00:00
- **Authors**: Clemens-Alexander Brust, Björn Barz, Joachim Denzler
- **Comment**: 9 pages. Accepted for publication at VISAPP 2022
- **Journal**: None
- **Summary**: Learning from imprecise labels such as "animal" or "bird", but making precise predictions like "snow bunting" at inference time is an important capability for any classifier when expertly labeled training data is scarce. Contributions by volunteers or results of web crawling lack precision in this manner, but are still valuable. And crucially, these weakly labeled examples are available in larger quantities for lower cost than high-quality bespoke training data. CHILLAX, a recently proposed method to tackle this task, leverages a hierarchical classifier to learn from imprecise labels. However, it has two major limitations. First, it does not learn from examples labeled as the root of the hierarchy, e.g., "object". Second, an extrapolation of annotations to precise labels is only performed at test time, where confident extrapolations could be already used as training data. In this work, we extend CHILLAX with a self-supervised scheme using constrained semantic extrapolation to generate pseudo-labels. This addresses the second concern, which in turn solves the first problem, enabling an even weaker supervision requirement than CHILLAX. We evaluate our approach empirically, showing that our method allows for a consistent accuracy improvement of 0.84 to 1.19 percent points over CHILLAX and is suitable as a drop-in replacement without any negative consequences such as longer training times.



### Automated Tackle Injury Risk Assessment in Contact-Based Sports -- A Rugby Union Example
- **Arxiv ID**: http://arxiv.org/abs/2104.10916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10916v1)
- **Published**: 2021-04-22 07:51:33+00:00
- **Updated**: 2021-04-22 07:51:33+00:00
- **Authors**: Zubair Martin, Amir Patel, Sharief Hendricks
- **Comment**: The paper has five figures and is eight pages long (excluding
  references). The paper has been entered into the 7th IEEE International
  Workshop on Computer Vision in Sports (CVSports)
- **Journal**: None
- **Summary**: Video analysis in tackle-collision based sports is highly subjective and exposed to bias, which is inherent in human observation, especially under time constraints. This limitation of match analysis in tackle-collision based sports can be seen as an opportunity for computer vision applications. Objectively tracking, detecting and recognising an athlete's movements and actions during match play from a distance using video, along with our improved understanding of injury aetiology and skill execution will enhance our understanding how injury occurs, assist match day injury management, reduce referee subjectivity. In this paper, we present a system of objectively evaluating in-game tackle risk in rugby union matches. First, a ball detection model is trained using the You Only Look Once (YOLO) framework, these detections are then tracked by a Kalman Filter (KF). Following this, a separate YOLO model is used to detect persons/players within a tackle segment and then the ball-carrier and tackler are identified. Subsequently, we utilize OpenPose to determine the pose of ball-carrier and tackle, the relative pose of these is then used to evaluate the risk of the tackle. We tested the system on a diverse collection of rugby tackles and achieved an evaluation accuracy of 62.50%. These results will enable referees in tackle-contact based sports to make more subjective decisions, ultimately making these sports safer.



### SBNet: Segmentation-based Network for Natural Language-based Vehicle Search
- **Arxiv ID**: http://arxiv.org/abs/2104.11589v1
- **DOI**: 10.1109/CVPRW53098.2021.00457
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2.10; I.5.1; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2104.11589v1)
- **Published**: 2021-04-22 08:06:17+00:00
- **Updated**: 2021-04-22 08:06:17+00:00
- **Authors**: Sangrok Lee, Taekang Woo, Sang Hun Lee
- **Comment**: 7 pages, 4 figures, CVPR Workshop Paper
- **Journal**: 2021 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW), pp. 4049-4055
- **Summary**: Natural language-based vehicle retrieval is a task to find a target vehicle within a given image based on a natural language description as a query. This technology can be applied to various areas including police searching for a suspect vehicle. However, it is challenging due to the ambiguity of language descriptions and the difficulty of processing multi-modal data. To tackle this problem, we propose a deep neural network called SBNet that performs natural language-based segmentation for vehicle retrieval. We also propose two task-specific modules to improve performance: a substitution module that helps features from different domains to be embedded in the same space and a future prediction module that learns temporal information. SBnet has been trained using the CityFlow-NL dataset that contains 2,498 tracks of vehicles with three unique natural language descriptions each and tested 530 unique vehicle tracks and their corresponding query sets. SBNet achieved a significant improvement over the baseline in the natural language-based vehicle tracking track in the AI City Challenge 2021.



### Continental-scale land cover mapping at 10 m resolution over Europe (ELC10)
- **Arxiv ID**: http://arxiv.org/abs/2104.10922v1
- **DOI**: 10.3390/rs13122301
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.10922v1)
- **Published**: 2021-04-22 08:24:15+00:00
- **Updated**: 2021-04-22 08:24:15+00:00
- **Authors**: Zander S. Venter, Markus A. K. Sydenham
- **Comment**: None
- **Journal**: Remote Sens. 2021, 13, 2301
- **Summary**: Widely used European land cover maps such as CORINE are produced at medium spatial resolutions (100 m) and rely on diverse data with complex workflows requiring significant institutional capacity. We present a high resolution (10 m) land cover map (ELC10) of Europe based on a satellite-driven machine learning workflow that is annually updatable. A Random Forest classification model was trained on 70K ground-truth points from the LUCAS (Land Use/Cover Area frame Survey) dataset. Within the Google Earth Engine cloud computing environment, the ELC10 map can be generated from approx. 700 TB of Sentinel imagery within approx. 4 days from a single research user account. The map achieved an overall accuracy of 90% across 8 land cover classes and could account for statistical unit land cover proportions within 3.9% (R2 = 0.83) of the actual value. These accuracies are higher than that of CORINE (100 m) and other 10-m land cover maps including S2GLC and FROM-GLC10. We found that atmospheric correction of Sentinel-2 and speckle filtering of Sentinel-1 imagery had minimal effect on enhancing classification accuracy (< 1%). However, combining optical and radar imagery increased accuracy by 3% compared to Sentinel-2 alone and by 10% compared to Sentinel-1 alone. The conversion of LUCAS points into homogenous polygons under the Copernicus module increased accuracy by <1%, revealing that Random Forests are robust against contaminated training data. Furthermore, the model requires very little training data to achieve moderate accuracies - the difference between 5K and 50K LUCAS points is only 3% (86 vs 89%). At 10-m resolution, the ELC10 map can distinguish detailed landscape features like hedgerows and gardens, and therefore holds potential for aerial statistics at the city borough level and monitoring property-level environmental interventions (e.g. tree planting).



### SoT: Delving Deeper into Classification Head for Transformer
- **Arxiv ID**: http://arxiv.org/abs/2104.10935v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.10935v2)
- **Published**: 2021-04-22 09:05:09+00:00
- **Updated**: 2021-12-18 04:28:10+00:00
- **Authors**: Jiangtao Xie, Ruiren Zeng, Qilong Wang, Ziqi Zhou, Peihua Li
- **Comment**: None
- **Journal**: None
- **Summary**: Transformer models are not only successful in natural language processing (NLP) but also demonstrate high potential in computer vision (CV). Despite great advance, most of works only focus on improvement of architectures but pay little attention to the classification head. For years transformer models base exclusively on classification token to construct the final classifier, without explicitly harnessing high-level word tokens. In this paper, we propose a novel transformer model called second-order transformer (SoT), exploiting simultaneously the classification token and word tokens for the classifier. Specifically, we empirically disclose that high-level word tokens contain rich information, which per se are very competent with the classifier and moreover, are complementary to the classification token. To effectively harness such rich information, we propose multi-headed global cross-covariance pooling with singular value power normalization, which shares similar philosophy and thus is compatible with the transformer block, better than commonly used pooling methods. Then, we study comprehensively how to explicitly combine word tokens with classification token for building the final classification head. For CV tasks, our SoT significantly improves state-of-the-art vision transformers on challenging benchmarks including ImageNet and ImageNet-A. For NLP tasks, through fine-tuning based on pretrained language transformers including GPT and BERT, our SoT greatly boosts the performance on widely used tasks such as CoLA and RTE. Code will be available at https://peihuali.org/SoT



### Distilling Audio-Visual Knowledge by Compositional Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.10955v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.10955v1)
- **Published**: 2021-04-22 09:31:20+00:00
- **Updated**: 2021-04-22 09:31:20+00:00
- **Authors**: Yanbei Chen, Yongqin Xian, A. Sophia Koepke, Ying Shan, Zeynep Akata
- **Comment**: Accepted to CVPR2021
- **Journal**: None
- **Summary**: Having access to multi-modal cues (e.g. vision and audio) empowers some cognitive tasks to be done faster compared to learning from a single modality. In this work, we propose to transfer knowledge across heterogeneous modalities, even though these data modalities may not be semantically correlated. Rather than directly aligning the representations of different modalities, we compose audio, image, and video representations across modalities to uncover richer multi-modal knowledge. Our main idea is to learn a compositional embedding that closes the cross-modal semantic gap and captures the task-relevant semantics, which facilitates pulling together representations across modalities by compositional contrastive learning. We establish a new, comprehensive multi-modal distillation benchmark on three video datasets: UCF101, ActivityNet, and VGGSound. Moreover, we demonstrate that our model significantly outperforms a variety of existing knowledge distillation methods in transferring audio-visual knowledge to improve video representation learning. Code is released here: https://github.com/yanbeic/CCL.



### FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.10956v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.10956v3)
- **Published**: 2021-04-22 09:35:35+00:00
- **Updated**: 2021-09-24 07:40:56+00:00
- **Authors**: Tai Wang, Xinge Zhu, Jiangmiao Pang, Dahua Lin
- **Comment**: Camera-ready version of 3DODI workshop at ICCV 2021; Technical report
  for the best vision-only method (1st place of the camera track) in the
  nuScenes 3D detection challenge of NeurIPS 2020
- **Journal**: None
- **Summary**: Monocular 3D object detection is an important task for autonomous driving considering its advantage of low cost. It is much more challenging than conventional 2D cases due to its inherent ill-posed property, which is mainly reflected in the lack of depth information. Recent progress on 2D detection offers opportunities to better solving this problem. However, it is non-trivial to make a general adapted 2D detector work in this 3D task. In this paper, we study this problem with a practice built on a fully convolutional single-stage detector and propose a general framework FCOS3D. Specifically, we first transform the commonly defined 7-DoF 3D targets to the image domain and decouple them as 2D and 3D attributes. Then the objects are distributed to different feature levels with consideration of their 2D scales and assigned only according to the projected 3D-center for the training procedure. Furthermore, the center-ness is redefined with a 2D Gaussian distribution based on the 3D-center to fit the 3D target formulation. All of these make this framework simple yet effective, getting rid of any 2D detection or 2D-3D correspondence priors. Our solution achieves 1st place out of all the vision-only methods in the nuScenes 3D detection challenge of NeurIPS 2020. Code and models are released at https://github.com/open-mmlab/mmdetection3d.



### Compressive lensless endoscopy with partial speckle scanning
- **Arxiv ID**: http://arxiv.org/abs/2104.10959v1
- **DOI**: None
- **Categories**: **physics.optics**, cs.CV, physics.med-ph, 78A46, 78A70
- **Links**: [PDF](http://arxiv.org/pdf/2104.10959v1)
- **Published**: 2021-04-22 09:40:28+00:00
- **Updated**: 2021-04-22 09:40:28+00:00
- **Authors**: Stéphanie Guérit, Siddharth Sivankutty, John Aldo Lee, Hervé Rigneault, Laurent Jacques
- **Comment**: None
- **Journal**: None
- **Summary**: The lensless endoscope (LE) is a promising device to acquire in vivo images at a cellular scale. The tiny size of the probe enables a deep exploration of the tissues. Lensless endoscopy with a multicore fiber (MCF) commonly uses a spatial light modulator (SLM) to coherently combine, at the output of the MCF, few hundreds of beamlets into a focus spot. This spot is subsequently scanned across the sample to generate a fluorescent image. We propose here a novel scanning scheme, partial speckle scanning (PSS), inspired by compressive sensing theory, that avoids the use of an SLM to perform fluorescent imaging in LE with reduced acquisition time. Such a strategy avoids photo-bleaching while keeping high reconstruction quality. We develop our approach on two key properties of the LE: (i) the ability to easily generate speckles, and (ii) the memory effect in MCF that allows to use fast scan mirrors to shift light patterns. First, we show that speckles are sub-exponential random fields. Despite their granular structure, an appropriate choice of the reconstruction parameters makes them good candidates to build efficient sensing matrices. Then, we numerically validate our approach and apply it on experimental data. The proposed sensing technique outperforms conventional raster scanning: higher reconstruction quality is achieved with far fewer observations. For a fixed reconstruction quality, our speckle scanning approach is faster than compressive sensing schemes which require to change the speckle pattern for each observation.



### ImageNet-21K Pretraining for the Masses
- **Arxiv ID**: http://arxiv.org/abs/2104.10972v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.10972v4)
- **Published**: 2021-04-22 10:10:14+00:00
- **Updated**: 2021-08-05 15:04:28+00:00
- **Authors**: Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, Lihi Zelnik-Manor
- **Comment**: Accepted to NeurIPS 2021 (Datasets and Benchmarks)
- **Journal**: None
- **Summary**: ImageNet-1K serves as the primary dataset for pretraining deep learning models for computer vision tasks. ImageNet-21K dataset, which is bigger and more diverse, is used less frequently for pretraining, mainly due to its complexity, low accessibility, and underestimation of its added value. This paper aims to close this gap, and make high-quality efficient pretraining on ImageNet-21K available for everyone. Via a dedicated preprocessing stage, utilization of WordNet hierarchical structure, and a novel training scheme called semantic softmax, we show that various models significantly benefit from ImageNet-21K pretraining on numerous datasets and tasks, including small mobile-oriented models. We also show that we outperform previous ImageNet-21K pretraining schemes for prominent new models like ViT and Mixer. Our proposed pretraining pipeline is efficient, accessible, and leads to SoTA reproducible results, from a publicly available dataset. The training code and pretrained models are available at: https://github.com/Alibaba-MIIL/ImageNet21K



### Neuro-inspired edge feature fusion using Choquet integrals
- **Arxiv ID**: http://arxiv.org/abs/2104.10984v1
- **DOI**: 10.1016/j.ins.2021.10.016
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.10984v1)
- **Published**: 2021-04-22 10:45:52+00:00
- **Updated**: 2021-04-22 10:45:52+00:00
- **Authors**: Cedric Marco-Detchart, Giancarlo Lucca, Carlos Lopez-Molina, Laura De Miguel, Graçaliz Pereira Dimuro, Humberto Bustince
- **Comment**: None
- **Journal**: None
- **Summary**: It is known that the human visual system performs a hierarchical information process in which early vision cues (or primitives) are fused in the visual cortex to compose complex shapes and descriptors. While different aspects of the process have been extensively studied, as the lens adaptation or the feature detection, some other,as the feature fusion, have been mostly left aside. In this work we elaborate on the fusion of early vision primitives using generalizations of the Choquet integral, and novel aggregation operators that have been extensively studied in recent years. We propose to use generalizations of the Choquet integral to sensibly fuse elementary edge cues, in an attempt to model the behaviour of neurons in the early visual cortex. Our proposal leads to a full-framed edge detection algorithm, whose performance is put to the test in state-of-the-art boundary detection datasets.



### VM-MODNet: Vehicle Motion aware Moving Object Detection for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2104.10985v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10985v2)
- **Published**: 2021-04-22 10:46:55+00:00
- **Updated**: 2021-07-11 00:24:49+00:00
- **Authors**: Hazem Rashed, Ahmad El Sallab, Senthil Yogamani
- **Comment**: Accepted for Oral Presentation at IEEE Intelligent Transportation
  Systems Conference (ITSC) 2021
- **Journal**: None
- **Summary**: Moving object Detection (MOD) is a critical task in autonomous driving as moving agents around the ego-vehicle need to be accurately detected for safe trajectory planning. It also enables appearance agnostic detection of objects based on motion cues. There are geometric challenges like motion-parallax ambiguity which makes it a difficult problem. In this work, we aim to leverage the vehicle motion information and feed it into the model to have an adaptation mechanism based on ego-motion. The motivation is to enable the model to implicitly perform ego-motion compensation to improve performance. We convert the six degrees of freedom vehicle motion into a pixel-wise tensor which can be fed as input to the CNN model. The proposed model using Vehicle Motion Tensor (VMT) achieves an absolute improvement of 5.6% in mIoU over the baseline architecture. We also achieve state-of-the-art results on the public KITTI_MoSeg_Extended dataset even compared to methods which make use of LiDAR and additional input frames. Our model is also lightweight and runs at 85 fps on a TitanX GPU. Qualitative results are provided in https://youtu.be/ezbfjti-kTk.



### METGAN: Generative Tumour Inpainting and Modality Synthesis in Light Sheet Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2104.10993v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.10993v2)
- **Published**: 2021-04-22 11:18:17+00:00
- **Updated**: 2021-04-23 10:50:07+00:00
- **Authors**: Izabela Horvath, Johannes C. Paetzold, Oliver Schoppe, Rami Al-Maskari, Ivan Ezhov, Suprosanna Shit, Hongwei Li, Ali Ertuerk, Bjoern H. Menze
- **Comment**: None
- **Journal**: None
- **Summary**: Novel multimodal imaging methods are capable of generating extensive, super high resolution datasets for preclinical research. Yet, a massive lack of annotations prevents the broad use of deep learning to analyze such data. So far, existing generative models fail to mitigate this problem because of frequent labeling errors. In this paper, we introduce a novel generative method which leverages real anatomical information to generate realistic image-label pairs of tumours. We construct a dual-pathway generator, for the anatomical image and label, trained in a cycle-consistent setup, constrained by an independent, pretrained segmentor. The generated images yield significant quantitative improvement compared to existing methods. To validate the quality of synthesis, we train segmentation networks on a dataset augmented with the synthetic data, substantially improving the segmentation over baseline.



### Hazy Re-ID: An Interference Suppression Model For Domain Adaptation Person Re-identification Under Inclement Weather Condition
- **Arxiv ID**: http://arxiv.org/abs/2104.11004v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.11004v1)
- **Published**: 2021-04-22 11:59:27+00:00
- **Updated**: 2021-04-22 11:59:27+00:00
- **Authors**: Jian Pang, Dacheng Zhang, Huafeng Li, Weifeng Liu, Zhengtao Yu
- **Comment**: Accepted by ICME2021 as oral
- **Journal**: None
- **Summary**: In a conventional domain adaptation person Re-identification (Re-ID) task, both the training and test images in target domain are collected under the sunny weather. However, in reality, the pedestrians to be retrieved may be obtained under severe weather conditions such as hazy, dusty and snowing, etc. This paper proposes a novel Interference Suppression Model (ISM) to deal with the interference caused by the hazy weather in domain adaptation person Re-ID. A teacherstudent model is used in the ISM to distill the interference information at the feature level by reducing the discrepancy between the clear and the hazy intrinsic similarity matrix. Furthermore, in the distribution level, the extra discriminator is introduced to assist the student model make the interference feature distribution more clear. The experimental results show that the proposed method achieves the superior performance on two synthetic datasets than the stateof-the-art methods. The related code will be released online https://github.com/pangjian123/ISM-ReID.



### Unsupervised anomaly detection for a Smart Autonomous Robotic Assistant Surgeon (SARAS)using a deep residual autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2104.11008v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.11008v1)
- **Published**: 2021-04-22 12:10:38+00:00
- **Updated**: 2021-04-22 12:10:38+00:00
- **Authors**: Dinesh Jackson Samuel, Fabio Cuzzolin
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection in Minimally-Invasive Surgery (MIS) traditionally requires a human expert monitoring the procedure from a console. Data scarcity, on the other hand, hinders what would be a desirable migration towards autonomous robotic-assisted surgical systems. Automated anomaly detection systems in this area typically rely on classical supervised learning. Anomalous events in a surgical setting, however, are rare, making it difficult to capture data to train a detection model in a supervised fashion. In this work we thus propose an unsupervised approach to anomaly detection for robotic-assisted surgery based on deep residual autoencoders. The idea is to make the autoencoder learn the 'normal' distribution of the data and detect abnormal events deviating from this distribution by measuring the reconstruction error. The model is trained and validated upon both the publicly available Cholec80 dataset, provided with extra annotation, and on a set of videos captured on procedures using artificial anatomies ('phantoms') produced as part of the Smart Autonomous Robotic Assistant Surgeon (SARAS) project. The system achieves recall and precision equal to 78.4%, 91.5%, respectively, on Cholec80 and of 95.6%, 88.1% on the SARAS phantom dataset. The end-to-end system was developed and deployed as part of the SARAS demonstration platform for real-time anomaly detection with a processing time of about 25 ms per frame.



### Network Space Search for Pareto-Efficient Spaces
- **Arxiv ID**: http://arxiv.org/abs/2104.11014v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.11014v6)
- **Published**: 2021-04-22 12:23:53+00:00
- **Updated**: 2021-06-20 01:51:58+00:00
- **Authors**: Min-Fong Hong, Hao-Yun Chen, Min-Hung Chen, Yu-Syuan Xu, Hsien-Kai Kuo, Yi-Min Tsai, Hung-Jen Chen, Kevin Jou
- **Comment**: CVPRW2021 [Oral] (Efficient Deep Learning for Computer Vision
  Workshop). Website: https://minhungchen.netlify.app/publication/nss
- **Journal**: None
- **Summary**: Network spaces have been known as a critical factor in both handcrafted network designs or defining search spaces for Neural Architecture Search (NAS). However, an effective space involves tremendous prior knowledge and/or manual effort, and additional constraints are required to discover efficiency-aware architectures. In this paper, we define a new problem, Network Space Search (NSS), as searching for favorable network spaces instead of a single architecture. We propose an NSS method to directly search for efficient-aware network spaces automatically, reducing the manual effort and immense cost in discovering satisfactory ones. The resultant network spaces, named Elite Spaces, are discovered from Expanded Search Space with minimal human expertise imposed. The Pareto-efficient Elite Spaces are aligned with the Pareto front under various complexity constraints and can be further served as NAS search spaces, benefiting differentiable NAS approaches (e.g. In CIFAR-100, an averagely 2.3% lower error rate and 3.7% closer to target constraint than the baseline with around 90% fewer samples required to find satisfactory networks). Moreover, our NSS approach is capable of searching for superior spaces in future unexplored spaces, revealing great potential in searching for network spaces automatically. Website: https://minhungchen.netlify.app/publication/nss.



### Multi-task Semi-supervised Learning for Pulmonary Lobe Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.11017v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.11017v1)
- **Published**: 2021-04-22 12:33:30+00:00
- **Updated**: 2021-04-22 12:33:30+00:00
- **Authors**: Jingnan Jia, Zhiwei Zhai, M. Els Bakker, I. Hernandez Giron, Marius Staring, Berend C. Stoel
- **Comment**: 4 pages, to be published in ISBI 2021
- **Journal**: None
- **Summary**: Pulmonary lobe segmentation is an important preprocessing task for the analysis of lung diseases. Traditional methods relying on fissure detection or other anatomical features, such as the distribution of pulmonary vessels and airways, could provide reasonably accurate lobe segmentations. Deep learning based methods can outperform these traditional approaches, but require large datasets. Deep multi-task learning is expected to utilize labels of multiple different structures. However, commonly such labels are distributed over multiple datasets. In this paper, we proposed a multi-task semi-supervised model that can leverage information of multiple structures from unannotated datasets and datasets annotated with different structures. A focused alternating training strategy is presented to balance the different tasks. We evaluated the trained model on an external independent CT dataset. The results show that our model significantly outperforms single-task alternatives, improving the mean surface distance from 7.174 mm to 4.196 mm. We also demonstrated that our approach is successful for different network architectures as backbones.



### A Data-Adaptive Loss Function for Incomplete Data and Incremental Learning in Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.11020v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.11020v1)
- **Published**: 2021-04-22 12:46:50+00:00
- **Updated**: 2021-04-22 12:46:50+00:00
- **Authors**: Minh H. Vu, Gabriella Norman, Tufve Nyholm, Tommy Löfstedt
- **Comment**: None
- **Journal**: None
- **Summary**: In the last years, deep learning has dramatically improved the performances in a variety of medical image analysis applications. Among different types of deep learning models, convolutional neural networks have been among the most successful and they have been used in many applications in medical imaging.   Training deep convolutional neural networks often requires large amounts of image data to generalize well to new unseen images. It is often time-consuming and expensive to collect large amounts of data in the medical image domain due to expensive imaging systems, and the need for experts to manually make ground truth annotations. A potential problem arises if new structures are added when a decision support system is already deployed and in use. Since the field of radiation therapy is constantly developing, the new structures would also have to be covered by the decision support system.   In the present work, we propose a novel loss function, that adapts to the available data in order to utilize all available data, even when some have missing annotations. We demonstrate that the proposed loss function also works well in an incremental learning setting, where it can automatically incorporate new structures as they appear. Experiments on a large in-house data set show that the proposed method performs on par with baseline models, while greatly reducing the training time.



### Cycle and Semantic Consistent Adversarial Domain Adaptation for Reducing Simulation-to-Real Domain Shift in LiDAR Bird's Eye View
- **Arxiv ID**: http://arxiv.org/abs/2104.11021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11021v1)
- **Published**: 2021-04-22 12:47:37+00:00
- **Updated**: 2021-04-22 12:47:37+00:00
- **Authors**: Alejandro Barrera, Jorge Beltrán, Carlos Guindel, Jose Antonio Iglesias, Fernando García
- **Comment**: Submitted to IEEE International Conference on Intelligent
  Transportation Systems (ITSC2021)
- **Journal**: None
- **Summary**: The performance of object detection methods based on LiDAR information is heavily impacted by the availability of training data, usually limited to certain laser devices. As a result, the use of synthetic data is becoming popular when training neural network models, as both sensor specifications and driving scenarios can be generated ad-hoc. However, bridging the gap between virtual and real environments is still an open challenge, as current simulators cannot completely mimic real LiDAR operation. To tackle this issue, domain adaptation strategies are usually applied, obtaining remarkable results on vehicle detection when applied to range view (RV) and bird's eye view (BEV) projections while failing for smaller road agents. In this paper, we present a BEV domain adaptation method based on CycleGAN that uses prior semantic classification in order to preserve the information of small objects of interest during the domain adaptation process. The quality of the generated BEVs has been evaluated using a state-of-the-art 3D object detection framework at KITTI 3D Object Detection Benchmark. The obtained results show the advantages of the proposed method over the existing alternatives.



### Semi-Supervised Segmentation of Concrete Aggregate Using Consensus Regularisation and Prior Guidance
- **Arxiv ID**: http://arxiv.org/abs/2104.11028v1
- **DOI**: 10.5194/isprs-annals-V-2-2021-83-2021
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11028v1)
- **Published**: 2021-04-22 13:01:28+00:00
- **Updated**: 2021-04-22 13:01:28+00:00
- **Authors**: Max Coenen, Tobias Schack, Dries Beyer, Christian Heipke, Michael Haist
- **Comment**: None
- **Journal**: None
- **Summary**: In order to leverage and profit from unlabelled data, semi-supervised frameworks for semantic segmentation based on consistency training have been proven to be powerful tools to significantly improve the performance of purely supervised segmentation learning. However, the consensus principle behind consistency training has at least one drawback, which we identify in this paper: imbalanced label distributions within the data. To overcome the limitations of standard consistency training, we propose a novel semi-supervised framework for semantic segmentation, introducing additional losses based on prior knowledge. Specifically, we propose a light-weight architecture consisting of a shared encoder and a main decoder, which is trained in a supervised manner. An auxiliary decoder is added as additional branch in order to make use of unlabelled data based on consensus training, and we add additional constraints derived from prior information on the class distribution and on auto-encoder regularisation. Experiments performed on our "concrete aggregate dataset" presented in this paper demonstrate the effectiveness of the proposed approach, outperforming the segmentation results achieved by purely supervised segmentation and standard consistency training.



### Domain Adaptation for Semantic Segmentation via Patch-Wise Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.11056v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11056v1)
- **Published**: 2021-04-22 13:39:12+00:00
- **Updated**: 2021-04-22 13:39:12+00:00
- **Authors**: Weizhe Liu, David Ferstl, Samuel Schulter, Lukas Zebedin, Pascal Fua, Christian Leistner
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel approach to unsupervised and semi-supervised domain adaptation for semantic segmentation. Unlike many earlier methods that rely on adversarial learning for feature alignment, we leverage contrastive learning to bridge the domain gap by aligning the features of structurally similar label patches across domains. As a result, the networks are easier to train and deliver better performance. Our approach consistently outperforms state-of-the-art unsupervised and semi-supervised methods on two challenging domain adaptive segmentation tasks, particularly with a small number of target domain annotations. It can also be naturally extended to weakly-supervised domain adaptation, where only a minor drop in accuracy can save up to 75% of annotation cost.



### Relational Subsets Knowledge Distillation for Long-tailed Retinal Diseases Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.11057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11057v1)
- **Published**: 2021-04-22 13:39:33+00:00
- **Updated**: 2021-04-22 13:39:33+00:00
- **Authors**: Lie Ju, Xin Wang, Lin Wang, Tongliang Liu, Xin Zhao, Tom Drummond, Dwarikanath Mahapatra, Zongyuan Ge
- **Comment**: None
- **Journal**: None
- **Summary**: In the real world, medical datasets often exhibit a long-tailed data distribution (i.e., a few classes occupy most of the data, while most classes have rarely few samples), which results in a challenging imbalance learning scenario. For example, there are estimated more than 40 different kinds of retinal diseases with variable morbidity, however with more than 30+ conditions are very rare from the global patient cohorts, which results in a typical long-tailed learning problem for deep learning-based screening models. In this study, we propose class subset learning by dividing the long-tailed data into multiple class subsets according to prior knowledge, such as regions and phenotype information. It enforces the model to focus on learning the subset-specific knowledge. More specifically, there are some relational classes that reside in the fixed retinal regions, or some common pathological features are observed in both the majority and minority conditions. With those subsets learnt teacher models, then we are able to distill the multiple teacher models into a unified model with weighted knowledge distillation loss. The proposed framework proved to be effective for the long-tailed retinal diseases recognition task. The experimental results on two different datasets demonstrate that our method is flexible and can be easily plugged into many other state-of-the-art techniques with significant improvements.



### Learning Transferable 3D Adversarial Cloaks for Deep Trained Detectors
- **Arxiv ID**: http://arxiv.org/abs/2104.11101v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.11101v1)
- **Published**: 2021-04-22 14:36:08+00:00
- **Updated**: 2021-04-22 14:36:08+00:00
- **Authors**: Arman Maesumi, Mingkang Zhu, Yi Wang, Tianlong Chen, Zhangyang Wang, Chandrajit Bajaj
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel patch-based adversarial attack pipeline that trains adversarial patches on 3D human meshes. We sample triangular faces on a reference human mesh, and create an adversarial texture atlas over those faces. The adversarial texture is transferred to human meshes in various poses, which are rendered onto a collection of real-world background images. Contrary to the traditional patch-based adversarial attacks, where prior work attempts to fool trained object detectors using appended adversarial patches, this new form of attack is mapped into the 3D object world and back-propagated to the texture atlas through differentiable rendering. As such, the adversarial patch is trained under deformation consistent with real-world materials. In addition, and unlike existing adversarial patches, our new 3D adversarial patch is shown to fool state-of-the-art deep object detectors robustly under varying views, potentially leading to an attacking scheme that is persistently strong in the physical world.



### Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation
- **Arxiv ID**: http://arxiv.org/abs/2104.11116v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.11116v1)
- **Published**: 2021-04-22 15:10:26+00:00
- **Updated**: 2021-04-22 15:10:26+00:00
- **Authors**: Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang, Ziwei Liu
- **Comment**: Accepted to IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR), 2021. Code and models are available at
  https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS
- **Journal**: None
- **Summary**: While accurate lip synchronization has been achieved for arbitrary-subject audio-driven talking face generation, the problem of how to efficiently drive the head pose remains. Previous methods rely on pre-estimated structural information such as landmarks and 3D parameters, aiming to generate personalized rhythmic movements. However, the inaccuracy of such estimated information under extreme conditions would lead to degradation problems. In this paper, we propose a clean yet effective framework to generate pose-controllable talking faces. We operate on raw face images, using only a single photo as an identity reference. The key is to modularize audio-visual representations by devising an implicit low-dimension pose code. Substantially, both speech content and head pose information lie in a joint non-identity embedding space. While speech content information can be defined by learning the intrinsic synchronization between audio-visual modalities, we identify that a pose code will be complementarily learned in a modulated convolution-based reconstruction framework.   Extensive experiments show that our method generates accurately lip-synced talking faces whose poses are controllable by other videos. Moreover, our model has multiple advanced capabilities including extreme view robustness and talking face frontalization. Code, models, and demo videos are available at https://hangz-nju-cuhk.github.io/projects/PC-AVS.



### Sketch-QNet: A Quadruplet ConvNet for Color Sketch-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2104.11130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11130v1)
- **Published**: 2021-04-22 15:28:39+00:00
- **Updated**: 2021-04-22 15:28:39+00:00
- **Authors**: Anibal Fuentes, Jose M. Saavedra
- **Comment**: None
- **Journal**: None
- **Summary**: Architectures based on siamese networks with triplet loss have shown outstanding performance on the image-based similarity search problem. This approach attempts to discriminate between positive (relevant) and negative (irrelevant) items. However, it undergoes a critical weakness. Given a query, it cannot discriminate weakly relevant items, for instance, items of the same type but different color or texture as the given query, which could be a serious limitation for many real-world search applications. Therefore, in this work, we present a quadruplet-based architecture that overcomes the aforementioned weakness. Moreover, we present an instance of this quadruplet network, which we call Sketch-QNet, to deal with the color sketch-based image retrieval (CSBIR) problem, achieving new state-of-the-art results.



### NanoNet: Real-Time Polyp Segmentation in Video Capsule Endoscopy and Colonoscopy
- **Arxiv ID**: http://arxiv.org/abs/2104.11138v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.11138v1)
- **Published**: 2021-04-22 15:40:28+00:00
- **Updated**: 2021-04-22 15:40:28+00:00
- **Authors**: Debesh Jha, Nikhil Kumar Tomar, Sharib Ali, Michael A. Riegler, Håvard D. Johansen, Dag Johansen, Thomas de Lange, Pål Halvorsen
- **Comment**: Accepted at CBMS 2021
- **Journal**: None
- **Summary**: Deep learning in gastrointestinal endoscopy can assist to improve clinical performance and be helpful to assess lesions more accurately. To this extent, semantic segmentation methods that can perform automated real-time delineation of a region-of-interest, e.g., boundary identification of cancer or precancerous lesions, can benefit both diagnosis and interventions. However, accurate and real-time segmentation of endoscopic images is extremely challenging due to its high operator dependence and high-definition image quality. To utilize automated methods in clinical settings, it is crucial to design lightweight models with low latency such that they can be integrated with low-end endoscope hardware devices. In this work, we propose NanoNet, a novel architecture for the segmentation of video capsule endoscopy and colonoscopy images. Our proposed architecture allows real-time performance and has higher segmentation accuracy compared to other more complex ones. We use video capsule endoscopy and standard colonoscopy datasets with polyps, and a dataset consisting of endoscopy biopsies and surgical instruments, to evaluate the effectiveness of our approach. Our experiments demonstrate the increased performance of our architecture in terms of a trade-off between model complexity, speed, model parameters, and metric performances. Moreover, the resulting model size is relatively tiny, with only nearly 36,000 parameters compared to traditional deep learning approaches having millions of parameters.



### An End-to-End Computer Vision Methodology for Quantitative Metallography
- **Arxiv ID**: http://arxiv.org/abs/2104.11159v3
- **DOI**: None
- **Categories**: **cond-mat.mtrl-sci**, cs.CV, cs.LG, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/2104.11159v3)
- **Published**: 2021-04-22 16:29:44+00:00
- **Updated**: 2022-03-01 12:37:48+00:00
- **Authors**: Matan Rusanovsky, Ofer Beeri, Gal Oren
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2003.04226 Same
  text as last submission, changed the author list to correspond to the pdf
- **Journal**: None
- **Summary**: Metallography is crucial for a proper assessment of material's properties. It involves mainly the investigation of spatial distribution of grains and the occurrence and characteristics of inclusions or precipitates. This work presents an holistic artificial intelligence model for Anomaly Detection that automatically quantifies the degree of anomaly of impurities in alloys. We suggest the following examination process: (1) Deep semantic segmentation is performed on the inclusions (based on a suitable metallographic database of alloys and corresponding tags of inclusions), producing inclusions masks that are saved into a separated database. (2) Deep image inpainting is performed to fill the removed inclusions parts, resulting in 'clean' metallographic images, which contain the background of grains. (3) Grains' boundaries are marked using deep semantic segmentation (based on another metallographic database of alloys), producing boundaries that are ready for further inspection on the distribution of grains' size. (4) Deep anomaly detection and pattern recognition is performed on the inclusions masks to determine spatial, shape and area anomaly detection of the inclusions. Finally, the system recommends to an expert on areas of interests for further examination. The performance of the model is presented and analyzed based on few representative cases. Although the models presented here were developed for metallography analysis, most of them can be generalized to a wider set of problems in which anomaly detection of geometrical objects is desired. All models as well as the data-sets that were created for this work, are publicly available at https://github.com/Scientific-Computing-Lab-NRCN/MLography.



### Hierarchical growing grid networks for skeleton based action recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.11165v1
- **DOI**: 10.1016/j.cogsys.2020.05.002
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.11165v1)
- **Published**: 2021-04-22 16:35:32+00:00
- **Updated**: 2021-04-22 16:35:32+00:00
- **Authors**: Zahra Gharaee
- **Comment**: None
- **Journal**: Cognitive Systems Research, vol.63, pp.11-29 (2020)
- **Summary**: In this paper, a novel cognitive architecture for action recognition is developed by applying layers of growing grid neural networks.Using these layers makes the system capable of automatically arranging its representational structure. In addition to the expansion of the neural map during the growth phase, the system is provided with a prior knowledge of the input space, which increases the processing speed of the learning phase. Apart from two layers of growing grid networks the architecture is composed of a preprocessing layer, an ordered vector representation layer and a one-layer supervised neural network. These layers are designed to solve the action recognition problem. The first-layer growing grid receives the input data of human actions and the neural map generates an action pattern vector representing each action sequence by connecting the elicited activation of the trained map. The pattern vectors are then sent to the ordered vector representation layer to build the time-invariant input vectors of key activations for the second-layer growing grid. The second-layer growing grid categorizes the input vectors to the corresponding action clusters/sub-clusters and finally the one-layer supervised neural network labels the shaped clusters with action labels. Three experiments using different datasets of actions show that the system is capable of learning to categorize the actions quickly and efficiently. The performance of the growing grid architecture is com-pared with the results from a system based on Self-Organizing Maps, showing that the growing grid architecture performs significantly superior on the action recognition tasks.



### Heterogeneous Grid Convolution for Adaptive, Efficient, and Controllable Computation
- **Arxiv ID**: http://arxiv.org/abs/2104.11176v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11176v1)
- **Published**: 2021-04-22 17:02:59+00:00
- **Updated**: 2021-04-22 17:02:59+00:00
- **Authors**: Ryuhei Hamaguchi, Yasutaka Furukawa, Masaki Onishi, Ken Sakurada
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: This paper proposes a novel heterogeneous grid convolution that builds a graph-based image representation by exploiting heterogeneity in the image content, enabling adaptive, efficient, and controllable computations in a convolutional architecture. More concretely, the approach builds a data-adaptive graph structure from a convolutional layer by a differentiable clustering method, pools features to the graph, performs a novel direction-aware graph convolution, and unpool features back to the convolutional layer. By using the developed module, the paper proposes heterogeneous grid convolutional networks, highly efficient yet strong extension of existing architectures. We have evaluated the proposed approach on four image understanding tasks, semantic segmentation, object localization, road extraction, and salient object detection. The proposed method is effective on three of the four tasks. Especially, the method outperforms a strong baseline with more than 90% reduction in floating-point operations for semantic segmentation, and achieves the state-of-the-art result for road extraction. We will share our code, model, and data.



### VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text
- **Arxiv ID**: http://arxiv.org/abs/2104.11178v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.11178v3)
- **Published**: 2021-04-22 17:07:41+00:00
- **Updated**: 2021-12-07 04:01:25+00:00
- **Authors**: Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, Boqing Gong
- **Comment**: Published in the 35th Conference on Neural Information Processing
  Systems (NeurIPS 2021)
- **Journal**: None
- **Summary**: We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training. VATT's source code is publicly available.



### Maneuver-based Anchor Trajectory Hypotheses at Roundabouts
- **Arxiv ID**: http://arxiv.org/abs/2104.11180v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.11180v1)
- **Published**: 2021-04-22 17:08:29+00:00
- **Updated**: 2021-04-22 17:08:29+00:00
- **Authors**: Mohamed Hasan, Evangelos Paschalidis, Albert Solernou, He Wang, Gustav Markkula, Richard Romano
- **Comment**: Under Review IROS 2021
- **Journal**: None
- **Summary**: Predicting future behavior of the surrounding vehicles is crucial for self-driving platforms to safely navigate through other traffic. This is critical when making decisions like crossing an unsignalized intersection. We address the problem of vehicle motion prediction in a challenging roundabout environment by learning from human driver data. We extend existing recurrent encoder-decoder models to be advantageously combined with anchor trajectories to predict vehicle behaviors on a roundabout. Drivers' intentions are encoded by a set of maneuvers that correspond to semantic driving concepts. Accordingly, our model employs a set of maneuver-specific anchor trajectories that cover the space of possible outcomes at the roundabout. The proposed model can output a multi-modal distribution over the predicted future trajectories based on the maneuver-specific anchors. We evaluate our model using the public RounD dataset and the experiment results show the effectiveness of the proposed maneuver-based anchor regression in improving prediction accuracy, reducing the average RMSE to 28% less than the best baseline. Our code is available at https://github.com/m-hasan-n/roundabout.



### H2O: Two Hands Manipulating Objects for First Person Interaction Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.11181v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11181v2)
- **Published**: 2021-04-22 17:10:42+00:00
- **Updated**: 2021-08-24 15:21:38+00:00
- **Authors**: Taein Kwon, Bugra Tekin, Jan Stuhmer, Federica Bogo, Marc Pollefeys
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: We present a comprehensive framework for egocentric interaction recognition using markerless 3D annotations of two hands manipulating objects. To this end, we propose a method to create a unified dataset for egocentric 3D interaction recognition. Our method produces annotations of the 3D pose of two hands and the 6D pose of the manipulated objects, along with their interaction labels for each frame. Our dataset, called H2O (2 Hands and Objects), provides synchronized multi-view RGB-D images, interaction labels, object classes, ground-truth 3D poses for left & right hands, 6D object poses, ground-truth camera poses, object meshes and scene point clouds. To the best of our knowledge, this is the first benchmark that enables the study of first-person actions with the use of the pose of both left and right hands manipulating objects and presents an unprecedented level of detail for egocentric 3D interaction recognition. We further propose the method to predict interaction classes by estimating the 3D pose of two hands and the 6D pose of the manipulated objects, jointly from RGB images. Our method models both inter- and intra-dependencies between both hands and objects by learning the topology of a graph convolutional network that predicts interactions. We show that our method facilitated by this dataset establishes a strong baseline for joint hand-object pose estimation and achieves state-of-the-art accuracy for first person interaction recognition.



### Aerial Scene Understanding in The Wild: Multi-Scene Recognition via Prototype-based Memory Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.11200v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11200v1)
- **Published**: 2021-04-22 17:32:14+00:00
- **Updated**: 2021-04-22 17:32:14+00:00
- **Authors**: Yuansheng Hua, Lichao Moua, Jianzhe Lin, Konrad Heidler, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Aerial scene recognition is a fundamental visual task and has attracted an increasing research interest in the last few years. Most of current researches mainly deploy efforts to categorize an aerial image into one scene-level label, while in real-world scenarios, there often exist multiple scenes in a single image. Therefore, in this paper, we propose to take a step forward to a more practical and challenging task, namely multi-scene recognition in single images. Moreover, we note that manually yielding annotations for such a task is extraordinarily time- and labor-consuming. To address this, we propose a prototype-based memory network to recognize multiple scenes in a single image by leveraging massive well-annotated single-scene images. The proposed network consists of three key components: 1) a prototype learning module, 2) a prototype-inhabiting external memory, and 3) a multi-head attention-based memory retrieval module. To be more specific, we first learn the prototype representation of each aerial scene from single-scene aerial image datasets and store it in an external memory. Afterwards, a multi-head attention-based memory retrieval module is devised to retrieve scene prototypes relevant to query multi-scene images for final predictions. Notably, only a limited number of annotated multi-scene images are needed in the training phase. To facilitate the progress of aerial scene recognition, we produce a new multi-scene aerial image (MAI) dataset. Experimental results on variant dataset configurations demonstrate the effectiveness of our network. Our dataset and codes are publicly available.



### Fully Convolutional Line Parsing
- **Arxiv ID**: http://arxiv.org/abs/2104.11207v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11207v3)
- **Published**: 2021-04-22 17:41:12+00:00
- **Updated**: 2022-12-29 15:25:46+00:00
- **Authors**: Xili Dai, Haigang Gong, Shuai Wu, Xiaojun Yuan, Yi Ma
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: We present a one-stage Fully Convolutional Line Parsing network (F-Clip) that detects line segments from images. The proposed network is very simple and flexible with variations that gracefully trade off between speed and accuracy for different applications. F-Clip detects line segments in an end-to-end fashion by predicting each line's center position, length, and angle. We further customize the design of convolution kernels of our fully convolutional network to effectively exploit the statistical priors of the distribution of line angles in real image datasets. We conduct extensive experiments and show that our method achieves a significantly better trade-off between efficiency and accuracy, resulting in a real-time line detector at up to 73 FPS on a single GPU. Such inference speed makes our method readily applicable to real-time tasks without compromising any accuracy of previous methods. Moreover, when equipped with a performance-improving backbone network, F-Clip is able to significantly outperform all state-of-the-art line detectors on accuracy at a similar or even higher frame rate. In other word, under same inference speed, F-Clip always achieving best accuracy compare with other methods. Source code https://github.com/Delay-Xili/F-Clip.



### Deep Video Matting via Spatio-Temporal Alignment and Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2104.11208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11208v1)
- **Published**: 2021-04-22 17:42:08+00:00
- **Updated**: 2021-04-22 17:42:08+00:00
- **Authors**: Yanan Sun, Guanzhi Wang, Qiao Gu, Chi-Keung Tang, Yu-Wing Tai
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the significant progress made by deep learning in natural image matting, there has been so far no representative work on deep learning for video matting due to the inherent technical challenges in reasoning temporal domain and lack of large-scale video matting datasets. In this paper, we propose a deep learning-based video matting framework which employs a novel and effective spatio-temporal feature aggregation module (ST-FAM). As optical flow estimation can be very unreliable within matting regions, ST-FAM is designed to effectively align and aggregate information across different spatial scales and temporal frames within the network decoder. To eliminate frame-by-frame trimap annotations, a lightweight interactive trimap propagation network is also introduced. The other contribution consists of a large-scale video matting dataset with groundtruth alpha mattes for quantitative evaluation and real-world high-resolution videos with trimaps for qualitative evaluation. Quantitative and qualitative experimental results show that our framework significantly outperforms conventional video matting and deep image matting methods applied to video in presence of multi-frame temporal information.



### ManipulaTHOR: A Framework for Visual Object Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2104.11213v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.11213v1)
- **Published**: 2021-04-22 17:49:04+00:00
- **Updated**: 2021-04-22 17:49:04+00:00
- **Authors**: Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, Roozbeh Mottaghi
- **Comment**: CVPR 2021 -- (Oral presentation)
- **Journal**: None
- **Summary**: The domain of Embodied AI has recently witnessed substantial progress, particularly in navigating agents within their environments. These early successes have laid the building blocks for the community to tackle tasks that require agents to actively interact with objects in their environment. Object manipulation is an established research domain within the robotics community and poses several challenges including manipulator motion, grasping and long-horizon planning, particularly when dealing with oft-overlooked practical setups involving visually rich and complex scenes, manipulation using mobile agents (as opposed to tabletop manipulation), and generalization to unseen environments and objects. We propose a framework for object manipulation built upon the physics-enabled, visually rich AI2-THOR framework and present a new challenge to the Embodied AI community known as ArmPointNav. This task extends the popular point navigation task to object manipulation and offers new challenges including 3D obstacle avoidance, manipulating objects in the presence of occlusion, and multi-object manipulation that necessitates long term planning. Popular learning paradigms that are successful on PointNav challenges show promise, but leave a large room for improvement.



### Hierarchical Motion Understanding via Motion Programs
- **Arxiv ID**: http://arxiv.org/abs/2104.11216v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.11216v1)
- **Published**: 2021-04-22 17:49:59+00:00
- **Updated**: 2021-04-22 17:49:59+00:00
- **Authors**: Sumith Kulal, Jiayuan Mao, Alex Aiken, Jiajun Wu
- **Comment**: CVPR 2021. First two authors contributed equally. Project page:
  https://sumith1896.github.io/motion2prog/
- **Journal**: None
- **Summary**: Current approaches to video analysis of human motion focus on raw pixels or keypoints as the basic units of reasoning. We posit that adding higher-level motion primitives, which can capture natural coarser units of motion such as backswing or follow-through, can be used to improve downstream analysis tasks. This higher level of abstraction can also capture key features, such as loops of repeated primitives, that are currently inaccessible at lower levels of representation. We therefore introduce Motion Programs, a neuro-symbolic, program-like representation that expresses motions as a composition of high-level primitives. We also present a system for automatically inducing motion programs from videos of human motion and for leveraging motion programs in video synthesis. Experiments show that motion programs can accurately describe a diverse set of human motions and the inferred programs contain semantically meaningful motion primitives, such as arm swings and jumping jacks. Our representation also benefits downstream tasks such as video interpolation and video prediction and outperforms off-the-shelf models. We further demonstrate how these programs can detect diverse kinds of repetitive motion and facilitate interactive video editing.



### Opening up Open-World Tracking
- **Arxiv ID**: http://arxiv.org/abs/2104.11221v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11221v2)
- **Published**: 2021-04-22 17:58:15+00:00
- **Updated**: 2022-03-28 23:51:32+00:00
- **Authors**: Yang Liu, Idil Esen Zulfikar, Jonathon Luiten, Achal Dave, Deva Ramanan, Bastian Leibe, Aljoša Ošep, Laura Leal-Taixé
- **Comment**: CVPR 2022 (Oral). https://openworldtracking.github.io/
- **Journal**: None
- **Summary**: Tracking and detecting any object, including ones never-seen-before during model training, is a crucial but elusive capability of autonomous systems. An autonomous agent that is blind to never-seen-before objects poses a safety hazard when operating in the real world - and yet this is how almost all current systems work. One of the main obstacles towards advancing tracking any object is that this task is notoriously difficult to evaluate. A benchmark that would allow us to perform an apples-to-apples comparison of existing efforts is a crucial first step towards advancing this important research field. This paper addresses this evaluation deficit and lays out the landscape and evaluation methodology for detecting and tracking both known and unknown objects in the open-world setting. We propose a new benchmark, TAO-OW: Tracking Any Object in an Open World, analyze existing efforts in multi-object tracking, and construct a baseline for this task while highlighting future challenges. We hope to open a new front in multi-object tracking research that will hopefully bring us a step closer to intelligent systems that can operate safely in the real world. https://openworldtracking.github.io/



### On Aliased Resizing and Surprising Subtleties in GAN Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2104.11222v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.11222v3)
- **Published**: 2021-04-22 17:58:38+00:00
- **Updated**: 2022-01-21 01:58:25+00:00
- **Authors**: Gaurav Parmar, Richard Zhang, Jun-Yan Zhu
- **Comment**: GitHub: https://www.github.com/GaParmar/clean-fid Website:
  https://www.cs.cmu.edu/~clean-fid/
- **Journal**: None
- **Summary**: Metrics for evaluating generative models aim to measure the discrepancy between real and generated images. The often-used Frechet Inception Distance (FID) metric, for example, extracts "high-level" features using a deep network from the two sets. However, we find that the differences in "low-level" preprocessing, specifically image resizing and compression, can induce large variations and have unforeseen consequences. For instance, when resizing an image, e.g., with a bilinear or bicubic kernel, signal processing principles mandate adjusting prefilter width depending on the downsampling factor, to antialias to the appropriate bandwidth. However, commonly-used implementations use a fixed-width prefilter, resulting in aliasing artifacts. Such aliasing leads to corruptions in the feature extraction downstream. Next, lossy compression, such as JPEG, is commonly used to reduce the file size of an image. Although designed to minimally degrade the perceptual quality of an image, the operation also produces variations downstream. Furthermore, we show that if compression is used on real training images, FID can actually improve if the generated images are also subsequently compressed. This paper shows that choices in low-level image processing have been an underappreciated aspect of generative modeling. We identify and characterize variations in generative modeling development pipelines, provide recommendations based on signal processing principles, and release a reference implementation to facilitate future comparisons.



### KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control
- **Arxiv ID**: http://arxiv.org/abs/2104.11224v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2104.11224v1)
- **Published**: 2021-04-22 17:59:08+00:00
- **Updated**: 2021-04-22 17:59:08+00:00
- **Authors**: Tomas Jakab, Richard Tucker, Ameesh Makadia, Jiajun Wu, Noah Snavely, Angjoo Kanazawa
- **Comment**: CVPR 2021 (oral). Project page:
  http://tomasjakab.github.io/KeypointDeformer
- **Journal**: None
- **Summary**: We introduce KeypointDeformer, a novel unsupervised method for shape control through automatically discovered 3D keypoints. We cast this as the problem of aligning a source 3D object to a target 3D object from the same object category. Our method analyzes the difference between the shapes of the two objects by comparing their latent representations. This latent representation is in the form of 3D keypoints that are learned in an unsupervised way. The difference between the 3D keypoints of the source and the target objects then informs the shape deformation algorithm that deforms the source object into the target object. The whole model is learned end-to-end and simultaneously discovers 3D keypoints while learning to use them for deforming object shapes. Our approach produces intuitive and semantically consistent control of shape deformations. Moreover, our discovered 3D keypoints are consistent across object category instances despite large shape variations. As our method is unsupervised, it can be readily deployed to new object categories without requiring annotations for 3D keypoints and deformations.



### Pri3D: Can 3D Priors Help 2D Representation Learning?
- **Arxiv ID**: http://arxiv.org/abs/2104.11225v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11225v3)
- **Published**: 2021-04-22 17:59:30+00:00
- **Updated**: 2021-12-18 17:15:57+00:00
- **Authors**: Ji Hou, Saining Xie, Benjamin Graham, Angela Dai, Matthias Nießner
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Recent advances in 3D perception have shown impressive progress in understanding geometric structures of 3Dshapes and even scenes. Inspired by these advances in geometric understanding, we aim to imbue image-based perception with representations learned under geometric constraints. We introduce an approach to learn view-invariant,geometry-aware representations for network pre-training, based on multi-view RGB-D data, that can then be effectively transferred to downstream 2D tasks. We propose to employ contrastive learning under both multi-view im-age constraints and image-geometry constraints to encode3D priors into learned 2D representations. This results not only in improvement over 2D-only representation learning on the image-based tasks of semantic segmentation, instance segmentation, and object detection on real-world in-door datasets, but moreover, provides significant improvement in the low data regime. We show a significant improvement of 6.0% on semantic segmentation on full data as well as 11.9% on 20% data against baselines on ScanNet.



### Multiscale Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.11227v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.11227v1)
- **Published**: 2021-04-22 17:59:45+00:00
- **Updated**: 2021-04-22 17:59:45+00:00
- **Authors**: Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, Christoph Feichtenhofer
- **Comment**: Technical report
- **Journal**: None
- **Summary**: We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10x more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast



### Cross-Domain and Disentangled Face Manipulation with 3D Guidance
- **Arxiv ID**: http://arxiv.org/abs/2104.11228v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2104.11228v3)
- **Published**: 2021-04-22 17:59:50+00:00
- **Updated**: 2022-02-28 16:53:45+00:00
- **Authors**: Can Wang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao
- **Comment**: Accepted by TVCG, final version
- **Journal**: None
- **Summary**: Face image manipulation via three-dimensional guidance has been widely applied in various interactive scenarios due to its semantically-meaningful understanding and user-friendly controllability. However, existing 3D-morphable-model-based manipulation methods are not directly applicable to out-of-domain faces, such as non-photorealistic paintings, cartoon portraits, or even animals, mainly due to the formidable difficulties in building the model for each specific face domain. To overcome this challenge, we propose, as far as we know, the first method to manipulate faces in arbitrary domains using human 3DMM. This is achieved through two major steps: 1) disentangled mapping from 3DMM parameters to the latent space embedding of a pre-trained StyleGAN2 that guarantees disentangled and precise controls for each semantic attribute; and 2) cross-domain adaptation that bridges domain discrepancies and makes human 3DMM applicable to out-of-domain faces by enforcing a consistent latent space embedding. Experiments and comparisons demonstrate the superiority of our high-quality semantic manipulation method on a variety of face domains with all major 3D facial attributes controllable-pose, expression, shape, albedo, and illumination. Moreover, we develop an intuitive editing interface to support user-friendly control and instant feedback. Our project page is https://cassiepython.github.io/cddfm3d/index.html



### Equivariant Wavelets: Fast Rotation and Translation Invariant Wavelet Scattering Transforms
- **Arxiv ID**: http://arxiv.org/abs/2104.11244v1
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.IM
- **Links**: [PDF](http://arxiv.org/pdf/2104.11244v1)
- **Published**: 2021-04-22 18:00:01+00:00
- **Updated**: 2021-04-22 18:00:01+00:00
- **Authors**: Andrew K. Saydjari, Douglas P. Finkbeiner
- **Comment**: 20 pages, 14 figures, submitted to IEEE Signal Processing
- **Journal**: None
- **Summary**: Wavelet scattering networks, which are convolutional neural networks (CNNs) with fixed filters and weights, are promising tools for image analysis. Imposing symmetry on image statistics can improve human interpretability, aid in generalization, and provide dimension reduction. In this work, we introduce a fast-to-compute, translationally invariant and rotationally equivariant wavelet scattering network (EqWS) and filter bank of wavelets (triglets). We demonstrate the interpretability and quantify the invariance/equivariance of the coefficients, briefly commenting on difficulties with implementing scale equivariance. On MNIST, we show that training on a rotationally invariant reduction of the coefficients maintains rotational invariance when generalized to test data and visualize residual symmetry breaking terms. Rotation equivariance is leveraged to estimate the rotation angle of digits and reconstruct the full rotation dependence of each coefficient from a single angle. We benchmark EqWS with linear classifiers on EMNIST and CIFAR-10/100, introducing a new second-order, cross-color channel coupling for the color images. We conclude by comparing the performance of an isotropic reduction of the scattering coefficients and RWST, a previous coefficient reduction, on an isotropic classification of magnetohydrodynamic simulations with astrophysical relevance.



### Landmark-Aware and Part-based Ensemble Transfer Learning Network for Facial Expression Recognition from Static images
- **Arxiv ID**: http://arxiv.org/abs/2104.11274v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.11274v2)
- **Published**: 2021-04-22 18:38:33+00:00
- **Updated**: 2021-11-04 10:40:50+00:00
- **Authors**: Rohan Wadhawan, Tapan K. Gandhi
- **Comment**: None
- **Journal**: None
- **Summary**: Facial Expression Recognition from static images is a challenging problem in computer vision applications. Convolutional Neural Network (CNN), the state-of-the-art method for various computer vision tasks, has had limited success in predicting expressions from faces having extreme poses, illumination, and occlusion conditions. To mitigate this issue, CNNs are often accompanied by techniques like transfer, multi-task, or ensemble learning that often provide high accuracy at the cost of increased computational complexity. In this work, we propose a Part-based Ensemble Transfer Learning network that models how humans recognize facial expressions by correlating the spatial orientation pattern of the facial features with a specific expression. It consists of 5 sub-networks, and each sub-network performs transfer learning from one of the five subsets of facial landmarks: eyebrows, eyes, nose, mouth, or jaw to expression classification. We show that our proposed ensemble network uses visual patterns emanating from facial muscles' motor movements to predict expressions and demonstrate the usefulness of transfer learning from Facial Landmark Localization to Facial Expression Recognition. We test the proposed network on the CK+, JAFFE, and SFEW datasets, and it outperforms the benchmark for CK+ and JAFFE datasets by 0.51% and 5.34%, respectively. Additionally, the proposed ensemble network consists of only 1.65M model parameters, ensuring computational efficiency during training and real-time deployment. Grad-CAM visualizations of our proposed ensemble highlight the complementary nature of its sub-networks, a key design parameter of an effective ensemble network. Lastly, cross-dataset evaluation results reveal that our proposed ensemble has a high generalization capacity, making it suitable for real-world usage.



### Motion Representations for Articulated Animation
- **Arxiv ID**: http://arxiv.org/abs/2104.11280v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11280v1)
- **Published**: 2021-04-22 18:53:56+00:00
- **Updated**: 2021-04-22 18:53:56+00:00
- **Authors**: Aliaksandr Siarohin, Oliver J. Woodford, Jian Ren, Menglei Chai, Sergey Tulyakov
- **Comment**: None
- **Journal**: CVPR 2021
- **Summary**: We propose novel motion representations for animating articulated objects consisting of distinct parts. In a completely unsupervised manner, our method identifies object parts, tracks them in a driving video, and infers their motions by considering their principal axes. In contrast to the previous keypoint-based works, our method extracts meaningful and consistent regions, describing locations, shape, and pose. The regions correspond to semantically relevant and distinct object parts, that are more easily detected in frames of the driving video. To force decoupling of foreground from background, we model non-object related global motion with an additional affine transformation. To facilitate animation and prevent the leakage of the shape of the driving object, we disentangle shape and pose of objects in the region space. Our model can animate a variety of objects, surpassing previous methods by a large margin on existing benchmarks. We present a challenging new benchmark with high-resolution videos and show that the improvement is particularly pronounced when articulated objects are considered, reaching 96.6% user preference vs. the state of the art.



### Connecting Hamilton--Jacobi partial differential equations with maximum a posteriori and posterior mean estimators for some non-convex priors
- **Arxiv ID**: http://arxiv.org/abs/2104.11285v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.11285v1)
- **Published**: 2021-04-22 19:00:37+00:00
- **Updated**: 2021-04-22 19:00:37+00:00
- **Authors**: Jérôme Darbon, Gabriel P. Langlois, Tingwei Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Many imaging problems can be formulated as inverse problems expressed as finite-dimensional optimization problems. These optimization problems generally consist of minimizing the sum of a data fidelity and regularization terms. In [23,26], connections between these optimization problems and (multi-time) Hamilton--Jacobi partial differential equations have been proposed under the convexity assumptions of both the data fidelity and regularization terms. In particular, under these convexity assumptions, some representation formulas for a minimizer can be obtained. From a Bayesian perspective, such a minimizer can be seen as a maximum a posteriori estimator. In this chapter, we consider a certain class of non-convex regularizations and show that similar representation formulas for the minimizer can also be obtained. This is achieved by leveraging min-plus algebra techniques that have been originally developed for solving certain Hamilton--Jacobi partial differential equations arising in optimal control. Note that connections between viscous Hamilton--Jacobi partial differential equations and Bayesian posterior mean estimators with Gaussian data fidelity terms and log-concave priors have been highlighted in [25]. We also present similar results for certain Bayesian posterior mean estimators with Gaussian data fidelity and certain non-log-concave priors using an analogue of min-plus algebra techniques.



### H-Net: Unsupervised Attention-based Stereo Depth Estimation Leveraging Epipolar Geometry
- **Arxiv ID**: http://arxiv.org/abs/2104.11288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.11288v1)
- **Published**: 2021-04-22 19:16:35+00:00
- **Updated**: 2021-04-22 19:16:35+00:00
- **Authors**: Baoru Huang, Jian-Qing Zheng, Stamatia Giannarou, Daniel S. Elson
- **Comment**: None
- **Journal**: None
- **Summary**: Depth estimation from a stereo image pair has become one of the most explored applications in computer vision, with most of the previous methods relying on fully supervised learning settings. However, due to the difficulty in acquiring accurate and scalable ground truth data, the training of fully supervised methods is challenging. As an alternative, self-supervised methods are becoming more popular to mitigate this challenge. In this paper, we introduce the H-Net, a deep-learning framework for unsupervised stereo depth estimation that leverages epipolar geometry to refine stereo matching. For the first time, a Siamese autoencoder architecture is used for depth estimation which allows mutual information between the rectified stereo images to be extracted. To enforce the epipolar constraint, the mutual epipolar attention mechanism has been designed which gives more emphasis to correspondences of features which lie on the same epipolar line while learning mutual information between the input stereo pair. Stereo correspondences are further enhanced by incorporating semantic information to the proposed attention mechanism. More specifically, the optimal transport algorithm is used to suppress attention and eliminate outliers in areas not visible in both cameras. Extensive experiments on KITTI2015 and Cityscapes show that our method outperforms the state-ofthe-art unsupervised stereo depth estimation methods while closing the gap with the fully supervised approaches.



