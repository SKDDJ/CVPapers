# Arxiv Papers in cs.CV on 2021-04-15
### PURE: Passive mUlti-peRson idEntification via Deep Footstep Separation and Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.07177v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.07177v1)
- **Published**: 2021-04-15 00:28:05+00:00
- **Updated**: 2021-04-15 00:28:05+00:00
- **Authors**: Chao Cai, Ruinan Jin, Peng Wang, Liyuan Ye, Hongbo Jiang, Jun Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, \textit{passive behavioral biometrics} (e.g., gesture or footstep) have become promising complements to conventional user identification methods (e.g., face or fingerprint) under special situations, yet existing sensing technologies require lengthy measurement traces and cannot identify multiple users at the same time. To this end, we propose \systemname\ as a passive multi-person identification system leveraging deep learning enabled footstep separation and recognition. \systemname\ passively identifies a user by deciphering the unique "footprints" in its footstep. Different from existing gait-enabled recognition systems incurring a long sensing delay to acquire many footsteps, \systemname\ can recognize a person by as few as only one step, substantially cutting the identification latency. To make \systemname\ adaptive to walking pace variations, environmental dynamics, and even unseen targets, we apply an adversarial learning technique to improve its domain generalisability and identification accuracy. Finally, \systemname\ can defend itself against replay attack, enabled by the richness of footstep and spatial awareness. We implement a \systemname\ prototype using commodity hardware and evaluate it in typical indoor settings. Evaluation results demonstrate a cross-domain identification accuracy of over 90\%.



### Convolutions for Spatial Interaction Modeling
- **Arxiv ID**: http://arxiv.org/abs/2104.07182v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07182v3)
- **Published**: 2021-04-15 00:41:30+00:00
- **Updated**: 2022-06-08 15:28:54+00:00
- **Authors**: Zhaoen Su, Chao Wang, David Bradley, Carlos Vallespi-Gonzalez, Carl Wellington, Nemanja Djuric
- **Comment**: Supplementary material included, to appear at CVPR2022
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (2022)
- **Summary**: In many different fields interactions between objects play a critical role in determining their behavior. Graph neural networks (GNNs) have emerged as a powerful tool for modeling interactions, although often at the cost of adding considerable complexity and latency. In this paper, we consider the problem of spatial interaction modeling in the context of predicting the motion of actors around autonomous vehicles, and investigate alternatives to GNNs. We revisit 2D convolutions and show that they can demonstrate comparable performance to graph networks in modeling spatial interactions with lower latency, thus providing an effective and efficient alternative in time-critical systems. Moreover, we propose a novel interaction loss to further improve the interaction modeling of the considered methods.



### Graph-based Thermal-Inertial SLAM with Probabilistic Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.07196v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.07196v3)
- **Published**: 2021-04-15 01:39:15+00:00
- **Updated**: 2021-10-29 11:59:15+00:00
- **Authors**: Muhamad Risqi U. Saputra, Chris Xiaoxuan Lu, Pedro P. B. de Gusmao, Bing Wang, Andrew Markham, Niki Trigoni
- **Comment**: Accepted to IEEE Transactions on Robotics
- **Journal**: None
- **Summary**: Simultaneous Localization and Mapping (SLAM) system typically employ vision-based sensors to observe the surrounding environment. However, the performance of such systems highly depends on the ambient illumination conditions. In scenarios with adverse visibility or in the presence of airborne particulates (e.g. smoke, dust, etc.), alternative modalities such as those based on thermal imaging and inertial sensors are more promising. In this paper, we propose the first complete thermal-inertial SLAM system which combines neural abstraction in the SLAM front end with robust pose graph optimization in the SLAM back end. We model the sensor abstraction in the front end by employing probabilistic deep learning parameterized by Mixture Density Networks (MDN). Our key strategies to successfully model this encoding from thermal imagery are the usage of normalized 14-bit radiometric data, the incorporation of hallucinated visual (RGB) features, and the inclusion of feature selection to estimate the MDN parameters. To enable a full SLAM system, we also design an efficient global image descriptor which is able to detect loop closures from thermal embedding vectors. We performed extensive experiments and analysis using three datasets, namely self-collected ground robot and handheld data taken in indoor environment, and one public dataset (SubT-tunnel) collected in underground tunnel. Finally, we demonstrate that an accurate thermal-inertial SLAM system can be realized in conditions of both benign and adverse visibility.



### Learning structure-aware semantic segmentation with image-level supervision
- **Arxiv ID**: http://arxiv.org/abs/2104.07216v1
- **DOI**: 10.1109/IJCNN52387.2021.9533846
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07216v1)
- **Published**: 2021-04-15 03:33:20+00:00
- **Updated**: 2021-04-15 03:33:20+00:00
- **Authors**: Jiawei Liu, Jing Zhang, Yicong Hong, Nick Barnes
- **Comment**: None
- **Journal**: 2021 International Joint Conference on Neural Networks (IJCNN)
- **Summary**: Compared with expensive pixel-wise annotations, image-level labels make it possible to learn semantic segmentation in a weakly-supervised manner. Within this pipeline, the class activation map (CAM) is obtained and further processed to serve as a pseudo label to train the semantic segmentation model in a fully-supervised manner. In this paper, we argue that the lost structure information in CAM limits its application in downstream semantic segmentation, leading to deteriorated predictions. Furthermore, the inconsistent class activation scores inside the same object contradicts the common sense that each region of the same object should belong to the same semantic category. To produce sharp prediction with structure information, we introduce an auxiliary semantic boundary detection module, which penalizes the deteriorated predictions. Furthermore, we adopt smoothness loss to encourage prediction inside the object to be consistent. Experimental results on the PASCAL-VOC dataset illustrate the effectiveness of the proposed solution.



### An Improved Real-Time Face Recognition System at Low Resolution Based on Local Binary Pattern Histogram Algorithm and CLAHE
- **Arxiv ID**: http://arxiv.org/abs/2104.07234v1
- **DOI**: 10.4236/opj.2021.114005
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.07234v1)
- **Published**: 2021-04-15 04:54:29+00:00
- **Updated**: 2021-04-15 04:54:29+00:00
- **Authors**: Kamal Chandra Paul, Semih Aslan
- **Comment**: Journal, Optics and Photonics Journal
- **Journal**: Optics and Photonics Journal, 2021, 11, 63-78
- **Summary**: This research presents an improved real-time face recognition system at a low resolution of 15 pixels with pose and emotion and resolution variations. We have designed our datasets named LRD200 and LRD100, which have been used for training and classification. The face detection part uses the Viola-Jones algorithm, and the face recognition part receives the face image from the face detection part to process it using the Local Binary Pattern Histogram (LBPH) algorithm with preprocessing using contrast limited adaptive histogram equalization (CLAHE) and face alignment. The face database in this system can be updated via our custom-built standalone android app and automatic restarting of the training and recognition process with an updated database. Using our proposed algorithm, a real-time face recognition accuracy of 78.40% at 15 px and 98.05% at 45 px have been achieved using the LRD200 database containing 200 images per person. With 100 images per person in the database (LRD100) the achieved accuracies are 60.60% at 15 px and 95% at 45 px respectively. A facial deflection of about 30 degrees on either side from the front face showed an average face recognition precision of 72.25% - 81.85%. This face recognition system can be employed for law enforcement purposes, where the surveillance camera captures a low-resolution image because of the distance of a person from the camera. It can also be used as a surveillance system in airports, bus stations, etc., to reduce the risk of possible criminal threats.



### Vision Transformer using Low-level Chest X-ray Feature Corpus for COVID-19 Diagnosis and Severity Quantification
- **Arxiv ID**: http://arxiv.org/abs/2104.07235v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.07235v1)
- **Published**: 2021-04-15 04:54:48+00:00
- **Updated**: 2021-04-15 04:54:48+00:00
- **Authors**: Sangjoon Park, Gwanghyun Kim, Yujin Oh, Joon Beom Seo, Sang Min Lee, Jin Hwan Kim, Sungjun Moon, Jae-Kwang Lim, Jong Chul Ye
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Developing a robust algorithm to diagnose and quantify the severity of COVID-19 using Chest X-ray (CXR) requires a large number of well-curated COVID-19 datasets, which is difficult to collect under the global COVID-19 pandemic. On the other hand, CXR data with other findings are abundant. This situation is ideally suited for the Vision Transformer (ViT) architecture, where a lot of unlabeled data can be used through structural modeling by the self-attention mechanism. However, the use of existing ViT is not optimal, since feature embedding through direct patch flattening or ResNet backbone in the standard ViT is not intended for CXR. To address this problem, here we propose a novel Vision Transformer that utilizes low-level CXR feature corpus obtained from a backbone network that extracts common CXR findings. Specifically, the backbone network is first trained with large public datasets to detect common abnormal findings such as consolidation, opacity, edema, etc. Then, the embedded features from the backbone network are used as corpora for a Transformer model for the diagnosis and the severity quantification of COVID-19. We evaluate our model on various external test datasets from totally different institutions to evaluate the generalization capability. The experimental results confirm that our model can achieve the state-of-the-art performance in both diagnosis and severity quantification tasks with superior generalization capability, which are sine qua non of widespread deployment.



### Learning Regional Attention over Multi-resolution Deep Convolutional Features for Trademark Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2104.07240v1
- **DOI**: 10.1109/ICIP42928.2021.9506223
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07240v1)
- **Published**: 2021-04-15 05:18:28+00:00
- **Updated**: 2021-04-15 05:18:28+00:00
- **Authors**: Osman Tursun, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale trademark retrieval is an important content-based image retrieval task. A recent study shows that off-the-shelf deep features aggregated with Regional-Maximum Activation of Convolutions (R-MAC) achieve state-of-the-art results. However, R-MAC suffers in the presence of background clutter/trivial regions and scale variance, and discards important spatial information. We introduce three simple but effective modifications to R-MAC to overcome these drawbacks. First, we propose the use of both sum and max pooling to minimise the loss of spatial information. We also employ domain-specific unsupervised soft-attention to eliminate background clutter and unimportant regions. Finally, we add multi-resolution inputs to enhance the scale-invariance of R-MAC. We evaluate these three modifications on the million-scale METU dataset. Our results show that all modifications bring non-trivial improvements, and surpass previous state-of-the-art results.



### Embedding Adaptation is Still Needed for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.07255v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.07255v1)
- **Published**: 2021-04-15 06:00:04+00:00
- **Updated**: 2021-04-15 06:00:04+00:00
- **Authors**: Sébastien M. R. Arnold, Fei Sha
- **Comment**: In submission
- **Journal**: None
- **Summary**: Constructing new and more challenging tasksets is a fruitful methodology to analyse and understand few-shot classification methods. Unfortunately, existing approaches to building those tasksets are somewhat unsatisfactory: they either assume train and test task distributions to be identical -- which leads to overly optimistic evaluations -- or take a "worst-case" philosophy -- which typically requires additional human labor such as obtaining semantic class relationships. We propose ATG, a principled clustering method to defining train and test tasksets without additional human knowledge. ATG models train and test task distributions while requiring them to share a predefined amount of information. We empirically demonstrate the effectiveness of ATG in generating tasksets that are easier, in-between, or harder than existing benchmarks, including those that rely on semantic information. Finally, we leverage our generated tasksets to shed a new light on few-shot classification: gradient-based methods -- previously believed to underperform -- can outperform metric-based ones when transfer is most challenging.



### A Simple Baseline for Semi-supervised Semantic Segmentation with Strong Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.07256v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07256v4)
- **Published**: 2021-04-15 06:01:39+00:00
- **Updated**: 2021-08-25 03:39:39+00:00
- **Authors**: Jianlong Yuan, Yifan Liu, Chunhua Shen, Zhibin Wang, Hao Li
- **Comment**: Accepted to Proc. Int. Conf. Computer Vision (ICCV) 2021
- **Journal**: None
- **Summary**: Recently, significant progress has been made on semantic segmentation. However, the success of supervised semantic segmentation typically relies on a large amount of labelled data, which is time-consuming and costly to obtain. Inspired by the success of semi-supervised learning methods in image classification, here we propose a simple yet effective semi-supervised learning framework for semantic segmentation. We demonstrate that the devil is in the details: a set of simple design and training techniques can collectively improve the performance of semi-supervised semantic segmentation significantly. Previous works [3, 27] fail to employ strong augmentation in pseudo label learning efficiently, as the large distribution change caused by strong augmentation harms the batch normalisation statistics. We design a new batch normalisation, namely distribution-specific batch normalisation (DSBN) to address this problem and demonstrate the importance of strong augmentation for semantic segmentation. Moreover, we design a self correction loss which is effective in noise resistance. We conduct a series of ablation studies to show the effectiveness of each component. Our method achieves state-of-the-art results in the semi-supervised settings on the Cityscapes and Pascal VOC datasets.



### ContactOpt: Optimizing Contact to Improve Grasps
- **Arxiv ID**: http://arxiv.org/abs/2104.07267v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07267v1)
- **Published**: 2021-04-15 06:40:51+00:00
- **Updated**: 2021-04-15 06:40:51+00:00
- **Authors**: Patrick Grady, Chengcheng Tang, Christopher D. Twigg, Minh Vo, Samarth Brahmbhatt, Charles C. Kemp
- **Comment**: Conference on Computer Vision and Pattern Recognition (CVPR) 2021
- **Journal**: None
- **Summary**: Physical contact between hands and objects plays a critical role in human grasps. We show that optimizing the pose of a hand to achieve expected contact with an object can improve hand poses inferred via image-based methods. Given a hand mesh and an object mesh, a deep model trained on ground truth contact data infers desirable contact across the surfaces of the meshes. Then, ContactOpt efficiently optimizes the pose of the hand to achieve desirable contact using a differentiable contact model. Notably, our contact model encourages mesh interpenetration to approximate deformable soft tissue in the hand. In our evaluations, our methods result in grasps that better match ground truth contact, have lower kinematic error, and are significantly preferred by human participants. Code and models are available online.



### Weakly Supervised Video Anomaly Detection via Center-guided Discriminative Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.07268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07268v1)
- **Published**: 2021-04-15 06:41:23+00:00
- **Updated**: 2021-04-15 06:41:23+00:00
- **Authors**: Boyang Wan, Yuming Fang, Xue Xia, Jiajie Mei
- **Comment**: Accepted in ICME 2020
- **Journal**: None
- **Summary**: Anomaly detection in surveillance videos is a challenging task due to the diversity of anomalous video content and duration. In this paper, we consider video anomaly detection as a regression problem with respect to anomaly scores of video clips under weak supervision. Hence, we propose an anomaly detection framework, called Anomaly Regression Net (AR-Net), which only requires video-level labels in training stage. Further, to learn discriminative features for anomaly detection, we design a dynamic multiple-instance learning loss and a center loss for the proposed AR-Net. The former is used to enlarge the inter-class distance between anomalous and normal instances, while the latter is proposed to reduce the intra-class distance of normal instances. Comprehensive experiments are performed on a challenging benchmark: ShanghaiTech. Our method yields a new state-of-the-art result for video anomaly detection on ShanghaiTech dataset



### Multiple feature fusion-based video face tracking for IoT big data
- **Arxiv ID**: http://arxiv.org/abs/2104.08096v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.08096v1)
- **Published**: 2021-04-15 07:10:13+00:00
- **Updated**: 2021-04-15 07:10:13+00:00
- **Authors**: Tianping Li, Zhifeng Liu, Jianping Qiao
- **Comment**: None
- **Journal**: None
- **Summary**: With the advancement of IoT and artificial intelligence technologies, and the need for rapid application growth in fields such as security entrance control and financial business trade, facial information processing has become an important means for achieving identity authentication and information security. In this paper, we propose a multi-feature fusion algorithm based on integral histograms and a real-time update tracking particle filtering module. First, edge and colour features are extracted, weighting methods are used to weight the colour histogram and edge features to describe facial features, and fusion of colour and edge features is made adaptive by using fusion coefficients to improve face tracking reliability. Then, the integral histogram is integrated into the particle filtering algorithm to simplify the calculation steps of complex particles. Finally, the tracking window size is adjusted in real time according to the change in the average distance from the particle centre to the edge of the current model and the initial model to reduce the drift problem and achieve stable tracking with significant changes in the target dimension. The results show that the algorithm improves video tracking accuracy, simplifies particle operation complexity, improves the speed, and has good anti-interference ability and robustness.



### COVID-19 detection using deep convolutional neural networks and binary-differential-algorithm-based feature selection on X-ray images
- **Arxiv ID**: http://arxiv.org/abs/2104.07279v4
- **DOI**: 10.1155/2021/9973277
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.07279v4)
- **Published**: 2021-04-15 07:12:58+00:00
- **Updated**: 2021-10-05 17:35:17+00:00
- **Authors**: Mohammad Saber Iraji, Mohammad-Reza Feizi-Derakhshi, Jafar Tanha
- **Comment**: This is a preprint of an article published in "Complexity". The final
  authenticated version is available online at:
  https://www.hindawi.com/journals/complexity/2021/9973277/
- **Journal**: None
- **Summary**: The new Coronavirus is spreading rapidly, and it has taken the lives of many people so far. The virus has destructive effects on the human lung, and early detection is very important. Deep Convolution neural networks are such powerful tools in classifying images. Therefore, in this paper, a hybrid approach based on a deep network is presented. Feature vectors were extracted by applying a deep convolution neural network on the images, and useful features were selected by the binary differential meta-heuristic algorithm. These optimized features were given to the SVM classifier. A database consisting of three categories of images such as COVID-19, pneumonia, and healthy included in 1092 X-ray samples was considered. The proposed method achieved an accuracy of 99.43%, a sensitivity of 99.16%, and a specificity of 99.57%. Our results demonstrate that the suggested approach is better than recent studies on COVID-19 detection with X-ray images.



### Learning to Estimate Robust 3D Human Mesh from In-the-Wild Crowded Scenes
- **Arxiv ID**: http://arxiv.org/abs/2104.07300v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07300v3)
- **Published**: 2021-04-15 08:21:28+00:00
- **Updated**: 2022-09-18 13:25:19+00:00
- **Authors**: Hongsuk Choi, Gyeongsik Moon, JoonKyu Park, Kyoung Mu Lee
- **Comment**: Accepted to CVPR 2022, 16 pages including the supplementary material
- **Journal**: None
- **Summary**: We consider the problem of recovering a single person's 3D human mesh from in-the-wild crowded scenes. While much progress has been in 3D human mesh estimation, existing methods struggle when test input has crowded scenes. The first reason for the failure is a domain gap between training and testing data. A motion capture dataset, which provides accurate 3D labels for training, lacks crowd data and impedes a network from learning crowded scene-robust image features of a target person. The second reason is a feature processing that spatially averages the feature map of a localized bounding box containing multiple people. Averaging the whole feature map makes a target person's feature indistinguishable from others. We present 3DCrowdNet that firstly explicitly targets in-the-wild crowded scenes and estimates a robust 3D human mesh by addressing the above issues. First, we leverage 2D human pose estimation that does not require a motion capture dataset with 3D labels for training and does not suffer from the domain gap. Second, we propose a joint-based regressor that distinguishes a target person's feature from others. Our joint-based regressor preserves the spatial activation of a target by sampling features from the target's joint locations and regresses human model parameters. As a result, 3DCrowdNet learns target-focused features and effectively excludes the irrelevant features of nearby persons. We conduct experiments on various benchmarks and prove the robustness of 3DCrowdNet to the in-the-wild crowded scenes both quantitatively and qualitatively. The code is available at https://github.com/hongsukchoi/3DCrowdNet_RELEASE.



### SiamCorners: Siamese Corner Networks for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2104.07303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07303v1)
- **Published**: 2021-04-15 08:23:30+00:00
- **Updated**: 2021-04-15 08:23:30+00:00
- **Authors**: Kai Yang, Zhenyu He, Wenjie Pei, Zikun Zhou, Xin Li, Di Yuan, Haijun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The current Siamese network based on region proposal network (RPN) has attracted great attention in visual tracking due to its excellent accuracy and high efficiency. However, the design of the RPN involves the selection of the number, scale, and aspect ratios of anchor boxes, which will affect the applicability and convenience of the model. Furthermore, these anchor boxes require complicated calculations, such as calculating their intersection-over-union (IoU) with ground truth bounding boxes.Due to the problems related to anchor boxes, we propose a simple yet effective anchor-free tracker (named Siamese corner networks, SiamCorners), which is end-to-end trained offline on large-scale image pairs. Specifically, we introduce a modified corner pooling layer to convert the bounding box estimate of the target into a pair of corner predictions (the bottom-right and the top-left corners). By tracking a target as a pair of corners, we avoid the need to design the anchor boxes. This will make the entire tracking algorithm more flexible and simple than anchorbased trackers. In our network design, we further introduce a layer-wise feature aggregation strategy that enables the corner pooling module to predict multiple corners for a tracking target in deep networks. We then introduce a new penalty term that is used to select an optimal tracking box in these candidate corners. Finally, SiamCorners achieves experimental results that are comparable to the state-of-art tracker while maintaining a high running speed. In particular, SiamCorners achieves a 53.7% AUC on NFS30 and a 61.4% AUC on UAV123, while still running at 42 frames per second (FPS).



### Spectral MVIR: Joint Reconstruction of 3D Shape and Spectral Reflectance
- **Arxiv ID**: http://arxiv.org/abs/2104.07308v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07308v1)
- **Published**: 2021-04-15 08:36:23+00:00
- **Updated**: 2021-04-15 08:36:23+00:00
- **Authors**: Chunyu Li, Yusuke Monno, Masatoshi Okutomi
- **Comment**: Accepted by ICCP 2021. Project homepage:
  http://www.ok.sc.e.titech.ac.jp/res/MVIR/smvir.html
- **Journal**: None
- **Summary**: Reconstructing an object's high-quality 3D shape with inherent spectral reflectance property, beyond typical device-dependent RGB albedos, opens the door to applications requiring a high-fidelity 3D model in terms of both geometry and photometry. In this paper, we propose a novel Multi-View Inverse Rendering (MVIR) method called Spectral MVIR for jointly reconstructing the 3D shape and the spectral reflectance for each point of object surfaces from multi-view images captured using a standard RGB camera and low-cost lighting equipment such as an LED bulb or an LED projector. Our main contributions are twofold: (i) We present a rendering model that considers both geometric and photometric principles in the image formation by explicitly considering camera spectral sensitivity, light's spectral power distribution, and light source positions. (ii) Based on the derived model, we build a cost-optimization MVIR framework for the joint reconstruction of the 3D shape and the per-vertex spectral reflectance while estimating the light source positions and the shadows. Different from most existing spectral-3D acquisition methods, our method does not require expensive special equipment and cumbersome geometric calibration. Experimental results using both synthetic and real-world data demonstrate that our Spectral MVIR can acquire a high-quality 3D model with accurate spectral reflectance property.



### Shoulder Implant X-Ray Manufacturer Classification: Exploring with Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2104.07667v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.07667v2)
- **Published**: 2021-04-15 09:13:47+00:00
- **Updated**: 2021-04-21 15:46:20+00:00
- **Authors**: Meng Zhou, Shanglin Mo
- **Comment**: 11 pages, 12 figures
- **Journal**: None
- **Summary**: Shoulder replacement surgery, also called total shoulder replacement, is a common and complex surgery in Orthopedics discipline. It involves replacing a dead shoulder joint with an artificial implant. In the market, there are many artificial implant manufacturers and each of them may produce different implants with different structures compares to other providers. The problem arises in the following situation: a patient has some problems with the shoulder implant accessory and the manufacturer of that implant maybe unknown to either the patient or the doctor, therefore, correctly identification of the manufacturer is the key prior to the treatment. In this paper, we will demonstrate different methods for classifying the manufacturer of a shoulder implant. We will use Vision Transformer approach to this task for the first time ever



### Depth Completion using Plane-Residual Representation
- **Arxiv ID**: http://arxiv.org/abs/2104.07350v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07350v1)
- **Published**: 2021-04-15 10:17:53+00:00
- **Updated**: 2021-04-15 10:17:53+00:00
- **Authors**: Byeong-Uk Lee, Kyunghyun Lee, In So Kweon
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: The basic framework of depth completion is to predict a pixel-wise dense depth map using very sparse input data. In this paper, we try to solve this problem in a more effective way, by reformulating the regression-based depth estimation problem into a combination of depth plane classification and residual regression. Our proposed approach is to initially densify sparse depth information by figuring out which plane a pixel should lie among a number of discretized depth planes, and then calculate the final depth value by predicting the distance from the specified plane. This will help the network to lessen the burden of directly regressing the absolute depth information from none, and to effectively obtain more accurate depth prediction result with less computation power and inference time. To do so, we firstly introduce a novel way of interpreting depth information with the closest depth plane label $p$ and a residual value $r$, as we call it, Plane-Residual (PR) representation. We also propose a depth completion network utilizing PR representation consisting of a shared encoder and two decoders, where one classifies the pixel's depth plane label, while the other one regresses the normalized distance from the classified depth plane. By interpreting depth information in PR representation and using our corresponding depth completion network, we were able to acquire improved depth completion performance with faster computation, compared to previous approaches.



### Do Deep Neural Networks Forget Facial Action Units? -- Exploring the Effects of Transfer Learning in Health Related Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.07389v2
- **DOI**: 10.1007/978-3-030-93080-6_16
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.07389v2)
- **Published**: 2021-04-15 11:37:19+00:00
- **Updated**: 2022-03-16 13:22:05+00:00
- **Authors**: Pooja Prajod, Dominik Schiller, Tobias Huber, Elisabeth André
- **Comment**: The 5th International Workshop on Health Intelligence (W3PHIAI-21)
- **Journal**: AI for Disease Surveillance and Pandemic Intelligence. W3PHAI
  2021. Studies in Computational Intelligence, vol 1013. Springer, Cham
- **Summary**: In this paper, we present a process to investigate the effects of transfer learning for automatic facial expression recognition from emotions to pain. To this end, we first train a VGG16 convolutional neural network to automatically discern between eight categorical emotions. We then fine-tune successively larger parts of this network to learn suitable representations for the task of automatic pain recognition. Subsequently, we apply those fine-tuned representations again to the original task of emotion recognition to further investigate the differences in performance between the models. In the second step, we use Layer-wise Relevance Propagation to analyze predictions of the model that have been predicted correctly previously but are now wrongly classified. Based on this analysis, we rely on the visual inspection of a human observer to generate hypotheses about what has been forgotten by the model. Finally, we test those hypotheses quantitatively utilizing concept embedding analysis methods. Our results show that the network, which was fully fine-tuned for pain recognition, indeed payed less attention to two action units that are relevant for expression recognition but not for pain recognition.



### Training Deep Capsule Networks with Residual Connections
- **Arxiv ID**: http://arxiv.org/abs/2104.07393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07393v1)
- **Published**: 2021-04-15 11:42:44+00:00
- **Updated**: 2021-04-15 11:42:44+00:00
- **Authors**: Josef Gugglberger, David Peer, Antonio Rodriguez-Sanchez
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Capsule networks are a type of neural network that have recently gained increased popularity. They consist of groups of neurons, called capsules, which encode properties of objects or object parts. The connections between capsules encrypt part-whole relationships between objects through routing algorithms which route the output of capsules from lower level layers to upper level layers. Capsule networks can reach state-of-the-art results on many challenging computer vision tasks, such as MNIST, Fashion-MNIST, and Small-NORB. However, most capsule network implementations use two to three capsule layers, which limits their applicability as expressivity grows exponentially with depth. One approach to overcome such limitations would be to train deeper network architectures, as it has been done for convolutional neural networks with much increased success. In this paper, we propose a methodology to train deeper capsule networks using residual connections, which is evaluated on four datasets and three different routing algorithms. Our experimental results show that in fact, performance increases when training deeper capsule networks. The source code is available on https://github.com/moejoe95/res-capsnet.



### TransRPPG: Remote Photoplethysmography Transformer for 3D Mask Face Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.07419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07419v1)
- **Published**: 2021-04-15 12:33:13+00:00
- **Updated**: 2021-04-15 12:33:13+00:00
- **Authors**: Zitong Yu, Xiaobai Li, Pichao Wang, Guoying Zhao
- **Comment**: Submitted to IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: 3D mask face presentation attack detection (PAD) plays a vital role in securing face recognition systems from emergent 3D mask attacks. Recently, remote photoplethysmography (rPPG) has been developed as an intrinsic liveness clue for 3D mask PAD without relying on the mask appearance. However, the rPPG features for 3D mask PAD are still needed expert knowledge to design manually, which limits its further progress in the deep learning and big data era. In this letter, we propose a pure rPPG transformer (TransRPPG) framework for learning intrinsic liveness representation efficiently. At first, rPPG-based multi-scale spatial-temporal maps (MSTmap) are constructed from facial skin and background regions. Then the transformer fully mines the global relationship within MSTmaps for liveness representation, and gives a binary prediction for 3D mask detection. Comprehensive experiments are conducted on two benchmark datasets to demonstrate the efficacy of the TransRPPG on both intra- and cross-dataset testings. Our TransRPPG is lightweight and efficient (with only 547K parameters and 763M FLOPs), which is promising for mobile-level applications.



### Points as Queries: Weakly Semi-supervised Object Detection by Points
- **Arxiv ID**: http://arxiv.org/abs/2104.07434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07434v1)
- **Published**: 2021-04-15 13:08:25+00:00
- **Updated**: 2021-04-15 13:08:25+00:00
- **Authors**: Liangyu Chen, Tong Yang, Xiangyu Zhang, Wei Zhang, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel point annotated setting for the weakly semi-supervised object detection task, in which the dataset comprises small fully annotated images and large weakly annotated images by points. It achieves a balance between tremendous annotation burden and detection performance. Based on this setting, we analyze existing detectors and find that these detectors have difficulty in fully exploiting the power of the annotated points. To solve this, we introduce a new detector, Point DETR, which extends DETR by adding a point encoder. Extensive experiments conducted on MS-COCO dataset in various data settings show the effectiveness of our method. In particular, when using 20% fully labeled data from COCO, our detector achieves a promising performance, 33.3 AP, which outperforms a strong baseline (FCOS) by 2.0 AP, and we demonstrate the point annotations bring over 10 points in various AR metrics.



### Rehearsal revealed: The limits and merits of revisiting samples in continual learning
- **Arxiv ID**: http://arxiv.org/abs/2104.07446v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.07446v1)
- **Published**: 2021-04-15 13:28:14+00:00
- **Updated**: 2021-04-15 13:28:14+00:00
- **Authors**: Eli Verwimp, Matthias De Lange, Tinne Tuytelaars
- **Comment**: Preprint, code publicly available
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2021, pp. 9385-9394
- **Summary**: Learning from non-stationary data streams and overcoming catastrophic forgetting still poses a serious challenge for machine learning research. Rather than aiming to improve state-of-the-art, in this work we provide insight into the limits and merits of rehearsal, one of continual learning's most established methods. We hypothesize that models trained sequentially with rehearsal tend to stay in the same low-loss region after a task has finished, but are at risk of overfitting on its sample memory, hence harming generalization. We provide both conceptual and strong empirical evidence on three benchmarks for both behaviors, bringing novel insights into the dynamics of rehearsal and continual learning in general. Finally, we interpret important continual learning works in the light of our findings, allowing for a deeper understanding of their successes.



### Audio-Driven Emotional Video Portraits
- **Arxiv ID**: http://arxiv.org/abs/2104.07452v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07452v2)
- **Published**: 2021-04-15 13:37:13+00:00
- **Updated**: 2021-05-20 02:48:26+00:00
- **Authors**: Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu, Chen Change Loy, Xun Cao, Feng Xu
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Despite previous success in generating audio-driven talking heads, most of the previous studies focus on the correlation between speech content and the mouth shape. Facial emotion, which is one of the most important features on natural human faces, is always neglected in their methods. In this work, we present Emotional Video Portraits (EVP), a system for synthesizing high-quality video portraits with vivid emotional dynamics driven by audios. Specifically, we propose the Cross-Reconstructed Emotion Disentanglement technique to decompose speech into two decoupled spaces, i.e., a duration-independent emotion space and a duration dependent content space. With the disentangled features, dynamic 2D emotional facial landmarks can be deduced. Then we propose the Target-Adaptive Face Synthesis technique to generate the final high-quality video portraits, by bridging the gap between the deduced landmarks and the natural head poses of target videos. Extensive experiments demonstrate the effectiveness of our method both qualitatively and quantitatively.



### Action Segmentation with Mixed Temporal Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2104.07461v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.07461v2)
- **Published**: 2021-04-15 13:48:14+00:00
- **Updated**: 2021-04-16 00:46:15+00:00
- **Authors**: Min-Hung Chen, Baopu Li, Yingze Bao, Ghassan AlRegib
- **Comment**: Winter Conference on Applications of Computer Vision (WACV) 2020.
  Website: https://minhungchen.netlify.app/publication/mtda
- **Journal**: None
- **Summary**: The main progress for action segmentation comes from densely-annotated data for fully-supervised learning. Since manual annotation for frame-level actions is time-consuming and challenging, we propose to exploit auxiliary unlabeled videos, which are much easier to obtain, by shaping this problem as a domain adaptation (DA) problem. Although various DA techniques have been proposed in recent years, most of them have been developed only for the spatial direction. Therefore, we propose Mixed Temporal Domain Adaptation (MTDA) to jointly align frame- and video-level embedded feature spaces across domains, and further integrate with the domain attention mechanism to focus on aligning the frame-level features with higher domain discrepancy, leading to more effective domain adaptation. Finally, we evaluate our proposed methods on three challenging datasets (GTEA, 50Salads, and Breakfast), and validate that MTDA outperforms the current state-of-the-art methods on all three datasets by large margins (e.g. 6.4% gain on F1@50 and 6.8% gain on the edit score for GTEA).



### Spatial-Temporal Correlation and Topology Learning for Person Re-Identification in Videos
- **Arxiv ID**: http://arxiv.org/abs/2104.08241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08241v1)
- **Published**: 2021-04-15 14:32:12+00:00
- **Updated**: 2021-04-15 14:32:12+00:00
- **Authors**: Jiawei Liu, Zheng-Jun Zha, Wei Wu, Kecheng Zheng, Qibin Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Video-based person re-identification aims to match pedestrians from video sequences across non-overlapping camera views. The key factor for video person re-identification is to effectively exploit both spatial and temporal clues from video sequences. In this work, we propose a novel Spatial-Temporal Correlation and Topology Learning framework (CTL) to pursue discriminative and robust representation by modeling cross-scale spatial-temporal correlation. Specifically, CTL utilizes a CNN backbone and a key-points estimator to extract semantic local features from human body at multiple granularities as graph nodes. It explores a context-reinforced topology to construct multi-scale graphs by considering both global contextual information and physical connections of human body. Moreover, a 3D graph convolution and a cross-scale graph convolution are designed, which facilitate direct cross-spacetime and cross-scale information propagation for capturing hierarchical spatial-temporal dependencies and structural information. By jointly performing the two convolutions, CTL effectively mines comprehensive clues that are complementary with appearance information to enhance representational capacity. Extensive experiments on two video benchmarks have demonstrated the effectiveness of the proposed method and the state-of-the-art performance.



### Ensemble of MRR and NDCG models for Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/2104.07511v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.07511v3)
- **Published**: 2021-04-15 15:09:32+00:00
- **Updated**: 2021-08-04 13:52:03+00:00
- **Authors**: Idan Schwartz
- **Comment**: Accepted to NAACL2021
- **Journal**: None
- **Summary**: Assessing an AI agent that can converse in human language and understand visual content is challenging. Generation metrics, such as BLEU scores favor correct syntax over semantics. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the model performance by taking into account the rank of a single human-derived answer. This approach, however, raises a new challenge: the ambiguity and synonymy of answers, for instance, semantic equivalence (e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative gain (NDCG) metric has been used to capture the relevance of all the correct answers via dense annotations. However, the NDCG metric favors the usually applicable uncertain answers such as `I don't know. Crafting a model that excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should answer a human-like reply and validate the correctness of any answer. To address this issue, we describe a two-step non-parametric ranking approach that can merge strong MRR and NDCG models. Using our approach, we manage to keep most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won the recent Visual Dialog 2020 challenge. Source code is available at https://github.com/idansc/mrr-ndcg.



### A Decomposition Model for Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2104.07516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07516v1)
- **Published**: 2021-04-15 15:16:23+00:00
- **Updated**: 2021-04-15 15:16:23+00:00
- **Authors**: Chengtang Yao, Yunde Jia, Huijun Di, Pengxiang Li, Yuwei Wu
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: In this paper, we present a decomposition model for stereo matching to solve the problem of excessive growth in computational cost (time and memory cost) as the resolution increases. In order to reduce the huge cost of stereo matching at the original resolution, our model only runs dense matching at a very low resolution and uses sparse matching at different higher resolutions to recover the disparity of lost details scale-by-scale. After the decomposition of stereo matching, our model iteratively fuses the sparse and dense disparity maps from adjacent scales with an occlusion-aware mask. A refinement network is also applied to improving the fusion result. Compared with high-performance methods like PSMNet and GANet, our method achieves $10-100\times$ speed increase while obtaining comparable disparity estimation results.



### Investigations on Output Parameterizations of Neural Networks for Single Shot 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2104.07528v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.07528v1)
- **Published**: 2021-04-15 15:29:53+00:00
- **Updated**: 2021-04-15 15:29:53+00:00
- **Authors**: Kilian Kleeberger, Markus Völk, Richard Bormann, Marco F. Huber
- **Comment**: Accepted at 2021 IEEE International Conference on Robotics and
  Automation (ICRA 2021)
- **Journal**: None
- **Summary**: Single shot approaches have demonstrated tremendous success on various computer vision tasks. Finding good parameterizations for 6D object pose estimation remains an open challenge. In this work, we propose different novel parameterizations for the output of the neural network for single shot 6D object pose estimation. Our learning-based approach achieves state-of-the-art performance on two public benchmark datasets. Furthermore, we demonstrate that the pose estimates can be used for real-world robotic grasping tasks without additional ICP refinement.



### Street-Map Based Validation of Semantic Segmentation in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2104.07538v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.07538v1)
- **Published**: 2021-04-15 15:48:11+00:00
- **Updated**: 2021-04-15 15:48:11+00:00
- **Authors**: Laura von Rueden, Tim Wirtz, Fabian Hueger, Jan David Schneider, Nico Piatkowski, Christian Bauckhage
- **Comment**: Final version accepted at the International Conference on Pattern
  Recognition (ICPR). arXiv admin note: substantial text overlap with
  arXiv:2011.08008
- **Journal**: None
- **Summary**: Artificial intelligence for autonomous driving must meet strict requirements on safety and robustness, which motivates the thorough validation of learned models. However, current validation approaches mostly require ground truth data and are thus both cost-intensive and limited in their applicability. We propose to overcome these limitations by a model agnostic validation using a-priori knowledge from street maps. In particular, we show how to validate semantic segmentation masks and demonstrate the potential of our approach using OpenStreetMap. We introduce validation metrics that indicate false positive or negative road segments. Besides the validation approach, we present a method to correct the vehicle's GPS position so that a more accurate localization can be used for the street-map based validation. Lastly, we present quantitative results on the Cityscapes dataset indicating that our validation approach can indeed uncover errors in semantic segmentation masks.



### Assessment of deep learning based blood pressure prediction from PPG and rPPG signals
- **Arxiv ID**: http://arxiv.org/abs/2104.09313v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2104.09313v1)
- **Published**: 2021-04-15 15:56:58+00:00
- **Updated**: 2021-04-15 15:56:58+00:00
- **Authors**: Fabian Schrumpf, Patrick Frenzel, Christoph Aust, Georg Osterhoff, Mirco Fuchs
- **Comment**: (Accepted / In press) 2021 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshop (CVPRW)
- **Journal**: None
- **Summary**: Exploiting photoplethysmography signals (PPG) for non-invasive blood pressure (BP) measurement is interesting for various reasons. First, PPG can easily be measured using fingerclip sensors. Second, camera-based approaches allow to derive remote PPG (rPPG) signals similar to PPG and therefore provide the opportunity for non-invasive measurements of BP. Various methods relying on machine learning techniques have recently been published. Performances are often reported as the mean average error (MAE) on the data which is problematic. This work aims to analyze the PPG- and rPPG-based BP prediction error with respect to the underlying data distribution. First, we train established neural network (NN) architectures and derive an appropriate parameterization of input segments drawn from continuous PPG signals. Second, we apply this parameterization to a larger PPG dataset and train NNs to predict BP. The resulting prediction errors increase towards less frequent BP values. Third, we use transfer learning to train the NNs for rPPG based BP prediction. The resulting performances are similar to the PPG-only case. Finally, we apply a personalization technique and retrain our NNs with subject-specific data. This slightly reduces the prediction errors.



### BAM: A Balanced Attention Mechanism for Single Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2104.07566v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.07566v3)
- **Published**: 2021-04-15 16:22:16+00:00
- **Updated**: 2021-09-10 13:09:47+00:00
- **Authors**: Fanyi Wang, Haotian Hu, Cheng Shen
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Recovering texture information from the aliasing regions has always been a major challenge for Single Image Super Resolution (SISR) task. These regions are often submerged in noise so that we have to restore texture details while suppressing noise. To address this issue, we propose a Balanced Attention Mechanism (BAM), which consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial Attention Module (MSAM) in parallel. ACAM is designed to suppress extreme noise in the large scale feature maps while MSAM preserves high-frequency texture details. Thanks to the parallel structure, these two modules not only conduct self-optimization, but also mutual optimization to obtain the balance of noise reduction and high-frequency texture restoration during the back propagation process, and the parallel structure makes the inference faster. To verify the effectiveness and robustness of BAM, we applied it to 10 SOTA SISR networks. The results demonstrate that BAM can efficiently improve the networks performance, and for those originally with attention mechanism, the substitution with BAM further reduces the amount of parameters and increases the inference speed. Moreover, we present a dataset with rich texture aliasing regions in real scenes, named realSR7. Experiments prove that BAM achieves better super-resolution results on the aliasing area.



### See through Gradients: Image Batch Recovery via GradInversion
- **Arxiv ID**: http://arxiv.org/abs/2104.07586v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.07586v1)
- **Published**: 2021-04-15 16:43:17+00:00
- **Updated**: 2021-04-15 16:43:17+00:00
- **Authors**: Hongxu Yin, Arun Mallya, Arash Vahdat, Jose M. Alvarez, Jan Kautz, Pavlo Molchanov
- **Comment**: CVPR 2021 accepted paper
- **Journal**: None
- **Summary**: Training deep neural networks requires gradient estimation from data batches to update parameters. Gradients per parameter are averaged over a set of data and this has been presumed to be safe for privacy-preserving training in joint, collaborative, and federated learning applications. Prior work only showed the possibility of recovering input data given gradients under very restrictive conditions - a single input point, or a network with no non-linearities, or a small 32x32 px input batch. Therefore, averaging gradients over larger batches was thought to be safe. In this work, we introduce GradInversion, using which input images from a larger batch (8 - 48 images) can also be recovered for large networks such as ResNets (50 layers), on complex datasets such as ImageNet (1000 classes, 224x224 px). We formulate an optimization task that converts random noise into natural images, matching gradients while regularizing image fidelity. We also propose an algorithm for target class label recovery given gradients. We further propose a group consistency regularization framework, where multiple agents starting from different random seeds work together to find an enhanced reconstruction of original data batch. We show that gradients encode a surprisingly large amount of information, such that all the individual images can be recovered with high fidelity via GradInversion, even for complex datasets, deep networks, and large batch sizes.



### Camera View Adjustment Prediction for Improving Image Composition
- **Arxiv ID**: http://arxiv.org/abs/2104.07608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07608v1)
- **Published**: 2021-04-15 17:18:31+00:00
- **Updated**: 2021-04-15 17:18:31+00:00
- **Authors**: Yu-Chuan Su, Raviteja Vemulapalli, Ben Weiss, Chun-Te Chu, Philip Andrew Mansfield, Lior Shapira, Colvin Pitts
- **Comment**: None
- **Journal**: None
- **Summary**: Image composition plays an important role in the quality of a photo. However, not every camera user possesses the knowledge and expertise required for capturing well-composed photos. While post-capture cropping can improve the composition sometimes, it does not work in many common scenarios in which the photographer needs to adjust the camera view to capture the best shot. To address this issue, we propose a deep learning-based approach that provides suggestions to the photographer on how to adjust the camera view before capturing. By optimizing the composition before a photo is captured, our system helps photographers to capture better photos. As there is no publicly-available dataset for this task, we create a view adjustment dataset by repurposing existing image cropping datasets. Furthermore, we propose a two-stage semi-supervised approach that utilizes both labeled and unlabeled images for training a view adjustment model. Experiment results show that the proposed semi-supervised approach outperforms the corresponding supervised alternatives, and our user study results show that the suggested view adjustment improves image composition 79% of the time.



### Image Super-Resolution via Iterative Refinement
- **Arxiv ID**: http://arxiv.org/abs/2104.07636v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.07636v2)
- **Published**: 2021-04-15 17:50:42+00:00
- **Updated**: 2021-06-30 07:34:57+00:00
- **Authors**: Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, Mohammad Norouzi
- **Comment**: None
- **Journal**: None
- **Summary**: We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet.



### A-SDF: Learning Disentangled Signed Distance Functions for Articulated Shape Representation
- **Arxiv ID**: http://arxiv.org/abs/2104.07645v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07645v1)
- **Published**: 2021-04-15 17:53:54+00:00
- **Updated**: 2021-04-15 17:53:54+00:00
- **Authors**: Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille, Nuno Vasconcelos, Xiaolong Wang
- **Comment**: Our project page is available at: https://jitengmu.github.io/A-SDF/
- **Journal**: None
- **Summary**: Recent work has made significant progress on using implicit functions, as a continuous representation for 3D rigid object shape reconstruction. However, much less effort has been devoted to modeling general articulated objects. Compared to rigid objects, articulated objects have higher degrees of freedom, which makes it hard to generalize to unseen shapes. To deal with the large shape variance, we introduce Articulated Signed Distance Functions (A-SDF) to represent articulated shapes with a disentangled latent space, where we have separate codes for encoding shape and articulation. We assume no prior knowledge on part geometry, articulation status, joint type, joint axis, and joint location. With this disentangled continuous representation, we demonstrate that we can control the articulation input and animate unseen instances with unseen joint angles. Furthermore, we propose a Test-Time Adaptation inference algorithm to adjust our model during inference. We demonstrate our model generalize well to out-of-distribution and unseen data, e.g., partial point clouds and real-world depth images.



### Geometry-Free View Synthesis: Transformers and no 3D Priors
- **Arxiv ID**: http://arxiv.org/abs/2104.07652v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07652v2)
- **Published**: 2021-04-15 17:58:05+00:00
- **Updated**: 2021-08-30 12:58:24+00:00
- **Authors**: Robin Rombach, Patrick Esser, Björn Ommer
- **Comment**: Published at ICCV 2021. Code available at https://git.io/JOnwn
- **Journal**: None
- **Summary**: Is a geometric model required to synthesize novel views from a single image? Being bound to local convolutions, CNNs need explicit 3D biases to model geometric transformations. In contrast, we demonstrate that a transformer-based model can synthesize entirely novel views without any hand-engineered 3D biases. This is achieved by (i) a global attention mechanism for implicitly learning long-range 3D correspondences between source and target views, and (ii) a probabilistic formulation necessary to capture the ambiguity inherent in predicting novel views from a single image, thereby overcoming the limitations of previous approaches that are restricted to relatively small viewpoint changes. We evaluate various ways to integrate 3D priors into a transformer architecture. However, our experiments show that no such geometric priors are required and that the transformer is capable of implicitly learning 3D relationships between images. Furthermore, this approach outperforms the state of the art in terms of visual quality while covering the full distribution of possible realizations. Code is available at https://git.io/JOnwn



### Demographic-Guided Attention in Recurrent Neural Networks for Modeling Neuropathophysiological Heterogeneity
- **Arxiv ID**: http://arxiv.org/abs/2104.07654v1
- **DOI**: 10.1007/978-3-030-59861-7_37
- **Categories**: **cs.LG**, cs.CV, eess.IV, q-bio.QM, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2104.07654v1)
- **Published**: 2021-04-15 17:58:36+00:00
- **Updated**: 2021-04-15 17:58:36+00:00
- **Authors**: Nicha C. Dvornek, Xiaoxiao Li, Juntang Zhuang, Pamela Ventola, James S. Duncan
- **Comment**: MLMI 2020 (MICCAI Workshop)
- **Journal**: None
- **Summary**: Heterogeneous presentation of a neurological disorder suggests potential differences in the underlying pathophysiological changes that occur in the brain. We propose to model heterogeneous patterns of functional network differences using a demographic-guided attention (DGA) mechanism for recurrent neural network models for prediction from functional magnetic resonance imaging (fMRI) time-series data. The context computed from the DGA head is used to help focus on the appropriate functional networks based on individual demographic information. We demonstrate improved classification on 3 subsets of the ABIDE I dataset used in published studies that have previously produced state-of-the-art results, evaluating performance under a leave-one-site-out cross-validation framework for better generalizeability to new data. Finally, we provide examples of interpreting functional network differences based on individual demographic variables.



### Zooming SlowMo: An Efficient One-Stage Framework for Space-Time Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2104.07473v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.07473v1)
- **Published**: 2021-04-15 17:59:23+00:00
- **Updated**: 2021-04-15 17:59:23+00:00
- **Authors**: Xiaoyu Xiang, Yapeng Tian, Yulun Zhang, Yun Fu, Jan P. Allebach, Chenliang Xu
- **Comment**: Journal version of "Zooming Slow-Mo: Fast and Accurate One-Stage
  Space-Time Video Super-Resolution"(CVPR-2020). 14 pages, 14 figures
- **Journal**: None
- **Summary**: In this paper, we address the space-time video super-resolution, which aims at generating a high-resolution (HR) slow-motion video from a low-resolution (LR) and low frame rate (LFR) video sequence. A na\"ive method is to decompose it into two sub-tasks: video frame interpolation (VFI) and video super-resolution (VSR). Nevertheless, temporal interpolation and spatial upscaling are intra-related in this problem. Two-stage approaches cannot fully make use of this natural property. Besides, state-of-the-art VFI or VSR deep networks usually have a large frame reconstruction module in order to obtain high-quality photo-realistic video frames, which makes the two-stage approaches have large models and thus be relatively time-consuming. To overcome the issues, we present a one-stage space-time video super-resolution framework, which can directly reconstruct an HR slow-motion video sequence from an input LR and LFR video. Instead of reconstructing missing LR intermediate frames as VFI models do, we temporally interpolate LR frame features of the missing LR frames capturing local temporal contexts by a feature temporal interpolation module. Extensive experiments on widely used benchmarks demonstrate that the proposed framework not only achieves better qualitative and quantitative performance on both clean and noisy LR frames but also is several times faster than recent state-of-the-art two-stage networks. The source code is released in https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020 .



### Self-supervised Video Object Segmentation by Motion Grouping
- **Arxiv ID**: http://arxiv.org/abs/2104.07658v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.07658v2)
- **Published**: 2021-04-15 17:59:32+00:00
- **Updated**: 2021-08-11 09:56:30+00:00
- **Authors**: Charig Yang, Hala Lamdouar, Erika Lu, Andrew Zisserman, Weidi Xie
- **Comment**: Best Paper in CVPR2021 RVSU Workshop. Accepted by ICCV
- **Journal**: None
- **Summary**: Animals have evolved highly functional visual systems to understand motion, assisting perception even under complex environments. In this paper, we work towards developing a computer vision system able to segment objects by exploiting motion cues, i.e. motion segmentation. We make the following contributions: First, we introduce a simple variant of the Transformer to segment optical flow frames into primary objects and the background. Second, we train the architecture in a self-supervised manner, i.e. without using any manual annotations. Third, we analyze several critical components of our method and conduct thorough ablation studies to validate their necessity. Fourth, we evaluate the proposed architecture on public benchmarks (DAVIS2016, SegTrackv2, and FBMS59). Despite using only optical flow as input, our approach achieves superior or comparable results to previous state-of-the-art self-supervised methods, while being an order of magnitude faster. We additionally evaluate on a challenging camouflage dataset (MoCA), significantly outperforming the other self-supervised approaches, and comparing favourably to the top supervised approach, highlighting the importance of motion cues, and the potential bias towards visual appearance in existing video segmentation models.



### GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds
- **Arxiv ID**: http://arxiv.org/abs/2104.07659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07659v1)
- **Published**: 2021-04-15 17:59:38+00:00
- **Updated**: 2021-04-15 17:59:38+00:00
- **Authors**: Zekun Hao, Arun Mallya, Serge Belongie, Ming-Yu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis. The project website is available at https://nvlabs.github.io/GANcraft/ .



### SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements
- **Arxiv ID**: http://arxiv.org/abs/2104.07660v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2104.07660v1)
- **Published**: 2021-04-15 17:59:39+00:00
- **Updated**: 2021-04-15 17:59:39+00:00
- **Authors**: Qianli Ma, Shunsuke Saito, Jinlong Yang, Siyu Tang, Michael J. Black
- **Comment**: In CVPR 2021. Project page: https://qianlim.github.io/SCALE
- **Journal**: None
- **Summary**: Learning to model and reconstruct humans in clothing is challenging due to articulation, non-rigid deformation, and varying clothing types and topologies. To enable learning, the choice of representation is the key. Recent work uses neural networks to parameterize local surface elements. This approach captures locally coherent geometry and non-planar details, can deal with varying topology, and does not require registered training data. However, naively using such methods to model 3D clothed humans fails to capture fine-grained local deformations and generalizes poorly. To address this, we present three key innovations: First, we deform surface elements based on a human body model such that large-scale deformations caused by articulation are explicitly separated from topological changes and local clothing deformations. Second, we address the limitations of existing neural surface elements by regressing local geometry from local features, significantly improving the expressiveness. Third, we learn a pose embedding on a 2D parameterization space that encodes posed body geometry, improving generalization to unseen poses by reducing non-local spurious correlations. We demonstrate the efficacy of our surface representation by learning models of complex clothing from point clouds. The clothing can change topology and deviate from the topology of the body. Once learned, we can animate previously unseen motions, producing high-quality point clouds, from which we generate realistic images with neural rendering. We assess the importance of each technical contribution and show that our approach outperforms the state-of-the-art methods in terms of reconstruction accuracy and inference time. The code is available for research purposes at https://qianlim.github.io/SCALE .



### E2Style: Improve the Efficiency and Effectiveness of StyleGAN Inversion
- **Arxiv ID**: http://arxiv.org/abs/2104.07661v2
- **DOI**: 10.1109/TIP.2022.3167305
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2104.07661v2)
- **Published**: 2021-04-15 17:59:49+00:00
- **Updated**: 2022-03-26 15:29:05+00:00
- **Authors**: Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Weiming Zhang, Lu Yuan, Gang Hua, Nenghai Yu
- **Comment**: To Appear at TIP 2022
- **Journal**: None
- **Summary**: This paper studies the problem of StyleGAN inversion, which plays an essential role in enabling the pretrained StyleGAN to be used for real image editing tasks. The goal of StyleGAN inversion is to find the exact latent code of the given image in the latent space of StyleGAN. This problem has a high demand for quality and efficiency. Existing optimization-based methods can produce high-quality results, but the optimization often takes a long time. On the contrary, forward-based methods are usually faster but the quality of their results is inferior. In this paper, we present a new feed-forward network "E2Style" for StyleGAN inversion, with significant improvement in terms of efficiency and effectiveness. In our inversion network, we introduce: 1) a shallower backbone with multiple efficient heads across scales; 2) multi-layer identity loss and multi-layer face parsing loss to the loss function; and 3) multi-stage refinement. Combining these designs together forms an effective and efficient method that exploits all benefits of optimization-based and forward-based methods. Quantitative and qualitative results show that our E2Style performs better than existing forward-based methods and comparably to state-of-the-art optimization-based methods while maintaining the high efficiency as well as forward-based methods. Moreover, a number of real image editing applications demonstrate the efficacy of our E2Style. Our code is available at \url{https://github.com/wty-ustc/e2style}



### Auto-Tuned Sim-to-Real Transfer
- **Arxiv ID**: http://arxiv.org/abs/2104.07662v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.07662v2)
- **Published**: 2021-04-15 17:59:55+00:00
- **Updated**: 2021-05-20 17:58:26+00:00
- **Authors**: Yuqing Du, Olivia Watkins, Trevor Darrell, Pieter Abbeel, Deepak Pathak
- **Comment**: ICRA 2021. First two authors contributed equally. Website at
  https://yuqingd.github.io/autotuned-sim2real/
- **Journal**: None
- **Summary**: Policies trained in simulation often fail when transferred to the real world due to the `reality gap' where the simulator is unable to accurately capture the dynamics and visual properties of the real world. Current approaches to tackle this problem, such as domain randomization, require prior knowledge and engineering to determine how much to randomize system parameters in order to learn a policy that is robust to sim-to-real transfer while also not being too conservative. We propose a method for automatically tuning simulator system parameters to match the real world using only raw RGB images of the real world without the need to define rewards or estimate state. Our key insight is to reframe the auto-tuning of parameters as a search problem where we iteratively shift the simulation system parameters to approach the real-world system parameters. We propose a Search Param Model (SPM) that, given a sequence of observations and actions and a set of system parameters, predicts whether the given parameters are higher or lower than the true parameters used to generate the observations. We evaluate our method on multiple robotic control tasks in both sim-to-sim and sim-to-real transfer, demonstrating significant improvement over naive domain randomization. Project videos and code at https://yuqingd.github.io/autotuned-sim2real/



### Dual Contrastive Learning for Unsupervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2104.07689v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.07689v1)
- **Published**: 2021-04-15 18:00:22+00:00
- **Updated**: 2021-04-15 18:00:22+00:00
- **Authors**: Junlin Han, Mehrdad Shoeiby, Lars Petersson, Mohammad Ali Armin
- **Comment**: Accepted to NTIRE, CVPRW 2021. Code is available at
  https://github.com/JunlinHan/DCLGAN
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation tasks aim to find a mapping between a source domain X and a target domain Y from unpaired training data. Contrastive learning for Unpaired image-to-image Translation (CUT) yields state-of-the-art results in modeling unsupervised image-to-image translation by maximizing mutual information between input and output patches using only one encoder for both domains. In this paper, we propose a novel method based on contrastive learning and a dual learning setting (exploiting two encoders) to infer an efficient mapping between unpaired data. Additionally, while CUT suffers from mode collapse, a variant of our method efficiently addresses this issue. We further demonstrate the advantage of our approach through extensive ablation studies demonstrating superior performance comparing to recent approaches in multiple challenging image translation tasks. Lastly, we demonstrate that the gap between unsupervised methods and supervised methods can be efficiently closed.



### Contrastive Learning with Stronger Augmentations
- **Arxiv ID**: http://arxiv.org/abs/2104.07713v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.07713v2)
- **Published**: 2021-04-15 18:40:04+00:00
- **Updated**: 2022-01-21 03:03:41+00:00
- **Authors**: Xiao Wang, Guo-Jun Qi
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: Representation learning has significantly been developed with the advance of contrastive learning methods. Most of those methods have benefited from various data augmentations that are carefully designated to maintain their identities so that the images transformed from the same instance can still be retrieved. However, those carefully designed transformations limited us to further explore the novel patterns exposed by other transformations. Meanwhile, as found in our experiments, the strong augmentations distorted the images' structures, resulting in difficult retrieval. Thus, we propose a general framework called Contrastive Learning with Stronger Augmentations~(CLSA) to complement current contrastive learning approaches. Here, the distribution divergence between the weakly and strongly augmented images over the representation bank is adopted to supervise the retrieval of strongly augmented queries from a pool of instances. Experiments on the ImageNet dataset and downstream datasets showed the information from the strongly augmented images can significantly boost the performance. For example, CLSA achieves top-1 accuracy of 76.2% on ImageNet with a standard ResNet-50 architecture with a single-layer classifier fine-tuned, which is almost the same level as 76.5% of supervised results. The code and pre-trained models are available in https://github.com/maple-research-lab/CLSA.



### Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment
- **Arxiv ID**: http://arxiv.org/abs/2104.07719v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.07719v4)
- **Published**: 2021-04-15 19:01:27+00:00
- **Updated**: 2022-06-02 17:33:36+00:00
- **Authors**: Guangxing Han, Shiyuan Huang, Jiawei Ma, Yicheng He, Shih-Fu Chang
- **Comment**: AAAI 2022 (Oral). Code is available at
  https://github.com/GuangxingHan/Meta-Faster-R-CNN
- **Journal**: None
- **Summary**: Few-shot object detection (FSOD) aims to detect objects using only a few examples. How to adapt state-of-the-art object detectors to the few-shot domain remains challenging. Object proposal is a key ingredient in modern object detectors. However, the quality of proposals generated for few-shot classes using existing methods is far worse than that of many-shot classes, e.g., missing boxes for few-shot classes due to misclassification or inaccurate spatial locations with respect to true objects. To address the noisy proposal problem, we propose a novel meta-learning based FSOD model by jointly optimizing the few-shot proposal generation and fine-grained few-shot proposal classification. To improve proposal generation for few-shot classes, we propose to learn a lightweight metric-learning based prototype matching network, instead of the conventional simple linear object/nonobject classifier, e.g., used in RPN. Our non-linear classifier with the feature fusion network could improve the discriminative prototype matching and the proposal recall for few-shot classes. To improve the fine-grained few-shot proposal classification, we propose a novel attentive feature alignment method to address the spatial misalignment between the noisy proposals and few-shot classes, thus improving the performance of few-shot object detection. Meanwhile we learn a separate Faster R-CNN detection head for many-shot base classes and show strong performance of maintaining base-classes knowledge. Our model achieves state-of-the-art performance on multiple FSOD benchmarks over most of the shots and metrics.



### Contrastive Learning for Sports Video: Unsupervised Player Classification
- **Arxiv ID**: http://arxiv.org/abs/2104.10068v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10068v2)
- **Published**: 2021-04-15 20:24:02+00:00
- **Updated**: 2021-05-03 18:30:09+00:00
- **Authors**: Maria Koshkina, Hemanth Pidaparthy, James H. Elder
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of unsupervised classification of players in a team sport according to their team affiliation, when jersey colours and design are not known a priori. We adopt a contrastive learning approach in which an embedding network learns to maximize the distance between representations of players on different teams relative to players on the same team, in a purely unsupervised fashion, without any labelled data. We evaluate the approach using a new hockey dataset and find that it outperforms prior unsupervised approaches by a substantial margin, particularly for real-time application when only a small number of frames are available for unsupervised learning before team assignments must be made. Remarkably, we show that our contrastive method achieves 94% accuracy after unsupervised training on only a single frame, with accuracy rising to 97% within 500 frames (17 seconds of game time). We further demonstrate how accurate team classification allows accurate team-conditional heat maps of player positioning to be computed.



### Exploring Visual Engagement Signals for Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.07767v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.07767v2)
- **Published**: 2021-04-15 20:50:40+00:00
- **Updated**: 2021-08-14 18:10:56+00:00
- **Authors**: Menglin Jia, Zuxuan Wu, Austin Reiter, Claire Cardie, Serge Belongie, Ser-Nam Lim
- **Comment**: ICCV2021 camera ready
- **Journal**: None
- **Summary**: Visual engagement in social media platforms comprises interactions with photo posts including comments, shares, and likes. In this paper, we leverage such visual engagement clues as supervisory signals for representation learning. However, learning from engagement signals is non-trivial as it is not clear how to bridge the gap between low-level visual information and high-level social interactions. We present VisE, a weakly supervised learning approach, which maps social images to pseudo labels derived by clustered engagement signals. We then study how models trained in this way benefit subjective downstream computer vision tasks such as emotion recognition or political bias detection. Through extensive studies, we empirically demonstrate the effectiveness of VisE across a diverse set of classification tasks beyond the scope of conventional recognition.



### AsymmNet: Towards ultralight convolution neural networks using asymmetrical bottlenecks
- **Arxiv ID**: http://arxiv.org/abs/2104.07770v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.07770v1)
- **Published**: 2021-04-15 20:58:39+00:00
- **Updated**: 2021-04-15 20:58:39+00:00
- **Authors**: Haojin Yang, Zhen Shen, Yucheng Zhao
- **Comment**: MAI@CVPR 2021
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNN) have achieved astonishing results in a large variety of applications. However, using these models on mobile or embedded devices is difficult due to the limited memory and computation resources. Recently, the inverted residual block becomes the dominating solution for the architecture design of compact CNNs. In this work, we comprehensively investigated the existing design concepts, rethink the functional characteristics of two pointwise convolutions in the inverted residuals. We propose a novel design, called asymmetrical bottlenecks. Precisely, we adjust the first pointwise convolution dimension, enrich the information flow by feature reuse, and migrate saved computations to the second pointwise convolution. By doing so we can further improve the accuracy without increasing the computation overhead. The asymmetrical bottlenecks can be adopted as a drop-in replacement for the existing CNN blocks. We can thus create AsymmNet by easily stack those blocks according to proper depth and width conditions. Extensive experiments demonstrate that our proposed block design is more beneficial than the original inverted residual bottlenecks for mobile networks, especially useful for those ultralight CNNs within the regime of <220M MAdds. Code is available at https://github.com/Spark001/AsymmNet



### Recent Advances in Domain Adaptation for the Classification of Remote Sensing Data
- **Arxiv ID**: http://arxiv.org/abs/2104.07778v1
- **DOI**: 10.1109/MGRS.2016.2548504
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.07778v1)
- **Published**: 2021-04-15 21:15:48+00:00
- **Updated**: 2021-04-15 21:15:48+00:00
- **Authors**: Devis Tuia, Claudio Persello, Lorenzo Bruzzone
- **Comment**: None
- **Journal**: IEEE Geoscience and Remote Sensing Magazine, 4(2): 41-57, 2016
- **Summary**: The success of supervised classification of remotely sensed images acquired over large geographical areas or at short time intervals strongly depends on the representativity of the samples used to train the classification algorithm and to define the model. When training samples are collected from an image (or a spatial region) different from the one used for mapping, spectral shifts between the two distributions are likely to make the model fail. Such shifts are generally due to differences in acquisition and atmospheric conditions or to changes in the nature of the object observed. In order to design classification methods that are robust to data-set shifts, recent remote sensing literature has considered solutions based on domain adaptation (DA) approaches. Inspired by machine learning literature, several DA methods have been proposed to solve specific problems in remote sensing data classification. This paper provides a critical review of the recent advances in DA for remote sensing and presents an overview of methods divided into four categories: i) invariant feature selection; ii) representation matching; iii) adaptation of classifiers and iv) selective sampling. We provide an overview of recent methodologies, as well as examples of application of the considered techniques to real remote sensing images characterized by very high spatial and spectral resolution. Finally, we propose guidelines to the selection of the method to use in real application scenarios.



### A survey of active learning algorithms for supervised remote sensing image classification
- **Arxiv ID**: http://arxiv.org/abs/2104.07784v1
- **DOI**: 10.1109/JSTSP.2011.2139193
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07784v1)
- **Published**: 2021-04-15 21:36:59+00:00
- **Updated**: 2021-04-15 21:36:59+00:00
- **Authors**: Devis Tuia, Michele Volpi, Loris Copa, Mikhail Kanevski, Jordi Munoz-Mari
- **Comment**: None
- **Journal**: IEEE Journal of Selected Topics in Signal Processing, 5(3): 606 -
  617, 2011
- **Summary**: Defining an efficient training set is one of the most delicate phases for the success of remote sensing image classification routines. The complexity of the problem, the limited temporal and financial resources, as well as the high intraclass variance can make an algorithm fail if it is trained with a suboptimal dataset. Active learning aims at building efficient training sets by iteratively improving the model performance through sampling. A user-defined heuristic ranks the unlabeled pixels according to a function of the uncertainty of their class membership and then the user is asked to provide labels for the most uncertain pixels. This paper reviews and tests the main families of active learning algorithms: committee, large margin and posterior probability-based. For each of them, the most recent advances in the remote sensing community are discussed and some heuristics are detailed and tested. Several challenging remote sensing scenarios are considered, including very high spatial resolution and hyperspectral image classification. Finally, guidelines for choosing the good architecture are provided for new and/or unexperienced user.



### Ridge Regression Neural Network for Pediatric Bone Age Assessment
- **Arxiv ID**: http://arxiv.org/abs/2104.07785v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.07785v1)
- **Published**: 2021-04-15 21:38:22+00:00
- **Updated**: 2021-04-15 21:38:22+00:00
- **Authors**: Ibrahim Salim, A. Ben Hamza
- **Comment**: None
- **Journal**: None
- **Summary**: Bone age is an important measure for assessing the skeletal and biological maturity of children. Delayed or increased bone age is a serious concern for pediatricians, and needs to be accurately assessed in a bid to determine whether bone maturity is occurring at a rate consistent with chronological age. In this paper, we introduce a unified deep learning framework for bone age assessment using instance segmentation and ridge regression. The proposed approach consists of two integrated stages. In the first stage, we employ an image annotation and segmentation model to annotate and segment the hand from the radiographic image, followed by background removal. In the second stage, we design a regression neural network architecture composed of a pre-trained convolutional neural network for learning salient features from the segmented pediatric hand radiographs and a ridge regression output layer for predicting the bone age. Experimental evaluation on a dataset of hand radiographs demonstrates the competitive performance of our approach in comparison with existing deep learning based methods for bone age assessment.



### Rethinking Text Line Recognition Models
- **Arxiv ID**: http://arxiv.org/abs/2104.07787v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.07787v2)
- **Published**: 2021-04-15 21:43:13+00:00
- **Updated**: 2021-04-21 21:44:58+00:00
- **Authors**: Daniel Hernandez Diaz, Siyang Qin, Reeve Ingle, Yasuhisa Fujii, Alessandro Bissacco
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: In this paper, we study the problem of text line recognition. Unlike most approaches targeting specific domains such as scene-text or handwritten documents, we investigate the general problem of developing a universal architecture that can extract text from any image, regardless of source or input modality. We consider two decoder families (Connectionist Temporal Classification and Transformer) and three encoder modules (Bidirectional LSTMs, Self-Attention, and GRCLs), and conduct extensive experiments to compare their accuracy and performance on widely used public datasets of scene and handwritten text. We find that a combination that so far has received little attention in the literature, namely a Self-Attention encoder coupled with the CTC decoder, when compounded with an external language model and trained on both public and internal data, outperforms all the others in accuracy and computational complexity. Unlike the more common Transformer-based models, this architecture can handle inputs of arbitrary length, a requirement for universal line recognition. Using an internal dataset collected from multiple sources, we also expose the limitations of current public datasets in evaluating the accuracy of line recognizers, as the relatively narrow image width and sequence length distributions do not allow to observe the quality degradation of the Transformer approach when applied to the transcription of long lines.



### Learning User's confidence for active learning
- **Arxiv ID**: http://arxiv.org/abs/2104.07791v1
- **DOI**: 10.1109/TGRS.2012.2203605
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.07791v1)
- **Published**: 2021-04-15 21:54:27+00:00
- **Updated**: 2021-04-15 21:54:27+00:00
- **Authors**: Devis Tuia, Jordi Munoz-Mari
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, 51(2): 872 -
  880, 2013
- **Summary**: In this paper, we study the applicability of active learning in operative scenarios: more particularly, we consider the well-known contradiction between the active learning heuristics, which rank the pixels according to their uncertainty, and the user's confidence in labeling, which is related to both the homogeneity of the pixel context and user's knowledge of the scene. We propose a filtering scheme based on a classifier that learns the confidence of the user in labeling, thus minimizing the queries where the user would not be able to provide a class for the pixel. The capacity of a model to learn the user's confidence is studied in detail, also showing the effect of resolution is such a learning task. Experiments on two QuickBird images of different resolutions (with and without pansharpening) and considering committees of users prove the efficiency of the filtering scheme proposed, which maximizes the number of useful queries with respect to traditional active learning.



### Semisupervised Manifold Alignment of Multimodal Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2104.07803v1
- **DOI**: 10.1109/TGRS.2014.2317499
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07803v1)
- **Published**: 2021-04-15 22:20:31+00:00
- **Updated**: 2021-04-15 22:20:31+00:00
- **Authors**: Devis Tuia, Michele Volpi, Maxime Trolliet, Gustau Camps-Valls
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, 52(12): 7708 -
  7720, 2014
- **Summary**: We introduce a method for manifold alignment of different modalities (or domains) of remote sensing images. The problem is recurrent when a set of multitemporal, multisource, multisensor and multiangular images is available. In these situations, images should ideally be spatially coregistred, corrected and compensated for differences in the image domains. Such procedures require the interaction of the user, involve tuning of many parameters and heuristics, and are usually applied separately. Changes of sensors and acquisition conditions translate into shifts, twists, warps and foldings of the image distributions (or manifolds). The proposed semisupervised manifold alignment (SS-MA) method aligns the images working directly on their manifolds, and is thus not restricted to images of the same resolutions, either spectral or spatial. SS-MA pulls close together samples of the same class while pushing those of different classes apart. At the same time, it preserves the geometry of each manifold along the transformation. The method builds a linear invertible transformation to a latent space where all images are alike, and reduces to solving a generalized eigenproblem of moderate size. We study the performance of SS-MA in toy examples and in real multiangular, multitemporal, and multisource image classification problems. The method performs well for strong deformations and leads to accurate classification for all domains.



### Out-of-Distribution Detection for Dermoscopic Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2104.07819v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.07819v2)
- **Published**: 2021-04-15 23:34:53+00:00
- **Updated**: 2021-04-19 05:47:57+00:00
- **Authors**: Mohammadreza Mohseni, Jordan Yap, William Yolland, Majid Razmara, M Stella Atkins
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Medical image diagnosis can be achieved by deep neural networks, provided there is enough varied training data for each disease class. However, a hitherto unknown disease class not encountered during training will inevitably be misclassified, even if predicted with low probability. This problem is especially important for medical image diagnosis, when an image of a hitherto unknown disease is presented for diagnosis, especially when the images come from the same image domain, such as dermoscopic skin images.   Current out-of-distribution detection algorithms act unfairly when the in-distribution classes are imbalanced, by favouring the most numerous disease in the training sets. This could lead to false diagnoses for rare cases which are often medically important. We developed a novel yet simple method to train neural networks, which enables them to classify in-distribution dermoscopic skin disease images and also detect novel diseases from dermoscopic images at test time. We show that our BinaryHeads model not only does not hurt classification balanced accuracy when the data is imbalanced, but also consistently improves the balanced accuracy. We also introduce an important method to investigate the effectiveness of out-of-distribution detection methods based on presence of varying amounts of out-of-distribution data, which may arise in real-world settings.



