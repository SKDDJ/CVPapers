# Arxiv Papers in cs.CV on 2021-04-14
### A Semi-Supervised Classification Method of Apicomplexan Parasites and Host Cell Using Contrastive Learning Strategy
- **Arxiv ID**: http://arxiv.org/abs/2104.06593v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.06593v1)
- **Published**: 2021-04-14 02:34:50+00:00
- **Updated**: 2021-04-14 02:34:50+00:00
- **Authors**: Yanni Ren, Hangyu Deng, Hao Jiang, Jinglu Hu
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: A common shortfall of supervised learning for medical imaging is the greedy need for human annotations, which is often expensive and time-consuming to obtain. This paper proposes a semi-supervised classification method for three kinds of apicomplexan parasites and non-infected host cells microscopic images, which uses a small number of labeled data and a large number of unlabeled data for training. There are two challenges in microscopic image recognition. The first is that salient structures of the microscopic images are more fuzzy and intricate than natural images' on a real-world scale. The second is that insignificant textures, like background staining, lightness, and contrast level, vary a lot in samples from different clinical scenarios. To address these challenges, we aim to learn a distinguishable and appearance-invariant representation by contrastive learning strategy. On one hand, macroscopic images, which share similar shape characteristics in morphology, are introduced to contrast for structure enhancement. On the other hand, different appearance transformations, including color distortion and flittering, are utilized to contrast for texture elimination. In the case where only 1% of microscopic images are labeled, the proposed method reaches an accuracy of 94.90% in a generalized testing set.



### Federated Generalized Face Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.06595v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06595v2)
- **Published**: 2021-04-14 02:44:53+00:00
- **Updated**: 2022-05-01 02:49:13+00:00
- **Authors**: Rui Shao, Pramuditha Perera, Pong C. Yuen, Vishal M. Patel
- **Comment**: Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS) 2022. arXiv admin note: substantial text overlap with
  arXiv:2005.14638
- **Journal**: None
- **Summary**: Face presentation attack detection plays a critical role in the modern face recognition pipeline. A face presentation attack detection model with good generalization can be obtained when it is trained with face images from different input distributions and different types of spoof attacks. In reality, training data (both real face images and spoof images) are not directly shared between data owners due to legal and privacy issues. In this paper, with the motivation of circumventing this challenge, we propose a Federated Face Presentation Attack Detection (FedPAD) framework that simultaneously takes advantage of rich fPAD information available at different data owners while preserving data privacy. In the proposed framework, each data center locally trains its own fPAD model. A server learns a global fPAD model by iteratively aggregating model updates from all data centers without accessing private data in each of them. To equip the aggregated fPAD model in the server with better generalization ability to unseen attacks from users, following the basic idea of FedPAD, we further propose a Federated Generalized Face Presentation Attack Detection (FedGPAD) framework. A federated domain disentanglement strategy is introduced in FedGPAD, which treats each data center as one domain and decomposes the fPAD model into domain-invariant and domain-specific parts in each data center. Two parts disentangle the domain-invariant and domain-specific features from images in each local data center, respectively. A server learns a global fPAD model by only aggregating domain-invariant parts of the fPAD models from data centers and thus a more generalized fPAD model can be aggregated in server. We introduce the experimental setting to evaluate the proposed FedPAD and FedGPAD frameworks and carry out extensive experiments to provide various insights about federated learning for fPAD.



### Zero-Shot Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.06601v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06601v2)
- **Published**: 2021-04-14 03:02:48+00:00
- **Updated**: 2021-06-01 03:05:23+00:00
- **Authors**: Ye Zheng, Jiahong Wu, Yongqiang Qin, Faen Zhang, Li Cui
- **Comment**: 8 pages, 6 figures
- **Journal**: CVPR2021
- **Summary**: Deep learning has significantly improved the precision of instance segmentation with abundant labeled data. However, in many areas like medical and manufacturing, collecting sufficient data is extremely hard and labeling this data requires high professional skills. We follow this motivation and propose a new task set named zero-shot instance segmentation (ZSI). In the training phase of ZSI, the model is trained with seen data, while in the testing phase, it is used to segment all seen and unseen instances. We first formulate the ZSI task and propose a method to tackle the challenge, which consists of Zero-shot Detector, Semantic Mask Head, Background Aware RPN and Synchronized Background Strategy. We present a new benchmark for zero-shot instance segmentation based on the MS-COCO dataset. The extensive empirical results in this benchmark show that our method not only surpasses the state-of-the-art results in zero-shot object detection task but also achieves promising performance on ZSI. Our approach will serve as a solid baseline and facilitate future research in zero-shot instance segmentation.



### Representative Forgery Mining for Fake Face Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.06609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06609v1)
- **Published**: 2021-04-14 03:24:19+00:00
- **Updated**: 2021-04-14 03:24:19+00:00
- **Authors**: Chengrui Wang, Weihong Deng
- **Comment**: None
- **Journal**: None
- **Summary**: Although vanilla Convolutional Neural Network (CNN) based detectors can achieve satisfactory performance on fake face detection, we observe that the detectors tend to seek forgeries on a limited region of face, which reveals that the detectors is short of understanding of forgery. Therefore, we propose an attention-based data augmentation framework to guide detector refine and enlarge its attention. Specifically, our method tracks and occludes the Top-N sensitive facial regions, encouraging the detector to mine deeper into the regions ignored before for more representative forgery. Especially, our method is simple-to-use and can be easily integrated with various CNN models. Extensive experiments show that the detector trained with our method is capable to separately point out the representative forgery of fake faces generated by different manipulation techniques, and our method enables a vanilla CNN-based detector to achieve state-of-the-art performance without structure modification.



### Deep Data Density Estimation through Donsker-Varadhan Representation
- **Arxiv ID**: http://arxiv.org/abs/2104.06612v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.IT, math.IT, math.PR, 68T09, 60F10
- **Links**: [PDF](http://arxiv.org/pdf/2104.06612v1)
- **Published**: 2021-04-14 03:38:32+00:00
- **Updated**: 2021-04-14 03:38:32+00:00
- **Authors**: Seonho Park, Panos M. Pardalos
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating the data density is one of the challenging problems in deep learning. In this paper, we present a simple yet effective method for estimating the data density using a deep neural network and the Donsker-Varadhan variational lower bound on the KL divergence. We show that the optimal critic function associated with the Donsker-Varadhan representation on the KL divergence between the data and the uniform distribution can estimate the data density. We also present the deep neural network-based modeling and its stochastic learning. The experimental results and possible applications of the proposed method demonstrate that it is competitive with the previous methods and has a lot of possibilities in applied to various applications.



### Perception Entropy: A Metric for Multiple Sensors Configuration Evaluation and Design
- **Arxiv ID**: http://arxiv.org/abs/2104.06615v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06615v1)
- **Published**: 2021-04-14 03:52:57+00:00
- **Updated**: 2021-04-14 03:52:57+00:00
- **Authors**: Tao Ma, Zhizheng Liu, Yikang Li
- **Comment**: 7 pages, 5 figures, submitted to IROS 2021
- **Journal**: None
- **Summary**: Sensor configuration, including the sensor selections and their installation locations, serves a crucial role in autonomous driving. A well-designed sensor configuration significantly improves the performance upper bound of the perception system. However, as leveraging multiple sensors is becoming the mainstream setting, existing methods mainly focusing on single-sensor configuration problems are hardly utilized in practice. To tackle these issues, we propose a novel method based on conditional entropy in Bayesian theory to evaluate the sensor configurations containing both cameras and LiDARs. Correspondingly, an evaluation metric, perception entropy, is introduced to measure the difference between two configurations, which considers both the perception algorithm performance and the selections of the sensors. To the best of our knowledge, this is the first method to tackle the multi-sensor configuration problem for autonomous vehicles. The simulation results, extensive comparisons, and analysis all demonstrate the superior performance of our proposed approach.



### Decoupled Spatial-Temporal Transformer for Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2104.06637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06637v1)
- **Published**: 2021-04-14 05:47:46+00:00
- **Updated**: 2021-04-14 05:47:46+00:00
- **Authors**: Rui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, Lewei Lu, Wenxiu Sun, Xiaogang Wang, Jifeng Dai, Hongsheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Video inpainting aims to fill the given spatiotemporal holes with realistic appearance but is still a challenging task even with prosperous deep learning approaches. Recent works introduce the promising Transformer architecture into deep video inpainting and achieve better performance. However, it still suffers from synthesizing blurry texture as well as huge computational cost. Towards this end, we propose a novel Decoupled Spatial-Temporal Transformer (DSTT) for improving video inpainting with exceptional efficiency. Our proposed DSTT disentangles the task of learning spatial-temporal attention into 2 sub-tasks: one is for attending temporal object movements on different frames at same spatial locations, which is achieved by temporally-decoupled Transformer block, and the other is for attending similar background textures on same frame of all spatial positions, which is achieved by spatially-decoupled Transformer block. The interweaving stack of such two blocks makes our proposed model attend background textures and moving objects more precisely, and thus the attended plausible and temporally-coherent appearance can be propagated to fill the holes. In addition, a hierarchical encoder is adopted before the stack of Transformer blocks, for learning robust and hierarchical features that maintain multi-level local spatial structure, resulting in the more representative token vectors. Seamless combination of these two novel designs forms a better spatial-temporal attention scheme and our proposed model achieves better performance than state-of-the-art video inpainting approaches with significant boosted efficiency.



### Learning Semantic Person Image Generation by Region-Adaptive Normalization
- **Arxiv ID**: http://arxiv.org/abs/2104.06650v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06650v1)
- **Published**: 2021-04-14 06:51:37+00:00
- **Updated**: 2021-04-14 06:51:37+00:00
- **Authors**: Zhengyao Lv, Xiaoming Li, Xin Li, Fu Li, Tianwei Lin, Dongliang He, Wangmeng Zuo
- **Comment**: None
- **Journal**: None
- **Summary**: Human pose transfer has received great attention due to its wide applications, yet is still a challenging task that is not well solved. Recent works have achieved great success to transfer the person image from the source to the target pose. However, most of them cannot well capture the semantic appearance, resulting in inconsistent and less realistic textures on the reconstructed results. To address this issue, we propose a new two-stage framework to handle the pose and appearance translation. In the first stage, we predict the target semantic parsing maps to eliminate the difficulties of pose transfer and further benefit the latter translation of per-region appearance style. In the second one, with the predicted target semantic maps, we suggest a new person image generation method by incorporating the region-adaptive normalization, in which it takes the per-region styles to guide the target appearance generation. Extensive experiments show that our proposed SPGNet can generate more semantic, consistent, and photo-realistic results and perform favorably against the state of the art methods in terms of quantitative and qualitative evaluation. The source code and model are available at https://github.com/cszy98/SPGNet.git.



### ADNet: Temporal Anomaly Detection in Surveillance Videos
- **Arxiv ID**: http://arxiv.org/abs/2104.06653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06653v1)
- **Published**: 2021-04-14 07:00:10+00:00
- **Updated**: 2021-04-14 07:00:10+00:00
- **Authors**: Halil İbrahim Öztürk, Ahmet Burak Can
- **Comment**: FGVRID workshop of ICPR conference, 15 pages
- **Journal**: None
- **Summary**: Anomaly detection in surveillance videos is an important research problem in computer vision. In this paper, we propose ADNet, an anomaly detection network, which utilizes temporal convolutions to localize anomalies in videos. The model works online by accepting consecutive windows consisting of fixed-number of video clips. Features extracted from video clips in a window are fed to ADNet, which allows to localize anomalies in videos effectively. We propose the AD Loss function to improve abnormal segment detection performance of ADNet. Additionally, we propose to use F1@k metric for temporal anomaly detection. F1@k is a better evaluation metric than AUC in terms of not penalizing minor shifts in temporal segments and punishing short false positive temporal segment predictions. Furthermore, we extend UCF Crime dataset by adding two more anomaly classes and providing temporal anomaly annotations for all classes. Finally, we thoroughly evaluate our model on the extended UCF Crime dataset. ADNet produces promising results with respect to F1@k metric. Dataset extensions and code will be publicly available upon publishing



### Learning Normal Dynamics in Videos with Meta Prototype Network
- **Arxiv ID**: http://arxiv.org/abs/2104.06689v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06689v2)
- **Published**: 2021-04-14 08:25:53+00:00
- **Updated**: 2021-05-10 10:53:26+00:00
- **Authors**: Hui Lv, Chen Chen, Zhen Cui, Chunyan Xu, Yong Li, Jian Yang
- **Comment**: 9 pages, 4 figures, 6 tables
- **Journal**: None
- **Summary**: Frame reconstruction (current or future frame) based on Auto-Encoder (AE) is a popular method for video anomaly detection. With models trained on the normal data, the reconstruction errors of anomalous scenes are usually much larger than those of normal ones. Previous methods introduced the memory bank into AE, for encoding diverse normal patterns across the training videos. However, they are memory-consuming and cannot cope with unseen new scenarios in the testing data. In this work, we propose a dynamic prototype unit (DPU) to encode the normal dynamics as prototypes in real time, free from extra memory cost. In addition, we introduce meta-learning to our DPU to form a novel few-shot normalcy learner, namely Meta-Prototype Unit (MPU). It enables the fast adaption capability on new scenes by only consuming a few iterations of update. Extensive experiments are conducted on various benchmarks. The superior performance over the state-of-the-art demonstrates the effectiveness of our method.



### Revisiting Hierarchical Approach for Persistent Long-Term Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2104.06697v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06697v1)
- **Published**: 2021-04-14 08:39:38+00:00
- **Updated**: 2021-04-14 08:39:38+00:00
- **Authors**: Wonkwang Lee, Whie Jung, Han Zhang, Ting Chen, Jing Yu Koh, Thomas Huang, Hyungsuk Yoon, Honglak Lee, Seunghoon Hong
- **Comment**: Accepted as a conference paper at ICLR 2021
- **Journal**: None
- **Summary**: Learning to predict the long-term future of video frames is notoriously challenging due to inherent ambiguities in the distant future and dramatic amplifications of prediction error through time. Despite the recent advances in the literature, existing approaches are limited to moderately short-term prediction (less than a few seconds), while extrapolating it to a longer future quickly leads to destruction in structure and content. In this work, we revisit hierarchical models in video prediction. Our method predicts future frames by first estimating a sequence of semantic structures and subsequently translating the structures to pixels by video-to-video translation. Despite the simplicity, we show that modeling structures and their dynamics in the discrete semantic structure space with a stochastic recurrent estimator leads to surprisingly successful long-term prediction. We evaluate our method on three challenging datasets involving car driving and human dancing, and demonstrate that it can generate complicated scene structures and motions over a very long time horizon (i.e., thousands frames), setting a new standard of video prediction with orders of magnitude longer prediction time than existing approaches. Full videos and codes are available at https://1konny.github.io/HVP/.



### Change Detection in Synthetic Aperture Radar Images Using a Dual-Domain Network
- **Arxiv ID**: http://arxiv.org/abs/2104.06699v2
- **DOI**: 10.1109/LGRS.2021.3073900
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06699v2)
- **Published**: 2021-04-14 08:41:48+00:00
- **Updated**: 2021-04-15 00:57:26+00:00
- **Authors**: Xiaofan Qu, Feng Gao, Junyu Dong, Qian Du, Heng-Chao Li
- **Comment**: Accepted by IEEE Geoscience and Remote Sensing Letters, Code:
  https://github.com/summitgao/SAR_CD_DDNet
- **Journal**: None
- **Summary**: Change detection from synthetic aperture radar (SAR) imagery is a critical yet challenging task. Existing methods mainly focus on feature extraction in spatial domain, and little attention has been paid to frequency domain. Furthermore, in patch-wise feature analysis, some noisy features in the marginal region may be introduced. To tackle the above two challenges, we propose a Dual-Domain Network. Specifically, we take features from the discrete cosine transform domain into consideration and the reshaped DCT coefficients are integrated into the proposed model as the frequency domain branch. Feature representations from both frequency and spatial domain are exploited to alleviate the speckle noise. In addition, we further propose a multi-region convolution module, which emphasizes the central region of each patch. The contextual information and central region features are modeled adaptively. The experimental results on three SAR datasets demonstrate the effectiveness of the proposed model. Our codes are available at https://github.com/summitgao/SAR_CD_DDNet.



### Deep Permutation Equivariant Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/2104.06703v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.06703v3)
- **Published**: 2021-04-14 08:50:06+00:00
- **Updated**: 2021-10-24 09:52:21+00:00
- **Authors**: Dror Moran, Hodaya Koslowsky, Yoni Kasten, Haggai Maron, Meirav Galun, Ronen Basri
- **Comment**: None
- **Journal**: None
- **Summary**: Existing deep methods produce highly accurate 3D reconstructions in stereo and multiview stereo settings, i.e., when cameras are both internally and externally calibrated. Nevertheless, the challenge of simultaneous recovery of camera poses and 3D scene structure in multiview settings with deep networks is still outstanding. Inspired by projective factorization for Structure from Motion (SFM) and by deep matrix completion techniques, we propose a neural network architecture that, given a set of point tracks in multiple images of a static scene, recovers both the camera parameters and a (sparse) scene structure by minimizing an unsupervised reprojection loss. Our network architecture is designed to respect the structure of the problem: the sought output is equivariant to permutations of both cameras and scene points. Notably, our method does not require initialization of camera parameters or 3D point locations. We test our architecture in two setups: (1) single scene reconstruction and (2) learning from multiple scenes. Our experiments, conducted on a variety of datasets in both internally calibrated and uncalibrated settings, indicate that our method accurately recovers pose and structure, on par with classical state of the art methods. Additionally, we show that a pre-trained network can be used to reconstruct novel scenes using inexpensive fine-tuning with no loss of accuracy.



### Adversarial Sticker: A Stealthy Attack Method in the Physical World
- **Arxiv ID**: http://arxiv.org/abs/2104.06728v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06728v2)
- **Published**: 2021-04-14 09:32:01+00:00
- **Updated**: 2022-12-19 15:16:43+00:00
- **Authors**: Xingxing Wei, Ying Guo, Jie Yu
- **Comment**: accepted by TPAMI 2022
- **Journal**: None
- **Summary**: To assess the vulnerability of deep learning in the physical world, recent works introduce adversarial patches and apply them on different tasks. In this paper, we propose another kind of adversarial patch: the Meaningful Adversarial Sticker, a physically feasible and stealthy attack method by using real stickers existing in our life. Unlike the previous adversarial patches by designing perturbations, our method manipulates the sticker's pasting position and rotation angle on the objects to perform physical attacks. Because the position and rotation angle are less affected by the printing loss and color distortion, adversarial stickers can keep good attacking performance in the physical world. Besides, to make adversarial stickers more practical in real scenes, we conduct attacks in the black-box setting with the limited information rather than the white-box setting with all the details of threat models. To effectively solve for the sticker's parameters, we design the Region based Heuristic Differential Evolution Algorithm, which utilizes the new-found regional aggregation of effective solutions and the adaptive adjustment strategy of the evaluation criteria. Our method is comprehensively verified in the face recognition and then extended to the image retrieval and traffic sign recognition. Extensive experiments show the proposed method is effective and efficient in complex physical conditions and has a good generalization for different tasks.



### Weakly But Deeply Supervised Occlusion-Reasoned Parametric Road Layouts
- **Arxiv ID**: http://arxiv.org/abs/2104.06730v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06730v2)
- **Published**: 2021-04-14 09:32:29+00:00
- **Updated**: 2022-04-13 09:42:20+00:00
- **Authors**: Buyu Liu, Bingbing Zhuang, Manmohan Chandraker
- **Comment**: to be appeared in CVPR22
- **Journal**: None
- **Summary**: We propose an end-to-end network that takes a single perspective RGB image of a complex road scene as input, to produce occlusion-reasoned layouts in perspective space as well as a parametric bird's-eye-view (BEV) space. In contrast to prior works that require dense supervision such as semantic labels in perspective view, our method only requires human annotations for parametric attributes that are cheaper and less ambiguous to obtain. To solve this challenging task, our design is comprised of modules that incorporate inductive biases to learn occlusion-reasoning, geometric transformation and semantic abstraction, where each module may be supervised by appropriately transforming the parametric annotations. We demonstrate how our design choices and proposed deep supervision help achieve meaningful representations and accurate predictions. We validate our approach on two public datasets, KITTI and NuScenes, to achieve state-of-the-art results with considerably less human supervision.



### VTGAN: Semi-supervised Retinal Image Synthesis and Disease Prediction using Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.06757v3
- **DOI**: 10.1109/ICCVW54120.2021.00362
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06757v3)
- **Published**: 2021-04-14 10:32:36+00:00
- **Updated**: 2021-08-13 04:30:46+00:00
- **Authors**: Sharif Amit Kamran, Khondker Fariha Hossain, Alireza Tavakkoli, Stewart Lee Zuckerbrod, Salah A. Baker
- **Comment**: Accepted to ICCV 2021 Workshop on Computer Vision for Automated
  Medical Diagnosis
- **Journal**: None
- **Summary**: In Fluorescein Angiography (FA), an exogenous dye is injected in the bloodstream to image the vascular structure of the retina. The injected dye can cause adverse reactions such as nausea, vomiting, anaphylactic shock, and even death. In contrast, color fundus imaging is a non-invasive technique used for photographing the retina but does not have sufficient fidelity for capturing its vascular structure. The only non-invasive method for capturing retinal vasculature is optical coherence tomography-angiography (OCTA). However, OCTA equipment is quite expensive, and stable imaging is limited to small areas on the retina. In this paper, we propose a novel conditional generative adversarial network (GAN) capable of simultaneously synthesizing FA images from fundus photographs while predicting retinal degeneration. The proposed system has the benefit of addressing the problem of imaging retinal vasculature in a non-invasive manner as well as predicting the existence of retinal abnormalities. We use a semi-supervised approach to train our GAN using multiple weighted losses on different modalities of data. Our experiments validate that the proposed architecture exceeds recent state-of-the-art generative networks for fundus-to-angiography synthesis. Moreover, our vision transformer-based discriminators generalize quite well on out-of-distribution data sets for retinal disease prediction.



### Towards NIR-VIS Masked Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.06761v1
- **DOI**: 10.1109/LSP.2021.3071663
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06761v1)
- **Published**: 2021-04-14 10:40:09+00:00
- **Updated**: 2021-04-14 10:40:09+00:00
- **Authors**: Hang Du, Hailin Shi, Yinglu Liu, Dan Zeng, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Near-infrared to visible (NIR-VIS) face recognition is the most common case in heterogeneous face recognition, which aims to match a pair of face images captured from two different modalities. Existing deep learning based methods have made remarkable progress in NIR-VIS face recognition, while it encounters certain newly-emerged difficulties during the pandemic of COVID-19, since people are supposed to wear facial masks to cut off the spread of the virus. We define this task as NIR-VIS masked face recognition, and find it problematic with the masked face in the NIR probe image. First, the lack of masked face data is a challenging issue for the network training. Second, most of the facial parts (cheeks, mouth, nose etc.) are fully occluded by the mask, which leads to a large amount of loss of information. Third, the domain gap still exists in the remaining facial parts. In such scenario, the existing methods suffer from significant performance degradation caused by the above issues. In this paper, we aim to address the challenge of NIR-VIS masked face recognition from the perspectives of training data and training method. Specifically, we propose a novel heterogeneous training method to maximize the mutual information shared by the face representation of two domains with the help of semi-siamese networks. In addition, a 3D face reconstruction based approach is employed to synthesize masked face from the existing NIR image. Resorting to these practices, our solution provides the domain-invariant face representation which is also robust to the mask occlusion. Extensive experiments on three NIR-VIS face datasets demonstrate the effectiveness and cross-dataset-generalization capacity of our method.



### Graph-based Person Signature for Person Re-Identifications
- **Arxiv ID**: http://arxiv.org/abs/2104.06770v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06770v2)
- **Published**: 2021-04-14 10:54:36+00:00
- **Updated**: 2021-04-17 14:57:11+00:00
- **Authors**: Binh X. Nguyen, Binh D. Nguyen, Tuong Do, Erman Tjiputra, Quang D. Tran, Anh Nguyen
- **Comment**: Accepted in CVPR 2021 Workshops
- **Journal**: None
- **Summary**: The task of person re-identification (ReID) is to match images of the same person over multiple non-overlapping camera views. Due to the variations in visual factors, previous works have investigated how the person identity, body parts, and attributes benefit the person ReID problem. However, the correlations between attributes, body parts, and within each attribute are not fully utilized. In this paper, we propose a new method to effectively aggregate detailed person descriptions (attributes labels) and visual features (body parts and global features) into a graph, namely Graph-based Person Signature, and utilize Graph Convolutional Networks to learn the topological structure of the visual signature of a person. The graph is integrated into a multi-branch multi-task framework for person re-identification. The extensive experiments are conducted to demonstrate the effectiveness of our proposed approach on two large-scale datasets, including Market-1501 and DukeMTMC-ReID. Our approach achieves competitive results among the state of the art and outperforms other attribute-based or mask-guided methods.



### Deep Evaluation Metric: Learning to Evaluate Simulated Radar Point Clouds for Virtual Testing of Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2104.06772v2
- **DOI**: 10.1109/RadarConf2147009.2021.9455235
- **Categories**: **cs.CV**, cs.AI, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2104.06772v2)
- **Published**: 2021-04-14 11:04:50+00:00
- **Updated**: 2021-06-21 08:30:09+00:00
- **Authors**: Anthony Ngo, Max Paul Bauer, Michael Resch
- **Comment**: 2021 IEEE Radar Conference (IEEE RadarConf 2021)
- **Journal**: None
- **Summary**: The usage of environment sensor models for virtual testing is a promising approach to reduce the testing effort of autonomous driving. However, in order to deduce any statements regarding the performance of an autonomous driving function based on simulation, the sensor model has to be validated to determine the discrepancy between the synthetic and real sensor data. Since a certain degree of divergence can be assumed to exist, the sufficient level of fidelity must be determined, which poses a major challenge. In particular, a method for quantifying the fidelity of a sensor model does not exist and the problem of defining an appropriate metric remains. In this work, we train a neural network to distinguish real and simulated radar sensor data with the purpose of learning the latent features of real radar point clouds. Furthermore, we propose the classifier's confidence score for the `real radar point cloud' class as a metric to determine the degree of fidelity of synthetically generated radar data. The presented approach is evaluated and it can be demonstrated that the proposed deep evaluation metric outperforms conventional metrics in terms of its capability to identify characteristic differences between real and simulated radar data.



### HoughNet: Integrating near and long-range evidence for visual detection
- **Arxiv ID**: http://arxiv.org/abs/2104.06773v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06773v2)
- **Published**: 2021-04-14 11:05:29+00:00
- **Updated**: 2022-08-17 14:58:51+00:00
- **Authors**: Nermin Samet, Samet Hicsonmez, Emre Akbas
- **Comment**: accepted to the IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI). arXiv admin note: substantial text overlap with
  arXiv:2007.02355
- **Journal**: None
- **Summary**: This paper presents HoughNet, a one-stage, anchor-free, voting-based, bottom-up object detection method. Inspired by the Generalized Hough Transform, HoughNet determines the presence of an object at a certain location by the sum of the votes cast on that location. Votes are collected from both near and long-distance locations based on a log-polar vote field. Thanks to this voting mechanism, HoughNet is able to integrate both near and long-range, class-conditional evidence for visual recognition, thereby generalizing and enhancing current object detection methodology, which typically relies on only local evidence. On the COCO dataset, HoughNet's best model achieves $46.4$ $AP$ (and $65.1$ $AP_{50}$), performing on par with the state-of-the-art in bottom-up object detection and outperforming most major one-stage and two-stage methods. We further validate the effectiveness of our proposal in other visual detection tasks, namely, video object detection, instance segmentation, 3D object detection and keypoint detection for human pose estimation, and an additional "labels to photo" image generation task, where the integration of our voting module consistently improves performance in all cases. Code is available at https://github.com/nerminsamet/houghnet.



### Temporally-Aware Feature Pooling for Action Spotting in Soccer Broadcasts
- **Arxiv ID**: http://arxiv.org/abs/2104.06779v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06779v1)
- **Published**: 2021-04-14 11:09:03+00:00
- **Updated**: 2021-04-14 11:09:03+00:00
- **Authors**: Silvio Giancola, Bernard Ghanem
- **Comment**: 8 pages, Camera-Ready for CVSports 2021 (CVPRW)
- **Journal**: None
- **Summary**: Toward the goal of automatic production for sports broadcasts, a paramount task consists in understanding the high-level semantic information of the game in play. For instance, recognizing and localizing the main actions of the game would allow producers to adapt and automatize the broadcast production, focusing on the important details of the game and maximizing the spectator engagement. In this paper, we focus our analysis on action spotting in soccer broadcast, which consists in temporally localizing the main actions in a soccer game. To that end, we propose a novel feature pooling method based on NetVLAD, dubbed NetVLAD++, that embeds temporally-aware knowledge. Different from previous pooling methods that consider the temporal context as a single set to pool from, we split the context before and after an action occurs. We argue that considering the contextual information around the action spot as a single entity leads to a sub-optimal learning for the pooling module. With NetVLAD++, we disentangle the context from the past and future frames and learn specific vocabularies of semantics for each subsets, avoiding to blend and blur such vocabulary in time. Injecting such prior knowledge creates more informative pooling modules and more discriminative pooled features, leading into a better understanding of the actions. We train and evaluate our methodology on the recent large-scale dataset SoccerNet-v2, reaching 53.4% Average-mAP for action spotting, a +12.7% improvement w.r.t the current state-of-the-art.



### Towards a Better Understanding of VR Sickness: Physical Symptom Prediction for VR Contents
- **Arxiv ID**: http://arxiv.org/abs/2104.06780v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06780v1)
- **Published**: 2021-04-14 11:09:03+00:00
- **Updated**: 2021-04-14 11:09:03+00:00
- **Authors**: Hak Gu Kim, Sangmin Lee, Seongyeop Kim, Heoun-taek Lim, Yong Man Ro
- **Comment**: AAAI 2021
- **Journal**: None
- **Summary**: We address the black-box issue of VR sickness assessment (VRSA) by evaluating the level of physical symptoms of VR sickness. For the VR contents inducing the similar VR sickness level, the physical symptoms can vary depending on the characteristics of the contents. Most of existing VRSA methods focused on assessing the overall VR sickness score. To make better understanding of VR sickness, it is required to predict and provide the level of major symptoms of VR sickness rather than overall degree of VR sickness. In this paper, we predict the degrees of main physical symptoms affecting the overall degree of VR sickness, which are disorientation, nausea, and oculomotor. In addition, we introduce a new large-scale dataset for VRSA including 360 videos with various frame rates, physiological signals, and subjective scores. On VRSA benchmark and our newly collected dataset, our approach shows a potential to not only achieve the highest correlation with subjective scores, but also to better understand which symptoms are the main causes of VR sickness.



### Context-Dependent Anomaly Detection for Low Altitude Traffic Surveillance
- **Arxiv ID**: http://arxiv.org/abs/2104.06781v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.06781v1)
- **Published**: 2021-04-14 11:12:04+00:00
- **Updated**: 2021-04-14 11:12:04+00:00
- **Authors**: Ilker Bozcan, Erdal Kayacan
- **Comment**: 7 pages, 4 figures, Accepted to IEEE International Conference on
  Robotics and Automation 2021 (ICRA 2021)
- **Journal**: None
- **Summary**: The detection of contextual anomalies is a challenging task for surveillance since an observation can be considered anomalous or normal in a specific environmental context. An unmanned aerial vehicle (UAV) can utilize its aerial monitoring capability and employ multiple sensors to gather contextual information about the environment and perform contextual anomaly detection. In this work, we introduce a deep neural network-based method (CADNet) to find point anomalies (i.e., single instance anomalous data) and contextual anomalies (i.e., context-specific abnormality) in an environment using a UAV. The method is based on a variational autoencoder (VAE) with a context sub-network. The context sub-network extracts contextual information regarding the environment using GPS and time data, then feeds it to the VAE to predict anomalies conditioned on the context. To the best of our knowledge, our method is the first contextual anomaly detection method for UAV-assisted aerial surveillance. We evaluate our method on the AU-AIR dataset in a traffic surveillance scenario. Quantitative comparisons against several baselines demonstrate the superiority of our approach in the anomaly detection tasks. The codes and data will be available at https://bozcani.github.io/cadnet.



### Visual Comfort Aware-Reinforcement Learning for Depth Adjustment of Stereoscopic 3D Images
- **Arxiv ID**: http://arxiv.org/abs/2104.06782v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06782v1)
- **Published**: 2021-04-14 11:15:08+00:00
- **Updated**: 2021-04-14 11:15:08+00:00
- **Authors**: Hak Gu Kim, Minho Park, Sangmin Lee, Seongyeop Kim, Yong Man Ro
- **Comment**: AAAI 2021
- **Journal**: None
- **Summary**: Depth adjustment aims to enhance the visual experience of stereoscopic 3D (S3D) images, which accompanied with improving visual comfort and depth perception. For a human expert, the depth adjustment procedure is a sequence of iterative decision making. The human expert iteratively adjusts the depth until he is satisfied with the both levels of visual comfort and the perceived depth. In this work, we present a novel deep reinforcement learning (DRL)-based approach for depth adjustment named VCA-RL (Visual Comfort Aware Reinforcement Learning) to explicitly model human sequential decision making in depth editing operations. We formulate the depth adjustment process as a Markov decision process where actions are defined as camera movement operations to control the distance between the left and right cameras. Our agent is trained based on the guidance of an objective visual comfort assessment metric to learn the optimal sequence of camera movement actions in terms of perceptual aspects in stereoscopic viewing. With extensive experiments and user studies, we show the effectiveness of our VCA-RL model on three different S3D databases.



### VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
- **Arxiv ID**: http://arxiv.org/abs/2104.06789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06789v1)
- **Published**: 2021-04-14 11:39:19+00:00
- **Updated**: 2021-04-14 11:39:19+00:00
- **Authors**: Zhixiang Min, Yiding Yang, Enrique Dunn
- **Comment**: Paper was accepted to CVPR20. Proceedings of the IEEE/CVF Conference
  on Computer Vision and Pattern Recognition. 2020. The arxiv version fixed a
  few typos
- **Journal**: None
- **Summary**: We propose a dense indirect visual odometry method taking as input externally estimated optical flow fields instead of hand-crafted feature correspondences. We define our problem as a probabilistic model and develop a generalized-EM formulation for the joint inference of camera motion, pixel depth, and motion-track confidence. Contrary to traditional methods assuming Gaussian-distributed observation errors, we supervise our inference framework under an (empirically validated) adaptive log-logistic distribution model. Moreover, the log-logistic residual model generalizes well to different state-of-the-art optical flow methods, making our approach modular and agnostic to the choice of optical flow estimators. Our method achieved top-ranking results on both TUM RGB-D and KITTI odometry benchmarks. Our open-sourced implementation is inherently GPU-friendly with only linear computational and storage growth.



### Revisiting Light Field Rendering with Deep Anti-Aliasing Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2104.06797v2
- **DOI**: 10.1109/TPAMI.2021.3073739
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06797v2)
- **Published**: 2021-04-14 12:03:25+00:00
- **Updated**: 2021-04-28 02:38:30+00:00
- **Authors**: Gaochang Wu, Yebin Liu, Lu Fang, Tianyou Chai
- **Comment**: 15 pages, 12 figures. Accepted by IEEE TPAMI
- **Journal**: IEEE TPAMI, 2021
- **Summary**: The light field (LF) reconstruction is mainly confronted with two challenges, large disparity and the non-Lambertian effect. Typical approaches either address the large disparity challenge using depth estimation followed by view synthesis or eschew explicit depth information to enable non-Lambertian rendering, but rarely solve both challenges in a unified framework. In this paper, we revisit the classic LF rendering framework to address both challenges by incorporating it with advanced deep learning techniques. First, we analytically show that the essential issue behind the large disparity and non-Lambertian challenges is the aliasing problem. Classic LF rendering approaches typically mitigate the aliasing with a reconstruction filter in the Fourier domain, which is, however, intractable to implement within a deep learning pipeline. Instead, we introduce an alternative framework to perform anti-aliasing reconstruction in the image domain and analytically show comparable efficacy on the aliasing issue. To explore the full potential, we then embed the anti-aliasing framework into a deep neural network through the design of an integrated architecture and trainable parameters. The network is trained through end-to-end optimization using a peculiar training set, including regular LFs and unstructured LFs. The proposed deep learning pipeline shows a substantial superiority in solving both the large disparity and the non-Lambertian challenges compared with other state-of-the-art approaches. In addition to the view interpolation for an LF, we also show that the proposed pipeline also benefits light field view extrapolation.



### VOLDOR-SLAM: For the Times When Feature-Based or Direct Methods Are Not Good Enough
- **Arxiv ID**: http://arxiv.org/abs/2104.06800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06800v1)
- **Published**: 2021-04-14 12:04:32+00:00
- **Updated**: 2021-04-14 12:04:32+00:00
- **Authors**: Zhixiang Min, Enrique Dunn
- **Comment**: Paper was accepted to ICRA21
- **Journal**: None
- **Summary**: We present a dense-indirect SLAM system using external dense optical flows as input. We extend the recent probabilistic visual odometry model VOLDOR [Min et al. CVPR'20], by incorporating the use of geometric priors to 1) robustly bootstrap estimation from monocular capture, while 2) seamlessly supporting stereo and/or RGB-D input imagery. Our customized back-end tightly couples our intermediate geometric estimates with an adaptive priority scheme managing the connectivity of an incremental pose graph. We leverage recent advances in dense optical flow methods to achieve accurate and robust camera pose estimates, while constructing fine-grain globally-consistent dense environmental maps. Our open source implementation [https://github.com/htkseason/VOLDOR] operates online at around 15 FPS on a single GTX1080Ti GPU.



### Continual learning in cross-modal retrieval
- **Arxiv ID**: http://arxiv.org/abs/2104.06806v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06806v2)
- **Published**: 2021-04-14 12:13:39+00:00
- **Updated**: 2021-04-19 14:20:57+00:00
- **Authors**: Kai Wang, Luis Herranz, Joost van de Weijer
- **Comment**: 2nd CLVISION workshop in CVPR 2021
- **Journal**: None
- **Summary**: Multimodal representations and continual learning are two areas closely related to human intelligence. The former considers the learning of shared representation spaces where information from different modalities can be compared and integrated (we focus on cross-modal retrieval between language and visual representations). The latter studies how to prevent forgetting a previously learned task when learning a new one. While humans excel in these two aspects, deep neural networks are still quite limited. In this paper, we propose a combination of both problems into a continual cross-modal retrieval setting, where we study how the catastrophic interference caused by new tasks impacts the embedding spaces and their cross-modal alignment required for effective retrieval. We propose a general framework that decouples the training, indexing and querying stages. We also identify and study different factors that may lead to forgetting, and propose tools to alleviate it. We found that the indexing stage pays an important role and that simply avoiding reindexing the database with updated embedding networks can lead to significant gains. We evaluated our methods in two image-text retrieval datasets, obtaining significant gains with respect to the fine tuning baseline.



### Global Information Guided Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.06813v1
- **DOI**: 10.1145/3394171.3416277
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06813v1)
- **Published**: 2021-04-14 12:32:13+00:00
- **Updated**: 2021-04-14 12:32:13+00:00
- **Authors**: Hui Lv, Chunyan Xu, Zhen Cui
- **Comment**: None
- **Journal**: None
- **Summary**: Video anomaly detection (VAD) is currently a challenging task due to the complexity of anomaly as well as the lack of labor-intensive temporal annotations. In this paper, we propose an end-to-end Global Information Guided (GIG) anomaly detection framework for anomaly detection using the video-level annotations (i.e., weak labels). We propose to first mine the global pattern cues by leveraging the weak labels in a GIG module. Then we build a spatial reasoning module to measure the relevance between vectors in spatial domain with the global cue vectors, and select the most related feature vectors for temporal anomaly detection. The experimental results on the CityScene challenge demonstrate the effectiveness of our model.



### Dewarping Document Image By Displacement Flow Estimation with Fully Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2104.06815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06815v1)
- **Published**: 2021-04-14 12:32:36+00:00
- **Updated**: 2021-04-14 12:32:36+00:00
- **Authors**: Guo-Wang Xie, Fei Yin, Xu-Yao Zhang, Cheng-Lin Liu
- **Comment**: None
- **Journal**: International Workshop on Document Analysis Systems. Springer,
  Cham, 2020: 131-144
- **Summary**: As camera-based documents are increasingly used, the rectification of distorted document images becomes a need to improve the recognition performance. In this paper, we propose a novel framework for both rectifying distorted document image and removing background finely, by estimating pixel-wise displacements using a fully convolutional network (FCN). The document image is rectified by transformation according to the displacements of pixels. The FCN is trained by regressing displacements of synthesized distorted documents, and to control the smoothness of displacements, we propose a Local Smooth Constraint (LSC) in regularization. Our approach is easy to implement and consumes moderate computing resource. Experiments proved that our approach can dewarp document images effectively under various geometric distortions, and has achieved the state-of-the-art performance in terms of local details and overall effect.



### Towards Automatic Model Specialization for Edge Video Analytics
- **Arxiv ID**: http://arxiv.org/abs/2104.06826v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06826v2)
- **Published**: 2021-04-14 12:57:40+00:00
- **Updated**: 2021-12-13 10:22:05+00:00
- **Authors**: Daniel Rivas, Francesc Guim, Jordà Polo, Pubudu M. Silva, Josep Ll. Berral, David Carrera
- **Comment**: None
- **Journal**: None
- **Summary**: Judging by popular and generic computer vision challenges, such as the ImageNet or PASCAL VOC, neural networks have proven to be exceptionally accurate in recognition tasks. However, state-of-the-art accuracy often comes at a high computational price, requiring hardware acceleration to achieve real-time performance, while use cases, such as smart cities, require images from fixed cameras to be analyzed in real-time. Due to the amount of network bandwidth these streams would generate, we cannot rely on offloading compute to a centralized cloud. Thus, a distributed edge cloud is expected to process images locally. However, the edge is, by nature, resource-constrained, which puts a limit on the computational complexity that can execute. Yet, there is a need for a meeting point between the edge and accurate real-time video analytics. Specializing lightweight models on a per-camera basis may help but it quickly becomes unfeasible as the number of cameras grows unless the process is automated. In this paper, we present and evaluate COVA (Contextually Optimized Video Analytics), a framework to assist in the automatic specialization of models for video analytics in edge cameras. COVA automatically improves the accuracy of lightweight models through their specialization. Moreover, we discuss and review each step involved in the process to understand the different trade-offs that each one entails. Additionally, we show how the sole assumption of static cameras allows us to make a series of considerations that greatly simplify the scope of the problem. Finally, experiments show that state-of-the-art models, i.e., able to generalize to unseen environments, can be effectively used as teachers to tailor smaller networks to a specific context, boosting accuracy at a constant computational cost. Results show that our COVA can automatically improve accuracy of pre-trained models by an average of 21%.



### Image Manipulation Detection by Multi-View Multi-Scale Supervision
- **Arxiv ID**: http://arxiv.org/abs/2104.06832v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.06832v2)
- **Published**: 2021-04-14 13:05:58+00:00
- **Updated**: 2021-07-25 06:45:00+00:00
- **Authors**: Xinru Chen, Chengbo Dong, Jiaqi Ji, Juan Cao, Xirong Li
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: The key challenge of image manipulation detection is how to learn generalizable features that are sensitive to manipulations in novel data, whilst specific to prevent false alarms on authentic images. Current research emphasizes the sensitivity, with the specificity overlooked. In this paper we address both aspects by multi-view feature learning and multi-scale supervision. By exploiting noise distribution and boundary artifact surrounding tampered regions, the former aims to learn semantic-agnostic and thus more generalizable features. The latter allows us to learn from authentic images which are nontrivial to be taken into account by current semantic segmentation network based methods. Our thoughts are realized by a new network which we term MVSS-Net. Extensive experiments on five benchmark sets justify the viability of MVSS-Net for both pixel-level and image-level manipulation detection.



### LEAP: Learning Articulated Occupancy of People
- **Arxiv ID**: http://arxiv.org/abs/2104.06849v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.06849v1)
- **Published**: 2021-04-14 13:41:56+00:00
- **Updated**: 2021-04-14 13:41:56+00:00
- **Authors**: Marko Mihajlovic, Yan Zhang, Michael J. Black, Siyu Tang
- **Comment**: In Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition. 2021
- **Journal**: None
- **Summary**: Substantial progress has been made on modeling rigid 3D objects using deep implicit representations. Yet, extending these methods to learn neural models of human shape is still in its infancy. Human bodies are complex and the key challenge is to learn a representation that generalizes such that it can express body shape deformations for unseen subjects in unseen, highly-articulated, poses. To address this challenge, we introduce LEAP (LEarning Articulated occupancy of People), a novel neural occupancy representation of the human body. Given a set of bone transformations (i.e. joint locations and rotations) and a query point in space, LEAP first maps the query point to a canonical space via learned linear blend skinning (LBS) functions and then efficiently queries the occupancy value via an occupancy network that models accurate identity- and pose-dependent deformations in the canonical space. Experiments show that our canonicalized occupancy estimation with the learned LBS functions greatly improves the generalization capability of the learned occupancy representation across various human shapes and poses, outperforming existing solutions in all settings.



### A Vision-based System for Traffic Anomaly Detection using Deep Learning and Decision Trees
- **Arxiv ID**: http://arxiv.org/abs/2104.06856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06856v1)
- **Published**: 2021-04-14 13:48:13+00:00
- **Updated**: 2021-04-14 13:48:13+00:00
- **Authors**: Armstrong Aboah, Maged Shoman, Vishal Mandal, Sayedomidreza Davami, Yaw Adu-Gyamfi, Anuj Sharma
- **Comment**: None
- **Journal**: None
- **Summary**: Any intelligent traffic monitoring system must be able to detect anomalies such as traffic accidents in real time. In this paper, we propose a Decision-Tree - enabled approach powered by Deep Learning for extracting anomalies from traffic cameras while accurately estimating the start and end time of the anomalous event. Our approach included creating a detection model, followed by anomaly detection and analysis. YOLOv5 served as the foundation for our detection model. The anomaly detection and analysis step entail traffic scene background estimation, road mask extraction, and adaptive thresholding. Candidate anomalies were passed through a decision tree to detect and analyze final anomalies. The proposed approach yielded an F1 score of 0.8571, and an S4 score of 0.5686, per the experimental validation.



### CelebHair: A New Large-Scale Dataset for Hairstyle Recommendation based on CelebA
- **Arxiv ID**: http://arxiv.org/abs/2104.06885v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.06885v1)
- **Published**: 2021-04-14 14:26:37+00:00
- **Updated**: 2021-04-14 14:26:37+00:00
- **Authors**: Yutao Chen, Yuxuan Zhang, Zhongrui Huang, Zhenyao Luo, Jinpeng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a new large-scale dataset for hairstyle recommendation, CelebHair, based on the celebrity facial attributes dataset, CelebA. Our dataset inherited the majority of facial images along with some beauty-related facial attributes from CelebA. Additionally, we employed facial landmark detection techniques to extract extra features such as nose length and pupillary distance, and deep convolutional neural networks for face shape and hairstyle classification. Empirical comparison has demonstrated the superiority of our dataset to other existing hairstyle-related datasets regarding variety, veracity, and volume. Analysis and experiments have been conducted on the dataset in order to evaluate its robustness and usability.



### Harmonious Semantic Line Detection via Maximal Weight Clique Selection
- **Arxiv ID**: http://arxiv.org/abs/2104.06903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06903v1)
- **Published**: 2021-04-14 14:54:27+00:00
- **Updated**: 2021-04-14 14:54:27+00:00
- **Authors**: Dongkwon Jin, Wonhui Park, Seong-Gyun Jeong, Chang-Su Kim
- **Comment**: Accepted to CVPR2021
- **Journal**: None
- **Summary**: A novel algorithm to detect an optimal set of semantic lines is proposed in this work. We develop two networks: selection network (S-Net) and harmonization network (H-Net). First, S-Net computes the probabilities and offsets of line candidates. Second, we filter out irrelevant lines through a selection-and-removal process. Third, we construct a complete graph, whose edge weights are computed by H-Net. Finally, we determine a maximal weight clique representing an optimal set of semantic lines. Moreover, to assess the overall harmony of detected lines, we propose a novel metric, called HIoU. Experimental results demonstrate that the proposed algorithm can detect harmonious semantic lines effectively and efficiently. Our codes are available at https://github.com/dongkwonjin/Semantic-Line-MWCS.



### Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes
- **Arxiv ID**: http://arxiv.org/abs/2104.06935v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.06935v1)
- **Published**: 2021-04-14 15:38:57+00:00
- **Updated**: 2021-04-14 15:38:57+00:00
- **Authors**: Julian Chibane, Aayush Bansal, Verica Lazova, Gerard Pons-Moll
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2021
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2021
- **Summary**: Recent neural view synthesis methods have achieved impressive quality and realism, surpassing classical pipelines which rely on multi-view reconstruction. State-of-the-Art methods, such as NeRF, are designed to learn a single scene with a neural network and require dense multi-view inputs. Testing on a new scene requires re-training from scratch, which takes 2-3 days. In this work, we introduce Stereo Radiance Fields (SRF), a neural view synthesis approach that is trained end-to-end, generalizes to new scenes, and requires only sparse views at test time. The core idea is a neural architecture inspired by classical multi-view stereo methods, which estimates surface points by finding similar image regions in stereo images. In SRF, we predict color and density for each 3D point given an encoding of its stereo correspondence in the input images. The encoding is implicitly learned by an ensemble of pair-wise similarities -- emulating classical stereo. Experiments show that SRF learns structure instead of overfitting on a scene. We train on multiple scenes of the DTU dataset and generalize to new ones without re-training, requiring only 10 sparse and spread-out views as input. We show that 10-15 minutes of fine-tuning further improve the results, achieving significantly sharper, more detailed results than scene-specific models. The code, model, and videos are available at https://virtualhumans.mpi-inf.mpg.de/srf/.



### IQDet: Instance-wise Quality Distribution Sampling for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.06936v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06936v1)
- **Published**: 2021-04-14 15:57:22+00:00
- **Updated**: 2021-04-14 15:57:22+00:00
- **Authors**: Yuchen Ma, Songtao Liu, Zeming Li, Jian Sun
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: We propose a dense object detector with an instance-wise sampling strategy, named IQDet. Instead of using human prior sampling strategies, we first extract the regional feature of each ground-truth to estimate the instance-wise quality distribution. According to a mixture model in spatial dimensions, the distribution is more noise-robust and adapted to the semantic pattern of each instance. Based on the distribution, we propose a quality sampling strategy, which automatically selects training samples in a probabilistic manner and trains with more high-quality samples. Extensive experiments on MS COCO show that our method steadily improves baseline by nearly 2.4 AP without bells and whistles. Moreover, our best model achieves 51.6 AP, outperforming all existing state-of-the-art one-stage detectors and it is completely cost-free in inference time.



### Shared memories driven by the intrinsic memorability of items
- **Arxiv ID**: http://arxiv.org/abs/2104.06937v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06937v1)
- **Published**: 2021-04-14 16:03:27+00:00
- **Updated**: 2021-04-14 16:03:27+00:00
- **Authors**: Wilma A. Bainbridge
- **Comment**: None
- **Journal**: None
- **Summary**: When we experience an event, it feels like our previous experiences, our interpretations of that event (e.g., aesthetics, emotions), and our current state will determine how we will remember it. However, recent work has revealed a strong sway of the visual world itself in influencing what we remember and forget. Certain items -- including certain faces, words, images, and movements -- are intrinsically memorable or forgettable across observers, regardless of individual differences. Further, neuroimaging research has revealed that the brain is sensitive to memorability both rapidly and automatically during late perception. These strong consistencies in memory across people may reflect the broad organizational principles of our sensory environment, and may reveal how the brain prioritizes information before encoding items into memory. In this chapter, I will discuss our current state-of-the-art understanding of memorability for visual information, and what these findings imply about how we perceive and remember visual events.



### In-field high throughput grapevine phenotyping with a consumer-grade depth camera
- **Arxiv ID**: http://arxiv.org/abs/2104.06945v1
- **DOI**: 10.1016/j.compag.2018.11.026
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06945v1)
- **Published**: 2021-04-14 16:16:27+00:00
- **Updated**: 2021-04-14 16:16:27+00:00
- **Authors**: Annalisa Milella, Roberto Marani, Antonio Petitti, Giulio Reina
- **Comment**: None
- **Journal**: Computers and Electronics in Agriculture 156 2019 293306
- **Summary**: Plant phenotyping, that is, the quantitative assessment of plant traits including growth, morphology, physiology, and yield, is a critical aspect towards efficient and effective crop management. Currently, plant phenotyping is a manually intensive and time consuming process, which involves human operators making measurements in the field, based on visual estimates or using hand-held devices. In this work, methods for automated grapevine phenotyping are developed, aiming to canopy volume estimation and bunch detection and counting. It is demonstrated that both measurements can be effectively performed in the field using a consumer-grade depth camera mounted onboard an agricultural vehicle.



### Temporally-Coherent Surface Reconstruction via Metric-Consistent Atlases
- **Arxiv ID**: http://arxiv.org/abs/2104.06950v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06950v2)
- **Published**: 2021-04-14 16:21:22+00:00
- **Updated**: 2021-08-23 12:40:23+00:00
- **Authors**: Jan Bednarik, Vladimir G. Kim, Siddhartha Chaudhuri, Shaifali Parashar, Mathieu Salzmann, Pascal Fua, Noam Aigerman
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: We propose a method for the unsupervised reconstruction of a temporally-coherent sequence of surfaces from a sequence of time-evolving point clouds, yielding dense, semantically meaningful correspondences between all keyframes. We represent the reconstructed surface as an atlas, using a neural network. Using canonical correspondences defined via the atlas, we encourage the reconstruction to be as isometric as possible across frames, leading to semantically-meaningful reconstruction. Through experiments and comparisons, we empirically show that our method achieves results that exceed that state of the art in the accuracy of unsupervised correspondences and accuracy of surface reconstruction.



### Aligning Latent and Image Spaces to Connect the Unconnectable
- **Arxiv ID**: http://arxiv.org/abs/2104.06954v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.06954v1)
- **Published**: 2021-04-14 16:29:20+00:00
- **Updated**: 2021-04-14 16:29:20+00:00
- **Authors**: Ivan Skorokhodov, Grigorii Sotnikov, Mohamed Elhoseiny
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we develop a method to generate infinite high-resolution images with diverse and complex content. It is based on a perfectly equivariant generator with synchronous interpolations in the image and latent spaces. Latent codes, when sampled, are positioned on the coordinate grid, and each pixel is computed from an interpolation of the nearby style codes. We modify the AdaIN mechanism to work in such a setup and train the generator in an adversarial setting to produce images positioned between any two latent vectors. At test time, this allows for generating complex and diverse infinite images and connecting any two unrelated scenes into a single arbitrarily large panorama. Apart from that, we introduce LHQ: a new dataset of \lhqsize high-resolution nature landscapes. We test the approach on LHQ, LSUN Tower and LSUN Bridge and outperform the baselines by at least 4 times in terms of quality and diversity of the produced infinite images. The project page is located at https://universome.github.io/alis.



### ComBiNet: Compact Convolutional Bayesian Neural Network for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.06957v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.06957v2)
- **Published**: 2021-04-14 16:33:48+00:00
- **Updated**: 2021-06-26 11:34:44+00:00
- **Authors**: Martin Ferianc, Divyansh Manocha, Hongxiang Fan, Miguel Rodrigues
- **Comment**: Accepted for publication at ICANN 2021. Code at:
  https://github.com/martinferianc/ComBiNet
- **Journal**: None
- **Summary**: Fully convolutional U-shaped neural networks have largely been the dominant approach for pixel-wise image segmentation. In this work, we tackle two defects that hinder their deployment in real-world applications: 1) Predictions lack uncertainty quantification that may be crucial to many decision-making systems; 2) Large memory storage and computational consumption demanding extensive hardware resources. To address these issues and improve their practicality we demonstrate a few-parameter compact Bayesian convolutional architecture, that achieves a marginal improvement in accuracy in comparison to related work using significantly fewer parameters and compute operations. The architecture combines parameter-efficient operations such as separable convolutions, bilinear interpolation, multi-scale feature propagation and Bayesian inference for per-pixel uncertainty quantification through Monte Carlo Dropout. The best performing configurations required fewer than 2.5 million parameters on diverse challenging datasets with few observations.



### Pose Recognition with Cascade Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.06976v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06976v1)
- **Published**: 2021-04-14 17:00:22+00:00
- **Updated**: 2021-04-14 17:00:22+00:00
- **Authors**: Ke Li, Shijie Wang, Xiang Zhang, Yifan Xu, Weijian Xu, Zhuowen Tu
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: In this paper, we present a regression-based pose recognition method using cascade Transformers. One way to categorize the existing approaches in this domain is to separate them into 1). heatmap-based and 2). regression-based. In general, heatmap-based methods achieve higher accuracy but are subject to various heuristic designs (not end-to-end mostly), whereas regression-based approaches attain relatively lower accuracy but they have less intermediate non-differentiable steps. Here we utilize the encoder-decoder structure in Transformers to perform regression-based person and keypoint detection that is general-purpose and requires less heuristic design compared with the existing approaches. We demonstrate the keypoint hypothesis (query) refinement process across different self-attention layers to reveal the recursive self-attention mechanism in Transformers. In the experiments, we report competitive results for pose recognition when compared with the competing regression-based methods.



### Discrete Cosine Transform Network for Guided Depth Map Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2104.06977v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06977v3)
- **Published**: 2021-04-14 17:01:03+00:00
- **Updated**: 2022-04-21 05:51:43+00:00
- **Authors**: Zixiang Zhao, Jiangshe Zhang, Shuang Xu, Zudi Lin, Hanspeter Pfister
- **Comment**: Accepted by CVPR 2022 (Oral)
- **Journal**: None
- **Summary**: Guided depth super-resolution (GDSR) is an essential topic in multi-modal image processing, which reconstructs high-resolution (HR) depth maps from low-resolution ones collected with suboptimal conditions with the help of HR RGB images of the same scene. To solve the challenges in interpreting the working mechanism, extracting cross-modal features and RGB texture over-transferred, we propose a novel Discrete Cosine Transform Network (DCTNet) to alleviate the problems from three aspects. First, the Discrete Cosine Transform (DCT) module reconstructs the multi-channel HR depth features by using DCT to solve the channel-wise optimization problem derived from the image domain. Second, we introduce a semi-coupled feature extraction module that uses shared convolutional kernels to extract common information and private kernels to extract modality-specific information. Third, we employ an edge attention mechanism to highlight the contours informative for guided upsampling. Extensive quantitative and qualitative evaluations demonstrate the effectiveness of our DCTNet, which outperforms previous state-of-the-art methods with a relatively small number of parameters. The code is available at \url{https://github.com/Zhaozixiang1228/GDSR-DCTNet}.



### Do Time Constraints Re-Prioritize Attention to Shapes During Visual Photo Inspection?
- **Arxiv ID**: http://arxiv.org/abs/2104.06984v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06984v1)
- **Published**: 2021-04-14 17:07:27+00:00
- **Updated**: 2021-04-14 17:07:27+00:00
- **Authors**: Yiyuan Yang, Kenneth Li, Fernanda Eliott, Maithilee Kunda
- **Comment**: None
- **Journal**: None
- **Summary**: People's visual experiences of the world are easy to carve up and examine along natural language boundaries, e.g., by category labels, attribute labels, etc. However, it is more difficult to elicit detailed visuospatial information about what a person attends to, e.g., the specific shape of a tree. Paying attention to the shapes of things not only feeds into well defined tasks like visual category learning, but it is also what enables us to differentiate similarly named objects and to take on creative visual pursuits, like poetically describing the shape of a thing, or finding shapes in the clouds or stars. We use a new data collection method that elicits people's prioritized attention to shapes during visual photo inspection by asking them to trace important parts of the image under varying time constraints. Using data collected via crowdsourcing over a set of 187 photographs, we examine changes in patterns of visual attention across individuals, across image types, and across time constraints.



### A hierarchical deep learning framework for the consistent classification of land use objects in geospatial databases
- **Arxiv ID**: http://arxiv.org/abs/2104.06991v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06991v1)
- **Published**: 2021-04-14 17:16:35+00:00
- **Updated**: 2021-04-14 17:16:35+00:00
- **Authors**: Chun Yang, Franz Rottensteiner, Christian Heipke
- **Comment**: None
- **Journal**: None
- **Summary**: Land use as contained in geospatial databases constitutes an essential input for different applica-tions such as urban management, regional planning and environmental monitoring. In this paper, a hierarchical deep learning framework is proposed to verify the land use information. For this purpose, a two-step strategy is applied. First, given high-resolution aerial images, the land cover information is determined. To achieve this, an encoder-decoder based convolutional neural net-work (CNN) is proposed. Second, the pixel-wise land cover information along with the aerial images serves as input for another CNN to classify land use. Because the object catalogue of geospatial databases is frequently constructed in a hierarchical manner, we propose a new CNN-based method aiming to predict land use in multiple levels hierarchically and simultaneously. A so called Joint Optimization (JO) is proposed where predictions are made by selecting the hier-archical tuple over all levels which has the maximum joint class scores, providing consistent results across the different levels. The conducted experiments show that the CNN relying on JO outperforms previous results, achieving an overall accuracy up to 92.5%. In addition to the individual experiments on two test sites, we investigate whether data showing different characteristics can improve the results of land cover and land use classification, when processed together. To do so, we combine the two datasets and undertake some additional experiments. The results show that adding more data helps both land cover and land use classification, especially the identification of underrepre-sented categories, despite their different characteristics.



### Do Neural Network Weights account for Classes Centers?
- **Arxiv ID**: http://arxiv.org/abs/2104.07004v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.5.1; I.2.10; G.4
- **Links**: [PDF](http://arxiv.org/pdf/2104.07004v1)
- **Published**: 2021-04-14 17:37:52+00:00
- **Updated**: 2021-04-14 17:37:52+00:00
- **Authors**: Ioannis Kansizoglou, Loukas Bampis, Antonios Gasteratos
- **Comment**: Index Terms: Discriminative feature learning, deep neural networks,
  symmetrical layer, geometric algebra
- **Journal**: None
- **Summary**: The exploitation of Deep Neural Networks (DNNs) as descriptors in feature learning challenges enjoys apparent popularity over the past few years. The above tendency focuses on the development of effective loss functions that ensure both high feature discrimination among different classes, as well as low geodesic distance between the feature vectors of a given class. The vast majority of the contemporary works rely their formulation on an empirical assumption about the feature space of a network's last hidden layer, claiming that the weight vector of a class accounts for its geometrical center in the studied space. The paper at hand follows a theoretical approach and indicates that the aforementioned hypothesis is not exclusively met. This fact raises stability issues regarding the training procedure of a DNN, as shown in our experimental study. Consequently, a specific symmetry is proposed and studied both analytically and empirically that satisfies the above assumption, addressing the established convergence issues.



### Dressing in Order: Recurrent Person Image Generation for Pose Transfer, Virtual Try-on and Outfit Editing
- **Arxiv ID**: http://arxiv.org/abs/2104.07021v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07021v3)
- **Published**: 2021-04-14 17:58:54+00:00
- **Updated**: 2022-10-18 17:46:34+00:00
- **Authors**: Aiyu Cui, Daniel McKee, Svetlana Lazebnik
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: We propose a flexible person generation framework called Dressing in Order (DiOr), which supports 2D pose transfer, virtual try-on, and several fashion editing tasks. The key to DiOr is a novel recurrent generation pipeline to sequentially put garments on a person, so that trying on the same garments in different orders will result in different looks. Our system can produce dressing effects not achievable by existing work, including different interactions of garments (e.g., wearing a top tucked into the bottom or over it), as well as layering of multiple garments of the same type (e.g., jacket over shirt over t-shirt). DiOr explicitly encodes the shape and texture of each garment, enabling these elements to be edited separately. Joint training on pose transfer and inpainting helps with detail preservation and coherence of generated garments. Extensive evaluations show that DiOr outperforms other recent methods like ADGAN in terms of output quality, and handles a wide range of editing functions for which there is no direct supervision.



### GridToPix: Training Embodied Agents with Minimal Supervision
- **Arxiv ID**: http://arxiv.org/abs/2105.00931v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2105.00931v2)
- **Published**: 2021-04-14 17:59:57+00:00
- **Updated**: 2021-10-13 17:26:12+00:00
- **Authors**: Unnat Jain, Iou-Jen Liu, Svetlana Lazebnik, Aniruddha Kembhavi, Luca Weihs, Alexander Schwing
- **Comment**: Project page: https://unnat.github.io/gridtopix/ ; last two authors
  contributed equally
- **Journal**: None
- **Summary**: While deep reinforcement learning (RL) promises freedom from hand-labeled data, great successes, especially for Embodied AI, require significant work to create supervision via carefully shaped rewards. Indeed, without shaped rewards, i.e., with only terminal rewards, present-day Embodied AI results degrade significantly across Embodied AI problems from single-agent Habitat-based PointGoal Navigation (SPL drops from 55 to 0) and two-agent AI2-THOR-based Furniture Moving (success drops from 58% to 1%) to three-agent Google Football-based 3 vs. 1 with Keeper (game score drops from 0.6 to 0.1). As training from shaped rewards doesn't scale to more realistic tasks, the community needs to improve the success of training with terminal rewards. For this we propose GridToPix: 1) train agents with terminal rewards in gridworlds that generically mirror Embodied AI environments, i.e., they are independent of the task; 2) distill the learned policy into agents that reside in complex visual worlds. Despite learning from only terminal rewards with identical models and RL algorithms, GridToPix significantly improves results across tasks: from PointGoal Navigation (SPL improves from 0 to 64) and Furniture Moving (success improves from 1% to 25%) to football gameplay (game score improves from 0.1 to 0.6). GridToPix even helps to improve the results of shaped reward training.



### Anatomy-guided Multimodal Registration by Learning Segmentation without Ground Truth: Application to Intraprocedural CBCT/MR Liver Segmentation and Registration
- **Arxiv ID**: http://arxiv.org/abs/2104.07056v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.07056v1)
- **Published**: 2021-04-14 18:07:03+00:00
- **Updated**: 2021-04-14 18:07:03+00:00
- **Authors**: Bo Zhou, Zachary Augenfeld, Julius Chapiro, S. Kevin Zhou, Chi Liu, James S. Duncan
- **Comment**: 12 pages, 8 figures, published at Medical Image Analysis (MedIA),
  code available at https://github.com/bbbbbbzhou/APA2Seg-Net
- **Journal**: None
- **Summary**: Multimodal image registration has many applications in diagnostic medical imaging and image-guided interventions, such as Transcatheter Arterial Chemoembolization (TACE) of liver cancer guided by intraprocedural CBCT and pre-operative MR. The ability to register peri-procedurally acquired diagnostic images into the intraprocedural environment can potentially improve the intra-procedural tumor targeting, which will significantly improve therapeutic outcomes. However, the intra-procedural CBCT often suffers from suboptimal image quality due to lack of signal calibration for Hounsfield unit, limited FOV, and motion/metal artifacts. These non-ideal conditions make standard intensity-based multimodal registration methods infeasible to generate correct transformation across modalities. While registration based on anatomic structures, such as segmentation or landmarks, provides an efficient alternative, such anatomic structure information is not always available. One can train a deep learning-based anatomy extractor, but it requires large-scale manual annotations on specific modalities, which are often extremely time-consuming to obtain and require expert radiological readers. To tackle these issues, we leverage annotated datasets already existing in a source modality and propose an anatomy-preserving domain adaptation to segmentation network (APA2Seg-Net) for learning segmentation without target modality ground truth. The segmenters are then integrated into our anatomy-guided multimodal registration based on the robust point matching machine. Our experimental results on in-house TACE patient data demonstrated that our APA2Seg-Net can generate robust CBCT and MR liver segmentation, and the anatomy-guided registration framework with these segmenters can provide high-quality multimodal registrations. Our code is available at https://github.com/bbbbbbzhou/APA2Seg-Net.



### Self-Supervised Learning of Remote Sensing Scene Representations Using Contrastive Multiview Coding
- **Arxiv ID**: http://arxiv.org/abs/2104.07070v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07070v2)
- **Published**: 2021-04-14 18:25:43+00:00
- **Updated**: 2021-06-03 17:59:23+00:00
- **Authors**: Vladan Stojnić, Vladimir Risojević
- **Comment**: EarthVision 2021 paper
- **Journal**: None
- **Summary**: In recent years self-supervised learning has emerged as a promising candidate for unsupervised representation learning. In the visual domain its applications are mostly studied in the context of images of natural scenes. However, its applicability is especially interesting in specific areas, like remote sensing and medicine, where it is hard to obtain huge amounts of labeled data. In this work, we conduct an extensive analysis of the applicability of self-supervised learning in remote sensing image classification. We analyze the influence of the number and domain of images used for self-supervised pre-training on the performance on downstream tasks. We show that, for the downstream task of remote sensing image classification, using self-supervised pre-training on remote sensing images can give better results than using supervised pre-training on images of natural scenes. Besides, we also show that self-supervised pre-training can be easily extended to multispectral images producing even better results on our downstream tasks.



### Self-supervised Learning of 3D Object Understanding by Data Association and Landmark Estimation for Image Sequence
- **Arxiv ID**: http://arxiv.org/abs/2104.07077v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07077v1)
- **Published**: 2021-04-14 18:59:08+00:00
- **Updated**: 2021-04-14 18:59:08+00:00
- **Authors**: Hyeonwoo Yu, Jean Oh
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a self-supervised learningmethod for multi-object pose estimation. 3D object under-standing from 2D image is a challenging task that infers ad-ditional dimension from reduced-dimensional information.In particular, the estimation of the 3D localization or orien-tation of an object requires precise reasoning, unlike othersimple clustering tasks such as object classification. There-fore, the scale of the training dataset becomes more cru-cial. However, it is challenging to obtain large amount of3D dataset since achieving 3D annotation is expensive andtime-consuming. If the scale of the training dataset can beincreased by involving the image sequence obtained fromsimple navigation, it is possible to overcome the scale lim-itation of the dataset and to have efficient adaptation tothe new environment. However, when the self annotation isconducted on single image by the network itself, trainingperformance of the network is bounded to the self perfor-mance. Therefore, we propose a strategy to exploit multipleobservations of the object in the image sequence in orderto surpass the self-performance: first, the landmarks for theglobal object map are estimated through network predic-tion and data association, and the corrected annotation fora single frame is obtained. Then, network fine-tuning is con-ducted including the dataset obtained by self-annotation,thereby exceeding the performance boundary of the networkitself. The proposed method was evaluated on the KITTIdriving scene dataset, and we demonstrate the performanceimprovement in the pose estimation of multi-object in 3D space.



### SVS-net: A Novel Semantic Segmentation Network in Optical Coherence Tomography Angiography Images
- **Arxiv ID**: http://arxiv.org/abs/2104.07083v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.07083v3)
- **Published**: 2021-04-14 19:19:17+00:00
- **Updated**: 2021-04-21 09:08:56+00:00
- **Authors**: Yih-Cherng Lee, Ling Yeung
- **Comment**: 6 pages, 6 figures
- **Journal**: None
- **Summary**: Automated vascular segmentation on optical coherence tomography angiography (OCTA) is important for the quantitative analyses of retinal microvasculature in neuroretinal and systemic diseases. Despite recent improvements, artifacts continue to pose challenges in segmentation. Our study focused on removing the speckle noise artifact from OCTA images when performing segmentation. Speckle noise is common in OCTA and is particularly prominent over large non-perfusion areas. It may interfere with the proper assessment of retinal vasculature. In this study, we proposed a novel Supervision Vessel Segmentation network (SVS-net) to detect vessels of different sizes. The SVS-net includes a new attention-based module to describe vessel positions and facilitate the understanding of the network learning process. The model is efficient and explainable and could be utilized to reduce the need for manual labeling. Our SVS-net had better performance in accuracy, recall, F1 score, and Kappa score when compared to other well recognized models.



### Fast Walsh-Hadamard Transform and Smooth-Thresholding Based Binary Layers in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.07085v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.07085v4)
- **Published**: 2021-04-14 19:23:36+00:00
- **Updated**: 2021-10-30 03:26:30+00:00
- **Authors**: Hongyi Pan, Diaa Dabawi, Ahmet Enis Cetin
- **Comment**: The paper (v1) has been accepted to CVPR 2021 BiVision Workshop. We
  notice the final Conv2D is also a 1x1 convolution layer so we update the
  result with changing the layer in v2. In v3, we update citation 37 because
  its authorship changes. In v4, we propose the improved version of smooth
  thresholding called "weighted smooth thresholding"
- **Journal**: None
- **Summary**: In this paper, we propose a novel layer based on fast Walsh-Hadamard transform (WHT) and smooth-thresholding to replace $1\times 1$ convolution layers in deep neural networks. In the WHT domain, we denoise the transform domain coefficients using the new smooth-thresholding non-linearity, a smoothed version of the well-known soft-thresholding operator. We also introduce a family of multiplication-free operators from the basic 2$\times$2 Hadamard transform to implement $3\times 3$ depthwise separable convolution layers. Using these two types of layers, we replace the bottleneck layers in MobileNet-V2 to reduce the network's number of parameters with a slight loss in accuracy. For example, by replacing the final third bottleneck layers, we reduce the number of parameters from 2.270M to 540K. This reduces the accuracy from 95.21\% to 92.98\% on the CIFAR-10 dataset. Our approach significantly improves the speed of data processing. The fast Walsh-Hadamard transform has a computational complexity of $O(m\log_2 m)$. As a result, it is computationally more efficient than the $1\times1$ convolution layer. The fast Walsh-Hadamard layer processes a tensor in $\mathbb{R}^{10\times32\times32\times1024}$ about 2 times faster than $1\times1$ convolution layer on NVIDIA Jetson Nano computer board.



### StEP: Style-based Encoder Pre-training for Multi-modal Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2104.07098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07098v1)
- **Published**: 2021-04-14 19:58:24+00:00
- **Updated**: 2021-04-14 19:58:24+00:00
- **Authors**: Moustafa Meshry, Yixuan Ren, Larry S Davis, Abhinav Shrivastava
- **Comment**: IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR), 2021
- **Journal**: None
- **Summary**: We propose a novel approach for multi-modal Image-to-image (I2I) translation. To tackle the one-to-many relationship between input and output domains, previous works use complex training objectives to learn a latent embedding, jointly with the generator, that models the variability of the output domain. In contrast, we directly model the style variability of images, independent of the image synthesis task. Specifically, we pre-train a generic style encoder using a novel proxy task to learn an embedding of images, from arbitrary domains, into a low-dimensional style latent space. The learned latent space introduces several advantages over previous traditional approaches to multi-modal I2I translation. First, it is not dependent on the target dataset, and generalizes well across multiple domains. Second, it learns a more powerful and expressive latent space, which improves the fidelity of style capture and transfer. The proposed style pre-training also simplifies the training objective and speeds up the training significantly. Furthermore, we provide a detailed study of the contribution of different loss terms to the task of multi-modal I2I translation, and propose a simple alternative to VAEs to enable sampling from unconstrained latent spaces. Finally, we achieve state-of-the-art results on six challenging benchmarks with a simple training objective that includes only a GAN loss and a reconstruction loss.



### Adaptive Intermediate Representations for Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/2104.07135v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07135v1)
- **Published**: 2021-04-14 21:37:23+00:00
- **Updated**: 2021-04-14 21:37:23+00:00
- **Authors**: Juhana Kangaspunta, AJ Piergiovanni, Rico Jonschkowski, Michael Ryoo, Anelia Angelova
- **Comment**: None
- **Journal**: None
- **Summary**: A common strategy to video understanding is to incorporate spatial and motion information by fusing features derived from RGB frames and optical flow. In this work, we introduce a new way to leverage semantic segmentation as an intermediate representation for video understanding and use it in a way that requires no additional labeling.   Second, we propose a general framework which learns the intermediate representations (optical flow and semantic segmentation) jointly with the final video understanding task and allows the adaptation of the representations to the end goal. Despite the use of intermediate representations within the network, during inference, no additional data beyond RGB sequences is needed, enabling efficient recognition with a single network.   Finally, we present a way to find the optimal learning configuration by searching the best loss weighting via evolution. We obtain more powerful visual representations for videos which lead to performance gains over the state-of-the-art.



### Federated Learning-based Active Authentication on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/2104.07158v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07158v1)
- **Published**: 2021-04-14 22:59:08+00:00
- **Updated**: 2021-04-14 22:59:08+00:00
- **Authors**: Poojan Oza, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: User active authentication on mobile devices aims to learn a model that can correctly recognize the enrolled user based on device sensor information. Due to lack of negative class data, it is often modeled as a one-class classification problem. In practice, mobile devices are connected to a central server, e.g, all android-based devices are connected to Google server through internet. This device-server structure can be exploited by recently proposed Federated Learning (FL) and Split Learning (SL) frameworks to perform collaborative learning over the data distributed among multiple devices. Using FL/SL frameworks, we can alleviate the lack of negative data problem by training a user authentication model over multiple user data distributed across devices. To this end, we propose a novel user active authentication training, termed as Federated Active Authentication (FAA), that utilizes the principles of FL/SL. We first show that existing FL/SL methods are suboptimal for FAA as they rely on the data to be distributed homogeneously (i.e. IID) across devices, which is not true in the case of FAA. Subsequently, we propose a novel method that is able to tackle heterogeneous/non-IID distribution of data in FAA. Specifically, we first extract feature statistics such as mean and variance corresponding to data from each user which are later combined in a central server to learn a multi-class classifier and sent back to the individual devices. We conduct extensive experiments using three active authentication benchmark datasets (MOBIO, UMDAA-01, UMDAA-02) and show that such approach performs better than state-of-the-art one-class based FAA methods and is also able to outperform traditional FL/SL methods.



### Unsupervised Continual Learning Via Pseudo Labels
- **Arxiv ID**: http://arxiv.org/abs/2104.07164v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.07164v3)
- **Published**: 2021-04-14 23:46:17+00:00
- **Updated**: 2021-07-30 23:22:08+00:00
- **Authors**: Jiangpeng He, Fengqing Zhu
- **Comment**: Accepted paper for 2021 IJCAI, Workshop
- **Journal**: None
- **Summary**: Continual learning aims to learn new tasks incrementally using less computation and memory resources instead of retraining the model from scratch whenever new task arrives. However, existing approaches are designed in supervised fashion assuming all data from new tasks have been manually annotated, which are not practical for many real-life applications. In this work, we propose to use pseudo label instead of the ground truth to make continual learning feasible in unsupervised mode. The pseudo labels of new data are obtained by applying global clustering algorithm and we propose to use the model updated from last incremental step as the feature extractor. Due to the scarcity of existing work, we introduce a new benchmark experimental protocol for unsupervised continual learning of image classification task under class-incremental setting where no class label is provided for each incremental learning step. Our method is evaluated on the CIFAR-100 and ImageNet (ILSVRC) datasets by incorporating the pseudo label with various existing supervised approaches and show promising results in unsupervised scenario.



