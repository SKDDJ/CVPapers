# Arxiv Papers in cs.CV on 2021-04-07
### ARC: A Vision-based Automatic Retail Checkout System
- **Arxiv ID**: http://arxiv.org/abs/2104.02832v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.02832v2)
- **Published**: 2021-04-07 00:07:53+00:00
- **Updated**: 2021-05-17 04:20:00+00:00
- **Authors**: Syed Talha Bukhari, Abdul Wahab Amin, Muhammad Abdullah Naveed, Muhammad Rzi Abbas
- **Comment**: Work was done during the academic year 2017-2018 as a Senior Year
  (undergraduate) Project (thesis)
- **Journal**: None
- **Summary**: Retail checkout systems employed at supermarkets primarily rely on barcode scanners, with some utilizing QR codes, to identify the items being purchased. These methods are time-consuming in practice, require a certain level of human supervision, and involve waiting in long queues. In this regard, we propose a system, that we call ARC, which aims at making the process of check-out at retail store counters faster, autonomous, and more convenient, while reducing dependency on a human operator. The approach makes use of a computer vision-based system, with a Convolutional Neural Network at its core, which scans objects placed beneath a webcam for identification. To evaluate the proposed system, we curated an image dataset of one-hundred local retail items of various categories. Within the given assumptions and considerations, the system achieves a reasonable test-time accuracy, pointing towards an ambitious future for the proposed setup. The project code and the dataset are made publicly available.



### Learning Triadic Belief Dynamics in Nonverbal Communication from Videos
- **Arxiv ID**: http://arxiv.org/abs/2104.02841v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.02841v1)
- **Published**: 2021-04-07 00:52:04+00:00
- **Updated**: 2021-04-07 00:52:04+00:00
- **Authors**: Lifeng Fan, Shuwen Qiu, Zilong Zheng, Tao Gao, Song-Chun Zhu, Yixin Zhu
- **Comment**: CVPR2021, Oral
- **Journal**: None
- **Summary**: Humans possess a unique social cognition capability; nonverbal communication can convey rich social information among agents. In contrast, such crucial social characteristics are mostly missing in the existing scene understanding literature. In this paper, we incorporate different nonverbal communication cues (e.g., gaze, human poses, and gestures) to represent, model, learn, and infer agents' mental states from pure visual inputs. Crucially, such a mental representation takes the agent's belief into account so that it represents what the true world state is and infers the beliefs in each agent's mental state, which may differ from the true world states. By aggregating different beliefs and true world states, our model essentially forms "five minds" during the interactions between two agents. This "five minds" model differs from prior works that infer beliefs in an infinite recursion; instead, agents' beliefs are converged into a "common mind". Based on this representation, we further devise a hierarchical energy-based model that jointly tracks and predicts all five minds. From this new perspective, a social event is interpreted by a series of nonverbal communication and belief dynamics, which transcends the classic keyframe video summary. In the experiments, we demonstrate that using such a social account provides a better video summary on videos with rich social interactions compared with state-of-the-art keyframe video summary methods.



### MultiScene: A Large-scale Dataset and Benchmark for Multi-scene Recognition in Single Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2104.02846v3
- **DOI**: 10.1109/TGRS.2021.3110314
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.02846v3)
- **Published**: 2021-04-07 01:09:12+00:00
- **Updated**: 2021-09-07 13:02:45+00:00
- **Authors**: Yuansheng Hua, Lichao Mou, Pu Jin, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Aerial scene recognition is a fundamental research problem in interpreting high-resolution aerial imagery. Over the past few years, most studies focus on classifying an image into one scene category, while in real-world scenarios, it is more often that a single image contains multiple scenes. Therefore, in this paper, we investigate a more practical yet underexplored task -- multi-scene recognition in single images. To this end, we create a large-scale dataset, called MultiScene, composed of 100,000 unconstrained high-resolution aerial images. Considering that manually labeling such images is extremely arduous, we resort to low-cost annotations from crowdsourcing platforms, e.g., OpenStreetMap (OSM). However, OSM data might suffer from incompleteness and incorrectness, which introduce noise into image labels. To address this issue, we visually inspect 14,000 images and correct their scene labels, yielding a subset of cleanly-annotated images, named MultiScene-Clean. With it, we can develop and evaluate deep networks for multi-scene recognition using clean data. Moreover, we provide crowdsourced annotations of all images for the purpose of studying network learning with noisy labels. We conduct experiments with extensive baseline models on both MultiScene-Clean and MultiScene to offer benchmarks for multi-scene recognition in single images and learning from noisy labels for this task, respectively. To facilitate progress, we make our dataset and trained models available on https://gitlab.lrz.de/ai4eo/reasoning/multiscene.



### Deep Implicit Statistical Shape Models for 3D Medical Image Delineation
- **Arxiv ID**: http://arxiv.org/abs/2104.02847v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.02847v2)
- **Published**: 2021-04-07 01:15:06+00:00
- **Updated**: 2022-01-04 08:36:47+00:00
- **Authors**: Ashwin Raju, Shun Miao, Dakai Jin, Le Lu, Junzhou Huang, Adam P. Harrison
- **Comment**: None
- **Journal**: None
- **Summary**: 3D delineation of anatomical structures is a cardinal goal in medical imaging analysis. Prior to deep learning, statistical shape models that imposed anatomical constraints and produced high quality surfaces were a core technology. Prior to deep learning, statistical shape models that imposed anatomical constraints and produced high quality surfaces were a core technology. Today fully-convolutional networks (FCNs), while dominant, do not offer these capabilities. We present deep implicit statistical shape models (DISSMs), a new approach to delineation that marries the representation power of convolutional neural networks (CNNs) with the robustness of SSMs. DISSMs use a deep implicit surface representation to produce a compact and descriptive shape latent space that permits statistical models of anatomical variance. To reliably fit anatomically plausible shapes to an image, we introduce a novel rigid and non-rigid pose estimation pipeline that is modelled as a Markov decision process(MDP). We outline a training regime that includes inverted episodic training and a deep realization of marginal space learning (MSL). Intra-dataset experiments on the task of pathological liver segmentation demonstrate that DISSMs can perform more robustly than three leading FCN models, including nnU-Net: reducing the mean Hausdorff distance (HD) by 7.7-14.3mm and improving the worst case Dice-Sorensen coefficient (DSC) by 1.2-2.3%. More critically, cross-dataset experiments on a dataset directly reflecting clinical deployment scenarios demonstrate that DISSMs improve the mean DSC and HD by 3.5-5.9% and 12.3-24.5mm, respectively, and the worst-case DSC by 5.4-7.3%. These improvements are over and above any benefits from representing delineations with high-quality surface.



### LI-Net: Large-Pose Identity-Preserving Face Reenactment Network
- **Arxiv ID**: http://arxiv.org/abs/2104.02850v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.02850v1)
- **Published**: 2021-04-07 01:41:21+00:00
- **Updated**: 2021-04-07 01:41:21+00:00
- **Authors**: Jin Liu, Peng Chen, Tao Liang, Zhaoxing Li, Cai Yu, Shuqiao Zou, Jiao Dai, Jizhong Han
- **Comment**: IEEE International Conference on Multimedia and Expo(ICME) 2021 Oral
- **Journal**: None
- **Summary**: Face reenactment is a challenging task, as it is difficult to maintain accurate expression, pose and identity simultaneously. Most existing methods directly apply driving facial landmarks to reenact source faces and ignore the intrinsic gap between two identities, resulting in the identity mismatch issue. Besides, they neglect the entanglement of expression and pose features when encoding driving faces, leading to inaccurate expressions and visual artifacts on large-pose reenacted faces. To address these problems, we propose a Large-pose Identity-preserving face reenactment network, LI-Net. Specifically, the Landmark Transformer is adopted to adjust driving landmark images, which aims to narrow the identity gap between driving and source landmark images. Then the Face Rotation Module and the Expression Enhancing Generator decouple the transformed landmark image into pose and expression features, and reenact those attributes separately to generate identity-preserving faces with accurate expressions and poses. Both qualitative and quantitative experimental results demonstrate the superiority of our method.



### Soft-Label Anonymous Gastric X-ray Image Distillation
- **Arxiv ID**: http://arxiv.org/abs/2104.02857v1
- **DOI**: 10.1109/ICIP40778.2020.9191357
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.02857v1)
- **Published**: 2021-04-07 02:04:12+00:00
- **Updated**: 2021-04-07 02:04:12+00:00
- **Authors**: Guang Li, Ren Togo, Takahiro Ogawa, Miki Haseyama
- **Comment**: Published as a conference paper at ICIP 2020
- **Journal**: None
- **Summary**: This paper presents a soft-label anonymous gastric X-ray image distillation method based on a gradient descent approach. The sharing of medical data is demanded to construct high-accuracy computer-aided diagnosis (CAD) systems. However, the large size of the medical dataset and privacy protection are remaining problems in medical data sharing, which hindered the research of CAD systems. The idea of our distillation method is to extract the valid information of the medical dataset and generate a tiny distilled dataset that has a different data distribution. Different from model distillation, our method aims to find the optimal distilled images, distilled labels and the optimized learning rate. Experimental results show that the proposed method can not only effectively compress the medical dataset but also anonymize medical images to protect the patient's private information. The proposed approach can improve the efficiency and security of medical data sharing.



### Farewell to Mutual Information: Variational Distillation for Cross-Modal Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2104.02862v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.02862v2)
- **Published**: 2021-04-07 02:19:41+00:00
- **Updated**: 2022-12-25 08:08:39+00:00
- **Authors**: Xudong Tian, Zhizhong Zhang, Shaohui Lin, Yanyun Qu, Yuan Xie, Lizhuang Ma
- **Comment**: Accepted by CVPR 2022 as Oral presentation
- **Journal**: None
- **Summary**: The Information Bottleneck (IB) provides an information theoretic principle for representation learning, by retaining all information relevant for predicting label while minimizing the redundancy. Though IB principle has been applied to a wide range of applications, its optimization remains a challenging problem which heavily relies on the accurate estimation of mutual information. In this paper, we present a new strategy, Variational Self-Distillation (VSD), which provides a scalable, flexible and analytic solution to essentially fitting the mutual information but without explicitly estimating it. Under rigorously theoretical guarantee, VSD enables the IB to grasp the intrinsic correlation between representation and label for supervised training. Furthermore, by extending VSD to multi-view learning, we introduce two other strategies, Variational Cross-Distillation (VCD) and Variational Mutual-Learning (VML), which significantly improve the robustness of representation to view-changes by eliminating view-specific and task-irrelevant information. To verify our theoretically grounded strategies, we apply our approaches to cross-modal person Re-ID, and conduct extensive experiments, where the superior performance against state-of-the-art methods are demonstrated. Our intriguing findings highlight the need to rethink the way to estimate mutual



### Self-Supervised Learning for Gastritis Detection with Gastric X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2104.02864v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.02864v4)
- **Published**: 2021-04-07 02:32:06+00:00
- **Updated**: 2023-03-27 08:46:45+00:00
- **Authors**: Guang Li, Ren Togo, Takahiro Ogawa, Miki Haseyama
- **Comment**: Published as a journal paper at Springer IJCARS
- **Journal**: None
- **Summary**: Purpose: Manual annotation of gastric X-ray images by doctors for gastritis detection is time-consuming and expensive. To solve this, a self-supervised learning method is developed in this study. The effectiveness of the proposed self-supervised learning method in gastritis detection is verified using a few annotated gastric X-ray images. Methods: In this study, we develop a novel method that can perform explicit self-supervised learning and learn discriminative representations from gastric X-ray images. Models trained based on the proposed method were fine-tuned on datasets comprising a few annotated gastric X-ray images. Five self-supervised learning methods, i.e., SimSiam, BYOL, PIRL-jigsaw, PIRL-rotation, and SimCLR, were compared with the proposed method. Furthermore, three previous methods, one pretrained on ImageNet, one trained from scratch, and one semi-supervised learning method, were compared with the proposed method. Results: The proposed method's harmonic mean score of sensitivity and specificity after fine-tuning with the annotated data of 10, 20, 30, and 40 patients were 0.875, 0.911, 0.915, and 0.931, respectively. The proposed method outperformed all comparative methods, including the five self-supervised learning and three previous methods. Experimental results showed the effectiveness of the proposed method in gastritis detection using a few annotated gastric X-ray images. Conclusions: This paper proposes a novel self-supervised learning method based on a teacher-student architecture for gastritis detection using gastric X-ray images. The proposed method can perform explicit self-supervised learning and learn discriminative representations from gastric X-ray images. The proposed method exhibits potential clinical use in gastritis detection using a few annotated gastric X-ray images.



### Deep Transformers for Fast Small Intestine Grounding in Capsule Endoscope Video
- **Arxiv ID**: http://arxiv.org/abs/2104.02866v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.02866v1)
- **Published**: 2021-04-07 02:35:18+00:00
- **Updated**: 2021-04-07 02:35:18+00:00
- **Authors**: Xinkai Zhao, Chaowei Fang, Feng Gao, De-Jun Fan, Xutao Lin, Guanbin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Capsule endoscopy is an evolutional technique for examining and diagnosing intractable gastrointestinal diseases. Because of the huge amount of data, analyzing capsule endoscope videos is very time-consuming and labor-intensive for gastrointestinal medicalists. The development of intelligent long video analysis algorithms for regional positioning and analysis of capsule endoscopic video is therefore essential to reduce the workload of clinicians and assist in improving the accuracy of disease diagnosis. In this paper, we propose a deep model to ground shooting range of small intestine from a capsule endoscope video which has duration of tens of hours. This is the first attempt to attack the small intestine grounding task using deep neural network method. We model the task as a 3-way classification problem, in which every video frame is categorized into esophagus/stomach, small intestine or colorectum. To explore long-range temporal dependency, a transformer module is built to fuse features of multiple neighboring frames. Based on the classification model, we devise an efficient search algorithm to efficiently locate the starting and ending shooting boundaries of the small intestine. Without searching the small intestine exhaustively in the full video, our method is implemented via iteratively separating the video segment along the direction to the target boundary in the middle. We collect 113 videos from a local hospital to validate our method. In the 5-fold cross validation, the average IoU between the small intestine segments located by our method and the ground-truths annotated by broad-certificated gastroenterologists reaches 0.945.



### Affordance Transfer Learning for Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.02867v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.02867v2)
- **Published**: 2021-04-07 02:37:04+00:00
- **Updated**: 2021-06-09 06:02:11+00:00
- **Authors**: Zhi Hou, Baosheng Yu, Yu Qiao, Xiaojiang Peng, Dacheng Tao
- **Comment**: Accepted to CVPR2021; add a new but important ablated experiment in
  appendix(union box verb representation);
- **Journal**: None
- **Summary**: Reasoning the human-object interactions (HOI) is essential for deeper scene understanding, while object affordances (or functionalities) are of great importance for human to discover unseen HOIs with novel objects. Inspired by this, we introduce an affordance transfer learning approach to jointly detect HOIs with novel objects and recognize affordances. Specifically, HOI representations can be decoupled into a combination of affordance and object representations, making it possible to compose novel interactions by combining affordance representations and novel object representations from additional images, i.e. transferring the affordance to novel objects. With the proposed affordance transfer learning, the model is also capable of inferring the affordances of novel objects from known affordance representations. The proposed method can thus be used to 1) improve the performance of HOI detection, especially for the HOIs with unseen objects; and 2) infer the affordances of novel objects. Experimental results on two datasets, HICO-DET and HOI-COCO (from V-COCO), demonstrate significant improvements over recent state-of-the-art methods for HOI detection and object affordance detection. Code is available at https://github.com/zhihou7/HOI-CL



### Information Bottleneck Attribution for Visual Explanations of Diagnosis and Prognosis
- **Arxiv ID**: http://arxiv.org/abs/2104.02869v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.02869v2)
- **Published**: 2021-04-07 02:43:52+00:00
- **Updated**: 2021-06-23 01:13:39+00:00
- **Authors**: Ugur Demir, Ismail Irmakci, Elif Keles, Ahmet Topcu, Ziyue Xu, Concetto Spampinato, Sachin Jambawalikar, Evrim Turkbey, Baris Turkbey, Ulas Bagci
- **Comment**: None
- **Journal**: None
- **Summary**: Visual explanation methods have an important role in the prognosis of the patients where the annotated data is limited or unavailable. There have been several attempts to use gradient-based attribution methods to localize pathology from medical scans without using segmentation labels. This research direction has been impeded by the lack of robustness and reliability. These methods are highly sensitive to the network parameters. In this study, we introduce a robust visual explanation method to address this problem for medical applications. We provide an innovative visual explanation algorithm for general purpose and as an example application, we demonstrate its effectiveness for quantifying lesions in the lungs caused by the Covid-19 with high accuracy and robustness without using dense segmentation labels. This approach overcomes the drawbacks of commonly used Grad-CAM and its extended versions. The premise behind our proposed strategy is that the information flow is minimized while ensuring the classifier prediction stays similar. Our findings indicate that the bottleneck condition provides a more stable severity estimation than the similar attribution methods.



### Speckles-Training-Based Denoising Convolutional Neural Network Ghost Imaging
- **Arxiv ID**: http://arxiv.org/abs/2104.02873v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.02873v1)
- **Published**: 2021-04-07 02:56:57+00:00
- **Updated**: 2021-04-07 02:56:57+00:00
- **Authors**: Yuchen He, Sihong Duan, Jianxing Li, Hui Chen, Huaibin Zheng, Jianbin Liu, Shitao Zhu, Zhuo Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Ghost imaging (GI) has been paid attention gradually because of its lens-less imaging capability, turbulence-free imaging and high detection sensitivity. However, low image quality and slow imaging speed restrict the application process of GI. In this paper, we propose a improved GI method based on Denoising Convolutional Neural Networks (DnCNN). Inspired by the corresponding between input (noisy image) and output (residual image) in DnCNN, we construct the mapping between speckles sequence and the corresponding noise distribution in GI through training. Then, the same speckles sequence is employed to illuminate unknown targets, and a de-noising target image will be obtained. The proposed method can be regarded as a general method for GI. Under two sampling rates, extensive experiments are carried out to compare with traditional GI method (basic correlation and compressed sensing) and DnCNN method on three data sets. Moreover, we set up a physical GI experiment system to verify the proposed method. The results show that the proposed method achieves promising performance.



### Document Layout Analysis via Dynamic Residual Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/2104.02874v1
- **DOI**: 10.1109/ICME51207.2021.9428465
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.02874v1)
- **Published**: 2021-04-07 02:57:09+00:00
- **Updated**: 2021-04-07 02:57:09+00:00
- **Authors**: Xingjiao Wu, Ziling Hu, Xiangcheng Du, Jing Yang, Liang He
- **Comment**: 7 pages, 6 figures
- **Journal**: IEEE ICME 2021 ORAL
- **Summary**: The document layout analysis (DLA) aims to split the document image into different interest regions and understand the role of each region, which has wide application such as optical character recognition (OCR) systems and document retrieval. However, it is a challenge to build a DLA system because the training data is very limited and lacks an efficient model. In this paper, we propose an end-to-end united network named Dynamic Residual Fusion Network (DRFN) for the DLA task. Specifically, we design a dynamic residual feature fusion module which can fully utilize low-dimensional information and maintain high-dimensional category information. Besides, to deal with the model overfitting problem that is caused by lacking enough data, we propose the dynamic select mechanism for efficient fine-tuning in limited train data. We experiment with two challenging datasets and demonstrate the effectiveness of the proposed module.



### Facial Attribute Transformers for Precise and Robust Makeup Transfer
- **Arxiv ID**: http://arxiv.org/abs/2104.02894v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.02894v1)
- **Published**: 2021-04-07 03:39:02+00:00
- **Updated**: 2021-04-07 03:39:02+00:00
- **Authors**: Zhaoyi Wan, Haoran Chen, Jielei Zhang, Wentao Jiang, Cong Yao, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of makeup transfer, which aims at transplanting the makeup from the reference face to the source face while preserving the identity of the source. Existing makeup transfer methods have made notable progress in generating realistic makeup faces, but do not perform well in terms of color fidelity and spatial transformation. To tackle these issues, we propose a novel Facial Attribute Transformer (FAT) and its variant Spatial FAT for high-quality makeup transfer. Drawing inspirations from the Transformer in NLP, FAT is able to model the semantic correspondences and interactions between the source face and reference face, and then precisely estimate and transfer the facial attributes. To further facilitate shape deformation and transformation of facial parts, we also integrate thin plate splines (TPS) into FAT, thus creating Spatial FAT, which is the first method that can transfer geometric attributes in addition to color and texture. Extensive qualitative and quantitative experiments demonstrate the effectiveness and superiority of our proposed FATs in the following aspects: (1) ensuring high-fidelity color transfer; (2) allowing for geometric transformation of facial parts; (3) handling facial variations (such as poses and shadows) and (4) supporting high-resolution face generation.



### PyNET-CA: Enhanced PyNET with Channel Attention for End-to-End Mobile Image Signal Processing
- **Arxiv ID**: http://arxiv.org/abs/2104.02895v1
- **DOI**: 10.1007/978-3-030-67070-2_12
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.02895v1)
- **Published**: 2021-04-07 03:40:11+00:00
- **Updated**: 2021-04-07 03:40:11+00:00
- **Authors**: Byung-Hoon Kim, Joonyoung Song, Jong Chul Ye, JaeHyun Baek
- **Comment**: ECCV 2020 AIM workshop accepted version
- **Journal**: None
- **Summary**: Reconstructing RGB image from RAW data obtained with a mobile device is related to a number of image signal processing (ISP) tasks, such as demosaicing, denoising, etc. Deep neural networks have shown promising results over hand-crafted ISP algorithms on solving these tasks separately, or even replacing the whole reconstruction process with one model. Here, we propose PyNET-CA, an end-to-end mobile ISP deep learning algorithm for RAW to RGB reconstruction. The model enhances PyNET, a recently proposed state-of-the-art model for mobile ISP, and improve its performance with channel attention and subpixel reconstruction module. We demonstrate the performance of the proposed method with comparative experiments and results from the AIM 2020 learned smartphone ISP challenge. The source code of our implementation is available at https://github.com/egyptdj/skyb-aim2020-public



### Multimodal Object Detection via Probabilistic Ensembling
- **Arxiv ID**: http://arxiv.org/abs/2104.02904v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.02904v3)
- **Published**: 2021-04-07 04:03:20+00:00
- **Updated**: 2022-07-26 01:45:21+00:00
- **Authors**: Yi-Ting Chen, Jinghao Shi, Zelin Ye, Christoph Mertz, Deva Ramanan, Shu Kong
- **Comment**: camera-ready with supplement for ECCV2022 (oral presentation);
  open-source code at https://github.com/Jamie725/RGBT-detection
- **Journal**: None
- **Summary**: Object detection with multimodal inputs can improve many safety-critical systems such as autonomous vehicles (AVs). Motivated by AVs that operate in both day and night, we study multimodal object detection with RGB and thermal cameras, since the latter provides much stronger object signatures under poor illumination. We explore strategies for fusing information from different modalities. Our key contribution is a probabilistic ensembling technique, ProbEn, a simple non-learned method that fuses together detections from multi-modalities. We derive ProbEn from Bayes' rule and first principles that assume conditional independence across modalities. Through probabilistic marginalization, ProbEn elegantly handles missing modalities when detectors do not fire on the same object. Importantly, ProbEn also notably improves multimodal detection even when the conditional independence assumption does not hold, e.g., fusing outputs from other fusion methods (both off-the-shelf and trained in-house). We validate ProbEn on two benchmarks containing both aligned (KAIST) and unaligned (FLIR) multimodal images, showing that ProbEn outperforms prior work by more than 13% in relative performance!



### Unsupervised Visual Attention and Invariance for Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.02921v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.02921v2)
- **Published**: 2021-04-07 05:28:01+00:00
- **Updated**: 2021-04-16 18:56:48+00:00
- **Authors**: Xudong Wang, Long Lian, Stella X. Yu
- **Comment**: Accepted at CVPR 2021
- **Journal**: None
- **Summary**: Vision-based reinforcement learning (RL) is successful, but how to generalize it to unknown test environments remains challenging. Existing methods focus on training an RL policy that is universal to changing visual domains, whereas we focus on extracting visual foreground that is universal, feeding clean invariant vision to the RL policy learner. Our method is completely unsupervised, without manual annotations or access to environment internals.   Given videos of actions in a training environment, we learn how to extract foregrounds with unsupervised keypoint detection, followed by unsupervised visual attention to automatically generate a foreground mask per video frame. We can then introduce artificial distractors and train a model to reconstruct the clean foreground mask from noisy observations. Only this learned model is needed during test to provide distraction-free visual input to the RL policy learner.   Our Visual Attention and Invariance (VAI) method significantly outperforms the state-of-the-art on visual domain generalization, gaining 15 to 49% (61 to 229%) more cumulative rewards per episode on DeepMind Control (our DrawerWorld Manipulation) benchmarks. Our results demonstrate that it is not only possible to learn domain-invariant vision without any supervision, but freeing RL from visual distractions also makes the policy more focused and thus far better.



### Sparse Oblique Decision Trees: A Tool to Understand and Manipulate Neural Net Features
- **Arxiv ID**: http://arxiv.org/abs/2104.02922v2
- **DOI**: 10.1007/s10618-022-00892-7
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.02922v2)
- **Published**: 2021-04-07 05:31:08+00:00
- **Updated**: 2023-01-30 07:49:30+00:00
- **Authors**: Suryabhan Singh Hada, Miguel Á. Carreira-Perpiñán, Arman Zharmagambetov
- **Comment**: Appears in Data Mining and Knowledge Discovery (2023), Special Issue
  on Explainable and Interpretable Machine Learning and Data Mining
- **Journal**: None
- **Summary**: The widespread deployment of deep nets in practical applications has lead to a growing desire to understand how and why such black-box methods perform prediction. Much work has focused on understanding what part of the input pattern (an image, say) is responsible for a particular class being predicted, and how the input may be manipulated to predict a different class. We focus instead on understanding which of the internal features computed by the neural net are responsible for a particular class. We achieve this by mimicking part of the neural net with an oblique decision tree having sparse weight vectors at the decision nodes. Using the recently proposed Tree Alternating Optimization (TAO) algorithm, we are able to learn trees that are both highly accurate and interpretable. Such trees can faithfully mimic the part of the neural net they replaced, and hence they can provide insights into the deep net black box. Further, we show we can easily manipulate the neural net features in order to make the net predict, or not predict, a given class, thus showing that it is possible to carry out adversarial attacks at the level of the features. These insights and manipulations apply globally to the entire training and test set, not just at a local (single-instance) level. We demonstrate this robustly in the MNIST and ImageNet datasets with LeNet5 and VGG networks.



### Pretrained equivariant features improve unsupervised landmark discovery
- **Arxiv ID**: http://arxiv.org/abs/2104.02925v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.02925v1)
- **Published**: 2021-04-07 05:42:11+00:00
- **Updated**: 2021-04-07 05:42:11+00:00
- **Authors**: Rahul Rahaman, Atin Ghosh, Alexandre H. Thiery
- **Comment**: None
- **Journal**: None
- **Summary**: Locating semantically meaningful landmark points is a crucial component of a large number of computer vision pipelines. Because of the small number of available datasets with ground truth landmark annotations, it is important to design robust unsupervised and semi-supervised methods for landmark detection.   Many of the recent unsupervised learning methods rely on the equivariance properties of landmarks to synthetic image deformations. Our work focuses on such widely used methods and sheds light on its core problem, its inability to produce equivariant intermediate convolutional features. This finding leads us to formulate a two-step unsupervised approach that overcomes this challenge by first learning powerful pixel-based features and then use the pre-trained features to learn a landmark detector by the traditional equivariance method. Our method produces state-of-the-art results in several challenging landmark detection datasets such as the BBC Pose dataset and the Cat-Head dataset. It performs comparably on a range of other benchmarks.



### OpenGAN: Open-Set Recognition via Open Data Generation
- **Arxiv ID**: http://arxiv.org/abs/2104.02939v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.02939v3)
- **Published**: 2021-04-07 06:19:24+00:00
- **Updated**: 2021-10-13 05:23:31+00:00
- **Authors**: Shu Kong, Deva Ramanan
- **Comment**: ICCV 2021 Best Paper Honorable Mention
- **Journal**: None
- **Summary**: Real-world machine learning systems need to analyze test data that may differ from training data. In K-way classification, this is crisply formulated as open-set recognition, core to which is the ability to discriminate open-set data outside the K closed-set classes. Two conceptually elegant ideas for open-set discrimination are: 1) discriminatively learning an open-vs-closed binary discriminator by exploiting some outlier data as the open-set, and 2) unsupervised learning the closed-set data distribution with a GAN, using its discriminator as the open-set likelihood function. However, the former generalizes poorly to diverse open test data due to overfitting to the training outliers, which are unlikely to exhaustively span the open-world. The latter does not work well, presumably due to the instable training of GANs. Motivated by the above, we propose OpenGAN, which addresses the limitation of each approach by combining them with several technical insights. First, we show that a carefully selected GAN-discriminator on some real outlier data already achieves the state-of-the-art. Second, we augment the available set of real open training examples with adversarially synthesized "fake" data. Third and most importantly, we build the discriminator over the features computed by the closed-world K-way networks. This allows OpenGAN to be implemented via a lightweight discriminator head built on top of an existing K-way network. Extensive experiments show that OpenGAN significantly outperforms prior open-set methods.



### The art of defense: letting networks fool the attacker
- **Arxiv ID**: http://arxiv.org/abs/2104.02963v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.02963v3)
- **Published**: 2021-04-07 07:28:46+00:00
- **Updated**: 2022-06-06 07:14:32+00:00
- **Authors**: Jinlai Zhang, Yinpeng Dong, Binbin Liu, Bo Ouyang, Jihong Zhu, Minchi Kuang, Houqing Wang, Yanmei Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Robust environment perception is critical for autonomous cars, and adversarial defenses are the most effective and widely studied ways to improve the robustness of environment perception. However, all of previous defense methods decrease the natural accuracy, and the nature of the DNNs itself has been overlooked. To this end, in this paper, we propose a novel adversarial defense for 3D point cloud classifier that makes full use of the nature of the DNNs. Due to the disorder of point cloud, all point cloud classifiers have the property of permutation invariant to the input point cloud. Based on this nature, we design invariant transformations defense (IT-Defense). We show that, even after accounting for obfuscated gradients, our IT-Defense is a resilient defense against state-of-the-art (SOTA) 3D attacks. Moreover, IT-Defense do not hurt clean accuracy compared to previous SOTA 3D defenses. Our code is available at: {\footnotesize{\url{https://github.com/cuge1995/IT-Defense}}}.



### Contrastive Learning of Global-Local Video Representations
- **Arxiv ID**: http://arxiv.org/abs/2104.05418v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05418v2)
- **Published**: 2021-04-07 07:35:08+00:00
- **Updated**: 2021-10-27 21:30:05+00:00
- **Authors**: Shuang Ma, Zhaoyang Zeng, Daniel McDuff, Yale Song
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive learning has delivered impressive results for various tasks in the self-supervised regime. However, existing approaches optimize for learning representations specific to downstream scenarios, i.e., \textit{global} representations suitable for tasks such as classification or \textit{local} representations for tasks such as detection and localization. While they produce satisfactory results in the intended downstream scenarios, they often fail to generalize to tasks that they were not originally designed for. In this work, we propose to learn video representations that generalize to both the tasks which require global semantic information (e.g., classification) and the tasks that require local fine-grained spatio-temporal information (e.g., localization). We achieve this by optimizing two contrastive objectives that together encourage our model to learn global-local visual information given audio signals. We show that the two objectives mutually improve the generalizability of the learned global-local representations, significantly outperforming their disjointly learned counterparts. We demonstrate our approach on various tasks including action/sound classification, lip reading, deepfake detection, event and sound localization (https://github.com/yunyikristy/global\_local).



### ACM-Net: Action Context Modeling Network for Weakly-Supervised Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2104.02967v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.02967v1)
- **Published**: 2021-04-07 07:39:57+00:00
- **Updated**: 2021-04-07 07:39:57+00:00
- **Authors**: Sanqing Qu, Guang Chen, Zhijun Li, Lijun Zhang, Fan Lu, Alois Knoll
- **Comment**: Submitted to TIP. Code is available at
  https://github.com/ispc-lab/ACM-Net
- **Journal**: None
- **Summary**: Weakly-supervised temporal action localization aims to localize action instances temporal boundary and identify the corresponding action category with only video-level labels. Traditional methods mainly focus on foreground and background frames separation with only a single attention branch and class activation sequence. However, we argue that apart from the distinctive foreground and background frames there are plenty of semantically ambiguous action context frames. It does not make sense to group those context frames to the same background class since they are semantically related to a specific action category. Consequently, it is challenging to suppress action context frames with only a single class activation sequence. To address this issue, in this paper, we propose an action-context modeling network termed ACM-Net, which integrates a three-branch attention module to measure the likelihood of each temporal point being action instance, context, or non-action background, simultaneously. Then based on the obtained three-branch attention values, we construct three-branch class activation sequences to represent the action instances, contexts, and non-action backgrounds, individually. To evaluate the effectiveness of our ACM-Net, we conduct extensive experiments on two benchmark datasets, THUMOS-14 and ActivityNet-1.3. The experiments show that our method can outperform current state-of-the-art methods, and even achieve comparable performance with fully-supervised methods. Code can be found at https://github.com/ispc-lab/ACM-Net



### MPN: Multimodal Parallel Network for Audio-Visual Event Localization
- **Arxiv ID**: http://arxiv.org/abs/2104.02971v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.02971v1)
- **Published**: 2021-04-07 07:44:22+00:00
- **Updated**: 2021-04-07 07:44:22+00:00
- **Authors**: Jiashuo Yu, Ying Cheng, Rui Feng
- **Comment**: IEEE International Conference on Multimedia and Expo (ICME) 2021 Oral
- **Journal**: None
- **Summary**: Audio-visual event localization aims to localize an event that is both audible and visible in the wild, which is a widespread audio-visual scene analysis task for unconstrained videos. To address this task, we propose a Multimodal Parallel Network (MPN), which can perceive global semantics and unmixed local information parallelly. Specifically, our MPN framework consists of a classification subnetwork to predict event categories and a localization subnetwork to predict event boundaries. The classification subnetwork is constructed by the Multimodal Co-attention Module (MCM) and obtains global contexts. The localization subnetwork consists of Multimodal Bottleneck Attention Module (MBAM), which is designed to extract fine-grained segment-level contents. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance both in fully supervised and weakly supervised settings on the Audio-Visual Event (AVE) dataset.



### Self-supervised Learning of Depth Inference for Multi-view Stereo
- **Arxiv ID**: http://arxiv.org/abs/2104.02972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.02972v1)
- **Published**: 2021-04-07 07:45:02+00:00
- **Updated**: 2021-04-07 07:45:02+00:00
- **Authors**: Jiayu Yang, Jose M. Alvarez, Miaomiao Liu
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Recent supervised multi-view depth estimation networks have achieved promising results. Similar to all supervised approaches, these networks require ground-truth data during training. However, collecting a large amount of multi-view depth data is very challenging. Here, we propose a self-supervised learning framework for multi-view stereo that exploit pseudo labels from the input data. We start by learning to estimate depth maps as initial pseudo labels under an unsupervised learning framework relying on image reconstruction loss as supervision. We then refine the initial pseudo labels using a carefully designed pipeline leveraging depth information inferred from higher resolution images and neighboring views. We use these high-quality pseudo labels as the supervision signal to train the network and improve, iteratively, its performance by self-training. Extensive experiments on the DTU dataset show that our proposed self-supervised learning framework outperforms existing unsupervised multi-view stereo networks by a large margin and performs on par compared to the supervised counterpart. Code is available at https://github.com/JiayuYANG/Self-supervised-CVP-MVSNet.



### Active learning using weakly supervised signals for quality inspection
- **Arxiv ID**: http://arxiv.org/abs/2104.02973v1
- **DOI**: 10.1117/12.2586595
- **Categories**: **cs.CV**, cs.AI, cs.LO, stat.ML, I.2.10; I.4.6; I.4.8; I.4.9; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2104.02973v1)
- **Published**: 2021-04-07 07:49:07+00:00
- **Updated**: 2021-04-07 07:49:07+00:00
- **Authors**: Antoine Cordier, Deepan Das, Pierre Gutierrez
- **Comment**: 8 pages, 3 Figures, QCAV 2021 conference (proceedings published in
  SPIE)
- **Journal**: None
- **Summary**: Because manufacturing processes evolve fast, and since production visual aspect can vary significantly on a daily basis, the ability to rapidly update machine vision based inspection systems is paramount. Unfortunately, supervised learning of convolutional neural networks requires a significant amount of annotated images for being able to learn effectively from new data. Acknowledging the abundance of continuously generated images coming from the production line and the cost of their annotation, we demonstrate it is possible to prioritize and accelerate the annotation process. In this work, we develop a methodology for learning actively, from rapidly mined, weakly (i.e. partially) annotated data, enabling a fast, direct feedback from the operators on the production line and tackling a big machine vision weakness: false positives. We also consider the problem of covariate shift, which arises inevitably due to changing conditions during data acquisition. In that regard, we show domain-adversarial training to be an efficient way to address this issue.



### Synthetic training data generation for deep learning based quality inspection
- **Arxiv ID**: http://arxiv.org/abs/2104.02980v1
- **DOI**: 10.1117/12.2586824
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2.10; I.3.3; I.4.1; I.4.6; I.4.8; I.4.9; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2104.02980v1)
- **Published**: 2021-04-07 08:07:57+00:00
- **Updated**: 2021-04-07 08:07:57+00:00
- **Authors**: Pierre Gutierrez, Maria Luschkova, Antoine Cordier, Mustafa Shukor, Mona Schappert, Tim Dahmen
- **Comment**: 8 pages, 4 figures, to be published in QCAV 2021 conference,
  proceedings will by published by SPIE
- **Journal**: None
- **Summary**: Deep learning is now the gold standard in computer vision-based quality inspection systems. In order to detect defects, supervised learning is often utilized, but necessitates a large amount of annotated images, which can be costly: collecting, cleaning, and annotating the data is tedious and limits the speed at which a system can be deployed as everything the system must detect needs to be observed first. This can impede the inspection of rare defects, since very few samples can be collected by the manufacturer. In this work, we focus on simulations to solve this issue. We first present a generic simulation pipeline to render images of defective or healthy (non defective) parts. As metallic parts can be highly textured with small defects like holes, we design a texture scanning and generation method. We assess the quality of the generated images by training deep learning networks and by testing them on real data from a manufacturer. We demonstrate that we can achieve encouraging results on real defect detection using purely simulated data. Additionally, we are able to improve global performances by concatenating simulated and real data, showing that simulations can complement real images to boost performances. Lastly, using domain adaptation techniques helps improving slightly our final results.



### [RE] CNN-generated images are surprisingly easy to spot...for now
- **Arxiv ID**: http://arxiv.org/abs/2104.02984v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.02984v1)
- **Published**: 2021-04-07 08:26:35+00:00
- **Updated**: 2021-04-07 08:26:35+00:00
- **Authors**: Joel Frank, Thorsten Holz
- **Comment**: Code available: https://github.com/Joool/ReproducabilityCNNEasyToSpot
- **Journal**: None
- **Summary**: This work evaluates the reproducibility of the paper "CNN-generated images are surprisingly easy to spot... for now" by Wang et al. published at CVPR 2020. The paper addresses the challenge of detecting CNN-generated imagery, which has reached the potential to even fool humans. The authors propose two methods which help an image classifier to generalize from being trained on one specific CNN to detecting imagery produced by unseen architectures, training methods, or data sets. The paper proposes two methods to help a classifier generalize: (i) utilizing different kinds of data augmentations and (ii) using a diverse data set. This report focuses on assessing if these techniques indeed help the generalization process. Furthermore, we perform additional experiments to study the limitations of the proposed techniques.



### Universal Adversarial Training with Class-Wise Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2104.03000v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.03000v1)
- **Published**: 2021-04-07 09:05:49+00:00
- **Updated**: 2021-04-07 09:05:49+00:00
- **Authors**: Philipp Benz, Chaoning Zhang, Adil Karjauv, In So Kweon
- **Comment**: Accepted to ICME 2021
- **Journal**: None
- **Summary**: Despite their overwhelming success on a wide range of applications, convolutional neural networks (CNNs) are widely recognized to be vulnerable to adversarial examples. This intriguing phenomenon led to a competition between adversarial attacks and defense techniques. So far, adversarial training is the most widely used method for defending against adversarial attacks. It has also been extended to defend against universal adversarial perturbations (UAPs). The SOTA universal adversarial training (UAT) method optimizes a single perturbation for all training samples in the mini-batch. In this work, we find that a UAP does not attack all classes equally. Inspired by this observation, we identify it as the source of the model having unbalanced robustness. To this end, we improve the SOTA UAT by proposing to utilize class-wise UAPs during adversarial training. On multiple benchmark datasets, our class-wise UAT leads superior performance for both clean accuracy and adversarial robustness against universal attack.



### CNN Based Segmentation of Infarcted Regions in Acute Cerebral Stroke Patients From Computed Tomography Perfusion Imaging
- **Arxiv ID**: http://arxiv.org/abs/2104.03002v2
- **DOI**: 10.1145/3388440.3412470
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.2; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2104.03002v2)
- **Published**: 2021-04-07 09:09:13+00:00
- **Updated**: 2021-04-21 14:25:34+00:00
- **Authors**: Luca Tomasetti, Kjersti Engan, Mahdieh Khanmohammadi, Kathinka Dæhli Kurz
- **Comment**: None
- **Journal**: None
- **Summary**: More than 13 million people suffer from ischemic cerebral stroke worldwide each year. Thrombolytic treatment can reduce brain damage but has a narrow treatment window. Computed Tomography Perfusion imaging is a commonly used primary assessment tool for stroke patients, and typically the radiologists will evaluate resulting parametric maps to estimate the affected areas, dead tissue (core), and the surrounding tissue at risk (penumbra), to decide further treatments. Different work has been reported, suggesting thresholds, and semi-automated methods, and in later years deep neural networks, for segmenting infarction areas based on the parametric maps. However, there is no consensus in terms of which thresholds to use, or how to combine the information from the parametric maps, and the presented methods all have limitations in terms of both accuracy and reproducibility.   We propose a fully automated convolutional neural network based segmentation method that uses the full four-dimensional computed tomography perfusion dataset as input, rather than the pre-filtered parametric maps. The suggested network is tested on an available dataset as a proof-of-concept, with very encouraging results. Cross-validated results show averaged Dice score of 0.78 and 0.53, and an area under the receiver operating characteristic curve of 0.97 and 0.94 for penumbra and core respectively



### FedFace: Collaborative Learning of Face Recognition Model
- **Arxiv ID**: http://arxiv.org/abs/2104.03008v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03008v2)
- **Published**: 2021-04-07 09:25:32+00:00
- **Updated**: 2021-06-24 13:34:14+00:00
- **Authors**: Divyansh Aggarwal, Jiayu Zhou, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: DNN-based face recognition models require large centrally aggregated face datasets for training. However, due to the growing data privacy concerns and legal restrictions, accessing and sharing face datasets has become exceedingly difficult. We propose FedFace, a federated learning (FL) framework for collaborative learning of face recognition models in a privacy-aware manner. FedFace utilizes the face images available on multiple clients to learn an accurate and generalizable face recognition model where the face images stored at each client are neither shared with other clients nor the central host and each client is a mobile device containing face images pertaining to only the owner of the device (one identity per client). Our experiments show the effectiveness of FedFace in enhancing the verification performance of pre-trained face recognition system on standard face verification benchmarks namely LFW, IJB-A, and IJB-C.



### RTIC: Residual Learning for Text and Image Composition using Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2104.03015v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03015v3)
- **Published**: 2021-04-07 09:41:52+00:00
- **Updated**: 2021-10-26 01:58:02+00:00
- **Authors**: Minchul Shin, Yoonjae Cho, Byungsoo Ko, Geonmo Gu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the compositional learning of images and texts for image retrieval. The query is given in the form of an image and text that describes the desired modifications to the image; the goal is to retrieve the target image that satisfies the given modifications and resembles the query by composing information in both the text and image modalities. To remedy this, we propose a novel architecture designed for the image-text composition task and show that the proposed structure can effectively encode the differences between the source and target images conditioned on the text. Furthermore, we introduce a new joint training technique based on the graph convolutional network that is generally applicable for any existing composition methods in a plug-and-play manner. We found that the proposed technique consistently improves performance and achieves state-of-the-art scores on various benchmarks. To avoid misleading experimental results caused by trivial training hyper-parameters, we reproduce all individual baselines and train models with a unified training environment. We expect this approach to suppress undesirable effects from irrelevant components and emphasize the image-text composition module's ability. Also, we achieve the state-of-the-art score without restricting the training environment, which implies the superiority of our method considering the gains from hyper-parameter tuning. The code, including all the baseline methods, are released https://github.com/nashory/rtic-gcn-pytorch.



### Graph-based Normalizing Flow for Human Motion Generation and Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2104.03020v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.03020v1)
- **Published**: 2021-04-07 09:51:15+00:00
- **Updated**: 2021-04-07 09:51:15+00:00
- **Authors**: Wenjie Yin, Hang Yin, Danica Kragic, Mårten Björkman
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Data-driven approaches for modeling human skeletal motion have found various applications in interactive media and social robotics. Challenges remain in these fields for generating high-fidelity samples and robustly reconstructing motion from imperfect input data, due to e.g. missed marker detection. In this paper, we propose a probabilistic generative model to synthesize and reconstruct long horizon motion sequences conditioned on past information and control signals, such as the path along which an individual is moving. Our method adapts the existing work MoGlow by introducing a new graph-based model. The model leverages the spatial-temporal graph convolutional network (ST-GCN) to effectively capture the spatial structure and temporal correlation of skeletal motion data at multiple scales. We evaluate the models on a mixture of motion capture datasets of human locomotion with foot-step and bone-length analysis. The results demonstrate the advantages of our model in reconstructing missing markers and achieving comparable results on generating realistic future poses. When the inputs are imperfect, our model shows improvements on robustness of generation.



### Multimodal Continuous Visual Attention Mechanisms
- **Arxiv ID**: http://arxiv.org/abs/2104.03046v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.03046v1)
- **Published**: 2021-04-07 10:47:51+00:00
- **Updated**: 2021-04-07 10:47:51+00:00
- **Authors**: António Farinhas, André F. T. Martins, Pedro M. Q. Aguiar
- **Comment**: None
- **Journal**: None
- **Summary**: Visual attention mechanisms are a key component of neural network models for computer vision. By focusing on a discrete set of objects or image regions, these mechanisms identify the most relevant features and use them to build more powerful representations. Recently, continuous-domain alternatives to discrete attention models have been proposed, which exploit the continuity of images. These approaches model attention as simple unimodal densities (e.g. a Gaussian), making them less suitable to deal with images whose region of interest has a complex shape or is composed of multiple non-contiguous patches. In this paper, we introduce a new continuous attention mechanism that produces multimodal densities, in the form of mixtures of Gaussians. We use the EM algorithm to obtain a clustering of relevant regions in the image, and a description length penalty to select the number of components in the mixture. Our densities decompose as a linear combination of unimodal attention mechanisms, enabling closed-form Jacobians for the backpropagation step. Experiments on visual question answering in the VQA-v2 dataset show competitive accuracies and a selection of regions that mimics human attention more closely in VQA-HAT. We present several examples that suggest how multimodal attention maps are naturally more interpretable than their unimodal counterparts, showing the ability of our model to automatically segregate objects from ground in complex scenes.



### Few-Shot Incremental Learning with Continually Evolved Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2104.03047v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.03047v1)
- **Published**: 2021-04-07 10:54:51+00:00
- **Updated**: 2021-04-07 10:54:51+00:00
- **Authors**: Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, Yinghui Xu
- **Comment**: Accpeted to CVPR2021
- **Journal**: None
- **Summary**: Few-shot class-incremental learning (FSCIL) aims to design machine learning algorithms that can continually learn new concepts from a few data points, without forgetting knowledge of old classes. The difficulty lies in that limited data from new classes not only lead to significant overfitting issues but also exacerbate the notorious catastrophic forgetting problems. Moreover, as training data come in sequence in FSCIL, the learned classifier can only provide discriminative information in individual sessions, while FSCIL requires all classes to be involved for evaluation. In this paper, we address the FSCIL problem from two aspects. First, we adopt a simple but effective decoupled learning strategy of representations and classifiers that only the classifiers are updated in each incremental session, which avoids knowledge forgetting in the representations. By doing so, we demonstrate that a pre-trained backbone plus a non-parametric class mean classifier can beat state-of-the-art methods. Second, to make the classifiers learned on individual sessions applicable to all classes, we propose a Continually Evolved Classifier (CEC) that employs a graph model to propagate context information between classifiers for adaptation. To enable the learning of CEC, we design a pseudo incremental learning paradigm that episodically constructs a pseudo incremental learning task to optimize the graph parameters by sampling data from the base dataset. Experiments on three popular benchmark datasets, including CIFAR100, miniImageNet, and Caltech-USCD Birds-200-2011 (CUB200), show that our method significantly outperforms the baselines and sets new state-of-the-art results with remarkable advantages.



### Artificial and beneficial -- Exploiting artificial images for aerial vehicle detection
- **Arxiv ID**: http://arxiv.org/abs/2104.03054v1
- **DOI**: 10.1016/j.isprsjprs.2021.02.015
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03054v1)
- **Published**: 2021-04-07 11:06:15+00:00
- **Updated**: 2021-04-07 11:06:15+00:00
- **Authors**: Immanuel Weber, Jens Bongartz, Ribana Roscher
- **Comment**: 14 pages, 13 figures, 4 tables
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, Volume 175,
  May 2021, Pages 158-170
- **Summary**: Object detection in aerial images is an important task in environmental, economic, and infrastructure-related tasks. One of the most prominent applications is the detection of vehicles, for which deep learning approaches are increasingly used. A major challenge in such approaches is the limited amount of data that arises, for example, when more specialized and rarer vehicles such as agricultural machinery or construction vehicles are to be detected. This lack of data contrasts with the enormous data hunger of deep learning methods in general and object recognition in particular. In this article, we address this issue in the context of the detection of road vehicles in aerial images. To overcome the lack of annotated data, we propose a generative approach that generates top-down images by overlaying artificial vehicles created from 2D CAD drawings on artificial or real backgrounds. Our experiments with a modified RetinaNet object detection network show that adding these images to small real-world datasets significantly improves detection performance. In cases of very limited or even no real-world images, we observe an improvement in average precision of up to 0.70 points. We address the remaining performance gap to real-world datasets by analyzing the effect of the image composition of background and objects and give insights into the importance of background.



### Differentiable Patch Selection for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.03059v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.03059v1)
- **Published**: 2021-04-07 11:15:51+00:00
- **Updated**: 2021-04-07 11:15:51+00:00
- **Authors**: Jean-Baptiste Cordonnier, Aravindh Mahendran, Alexey Dosovitskiy, Dirk Weissenborn, Jakob Uszkoreit, Thomas Unterthiner
- **Comment**: Accepted to IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2021. Code available at
  https://github.com/google-research/google-research/tree/master/ptopk_patch_selection/
- **Journal**: None
- **Summary**: Neural Networks require large amounts of memory and compute to process high resolution images, even when only a small part of the image is actually informative for the task at hand. We propose a method based on a differentiable Top-K operator to select the most relevant parts of the input to efficiently process high resolution images. Our method may be interfaced with any downstream neural network, is able to aggregate information from different patches in a flexible way, and allows the whole model to be trained end-to-end using backpropagation. We show results for traffic sign recognition, inter-patch relationship reasoning, and fine-grained recognition without using object/part bounding box annotations during training.



### Everything's Talkin': Pareidolia Face Reenactment
- **Arxiv ID**: http://arxiv.org/abs/2104.03061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03061v1)
- **Published**: 2021-04-07 11:19:13+00:00
- **Updated**: 2021-04-07 11:19:13+00:00
- **Authors**: Linsen Song, Wayne Wu, Chaoyou Fu, Chen Qian, Chen Change Loy, Ran He
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: We present a new application direction named Pareidolia Face Reenactment, which is defined as animating a static illusory face to move in tandem with a human face in the video. For the large differences between pareidolia face reenactment and traditional human face reenactment, two main challenges are introduced, i.e., shape variance and texture variance. In this work, we propose a novel Parametric Unsupervised Reenactment Algorithm to tackle these two challenges. Specifically, we propose to decompose the reenactment into three catenate processes: shape modeling, motion transfer and texture synthesis. With the decomposition, we introduce three crucial components, i.e., Parametric Shape Modeling, Expansionary Motion Transfer and Unsupervised Texture Synthesizer, to overcome the problems brought by the remarkably variances on pareidolia faces. Extensive experiments show the superior performance of our method both qualitatively and quantitatively. Code, model and data are available on our project page.



### Where and What? Examining Interpretable Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/2104.05622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05622v1)
- **Published**: 2021-04-07 11:22:02+00:00
- **Updated**: 2021-04-07 11:22:02+00:00
- **Authors**: Xinqi Zhu, Chang Xu, Dacheng Tao
- **Comment**: CVPR2021
- **Journal**: None
- **Summary**: Capturing interpretable variations has long been one of the goals in disentanglement learning. However, unlike the independence assumption, interpretability has rarely been exploited to encourage disentanglement in the unsupervised setting. In this paper, we examine the interpretability of disentangled representations by investigating two questions: where to be interpreted and what to be interpreted? A latent code is easily to be interpreted if it would consistently impact a certain subarea of the resulting generated image. We thus propose to learn a spatial mask to localize the effect of each individual latent dimension. On the other hand, interpretability usually comes from latent dimensions that capture simple and basic variations in data. We thus impose a perturbation on a certain dimension of the latent code, and expect to identify the perturbation along this dimension from the generated images so that the encoding of simple variations can be enforced. Additionally, we develop an unsupervised model selection method, which accumulates perceptual distance scores along axes in the latent space. On various datasets, our models can learn high-quality disentangled representations without supervision, showing the proposed modeling of interpretability is an effective proxy for achieving unsupervised disentanglement.



### DG-Font: Deformable Generative Networks for Unsupervised Font Generation
- **Arxiv ID**: http://arxiv.org/abs/2104.03064v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03064v2)
- **Published**: 2021-04-07 11:32:32+00:00
- **Updated**: 2021-04-08 14:19:26+00:00
- **Authors**: Yangchen Xie, Xinyuan Chen, Li Sun, Yue Lu
- **Comment**: Accepted by CVPR-2021
- **Journal**: None
- **Summary**: Font generation is a challenging problem especially for some writing systems that consist of a large number of characters and has attracted a lot of attention in recent years. However, existing methods for font generation are often in supervised learning. They require a large number of paired data, which is labor-intensive and expensive to collect. Besides, common image-to-image translation models often define style as the set of textures and colors, which cannot be directly applied to font generation. To address these problems, we propose novel deformable generative networks for unsupervised font generation (DGFont). We introduce a feature deformation skip connection (FDSC) which predicts pairs of displacement maps and employs the predicted maps to apply deformable convolution to the low-level feature maps from the content encoder. The outputs of FDSC are fed into a mixer to generate the final results. Taking advantage of FDSC, the mixer outputs a high-quality character with a complete structure. To further improve the quality of generated images, we use three deformable convolution layers in the content encoder to learn style-invariant feature representations. Experiments demonstrate that our model generates characters in higher quality than state-of-art methods. The source code is available at https://github.com/ecnuycxie/DG-Font.



### Distributional Robustness Loss for Long-tail Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.03066v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.03066v2)
- **Published**: 2021-04-07 11:34:04+00:00
- **Updated**: 2021-10-30 22:31:51+00:00
- **Authors**: Dvir Samuel, Gal Chechik
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Real-world data is often unbalanced and long-tailed, but deep models struggle to recognize rare classes in the presence of frequent classes. To address unbalanced data, most studies try balancing the data, the loss, or the classifier to reduce classification bias towards head classes. Far less attention has been given to the latent representations learned with unbalanced data. We show that the feature extractor part of deep networks suffers greatly from this bias. We propose a new loss based on robustness theory, which encourages the model to learn high-quality representations for both head and tail classes. While the general form of the robustness loss may be hard to compute, we further derive an easy-to-compute upper bound that can be minimized efficiently. This procedure reduces representation bias towards head classes in the feature space and achieves new SOTA results on CIFAR100-LT, ImageNet-LT, and iNaturalist long-tail benchmarks. We find that training with robustness increases recognition accuracy of tail classes while largely maintaining the accuracy of head classes. The new robustness loss can be combined with various classifier balancing techniques and can be applied to representations at several layers of the deep model.



### Analysis Towards Classification of Infection and Ischaemia of Diabetic Foot Ulcers
- **Arxiv ID**: http://arxiv.org/abs/2104.03068v2
- **DOI**: 10.1109/BHI50953.2021.9508563
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03068v2)
- **Published**: 2021-04-07 11:38:57+00:00
- **Updated**: 2021-06-21 06:49:30+00:00
- **Authors**: Moi Hoon Yap, Bill Cassidy, Joseph M. Pappachan, Claire O'Shea, David Gillespie, Neil Reeves
- **Comment**: 4 pages, 6 figures and 3 tables
- **Journal**: Conference: 2021 IEEE EMBS International Conference on Biomedical
  and Health Informatics (BHI)
- **Summary**: This paper introduces the Diabetic Foot Ulcers dataset (DFUC2021) for analysis of pathology, focusing on infection and ischaemia. We describe the data preparation of DFUC2021 for ground truth annotation, data curation and data analysis. The final release of DFUC2021 consists of 15,683 DFU patches, with 5,955 training, 5,734 for testing and 3,994 unlabeled DFU patches. The ground truth labels are four classes, i.e. control, infection, ischaemia and both conditions. We curate the dataset using image hashing techniques and analyse the separability using UMAP projection. We benchmark the performance of five key backbones of deep learning, i.e. VGG16, ResNet101, InceptionV3, DenseNet121 and EfficientNet on DFUC2021. We report the optimised results of these key backbones with different strategies. Based on our observations, we conclude that EfficientNetB0 with data augmentation and transfer learning provided the best results for multi-class (4-class) classification with macro-average Precision, Recall and F1-score of 0.57, 0.62 and 0.55, respectively. In ischaemia and infection recognition, when trained on one-versus-all, EfficientNetB0 achieved comparable results with the state of the art. Finally, we interpret the results with statistical analysis and Grad-CAM visualisation.



### Universal and Flexible Optical Aberration Correction Using Deep-Prior Based Deconvolution
- **Arxiv ID**: http://arxiv.org/abs/2104.03078v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.03078v2)
- **Published**: 2021-04-07 12:00:38+00:00
- **Updated**: 2021-08-19 03:48:48+00:00
- **Authors**: Xiu Li, Jinli Suo, Weihang Zhang, Xin Yuan, Qionghai Dai
- **Comment**: ICCV2021
- **Journal**: None
- **Summary**: High quality imaging usually requires bulky and expensive lenses to compensate geometric and chromatic aberrations. This poses high constraints on the optical hash or low cost applications. Although one can utilize algorithmic reconstruction to remove the artifacts of low-end lenses, the degeneration from optical aberrations is spatially varying and the computation has to trade off efficiency for performance. For example, we need to conduct patch-wise optimization or train a large set of local deep neural networks to achieve high reconstruction performance across the whole image. In this paper, we propose a PSF aware plug-and-play deep network, which takes the aberrant image and PSF map as input and produces the latent high quality version via incorporating lens-specific deep priors, thus leading to a universal and flexible optical aberration correction method. Specifically, we pre-train a base model from a set of diverse lenses and then adapt it to a given lens by quickly refining the parameters, which largely alleviates the time and memory consumption of model learning. The approach is of high efficiency in both training and testing stages. Extensive results verify the promising applications of our proposed approach for compact low-end cameras.



### LIFE: Lighting Invariant Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2104.03097v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03097v2)
- **Published**: 2021-04-07 12:43:40+00:00
- **Updated**: 2021-04-19 09:13:32+00:00
- **Authors**: Zhaoyang Huang, Xiaokun Pan, Runsen Xu, Yan Xu, Ka chun Cheung, Guofeng Zhang, Hongsheng Li
- **Comment**: Project page: https://drinkingcoder.github.io/publication/life/
- **Journal**: None
- **Summary**: We tackle the problem of estimating flow between two images with large lighting variations. Recent learning-based flow estimation frameworks have shown remarkable performance on image pairs with small displacement and constant illuminations, but cannot work well on cases with large viewpoint change and lighting variations because of the lack of pixel-wise flow annotations for such cases. We observe that via the Structure-from-Motion (SfM) techniques, one can easily estimate relative camera poses between image pairs with large viewpoint change and lighting variations. We propose a novel weakly supervised framework LIFE to train a neural network for estimating accurate lighting-invariant flows between image pairs. Sparse correspondences are conventionally established via feature matching with descriptors encoding local image contents. However, local image contents are inevitably ambiguous and error-prone during the cross-image feature matching process, which hinders downstream tasks. We propose to guide feature matching with the flows predicted by LIFE, which addresses the ambiguous matching by utilizing abundant context information in the image pairs. We show that LIFE outperforms previous flow learning frameworks by large margins in challenging scenarios, consistently improves feature matching, and benefits downstream tasks.



### HIH: Towards More Accurate Face Alignment via Heatmap in Heatmap
- **Arxiv ID**: http://arxiv.org/abs/2104.03100v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03100v2)
- **Published**: 2021-04-07 12:53:37+00:00
- **Updated**: 2022-04-18 10:12:31+00:00
- **Authors**: Xing Lan, Qinghao Hu, Qiang Chen, Jian Xue, Jian Cheng
- **Comment**: A large update(v2) for HIH
- **Journal**: None
- **Summary**: Heatmap-based regression overcomes the lack of spatial and contextual information of direct coordinate regression, and has revolutionized the task of face alignment. Yet it suffers from quantization errors caused by neglecting subpixel coordinates in image resizing and network downsampling. In this paper, we first quantitatively analyze the quantization error on benchmarks, which accounts for more than 1/3 of the whole prediction errors for state-of-the-art methods. To tackle this problem, we propose a novel Heatmap In Heatmap(HIH) representation and a coordinate soft-classification (CSC) method, which are seamlessly integrated into the classic hourglass network. The HIH representation utilizes nested heatmaps to jointly represent the coordinate label: one heatmap called integer heatmap stands for the integer coordinate, and the other heatmap named decimal heatmap represents the subpixel coordinate. The range of a decimal heatmap makes up one pixel in the corresponding integer heatmap. Besides, we transfer the offset regression problem to an interval classification task, and CSC regards the confidence of the pixel as the probability of the interval. Meanwhile, CSC applying the distribution loss leverage the soft labels generated from the Gaussian distribution function to guide the offset heatmap training, which makes it easier to learn the distribution of coordinate offsets. Extensive experiments on challenging benchmark datasets demonstrate that our HIH can achieve state-of-the-art results. In particular, our HIH reaches 4.08 NME (Normalized Mean Error) on WFLW, and 3.21 on COFW, which exceeds previous methods by a significant margin.



### V2F-Net: Explicit Decomposition of Occluded Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.03106v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03106v1)
- **Published**: 2021-04-07 13:12:16+00:00
- **Updated**: 2021-04-07 13:12:16+00:00
- **Authors**: Mingyang Shang, Dawei Xiang, Zhicheng Wang, Erjin Zhou
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Occlusion is very challenging in pedestrian detection. In this paper, we propose a simple yet effective method named V2F-Net, which explicitly decomposes occluded pedestrian detection into visible region detection and full body estimation. V2F-Net consists of two sub-networks: Visible region Detection Network (VDN) and Full body Estimation Network (FEN). VDN tries to localize visible regions and FEN estimates full-body box on the basis of the visible box. Moreover, to further improve the estimation of full body, we propose a novel Embedding-based Part-aware Module (EPM). By supervising the visibility for each part, the network is encouraged to extract features with essential part information. We experimentally show the effectiveness of V2F-Net by conducting several experiments on two challenging datasets. V2F-Net achieves 5.85% AP gains on CrowdHuman and 2.24% MR-2 improvements on CityPersons compared to FPN baseline. Besides, the consistent gain on both one-stage and two-stage detector validates the generalizability of our method.



### VGF-Net: Visual-Geometric Fusion Learning for Simultaneous Drone Navigation and Height Mapping
- **Arxiv ID**: http://arxiv.org/abs/2104.03109v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03109v1)
- **Published**: 2021-04-07 13:18:40+00:00
- **Updated**: 2021-04-07 13:18:40+00:00
- **Authors**: Yilin Liu, Ke Xie, Hui Huang
- **Comment**: Accepted by CVM 2021
- **Journal**: Graphical Models 2021
- **Summary**: The drone navigation requires the comprehensive understanding of both visual and geometric information in the 3D world. In this paper, we present a Visual-Geometric Fusion Network(VGF-Net), a deep network for the fusion analysis of visual/geometric data and the construction of 2.5D height maps for simultaneous drone navigation in novel environments. Given an initial rough height map and a sequence of RGB images, our VGF-Net extracts the visual information of the scene, along with a sparse set of 3D keypoints that capture the geometric relationship between objects in the scene. Driven by the data, VGF-Net adaptively fuses visual and geometric information, forming a unified Visual-Geometric Representation. This representation is fed to a new Directional Attention Model(DAM), which helps enhance the visual-geometric object relationship and propagates the informative data to dynamically refine the height map and the corresponding keypoints. An entire end-to-end information fusion and mapping system is formed, demonstrating remarkable robustness and high accuracy on the autonomous drone navigation across complex indoor and large-scale outdoor scenes. The dataset can be found in http://vcc.szu.edu.cn/research/2021/VGFNet.



### Neural Articulated Radiance Field
- **Arxiv ID**: http://arxiv.org/abs/2104.03110v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03110v2)
- **Published**: 2021-04-07 13:23:14+00:00
- **Updated**: 2021-08-18 14:33:05+00:00
- **Authors**: Atsuhiro Noguchi, Xiao Sun, Stephen Lin, Tatsuya Harada
- **Comment**: 16 pages, ICCV 2021
- **Journal**: None
- **Summary**: We present Neural Articulated Radiance Field (NARF), a novel deformable 3D representation for articulated objects learned from images. While recent advances in 3D implicit representation have made it possible to learn models of complex objects, learning pose-controllable representations of articulated objects remains a challenge, as current methods require 3D shape supervision and are unable to render appearance. In formulating an implicit representation of 3D articulated objects, our method considers only the rigid transformation of the most relevant object part in solving for the radiance field at each 3D location. In this way, the proposed method represents pose-dependent changes without significantly increasing the computational complexity. NARF is fully differentiable and can be trained from images with pose annotations. Moreover, through the use of an autoencoder, it can learn appearance variations over multiple instances of an object class. Experiments show that the proposed method is efficient and can generalize well to novel poses. The code is available for research purposes at https://github.com/nogu-atsu/NARF



### Learning Residue-Aware Correlation Filters and Refining Scale Estimates with the GrabCut for Real-Time UAV Tracking
- **Arxiv ID**: http://arxiv.org/abs/2104.03114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03114v1)
- **Published**: 2021-04-07 13:35:01+00:00
- **Updated**: 2021-04-07 13:35:01+00:00
- **Authors**: Shuiwang Li, Yuting Liu, Qijun Zhao, Ziliang Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Unmanned aerial vehicle (UAV)-based tracking is attracting increasing attention and developing rapidly in applications such as agriculture, aviation, navigation, transportation and public security. Recently, discriminative correlation filters (DCF)-based trackers have stood out in UAV tracking community for their high efficiency and appealing robustness on a single CPU. However, due to limited onboard computation resources and other challenges the efficiency and accuracy of existing DCF-based approaches is still not satisfying. In this paper, we explore using segmentation by the GrabCut to improve the wildly adopted discriminative scale estimation in DCF-based trackers, which, as a mater of fact, greatly impacts the precision and accuracy of the trackers since accumulated scale error degrades the appearance model as online updating goes on. Meanwhile, inspired by residue representation, we exploit the residue nature inherent to videos and propose residue-aware correlation filters that show better convergence properties in filter learning. Extensive experiments are conducted on four UAV benchmarks, namely, UAV123@10fps, DTB70, UAVDT and Vistrone2018 (VisDrone2018-test-dev). The results show that our method achieves state-of-the-art performance.



### Single Source One Shot Reenactment using Weighted motion From Paired Feature Points
- **Arxiv ID**: http://arxiv.org/abs/2104.03117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03117v1)
- **Published**: 2021-04-07 13:45:34+00:00
- **Updated**: 2021-04-07 13:45:34+00:00
- **Authors**: Soumya Tripathy, Juho Kannala, Esa Rahtu
- **Comment**: None
- **Journal**: None
- **Summary**: Image reenactment is a task where the target object in the source image imitates the motion represented in the driving image. One of the most common reenactment tasks is face image animation. The major challenge in the current face reenactment approaches is to distinguish between facial motion and identity. For this reason, the previous models struggle to produce high-quality animations if the driving and source identities are different (cross-person reenactment). We propose a new (face) reenactment model that learns shape-independent motion features in a self-supervised setup. The motion is represented using a set of paired feature points extracted from the source and driving images simultaneously. The model is generalised to multiple reenactment tasks including faces and non-face objects using only a single source image. The extensive experiments show that the model faithfully transfers the driving motion to the source while retaining the source identity intact.



### Dense Dilated UNet: Deep Learning for 3D Photoacoustic Tomography Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2104.03130v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.03130v1)
- **Published**: 2021-04-07 14:01:48+00:00
- **Updated**: 2021-04-07 14:01:48+00:00
- **Authors**: Steven Guan, Ko-Tsung Hsu, Matthias Eyassu, Parag V. Chitnis
- **Comment**: None
- **Journal**: None
- **Summary**: In photoacoustic tomography (PAT), the acoustic pressure waves produced by optical excitation are measured by an array of detectors and used to reconstruct an image. Sparse spatial sampling and limited-view detection are two common challenges faced in PAT. Reconstructing from incomplete data using standard methods results in severe streaking artifacts and blurring. We propose a modified convolutional neural network (CNN) architecture termed Dense Dilation UNet (DD-UNet) for correcting artifacts in 3D PAT. The DD-Net leverages the benefits of dense connectivity and dilated convolutions to improve CNN performance. We compare the proposed CNN in terms of image quality as measured by the multiscale structural similarity index metric to the Fully Dense UNet (FD-UNet). Results demonstrate that the DD-Net consistently outperforms the FD-UNet and is able to more reliably reconstruct smaller image features.



### Image Composition Assessment with Saliency-augmented Multi-pattern Pooling
- **Arxiv ID**: http://arxiv.org/abs/2104.03133v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03133v2)
- **Published**: 2021-04-07 14:07:17+00:00
- **Updated**: 2021-10-18 02:09:40+00:00
- **Authors**: Bo Zhang, Li Niu, Liqing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Image composition assessment is crucial in aesthetic assessment, which aims to assess the overall composition quality of a given image. However, to the best of our knowledge, there is neither dataset nor method specifically proposed for this task. In this paper, we contribute the first composition assessment dataset CADB with composition scores for each image provided by multiple professional raters. Besides, we propose a composition assessment network SAMP-Net with a novel Saliency-Augmented Multi-pattern Pooling (SAMP) module, which analyses visual layout from the perspectives of multiple composition patterns. We also leverage composition-relevant attributes to further boost the performance, and extend Earth Mover's Distance (EMD) loss to weighted EMD loss to eliminate the content bias. The experimental results show that our SAMP-Net can perform more favorably than previous aesthetic assessment approaches.



### Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.03135v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03135v2)
- **Published**: 2021-04-07 14:07:20+00:00
- **Updated**: 2021-04-08 01:03:43+00:00
- **Authors**: Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, Jianlong Fu
- **Comment**: Accepted by CVPR2021 oral
- **Journal**: None
- **Summary**: We study joint learning of Convolutional Neural Network (CNN) and Transformer for vision-language pre-training (VLPT) which aims to learn cross-modal alignments from millions of image-text pairs. State-of-the-art approaches extract salient image regions and align regions with words step-by-step. As region-based visual features usually represent parts of an image, it is challenging for existing vision-language models to fully understand the semantics from paired natural languages. In this paper, we propose SOHO to "See Out of tHe bOx" that takes a whole image as input, and learns vision-language representation in an end-to-end manner. SOHO does not require bounding box annotations which enables inference 10 times faster than region-based approaches. In particular, SOHO learns to extract comprehensive yet compact image features through a visual dictionary (VD) that facilitates cross-modal understanding. VD is designed to represent consistent visual abstractions of similar semantics. It is updated on-the-fly and utilized in our proposed pre-training task Masked Visual Modeling (MVM). We conduct experiments on four well-established vision-language tasks by following standard VLPT settings. In particular, SOHO achieves absolute gains of 2.0% R@1 score on MSCOCO text retrieval 5k test split, 1.5% accuracy on NLVR$^2$ test-P split, 6.7% accuracy on SNLI-VE test split, respectively.



### Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2104.03149v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.03149v3)
- **Published**: 2021-04-07 14:28:22+00:00
- **Updated**: 2021-09-01 09:11:58+00:00
- **Authors**: Corentin Dancette, Remi Cadene, Damien Teney, Matthieu Cord
- **Comment**: Accepted at ICCV 2021. Code is available at
  https://github.com/cdancette/detect-shortcuts
- **Journal**: None
- **Summary**: We introduce an evaluation methodology for visual question answering (VQA) to better diagnose cases of shortcut learning. These cases happen when a model exploits spurious statistical regularities to produce correct answers but does not actually deploy the desired behavior. There is a need to identify possible shortcuts in a dataset and assess their use before deploying a model in the real world. The research community in VQA has focused exclusively on question-based shortcuts, where a model might, for example, answer "What is the color of the sky" with "blue" by relying mostly on the question-conditional training prior and give little weight to visual evidence. We go a step further and consider multimodal shortcuts that involve both questions and images. We first identify potential shortcuts in the popular VQA v2 training set by mining trivial predictive rules such as co-occurrences of words and visual elements. We then introduce VQA-CounterExamples (VQA-CE), an evaluation protocol based on our subset of CounterExamples i.e. image-question-answer triplets where our rules lead to incorrect answers. We use this new evaluation in a large-scale study of existing approaches for VQA. We demonstrate that even state-of-the-art models perform poorly and that existing techniques to reduce biases are largely ineffective in this context. Our findings suggest that past work on question-based biases in VQA has only addressed one facet of a complex issue. The code for our method is available at https://github.com/cdancette/detect-shortcuts.



### Distilling and Transferring Knowledge via cGAN-generated Samples for Image Classification and Regression
- **Arxiv ID**: http://arxiv.org/abs/2104.03164v5
- **DOI**: 10.1016/j.eswa.2022.119060
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.03164v5)
- **Published**: 2021-04-07 14:52:49+00:00
- **Updated**: 2022-12-27 04:53:36+00:00
- **Authors**: Xin Ding, Yongwei Wang, Zuheng Xu, Z. Jane Wang, William J. Welch
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge distillation (KD) has been actively studied for image classification tasks in deep learning, aiming to improve the performance of a student based on the knowledge from a teacher. However, applying KD in image regression with a scalar response variable has been rarely studied, and there exists no KD method applicable to both classification and regression tasks yet. Moreover, existing KD methods often require a practitioner to carefully select or adjust the teacher and student architectures, making these methods less flexible in practice. To address the above problems in a unified way, we propose a comprehensive KD framework based on cGANs, termed cGAN-KD. Fundamentally different from existing KD methods, cGAN-KD distills and transfers knowledge from a teacher model to a student model via cGAN-generated samples. This novel mechanism makes cGAN-KD suitable for both classification and regression tasks, compatible with other KD methods, and insensitive to the teacher and student architectures. An error bound for a student model trained in the cGAN-KD framework is derived in this work, providing a theory for why cGAN-KD is effective as well as guiding the practical implementation of cGAN-KD. Extensive experiments on CIFAR-100 and ImageNet-100 show that we can combine state of the art KD methods with the cGAN-KD framework to yield a new state of the art. Moreover, experiments on Steering Angle and UTKFace demonstrate the effectiveness of cGAN-KD in image regression tasks, where existing KD methods are inapplicable.



### On Self-Contact and Human Pose
- **Arxiv ID**: http://arxiv.org/abs/2104.03176v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03176v2)
- **Published**: 2021-04-07 15:10:38+00:00
- **Updated**: 2021-04-08 07:29:50+00:00
- **Authors**: Lea Müller, Ahmed A. A. Osman, Siyu Tang, Chun-Hao P. Huang, Michael J. Black
- **Comment**: Accepted in CVPR'21 (oral). Project page: https://tuch.is.tue.mpg.de/
- **Journal**: None
- **Summary**: People touch their face 23 times an hour, they cross their arms and legs, put their hands on their hips, etc. While many images of people contain some form of self-contact, current 3D human pose and shape (HPS) regression methods typically fail to estimate this contact. To address this, we develop new datasets and methods that significantly improve human pose estimation with self-contact. First, we create a dataset of 3D Contact Poses (3DCP) containing SMPL-X bodies fit to 3D scans as well as poses from AMASS, which we refine to ensure good contact. Second, we leverage this to create the Mimic-The-Pose (MTP) dataset of images, collected via Amazon Mechanical Turk, containing people mimicking the 3DCP poses with selfcontact. Third, we develop a novel HPS optimization method, SMPLify-XMC, that includes contact constraints and uses the known 3DCP body pose during fitting to create near ground-truth poses for MTP images. Fourth, for more image variety, we label a dataset of in-the-wild images with Discrete Self-Contact (DSC) information and use another new optimization method, SMPLify-DC, that exploits discrete contacts during pose optimization. Finally, we use our datasets during SPIN training to learn a new 3D human pose regressor, called TUCH (Towards Understanding Contact in Humans). We show that the new self-contact training data significantly improves 3D human pose estimates on withheld test data and existing datasets like 3DPW. Not only does our method improve results for self-contact poses, but it also improves accuracy for non-contact poses. The code and data are available for research purposes at https://tuch.is.tue.mpg.de.



### The SARAS Endoscopic Surgeon Action Detection (ESAD) dataset: Challenges and methods
- **Arxiv ID**: http://arxiv.org/abs/2104.03178v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03178v1)
- **Published**: 2021-04-07 15:11:51+00:00
- **Updated**: 2021-04-07 15:11:51+00:00
- **Authors**: Vivek Singh Bawa, Gurkirt Singh, Francis KapingA, Inna Skarga-Bandurova, Elettra Oleari, Alice Leporini, Carmela Landolfo, Pengfei Zhao, Xi Xiang, Gongning Luo, Kuanquan Wang, Liangzhi Li, Bowen Wang, Shang Zhao, Li Li, Armando Stabile, Francesco Setti, Riccardo Muradore, Fabio Cuzzolin
- **Comment**: None
- **Journal**: None
- **Summary**: For an autonomous robotic system, monitoring surgeon actions and assisting the main surgeon during a procedure can be very challenging. The challenges come from the peculiar structure of the surgical scene, the greater similarity in appearance of actions performed via tools in a cavity compared to, say, human actions in unconstrained environments, as well as from the motion of the endoscopic camera. This paper presents ESAD, the first large-scale dataset designed to tackle the problem of surgeon action detection in endoscopic minimally invasive surgery. ESAD aims at contributing to increase the effectiveness and reliability of surgical assistant robots by realistically testing their awareness of the actions performed by a surgeon. The dataset provides bounding box annotation for 21 action classes on real endoscopic video frames captured during prostatectomy, and was used as the basis of a recent MIDL 2020 challenge. We also present an analysis of the dataset conducted using the baseline model which was released as part of the challenge, and a description of the top performing models submitted to the challenge together with the results they obtained. This study provides significant insight into what approaches can be effective and can be extended further. We believe that ESAD will serve in the future as a useful benchmark for all researchers active in surgeon action detection and assistive robotics at large.



### The Use of Video Captioning for Fostering Physical Activity
- **Arxiv ID**: http://arxiv.org/abs/2104.03207v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.03207v1)
- **Published**: 2021-04-07 15:52:48+00:00
- **Updated**: 2021-04-07 15:52:48+00:00
- **Authors**: Soheyla Amirian, Abolfazl Farahani, Hamid R. Arabnia, Khaled Rasheed, Thiab R. Taha
- **Comment**: None
- **Journal**: None
- **Summary**: Video Captioning is considered to be one of the most challenging problems in the field of computer vision. Video Captioning involves the combination of different deep learning models to perform object detection, action detection, and localization by processing a sequence of image frames. It is crucial to consider the sequence of actions in a video in order to generate a meaningful description of the overall action event. A reliable, accurate, and real-time video captioning method can be used in many applications. However, this paper focuses on one application: video captioning for fostering and facilitating physical activities. In broad terms, the work can be considered to be assistive technology. Lack of physical activity appears to be increasingly widespread in many nations due to many factors, the most important being the convenience that technology has provided in workplaces. The adopted sedentary lifestyle is becoming a significant public health issue. Therefore, it is essential to incorporate more physical movements into our daily lives. Tracking one's daily physical activities would offer a base for comparison with activities performed in subsequent days. With the above in mind, this paper proposes a video captioning framework that aims to describe the activities in a video and estimate a person's daily physical activity level. This framework could potentially help people trace their daily movements to reduce an inactive lifestyle's health risks. The work presented in this paper is still in its infancy. The initial steps of the application are outlined in this paper. Based on our preliminary research, this project has great merit.



### Self-Supervised Learning for Semi-Supervised Temporal Action Proposal
- **Arxiv ID**: http://arxiv.org/abs/2104.03214v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03214v1)
- **Published**: 2021-04-07 16:03:25+00:00
- **Updated**: 2021-04-07 16:03:25+00:00
- **Authors**: Xiang Wang, Shiwei Zhang, Zhiwu Qing, Yuanjie Shao, Changxin Gao, Nong Sang
- **Comment**: Accepted by CVPR-2021
- **Journal**: None
- **Summary**: Self-supervised learning presents a remarkable performance to utilize unlabeled data for various video tasks. In this paper, we focus on applying the power of self-supervised methods to improve semi-supervised action proposal generation. Particularly, we design an effective Self-supervised Semi-supervised Temporal Action Proposal (SSTAP) framework. The SSTAP contains two crucial branches, i.e., temporal-aware semi-supervised branch and relation-aware self-supervised branch. The semi-supervised branch improves the proposal model by introducing two temporal perturbations, i.e., temporal feature shift and temporal feature flip, in the mean teacher framework. The self-supervised branch defines two pretext tasks, including masked feature reconstruction and clip-order prediction, to learn the relation of temporal clues. By this means, SSTAP can better explore unlabeled videos, and improve the discriminative abilities of learned action features. We extensively evaluate the proposed SSTAP on THUMOS14 and ActivityNet v1.3 datasets. The experimental results demonstrate that SSTAP significantly outperforms state-of-the-art semi-supervised methods and even matches fully-supervised methods. Code is available at https://github.com/wangxiang1230/SSTAP.



### OXnet: Omni-supervised Thoracic Disease Detection from Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2104.03218v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03218v2)
- **Published**: 2021-04-07 16:12:31+00:00
- **Updated**: 2021-07-08 08:38:30+00:00
- **Authors**: Luyang Luo, Hao Chen, Yanning Zhou, Huangjing Lin, Pheng-Ann Pheng
- **Comment**: Accepted to MICCAI2021. Code is available at
  https://github.com/LLYXC/OXnet
- **Journal**: None
- **Summary**: Chest X-ray (CXR) is the most typical diagnostic X-ray examination for screening various thoracic diseases. Automatically localizing lesions from CXR is promising for alleviating radiologists' reading burden. However, CXR datasets are often with massive image-level annotations and scarce lesion-level annotations, and more often, without annotations. Thus far, unifying different supervision granularities to develop thoracic disease detection algorithms has not been comprehensively addressed. In this paper, we present OXnet, the first deep omni-supervised thoracic disease detection network to our best knowledge that uses as much available supervision as possible for CXR diagnosis. We first introduce supervised learning via a one-stage detection model. Then, we inject a global classification head to the detection model and propose dual attention alignment to guide the global gradient to the local detection branch, which enables learning lesion detection from image-level annotations. We also impose intra-class compactness and inter-class separability with global prototype alignment to further enhance the global information learning. Moreover, we leverage a soft focal loss to distill the soft pseudo-labels of unlabeled data generated by a teacher model. Extensive experiments on a large-scale chest X-ray dataset show the proposed OXnet outperforms competitive methods with significant margins. Further, we investigate omni-supervision under various annotation granularities and corroborate OXnet is a promising choice to mitigate the plight of annotation shortage for medical image diagnosis.



### Dual-Consistency Semi-Supervised Learning with Uncertainty Quantification for COVID-19 Lesion Segmentation from CT Images
- **Arxiv ID**: http://arxiv.org/abs/2104.03225v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.03225v2)
- **Published**: 2021-04-07 16:23:35+00:00
- **Updated**: 2021-07-08 08:47:49+00:00
- **Authors**: Yanwen Li, Luyang Luo, Huangjing Lin, Hao Chen, Pheng-Ann Heng
- **Comment**: Accepted to MICCAI2021. The first two authors contributed equally.
  Code is available at https://github.com/poiuohke/UDC-Net
- **Journal**: None
- **Summary**: The novel coronavirus disease 2019 (COVID-19) characterized by atypical pneumonia has caused millions of deaths worldwide. Automatically segmenting lesions from chest Computed Tomography (CT) is a promising way to assist doctors in COVID-19 screening, treatment planning, and follow-up monitoring. However, voxel-wise annotations are extremely expert-demanding and scarce, especially when it comes to novel diseases, while an abundance of unlabeled data could be available. To tackle the challenge of limited annotations, in this paper, we propose an uncertainty-guided dual-consistency learning network (UDC-Net) for semi-supervised COVID-19 lesion segmentation from CT images. Specifically, we present a dual-consistency learning scheme that simultaneously imposes image transformation equivalence and feature perturbation invariance to effectively harness the knowledge from unlabeled data. We then quantify the segmentation uncertainty in two forms and employ them together to guide the consistency regularization for more reliable unsupervised learning. Extensive experiments showed that our proposed UDC-Net improves the fully supervised method by 6.3% in Dice and outperforms other competitive semi-supervised approaches by significant margins, demonstrating high potential in real-world clinical practice.



### A Unified Model for Fingerprint Authentication and Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.03255v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03255v3)
- **Published**: 2021-04-07 16:57:38+00:00
- **Updated**: 2021-07-23 16:36:44+00:00
- **Authors**: Additya Popli, Saraansh Tandon, Joshua J. Engelsma, Naoyuki Onoe, Atsushi Okubo, Anoop Namboodiri
- **Comment**: Accepted at IJCB2021; 12 pages
- **Journal**: None
- **Summary**: Typical fingerprint recognition systems are comprised of a spoof detection module and a subsequent recognition module, running one after the other. In this paper, we reformulate the workings of a typical fingerprint recognition system. In particular, we posit that both spoof detection and fingerprint recognition are correlated tasks. Therefore, rather than performing the two tasks separately, we propose a joint model for spoof detection and matching to simultaneously perform both tasks without compromising the accuracy of either task. We demonstrate the capability of our joint model to obtain an authentication accuracy (1:1 matching) of TAR = 100% @ FAR = 0.1% on the FVC 2006 DB2A dataset while achieving a spoof detection ACE of 1.44% on the LiveDet 2015 dataset, both maintaining the performance of stand-alone methods. In practice, this reduces the time and memory requirements of the fingerprint recognition system by 50% and 40%, respectively; a significant advantage for recognition systems running on resource-constrained devices and communication channels. The project page for our work is available at https://www.bit.ly/ijcb2021-unified .



### Hand-Object Contact Consistency Reasoning for Human Grasps Generation
- **Arxiv ID**: http://arxiv.org/abs/2104.03304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03304v1)
- **Published**: 2021-04-07 17:57:14+00:00
- **Updated**: 2021-04-07 17:57:14+00:00
- **Authors**: Hanwen Jiang, Shaowei Liu, Jiashun Wang, Xiaolong Wang
- **Comment**: Project page: https://hwjiang1510.github.io/GraspTTA/
- **Journal**: None
- **Summary**: While predicting robot grasps with parallel jaw grippers have been well studied and widely applied in robot manipulation tasks, the study on natural human grasp generation with a multi-finger hand remains a very challenging problem. In this paper, we propose to generate human grasps given a 3D object in the world. Our key observation is that it is crucial to model the consistency between the hand contact points and object contact regions. That is, we encourage the prior hand contact points to be close to the object surface and the object common contact regions to be touched by the hand at the same time. Based on the hand-object contact consistency, we design novel objectives in training the human grasp generation model and also a new self-supervised task which allows the grasp generation network to be adjusted even during test time. Our experiments show significant improvement in human grasp generation over state-of-the-art approaches by a large margin. More interestingly, by optimizing the model during test time with the self-supervised task, it helps achieve larger gain on unseen and out-of-domain objects. Project page: https://hwjiang1510.github.io/GraspTTA/



### Warp Consistency for Unsupervised Learning of Dense Correspondences
- **Arxiv ID**: http://arxiv.org/abs/2104.03308v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03308v3)
- **Published**: 2021-04-07 17:58:22+00:00
- **Updated**: 2021-08-18 14:08:18+00:00
- **Authors**: Prune Truong, Martin Danelljan, Fisher Yu, Luc Van Gool
- **Comment**: Accepted to ICCV 2021 as an ORAL!
- **Journal**: 2021 IEEE/CVF International Conference on Computer Vision (ICCV)
- **Summary**: The key challenge in learning dense correspondences lies in the lack of ground-truth matches for real image pairs. While photometric consistency losses provide unsupervised alternatives, they struggle with large appearance changes, which are ubiquitous in geometric and semantic matching tasks. Moreover, methods relying on synthetic training pairs often suffer from poor generalisation to real data.   We propose Warp Consistency, an unsupervised learning objective for dense correspondence regression. Our objective is effective even in settings with large appearance and view-point changes. Given a pair of real images, we first construct an image triplet by applying a randomly sampled warp to one of the original images. We derive and analyze all flow-consistency constraints arising between the triplet. From our observations and empirical results, we design a general unsupervised objective employing two of the derived constraints. We validate our warp consistency loss by training three recent dense correspondence networks for the geometric and semantic matching tasks. Our approach sets a new state-of-the-art on several challenging benchmarks, including MegaDepth, RobotCar and TSS. Code and models are at github.com/PruneTruong/DenseMatching.



### Streaming Self-Training via Domain-Agnostic Unlabeled Images
- **Arxiv ID**: http://arxiv.org/abs/2104.03309v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.03309v1)
- **Published**: 2021-04-07 17:58:39+00:00
- **Updated**: 2021-04-07 17:58:39+00:00
- **Authors**: Zhiqiu Lin, Deva Ramanan, Aayush Bansal
- **Comment**: Project Page: https://www.cs.cmu.edu/~aayushb/SST/
- **Journal**: None
- **Summary**: We present streaming self-training (SST) that aims to democratize the process of learning visual recognition models such that a non-expert user can define a new task depending on their needs via a few labeled examples and minimal domain knowledge. Key to SST are two crucial observations: (1) domain-agnostic unlabeled images enable us to learn better models with a few labeled examples without any additional knowledge or supervision; and (2) learning is a continuous process and can be done by constructing a schedule of learning updates that iterates between pre-training on novel segments of the streams of unlabeled data, and fine-tuning on the small and fixed labeled dataset. This allows SST to overcome the need for a large number of domain-specific labeled and unlabeled examples, exorbitant computational resources, and domain/task-specific knowledge. In this setting, classical semi-supervised approaches require a large amount of domain-specific labeled and unlabeled examples, immense resources to process data, and expert knowledge of a particular task. Due to these reasons, semi-supervised learning has been restricted to a few places that can house required computational and human resources. In this work, we overcome these challenges and demonstrate our findings for a wide range of visual recognition tasks including fine-grained image classification, surface normal estimation, and semantic segmentation. We also demonstrate our findings for diverse domains including medical, satellite, and agricultural imagery, where there does not exist a large amount of labeled or unlabeled data.



### Regularizing Generative Adversarial Networks under Limited Data
- **Arxiv ID**: http://arxiv.org/abs/2104.03310v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.03310v1)
- **Published**: 2021-04-07 17:59:06+00:00
- **Updated**: 2021-04-07 17:59:06+00:00
- **Authors**: Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, Weilong Yang
- **Comment**: CVPR 2021. Project Page: https://hytseng0509.github.io/lecam-gan
  Code: https://github.com/google/lecam-gan
- **Journal**: None
- **Summary**: Recent years have witnessed the rapid progress of generative adversarial networks (GANs). However, the success of the GAN models hinges on a large amount of training data. This work proposes a regularization approach for training robust GAN models on limited data. We theoretically show a connection between the regularized loss and an f-divergence called LeCam-divergence, which we find is more robust under limited training data. Extensive experiments on several benchmark datasets demonstrate that the proposed regularization scheme 1) improves the generalization performance and stabilizes the learning dynamics of GAN models under limited training data, and 2) complements the recent data augmentation methods. These properties facilitate training GAN models to achieve state-of-the-art performance when only limited training data of the ImageNet benchmark is available.



### PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable Physics
- **Arxiv ID**: http://arxiv.org/abs/2104.03311v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.03311v1)
- **Published**: 2021-04-07 17:59:23+00:00
- **Updated**: 2021-04-07 17:59:23+00:00
- **Authors**: Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B. Tenenbaum, Chuang Gan
- **Comment**: Accepted to ICLR 2021 as a spotlight presentation. Project page:
  http://plasticinelab.csail.mit.edu/
- **Journal**: None
- **Summary**: Simulated virtual environments serve as one of the main driving forces behind developing and evaluating skill learning algorithms. However, existing environments typically only simulate rigid body physics. Additionally, the simulation process usually does not provide gradients that might be useful for planning and control optimizations. We introduce a new differentiable physics benchmark called PasticineLab, which includes a diverse collection of soft body manipulation tasks. In each task, the agent uses manipulators to deform the plasticine into the desired configuration. The underlying physics engine supports differentiable elastic and plastic deformation using the DiffTaichi system, posing many under-explored challenges to robotic agents. We evaluate several existing reinforcement learning (RL) methods and gradient-based methods on this benchmark. Experimental results suggest that 1) RL-based approaches struggle to solve most of the tasks efficiently; 2) gradient-based approaches, by optimizing open-loop control sequences with the built-in differentiable physics engine, can rapidly find a solution within tens of iterations, but still fall short on multi-stage tasks that require long-term planning. We expect that PlasticineLab will encourage the development of novel algorithms that combine differentiable physics and RL for more complex physics-based skill learning tasks.



### SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.03313v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03313v2)
- **Published**: 2021-04-07 17:59:58+00:00
- **Updated**: 2021-04-08 06:31:02+00:00
- **Authors**: Shunsuke Saito, Jinlong Yang, Qianli Ma, Michael J. Black
- **Comment**: CVPR 2021 (oral). Project page: https://scanimate.is.tue.mpg.de
- **Journal**: None
- **Summary**: We present SCANimate, an end-to-end trainable framework that takes raw 3D scans of a clothed human and turns them into an animatable avatar. These avatars are driven by pose parameters and have realistic clothing that moves and deforms naturally. SCANimate does not rely on a customized mesh template or surface mesh registration. We observe that fitting a parametric 3D body model, like SMPL, to a clothed human scan is tractable while surface registration of the body topology to the scan is often not, because clothing can deviate significantly from the body shape. We also observe that articulated transformations are invertible, resulting in geometric cycle consistency in the posed and unposed shapes. These observations lead us to a weakly supervised learning method that aligns scans into a canonical pose by disentangling articulated deformations without template-based surface registration. Furthermore, to complete missing regions in the aligned scans while modeling pose-dependent deformations, we introduce a locally pose-aware implicit function that learns to complete and model geometry with learned pose correctives. In contrast to commonly used global pose embeddings, our local pose conditioning significantly reduces long-range spurious correlations and improves generalization to unseen poses, especially when training data is limited. Our method can be applied to pose-aware appearance modeling to generate a fully textured avatar. We demonstrate our approach on various clothing types with different amounts of training data, outperforming existing solutions and other variants in terms of fidelity and generality in every setting. The code is available at https://scanimate.is.tue.mpg.de.



### Automatic Generation of Descriptive Titles for Video Clips Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.03337v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.03337v1)
- **Published**: 2021-04-07 18:14:18+00:00
- **Updated**: 2021-04-07 18:14:18+00:00
- **Authors**: Soheyla Amirian, Khaled Rasheed, Thiab R. Taha, Hamid R. Arabnia
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last decade, the use of Deep Learning in many applications produced results that are comparable to and in some cases surpassing human expert performance. The application domains include diagnosing diseases, finance, agriculture, search engines, robot vision, and many others. In this paper, we are proposing an architecture that utilizes image/video captioning methods and Natural Language Processing systems to generate a title and a concise abstract for a video. Such a system can potentially be utilized in many application domains, including, the cinema industry, video search engines, security surveillance, video databases/warehouses, data centers, and others. The proposed system functions and operates as followed: it reads a video; representative image frames are identified and selected; the image frames are captioned; NLP is applied to all generated captions together with text summarization; and finally, a title and an abstract are generated for the video. All functions are performed automatically. Preliminary results are provided in this paper using publicly available datasets. This paper is not concerned about the efficiency of the system at the execution time. We hope to be able to address execution efficiency issues in our subsequent publications.



### OVANet: One-vs-All Network for Universal Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2104.03344v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03344v4)
- **Published**: 2021-04-07 18:36:31+00:00
- **Updated**: 2021-08-24 17:52:40+00:00
- **Authors**: Kuniaki Saito, Kate Saenko
- **Comment**: Accepted by ICCV2021
  [Code](https://github.com/VisionLearningGroup/OVANet)
- **Journal**: None
- **Summary**: Universal Domain Adaptation (UNDA) aims to handle both domain-shift and category-shift between two datasets, where the main challenge is to transfer knowledge while rejecting unknown classes which are absent in the labeled source data but present in the unlabeled target data. Existing methods manually set a threshold to reject unknown samples based on validation or a pre-defined ratio of unknown samples, but this strategy is not practical. In this paper, we propose a method to learn the threshold using source samples and to adapt it to the target domain. Our idea is that a minimum inter-class distance in the source domain should be a good threshold to decide between known or unknown in the target. To learn the inter-and intra-class distance, we propose to train a one-vs-all classifier for each class using labeled source data. Then, we adapt the open-set classifier to the target domain by minimizing class entropy. The resulting framework is the simplest of all baselines of UNDA and is insensitive to the value of a hyper-parameter yet outperforms baselines with a large margin.



### Monitoring Social-distance in Wide Areas during Pandemics: a Density Map and Segmentation Approach
- **Arxiv ID**: http://arxiv.org/abs/2104.03361v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.03361v1)
- **Published**: 2021-04-07 19:26:26+00:00
- **Updated**: 2021-04-07 19:26:26+00:00
- **Authors**: Javier A. González-Trejo, Diego A. Mercado-Ravell
- **Comment**: Video: https://youtu.be/TwzBMKg7h_U
- **Journal**: None
- **Summary**: With the relaxation of the containment measurements around the globe, monitoring the social distancing in crowded public places is of grate importance to prevent a new massive wave of COVID-19 infections. Recent works in that matter have limited themselves by detecting social distancing in corridors up to small crowds by detecting each person individually considering the full body in the image. In this work, we propose a new framework for monitoring the social-distance using end-to-end Deep Learning, to detect crowds violating the social-distance in wide areas where important occlusions may be present. Our framework consists in the creation of a new ground truth based on the ground truth density maps and the proposal of two different solutions, a density-map-based and a segmentation-based, to detect the crowds violating the social-distance constrain. We assess the results of both approaches by using the generated ground truth from the PET2009 and CityStreet datasets. We show that our framework performs well at providing the zones where people are not following the social-distance even when heavily occluded or far away from one camera.



### SOLD2: Self-supervised Occlusion-aware Line Description and Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.03362v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03362v2)
- **Published**: 2021-04-07 19:27:17+00:00
- **Updated**: 2021-04-09 07:38:38+00:00
- **Authors**: Rémi Pautrat, Juan-Ting Lin, Viktor Larsson, Martin R. Oswald, Marc Pollefeys
- **Comment**: 17 pages, Accepted at CVPR 2021 (Oral)
- **Journal**: None
- **Summary**: Compared to feature point detection and description, detecting and matching line segments offer additional challenges. Yet, line features represent a promising complement to points for multi-view tasks. Lines are indeed well-defined by the image gradient, frequently appear even in poorly textured areas and offer robust structural cues. We thus hereby introduce the first joint detection and description of line segments in a single deep network. Thanks to a self-supervised training, our method does not require any annotated line labels and can therefore generalize to any dataset. Our detector offers repeatable and accurate localization of line segments in images, departing from the wireframe parsing approach. Leveraging the recent progresses in descriptor learning, our proposed line descriptor is highly discriminative, while remaining robust to viewpoint changes and occlusions. We evaluate our approach against previous line detection and description methods on several multi-view datasets created with homographic warps as well as real-world viewpoint changes. Our full pipeline yields higher repeatability, localization accuracy and matching metrics, and thus represents a first step to bridge the gap with learned feature points methods. Code and trained weights are available at https://github.com/cvg/SOLD2.



### An Object Detection based Solver for Google's Image reCAPTCHA v2
- **Arxiv ID**: http://arxiv.org/abs/2104.03366v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.03366v1)
- **Published**: 2021-04-07 19:35:33+00:00
- **Updated**: 2021-04-07 19:35:33+00:00
- **Authors**: Md Imran Hossen, Yazhou Tu, Md Fazle Rabby, Md Nazmul Islam, Hui Cao, Xiali Hei
- **Comment**: Accepted at the 23rd International Symposium on Research in Attacks,
  Intrusions and Defenses (RAID 2020)
- **Journal**: None
- **Summary**: Previous work showed that reCAPTCHA v2's image challenges could be solved by automated programs armed with Deep Neural Network (DNN) image classifiers and vision APIs provided by off-the-shelf image recognition services. In response to emerging threats, Google has made significant updates to its image reCAPTCHA v2 challenges that can render the prior approaches ineffective to a great extent. In this paper, we investigate the robustness of the latest version of reCAPTCHA v2 against advanced object detection based solvers. We propose a fully automated object detection based system that breaks the most advanced challenges of reCAPTCHA v2 with an online success rate of 83.25%, the highest success rate to date, and it takes only 19.93 seconds (including network delays) on average to crack a challenge. We also study the updated security features of reCAPTCHA v2, such as anti-recognition mechanisms, improved anti-bot detection techniques, and adjustable security preferences. Our extensive experiments show that while these security features can provide some resistance against automated attacks, adversaries can still bypass most of them. Our experimental findings indicate that the recent advances in object detection technologies pose a severe threat to the security of image captcha designs relying on simple object detection as their underlying AI problem.



### Contour Proposal Networks for Biomedical Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.03393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03393v1)
- **Published**: 2021-04-07 21:00:45+00:00
- **Updated**: 2021-04-07 21:00:45+00:00
- **Authors**: Eric Upschulte, Stefan Harmeling, Katrin Amunts, Timo Dickscheid
- **Comment**: None
- **Journal**: None
- **Summary**: We present a conceptually simple framework for object instance segmentation called Contour Proposal Network (CPN), which detects possibly overlapping objects in an image while simultaneously fitting closed object contours using an interpretable, fixed-sized representation based on Fourier Descriptors. The CPN can incorporate state of the art object detection architectures as backbone networks into a single-stage instance segmentation model that can be trained end-to-end. We construct CPN models with different backbone networks, and apply them to instance segmentation of cells in datasets from different modalities. In our experiments, we show CPNs that outperform U-Nets and Mask R-CNNs in instance segmentation accuracy, and present variants with execution times suitable for real-time applications. The trained models generalize well across different domains of cell types. Since the main assumption of the framework are closed object contours, it is applicable to a wide range of detection problems also outside the biomedical domain. An implementation of the model architecture in PyTorch is freely available.



### PrivateSNN: Privacy-Preserving Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.03414v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.03414v3)
- **Published**: 2021-04-07 22:14:02+00:00
- **Updated**: 2022-05-21 17:47:13+00:00
- **Authors**: Youngeun Kim, Yeshwanth Venkatesha, Priyadarshini Panda
- **Comment**: Accepted to AAAI2022
- **Journal**: None
- **Summary**: How can we bring both privacy and energy-efficiency to a neural system? In this paper, we propose PrivateSNN, which aims to build low-power Spiking Neural Networks (SNNs) from a pre-trained ANN model without leaking sensitive information contained in a dataset. Here, we tackle two types of leakage problems: 1) Data leakage is caused when the networks access real training data during an ANN-SNN conversion process. 2) Class leakage is caused when class-related features can be reconstructed from network parameters. In order to address the data leakage issue, we generate synthetic images from the pre-trained ANNs and convert ANNs to SNNs using the generated images. However, converted SNNs remain vulnerable to class leakage since the weight parameters have the same (or scaled) value with respect to ANN parameters. Therefore, we encrypt SNN weights by training SNNs with a temporal spike-based learning rule. Updating weight parameters with temporal data makes SNNs difficult to be interpreted in the spatial domain. We observe that the encrypted PrivateSNN eliminates data and class leakage issues with a slight performance drop (less than ~2) and significant energy-efficiency gain (about 55x) compared to the standard ANN. We conduct extensive experiments on various datasets including CIFAR10, CIFAR100, and TinyImageNet, highlighting the importance of privacy-preserving SNN training.



### Towards On-Device Face Recognition in Body-worn Cameras
- **Arxiv ID**: http://arxiv.org/abs/2104.03419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03419v1)
- **Published**: 2021-04-07 22:24:57+00:00
- **Updated**: 2021-04-07 22:24:57+00:00
- **Authors**: Ali Almadan, Ajita Rattani
- **Comment**: 6 pages
- **Journal**: IEEE International Workshop on Biometrics and Forensics (IWBF)
  2021
- **Summary**: Face recognition technology related to recognizing identities is widely adopted in intelligence gathering, law enforcement, surveillance, and consumer applications. Recently, this technology has been ported to smartphones and body-worn cameras (BWC). Face recognition technology in body-worn cameras is used for surveillance, situational awareness, and keeping the officer safe. Only a handful of academic studies exist in face recognition using the body-worn camera. A recent study has assembled BWCFace facial image dataset acquired using a body-worn camera and evaluated the ResNet-50 model for face identification. However, for real-time inference in resource constraint body-worn cameras and privacy concerns involving facial images, on-device face recognition is required. To this end, this study evaluates lightweight MobileNet-V2, EfficientNet-B0, LightCNN-9 and LightCNN-29 models for face identification using body-worn camera. Experiments are performed on a publicly available BWCface dataset. The real-time inference is evaluated on three mobile devices. The comparative analysis is done with heavy-weight VGG-16 and ResNet-50 models along with six hand-crafted features to evaluate the trade-off between the performance and model size. Experimental results suggest the difference in maximum rank-1 accuracy of lightweight LightCNN-29 over best-performing ResNet-50 is \textbf{1.85\%} and the reduction in model parameters is \textbf{23.49M}. Most of the deep models obtained similar performances at rank-5 and rank-10. The inference time of LightCNNs is 2.1x faster than other models on mobile devices. The least performance difference of \textbf{14\%} is noted between LightCNN-29 and Local Phase Quantization (LPQ) descriptor at rank-1. In most of the experimental settings, lightweight LightCNN models offered the best trade-off between accuracy and the model size in comparison to most of the models.



### Track, Check, Repeat: An EM Approach to Unsupervised Tracking
- **Arxiv ID**: http://arxiv.org/abs/2104.03424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03424v1)
- **Published**: 2021-04-07 22:51:39+00:00
- **Updated**: 2021-04-07 22:51:39+00:00
- **Authors**: Adam W. Harley, Yiming Zuo, Jing Wen, Ayush Mangal, Shubhankar Potdar, Ritwick Chaudhry, Katerina Fragkiadaki
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an unsupervised method for detecting and tracking moving objects in 3D, in unlabelled RGB-D videos. The method begins with classic handcrafted techniques for segmenting objects using motion cues: we estimate optical flow and camera motion, and conservatively segment regions that appear to be moving independently of the background. Treating these initial segments as pseudo-labels, we learn an ensemble of appearance-based 2D and 3D detectors, under heavy data augmentation. We use this ensemble to detect new instances of the "moving" type, even if they are not moving, and add these as new pseudo-labels. Our method is an expectation-maximization algorithm, where in the expectation step we fire all modules and look for agreement among them, and in the maximization step we re-train the modules to improve this agreement. The constraint of ensemble agreement helps combat contamination of the generated pseudo-labels (during the E step), and data augmentation helps the modules generalize to yet-unlabelled data (during the M step). We compare against existing unsupervised object discovery and tracking methods, using challenging videos from CATER and KITTI, and show strong improvements over the state-of-the-art.



### FatNet: A Feature-attentive Network for 3D Point Cloud Processing
- **Arxiv ID**: http://arxiv.org/abs/2104.03427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.03427v1)
- **Published**: 2021-04-07 23:13:56+00:00
- **Updated**: 2021-04-07 23:13:56+00:00
- **Authors**: Chaitanya Kaul, Nick Pears, Suresh Manandhar
- **Comment**: Published at ICPR 2020 (Oral). arXiv admin note: substantial text
  overlap with arXiv:1905.07650
- **Journal**: None
- **Summary**: The application of deep learning to 3D point clouds is challenging due to its lack of order. Inspired by the point embeddings of PointNet and the edge embeddings of DGCNNs, we propose three improvements to the task of point cloud analysis. First, we introduce a novel feature-attentive neural network layer, a FAT layer, that combines both global point-based features and local edge-based features in order to generate better embeddings. Second, we find that applying the same attention mechanism across two different forms of feature map aggregation, max pooling and average pooling, gives better performance than either alone. Third, we observe that residual feature reuse in this setting propagates information more effectively between the layers, and makes the network easier to train. Our architecture achieves state-of-the-art results on the task of point cloud classification, as demonstrated on the ModelNet40 dataset, and an extremely competitive performance on the ShapeNet part segmentation challenge.



