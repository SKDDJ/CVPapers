# Arxiv Papers in cs.CV on 2021-04-18
### Self-Supervised Pillar Motion Learning for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2104.08683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08683v1)
- **Published**: 2021-04-18 02:32:08+00:00
- **Updated**: 2021-04-18 02:32:08+00:00
- **Authors**: Chenxu Luo, Xiaodong Yang, Alan Yuille
- **Comment**: cvpr2021
- **Journal**: None
- **Summary**: Autonomous driving can benefit from motion behavior comprehension when interacting with diverse traffic participants in highly dynamic environments. Recently, there has been a growing interest in estimating class-agnostic motion directly from point clouds. Current motion estimation methods usually require vast amount of annotated training data from self-driving scenes. However, manually labeling point clouds is notoriously difficult, error-prone and time-consuming. In this paper, we seek to answer the research question of whether the abundant unlabeled data collections can be utilized for accurate and efficient motion learning. To this end, we propose a learning framework that leverages free supervisory signals from point clouds and paired camera images to estimate motion purely via self-supervision. Our model involves a point cloud based structural consistency augmented with probabilistic motion masking as well as a cross-sensor motion regularization to realize the desired self-supervision. Experiments reveal that our approach performs competitively to supervised methods, and achieves the state-of-the-art result when combining our self-supervised model with supervised fine-tuning.



### Signal Processing Challenges and Examples for {\it in-situ} Transmission Electron Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2104.08688v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08688v2)
- **Published**: 2021-04-18 02:56:04+00:00
- **Updated**: 2021-08-20 19:46:35+00:00
- **Authors**: Josh Kacher, Yao Xie, Sven P. Voigt, Shixiang Zhu, Henry Yuchi, Jordan Key, Surya R. Kalidindi
- **Comment**: None
- **Journal**: None
- **Summary**: Transmission Electron Microscopy (TEM) is a powerful tool for imaging material structure and characterizing material chemistry. Recent advances in data collection technology for TEM have enabled high-volume and high-resolution data collection at a microsecond frame rate. Taking advantage of these advances in data collection rates requires the development and application of data processing tools, including image analysis, feature extraction, and streaming data processing techniques. In this paper, we highlight a few areas in materials science that have benefited from combining signal processing and statistical analysis with data collection capabilities in TEM and present a future outlook on opportunities of integrating signal processing with automated TEM data analysis.



### RPCL: A Framework for Improving Cross-Domain Detection with Auxiliary Tasks
- **Arxiv ID**: http://arxiv.org/abs/2104.08689v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08689v1)
- **Published**: 2021-04-18 02:56:19+00:00
- **Updated**: 2021-04-18 02:56:19+00:00
- **Authors**: Kai Li, Curtis Wigington, Chris Tensmeyer, Vlad I. Morariu, Handong Zhao, Varun Manjunatha, Nikolaos Barmpalios, Yun Fu
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Cross-Domain Detection (XDD) aims to train an object detector using labeled image from a source domain but have good performance in the target domain with only unlabeled images. Existing approaches achieve this either by aligning the feature maps or the region proposals from the two domains, or by transferring the style of source images to that of target image. Contrasted with prior work, this paper provides a complementary solution to align domains by learning the same auxiliary tasks in both domains simultaneously. These auxiliary tasks push image from both domains towards shared spaces, which bridges the domain gap. Specifically, this paper proposes Rotation Prediction and Consistency Learning (PRCL), a framework complementing existing XDD methods for domain alignment by leveraging the two auxiliary tasks. The first one encourages the model to extract region proposals from foreground regions by rotating an image and predicting the rotation angle from the extracted region proposals. The second task encourages the model to be robust to changes in the image space by optimizing the model to make consistent class predictions for region proposals regardless of image perturbations. Experiments show the detection performance can be consistently and significantly enhanced by applying the two proposed tasks to existing XDD methods.



### OSKDet: Towards Orientation-sensitive Keypoint Localization for Rotated Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.08697v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08697v2)
- **Published**: 2021-04-18 03:40:52+00:00
- **Updated**: 2021-06-28 14:57:11+00:00
- **Authors**: Dongchen Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Rotated object detection is a challenging issue of computer vision field. Loss of spatial information and confusion of parametric order have been the bottleneck for rotated detection accuracy. In this paper, we propose an orientation-sensitive keypoint based rotated detector OSKDet. We adopt a set of keypoints to characterize the target and predict the keypoint heatmap on ROI to form a rotated target. By proposing the orientation-sensitive heatmap, OSKDet could learn the shape and direction of rotated target implicitly and has stronger modeling capabilities for target representation, which improves the localization accuracy and acquires high quality detection results. To extract highly effective features at border areas, we design a rotation-aware deformable convolution module. Furthermore, we explore a new keypoint reorder algorithm and feature fusion module based on the angle distribution to eliminate the confusion of keypoint order. Experimental results on several public benchmarks show the state-of-the-art performance of OSKDet. Specifically, we achieve an AP of 77.81% on DOTA, 89.91% on HRSC2016, and 97.18% on UCAS-AOD, respectively.



### Lottery Jackpots Exist in Pre-trained Models
- **Arxiv ID**: http://arxiv.org/abs/2104.08700v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08700v6)
- **Published**: 2021-04-18 03:50:28+00:00
- **Updated**: 2022-12-13 03:08:41+00:00
- **Authors**: Yuxin Zhang, Mingbao Lin, Yunshan Zhong, Fei Chao, Rongrong Ji
- **Comment**: 14 pages, 9 figures
- **Journal**: None
- **Summary**: Network pruning is an effective approach to reduce network complexity with acceptable performance compromise. Existing studies achieve the sparsity of neural networks via time-consuming weight training or complex searching on networks with expanded width, which greatly limits the applications of network pruning. In this paper, we show that high-performing and sparse sub-networks without the involvement of weight training, termed "lottery jackpots", exist in pre-trained models with unexpanded width. Furthermore, we improve the efficiency for searching lottery jackpots from two perspectives. Firstly, we observe that the sparse masks derived from many existing pruning criteria have a high overlap with the searched mask of our lottery jackpot, among which, the magnitude-based pruning results in the most similar mask with ours. Consequently, our searched lottery jackpot removes 90% weights in ResNet-50, while it easily obtains more than 70% top-1 accuracy using only 5 searching epochs on ImageNet. In compliance with this insight, we initialize our sparse mask using the magnitude-based pruning, resulting in at least 3x cost reduction on the lottery jackpot searching while achieving comparable or even better performance. Secondly, we conduct an in-depth analysis of the searching process for lottery jackpots. Our theoretical result suggests that the decrease in training loss during weight searching can be disturbed by the dependency between weights in modern networks. To mitigate this, we propose a novel short restriction method to restrict change of masks that may have potential negative impacts on the training loss. Our code is available at https://github.com/zyxxmu/lottery-jackpots.



### Reconsidering CO2 emissions from Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2104.08702v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08702v1)
- **Published**: 2021-04-18 04:01:40+00:00
- **Updated**: 2021-04-18 04:01:40+00:00
- **Authors**: Andre Fu, Mahdi S. Hosseini, Konstantinos N. Plataniotis
- **Comment**: Accepted for publication in CVPR 2021 Workshop
- **Journal**: None
- **Summary**: Climate change is a pressing issue that is currently affecting and will affect every part of our lives. It's becoming incredibly vital we, as a society, address the climate crisis as a universal effort, including those in the Computer Vision (CV) community. In this work, we analyze the total cost of CO2 emissions by breaking it into (1) the architecture creation cost and (2) the life-time evaluation cost. We show that over time, these costs are non-negligible and are having a direct impact on our future. Importantly, we conduct an ethical analysis of how the CV-community is unintentionally overlooking its own ethical AI principles by emitting this level of CO2. To address these concerns, we propose adding "enforcement" as a pillar of ethical AI and provide some recommendations for how architecture designers and broader CV community can curb the climate crisis.



### The hidden label-marginal biases of segmentation losses
- **Arxiv ID**: http://arxiv.org/abs/2104.08717v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08717v3)
- **Published**: 2021-04-18 04:59:39+00:00
- **Updated**: 2022-03-29 20:36:17+00:00
- **Authors**: Bingyuan Liu, Jose Dolz, Adrian Galdran, Riadh Kobbi, Ismail Ben Ayed
- **Comment**: Code available at https://github.com/by-liu/SegLossBias
- **Journal**: None
- **Summary**: Most segmentation losses are arguably variants of the Cross-Entropy (CE) or Dice losses. In the abundant segmentation literature, there is no clear consensus as to which of these losses is a better choice, with varying performances for each across different benchmarks and applications. In this work, we develop a theoretical analysis that links these two types of losses, exposing their advantages and weaknesses. First, we provide a constrained-optimization perspective showing that CE and Dice share a much deeper connection than previously thought: They both decompose into label-marginal penalties and closely related ground-truth matching penalties. Then, we provide bound relationships and an information-theoretic analysis, which uncover hidden label-marginal biases: Dice has an intrinsic bias towards specific extremely imbalanced solutions, whereas CE implicitly encourages the ground-truth region proportions. Our theoretical results explain the wide experimental evidence in the medical-imaging literature, whereby Dice losses bring improvements for imbalanced segmentation. It also explains why CE dominates natural-image problems with diverse class proportions, in which case Dice might have difficulty adapting to different label-marginal distributions. Based on our theoretical analysis, we propose a principled and simple solution, which enables to control explicitly the label-marginal bias. Our loss integrates CE with explicit ${\cal L}_1$ regularization, which encourages label marginals to match target class proportions, thereby mitigating class imbalance but without losing generality. Comprehensive experiments and ablation studies over different losses and applications validate our theoretical analysis, as well as the effectiveness of our explicit label-marginal regularizers.



### CLIPScore: A Reference-free Evaluation Metric for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2104.08718v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2104.08718v3)
- **Published**: 2021-04-18 05:00:29+00:00
- **Updated**: 2022-03-23 19:47:21+00:00
- **Authors**: Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, Yejin Choi
- **Comment**: None
- **Journal**: EMNLP 2021
- **Summary**: Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality.   In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge.



### Application of Computer Vision and Machine Learning for Digitized Herbarium Specimens: A Systematic Literature Review
- **Arxiv ID**: http://arxiv.org/abs/2104.08732v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08732v1)
- **Published**: 2021-04-18 06:08:51+00:00
- **Updated**: 2021-04-18 06:08:51+00:00
- **Authors**: Burhan Rashid Hussein, Owais Ahmed Malik, Wee-Hong Ong, Johan Willem Frederik Slik
- **Comment**: 42 pages, 9 figures, journal
- **Journal**: None
- **Summary**: Herbarium contains treasures of millions of specimens which have been preserved for several years for scientific studies. To speed up more scientific discoveries, a digitization of these specimens is currently on going to facilitate easy access and sharing of its data to a wider scientific community. Online digital repositories such as IDigBio and GBIF have already accumulated millions of specimen images yet to be explored. This presents a perfect time to automate and speed up more novel discoveries using machine learning and computer vision. In this study, a thorough analysis and comparison of more than 50 peer-reviewed studies which focus on application of computer vision and machine learning techniques to digitized herbarium specimen have been examined. The study categorizes different techniques and applications which have been commonly used and it also highlights existing challenges together with their possible solutions. It is our hope that the outcome of this study will serve as a strong foundation for beginners of the relevant field and will also shed more light for both computer science and ecology experts.



### Stochastic Optimization of Areas Under Precision-Recall Curves with Provable Convergence
- **Arxiv ID**: http://arxiv.org/abs/2104.08736v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2104.08736v5)
- **Published**: 2021-04-18 06:22:21+00:00
- **Updated**: 2023-04-12 22:42:58+00:00
- **Authors**: Qi Qi, Youzhi Luo, Zhao Xu, Shuiwang Ji, Tianbao Yang
- **Comment**: Published on NeurIPS 2021, 24 pages, 10 figures
- **Journal**: None
- **Summary**: Areas under ROC (AUROC) and precision-recall curves (AUPRC) are common metrics for evaluating classification performance for imbalanced problems. Compared with AUROC, AUPRC is a more appropriate metric for highly imbalanced datasets. While stochastic optimization of AUROC has been studied extensively, principled stochastic optimization of AUPRC has been rarely explored. In this work, we propose a principled technical method to optimize AUPRC for deep learning. Our approach is based on maximizing the averaged precision (AP), which is an unbiased point estimator of AUPRC. We cast the objective into a sum of {\it dependent compositional functions} with inner functions dependent on random variables of the outer level. We propose efficient adaptive and non-adaptive stochastic algorithms named SOAP with {\it provable convergence guarantee under mild conditions} by leveraging recent advances in stochastic compositional optimization. Extensive experimental results on image and graph datasets demonstrate that our proposed method outperforms prior methods on imbalanced problems in terms of AUPRC. To the best of our knowledge, our work represents the first attempt to optimize AUPRC with provable convergence. The SOAP has been implemented in the libAUC library at~\url{https://libauc.org/}.



### Continuity-Discrimination Convolutional Neural Network for Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2104.08739v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08739v1)
- **Published**: 2021-04-18 06:35:03+00:00
- **Updated**: 2021-04-18 06:35:03+00:00
- **Authors**: Shen Li, Bingpeng Ma, Hong Chang, Shiguang Shan, Xilin Chen
- **Comment**: Accepted to ICME2018
- **Journal**: None
- **Summary**: This paper proposes a novel model, named Continuity-Discrimination Convolutional Neural Network (CD-CNN), for visual object tracking. Existing state-of-the-art tracking methods do not deal with temporal relationship in video sequences, which leads to imperfect feature representations. To address this problem, CD-CNN models temporal appearance continuity based on the idea of temporal slowness. Mathematically, we prove that, by introducing temporal appearance continuity into tracking, the upper bound of target appearance representation error can be sufficiently small with high probability. Further, in order to alleviate inaccurate target localization and drifting, we propose a novel notion, object-centroid, to characterize not only objectness but also the relative position of the target within a given patch. Both temporal appearance continuity and object-centroid are jointly learned during offline training and then transferred for online tracking. We evaluate our tracker through extensive experiments on two challenging benchmarks and show its competitive tracking performance compared with state-of-the-art trackers.



### Solving Inefficiency of Self-supervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.08760v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.08760v3)
- **Published**: 2021-04-18 07:47:10+00:00
- **Updated**: 2021-10-21 10:19:10+00:00
- **Authors**: Guangrun Wang, Keze Wang, Guangcong Wang, Philip H. S. Torr, Liang Lin
- **Comment**: ICCV 2021 paper, oral presentation
- **Journal**: None
- **Summary**: Self-supervised learning (especially contrastive learning) has attracted great interest due to its huge potential in learning discriminative representations in an unsupervised manner. Despite the acknowledged successes, existing contrastive learning methods suffer from very low learning efficiency, e.g., taking about ten times more training epochs than supervised learning for comparable recognition accuracy. In this paper, we reveal two contradictory phenomena in contrastive learning that we call under-clustering and over-clustering problems, which are major obstacles to learning efficiency. Under-clustering means that the model cannot efficiently learn to discover the dissimilarity between inter-class samples when the negative sample pairs for contrastive learning are insufficient to differentiate all the actual object classes. Over-clustering implies that the model cannot efficiently learn features from excessive negative sample pairs, forcing the model to over-cluster samples of the same actual classes into different clusters. To simultaneously overcome these two problems, we propose a novel self-supervised learning framework using a truncated triplet loss. Precisely, we employ a triplet loss tending to maximize the relative distance between the positive pair and negative pairs to address the under-clustering problem; and we construct the negative pair by selecting a negative sample deputy from all negative samples to avoid the over-clustering problem, guaranteed by the Bernoulli Distribution model. We extensively evaluate our framework in several large-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results demonstrate our model's superiority (e.g., the learning efficiency) over the latest state-of-the-art methods by a clear margin. Codes available at: https://github.com/wanggrun/triplet .



### Cross-Task Generalization via Natural Language Crowdsourcing Instructions
- **Arxiv ID**: http://arxiv.org/abs/2104.08773v4
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.08773v4)
- **Published**: 2021-04-18 08:44:56+00:00
- **Updated**: 2022-03-14 09:15:08+00:00
- **Authors**: Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi
- **Comment**: ACL 2022
- **Journal**: None
- **Summary**: Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.



### Line Segmentation from Unconstrained Handwritten Text Images using Adaptive Approach
- **Arxiv ID**: http://arxiv.org/abs/2104.08777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08777v1)
- **Published**: 2021-04-18 08:52:52+00:00
- **Updated**: 2021-04-18 08:52:52+00:00
- **Authors**: Nidhi Gupta, Wenju Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Line segmentation from handwritten text images is one of the challenging task due to diversity and unknown variations as undefined spaces, styles, orientations, stroke heights, overlapping, and alignments. Though abundant researches, there is a need of improvement to achieve robustness and higher segmentation rates. In the present work, an adaptive approach is used for the line segmentation from handwritten text images merging the alignment of connected component coordinates and text height. The mathematical justification is provided for measuring the text height respective to the image size. The novelty of the work lies in the text height calculation dynamically. The experiments are tested on the dataset provided by the Chinese company for the project. The proposed scheme is tested on two different type of datasets; document pages having base lines and plain pages. Dataset is highly complex and consists of abundant and uncommon variations in handwriting patterns. The performance of the proposed method is tested on our datasets as well as benchmark datasets, namely IAM and ICDAR09 to achieve 98.01% detection rate on average. The performance is examined on the above said datasets to observe 91.99% and 96% detection rates, respectively.



### Gaussian Dynamic Convolution for Efficient Single-Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.08783v2
- **DOI**: 10.1109/TCSVT.2021.3096814
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08783v2)
- **Published**: 2021-04-18 09:20:55+00:00
- **Updated**: 2021-05-23 11:28:04+00:00
- **Authors**: Xin Sun, Changrui Chen, Xiaorui Wang, Junyu Dong, Huiyu Zhou, Sheng Chen
- **Comment**: None
- **Journal**: IEEE Transactions on Circuits and Systems for Video Technology,
  2021
- **Summary**: Interactive single-image segmentation is ubiquitous in the scientific and commercial imaging software. In this work, we focus on the single-image segmentation problem only with some seeds such as scribbles. Inspired by the dynamic receptive field in the human being's visual system, we propose the Gaussian dynamic convolution (GDC) to fast and efficiently aggregate the contextual information for neural networks. The core idea is randomly selecting the spatial sampling area according to the Gaussian distribution offsets. Our GDC can be easily used as a module to build lightweight or complex segmentation networks. We adopt the proposed GDC to address the typical single-image segmentation tasks. Furthermore, we also build a Gaussian dynamic pyramid Pooling to show its potential and generality in common semantic segmentation. Experiments demonstrate that the GDC outperforms other existing convolutions on three benchmark segmentation datasets including Pascal-Context, Pascal-VOC 2012, and Cityscapes. Additional experiments are also conducted to illustrate that the GDC can produce richer and more vivid features compared with other convolutions. In general, our GDC is conducive to the convolutional neural networks to form an overall impression of the image.



### An Uncertainty-aware Hierarchical Probabilistic Network for Early Prediction, Quantification and Segmentation of Pulmonary Tumour Growth
- **Arxiv ID**: http://arxiv.org/abs/2104.08789v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, 68T07, 68T35, I.5.2; I.4.0; I.2.1; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2104.08789v1)
- **Published**: 2021-04-18 09:48:58+00:00
- **Updated**: 2021-04-18 09:48:58+00:00
- **Authors**: Xavier Rafael-Palou, Anton Aubanell, Mario Ceresa, Vicent Ribas, Gemma Piella, Miguel A. González Ballester
- **Comment**: 24 pages, 9 figures
- **Journal**: None
- **Summary**: Early detection and quantification of tumour growth would help clinicians to prescribe more accurate treatments and provide better surgical planning. However, the multifactorial and heterogeneous nature of lung tumour progression hampers identification of growth patterns. In this study, we present a novel method based on a deep hierarchical generative and probabilistic framework that, according to radiological guidelines, predicts tumour growth, quantifies its size and provides a semantic appearance of the future nodule. Unlike previous deterministic solutions, the generative characteristic of our approach also allows us to estimate the uncertainty in the predictions, especially important for complex and doubtful cases. Results of evaluating this method on an independent test set reported a tumour growth balanced accuracy of 74%, a tumour growth size MAE of 1.77 mm and a tumour segmentation Dice score of 78%. These surpassed the performances of equivalent deterministic and alternative generative solutions (i.e. probabilistic U-Net, Bayesian test dropout and Pix2Pix GAN) confirming the suitability of our approach.



### MonoGRNet: A General Framework for Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.08797v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08797v1)
- **Published**: 2021-04-18 10:07:52+00:00
- **Updated**: 2021-04-18 10:07:52+00:00
- **Authors**: Zengyi Qin, Jinglu Wang, Yan Lu
- **Comment**: The IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI)
- **Journal**: None
- **Summary**: Detecting and localizing objects in the real 3D space, which plays a crucial role in scene understanding, is particularly challenging given only a monocular image due to the geometric information loss during imagery projection. We propose MonoGRNet for the amodal 3D object detection from a monocular image via geometric reasoning in both the observed 2D projection and the unobserved depth dimension. MonoGRNet decomposes the monocular 3D object detection task into four sub-tasks including 2D object detection, instance-level depth estimation, projected 3D center estimation and local corner regression. The task decomposition significantly facilitates the monocular 3D object detection, allowing the target 3D bounding boxes to be efficiently predicted in a single forward pass, without using object proposals, post-processing or the computationally expensive pixel-level depth estimation utilized by previous methods. In addition, MonoGRNet flexibly adapts to both fully and weakly supervised learning, which improves the feasibility of our framework in diverse settings. Experiments are conducted on KITTI, Cityscapes and MS COCO datasets. Results demonstrate the promising performance of our framework in various scenarios.



### Multi-scale Self-calibrated Network for Image Light Source Transfer
- **Arxiv ID**: http://arxiv.org/abs/2104.08838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08838v1)
- **Published**: 2021-04-18 12:23:01+00:00
- **Updated**: 2021-04-18 12:23:01+00:00
- **Authors**: Yuanzhi Wang, Tao Lu, Yanduo Zhang, Yuntao Wu
- **Comment**: 8 pages,4 figures
- **Journal**: CVPR 2021
- **Summary**: Image light source transfer (LLST), as the most challenging task in the domain of image relighting, has attracted extensive attention in recent years. In the latest research, LLST is decomposed three sub-tasks: scene reconversion, shadow estimation, and image re-rendering, which provides a new paradigm for image relighting. However, many problems for scene reconversion and shadow estimation tasks, including uncalibrated feature information and poor semantic information, are still unresolved, thereby resulting in insufficient feature representation. In this paper, we propose novel down-sampling feature self-calibrated block (DFSB) and up-sampling feature self-calibrated block (UFSB) as the basic blocks of feature encoder and decoder to calibrate feature representation iteratively because the LLST is similar to the recalibration of image light source. In addition, we fuse the multi-scale features of the decoder in scene reconversion task to further explore and exploit more semantic information, thereby providing more accurate primary scene structure for image re-rendering. Experimental results in the VIDIT dataset show that the proposed approach significantly improves the performance for LLST.



### Lesion-Inspired Denoising Network: Connecting Medical Image Denoising and Lesion Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.08845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.08845v1)
- **Published**: 2021-04-18 12:53:36+00:00
- **Updated**: 2021-04-18 12:53:36+00:00
- **Authors**: Kecheng Chen, Kun Long, Yazhou Ren, Jiayu Sun, Xiaorong Pu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has achieved notable performance in the denoising task of low-quality medical images and the detection task of lesions, respectively. However, existing low-quality medical image denoising approaches are disconnected from the detection task of lesions. Intuitively, the quality of denoised images will influence the lesion detection accuracy that in turn can be used to affect the denoising performance. To this end, we propose a play-and-plug medical image denoising framework, namely Lesion-Inspired Denoising Network (LIDnet), to collaboratively improve both denoising performance and detection accuracy of denoised medical images. Specifically, we propose to insert the feedback of downstream detection task into existing denoising framework by jointly learning a multi-loss objective. Instead of using perceptual loss calculated on the entire feature map, a novel region-of-interest (ROI) perceptual loss induced by the lesion detection task is proposed to further connect these two tasks. To achieve better optimization for overall framework, we propose a customized collaborative training strategy for LIDnet. On consideration of clinical usability and imaging characteristics, three low-dose CT images datasets are used to evaluate the effectiveness of the proposed LIDnet. Experiments show that, by equipping with LIDnet, both of the denoising and lesion detection performance of baseline methods can be significantly improved.



### On Training Sketch Recognizers for New Domains
- **Arxiv ID**: http://arxiv.org/abs/2104.08850v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.08850v1)
- **Published**: 2021-04-18 13:24:49+00:00
- **Updated**: 2021-04-18 13:24:49+00:00
- **Authors**: Kemal Tugrul Yesilbek, T. Metin Sezgin
- **Comment**: Accepted for The 1st Workshop on Sketch-Oriented Deep Learning
  (SketchDL) @ CVPR 2021
- **Journal**: None
- **Summary**: Sketch recognition algorithms are engineered and evaluated using publicly available datasets contributed by the sketch recognition community over the years. While existing datasets contain sketches of a limited set of generic objects, each new domain inevitably requires collecting new data for training domain specific recognizers. This gives rise to two fundamental concerns: First, will the data collection protocol yield ecologically valid data? Second, will the amount of collected data suffice to train sufficiently accurate classifiers? In this paper, we draw attention to these two concerns. We show that the ecological validity of the data collection protocol and the ability to accommodate small datasets are significant factors impacting recognizer accuracy in realistic scenarios. More specifically, using sketch-based gaming as a use case, we show that deep learning methods, as well as more traditional methods, suffer significantly from dataset shift. Furthermore, we demonstrate that in realistic scenarios where data is scarce and expensive, standard measures taken for adapting deep learners to small datasets fall short of comparing favorably with alternatives. Although transfer learning, and extensive data augmentation help deep learners, they still perform significantly worse compared to standard setups (e.g., SVMs and GBMs with standard feature representations). We pose learning from small datasets as a key problem for the deep sketch recognition field, one which has been ignored in the bulk of the existing literature.



### Let's See Clearly: Contaminant Artifact Removal for Moving Cameras
- **Arxiv ID**: http://arxiv.org/abs/2104.08852v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.6; I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2104.08852v1)
- **Published**: 2021-04-18 13:37:34+00:00
- **Updated**: 2021-04-18 13:37:34+00:00
- **Authors**: Xiaoyu Li, Bo Zhang, Jing Liao, Pedro V. Sander
- **Comment**: 10 pages, 11 figures
- **Journal**: None
- **Summary**: Contaminants such as dust, dirt and moisture adhering to the camera lens can greatly affect the quality and clarity of the resulting image or video. In this paper, we propose a video restoration method to automatically remove these contaminants and produce a clean video. Our approach first seeks to detect attention maps that indicate the regions that need to be restored. In order to leverage the corresponding clean pixels from adjacent frames, we propose a flow completion module to hallucinate the flow of the background scene to the attention regions degraded by the contaminants. Guided by the attention maps and completed flows, we propose a recurrent technique to restore the input frame by fetching clean pixels from adjacent frames. Finally, a multi-frame processing stage is used to further process the entire video sequence in order to enforce temporal consistency. The entire network is trained on a synthetic dataset that approximates the physical lighting properties of contaminant artifacts. This new dataset and our novel framework lead to our method that is able to address different contaminants and outperforms competitive restoration approaches both qualitatively and quantitatively.



### An Improved Discriminative Optimization for 3D Rigid Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2104.08854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08854v1)
- **Published**: 2021-04-18 13:39:52+00:00
- **Updated**: 2021-04-18 13:39:52+00:00
- **Authors**: Jia Wang, Ping Wang, Biao Li, Ruigang Fu, Junzheng Wu
- **Comment**: None
- **Journal**: None
- **Summary**: The Discriminative Optimization (DO) algorithm has been proved much successful in 3D point cloud registration. In the original DO, the feature (descriptor) of two point cloud was defined as a histogram, and the element of histogram indicates the weights of scene points in "front" or "back" side of a model point. In this paper, we extended the histogram which indicate the sides from "front-back" to "front-back", "up-down", and "clockwise-anticlockwise". In addition, we reweighted the extended histogram according to the model points' distribution. We evaluated the proposed Improved DO on the Stanford Bunny and Oxford SensatUrban dataset, and compared it with six classical State-Of-The-Art point cloud registration algorithms. The experimental result demonstrates our algorithm achieves comparable performance in point registration accuracy and root-mean-sqart-error.



### Learning Interpretable End-to-End Vision-Based Motion Planning for Autonomous Driving with Optical Flow Distillation
- **Arxiv ID**: http://arxiv.org/abs/2104.12861v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.12861v1)
- **Published**: 2021-04-18 13:51:25+00:00
- **Updated**: 2021-04-18 13:51:25+00:00
- **Authors**: Hengli Wang, Peide Cai, Yuxiang Sun, Lujia Wang, Ming Liu
- **Comment**: 7 pages, 5 figures and 1 table. This paper is accepted by ICRA 2021.
  arXiv admin note: text overlap with arXiv:2104.08862
- **Journal**: None
- **Summary**: Recently, deep-learning based approaches have achieved impressive performance for autonomous driving. However, end-to-end vision-based methods typically have limited interpretability, making the behaviors of the deep networks difficult to explain. Hence, their potential applications could be limited in practice. To address this problem, we propose an interpretable end-to-end vision-based motion planning approach for autonomous driving, referred to as IVMP. Given a set of past surrounding-view images, our IVMP first predicts future egocentric semantic maps in bird's-eye-view space, which are then employed to plan trajectories for self-driving vehicles. The predicted future semantic maps not only provide useful interpretable information, but also allow our motion planning module to handle objects with low probability, thus improving the safety of autonomous driving. Moreover, we also develop an optical flow distillation paradigm, which can effectively enhance the network while still maintaining its real-time performance. Extensive experiments on the nuScenes dataset and closed-loop simulation show that our IVMP significantly outperforms the state-of-the-art approaches in imitating human drivers with a much higher success rate. Our project page is available at https://sites.google.com/view/ivmp.



### Filtering Empty Camera Trap Images in Embedded Systems
- **Arxiv ID**: http://arxiv.org/abs/2104.08859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08859v1)
- **Published**: 2021-04-18 13:56:22+00:00
- **Updated**: 2021-04-18 13:56:22+00:00
- **Authors**: Fagner Cunha, Eulanda M. dos Santos, Raimundo Barreto, Juan G. Colonna
- **Comment**: Accepted to CVPR 2021 (Mobile AI workshop and challenges)
- **Journal**: None
- **Summary**: Monitoring wildlife through camera traps produces a massive amount of images, whose a significant portion does not contain animals, being later discarded. Embedding deep learning models to identify animals and filter these images directly in those devices brings advantages such as savings in the storage and transmission of data, usually resource-constrained in this type of equipment. In this work, we present a comparative study on animal recognition models to analyze the trade-off between precision and inference latency on edge devices. To accomplish this objective, we investigate classifiers and object detectors of various input resolutions and optimize them using quantization and reducing the number of model filters. The confidence threshold of each model was adjusted to obtain 96% recall for the nonempty class, since instances from the empty class are expected to be discarded. The experiments show that, when using the same set of images for training, detectors achieve superior performance, eliminating at least 10% more empty images than classifiers with comparable latencies. Considering the high cost of generating labels for the detection problem, when there is a massive number of images labeled for classification (about one million instances, ten times more than those available for detection), classifiers are able to reach results comparable to detectors but with half latency.



### CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2104.08860v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08860v2)
- **Published**: 2021-04-18 13:59:50+00:00
- **Updated**: 2021-05-08 08:25:57+00:00
- **Authors**: Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, Tianrui Li
- **Comment**: None
- **Journal**: None
- **Summary**: Video-text retrieval plays an essential role in multi-modal research and has been widely used in many real-world web applications. The CLIP (Contrastive Language-Image Pre-training), an image-language pre-training model, has demonstrated the power of visual concepts learning from web collected image-text datasets. In this paper, we propose a CLIP4Clip model to transfer the knowledge of the CLIP model to video-language retrieval in an end-to-end manner. Several questions are investigated via empirical studies: 1) Whether image feature is enough for video-text retrieval? 2) How a post-pretraining on a large-scale video-text dataset based on the CLIP affect the performance? 3) What is the practical mechanism to model temporal dependency between video frames? And 4) The Hyper-parameters sensitivity of the model on video-text retrieval task. Extensive experimental results present that the CLIP4Clip model transferred from the CLIP can achieve SOTA results on various video-text retrieval datasets, including MSR-VTT, MSVC, LSMDC, ActivityNet, and DiDeMo. We release our code at https://github.com/ArrowLuo/CLIP4Clip.



### End-to-End Interactive Prediction and Planning with Optical Flow Distillation for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2104.08862v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.08862v1)
- **Published**: 2021-04-18 14:05:18+00:00
- **Updated**: 2021-04-18 14:05:18+00:00
- **Authors**: Hengli Wang, Peide Cai, Rui Fan, Yuxiang Sun, Ming Liu
- **Comment**: 10 pages, 5 figures and 4 tables. This paper is accepted by CVPRW
  2021
- **Journal**: None
- **Summary**: With the recent advancement of deep learning technology, data-driven approaches for autonomous car prediction and planning have achieved extraordinary performance. Nevertheless, most of these approaches follow a non-interactive prediction and planning paradigm, hypothesizing that a vehicle's behaviors do not affect others. The approaches based on such a non-interactive philosophy typically perform acceptably in sparse traffic scenarios but can easily fail in dense traffic scenarios. Therefore, we propose an end-to-end interactive neural motion planner (INMP) for autonomous driving in this paper. Given a set of past surrounding-view images and a high definition map, our INMP first generates a feature map in bird's-eye-view space, which is then processed to detect other agents and perform interactive prediction and planning jointly. Also, we adopt an optical flow distillation paradigm, which can effectively improve the network performance while still maintaining its real-time inference speed. Extensive experiments on the nuScenes dataset and in the closed-loop Carla simulation environment demonstrate the effectiveness and efficiency of our INMP for the detection, prediction, and planning tasks. Our project page is at sites.google.com/view/inmp-ofd.



### A survey of image labelling for computer vision applications
- **Arxiv ID**: http://arxiv.org/abs/2104.08885v1
- **DOI**: 10.1080/2573234X.2021.1908861
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.08885v1)
- **Published**: 2021-04-18 16:01:55+00:00
- **Updated**: 2021-04-18 16:01:55+00:00
- **Authors**: Christoph Sager, Christian Janiesch, Patrick Zschech
- **Comment**: Published online first in Journal of Business Analytics
- **Journal**: None
- **Summary**: Supervised machine learning methods for image analysis require large amounts of labelled training data to solve computer vision problems. The recent rise of deep learning algorithms for recognising image content has led to the emergence of many ad-hoc labelling tools. With this survey, we capture and systematise the commonalities as well as the distinctions between existing image labelling software. We perform a structured literature review to compile the underlying concepts and features of image labelling software such as annotation expressiveness and degree of automation. We structure the manual labelling task by its organisation of work, user interface design options, and user support techniques to derive a systematisation schema for this survey. Applying it to available software and the body of literature, enabled us to uncover several application archetypes and key domains such as image retrieval or instance identification in healthcare or television.



### Convolutional Neural Networks in Orthodontics: a review
- **Arxiv ID**: http://arxiv.org/abs/2104.08886v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.08886v1)
- **Published**: 2021-04-18 16:02:30+00:00
- **Updated**: 2021-04-18 16:02:30+00:00
- **Authors**: Szymon Płotka, Tomasz Włodarczyk, Ryszard Szczerba, Przemysław Rokita, Patrycja Bartkowska, Oskar Komisarek, Artur Matthews-Brzozowski, Tomasz Trzciński
- **Comment**: Preprint to Elsevier
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are used in many areas of computer vision, such as object tracking and recognition, security, military, and biomedical image analysis. This review presents the application of convolutional neural networks in one of the fields of dentistry - orthodontics. Advances in medical imaging technologies and methods allow CNNs to be used in orthodontics to shorten the planning time of orthodontic treatment, including an automatic search of landmarks on cephalometric X-ray images, tooth segmentation on Cone-Beam Computed Tomography (CBCT) images or digital models, and classification of defects on X-Ray panoramic images. In this work, we describe the current methods, the architectures of deep convolutional neural networks used, and their implementations, together with a comparison of the results achieved by them. The promising results and visualizations of the described studies show that the use of methods based on convolutional neural networks allows for the improvement of computer-based orthodontic treatment planning, both by reducing the examination time and, in many cases, by performing the analysis much more accurately than a manual orthodontist does.



### The Intrinsic Dimension of Images and Its Impact on Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.08894v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, I.2.6; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2104.08894v1)
- **Published**: 2021-04-18 16:29:23+00:00
- **Updated**: 2021-04-18 16:29:23+00:00
- **Authors**: Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, Tom Goldstein
- **Comment**: To appear at ICLR 2021 (spotlight), 17 pages with appendix, 15
  figures
- **Journal**: None
- **Summary**: It is widely believed that natural image data exhibits low-dimensional structure despite the high dimensionality of conventional pixel representations. This idea underlies a common intuition for the remarkable success of deep learning in computer vision. In this work, we apply dimension estimation tools to popular datasets and investigate the role of low-dimensional structure in deep learning. We find that common natural image datasets indeed have very low intrinsic dimension relative to the high number of pixels in the images. Additionally, we find that low dimensional datasets are easier for neural networks to learn, and models solving these tasks generalize better from training to test data. Along the way, we develop a technique for validating our dimension estimation tools on synthetic data generated by GANs allowing us to actively manipulate the intrinsic dimension by controlling the image generation process. Code for our experiments may be found here https://github.com/ppope/dimensions.



### Texture Based Classification of High Resolution Remotely Sensed Imagery using Weber Local Descriptor
- **Arxiv ID**: http://arxiv.org/abs/2104.08899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08899v1)
- **Published**: 2021-04-18 16:37:34+00:00
- **Updated**: 2021-04-18 16:37:34+00:00
- **Authors**: Decky Aspandi-Latif, Sally Goldin, Preesan Rakwatin, Kurt Rudahl
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional image classification techniques often produce unsatisfactory results when applied to high spatial resolution data because classes in high resolution images are not spectrally homogeneous. Texture offers an alternative source of information for classifying these images. This paper evaluates a recently developed, computationally simple texture metric called Weber Local Descriptor (WLD) for use in classifying high resolution QuickBird panchromatic data. We compared WLD with state-of-the art texture descriptors (TD) including Local Binary Pattern (LBP) and its rotation-invariant version LBPRIU. We also investigated whether incorporating VAR, a TD that captures brightness variation, would improve the accuracy of LBPRIU and WLD. We found that WLD generally produces more accurate classification results than the other TD we examined, and is also more robust to varying parameters. We have implemented an optimised algorithm for calculating WLD which makes the technique practical in terms of computation time. Overall, our results indicate that WLD is a promising approach for classifying high resolution remote sensing data.



### A Two-branch Neural Network for Non-homogeneous Dehazing via Ensemble Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.08902v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.08902v1)
- **Published**: 2021-04-18 16:39:13+00:00
- **Updated**: 2021-04-18 16:39:13+00:00
- **Authors**: Yankun Yu, Huan Liu, Minghan Fu, Jun Chen, Xiyao Wang, Keyan Wang
- **Comment**: Accepted in CVPRW 2021
- **Journal**: None
- **Summary**: Recently, there has been rapid and significant progress on image dehazing. Many deep learning based methods have shown their superb performance in handling homogeneous dehazing problems. However, we observe that even if a carefully designed convolutional neural network (CNN) can perform well on large-scaled dehazing benchmarks, the network usually fails on the non-homogeneous dehazing datasets introduced by NTIRE challenges. The reasons are mainly in two folds. Firstly, due to its non-homogeneous nature, the non-uniformly distributed haze is harder to be removed than the homogeneous haze. Secondly, the research challenge only provides limited data (there are only 25 training pairs in NH-Haze 2021 dataset). Thus, learning the mapping from the domain of hazy images to that of clear ones based on very limited data is extremely hard. To this end, we propose a simple but effective approach for non-homogeneous dehazing via ensemble learning. To be specific, we introduce a two-branch neural network to separately deal with the aforementioned problems and then map their distinct features by a learnable fusion tail. We show extensive experimental results to illustrate the effectiveness of our proposed method.



### Towards Open-World Text-Guided Face Image Generation and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2104.08910v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.08910v1)
- **Published**: 2021-04-18 16:56:07+00:00
- **Updated**: 2021-04-18 16:56:07+00:00
- **Authors**: Weihao Xia, Yujiu Yang, Jing-Hao Xue, Baoyuan Wu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2012.03308
- **Journal**: None
- **Summary**: The existing text-guided image synthesis methods can only produce limited quality results with at most \mbox{$\text{256}^2$} resolution and the textual instructions are constrained in a small Corpus. In this work, we propose a unified framework for both face image generation and manipulation that produces diverse and high-quality images with an unprecedented resolution at 1024 from multimodal inputs. More importantly, our method supports open-world scenarios, including both image and text, without any re-training, fine-tuning, or post-processing. To be specific, we propose a brand new paradigm of text-guided image generation and manipulation based on the superior characteristics of a pretrained GAN model. Our proposed paradigm includes two novel strategies. The first strategy is to train a text encoder to obtain latent codes that align with the hierarchically semantic of the aforementioned pretrained GAN model. The second strategy is to directly optimize the latent codes in the latent space of the pretrained GAN model with guidance from a pretrained language model. The latent codes can be randomly sampled from a prior distribution or inverted from a given image, which provides inherent supports for both image generation and manipulation from multi-modal inputs, such as sketches or semantic labels, with textual guidance. To facilitate text-guided multi-modal synthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real face images and corresponding semantic segmentation map, sketch, and textual descriptions. Extensive experiments on the introduced dataset demonstrate the superior performance of our proposed method. Code and data are available at https://github.com/weihaox/TediGAN.



### Motion Vector Extrapolation for Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.08918v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08918v2)
- **Published**: 2021-04-18 17:26:37+00:00
- **Updated**: 2021-06-13 16:49:54+00:00
- **Authors**: Julian True, Naimul Khan
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the continued successes of computationally efficient deep neural network architectures for video object detection, performance continually arrives at the great trilemma of speed versus accuracy versus computational resources (pick two). Current attempts to exploit temporal information in video data to overcome this trilemma are bottlenecked by the state-of-the-art in object detection models. We present, a technique which performs video object detection through the use of off-the-shelf object detectors alongside existing optical flow based motion estimation techniques in parallel. Through a set of experiments on the benchmark MOT20 dataset, we demonstrate that our approach significantly reduces the baseline latency of any given object detector without sacrificing any accuracy. Further latency reduction, up to 25x lower than the original latency, can be achieved with minimal accuracy loss. MOVEX enables low latency video object detection on common CPU based systems, thus allowing for high performance video object detection beyond the domain of GPU computing. The code is available at https://github.com/juliantrue/movex.



### Data-Efficient Language-Supervised Zero-Shot Learning with Self-Distillation
- **Arxiv ID**: http://arxiv.org/abs/2104.08945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08945v1)
- **Published**: 2021-04-18 19:55:31+00:00
- **Updated**: 2021-04-18 19:55:31+00:00
- **Authors**: Ruizhe Cheng, Bichen Wu, Peizhao Zhang, Peter Vajda, Joseph E. Gonzalez
- **Comment**: 4 pages, 1 figure
- **Journal**: None
- **Summary**: Traditional computer vision models are trained to predict a fixed set of predefined categories. Recently, natural language has been shown to be a broader and richer source of supervision that provides finer descriptions to visual concepts than supervised "gold" labels. Previous works, such as CLIP, use a simple pretraining task of predicting the pairings between images and text captions. CLIP, however, is data hungry and requires more than 400M image text pairs for training. We propose a data-efficient contrastive distillation method that uses soft labels to learn from noisy image-text pairs. Our model transfers knowledge from pretrained image and sentence encoders and achieves strong performance with only 3M image text pairs, 133x smaller than CLIP. Our method exceeds the previous SoTA of general zero-shot learning on ImageNet 21k+1k by 73% relatively with a ResNet50 image encoder and DeCLUTR text encoder. We also beat CLIP by 10.5% relatively on zero-shot evaluation on Google Open Images (19,958 classes).



### Style-Aware Normalized Loss for Improving Arbitrary Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2104.10064v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10064v1)
- **Published**: 2021-04-18 20:02:08+00:00
- **Updated**: 2021-04-18 20:02:08+00:00
- **Authors**: Jiaxin Cheng, Ayush Jaiswal, Yue Wu, Pradeep Natarajan, Prem Natarajan
- **Comment**: Accepted as CVPR 2021 Oral Paper
- **Journal**: None
- **Summary**: Neural Style Transfer (NST) has quickly evolved from single-style to infinite-style models, also known as Arbitrary Style Transfer (AST). Although appealing results have been widely reported in literature, our empirical studies on four well-known AST approaches (GoogleMagenta, AdaIN, LinearTransfer, and SANet) show that more than 50% of the time, AST stylized images are not acceptable to human users, typically due to under- or over-stylization. We systematically study the cause of this imbalanced style transferability (IST) and propose a simple yet effective solution to mitigate this issue. Our studies show that the IST issue is related to the conventional AST style loss, and reveal that the root cause is the equal weightage of training samples irrespective of the properties of their corresponding style images, which biases the model towards certain styles. Through investigation of the theoretical bounds of the AST style loss, we propose a new loss that largely overcomes IST. Theoretical analysis and experimental results validate the effectiveness of our loss, with over 80% relative improvement in style deception rate and 98% relatively higher preference in human evaluation.



### Combining a Convolutional Neural Network with Autoencoders to Predict the Survival Chance of COVID-19 Patients
- **Arxiv ID**: http://arxiv.org/abs/2104.08954v2
- **DOI**: 10.1038/s41598-021-93543-8
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.08954v2)
- **Published**: 2021-04-18 20:31:17+00:00
- **Updated**: 2021-08-09 03:03:52+00:00
- **Authors**: Fahime Khozeimeh, Danial Sharifrazi, Navid Hoseini Izadi, Javad Hassannataj Joloudari, Afshin Shoeibi, Roohallah Alizadehsani, Juan M. Gorriz, Sadiq Hussain, Zahra Alizadeh Sani, Hossein Moosaei, Abbas Khosravi, Saeid Nahavandi, Sheikh Mohammed Shariful Islam
- **Comment**: None
- **Journal**: Scientific Reports, 11(1), 1-18 (2021)
- **Summary**: COVID-19 has caused many deaths worldwide. The automation of the diagnosis of this virus is highly desired. Convolutional neural networks (CNNs) have shown outstanding classification performance on image datasets. To date, it appears that COVID computer-aided diagnosis systems based on CNNs and clinical information have not yet been analysed or explored. We propose a novel method, named the CNN-AE, to predict the survival chance of COVID-19 patients using a CNN trained with clinical information. Notably, the required resources to prepare CT images are expensive and limited compared to those required to collect clinical data, such as blood pressure, liver disease, etc. We evaluated our method using a publicly available clinical dataset that we collected. The dataset properties were carefully analysed to extract important features and compute the correlations of features. A data augmentation procedure based on autoencoders (AEs) was proposed to balance the dataset. The experimental results revealed that the average accuracy of the CNN-AE (96.05%) was higher than that of the CNN (92.49%). To demonstrate the generality of our augmentation method, we trained some existing mortality risk prediction methods on our dataset (with and without data augmentation) and compared their performances. We also evaluated our method using another dataset for further generality verification. To show that clinical data can be used for COVID-19 survival chance prediction, the CNN-AE was compared with multiple pre-trained deep models that were tuned based on CT images.



