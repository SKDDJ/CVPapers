# Arxiv Papers in cs.CV on 2021-04-13
### Simpler Certified Radius Maximization by Propagating Covariances
- **Arxiv ID**: http://arxiv.org/abs/2104.05888v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05888v1)
- **Published**: 2021-04-13 01:38:36+00:00
- **Updated**: 2021-04-13 01:38:36+00:00
- **Authors**: Xingjian Zhen, Rudrasis Chakraborty, Vikas Singh
- **Comment**: This paper has been accepted by CVPR 2021 as an oral presentation. An
  introduction video can be found: https://youtu.be/m1ya2oNf5iE
- **Journal**: None
- **Summary**: One strategy for adversarially training a robust model is to maximize its certified radius -- the neighborhood around a given training sample for which the model's prediction remains unchanged. The scheme typically involves analyzing a "smoothed" classifier where one estimates the prediction corresponding to Gaussian samples in the neighborhood of each sample in the mini-batch, accomplished in practice by Monte Carlo sampling. In this paper, we investigate the hypothesis that this sampling bottleneck can potentially be mitigated by identifying ways to directly propagate the covariance matrix of the smoothed distribution through the network. To this end, we find that other than certain adjustments to the network, propagating the covariances must also be accompanied by additional accounting that keeps track of how the distributional moments transform and interact at each stage in the network. We show how satisfying these criteria yields an algorithm for maximizing the certified radius on datasets including Cifar-10, ImageNet, and Places365 while offering runtime savings on networks with moderate depth, with a small compromise in overall accuracy. We describe the details of the key modifications that enable practical use. Via various experiments, we evaluate when our simplifications are sensible, and what the key benefits and limitations are.



### Fibro-CoSANet: Pulmonary Fibrosis Prognosis Prediction using a Convolutional Self Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2104.05889v1
- **DOI**: 10.1088/1361-6560/ac36a2
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.05889v1)
- **Published**: 2021-04-13 01:44:08+00:00
- **Updated**: 2021-04-13 01:44:08+00:00
- **Authors**: Zabir Al Nazi, Fazla Rabbi Mashrur, Md Amirul Islam, Shumit Saha
- **Comment**: 12 Pages
- **Journal**: None
- **Summary**: Idiopathic pulmonary fibrosis (IPF) is a restrictive interstitial lung disease that causes lung function decline by lung tissue scarring. Although lung function decline is assessed by the forced vital capacity (FVC), determining the accurate progression of IPF remains a challenge. To address this challenge, we proposed Fibro-CoSANet, a novel end-to-end multi-modal learning-based approach, to predict the FVC decline. Fibro-CoSANet utilized CT images and demographic information in convolutional neural network frameworks with a stacked attention layer. Extensive experiments on the OSIC Pulmonary Fibrosis Progression Dataset demonstrated the superiority of our proposed Fibro-CoSANet by achieving the new state-of-the-art modified Laplace Log-Likelihood score of -6.68. This network may benefit research areas concerned with designing networks to improve the prognostic accuracy of IPF. The source-code for Fibro-CoSANet is available at: \url{https://github.com/zabir-nabil/Fibro-CoSANet}.



### CXR Segmentation by AdaIN-based Domain Adaptation and Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2104.05892v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.05892v4)
- **Published**: 2021-04-13 01:53:04+00:00
- **Updated**: 2022-10-11 10:40:24+00:00
- **Authors**: Yujin Oh, Jong Chul Ye
- **Comment**: Accepted to ECCV 2022
- **Journal**: ECCV 2022, Part XXI, LNCS 13681
- **Summary**: As segmentation labels are scarce, extensive researches have been conducted to train segmentation networks with domain adaptation, semi-supervised or self-supervised learning techniques to utilize abundant unlabeled dataset. However, these approaches appear different from each other, so it is not clear how these approaches can be combined for better performance. Inspired by recent multi-domain image translation approaches, here we propose a novel segmentation framework using adaptive instance normalization (AdaIN), so that a single generator is trained to perform both domain adaptation and semi-supervised segmentation tasks via knowledge distillation by simply changing task-specific AdaIN codes. Specifically, our framework is designed to deal with difficult situations in chest X-ray radiograph (CXR) segmentation, where labels are only available for normal data, but the trained model should be applied to both normal and abnormal data. The proposed network demonstrates great generalizability under domain shift and achieves the state-of-the-art performance for abnormal CXR segmentation.



### NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media
- **Arxiv ID**: http://arxiv.org/abs/2104.05893v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2104.05893v2)
- **Published**: 2021-04-13 01:53:26+00:00
- **Updated**: 2021-09-21 20:38:45+00:00
- **Authors**: Grace Luo, Trevor Darrell, Anna Rohrbach
- **Comment**: EMNLP 2021
- **Journal**: None
- **Summary**: Online misinformation is a prevalent societal issue, with adversaries relying on tools ranging from cheap fakes to sophisticated deep fakes. We are motivated by the threat scenario where an image is used out of context to support a certain narrative. While some prior datasets for detecting image-text inconsistency generate samples via text manipulation, we propose a dataset where both image and text are unmanipulated but mismatched. We introduce several strategies for automatically retrieving convincing images for a given caption, capturing cases with inconsistent entities or semantic context. Our large-scale automatically generated NewsCLIPpings Dataset: (1) demonstrates that machine-driven image repurposing is now a realistic threat, and (2) provides samples that represent challenging instances of mismatch between text and image in news that are able to mislead humans. We benchmark several state-of-the-art multimodal models on our dataset and analyze their performance across different pretraining domains and visual backbones.



### IMAGINE: Image Synthesis by Image-Guided Model Inversion
- **Arxiv ID**: http://arxiv.org/abs/2104.05895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05895v1)
- **Published**: 2021-04-13 02:00:24+00:00
- **Updated**: 2021-04-13 02:00:24+00:00
- **Authors**: Pei Wang, Yijun Li, Krishna Kumar Singh, Jingwan Lu, Nuno Vasconcelos
- **Comment**: Published in CVPR2021
- **Journal**: None
- **Summary**: We introduce an inversion based method, denoted as IMAge-Guided model INvErsion (IMAGINE), to generate high-quality and diverse images from only a single training sample. We leverage the knowledge of image semantics from a pre-trained classifier to achieve plausible generations via matching multi-level feature representations in the classifier, associated with adversarial training with an external discriminator. IMAGINE enables the synthesis procedure to simultaneously 1) enforce semantic specificity constraints during the synthesis, 2) produce realistic images without generator training, and 3) give users intuitive control over the generation process. With extensive experimental results, we demonstrate qualitatively and quantitatively that IMAGINE performs favorably against state-of-the-art GAN-based and inversion-based methods, across three different image domains (i.e., objects, scenes, and textures).



### SRR-Net: A Super-Resolution-Involved Reconstruction Method for High Resolution MR Imaging
- **Arxiv ID**: http://arxiv.org/abs/2104.05901v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05901v1)
- **Published**: 2021-04-13 02:19:12+00:00
- **Updated**: 2021-04-13 02:19:12+00:00
- **Authors**: Wenqi Huang, Sen Jia, Ziwen Ke, Zhuo-Xu Cui, Jing Cheng, Yanjie Zhu, Dong Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Improving the image resolution and acquisition speed of magnetic resonance imaging (MRI) is a challenging problem. There are mainly two strategies dealing with the speed-resolution trade-off: (1) $k$-space undersampling with high-resolution acquisition, and (2) a pipeline of lower resolution image reconstruction and image super-resolution. However, these approaches either have limited performance at certain high acceleration factor or suffer from the error accumulation of two-step structure. In this paper, we combine the idea of MR reconstruction and image super-resolution, and work on recovering HR images from low-resolution under-sampled $k$-space data directly. Particularly, the SR-involved reconstruction can be formulated as a variational problem, and a learnable network unrolled from its solution algorithm is proposed. A discriminator was introduced to enhance the detail refining performance. Experiment results using in-vivo HR multi-coil brain data indicate that the proposed SRR-Net is capable of recovering high-resolution brain images with both good visual quality and perceptual quality.



### MESD: Exploring Optical Flow Assessment on Edge of Motion Objects with Motion Edge Structure Difference
- **Arxiv ID**: http://arxiv.org/abs/2104.05916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05916v1)
- **Published**: 2021-04-13 03:27:41+00:00
- **Updated**: 2021-04-13 03:27:41+00:00
- **Authors**: Bin Liao, Jinlong Hu
- **Comment**: None
- **Journal**: None
- **Summary**: The optical flow estimation has been assessed in various applications. In this paper, we propose a novel method named motion edge structure difference(MESD) to assess estimation errors of optical flow fields on edge of motion objects. We implement comparison experiments for MESD by evaluating five representative optical flow algorithms on four popular benchmarks: MPI Sintel, Middlebury, KITTI 2012 and KITTI 2015. Our experimental results demonstrate that MESD can reasonably and discriminatively assess estimation errors of optical flow fields on motion edge. The results indicate that MESD could be a supplementary metric to existing general assessment metrics for evaluating optical flow algorithms in related computer vision applications.



### Thief, Beware of What Get You There: Towards Understanding Model Extraction Attack
- **Arxiv ID**: http://arxiv.org/abs/2104.05921v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.05921v1)
- **Published**: 2021-04-13 03:46:59+00:00
- **Updated**: 2021-04-13 03:46:59+00:00
- **Authors**: Xinyi Zhang, Chengfang Fang, Jie Shi
- **Comment**: 8 pages, 1 figure
- **Journal**: None
- **Summary**: Model extraction increasingly attracts research attentions as keeping commercial AI models private can retain a competitive advantage. In some scenarios, AI models are trained proprietarily, where neither pre-trained models nor sufficient in-distribution data is publicly available. Model extraction attacks against these models are typically more devastating. Therefore, in this paper, we empirically investigate the behaviors of model extraction under such scenarios. We find the effectiveness of existing techniques significantly affected by the absence of pre-trained models. In addition, the impacts of the attacker's hyperparameters, e.g. model architecture and optimizer, as well as the utilities of information retrieved from queries, are counterintuitive. We provide some insights on explaining the possible causes of these phenomena. With these observations, we formulate model extraction attacks into an adaptive framework that captures these factors with deep reinforcement learning. Experiments show that the proposed framework can be used to improve existing techniques, and show that model extraction is still possible in such strict scenarios. Our research can help system designers to construct better defense strategies based on their scenarios.



### VR3Dense: Voxel Representation Learning for 3D Object Detection and Monocular Dense Depth Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2104.05932v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.05932v2)
- **Published**: 2021-04-13 04:25:54+00:00
- **Updated**: 2021-09-14 22:50:52+00:00
- **Authors**: Shubham Shrivastava
- **Comment**: Accepted at IJCAI 2021 Artificial Intelligence for Autonomous Driving
  Workshop
- **Journal**: None
- **Summary**: 3D object detection and dense depth estimation are one of the most vital tasks in autonomous driving. Multiple sensor modalities can jointly attribute towards better robot perception, and to that end, we introduce a method for jointly training 3D object detection and monocular dense depth reconstruction neural networks. It takes as inputs, a LiDAR point-cloud, and a single RGB image during inference and produces object pose predictions as well as a densely reconstructed depth map. LiDAR point-cloud is converted into a set of voxels, and its features are extracted using 3D convolution layers, from which we regress object pose parameters. Corresponding RGB image features are extracted using another 2D convolutional neural network. We further use these combined features to predict a dense depth map. While our object detection is trained in a supervised manner, the depth prediction network is trained with both self-supervised and supervised loss functions. We also introduce a loss function, edge-preserving smooth loss, and show that this results in better depth estimation compared to the edge-aware smooth loss function, frequently used in depth prediction works.



### Dynamic Texture Synthesis by Incorporating Long-range Spatial and Temporal Correlations
- **Arxiv ID**: http://arxiv.org/abs/2104.05940v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.05940v2)
- **Published**: 2021-04-13 05:04:51+00:00
- **Updated**: 2021-04-14 04:15:31+00:00
- **Authors**: Kaitai Zhang, Bin Wang, Hong-Shuo Chen, Ye Wang, Shiyu Mou, C. -C. Jay Kuo
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: The main challenge of dynamic texture synthesis lies in how to maintain spatial and temporal consistency in synthesized videos. The major drawback of existing dynamic texture synthesis models comes from poor treatment of the long-range texture correlation and motion information. To address this problem, we incorporate a new loss term, called the Shifted Gram loss, to capture the structural and long-range correlation of the reference texture video. Furthermore, we introduce a frame sampling strategy to exploit long-period motion across multiple frames. With these two new techniques, the application scope of existing texture synthesis models can be extended. That is, they can synthesize not only homogeneous but also structured dynamic texture patterns. Thorough experimental results are provided to demonstrate that our proposed dynamic texture synthesis model offers state-of-the-art visual performance.



### Dealing with Missing Modalities in the Visual Question Answer-Difference Prediction Task through Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2104.05965v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2104.05965v1)
- **Published**: 2021-04-13 06:41:11+00:00
- **Updated**: 2021-04-13 06:41:11+00:00
- **Authors**: Jae Won Cho, Dong-Jin Kim, Jinsoo Choi, Yunjae Jung, In So Kweon
- **Comment**: To appear in CVPR MULA Workshop
- **Journal**: None
- **Summary**: In this work, we address the issues of missing modalities that have arisen from the Visual Question Answer-Difference prediction task and find a novel method to solve the task at hand. We address the missing modality-the ground truth answers-that are not present at test time and use a privileged knowledge distillation scheme to deal with the issue of the missing modality. In order to efficiently do so, we first introduce a model, the "Big" Teacher, that takes the image/question/answer triplet as its input and outperforms the baseline, then use a combination of models to distill knowledge to a target network (student) that only takes the image/question pair as its inputs. We experiment our models on the VizWiz and VQA-V2 Answer Difference datasets and show through extensive experimentation and ablation the performances of our method and a diverse possibility for future research.



### Dynamic Fusion Network For Light Field Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2104.05969v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05969v1)
- **Published**: 2021-04-13 06:45:11+00:00
- **Updated**: 2021-04-13 06:45:11+00:00
- **Authors**: Yongri Piao, Yukun Zhang, Miao Zhang, Xinxin Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Focus based methods have shown promising results for the task of depth estimation. However, most existing focus based depth estimation approaches depend on maximal sharpness of the focal stack. Out of focus information in the focal stack poses challenges for this task. In this paper, we propose a dynamically multi modal learning strategy which incorporates RGB data and the focal stack in our framework. Our goal is to deeply excavate the spatial correlation in the focal stack by designing the spatial correlation perception module and dynamically fuse multi modal information between RGB data and the focal stack in a adaptive way by designing the multi modal dynamic fusion module. The success of our method is demonstrated by achieving the state of the art performance on two datasets. Furthermore, we test our network on a set of different focused images generated by a smart phone camera to prove that the proposed method not only broke the limitation of only using light field data, but also open a path toward practical applications of depth estimation on common consumer level cameras data.



### Crossover Learning for Fast Online Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.05970v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05970v1)
- **Published**: 2021-04-13 06:47:40+00:00
- **Updated**: 2021-04-13 06:47:40+00:00
- **Authors**: Shusheng Yang, Yuxin Fang, Xinggang Wang, Yu Li, Chen Fang, Ying Shan, Bin Feng, Wenyu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Modeling temporal visual context across frames is critical for video instance segmentation (VIS) and other video understanding tasks. In this paper, we propose a fast online VIS model named CrossVIS. For temporal information modeling in VIS, we present a novel crossover learning scheme that uses the instance feature in the current frame to pixel-wisely localize the same instance in other frames. Different from previous schemes, crossover learning does not require any additional network parameters for feature enhancement. By integrating with the instance segmentation loss, crossover learning enables efficient cross-frame instance-to-pixel relation learning and brings cost-free improvement during inference. Besides, a global balanced instance embedding branch is proposed for more accurate and more stable online instance association. We conduct extensive experiments on three challenging VIS benchmarks, \ie, YouTube-VIS-2019, OVIS, and YouTube-VIS-2021 to evaluate our methods. To our knowledge, CrossVIS achieves state-of-the-art performance among all online VIS methods and shows a decent trade-off between latency and accuracy. Code will be available to facilitate future research.



### Learning Multi-modal Information for Robust Light Field Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2104.05971v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05971v1)
- **Published**: 2021-04-13 06:51:27+00:00
- **Updated**: 2021-04-13 06:51:27+00:00
- **Authors**: Yongri Piao, Xinxin Ji, Miao Zhang, Yukun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Light field data has been demonstrated to facilitate the depth estimation task. Most learning-based methods estimate the depth infor-mation from EPI or sub-aperture images, while less methods pay attention to the focal stack. Existing learning-based depth estimation methods from the focal stack lead to suboptimal performance because of the defocus blur. In this paper, we propose a multi-modal learning method for robust light field depth estimation. We first excavate the internal spatial correlation by designing a context reasoning unit which separately extracts comprehensive contextual information from the focal stack and RGB images. Then we integrate the contextual information by exploiting a attention-guide cross-modal fusion module. Extensive experiments demonstrate that our method achieves superior performance than existing representative methods on two light field datasets. Moreover, visual results on a mobile phone dataset show that our method can be widely used in daily life.



### SPARK: SPAcecraft Recognition leveraging Knowledge of Space Environment
- **Arxiv ID**: http://arxiv.org/abs/2104.05978v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.05978v2)
- **Published**: 2021-04-13 07:16:55+00:00
- **Updated**: 2021-04-14 01:58:18+00:00
- **Authors**: Mohamed Adel Musallam, Kassem Al Ismaeil, Oyebade Oyedotun, Marcos Damian Perez, Michel Poucet, Djamila Aouada
- **Comment**: 5 pages, 7 figures
- **Journal**: None
- **Summary**: This paper proposes the SPARK dataset as a new unique space object multi-modal image dataset. Image-based object recognition is an important component of Space Situational Awareness, especially for applications such as on-orbit servicing, active debris removal, and satellite formation. However, the lack of sufficient annotated space data has limited research efforts in developing data-driven spacecraft recognition approaches. The SPARK dataset has been generated under a realistic space simulation environment, with a large diversity in sensing conditions for different orbital scenarios. It provides about 150k images per modality, RGB and depth, and 11 classes for spacecrafts and debris. This dataset offers an opportunity to benchmark and further develop object recognition, classification and detection algorithms, as well as multi-modal RGB-Depth approaches under space sensing conditions. Preliminary experimental evaluation validates the relevance of the data, and highlights interesting challenging scenarios specific to the space environment.



### CLEVR_HYP: A Challenge Dataset and Baselines for Visual Question Answering with Hypothetical Actions over Images
- **Arxiv ID**: http://arxiv.org/abs/2104.05981v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05981v1)
- **Published**: 2021-04-13 07:29:21+00:00
- **Updated**: 2021-04-13 07:29:21+00:00
- **Authors**: Shailaja Keyur Sampat, Akshay Kumar, Yezhou Yang, Chitta Baral
- **Comment**: 16 pages, 11 figures, Accepted as a Long Paper at NAACL-HLT 2021
- **Journal**: None
- **Summary**: Most existing research on visual question answering (VQA) is limited to information explicitly present in an image or a video. In this paper, we take visual understanding to a higher level where systems are challenged to answer questions that involve mentally simulating the hypothetical consequences of performing specific actions in a given scenario. Towards that end, we formulate a vision-language question answering task based on the CLEVR (Johnson et. al., 2017) dataset. We then modify the best existing VQA methods and propose baseline solvers for this task. Finally, we motivate the development of better vision-language models by providing insights about the capability of diverse architectures to perform joint reasoning over image-text modality. Our dataset setup scripts and codes will be made publicly available at https://github.com/shailaja183/clevr_hyp.



### VariTex: Variational Neural Face Textures
- **Arxiv ID**: http://arxiv.org/abs/2104.05988v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.05988v3)
- **Published**: 2021-04-13 07:47:53+00:00
- **Updated**: 2021-08-18 08:13:58+00:00
- **Authors**: Marcel C. Bühler, Abhimitra Meka, Gengyan Li, Thabo Beeler, Otmar Hilliges
- **Comment**: In Proceedings of the IEEE/CVF International Conference on Computer
  Vision, 2021
- **Journal**: None
- **Summary**: Deep generative models can synthesize photorealistic images of human faces with novel identities. However, a key challenge to the wide applicability of such techniques is to provide independent control over semantically meaningful parameters: appearance, head pose, face shape, and facial expressions. In this paper, we propose VariTex - to the best of our knowledge the first method that learns a variational latent feature space of neural face textures, which allows sampling of novel identities. We combine this generative model with a parametric face model and gain explicit control over head pose and facial expressions. To generate complete images of human heads, we propose an additive decoder that adds plausible details such as hair. A novel training scheme enforces a pose-independent latent space and in consequence, allows learning a one-to-many mapping between latent codes and pose-conditioned exterior regions. The resulting method can generate geometrically consistent images of novel identities under fine-grained control over head pose, face shape, and facial expressions. This facilitates a broad range of downstream tasks, like sampling novel identities, changing the head pose, expression transfer, and more. Code and models are available for research on https://mcbuehler.github.io/VariTex.



### Disentangled Motif-aware Graph Learning for Phrase Grounding
- **Arxiv ID**: http://arxiv.org/abs/2104.06008v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2104.06008v1)
- **Published**: 2021-04-13 08:20:07+00:00
- **Updated**: 2021-04-13 08:20:07+00:00
- **Authors**: Zongshen Mu, Siliang Tang, Jie Tan, Qiang Yu, Yueting Zhuang
- **Comment**: 10 pages, 6 figures, AAAI 2021 conference
- **Journal**: None
- **Summary**: In this paper, we propose a novel graph learning framework for phrase grounding in the image. Developing from the sequential to the dense graph model, existing works capture coarse-grained context but fail to distinguish the diversity of context among phrases and image regions. In contrast, we pay special attention to different motifs implied in the context of the scene graph and devise the disentangled graph network to integrate the motif-aware contextual information into representations. Besides, we adopt interventional strategies at the feature and the structure levels to consolidate and generalize representations. Finally, the cross-modal attention network is utilized to fuse intra-modal features, where each phrase can be computed similarity with regions to select the best-grounded one. We validate the efficiency of disentangled and interventional graph network (DIGN) through a series of ablation studies, and our model achieves state-of-the-art performance on Flickr30K Entities and ReferIt Game benchmarks.



### Global Transport for Fluid Reconstruction with Learned Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2104.06031v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/2104.06031v1)
- **Published**: 2021-04-13 09:00:46+00:00
- **Updated**: 2021-04-13 09:00:46+00:00
- **Authors**: Erik Franz, Barbara Solenthaler, Nils Thuerey
- **Comment**: CVPR 2021 oral, source code:
  https://github.com/tum-pbs/Global-Flow-Transport
- **Journal**: None
- **Summary**: We propose a novel method to reconstruct volumetric flows from sparse views via a global transport formulation. Instead of obtaining the space-time function of the observations, we reconstruct its motion based on a single initial state. In addition we introduce a learned self-supervision that constrains observations from unseen angles. These visual constraints are coupled via the transport constraints and a differentiable rendering step to arrive at a robust end-to-end reconstruction algorithm. This makes the reconstruction of highly realistic flow motions possible, even from only a single input view. We show with a variety of synthetic and real flows that the proposed global reconstruction of the transport process yields an improved reconstruction of the fluid motion.



### OCM3D: Object-Centric Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.06041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06041v1)
- **Published**: 2021-04-13 09:15:40+00:00
- **Updated**: 2021-04-13 09:15:40+00:00
- **Authors**: Liang Peng, Fei Liu, Senbo Yan, Xiaofei He, Deng Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Image-only and pseudo-LiDAR representations are commonly used for monocular 3D object detection. However, methods based on them have shortcomings of either not well capturing the spatial relationships in neighbored image pixels or being hard to handle the noisy nature of the monocular pseudo-LiDAR point cloud. To overcome these issues, in this paper we propose a novel object-centric voxel representation tailored for monocular 3D object detection. Specifically, voxels are built on each object proposal, and their sizes are adaptively determined by the 3D spatial distribution of the points, allowing the noisy point cloud to be organized effectively within a voxel grid. This representation is proved to be able to locate the object in 3D space accurately. Furthermore, prior works would like to estimate the orientation via deep features extracted from an entire image or a noisy point cloud. By contrast, we argue that the local RoI information from the object image patch alone with a proper resizing scheme is a better input as it provides complete semantic clues meanwhile excludes irrelevant interferences. Besides, we decompose the confidence mechanism in monocular 3D object detection by considering the relationship between 3D objects and the associated 2D boxes. Evaluated on KITTI, our method outperforms state-of-the-art methods by a large margin. The code will be made publicly available soon.



### First and Second Order Dynamics in a Hierarchical SOM system for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.06059v1
- **DOI**: 10.1016/j.asoc.2017.06.007
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2104.06059v1)
- **Published**: 2021-04-13 09:46:40+00:00
- **Updated**: 2021-04-13 09:46:40+00:00
- **Authors**: Zahra Gharaee, Peter Gärdenfors, Magnus Johnsson
- **Comment**: None
- **Journal**: None
- **Summary**: Human recognition of the actions of other humans is very efficient and is based on patterns of movements. Our theoretical starting point is that the dynamics of the joint movements is important to action categorization. On the basis of this theory, we present a novel action recognition system that employs a hierarchy of Self-Organizing Maps together with a custom supervised neural network that learns to categorize actions. The system preprocesses the input from a Kinect like 3D camera to exploit the information not only about joint positions, but also their first and second order dynamics. We evaluate our system in two experiments with publicly available data sets, and compare its performance to the performance with less sophisticated preprocessing of the input. The results show that including the dynamics of the actions improves the performance. We also apply an attention mechanism that focuses on the parts of the body that are the most involved in performing the actions.



### Mixed supervision for surface-defect detection: from weakly to fully supervised learning
- **Arxiv ID**: http://arxiv.org/abs/2104.06064v3
- **DOI**: 10.1016/j.compind.2021.103459
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06064v3)
- **Published**: 2021-04-13 10:00:10+00:00
- **Updated**: 2021-04-20 14:09:32+00:00
- **Authors**: Jakob Božič, Domen Tabernik, Danijel Skočaj
- **Comment**: Accepted for publication in Computers in Industry
- **Journal**: None
- **Summary**: Deep-learning methods have recently started being employed for addressing surface-defect detection problems in industrial quality control. However, with a large amount of data needed for learning, often requiring high-precision labels, many industrial problems cannot be easily solved, or the cost of the solutions would significantly increase due to the annotation requirements. In this work, we relax heavy requirements of fully supervised learning methods and reduce the need for highly detailed annotations. By proposing a deep-learning architecture, we explore the use of annotations of different details ranging from weak (image-level) labels through mixed supervision to full (pixel-level) annotations on the task of surface-defect detection. The proposed end-to-end architecture is composed of two sub-networks yielding defect segmentation and classification results. The proposed method is evaluated on several datasets for industrial quality inspection: KolektorSDD, DAGM and Severstal Steel Defect. We also present a new dataset termed KolektorSDD2 with over 3000 images containing several types of defects, obtained while addressing a real-world industrial problem. We demonstrate state-of-the-art results on all four datasets. The proposed method outperforms all related approaches in fully supervised settings and also outperforms weakly-supervised methods when only image-level labels are available. We also show that mixed supervision with only a handful of fully annotated samples added to weakly labelled training images can result in performance comparable to the fully supervised model's performance but at a significantly lower annotation cost.



### Online Recognition of Actions Involving Objects
- **Arxiv ID**: http://arxiv.org/abs/2104.06070v1
- **DOI**: 10.1016/j.bica.2017.09.007
- **Categories**: **cs.RO**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.06070v1)
- **Published**: 2021-04-13 10:08:20+00:00
- **Updated**: 2021-04-13 10:08:20+00:00
- **Authors**: Zahra Gharaee, Peter Gärdenfors, Magnus Johnsson
- **Comment**: None
- **Journal**: None
- **Summary**: We present an online system for real time recognition of actions involving objects working in online mode. The system merges two streams of information processing running in parallel. One is carried out by a hierarchical self-organizing map (SOM) system that recognizes the performed actions by analysing the spatial trajectories of the agent's movements. It consists of two layers of SOMs and a custom made supervised neural network. The activation sequences in the first layer SOM represent the sequences of significant postures of the agent during the performance of actions. These activation sequences are subsequently recoded and clustered in the second layer SOM, and then labeled by the activity in the third layer custom made supervised neural network. The second information processing stream is carried out by a second system that determines which object among several in the agent's vicinity the action is applied to. This is achieved by applying a proximity measure. The presented method combines the two information processing streams to determine what action the agent performed and on what object. The action recognition system has been tested with excellent performance.



### Spatiotemporal Entropy Model is All You Need for Learned Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2104.06083v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06083v1)
- **Published**: 2021-04-13 10:38:32+00:00
- **Updated**: 2021-04-13 10:38:32+00:00
- **Authors**: Zhenhong Sun, Zhiyu Tan, Xiuyu Sun, Fangyi Zhang, Dongyang Li, Yichen Qian, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: The framework of dominant learned video compression methods is usually composed of motion prediction modules as well as motion vector and residual image compression modules, suffering from its complex structure and error propagation problem. Approaches have been proposed to reduce the complexity by replacing motion prediction modules with implicit flow networks. Error propagation aware training strategy is also proposed to alleviate incremental reconstruction errors from previously decoded frames. Although these methods have brought some improvement, little attention has been paid to the framework itself. Inspired by the success of learned image compression through simplifying the framework with a single deep neural network, it is natural to expect a better performance in video compression via a simple yet appropriate framework. Therefore, we propose a framework to directly compress raw-pixel frames (rather than residual images), where no extra motion prediction module is required. Instead, an entropy model is used to estimate the spatiotemporal redundancy in a latent space rather than pixel level, which significantly reduces the complexity of the framework. Specifically, the whole framework is a compression module, consisting of a unified auto-encoder which produces identically distributed latents for all frames, and a spatiotemporal entropy estimation model to minimize the entropy of these latents. Experiments showed that the proposed method outperforms state-of-the-art (SOTA) performance under the metric of multiscale structural similarity (MS-SSIM) and achieves competitive results under the metric of PSNR.



### Interpretability-Driven Sample Selection Using Self Supervised Learning For Disease Classification And Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.06087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06087v1)
- **Published**: 2021-04-13 10:46:33+00:00
- **Updated**: 2021-04-13 10:46:33+00:00
- **Authors**: Dwarikanath Mahapatra
- **Comment**: None
- **Journal**: None
- **Summary**: In supervised learning for medical image analysis, sample selection methodologies are fundamental to attain optimum system performance promptly and with minimal expert interactions (e.g. label querying in an active learning setup). In this paper we propose a novel sample selection methodology based on deep features leveraging information contained in interpretability saliency maps. In the absence of ground truth labels for informative samples, we use a novel self supervised learning based approach for training a classifier that learns to identify the most informative sample in a given batch of images. We demonstrate the benefits of the proposed approach, termed Interpretability-Driven Sample Selection (IDEAL), in an active learning setup aimed at lung disease classification and histopathology image segmentation. We analyze three different approaches to determine sample informativeness from interpretability saliency maps: (i) an observational model stemming from findings on previous uncertainty-based sample selection approaches, (ii) a radiomics-based model, and (iii) a novel data-driven self-supervised approach. We compare IDEAL to other baselines using the publicly available NIH chest X-ray dataset for lung disease classification, and a public histopathology segmentation dataset (GLaS), demonstrating the potential of using interpretability information for sample selection in active learning systems. Results show our proposed self supervised approach outperforms other approaches in selecting informative samples leading to state of the art performance with fewer samples.



### Adaptive Logit Adjustment Loss for Long-Tailed Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.06094v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06094v2)
- **Published**: 2021-04-13 11:00:19+00:00
- **Updated**: 2021-09-23 09:09:16+00:00
- **Authors**: Yan Zhao, Weicong Chen, Xu Tan, Kai Huang, Jihong Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Data in the real world tends to exhibit a long-tailed label distribution, which poses great challenges for the training of neural networks in visual recognition. Existing methods tackle this problem mainly from the perspective of data quantity, i.e., the number of samples in each class. To be specific, they pay more attention to tail classes, like applying larger adjustments to the logit. However, in the training process, the quantity and difficulty of data are two intertwined and equally crucial problems. For some tail classes, the features of their instances are distinct and discriminative, which can also bring satisfactory accuracy; for some head classes, although with sufficient samples, the high semantic similarity with other classes and lack of discriminative features will bring bad accuracy. Based on these observations, we propose Adaptive Logit Adjustment Loss (ALA Loss) to apply an adaptive adjusting term to the logit. The adaptive adjusting term is composed of two complementary factors: 1) quantity factor, which pays more attention to tail classes, and 2) difficulty factor, which adaptively pays more attention to hard instances in the training process. The difficulty factor can alleviate the over-optimization on tail yet easy instances and under-optimization on head yet hard instances. The synergy of the two factors can not only advance the performance on tail classes even further, but also promote the accuracy on head classes. Unlike previous logit adjusting methods that only concerned about data quantity, ALA Loss tackles the long-tailed problem from a more comprehensive, fine-grained and adaptive perspective. Extensive experimental results show that our method achieves the state-of-the-art performance on challenging recognition benchmarks, including ImageNet-LT, iNaturalist 2018, and Places-LT.



### Back-tracing Representative Points for Voting-based 3D Object Detection in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2104.06114v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06114v2)
- **Published**: 2021-04-13 11:39:42+00:00
- **Updated**: 2021-04-14 06:38:30+00:00
- **Authors**: Bowen Cheng, Lu Sheng, Shaoshuai Shi, Ming Yang, Dong Xu
- **Comment**: CVPR2021
- **Journal**: None
- **Summary**: 3D object detection in point clouds is a challenging vision task that benefits various applications for understanding the 3D visual world. Lots of recent research focuses on how to exploit end-to-end trainable Hough voting for generating object proposals. However, the current voting strategy can only receive partial votes from the surfaces of potential objects together with severe outlier votes from the cluttered backgrounds, which hampers full utilization of the information from the input point clouds. Inspired by the back-tracing strategy in the conventional Hough voting methods, in this work, we introduce a new 3D object detection method, named as Back-tracing Representative Points Network (BRNet), which generatively back-traces the representative points from the vote centers and also revisits complementary seed points around these generated points, so as to better capture the fine local structural features surrounding the potential objects from the raw point clouds. Therefore, this bottom-up and then top-down strategy in our BRNet enforces mutual consistency between the predicted vote centers and the raw surface points and thus achieves more reliable and flexible object localization and class prediction results. Our BRNet is simple but effective, which significantly outperforms the state-of-the-art methods on two large-scale point cloud datasets, ScanNet V2 (+7.5% in terms of mAP@0.50) and SUN RGB-D (+4.7% in terms of mAP@0.50), while it is still lightweight and efficient. Code will be available at https://github.com/cheng052/BRNet.



### Automatic Correction of Internal Units in Generative Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.06118v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06118v1)
- **Published**: 2021-04-13 11:46:45+00:00
- **Updated**: 2021-04-13 11:46:45+00:00
- **Authors**: Ali Tousi, Haedong Jeong, Jiyeon Han, Hwanil Choi, Jaesik Choi
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have shown satisfactory performance in synthetic image generation by devising complex network structure and adversarial training scheme. Even though GANs are able to synthesize realistic images, there exists a number of generated images with defective visual patterns which are known as artifacts. While most of the recent work tries to fix artifact generations by perturbing latent code, few investigate internal units of a generator to fix them. In this work, we devise a method that automatically identifies the internal units generating various types of artifact images. We further propose the sequential correction algorithm which adjusts the generation flow by modifying the detected artifact units to improve the quality of generation while preserving the original outline. Our method outperforms the baseline method in terms of FID-score and shows satisfactory results with human evaluation.



### Contrastive Context-Aware Learning for 3D High-Fidelity Mask Face Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.06148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06148v1)
- **Published**: 2021-04-13 12:48:38+00:00
- **Updated**: 2021-04-13 12:48:38+00:00
- **Authors**: Ajian Liu, Chenxu Zhao, Zitong Yu, Jun Wan, Anyang Su, Xing Liu, Zichang Tan, Sergio Escalera, Junliang Xing, Yanyan Liang, Guodong Guo, Zhen Lei, Stan Z. Li, Du Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Face presentation attack detection (PAD) is essential to secure face recognition systems primarily from high-fidelity mask attacks. Most existing 3D mask PAD benchmarks suffer from several drawbacks: 1) a limited number of mask identities, types of sensors, and a total number of videos; 2) low-fidelity quality of facial masks. Basic deep models and remote photoplethysmography (rPPG) methods achieved acceptable performance on these benchmarks but still far from the needs of practical scenarios. To bridge the gap to real-world applications, we introduce a largescale High-Fidelity Mask dataset, namely CASIA-SURF HiFiMask (briefly HiFiMask). Specifically, a total amount of 54,600 videos are recorded from 75 subjects with 225 realistic masks by 7 new kinds of sensors. Together with the dataset, we propose a novel Contrastive Context-aware Learning framework, namely CCL. CCL is a new training methodology for supervised PAD tasks, which is able to learn by leveraging rich contexts accurately (e.g., subjects, mask material and lighting) among pairs of live faces and high-fidelity mask attacks. Extensive experimental evaluations on HiFiMask and three additional 3D mask datasets demonstrate the effectiveness of our method.



### Visually Informed Binaural Audio Generation without Binaural Audios
- **Arxiv ID**: http://arxiv.org/abs/2104.06162v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2104.06162v1)
- **Published**: 2021-04-13 13:07:33+00:00
- **Updated**: 2021-04-13 13:07:33+00:00
- **Authors**: Xudong Xu, Hang Zhou, Ziwei Liu, Bo Dai, Xiaogang Wang, Dahua Lin
- **Comment**: Accepted by CVPR 2021. Code, models, and demo video are available on
  our webpage: \<https://sheldontsui.github.io/projects/PseudoBinaural>
- **Journal**: None
- **Summary**: Stereophonic audio, especially binaural audio, plays an essential role in immersive viewing environments. Recent research has explored generating visually guided stereophonic audios supervised by multi-channel audio collections. However, due to the requirement of professional recording devices, existing datasets are limited in scale and variety, which impedes the generalization of supervised methods in real-world scenarios. In this work, we propose PseudoBinaural, an effective pipeline that is free of binaural recordings. The key insight is to carefully build pseudo visual-stereo pairs with mono data for training. Specifically, we leverage spherical harmonic decomposition and head-related impulse response (HRIR) to identify the relationship between spatial locations and received binaural audios. Then in the visual modality, corresponding visual cues of the mono data are manually placed at sound source positions to form the pairs. Compared to fully-supervised paradigms, our binaural-recording-free pipeline shows great stability in cross-dataset evaluation and achieves comparable performance under subjective preference. Moreover, combined with binaural recordings, our method is able to further boost the performance of binaural audio generation under supervised settings.



### Fast Hierarchical Games for Image Explanations
- **Arxiv ID**: http://arxiv.org/abs/2104.06164v2
- **DOI**: 10.1109/TPAMI.2022.3189849
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.06164v2)
- **Published**: 2021-04-13 13:11:02+00:00
- **Updated**: 2022-06-09 15:54:11+00:00
- **Authors**: Jacopo Teneggi, Alexandre Luster, Jeremias Sulam
- **Comment**: 20 pages, 8 figures
- **Journal**: None
- **Summary**: As modern complex neural networks keep breaking records and solving harder problems, their predictions also become less and less intelligible. The current lack of interpretability often undermines the deployment of accurate machine learning tools in sensitive settings. In this work, we present a model-agnostic explanation method for image classification based on a hierarchical extension of Shapley coefficients--Hierarchical Shap (h-Shap)--that resolves some of the limitations of current approaches. Unlike other Shapley-based explanation methods, h-Shap is scalable and can be computed without the need of approximation. Under certain distributional assumptions, such as those common in multiple instance learning, h-Shap retrieves the exact Shapley coefficients with an exponential improvement in computational complexity. We compare our hierarchical approach with popular Shapley-based and non-Shapley-based methods on a synthetic dataset, a medical imaging scenario, and a general computer vision problem, showing that h-Shap outperforms the state of the art in both accuracy and runtime. Code and experiments are made publicly available.



### PHI-MVS: Plane Hypothesis Inference Multi-view Stereo for Large-Scale Scene Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2104.06165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06165v1)
- **Published**: 2021-04-13 13:16:00+00:00
- **Updated**: 2021-04-13 13:16:00+00:00
- **Authors**: Shang Sun, Yunan Zheng, Xuelei Shi, Zhenyu Xu, Yiguang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: PatchMatch based Multi-view Stereo (MVS) algorithms have achieved great success in large-scale scene reconstruction tasks. However, reconstruction of texture-less planes often fails as similarity measurement methods may become ineffective on these regions. Thus, a new plane hypothesis inference strategy is proposed to handle the above issue. The procedure consists of two steps: First, multiple plane hypotheses are generated using filtered initial depth maps on regions that are not successfully recovered; Second, depth hypotheses are selected using Markov Random Field (MRF). The strategy can significantly improve the completeness of reconstruction results with only acceptable computing time increasing. Besides, a new acceleration scheme similar to dilated convolution can speed up the depth map estimating process with only a slight influence on the reconstruction. We integrated the above ideas into a new MVS pipeline, Plane Hypothesis Inference Multi-view Stereo (PHI-MVS). The result of PHI-MVS is validated on ETH3D public benchmarks, and it demonstrates competing performance against the state-of-the-art.



### Towards Fast and Accurate Real-World Depth Super-Resolution: Benchmark Dataset and Baseline
- **Arxiv ID**: http://arxiv.org/abs/2104.06174v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.06174v1)
- **Published**: 2021-04-13 13:27:26+00:00
- **Updated**: 2021-04-13 13:27:26+00:00
- **Authors**: Lingzhi He, Hongguang Zhu, Feng Li, Huihui Bai, Runmin Cong, Chunjie Zhang, Chunyu Lin, Meiqin Liu, Yao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Depth maps obtained by commercial depth sensors are always in low-resolution, making it difficult to be used in various computer vision tasks. Thus, depth map super-resolution (SR) is a practical and valuable task, which upscales the depth map into high-resolution (HR) space. However, limited by the lack of real-world paired low-resolution (LR) and HR depth maps, most existing methods use downsampling to obtain paired training samples. To this end, we first construct a large-scale dataset named "RGB-D-D", which can greatly promote the study of depth map SR and even more depth-related real-world tasks. The "D-D" in our dataset represents the paired LR and HR depth maps captured from mobile phone and Lucid Helios respectively ranging from indoor scenes to challenging outdoor scenes. Besides, we provide a fast depth map super-resolution (FDSR) baseline, in which the high-frequency component adaptively decomposed from RGB image to guide the depth map SR. Extensive experiments on existing public datasets demonstrate the effectiveness and efficiency of our network compared with the state-of-the-art methods. Moreover, for the real-world LR depth maps, our algorithm can produce more accurate HR depth maps with clearer boundaries and to some extent correct the depth value errors.



### Lucas-Kanade Reloaded: End-to-End Super-Resolution from Raw Image Bursts
- **Arxiv ID**: http://arxiv.org/abs/2104.06191v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06191v2)
- **Published**: 2021-04-13 13:39:43+00:00
- **Updated**: 2021-08-23 08:57:19+00:00
- **Authors**: Bruno Lecouat, Jean Ponce, Julien Mairal
- **Comment**: None
- **Journal**: ICCV 2021
- **Summary**: This presentation addresses the problem of reconstructing a high-resolution image from multiple lower-resolution snapshots captured from slightly different viewpoints in space and time. Key challenges for solving this problem include (i) aligning the input pictures with sub-pixel accuracy, (ii) handling raw (noisy) images for maximal faithfulness to native camera data, and (iii) designing/learning an image prior (regularizer) well suited to the task. We address these three challenges with a hybrid algorithm building on the insight from Wronski et al. that aliasing is an ally in this setting, with parameters that can be learned end to end, while retaining the interpretability of classical approaches to inverse problems. The effectiveness of our approach is demonstrated on synthetic and real image bursts, setting a new state of the art on several benchmarks and delivering excellent qualitative results on real raw bursts captured by smartphones and prosumer cameras.



### Anomaly Detection in Image Datasets Using Convolutional Neural Networks, Center Loss, and Mahalanobis Distance
- **Arxiv ID**: http://arxiv.org/abs/2104.06193v1
- **DOI**: 10.1109/USBEREIT51232.2021.9455004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06193v1)
- **Published**: 2021-04-13 13:44:03+00:00
- **Updated**: 2021-04-13 13:44:03+00:00
- **Authors**: Garnik Vareldzhan, Kirill Yurkov, Konstantin Ushenin
- **Comment**: None
- **Journal**: None
- **Summary**: User activities generate a significant number of poor-quality or irrelevant images and data vectors that cannot be processed in the main data processing pipeline or included in the training dataset. Such samples can be found with manual analysis by an expert or with anomalous detection algorithms. There are several formal definitions for the anomaly samples. For neural networks, the anomalous is usually defined as out-of-distribution samples. This work proposes methods for supervised and semi-supervised detection of out-of-distribution samples in image datasets. Our approach extends a typical neural network that solves the image classification problem. Thus, one neural network after extension can solve image classification and anomalous detection problems simultaneously. Proposed methods are based on the center loss and its effect on a deep feature distribution in a last hidden layer of the neural network. This paper provides an analysis of the proposed methods for the LeNet and EfficientNet-B0 on the MNIST and ImageNet-30 datasets.



### UAV-ReID: A Benchmark on Unmanned Aerial Vehicle Re-identification in Video Imagery
- **Arxiv ID**: http://arxiv.org/abs/2104.06219v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.06219v3)
- **Published**: 2021-04-13 14:13:09+00:00
- **Updated**: 2021-12-02 13:42:01+00:00
- **Authors**: Daniel Organisciak, Matthew Poyser, Aishah Alsehaim, Shanfeng Hu, Brian K. S. Isaac-Medina, Toby P. Breckon, Hubert P. H. Shum
- **Comment**: None
- **Journal**: None
- **Summary**: As unmanned aerial vehicles (UAVs) become more accessible with a growing range of applications, the potential risk of UAV disruption increases. Recent development in deep learning allows vision-based counter-UAV systems to detect and track UAVs with a single camera. However, the coverage of a single camera is limited, necessitating the need for multicamera configurations to match UAVs across cameras - a problem known as re-identification (reID). While there has been extensive research on person and vehicle reID to match objects across time and viewpoints, to the best of our knowledge, there has been no research in UAV reID. UAVs are challenging to re-identify: they are much smaller than pedestrians and vehicles and they are often detected in the air so appear at a greater range of angles. Because no UAV data sets currently use multiple cameras, we propose the first new UAV re-identification data set, UAV-reID, that facilitates the development of machine learning solutions in this emerging area. UAV-reID has two settings: Temporally-Near to evaluate performance across views to assist tracking frameworks, and Big-to-Small to evaluate reID performance across scale and to allow early reID when UAVs are detected from a long distance. We conduct a benchmark study by extensively evaluating different reID backbones and loss functions. We demonstrate that with the right setup, deep networks are powerful enough to learn good representations for UAVs, achieving 81.9% mAP on the Temporally-Near setting and 46.5% on the challenging Big-to-Small setting. Furthermore, we find that vision transformers are the most robust to extreme variance of scale.



### Latent Correlation Representation Learning for Brain Tumor Segmentation with Missing MRI Modalities
- **Arxiv ID**: http://arxiv.org/abs/2104.06231v2
- **DOI**: 10.1109/TIP.2021.3070752
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06231v2)
- **Published**: 2021-04-13 14:21:09+00:00
- **Updated**: 2021-04-20 13:51:09+00:00
- **Authors**: Tongxue Zhou, Stéphane Canu, Pierre Vera, Su Ruan
- **Comment**: 12 pages, 10 figures, accepted by IEEE Transactions on Image
  Processing (8 April 2021). arXiv admin note: text overlap with
  arXiv:2003.08870, arXiv:2102.03111
- **Journal**: IEEE Transactions on Image Processing On page(s): 4263-4274 Print
  ISSN: 1057-7149 Online ISSN: 1941-0042
- **Summary**: Magnetic Resonance Imaging (MRI) is a widely used imaging technique to assess brain tumor. Accurately segmenting brain tumor from MR images is the key to clinical diagnostics and treatment planning. In addition, multi-modal MR images can provide complementary information for accurate brain tumor segmentation. However, it's common to miss some imaging modalities in clinical practice. In this paper, we present a novel brain tumor segmentation algorithm with missing modalities. Since it exists a strong correlation between multi-modalities, a correlation model is proposed to specially represent the latent multi-source correlation. Thanks to the obtained correlation representation, the segmentation becomes more robust in the case of missing modality. First, the individual representation produced by each encoder is used to estimate the modality independent parameter. Then, the correlation model transforms all the individual representations to the latent multi-source correlation representations. Finally, the correlation representations across modalities are fused via attention mechanism into a shared representation to emphasize the most important features for segmentation. We evaluate our model on BraTS 2018 and BraTS 2019 dataset, it outperforms the current state-of-the-art methods and produces robust results when one or more modalities are missing.



### Learning to recover orientations from projections in single-particle cryo-EM
- **Arxiv ID**: http://arxiv.org/abs/2104.06237v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.06237v1)
- **Published**: 2021-04-13 14:31:37+00:00
- **Updated**: 2021-04-13 14:31:37+00:00
- **Authors**: Jelena Banjac, Laurène Donati, Michaël Defferrard
- **Comment**: None
- **Journal**: None
- **Summary**: A major challenge in single-particle cryo-electron microscopy (cryo-EM) is that the orientations adopted by the 3D particles prior to imaging are unknown; yet, this knowledge is essential for high-resolution reconstruction. We present a method to recover these orientations directly from the acquired set of 2D projections. Our approach consists of two steps: (i) the estimation of distances between pairs of projections, and (ii) the recovery of the orientation of each projection from these distances. In step (i), pairwise distances are estimated by a Siamese neural network trained on synthetic cryo-EM projections from resolved bio-structures. In step (ii), orientations are recovered by minimizing the difference between the distances estimated from the projections and the distances induced by the recovered orientations. We evaluated the method on synthetic cryo-EM datasets. Current results demonstrate that orientations can be accurately recovered from projections that are shifted and corrupted with a high level of noise. The accuracy of the recovery depends on the accuracy of the distance estimator. While not yet deployed in a real experimental setup, the proposed method offers a novel learning-based take on orientation recovery in SPA. Our code is available at https://github.com/JelenaBanjac/protein-reconstruction



### A State-of-the-art Survey of Artificial Neural Networks for Whole-slide Image Analysis:from Popular Convolutional Neural Networks to Potential Visual Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.06243v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.06243v3)
- **Published**: 2021-04-13 14:39:33+00:00
- **Updated**: 2022-02-26 08:26:00+00:00
- **Authors**: Xintong Li, Weiming Hu, Chen Li, Tao Jiang, Hongzan Sun, Xiaoyan Li, Xinyu Huang, Marcin Grzegorzek
- **Comment**: 22 pages, 38 figures. arXiv admin note: substantial text overlap with
  arXiv:2102.10553
- **Journal**: None
- **Summary**: To increase the objectivity and accuracy of pathologists' work, artificial neural network(ANN) methods have been generally needed in the segmentation, classification, and detection of histopathological WSI. In this paper, WSI analysis methods based on ANN are reviewed. Firstly, the development status of WSI and ANN methods is introduced. Secondly, we summarize the common ANN methods. Next, we discuss publicly available WSI datasets and evaluation metrics. These ANN architectures for WSI processing are divided into classical neural networks and deep neural networks(DNNs) and then analyzed. Finally, the application prospect of the analytical method in this field is discussed. The important potential method is Visual Transformers.



### Very Lightweight Photo Retouching Network with Conditional Sequential Modulation
- **Arxiv ID**: http://arxiv.org/abs/2104.06279v2
- **DOI**: 10.1109/TMM.2022.3179904
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.06279v2)
- **Published**: 2021-04-13 15:11:02+00:00
- **Updated**: 2022-06-05 17:04:16+00:00
- **Authors**: Yihao Liu, Jingwen He, Xiangyu Chen, Zhengwen Zhang, Hengyuan Zhao, Chao Dong, Yu Qiao
- **Comment**: Accepted by TMM. arXiv admin note: substantial text overlap with
  arXiv:2009.10390
- **Journal**: None
- **Summary**: Photo retouching aims at improving the aesthetic visual quality of images that suffer from photographic defects, especially for poor contrast, over/under exposure, and inharmonious saturation. In practice, photo retouching can be accomplished by a series of image processing operations. As most commonly-used retouching operations are pixel-independent, i.e., the manipulation on one pixel is uncorrelated with its neighboring pixels, we can take advantage of this property and design a specialized algorithm for efficient global photo retouching. We analyze these global operations and find that they can be mathematically formulated by a Multi-Layer Perceptron (MLP). Based on this observation, we propose an extremely lightweight framework -- Conditional Sequential Retouching Network (CSRNet). Benefiting from the utilization of $1\times1$ convolution, CSRNet only contains less than 37K trainable parameters, which are orders of magnitude smaller than existing learning-based methods. Experiments show that our method achieves state-of-the-art performance on the benchmark MIT-Adobe FiveK dataset quantitively and qualitatively. In addition to achieve global photo retouching, the proposed framework can be easily extended to learn local enhancement effects. The extended model, namely CSRNet-L, also achieves competitive results in various local enhancement tasks. Codes are available at https://github.com/lyh-18/CSRNet.



### Neuro-Symbolic VQA: A review from the perspective of AGI desiderata
- **Arxiv ID**: http://arxiv.org/abs/2104.06365v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06365v1)
- **Published**: 2021-04-13 17:23:19+00:00
- **Updated**: 2021-04-13 17:23:19+00:00
- **Authors**: Ian Berlot-Attwell
- **Comment**: None
- **Journal**: None
- **Summary**: An ultimate goal of the AI and ML fields is artificial general intelligence (AGI); although such systems remain science fiction, various models exhibit aspects of AGI. In this work, we look at neuro-symbolic (NS)approaches to visual question answering (VQA) from the perspective of AGI desiderata. We see how well these systems meet these desiderata, and how the desiderata often pull the scientist in opposing directions. It is my hope that through this work we can temper model evaluation on benchmarks with a discussion of the properties of these systems and their potential for future extension.



### ShapeMOD: Macro Operation Discovery for 3D Shape Programs
- **Arxiv ID**: http://arxiv.org/abs/2104.06392v3
- **DOI**: 10.1145/3450626.3459821
- **Categories**: **cs.GR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.06392v3)
- **Published**: 2021-04-13 17:54:03+00:00
- **Updated**: 2022-03-22 21:09:06+00:00
- **Authors**: R. Kenny Jones, David Charatan, Paul Guerrero, Niloy J. Mitra, Daniel Ritchie
- **Comment**: SIGGRAPH 2021. Project Page: https://rkjones4.github.io/shapeMOD.html
- **Journal**: None
- **Summary**: A popular way to create detailed yet easily controllable 3D shapes is via procedural modeling, i.e. generating geometry using programs. Such programs consist of a series of instructions along with their associated parameter values. To fully realize the benefits of this representation, a shape program should be compact and only expose degrees of freedom that allow for meaningful manipulation of output geometry. One way to achieve this goal is to design higher-level macro operators that, when executed, expand into a series of commands from the base shape modeling language. However, manually authoring such macros, much like shape programs themselves, is difficult and largely restricted to domain experts. In this paper, we present ShapeMOD, an algorithm for automatically discovering macros that are useful across large datasets of 3D shape programs. ShapeMOD operates on shape programs expressed in an imperative, statement-based language. It is designed to discover macros that make programs more compact by minimizing the number of function calls and free parameters required to represent an input shape collection. We run ShapeMOD on multiple collections of programs expressed in a domain-specific language for 3D shape structures. We show that it automatically discovers a concise set of macros that abstract out common structural and parametric patterns that generalize over large shape collections. We also demonstrate that the macros found by ShapeMOD improve performance on downstream tasks including shape generative modeling and inferring programs from point clouds. Finally, we conduct a user study that indicates that ShapeMOD's discovered macros make interactive shape editing more efficient.



### All you need are a few pixels: semantic segmentation with PixelPick
- **Arxiv ID**: http://arxiv.org/abs/2104.06394v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06394v2)
- **Published**: 2021-04-13 17:55:33+00:00
- **Updated**: 2021-04-15 17:04:20+00:00
- **Authors**: Gyungin Shin, Weidi Xie, Samuel Albanie
- **Comment**: 14 pages, 8 figures; references added
- **Journal**: None
- **Summary**: A central challenge for the task of semantic segmentation is the prohibitive cost of obtaining dense pixel-level annotations to supervise model training. In this work, we show that in order to achieve a good level of segmentation performance, all you need are a few well-chosen pixel labels. We make the following contributions: (i) We investigate the novel semantic segmentation setting in which labels are supplied only at sparse pixel locations, and show that deep neural networks can use a handful of such labels to good effect; (ii) We demonstrate how to exploit this phenomena within an active learning framework, termed PixelPick, to radically reduce labelling cost, and propose an efficient "mouse-free" annotation strategy to implement our approach; (iii) We conduct extensive experiments to study the influence of annotation diversity under a fixed budget, model pretraining, model capacity and the sampling mechanism for picking pixels in this low annotation regime; (iv) We provide comparisons to the existing state of the art in semantic segmentation with active learning, and demonstrate comparable performance with up to two orders of magnitude fewer pixel annotations on the CamVid, Cityscapes and PASCAL VOC 2012 benchmarks; (v) Finally, we evaluate the efficiency of our annotation pipeline and its sensitivity to annotator error to demonstrate its practicality.



### Shape and Material Capture at Home
- **Arxiv ID**: http://arxiv.org/abs/2104.06397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06397v1)
- **Published**: 2021-04-13 17:57:34+00:00
- **Updated**: 2021-04-13 17:57:34+00:00
- **Authors**: Daniel Lichy, Jiaye Wu, Soumyadip Sengupta, David W. Jacobs
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: In this paper, we present a technique for estimating the geometry and reflectance of objects using only a camera, flashlight, and optionally a tripod. We propose a simple data capture technique in which the user goes around the object, illuminating it with a flashlight and capturing only a few images. Our main technical contribution is the introduction of a recursive neural architecture, which can predict geometry and reflectance at 2^{k}*2^{k} resolution given an input image at 2^{k}*2^{k} and estimated geometry and reflectance from the previous step at 2^{k-1}*2^{k-1}. This recursive architecture, termed RecNet, is trained with 256x256 resolution but can easily operate on 1024x1024 images during inference. We show that our method produces more accurate surface normal and albedo, especially in regions of specular highlights and cast shadows, compared to previous approaches, given three or fewer input images. For the video and code, please visit the project website http://dlichy.github.io/ShapeAndMaterialAtHome/.



### Co-Scale Conv-Attentional Image Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.06399v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2104.06399v2)
- **Published**: 2021-04-13 17:58:29+00:00
- **Updated**: 2021-08-26 17:54:30+00:00
- **Authors**: Weijian Xu, Yifan Xu, Tyler Chang, Zhuowen Tu
- **Comment**: Accepted to ICCV 2021 (Oral)
- **Journal**: None
- **Summary**: In this paper, we present Co-scale conv-attentional image Transformers (CoaT), a Transformer-based image classifier equipped with co-scale and conv-attentional mechanisms. First, the co-scale mechanism maintains the integrity of Transformers' encoder branches at individual scales, while allowing representations learned at different scales to effectively communicate with each other; we design a series of serial and parallel blocks to realize the co-scale mechanism. Second, we devise a conv-attentional mechanism by realizing a relative position embedding formulation in the factorized attention module with an efficient convolution-like implementation. CoaT empowers image Transformers with enriched multi-scale and contextual modeling capabilities. On ImageNet, relatively small CoaT models attain superior classification results compared with similar-sized convolutional neural networks and image/vision Transformers. The effectiveness of CoaT's backbone is also illustrated on object detection and instance segmentation, demonstrating its applicability to downstream computer vision tasks.



### Self-supervised object detection from audio-visual correspondence
- **Arxiv ID**: http://arxiv.org/abs/2104.06401v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06401v2)
- **Published**: 2021-04-13 17:59:03+00:00
- **Updated**: 2022-07-09 18:20:19+00:00
- **Authors**: Triantafyllos Afouras, Yuki M. Asano, Francois Fagan, Andrea Vedaldi, Florian Metze
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: We tackle the problem of learning object detectors without supervision. Differently from weakly-supervised object detection, we do not assume image-level class labels. Instead, we extract a supervisory signal from audio-visual data, using the audio component to "teach" the object detector. While this problem is related to sound source localisation, it is considerably harder because the detector must classify the objects by type, enumerate each instance of the object, and do so even when the object is silent. We tackle this problem by first designing a self-supervised framework with a contrastive objective that jointly learns to classify and localise objects. Then, without using any supervision, we simply use these self-supervised labels and boxes to train an image-based object detector. With this, we outperform previous unsupervised and weakly-supervised detectors for the task of object detection and sound source localization. We also show that we can align this detector to ground-truth classes with as little as one label per pseudo-class, and show how our method can learn to detect generic objects that go beyond instruments, such as airplanes and cats.



### DropLoss for Long-Tail Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.06402v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06402v2)
- **Published**: 2021-04-13 17:59:22+00:00
- **Updated**: 2021-04-17 15:52:56+00:00
- **Authors**: Ting-I Hsieh, Esther Robb, Hwann-Tzong Chen, Jia-Bin Huang
- **Comment**: Code at https://github.com/timy90022/DropLoss
- **Journal**: AAAI 2021
- **Summary**: Long-tailed class distributions are prevalent among the practical applications of object detection and instance segmentation. Prior work in long-tail instance segmentation addresses the imbalance of losses between rare and frequent categories by reducing the penalty for a model incorrectly predicting a rare class label. We demonstrate that the rare categories are heavily suppressed by correct background predictions, which reduces the probability for all foreground categories with equal weight. Due to the relative infrequency of rare categories, this leads to an imbalance that biases towards predicting more frequent categories. Based on this insight, we develop DropLoss -- a novel adaptive loss to compensate for this imbalance without a trade-off between rare and frequent categories. With this loss, we show state-of-the-art mAP across rare, common, and frequent categories on the LVIS dataset.



### Lite-HRNet: A Lightweight High-Resolution Network
- **Arxiv ID**: http://arxiv.org/abs/2104.06403v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06403v1)
- **Published**: 2021-04-13 17:59:31+00:00
- **Updated**: 2021-04-13 17:59:31+00:00
- **Authors**: Changqian Yu, Bin Xiao, Changxin Gao, Lu Yuan, Lei Zhang, Nong Sang, Jingdong Wang
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: We present an efficient high-resolution network, Lite-HRNet, for human pose estimation. We start by simply applying the efficient shuffle block in ShuffleNet to HRNet (high-resolution network), yielding stronger performance over popular lightweight networks, such as MobileNet, ShuffleNet, and Small HRNet.   We find that the heavily-used pointwise (1x1) convolutions in shuffle blocks become the computational bottleneck. We introduce a lightweight unit, conditional channel weighting, to replace costly pointwise (1x1) convolutions in shuffle blocks. The complexity of channel weighting is linear w.r.t the number of channels and lower than the quadratic time complexity for pointwise convolutions. Our solution learns the weights from all the channels and over multiple resolutions that are readily available in the parallel branches in HRNet. It uses the weights as the bridge to exchange information across channels and resolutions, compensating the role played by the pointwise (1x1) convolution. Lite-HRNet demonstrates superior results on human pose estimation over popular lightweight networks. Moreover, Lite-HRNet can be easily applied to semantic segmentation task in the same lightweight manner. The code and models have been publicly available at https://github.com/HRNet/Lite-HRNet.



### Few-shot Image Generation via Cross-domain Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2104.06820v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.06820v1)
- **Published**: 2021-04-13 17:59:35+00:00
- **Updated**: 2021-04-13 17:59:35+00:00
- **Authors**: Utkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A. Efros, Yong Jae Lee, Eli Shechtman, Richard Zhang
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Training generative models, such as GANs, on a target domain containing limited examples (e.g., 10) can easily result in overfitting. In this work, we seek to utilize a large source domain for pretraining and transfer the diversity information from source to target. We propose to preserve the relative similarities and differences between instances in the source via a novel cross-domain distance consistency loss. To further reduce overfitting, we present an anchor-based strategy to encourage different levels of realism over different regions in the latent space. With extensive results in both photorealistic and non-photorealistic domains, we demonstrate qualitatively and quantitatively that our few-shot model automatically discovers correspondences between source and target domains and generates more diverse and realistic images than previous methods.



### Pointly-Supervised Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.06404v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06404v2)
- **Published**: 2021-04-13 17:59:40+00:00
- **Updated**: 2022-06-15 21:07:43+00:00
- **Authors**: Bowen Cheng, Omkar Parkhi, Alexander Kirillov
- **Comment**: CVPR 2022, Oral. Project page: https://bowenc0221.github.io/point-sup
- **Journal**: None
- **Summary**: We propose an embarrassingly simple point annotation scheme to collect weak supervision for instance segmentation. In addition to bounding boxes, we collect binary labels for a set of points uniformly sampled inside each bounding box. We show that the existing instance segmentation models developed for full mask supervision can be seamlessly trained with point-based supervision collected via our scheme. Remarkably, Mask R-CNN trained on COCO, PASCAL VOC, Cityscapes, and LVIS with only 10 annotated random points per object achieves 94%--98% of its fully-supervised performance, setting a strong baseline for weakly-supervised instance segmentation. The new point annotation scheme is approximately 5 times faster than annotating full object masks, making high-quality instance segmentation more accessible in practice.   Inspired by the point-based annotation form, we propose a modification to PointRend instance segmentation module. For each object, the new architecture, called Implicit PointRend, generates parameters for a function that makes the final point-level mask prediction. Implicit PointRend is more straightforward and uses a single point-level mask loss. Our experiments show that the new module is more suitable for the point-based supervision.



### BARF: Bundle-Adjusting Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2104.06405v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.06405v2)
- **Published**: 2021-04-13 17:59:51+00:00
- **Updated**: 2021-08-19 08:37:45+00:00
- **Authors**: Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, Simon Lucey
- **Comment**: Accepted to ICCV 2021 as oral presentation (project page & code:
  https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF)
- **Journal**: None
- **Summary**: Neural Radiance Fields (NeRF) have recently gained a surge of interest within the computer vision community for its power to synthesize photorealistic novel views of real-world scenes. One limitation of NeRF, however, is its requirement of accurate camera poses to learn the scene representations. In this paper, we propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from imperfect (or even unknown) camera poses -- the joint problem of learning neural 3D representations and registering camera frames. We establish a theoretical connection to classical image alignment and show that coarse-to-fine registration is also applicable to NeRF. Furthermore, we show that na\"ively applying positional encoding in NeRF has a negative impact on registration with a synthesis-based objective. Experiments on synthetic and real-world data show that BARF can effectively optimize the neural scene representations and resolve large camera pose misalignment at the same time. This enables view synthesis and localization of video sequences from unknown camera poses, opening up new avenues for visual localization systems (e.g. SLAM) and potential applications for dense 3D mapping and reconstruction.



### Single Image Depth Estimation: An Overview
- **Arxiv ID**: http://arxiv.org/abs/2104.06456v1
- **DOI**: 10.1016/j.dsp.2022.103441
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06456v1)
- **Published**: 2021-04-13 18:58:37+00:00
- **Updated**: 2021-04-13 18:58:37+00:00
- **Authors**: Alican Mertan, Damien Jade Duff, Gozde Unal
- **Comment**: This is a preprint
- **Journal**: None
- **Summary**: We review solutions to the problem of depth estimation, arguably the most important subtask in scene understanding. We focus on the single image depth estimation problem. Due to its properties, the single image depth estimation problem is currently best tackled with machine learning methods, most successfully with convolutional neural networks. We provide an overview of the field by examining key works. We examine non-deep learning approaches that mostly predate deep learning and utilize hand-crafted features and assumptions, and more recent works that mostly use deep learning techniques. The single image depth estimation problem is tackled first in a supervised fashion with absolute or relative depth information acquired from human or sensor-labeled data, or in an unsupervised way using unlabelled stereo images or video datasets. We also study multitask approaches that combine the depth estimation problem with related tasks such as semantic segmentation and surface normal estimation. Finally, we discuss investigations into the mechanisms, principles, and failure cases of contemporary solutions.



### Learning to Jointly Deblur, Demosaick and Denoise Raw Images
- **Arxiv ID**: http://arxiv.org/abs/2104.06459v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06459v1)
- **Published**: 2021-04-13 19:02:59+00:00
- **Updated**: 2021-04-13 19:02:59+00:00
- **Authors**: Thomas Eboli, Jian Sun, Jean Ponce
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of non-blind deblurring and demosaicking of noisy raw images. We adapt an existing learning-based approach to RGB image deblurring to handle raw images by introducing a new interpretable module that jointly demosaicks and deblurs them. We train this model on RGB images converted into raw ones following a realistic invertible camera pipeline. We demonstrate the effectiveness of this model over two-stage approaches stacking demosaicking and deblurring modules on quantitive benchmarks. We also apply our approach to remove a camera's inherent blur (its color-dependent point-spread function) from real images, in essence deblurring sharp images.



### Learning Log-Determinant Divergences for Positive Definite Matrices
- **Arxiv ID**: http://arxiv.org/abs/2104.06461v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06461v2)
- **Published**: 2021-04-13 19:09:43+00:00
- **Updated**: 2021-12-22 14:22:12+00:00
- **Authors**: Anoop Cherian, Panagiotis Stanitsas, Jue Wang, Mehrtash Harandi, Vassilios Morellas, Nikolaos Papanikolopoulos
- **Comment**: Accepted at Trans. PAMI (extended version of ICCV 2017 paper). arXiv
  admin note: substantial text overlap with arXiv:1708.01741
- **Journal**: None
- **Summary**: Representations in the form of Symmetric Positive Definite (SPD) matrices have been popularized in a variety of visual learning applications due to their demonstrated ability to capture rich second-order statistics of visual data. There exist several similarity measures for comparing SPD matrices with documented benefits. However, selecting an appropriate measure for a given problem remains a challenge and in most cases, is the result of a trial-and-error process. In this paper, we propose to learn similarity measures in a data-driven manner. To this end, we capitalize on the \alpha\beta-log-det divergence, which is a meta-divergence parametrized by scalars \alpha and \beta, subsuming a wide family of popular information divergences on SPD matrices for distinct and discrete values of these parameters. Our key idea is to cast these parameters in a continuum and learn them from data. We systematically extend this idea to learn vector-valued parameters, thereby increasing the expressiveness of the underlying non-linear measure. We conjoin the divergence learning problem with several standard tasks in machine learning, including supervised discriminative dictionary learning and unsupervised SPD matrix clustering. We present Riemannian gradient descent schemes for optimizing our formulations efficiently, and show the usefulness of our method on eight standard computer vision tasks.



### ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2104.06468v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06468v1)
- **Published**: 2021-04-13 19:18:19+00:00
- **Updated**: 2021-04-13 19:18:19+00:00
- **Authors**: Junyu Chen, Yufan He, Eric C. Frey, Ye Li, Yong Du
- **Comment**: None
- **Journal**: None
- **Summary**: In the last decade, convolutional neural networks (ConvNets) have dominated and achieved state-of-the-art performances in a variety of medical imaging applications. However, the performances of ConvNets are still limited by lacking the understanding of long-range spatial relations in an image. The recently proposed Vision Transformer (ViT) for image classification uses a purely self-attention-based model that learns long-range spatial relations to focus on the relevant parts of an image. Nevertheless, ViT emphasizes the low-resolution features because of the consecutive downsamplings, result in a lack of detailed localization information, making it unsuitable for image registration. Recently, several ViT-based image segmentation methods have been combined with ConvNets to improve the recovery of detailed localization information. Inspired by them, we present ViT-V-Net, which bridges ViT and ConvNet to provide volumetric medical image registration. The experimental results presented here demonstrate that the proposed architecture achieves superior performance to several top-performing registration methods.



### Incremental Multi-Target Domain Adaptation for Object Detection with Efficient Domain Transfer
- **Arxiv ID**: http://arxiv.org/abs/2104.06476v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06476v4)
- **Published**: 2021-04-13 19:35:54+00:00
- **Updated**: 2022-05-11 18:52:37+00:00
- **Authors**: Le Thanh Nguyen-Meidine, Madhu Kiran, Marco Pedersoli, Jose Dolz, Louis-Antoine Blais-Morin, Eric Granger
- **Comment**: Accepted for Journal of Pattern Recognition. Code available at
  https://github.com/Natlem/M-HTCN
- **Journal**: None
- **Summary**: Recent advances in unsupervised domain adaptation have significantly improved the recognition accuracy of CNNs by alleviating the domain shift between (labeled) source and (unlabeled) target data distributions. While the problem of single-target domain adaptation (STDA) for object detection has recently received much attention, multi-target domain adaptation (MTDA) remains largely unexplored, despite its practical relevance in several real-world applications, such as multi-camera video surveillance. Compared to the STDA problem that may involve large domain shifts between complex source and target distributions, MTDA faces additional challenges, most notably the computational requirements and catastrophic forgetting of previously-learned targets, which can depend on the order of target adaptations. STDA for detection can be applied to MTDA by adapting one model per target, or one common model with a mixture of data from target domains. However, these approaches are either costly or inaccurate. The only state-of-art MTDA method specialized for detection learns targets incrementally, one target at a time, and mitigates the loss of knowledge by using a duplicated detection model for knowledge distillation, which is computationally expensive and does not scale well to many domains. In this paper, we introduce an efficient approach for incremental learning that generalizes well to multiple target domains. Our MTDA approach is more suitable for real-world applications since it allows updating the detection model incrementally, without storing data from previous-learned target domains, nor retraining when a new target domain becomes available. Our proposed method, MTDA-DTM, achieved the highest level of detection accuracy compared against state-of-the-art approaches on several MTDA detection benchmarks and Wildtrack, a benchmark for multi-camera pedestrian detection.



### Machine-learned 3D Building Vectorization from Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2104.06485v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.06485v1)
- **Published**: 2021-04-13 19:57:30+00:00
- **Updated**: 2021-04-13 19:57:30+00:00
- **Authors**: Yi Wang, Stefano Zorzi, Ksenia Bittner
- **Comment**: Accepted to CVPR workshop (EarthVision 2021)
- **Journal**: None
- **Summary**: We propose a machine learning based approach for automatic 3D building reconstruction and vectorization. Taking a single-channel photogrammetric digital surface model (DSM) and panchromatic (PAN) image as input, we first filter out non-building objects and refine the building shapes of input DSM with a conditional generative adversarial network (cGAN). The refined DSM and the input PAN image are then used through a semantic segmentation network to detect edges and corners of building roofs. Later, a set of vectorization algorithms are proposed to build roof polygons. Finally, the height information from the refined DSM is added to the polygons to obtain a fully vectorized level of detail (LoD)-2 building model. We verify the effectiveness of our method on large-scale satellite images, where we obtain state-of-the-art performance.



### DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort
- **Arxiv ID**: http://arxiv.org/abs/2104.06490v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.06490v2)
- **Published**: 2021-04-13 20:08:29+00:00
- **Updated**: 2021-04-20 00:27:10+00:00
- **Authors**: Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, Sanja Fidler
- **Comment**: Accepted to CVPR 2021 as an Oral paper. Webpage:
  https://nv-tlabs.github.io/datasetGAN/
- **Journal**: None
- **Summary**: We introduce DatasetGAN: an automatic procedure to generate massive datasets of high-quality semantically segmented images requiring minimal human effort. Current deep networks are extremely data-hungry, benefiting from training on large-scale datasets, which are time consuming to annotate. Our method relies on the power of recent GANs to generate realistic images. We show how the GAN latent code can be decoded to produce a semantic segmentation of the image. Training the decoder only needs a few labeled examples to generalize to the rest of the latent space, resulting in an infinite annotated dataset generator! These generated datasets can then be used for training any computer vision architecture just as real datasets are. As only a few images need to be manually segmented, it becomes possible to annotate images in extreme detail and generate datasets with rich object and part segmentations. To showcase the power of our approach, we generated datasets for 7 image segmentation tasks which include pixel-level labels for 34 human face parts, and 32 car parts. Our approach outperforms all semi-supervised baselines significantly and is on par with fully supervised methods, which in some cases require as much as 100x more annotated data as our method.



### Holistic Guidance for Occluded Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2104.06524v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.06524v2)
- **Published**: 2021-04-13 21:50:29+00:00
- **Updated**: 2023-07-22 13:24:34+00:00
- **Authors**: Madhu Kiran, R Gnana Praveen, Le Thanh Nguyen-Meidine, Soufiane Belharbi, Louis-Antoine Blais-Morin, Eric Granger
- **Comment**: British Machine Vision Conference (BMVC) 2021
- **Journal**: None
- **Summary**: In real-world video surveillance applications, person re-identification (ReID) suffers from the effects of occlusions and detection errors. Despite recent advances, occlusions continue to corrupt the features extracted by state-of-art CNN backbones, and thereby deteriorate the accuracy of ReID systems. To address this issue, methods in the literature use an additional costly process such as pose estimation, where pose maps provide supervision to exclude occluded regions. In contrast, we introduce a novel Holistic Guidance (HG) method that relies only on person identity labels, and on the distribution of pairwise matching distances of datasets to alleviate the problem of occlusion, without requiring additional supervision. Hence, our proposed student-teacher framework is trained to address the occlusion problem by matching the distributions of between- and within-class distances (DCDs) of occluded samples with that of holistic (non-occluded) samples, thereby using the latter as a soft labeled reference to learn well separated DCDs. This approach is supported by our empirical study where the distribution of between- and within-class distances between images have more overlap in occluded than holistic datasets. In particular, features extracted from both datasets are jointly learned using the student model to produce an attention map that allows separating visible regions from occluded ones. In addition to this, a joint generative-discriminative backbone is trained with a denoising autoencoder, allowing the system to self-recover from occlusions. Extensive experiments on several challenging public datasets indicate that the proposed approach can outperform state-of-the-art methods on both occluded and holistic datasets



### Simultaneous Face Hallucination and Translation for Thermal to Visible Face Verification using Axial-GAN
- **Arxiv ID**: http://arxiv.org/abs/2104.06534v2
- **DOI**: 10.1109/IJCB52358.2021.9484353
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.06534v2)
- **Published**: 2021-04-13 22:34:28+00:00
- **Updated**: 2021-08-07 22:57:59+00:00
- **Authors**: Rakhil Immidisetti, Shuowen Hu, Vishal M. Patel
- **Comment**: International Joint Conference on Biometrics (IJCB)
- **Journal**: 2021 IEEE International Joint Conference on Biometrics (IJCB)
- **Summary**: Existing thermal-to-visible face verification approaches expect the thermal and visible face images to be of similar resolution. This is unlikely in real-world long-range surveillance systems, since humans are distant from the cameras. To address this issue, we introduce the task of thermal-to-visible face verification from low-resolution thermal images. Furthermore, we propose Axial-Generative Adversarial Network (Axial-GAN) to synthesize high-resolution visible images for matching. In the proposed approach we augment the GAN framework with axial-attention layers which leverage the recent advances in transformers for modelling long-range dependencies. We demonstrate the effectiveness of the proposed method by evaluating on two different thermal-visible face datasets. When compared to related state-of-the-art works, our results show significant improvements in both image quality and face verification performance, and are also much more efficient.



