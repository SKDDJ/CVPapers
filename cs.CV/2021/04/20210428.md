# Arxiv Papers in cs.CV on 2021-04-28
### ZePHyR: Zero-shot Pose Hypothesis Rating
- **Arxiv ID**: http://arxiv.org/abs/2104.13526v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.13526v2)
- **Published**: 2021-04-28 01:48:39+00:00
- **Updated**: 2021-04-30 04:11:27+00:00
- **Authors**: Brian Okorn, Qiao Gu, Martial Hebert, David Held
- **Comment**: 8 pages, 4 figures. Accepted to ICRA 2021. Brian and Qiao have equal
  contributions
- **Journal**: None
- **Summary**: Pose estimation is a basic module in many robot manipulation pipelines. Estimating the pose of objects in the environment can be useful for grasping, motion planning, or manipulation. However, current state-of-the-art methods for pose estimation either rely on large annotated training sets or simulated data. Further, the long training times for these methods prohibit quick interaction with novel objects. To address these issues, we introduce a novel method for zero-shot object pose estimation in clutter. Our approach uses a hypothesis generation and scoring framework, with a focus on learning a scoring function that generalizes to objects not used for training. We achieve zero-shot generalization by rating hypotheses as a function of unordered point differences. We evaluate our method on challenging datasets with both textured and untextured objects in cluttered scenes and demonstrate that our method significantly outperforms previous methods on this task. We also demonstrate how our system can be used by quickly scanning and building a model of a novel object, which can immediately be used by our method for pose estimation. Our work allows users to estimate the pose of novel objects without requiring any retraining. Additional information can be found on our website https://bokorn.github.io/zephyr/



### Extreme Rotation Estimation using Dense Correlation Volumes
- **Arxiv ID**: http://arxiv.org/abs/2104.13530v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13530v2)
- **Published**: 2021-04-28 02:00:04+00:00
- **Updated**: 2021-07-19 05:00:22+00:00
- **Authors**: Ruojin Cai, Bharath Hariharan, Noah Snavely, Hadar Averbuch-Elor
- **Comment**: Published in CVPR 2021; Project page:
  https://ruojincai.github.io/ExtremeRotation/
- **Journal**: None
- **Summary**: We present a technique for estimating the relative 3D rotation of an RGB image pair in an extreme setting, where the images have little or no overlap. We observe that, even when images do not overlap, there may be rich hidden cues as to their geometric relationship, such as light source directions, vanishing points, and symmetries present in the scene. We propose a network design that can automatically learn such implicit cues by comparing all pairs of points between the two input images. Our method therefore constructs dense feature correlation volumes and processes these to predict relative 3D rotations. Our predictions are formed over a fine-grained discretization of rotations, bypassing difficulties associated with regressing 3D rotations. We demonstrate our approach on a large variety of extreme RGB image pairs, including indoor and outdoor images captured under different lighting conditions and geographic locations. Our evaluation shows that our model can successfully estimate relative rotations among non-overlapping images without compromising performance over overlapping image pairs.



### PAFNet: An Efficient Anchor-Free Object Detector Guidance
- **Arxiv ID**: http://arxiv.org/abs/2104.13534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13534v1)
- **Published**: 2021-04-28 02:32:54+00:00
- **Updated**: 2021-04-28 02:32:54+00:00
- **Authors**: Ying Xin, Guanzhong Wang, Mingyuan Mao, Yuan Feng, Qingqing Dang, Yanjun Ma, Errui Ding, Shumin Han
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is a basic but challenging task in computer vision, which plays a key role in a variety of industrial applications. However, object detectors based on deep learning usually require greater storage requirements and longer inference time, which hinders its practicality seriously. Therefore, a trade-off between effectiveness and efficiency is necessary in practical scenarios. Considering that without constraint of pre-defined anchors, anchor-free detectors can achieve acceptable accuracy and inference speed simultaneously. In this paper, we start from an anchor-free detector called TTFNet, modify the structure of TTFNet and introduce multiple existing tricks to realize effective server and mobile solutions respectively. Since all experiments in this paper are conducted based on PaddlePaddle, we call the model as PAFNet(Paddle Anchor Free Network). For server side, PAFNet can achieve a better balance between effectiveness (42.2% mAP) and efficiency (67.15 FPS) on a single V100 GPU. For moblie side, PAFNet-lite can achieve a better accuracy of (23.9% mAP) and 26.00 ms on Kirin 990 ARM CPU, outperforming the existing state-of-the-art anchor-free detectors by significant margins. Source code is at https://github.com/PaddlePaddle/PaddleDetection.



### Shot Contrastive Self-Supervised Learning for Scene Boundary Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.13537v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13537v1)
- **Published**: 2021-04-28 02:35:09+00:00
- **Updated**: 2021-04-28 02:35:09+00:00
- **Authors**: Shixing Chen, Xiaohan Nie, David Fan, Dongqing Zhang, Vimal Bhat, Raffay Hamid
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Scenes play a crucial role in breaking the storyline of movies and TV episodes into semantically cohesive parts. However, given their complex temporal structure, finding scene boundaries can be a challenging task requiring large amounts of labeled training data. To address this challenge, we present a self-supervised shot contrastive learning approach (ShotCoL) to learn a shot representation that maximizes the similarity between nearby shots compared to randomly selected shots. We show how to apply our learned shot representation for the task of scene boundary detection to offer state-of-the-art performance on the MovieNet dataset while requiring only ~25% of the training labels, using 9x fewer model parameters and offering 7x faster runtime. To assess the effectiveness of ShotCoL on novel applications of scene boundary detection, we take on the problem of finding timestamps in movies and TV episodes where video-ads can be inserted while offering a minimally disruptive viewing experience. To this end, we collected a new dataset called AdCuepoints with 3,975 movies and TV episodes, 2.2 million shots and 19,119 minimally disruptive ad cue-point labels. We present a thorough empirical analysis on this dataset demonstrating the effectiveness of ShotCoL for ad cue-points detection.



### Multi-scale Deep Learning Architecture for Nucleus Detection in Renal Cell Carcinoma Microscopy Image
- **Arxiv ID**: http://arxiv.org/abs/2104.13557v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13557v1)
- **Published**: 2021-04-28 03:36:02+00:00
- **Updated**: 2021-04-28 03:36:02+00:00
- **Authors**: Shiba Kuanar, Vassilis Athitsos, Dwarikanath Mahapatra, Anand Rajan
- **Comment**: This article has been removed by arXiv administrators because the
  submitter did not have the authority to grant the license applied at the time
  of submission
- **Journal**: None
- **Summary**: Clear cell renal cell carcinoma (ccRCC) is one of the most common forms of intratumoral heterogeneity in the study of renal cancer. ccRCC originates from the epithelial lining of proximal convoluted renal tubules. These cells undergo abnormal mutations in the presence of Ki67 protein and create a lump-like structure through cell proliferation. Manual counting of tumor cells in the tissue-affected sections is one of the strongest prognostic markers for renal cancer. However, this procedure is time-consuming and also prone to subjectivity. These assessments are based on the physical cell appearance and suffer wide intra-observer variations. Therefore, better cell nucleus detection and counting techniques can be an important biomarker for the assessment of tumor cell proliferation in routine pathological investigations. In this paper, we introduce a deep learning-based detection model for cell classification on IHC stained histology images. These images are classified into binary classes to find the presence of Ki67 protein in cancer-affected nucleus regions. Our model maps the multi-scale pyramid features and saliency information from local bounded regions and predicts the bounding box coordinates through regression. Our method validates the impact of Ki67 expression across a cohort of four hundred histology images treated with localized ccRCC and compares our results with the existing state-of-the-art nucleus detection methods. The precision and recall scores of the proposed method are computed and compared on the clinical data sets. The experimental results demonstrate that our model improves the F1 score up to 86.3% and an average area under the Precision-Recall curve as 85.73%.



### Interpretable Embedding Procedure Knowledge Transfer via Stacked Principal Component Analysis and Graph Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2104.13561v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13561v1)
- **Published**: 2021-04-28 03:40:37+00:00
- **Updated**: 2021-04-28 03:40:37+00:00
- **Authors**: Seunghyun Lee, Byung Cheol Song
- **Comment**: accepted at AAAI2021
- **Journal**: None
- **Summary**: Knowledge distillation (KD) is one of the most useful techniques for light-weight neural networks. Although neural networks have a clear purpose of embedding datasets into the low-dimensional space, the existing knowledge was quite far from this purpose and provided only limited information. We argue that good knowledge should be able to interpret the embedding procedure. This paper proposes a method of generating interpretable embedding procedure (IEP) knowledge based on principal component analysis, and distilling it based on a message passing neural network. Experimental results show that the student network trained by the proposed KD method improves 2.28% in the CIFAR100 dataset, which is higher performance than the state-of-the-art (SOTA) method. We also demonstrate that the embedding procedure knowledge is interpretable via visualization of the proposed KD process. The implemented code is available at https://github.com/sseung0703/IEPKT.



### Neural Ray-Tracing: Learning Surfaces and Reflectance for Relighting and View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2104.13562v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13562v2)
- **Published**: 2021-04-28 03:47:48+00:00
- **Updated**: 2021-12-04 20:40:20+00:00
- **Authors**: Julian Knodt, Joe Bartusek, Seung-Hwan Baek, Felix Heide
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Recent neural rendering methods have demonstrated accurate view interpolation by predicting volumetric density and color with a neural network. Although such volumetric representations can be supervised on static and dynamic scenes, existing methods implicitly bake the complete scene light transport into a single neural network for a given scene, including surface modeling, bidirectional scattering distribution functions, and indirect lighting effects. In contrast to traditional rendering pipelines, this prohibits changing surface reflectance, illumination, or composing other objects in the scene.   In this work, we explicitly model the light transport between scene surfaces and we rely on traditional integration schemes and the rendering equation to reconstruct a scene. The proposed method allows BSDF recovery with unknown light conditions and classic light transports such as pathtracing. By learning decomposed transport with surface representations established in conventional rendering methods, the method naturally facilitates editing shape, reflectance, lighting and scene composition. The method outperforms NeRV for relighting under known lighting conditions, and produces realistic reconstructions for relit and edited scenes. We validate the proposed approach for scene editing, relighting and reflectance estimation learned from synthetic and captured views on a subset of NeRV's datasets.



### Interactive Visualization for Exploring Information Fragments in Software Repositories
- **Arxiv ID**: http://arxiv.org/abs/2104.13568v1
- **DOI**: None
- **Categories**: **cs.SE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.13568v1)
- **Published**: 2021-04-28 04:57:18+00:00
- **Updated**: 2021-04-28 04:57:18+00:00
- **Authors**: Youngtaek Kim, Hyeon Jeon, Kiroong Choe, Hyunjoo Song, Bohyoung Kim, Jinwook Seo
- **Comment**: 14th IEEE Pacific Visualization Symposium (PacificVis '21) Poster
- **Journal**: None
- **Summary**: Software developers explore and inspect software repository data to obtain detailed information archived in the development history. However, developers who are not acquainted with the development context suffer from delving into the repositories with a handful of information; they have difficulty discovering and expanding information fragments considering the topological and sequential multi-dimensional structure of repositories. We introduce ExIF, an interactive visualization for exploring information fragments in software repositories. ExIF helps users discover new information fragments within clusters or topological neighbors and identify revisions incorporating user-collected fragments.



### Time series forecasting of new cases and new deaths rate for COVID-19 using deep learning methods
- **Arxiv ID**: http://arxiv.org/abs/2104.15007v3
- **DOI**: 10.1016/j.rinp.2021.104495
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.15007v3)
- **Published**: 2021-04-28 05:44:02+00:00
- **Updated**: 2021-12-25 04:45:35+00:00
- **Authors**: Nooshin Ayoobi, Danial Sharifrazi, Roohallah Alizadehsani, Afshin Shoeibi, Juan M. Gorriz, Hossein Moosaei, Abbas Khosravi, Saeid Nahavandi, Abdoulmohammad Gholamzadeh Chofreh, Feybi Ariani Goni, Jiri Jaromir Klemes, Amir Mosavi
- **Comment**: None
- **Journal**: Results in Physics,Volume 27,2021,104495
- **Summary**: The first known case of Coronavirus disease 2019 (COVID-19) was identified in December 2019. It has spread worldwide, leading to an ongoing pandemic, imposed restrictions and costs to many countries. Predicting the number of new cases and deaths during this period can be a useful step in predicting the costs and facilities required in the future. The purpose of this study is to predict new cases and deaths rate one, three and seven-day ahead during the next 100 days. The motivation for predicting every n days (instead of just every day) is the investigation of the possibility of computational cost reduction and still achieving reasonable performance. Such a scenario may be encountered in real-time forecasting of time series. Six different deep learning methods are examined on the data adopted from the WHO website. Three methods are LSTM, Convolutional LSTM, and GRU. The bidirectional extension is then considered for each method to forecast the rate of new cases and new deaths in Australia and Iran countries.   This study is novel as it carries out a comprehensive evaluation of the aforementioned three deep learning methods and their bidirectional extensions to perform prediction on COVID-19 new cases and new death rate time series. To the best of our knowledge, this is the first time that Bi-GRU and Bi-Conv-LSTM models are used for prediction on COVID-19 new cases and new deaths time series. The evaluation of the methods is presented in the form of graphs and Friedman statistical test. The results show that the bidirectional models have lower errors than other models. A several error evaluation metrics are presented to compare all models, and finally, the superiority of bidirectional methods is determined. This research could be useful for organisations working against COVID-19 and determining their long-term plans.



### Deep Domain Generalization with Feature-norm Network
- **Arxiv ID**: http://arxiv.org/abs/2104.13581v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13581v1)
- **Published**: 2021-04-28 06:13:47+00:00
- **Updated**: 2021-04-28 06:13:47+00:00
- **Authors**: Mohammad Mahfujur Rahman, Clinton Fookes, Sridha Sridharan
- **Comment**: Submitted to Pattern Recognition
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of training with multiple source domains with the aim to generalize to new domains at test time without an adaptation step. This is known as domain generalization (DG). Previous works on DG assume identical categories or label space across the source domains. In the case of category shift among the source domains, previous methods on DG are vulnerable to negative transfer due to the large mismatch among label spaces, decreasing the target classification accuracy. To tackle the aforementioned problem, we introduce an end-to-end feature-norm network (FNN) which is robust to negative transfer as it does not need to match the feature distribution among the source domains. We also introduce a collaborative feature-norm network (CFNN) to further improve the generalization capability of FNN. The CFNN matches the predictions of the next most likely categories for each training sample which increases each network's posterior entropy. We apply the proposed FNN and CFNN networks to the problem of DG for image classification tasks and demonstrate significant improvement over the state-of-the-art.



### [Re] Don't Judge an Object by Its Context: Learning to Overcome Contextual Bias
- **Arxiv ID**: http://arxiv.org/abs/2104.13582v1
- **DOI**: 10.5281/zenodo.4834352
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13582v1)
- **Published**: 2021-04-28 06:21:28+00:00
- **Updated**: 2021-04-28 06:21:28+00:00
- **Authors**: Sunnie S. Y. Kim, Sharon Zhang, Nicole Meister, Olga Russakovsky
- **Comment**: ML Reproducibility Challenge 2020. Accepted for publication in the
  ReScience C journal
- **Journal**: None
- **Summary**: Singh et al. (2020) point out the dangers of contextual bias in visual recognition datasets. They propose two methods, CAM-based and feature-split, that better recognize an object or attribute in the absence of its typical context while maintaining competitive within-context accuracy. To verify their performance, we attempted to reproduce all 12 tables in the original paper, including those in the appendix. We also conducted additional experiments to better understand the proposed methods, including increasing the regularization in CAM-based and removing the weighted loss in feature-split. As the original code was not made available, we implemented the entire pipeline from scratch in PyTorch 1.7.0. Our implementation is based on the paper and email exchanges with the authors. We found that both proposed methods in the original paper help mitigate contextual bias, although for some methods, we could not completely replicate the quantitative results in the paper even after completing an extensive hyperparameter search. For example, on COCO-Stuff, DeepFashion, and UnRel, our feature-split model achieved an increase in accuracy on out-of-context images over the standard baseline, whereas on AwA, we saw a drop in performance. For the proposed CAM-based method, we were able to reproduce the original paper's results to within 0.5$\%$ mAP. Our implementation can be found at https://github.com/princetonvisualai/ContextualBias.



### Revisiting Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.13586v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13586v2)
- **Published**: 2021-04-28 06:32:17+00:00
- **Updated**: 2022-04-02 04:10:36+00:00
- **Authors**: Haodong Duan, Yue Zhao, Kai Chen, Dahua Lin, Bo Dai
- **Comment**: CVPR 2022 Oral
- **Journal**: None
- **Summary**: Human skeleton, as a compact representation of human action, has received increasing attention in recent years. Many skeleton-based action recognition methods adopt graph convolutional networks (GCN) to extract features on top of human skeletons. Despite the positive results shown in previous works, GCN-based methods are subject to limitations in robustness, interoperability, and scalability. In this work, we propose PoseC3D, a new approach to skeleton-based action recognition, which relies on a 3D heatmap stack instead of a graph sequence as the base representation of human skeletons. Compared to GCN-based methods, PoseC3D is more effective in learning spatiotemporal features, more robust against pose estimation noises, and generalizes better in cross-dataset settings. Also, PoseC3D can handle multiple-person scenarios without additional computation cost, and its features can be easily integrated with other modalities at early fusion stages, which provides a great design space to further boost the performance. On four challenging datasets, PoseC3D consistently obtains superior performance, when used alone on skeletons and in combination with the RGB modality.



### DeRenderNet: Intrinsic Image Decomposition of Urban Scenes with Shape-(In)dependent Shading Rendering
- **Arxiv ID**: http://arxiv.org/abs/2104.13602v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13602v1)
- **Published**: 2021-04-28 07:22:38+00:00
- **Updated**: 2021-04-28 07:22:38+00:00
- **Authors**: Yongjie Zhu, Jiajun Tang, Si Li, Boxin Shi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose DeRenderNet, a deep neural network to decompose the albedo and latent lighting, and render shape-(in)dependent shadings, given a single image of an outdoor urban scene, trained in a self-supervised manner. To achieve this goal, we propose to use the albedo maps extracted from scenes in videogames as direct supervision and pre-compute the normal and shadow prior maps based on the depth maps provided as indirect supervision. Compared with state-of-the-art intrinsic image decomposition methods, DeRenderNet produces shadow-free albedo maps with clean details and an accurate prediction of shadows in the shape-independent shading, which is shown to be effective in re-rendering and improving the accuracy of high-level vision tasks for urban scenes.



### Domain Adaptive Semantic Segmentation with Self-Supervised Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2104.13613v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13613v2)
- **Published**: 2021-04-28 07:47:36+00:00
- **Updated**: 2021-08-24 16:13:06+00:00
- **Authors**: Qin Wang, Dengxin Dai, Lukas Hoyer, Luc Van Gool, Olga Fink
- **Comment**: To appear in ICCV 2021
- **Journal**: None
- **Summary**: Domain adaptation for semantic segmentation aims to improve the model performance in the presence of a distribution shift between source and target domain. Leveraging the supervision from auxiliary tasks~(such as depth estimation) has the potential to heal this shift because many visual tasks are closely related to each other. However, such a supervision is not always available. In this work, we leverage the guidance from self-supervised depth estimation, which is available on both domains, to bridge the domain gap. On the one hand, we propose to explicitly learn the task feature correlation to strengthen the target semantic predictions with the help of target depth estimation. On the other hand, we use the depth prediction discrepancy from source and target depth decoders to approximate the pixel-wise adaptation difficulty. The adaptation difficulty, inferred from depth, is then used to refine the target semantic segmentation pseudo-labels. The proposed method can be easily implemented into existing segmentation frameworks. We demonstrate the effectiveness of our approach on the benchmark tasks SYNTHIA-to-Cityscapes and GTA-to-Cityscapes, on which we achieve the new state-of-the-art performance of $55.0\%$ and $56.6\%$, respectively. Our code is available at \url{https://qin.ee/corda}.



### Preserving Earlier Knowledge in Continual Learning with the Help of All Previous Feature Extractors
- **Arxiv ID**: http://arxiv.org/abs/2104.13614v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13614v1)
- **Published**: 2021-04-28 07:49:24+00:00
- **Updated**: 2021-04-28 07:49:24+00:00
- **Authors**: Zhuoyun Li, Changhong Zhong, Sijia Liu, Ruixuan Wang, Wei-Shi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Continual learning of new knowledge over time is one desirable capability for intelligent systems to recognize more and more classes of objects. Without or with very limited amount of old data stored, an intelligent system often catastrophically forgets previously learned old knowledge when learning new knowledge. Recently, various approaches have been proposed to alleviate the catastrophic forgetting issue. However, old knowledge learned earlier is commonly less preserved than that learned more recently. In order to reduce the forgetting of particularly earlier learned old knowledge and improve the overall continual learning performance, we propose a simple yet effective fusion mechanism by including all the previously learned feature extractors into the intelligent model. In addition, a new feature extractor is included to the model when learning a new set of classes each time, and a feature extractor pruning is also applied to prevent the whole model size from growing rapidly. Experiments on multiple classification tasks show that the proposed approach can effectively reduce the forgetting of old knowledge, achieving state-of-the-art continual learning performance.



### Medical Transformer: Universal Brain Encoder for 3D MRI Analysis
- **Arxiv ID**: http://arxiv.org/abs/2104.13633v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.13633v1)
- **Published**: 2021-04-28 08:34:21+00:00
- **Updated**: 2021-04-28 08:34:21+00:00
- **Authors**: Eunji Jun, Seungwoo Jeong, Da-Woon Heo, Heung-Il Suk
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Transfer learning has gained attention in medical image analysis due to limited annotated 3D medical datasets for training data-driven deep learning models in the real world. Existing 3D-based methods have transferred the pre-trained models to downstream tasks, which achieved promising results with only a small number of training samples. However, they demand a massive amount of parameters to train the model for 3D medical imaging. In this work, we propose a novel transfer learning framework, called Medical Transformer, that effectively models 3D volumetric images in the form of a sequence of 2D image slices. To make a high-level representation in 3D-form empowering spatial relations better, we take a multi-view approach that leverages plenty of information from the three planes of 3D volume, while providing parameter-efficient training. For building a source model generally applicable to various tasks, we pre-train the model in a self-supervised learning manner for masked encoding vector prediction as a proxy task, using a large-scale normal, healthy brain magnetic resonance imaging (MRI) dataset. Our pre-trained model is evaluated on three downstream tasks: (i) brain disease diagnosis, (ii) brain age prediction, and (iii) brain tumor segmentation, which are actively studied in brain MRI research. The experimental results show that our Medical Transformer outperforms the state-of-the-art transfer learning methods, efficiently reducing the number of parameters up to about 92% for classification and



### A Deep Learning Object Detection Method for an Efficient Clusters Initialization
- **Arxiv ID**: http://arxiv.org/abs/2104.13634v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13634v3)
- **Published**: 2021-04-28 08:34:25+00:00
- **Updated**: 2021-07-04 18:43:43+00:00
- **Authors**: Raphaël Couturier, Hassan N. Noura, Ola Salman, Abderrahmane Sider
- **Comment**: None
- **Journal**: None
- **Summary**: Clustering is an unsupervised machine learning method grouping data samples into clusters of similar objects. In practice, clustering has been used in numerous applications such as banking customers profiling, document retrieval, image segmentation, and e-commerce recommendation engines. However, the existing clustering techniques present significant limitations, from which is the dependability of their stability on the initialization parameters (e.g. number of clusters, centroids). Different solutions were presented in the literature to overcome this limitation (i.e. internal and external validation metrics). However, these solutions require high computational complexity and memory consumption, especially when dealing with big data. In this paper, we apply the recent object detection Deep Learning (DL) model, named YOLO-v5, to detect the initial clustering parameters such as the number of clusters with their sizes and centroids. Mainly, the proposed solution consists of adding a DL-based initialization phase making the clustering algorithms free of initialization. Two model solutions are provided in this work, one for isolated clusters and the other one for overlapping clusters. The features of the incoming dataset determine which model to use. Moreover, The results show that the proposed solution can provide near-optimal clusters initialization parameters with low computational and resources overhead compared to existing solutions.



### Point Cloud Learning with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2104.13636v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13636v2)
- **Published**: 2021-04-28 08:39:21+00:00
- **Updated**: 2022-10-25 02:30:30+00:00
- **Authors**: Qi Zhong, Xian-Feng Han
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Remarkable performance from Transformer networks in Natural Language Processing promote the development of these models in dealing with computer vision tasks such as image recognition and segmentation. In this paper, we introduce a novel framework, called Multi-level Multi-scale Point Transformer (MLMSPT) that works directly on the irregular point clouds for representation learning. Specifically, a point pyramid transformer is investigated to model features with diverse resolutions or scales we defined, followed by a multi-level transformer module to aggregate contextual information from different levels of each scale and enhance their interactions. While a multi-scale transformer module is designed to capture the dependencies among representations across different scales. Extensive evaluation on public benchmark datasets demonstrate the effectiveness and the competitive performance of our methods on 3D shape classification, segmentation tasks.



### On the Unreasonable Effectiveness of Centroids in Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2104.13643v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2104.13643v1)
- **Published**: 2021-04-28 08:57:57+00:00
- **Updated**: 2021-04-28 08:57:57+00:00
- **Authors**: Mikolaj Wieczorek, Barbara Rychalska, Jacek Dabrowski
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: Image retrieval task consists of finding similar images to a query image from a set of gallery (database) images. Such systems are used in various applications e.g. person re-identification (ReID) or visual product search. Despite active development of retrieval models it still remains a challenging task mainly due to large intra-class variance caused by changes in view angle, lighting, background clutter or occlusion, while inter-class variance may be relatively low. A large portion of current research focuses on creating more robust features and modifying objective functions, usually based on Triplet Loss. Some works experiment with using centroid/proxy representation of a class to alleviate problems with computing speed and hard samples mining used with Triplet Loss. However, these approaches are used for training alone and discarded during the retrieval stage. In this paper we propose to use the mean centroid representation both during training and retrieval. Such an aggregated representation is more robust to outliers and assures more stable features. As each class is represented by a single embedding - the class centroid - both retrieval time and storage requirements are reduced significantly. Aggregating multiple embeddings results in a significant reduction of the search space due to lowering the number of candidate target vectors, which makes the method especially suitable for production deployments. Comprehensive experiments conducted on two ReID and Fashion Retrieval datasets demonstrate effectiveness of our method, which outperforms the current state-of-the-art. We propose centroid training and retrieval as a viable method for both Fashion Retrieval and ReID applications.



### Two stages for visual object tracking
- **Arxiv ID**: http://arxiv.org/abs/2104.13648v2
- **DOI**: 10.1109/ICAA53760.2021.00037
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.13648v2)
- **Published**: 2021-04-28 09:11:33+00:00
- **Updated**: 2022-01-05 04:36:27+00:00
- **Authors**: Fei Chen, Fuhan Zhang, Xiaodong Wang
- **Comment**: 2021 International Conference on Intelligent Computing, Automation
  and Applications (ICAA)
- **Journal**: None
- **Summary**: Siamese-based trackers have achived promising performance on visual object tracking tasks. Most existing Siamese-based trackers contain two separate branches for tracking, including classification branch and bounding box regression branch. In addition, image segmentation provides an alternative way to obetain the more accurate target region. In this paper, we propose a novel tracker with two-stages: detection and segmentation. The detection stage is capable of locating the target by Siamese networks. Then more accurate tracking results are obtained by segmentation module given the coarse state estimation in the first stage. We conduct experiments on four benchmarks. Our approach achieves state-of-the-art results, with the EAO of 52.6$\%$ on VOT2016, 51.3$\%$ on VOT2018, and 39.0$\%$ on VOT2019 datasets, respectively.



### Robust Face-Swap Detection Based on 3D Facial Shape Information
- **Arxiv ID**: http://arxiv.org/abs/2104.13665v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13665v1)
- **Published**: 2021-04-28 09:35:48+00:00
- **Updated**: 2021-04-28 09:35:48+00:00
- **Authors**: Weinan Guan, Wei Wang, Jing Dong, Bo Peng, Tieniu Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Maliciously-manipulated images or videos - so-called deep fakes - especially face-swap images and videos have attracted more and more malicious attackers to discredit some key figures. Previous pixel-level artifacts based detection techniques always focus on some unclear patterns but ignore some available semantic clues. Therefore, these approaches show weak interpretability and robustness. In this paper, we propose a biometric information based method to fully exploit the appearance and shape feature for face-swap detection of key figures. The key aspect of our method is obtaining the inconsistency of 3D facial shape and facial appearance, and the inconsistency based clue offers natural interpretability for the proposed face-swap detection method. Experimental results show the superiority of our method in robustness on various laundering and cross-domain data, which validates the effectiveness of the proposed method.



### End-to-End Approach for Recognition of Historical Digit Strings
- **Arxiv ID**: http://arxiv.org/abs/2104.13666v1
- **DOI**: 10.1007/978-3-030-86334-0_39
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.13666v1)
- **Published**: 2021-04-28 09:39:29+00:00
- **Updated**: 2021-04-28 09:39:29+00:00
- **Authors**: Mengqiao Zhao, Andre G. Hochuli, Abbas Cheddad
- **Comment**: Cite as: Mengqiao Zhao, Andre G. Hochuli and Abbas Cheddad,
  End-to-End Approach for Recognition of Historical Digit Strings, to appear in
  the 16th International Conference on Document Analysis and Recognition (ICDAR
  2021), LNCS, Springer, Lausanne, Switzerland
- **Journal**: None
- **Summary**: The plethora of digitalised historical document datasets released in recent years has rekindled interest in advancing the field of handwriting pattern recognition. In the same vein, a recently published data set, known as ARDIS, presents handwritten digits manually cropped from 15.000 scanned documents of Swedish church books and exhibiting various handwriting styles. To this end, we propose an end-to-end segmentation-free deep learning approach to handle this challenging ancient handwriting style of dates present in the ARDIS dataset (4-digits long strings). We show that with slight modifications in the VGG-16 deep model, the framework can achieve a recognition rate of 93.2%, resulting in a feasible solution free of heuristic methods, segmentation, and fusion methods. Moreover, the proposed approach outperforms the well-known CRNN method (a model widely applied in handwriting recognition tasks).



### AdvHaze: Adversarial Haze Attack
- **Arxiv ID**: http://arxiv.org/abs/2104.13673v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13673v1)
- **Published**: 2021-04-28 09:52:25+00:00
- **Updated**: 2021-04-28 09:52:25+00:00
- **Authors**: Ruijun Gao, Qing Guo, Felix Juefei-Xu, Hongkai Yu, Wei Feng
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, adversarial attacks have drawn more attention for their value on evaluating and improving the robustness of machine learning models, especially, neural network models. However, previous attack methods have mainly focused on applying some $l^p$ norm-bounded noise perturbations. In this paper, we instead introduce a novel adversarial attack method based on haze, which is a common phenomenon in real-world scenery. Our method can synthesize potentially adversarial haze into an image based on the atmospheric scattering model with high realisticity and mislead classifiers to predict an incorrect class. We launch experiments on two popular datasets, i.e., ImageNet and NIPS~2017. We demonstrate that the proposed method achieves a high success rate, and holds better transferability across different classification models than the baselines. We also visualize the correlation matrices, which inspire us to jointly apply different perturbations to improve the success rate of the attack. We hope this work can boost the development of non-noise-based adversarial attacks and help evaluate and improve the robustness of DNNs.



### HOTR: End-to-End Human-Object Interaction Detection with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.13682v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13682v1)
- **Published**: 2021-04-28 10:10:29+00:00
- **Updated**: 2021-04-28 10:10:29+00:00
- **Authors**: Bumsoo Kim, Junhyun Lee, Jaewoo Kang, Eun-Sol Kim, Hyunwoo J. Kim
- **Comment**: Accepted to CVPR 2021 (Oral Presentation)
- **Journal**: None
- **Summary**: Human-Object Interaction (HOI) detection is a task of identifying "a set of interactions" in an image, which involves the i) localization of the subject (i.e., humans) and target (i.e., objects) of interaction, and ii) the classification of the interaction labels. Most existing methods have indirectly addressed this task by detecting human and object instances and individually inferring every pair of the detected instances. In this paper, we present a novel framework, referred to by HOTR, which directly predicts a set of <human, object, interaction> triplets from an image based on a transformer encoder-decoder architecture. Through the set prediction, our method effectively exploits the inherent semantic relationships in an image and does not require time-consuming post-processing which is the main bottleneck of existing methods. Our proposed algorithm achieves the state-of-the-art performance in two HOI detection benchmarks with an inference time under 1 ms after object detection.



### Deep Neural Networks Based Weight Approximation and Computation Reuse for 2-D Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2105.02954v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.02954v1)
- **Published**: 2021-04-28 10:16:53+00:00
- **Updated**: 2021-04-28 10:16:53+00:00
- **Authors**: Mohammed F. Tolba, Huruy Tekle Tesfai, Hani Saleh, Baker Mohammad, Mahmoud Al-Qutayri
- **Comment**: 10 pages 9 figures
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are computationally and memory intensive, which makes their hardware implementation a challenging task especially for resource constrained devices such as IoT nodes. To address this challenge, this paper introduces a new method to improve DNNs performance by fusing approximate computing with data reuse techniques to be used for image recognition applications. DNNs weights are approximated based on the linear and quadratic approximation methods during the training phase, then, all of the weights are replaced with the linear/quadratic coefficients to execute the inference in a way where different weights could be computed using the same coefficients. This leads to a repetition of the weights across the processing element (PE) array, which in turn enables the reuse of the DNN sub-computations (computational reuse) and leverage the same data (data reuse) to reduce DNNs computations, memory accesses, and improve energy efficiency albeit at the cost of increased training time. Complete analysis for both MNIST and CIFAR 10 datasets is presented for image recognition , where LeNet 5 revealed a reduction in the number of parameters by a factor of 1211.3x with a drop of less than 0.9% in accuracy. When compared to the state of the art Row Stationary (RS) method, the proposed architecture saved 54% of the total number of adders and multipliers needed. Overall, the proposed approach is suitable for IoT edge devices as it reduces the memory size requirement as well as the number of needed memory accesses.



### A review on physical and data-driven based nowcasting methods using sky images
- **Arxiv ID**: http://arxiv.org/abs/2105.02959v1
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.IM, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.02959v1)
- **Published**: 2021-04-28 10:20:52+00:00
- **Updated**: 2021-04-28 10:20:52+00:00
- **Authors**: Ekanki Sharma, Wilfried Elmenreich
- **Comment**: None
- **Journal**: None
- **Summary**: Amongst all the renewable energy resources (RES), solar is the most popular form of energy source and is of particular interest for its widely integration into the power grid. However, due to the intermittent nature of solar source, it is of the greatest significance to forecast solar irradiance to ensure uninterrupted and reliable power supply to serve the energy demand. There are several approaches to perform solar irradiance forecasting, for instance satellite-based methods, sky image-based methods, machine learning-based methods, and numerical weather prediction-based methods. In this paper, we present a review on short-term intra-hour solar prediction techniques known as nowcasting methods using sky images. Along with this, we also report and discuss which sky image features are significant for the nowcasting methods.



### PANDA : Perceptually Aware Neural Detection of Anomalies
- **Arxiv ID**: http://arxiv.org/abs/2104.13702v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13702v1)
- **Published**: 2021-04-28 11:03:50+00:00
- **Updated**: 2021-04-28 11:03:50+00:00
- **Authors**: Jack W. Barker, Toby P. Breckon
- **Comment**: In-proceedings at 2021 International Joint Conference on Neural
  Networks (IJCNN)
- **Journal**: None
- **Summary**: Semi-supervised methods of anomaly detection have seen substantial advancement in recent years. Of particular interest are applications of such methods to diverse, real-world anomaly detection problems where anomalous variations can vary from the visually obvious to the very subtle. In this work, we propose a novel fine-grained VAE-GAN architecture trained in a semi-supervised manner in order to detect both visually distinct and subtle anomalies. With the use of a residually connected dual-feature extractor, a fine-grained discriminator and a perceptual loss function, we are able to detect subtle, low inter-class (anomaly vs. normal) variant anomalies with greater detection capability and smaller margins of deviation in AUC value during inference compared to prior work whilst also remaining time-efficient during inference. We achieve state of-the-art anomaly detection results when compared extensively with prior semi-supervised approaches across a multitude of anomaly detection benchmark tasks including trivial leave-one out tasks (CIFAR-10 - AUPRCavg: 0.91; MNIST - AUPRCavg: 0.90) in addition to challenging real-world anomaly detection tasks (plant leaf disease - AUC: 0.776; threat item X-ray - AUC: 0.51), video frame-level anomaly detection (UCSDPed1 - AUC: 0.95) and high frequency texture with object anomalous defect detection (MVTEC - AUCavg: 0.83).



### Hybrid Approach for 3D Head Reconstruction: Using Neural Networks and Visual Geometry
- **Arxiv ID**: http://arxiv.org/abs/2104.13710v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13710v1)
- **Published**: 2021-04-28 11:31:35+00:00
- **Updated**: 2021-04-28 11:31:35+00:00
- **Authors**: Oussema Bouafif, Bogdan Khomutenko, Mohamed Daoudi
- **Comment**: 25th International Conference On Pattern Recognition 2020
- **Journal**: None
- **Summary**: Recovering the 3D geometric structure of a face from a single input image is a challenging active research area in computer vision. In this paper, we present a novel method for reconstructing 3D heads from a single or multiple image(s) using a hybrid approach based on deep learning and geometric techniques. We propose an encoder-decoder network based on the U-net architecture and trained on synthetic data only. It predicts both pixel-wise normal vectors and landmarks maps from a single input photo. Landmarks are used for the pose computation and the initialization of the optimization problem, which, in turn, reconstructs the 3D head geometry by using a parametric morphable model and normal vector fields. State-of-the-art results are achieved through qualitative and quantitative evaluation tests on both single and multi-view settings. Despite the fact that the model was trained only on synthetic data, it successfully recovers 3D geometry and precise poses for real-world images.



### The Algonauts Project 2021 Challenge: How the Human Brain Makes Sense of a World in Motion
- **Arxiv ID**: http://arxiv.org/abs/2104.13714v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2104.13714v1)
- **Published**: 2021-04-28 11:38:31+00:00
- **Updated**: 2021-04-28 11:38:31+00:00
- **Authors**: R. M. Cichy, K. Dwivedi, B. Lahner, A. Lascelles, P. Iamshchinina, M. Graumann, A. Andonian, N. A. R. Murty, K. Kay, G. Roig, A. Oliva
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: The sciences of natural and artificial intelligence are fundamentally connected. Brain-inspired human-engineered AI are now the standard for predicting human brain responses during vision, and conversely, the brain continues to inspire invention in AI. To promote even deeper connections between these fields, we here release the 2021 edition of the Algonauts Project Challenge: How the Human Brain Makes Sense of a World in Motion (http://algonauts.csail.mit.edu/). We provide whole-brain fMRI responses recorded while 10 human participants viewed a rich set of over 1,000 short video clips depicting everyday events. The goal of the challenge is to accurately predict brain responses to these video clips. The format of our challenge ensures rapid development, makes results directly comparable and transparent, and is open to all. In this way it facilitates interdisciplinary collaboration towards a common goal of understanding visual intelligence. The 2021 Algonauts Project is conducted in collaboration with the Cognitive Computational Neuroscience (CCN) conference.



### Preserving Semantic Consistency in Unsupervised Domain Adaptation Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.13725v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13725v1)
- **Published**: 2021-04-28 12:23:30+00:00
- **Updated**: 2021-04-28 12:23:30+00:00
- **Authors**: Mohammad Mahfujur Rahman, Clinton Fookes, Sridha Sridharan
- **Comment**: Submitted to Pattern Recognition Letters
- **Journal**: None
- **Summary**: Unsupervised domain adaptation seeks to mitigate the distribution discrepancy between source and target domains, given labeled samples of the source domain and unlabeled samples of the target domain. Generative adversarial networks (GANs) have demonstrated significant improvement in domain adaptation by producing images which are domain specific for training. However, most of the existing GAN based techniques for unsupervised domain adaptation do not consider semantic information during domain matching, hence these methods degrade the performance when the source and target domain data are semantically different. In this paper, we propose an end-to-end novel semantic consistent generative adversarial network (SCGAN). This network can achieve source to target domain matching by capturing semantic information at the feature level and producing images for unsupervised domain adaptation from both the source and the target domains. We demonstrate the robustness of our proposed method which exceeds the state-of-the-art performance in unsupervised domain adaptation settings by performing experiments on digit and object classification tasks.



### MineGAN++: Mining Generative Models for Efficient Knowledge Transfer to Limited Data Domains
- **Arxiv ID**: http://arxiv.org/abs/2104.13742v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13742v1)
- **Published**: 2021-04-28 13:10:56+00:00
- **Updated**: 2021-04-28 13:10:56+00:00
- **Authors**: Yaxing Wang, Abel Gonzalez-Garcia, Chenshen Wu, Luis Herranz, Fahad Shahbaz Khan, Shangling Jui, Joost van de Weijer
- **Comment**: Technical report. arXiv admin note: substantial text overlap with
  arXiv:1912.05270
- **Journal**: None
- **Summary**: GANs largely increases the potential impact of generative models. Therefore, we propose a novel knowledge transfer method for generative models based on mining the knowledge that is most beneficial to a specific target domain, either from a single or multiple pretrained GANs. This is done using a miner network that identifies which part of the generative distribution of each pretrained GAN outputs samples closest to the target domain. Mining effectively steers GAN sampling towards suitable regions of the latent space, which facilitates the posterior finetuning and avoids pathologies of other methods, such as mode collapse and lack of flexibility. Furthermore, to prevent overfitting on small target domains, we introduce sparse subnetwork selection, that restricts the set of trainable neurons to those that are relevant for the target dataset. We perform comprehensive experiments on several challenging datasets using various GAN architectures (BigGAN, Progressive GAN, and StyleGAN) and show that the proposed method, called MineGAN, effectively transfers knowledge to domains with few target images, outperforming existing methods. In addition, MineGAN can successfully transfer knowledge from multiple pretrained GANs.



### Image Inpainting by End-to-End Cascaded Refinement with Mask Awareness
- **Arxiv ID**: http://arxiv.org/abs/2104.13743v1
- **DOI**: 10.1109/TIP.2021.3076310
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13743v1)
- **Published**: 2021-04-28 13:17:47+00:00
- **Updated**: 2021-04-28 13:17:47+00:00
- **Authors**: Manyu Zhu, Dongliang He, Xin Li, Chao Li, Fu Li, Xiao Liu, Errui Ding, Zhaoxiang Zhang
- **Comment**: IEEE TIP, to appear
- **Journal**: None
- **Summary**: Inpainting arbitrary missing regions is challenging because learning valid features for various masked regions is nontrivial. Though U-shaped encoder-decoder frameworks have been witnessed to be successful, most of them share a common drawback of mask unawareness in feature extraction because all convolution windows (or regions), including those with various shapes of missing pixels, are treated equally and filtered with fixed learned kernels. To this end, we propose our novel mask-aware inpainting solution. Firstly, a Mask-Aware Dynamic Filtering (MADF) module is designed to effectively learn multi-scale features for missing regions in the encoding phase. Specifically, filters for each convolution window are generated from features of the corresponding region of the mask. The second fold of mask awareness is achieved by adopting Point-wise Normalization (PN) in our decoding phase, considering that statistical natures of features at masked points differentiate from those of unmasked points. The proposed PN can tackle this issue by dynamically assigning point-wise scaling factor and bias. Lastly, our model is designed to be an end-to-end cascaded refinement one. Supervision information such as reconstruction loss, perceptual loss and total variation loss is incrementally leveraged to boost the inpainting results from coarse to fine. Effectiveness of the proposed framework is validated both quantitatively and qualitatively via extensive experiments on three public datasets including Places2, CelebA and Paris StreetView.



### LGA-RCNN: Loss-Guided Attention for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.13763v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13763v4)
- **Published**: 2021-04-28 13:52:58+00:00
- **Updated**: 2021-05-12 10:45:24+00:00
- **Authors**: Xin Yi, Jiahao Wu, Bo Ma, Yangtong Ou, Longyao Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is widely studied in computer vision filed. In recent years, certain representative deep learning based detection methods along with solid benchmarks are proposed, which boosts the development of related researchs. However, existing detection methods still suffer from undesirable performance under challenges such as camouflage, blur, inter-class similarity, intra-class variance and complex environment. To address this issue, we propose LGA-RCNN which utilizes a loss-guided attention (LGA) module to highlight representative region of objects. Then, those highlighted local information are fused with global information for precise classification and localization.



### Segmentation-Based Bounding Box Generation for Omnidirectional Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.13764v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13764v3)
- **Published**: 2021-04-28 13:53:21+00:00
- **Updated**: 2023-06-04 01:20:22+00:00
- **Authors**: Masato Tamura, Tomoaki Yoshinaga
- **Comment**: Pre-print version of a paper accepted to The Visual Computer
- **Journal**: None
- **Summary**: We propose a segmentation-based bounding box generation method for omnidirectional pedestrian detection that enables detectors to tightly fit bounding boxes to pedestrians without omnidirectional images for training. Due to the wide angle of view, omnidirectional cameras are more cost-effective than standard cameras and hence suitable for large-scale monitoring. The problem of using omnidirectional cameras for pedestrian detection is that the performance of standard pedestrian detectors is likely to be substantially degraded because pedestrians' appearance in omnidirectional images may be rotated to any angle. Existing methods mitigate this issue by transforming images during inference. However, the transformation substantially degrades the detection accuracy and speed. A recently proposed method obviates the transformation by training detectors with omnidirectional images, which instead incurs huge annotation costs. To obviate both the transformation and annotation works, we leverage an existing large-scale object detection dataset. We train a detector with rotated images and tightly fitted bounding box annotations generated from the segmentation annotations in the dataset, resulting in detecting pedestrians in omnidirectional images with tightly fitted bounding boxes. We also develop pseudo-fisheye distortion augmentation, which further enhances the performance. Extensive analysis shows that our detector successfully fits bounding boxes to pedestrians and demonstrates substantial performance improvement.



### Boosting Co-teaching with Compression Regularization for Label Noise
- **Arxiv ID**: http://arxiv.org/abs/2104.13766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13766v1)
- **Published**: 2021-04-28 13:56:12+00:00
- **Updated**: 2021-04-28 13:56:12+00:00
- **Authors**: Yingyi Chen, Xi Shen, Shell Xu Hu, Johan A. K. Suykens
- **Comment**: Accepted by CVPR Workshop 2021. Project page:
  https://github.com/yingyichen-cyy/Nested-Co-teaching
- **Journal**: None
- **Summary**: In this paper, we study the problem of learning image classification models in the presence of label noise. We revisit a simple compression regularization named Nested Dropout. We find that Nested Dropout, though originally proposed to perform fast information retrieval and adaptive data compression, can properly regularize a neural network to combat label noise. Moreover, owing to its simplicity, it can be easily combined with Co-teaching to further boost the performance.   Our final model remains simple yet effective: it achieves comparable or even better performance than the state-of-the-art approaches on two real-world datasets with label noise which are Clothing1M and ANIMAL-10N. On Clothing1M, our approach obtains 74.9% accuracy which is slightly better than that of DivideMix. On ANIMAL-10N, we achieve 84.1% accuracy while the best public result by PLC is 83.4%. We hope that our simple approach can be served as a strong baseline for learning with label noise. Our implementation is available at https://github.com/yingyichen-cyy/Nested-Co-teaching.



### Pose-driven Attention-guided Image Generation for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2104.13773v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13773v1)
- **Published**: 2021-04-28 14:02:24+00:00
- **Updated**: 2021-04-28 14:02:24+00:00
- **Authors**: Amena Khatun, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: Submitted to Pattern Recognition
- **Journal**: None
- **Summary**: Person re-identification (re-ID) concerns the matching of subject images across different camera views in a multi camera surveillance system. One of the major challenges in person re-ID is pose variations across the camera network, which significantly affects the appearance of a person. Existing development data lack adequate pose variations to carry out effective training of person re-ID systems. To solve this issue, in this paper we propose an end-to-end pose-driven attention-guided generative adversarial network, to generate multiple poses of a person. We propose to attentively learn and transfer the subject pose through an attention mechanism. A semantic-consistency loss is proposed to preserve the semantic information of the person during pose transfer. To ensure fine image details are realistic after pose translation, an appearance discriminator is used while a pose discriminator is used to ensure the pose of the transferred images will exactly be the same as the target pose. We show that by incorporating the proposed approach in a person re-identification framework, realistic pose transferred images and state-of-the-art re-identification results can be achieved.



### Semantic Consistency and Identity Mapping Multi-Component Generative Adversarial Network for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2104.13780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13780v1)
- **Published**: 2021-04-28 14:12:29+00:00
- **Updated**: 2021-04-28 14:12:29+00:00
- **Authors**: Amena Khatun, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: Accepted in WACV 2020
- **Journal**: WACV, 2020
- **Summary**: In a real world environment, person re-identification (Re-ID) is a challenging task due to variations in lighting conditions, viewing angles, pose and occlusions. Despite recent performance gains, current person Re-ID algorithms still suffer heavily when encountering these variations. To address this problem, we propose a semantic consistency and identity mapping multi-component generative adversarial network (SC-IMGAN) which provides style adaptation from one to many domains. To ensure that transformed images are as realistic as possible, we propose novel identity mapping and semantic consistency losses to maintain identity across the diverse domains. For the Re-ID task, we propose a joint verification-identification quartet network which is trained with generated and real images, followed by an effective quartet loss for verification. Our proposed method outperforms state-of-the-art techniques on six challenging person Re-ID datasets: CUHK01, CUHK03, VIPeR, PRID2011, iLIDS and Market-1501.



### Unsupervised Detection of Cancerous Regions in Histology Imagery using Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2104.13786v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13786v1)
- **Published**: 2021-04-28 14:19:00+00:00
- **Updated**: 2021-04-28 14:19:00+00:00
- **Authors**: Dejan Stepec, Danijel Skocaj
- **Comment**: CVPR 2021 CVMI workshop
- **Journal**: None
- **Summary**: Detection of visual anomalies refers to the problem of finding patterns in different imaging data that do not conform to the expected visual appearance and is a widely studied problem in different domains. Due to the nature of anomaly occurrences and underlying generating processes, it is hard to characterize them and obtain labeled data. Obtaining labeled data is especially difficult in biomedical applications, where only trained domain experts can provide labels, which often come in large diversity and complexity. Recently presented approaches for unsupervised detection of visual anomalies approaches omit the need for labeled data and demonstrate promising results in domains, where anomalous samples significantly deviate from the normal appearance. Despite promising results, the performance of such approaches still lags behind supervised approaches and does not provide a one-fits-all solution. In this work, we present an image-to-image translation-based framework that significantly surpasses the performance of existing unsupervised methods and approaches the performance of supervised methods in a challenging domain of cancerous region detection in histology imagery.



### Image Synthesis as a Pretext for Unsupervised Histopathological Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2104.13797v1
- **DOI**: 10.1007/978-3-030-59520-3_18
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.13797v1)
- **Published**: 2021-04-28 14:37:23+00:00
- **Updated**: 2021-04-28 14:37:23+00:00
- **Authors**: Dejan Stepec, Danijel Skocaj
- **Comment**: MICCAI 2020 SASHIMI workshop
- **Journal**: None
- **Summary**: Anomaly detection in visual data refers to the problem of differentiating abnormal appearances from normal cases. Supervised approaches have been successfully applied to different domains, but require an abundance of labeled data. Due to the nature of how anomalies occur and their underlying generating processes, it is hard to characterize and label them. Recent advances in deep generative-based models have sparked interest in applying such methods for unsupervised anomaly detection and have shown promising results in medical and industrial inspection domains. In this work we evaluate a crucial part of the unsupervised visual anomaly detection pipeline, that is needed for normal appearance modeling, as well as the ability to reconstruct closest looking normal and tumor samples. We adapt and evaluate different high-resolution state-of-the-art generative models from the face synthesis domain and demonstrate their superiority over currently used approaches on a challenging domain of digital pathology. Multifold improvement in image synthesis is demonstrated in terms of the quality and resolution of the generated images, validated also against the supervised model.



### Does Face Recognition Error Echo Gender Classification Error?
- **Arxiv ID**: http://arxiv.org/abs/2104.13803v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2104.13803v1)
- **Published**: 2021-04-28 14:43:31+00:00
- **Updated**: 2021-04-28 14:43:31+00:00
- **Authors**: Ying Qiu, Vítor Albiero, Michael C. King, Kevin W. Bowyer
- **Comment**: None
- **Journal**: None
- **Summary**: This paper is the first to explore the question of whether images that are classified incorrectly by a face analytics algorithm (e.g., gender classification) are any more or less likely to participate in an image pair that results in a face recognition error. We analyze results from three different gender classification algorithms (one open-source and two commercial), and two face recognition algorithms (one open-source and one commercial), on image sets representing four demographic groups (African-American female and male, Caucasian female and male). For impostor image pairs, our results show that pairs in which one image has a gender classification error have a better impostor distribution than pairs in which both images have correct gender classification, and so are less likely to generate a false match error. For genuine image pairs, our results show that individuals whose images have a mix of correct and incorrect gender classification have a worse genuine distribution (increased false non-match rate) compared to individuals whose images all have correct gender classification. Thus, compared to images that generate correct gender classification, images that generate gender classification errors do generate a different pattern of recognition errors, both better (false match) and worse (false non-match).



### Recent Advances on Non-Line-of-Sight Imaging: Conventional Physical Models, Deep Learning, and New Scenes
- **Arxiv ID**: http://arxiv.org/abs/2104.13807v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.13807v2)
- **Published**: 2021-04-28 14:56:04+00:00
- **Updated**: 2021-11-21 06:04:33+00:00
- **Authors**: Ruixu Geng, Yang Hu, Yan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: As an emerging technology that has attracted huge attention, non-line-of-sight (NLOS) imaging can reconstruct hidden objects by analyzing the diffuse reflection on a relay surface, with broad application prospects in the fields of autonomous driving, medical imaging, and defense. Despite the challenges of low signal-to-noise ratio (SNR) and high ill-posedness, NLOS imaging has been developed rapidly in recent years. Most current NLOS imaging technologies use conventional physical models, constructing imaging models through active or passive illumination and using reconstruction algorithms to restore hidden scenes. Moreover, deep learning algorithms for NLOS imaging have also received much attention recently. This paper presents a comprehensive overview of both conventional and deep learning-based NLOS imaging techniques. Besides, we also survey new proposed NLOS scenes, and discuss the challenges and prospects of existing technologies. Such a survey can help readers have an overview of different types of NLOS imaging, thus expediting the development of seeing around corners.



### Sign Segmentation with Changepoint-Modulated Pseudo-Labelling
- **Arxiv ID**: http://arxiv.org/abs/2104.13817v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13817v1)
- **Published**: 2021-04-28 15:05:19+00:00
- **Updated**: 2021-04-28 15:05:19+00:00
- **Authors**: Katrin Renz, Nicolaj C. Stache, Neil Fox, Gül Varol, Samuel Albanie
- **Comment**: Appears in: 2021 IEEE Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW'21). 11 pages
- **Journal**: None
- **Summary**: The objective of this work is to find temporal boundaries between signs in continuous sign language. Motivated by the paucity of annotation available for this task, we propose a simple yet effective algorithm to improve segmentation performance on unlabelled signing footage from a domain of interest. We make the following contributions: (1) We motivate and introduce the task of source-free domain adaptation for sign language segmentation, in which labelled source data is available for an initial training phase, but is not available during adaptation. (2) We propose the Changepoint-Modulated Pseudo-Labelling (CMPL) algorithm to leverage cues from abrupt changes in motion-sensitive feature space to improve pseudo-labelling quality for adaptation. (3) We showcase the effectiveness of our approach for category-agnostic sign segmentation, transferring from the BSLCORPUS to the BSL-1K and RWTH-PHOENIX-Weather 2014 datasets, where we outperform the prior state of the art.



### Automated System for Ship Detection from Medium Resolution Satellite Optical Imagery
- **Arxiv ID**: http://arxiv.org/abs/2104.13923v1
- **DOI**: 10.23919/OCEANS40490.2019.8962707
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13923v1)
- **Published**: 2021-04-28 15:06:18+00:00
- **Updated**: 2021-04-28 15:06:18+00:00
- **Authors**: Dejan Stepec, Tomaz Martincic, Danijel Skocaj
- **Comment**: OCEANS 2019 paper
- **Journal**: None
- **Summary**: In this paper, we present a ship detection pipeline for low-cost medium resolution satellite optical imagery obtained from ESA Sentinel-2 and Planet Labs Dove constellations. This optical satellite imagery is readily available for any place on Earth and underutilized in the maritime domain, compared to existing solutions based on synthetic-aperture radar (SAR) imagery. We developed a ship detection method based on a state-of-the-art deep-learning-based object detection method which was developed and evaluated on a large-scale dataset that was collected and automatically annotated with the help of Automatic Identification System (AIS) data.



### DeepSatData: Building large scale datasets of satellite images for training machine learning models
- **Arxiv ID**: http://arxiv.org/abs/2104.13824v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13824v2)
- **Published**: 2021-04-28 15:13:12+00:00
- **Updated**: 2021-05-06 17:02:06+00:00
- **Authors**: Michail Tarasiou, Stefanos Zafeiriou
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: This report presents design considerations for automatically generating satellite imagery datasets for training machine learning models with emphasis placed on dense classification tasks, e.g. semantic segmentation. The implementation presented makes use of freely available Sentinel-2 data which allows generation of large scale datasets required for training deep neural networks. We discuss issues faced from the point of view of deep neural network training and evaluation such as checking the quality of ground truth data and comment on the scalability of the approach. Accompanying code is provided in https://github.com/michaeltrs/DeepSatData.



### Deep Learning Body Region Classification of MRI and CT examinations
- **Arxiv ID**: http://arxiv.org/abs/2104.13826v2
- **DOI**: 10.1007/s10278-022-00767-9
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13826v2)
- **Published**: 2021-04-28 15:20:21+00:00
- **Updated**: 2021-06-29 15:17:03+00:00
- **Authors**: Philippe Raffy, Jean-François Pambrun, Ashish Kumar, David Dubois, Jay Waldron Patti, Robyn Alexandra Cairns, Ryan Young
- **Comment**: 21 pages, 2 figures, 4 tables
- **Journal**: Journal of Digital Imaging (2023) 1-11
- **Summary**: Standardized body region labelling of individual images provides data that can improve human and computer use of medical images. A CNN-based classifier was developed to identify body regions in CT and MRI. 17 CT (18 MRI) body regions covering the entire human body were defined for the classification task. Three retrospective databases were built for the AI model training, validation, and testing, with a balanced distribution of studies per body region. The test databases originated from a different healthcare network. Accuracy, recall and precision of the classifier was evaluated for patient age, patient gender, institution, scanner manufacturer, contrast, slice thickness, MRI sequence, and CT kernel. The data included a retrospective cohort of 2,934 anonymized CT cases (training: 1,804 studies, validation: 602 studies, test: 528 studies) and 3,185 anonymized MRI cases (training: 1,911 studies, validation: 636 studies, test: 638 studies). 27 institutions from primary care hospitals, community hospitals and imaging centers contributed to the test datasets. The data included cases of all genders in equal proportions and subjects aged from a few months old to +90 years old. An image-level prediction accuracy of 91.9% (90.2 - 92.1) for CT, and 94.2% (92.0 - 95.6) for MRI was achieved. The classification results were robust across all body regions and confounding factors. Due to limited data, performance results for subjects under 10 years-old could not be reliably evaluated. We show that deep learning models can classify CT and MRI images by body region including lower and upper extremities with high accuracy.



### Twins: Revisiting the Design of Spatial Attention in Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.13840v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13840v4)
- **Published**: 2021-04-28 15:42:31+00:00
- **Updated**: 2021-09-30 02:16:36+00:00
- **Authors**: Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, Chunhua Shen
- **Comment**: Accepted to NeurIPS2021
- **Journal**: None
- **Summary**: Very recently, a variety of vision transformer architectures for dense prediction tasks have been proposed and they show that the design of spatial attention is critical to their success in these tasks. In this work, we revisit the design of the spatial attention and demonstrate that a carefully-devised yet simple spatial attention mechanism performs favourably against the state-of-the-art schemes. As a result, we propose two vision transformer architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures are highly-efficient and easy to implement, only involving matrix multiplications that are highly optimized in modern deep learning frameworks. More importantly, the proposed architectures achieve excellent performance on a wide range of visual tasks, including image level classification as well as dense detection and segmentation. The simplicity and strong performance suggest that our proposed architectures may serve as stronger backbones for many vision tasks. Our code is released at https://github.com/Meituan-AutoML/Twins .



### D-OccNet: Detailed 3D Reconstruction Using Cross-Domain Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.13854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13854v1)
- **Published**: 2021-04-28 16:00:54+00:00
- **Updated**: 2021-04-28 16:00:54+00:00
- **Authors**: Minhaj Uddin Ansari, Talha Bilal, Naeem Akhter
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning based 3D reconstruction of single view 2D image is becoming increasingly popular due to their wide range of real-world applications, but this task is inherently challenging because of the partial observability of an object from a single perspective. Recently, state of the art probability based Occupancy Networks reconstructed 3D surfaces from three different types of input domains: single view 2D image, point cloud and voxel. In this study, we extend the work on Occupancy Networks by exploiting cross-domain learning of image and point cloud domains. Specifically, we first convert the single view 2D image into a simpler point cloud representation, and then reconstruct a 3D surface from it. Our network, the Double Occupancy Network (D-OccNet) outperforms Occupancy Networks in terms of visual quality and details captured in the 3D reconstruction.



### Removing Word-Level Spurious Alignment between Images and Pseudo-Captions in Unsupervised Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2104.13872v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.13872v2)
- **Published**: 2021-04-28 16:36:52+00:00
- **Updated**: 2021-06-01 07:04:37+00:00
- **Authors**: Ukyo Honda, Yoshitaka Ushiku, Atsushi Hashimoto, Taro Watanabe, Yuji Matsumoto
- **Comment**: EACL 2021 (11 pages, 3 figures; added references)
- **Journal**: None
- **Summary**: Unsupervised image captioning is a challenging task that aims at generating captions without the supervision of image-sentence pairs, but only with images and sentences drawn from different sources and object labels detected from the images. In previous work, pseudo-captions, i.e., sentences that contain the detected object labels, were assigned to a given image. The focus of the previous work was on the alignment of input images and pseudo-captions at the sentence level. However, pseudo-captions contain many words that are irrelevant to a given image. In this work, we investigate the effect of removing mismatched words from image-sentence alignment to determine how they make this task difficult. We propose a simple gating mechanism that is trained to align image features with only the most reliable words in pseudo-captions: the detected object labels. The experimental results show that our proposed method outperforms the previous methods without introducing complex sentence-level learning objectives. Combined with the sentence-level alignment method of previous work, our method further improves its performance. These results confirm the importance of careful alignment in word-level details.



### Exploring Relational Context for Multi-Task Dense Prediction
- **Arxiv ID**: http://arxiv.org/abs/2104.13874v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13874v2)
- **Published**: 2021-04-28 16:45:56+00:00
- **Updated**: 2021-08-23 12:00:27+00:00
- **Authors**: David Bruggemann, Menelaos Kanakis, Anton Obukhov, Stamatios Georgoulis, Luc Van Gool
- **Comment**: International Conference on Computer Vision (ICCV) 2021
- **Journal**: None
- **Summary**: The timeline of computer vision research is marked with advances in learning and utilizing efficient contextual representations. Most of them, however, are targeted at improving model performance on a single downstream task. We consider a multi-task environment for dense prediction tasks, represented by a common backbone and independent task-specific heads. Our goal is to find the most efficient way to refine each task prediction by capturing cross-task contexts dependent on tasks' relations. We explore various attention-based contexts, such as global and local, in the multi-task setting and analyze their behavior when applied to refine each task independently. Empirical findings confirm that different source-target task pairs benefit from different context types. To automate the selection process, we propose an Adaptive Task-Relational Context (ATRC) module, which samples the pool of all available contexts for each task pair using neural architecture search and outputs the optimal configuration for deployment. Our method achieves state-of-the-art performance on two important multi-task benchmarks, namely NYUD-v2 and PASCAL-Context. The proposed ATRC has a low computational toll and can be used as a drop-in refinement module for any supervised multi-task architecture.



### PDNet: Toward Better One-Stage Object Detection With Prediction Decoupling
- **Arxiv ID**: http://arxiv.org/abs/2104.13876v2
- **DOI**: 10.1109/TIP.2022.3193223
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13876v2)
- **Published**: 2021-04-28 16:48:04+00:00
- **Updated**: 2022-12-01 13:00:49+00:00
- **Authors**: Li Yang, Yan Xu, Shaoru Wang, Chunfeng Yuan, Ziqi Zhang, Bing Li, Weiming Hu
- **Comment**: IEEE Transactions on Image Processing, 2022
- **Journal**: None
- **Summary**: Recent one-stage object detectors follow a per-pixel prediction approach that predicts both the object category scores and boundary positions from every single grid location. However, the most suitable positions for inferring different targets, i.e., the object category and boundaries, are generally different. Predicting all these targets from the same grid location thus may lead to sub-optimal results. In this paper, we analyze the suitable inference positions for object category and boundaries, and propose a prediction-target-decoupled detector named PDNet to establish a more flexible detection paradigm. Our PDNet with the prediction decoupling mechanism encodes different targets separately in different locations. A learnable prediction collection module is devised with two sets of dynamic points, i.e., dynamic boundary points and semantic points, to collect and aggregate the predictions from the favorable regions for localization and classification. We adopt a two-step strategy to learn these dynamic point positions, where the prior positions are estimated for different targets first, and the network further predicts residual offsets to the positions with better perceptions of the object properties. Extensive experiments on the MS COCO benchmark demonstrate the effectiveness and efficiency of our method. With a single ResNeXt-64x4d-101-DCN as the backbone, our detector achieves 50.1 AP with single-scale testing, which outperforms the state-of-the-art methods by an appreciable margin under the same experimental settings.Moreover, our detector is highly efficient as a one-stage framework. Our code is public at https://github.com/yangli18/PDNet.



### A Smartphone based Application for Skin Cancer Classification Using Deep Learning with Clinical Images and Lesion Information
- **Arxiv ID**: http://arxiv.org/abs/2104.14353v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.14353v1)
- **Published**: 2021-04-28 16:51:00+00:00
- **Updated**: 2021-04-28 16:51:00+00:00
- **Authors**: Breno Krohling, Pedro B. C. Castro, Andre G. C. Pacheco, Renato A. Krohling
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last decades, the incidence of skin cancer, melanoma and non-melanoma, has increased at a continuous rate. In particular for melanoma, the deadliest type of skin cancer, early detection is important to increase patient prognosis. Recently, deep neural networks (DNNs) have become viable to deal with skin cancer detection. In this work, we present a smartphone-based application to assist on skin cancer detection. This application is based on a Convolutional Neural Network(CNN) trained on clinical images and patients demographics, both collected from smartphones. Also, as skin cancer datasets are imbalanced, we present an approach, based on the mutation operator of Differential Evolution (DE) algorithm, to balance data. In this sense, beyond provides a flexible tool to assist doctors on skin cancer screening phase, the method obtains promising results with a balanced accuracy of 85% and a recall of 96%.



### A Deep Transfer Learning-based Edge Computing Method for Home Health Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2105.02960v1
- **DOI**: 10.1109/CISS50987.2021.9400321
- **Categories**: **cs.CV**, cs.HC, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2105.02960v1)
- **Published**: 2021-04-28 17:01:41+00:00
- **Updated**: 2021-04-28 17:01:41+00:00
- **Authors**: Abu Sufian, Changsheng You, Mianxiong Dong
- **Comment**: 6 pages, 4 figures. Pre-print copy
- **Journal**: 55th Annual Conference on Information Sciences and Systems (CISS),
  2021
- **Summary**: The health-care gets huge stress in a pandemic or epidemic situation. Some diseases such as COVID-19 that causes a pandemic is highly spreadable from an infected person to others. Therefore, providing health services at home for non-critical infected patients with isolation shall assist to mitigate this kind of stress. In addition, this practice is also very useful for monitoring the health-related activities of elders who live at home. The home health monitoring, a continuous monitoring of a patient or elder at home using visual sensors is one such non-intrusive sub-area of health services at home. In this article, we propose a transfer learning-based edge computing method for home health monitoring. Specifically, a pre-trained convolutional neural network-based model can leverage edge devices with a small amount of ground-labeled data and fine-tuning method to train the model. Therefore, on-site computing of visual data captured by RGB, depth, or thermal sensor could be possible in an affordable way. As a result, raw data captured by these types of sensors is not required to be sent outside from home. Therefore, privacy, security, and bandwidth scarcity shall not be issues. Moreover, real-time computing for the above-mentioned purposes shall be possible in an economical way.



### Classification and comparison of license plates localization algorithms
- **Arxiv ID**: http://arxiv.org/abs/2104.13896v1
- **DOI**: 10.5121/sipij.2021.12201
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13896v1)
- **Published**: 2021-04-28 17:26:52+00:00
- **Updated**: 2021-04-28 17:26:52+00:00
- **Authors**: Mustapha Saidallah, Fatimazahra Taki, Abdelbaki El Belrhiti El Alaoui, Abdeslam El Fergougui
- **Comment**: 11 pages
- **Journal**: April 2021, Volume 12
- **Summary**: The Intelligent Transportation Systems (ITS) are the subject of a world economic competition. They are the application of new information and communication technologies in the transport sector, to make the infrastructures more efficient, more reliable and more ecological. License Plates Recognition (LPR) is the key module of these systems, in which the License Plate Localization (LPL) is the most important stage, because it determines the speed and robustness of this module. Thus, during this step the algorithm must process the image and overcome several constraints as climatic and lighting conditions, sensors and angles variety, LPs no-standardization, and the real time processing. This paper presents a classification and comparison of License Plates Localization (LPL) algorithms and describes the advantages, disadvantages and improvements made by each of them



### Inpainting Transformer for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.13897v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13897v3)
- **Published**: 2021-04-28 17:27:44+00:00
- **Updated**: 2021-11-26 09:05:20+00:00
- **Authors**: Jonathan Pirnay, Keng Chai
- **Comment**: Accepted to ICIAP2021
- **Journal**: None
- **Summary**: Anomaly detection in computer vision is the task of identifying images which deviate from a set of normal images. A common approach is to train deep convolutional autoencoders to inpaint covered parts of an image and compare the output with the original image. By training on anomaly-free samples only, the model is assumed to not being able to reconstruct anomalous regions properly. For anomaly detection by inpainting we suggest it to be beneficial to incorporate information from potentially distant regions. In particular we pose anomaly detection as a patch-inpainting problem and propose to solve it with a purely self-attention based approach discarding convolutions. The proposed Inpainting Transformer (InTra) is trained to inpaint covered patches in a large sequence of image patches, thereby integrating information across large regions of the input image. When training from scratch, in comparison to other methods not using extra training data, InTra achieves results on par with the current state-of-the-art on the MVTec AD dataset for detection and surpassing them on segmentation.



### Deep Learning for Rheumatoid Arthritis: Joint Detection and Damage Scoring in X-rays
- **Arxiv ID**: http://arxiv.org/abs/2104.13915v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13915v2)
- **Published**: 2021-04-28 17:53:19+00:00
- **Updated**: 2022-11-04 13:52:43+00:00
- **Authors**: Krzysztof Maziarz, Anna Krason, Zbigniew Wojna
- **Comment**: Presented at the Workshop on AI for Public Health at ICLR 2021
- **Journal**: None
- **Summary**: Recent advancements in computer vision promise to automate medical image analysis. Rheumatoid arthritis is an autoimmune disease that would profit from computer-based diagnosis, as there are no direct markers known, and doctors have to rely on manual inspection of X-ray images. In this work, we present a multi-task deep learning model that simultaneously learns to localize joints on X-ray images and diagnose two kinds of joint damage: narrowing and erosion. Additionally, we propose a modification of label smoothing, which combines classification and regression cues into a single loss and achieves 5% relative error reduction compared to standard loss functions. Our final model obtained 4th place in joint space narrowing and 5th place in joint erosion in the global RA2 DREAM challenge.



### Learning Synergistic Attention for Light Field Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.13916v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13916v4)
- **Published**: 2021-04-28 17:56:04+00:00
- **Updated**: 2021-10-22 18:41:08+00:00
- **Authors**: Yi Zhang, Geng Chen, Qian Chen, Yujia Sun, Yong Xia, Olivier Deforges, Wassim Hamidouche, Lu Zhang
- **Comment**: 20 pages, 12 figures; Project Page https://github.com/PanoAsh/SA-Net
  ; Accepted to BMVC-21
- **Journal**: None
- **Summary**: We propose a novel Synergistic Attention Network (SA-Net) to address the light field salient object detection by establishing a synergistic effect between multi-modal features with advanced attention mechanisms. Our SA-Net exploits the rich information of focal stacks via 3D convolutional neural networks, decodes the high-level features of multi-modal light field data with two cascaded synergistic attention modules, and predicts the saliency map using an effective feature fusion module in a progressive manner. Extensive experiments on three widely-used benchmark datasets show that our SA-Net outperforms 28 state-of-the-art models, sufficiently demonstrating its effectiveness and superiority. Our code is available at https://github.com/PanoAsh/SA-Net.



### LambdaUNet: 2.5D Stroke Lesion Segmentation of Diffusion-weighted MR Images
- **Arxiv ID**: http://arxiv.org/abs/2104.13917v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.13917v2)
- **Published**: 2021-04-28 17:56:04+00:00
- **Updated**: 2023-05-30 00:00:51+00:00
- **Authors**: Yanglan Ou, Ye Yuan, Xiaolei Huang, Kelvin Wong, John Volpi, James Z. Wang, Stephen T. C. Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion-weighted (DW) magnetic resonance imaging is essential for the diagnosis and treatment of ischemic stroke. DW images (DWIs) are usually acquired in multi-slice settings where lesion areas in two consecutive 2D slices are highly discontinuous due to large slice thickness and sometimes even slice gaps. Therefore, although DWIs contain rich 3D information, they cannot be treated as regular 3D or 2D images. Instead, DWIs are somewhere in-between (or 2.5D) due to the volumetric nature but inter-slice discontinuities. Thus, it is not ideal to apply most existing segmentation methods as they are designed for either 2D or 3D images. To tackle this problem, we propose a new neural network architecture tailored for segmenting highly-discontinuous 2.5D data such as DWIs. Our network, termed LambdaUNet, extends UNet by replacing convolutional layers with our proposed Lambda+ layers. In particular, Lambda+ layers transform both intra-slice and inter-slice context around a pixel into linear functions, called lambdas, which are then applied to the pixel to produce informative 2.5D features. LambdaUNet is simple yet effective in combining sparse inter-slice information from adjacent slices while also capturing dense contextual features within a single slice. Experiments on a unique clinical dataset demonstrate that LambdaUNet outperforms existing 3D/2D image segmentation methods including recent variants of UNet. Code for LambdaUNet is released with the publication to facilitate future research.



### High-Resolution Optical Flow from 1D Attention and Correlation
- **Arxiv ID**: http://arxiv.org/abs/2104.13918v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13918v2)
- **Published**: 2021-04-28 17:56:34+00:00
- **Updated**: 2021-08-29 13:07:14+00:00
- **Authors**: Haofei Xu, Jiaolong Yang, Jianfei Cai, Juyong Zhang, Xin Tong
- **Comment**: ICCV 2021, Oral
- **Journal**: None
- **Summary**: Optical flow is inherently a 2D search problem, and thus the computational complexity grows quadratically with respect to the search window, making large displacements matching infeasible for high-resolution images. In this paper, we take inspiration from Transformers and propose a new method for high-resolution optical flow estimation with significantly less computation. Specifically, a 1D attention operation is first applied in the vertical direction of the target image, and then a simple 1D correlation in the horizontal direction of the attended image is able to achieve 2D correspondence modeling effect. The directions of attention and correlation can also be exchanged, resulting in two 3D cost volumes that are concatenated for optical flow estimation. The novel 1D formulation empowers our method to scale to very high-resolution input images while maintaining competitive performance. Extensive experiments on Sintel, KITTI and real-world 4K ($2160 \times 3840$) resolution images demonstrated the effectiveness and superiority of our proposed method. Code and models are available at \url{https://github.com/haofeixu/flow1d}.



### Open-vocabulary Object Detection via Vision and Language Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2104.13921v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13921v3)
- **Published**: 2021-04-28 17:58:57+00:00
- **Updated**: 2022-05-12 01:27:40+00:00
- **Authors**: Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, Yin Cui
- **Comment**: ICLR Camera Ready
- **Journal**: ICLR 2022
- **Summary**: We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask AP$_r$ with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 AP$_r$. The model can directly transfer to other datasets without finetuning, achieving 72.2 AP$_{50}$ on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-the-art by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild.



### Motion-guided Non-local Spatial-Temporal Network for Video Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2104.13946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13946v1)
- **Published**: 2021-04-28 18:05:13+00:00
- **Updated**: 2021-04-28 18:05:13+00:00
- **Authors**: Haoyue Bai, S. -H. Gary Chan
- **Comment**: None
- **Journal**: None
- **Summary**: We study video crowd counting, which is to estimate the number of objects (people in this paper) in all the frames of a video sequence. Previous work on crowd counting is mostly on still images. There has been little work on how to properly extract and take advantage of the spatial-temporal correlation between neighboring frames in both short and long ranges to achieve high estimation accuracy for a video sequence. In this work, we propose Monet, a novel and highly accurate motion-guided non-local spatial-temporal network for video crowd counting. Monet first takes people flow (motion information) as guidance to coarsely segment the regions of pixels where a person may be. Given these regions, Monet then uses a non-local spatial-temporal network to extract spatial-temporally both short and long-range contextual information. The whole network is finally trained end-to-end with a fused loss to generate a high-quality density map. Noting the scarcity and low quality (in terms of resolution and scene diversity) of the publicly available video crowd datasets, we have collected and built a large-scale video crowd counting datasets, VidCrowd, to contribute to the community. VidCrowd contains 9,000 frames of high resolution (2560 x 1440), with 1,150,239 head annotations captured in different scenes, crowd density and lighting in two cities. We have conducted extensive experiments on the challenging VideoCrowd and two public video crowd counting datasets: UCSD and Mall. Our approach achieves substantially better performance in terms of MAE and MSE as compared with other state-of-the-art approaches.



### MeerCRAB: MeerLICHT Classification of Real and Bogus Transients using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.13950v1
- **DOI**: 10.1007/s10686-021-09757-1
- **Categories**: **astro-ph.IM**, astro-ph.GA, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13950v1)
- **Published**: 2021-04-28 18:12:51+00:00
- **Updated**: 2021-04-28 18:12:51+00:00
- **Authors**: Zafiirah Hosenie, Steven Bloemen, Paul Groot, Robert Lyon, Bart Scheers, Benjamin Stappers, Fiorenzo Stoppa, Paul Vreeswijk, Simon De Wet, Marc Klein Wolt, Elmar Körding, Vanessa McBride, Rudolf Le Poole, Kerry Paterson, Daniëlle L. A. Pieterse, Patrick Woudt
- **Comment**: 15 pages, 13 figures, Accepted for publication in Experimental
  Astronomy and appeared in the 3rd Workshop on Machine Learning and the
  Physical Sciences, NeurIPS 2020
- **Journal**: Exp Astron (2021)
- **Summary**: Astronomers require efficient automated detection and classification pipelines when conducting large-scale surveys of the (optical) sky for variable and transient sources. Such pipelines are fundamentally important, as they permit rapid follow-up and analysis of those detections most likely to be of scientific value. We therefore present a deep learning pipeline based on the convolutional neural network architecture called $\texttt{MeerCRAB}$. It is designed to filter out the so called 'bogus' detections from true astrophysical sources in the transient detection pipeline of the MeerLICHT telescope. Optical candidates are described using a variety of 2D images and numerical features extracted from those images. The relationship between the input images and the target classes is unclear, since the ground truth is poorly defined and often the subject of debate. This makes it difficult to determine which source of information should be used to train a classification algorithm. We therefore used two methods for labelling our data (i) thresholding and (ii) latent class model approaches. We deployed variants of $\texttt{MeerCRAB}$ that employed different network architectures trained using different combinations of input images and training set choices, based on classification labels provided by volunteers. The deepest network worked best with an accuracy of 99.5$\%$ and Matthews correlation coefficient (MCC) value of 0.989. The best model was integrated to the MeerLICHT transient vetting pipeline, enabling the accurate and efficient classification of detected transients that allows researchers to select the most promising candidates for their research goals.



### UVStyle-Net: Unsupervised Few-shot Learning of 3D Style Similarity Measure for B-Reps
- **Arxiv ID**: http://arxiv.org/abs/2105.02961v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T07, 68T10, I.5.1; I.3.5
- **Links**: [PDF](http://arxiv.org/pdf/2105.02961v3)
- **Published**: 2021-04-28 18:14:01+00:00
- **Updated**: 2021-10-11 15:12:52+00:00
- **Authors**: Peter Meltzer, Hooman Shayani, Amir Khasahmadi, Pradeep Kumar Jayaraman, Aditya Sanghi, Joseph Lambourne
- **Comment**: ICCV 2021 main track 13 pages, 20 figures, 5 tables
- **Journal**: None
- **Summary**: Boundary Representations (B-Reps) are the industry standard in 3D Computer Aided Design/Manufacturing (CAD/CAM) and industrial design due to their fidelity in representing stylistic details. However, they have been ignored in the 3D style research. Existing 3D style metrics typically operate on meshes or pointclouds, and fail to account for end-user subjectivity by adopting fixed definitions of style, either through crowd-sourcing for style labels or hand-crafted features. We propose UVStyle-Net, a style similarity measure for B-Reps that leverages the style signals in the second order statistics of the activations in a pre-trained (unsupervised) 3D encoder, and learns their relative importance to a subjective end-user through few-shot learning. Our approach differs from all existing data-driven 3D style methods since it may be used in completely unsupervised settings, which is desirable given the lack of publicly available labelled B-Rep datasets. More importantly, the few-shot learning accounts for the inherent subjectivity associated with style. We show quantitatively that our proposed method with B-Reps is able to capture stronger style signals than alternative methods on meshes and pointclouds despite its significantly greater computational efficiency. We also show it is able to generate meaningful style gradients with respect to the input shape, and that few-shot learning with as few as two positive examples selected by an end-user is sufficient to significantly improve the style measure. Finally, we demonstrate its efficacy on a large unlabeled public dataset of CAD models. Source code and data will be released in the future.



### Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples
- **Arxiv ID**: http://arxiv.org/abs/2104.13963v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.13963v3)
- **Published**: 2021-04-28 18:44:07+00:00
- **Updated**: 2021-07-30 18:39:16+00:00
- **Authors**: Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand Joulin, Nicolas Ballas, Michael Rabbat
- **Comment**: None
- **Journal**: ICCV 2021
- **Summary**: This paper proposes a novel method of learning by predicting view assignments with support samples (PAWS). The method trains a model to minimize a consistency loss, which ensures that different views of the same unlabeled instance are assigned similar pseudo-labels. The pseudo-labels are generated non-parametrically, by comparing the representations of the image views to those of a set of randomly sampled labeled images. The distance between the view representations and labeled representations is used to provide a weighting over class labels, which we interpret as a soft pseudo-label. By non-parametrically incorporating labeled samples in this way, PAWS extends the distance-metric loss used in self-supervised methods such as BYOL and SwAV to the semi-supervised setting. Despite the simplicity of the approach, PAWS outperforms other semi-supervised methods across architectures, setting a new state-of-the-art for a ResNet-50 on ImageNet trained with either 10% or 1% of the labels, reaching 75.5% and 66.5% top-1 respectively. PAWS requires 4x to 12x less training than the previous best methods.



### Filter Distribution Templates in Convolutional Networks for Image Classification Tasks
- **Arxiv ID**: http://arxiv.org/abs/2104.13993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.13993v1)
- **Published**: 2021-04-28 19:47:22+00:00
- **Updated**: 2021-04-28 19:47:22+00:00
- **Authors**: Ramon Izquierdo-Cordova, Walterio Mayol-Cuevas
- **Comment**: arXiv admin note: text overlap with arXiv:2104.08446
- **Journal**: None
- **Summary**: Neural network designers have reached progressive accuracy by increasing models depth, introducing new layer types and discovering new combinations of layers. A common element in many architectures is the distribution of the number of filters in each layer. Neural network models keep a pattern design of increasing filters in deeper layers such as those in LeNet, VGG, ResNet, MobileNet and even in automatic discovered architectures such as NASNet. It remains unknown if this pyramidal distribution of filters is the best for different tasks and constrains. In this work we present a series of modifications in the distribution of filters in four popular neural network models and their effects in accuracy and resource consumption. Results show that by applying this approach, some models improve up to 8.9% in accuracy showing reductions in parameters up to 54%.



### EmergencyNet: Efficient Aerial Image Classification for Drone-Based Emergency Monitoring Using Atrous Convolutional Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/2104.14006v1
- **DOI**: 10.1109/JSTARS.2020.2969809
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.14006v1)
- **Published**: 2021-04-28 20:24:10+00:00
- **Updated**: 2021-04-28 20:24:10+00:00
- **Authors**: Christos Kyrkou, Theocharis Theocharides
- **Comment**: C.Kyrkou and T. Theocharides, "EmergencyNet: Efficient Aerial Image
  Classification for Drone-Based Emergency Monitoring Using Atrous
  Convolutional Feature Fusion," in IEEE J Sel Top Appl Earth Obs Remote Sens.
  (JSTARS), vol. 13, pp. 1687-1699, 2020. arXiv admin note: substantial text
  overlap with arXiv:1906.08716
- **Journal**: IEEE Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing ( Volume: 13), Page(s): 1687 - 1699, 2020
- **Summary**: Deep learning-based algorithms can provide state-of-the-art accuracy for remote sensing technologies such as unmanned aerial vehicles (UAVs)/drones, potentially enhancing their remote sensing capabilities for many emergency response and disaster management applications. In particular, UAVs equipped with camera sensors can operating in remote and difficult to access disaster-stricken areas, analyze the image and alert in the presence of various calamities such as collapsed buildings, flood, or fire in order to faster mitigate their effects on the environment and on human population. However, the integration of deep learning introduces heavy computational requirements, preventing the deployment of such deep neural networks in many scenarios that impose low-latency constraints on inference, in order to make mission-critical decisions in real time. To this end, this article focuses on the efficient aerial image classification from on-board a UAV for emergency response/monitoring applications. Specifically, a dedicated Aerial Image Database for Emergency Response applications is introduced and a comparative analysis of existing approaches is performed. Through this analysis a lightweight convolutional neural network architecture is proposed, referred to as EmergencyNet, based on atrous convolutions to process multiresolution features and capable of running efficiently on low-power embedded platforms achieving upto 20x higher performance compared to existing models with minimal memory requirements with less than 1% accuracy drop compared to state-of-the-art models.



### Interaction-GCN: A Graph Convolutional Network based framework for social interaction recognition in egocentric videos
- **Arxiv ID**: http://arxiv.org/abs/2104.14007v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14007v2)
- **Published**: 2021-04-28 20:25:40+00:00
- **Updated**: 2021-06-08 17:47:20+00:00
- **Authors**: Simone Felicioni, Mariella Dimiccoli
- **Comment**: Accepted to ICIP 2021
- **Journal**: None
- **Summary**: In this paper we propose a new framework to categorize social interactions in egocentric videos, we named InteractionGCN. Our method extracts patterns of relational and non-relational cues at the frame level and uses them to build a relational graph from which the interactional context at the frame level is estimated via a Graph Convolutional Network based approach. Then it propagates this context over time, together with first-person motion information, through a Gated Recurrent Unit architecture. Ablation studies and experimental evaluation on two publicly available datasets validate the proposed approach and establish state of the art results.



### Reducing Risk and Uncertainty of Deep Neural Networks on Diagnosing COVID-19 Infection
- **Arxiv ID**: http://arxiv.org/abs/2104.14029v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.14029v1)
- **Published**: 2021-04-28 21:36:25+00:00
- **Updated**: 2021-04-28 21:36:25+00:00
- **Authors**: Krishanu Sarker, Sharbani Pandit, Anupam Sarker, Saeid Belkasim, Shihao Ji
- **Comment**: AAAI, TAIH workshop, 2021
- **Journal**: None
- **Summary**: Effective and reliable screening of patients via Computer-Aided Diagnosis can play a crucial part in the battle against COVID-19. Most of the existing works focus on developing sophisticated methods yielding high detection performance, yet not addressing the issue of predictive uncertainty. In this work, we introduce uncertainty estimation to detect confusing cases for expert referral to address the unreliability of state-of-the-art (SOTA) DNNs on COVID-19 detection. To the best of our knowledge, we are the first to address this issue on the COVID-19 detection problem. In this work, we investigate a number of SOTA uncertainty estimation methods on publicly available COVID dataset and present our experimental findings. In collaboration with medical professionals, we further validate the results to ensure the viability of the best performing method in clinical practice.



### Randomized Histogram Matching: A Simple Augmentation for Unsupervised Domain Adaptation in Overhead Imagery
- **Arxiv ID**: http://arxiv.org/abs/2104.14032v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.14032v3)
- **Published**: 2021-04-28 21:59:54+00:00
- **Updated**: 2023-08-11 21:09:25+00:00
- **Authors**: Can Yaras, Kaleb Kassaw, Bohao Huang, Kyle Bradbury, Jordan M. Malof
- **Comment**: Includes a main paper (10 pages). This paper is currently undergoing
  peer review
- **Journal**: None
- **Summary**: Modern deep neural networks (DNNs) are highly accurate on many recognition tasks for overhead (e.g., satellite) imagery. However, visual domain shifts (e.g., statistical changes due to geography, sensor, or atmospheric conditions) remain a challenge, causing the accuracy of DNNs to degrade substantially and unpredictably when testing on new sets of imagery. In this work, we model domain shifts caused by variations in imaging hardware, lighting, and other conditions as non-linear pixel-wise transformations, and we perform a systematic study indicating that modern DNNs can become largely robust to these types of transformations, if provided with appropriate training data augmentation. In general, however, we do not know the transformation between two sets of imagery. To overcome this, we propose a fast real-time unsupervised training augmentation technique, termed randomized histogram matching (RHM). We conduct experiments with two large benchmark datasets for building segmentation and find that despite its simplicity, RHM consistently yields similar or superior performance compared to state-of-the-art unsupervised domain adaptation approaches, while being significantly simpler and more computationally efficient. RHM also offers substantially better performance than other comparably simple approaches that are widely used for overhead imagery.



### Pushing it out of the Way: Interactive Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/2104.14040v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.14040v2)
- **Published**: 2021-04-28 22:46:41+00:00
- **Updated**: 2021-05-02 00:41:22+00:00
- **Authors**: Kuo-Hao Zeng, Luca Weihs, Ali Farhadi, Roozbeh Mottaghi
- **Comment**: 14 pages, 13 figures, CVPR 2021,
  https://prior.allenai.org/projects/interactive-visual-navigation,
  https://youtu.be/GvTI5XCMvPw
- **Journal**: None
- **Summary**: We have observed significant progress in visual navigation for embodied agents. A common assumption in studying visual navigation is that the environments are static; this is a limiting assumption. Intelligent navigation may involve interacting with the environment beyond just moving forward/backward and turning left/right. Sometimes, the best way to navigate is to push something out of the way. In this paper, we study the problem of interactive navigation where agents learn to change the environment to navigate more efficiently to their goals. To this end, we introduce the Neural Interaction Engine (NIE) to explicitly predict the change in the environment caused by the agent's actions. By modeling the changes while planning, we find that agents exhibit significant improvements in their navigational capabilities. More specifically, we consider two downstream tasks in the physics-enabled, visually rich, AI2-THOR environment: (1) reaching a target while the path to the target is blocked (2) moving an object to a target location by pushing it. For both tasks, agents equipped with an NIE significantly outperform agents without the understanding of the effect of the actions indicating the benefits of our approach.



### Weather and Light Level Classification for Autonomous Driving: Dataset, Baseline and Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.14042v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.14042v3)
- **Published**: 2021-04-28 22:53:10+00:00
- **Updated**: 2021-11-29 22:26:28+00:00
- **Authors**: Mahesh M Dhananjaya, Varun Ravi Kumar, Senthil Yogamani
- **Comment**: Accepted for Oral Presentation at IEEE Intelligent Transportation
  Systems Conference (ITSC) 2021. Dataset is released in
  https://drive.google.com/drive/folders/1t3hwbPCfbokUaaWROr6PBA4WTW0GuQJi
- **Journal**: None
- **Summary**: Autonomous driving is rapidly advancing, and Level 2 functions are becoming a standard feature. One of the foremost outstanding hurdles is to obtain robust visual perception in harsh weather and low light conditions where accuracy degradation is severe. It is critical to have a weather classification model to decrease visual perception confidence during these scenarios. Thus, we have built a new dataset for weather (fog, rain, and snow) classification and light level (bright, moderate, and low) classification. Furthermore, we provide street type (asphalt, grass, and cobblestone) classification, leading to 9 labels. Each image has three labels corresponding to weather, light level, and street type. We recorded the data utilizing an industrial front camera of RCCC (red/clear) format with a resolution of $1024\times1084$. We collected 15k video sequences and sampled 60k images. We implement an active learning framework to reduce the dataset's redundancy and find the optimal set of frames for training a model. We distilled the 60k images further to 1.1k images, which will be shared publicly after privacy anonymization. There is no public dataset for weather and light level classification focused on autonomous driving to the best of our knowledge. The baseline ResNet18 network used for weather classification achieves state-of-the-art results in two non-automotive weather classification public datasets but significantly lower accuracy on our proposed dataset, demonstrating it is not saturated and needs further research.



