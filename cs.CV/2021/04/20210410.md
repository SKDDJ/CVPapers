# Arxiv Papers in cs.CV on 2021-04-10
### Towards Automated and Marker-less Parkinson Disease Assessment: Predicting UPDRS Scores using Sit-stand videos
- **Arxiv ID**: http://arxiv.org/abs/2104.04650v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.04650v1)
- **Published**: 2021-04-10 00:05:51+00:00
- **Updated**: 2021-04-10 00:05:51+00:00
- **Authors**: Deval Mehta, Umar Asif, Tian Hao, Erhan Bilal, Stefan Von Cavallar, Stefan Harrer, Jeffrey Rogers
- **Comment**: Accepted by CVPR Workshops 2021
- **Journal**: None
- **Summary**: This paper presents a novel deep learning enabled, video based analysis framework for assessing the Unified Parkinsons Disease Rating Scale (UPDRS) that can be used in the clinic or at home. We report results from comparing the performance of the framework to that of trained clinicians on a population of 32 Parkinsons disease (PD) patients. In-person clinical assessments by trained neurologists are used as the ground truth for training our framework and for comparing the performance. We find that the standard sit-to-stand activity can be used to evaluate the UPDRS sub-scores of bradykinesia (BRADY) and posture instability and gait disorders (PIGD). For BRADY we find F1-scores of 0.75 using our framework compared to 0.50 for the video based rater clinicians, while for PIGD we find 0.78 for the framework and 0.45 for the video based rater clinicians. We believe our proposed framework has potential to provide clinically acceptable end points of PD in greater granularity without imposing burdens on patients and clinicians, which empowers a variety of use cases such as passive tracking of PD progression in spaces such as nursing homes, in-home self-assessment, and enhanced tele-medicine.



### Unveiling personnel movement in a larger indoor area with a non-overlapping multi-camera system
- **Arxiv ID**: http://arxiv.org/abs/2104.04662v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04662v1)
- **Published**: 2021-04-10 01:44:26+00:00
- **Updated**: 2021-04-10 01:44:26+00:00
- **Authors**: Ping Zhang, Zhenxiang Tao, Wenjie Yang, Minze Chen, Shan Ding, Xiaodong Liu, Rui Yang, Hui Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Surveillance cameras are widely applied for indoor occupancy measurement and human movement perception, which benefit for building energy management and social security. To address the challenges of limited view angle of single camera as well as lacking of inter-camera collaboration, this study presents a non-overlapping multi-camera system to enlarge the surveillance area and devotes to retrieve the same person appeared from different camera views. The system is deployed in an office building and four-day videos are collected. By training a deep convolutional neural network, the proposed system first extracts the appearance feature embeddings of each personal image, which detected from different cameras, for similarity comparison. Then, a stochastic inter-camera transition matrix is associated with appearance feature for further improving the person re-identification ranking results. Finally, a noise-suppression explanation is given for analyzing the matching improvements. This paper expands the scope of indoor movement perception based on non-overlapping multiple cameras and improves the accuracy of pedestrian re-identification without introducing additional types of sensors.



### On Universal Black-Box Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2104.04665v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.04665v1)
- **Published**: 2021-04-10 02:21:09+00:00
- **Updated**: 2021-04-10 02:21:09+00:00
- **Authors**: Bin Deng, Yabin Zhang, Hui Tang, Changxing Ding, Kui Jia
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study an arguably least restrictive setting of domain adaptation in a sense of practical deployment, where only the interface of source model is available to the target domain, and where the label-space relations between the two domains are allowed to be different and unknown. We term such a setting as Universal Black-Box Domain Adaptation (UB$^2$DA). The great promise that UB$^2$DA makes, however, brings significant learning challenges, since domain adaptation can only rely on the predictions of unlabeled target data in a partially overlapped label space, by accessing the interface of source model. To tackle the challenges, we first note that the learning task can be converted as two subtasks of in-class\footnote{In this paper we use in-class (out-class) to describe the classes observed (not observed) in the source black-box model.} discrimination and out-class detection, which can be respectively learned by model distillation and entropy separation. We propose to unify them into a self-training framework, regularized by consistency of predictions in local neighborhoods of target samples. Our framework is simple, robust, and easy to be optimized. Experiments on domain adaptation benchmarks show its efficacy. Notably, by accessing the interface of source model only, our framework outperforms existing methods of universal domain adaptation that make use of source data and/or source models, with a newly proposed (and arguably more reasonable) metric of H-score, and performs on par with them with the metric of averaged class accuracy.



### Deep Learning Identifies Neuroimaging Signatures of Alzheimer's Disease Using Structural and Synthesized Functional MRI Data
- **Arxiv ID**: http://arxiv.org/abs/2104.04672v2
- **DOI**: 10.1109/ISBI48211.2021.9433808
- **Categories**: **q-bio.QM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.04672v2)
- **Published**: 2021-04-10 03:16:33+00:00
- **Updated**: 2021-05-28 15:56:28+00:00
- **Authors**: Nanyan Zhu, Chen Liu, Xinyang Feng, Dipika Sikka, Sabrina Gjerswold-Selleck, Scott A. Small, Jia Guo
- **Comment**: Published in IEEE ISBI 2021. Available at
  https://ieeexplore.ieee.org/document/9433808
- **Journal**: None
- **Summary**: Current neuroimaging techniques provide paths to investigate the structure and function of the brain in vivo and have made great advances in understanding Alzheimer's disease (AD). However, the group-level analyses prevalently used for investigation and understanding of the disease are not applicable for diagnosis of individuals. More recently, deep learning, which can efficiently analyze large-scale complex patterns in 3D brain images, has helped pave the way for computer-aided individual diagnosis by providing accurate and automated disease classification. Great progress has been made in classifying AD with deep learning models developed upon increasingly available structural MRI data. The lack of scale-matched functional neuroimaging data prevents such models from being further improved by observing functional changes in pathophysiology. Here we propose a potential solution by first learning a structural-to-functional transformation in brain MRI, and further synthesizing spatially matched functional images from large-scale structural scans. We evaluated our approach by building computational models to discriminate patients with AD from healthy normal subjects and demonstrated a performance boost after combining the structural and synthesized functional brain images into the same model. Furthermore, our regional analyses identified the temporal lobe to be the most predictive structural-region and the parieto-occipital lobe to be the most predictive functional-region of our model, which are both in concordance with previous group-level neuroimaging findings. Together, we demonstrate the potential of deep learning with large-scale structural and synthesized functional MRI to impact AD classification and to identify AD's neuroimaging signatures.



### Learning from 2D: Contrastive Pixel-to-Point Knowledge Transfer for 3D Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2104.04687v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04687v3)
- **Published**: 2021-04-10 05:40:42+00:00
- **Updated**: 2021-12-27 15:19:51+00:00
- **Authors**: Yueh-Cheng Liu, Yu-Kai Huang, Hung-Yueh Chiang, Hung-Ting Su, Zhe-Yu Liu, Chin-Tang Chen, Ching-Yu Tseng, Winston H. Hsu
- **Comment**: None
- **Journal**: None
- **Summary**: Most 3D neural networks are trained from scratch owing to the lack of large-scale labeled 3D datasets. In this paper, we present a novel 3D pretraining method by leveraging 2D networks learned from rich 2D datasets. We propose the contrastive pixel-to-point knowledge transfer to effectively utilize the 2D information by mapping the pixel-level and point-level features into the same embedding space. Due to the heterogeneous nature between 2D and 3D networks, we introduce the back-projection function to align the features between 2D and 3D to make the transfer possible. Additionally, we devise an upsampling feature projection layer to increase the spatial resolution of high-level 2D feature maps, which enables learning fine-grained 3D representations. With a pretrained 2D network, the proposed pretraining process requires no additional 2D or 3D labeled data, further alleviating the expensive 3D data annotation cost. To the best of our knowledge, we are the first to exploit existing 2D trained weights to pretrain 3D deep neural networks. Our intensive experiments show that the 3D models pretrained with 2D knowledge boost the performances of 3D networks across various real-world 3D downstream tasks.



### Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.04691v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04691v1)
- **Published**: 2021-04-10 06:16:25+00:00
- **Updated**: 2021-04-10 06:16:25+00:00
- **Authors**: Weiyao Wang, Matt Feiszli, Heng Wang, Du Tran
- **Comment**: None
- **Journal**: None
- **Summary**: Current state-of-the-art object detection and segmentation methods work well under the closed-world assumption. This closed-world setting assumes that the list of object categories is available during training and deployment. However, many real-world applications require detecting or segmenting novel objects, i.e., object categories never seen during training. In this paper, we present, UVO (Unidentified Video Objects), a new benchmark for open-world class-agnostic object segmentation in videos. Besides shifting the problem focus to the open-world setup, UVO is significantly larger, providing approximately 8 times more videos compared with DAVIS, and 7 times more mask (instance) annotations per video compared with YouTube-VOS and YouTube-VIS. UVO is also more challenging as it includes many videos with crowded scenes and complex background motions. We demonstrated that UVO can be used for other applications, such as object tracking and super-voxel segmentation, besides open-world object segmentation. We believe that UVo is a versatile testbed for researchers to develop novel approaches for open-world class-agnostic object segmentation, and inspires new research directions towards a more comprehensive video understanding beyond classification and detection.



### Object Priors for Classifying and Localizing Unseen Actions
- **Arxiv ID**: http://arxiv.org/abs/2104.04715v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04715v1)
- **Published**: 2021-04-10 08:56:58+00:00
- **Updated**: 2021-04-10 08:56:58+00:00
- **Authors**: Pascal Mettes, William Thong, Cees G. M. Snoek
- **Comment**: Accepted to IJCV
- **Journal**: None
- **Summary**: This work strives for the classification and localization of human actions in videos, without the need for any labeled video training examples. Where existing work relies on transferring global attribute or object information from seen to unseen action videos, we seek to classify and spatio-temporally localize unseen actions in videos from image-based object information only. We propose three spatial object priors, which encode local person and object detectors along with their spatial relations. On top we introduce three semantic object priors, which extend semantic matching through word embeddings with three simple functions that tackle semantic ambiguity, object discrimination, and object naming. A video embedding combines the spatial and semantic object priors. It enables us to introduce a new video retrieval task that retrieves action tubes in video collections based on user-specified objects, spatial relations, and object size. Experimental evaluation on five action datasets shows the importance of spatial and semantic object priors for unseen actions. We find that persons and objects have preferred spatial relations that benefit unseen action localization, while using multiple languages and simple object filtering directly improves semantic matching, leading to state-of-the-art results for both unseen action classification and localization.



### Do as we do: Multiple Person Video-To-Video Transfer
- **Arxiv ID**: http://arxiv.org/abs/2104.04721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04721v1)
- **Published**: 2021-04-10 09:26:31+00:00
- **Updated**: 2021-04-10 09:26:31+00:00
- **Authors**: Mickael Cormier, Houraalsadat Mortazavi Moshkenan, Franz Lörch, Jürgen Metzler, Jürgen Beyerer
- **Comment**: Accepted to IEEE MIPR 2021
- **Journal**: None
- **Summary**: Our goal is to transfer the motion of real people from a source video to a target video with realistic results. While recent advances significantly improved image-to-image translations, only few works account for body motions and temporal consistency. However, those focus only on video re-targeting for a single actor/ for single actors. In this work, we propose a marker-less approach for multiple-person video-to-video transfer using pose as an intermediate representation. Given a source video with multiple persons dancing or working out, our method transfers the body motion of all actors to a new set of actors in a different video. Differently from recent "do as I do" methods, we focus specifically on transferring multiple person at the same time and tackle the related identity switch problem. Our method is able to convincingly transfer body motion to the target video, while preserving specific features of the target video, such as feet touching the floor and relative position of the actors. The evaluation is performed with visual quality and appearance metrics using publicly available videos with the permission of their owners.



### Coastline extraction from ALOS-2 satellite SAR images
- **Arxiv ID**: http://arxiv.org/abs/2104.04722v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.04722v1)
- **Published**: 2021-04-10 09:29:58+00:00
- **Updated**: 2021-04-10 09:29:58+00:00
- **Authors**: Petr Hurtik, Marek Vajgl
- **Comment**: None
- **Journal**: None
- **Summary**: The continuous monitoring of a shore plays an essential role in designing strategies for shore protection against erosion. To avoid the effect of clouds and sunlight, satellite-based imagery with synthetic aperture radar is used to provide the required data. We show how such data can be processed using state-of-the-art methods, namely, by a deep-learning-based approach, to detect the coastline location. We split the process into data reading, data preprocessing, model training, inference, ensembling, and postprocessing, and describe the best techniques for each of the parts. Finally, we present our own solution that is able to precisely extract the coastline from an image even if it is not recognizable by a human. Our solution has been validated against the real GPS location of the coastline during Signate's competition, where it was runner-up among 109 teams across the whole world.



### Occlusion Guided Self-supervised Scene Flow Estimation on 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2104.04724v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04724v2)
- **Published**: 2021-04-10 09:55:19+00:00
- **Updated**: 2021-10-17 02:53:24+00:00
- **Authors**: Bojun Ouyang, Dan Raviv
- **Comment**: Accepted at 3DV 2021 (Poster)
- **Journal**: None
- **Summary**: Understanding the flow in 3D space of sparsely sampled points between two consecutive time frames is the core stone of modern geometric-driven systems such as VR/AR, Robotics, and Autonomous driving. The lack of real, non-simulated, labeled data for this task emphasizes the importance of self- or un-supervised deep architectures. This work presents a new self-supervised training method and an architecture for the 3D scene flow estimation under occlusions. Here we show that smart multi-layer fusion between flow prediction and occlusion detection outperforms traditional architectures by a large margin for occluded and non-occluded scenarios. We report state-of-the-art results on Flyingthings3D and KITTI datasets for both the supervised and self-supervised training.



### A Novel Unified Model for Multi-exposure Stereo Coding Based on Low Rank Tucker-ALS and 3D-HEVC
- **Arxiv ID**: http://arxiv.org/abs/2104.04726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04726v1)
- **Published**: 2021-04-10 10:10:14+00:00
- **Updated**: 2021-04-10 10:10:14+00:00
- **Authors**: Mansi Sharma, Aditya Wadaskar
- **Comment**: None
- **Journal**: None
- **Summary**: Display technology must offer high dynamic range (HDR) contrast-based depth induction and 3D personalization simultaneously. Efficient algorithms to compress HDR stereo data is critical. Direct capturing of HDR content is complicated due to the high expense and scarcity of HDR cameras. The HDR 3D images could be generated in low-cost by fusing low-dynamic-range (LDR) images acquired using a stereo camera with various exposure settings. In this paper, an efficient scheme for coding multi-exposure stereo images is proposed based on a tensor low-rank approximation scheme. The multi-exposure fusion can be realized to generate HDR stereo output at the decoder for increased realism and exaggerated binocular 3D depth cues.   For exploiting spatial redundancy in LDR stereo images, the stack of multi-exposure stereo images is decomposed into a set of projection matrices and a core tensor following an alternating least squares Tucker decomposition model. The compact, low-rank representation of the scene, thus, generated is further processed by 3D extension of High Efficiency Video Coding standard. The encoding with 3D-HEVC enhance the proposed scheme efficiency by exploiting intra-frame, inter-view and the inter-component redundancies in low-rank approximated representation. We consider constant luminance property of IPT and Y'CbCr color space to precisely approximate intensity prediction and perceptually minimize the encoding distortion. Besides, the proposed scheme gives flexibility to adjust the bitrate of tensor latent components by changing the rank of core tensor and its quantization. Extensive experiments on natural scenes demonstrate that the proposed scheme outperforms state-of-the-art JPEG-XT and 3D-HEVC range coding standards.



### Estimation of BMI from Facial Images using Semantic Segmentation based Region-Aware Pooling
- **Arxiv ID**: http://arxiv.org/abs/2104.04733v1
- **DOI**: 10.1016/j.compbiomed.2021.104392
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2104.04733v1)
- **Published**: 2021-04-10 10:53:21+00:00
- **Updated**: 2021-04-10 10:53:21+00:00
- **Authors**: Nadeem Yousaf, Sarfaraz Hussein, Waqas Sultani
- **Comment**: Accepted for publication in computers in biology and medicine
- **Journal**: Computers in Biology and Medicine Volume 133, June 2021, Pages
  104392
- **Summary**: Body-Mass-Index (BMI) conveys important information about one's life such as health and socio-economic conditions. Large-scale automatic estimation of BMIs can help predict several societal behaviors such as health, job opportunities, friendships, and popularity. The recent works have either employed hand-crafted geometrical face features or face-level deep convolutional neural network features for face to BMI prediction. The hand-crafted geometrical face feature lack generalizability and face-level deep features don't have detailed local information. Although useful, these methods missed the detailed local information which is essential for exact BMI prediction. In this paper, we propose to use deep features that are pooled from different face regions (eye, nose, eyebrow, lips, etc.,) and demonstrate that this explicit pooling from face regions can significantly boost the performance of BMI prediction. To address the problem of accurate and pixel-level face regions localization, we propose to use face semantic segmentation in our framework. Extensive experiments are performed using different Convolutional Neural Network (CNN) backbones including FaceNet and VGG-face on three publicly available datasets: VisualBMI, Bollywood and VIP attributes. Experimental results demonstrate that, as compared to the recent works, the proposed Reg-GAP gives a percentage improvement of 22.4\% on VIP-attribute, 3.3\% on VisualBMI, and 63.09\% on the Bollywood dataset.



### Preprocessing Methods of Lane Detection and Tracking for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2104.04755v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04755v1)
- **Published**: 2021-04-10 13:03:52+00:00
- **Updated**: 2021-04-10 13:03:52+00:00
- **Authors**: Akram Heidarizadeh
- **Comment**: None
- **Journal**: None
- **Summary**: In the past few years, researches on advanced driver assistance systems (ADASs) have been carried out and deployed in intelligent vehicles. Systems that have been developed can perform different tasks, such as lane keeping assistance (LKA), lane departure warning (LDW), lane change warning (LCW) and adaptive cruise control (ACC). Real time lane detection and tracking (LDT) is one of the most consequential parts to performing the above tasks. Images which are extracted from the video, contain noise and other unwanted factors such as variation in lightening, shadow from nearby objects and etc. that requires robust preprocessing methods for lane marking detection and tracking. Preprocessing is critical for the subsequent steps and real time performance because its main function is to remove the irrelevant image parts and enhance the feature of interest. In this paper, we survey preprocessing methods for detecting lane marking as well as tracking lane boundaries in real time focusing on vision-based system.



### MobileStyleGAN: A Lightweight Convolutional Neural Network for High-Fidelity Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2104.04767v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.04767v2)
- **Published**: 2021-04-10 13:46:49+00:00
- **Updated**: 2021-09-09 21:03:58+00:00
- **Authors**: Sergei Belousov
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the use of Generative Adversarial Networks (GANs) has become very popular in generative image modeling. While style-based GAN architectures yield state-of-the-art results in high-fidelity image synthesis, computationally, they are highly complex. In our work, we focus on the performance optimization of style-based generative models. We analyze the most computationally hard parts of StyleGAN2, and propose changes in the generator network to make it possible to deploy style-based generative networks in the edge devices. We introduce MobileStyleGAN architecture, which has x3.5 fewer parameters and is x9.5 less computationally complex than StyleGAN2, while providing comparable quality.



### Target-Aware Object Discovery and Association for Unsupervised Video Multi-Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.04782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04782v1)
- **Published**: 2021-04-10 14:39:44+00:00
- **Updated**: 2021-04-10 14:39:44+00:00
- **Authors**: Tianfei Zhou, Jianwu Li, Xueyi Li, Ling Shao
- **Comment**: CVPR21
- **Journal**: None
- **Summary**: This paper addresses the task of unsupervised video multi-object segmentation. Current approaches follow a two-stage paradigm: 1) detect object proposals using pre-trained Mask R-CNN, and 2) conduct generic feature matching for temporal association using re-identification techniques. However, the generic features, widely used in both stages, are not reliable for characterizing unseen objects, leading to poor generalization. To address this, we introduce a novel approach for more accurate and efficient spatio-temporal segmentation. In particular, to address \textbf{instance discrimination}, we propose to combine foreground region estimation and instance grouping together in one network, and additionally introduce temporal guidance for segmenting each frame, enabling more accurate object discovery. For \textbf{temporal association}, we complement current video object segmentation architectures with a discriminative appearance model, capable of capturing more fine-grained target-specific information. Given object proposals from the instance discrimination network, three essential strategies are adopted to achieve accurate segmentation: 1) target-specific tracking using a memory-augmented appearance model; 2) target-agnostic verification to trace possible tracklets for the proposal; 3) adaptive memory updating using the verified segments. We evaluate the proposed approach on DAVIS$_{17}$ and YouTube-VIS, and the results demonstrate that it outperforms state-of-the-art methods both in segmentation accuracy and inference speed.



### Lip reading using external viseme decoding
- **Arxiv ID**: http://arxiv.org/abs/2104.04784v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04784v2)
- **Published**: 2021-04-10 14:49:11+00:00
- **Updated**: 2021-11-07 18:23:49+00:00
- **Authors**: Javad Peymanfard, Mohammad Reza Mohammadi, Hossein Zeinali, Nasser Mozayani
- **Comment**: None
- **Journal**: None
- **Summary**: Lip-reading is the operation of recognizing speech from lip movements. This is a difficult task because the movements of the lips when pronouncing the words are similar for some of them. Viseme is used to describe lip movements during a conversation. This paper aims to show how to use external text data (for viseme-to-character mapping) by dividing video-to-character into two stages, namely converting video to viseme, and then converting viseme to character by using separate models. Our proposed method improves word error rate by 4\% compared to the normal sequence to sequence lip-reading model on the BBC-Oxford Lip Reading Sentences 2 (LRS2) dataset.



### Physically-Consistent Generative Adversarial Networks for Coastal Flood Visualization
- **Arxiv ID**: http://arxiv.org/abs/2104.04785v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.04785v4)
- **Published**: 2021-04-10 15:00:15+00:00
- **Updated**: 2023-02-21 22:57:08+00:00
- **Authors**: Björn Lütjens, Brandon Leshchinskiy, Christian Requena-Mesa, Farrukh Chishtie, Natalia Díaz-Rodríguez, Océane Boulais, Aruna Sankaranarayanan, Margaux Masson-Forsythe, Aaron Piña, Yarin Gal, Chedy Raïssi, Alexander Lavin, Dava Newman
- **Comment**: arXiv admin note: text overlap with arXiv:2010.08103
- **Journal**: None
- **Summary**: As climate change increases the intensity of natural disasters, society needs better tools for adaptation. Floods, for example, are the most frequent natural disaster, and better tools for flood risk communication could increase the support for flood-resilient infrastructure development. Our work aims to enable more visual communication of large-scale climate impacts via visualizing the output of coastal flood models as satellite imagery. We propose the first deep learning pipeline to ensure physical-consistency in synthetic visual satellite imagery. We advanced a state-of-the-art GAN called pix2pixHD, such that it produces imagery that is physically-consistent with the output of an expert-validated storm surge model (NOAA SLOSH). By evaluating the imagery relative to physics-based flood maps, we find that our proposed framework outperforms baseline models in both physical-consistency and photorealism. We envision our work to be the first step towards a global visualization of how the climate challenge will shape our landscape. Continuing on this path, we show that the proposed pipeline generalizes to visualize reforestation. We also publish a dataset of over 25k labelled image-triplets to study image-to-image translation in Earth observation.



### Robust Egocentric Photo-realistic Facial Expression Transfer for Virtual Reality
- **Arxiv ID**: http://arxiv.org/abs/2104.04794v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04794v2)
- **Published**: 2021-04-10 15:48:53+00:00
- **Updated**: 2022-07-04 07:22:03+00:00
- **Authors**: Amin Jourabloo, Baris Gecer, Fernando De la Torre, Jason Saragih, Shih-En Wei, Te-Li Wang, Stephen Lombardi, Danielle Belko, Autumn Trimble, Hernan Badino
- **Comment**: None
- **Journal**: None
- **Summary**: Social presence, the feeling of being there with a real person, will fuel the next generation of communication systems driven by digital humans in virtual reality (VR). The best 3D video-realistic VR avatars that minimize the uncanny effect rely on person-specific (PS) models. However, these PS models are time-consuming to build and are typically trained with limited data variability, which results in poor generalization and robustness. Major sources of variability that affects the accuracy of facial expression transfer algorithms include using different VR headsets (e.g., camera configuration, slop of the headset), facial appearance changes over time (e.g., beard, make-up), and environmental factors (e.g., lighting, backgrounds). This is a major drawback for the scalability of these models in VR. This paper makes progress in overcoming these limitations by proposing an end-to-end multi-identity architecture (MIA) trained with specialized augmentation strategies. MIA drives the shape component of the avatar from three cameras in the VR headset (two eyes, one mouth), in untrained subjects, using minimal personalized information (i.e., neutral 3D mesh shape). Similarly, if the PS texture decoder is available, MIA is able to drive the full avatar (shape+texture) robustly outperforming PS models in challenging scenarios. Our key contribution to improve robustness and generalization, is that our method implicitly decouples, in an unsupervised manner, the facial expression from nuisance factors (e.g., headset, environment, facial appearance). We demonstrate the superior performance and robustness of the proposed method versus state-of-the-art PS approaches in a variety of experiments.



### Two layer Ensemble of Deep Learning Models for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.04809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04809v1)
- **Published**: 2021-04-10 16:52:34+00:00
- **Updated**: 2021-04-10 16:52:34+00:00
- **Authors**: Truong Dang, Tien Thanh Nguyen, John McCall, Eyad Elyan, Carlos Francisco Moreno-García
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: In recent years, deep learning has rapidly become a method of choice for the segmentation of medical images. Deep Neural Network (DNN) architectures such as UNet have achieved state-of-the-art results on many medical datasets. To further improve the performance in the segmentation task, we develop an ensemble system which combines various deep learning architectures. We propose a two-layer ensemble of deep learning models for the segmentation of medical images. The prediction for each training image pixel made by each model in the first layer is used as the augmented data of the training image for the second layer of the ensemble. The prediction of the second layer is then combined by using a weights-based scheme in which each model contributes differently to the combined result. The weights are found by solving linear regression problems. Experiments conducted on two popular medical datasets namely CAMUS and Kvasir-SEG show that the proposed method achieves better results concerning two performance metrics (Dice Coefficient and Hausdorff distance) compared to some well-known benchmark algorithms.



### Latent Code-Based Fusion: A Volterra Neural Network Approach
- **Arxiv ID**: http://arxiv.org/abs/2104.04829v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.04829v1)
- **Published**: 2021-04-10 18:29:01+00:00
- **Updated**: 2021-04-10 18:29:01+00:00
- **Authors**: Sally Ghanem, Siddharth Roheda, Hamid Krim
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deep structure encoder using the recently introduced Volterra Neural Networks (VNNs) to seek a latent representation of multi-modal data whose features are jointly captured by a union of subspaces. The so-called self-representation embedding of the latent codes leads to a simplified fusion which is driven by a similarly constructed decoding. The Volterra Filter architecture achieved reduction in parameter complexity is primarily due to controlled non-linearities being introduced by the higher-order convolutions in contrast to generalized activation functions. Experimental results on two different datasets have shown a significant improvement in the clustering performance for VNNs auto-encoder over conventional Convolutional Neural Networks (CNNs) auto-encoder. In addition, we also show that the proposed approach demonstrates a much-improved sample complexity over CNN-based auto-encoder with a superb robust classification performance.



### Ensemble Learning based on Classifier Prediction Confidence and Comprehensive Learning Particle Swarm Optimisation for polyp localisation
- **Arxiv ID**: http://arxiv.org/abs/2104.04832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.04832v1)
- **Published**: 2021-04-10 18:34:42+00:00
- **Updated**: 2021-04-10 18:34:42+00:00
- **Authors**: Truong Dang, Thanh Nguyen, John McCall, Alan Wee-Chung Liew
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Colorectal cancer (CRC) is the first cause of death in many countries. CRC originates from a small clump of cells on the lining of the colon called polyps, which over time might grow and become malignant. Early detection and removal of polyps are therefore necessary for the prevention of colon cancer. In this paper, we introduce an ensemble of medical polyp segmentation algorithms. Based on an observation that different segmentation algorithms will perform well on different subsets of examples because of the nature and size of training sets they have been exposed to and because of method-intrinsic factors, we propose to measure the confidence in the prediction of each algorithm and then use an associate threshold to determine whether the confidence is acceptable or not. An algorithm is selected for the ensemble if the confidence is below its associate threshold. The optimal threshold for each segmentation algorithm is found by using Comprehensive Learning Particle Swarm Optimization (CLPSO), a swarm intelligence algorithm. The Dice coefficient, a popular performance metric for image segmentation, is used as the fitness criteria. Experimental results on two polyp segmentation datasets MICCAI2015 and Kvasir-SEG confirm that our ensemble achieves better results compared to some well-known segmentation algorithms.



### Error Propagation in Satellite Multi-image Geometry
- **Arxiv ID**: http://arxiv.org/abs/2104.04843v1
- **DOI**: 10.1109/TGRS.2021.3128776
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.04843v1)
- **Published**: 2021-04-10 19:16:57+00:00
- **Updated**: 2021-04-10 19:16:57+00:00
- **Authors**: Joseph L Mundy, Hank Theiss
- **Comment**: 15 pages, 27 figures
- **Journal**: None
- **Summary**: This paper describes an investigation of the source of geospatial error in digital surface models (DSMs) constructed from multiple satellite images. In this study the uncertainty in surface geometry is separated into two spatial components; global error that affects the absolute position of the surface, and local error that varies from surface point to surface point. The global error component is caused by inaccuracy in the satellite imaging process, mainly due to uncertainty in the satellite position and orientation (pose) during image collection. A key result of the investigation is a new algorithm for determining the absolute geoposition of the DSM that takes into account the pose covariance of each satellite during image collection. This covariance information is used to weigh the evidence from each image in the computation of the global position of the DSM. The use of covariance information significantly decreases the overall uncertainty in global position. The paper also describes an approach to the prediction of local error in the DSM surface. The observed variance in surface position within a single stereo surface reconstruction defines the local horizontal error. The variance in the fused set of elevations from multiple stereo pairs at a single DSM location defines the local vertical error. These accuracy predictions are compared to ground truth provided by LiDAR scans of the same geographic region of interest.



### Deep Weakly Supervised Positioning
- **Arxiv ID**: http://arxiv.org/abs/2104.04866v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.04866v1)
- **Published**: 2021-04-10 21:19:08+00:00
- **Updated**: 2021-04-10 21:19:08+00:00
- **Authors**: Ruoyu Wang, Xuchu Xu, Li Ding, Yang Huang, Chen Feng
- **Comment**: 8 pages, 8 figures, submitted to IEEE Robotics and Automation Letters
  (RA-L) and 2021 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS 2021)
- **Journal**: None
- **Summary**: PoseNet can map a photo to the position where it is taken, which is appealing in robotics. However, training PoseNet requires full supervision, where ground truth positions are non-trivial to obtain. Can we train PoseNet without knowing the ground truth positions for each observation? We show that this is possible via constraint-based weak-supervision, leading to the proposed framework: DeepGPS. Particularly, using wheel-encoder-estimated distances traveled by a robot along random straight line segments as constraints between PoseNet outputs, DeepGPS can achieve a relative positioning error of less than 2%. Moreover, training DeepGPS can be done as auto-calibration with almost no human attendance, which is more attractive than its competing methods that typically require careful and expert-level manual calibration. We conduct various experiments on simulated and real datasets to demonstrate the general applicability, effectiveness, and accuracy of DeepGPS, and perform a comprehensive analysis of its robustness. Our code is available at https://ai4ce.github.io/DeepGPS/.



