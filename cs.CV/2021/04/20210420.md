# Arxiv Papers in cs.CV on 2021-04-20
### Domain adaptation based self-correction model for COVID-19 infection segmentation in CT images
- **Arxiv ID**: http://arxiv.org/abs/2104.09699v1
- **DOI**: 10.1016/j.eswa.2021.114848
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.09699v1)
- **Published**: 2021-04-20 00:45:01+00:00
- **Updated**: 2021-04-20 00:45:01+00:00
- **Authors**: Qiangguo Jin, Hui Cui, Changming Sun, Zhaopeng Meng, Leyi Wei, Ran Su
- **Comment**: None
- **Journal**: None
- **Summary**: The capability of generalization to unseen domains is crucial for deep learning models when considering real-world scenarios. However, current available medical image datasets, such as those for COVID-19 CT images, have large variations of infections and domain shift problems. To address this issue, we propose a prior knowledge driven domain adaptation and a dual-domain enhanced self-correction learning scheme. Based on the novel learning schemes, a domain adaptation based self-correction model (DASC-Net) is proposed for COVID-19 infection segmentation on CT images. DASC-Net consists of a novel attention and feature domain enhanced domain adaptation model (AFD-DA) to solve the domain shifts and a self-correction learning process to refine segmentation results. The innovations in AFD-DA include an image-level activation feature extractor with attention to lung abnormalities and a multi-level discrimination module for hierarchical feature domain alignment. The proposed self-correction learning process adaptively aggregates the learned model and corresponding pseudo labels for the propagation of aligned source and target domain information to alleviate the overfitting to noises caused by pseudo labels. Extensive experiments over three publicly available COVID-19 CT datasets demonstrate that DASC-Net consistently outperforms state-of-the-art segmentation, domain shift, and coronavirus infection segmentation methods. Ablation analysis further shows the effectiveness of the major components in our model. The DASC-Net enriches the theory of domain adaptation and self-correction learning in medical imaging and can be generalized to multi-site COVID-19 infection segmentation on CT images for clinical deployment.



### Free-form tumor synthesis in computed tomography images via richer generative adversarial network
- **Arxiv ID**: http://arxiv.org/abs/2104.09701v1
- **DOI**: 10.1016/j.knosys.2021.106753
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.09701v1)
- **Published**: 2021-04-20 00:49:35+00:00
- **Updated**: 2021-04-20 00:49:35+00:00
- **Authors**: Qiangguo Jin, Hui Cui, Changming Sun, Zhaopeng Meng, Ran Su
- **Comment**: None
- **Journal**: None
- **Summary**: The insufficiency of annotated medical imaging scans for cancer makes it challenging to train and validate data-hungry deep learning models in precision oncology. We propose a new richer generative adversarial network for free-form 3D tumor/lesion synthesis in computed tomography (CT) images. The network is composed of a new richer convolutional feature enhanced dilated-gated generator (RicherDG) and a hybrid loss function. The RicherDG has dilated-gated convolution layers to enable tumor-painting and to enlarge perceptive fields; and it has a novel richer convolutional feature association branch to recover multi-scale convolutional features especially from uncertain boundaries between tumor and surrounding healthy tissues. The hybrid loss function, which consists of a diverse range of losses, is designed to aggregate complementary information to improve optimization.   We perform a comprehensive evaluation of the synthesis results on a wide range of public CT image datasets covering the liver, kidney tumors, and lung nodules. The qualitative and quantitative evaluations and ablation study demonstrated improved synthesizing results over advanced tumor synthesis methods.



### Staircase Sign Method for Boosting Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2104.09722v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09722v2)
- **Published**: 2021-04-20 02:31:55+00:00
- **Updated**: 2022-04-12 08:16:12+00:00
- **Authors**: Qilong Zhang, Xiaosu Zhu, Jingkuan Song, Lianli Gao, Heng Tao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Crafting adversarial examples for the transfer-based attack is challenging and remains a research hot spot. Currently, such attack methods are based on the hypothesis that the substitute model and the victim model learn similar decision boundaries, and they conventionally apply Sign Method (SM) to manipulate the gradient as the resultant perturbation. Although SM is efficient, it only extracts the sign of gradient units but ignores their value difference, which inevitably leads to a deviation. Therefore, we propose a novel Staircase Sign Method (S$^2$M) to alleviate this issue, thus boosting attacks. Technically, our method heuristically divides the gradient sign into several segments according to the values of the gradient units, and then assigns each segment with a staircase weight for better crafting adversarial perturbation. As a result, our adversarial examples perform better in both white-box and black-box manner without being more visible. Since S$^2$M just manipulates the resultant gradient, our method can be generally integrated into the family of FGSM algorithms, and the computational overhead is negligible. Extensive experiments on the ImageNet dataset demonstrate the effectiveness of our proposed methods, which significantly improve the transferability (i.e., on average, \textbf{5.1\%} for normally trained models and \textbf{12.8\%} for adversarially trained defenses). Our code is available at \url{https://github.com/qilong-zhang/Staircase-sign-method}.



### Systematic investigation into generalization of COVID-19 CT deep learning models with Gabor ensemble for lung involvement scoring
- **Arxiv ID**: http://arxiv.org/abs/2105.15094v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.15094v1)
- **Published**: 2021-04-20 03:49:48+00:00
- **Updated**: 2021-04-20 03:49:48+00:00
- **Authors**: Michael J. Horry, Subrata Chakraborty, Biswajeet Pradhan, Maryam Fallahpoor, Chegeni Hossein, Manoranjan Paul
- **Comment**: 39 Pages, 8 figures, 14 tables comparing the generalization of
  COVID-19 CT Deep Learning Models
- **Journal**: None
- **Summary**: The COVID-19 pandemic has inspired unprecedented data collection and computer vision modelling efforts worldwide, focusing on diagnosis and stratification of COVID-19 from medical images. Despite this large-scale research effort, these models have found limited practical application due in part to unproven generalization of these models beyond their source study. This study investigates the generalizability of key published models using the publicly available COVID-19 Computed Tomography data through cross dataset validation. We then assess the predictive ability of these models for COVID-19 severity using an independent new dataset that is stratified for COVID-19 lung involvement. Each inter-dataset study is performed using histogram equalization, and contrast limited adaptive histogram equalization with and without a learning Gabor filter. The study shows high variability in the generalization of models trained on these datasets due to varied sample image provenances and acquisition processes amongst other factors. We show that under certain conditions, an internally consistent dataset can generalize well to an external dataset despite structural differences between these datasets with f1 scores up to 86%. Our best performing model shows high predictive accuracy for lung involvement score for an independent dataset for which expertly labelled lung involvement stratification is available. Creating an ensemble of our best model for disease positive prediction with our best model for disease negative prediction using a min-max function resulted in a superior model for lung involvement prediction with average predictive accuracy of 75% for zero lung involvement and 96% for 75-100% lung involvement with almost linear relationship between these stratifications.



### Flow-based Video Segmentation for Human Head and Shoulders
- **Arxiv ID**: http://arxiv.org/abs/2104.09752v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09752v1)
- **Published**: 2021-04-20 04:05:36+00:00
- **Updated**: 2021-04-20 04:05:36+00:00
- **Authors**: Zijian Kuang, Xinran Tie
- **Comment**: None
- **Journal**: None
- **Summary**: Video segmentation for the human head and shoulders is essential in creating elegant media for videoconferencing and virtual reality applications. The main challenge is to process high-quality background subtraction in a real-time manner and address the segmentation issues under motion blurs, e.g., shaking the head or waving hands during conference video. To overcome the motion blur problem in video segmentation, we propose a novel flow-based encoder-decoder network (FUNet) that combines both traditional Horn-Schunck optical-flow estimation technique and convolutional neural networks to perform robust real-time video segmentation. We also introduce a video and image segmentation dataset: ConferenceVideoSegmentationDataset. Code and pre-trained models are available on our GitHub repository: \url{https://github.com/kuangzijian/Flow-Based-Video-Matting}.



### Hierarchical entropy and domain interaction to understand the structure in an image
- **Arxiv ID**: http://arxiv.org/abs/2104.09754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09754v1)
- **Published**: 2021-04-20 04:29:13+00:00
- **Updated**: 2021-04-20 04:29:13+00:00
- **Authors**: Nao Uehara, Teruaki Hayashi, Yukio Ohsawa
- **Comment**: 20pages,17figures
- **Journal**: None
- **Summary**: In this study, we devise a model that introduces two hierarchies into information entropy. The two hierarchies are the size of the region for which entropy is calculated and the size of the component that determines whether the structures in the image are integrated or not. And this model uses two indicators, hierarchical entropy and domain interaction. Both indicators increase or decrease due to the integration or fragmentation of the structure in the image. It aims to help people interpret and explain what the structure in an image looks like from two indicators that change with the size of the region and the component. First, we conduct experiments using images and qualitatively evaluate how the two indicators change. Next, we explain the relationship with the hidden structure of Vermeer's girl with a pearl earring using the change of hierarchical entropy. Finally, we clarify the relationship between the change of domain interaction and the appropriate segment result of the image by an experiment using a questionnaire.



### Imaginative Walks: Generative Random Walk Deviation Loss for Improved Unseen Learning Representation
- **Arxiv ID**: http://arxiv.org/abs/2104.09757v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.09757v2)
- **Published**: 2021-04-20 04:34:28+00:00
- **Updated**: 2021-09-24 12:22:25+00:00
- **Authors**: Divyansh Jha, Kai Yi, Ivan Skorokhodov, Mohamed Elhoseiny
- **Comment**: Project homepage: https://imaginative-walks.github.io
- **Journal**: None
- **Summary**: We propose a novel loss for generative models, dubbed as GRaWD (Generative Random Walk Deviation), to improve learning representations of unexplored visual spaces. Quality learning representation of unseen classes (or styles) is critical to facilitate novel image generation and better generative understanding of unseen visual classes, i.e., zero-shot learning (ZSL). By generating representations of unseen classes based on their semantic descriptions, e.g., attributes or text, generative ZSL attempts to differentiate unseen from seen categories. The proposed GRaWD loss is defined by constructing a dynamic graph that includes the seen class/style centers and generated samples in the current minibatch. Our loss initiates a random walk probability from each center through visual generations produced from hallucinated unseen classes. As a deviation signal, we encourage the random walk to eventually land after t steps in a feature representation that is difficult to classify as any of the seen classes. We demonstrate that the proposed loss can improve unseen class representation quality inductively on text-based ZSL benchmarks on CUB and NABirds datasets and attribute-based ZSL benchmarks on AWA2, SUN, and aPY datasets. In addition, we investigate the ability of the proposed loss to generate meaningful novel visual art on the WikiArt dataset. The results of experiments and human evaluations demonstrate that the proposed GRaWD loss can improve StyleGAN1 and StyleGAN2 generation quality and create novel art that is significantly more preferable. Our code is made publicly available at https://github.com/Vision-CAIR/GRaWD.



### An Efficient Approach for Anomaly Detection in Traffic Videos
- **Arxiv ID**: http://arxiv.org/abs/2104.09758v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.09758v1)
- **Published**: 2021-04-20 04:43:18+00:00
- **Updated**: 2021-04-20 04:43:18+00:00
- **Authors**: Keval Doshi, Yasin Yilmaz
- **Comment**: Accepted to CVPR 2021 - AI City Workshop
- **Journal**: None
- **Summary**: Due to its relevance in intelligent transportation systems, anomaly detection in traffic videos has recently received much interest. It remains a difficult problem due to a variety of factors influencing the video quality of a real-time traffic feed, such as temperature, perspective, lighting conditions, and so on. Even though state-of-the-art methods perform well on the available benchmark datasets, they need a large amount of external training data as well as substantial computational resources. In this paper, we propose an efficient approach for a video anomaly detection system which is capable of running at the edge devices, e.g., on a roadside camera. The proposed approach comprises a pre-processing module that detects changes in the scene and removes the corrupted frames, a two-stage background modelling module and a two-stage object detector. Finally, a backtracking anomaly detection algorithm computes a similarity statistic and decides on the onset time of the anomaly. We also propose a sequential change detection algorithm that can quickly adapt to a new scene and detect changes in the similarity statistic. Experimental results on the Track 4 test set of the 2021 AI City Challenge show the efficacy of the proposed framework as we achieve an F1-score of 0.9157 along with 8.4027 root mean square error (RMSE) and are ranked fourth in the competition.



### HCMS: Hierarchical and Conditional Modality Selection for Efficient Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.09760v3
- **DOI**: 10.1145/3572776
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09760v3)
- **Published**: 2021-04-20 04:47:04+00:00
- **Updated**: 2022-12-06 04:13:50+00:00
- **Authors**: Zejia Weng, Zuxuan Wu, Hengduo Li, Jingjing Chen, Yu-Gang Jiang
- **Comment**: 18 pages, 7 figures
- **Journal**: None
- **Summary**: Videos are multimodal in nature. Conventional video recognition pipelines typically fuse multimodal features for improved performance. However, this is not only computationally expensive but also neglects the fact that different videos rely on different modalities for predictions. This paper introduces Hierarchical and Conditional Modality Selection (HCMS), a simple yet efficient multimodal learning framework for efficient video recognition. HCMS operates on a low-cost modality, i.e., audio clues, by default, and dynamically decides on-the-fly whether to use computationally-expensive modalities, including appearance and motion clues, on a per-input basis. This is achieved by the collaboration of three LSTMs that are organized in a hierarchical manner. In particular, LSTMs that operate on high-cost modalities contain a gating module, which takes as inputs lower-level features and historical information to adaptively determine whether to activate its corresponding modality; otherwise it simply reuses historical information. We conduct extensive experiments on two large-scale video benchmarks, FCVID and ActivityNet, and the results demonstrate the proposed approach can effectively explore multimodal information for improved classification performance while requiring much less computation.



### Learning Semantic-Aware Dynamics for Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2104.09762v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09762v1)
- **Published**: 2021-04-20 05:00:24+00:00
- **Updated**: 2021-04-20 05:00:24+00:00
- **Authors**: Xinzhu Bei, Yanchao Yang, Stefano Soatto
- **Comment**: Paper accepted at CVPR 2021
- **Journal**: None
- **Summary**: We propose an architecture and training scheme to predict video frames by explicitly modeling dis-occlusions and capturing the evolution of semantically consistent regions in the video. The scene layout (semantic map) and motion (optical flow) are decomposed into layers, which are predicted and fused with their context to generate future layouts and motions. The appearance of the scene is warped from past frames using the predicted motion in co-visible regions; dis-occluded regions are synthesized with content-aware inpainting utilizing the predicted scene layout. The result is a predictive model that explicitly represents objects and learns their class-specific motion, which we evaluate on video prediction benchmarks.



### M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.09770v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09770v3)
- **Published**: 2021-04-20 05:43:44+00:00
- **Updated**: 2022-04-19 06:08:33+00:00
- **Authors**: Junke Wang, Zuxuan Wu, Wenhao Ouyang, Xintong Han, Jingjing Chen, Ser-Nam Lim, Yu-Gang Jiang
- **Comment**: accepted by ICMR 2022
- **Journal**: None
- **Summary**: The widespread dissemination of Deepfakes demands effective approaches that can detect perceptually convincing forged images. In this paper, we aim to capture the subtle manipulation artifacts at different scales using transformer models. In particular, we introduce a Multi-modal Multi-scale TRansformer (M2TR), which operates on patches of different sizes to detect local inconsistencies in images at different spatial levels. M2TR further learns to detect forgery artifacts in the frequency domain to complement RGB information through a carefully designed cross modality fusion block. In addition, to stimulate Deepfake detection research, we introduce a high-quality Deepfake dataset, SR-DF, which consists of 4,000 DeepFake videos generated by state-of-the-art face swapping and facial reenactment methods. We conduct extensive experiments to verify the effectiveness of the proposed method, which outperforms state-of-the-art Deepfake detection methods by clear margins.



### Does enhanced shape bias improve neural network robustness to common corruptions?
- **Arxiv ID**: http://arxiv.org/abs/2104.09789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09789v1)
- **Published**: 2021-04-20 07:06:53+00:00
- **Updated**: 2021-04-20 07:06:53+00:00
- **Authors**: Chaithanya Kumar Mummadi, Ranjitha Subramaniam, Robin Hutmacher, Julien Vitay, Volker Fischer, Jan Hendrik Metzen
- **Comment**: 20 pages, 9 figures, 12 tables, accepted at ICLR 2021
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) learn to extract representations of complex features, such as object shapes and textures to solve image recognition tasks. Recent work indicates that CNNs trained on ImageNet are biased towards features that encode textures and that these alone are sufficient to generalize to unseen test data from the same distribution as the training data but often fail to generalize to out-of-distribution data. It has been shown that augmenting the training data with different image styles decreases this texture bias in favor of increased shape bias while at the same time improving robustness to common corruptions, such as noise and blur. Commonly, this is interpreted as shape bias increasing corruption robustness. However, this relationship is only hypothesized. We perform a systematic study of different ways of composing inputs based on natural images, explicit edge information, and stylization. While stylization is essential for achieving high corruption robustness, we do not find a clear correlation between shape bias and robustness. We conclude that the data augmentation caused by style-variation accounts for the improved corruption robustness and increased shape bias is only a byproduct.



### What is Wrong with One-Class Anomaly Detection?
- **Arxiv ID**: http://arxiv.org/abs/2104.09793v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.09793v1)
- **Published**: 2021-04-20 07:10:00+00:00
- **Updated**: 2021-04-20 07:10:00+00:00
- **Authors**: JuneKyu Park, Jeong-Hyeon Moon, Namhyuk Ahn, Kyung-Ah Sohn
- **Comment**: None
- **Journal**: None
- **Summary**: From a safety perspective, a machine learning method embedded in real-world applications is required to distinguish irregular situations. For this reason, there has been a growing interest in the anomaly detection (AD) task. Since we cannot observe abnormal samples for most of the cases, recent AD methods attempt to formulate it as a task of classifying whether the sample is normal or not. However, they potentially fail when the given normal samples are inherited from diverse semantic labels. To tackle this problem, we introduce a latent class-condition-based AD scenario. In addition, we propose a confidence-based self-labeling AD framework tailored to our proposed scenario. Since our method leverages the hidden class information, it successfully avoids generating the undesirable loose decision region that one-class methods suffer. Our proposed framework outperforms the recent one-class AD methods in the latent multi-class scenarios.



### SE-SSD: Self-Ensembling Single-Stage Object Detector From Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2104.09804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09804v1)
- **Published**: 2021-04-20 07:33:03+00:00
- **Updated**: 2021-04-20 07:33:03+00:00
- **Authors**: Wu Zheng, Weiliang Tang, Li Jiang, Chi-Wing Fu
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: We present Self-Ensembling Single-Stage object Detector (SE-SSD) for accurate and efficient 3D object detection in outdoor point clouds. Our key focus is on exploiting both soft and hard targets with our formulated constraints to jointly optimize the model, without introducing extra computation in the inference. Specifically, SE-SSD contains a pair of teacher and student SSDs, in which we design an effective IoU-based matching strategy to filter soft targets from the teacher and formulate a consistency loss to align student predictions with them. Also, to maximize the distilled knowledge for ensembling the teacher, we design a new augmentation scheme to produce shape-aware augmented samples to train the student, aiming to encourage it to infer complete object shapes. Lastly, to better exploit hard targets, we design an ODIoU loss to supervise the student with constraints on the predicted box centers and orientations. Our SE-SSD attains top performance compared with all prior published works. Also, it attains top precisions for car detection in the KITTI benchmark (ranked 1st and 2nd on the BEV and 3D leaderboards, respectively) with an ultra-high inference speed. The code is available at https://github.com/Vegeta2020/SE-SSD.



### CTNet: Context-based Tandem Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.09805v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09805v1)
- **Published**: 2021-04-20 07:33:11+00:00
- **Updated**: 2021-04-20 07:33:11+00:00
- **Authors**: Zechao Li, Yanpeng Sun, Jinhui Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Contextual information has been shown to be powerful for semantic segmentation. This work proposes a novel Context-based Tandem Network (CTNet) by interactively exploring the spatial contextual information and the channel contextual information, which can discover the semantic context for semantic segmentation. Specifically, the Spatial Contextual Module (SCM) is leveraged to uncover the spatial contextual dependency between pixels by exploring the correlation between pixels and categories. Meanwhile, the Channel Contextual Module (CCM) is introduced to learn the semantic features including the semantic feature maps and class-specific features by modeling the long-term semantic dependence between channels. The learned semantic features are utilized as the prior knowledge to guide the learning of SCM, which can make SCM obtain more accurate long-range spatial dependency. Finally, to further improve the performance of the learned representations for semantic segmentation, the results of the two context modules are adaptively integrated to achieve better results. Extensive experiments are conducted on three widely-used datasets, i.e., PASCAL-Context, ADE20K and PASCAL VOC2012. The results demonstrate the superior performance of the proposed CTNet by comparison with several state-of-the-art methods.



### Visual Navigation with Spatial Attention
- **Arxiv ID**: http://arxiv.org/abs/2104.09807v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.09807v1)
- **Published**: 2021-04-20 07:39:52+00:00
- **Updated**: 2021-04-20 07:39:52+00:00
- **Authors**: Bar Mayo, Tamir Hazan, Ayellet Tal
- **Comment**: None
- **Journal**: None
- **Summary**: This work focuses on object goal visual navigation, aiming at finding the location of an object from a given class, where in each step the agent is provided with an egocentric RGB image of the scene. We propose to learn the agent's policy using a reinforcement learning algorithm. Our key contribution is a novel attention probability model for visual navigation tasks. This attention encodes semantic information about observed objects, as well as spatial information about their place. This combination of the "what" and the "where" allows the agent to navigate toward the sought-after object effectively. The attention model is shown to improve the agent's policy and to achieve state-of-the-art results on commonly-used datasets.



### Measuring the Ripeness of Fruit with Hyperspectral Imaging and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.09808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09808v1)
- **Published**: 2021-04-20 07:43:19+00:00
- **Updated**: 2021-04-20 07:43:19+00:00
- **Authors**: Leon Amadeus Varga, Jan Makowski, Andreas Zell
- **Comment**: IJCNN 2021 (Accepted 10.04.21)
- **Journal**: None
- **Summary**: We present a system to measure the ripeness of fruit with a hyperspectral camera and a suitable deep neural network architecture. This architecture did outperform competitive baseline models on the prediction of the ripeness state of fruit. For this, we recorded a data set of ripening avocados and kiwis, which we make public. We also describe the process of data collection in a manner that the adaption for other fruit is easy. The trained network is validated empirically, and we investigate the trained features. Furthermore, a technique is introduced to visualize the ripening process.



### A simple vision-based navigation and control strategy for autonomous drone racing
- **Arxiv ID**: http://arxiv.org/abs/2104.09815v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.SY, eess.IV, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2104.09815v1)
- **Published**: 2021-04-20 08:02:02+00:00
- **Updated**: 2021-04-20 08:02:02+00:00
- **Authors**: Artur Cyba, Hubert Szolc, Tomasz Kryjak
- **Comment**: Submitted to the MMAR 2021 conference
- **Journal**: None
- **Summary**: In this paper, we present a control system that allows a drone to fly autonomously through a series of gates marked with ArUco tags. A simple and low-cost DJI Tello EDU quad-rotor platform was used. Based on the API provided by the manufacturer, we have created a Python application that enables the communication with the drone over WiFi, realises drone positioning based on visual feedback, and generates control. Two control strategies were proposed, compared, and critically analysed. In addition, the accuracy of the positioning method used was measured. The application was evaluated on a laptop computer (about 40 fps) and a Nvidia Jetson TX2 embedded GPU platform (about 25 fps). We provide the developed code on GitHub.



### Detector-Free Weakly Supervised Grounding by Separation
- **Arxiv ID**: http://arxiv.org/abs/2104.09829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09829v1)
- **Published**: 2021-04-20 08:27:31+00:00
- **Updated**: 2021-04-20 08:27:31+00:00
- **Authors**: Assaf Arbelle, Sivan Doveh, Amit Alfassy, Joseph Shtok, Guy Lev, Eli Schwartz, Hilde Kuehne, Hila Barak Levi, Prasanna Sattigeri, Rameswar Panda, Chun-Fu Chen, Alex Bronstein, Kate Saenko, Shimon Ullman, Raja Giryes, Rogerio Feris, Leonid Karlinsky
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, there is an abundance of data involving images and surrounding free-form text weakly corresponding to those images. Weakly Supervised phrase-Grounding (WSG) deals with the task of using this data to learn to localize (or to ground) arbitrary text phrases in images without any additional annotations. However, most recent SotA methods for WSG assume the existence of a pre-trained object detector, relying on it to produce the ROIs for localization. In this work, we focus on the task of Detector-Free WSG (DF-WSG) to solve WSG without relying on a pre-trained detector. We directly learn everything from the images and associated free-form text pairs, thus potentially gaining an advantage on the categories unsupported by the detector. The key idea behind our proposed Grounding by Separation (GbS) method is synthesizing `text to image-regions' associations by random alpha-blending of arbitrary image pairs and using the corresponding texts of the pair as conditions to recover the alpha map from the blended image via a segmentation network. At test time, this allows using the query phrase as a condition for a non-blended query image, thus interpreting the test image as a composition of a region corresponding to the phrase and the complement region. Using this approach we demonstrate a significant accuracy improvement, of up to $8.5\%$ over previous DF-WSG SotA, for a range of benchmarks including Flickr30K, Visual Genome, and ReferIt, as well as a significant complementary improvement (above $7\%$) over the detector-based approaches for WSG.



### A novel three-stage training strategy for long-tailed classification
- **Arxiv ID**: http://arxiv.org/abs/2104.09830v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09830v2)
- **Published**: 2021-04-20 08:29:27+00:00
- **Updated**: 2022-04-20 01:50:12+00:00
- **Authors**: Gongzhe Li, Zhiwen Tan, Linpeng Pan
- **Comment**: There are some errors in the article, the latest version will be
  officially released soon
- **Journal**: None
- **Summary**: The long-tailed distribution datasets poses great challenges for deep learning based classification models on how to handle the class imbalance problem. Existing solutions usually involve class-balacing strategies or transfer learing from head- to tail-classes or use two-stages learning strategy to re-train the classifier. However, the existing methods are difficult to solve the low quality problem when images are obtained by SAR. To address this problem, we establish a novel three-stages training strategy, which has excellent results for processing SAR image datasets with long-tailed distribution. Specifically, we divide training procedure into three stages. The first stage is to use all kinds of images for rough-training, so as to get the rough-training model with rich content. The second stage is to make the rough model learn the feature expression by using the residual dataset with the class 0 removed. The third stage is to fine tune the model using class-balanced datasets with all 10 classes (including the overall model fine tuning and classifier re-optimization). Through this new training strategy, we only use the information of SAR image dataset and the network model with very small parameters to achieve the top 1 accuracy of 22.34 in development phase.



### SelfReg: Self-supervised Contrastive Regularization for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2104.09841v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.5
- **Links**: [PDF](http://arxiv.org/pdf/2104.09841v1)
- **Published**: 2021-04-20 09:08:29+00:00
- **Updated**: 2021-04-20 09:08:29+00:00
- **Authors**: Daehee Kim, Seunghyun Park, Jinkyu Kim, Jaekoo Lee
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: In general, an experimental environment for deep learning assumes that the training and the test dataset are sampled from the same distribution. However, in real-world situations, a difference in the distribution between two datasets, domain shift, may occur, which becomes a major factor impeding the generalization performance of the model. The research field to solve this problem is called domain generalization, and it alleviates the domain shift problem by extracting domain-invariant features explicitly or implicitly. In recent studies, contrastive learning-based domain generalization approaches have been proposed and achieved high performance. These approaches require sampling of the negative data pair. However, the performance of contrastive learning fundamentally depends on quality and quantity of negative data pairs. To address this issue, we propose a new regularization method for domain generalization based on contrastive learning, self-supervised contrastive regularization (SelfReg). The proposed approach use only positive data pairs, thus it resolves various problems caused by negative pair sampling. Moreover, we propose a class-specific domain perturbation layer (CDPL), which makes it possible to effectively apply mixup augmentation even when only positive data pairs are used. The experimental results show that the techniques incorporated by SelfReg contributed to the performance in a compatible manner. In the recent benchmark, DomainBed, the proposed method shows comparable performance to the conventional state-of-the-art alternatives. Codes are available at https://github.com/dnap512/SelfReg.



### Distill on the Go: Online knowledge distillation in self-supervised learning
- **Arxiv ID**: http://arxiv.org/abs/2104.09866v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.09866v2)
- **Published**: 2021-04-20 09:59:23+00:00
- **Updated**: 2021-06-30 13:03:14+00:00
- **Authors**: Prashant Bhat, Elahe Arani, Bahram Zonooz
- **Comment**: Spotlight @ Learning from Limited or Imperfect Data (L2ID) Workshop -
  CVPR 2021
- **Journal**: None
- **Summary**: Self-supervised learning solves pretext prediction tasks that do not require annotations to learn feature representations. For vision tasks, pretext tasks such as predicting rotation, solving jigsaw are solely created from the input data. Yet, predicting this known information helps in learning representations useful for downstream tasks. However, recent works have shown that wider and deeper models benefit more from self-supervised learning than smaller models. To address the issue of self-supervised pre-training of smaller models, we propose Distill-on-the-Go (DoGo), a self-supervised learning paradigm using single-stage online knowledge distillation to improve the representation quality of the smaller models. We employ deep mutual learning strategy in which two models collaboratively learn from each other to improve one another. Specifically, each model is trained using self-supervised learning along with distillation that aligns each model's softmax probabilities of similarity scores with that of the peer model. We conduct extensive experiments on multiple benchmark datasets, learning objectives, and architectures to demonstrate the potential of our proposed method. Our results show significant performance gain in the presence of noisy and limited labels and generalization to out-of-distribution data.



### Boosting Masked Face Recognition with Multi-Task ArcFace
- **Arxiv ID**: http://arxiv.org/abs/2104.09874v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2104.09874v2)
- **Published**: 2021-04-20 10:12:04+00:00
- **Updated**: 2021-04-21 06:54:29+00:00
- **Authors**: David Montero, Marcos Nieto, Peter Leskovsky, Naiara Aginako
- **Comment**: 6 pages, 4 figures. The paper is under consideration at Pattern
  Recognition Letters
- **Journal**: None
- **Summary**: In this paper, we address the problem of face recognition with masks. Given the global health crisis caused by COVID-19, mouth and nose-covering masks have become an essential everyday-clothing-accessory. This sanitary measure has put the state-of-the-art face recognition models on the ropes since they have not been designed to work with masked faces. In addition, the need has arisen for applications capable of detecting whether the subjects are wearing masks to control the spread of the virus. To overcome these problems a full training pipeline is presented based on the ArcFace work, with several modifications for the backbone and the loss function. From the original face-recognition dataset, a masked version is generated using data augmentation, and both datasets are combined during the training process. The selected network, based on ResNet-50, is modified to also output the probability of mask usage without adding any computational cost. Furthermore, the ArcFace loss is combined with the mask-usage classification loss, resulting in a new function named Multi-Task ArcFace (MTArcFace). Experimental results show that the proposed approach highly boosts the original model accuracy when dealing with masked faces, while preserving almost the same accuracy on the original non-masked datasets. Furthermore, it achieves an average accuracy of 99.78% in mask-usage classification.



### Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry
- **Arxiv ID**: http://arxiv.org/abs/2104.09877v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09877v1)
- **Published**: 2021-04-20 10:17:34+00:00
- **Updated**: 2021-04-20 10:17:34+00:00
- **Authors**: Dawa Derksen, Dario Izzo
- **Comment**: Accepted to CVPR2021 - EarthVision
- **Journal**: None
- **Summary**: We present a new generic method for shadow-aware multi-view satellite photogrammetry of Earth Observation scenes. Our proposed method, the Shadow Neural Radiance Field (S-NeRF) follows recent advances in implicit volumetric representation learning. For each scene, we train S-NeRF using very high spatial resolution optical images taken from known viewing angles. The learning requires no labels or shape priors: it is self-supervised by an image reconstruction loss. To accommodate for changing light source conditions both from a directional light source (the Sun) and a diffuse light source (the sky), we extend the NeRF approach in two ways. First, direct illumination from the Sun is modeled via a local light source visibility field. Second, indirect illumination from a diffuse light source is learned as a non-local color field as a function of the position of the Sun. Quantitatively, the combination of these factors reduces the altitude and color errors in shaded areas, compared to NeRF. The S-NeRF methodology not only performs novel view synthesis and full 3D shape estimation, it also enables shadow detection, albedo synthesis, and transient object filtering, without any explicit shape supervision.



### An Attention-based Weakly Supervised framework for Spitzoid Melanocytic Lesion Diagnosis in WSI
- **Arxiv ID**: http://arxiv.org/abs/2104.09878v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2104.09878v1)
- **Published**: 2021-04-20 10:18:57+00:00
- **Updated**: 2021-04-20 10:18:57+00:00
- **Authors**: Rocío del Amor, Laëtitia Launet, Adrián Colomer, Anaïs Moscardó, Andrés Mosquera-Zamudio, Carlos Monteagudo, Valery Naranjo
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Melanoma is an aggressive neoplasm responsible for the majority of deaths from skin cancer. Specifically, spitzoid melanocytic tumors are one of the most challenging melanocytic lesions due to their ambiguous morphological features. The gold standard for its diagnosis and prognosis is the analysis of skin biopsies. In this process, dermatopathologists visualize skin histology slides under a microscope, in a high time-consuming and subjective task. In the last years, computer-aided diagnosis (CAD) systems have emerged as a promising tool that could support pathologists in daily clinical practice. Nevertheless, no automatic CAD systems have yet been proposed for the analysis of spitzoid lesions. Regarding common melanoma, no proposed system allows both the selection of the tumoral region and the prediction of the diagnosis as benign or malignant. Motivated by this, we propose a novel end-to-end weakly-supervised deep learning model, based on inductive transfer learning with an improved convolutional neural network (CNN) to refine the embedding features of the latent space. The framework is composed of a source model in charge of finding the tumor patch-level patterns, and a target model focuses on the specific diagnosis of a biopsy. The latter retrains the backbone of the source model through a multiple instance learning workflow to obtain the biopsy-level scoring. To evaluate the performance of the proposed methods, we perform extensive experiments on a private skin database with spitzoid lesions. Test results reach an accuracy of 0.9231 and 0.80 for the source and the target models, respectively. Besides, the heat map findings are directly in line with the clinicians' medical decision and even highlight, in some cases, patterns of interest that were overlooked by the pathologist due to the huge workload.



### Lighting, Reflectance and Geometry Estimation from 360$^{\circ}$ Panoramic Stereo
- **Arxiv ID**: http://arxiv.org/abs/2104.09886v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09886v1)
- **Published**: 2021-04-20 10:41:50+00:00
- **Updated**: 2021-04-20 10:41:50+00:00
- **Authors**: Junxuan Li, Hongdong Li, Yasuyuki Matsushita
- **Comment**: Accepted to CVPR 2021. Codes in:
  https://github.com/junxuan-li/LRG_360Panoramic
- **Journal**: None
- **Summary**: We propose a method for estimating high-definition spatially-varying lighting, reflectance, and geometry of a scene from 360$^{\circ}$ stereo images. Our model takes advantage of the 360$^{\circ}$ input to observe the entire scene with geometric detail, then jointly estimates the scene's properties with physical constraints. We first reconstruct a near-field environment light for predicting the lighting at any 3D location within the scene. Then we present a deep learning model that leverages the stereo information to infer the reflectance and surface normal. Lastly, we incorporate the physical constraints between lighting and geometry to refine the reflectance of the scene. Both quantitative and qualitative experiments show that our method, benefiting from the 360$^{\circ}$ observation of the scene, outperforms prior state-of-the-art methods and enables more augmented reality applications such as mirror-objects insertion.



### Comparing Representations in Tracking for Event Camera-based SLAM
- **Arxiv ID**: http://arxiv.org/abs/2104.09887v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.09887v1)
- **Published**: 2021-04-20 10:41:57+00:00
- **Updated**: 2021-04-20 10:41:57+00:00
- **Authors**: Jianhao Jiao, Huaiyang Huang, Liang Li, Zhijian He, Yilong Zhu, Ming Liu
- **Comment**: 9 pages, 7 figures, accepted by CVPR Workshop 2021
- **Journal**: None
- **Summary**: This paper investigates two typical image-type representations for event camera-based tracking: time surface (TS) and event map (EM). Based on the original TS-based tracker, we make use of these two representations' complementary strengths to develop an enhanced version. The proposed tracker consists of a general strategy to evaluate the optimization problem's degeneracy online and then switch proper representations. Both TS and EM are motion- and scene-dependent, and thus it is important to figure out their limitations in tracking. We develop six tracker variations and conduct a thorough comparison of them on sequences covering various scenarios and motion complexities. We release our implementations and detailed results to benefit the research community on event cameras: https: //github.com/gogojjh/ESVO_extension.



### Posterior Sampling for Image Restoration using Explicit Patch Priors
- **Arxiv ID**: http://arxiv.org/abs/2104.09895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09895v1)
- **Published**: 2021-04-20 11:11:33+00:00
- **Updated**: 2021-04-20 11:11:33+00:00
- **Authors**: Roy Friedman, Yair Weiss
- **Comment**: None
- **Journal**: None
- **Summary**: Almost all existing methods for image restoration are based on optimizing the mean squared error (MSE), even though it is known that the best estimate in terms of MSE may yield a highly atypical image due to the fact that there are many plausible restorations for a given noisy image. In this paper, we show how to combine explicit priors on patches of natural images in order to sample from the posterior probability of a full image given a degraded image. We prove that our algorithm generates correct samples from the distribution $p(x|y) \propto \exp(-E(x|y))$ where $E(x|y)$ is the cost function minimized in previous patch-based approaches that compute a single restoration. Unlike previous approaches that computed a single restoration using MAP or MMSE, our method makes explicit the uncertainty in the restored images and guarantees that all patches in the restored images will be typical given the patch prior. Unlike previous approaches that used implicit priors on fixed-size images, our approach can be used with images of any size. Our experimental results show that posterior sampling using patch priors yields images of high perceptual quality and high PSNR on a range of challenging image restoration problems.



### Data-driven vehicle speed detection from synthetic driving simulator images
- **Arxiv ID**: http://arxiv.org/abs/2104.09903v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.09903v1)
- **Published**: 2021-04-20 11:26:13+00:00
- **Updated**: 2021-04-20 11:26:13+00:00
- **Authors**: Antonio Hernández Martínez, Javier Lorenzo Díaz, Iván García Daza, David Fernández Llorca
- **Comment**: Submitted to the IEEE Intelligent Transportation Systems Conference
  2021 (ITSC2021)
- **Journal**: None
- **Summary**: Despite all the challenges and limitations, vision-based vehicle speed detection is gaining research interest due to its great potential benefits such as cost reduction, and enhanced additional functions. As stated in a recent survey [1], the use of learning-based approaches to address this problem is still in its infancy. One of the main difficulties is the need for a large amount of data, which must contain the input sequences and, more importantly, the output values corresponding to the actual speed of the vehicles. Data collection in this context requires a complex and costly setup to capture the images from the camera synchronized with a high precision speed sensor to generate the ground truth speed values. In this paper we explore, for the first time, the use of synthetic images generated from a driving simulator (e.g., CARLA) to address vehicle speed detection using a learning-based approach. We simulate a virtual camera placed over a stretch of road, and generate thousands of images with variability corresponding to multiple speeds, different vehicle types and colors, and lighting and weather conditions. Two different approaches to map the sequence of images to an output speed (regression) are studied, including CNN-GRU and 3D-CNN. We present preliminary results that support the high potential of this approach to address vehicle speed detection.



### Table Tennis Stroke Recognition Using Two-Dimensional Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2104.09907v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.09907v2)
- **Published**: 2021-04-20 11:32:43+00:00
- **Updated**: 2021-05-31 18:59:57+00:00
- **Authors**: Kaustubh Milind Kulkarni, Sucheth Shenoy
- **Comment**: Accepted at CVPR Sports Workshop 2021 (7th International Workshop on
  Computer Vision in Sports) (CVSports)
- **Journal**: None
- **Summary**: We introduce a novel method for collecting table tennis video data and perform stroke detection and classification. A diverse dataset containing video data of 11 basic strokes obtained from 14 professional table tennis players, summing up to a total of 22111 videos has been collected using the proposed setup. The temporal convolutional neural network model developed using 2D pose estimation performs multiclass classification of these 11 table tennis strokes with a validation accuracy of 99.37%. Moreover, the neural network generalizes well over the data of a player excluded from the training and validation dataset, classifying the fresh strokes with an overall best accuracy of 98.72%. Various model architectures using machine learning and deep learning based approaches have been trained for stroke recognition and their performances have been compared and benchmarked. Inferences such as performance monitoring and stroke comparison of the players using the model have been discussed. Therefore, we are contributing to the development of a computer vision based sports analytics system for the sport of table tennis that focuses on the previously unexploited aspect of the sport i.e., a player's strokes, which is extremely insightful for performance improvement.



### Semantic Segmentation by Improved Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.09917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09917v1)
- **Published**: 2021-04-20 11:59:29+00:00
- **Updated**: 2021-04-20 11:59:29+00:00
- **Authors**: ZengShun Zhaoa, Yulong Wang, Ke Liu, Haoran Yang, Qian Sun, Heng Qiao
- **Comment**: arXiv admin note: text overlap with arXiv:1802.07934 by other authors
- **Journal**: None
- **Summary**: While most existing segmentation methods usually combined the powerful feature extraction capabilities of CNNs with Conditional Random Fields (CRFs) post-processing, the result always limited by the fault of CRFs . Due to the notoriously slow calculation speeds and poor efficiency of CRFs, in recent years, CRFs post-processing has been gradually eliminated. In this paper, an improved Generative Adversarial Networks (GANs) for image semantic segmentation task (semantic segmentation by GANs, Seg-GAN) is proposed to facilitate further segmentation research. In addition, we introduce Convolutional CRFs (ConvCRFs) as an effective improvement solution for the image semantic segmentation task. Towards the goal of differentiating the segmentation results from the ground truth distribution and improving the details of the output images, the proposed discriminator network is specially designed in a full convolutional manner combined with cascaded ConvCRFs. Besides, the adversarial loss aggressively encourages the output image to be close to the distribution of the ground truth. Our method not only learns an end-to-end mapping from input image to corresponding output image, but also learns a loss function to train this mapping. The experiments show that our method achieves better performance than state-of-the-art methods.



### CrossATNet - A Novel Cross-Attention Based Framework for Sketch-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2104.09918v1
- **DOI**: 10.1016/j.imavis.2020.104003
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.09918v1)
- **Published**: 2021-04-20 12:11:12+00:00
- **Updated**: 2021-04-20 12:11:12+00:00
- **Authors**: Ushasi Chaudhuri, Biplab Banerjee, Avik Bhattacharya, Mihai Datcu
- **Comment**: Accepted in Journal of Image and Vision Computing
- **Journal**: None
- **Summary**: We propose a novel framework for cross-modal zero-shot learning (ZSL) in the context of sketch-based image retrieval (SBIR). Conventionally, the SBIR schema mainly considers simultaneous mappings among the two image views and the semantic side information. Therefore, it is desirable to consider fine-grained classes mainly in the sketch domain using highly discriminative and semantically rich feature space. However, the existing deep generative modeling-based SBIR approaches majorly focus on bridging the gaps between the seen and unseen classes by generating pseudo-unseen-class samples. Besides, violating the ZSL protocol by not utilizing any unseen-class information during training, such techniques do not pay explicit attention to modeling the discriminative nature of the shared space. Also, we note that learning a unified feature space for both the multi-view visual data is a tedious task considering the significant domain difference between sketches and color images. In this respect, as a remedy, we introduce a novel framework for zero-shot SBIR. While we define a cross-modal triplet loss to ensure the discriminative nature of the shared space, an innovative cross-modal attention learning strategy is also proposed to guide feature extraction from the image domain exploiting information from the respective sketch counterpart. In order to preserve the semantic consistency of the shared space, we consider a graph CNN-based module that propagates the semantic class topology to the shared space. To ensure an improved response time during inference, we further explore the possibility of representing the shared space in terms of hash codes. Experimental results obtained on the benchmark TU-Berlin and the Sketchy datasets confirm the superiority of CrossATNet in yielding state-of-the-art results.



### GAN-Based Data Augmentation and Anonymization for Skin-Lesion Analysis: A Critical Review
- **Arxiv ID**: http://arxiv.org/abs/2104.10603v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.10603v1)
- **Published**: 2021-04-20 12:47:22+00:00
- **Updated**: 2021-04-20 12:47:22+00:00
- **Authors**: Alceu Bissoto, Eduardo Valle, Sandra Avila
- **Comment**: Accepted to the ISIC Skin Image Analysis Workshop @ CVPR 2021
- **Journal**: None
- **Summary**: Despite the growing availability of high-quality public datasets, the lack of training samples is still one of the main challenges of deep-learning for skin lesion analysis. Generative Adversarial Networks (GANs) appear as an enticing alternative to alleviate the issue, by synthesizing samples indistinguishable from real images, with a plethora of works employing them for medical applications. Nevertheless, carefully designed experiments for skin-lesion diagnosis with GAN-based data augmentation show favorable results only on out-of-distribution test sets. For GAN-based data anonymization $-$ where the synthetic images replace the real ones $-$ favorable results also only appear for out-of-distribution test sets. Because of the costs and risks associated with GAN usage, those results suggest caution in their adoption for medical applications.



### DynO: Dynamic Onloading of Deep Neural Networks from Cloud to Device
- **Arxiv ID**: http://arxiv.org/abs/2104.09949v2
- **DOI**: 10.1145/3510831
- **Categories**: **cs.DC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.09949v2)
- **Published**: 2021-04-20 13:20:15+00:00
- **Updated**: 2022-01-11 10:11:50+00:00
- **Authors**: Mario Almeida, Stefanos Laskaridis, Stylianos I. Venieris, Ilias Leontiadis, Nicholas D. Lane
- **Comment**: Accepted for publication at the ACM Transactions on Embedded
  Computing Systems (TECS) in the special issue on Accelerating AI on the Edge
- **Journal**: None
- **Summary**: Recently, there has been an explosive growth of mobile and embedded applications using convolutional neural networks(CNNs). To alleviate their excessive computational demands, developers have traditionally resorted to cloud offloading, inducing high infrastructure costs and a strong dependence on networking conditions. On the other end, the emergence of powerful SoCs is gradually enabling on-device execution. Nonetheless, low- and mid-tier platforms still struggle to run state-of-the-art CNNs sufficiently. In this paper, we present DynO, a distributed inference framework that combines the best of both worlds to address several challenges, such as device heterogeneity, varying bandwidth and multi-objective requirements. Key components that enable this are its novel CNN-specific data packing method, which exploits the variability of precision needs in different parts of the CNN when onloading computation, and its novel scheduler that jointly tunes the partition point and transferred data precision at run time to adapt inference to its execution environment. Quantitative evaluation shows that DynO outperforms the current state-of-the-art, improving throughput by over an order of magnitude over device-only execution and up to 7.9x over competing CNN offloading systems, with up to 60x less data transferred.



### MGSampler: An Explainable Sampling Strategy for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.09952v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09952v2)
- **Published**: 2021-04-20 13:24:01+00:00
- **Updated**: 2021-08-20 07:15:58+00:00
- **Authors**: Yuan Zhi, Zhan Tong, Limin Wang, Gangshan Wu
- **Comment**: ICCV 2021 camera ready version
- **Journal**: None
- **Summary**: Frame sampling is a fundamental problem in video action recognition due to the essential redundancy in time and limited computation resources. The existing sampling strategy often employs a fixed frame selection and lacks the flexibility to deal with complex variations in videos. In this paper, we present a simple, sparse, and explainable frame sampler, termed as Motion-Guided Sampler (MGSampler). Our basic motivation is that motion is an important and universal signal that can drive us to adaptively select frames from videos. Accordingly, we propose two important properties in our MGSampler design: motion sensitive and motion uniform. First, we present two different motion representations to enable us to efficiently distinguish the motion-salient frames from the background. Then, we devise a motion-uniform sampling strategy based on the cumulative motion distribution to ensure the sampled frames evenly cover all the important segments with high motion salience. Our MGSampler yields a new principled and holistic sampling scheme, that could be incorporated into any existing video architecture. Experiments on five benchmarks demonstrate the effectiveness of our MGSampler over the previous fixed sampling strategies, and its generalization power across different backbones, video models, and datasets.



### Evaluating Deep Neural Networks Trained on Clinical Images in Dermatology with the Fitzpatrick 17k Dataset
- **Arxiv ID**: http://arxiv.org/abs/2104.09957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09957v1)
- **Published**: 2021-04-20 13:37:30+00:00
- **Updated**: 2021-04-20 13:37:30+00:00
- **Authors**: Matthew Groh, Caleb Harris, Luis Soenksen, Felix Lau, Rachel Han, Aerin Kim, Arash Koochek, Omar Badri
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition, pp. 1820-1828. 2021
- **Summary**: How does the accuracy of deep neural network models trained to classify clinical images of skin conditions vary across skin color? While recent studies demonstrate computer vision models can serve as a useful decision support tool in healthcare and provide dermatologist-level classification on a number of specific tasks, darker skin is underrepresented in the data. Most publicly available data sets do not include Fitzpatrick skin type labels. We annotate 16,577 clinical images sourced from two dermatology atlases with Fitzpatrick skin type labels and open-source these annotations. Based on these labels, we find that there are significantly more images of light skin types than dark skin types in this dataset. We train a deep neural network model to classify 114 skin conditions and find that the model is most accurate on skin types similar to those it was trained on. In addition, we evaluate how an algorithmic approach to identifying skin tones, individual typology angle, compares with Fitzpatrick skin type labels annotated by a team of human labelers.



### Fine-grained Anomaly Detection via Multi-task Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2104.09993v2
- **DOI**: 10.1109/AVSS52988.2021.9663783
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09993v2)
- **Published**: 2021-04-20 14:19:08+00:00
- **Updated**: 2022-03-17 09:53:56+00:00
- **Authors**: Loic Jezequel, Ngoc-Son Vu, Jean Beaudet, Aymeric Histace
- **Comment**: None
- **Journal**: L. J\'ez\'equel, N. -S. Vu, J. Beaudet and A. Histace,
  "Fine-grained anomaly detection via multi-task self-supervision," 2021 17th
  IEEE International Conference on Advanced Video and Signal Based Surveillance
  (AVSS), 2021, pp. 1-8
- **Summary**: Detecting anomalies using deep learning has become a major challenge over the last years, and is becoming increasingly promising in several fields. The introduction of self-supervised learning has greatly helped many methods including anomaly detection where simple geometric transformation recognition tasks are used. However these methods do not perform well on fine-grained problems since they lack finer features. By combining in a multi-task framework high-scale shape features oriented task with low-scale fine features oriented task, our method greatly improves fine-grained anomaly detection. It outperforms state-of-the-art with up to 31% relative error reduction measured with AUROC on various anomaly detection problems.



### An Exact Hypergraph Matching Algorithm for Nuclear Identification in Embryonic Caenorhabditis elegans
- **Arxiv ID**: http://arxiv.org/abs/2104.10003v3
- **DOI**: 10.1371/journal.pone.0277343
- **Categories**: **cs.CV**, cs.DM, math.CO
- **Links**: [PDF](http://arxiv.org/pdf/2104.10003v3)
- **Published**: 2021-04-20 14:34:30+00:00
- **Updated**: 2022-07-09 20:16:47+00:00
- **Authors**: Andrew Lauziere, Ryan Christensen, Hari Shroff, Radu Balan
- **Comment**: 20 pages, 11 figures
- **Journal**: None
- **Summary**: Finding an optimal correspondence between point sets is a common task in computer vision. Existing techniques assume relatively simple relationships among points and do not guarantee an optimal match. We introduce an algorithm capable of exactly solving point set matching by modeling the task as hypergraph matching. The algorithm extends the classical branch and bound paradigm to select and aggregate vertices under a proposed decomposition of the multilinear objective function. The methodology is motivated by Caenorhabditis elegans, a model organism used frequently in developmental biology and neurobiology. The embryonic C. elegans contains seam cells that can act as fiducial markers allowing the identification of other nuclei during embryo development. The proposed algorithm identifies seam cells more accurately than established point-set matching methods, while providing a framework to approach other similarly complex point set matching tasks.



### Perceptual Loss for Robust Unsupervised Homography Estimation
- **Arxiv ID**: http://arxiv.org/abs/2104.10011v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.10011v1)
- **Published**: 2021-04-20 14:41:54+00:00
- **Updated**: 2021-04-20 14:41:54+00:00
- **Authors**: Daniel Koguciuk, Elahe Arani, Bahram Zonooz
- **Comment**: Accepted at Image Matching: Local Features & Beyond (CVPR 2021
  Workshop)
- **Journal**: None
- **Summary**: Homography estimation is often an indispensable step in many computer vision tasks. The existing approaches, however, are not robust to illumination and/or larger viewpoint changes. In this paper, we propose bidirectional implicit Homography Estimation (biHomE) loss for unsupervised homography estimation. biHomE minimizes the distance in the feature space between the warped image from the source viewpoint and the corresponding image from the target viewpoint. Since we use a fixed pre-trained feature extractor and the only learnable component of our framework is the homography network, we effectively decouple the homography estimation from representation learning. We use an additional photometric distortion step in the synthetic COCO dataset generation to better represent the illumination variation of the real-world scenarios. We show that biHomE achieves state-of-the-art performance on synthetic COCO dataset, which is also comparable or better compared to supervised approaches. Furthermore, the empirical results demonstrate the robustness of our approach to illumination variation compared to existing methods.



### GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement
- **Arxiv ID**: http://arxiv.org/abs/2104.09958v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.09958v3)
- **Published**: 2021-04-20 14:59:27+00:00
- **Updated**: 2022-01-25 18:15:16+00:00
- **Authors**: Martin Engelcke, Oiwi Parker Jones, Ingmar Posner
- **Comment**: NeurIPS 2021 camera-ready version; 26 pages, 19 figures
- **Journal**: None
- **Summary**: Advances in unsupervised learning of object-representations have culminated in the development of a broad range of methods for unsupervised object segmentation and interpretable object-centric scene generation. These methods, however, are limited to simulated and real-world datasets with limited visual complexity. Moreover, object representations are often inferred using RNNs which do not scale well to large images or iterative refinement which avoids imposing an unnatural ordering on objects in an image but requires the a priori initialisation of a fixed number of object representations. In contrast to established paradigms, this work proposes an embedding-based approach in which embeddings of pixels are clustered in a differentiable fashion using a stochastic stick-breaking process. Similar to iterative refinement, this clustering procedure also leads to randomly ordered object representations, but without the need of initialising a fixed number of clusters a priori. This is used to develop a new model, GENESIS-v2, which can infer a variable number of object representations without using RNNs or iterative refinement. We show that GENESIS-v2 performs strongly in comparison to recent baselines in terms of unsupervised image segmentation and object-centric scene generation on established synthetic datasets as well as more complex real-world datasets.



### Multiple Sclerosis Lesion Analysis in Brain Magnetic Resonance Images: Techniques and Clinical Applications
- **Arxiv ID**: http://arxiv.org/abs/2104.10029v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2104.10029v3)
- **Published**: 2021-04-20 15:08:51+00:00
- **Updated**: 2022-01-28 00:43:12+00:00
- **Authors**: Yang Ma, Chaoyi Zhang, Mariano Cabezas, Yang Song, Zihao Tang, Dongnan Liu, Weidong Cai, Michael Barnett, Chenyu Wang
- **Comment**: Accepted to appear in IEEE Journal of Biomedical And Health
  Informatics
- **Journal**: None
- **Summary**: Multiple sclerosis (MS) is a chronic inflammatory and degenerative disease of the central nervous system, characterized by the appearance of focal lesions in the white and gray matter that topographically correlate with an individual patient's neurological symptoms and signs. Magnetic resonance imaging (MRI) provides detailed in-vivo structural information, permitting the quantification and categorization of MS lesions that critically inform disease management. Traditionally, MS lesions have been manually annotated on 2D MRI slices, a process that is inefficient and prone to inter-/intra-observer errors. Recently, automated statistical imaging analysis techniques have been proposed to detect and segment MS lesions based on MRI voxel intensity. However, their effectiveness is limited by the heterogeneity of both MRI data acquisition techniques and the appearance of MS lesions. By learning complex lesion representations directly from images, deep learning techniques have achieved remarkable breakthroughs in the MS lesion segmentation task. Here, we provide a comprehensive review of state-of-the-art automatic statistical and deep-learning MS segmentation methods and discuss current and future clinical applications. Further, we review technical strategies, such as domain adaptation, to enhance MS lesion segmentation in real-world clinical settings.



### VT-ADL: A Vision Transformer Network for Image Anomaly Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/2104.10036v1
- **DOI**: 10.1109/ISIE45552.2021.9576231
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.10036v1)
- **Published**: 2021-04-20 15:12:30+00:00
- **Updated**: 2021-04-20 15:12:30+00:00
- **Authors**: Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, Gian Luca Foresti
- **Comment**: 6 Pages, 4 images, conference published paper
- **Journal**: IEEE 30th International Symposium on Industrial Electronics
  (ISIE), 2021
- **Summary**: We present a transformer-based image anomaly detection and localization network. Our proposed model is a combination of a reconstruction-based approach and patch embedding. The use of transformer networks helps to preserve the spatial information of the embedded patches, which are later processed by a Gaussian mixture density network to localize the anomalous areas. In addition, we also publish BTAD, a real-world industrial anomaly dataset. Our results are compared with other state-of-the-art algorithms using publicly available datasets like MNIST and MVTec.



### Geometric Deep Learning on Anatomical Meshes for the Prediction of Alzheimer's Disease
- **Arxiv ID**: http://arxiv.org/abs/2104.10047v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.10047v1)
- **Published**: 2021-04-20 15:17:13+00:00
- **Updated**: 2021-04-20 15:17:13+00:00
- **Authors**: Ignacio Sarasua, Jonwong Lee, Christian Wachinger
- **Comment**: None
- **Journal**: None
- **Summary**: Geometric deep learning can find representations that are optimal for a given task and therefore improve the performance over pre-defined representations.   While current work has mainly focused on point representations, meshes also contain connectivity information and are therefore a more comprehensive characterization of the underlying anatomical surface.   In this work, we evaluate four recent geometric deep learning approaches that operate on mesh representations.   These approaches can be grouped into template-free and template-based approaches, where the template-based methods need a more elaborate pre-processing step with the definition of a common reference template and correspondences.   We compare the different networks for the prediction of Alzheimer's disease based on the meshes of the hippocampus.   Our results show advantages for template-based methods in terms of accuracy, number of learnable parameters, and training speed.   While the template creation may be limiting for some applications, neuroimaging has a long history of building templates with automated tools readily available.   Overall, working with meshes is more involved than working with simplistic point clouds, but they also offer new avenues for designing geometric deep learning architectures.



### Semantic similarity metrics for learned image registration
- **Arxiv ID**: http://arxiv.org/abs/2104.10051v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.10051v1)
- **Published**: 2021-04-20 15:23:58+00:00
- **Updated**: 2021-04-20 15:23:58+00:00
- **Authors**: Steffen Czolbe, Oswin Krause, Aasa Feragen
- **Comment**: Published at MIDL 2021 (Oral). Reviews and discussion on Open Review:
  https://openreview.net/forum?id=9M5cH--UdcC. arXiv admin note: text overlap
  with arXiv:2011.05735
- **Journal**: None
- **Summary**: We propose a semantic similarity metric for image registration. Existing metrics like Euclidean Distance or Normalized Cross-Correlation focus on aligning intensity values, giving difficulties with low intensity contrast or noise. Our approach learns dataset-specific features that drive the optimization of a learning-based registration model. We train both an unsupervised approach using an auto-encoder, and a semi-supervised approach using supplemental segmentation data to extract semantic features for image registration. Comparing to existing methods across multiple image modalities and applications, we achieve consistently high registration accuracy. A learned invariance to noise gives smoother transformations on low-quality images.



### T2VLAD: Global-Local Sequence Alignment for Text-Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2104.10054v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.10054v1)
- **Published**: 2021-04-20 15:26:24+00:00
- **Updated**: 2021-04-20 15:26:24+00:00
- **Authors**: Xiaohan Wang, Linchao Zhu, Yi Yang
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Text-video retrieval is a challenging task that aims to search relevant video contents based on natural language descriptions. The key to this problem is to measure text-video similarities in a joint embedding space. However, most existing methods only consider the global cross-modal similarity and overlook the local details. Some works incorporate the local comparisons through cross-modal local matching and reasoning. These complex operations introduce tremendous computation. In this paper, we design an efficient global-local alignment method. The multi-modal video sequences and text features are adaptively aggregated with a set of shared semantic centers. The local cross-modal similarities are computed between the video feature and text feature within the same center. This design enables the meticulous local comparison and reduces the computational cost of the interaction between each text-video pair. Moreover, a global alignment method is proposed to provide a global cross-modal measurement that is complementary to the local perspective. The global aggregated visual features also provide additional supervision, which is indispensable to the optimization of the learnable semantic centers. We achieve consistent improvements on three standard text-video retrieval benchmarks and outperform the state-of-the-art by a clear margin.



### UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2104.10078v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10078v2)
- **Published**: 2021-04-20 15:59:38+00:00
- **Updated**: 2021-10-08 15:25:13+00:00
- **Authors**: Michael Oechsle, Songyou Peng, Andreas Geiger
- **Comment**: ICCV 2021 oral
- **Journal**: None
- **Summary**: Neural implicit 3D representations have emerged as a powerful paradigm for reconstructing surfaces from multi-view images and synthesizing novel views. Unfortunately, existing methods such as DVR or IDR require accurate per-pixel object masks as supervision. At the same time, neural radiance fields have revolutionized novel view synthesis. However, NeRF's estimated volume density does not admit accurate surface reconstruction. Our key insight is that implicit surface models and radiance fields can be formulated in a unified way, enabling both surface and volume rendering using the same model. This unified perspective enables novel, more efficient sampling procedures and the ability to reconstruct accurate surfaces without input masks. We compare our method on the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments demonstrate that we outperform NeRF in terms of reconstruction quality while performing on par with IDR without requiring masks.



### Class-Incremental Learning with Generative Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2104.10093v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.10093v2)
- **Published**: 2021-04-20 16:26:14+00:00
- **Updated**: 2021-04-28 09:19:48+00:00
- **Authors**: Gido M. van de Ven, Zhe Li, Andreas S. Tolias
- **Comment**: To appear in the IEEE Conference on Computer Vision and Pattern
  Recognition Workshop (CVPR-W) on Continual Learning in Computer Vision
  (CLVision) 2021
- **Journal**: None
- **Summary**: Incrementally training deep neural networks to recognize new classes is a challenging problem. Most existing class-incremental learning methods store data or use generative replay, both of which have drawbacks, while 'rehearsal-free' alternatives such as parameter regularization or bias-correction methods do not consistently achieve high performance. Here, we put forward a new strategy for class-incremental learning: generative classification. Rather than directly learning the conditional distribution p(y|x), our proposal is to learn the joint distribution p(x,y), factorized as p(x|y)p(y), and to perform classification using Bayes' rule. As a proof-of-principle, here we implement this strategy by training a variational autoencoder for each class to be learned and by using importance sampling to estimate the likelihoods p(x|y). This simple approach performs very well on a diverse set of continual learning benchmarks, outperforming generative replay and other existing baselines that do not store data.



### Detection of Audio-Video Synchronization Errors Via Event Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.10116v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2104.10116v1)
- **Published**: 2021-04-20 16:54:44+00:00
- **Updated**: 2021-04-20 16:54:44+00:00
- **Authors**: Joshua P. Ebenezer, Yongjun Wu, Hai Wei, Sriram Sethuraman, Zongyi Liu
- **Comment**: To be published in ICASSP 2021
- **Journal**: None
- **Summary**: We present a new method and a large-scale database to detect audio-video synchronization(A/V sync) errors in tennis videos. A deep network is trained to detect the visual signature of the tennis ball being hit by the racquet in the video stream. Another deep network is trained to detect the auditory signature of the same event in the audio stream. During evaluation, the audio stream is searched by the audio network for the audio event of the ball being hit. If the event is found in audio, the neighboring interval in video is searched for the corresponding visual signature. If the event is not found in the video stream but is found in the audio stream, A/V sync error is flagged. We developed a large-scaled database of 504,300 frames from 6 hours of videos of tennis events, simulated A/V sync errors, and found our method achieves high accuracy on the task.



### Improving state-of-the-art in Detecting Student Engagement with Resnet and TCN Hybrid Network
- **Arxiv ID**: http://arxiv.org/abs/2104.10122v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2104.10122v2)
- **Published**: 2021-04-20 17:10:13+00:00
- **Updated**: 2021-10-16 18:45:49+00:00
- **Authors**: Ali Abedi, Shehroz S. Khan
- **Comment**: 7 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: Automatic detection of students' engagement in online learning settings is a key element to improve the quality of learning and to deliver personalized learning materials to them. Varying levels of engagement exhibited by students in an online classroom is an affective behavior that takes place over space and time. Therefore, we formulate detecting levels of students' engagement from videos as a spatio-temporal classification problem. In this paper, we present a novel end-to-end Residual Network (ResNet) and Temporal Convolutional Network (TCN) hybrid neural network architecture for students' engagement level detection in videos. The 2D ResNet extracts spatial features from consecutive video frames, and the TCN analyzes the temporal changes in video frames to detect the level of engagement. The spatial and temporal arms of the hybrid network are jointly trained on raw video frames of a large publicly available students' engagement detection dataset, DAiSEE. We compared our method with several competing students' engagement detection methods on this dataset. The ResNet+TCN architecture outperforms all other studied methods, improves the state-of-the-art engagement level detection accuracy, and sets a new baseline for future research.



### Generative Transformer for Accurate and Reliable Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.10127v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10127v5)
- **Published**: 2021-04-20 17:12:51+00:00
- **Updated**: 2022-12-30 12:12:38+00:00
- **Authors**: Yuxin Mao, Jing Zhang, Zhexiong Wan, Yuchao Dai, Aixuan Li, Yunqiu Lv, Xinyu Tian, Deng-Ping Fan, Nick Barnes
- **Comment**: Technical report, 18 pages, 17 figures
- **Journal**: None
- **Summary**: Transformer, which originates from machine translation, is particularly powerful at modeling long-range dependencies. Currently, the transformer is making revolutionary progress in various vision tasks, leading to significant performance improvements compared with the convolutional neural network (CNN) based frameworks. In this paper, we conduct extensive research on exploiting the contributions of transformers for accurate and reliable salient object detection. For the former, we apply transformer to a deterministic model, and explain that the effective structure modeling and global context modeling abilities lead to its superior performance compared with the CNN based frameworks. For the latter, we observe that both CNN and transformer based frameworks suffer greatly from the over-confidence issue, where the models tend to generate wrong predictions with high confidence. To estimate the reliability degree of both CNN- and transformer-based frameworks, we further present a latent variable model, namely inferential generative adversarial network (iGAN), based on the generative adversarial network (GAN). The stochastic attribute of the latent variable makes it convenient to estimate the predictive uncertainty, serving as an auxiliary output to evaluate the reliability of model prediction. Different from the conventional GAN, which defines the distribution of the latent variable as fixed standard normal distribution $\mathcal{N}(0,\mathbf{I})$, the proposed iGAN infers the latent variable by gradient-based Markov Chain Monte Carlo (MCMC), namely Langevin dynamics, leading to an input-dependent latent variable model. We apply our proposed iGAN to both fully and weakly supervised salient object detection, and explain that iGAN within the transformer framework leads to both accurate and reliable salient object detection.



### Large Scale Interactive Motion Forecasting for Autonomous Driving : The Waymo Open Motion Dataset
- **Arxiv ID**: http://arxiv.org/abs/2104.10133v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.10133v1)
- **Published**: 2021-04-20 17:19:05+00:00
- **Updated**: 2021-04-20 17:19:05+00:00
- **Authors**: Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles Qi, Yin Zhou, Zoey Yang, Aurelien Chouard, Pei Sun, Jiquan Ngiam, Vijay Vasudevan, Alexander McCauley, Jonathon Shlens, Dragomir Anguelov
- **Comment**: 15 pages, 10 figures
- **Journal**: None
- **Summary**: As autonomous driving systems mature, motion forecasting has received increasing attention as a critical requirement for planning. Of particular importance are interactive situations such as merges, unprotected turns, etc., where predicting individual object motion is not sufficient. Joint predictions of multiple objects are required for effective route planning. There has been a critical need for high-quality motion data that is rich in both interactions and annotation to develop motion planning models. In this work, we introduce the most diverse interactive motion dataset to our knowledge, and provide specific labels for interacting objects suitable for developing joint prediction models. With over 100,000 scenes, each 20 seconds long at 10 Hz, our new dataset contains more than 570 hours of unique data over 1750 km of roadways. It was collected by mining for interesting interactions between vehicles, pedestrians, and cyclists across six cities within the United States. We use a high-accuracy 3D auto-labeling system to generate high quality 3D bounding boxes for each road agent, and provide corresponding high definition 3D maps for each scene. Furthermore, we introduce a new set of metrics that provides a comprehensive evaluation of both single agent and joint agent interaction motion forecasting models. Finally, we provide strong baseline models for individual-agent prediction and joint-prediction. We hope that this new large-scale interactive motion dataset will provide new opportunities for advancing motion forecasting models.



### Variational Relational Point Completion Network
- **Arxiv ID**: http://arxiv.org/abs/2104.10154v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.10154v1)
- **Published**: 2021-04-20 17:53:40+00:00
- **Updated**: 2021-04-20 17:53:40+00:00
- **Authors**: Liang Pan, Xinyi Chen, Zhongang Cai, Junzhe Zhang, Haiyu Zhao, Shuai Yi, Ziwei Liu
- **Comment**: 15 pages, 13 figures, accepted to CVPR 2021 (Oral), project webpage:
  https://paul007pl.github.io/projects/VRCNet.html
- **Journal**: None
- **Summary**: Real-scanned point clouds are often incomplete due to viewpoint, occlusion, and noise. Existing point cloud completion methods tend to generate global shape skeletons and hence lack fine local details. Furthermore, they mostly learn a deterministic partial-to-complete mapping, but overlook structural relations in man-made objects. To tackle these challenges, this paper proposes a variational framework, Variational Relational point Completion network (VRCNet) with two appealing properties: 1) Probabilistic Modeling. In particular, we propose a dual-path architecture to enable principled probabilistic modeling across partial and complete clouds. One path consumes complete point clouds for reconstruction by learning a point VAE. The other path generates complete shapes for partial point clouds, whose embedded distribution is guided by distribution obtained from the reconstruction path during training. 2) Relational Enhancement. Specifically, we carefully design point self-attention kernel and point selective kernel module to exploit relational point features, which refines local shape details conditioned on the coarse completion. In addition, we contribute a multi-view partial point cloud dataset (MVP dataset) containing over 100,000 high-quality scans, which renders partial 3D shapes from 26 uniformly distributed camera poses for each 3D CAD model. Extensive experiments demonstrate that VRCNet outperforms state-of-theart methods on all standard point cloud completion benchmarks. Notably, VRCNet shows great generalizability and robustness on real-world point cloud scans.



### Understanding Synonymous Referring Expressions via Contrastive Features
- **Arxiv ID**: http://arxiv.org/abs/2104.10156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10156v1)
- **Published**: 2021-04-20 17:56:24+00:00
- **Updated**: 2021-04-20 17:56:24+00:00
- **Authors**: Yi-Wen Chen, Yi-Hsuan Tsai, Ming-Hsuan Yang
- **Comment**: Codes and models will be available at
  https://github.com/wenz116/RefContrast
- **Journal**: None
- **Summary**: Referring expression comprehension aims to localize objects identified by natural language descriptions. This is a challenging task as it requires understanding of both visual and language domains. One nature is that each object can be described by synonymous sentences with paraphrases, and such varieties in languages have critical impact on learning a comprehension model. While prior work usually treats each sentence and attends it to an object separately, we focus on learning a referring expression comprehension model that considers the property in synonymous sentences. To this end, we develop an end-to-end trainable framework to learn contrastive features on the image and object instance levels, where features extracted from synonymous sentences to describe the same object should be closer to each other after mapping to the visual domain. We conduct extensive experiments to evaluate the proposed algorithm on several benchmark datasets, and demonstrate that our method performs favorably against the state-of-the-art approaches. Furthermore, since the varieties in expressions become larger across datasets when they describe objects in different ways, we present the cross-dataset and transfer learning settings to validate the ability of our learned transferable features.



### VideoGPT: Video Generation using VQ-VAE and Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.10157v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.10157v2)
- **Published**: 2021-04-20 17:58:03+00:00
- **Updated**: 2021-09-14 21:20:06+00:00
- **Authors**: Wilson Yan, Yunzhi Zhang, Pieter Abbeel, Aravind Srinivas
- **Comment**: Project website: https://wilson1yan.github.io/videogpt/index.html
- **Journal**: None
- **Summary**: We present VideoGPT: a conceptually simple architecture for scaling likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention. A simple GPT-like architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings. Despite the simplicity in formulation and ease of training, our architecture is able to generate samples competitive with state-of-the-art GAN models for video generation on the BAIR Robot dataset, and generate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset (TGIF). We hope our proposed architecture serves as a reproducible reference for a minimalistic implementation of transformer based video generation models. Samples and code are available at https://wilson1yan.github.io/videogpt/index.html



### Visualizing Adapted Knowledge in Domain Transfer
- **Arxiv ID**: http://arxiv.org/abs/2104.10602v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.10602v2)
- **Published**: 2021-04-20 17:59:05+00:00
- **Updated**: 2021-05-01 11:11:24+00:00
- **Authors**: Yunzhong Hou, Liang Zheng
- **Comment**: None
- **Journal**: CVPR 2021
- **Summary**: A source model trained on source data and a target model learned through unsupervised domain adaptation (UDA) usually encode different knowledge. To understand the adaptation process, we portray their knowledge difference with image translation. Specifically, we feed a translated image and its original version to the two models respectively, formulating two branches. Through updating the translated image, we force similar outputs from the two branches. When such requirements are met, differences between the two images can compensate for and hence represent the knowledge difference between models. To enforce similar outputs from the two branches and depict the adapted knowledge, we propose a source-free image translation method that generates source-style images using only target images and the two models. We visualize the adapted knowledge on several datasets with different UDA methods and find that generated images successfully capture the style difference between the two domains. For application, we show that generated images enable further tuning of the target model without accessing source data. Code available at https://github.com/hou-yz/DA_visualization.



### Recognition of handwritten MNIST digits on low-memory 2 Kb RAM Arduino board using LogNNet reservoir neural network
- **Arxiv ID**: http://arxiv.org/abs/2105.02953v1
- **DOI**: 10.1088/1757-899X/1155/1/012056
- **Categories**: **cs.LG**, cs.CV, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.02953v1)
- **Published**: 2021-04-20 18:16:23+00:00
- **Updated**: 2021-04-20 18:16:23+00:00
- **Authors**: Y. A. Izotov, A. A. Velichko, A. A. Ivshin, R. E. Novitskiy
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: The presented compact algorithm for recognizing handwritten digits of the MNIST database, created on the LogNNet reservoir neural network, reaches the recognition accuracy of 82%. The algorithm was tested on a low-memory Arduino board with 2 Kb static RAM low-power microcontroller. The dependences of the accuracy and time of image recognition on the number of neurons in the reservoir have been investigated. The memory allocation demonstrates that the algorithm stores all the necessary information in RAM without using additional data storage, and operates with original images without preliminary processing. The simple structure of the algorithm, with appropriate training, can be adapted for wide practical application, for example, for creating mobile biosensors for early diagnosis of adverse events in medicine. The study results are important for the implementation of artificial intelligence on peripheral constrained IoT devices and for edge computing.



### Auto-FedAvg: Learnable Federated Averaging for Multi-Institutional Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.10195v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.10195v1)
- **Published**: 2021-04-20 18:29:44+00:00
- **Updated**: 2021-04-20 18:29:44+00:00
- **Authors**: Yingda Xia, Dong Yang, Wenqi Li, Andriy Myronenko, Daguang Xu, Hirofumi Obinata, Hitoshi Mori, Peng An, Stephanie Harmon, Evrim Turkbey, Baris Turkbey, Bradford Wood, Francesca Patella, Elvira Stellato, Gianpaolo Carrafiello, Anna Ierardi, Alan Yuille, Holger Roth
- **Comment**: None
- **Journal**: None
- **Summary**: Federated learning (FL) enables collaborative model training while preserving each participant's privacy, which is particularly beneficial to the medical field. FedAvg is a standard algorithm that uses fixed weights, often originating from the dataset sizes at each client, to aggregate the distributed learned models on a server during the FL process. However, non-identical data distribution across clients, known as the non-i.i.d problem in FL, could make this assumption for setting fixed aggregation weights sub-optimal. In this work, we design a new data-driven approach, namely Auto-FedAvg, where aggregation weights are dynamically adjusted, depending on data distributions across data silos and the current training progress of the models. We disentangle the parameter set into two parts, local model parameters and global aggregation parameters, and update them iteratively with a communication-efficient algorithm. We first show the validity of our approach by outperforming state-of-the-art FL methods for image recognition on a heterogeneous data split of CIFAR-10. Furthermore, we demonstrate our algorithm's effectiveness on two multi-institutional medical image analysis tasks, i.e., COVID-19 lesion segmentation in chest CT and pancreas segmentation in abdominal CT.



### More Than Meets The Eye: Semi-supervised Learning Under Non-IID Data
- **Arxiv ID**: http://arxiv.org/abs/2104.10223v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.10223v1)
- **Published**: 2021-04-20 19:51:10+00:00
- **Updated**: 2021-04-20 19:51:10+00:00
- **Authors**: Saul Calderon-Ramirez, Luis Oala
- **Comment**: Presented as a RobustML workshop paper at ICLR 2021. Both authors
  contributed equally. This article extends arXiv:2006.07767
- **Journal**: None
- **Summary**: A common heuristic in semi-supervised deep learning (SSDL) is to select unlabelled data based on a notion of semantic similarity to the labelled data. For example, labelled images of numbers should be paired with unlabelled images of numbers instead of, say, unlabelled images of cars. We refer to this practice as semantic data set matching. In this work, we demonstrate the limits of semantic data set matching. We show that it can sometimes even degrade the performance for a state of the art SSDL algorithm. We present and make available a comprehensive simulation sandbox, called non-IID-SSDL, for stress testing an SSDL algorithm under different degrees of distribution mismatch between the labelled and unlabelled data sets. In addition, we demonstrate that simple density based dissimilarity measures in the feature space of a generic classifier offer a promising and more reliable quantitative matching criterion to select unlabelled data before SSDL training.



### Superpixels and Graph Convolutional Neural Networks for Efficient Detection of Nutrient Deficiency Stress from Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/2104.10249v3
- **DOI**: 10.1109/CVPRW53098.2021.00330
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.10249v3)
- **Published**: 2021-04-20 21:18:16+00:00
- **Updated**: 2022-11-15 23:27:59+00:00
- **Authors**: Saba Dadsetan, David Pichler, David Wilson, Naira Hovakimyan, Jennifer Hobbs
- **Comment**: None
- **Journal**: 2021 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW)
- **Summary**: Advances in remote sensing technology have led to the capture of massive amounts of data. Increased image resolution, more frequent revisit times, and additional spectral channels have created an explosion in the amount of data that is available to provide analyses and intelligence across domains, including agriculture. However, the processing of this data comes with a cost in terms of computation time and money, both of which must be considered when the goal of an algorithm is to provide real-time intelligence to improve efficiencies. Specifically, we seek to identify nutrient deficient areas from remotely sensed data to alert farmers to regions that require attention; detection of nutrient deficient areas is a key task in precision agriculture as farmers must quickly respond to struggling areas to protect their harvests. Past methods have focused on pixel-level classification (i.e. semantic segmentation) of the field to achieve these tasks, often using deep learning models with tens-of-millions of parameters. In contrast, we propose a much lighter graph-based method to perform node-based classification. We first use Simple Linear Iterative Cluster (SLIC) to produce superpixels across the field. Then, to perform segmentation across the non-Euclidean domain of superpixels, we leverage a Graph Convolutional Neural Network (GCN). This model has 4-orders-of-magnitude fewer parameters than a CNN model and trains in a matter of minutes.



### Revisiting The Evaluation of Class Activation Mapping for Explainability: A Novel Metric and Experimental Analysis
- **Arxiv ID**: http://arxiv.org/abs/2104.10252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10252v1)
- **Published**: 2021-04-20 21:34:24+00:00
- **Updated**: 2021-04-20 21:34:24+00:00
- **Authors**: Samuele Poppi, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
- **Comment**: CVPR 2021 Workshop on Responsible Computer Vision
- **Journal**: None
- **Summary**: As the request for deep learning solutions increases, the need for explainability is even more fundamental. In this setting, particular attention has been given to visualization techniques, that try to attribute the right relevance to each input pixel with respect to the output of the network. In this paper, we focus on Class Activation Mapping (CAM) approaches, which provide an effective visualization by taking weighted averages of the activation maps. To enhance the evaluation and the reproducibility of such approaches, we propose a novel set of metrics to quantify explanation maps, which show better effectiveness and simplify comparisons between approaches. To evaluate the appropriateness of the proposal, we compare different CAM-based visualization methods on the entire ImageNet validation set, fostering proper comparisons and reproducibility.



### TWIST-GAN: Towards Wavelet Transform and Transferred GAN for Spatio-Temporal Single Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2104.10268v1
- **DOI**: 10.1145/3456726
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.10268v1)
- **Published**: 2021-04-20 22:12:38+00:00
- **Updated**: 2021-04-20 22:12:38+00:00
- **Authors**: Fayaz Ali Dharejo, Farah Deeba, Yuanchun Zhou, Bhagwan Das, Munsif Ali Jatoi, Muhammad Zawish, Yi Du, Xuezhi Wang
- **Comment**: Accepted: ACM TIST (10-03-2021)
- **Journal**: 20 December 2021
- **Summary**: Single Image Super-resolution (SISR) produces high-resolution images with fine spatial resolutions from aremotely sensed image with low spatial resolution. Recently, deep learning and generative adversarial networks(GANs) have made breakthroughs for the challenging task of single image super-resolution (SISR). However, thegenerated image still suffers from undesirable artifacts such as, the absence of texture-feature representationand high-frequency information. We propose a frequency domain-based spatio-temporal remote sensingsingle image super-resolution technique to reconstruct the HR image combined with generative adversarialnetworks (GANs) on various frequency bands (TWIST-GAN). We have introduced a new method incorporatingWavelet Transform (WT) characteristics and transferred generative adversarial network. The LR image hasbeen split into various frequency bands by using the WT, whereas, the transfer generative adversarial networkpredicts high-frequency components via a proposed architecture. Finally, the inverse transfer of waveletsproduces a reconstructed image with super-resolution. The model is first trained on an external DIV2 Kdataset and validated with the UC Merceed Landsat remote sensing dataset and Set14 with each image sizeof 256x256. Following that, transferred GANs are used to process spatio-temporal remote sensing images inorder to minimize computation cost differences and improve texture information. The findings are comparedqualitatively and qualitatively with the current state-of-art approaches. In addition, we saved about 43% of theGPU memory during training and accelerated the execution of our simplified version by eliminating batchnormalization layers.



### Disentangled Face Identity Representations for joint 3D Face Recognition and Expression Neutralisation
- **Arxiv ID**: http://arxiv.org/abs/2104.10273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10273v1)
- **Published**: 2021-04-20 22:33:10+00:00
- **Updated**: 2021-04-20 22:33:10+00:00
- **Authors**: Anis Kacem, Kseniya Cherenkova, Djamila Aouada
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new deep learning-based approach for disentangling face identity representations from expressive 3D faces. Given a 3D face, our approach not only extracts a disentangled identity representation but also generates a realistic 3D face with a neutral expression while predicting its identity. The proposed network consists of three components; (1) a Graph Convolutional Autoencoder (GCA) to encode the 3D faces into latent representations, (2) a Generative Adversarial Network (GAN) that translates the latent representations of expressive faces into those of neutral faces, (3) and an identity recognition sub-network taking advantage of the neutralized latent representations for 3D face recognition. The whole network is trained in an end-to-end manner. Experiments are conducted on three publicly available datasets showing the effectiveness of the proposed approach.



### Compact and Effective Representations for Sketch-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2104.10278v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.10278v1)
- **Published**: 2021-04-20 22:48:19+00:00
- **Updated**: 2021-04-20 22:48:19+00:00
- **Authors**: Pablo Torres, Jose M. Saavedra
- **Comment**: None
- **Journal**: None
- **Summary**: Sketch-based image retrieval (SBIR) has undergone an increasing interest in the community of computer vision bringing high impact in real applications. For instance, SBIR brings an increased benefit to eCommerce search engines because it allows users to formulate a query just by drawing what they need to buy. However, current methods showing high precision in retrieval work in a high dimensional space, which negatively affects aspects like memory consumption and time processing. Although some authors have also proposed compact representations, these drastically degrade the performance in a low dimension. Therefore in this work, we present different results of evaluating methods for producing compact embeddings in the context of sketch-based image retrieval. Our main interest is in strategies aiming to keep the local structure of the original space. The recent unsupervised local-topology preserving dimension reduction method UMAP fits our requirements and shows outstanding performance, improving even the precision achieved by SOTA methods. We evaluate six methods in two different datasets. We use Flickr15K and eCommerce datasets; the latter is another contribution of this work. We show that UMAP allows us to have feature vectors of 16 bytes improving precision by more than 35%.



### GraghVQA: Language-Guided Graph Neural Networks for Graph-based Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2104.10283v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.10283v2)
- **Published**: 2021-04-20 23:54:41+00:00
- **Updated**: 2021-06-02 05:29:00+00:00
- **Authors**: Weixin Liang, Yanhao Jiang, Zixuan Liu
- **Comment**: NAACL 2021 MAI-Workshop. Code available at
  https://github.com/codexxxl/GraphVQA
- **Journal**: None
- **Summary**: Images are more than a collection of objects or attributes -- they represent a web of relationships among interconnected objects. Scene Graph has emerged as a new modality for a structured graphical representation of images. Scene Graph encodes objects as nodes connected via pairwise relations as edges. To support question answering on scene graphs, we propose GraphVQA, a language-guided graph neural network framework that translates and executes a natural language question as multiple iterations of message passing among graph nodes. We explore the design space of GraphVQA framework, and discuss the trade-off of different design choices. Our experiments on GQA dataset show that GraphVQA outperforms the state-of-the-art model by a large margin (88.43% vs. 94.78%).



