# Arxiv Papers in cs.CV on 2021-04-19
### Contrastive Learning Improves Model Robustness Under Label Noise
- **Arxiv ID**: http://arxiv.org/abs/2104.08984v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.08984v1)
- **Published**: 2021-04-19 00:27:58+00:00
- **Updated**: 2021-04-19 00:27:58+00:00
- **Authors**: Aritra Ghosh, Andrew Lan
- **Comment**: Learning from Limited or Imperfect Data (L^2ID) Workshop @ CVPR 2021
- **Journal**: None
- **Summary**: Deep neural network-based classifiers trained with the categorical cross-entropy (CCE) loss are sensitive to label noise in the training data. One common type of method that can mitigate the impact of label noise can be viewed as supervised robust methods; one can simply replace the CCE loss with a loss that is robust to label noise, or re-weight training samples and down-weight those with higher loss values. Recently, another type of method using semi-supervised learning (SSL) has been proposed, which augments these supervised robust methods to exploit (possibly) noisy samples more effectively. Although supervised robust methods perform well across different data types, they have been shown to be inferior to the SSL methods on image classification tasks under label noise. Therefore, it remains to be seen that whether these supervised robust methods can also perform well if they can utilize the unlabeled samples more effectively. In this paper, we show that by initializing supervised robust methods using representations learned through contrastive learning leads to significantly improved performance under label noise. Surprisingly, even the simplest method (training a classifier with the CCE loss) can outperform the state-of-the-art SSL method by more than 50\% under high label noise when initialized with contrastive learning. Our implementation will be publicly available at {\url{https://github.com/arghosh/noisy_label_pretrain}}.



### Masked Face Recognition using ResNet-50
- **Arxiv ID**: http://arxiv.org/abs/2104.08997v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.08997v1)
- **Published**: 2021-04-19 01:09:47+00:00
- **Updated**: 2021-04-19 01:09:47+00:00
- **Authors**: Bishwas Mandal, Adaeze Okeukwu, Yihong Theis
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last twenty years, there have seen several outbreaks of different coronavirus diseases across the world. These outbreaks often led to respiratory tract diseases and have proved to be fatal sometimes. Currently, we are facing an elusive health crisis with the emergence of COVID-19 disease of the coronavirus family. One of the modes of transmission of COVID- 19 is airborne transmission. This transmission occurs as humans breathe in the droplets released by an infected person through breathing, speaking, singing, coughing, or sneezing. Hence, public health officials have mandated the use of face masks which can reduce disease transmission by 65%. For face recognition programs, commonly used for security verification purposes, the use of face mask presents an arduous challenge since these programs were typically trained with human faces devoid of masks but now due to the onset of Covid-19 pandemic, they are forced to identify faces with masks. Hence, this paper investigates the same problem by developing a deep learning based model capable of accurately identifying people with face-masks. In this paper, the authors train a ResNet-50 based architecture that performs well at recognizing masked faces. The outcome of this study could be seamlessly integrated into existing face recognition programs that are designed to detect faces for security verification purposes.



### Kernel Adversarial Learning for Real-world Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2104.09008v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09008v2)
- **Published**: 2021-04-19 01:51:21+00:00
- **Updated**: 2022-07-27 04:37:42+00:00
- **Authors**: Hu Wang, Congbo Ma, Jianpeng Zhang, Gustavo Carneiro
- **Comment**: None
- **Journal**: None
- **Summary**: Current deep image super-resolution (SR) approaches attempt to restore high-resolution images from down-sampled images or by assuming degradation from simple Gaussian kernels and additive noises. However, such simple image processing techniques represent crude approximations of the real-world procedure of lowering image resolution. In this paper, we propose a more realistic process to lower image resolution by introducing a new Kernel Adversarial Learning Super-resolution (KASR) framework to deal with the real-world image SR problem. In the proposed framework, degradation kernels and noises are adaptively modeled rather than explicitly specified. Moreover, we also propose an iterative supervision process and high-frequency selective objective to further boost the model SR reconstruction accuracy. Extensive experiments validate the effectiveness of the proposed framework on real-world datasets.



### Writing in The Air: Unconstrained Text Recognition from Finger Movement Using Spatio-Temporal Convolution
- **Arxiv ID**: http://arxiv.org/abs/2104.09021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09021v1)
- **Published**: 2021-04-19 02:37:46+00:00
- **Updated**: 2021-04-19 02:37:46+00:00
- **Authors**: Ue-Hwan Kim, Yewon Hwang, Sun-Kyung Lee, Jong-Hwan Kim
- **Comment**: 10 pages, 6 figures, 6 tables
- **Journal**: None
- **Summary**: In this paper, we introduce a new benchmark dataset for the challenging writing in the air (WiTA) task -- an elaborate task bridging vision and NLP. WiTA implements an intuitive and natural writing method with finger movement for human-computer interaction (HCI). Our WiTA dataset will facilitate the development of data-driven WiTA systems which thus far have displayed unsatisfactory performance -- due to lack of dataset as well as traditional statistical models they have adopted. Our dataset consists of five sub-datasets in two languages (Korean and English) and amounts to 209,926 video instances from 122 participants. We capture finger movement for WiTA with RGB cameras to ensure wide accessibility and cost-efficiency. Next, we propose spatio-temporal residual network architectures inspired by 3D ResNet. These models perform unconstrained text recognition from finger movement, guarantee a real-time operation by processing 435 and 697 decoding frames-per-second for Korean and English, respectively, and will serve as an evaluation standard. Our dataset and the source codes are available at https://github.com/Uehwan/WiTA.



### Unsupervised Shape Completion via Deep Prior in the Neural Tangent Kernel Perspective
- **Arxiv ID**: http://arxiv.org/abs/2104.09023v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.3.5; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2104.09023v1)
- **Published**: 2021-04-19 02:41:15+00:00
- **Updated**: 2021-04-19 02:41:15+00:00
- **Authors**: Lei Chu, Hao Pan, Wenping Wang
- **Comment**: This is the author's preprint; see the final publication at ACM TOG
- **Journal**: None
- **Summary**: We present a novel approach for completing and reconstructing 3D shapes from incomplete scanned data by using deep neural networks. Rather than being trained on supervised completion tasks and applied on a testing shape, the network is optimized from scratch on the single testing shape, to fully adapt to the shape and complete the missing data using contextual guidance from the known regions. The ability to complete missing data by an untrained neural network is usually referred to as the deep prior. In this paper, we interpret the deep prior from a neural tangent kernel (NTK) perspective and show that the completed shape patches by the trained CNN are naturally similar to existing patches, as they are proximate in the kernel feature space induced by NTK. The interpretation allows us to design more efficient network structures and learning mechanisms for the shape completion and reconstruction task. Being more aware of structural regularities than both traditional and other unsupervised learning-based reconstruction methods, our approach completes large missing regions with plausible shapes and complements supervised learning-based methods that use database priors by requiring no extra training data set and showing flexible adaptation to a particular shape instance.



### Lidar Point Cloud Guided Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.09035v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09035v3)
- **Published**: 2021-04-19 03:41:09+00:00
- **Updated**: 2022-07-25 03:11:46+00:00
- **Authors**: Liang Peng, Fei Liu, Zhengxu Yu, Senbo Yan, Dan Deng, Zheng Yang, Haifeng Liu, Deng Cai
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Monocular 3D object detection is a challenging task in the self-driving and computer vision community. As a common practice, most previous works use manually annotated 3D box labels, where the annotating process is expensive. In this paper, we find that the precisely and carefully annotated labels may be unnecessary in monocular 3D detection, which is an interesting and counterintuitive finding. Using rough labels that are randomly disturbed, the detector can achieve very close accuracy compared to the one using the ground-truth labels. We delve into this underlying mechanism and then empirically find that: concerning the label accuracy, the 3D location part in the label is preferred compared to other parts of labels. Motivated by the conclusions above and considering the precise LiDAR 3D measurement, we propose a simple and effective framework, dubbed LiDAR point cloud guided monocular 3D object detection (LPCG). This framework is capable of either reducing the annotation costs or considerably boosting the detection accuracy without introducing extra annotation costs. Specifically, It generates pseudo labels from unlabeled LiDAR point clouds. Thanks to accurate LiDAR 3D measurements in 3D space, such pseudo labels can replace manually annotated labels in the training of monocular 3D detectors, since their 3D location information is precise. LPCG can be applied into any monocular 3D detector to fully use massive unlabeled data in a self-driving system. As a result, in KITTI benchmark, we take the first place on both monocular 3D and BEV (bird's-eye-view) detection with a significant margin. In Waymo benchmark, our method using 10% labeled data achieves comparable accuracy to the baseline detector using 100% labeled data. The codes are released at https://github.com/SPengLiang/LPCG.



### Distilling Knowledge via Knowledge Review
- **Arxiv ID**: http://arxiv.org/abs/2104.09044v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09044v1)
- **Published**: 2021-04-19 04:36:24+00:00
- **Updated**: 2021-04-19 04:36:24+00:00
- **Authors**: Pengguang Chen, Shu Liu, Hengshuang Zhao, Jiaya Jia
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Knowledge distillation transfers knowledge from the teacher network to the student one, with the goal of greatly improving the performance of the student network. Previous methods mostly focus on proposing feature transformation and loss functions between the same level's features to improve the effectiveness. We differently study the factor of connection path cross levels between teacher and student networks, and reveal its great importance. For the first time in knowledge distillation, cross-stage connection paths are proposed. Our new review mechanism is effective and structurally simple. Our finally designed nested and compact framework requires negligible computation overhead, and outperforms other methods on a variety of tasks. We apply our method to classification, object detection, and instance segmentation tasks. All of them witness significant student network performance improvement. Code is available at https://github.com/Jia-Research-Lab/ReviewKD



### Do We Really Need Gold Samples for Sample Weighting Under Label Noise?
- **Arxiv ID**: http://arxiv.org/abs/2104.09045v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.09045v1)
- **Published**: 2021-04-19 04:36:51+00:00
- **Updated**: 2021-04-19 04:36:51+00:00
- **Authors**: Aritra Ghosh, Andrew Lan
- **Comment**: WACV 2021
- **Journal**: None
- **Summary**: Learning with labels noise has gained significant traction recently due to the sensitivity of deep neural networks under label noise under common loss functions. Losses that are theoretically robust to label noise, however, often makes training difficult. Consequently, several recently proposed methods, such as Meta-Weight-Net (MW-Net), use a small number of unbiased, clean samples to learn a weighting function that downweights samples that are likely to have corrupted labels under the meta-learning framework. However, obtaining such a set of clean samples is not always feasible in practice. In this paper, we analytically show that one can easily train MW-Net without access to clean samples simply by using a loss function that is robust to label noise, such as mean absolute error, as the meta objective to train the weighting network. We experimentally show that our method beats all existing methods that do not use clean samples and performs on-par with methods that use gold samples on benchmark datasets across various noise types and noise rates.



### Neural Architecture Search for Image Super-Resolution Using Densely Constructed Search Space: DeCoNAS
- **Arxiv ID**: http://arxiv.org/abs/2104.09048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09048v1)
- **Published**: 2021-04-19 04:51:16+00:00
- **Updated**: 2021-04-19 04:51:16+00:00
- **Authors**: Joon Young Ahn, Nam Ik Cho
- **Comment**: None
- **Journal**: None
- **Summary**: The recent progress of deep convolutional neural networks has enabled great success in single image super-resolution (SISR) and many other vision tasks. Their performances are also being increased by deepening the networks and developing more sophisticated network structures. However, finding an optimal structure for the given problem is a difficult task, even for human experts. For this reason, neural architecture search (NAS) methods have been introduced, which automate the procedure of constructing the structures. In this paper, we expand the NAS to the super-resolution domain and find a lightweight densely connected network named DeCoNASNet. We use a hierarchical search strategy to find the best connection with local and global features. In this process, we define a complexity-based penalty for solving image super-resolution, which can be considered a multi-objective problem. Experiments show that our DeCoNASNet outperforms the state-of-the-art lightweight super-resolution networks designed by handcraft methods and existing NAS-based design.



### A Competitive Method to VIPriors Object Detection Challenge
- **Arxiv ID**: http://arxiv.org/abs/2104.09059v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.09059v1)
- **Published**: 2021-04-19 05:33:39+00:00
- **Updated**: 2021-04-19 05:33:39+00:00
- **Authors**: Fei Shen, Xin He, Mengwan Wei, Yi Xie
- **Comment**: None
- **Journal**: None
- **Summary**: In this report, we introduce the technical details of our submission to the VIPriors object detection challenge. Our solution is based on mmdetction of a strong baseline open-source detection toolbox. Firstly, we introduce an effective data augmentation method to address the lack of data problem, which contains bbox-jitter, grid-mask, and mix-up. Secondly, we present a robust region of interest (ROI) extraction method to learn more significant ROI features via embedding global context features. Thirdly, we propose a multi-model integration strategy to refinement the prediction box, which weighted boxes fusion (WBF). Experimental results demonstrate that our approach can significantly improve the average precision (AP) of object detection on the subset of the COCO2017 dataset.



### Surrogate Gradient Field for Latent Space Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2104.09065v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09065v2)
- **Published**: 2021-04-19 06:15:06+00:00
- **Updated**: 2021-04-20 15:55:27+00:00
- **Authors**: Minjun Li, Yanghua Jin, Huachun Zhu
- **Comment**: 19 pages, 18 figures, CVPR 2021
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) can generate high-quality images from sampled latent codes. Recent works attempt to edit an image by manipulating its underlying latent code, but rarely go beyond the basic task of attribute adjustment. We propose the first method that enables manipulation with multidimensional condition such as keypoints and captions. Specifically, we design an algorithm that searches for a new latent code that satisfies the target condition based on the Surrogate Gradient Field (SGF) induced by an auxiliary mapping network. For quantitative comparison, we propose a metric to evaluate the disentanglement of manipulation methods. Thorough experimental analysis on the facial attribute adjustment task shows that our method outperforms state-of-the-art methods in disentanglement. We further apply our method to tasks of various condition modalities to demonstrate that our method can alter complex image properties such as keypoints and captions.



### Image Inpainting with External-internal Learning and Monochromic Bottleneck
- **Arxiv ID**: http://arxiv.org/abs/2104.09068v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09068v1)
- **Published**: 2021-04-19 06:22:10+00:00
- **Updated**: 2021-04-19 06:22:10+00:00
- **Authors**: Tengfei Wang, Hao Ouyang, Qifeng Chen
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Although recent inpainting approaches have demonstrated significant improvements with deep neural networks, they still suffer from artifacts such as blunt structures and abrupt colors when filling in the missing regions. To address these issues, we propose an external-internal inpainting scheme with a monochromic bottleneck that helps image inpainting models remove these artifacts. In the external learning stage, we reconstruct missing structures and details in the monochromic space to reduce the learning dimension. In the internal learning stage, we propose a novel internal color propagation method with progressive learning strategies for consistent color restoration. Extensive experiments demonstrate that our proposed scheme helps image inpainting models produce more structure-preserved and visually compelling results.



### TransCrowd: weakly-supervised crowd counting with transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.09116v3
- **DOI**: 10.1007/s11432-021-3445-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09116v3)
- **Published**: 2021-04-19 08:12:50+00:00
- **Updated**: 2022-09-08 07:08:18+00:00
- **Authors**: Dingkang Liang, Xiwu Chen, Wei Xu, Yu Zhou, Xiang Bai
- **Comment**: Accepted by Science China Information Sciences (SCIS). Code is
  available at https://github.com/dk-liang/TransCrowd
- **Journal**: None
- **Summary**: The mainstream crowd counting methods usually utilize the convolution neural network (CNN) to regress a density map, requiring point-level annotations. However, annotating each person with a point is an expensive and laborious process. During the testing phase, the point-level annotations are not considered to evaluate the counting accuracy, which means the point-level annotations are redundant. Hence, it is desirable to develop weakly-supervised counting methods that just rely on count-level annotations, a more economical way of labeling. Current weakly-supervised counting methods adopt the CNN to regress a total count of the crowd by an image-to-count paradigm. However, having limited receptive fields for context modeling is an intrinsic limitation of these weakly-supervised CNN-based methods. These methods thus cannot achieve satisfactory performance, with limited applications in the real world. The transformer is a popular sequence-to-sequence prediction model in natural language processing (NLP), which contains a global receptive field. In this paper, we propose TransCrowd, which reformulates the weakly-supervised crowd counting problem from the perspective of sequence-to-count based on transformers. We observe that the proposed TransCrowd can effectively extract the semantic crowd information by using the self-attention mechanism of transformer. To the best of our knowledge, this is the first work to adopt a pure transformer for crowd counting research. Experiments on five benchmark datasets demonstrate that the proposed TransCrowd achieves superior performance compared with all the weakly-supervised CNN-based counting methods and gains highly competitive counting performance compared with some popular fully-supervised counting methods.



### TetraPackNet: Four-Corner-Based Object Detection in Logistics Use-Cases
- **Arxiv ID**: http://arxiv.org/abs/2104.09123v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.09123v2)
- **Published**: 2021-04-19 08:22:14+00:00
- **Updated**: 2021-06-23 08:52:49+00:00
- **Authors**: Laura Dörr, Felix Brandt, Alexander Naumann, Martin Pouls
- **Comment**: None
- **Journal**: None
- **Summary**: While common image object detection tasks focus on bounding boxes or segmentation masks as object representations, we consider the problem of finding objects based on four arbitrary vertices. We propose a novel model, named TetraPackNet, to tackle this problem. TetraPackNet is based on CornerNet and uses similar algorithms and ideas. It is designated for applications requiring high-accuracy detection of regularly shaped objects, which is the case in the logistics use-case of packaging structure recognition. We evaluate our model on our specific real-world dataset for this use-case. Baselined against a previous solution, consisting of a Mask R-CNN model and suitable post-processing steps, TetraPackNet achieves superior results (9% higher in accuracy) in the sub-task of four-corner based transport unit side detection.



### DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.09124v7
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.09124v7)
- **Published**: 2021-04-19 08:22:52+00:00
- **Updated**: 2022-07-04 14:03:59+00:00
- **Authors**: Yuting Gao, Jia-Xin Zhuang, Shaohui Lin, Hao Cheng, Xing Sun, Ke Li, Chunhua Shen
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: While self-supervised representation learning (SSL) has received widespread attention from the community, recent research argue that its performance will suffer a cliff fall when the model size decreases. The current method mainly relies on contrastive learning to train the network and in this work, we propose a simple yet effective Distilled Contrastive Learning (DisCo) to ease the issue by a large margin. Specifically, we find the final embedding obtained by the mainstream SSL methods contains the most fruitful information, and propose to distill the final embedding to maximally transmit a teacher's knowledge to a lightweight model by constraining the last embedding of the student to be consistent with that of the teacher. In addition, in the experiment, we find that there exists a phenomenon termed Distilling BottleNeck and present to enlarge the embedding dimension to alleviate this problem. Our method does not introduce any extra parameter to lightweight models during deployment. Experimental results demonstrate that our method achieves the state-of-the-art on all lightweight models. Particularly, when ResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear result of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50, but the number of parameters of EfficientNet-B0 is only 9.4\%/16.3\% of ResNet-101/ResNet-50. Code is available at https://github. com/Yuting-Gao/DisCo-pytorch.



### SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization
- **Arxiv ID**: http://arxiv.org/abs/2104.09125v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2104.09125v2)
- **Published**: 2021-04-19 08:22:55+00:00
- **Updated**: 2021-05-28 15:46:32+00:00
- **Authors**: Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung, Daniel Cohen-Or
- **Comment**: None
- **Journal**: None
- **Summary**: Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.



### RANSIC: Fast and Highly Robust Estimation for Rotation Search and Point Cloud Registration using Invariant Compatibility
- **Arxiv ID**: http://arxiv.org/abs/2104.09133v3
- **DOI**: 10.1109/LRA.2021.3116313
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.09133v3)
- **Published**: 2021-04-19 08:29:34+00:00
- **Updated**: 2021-04-21 07:46:20+00:00
- **Authors**: Lei Sun
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters ( Volume: 7, Issue: 1, Jan.
  2022)
- **Summary**: Correspondence-based rotation search and point cloud registration are two fundamental problems in robotics and computer vision. However, the presence of outliers, sometimes even occupying the great majority of the putative correspondences, can make many existing algorithms either fail or have very high computational cost. In this paper, we present RANSIC (RANdom Sampling with Invariant Compatibility), a fast and highly robust method applicable to both problems based on a new paradigm combining random sampling with invariance and compatibility. Generally, RANSIC starts with randomly selecting small subsets from the correspondence set, then seeks potential inliers as graph vertices from the random subsets through the compatibility tests of invariants established in each problem, and eventually returns the eligible inliers when there exists at least one K-degree vertex (K is automatically updated depending on the problem) and the residual errors satisfy a certain termination condition at the same time. In multiple synthetic and real experiments, we demonstrate that RANSIC is fast for use, robust against over 95% outliers, and also able to recall approximately 100% inliers, outperforming other state-of-the-art solvers for both the rotation search and the point cloud registration problems.



### Restoration of Video Frames from a Single Blurred Image with Motion Understanding
- **Arxiv ID**: http://arxiv.org/abs/2104.09134v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09134v1)
- **Published**: 2021-04-19 08:32:57+00:00
- **Updated**: 2021-04-19 08:32:57+00:00
- **Authors**: Dawit Mureja Argaw, Junsik Kim, Francois Rameau, Chaoning Zhang, In So Kweon
- **Comment**: Accepted to CVPRW, NTIRE 2021
- **Journal**: None
- **Summary**: We propose a novel framework to generate clean video frames from a single motion-blurred image. While a broad range of literature focuses on recovering a single image from a blurred image, in this work, we tackle a more challenging task i.e. video restoration from a blurred image. We formulate video restoration from a single blurred image as an inverse problem by setting clean image sequence and their respective motion as latent factors, and the blurred image as an observation. Our framework is based on an encoder-decoder structure with spatial transformer network modules to restore a video sequence and its underlying motion in an end-to-end manner. We design a loss function and regularizers with complementary properties to stabilize the training and analyze variant models of the proposed network. The effectiveness and transferability of our network are highlighted through a large set of experiments on two different types of datasets: camera rotation blurs generated from panorama scenes and dynamic motion blurs in high speed videos.



### ECACL: A Holistic Framework for Semi-Supervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2104.09136v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09136v2)
- **Published**: 2021-04-19 08:46:08+00:00
- **Updated**: 2021-08-21 01:51:55+00:00
- **Authors**: Kai Li, Chang Liu, Handong Zhao, Yulun Zhang, Yun Fu
- **Comment**: To appear in ICCV 2021
- **Journal**: None
- **Summary**: This paper studies Semi-Supervised Domain Adaptation (SSDA), a practical yet under-investigated research topic that aims to learn a model of good performance using unlabeled samples and a few labeled samples in the target domain, with the help of labeled samples from a source domain. Several SSDA methods have been proposed recently, which however fail to fully exploit the value of the few labeled target samples. In this paper, we propose Enhanced Categorical Alignment and Consistency Learning (ECACL), a holistic SSDA framework that incorporates multiple mutually complementary domain alignment techniques. ECACL includes two categorical domain alignment techniques that achieve class-level alignment, a strong data augmentation based technique that enhances the model's generalizability and a consistency learning based technique that forces the model to be robust with image perturbations. These techniques are applied on one or multiple of the three inputs (labeled source, unlabeled target, and labeled target) and align the domains from different perspectives. ECACL unifies them together and achieves fairly comprehensive domain alignments that are much better than the existing methods: For example, ECACL raises the state-of-the-art accuracy from 68.4 to 81.1 on VisDA2017 and from 45.5 to 53.4 on DomainNet for the 1-shot setting. Our code is available at \url{https://github.com/kailigo/pacl}.



### Face-GCN: A Graph Convolutional Network for 3D Dynamic Face Identification/Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.09145v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09145v2)
- **Published**: 2021-04-19 09:05:39+00:00
- **Updated**: 2021-04-20 08:36:36+00:00
- **Authors**: Konstantinos Papadopoulos, Anis Kacem, Abdelrahman Shabayek, Djamila Aouada
- **Comment**: None
- **Journal**: None
- **Summary**: Face identification/recognition has significantly advanced over the past years. However, most of the proposed approaches rely on static RGB frames and on neutral facial expressions. This has two disadvantages. First, important facial shape cues are ignored. Second, facial deformations due to expressions can have an impact on the performance of such a method. In this paper, we propose a novel framework for dynamic 3D face identification/recognition based on facial keypoints. Each dynamic sequence of facial expressions is represented as a spatio-temporal graph, which is constructed using 3D facial landmarks. Each graph node contains local shape and texture features that are extracted from its neighborhood. For the classification/identification of faces, a Spatio-temporal Graph Convolutional Network (ST-GCN) is used. Finally, we evaluate our approach on a challenging dynamic 3D facial expression dataset.



### Self-Paced Uncertainty Estimation for One-shot Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2104.09152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09152v1)
- **Published**: 2021-04-19 09:20:30+00:00
- **Updated**: 2021-04-19 09:20:30+00:00
- **Authors**: Yulin Zhang, Bo Ma, Longyao Liu, Xin Yi
- **Comment**: None
- **Journal**: None
- **Summary**: The one-shot Person Re-ID scenario faces two kinds of uncertainties when constructing the prediction model from $X$ to $Y$. The first is model uncertainty, which captures the noise of the parameters in DNNs due to a lack of training data. The second is data uncertainty, which can be divided into two sub-types: one is image noise, where severe occlusion and the complex background contain irrelevant information about the identity; the other is label noise, where mislabeled affects visual appearance learning. In this paper, to tackle these issues, we propose a novel Self-Paced Uncertainty Estimation Network (SPUE-Net) for one-shot Person Re-ID. By introducing a self-paced sampling strategy, our method can estimate the pseudo-labels of unlabeled samples iteratively to expand the labeled samples gradually and remove model uncertainty without extra supervision. We divide the pseudo-label samples into two subsets to make the use of training samples more reasonable and effective. In addition, we apply a Co-operative learning method of local uncertainty estimation combined with determinacy estimation to achieve better hidden space feature mining and to improve the precision of selected pseudo-labeled samples, which reduces data uncertainty. Extensive comparative evaluation experiments on video-based and image-based datasets show that SPUE-Net has significant advantages over the state-of-the-art methods.



### Conditional Variational Capsule Network for Open Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.09159v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.09159v2)
- **Published**: 2021-04-19 09:39:30+00:00
- **Updated**: 2021-08-17 09:09:30+00:00
- **Authors**: Yunrui Guo, Guglielmo Camporese, Wenjing Yang, Alessandro Sperduti, Lamberto Ballan
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: In open set recognition, a classifier has to detect unknown classes that are not known at training time. In order to recognize new categories, the classifier has to project the input samples of known classes in very compact and separated regions of the features space for discriminating samples of unknown classes. Recently proposed Capsule Networks have shown to outperform alternatives in many fields, particularly in image recognition, however they have not been fully applied yet to open-set recognition. In capsule networks, scalar neurons are replaced by capsule vectors or matrices, whose entries represent different properties of objects. In our proposal, during training, capsules features of the same known class are encouraged to match a pre-defined gaussian, one for each class. To this end, we use the variational autoencoder framework, with a set of gaussian priors as the approximation for the posterior distribution. In this way, we are able to control the compactness of the features of the same class around the center of the gaussians, thus controlling the ability of the classifier in detecting samples from unknown classes. We conducted several experiments and ablation of our model, obtaining state of the art results on different datasets in the open set recognition and unknown detection tasks.



### LaLaLoc: Latent Layout Localisation in Dynamic, Unvisited Environments
- **Arxiv ID**: http://arxiv.org/abs/2104.09169v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09169v2)
- **Published**: 2021-04-19 09:49:13+00:00
- **Updated**: 2021-10-12 13:08:03+00:00
- **Authors**: Henry Howard-Jenkins, Jose-Raul Ruiz-Sarmiento, Victor Adrian Prisacariu
- **Comment**: As presented at the International Conference on Computer Vision
  (ICCV) 2021
- **Journal**: None
- **Summary**: We present LaLaLoc to localise in environments without the need for prior visitation, and in a manner that is robust to large changes in scene appearance, such as a full rearrangement of furniture. Specifically, LaLaLoc performs localisation through latent representations of room layout. LaLaLoc learns a rich embedding space shared between RGB panoramas and layouts inferred from a known floor plan that encodes the structural similarity between locations. Further, LaLaLoc introduces direct, cross-modal pose optimisation in its latent space. Thus, LaLaLoc enables fine-grained pose estimation in a scene without the need for prior visitation, as well as being robust to dynamics, such as a change in furniture configuration. We show that in a domestic environment LaLaLoc is able to accurately localise a single RGB panorama image to within 8.3cm, given only a floor plan as a prior.



### Cyclist Intention Detection: A Probabilistic Approach
- **Arxiv ID**: http://arxiv.org/abs/2104.09176v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.09176v1)
- **Published**: 2021-04-19 09:59:04+00:00
- **Updated**: 2021-04-19 09:59:04+00:00
- **Authors**: Stefan Zernetsch, Hannes Reichert, Viktor Kress, Konrad Doll, Bernhard Sick
- **Comment**: None
- **Journal**: None
- **Summary**: This article presents a holistic approach for probabilistic cyclist intention detection. A basic movement detection based on motion history images (MHI) and a residual convolutional neural network (ResNet) are used to estimate probabilities for the current cyclist motion state. These probabilities are used as weights in a probabilistic ensemble trajectory forecast. The ensemble consists of specialized models, which produce individual forecasts in the form of Gaussian distributions under the assumption of a certain motion state of the cyclist (e.g. cyclist is starting or turning left). By weighting the specialized models, we create forecasts in the from of Gaussian mixtures that define regions within which the cyclists will reside with a certain probability. To evaluate our method, we rate the reliability, sharpness, and positional accuracy of our forecasted distributions. We compare our method to a single model approach which produces forecasts in the form of Gaussian distributions and show that our method is able to produce more reliable and sharper outputs while retaining comparable positional accuracy. Both methods are evaluated using a dataset created at a public traffic intersection. Our code and the dataset are made publicly available.



### Compact CNN Structure Learning by Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2104.09191v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09191v1)
- **Published**: 2021-04-19 10:34:22+00:00
- **Updated**: 2021-04-19 10:34:22+00:00
- **Authors**: Waqar Ahmed, Andrea Zunino, Pietro Morerio, Vittorio Murino
- **Comment**: This paper has been accepted to ICPR 2020
- **Journal**: None
- **Summary**: The concept of compressing deep Convolutional Neural Networks (CNNs) is essential to use limited computation, power, and memory resources on embedded devices. However, existing methods achieve this objective at the cost of a drop in inference accuracy in computer vision tasks. To address such a drawback, we propose a framework that leverages knowledge distillation along with customizable block-wise optimization to learn a lightweight CNN structure while preserving better control over the compression-performance tradeoff. Considering specific resource constraints, e.g., floating-point operations per inference (FLOPs) or model-parameters, our method results in a state of the art network compression while being capable of achieving better inference accuracy. In a comprehensive evaluation, we demonstrate that our method is effective, robust, and consistent with results over a variety of network architectures and datasets, at negligible training overhead. In particular, for the already compact network MobileNet_v2, our method offers up to 2x and 5.2x better model compression in terms of FLOPs and model-parameters, respectively, while getting 1.05% better model performance than the baseline network.



### SCNet: Enhancing Few-Shot Semantic Segmentation by Self-Contrastive Background Prototypes
- **Arxiv ID**: http://arxiv.org/abs/2104.09216v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.09216v3)
- **Published**: 2021-04-19 11:21:47+00:00
- **Updated**: 2021-04-28 04:29:59+00:00
- **Authors**: Jiacheng Chen, Bin-Bin Gao, Zongqing Lu, Jing-Hao Xue, Chengjie Wang, Qingmin Liao
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Few-shot semantic segmentation aims to segment novel-class objects in a query image with only a few annotated examples in support images. Most of advanced solutions exploit a metric learning framework that performs segmentation through matching each pixel to a learned foreground prototype. However, this framework suffers from biased classification due to incomplete construction of sample pairs with the foreground prototype only. To address this issue, in this paper, we introduce a complementary self-contrastive task into few-shot semantic segmentation. Our new model is able to associate the pixels in a region with the prototype of this region, no matter they are in the foreground or background. To this end, we generate self-contrastive background prototypes directly from the query image, with which we enable the construction of complete sample pairs and thus a complementary and auxiliary segmentation task to achieve the training of a better segmentation model. Extensive experiments on PASCAL-5$^i$ and COCO-20$^i$ demonstrate clearly the superiority of our proposal. At no expense of inference efficiency, our model achieves state-of-the results in both 1-shot and 5-shot settings for few-shot semantic segmentation.



### Coarse-to-Fine Searching for Efficient Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.09223v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09223v1)
- **Published**: 2021-04-19 11:46:20+00:00
- **Updated**: 2021-04-19 11:46:20+00:00
- **Authors**: Jiahao Wang, Han Shu, Weihao Xia, Yujiu Yang, Yunhe Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies the neural architecture search (NAS) problem for developing efficient generator networks. Compared with deep models for visual recognition tasks, generative adversarial network (GAN) are usually designed to conduct various complex image generation. We first discover an intact search space of generator networks including three dimensionalities, i.e., path, operator, channel for fully excavating the network performance. To reduce the huge search cost, we explore a coarse-to-fine search strategy which divides the overall search process into three sub-optimization problems accordingly. In addition, a fair supernet training approach is utilized to ensure that all sub-networks can be updated fairly and stably. Experiments results on benchmarks show that we can provide generator networks with better image quality and lower computational costs over the state-of-the-art methods. For example, with our method, it takes only about 8 GPU hours on the entire edges-to-shoes dataset to get a 2.56 MB model with a 24.13 FID score and 10 GPU hours on the entire Urban100 dataset to get a 1.49 MB model with a 24.94 PSNR score.



### Multi-Modal Fusion Transformer for End-to-End Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2104.09224v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.09224v1)
- **Published**: 2021-04-19 11:48:13+00:00
- **Updated**: 2021-04-19 11:48:13+00:00
- **Authors**: Aditya Prakash, Kashyap Chitta, Andreas Geiger
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: How should representations from complementary sensors be integrated for autonomous driving? Geometry-based sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in traffic light state can affect the behavior of a vehicle geometrically distant from that traffic light. Geometry alone may therefore be insufficient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling traffic oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efficacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 76% compared to geometry-based fusion.



### Image Modeling with Deep Convolutional Gaussian Mixture Models
- **Arxiv ID**: http://arxiv.org/abs/2104.12686v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.12686v1)
- **Published**: 2021-04-19 12:08:53+00:00
- **Updated**: 2021-04-19 12:08:53+00:00
- **Authors**: Alexander Gepperth, Benedikt Pfülb
- **Comment**: accepted at IJCNN2021, 9 pages, 7 figures
- **Journal**: None
- **Summary**: In this conceptual work, we present Deep Convolutional Gaussian Mixture Models (DCGMMs): a new formulation of deep hierarchical Gaussian Mixture Models (GMMs) that is particularly suitable for describing and generating images. Vanilla (i.e., flat) GMMs require a very large number of components to describe images well, leading to long training times and memory issues. DCGMMs avoid this by a stacked architecture of multiple GMM layers, linked by convolution and pooling operations. This allows to exploit the compositionality of images in a similar way as deep CNNs do. DCGMMs can be trained end-to-end by Stochastic Gradient Descent. This sets them apart from vanilla GMMs which are trained by Expectation-Maximization, requiring a prior k-means initialization which is infeasible in a layered structure. For generating sharp images with DCGMMs, we introduce a new gradient-based technique for sampling through non-invertible operations like convolution and pooling. Based on the MNIST and FashionMNIST datasets, we validate the DCGMMs model by demonstrating its superiority over flat GMMs for clustering, sampling and outlier detection.



### LSPnet: A 2D Localization-oriented Spacecraft Pose Estimation Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2104.09248v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.09248v2)
- **Published**: 2021-04-19 12:46:05+00:00
- **Updated**: 2021-08-23 09:14:04+00:00
- **Authors**: Albert Garcia, Mohamed Adel Musallam, Vincent Gaudilliere, Enjie Ghorbel, Kassem Al Ismaeil, Marcos Perez, Djamila Aouada
- **Comment**: 9 pages, 5 figures, published at AI4Space 2021 IEEE/CVF Conference on
  Computer Vision and Pattern Recognition Workshops (CVPRW)
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops (2021) p. 2048-2056
- **Summary**: Being capable of estimating the pose of uncooperative objects in space has been proposed as a key asset for enabling safe close-proximity operations such as space rendezvous, in-orbit servicing and active debris removal. Usual approaches for pose estimation involve classical computer vision-based solutions or the application of Deep Learning (DL) techniques. This work explores a novel DL-based methodology, using Convolutional Neural Networks (CNNs), for estimating the pose of uncooperative spacecrafts. Contrary to other approaches, the proposed CNN directly regresses poses without needing any prior 3D information. Moreover, bounding boxes of the spacecraft in the image are predicted in a simple, yet efficient manner. The performed experiments show how this work competes with the state-of-the-art in uncooperative spacecraft pose estimation, including works which require 3D information as well as works which predict bounding boxes through sophisticated CNNs.



### Plants Don't Walk on the Street: Common-Sense Reasoning for Reliable Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.09254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09254v1)
- **Published**: 2021-04-19 12:51:06+00:00
- **Updated**: 2021-04-19 12:51:06+00:00
- **Authors**: Linara Adilova, Elena Schulz, Maram Akila, Sebastian Houben, Jan David Schneider, Fabian Hueger, Tim Wirtz
- **Comment**: Published at SAIAD (Safe Artificial Intelligence for Automated
  Driving) workshop at CVPR2021
- **Journal**: None
- **Summary**: Data-driven sensor interpretation in autonomous driving can lead to highly implausible predictions as can most of the time be verified with common-sense knowledge. However, learning common knowledge only from data is hard and approaches for knowledge integration are an active research area. We propose to use a partly human-designed, partly learned set of rules to describe relations between objects of a traffic scene on a high level of abstraction. In doing so, we improve and robustify existing deep neural networks consuming low-level sensor information. We present an initial study adapting the well-established Probabilistic Soft Logic (PSL) framework to validate and improve on the problem of semantic segmentation. We describe in detail how we integrate common knowledge into the segmentation pipeline using PSL and verify our approach in a set of experiments demonstrating the increase in robustness against several severe image distortions applied to the A2D2 autonomous driving data set.



### Temporal Consistency Loss for High Resolution Textured and Clothed 3DHuman Reconstruction from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/2104.09259v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09259v1)
- **Published**: 2021-04-19 13:04:29+00:00
- **Updated**: 2021-04-19 13:04:29+00:00
- **Authors**: Akin Caliskan, Armin Mustafa, Adrian Hilton
- **Comment**: To appear in Dynavis Workshop, CVPR 2021
- **Journal**: None
- **Summary**: We present a novel method to learn temporally consistent 3D reconstruction of clothed people from a monocular video. Recent methods for 3D human reconstruction from monocular video using volumetric, implicit or parametric human shape models, produce per frame reconstructions giving temporally inconsistent output and limited performance when applied to video. In this paper, we introduce an approach to learn temporally consistent features for textured reconstruction of clothed 3D human sequences from monocular video by proposing two advances: a novel temporal consistency loss function; and hybrid representation learning for implicit 3D reconstruction from 2D images and coarse 3D geometry. The proposed advances improve the temporal consistency and accuracy of both the 3D reconstruction and texture prediction from a monocular video. Comprehensive comparative performance evaluation on images of people demonstrates that the proposed method significantly outperforms the state-of-the-art learning-based single image 3D human shape estimation approaches achieving significant improvement of reconstruction accuracy, completeness, quality and temporal consistency.



### Multi-person Implicit Reconstruction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2104.09283v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09283v1)
- **Published**: 2021-04-19 13:21:55+00:00
- **Updated**: 2021-04-19 13:21:55+00:00
- **Authors**: Armin Mustafa, Akin Caliskan, Lourdes Agapito, Adrian Hilton
- **Comment**: To appear in The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) 2021
- **Journal**: None
- **Summary**: We present a new end-to-end learning framework to obtain detailed and spatially coherent reconstructions of multiple people from a single image. Existing multi-person methods suffer from two main drawbacks: they are often model-based and therefore cannot capture accurate 3D models of people with loose clothing and hair; or they require manual intervention to resolve occlusions or interactions. Our method addresses both limitations by introducing the first end-to-end learning approach to perform model-free implicit reconstruction for realistic 3D capture of multiple clothed people in arbitrary poses (with occlusions) from a single image. Our network simultaneously estimates the 3D geometry of each person and their 6DOF spatial locations, to obtain a coherent multi-human reconstruction. In addition, we introduce a new synthetic dataset that depicts images with a varying number of inter-occluded humans and a variety of clothing and hair styles. We demonstrate robust, high-resolution reconstructions on images of multiple humans with complex occlusions, loose clothing and a large variety of poses and scenes. Our quantitative evaluation on both synthetic and real-world datasets demonstrates state-of-the-art performance with significant improvements in the accuracy and completeness of the reconstructions over competing approaches.



### LAFEAT: Piercing Through Adversarial Defenses with Latent Features
- **Arxiv ID**: http://arxiv.org/abs/2104.09284v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.09284v2)
- **Published**: 2021-04-19 13:22:20+00:00
- **Updated**: 2021-04-20 07:35:16+00:00
- **Authors**: Yunrui Yu, Xitong Gao, Cheng-Zhong Xu
- **Comment**: Accepted as an oral paper in Conference on Computer Vision and
  Pattern Recognition (CVPR) 2021. 11 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: Deep convolutional neural networks are susceptible to adversarial attacks. They can be easily deceived to give an incorrect output by adding a tiny perturbation to the input. This presents a great challenge in making CNNs robust against such attacks. An influx of new defense techniques have been proposed to this end. In this paper, we show that latent features in certain "robust" models are surprisingly susceptible to adversarial attacks. On top of this, we introduce a unified $\ell_\infty$-norm white-box attack algorithm which harnesses latent features in its gradient descent steps, namely LAFEAT. We show that not only is it computationally much more efficient for successful attacks, but it is also a stronger adversary than the current state-of-the-art across a wide range of defense mechanisms. This suggests that model robustness could be contingent on the effective use of the defender's hidden components, and it should no longer be viewed from a holistic perspective.



### Vision-Based Guidance for Tracking Dynamic Objects
- **Arxiv ID**: http://arxiv.org/abs/2104.09301v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.09301v3)
- **Published**: 2021-04-19 13:45:56+00:00
- **Updated**: 2022-05-15 19:33:14+00:00
- **Authors**: Pritam Karmokar, Kashish Dhal, William J. Beksi, Animesh Chakravarthy
- **Comment**: To be published in the 2021 International Conference on Unmanned
  Aircraft Systems (ICUAS)
- **Journal**: None
- **Summary**: In this paper, we present a novel vision-based framework for tracking dynamic objects using guidance laws based on a rendezvous cone approach. These guidance laws enable an unmanned aircraft system equipped with a monocular camera to continuously follow a moving object within the sensor's field of view. We identify and classify feature point estimators for managing the occurrence of occlusions during the tracking process in an exclusive manner. Furthermore, we develop an open-source simulation environment and perform a series of simulations to show the efficacy of our methods.



### Transcriptome-wide prediction of prostate cancer gene expression from histopathology images using co-expression based convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2104.09310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09310v1)
- **Published**: 2021-04-19 13:50:25+00:00
- **Updated**: 2021-04-19 13:50:25+00:00
- **Authors**: Philippe Weitz, Yinxi Wang, Kimmo Kartasalo, Lars Egevad, Johan Lindberg, Henrik Grönberg, Martin Eklund, Mattias Rantalainen
- **Comment**: None
- **Journal**: None
- **Summary**: Molecular phenotyping by gene expression profiling is common in contemporary cancer research and in molecular diagnostics. However, molecular profiling remains costly and resource intense to implement, and is just starting to be introduced into clinical diagnostics. Molecular changes, including genetic alterations and gene expression changes, occuring in tumors cause morphological changes in tissue, which can be observed on the microscopic level. The relationship between morphological patterns and some of the molecular phenotypes can be exploited to predict molecular phenotypes directly from routine haematoxylin and eosin (H&E) stained whole slide images (WSIs) using deep convolutional neural networks (CNNs). In this study, we propose a new, computationally efficient approach for disease specific modelling of relationships between morphology and gene expression, and we conducted the first transcriptome-wide analysis in prostate cancer, using CNNs to predict bulk RNA-sequencing estimates from WSIs of H&E stained tissue. The work is based on the TCGA PRAD study and includes both WSIs and RNA-seq data for 370 patients. Out of 15586 protein coding and sufficiently frequently expressed transcripts, 6618 had predicted expression significantly associated with RNA-seq estimates (FDR-adjusted p-value < 1*10-4) in a cross-validation. 5419 (81.9%) of these were subsequently validated in a held-out test set. We also demonstrate the ability to predict a prostate cancer specific cell cycle progression score directly from WSIs. These findings suggest that contemporary computer vision models offer an inexpensive and scalable solution for prediction of gene expression phenotypes directly from WSIs, providing opportunity for cost-effective large-scale research studies and molecular diagnostics.



### A Mathematical Analysis of Learning Loss for Active Learning in Regression
- **Arxiv ID**: http://arxiv.org/abs/2104.09315v1
- **DOI**: 10.1109/CVPRW53098.2021.00370
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.09315v1)
- **Published**: 2021-04-19 13:54:20+00:00
- **Updated**: 2021-04-19 13:54:20+00:00
- **Authors**: Megh Shukla, Shuaib Ahmed
- **Comment**: Accepted: 2021 IEEE CVPR Workshop on Fair, Data Efficient and Trusted
  Computer Vision
- **Journal**: None
- **Summary**: Active learning continues to remain significant in the industry since it is data efficient. Not only is it cost effective on a constrained budget, continuous refinement of the model allows for early detection and resolution of failure scenarios during the model development stage. Identifying and fixing failures with the model is crucial as industrial applications demand that the underlying model performs accurately in all foreseeable use cases. One popular state-of-the-art technique that specializes in continuously refining the model via failure identification is Learning Loss. Although simple and elegant, this approach is empirically motivated. Our paper develops a foundation for Learning Loss which enables us to propose a novel modification we call LearningLoss++. We show that gradients are crucial in interpreting how Learning Loss works, with rigorous analysis and comparison of the gradients between Learning Loss and LearningLoss++. We also propose a convolutional architecture that combines features at different scales to predict the loss. We validate LearningLoss++ for regression on the task of human pose estimation (using MPII and LSP datasets), as done in Learning Loss. We show that LearningLoss++ outperforms in identifying scenarios where the model is likely to perform poorly, which on model refinement translates into reliable performance in the open world.



### Camera Calibration and Player Localization in SoccerNet-v2 and Investigation of their Representations for Action Spotting
- **Arxiv ID**: http://arxiv.org/abs/2104.09333v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09333v1)
- **Published**: 2021-04-19 14:21:05+00:00
- **Updated**: 2021-04-19 14:21:05+00:00
- **Authors**: Anthony Cioppa, Adrien Deliège, Floriane Magera, Silvio Giancola, Olivier Barnich, Bernard Ghanem, Marc Van Droogenbroeck
- **Comment**: Paper accepted at the CVsports workshop at CVPR2021
- **Journal**: None
- **Summary**: Soccer broadcast video understanding has been drawing a lot of attention in recent years within data scientists and industrial companies. This is mainly due to the lucrative potential unlocked by effective deep learning techniques developed in the field of computer vision. In this work, we focus on the topic of camera calibration and on its current limitations for the scientific community. More precisely, we tackle the absence of a large-scale calibration dataset and of a public calibration network trained on such a dataset. Specifically, we distill a powerful commercial calibration tool in a recent neural network architecture on the large-scale SoccerNet dataset, composed of untrimmed broadcast videos of 500 soccer games. We further release our distilled network, and leverage it to provide 3 ways of representing the calibration results along with player localization. Finally, we exploit those representations within the current best architecture for the action spotting task of SoccerNet-v2, and achieve new state-of-the-art performances.



### Investigating Outdoor Recognition Performance of Infrared Beacons for Infrastructure-based Localization
- **Arxiv ID**: http://arxiv.org/abs/2104.09335v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.09335v2)
- **Published**: 2021-04-19 14:23:20+00:00
- **Updated**: 2022-05-30 19:09:11+00:00
- **Authors**: Alexandru Kampmann, Michael Lamberti, Nikola Petrovic, Stefan Kowalewski, Bassam Alrifaee
- **Comment**: Accepted at IEEE Intelligent Vehicle 2022
- **Journal**: None
- **Summary**: This paper demonstrates a system comprised of infrared beacons and a camera equipped with an optical band-pass filter. Our system can reliably detect and identify individual beacons at 100m distance regardless of lighting conditions. We describe the camera and beacon design as well as the image processing pipeline in detail. In our experiments, we investigate and demonstrate the ability of the system to recognize our beacons in both daytime and nighttime conditions. High precision localization is a key enabler for automated vehicles but remains unsolved, despite strong recent improvements. Our low-cost, infrastructure-based approach is a potential step towards solving the localization problem. All datasets are made available here https://embedded.rwth-aachen.de/doku.php?id=forschung:mobility:infralocalization:concept.



### Single-view robot pose and joint angle estimation via render & compare
- **Arxiv ID**: http://arxiv.org/abs/2104.09359v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.09359v1)
- **Published**: 2021-04-19 14:48:29+00:00
- **Updated**: 2021-04-19 14:48:29+00:00
- **Authors**: Yann Labbé, Justin Carpentier, Mathieu Aubry, Josef Sivic
- **Comment**: Accepted at CVPR 2021 (Oral)
- **Journal**: None
- **Summary**: We introduce RoboPose, a method to estimate the joint angles and the 6D camera-to-robot pose of a known articulated robot from a single RGB image. This is an important problem to grant mobile and itinerant autonomous systems the ability to interact with other robots using only visual information in non-instrumented environments, especially in the context of collaborative robotics. It is also challenging because robots have many degrees of freedom and an infinite space of possible configurations that often result in self-occlusions and depth ambiguities when imaged by a single camera. The contributions of this work are three-fold. First, we introduce a new render & compare approach for estimating the 6D pose and joint angles of an articulated robot that can be trained from synthetic data, generalizes to new unseen robot configurations at test time, and can be applied to a variety of robots. Second, we experimentally demonstrate the importance of the robot parametrization for the iterative pose updates and design a parametrization strategy that is independent of the robot structure. Finally, we show experimental results on existing benchmark datasets for four different robots and demonstrate that our method significantly outperforms the state of the art. Code and pre-trained models are available on the project webpage https://www.di.ens.fr/willow/research/robopose/.



### Contrastive Learning for Compact Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2104.09367v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.09367v1)
- **Published**: 2021-04-19 14:56:21+00:00
- **Updated**: 2021-04-19 14:56:21+00:00
- **Authors**: Haiyan Wu, Yanyun Qu, Shaohui Lin, Jian Zhou, Ruizhi Qiao, Zhizhong Zhang, Yuan Xie, Lizhuang Ma
- **Comment**: CVPR 2021 accepted paper. Code: https://github.com/GlassyWu/AECR-Net
- **Journal**: None
- **Summary**: Single image dehazing is a challenging ill-posed problem due to the severe information degeneration. However, existing deep learning based dehazing methods only adopt clear images as positive samples to guide the training of dehazing network while negative information is unexploited. Moreover, most of them focus on strengthening the dehazing network with an increase of depth and width, leading to a significant requirement of computation and memory. In this paper, we propose a novel contrastive regularization (CR) built upon contrastive learning to exploit both the information of hazy images and clear images as negative and positive samples, respectively. CR ensures that the restored image is pulled to closer to the clear image and pushed to far away from the hazy image in the representation space. Furthermore, considering trade-off between performance and memory storage, we develop a compact dehazing network based on autoencoder-like (AE) framework. It involves an adaptive mixup operation and a dynamic feature enhancement module, which can benefit from preserving information flow adaptively and expanding the receptive field to improve the network's transformation capability, respectively. We term our dehazing network with autoencoder and contrastive regularization as AECR-Net. The extensive experiments on synthetic and real-world datasets demonstrate that our AECR-Net surpass the state-of-the-art approaches. The code is released in https://github.com/GlassyWu/AECR-Net.



### DANICE: Domain adaptation without forgetting in neural image compression
- **Arxiv ID**: http://arxiv.org/abs/2104.09370v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.4.2
- **Links**: [PDF](http://arxiv.org/pdf/2104.09370v1)
- **Published**: 2021-04-19 14:58:37+00:00
- **Updated**: 2021-04-19 14:58:37+00:00
- **Authors**: Sudeep Katakol, Luis Herranz, Fei Yang, Marta Mrak
- **Comment**: Accepted to CLIC Workshop at CVPR 2021
- **Journal**: None
- **Summary**: Neural image compression (NIC) is a new coding paradigm where coding capabilities are captured by deep models learned from data. This data-driven nature enables new potential functionalities. In this paper, we study the adaptability of codecs to custom domains of interest. We show that NIC codecs are transferable and that they can be adapted with relatively few target domain images. However, naive adaptation interferes with the solution optimized for the original source domain, resulting in forgetting the original coding capabilities in that domain, and may even break the compatibility with previously encoded bitstreams. Addressing these problems, we propose Codec Adaptation without Forgetting (CAwF), a framework that can avoid these problems by adding a small amount of custom parameters, where the source codec remains embedded and unchanged during the adaptation process. Experiments demonstrate its effectiveness and provide useful insights on the characteristics of catastrophic interference in NIC.



### A Multi-Task Deep Learning Framework for Building Footprint Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.09375v1
- **DOI**: 10.1109/IGARSS47720.2021.9554766
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09375v1)
- **Published**: 2021-04-19 15:07:27+00:00
- **Updated**: 2021-04-19 15:07:27+00:00
- **Authors**: Burak Ekim, Elif Sertel
- **Comment**: International Geoscience and Remote Sensing Symposium (IGARSS), Jul
  2021, Brussels, Belgium
- **Journal**: None
- **Summary**: The task of building footprint segmentation has been well-studied in the context of remote sensing (RS) as it provides valuable information in many aspects, however, difficulties brought by the nature of RS images such as variations in the spatial arrangements and in-consistent constructional patterns require studying further, since it often causes poorly classified segmentation maps. We address this need by designing a joint optimization scheme for the task of building footprint delineation and introducing two auxiliary tasks; image reconstruction and building footprint boundary segmentation with the intent to reveal the common underlying structure to advance the classification accuracy of a single task model under the favor of auxiliary tasks. In particular, we propose a deep multi-task learning (MTL) based unified fully convolutional framework which operates in an end-to-end manner by making use of joint loss function with learnable loss weights considering the homoscedastic uncertainty of each task loss. Experimental results conducted on the SpaceNet6 dataset demonstrate the potential of the proposed MTL framework as it improves the classification accuracy considerably compared to single-task and lesser compounded tasks.



### A Hierarchical Coding Scheme for Glasses-free 3D Displays Based on Scalable Hybrid Layered Representation of Real-World Light Fields
- **Arxiv ID**: http://arxiv.org/abs/2104.09378v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09378v1)
- **Published**: 2021-04-19 15:09:21+00:00
- **Updated**: 2021-04-19 15:09:21+00:00
- **Authors**: Joshitha R, Mansi Sharma
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel hierarchical coding scheme for light fields based on transmittance patterns of low-rank multiplicative layers and Fourier disparity layers. The proposed scheme learns stacked multiplicative layers from subsets of light field views determined from different scanning orders. The multiplicative layers are optimized using a fast data-driven convolutional neural network (CNN). The spatial correlation in layer patterns is exploited with varying low ranks in factorization derived from singular value decomposition on a Krylov subspace. Further, encoding with HEVC efficiently removes intra-view and inter-view correlation in low-rank approximated layers. The initial subset of approximated decoded views from multiplicative representation is used to construct Fourier disparity layer (FDL) representation. The FDL model synthesizes second subset of views which is identified by a pre-defined hierarchical prediction order. The correlations between the prediction residue of synthesized views is further eliminated by encoding the residual signal. The set of views obtained from decoding the residual is employed in order to refine the FDL model and predict the next subset of views with improved accuracy. This hierarchical procedure is repeated until all light field views are encoded. The critical advantage of proposed hybrid layered representation and coding scheme is that it utilizes not just spatial and temporal redundancies, but efficiently exploits the strong intrinsic similarities among neighboring sub-aperture images in both horizontal and vertical directions as specified by different predication orders. Besides, the scheme is flexible to realize a range of multiple bitrates at the decoder within a single integrated system. The compression performance analyzed with real light field shows substantial bitrate savings, maintaining good reconstruction quality.



### BM-NAS: Bilevel Multimodal Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2104.09379v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.09379v2)
- **Published**: 2021-04-19 15:09:49+00:00
- **Updated**: 2022-01-02 16:06:43+00:00
- **Authors**: Yihang Yin, Siyu Huang, Xiang Zhang
- **Comment**: Accepted by AAAI 2022
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have shown superior performances on various multimodal learning problems. However, it often requires huge efforts to adapt DNNs to individual multimodal tasks by manually engineering unimodal features and designing multimodal feature fusion strategies. This paper proposes Bilevel Multimodal Neural Architecture Search (BM-NAS) framework, which makes the architecture of multimodal fusion models fully searchable via a bilevel searching scheme. At the upper level, BM-NAS selects the inter/intra-modal feature pairs from the pretrained unimodal backbones. At the lower level, BM-NAS learns the fusion strategy for each feature pair, which is a combination of predefined primitive operations. The primitive operations are elaborately designed and they can be flexibly combined to accommodate various effective feature fusion modules such as multi-head attention (Transformer) and Attention on Attention (AoA). Experimental results on three multimodal tasks demonstrate the effectiveness and efficiency of the proposed BM-NAS framework. BM-NAS achieves competitive performances with much less search time and fewer model parameters in comparison with the existing generalized multimodal NAS methods.



### A Two-stage Deep Network for High Dynamic Range Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2104.09386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09386v1)
- **Published**: 2021-04-19 15:19:17+00:00
- **Updated**: 2021-04-19 15:19:17+00:00
- **Authors**: SMA Sharif, Rizwan Ali Naqvi, Mithun Biswas, Kim Sungjun
- **Comment**: None
- **Journal**: None
- **Summary**: Mapping a single exposure low dynamic range (LDR) image into a high dynamic range (HDR) is considered among the most strenuous image to image translation tasks due to exposure-related missing information. This study tackles the challenges of single-shot LDR to HDR mapping by proposing a novel two-stage deep network. Notably, our proposed method aims to reconstruct an HDR image without knowing hardware information, including camera response function (CRF) and exposure settings. Therefore, we aim to perform image enhancement task like denoising, exposure correction, etc., in the first stage. Additionally, the second stage of our deep network learns tone mapping and bit-expansion from a convex set of data samples. The qualitative and quantitative comparisons demonstrate that the proposed method can outperform the existing LDR to HDR works with a marginal difference. Apart from that, we collected an LDR image dataset incorporating different camera systems. The evaluation with our collected real-world LDR images illustrates that the proposed method can reconstruct plausible HDR images without presenting any visual artefacts. Code available: https://github. com/sharif-apu/twostageHDR_NTIRE21.



### Beyond Joint Demosaicking and Denoising: An Image Processing Pipeline for a Pixel-bin Image Sensor
- **Arxiv ID**: http://arxiv.org/abs/2104.09398v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09398v1)
- **Published**: 2021-04-19 15:41:28+00:00
- **Updated**: 2021-04-19 15:41:28+00:00
- **Authors**: SMA Sharif, Rizwan Ali Naqvi, Mithun Biswas
- **Comment**: None
- **Journal**: None
- **Summary**: Pixel binning is considered one of the most prominent solutions to tackle the hardware limitation of smartphone cameras. Despite numerous advantages, such an image sensor has to appropriate an artefact-prone non-Bayer colour filter array (CFA) to enable the binning capability. Contrarily, performing essential image signal processing (ISP) tasks like demosaicking and denoising, explicitly with such CFA patterns, makes the reconstruction process notably complicated. In this paper, we tackle the challenges of joint demosaicing and denoising (JDD) on such an image sensor by introducing a novel learning-based method. The proposed method leverages the depth and spatial attention in a deep network. The proposed network is guided by a multi-term objective function, including two novel perceptual losses to produce visually plausible images. On top of that, we stretch the proposed image processing pipeline to comprehensively reconstruct and enhance the images captured with a smartphone camera, which uses pixel binning techniques. The experimental results illustrate that the proposed method can outperform the existing methods by a noticeable margin in qualitative and quantitative comparisons. Code available: https://github.com/sharif-apu/BJDD_CVPR21.



### OmniLayout: Room Layout Reconstruction from Indoor Spherical Panoramas
- **Arxiv ID**: http://arxiv.org/abs/2104.09403v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09403v1)
- **Published**: 2021-04-19 15:44:10+00:00
- **Updated**: 2021-04-19 15:44:10+00:00
- **Authors**: Shivansh Rao, Vikas Kumar, Daniel Kifer, Lee Giles, Ankur Mali
- **Comment**: Accepted at CVPR, OmniCV Workshop. 10 Pages, 9 Figures, 6 Tables
- **Journal**: None
- **Summary**: Given a single RGB panorama, the goal of 3D layout reconstruction is to estimate the room layout by predicting the corners, floor boundary, and ceiling boundary. A common approach has been to use standard convolutional networks to predict the corners and boundaries, followed by post-processing to generate the 3D layout. However, the space-varying distortions in panoramic images are not compatible with the translational equivariance property of standard convolutions, thus degrading performance. Instead, we propose to use spherical convolutions. The resulting network, which we call OmniLayout performs convolutions directly on the sphere surface, sampling according to inverse equirectangular projection and hence invariant to equirectangular distortions. Using a new evaluation metric, we show that our network reduces the error in the heavily distorted regions (near the poles) by approx 25 % when compared to standard convolutional networks. Experimental results show that OmniLayout outperforms the state-of-the-art by approx 4% on two different benchmark datasets (PanoContext and Stanford 2D-3D). Code is available at https://github.com/rshivansh/OmniLayout.



### Understanding Chinese Video and Language via Contrastive Multimodal Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2104.09411v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.09411v1)
- **Published**: 2021-04-19 15:58:45+00:00
- **Updated**: 2021-04-19 15:58:45+00:00
- **Authors**: Chenyi Lei, Shixian Luo, Yong Liu, Wanggui He, Jiamang Wang, Guoxin Wang, Haihong Tang, Chunyan Miao, Houqiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: The pre-trained neural models have recently achieved impressive performances in understanding multimodal content. However, it is still very challenging to pre-train neural models for video and language understanding, especially for Chinese video-language data, due to the following reasons. Firstly, existing video-language pre-training algorithms mainly focus on the co-occurrence of words and video frames, but ignore other valuable semantic and structure information of video-language content, e.g., sequential order and spatiotemporal relationships. Secondly, there exist conflicts between video sentence alignment and other proxy tasks. Thirdly, there is a lack of large-scale and high-quality Chinese video-language datasets (e.g., including 10 million unique videos), which are the fundamental success conditions for pre-training techniques.   In this work, we propose a novel video-language understanding framework named VICTOR, which stands for VIdeo-language understanding via Contrastive mulTimOdal pRe-training. Besides general proxy tasks such as masked language modeling, VICTOR constructs several novel proxy tasks under the contrastive learning paradigm, making the model be more robust and able to capture more complex multimodal semantic and structural relationships from different perspectives. VICTOR is trained on a large-scale Chinese video-language dataset, including over 10 million complete videos with corresponding high-quality textual descriptions. We apply the pre-trained VICTOR model to a series of downstream applications and demonstrate its superior performances, comparing against the state-of-the-art pre-training methods such as VideoBERT and UniVL. The codes and trained checkpoints will be publicly available to nourish further developments of the research community.



### Cross-Domain Adaptive Clustering for Semi-Supervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2104.09415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09415v1)
- **Published**: 2021-04-19 16:07:32+00:00
- **Updated**: 2021-04-19 16:07:32+00:00
- **Authors**: Jichang Li, Guanbin Li, Yemin Shi, Yizhou Yu
- **Comment**: To appear in CVPR2021
- **Journal**: None
- **Summary**: In semi-supervised domain adaptation, a few labeled samples per class in the target domain guide features of the remaining target samples to aggregate around them. However, the trained model cannot produce a highly discriminative feature representation for the target domain because the training data is dominated by labeled samples from the source domain. This could lead to disconnection between the labeled and unlabeled target samples as well as misalignment between unlabeled target samples and the source domain. In this paper, we propose a novel approach called Cross-domain Adaptive Clustering to address this problem. To achieve both inter-domain and intra-domain adaptation, we first introduce an adversarial adaptive clustering loss to group features of unlabeled target data into clusters and perform cluster-wise feature alignment across the source and target domains. We further apply pseudo labeling to unlabeled samples in the target domain and retain pseudo-labels with high confidence. Pseudo labeling expands the number of ``labeled" samples in each class in the target domain, and thus produces a more robust and powerful cluster core for each class to facilitate adversarial learning. Extensive experiments on benchmark datasets, including DomainNet, Office-Home and Office, demonstrate that our proposed approach achieves the state-of-the-art performance in semi-supervised domain adaptation.



### Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?
- **Arxiv ID**: http://arxiv.org/abs/2104.09425v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.09425v3)
- **Published**: 2021-04-19 16:17:12+00:00
- **Updated**: 2022-03-03 15:38:12+00:00
- **Authors**: Vikash Sehwag, Saeed Mahloujifar, Tinashe Handina, Sihui Dai, Chong Xiang, Mung Chiang, Prateek Mittal
- **Comment**: ICLR 2022 version (30 pages, 13 figures, 12 tables)
- **Journal**: None
- **Summary**: While additional training data improves the robustness of deep neural networks against adversarial examples, it presents the challenge of curating a large number of specific real-world samples. We circumvent this challenge by using additional data from proxy distributions learned by advanced generative models. We first seek to formally understand the transfer of robustness from classifiers trained on proxy distributions to the real data distribution. We prove that the difference between the robustness of a classifier on the two distributions is upper bounded by the conditional Wasserstein distance between them. Next we use proxy distributions to significantly improve the performance of adversarial training on five different datasets. For example, we improve robust accuracy by up to 7.5% and 6.7% in $\ell_{\infty}$ and $\ell_2$ threat model over baselines that are not using proxy distributions on the CIFAR-10 dataset. We also improve certified robust accuracy by 7.6% on the CIFAR-10 dataset. We further demonstrate that different generative models bring a disparate improvement in the performance in robust training. We propose a robust discrimination approach to characterize the impact of individual generative models and further provide a deeper understanding of why current state-of-the-art in diffusion-based generative models are a better choice for proxy distribution than generative adversarial networks.



### Deep learning enables reference-free isotropic super-resolution for volumetric fluorescence microscopy
- **Arxiv ID**: http://arxiv.org/abs/2104.09435v2
- **DOI**: 10.1038/s41467-022-30949-6
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.09435v2)
- **Published**: 2021-04-19 16:31:12+00:00
- **Updated**: 2021-06-07 02:45:47+00:00
- **Authors**: Hyoungjun Park, Myeongsu Na, Bumju Kim, Soohyun Park, Ki Hean Kim, Sunghoe Chang, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Volumetric imaging by fluorescence microscopy is often limited by anisotropic spatial resolution from inferior axial resolution compared to the lateral resolution. To address this problem, here we present a deep-learning-enabled unsupervised super-resolution technique that enhances anisotropic images in volumetric fluorescence microscopy. In contrast to the existing deep learning approaches that require matched high-resolution target volume images, our method greatly reduces the effort to put into practice as the training of a network requires as little as a single 3D image stack, without a priori knowledge of the image formation process, registration of training data, or separate acquisition of target data. This is achieved based on the optimal transport driven cycle-consistent generative adversarial network that learns from an unpaired matching between high-resolution 2D images in lateral image plane and low-resolution 2D images in the other planes. Using fluorescence confocal microscopy and light-sheet microscopy, we demonstrate that the trained network not only enhances axial resolution, but also restores suppressed visual details between the imaging planes and removes imaging artifacts.



### One More Check: Making "Fake Background" Be Tracked Again
- **Arxiv ID**: http://arxiv.org/abs/2104.09441v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09441v2)
- **Published**: 2021-04-19 16:42:47+00:00
- **Updated**: 2021-12-12 11:43:43+00:00
- **Authors**: Chao Liang, Zhipeng Zhang, Xue Zhou, Bing Li, Weiming Hu
- **Comment**: Accepted by AAAI 2022
- **Journal**: None
- **Summary**: The one-shot multi-object tracking, which integrates object detection and ID embedding extraction into a unified network, has achieved groundbreaking results in recent years. However, current one-shot trackers solely rely on single-frame detections to predict candidate bounding boxes, which may be unreliable when facing disastrous visual degradation, e.g., motion blur, occlusions. Once a target bounding box is mistakenly classified as background by the detector, the temporal consistency of its corresponding tracklet will be no longer maintained. In this paper, we set out to restore the bounding boxes misclassified as ``fake background'' by proposing a re-check network. The re-check network innovatively expands the role of ID embedding from data association to motion forecasting by effectively propagating previous tracklets to the current frame with a small overhead. Note that the propagation results are yielded by an independent and efficient embedding search, preventing the model from over-relying on detection results. Eventually, it helps to reload the ``fake background'' and repair the broken tracklets. Building on a strong baseline CSTrack, we construct a new one-shot tracker and achieve favorable gains by 70.7 $\rightarrow$ 76.4, 70.6 $\rightarrow$ 76.3 MOTA on MOT16 and MOT17, respectively. It also reaches a new state-of-the-art MOTA and IDF1 performance. Code is released at https://github.com/JudasDie/SOTS.



### What can human minimal videos tell us about dynamic recognition models?
- **Arxiv ID**: http://arxiv.org/abs/2104.09447v1
- **DOI**: 10.1016/j.cognition.2020.104263
- **Categories**: **cs.CV**, cs.AI, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2104.09447v1)
- **Published**: 2021-04-19 16:53:25+00:00
- **Updated**: 2021-04-19 16:53:25+00:00
- **Authors**: Guy Ben-Yosef, Gabriel Kreiman, Shimon Ullman
- **Comment**: Published as a workshop paper at Bridging AI and Cognitive Science
  (ICLR 2020). Extended paper was published at Cognition
- **Journal**: None
- **Summary**: In human vision objects and their parts can be visually recognized from purely spatial or purely temporal information but the mechanisms integrating space and time are poorly understood. Here we show that human visual recognition of objects and actions can be achieved by efficiently combining spatial and motion cues in configurations where each source on its own is insufficient for recognition. This analysis is obtained by identifying minimal videos: these are short and tiny video clips in which objects, parts, and actions can be reliably recognized, but any reduction in either space or time makes them unrecognizable. State-of-the-art deep networks for dynamic visual recognition cannot replicate human behavior in these configurations. This gap between humans and machines points to critical mechanisms in human dynamic vision that are lacking in current models.



### Inharmonious Region Localization
- **Arxiv ID**: http://arxiv.org/abs/2104.09453v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09453v1)
- **Published**: 2021-04-19 17:12:58+00:00
- **Updated**: 2021-04-19 17:12:58+00:00
- **Authors**: Jing Liang, Li Niu, Liqing Zhang
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: The advance of image editing techniques allows users to create artistic works, but the manipulated regions may be incompatible with the background. Localizing the inharmonious region is an appealing yet challenging task. Realizing that this task requires effective aggregation of multi-scale contextual information and suppression of redundant information, we design novel Bi-directional Feature Integration (BFI) block and Global-context Guided Decoder (GGD) block to fuse multi-scale features in the encoder and decoder respectively. We also employ Mask-guided Dual Attention (MDA) block between the encoder and decoder to suppress the redundant information. Experiments on the image harmonization dataset demonstrate that our method achieves competitive performance for inharmonious region localization. The source code is available at https://github.com/bcmi/DIRL.



### Few-Shot Model Adaptation for Customized Facial Landmark Detection, Segmentation, Stylization and Shadow Removal
- **Arxiv ID**: http://arxiv.org/abs/2104.09457v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09457v1)
- **Published**: 2021-04-19 17:15:56+00:00
- **Updated**: 2021-04-19 17:15:56+00:00
- **Authors**: Zhen Wei, Bingkun Liu, Weinong Wang, Yu-Wing Tai
- **Comment**: with supplementary file
- **Journal**: None
- **Summary**: Despite excellent progress has been made, the performance of deep learning based algorithms still heavily rely on specific datasets, which are difficult to extend due to labor-intensive labeling. Moreover, because of the advancement of new applications, initial definition of data annotations might not always meet the requirements of new functionalities. Thus, there is always a great demand in customized data annotations. To address the above issues, we propose the Few-Shot Model Adaptation (FSMA) framework and demonstrate its potential on several important tasks on Faces. The FSMA first acquires robust facial image embeddings by training an adversarial auto-encoder using large-scale unlabeled data. Then the model is equipped with feature adaptation and fusion layers, and adapts to the target task efficiently using a minimal amount of annotated images. The FSMA framework is prominent in its versatility across a wide range of facial image applications. The FSMA achieves state-of-the-art few-shot landmark detection performance and it offers satisfying solutions for few-shot face segmentation, stylization and facial shadow removal tasks for the first time.



### Entropy-based Optimization via A* Algorithm for Parking Space Recommendation
- **Arxiv ID**: http://arxiv.org/abs/2104.09461v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.09461v1)
- **Published**: 2021-04-19 17:24:51+00:00
- **Updated**: 2021-04-19 17:24:51+00:00
- **Authors**: Xin Wei, Runqi Qiu, Houyu Yu, Yurun Yang, Haoyu Tian, Xiang Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the path planning problems for recommending parking spaces, given the difficulties of identifying the most optimal route to vacant parking spaces and the shortest time to leave the parking space. Our optimization approach is based on the entropy method and realized by the A* algorithm. Experiments have shown that the combination of A* and the entropy value induces the optimal parking solution with the shortest route while being robust to environmental factors.



### NeRD: Neural 3D Reflection Symmetry Detector
- **Arxiv ID**: http://arxiv.org/abs/2105.03211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.03211v1)
- **Published**: 2021-04-19 17:25:51+00:00
- **Updated**: 2021-04-19 17:25:51+00:00
- **Authors**: Yichao Zhou, Shichen Liu, Yi Ma
- **Comment**: CVPR 2021. overlaps with arXiv:2006.10042
- **Journal**: None
- **Summary**: Recent advances have shown that symmetry, a structural prior that most objects exhibit, can support a variety of single-view 3D understanding tasks. However, detecting 3D symmetry from an image remains a challenging task. Previous works either assume that the symmetry is given or detect the symmetry with a heuristic-based method. In this paper, we present NeRD, a Neural 3D Reflection Symmetry Detector, which combines the strength of learning-based recognition and geometry-based reconstruction to accurately recover the normal direction of objects' mirror planes. Specifically, we first enumerate the symmetry planes with a coarse-to-fine strategy and then find the best ones by building 3D cost volumes to examine the intra-image pixel correspondence from the symmetry. Our experiments show that the symmetry planes detected with our method are significantly more accurate than the planes from direct CNN regression on both synthetic and real-world datasets. We also demonstrate that the detected symmetry can be used to improve the performance of downstream tasks such as pose estimation and depth map regression. The code of this paper has been made public at https://github.com/zhou13/nerd.



### Few-shot learning via tensor hallucination
- **Arxiv ID**: http://arxiv.org/abs/2104.09467v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09467v1)
- **Published**: 2021-04-19 17:30:33+00:00
- **Updated**: 2021-04-19 17:30:33+00:00
- **Authors**: Michalis Lazarou, Yannis Avrithis, Tania Stathaki
- **Comment**: Accepted as oral at ICLR2021 workshop: "Synthetic Data Generation:
  Quality, Privacy, Bias"
- **Journal**: None
- **Summary**: Few-shot classification addresses the challenge of classifying examples given only limited labeled data. A powerful approach is to go beyond data augmentation, towards data synthesis. However, most of data augmentation/synthesis methods for few-shot classification are overly complex and sophisticated, e.g. training a wGAN with multiple regularizers or training a network to transfer latent diversities from known to novel classes. We make two contributions, namely we show that: (1) using a simple loss function is more than enough for training a feature generator in the few-shot setting; and (2) learning to generate tensor features instead of vector features is superior. Extensive experiments on miniImagenet, CUB and CIFAR-FS datasets show that our method sets a new state of the art, outperforming more sophisticated few-shot data augmentation methods.



### Bayesian Uncertainty and Expected Gradient Length -- Regression: Two Sides Of The Same Coin?
- **Arxiv ID**: http://arxiv.org/abs/2104.09493v3
- **DOI**: 10.1109/WACV51458.2022.00208
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.09493v3)
- **Published**: 2021-04-19 17:56:59+00:00
- **Updated**: 2021-10-22 07:56:03+00:00
- **Authors**: Megh Shukla
- **Comment**: Accepted: WACV 2022, Algorithms track
- **Journal**: None
- **Summary**: Active learning algorithms select a subset of data for annotation to maximize the model performance on a budget. One such algorithm is Expected Gradient Length, which as the name suggests uses the approximate gradient induced per example in the sampling process. While Expected Gradient Length has been successfully used for classification and regression, the formulation for regression remains intuitively driven. Hence, our theoretical contribution involves deriving this formulation, thereby supporting the experimental evidence. Subsequently, we show that expected gradient length in regression is equivalent to Bayesian uncertainty. If certain assumptions are infeasible, our algorithmic contribution (EGL++) approximates the effect of ensembles with a single deterministic network. Instead of computing multiple possible inferences per input, we leverage previously annotated samples to quantify the probability of previous labels being the true label. Such an approach allows us to extend expected gradient length to a new task: human pose estimation. We perform experimental validation on two human pose datasets (MPII and LSP/LSPET), highlighting the interpretability and competitiveness of EGL++ with different active learning algorithms for human pose estimation.



### Temporal Query Networks for Fine-grained Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/2104.09496v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09496v1)
- **Published**: 2021-04-19 17:58:48+00:00
- **Updated**: 2021-04-19 17:58:48+00:00
- **Authors**: Chuhan Zhang, Ankush Gupta, Andrew Zisserman
- **Comment**: Accepted to CVPR 2021(Oral). Project page:
  http://www.robots.ox.ac.uk/~vgg/research/tqn/
- **Journal**: None
- **Summary**: Our objective in this work is fine-grained classification of actions in untrimmed videos, where the actions may be temporally extended or may span only a few frames of the video. We cast this into a query-response mechanism, where each query addresses a particular question, and has its own response label set. We make the following four contributions: (I) We propose a new model - a Temporal Query Network - which enables the query-response functionality, and a structural understanding of fine-grained actions. It attends to relevant segments for each query with a temporal attention mechanism, and can be trained using only the labels for each query. (ii) We propose a new way - stochastic feature bank update - to train a network on videos of various lengths with the dense sampling required to respond to fine-grained queries. (iii) We compare the TQN to other architectures and text supervision methods, and analyze their pros and cons. Finally, (iv) we evaluate the method extensively on the FineGym and Diving48 benchmarks for fine-grained action classification and surpass the state-of-the-art using only RGB features.



### Attention in Attention Network for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2104.09497v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09497v3)
- **Published**: 2021-04-19 17:59:06+00:00
- **Updated**: 2021-11-07 03:53:04+00:00
- **Authors**: Haoyu Chen, Jinjin Gu, Zhi Zhang
- **Comment**: 11 pages, 10 figures. Codes are available at
  https://github.com/haoyuc/A2N
- **Journal**: None
- **Summary**: Convolutional neural networks have allowed remarkable advances in single image super-resolution (SISR) over the last decade. Among recent advances in SISR, attention mechanisms are crucial for high-performance SR models. However, the attention mechanism remains unclear on why and how it works in SISR. In this work, we attempt to quantify and visualize attention mechanisms in SISR and show that not all attention modules are equally beneficial. We then propose attention in attention network (A$^2$N) for more efficient and accurate SISR. Specifically, A$^2$N consists of a non-attention branch and a coupling attention branch. A dynamic attention module is proposed to generate weights for these two branches to suppress unwanted attention adjustments dynamically, where the weights change adaptively according to the input features. This allows attention modules to specialize to beneficial examples without otherwise penalties and thus greatly improve the capacity of the attention network with few parameters overhead. Experimental results demonstrate that our final model A$^2$N could achieve superior trade-off performances comparing with state-of-the-art networks of similar sizes. Codes are available at https://github.com/haoyuc/A2N.



### Comparing Correspondences: Video Prediction with Correspondence-wise Losses
- **Arxiv ID**: http://arxiv.org/abs/2104.09498v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.09498v2)
- **Published**: 2021-04-19 17:59:29+00:00
- **Updated**: 2022-03-31 18:10:51+00:00
- **Authors**: Daniel Geng, Max Hamilton, Andrew Owens
- **Comment**: CVPR 2022 Camera Ready
- **Journal**: None
- **Summary**: Image prediction methods often struggle on tasks that require changing the positions of objects, such as video prediction, producing blurry images that average over the many positions that objects might occupy. In this paper, we propose a simple change to existing image similarity metrics that makes them more robust to positional errors: we match the images using optical flow, then measure the visual similarity of corresponding pixels. This change leads to crisper and more perceptually accurate predictions, and does not require modifications to the image prediction network. We apply our method to a variety of video prediction tasks, where it obtains strong performance with simple network architectures, and to the closely related task of video interpolation. Code and results are available at our webpage: https://dangeng.github.io/CorrWiseLosses



### Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network
- **Arxiv ID**: http://arxiv.org/abs/2104.09556v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.09556v1)
- **Published**: 2021-04-19 18:41:45+00:00
- **Updated**: 2021-04-19 18:41:45+00:00
- **Authors**: Ruicheng Feng, Chongyi Li, Huaijin Chen, Shuai Li, Chen Change Loy, Jinwei Gu
- **Comment**: CVPR 2021 camera-ready version
- **Journal**: None
- **Summary**: Recent development of Under-Display Camera (UDC) systems provides a true bezel-less and notch-free viewing experience on smartphones (and TV, laptops, tablets), while allowing images to be captured from the selfie camera embedded underneath. In a typical UDC system, the microstructure of the semi-transparent organic light-emitting diode (OLED) pixel array attenuates and diffracts the incident light on the camera, resulting in significant image quality degradation. Oftentimes, noise, flare, haze, and blur can be observed in UDC images. In this work, we aim to analyze and tackle the aforementioned degradation problems. We define a physics-based image formation model to better understand the degradation. In addition, we utilize one of the world's first commodity UDC smartphone prototypes to measure the real-world Point Spread Function (PSF) of the UDC system, and provide a model-based data synthesis pipeline to generate realistically degraded images. We specially design a new domain knowledge-enabled Dynamic Skip Connection Network (DISCNet) to restore the UDC images. We demonstrate the effectiveness of our method through extensive experiments on both synthetic and real UDC data. Our physics-based image formation model and proposed DISCNet can provide foundations for further exploration in UDC image restoration, and even for general diffraction artifact removal in a broader sense.



### A Framework using Contrastive Learning for Classification with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2104.09563v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.09563v1)
- **Published**: 2021-04-19 18:51:22+00:00
- **Updated**: 2021-04-19 18:51:22+00:00
- **Authors**: Madalina Ciortan, Romain Dupuis, Thomas Peel
- **Comment**: 22 pages
- **Journal**: None
- **Summary**: We propose a framework using contrastive learning as a pre-training task to perform image classification in the presence of noisy labels. Recent strategies such as pseudo-labeling, sample selection with Gaussian Mixture models, weighted supervised contrastive learning have been combined into a fine-tuning phase following the pre-training. This paper provides an extensive empirical study showing that a preliminary contrastive learning step brings a significant gain in performance when using different loss functions: non-robust, robust, and early-learning regularized. Our experiments performed on standard benchmarks and real-world datasets demonstrate that: i) the contrastive pre-training increases the robustness of any loss function to noisy labels and ii) the additional fine-tuning phase can further improve accuracy but at the cost of additional complexity.



### Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information
- **Arxiv ID**: http://arxiv.org/abs/2104.09580v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.09580v1)
- **Published**: 2021-04-19 19:18:41+00:00
- **Updated**: 2021-04-19 19:18:41+00:00
- **Authors**: Jialu Li, Hao Tan, Mohit Bansal
- **Comment**: NAACL 2021 (10 pages)
- **Journal**: None
- **Summary**: Vision language navigation is the task that requires an agent to navigate through a 3D environment based on natural language instructions. One key challenge in this task is to ground instructions with the current visual information that the agent perceives. Most of the existing work employs soft attention over individual words to locate the instruction required for the next action. However, different words have different functions in a sentence (e.g., modifiers convey attributes, verbs convey actions). Syntax information like dependencies and phrase structures can aid the agent to locate important parts of the instruction. Hence, in this paper, we propose a navigation agent that utilizes syntax information derived from a dependency tree to enhance alignment between the instruction and the current visual scenes. Empirically, our agent outperforms the baseline model that does not use syntax information on the Room-to-Room dataset, especially in the unseen environment. Besides, our agent achieves the new state-of-the-art on Room-Across-Room dataset, which contains instructions in 3 languages (English, Hindi, and Telugu). We also show that our agent is better at aligning instructions with the current visual information via qualitative visualizations. Code and models: https://github.com/jialuli-luka/SyntaxVLN



### End-to-End Jet Classification of Boosted Top Quarks with the CMS Open Data
- **Arxiv ID**: http://arxiv.org/abs/2104.14659v3
- **DOI**: 10.1103/PhysRevD.105.052008
- **Categories**: **physics.data-an**, cs.CV, cs.LG, hep-ex
- **Links**: [PDF](http://arxiv.org/pdf/2104.14659v3)
- **Published**: 2021-04-19 19:36:43+00:00
- **Updated**: 2022-01-21 17:18:10+00:00
- **Authors**: Michael Andrews, Bjorn Burkle, Yi-fan Chen, Davide DiCroce, Sergei Gleyzer, Ulrich Heintz, Meenakshi Narain, Manfred Paulini, Nikolas Pervan, Yusef Shafi, Wei Sun, Emanuele Usai, Kun Yang
- **Comment**: 9 pages, 3 figures, 4 tables; v3: unpublished
- **Journal**: None
- **Summary**: We describe a novel application of the end-to-end deep learning technique to the task of discriminating top quark-initiated jets from those originating from the hadronization of a light quark or a gluon. The end-to-end deep learning technique combines deep learning algorithms and low-level detector representation of the high-energy collision event. In this study, we use low-level detector information from the simulated CMS Open Data samples to construct the top jet classifiers. To optimize classifier performance we progressively add low-level information from the CMS tracking detector, including pixel detector reconstructed hits and impact parameters, and demonstrate the value of additional tracking information even when no new spatial structures are added. Relying only on calorimeter energy deposits and reconstructed pixel detector hits, the end-to-end classifier achieves an AUC score of 0.975$\pm$0.002 for the task of classifying boosted top quark jets. After adding derived track quantities, the classifier AUC score increases to 0.9824$\pm$0.0013, serving as the first performance benchmark for these CMS Open Data samples. We additionally provide a timing performance comparison of different processor unit architectures for training the network.



### ASFM-Net: Asymmetrical Siamese Feature Matching Network for Point Completion
- **Arxiv ID**: http://arxiv.org/abs/2104.09587v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.09587v3)
- **Published**: 2021-04-19 19:42:42+00:00
- **Updated**: 2021-08-04 08:53:32+00:00
- **Authors**: Yaqi Xia, Yan Xia, Wei Li, Rui Song, Kailang Cao, Uwe Stilla
- **Comment**: Accepted by ACM MM2021. This work achieves the 1st place in the
  leaderboard of Completion3D
- **Journal**: None
- **Summary**: We tackle the problem of object completion from point clouds and propose a novel point cloud completion network employing an Asymmetrical Siamese Feature Matching strategy, termed as ASFM-Net. Specifically, the Siamese auto-encoder neural network is adopted to map the partial and complete input point cloud into a shared latent space, which can capture detailed shape prior. Then we design an iterative refinement unit to generate complete shapes with fine-grained details by integrating prior information. Experiments are conducted on the PCN dataset and the Completion3D benchmark, demonstrating the state-of-the-art performance of the proposed ASFM-Net. Our method achieves the 1st place in the leaderboard of Completion3D and outperforms existing methods with a large margin, about 12%. The codes and trained models are released publicly at https://github.com/Yan-Xia/ASFM-Net.



### Engineering Sketch Generation for Computer-Aided Design
- **Arxiv ID**: http://arxiv.org/abs/2104.09621v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2104.09621v1)
- **Published**: 2021-04-19 20:38:36+00:00
- **Updated**: 2021-04-19 20:38:36+00:00
- **Authors**: Karl D. D. Willis, Pradeep Kumar Jayaraman, Joseph G. Lambourne, Hang Chu, Yewen Pu
- **Comment**: The 1st Workshop on Sketch-Oriented Deep Learning @ CVPR 2021
- **Journal**: None
- **Summary**: Engineering sketches form the 2D basis of parametric Computer-Aided Design (CAD), the foremost modeling paradigm for manufactured objects. In this paper we tackle the problem of learning based engineering sketch generation as a first step towards synthesis and composition of parametric CAD models. We propose two generative models, CurveGen and TurtleGen, for engineering sketch generation. Both models generate curve primitives without the need for a sketch constraint solver and explicitly consider topology for downstream use with constraints and 3D CAD modeling operations. We find in our perceptual evaluation using human subjects that both CurveGen and TurtleGen produce more realistic engineering sketches when compared with the current state-of-the-art for engineering sketch generation.



### Quaternion Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.09630v2
- **DOI**: 10.1007/978-3-030-91390-8_4
- **Categories**: **cs.LG**, cs.AI, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.09630v2)
- **Published**: 2021-04-19 20:46:18+00:00
- **Updated**: 2021-07-27 15:30:42+00:00
- **Authors**: Eleonora Grassucci, Edoardo Cicero, Danilo Comminiello
- **Comment**: Accepted as a Chapter for the SPRINGER book "Generative Adversarial
  Learning: Architectures and Applications"
- **Journal**: Generative Adversarial Learning: Architectures and Applications.
  Intelligent Systems Reference Library, vol 217. Springer, Cham, Feb. 2022
- **Summary**: Latest Generative Adversarial Networks (GANs) are gathering outstanding results through a large-scale training, thus employing models composed of millions of parameters requiring extensive computational capabilities. Building such huge models undermines their replicability and increases the training instability. Moreover, multi-channel data, such as images or audio, are usually processed by realvalued convolutional networks that flatten and concatenate the input, often losing intra-channel spatial relations. To address these issues related to complexity and information loss, we propose a family of quaternion-valued generative adversarial networks (QGANs). QGANs exploit the properties of quaternion algebra, e.g., the Hamilton product, that allows to process channels as a single entity and capture internal latent relations, while reducing by a factor of 4 the overall number of parameters. We show how to design QGANs and to extend the proposed approach even to advanced models.We compare the proposed QGANs with real-valued counterparts on several image generation benchmarks. Results show that QGANs are able to obtain better FID scores than real-valued GANs and to generate visually pleasing images. Furthermore, QGANs save up to 75% of the training parameters. We believe these results may pave the way to novel, more accessible, GANs capable of improving performance and saving computational resources.



### Memory Efficient 3D U-Net with Reversible Mobile Inverted Bottlenecks for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.09648v2
- **DOI**: 10.1007/978-3-030-72087-2_34
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.09648v2)
- **Published**: 2021-04-19 21:23:55+00:00
- **Updated**: 2021-04-21 01:02:05+00:00
- **Authors**: Mihir Pendse, Vithursan Thangarasa, Vitaliy Chiley, Ryan Holmdahl, Joel Hestness, Dennis DeCoste
- **Comment**: 11 pages, 5 figures, Published at MICCAI Brainles 2020
- **Journal**: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic
  Brain Injuries (2021) 388-397
- **Summary**: We propose combining memory saving techniques with traditional U-Net architectures to increase the complexity of the models on the Brain Tumor Segmentation (BraTS) challenge. The BraTS challenge consists of a 3D segmentation of a 240x240x155x4 input image into a set of tumor classes. Because of the large volume and need for 3D convolutional layers, this task is very memory intensive. To address this, prior approaches use smaller cropped images while constraining the model's depth and width. Our 3D U-Net uses a reversible version of the mobile inverted bottleneck block defined in MobileNetV2, MnasNet and the more recent EfficientNet architectures to save activation memory during training. Using reversible layers enables the model to recompute input activations given the outputs of that layer, saving memory by eliminating the need to store activations during the forward pass. The inverted residual bottleneck block uses lightweight depthwise separable convolutions to reduce computation by decomposing convolutions into a pointwise convolution and a depthwise convolution. Further, this block inverts traditional bottleneck blocks by placing an intermediate expansion layer between the input and output linear 1x1 convolution, reducing the total number of channels. Given a fixed memory budget, with these memory saving techniques, we are able to train image volumes up to 3x larger, models with 25% more depth, or models with up to 2x the number of channels than a corresponding non-reversible network.



### Manipulating SGD with Data Ordering Attacks
- **Arxiv ID**: http://arxiv.org/abs/2104.09667v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.09667v2)
- **Published**: 2021-04-19 22:17:27+00:00
- **Updated**: 2021-06-05 10:22:15+00:00
- **Authors**: Ilia Shumailov, Zakhar Shumaylov, Dmitry Kazhdan, Yiren Zhao, Nicolas Papernot, Murat A. Erdogdu, Ross Anderson
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning is vulnerable to a wide variety of attacks. It is now well understood that by changing the underlying data distribution, an adversary can poison the model trained with it or introduce backdoors. In this paper we present a novel class of training-time attacks that require no changes to the underlying dataset or model architecture, but instead only change the order in which data are supplied to the model. In particular, we find that the attacker can either prevent the model from learning, or poison it to learn behaviours specified by the attacker. Furthermore, we find that even a single adversarially-ordered epoch can be enough to slow down model learning, or even to reset all of the learning progress. Indeed, the attacks presented here are not specific to the model or dataset, but rather target the stochastic nature of modern learning procedures. We extensively evaluate our attacks on computer vision and natural language benchmarks to find that the adversary can disrupt model training and even introduce backdoors.



### A Novel Interaction-based Methodology Towards Explainable AI with Better Understanding of Pneumonia Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2104.12672v3
- **DOI**: 10.3390/a14110337
- **Categories**: **cs.LG**, cs.CV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2104.12672v3)
- **Published**: 2021-04-19 23:02:43+00:00
- **Updated**: 2021-06-15 05:29:26+00:00
- **Authors**: Shaw-Hwa Lo, Yiqiao Yin
- **Comment**: None
- **Journal**: Algorithms 2021, 14(11), 337
- **Summary**: In the field of eXplainable AI (XAI), robust "blackbox" algorithms such as Convolutional Neural Networks (CNNs) are known for making high prediction performance. However, the ability to explain and interpret these algorithms still require innovation in the understanding of influential and, more importantly, explainable features that directly or indirectly impact the performance of predictivity. A number of methods existing in literature focus on visualization techniques but the concepts of explainability and interpretability still require rigorous definition. In view of the above needs, this paper proposes an interaction-based methodology -- Influence Score (I-score) -- to screen out the noisy and non-informative variables in the images hence it nourishes an environment with explainable and interpretable features that are directly associated to feature predictivity. We apply the proposed method on a real world application in Pneumonia Chest X-ray Image data set and produced state-of-the-art results. We demonstrate how to apply the proposed approach for more general big data problems by improving the explainability and interpretability without sacrificing the prediction performance. The contribution of this paper opens a novel angle that moves the community closer to the future pipelines of XAI problems.



