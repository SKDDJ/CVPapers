# Arxiv Papers in cs.CV on 2021-04-02
### Partition-Guided GANs
- **Arxiv ID**: http://arxiv.org/abs/2104.00816v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.00816v2)
- **Published**: 2021-04-02 00:06:53+00:00
- **Updated**: 2021-06-18 00:53:26+00:00
- **Authors**: Mohammadreza Armandpour, Ali Sadeghian, Chunyuan Li, Mingyuan Zhou
- **Comment**: Accepted for publication at CVPR 2021
- **Journal**: None
- **Summary**: Despite the success of Generative Adversarial Networks (GANs), their training suffers from several well-known problems, including mode collapse and difficulties learning a disconnected set of manifolds. In this paper, we break down the challenging task of learning complex high dimensional distributions, supporting diverse data samples, to simpler sub-tasks. Our solution relies on designing a partitioner that breaks the space into smaller regions, each having a simpler distribution, and training a different generator for each partition. This is done in an unsupervised manner without requiring any labels.   We formulate two desired criteria for the space partitioner that aid the training of our mixture of generators: 1) to produce connected partitions and 2) provide a proxy of distance between partitions and data samples, along with a direction for reducing that distance. These criteria are developed to avoid producing samples from places with non-existent data density, and also facilitate training by providing additional direction to the generators. We develop theoretical constraints for a space partitioner to satisfy the above criteria. Guided by our theoretical analysis, we design an effective neural architecture for the space partitioner that empirically assures these conditions. Experimental results on various standard benchmarks show that the proposed unsupervised model outperforms several recent methods.



### LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions
- **Arxiv ID**: http://arxiv.org/abs/2104.00820v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.00820v2)
- **Published**: 2021-04-02 00:11:22+00:00
- **Updated**: 2021-10-06 16:21:07+00:00
- **Authors**: Oğuz Kaan Yüksel, Enis Simsar, Ezgi Gülperi Er, Pinar Yanardag
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research has shown that it is possible to find interpretable directions in the latent spaces of pre-trained Generative Adversarial Networks (GANs). These directions enable controllable image generation and support a wide range of semantic editing operations, such as zoom or rotation. The discovery of such directions is often done in a supervised or semi-supervised manner and requires manual annotations which limits their use in practice. In comparison, unsupervised discovery allows finding subtle directions that are difficult to detect a priori. In this work, we propose a contrastive learning-based approach to discover semantic directions in the latent space of pre-trained GANs in a self-supervised manner. Our approach finds semantically meaningful dimensions comparable with state-of-the-art methods.



### Towards High Fidelity Face Relighting with Realistic Shadows
- **Arxiv ID**: http://arxiv.org/abs/2104.00825v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00825v2)
- **Published**: 2021-04-02 00:28:40+00:00
- **Updated**: 2021-06-05 20:55:04+00:00
- **Authors**: Andrew Hou, Ze Zhang, Michel Sarkis, Ning Bi, Yiying Tong, Xiaoming Liu
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Existing face relighting methods often struggle with two problems: maintaining the local facial details of the subject and accurately removing and synthesizing shadows in the relit image, especially hard shadows. We propose a novel deep face relighting method that addresses both problems. Our method learns to predict the ratio (quotient) image between a source image and the target image with the desired lighting, allowing us to relight the image while maintaining the local facial details. During training, our model also learns to accurately modify shadows by using estimated shadow masks to emphasize on the high-contrast shadow borders. Furthermore, we introduce a method to use the shadow mask to estimate the ambient light intensity in an image, and are thus able to leverage multiple datasets during training with different global lighting intensities. With quantitative and qualitative evaluations on the Multi-PIE and FFHQ datasets, we demonstrate that our proposed method faithfully maintains the local facial details of the subject and can accurately handle hard shadows while achieving state-of-the-art face relighting performance.



### How Are Learned Perception-Based Controllers Impacted by the Limits of Robust Control?
- **Arxiv ID**: http://arxiv.org/abs/2104.00827v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.00827v1)
- **Published**: 2021-04-02 00:31:31+00:00
- **Updated**: 2021-04-02 00:31:31+00:00
- **Authors**: Jingxi Xu, Bruce Lee, Nikolai Matni, Dinesh Jayaraman
- **Comment**: Accepted to L4DC 2021
- **Journal**: None
- **Summary**: The difficulty of optimal control problems has classically been characterized in terms of system properties such as minimum eigenvalues of controllability/observability gramians. We revisit these characterizations in the context of the increasing popularity of data-driven techniques like reinforcement learning (RL), and in control settings where input observations are high-dimensional images and transition dynamics are unknown. Specifically, we ask: to what extent are quantifiable control and perceptual difficulty metrics of a task predictive of the performance and sample complexity of data-driven controllers? We modulate two different types of partial observability in a cartpole "stick-balancing" problem -- (i) the height of one visible fixation point on the cartpole, which can be used to tune fundamental limits of performance achievable by any controller, and by (ii) the level of perception noise in the fixation point position inferred from depth or RGB images of the cartpole. In these settings, we empirically study two popular families of controllers: RL and system identification-based $H_\infty$ control, using visually estimated system state. Our results show that the fundamental limits of robust control have corresponding implications for the sample-efficiency and performance of learned perception-based controllers. Visit our project website https://jxu.ai/rl-vs-control-web for more information.



### Learning to Filter: Siamese Relation Network for Robust Tracking
- **Arxiv ID**: http://arxiv.org/abs/2104.00829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00829v1)
- **Published**: 2021-04-02 00:53:33+00:00
- **Updated**: 2021-04-02 00:53:33+00:00
- **Authors**: Siyuan Cheng, Bineng Zhong, Guorong Li, Xin Liu, Zhenjun Tang, Xianxian Li, Jing Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the great success of Siamese-based trackers, their performance under complicated scenarios is still not satisfying, especially when there are distractors. To this end, we propose a novel Siamese relation network, which introduces two efficient modules, i.e. Relation Detector (RD) and Refinement Module (RM). RD performs in a meta-learning way to obtain a learning ability to filter the distractors from the background while RM aims to effectively integrate the proposed RD into the Siamese framework to generate accurate tracking result. Moreover, to further improve the discriminability and robustness of the tracker, we introduce a contrastive training strategy that attempts not only to learn matching the same target but also to learn how to distinguish the different objects. Therefore, our tracker can achieve accurate tracking results when facing background clutters, fast motion, and occlusion. Experimental results on five popular benchmarks, including VOT2018, VOT2019, OTB100, LaSOT, and UAV123, show that the proposed method is effective and can achieve state-of-the-art results. The code will be available at https://github.com/hqucv/siamrn



### Unconstrained Face Recognition using ASURF and Cloud-Forest Classifier optimized with VLAD
- **Arxiv ID**: http://arxiv.org/abs/2104.00842v1
- **DOI**: 10.1016/j.procs.2018.10.433
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.00842v1)
- **Published**: 2021-04-02 01:26:26+00:00
- **Updated**: 2021-04-02 01:26:26+00:00
- **Authors**: A Vinay, Aviral Joshi, Hardik Mahipal Surana, Harsh Garg, K N BalasubramanyaMurthy, S Natarajan
- **Comment**: 8 Pages, 3 Figures
- **Journal**: Procedia computer science, 143, 570-578 (2018)
- **Summary**: The paper posits a computationally-efficient algorithm for multi-class facial image classification in which images are constrained with translation, rotation, scale, color, illumination and affine distortion. The proposed method is divided into five main building blocks including Haar-Cascade for face detection, Bilateral Filter for image preprocessing to remove unwanted noise, Affine Speeded-Up Robust Features (ASURF) for keypoint detection and description, Vector of Locally Aggregated Descriptors (VLAD) for feature quantization and Cloud Forest for image classification. The proposed method aims at improving the accuracy and the time taken for face recognition systems. The usage of the Cloud Forest algorithm as a classifier on three benchmark datasets, namely the FACES95, FACES96 and ORL facial datasets, showed promising results. The proposed methodology using Cloud Forest algorithm successfully improves the recognition model by 2-12\% when differentiated against other ensemble techniques like the Random Forest classifier depending upon the dataset used.



### Bridging Global Context Interactions for High-Fidelity Image Completion
- **Arxiv ID**: http://arxiv.org/abs/2104.00845v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00845v2)
- **Published**: 2021-04-02 01:42:01+00:00
- **Updated**: 2021-11-22 07:46:56+00:00
- **Authors**: Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai, Dinh Phung
- **Comment**: None
- **Journal**: None
- **Summary**: Bridging global context interactions correctly is important for high-fidelity image completion with large masks. Previous methods attempting this via deep or large receptive field (RF) convolutions cannot escape from the dominance of nearby interactions, which may be inferior. In this paper, we propose to treat image completion as a directionless sequence-to-sequence prediction task, and deploy a transformer to directly capture long-range dependence in the encoder. Crucially, we employ a restrictive CNN with small and non-overlapping RF for weighted token representation, which allows the transformer to explicitly model the long-range visible context relations with equal importance in all layers, without implicitly confounding neighboring tokens when larger RFs are used. To improve appearance consistency between visible and generated regions, a novel attention-aware layer (AAL) is introduced to better exploit distantly related high-frequency features. Overall, extensive experiments demonstrate superior performance compared to state-of-the-art methods on several datasets.



### SDAN: Squared Deformable Alignment Network for Learning Misaligned Optical Zoom
- **Arxiv ID**: http://arxiv.org/abs/2104.00848v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00848v2)
- **Published**: 2021-04-02 01:58:00+00:00
- **Updated**: 2021-11-25 21:45:08+00:00
- **Authors**: Kangfu Mei, Shenglong Ye, Rui Huang
- **Comment**: ICME21. Code is available at https://github.com/MKFMIKU/SDAN
- **Journal**: None
- **Summary**: Deep Neural Network (DNN) based super-resolution algorithms have greatly improved the quality of the generated images. However, these algorithms often yield significant artifacts when dealing with real-world super-resolution problems due to the difficulty in learning misaligned optical zoom. In this paper, we introduce a Squared Deformable Alignment Network (SDAN) to address this issue. Our network learns squared per-point offsets for convolutional kernels, and then aligns features in corrected convolutional windows based on the offsets. So the misalignment will be minimized by the extracted aligned features. Different from the per-point offsets used in the vanilla Deformable Convolutional Network (DCN), our proposed squared offsets not only accelerate the offset learning but also improve the generation quality with fewer parameters. Besides, we further propose an efficient cross packing attention layer to boost the accuracy of the learned offsets. It leverages the packing and unpacking operations to enlarge the receptive field of the offset learning and to enhance the ability of extracting the spatial connection between the low-resolution images and the referenced images. Comprehensive experiments show the superiority of our method over other state-of-the-art methods in both computational efficiency and realistic details.



### Deep ensembles based on Stochastic Activation Selection for Polyp Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.00850v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.00850v2)
- **Published**: 2021-04-02 02:07:37+00:00
- **Updated**: 2021-04-07 21:31:56+00:00
- **Authors**: Alessandra Lumini, Loris Nanni, Gianluca Maguolo
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation has a wide array of applications ranging from medical-image analysis, scene understanding, autonomous driving and robotic navigation. This work deals with medical image segmentation and in particular with accurate polyp detection and segmentation during colonoscopy examinations. Several convolutional neural network architectures have been proposed to effectively deal with this task and with the problem of segmenting objects at different scale input. The basic architecture in image segmentation consists of an encoder and a decoder: the first uses convolutional filters to extract features from the image, the second is responsible for generating the final output. In this work, we compare some variant of the DeepLab architecture obtained by varying the decoder backbone. We compare several decoder architectures, including ResNet, Xception, EfficentNet, MobileNet and we perturb their layers by substituting ReLU activation layers with other functions. The resulting methods are used to create deep ensembles which are shown to be very effective. Our experimental evaluations show that our best ensemble produces good segmentation results by achieving high evaluation scores with a dice coefficient of 0.884, and a mean Intersection over Union (mIoU) of 0.818 for the Kvasir-SEG dataset. To improve reproducibility and research efficiency the MATLAB source code used for this research is available at GitHub: https://github.com/LorisNanni.



### Estimating the Generalization in Deep Neural Networks via Sparsity
- **Arxiv ID**: http://arxiv.org/abs/2104.00851v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2104.00851v2)
- **Published**: 2021-04-02 02:10:32+00:00
- **Updated**: 2022-01-16 15:00:02+00:00
- **Authors**: Yang Zhao, Hao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Generalization is the key capability for deep neural networks (DNNs). However, it is challenging to give a reliable measure of the generalization ability of a DNN via only its nature. In this paper, we propose a novel method for estimating the generalization gap based on network sparsity. In our method, two key quantities are proposed first. They have close relationship with the generalization ability and can be calculated directly from the training results alone. Then a simple linear model involving two key quantities are constructed to give accurate estimation of the generalization gap. By training DNNs with a wide range of generalization gap on popular datasets, we show that our key quantities and linear model could be efficient tools for estimating the generalization gap of DNNs.



### The Spatially-Correlative Loss for Various Image Translation Tasks
- **Arxiv ID**: http://arxiv.org/abs/2104.00854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00854v1)
- **Published**: 2021-04-02 02:13:30+00:00
- **Updated**: 2021-04-02 02:13:30+00:00
- **Authors**: Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai
- **Comment**: 14 pages, 12 figures
- **Journal**: None
- **Summary**: We propose a novel spatially-correlative loss that is simple, efficient and yet effective for preserving scene structure consistency while supporting large appearance changes during unpaired image-to-image (I2I) translation. Previous methods attempt this by using pixel-level cycle-consistency or feature-level matching losses, but the domain-specific nature of these losses hinder translation across large domain gaps. To address this, we exploit the spatial patterns of self-similarity as a means of defining scene structure. Our spatially-correlative loss is geared towards only capturing spatial relationships within an image rather than domain appearance. We also introduce a new self-supervised learning method to explicitly learn spatially-correlative maps for each specific translation task. We show distinct improvement over baseline models in all three modes of unpaired I2I translation: single-modal, multi-modal, and even single-image translation. This new loss can easily be integrated into existing network architectures and thus allows wide applicability.



### Fully Understanding Generic Objects: Modeling, Segmentation, and Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2104.00858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00858v1)
- **Published**: 2021-04-02 02:39:29+00:00
- **Updated**: 2021-04-02 02:39:29+00:00
- **Authors**: Feng Liu, Luan Tran, Xiaoming Liu
- **Comment**: To appear in CVPR 2021
- **Journal**: None
- **Summary**: Inferring 3D structure of a generic object from a 2D image is a long-standing objective of computer vision. Conventional approaches either learn completely from CAD-generated synthetic data, which have difficulty in inference from real images, or generate 2.5D depth image via intrinsic decomposition, which is limited compared to the full 3D reconstruction. One fundamental challenge lies in how to leverage numerous real 2D images without any 3D ground truth. To address this issue, we take an alternative approach with semi-supervised learning. That is, for a 2D image of a generic object, we decompose it into latent representations of category, shape and albedo, lighting and camera projection matrix, decode the representations to segmented 3D shape and albedo respectively, and fuse these components to render an image well approximating the input image. Using a category-adaptive 3D joint occupancy field (JOF), we show that the complete shape and albedo modeling enables us to leverage real 2D images in both modeling and model fitting. The effectiveness of our approach is demonstrated through superior 3D reconstruction from a single image, being either synthetic or real, and shape segmentation.



### Self-supervised Video Representation Learning by Context and Motion Decoupling
- **Arxiv ID**: http://arxiv.org/abs/2104.00862v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.00862v1)
- **Published**: 2021-04-02 02:47:34+00:00
- **Updated**: 2021-04-02 02:47:34+00:00
- **Authors**: Lianghua Huang, Yu Liu, Bin Wang, Pan Pan, Yinghui Xu, Rong Jin
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: A key challenge in self-supervised video representation learning is how to effectively capture motion information besides context bias. While most existing works implicitly achieve this with video-specific pretext tasks (e.g., predicting clip orders, time arrows, and paces), we develop a method that explicitly decouples motion supervision from context bias through a carefully designed pretext task. Specifically, we take the keyframes and motion vectors in compressed videos (e.g., in H.264 format) as the supervision sources for context and motion, respectively, which can be efficiently extracted at over 500 fps on the CPU. Then we design two pretext tasks that are jointly optimized: a context matching task where a pairwise contrastive loss is cast between video clip and keyframe features; and a motion prediction task where clip features, passed through an encoder-decoder network, are used to estimate motion features in a near future. These two tasks use a shared video backbone and separate MLP heads. Experiments show that our approach improves the quality of the learned video representation over previous works, where we obtain absolute gains of 16.0% and 11.1% in video retrieval recall on UCF101 and HMDB51, respectively. Moreover, we find the motion prediction to be a strong regularization for video networks, where using it as an auxiliary task improves the accuracy of action recognition with a margin of 7.4%~13.8%.



### Inference of Recyclable Objects with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.00868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00868v1)
- **Published**: 2021-04-02 03:13:46+00:00
- **Updated**: 2021-04-02 03:13:46+00:00
- **Authors**: Jaime Caballero, Francisco Vergara, Randal Miranda, José Serracín
- **Comment**: 11 pages, preprint version, comments are welcome!
- **Journal**: None
- **Summary**: Population growth in the last decades has resulted in the production of about 2.01 billion tons of municipal waste per year. The current waste management systems are not capable of providing adequate solutions for the disposal and use of these wastes. Recycling and reuse have proven to be a solution to the problem, but large-scale waste segregation is a tedious task and on a small scale it depends on public awareness. This research used convolutional neural networks and computer vision to develop a tool for the automation of solid waste sorting. The Fotini10k dataset was constructed, which has more than 10,000 images divided into the categories of 'plastic bottles', 'aluminum cans' and 'paper and cardboard'. ResNet50, MobileNetV1 and MobileNetV2 were retrained with ImageNet weights on the Fotini10k dataset. As a result, top-1 accuracy of 99% was obtained in the test dataset with all three networks. To explore the possible use of these networks in mobile applications, the three nets were quantized in float16 weights. By doing so, it was possible to obtain inference times twice as low for Raspberry Pi and three times as low for computer processing units. It was also possible to reduce the size of the networks by half. When quantizing the top-1 accuracy of 99% was maintained with all three networks. When quantizing MobileNetV2 to int-8, it obtained a top-1 accuracy of 97%.



### Half-Real Half-Fake Distillation for Class-Incremental Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.00875v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00875v1)
- **Published**: 2021-04-02 03:47:16+00:00
- **Updated**: 2021-04-02 03:47:16+00:00
- **Authors**: Zilong Huang, Wentian Hao, Xinggang Wang, Mingyuan Tao, Jianqiang Huang, Wenyu Liu, Xian-Sheng Hua
- **Comment**: None
- **Journal**: None
- **Summary**: Despite their success for semantic segmentation, convolutional neural networks are ill-equipped for incremental learning, \ie, adapting the original segmentation model as new classes are available but the initial training data is not retained. Actually, they are vulnerable to catastrophic forgetting problem. We try to address this issue by "inverting" the trained segmentation network to synthesize input images starting from random noise. To avoid setting detailed pixel-wise segmentation maps as the supervision manually, we propose the SegInversion to synthesize images using the image-level labels. To increase the diversity of synthetic images, the Scale-Aware Aggregation module is integrated into SegInversion for controlling the scale (the number of pixels) of synthetic objects. Along with real images of new classes, the synthesized images will be fed into the distillation-based framework to train the new segmentation model which retains the information about previously learned classes, whilst updating the current model to learn the new ones. The proposed method significantly outperforms other incremental learning methods and obtains state-of-the-art performance on the PASCAL VOC 2012 and ADE20K datasets. The code and models will be made publicly available.



### S2R-DepthNet: Learning a Generalizable Depth-specific Structural Representation
- **Arxiv ID**: http://arxiv.org/abs/2104.00877v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00877v2)
- **Published**: 2021-04-02 03:55:41+00:00
- **Updated**: 2021-06-15 07:24:40+00:00
- **Authors**: Xiaotian Chen, Yuwang Wang, Xuejin Chen, Wenjun Zeng
- **Comment**: Accepted by CVPR2021(oral)
- **Journal**: None
- **Summary**: Human can infer the 3D geometry of a scene from a sketch instead of a realistic image, which indicates that the spatial structure plays a fundamental role in understanding the depth of scenes. We are the first to explore the learning of a depth-specific structural representation, which captures the essential feature for depth estimation and ignores irrelevant style information. Our S2R-DepthNet (Synthetic to Real DepthNet) can be well generalized to unseen real-world data directly even though it is only trained on synthetic data. S2R-DepthNet consists of: a) a Structure Extraction (STE) module which extracts a domaininvariant structural representation from an image by disentangling the image into domain-invariant structure and domain-specific style components, b) a Depth-specific Attention (DSA) module, which learns task-specific knowledge to suppress depth-irrelevant structures for better depth estimation and generalization, and c) a depth prediction module (DP) to predict depth from the depth-specific representation. Without access of any real-world images, our method even outperforms the state-of-the-art unsupervised domain adaptation methods which use real-world images of the target domain for training. In addition, when using a small amount of labeled real-world data, we achieve the state-ofthe-art performance under the semi-supervised setting. The code and trained models are available at https://github.com/microsoft/S2R-DepthNet.



### Adaptive Class Suppression Loss for Long-Tail Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.00885v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00885v1)
- **Published**: 2021-04-02 05:12:31+00:00
- **Updated**: 2021-04-02 05:12:31+00:00
- **Authors**: Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, Ming Tang
- **Comment**: CVPR2021 camera ready version
- **Journal**: None
- **Summary**: To address the problem of long-tail distribution for the large vocabulary object detection task, existing methods usually divide the whole categories into several groups and treat each group with different strategies. These methods bring the following two problems. One is the training inconsistency between adjacent categories of similar sizes, and the other is that the learned model is lack of discrimination for tail categories which are semantically similar to some of the head categories. In this paper, we devise a novel Adaptive Class Suppression Loss (ACSL) to effectively tackle the above problems and improve the detection performance of tail categories. Specifically, we introduce a statistic-free perspective to analyze the long-tail distribution, breaking the limitation of manual grouping. According to this perspective, our ACSL adjusts the suppression gradients for each sample of each class adaptively, ensuring the training consistency and boosting the discrimination for rare categories. Extensive experiments on long-tail datasets LVIS and Open Images show that the our ACSL achieves 5.18% and 5.2% improvements with ResNet50-FPN, and sets a new state of the art. Code and models are available at https://github.com/CASIA-IVA-Lab/ACSL.



### Multiple Heads are Better than One: Few-shot Font Generation with Multiple Localized Experts
- **Arxiv ID**: http://arxiv.org/abs/2104.00887v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00887v1)
- **Published**: 2021-04-02 05:20:51+00:00
- **Updated**: 2021-04-02 05:20:51+00:00
- **Authors**: Song Park, Sanghyuk Chun, Junbum Cha, Bado Lee, Hyunjung Shim
- **Comment**: 14 pages, 11 figures
- **Journal**: None
- **Summary**: A few-shot font generation (FFG) method has to satisfy two objectives: the generated images should preserve the underlying global structure of the target character and present the diverse local reference style. Existing FFG methods aim to disentangle content and style either by extracting a universal representation style or extracting multiple component-wise style representations. However, previous methods either fail to capture diverse local styles or cannot be generalized to a character with unseen components, e.g., unseen language systems. To mitigate the issues, we propose a novel FFG method, named Multiple Localized Experts Few-shot Font Generation Network (MX-Font). MX-Font extracts multiple style features not explicitly conditioned on component labels, but automatically by multiple experts to represent different local concepts, e.g., left-side sub-glyph. Owing to the multiple experts, MX-Font can capture diverse local concepts and show the generalizability to unseen languages. During training, we utilize component labels as weak supervision to guide each expert to be specialized for different local concepts. We formulate the component assign problem to each expert as the graph matching problem, and solve it by the Hungarian algorithm. We also employ the independence loss and the content-style adversarial loss to impose the content-style disentanglement. In our experiments, MX-Font outperforms previous state-of-the-art FFG methods in the Chinese generation and cross-lingual, e.g., Chinese to Korean, generation. Source code is available at https://github.com/clovaai/mxfont.



### Low Dose Helical CBCT denoising by using domain filtering with deep reinforcement learning
- **Arxiv ID**: http://arxiv.org/abs/2104.00889v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.00889v1)
- **Published**: 2021-04-02 05:28:04+00:00
- **Updated**: 2021-04-02 05:28:04+00:00
- **Authors**: Wooram Kang, Mayank Patwari
- **Comment**: Research project report. 5 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: Cone Beam Computed Tomography(CBCT) is a now known method to conduct CT imaging. Especially, The Low Dose CT imaging is one of possible options to protect organs of patients when conducting CT imaging. Therefore Low Dose CT imaging can be an alternative instead of Standard dose CT imaging. However Low Dose CT imaging has a fundamental issue with noises within results compared to Standard Dose CT imaging. Currently, there are lots of attempts to erase the noises. Most of methods with artificial intelligence have many parameters and unexplained layers or a kind of black-box methods. Therefore, our research has purposes related to these issues. Our approach has less parameters than usual methods by having Iterative learn-able bilateral filtering approach with Deep reinforcement learning. And we applied The Iterative learn-able filtering approach with deep reinforcement learning to sinograms and reconstructed volume domains. The method and the results of the method can be much more explainable than The other black box AI approaches. And we applied the method to Helical Cone Beam Computed Tomography(CBCT), which is the recent CBCT trend. We tested this method with on 2 abdominal scans(L004, L014) from Mayo Clinic TCIA dataset. The results and the performances of our approach overtake the results of the other previous methods.



### HVPR: Hybrid Voxel-Point Representation for Single-stage 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.00902v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00902v1)
- **Published**: 2021-04-02 06:34:49+00:00
- **Updated**: 2021-04-02 06:34:49+00:00
- **Authors**: Jongyoun Noh, Sanghoon Lee, Bumsub Ham
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: We address the problem of 3D object detection, that is, estimating 3D object bounding boxes from point clouds. 3D object detection methods exploit either voxel-based or point-based features to represent 3D objects in a scene. Voxel-based features are efficient to extract, while they fail to preserve fine-grained 3D structures of objects. Point-based features, on the other hand, represent the 3D structures more accurately, but extracting these features is computationally expensive. We introduce in this paper a novel single-stage 3D detection method having the merit of both voxel-based and point-based features. To this end, we propose a new convolutional neural network (CNN) architecture, dubbed HVPR, that integrates both features into a single 3D representation effectively and efficiently. Specifically, we augment the point-based features with a memory module to reduce the computational cost. We then aggregate the features in the memory, semantically similar to each voxel-based one, to obtain a hybrid 3D representation in a form of a pseudo image, allowing to localize 3D objects in a single stage efficiently. We also propose an Attentive Multi-scale Feature Module (AMFM) that extracts scale-aware features considering the sparse and irregular patterns of point clouds. Experimental results on the KITTI dataset demonstrate the effectiveness and efficiency of our approach, achieving a better compromise in terms of speed and accuracy.



### Network Quantization with Element-wise Gradient Scaling
- **Arxiv ID**: http://arxiv.org/abs/2104.00903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00903v1)
- **Published**: 2021-04-02 06:34:53+00:00
- **Updated**: 2021-04-02 06:34:53+00:00
- **Authors**: Junghyup Lee, Dohyung Kim, Bumsub Ham
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Network quantization aims at reducing bit-widths of weights and/or activations, particularly important for implementing deep neural networks with limited hardware resources. Most methods use the straight-through estimator (STE) to train quantized networks, which avoids a zero-gradient problem by replacing a derivative of a discretizer (i.e., a round function) with that of an identity function. Although quantized networks exploiting the STE have shown decent performance, the STE is sub-optimal in that it simply propagates the same gradient without considering discretization errors between inputs and outputs of the discretizer. In this paper, we propose an element-wise gradient scaling (EWGS), a simple yet effective alternative to the STE, training a quantized network better than the STE in terms of stability and accuracy. Given a gradient of the discretizer output, EWGS adaptively scales up or down each gradient element, and uses the scaled gradient as the one for the discretizer input to train quantized networks via backpropagation. The scaling is performed depending on both the sign of each gradient element and an error between the continuous input and discrete output of the discretizer. We adjust a scaling factor adaptively using Hessian information of a network. We show extensive experimental results on the image classification datasets, including CIFAR-10 and ImageNet, with diverse network architectures under a wide range of bit-width settings, demonstrating the effectiveness of our method.



### Background-Aware Pooling and Noise-Aware Loss for Weakly-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.00905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00905v1)
- **Published**: 2021-04-02 06:38:41+00:00
- **Updated**: 2021-04-02 06:38:41+00:00
- **Authors**: Youngmin Oh, Beomjun Kim, Bumsub Ham
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: We address the problem of weakly-supervised semantic segmentation (WSSS) using bounding box annotations. Although object bounding boxes are good indicators to segment corresponding objects, they do not specify object boundaries, making it hard to train convolutional neural networks (CNNs) for semantic segmentation. We find that background regions are perceptually consistent in part within an image, and this can be leveraged to discriminate foreground and background regions inside object bounding boxes. To implement this idea, we propose a novel pooling method, dubbed background-aware pooling (BAP), that focuses more on aggregating foreground features inside the bounding boxes using attention maps. This allows to extract high-quality pseudo segmentation labels to train CNNs for semantic segmentation, but the labels still contain noise especially at object boundaries. To address this problem, we also introduce a noise-aware loss (NAL) that makes the networks less susceptible to incorrect labels. Experimental results demonstrate that learning with our pseudo labels already outperforms state-of-the-art weakly- and semi-supervised methods on the PASCAL VOC 2012 dataset, and the NAL further boosts the performance.



### Datacentric analysis to reduce pedestrians accidents: A case study in Colombia
- **Arxiv ID**: http://arxiv.org/abs/2104.00912v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2104.00912v1)
- **Published**: 2021-04-02 06:59:50+00:00
- **Updated**: 2021-04-02 06:59:50+00:00
- **Authors**: Michael Puentes, Diana Novoa, John Delgado Nivia, Carlos Barrios Hernández, Oscar Carrillo, Frédéric Le Mouël
- **Comment**: None
- **Journal**: International Conference on Sustainable Smart Cities and
  Territories (SSCt2021), Apr 2021, Doha, Qatar
- **Summary**: Since 2012, in a case-study in Bucaramanga-Colombia, 179 pedestrians died in car accidents, and another 2873 pedestrians were injured. Each day, at least one passerby is involved in a tragedy. Knowing the causes to decrease accidents is crucial, and using system-dynamics to reproduce the collisions' events is critical to prevent further accidents. This work implements simulations to save lives by reducing the city's accidental rate and suggesting new safety policies to implement. Simulation's inputs are video recordings in some areas of the city. Deep Learning analysis of the images results in the segmentation of the different objects in the scene, and an interaction model identifies the primary reasons which prevail in the pedestrians or vehicles' behaviours. The first and most efficient safety policy to implement-validated by our simulations-would be to build speed bumps in specific places before the crossings reducing the accident rate by 80%.



### AAformer: Auto-Aligned Transformer for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2104.00921v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00921v2)
- **Published**: 2021-04-02 08:00:25+00:00
- **Updated**: 2021-09-10 12:08:14+00:00
- **Authors**: Kuan Zhu, Haiyun Guo, Shiliang Zhang, Yaowei Wang, Gaopan Huang, Honglin Qiao, Jing Liu, Jinqiao Wang, Ming Tang
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: In person re-identification, extracting part-level features from person images has been verified to be crucial. Most of existing CNN-based methods only locate the human parts coarsely, or rely on pre-trained human parsing models and fail in locating the identifiable non-human parts (e.g., knapsack). In this paper, we introduce an alignment scheme in Transformer architecture for the first time and propose the Auto-Aligned Transformer (AAformer) to automatically locate both the human parts and non-human ones at patch-level. We introduce the "part tokens", which are learnable vectors, to extract part features in Transformer. A part token only interacts with a local subset of patches in self-attention and learns to be the part representation. To adaptively group the image patches into different subsets, we design the Auto-Alignment. Auto-Alignment employs a fast variant of Optimal Transport algorithm to online cluster the patch embeddings into several groups with the part tokens as their prototypes. We harmoniously integrate the part alignment into the self-attention and the output part tokens can be directly used for retrieval. Extensive experiments validate the effectiveness of part tokens and the superiority of AAformer over various state-of-the-art methods.



### Video Prediction Recalling Long-term Motion Context via Memory Alignment Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.00924v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00924v1)
- **Published**: 2021-04-02 08:05:58+00:00
- **Updated**: 2021-04-02 08:05:58+00:00
- **Authors**: Sangmin Lee, Hak Gu Kim, Dae Hwi Choi, Hyung-Il Kim, Yong Man Ro
- **Comment**: CVPR 2021 (Oral)
- **Journal**: None
- **Summary**: Our work addresses long-term motion context issues for predicting future frames. To predict the future precisely, it is required to capture which long-term motion context (e.g., walking or running) the input motion (e.g., leg movement) belongs to. The bottlenecks arising when dealing with the long-term motion context are: (i) how to predict the long-term motion context naturally matching input sequences with limited dynamics, (ii) how to predict the long-term motion context with high-dimensionality (e.g., complex motion). To address the issues, we propose novel motion context-aware video prediction. To solve the bottleneck (i), we introduce a long-term motion context memory (LMC-Memory) with memory alignment learning. The proposed memory alignment learning enables to store long-term motion contexts into the memory and to match them with sequences including limited dynamics. As a result, the long-term context can be recalled from the limited input sequence. In addition, to resolve the bottleneck (ii), we propose memory query decomposition to store local motion context (i.e., low-dimensional dynamics) and recall the suitable local context for each local part of the input individually. It enables to boost the alignment effects of the memory. Experimental results show that the proposed method outperforms other sophisticated RNN-based methods, especially in long-term condition. Further, we validate the effectiveness of the proposed network designs by conducting ablation studies and memory feature analysis. The source code of this work is available.



### Landmarks Augmentation with Manifold-Barycentric Oversampling
- **Arxiv ID**: http://arxiv.org/abs/2104.00925v2
- **DOI**: 10.1109/ACCESS.2022.3219934
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00925v2)
- **Published**: 2021-04-02 08:07:21+00:00
- **Updated**: 2021-12-20 14:03:35+00:00
- **Authors**: Iaroslav Bespalov, Nazar Buzun, Oleg Kachan, Dmitry V. Dylov
- **Comment**: 11 pages, 4 figures, 3 tables. I.B. and N.B. contributed equally.
  D.V.D. is the corresponding author
- **Journal**: IEEE Access 2022
- **Summary**: The training of Generative Adversarial Networks (GANs) requires a large amount of data, stimulating the development of new augmentation methods to alleviate the challenge. Oftentimes, these methods either fail to produce enough new data or expand the dataset beyond the original manifold. In this paper, we propose a new augmentation method that guarantees to keep the new data within the original data manifold thanks to the optimal transport theory. The proposed algorithm finds cliques in the nearest-neighbors graph and, at each sampling iteration, randomly draws one clique to compute the Wasserstein barycenter with random uniform weights. These barycenters then become the new natural-looking elements that one could add to the dataset. We apply this approach to the problem of landmarks detection and augment the available annotation in both unpaired and in semi-supervised scenarios. Additionally, the idea is validated on cardiac data for the task of medical segmentation. Our approach reduces the overfitting and improves the quality metrics beyond the original data outcome and beyond the result obtained with popular modern augmentation methods.



### VisQA: X-raying Vision and Language Reasoning in Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.00926v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2104.00926v2)
- **Published**: 2021-04-02 08:08:25+00:00
- **Updated**: 2021-07-20 09:57:29+00:00
- **Authors**: Theo Jaunet, Corentin Kervadec, Romain Vuillemot, Grigory Antipov, Moez Baccouche, Christian Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Question Answering systems target answering open-ended textual questions given input images. They are a testbed for learning high-level reasoning with a primary use in HCI, for instance assistance for the visually impaired. Recent research has shown that state-of-the-art models tend to produce answers exploiting biases and shortcuts in the training data, and sometimes do not even look at the input image, instead of performing the required reasoning steps. We present VisQA, a visual analytics tool that explores this question of reasoning vs. bias exploitation. It exposes the key element of state-of-the-art neural models -- attention maps in transformers. Our working hypothesis is that reasoning steps leading to model predictions are observable from attention distributions, which are particularly useful for visualization. The design process of VisQA was motivated by well-known bias examples from the fields of deep learning and vision-language reasoning and evaluated in two ways. First, as a result of a collaboration of three fields, machine learning, vision and language reasoning, and data analytics, the work lead to a better understanding of bias exploitation of neural models for VQA, which eventually resulted in an impact on its design and training through the proposition of a method for the transfer of reasoning patterns from an oracle model. Second, we also report on the design of VisQA, and a goal-oriented evaluation of VisQA targeting the analysis of a model decision process from multiple experts, providing evidence that it makes the inner workings of models accessible to users.



### UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2104.00946v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00946v4)
- **Published**: 2021-04-02 08:54:04+00:00
- **Updated**: 2021-08-15 09:10:15+00:00
- **Authors**: Tianjiao Li, Jun Liu, Wei Zhang, Yun Ni, Wenqian Wang, Zhiheng Li
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Human behavior understanding with unmanned aerial vehicles (UAVs) is of great significance for a wide range of applications, which simultaneously brings an urgent demand of large, challenging, and comprehensive benchmarks for the development and evaluation of UAV-based models. However, existing benchmarks have limitations in terms of the amount of captured data, types of data modalities, categories of provided tasks, and diversities of subjects and environments. Here we propose a new benchmark - UAVHuman - for human behavior understanding with UAVs, which contains 67,428 multi-modal video sequences and 119 subjects for action recognition, 22,476 frames for pose estimation, 41,290 frames and 1,144 identities for person re-identification, and 22,263 frames for attribute recognition. Our dataset was collected by a flying UAV in multiple urban and rural districts in both daytime and nighttime over three months, hence covering extensive diversities w.r.t subjects, backgrounds, illuminations, weathers, occlusions, camera motions, and UAV flying attitudes. Such a comprehensive and challenging benchmark shall be able to promote the research of UAV-based human behavior understanding, including action recognition, pose estimation, re-identification, and attribute recognition. Furthermore, we propose a fisheye-based action recognition method that mitigates the distortions in fisheye videos via learning unbounded transformations guided by flat RGB videos. Experiments show the efficacy of our method on the UAV-Human dataset. The project page: https://github.com/SUTDCV/UAV-Human



### A Detector-oblivious Multi-arm Network for Keypoint Matching
- **Arxiv ID**: http://arxiv.org/abs/2104.00947v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00947v2)
- **Published**: 2021-04-02 08:55:04+00:00
- **Updated**: 2021-04-05 05:08:48+00:00
- **Authors**: Xuelun Shen, Cheng Wang, Xin Li, Qian Hu, Jingyi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a matching network to establish point correspondence between images. We propose a Multi-Arm Network (MAN) to learn region overlap and depth, which can greatly improve the keypoint matching robustness while bringing little computational cost during the inference stage. Another design that makes this framework different from many existing learning based pipelines that require re-training when a different keypoint detector is adopted, our network can directly work with different keypoint detectors without such a time-consuming re-training process. Comprehensive experiments conducted on outdoor and indoor datasets demonstrated that our proposed MAN outperforms state-of-the-art methods. Code will be made publicly available.



### Learning Transferable Kinematic Dictionary for 3D Human Pose and Shape Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2104.00953v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00953v2)
- **Published**: 2021-04-02 09:24:29+00:00
- **Updated**: 2021-04-21 00:52:19+00:00
- **Authors**: Ze Ma, Yifan Yao, Pan Ji, Chao Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating 3D human pose and shape from a single image is highly under-constrained. To address this ambiguity, we propose a novel prior, namely kinematic dictionary, which explicitly regularizes the solution space of relative 3D rotations of human joints in the kinematic tree. Integrated with a statistical human model and a deep neural network, our method achieves end-to-end 3D reconstruction without the need of using any shape annotations during the training of neural networks. The kinematic dictionary bridges the gap between in-the-wild images and 3D datasets, and thus facilitates end-to-end training across all types of datasets. The proposed method achieves competitive results on large-scale datasets including Human3.6M, MPI-INF-3DHP, and LSP, while running in real-time given the human bounding boxes.



### Variational Deep Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2104.00965v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.00965v1)
- **Published**: 2021-04-02 10:10:11+00:00
- **Updated**: 2021-04-02 10:10:11+00:00
- **Authors**: Jae Woong Soh, Nam Ik Cho
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have shown outstanding performance on image denoising with the help of large-scale datasets. Earlier methods naively trained a single CNN with many pairs of clean-noisy images. However, the conditional distribution of the clean image given a noisy one is too complicated and diverse, so that a single CNN cannot well learn such distributions. Therefore, there have also been some methods that exploit additional noise level parameters or train a separate CNN for a specific noise level parameter. These methods separate the original problem into easier sub-problems and thus have shown improved performance than the naively trained CNN. In this step, we raise two questions. The first one is whether it is an optimal approach to relate the conditional distribution only to noise level parameters. The second is what if we do not have noise level information, such as in a real-world scenario. To answer the questions and provide a better solution, we propose a novel Bayesian framework based on the variational approximation of objective functions. This enables us to separate the complicated target distribution into simpler sub-distributions. Eventually, the denoising CNN can conquer noise from each sub-distribution, which is generally an easier problem than the original. Experiments show that the proposed method provides remarkable performance on additive white Gaussian noise (AWGN) and real-noise denoising while requiring fewer parameters than recent state-of-the-art denoisers.



### TubeR: Tubelet Transformer for Video Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.00969v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00969v5)
- **Published**: 2021-04-02 10:21:22+00:00
- **Updated**: 2022-05-10 07:39:03+00:00
- **Authors**: Jiaojiao Zhao, Yanyi Zhang, Xinyu Li, Hao Chen, Shuai Bing, Mingze Xu, Chunhui Liu, Kaustav Kundu, Yuanjun Xiong, Davide Modolo, Ivan Marsic, Cees G. M. Snoek, Joseph Tighe
- **Comment**: Accepted at CVPR 2022 (Oral)
- **Journal**: None
- **Summary**: We propose TubeR: a simple solution for spatio-temporal video action detection. Different from existing methods that depend on either an off-line actor detector or hand-designed actor-positional hypotheses like proposals or anchors, we propose to directly detect an action tubelet in a video by simultaneously performing action localization and recognition from a single representation. TubeR learns a set of tubelet-queries and utilizes a tubelet-attention module to model the dynamic spatio-temporal nature of a video clip, which effectively reinforces the model capacity compared to using actor-positional hypotheses in the spatio-temporal space. For videos containing transitional states or scene changes, we propose a context aware classification head to utilize short-term and long-term context to strengthen action classification, and an action switch regression head for detecting the precise temporal action extent. TubeR directly produces action tubelets with variable lengths and even maintains good results for long video clips. TubeR outperforms the previous state-of-the-art on commonly used action detection datasets AVA, UCF101-24 and JHMDB51-21.



### Glioma Prognosis: Segmentation of the Tumor and Survival Prediction using Shape, Geometric and Clinical Information
- **Arxiv ID**: http://arxiv.org/abs/2104.00980v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.00980v1)
- **Published**: 2021-04-02 10:49:05+00:00
- **Updated**: 2021-04-02 10:49:05+00:00
- **Authors**: Mobarakol Islam, V Jeya Maria Jose, Hongliang Ren
- **Comment**: MICCAI-BrainLes Workshop
- **Journal**: None
- **Summary**: Segmentation of brain tumor from magnetic resonance imaging (MRI) is a vital process to improve diagnosis, treatment planning and to study the difference between subjects with tumor and healthy subjects. In this paper, we exploit a convolutional neural network (CNN) with hypercolumn technique to segment tumor from healthy brain tissue. Hypercolumn is the concatenation of a set of vectors which form by extracting convolutional features from multiple layers. Proposed model integrates batch normalization (BN) approach with hypercolumn. BN layers help to alleviate the internal covariate shift during stochastic gradient descent (SGD) training by zero-mean and unit variance of each mini-batch. Survival Prediction is done by first extracting features(Geometric, Fractal, and Histogram) from the segmented brain tumor data. Then, the number of days of overall survival is predicted by implementing regression on the extracted features using an artificial neural network (ANN). Our model achieves a mean dice score of 89.78%, 82.53% and 76.54% for the whole tumor, tumor core and enhancing tumor respectively in segmentation task and 67.90% in overall survival prediction task with the validation set of BraTS 2018 challenge. It obtains a mean dice accuracy of 87.315%, 77.04% and 70.22% for the whole tumor, tumor core and enhancing tumor respectively in the segmentation task and a 46.80% in overall survival prediction task in the BraTS 2018 test data set.



### Brain Tumor Segmentation and Survival Prediction using 3D Attention UNet
- **Arxiv ID**: http://arxiv.org/abs/2104.00985v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.00985v1)
- **Published**: 2021-04-02 11:04:40+00:00
- **Updated**: 2021-04-02 11:04:40+00:00
- **Authors**: Mobarakol Islam, Vibashan VS, V Jeya Maria Jose, Navodini Wijethilake, Uppal Utkarsh, Hongliang Ren
- **Comment**: MICCAI-BrainLes Workshop
- **Journal**: None
- **Summary**: In this work, we develop an attention convolutional neural network (CNN) to segment brain tumors from Magnetic Resonance Images (MRI). Further, we predict the survival rate using various machine learning methods. We adopt a 3D UNet architecture and integrate channel and spatial attention with the decoder network to perform segmentation. For survival prediction, we extract some novel radiomic features based on geometry, location, the shape of the segmented tumor and combine them with clinical information to estimate the survival duration for each patient. We also perform extensive experiments to show the effect of each feature for overall survival (OS) prediction. The experimental results infer that radiomic features such as histogram, location, and shape of the necrosis region and clinical features like age are the most critical parameters to estimate the OS.



### Visual Semantic Role Labeling for Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/2104.00990v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2104.00990v1)
- **Published**: 2021-04-02 11:23:22+00:00
- **Updated**: 2021-04-02 11:23:22+00:00
- **Authors**: Arka Sadhu, Tanmay Gupta, Mark Yatskar, Ram Nevatia, Aniruddha Kembhavi
- **Comment**: CVPR21 camera-ready including appendix. Project Page at
  https://vidsitu.org/
- **Journal**: None
- **Summary**: We propose a new framework for understanding and representing related salient events in a video using visual semantic role labeling. We represent videos as a set of related events, wherein each event consists of a verb and multiple entities that fulfill various roles relevant to that event. To study the challenging task of semantic role labeling in videos or VidSRL, we introduce the VidSitu benchmark, a large-scale video understanding data source with $29K$ $10$-second movie clips richly annotated with a verb and semantic-roles every $2$ seconds. Entities are co-referenced across events within a movie clip and events are connected to each other via event-event relations. Clips in VidSitu are drawn from a large collection of movies (${\sim}3K$) and have been chosen to be both complex (${\sim}4.2$ unique verbs within a video) as well as diverse (${\sim}200$ verbs have more than $100$ annotations each). We provide a comprehensive analysis of the dataset in comparison to other publicly available video understanding benchmarks, several illustrative baselines and evaluate a range of standard video recognition models. Our code and dataset is available at vidsitu.org.



### LiftPool: Bidirectional ConvNet Pooling
- **Arxiv ID**: http://arxiv.org/abs/2104.00996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00996v1)
- **Published**: 2021-04-02 11:46:35+00:00
- **Updated**: 2021-04-02 11:46:35+00:00
- **Authors**: Jiaojiao Zhao, Cees G. M. Snoek
- **Comment**: published on ICLR 2021
- **Journal**: None
- **Summary**: Pooling is a critical operation in convolutional neural networks for increasing receptive fields and improving robustness to input variations. Most existing pooling operations downsample the feature maps, which is a lossy process. Moreover, they are not invertible: upsampling a downscaled feature map can not recover the lost information in the downsampling. By adopting the philosophy of the classical Lifting Scheme from signal processing, we propose LiftPool for bidirectional pooling layers, including LiftDownPool and LiftUpPool. LiftDownPool decomposes a feature map into various downsized sub-bands, each of which contains information with different frequencies. As the pooling function in LiftDownPool is perfectly invertible, by performing LiftDownPool backward, a corresponding up-pooling layer LiftUpPool is able to generate a refined upsampled feature map using the detail sub-bands, which is useful for image-to-image translation challenges. Experiments show the proposed methods achieve better results on image classification and semantic segmentation, using various backbones. Moreover, LiftDownPool offers better robustness to input corruptions and perturbations.



### Toward Generating Synthetic CT Volumes using a 3D-Conditional Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2104.02060v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.02060v1)
- **Published**: 2021-04-02 12:25:37+00:00
- **Updated**: 2021-04-02 12:25:37+00:00
- **Authors**: Jayalakshmi Mangalagiri, David Chapman, Aryya Gangopadhyay, Yaacov Yesha, Joshua Galita, Sumeet Menon, Yelena Yesha, Babak Saboury, Michael Morris, Phuong Nguyen
- **Comment**: It is a short paper accepted in CSCI 2020 conference and is accepted
  to publication in the IEEE CPS proceedings
- **Journal**: None
- **Summary**: We present a novel conditional Generative Adversarial Network (cGAN) architecture that is capable of generating 3D Computed Tomography scans in voxels from noisy and/or pixelated approximations and with the potential to generate full synthetic 3D scan volumes. We believe conditional cGAN to be a tractable approach to generate 3D CT volumes, even though the problem of generating full resolution deep fakes is presently impractical due to GPU memory limitations. We present results for autoencoder, denoising, and depixelating tasks which are trained and tested on two novel COVID19 CT datasets. Our evaluation metrics, Peak Signal to Noise ratio (PSNR) range from 12.53 - 46.46 dB, and the Structural Similarity index ( SSIM) range from 0.89 to 1.



### Plot2API: Recommending Graphic API from Plot via Semantic Parsing Guided Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2104.01032v1
- **DOI**: None
- **Categories**: **cs.SE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01032v1)
- **Published**: 2021-04-02 13:08:56+00:00
- **Updated**: 2021-04-02 13:08:56+00:00
- **Authors**: Zeyu Wang, Sheng Huang, Zhongxin Liu, Meng Yan, Xin Xia, Bei Wang, Dan Yang
- **Comment**: Accepted by SANER2021
- **Journal**: None
- **Summary**: Plot-based Graphic API recommendation (Plot2API) is an unstudied but meaningful issue, which has several important applications in the context of software engineering and data visualization, such as the plotting guidance of the beginner, graphic API correlation analysis, and code conversion for plotting. Plot2API is a very challenging task, since each plot is often associated with multiple APIs and the appearances of the graphics drawn by the same API can be extremely varied due to the different settings of the parameters. Additionally, the samples of different APIs also suffer from extremely imbalanced. Considering the lack of technologies in Plot2API, we present a novel deep multi-task learning approach named Semantic Parsing Guided Neural Network (SPGNN) which translates the Plot2API issue as a multi-label image classification and an image semantic parsing tasks for the solution. In SPGNN, the recently advanced Convolutional Neural Network (CNN) named EfficientNet is employed as the backbone network for API recommendation. Meanwhile, a semantic parsing module is complemented to exploit the semantic relevant visual information in feature learning and eliminate the appearance-relevant visual information which may confuse the visual-information-based API recommendation. Moreover, the recent data augmentation technique named random erasing is also applied for alleviating the imbalance of API categories. We collect plots with the graphic APIs used to drawn them from Stack Overflow, and release three new Plot2API datasets corresponding to the graphic APIs of R and Python programming languages for evaluating the effectiveness of Plot2API techniques. Extensive experimental results not only demonstrate the superiority of our method over the recent deep learning baselines but also show the practicability of our method in the recommendation of graphic APIs.



### MOST: A Multi-Oriented Scene Text Detector with Localization Refinement
- **Arxiv ID**: http://arxiv.org/abs/2104.01070v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01070v2)
- **Published**: 2021-04-02 14:34:41+00:00
- **Updated**: 2021-04-05 08:52:47+00:00
- **Authors**: Minghang He, Minghui Liao, Zhibo Yang, Humen Zhong, Jun Tang, Wenqing Cheng, Cong Yao, Yongpan Wang, Xiang Bai
- **Comment**: Accepted by CVPR21
- **Journal**: None
- **Summary**: Over the past few years, the field of scene text detection has progressed rapidly that modern text detectors are able to hunt text in various challenging scenarios. However, they might still fall short when handling text instances of extreme aspect ratios and varying scales. To tackle such difficulties, we propose in this paper a new algorithm for scene text detection, which puts forward a set of strategies to significantly improve the quality of text localization. Specifically, a Text Feature Alignment Module (TFAM) is proposed to dynamically adjust the receptive fields of features based on initial raw detections; a Position-Aware Non-Maximum Suppression (PA-NMS) module is devised to selectively concentrate on reliable raw detections and exclude unreliable ones; besides, we propose an Instance-wise IoU loss for balanced training to deal with text instances of different scales. An extensive ablation study demonstrates the effectiveness and superiority of the proposed strategies. The resulting text detection system, which integrates the proposed strategies with a leading scene text detector EAST, achieves state-of-the-art or competitive performance on various standard benchmarks for text detection while keeping a fast running speed.



### Prediction of Tuberculosis using U-Net and segmentation techniques
- **Arxiv ID**: http://arxiv.org/abs/2104.01071v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01071v1)
- **Published**: 2021-04-02 14:35:00+00:00
- **Updated**: 2021-04-02 14:35:00+00:00
- **Authors**: Dennis Núñez-Fernández, Lamberto Ballan, Gabriel Jiménez-Avalos, Jorge Coronel, Patricia Sheen, Mirko Zimic
- **Comment**: AI for Public Health Workshop at ICLR 2021. arXiv admin note: text
  overlap with arXiv:2007.02482
- **Journal**: None
- **Summary**: One of the most serious public health problems in Peru and worldwide is Tuberculosis (TB), which is produced by a bacterium known as Mycobacterium tuberculosis. The purpose of this work is to facilitate and automate the diagnosis of tuberculosis using the MODS method and using lens-free microscopy, as it is easier to calibrate and easier to use by untrained personnel compared to lens microscopy. Therefore, we employed a U-Net network on our collected data set to perform automatic segmentation of cord shape bacterial accumulation and then predict tuberculosis. Our results show promising evidence for automatic segmentation of TB cords, and thus good accuracy for TB prediction.



### Enhancing Underwater Image via Adaptive Color and Contrast Enhancement, and Denoising
- **Arxiv ID**: http://arxiv.org/abs/2104.01073v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01073v2)
- **Published**: 2021-04-02 14:37:20+00:00
- **Updated**: 2021-08-02 04:42:02+00:00
- **Authors**: Xinjie Li, Guojia Hou, Kunqian Li, Zhenkuan Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Images captured underwater are often characterized by low contrast, color distortion, and noise. To address these visual degradations, we propose a novel scheme by constructing an adaptive color and contrast enhancement, and denoising (ACCE-D) framework for underwater image enhancement. In the proposed framework, Difference of Gaussian (DoG) filter and bilateral filter are respectively employed to decompose the high-frequency and low-frequency components. Benefited from this separation, we utilize soft-thresholding operation to suppress the noise in the high-frequency component. Specially, the low-frequency component is enhanced by using an adaptive color and contrast enhancement (ACCE) strategy. The proposed ACCE is an adaptive variational framework implemented in the HSI color space, which integrates data term and regularized term, as well as introduces Gaussian weight and Heaviside function to avoid over-enhancement and oversaturation. Moreover, we derive a numerical solution for ACCE, and adopt a pyramid-based strategy to accelerate the solving procedure. Experimental results demonstrate that our strategy is effective in color correction, visibility improvement, and detail revealing. Comparison with state-of-the-art techniques also validate the superiority of proposed method. Furthermore, we have verified the utility of our proposed ACCE-D for enhancing other types of degraded scenes, including foggy scene, sandstorm scene and low-light scene.



### End-to-end learning of keypoint detection and matching for relative pose estimation
- **Arxiv ID**: http://arxiv.org/abs/2104.01085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01085v1)
- **Published**: 2021-04-02 15:16:17+00:00
- **Updated**: 2021-04-02 15:16:17+00:00
- **Authors**: Antoine Fond, Luca Del Pero, Nikola Sivacki, Marco Paladini
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new method for estimating the relative pose between two images, where we jointly learn keypoint detection, description extraction, matching and robust pose estimation. While our architecture follows the traditional pipeline for pose estimation from geometric computer vision, all steps are learnt in an end-to-end fashion, including feature matching. We demonstrate our method for the task of visual localization of a query image within a database of images with known pose. Pairwise pose estimation has many practical applications for robotic mapping, navigation, and AR. For example, the display of persistent AR objects in the scene relies on a precise camera localization to make the digital models appear anchored to the physical environment. We train our pipeline end-to-end specifically for the problem of visual localization. We evaluate our proposed approach on localization accuracy, robustness and runtime speed. Our method achieves state of the art localization accuracy on the 7 Scenes dataset.



### Defending Against Image Corruptions Through Adversarial Augmentations
- **Arxiv ID**: http://arxiv.org/abs/2104.01086v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.01086v3)
- **Published**: 2021-04-02 15:16:39+00:00
- **Updated**: 2021-12-16 19:50:26+00:00
- **Authors**: Dan A. Calian, Florian Stimberg, Olivia Wiles, Sylvestre-Alvise Rebuffi, Andras Gyorgy, Timothy Mann, Sven Gowal
- **Comment**: None
- **Journal**: None
- **Summary**: Modern neural networks excel at image classification, yet they remain vulnerable to common image corruptions such as blur, speckle noise or fog. Recent methods that focus on this problem, such as AugMix and DeepAugment, introduce defenses that operate in expectation over a distribution of image corruptions. In contrast, the literature on $\ell_p$-norm bounded perturbations focuses on defenses against worst-case corruptions. In this work, we reconcile both approaches by proposing AdversarialAugment, a technique which optimizes the parameters of image-to-image models to generate adversarially corrupted augmented images. We theoretically motivate our method and give sufficient conditions for the consistency of its idealized version as well as that of DeepAugment. Our classifiers improve upon the state-of-the-art on common image corruption benchmarks conducted in expectation on CIFAR-10-C and improve worst-case performance against $\ell_p$-norm bounded perturbations on both CIFAR-10 and ImageNet.



### Semi-supervised Viewpoint Estimation with Geometry-aware Conditional Generation
- **Arxiv ID**: http://arxiv.org/abs/2104.01103v1
- **DOI**: 10.1007/978-3-030-66096-3_42
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01103v1)
- **Published**: 2021-04-02 15:55:27+00:00
- **Updated**: 2021-04-02 15:55:27+00:00
- **Authors**: Octave Mariotti, Hakan Bilen
- **Comment**: None
- **Journal**: ECCV 2020: Computer Vision - ECCV 2020 Workshops pp 631-647
- **Summary**: There is a growing interest in developing computer vision methods that can learn from limited supervision. In this paper, we consider the problem of learning to predict camera viewpoints, where obtaining ground-truth annotations are expensive and require special equipment, from a limited number of labeled images. We propose a semi-supervised viewpoint estimation method that can learn to infer viewpoint information from unlabeled image pairs, where two images differ by a viewpoint change. In particular our method learns to synthesize the second image by combining the appearance from the first one and viewpoint from the second one. We demonstrate that our method significantly improves the supervised techniques, especially in the low-label regime and outperforms the state-of-the-art semi-supervised methods.



### M3L: Language-based Video Editing via Multi-Modal Multi-Level Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.01122v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01122v2)
- **Published**: 2021-04-02 15:59:52+00:00
- **Updated**: 2022-03-18 20:08:30+00:00
- **Authors**: Tsu-Jui Fu, Xin Eric Wang, Scott T. Grafton, Miguel P. Eckstein, William Yang Wang
- **Comment**: CVPR'22
- **Journal**: None
- **Summary**: Video editing tools are widely used nowadays for digital design. Although the demand for these tools is high, the prior knowledge required makes it difficult for novices to get started. Systems that could follow natural language instructions to perform automatic editing would significantly improve accessibility. This paper introduces the language-based video editing (LBVE) task, which allows the model to edit, guided by text instruction, a source video into a target video. LBVE contains two features: 1) the scenario of the source video is preserved instead of generating a completely different video; 2) the semantic is presented differently in the target video, and all changes are controlled by the given instruction. We propose a Multi-Modal Multi-Level Transformer (M$^3$L) to carry out LBVE. M$^3$L dynamically learns the correspondence between video perception and language semantic at different levels, which benefits both the video understanding and video frame synthesis. We build three new datasets for evaluation, including two diagnostic and one from natural videos with human-labeled text. Extensive experimental results show that M$^3$L is effective for video editing and that LBVE can lead to a new field toward vision-and-language research.



### LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference
- **Arxiv ID**: http://arxiv.org/abs/2104.01136v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01136v2)
- **Published**: 2021-04-02 16:29:57+00:00
- **Updated**: 2021-05-06 15:25:03+00:00
- **Authors**: Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze
- **Comment**: None
- **Journal**: None
- **Summary**: We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT



### Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2104.01148v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.01148v1)
- **Published**: 2021-04-02 16:59:29+00:00
- **Updated**: 2021-04-02 16:59:29+00:00
- **Authors**: Karl Stelzner, Kristian Kersting, Adam R. Kosiorek
- **Comment**: 15 pages, 3 figures. For project page with videos, see
  http://stelzner.github.io/obsurf/
- **Journal**: None
- **Summary**: We present ObSuRF, a method which turns a single image of a scene into a 3D model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF corresponding to a different object. A single forward pass of an encoder network outputs a set of latent vectors describing the objects in the scene. These vectors are used independently to condition a NeRF decoder, defining the geometry and appearance of each object. We make learning more computationally efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs without explicit ray marching. After confirming that the model performs equal or better than state of the art on three 2D image segmentation benchmarks, we apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a novel dataset in which scenes are populated by ShapeNet models. We find that after training ObSuRF on RGB-D views of training scenes, it is capable of not only recovering the 3D geometry of a scene depicted in a single input image, but also to segment it into objects, despite receiving no supervision in that regard.



### Developing a New Autism Diagnosis Process Based on a Hybrid Deep Learning Architecture Through Analyzing Home Videos
- **Arxiv ID**: http://arxiv.org/abs/2104.01137v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.01137v1)
- **Published**: 2021-04-02 17:30:35+00:00
- **Updated**: 2021-04-02 17:30:35+00:00
- **Authors**: Spencer He, Ryan Liu
- **Comment**: 11 pages, 3 figures, 4 tables Accepted by International Conference on
  Artificial Intelligence and Machine Learning for Healthcare Applications
  (ICAIMLHA 2021)
- **Journal**: None
- **Summary**: Currently, every 1 in 54 children have been diagnosed with Autism Spectrum Disorder (ASD), which is 178% higher than it was in 2000. An early diagnosis and treatment can significantly increase the chances of going off the spectrum and making a full recovery. With a multitude of physical and behavioral tests for neurological and communication skills, diagnosing ASD is very complex, subjective, time-consuming, and expensive. We hypothesize that the use of machine learning analysis on facial features and social behavior can speed up the diagnosis of ASD without compromising real-world performance. We propose to develop a hybrid architecture using both categorical data and image data to automate traditional ASD pre-screening, which makes diagnosis a quicker and easier process. We created and tested a Logistic Regression model and a Linear Support Vector Machine for Module 1, which classifies ADOS categorical data. A Convolutional Neural Network and a DenseNet network are used for module 2, which classifies video data. Finally, we combined the best performing models, a Linear SVM and DenseNet, using three data averaging strategies. We used a standard average, weighted based on number of training data, and weighted based on the number of ASD patients in the training data to average the results, thereby increasing accuracy in clinical applications. The results we obtained support our hypothesis. Our novel architecture is able to effectively automate ASD pre-screening with a maximum weighted accuracy of 84%.



### On the Pitfalls of Learning with Limited Data: A Facial Expression Recognition Case Study
- **Arxiv ID**: http://arxiv.org/abs/2104.02653v1
- **DOI**: 10.1016/j.eswa.2021.114991
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.02653v1)
- **Published**: 2021-04-02 18:53:41+00:00
- **Updated**: 2021-04-02 18:53:41+00:00
- **Authors**: Miguel Rodríguez Santander, Juan Hernández Albarracín, Adín Ramírez Rivera
- **Comment**: To appear in Expert Systems with Applications
- **Journal**: Expert Syst. Appl. 2021, 18 (1) 114991
- **Summary**: Deep learning models need large amounts of data for training. In video recognition and classification, significant advances were achieved with the introduction of new large databases. However, the creation of large-databases for training is infeasible in several scenarios. Thus, existing or small collected databases are typically joined and amplified to train these models. Nevertheless, training neural networks on limited data is not straightforward and comes with a set of problems. In this paper, we explore the effects of stacking databases, model initialization, and data amplification techniques when training with limited data on deep learning models' performance. We focused on the problem of Facial Expression Recognition from videos. We performed an extensive study with four databases at a different complexity and nine deep-learning architectures for video classification. We found that (i) complex training sets translate better to more stable test sets when trained with transfer learning and synthetically generated data, but their performance yields a high variance; (ii) training with more detailed data translates to more stable performance on novel scenarios (albeit with lower performance); (iii) merging heterogeneous data is not a straightforward improvement, as the type of augmentation and initialization is crucial; (iv) classical data augmentation cannot fill the holes created by joining largely separated datasets; and (v) inductive biases help to bridge the gap when paired with synthetic data, but this data is not enough when working with standard initialization techniques.



### Beyond Short Clips: End-to-End Video-Level Learning with Collaborative Memories
- **Arxiv ID**: http://arxiv.org/abs/2104.01198v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01198v1)
- **Published**: 2021-04-02 18:59:09+00:00
- **Updated**: 2021-04-02 18:59:09+00:00
- **Authors**: Xitong Yang, Haoqi Fan, Lorenzo Torresani, Larry Davis, Heng Wang
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: The standard way of training video models entails sampling at each iteration a single clip from a video and optimizing the clip prediction with respect to the video-level label. We argue that a single clip may not have enough temporal coverage to exhibit the label to recognize, since video datasets are often weakly labeled with categorical information but without dense temporal annotations. Furthermore, optimizing the model over brief clips impedes its ability to learn long-term temporal dependencies. To overcome these limitations, we introduce a collaborative memory mechanism that encodes information across multiple sampled clips of a video at each training iteration. This enables the learning of long-range dependencies beyond a single clip. We explore different design choices for the collaborative memory to ease the optimization difficulties. Our proposed framework is end-to-end trainable and significantly improves the accuracy of video classification at a negligible computational overhead. Through extensive experiments, we demonstrate that our framework generalizes to different video architectures and tasks, outperforming the state of the art on both action recognition (e.g., Kinetics-400 & 700, Charades, Something-Something-V1) and action detection (e.g., AVA v2.1 & v2.2).



### Uncertainty-Aware Annotation Protocol to Evaluate Deformable Registration Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2104.01217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01217v1)
- **Published**: 2021-04-02 19:31:19+00:00
- **Updated**: 2021-04-02 19:31:19+00:00
- **Authors**: Loic Peter, Daniel C. Alexander, Caroline Magnain, Juan Eugenio Iglesias
- **Comment**: Accepted at IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Landmark correspondences are a widely used type of gold standard in image registration. However, the manual placement of corresponding points is subject to high inter-user variability in the chosen annotated locations and in the interpretation of visual ambiguities. In this paper, we introduce a principled strategy for the construction of a gold standard in deformable registration. Our framework: (i) iteratively suggests the most informative location to annotate next, taking into account its redundancy with previous annotations; (ii) extends traditional pointwise annotations by accounting for the spatial uncertainty of each annotation, which can either be directly specified by the user, or aggregated from pointwise annotations from multiple experts; and (iii) naturally provides a new strategy for the evaluation of deformable registration algorithms. Our approach is validated on four different registration tasks. The experimental results show the efficacy of suggesting annotations according to their informativeness, and an improved capacity to assess the quality of the outputs of registration algorithms. In addition, our approach yields, from sparse annotations only, a dense visualization of the errors made by a registration method. The source code of our approach supporting both 2D and 3D data is publicly available at https://github.com/LoicPeter/evaluation-deformable-registration.



### Diverse Gaussian Noise Consistency Regularization for Robustness and Uncertainty Calibration
- **Arxiv ID**: http://arxiv.org/abs/2104.01231v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01231v6)
- **Published**: 2021-04-02 20:25:53+00:00
- **Updated**: 2023-05-29 15:06:24+00:00
- **Authors**: Theodoros Tsiligkaridis, Athanasios Tsiligkaridis
- **Comment**: Accepted to IJCNN 2023. Preliminary version accepted to ICML 2021
  Uncertainty & Robustness in Deep Learning Workshop
- **Journal**: None
- **Summary**: Deep neural networks achieve high prediction accuracy when the train and test distributions coincide. In practice though, various types of corruptions occur which deviate from this setup and cause severe performance degradations. Few methods have been proposed to address generalization in the presence of unforeseen domain shifts. In particular, digital noise corruptions arise commonly in practice during the image acquisition stage and present a significant challenge for current methods. In this paper, we propose a diverse Gaussian noise consistency regularization method for improving robustness of image classifiers under a variety of corruptions while still maintaining high clean accuracy. We derive bounds to motivate and understand the behavior of our Gaussian noise consistency regularization using a local loss landscape analysis. Our approach improves robustness against unforeseen noise corruptions by 4.2-18.4% over adversarial training and other strong diverse data augmentation baselines across several benchmarks. Furthermore, it improves robustness and uncertainty calibration by 3.7% and 5.5%, respectively, against all common corruptions (weather, digital, blur, noise) when combined with state-of-the-art diverse data augmentations.



### Malignancy Prediction and Lesion Identification from Clinical Dermatological Images
- **Arxiv ID**: http://arxiv.org/abs/2104.02652v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.02652v1)
- **Published**: 2021-04-02 20:52:05+00:00
- **Updated**: 2021-04-02 20:52:05+00:00
- **Authors**: Meng Xia, Meenal K. Kheterpal, Samantha C. Wong, Christine Park, William Ratliff, Lawrence Carin, Ricardo Henao
- **Comment**: None
- **Journal**: None
- **Summary**: We consider machine-learning-based malignancy prediction and lesion identification from clinical dermatological images, which can be indistinctly acquired via smartphone or dermoscopy capture. Additionally, we do not assume that images contain single lesions, thus the framework supports both focal or wide-field images. Specifically, we propose a two-stage approach in which we first identify all lesions present in the image regardless of sub-type or likelihood of malignancy, then it estimates their likelihood of malignancy, and through aggregation, it also generates an image-level likelihood of malignancy that can be used for high-level screening processes. Further, we consider augmenting the proposed approach with clinical covariates (from electronic health records) and publicly available data (the ISIC dataset). Comprehensive experiments validated on an independent test dataset demonstrate that i) the proposed approach outperforms alternative model architectures; ii) the model based on images outperforms a pure clinical model by a large margin, and the combination of images and clinical data does not significantly improves over the image-only model; and iii) the proposed framework offers comparable performance in terms of malignancy classification relative to three board certified dermatologists with different levels of experience.



### Unsupervised Discovery of the Long-Tail in Instance Segmentation Using Hierarchical Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2104.01257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01257v1)
- **Published**: 2021-04-02 22:05:03+00:00
- **Updated**: 2021-04-02 22:05:03+00:00
- **Authors**: Zhenzhen Weng, Mehmet Giray Ogut, Shai Limonchik, Serena Yeung
- **Comment**: None
- **Journal**: None
- **Summary**: Instance segmentation is an active topic in computer vision that is usually solved by using supervised learning approaches over very large datasets composed of object level masks. Obtaining such a dataset for any new domain can be very expensive and time-consuming. In addition, models trained on certain annotated categories do not generalize well to unseen objects. The goal of this paper is to propose a method that can perform unsupervised discovery of long-tail categories in instance segmentation, through learning instance embeddings of masked regions. Leveraging rich relationship and hierarchical structure between objects in the images, we propose self-supervised losses for learning mask embeddings. Trained on COCO dataset without additional annotations of the long-tail objects, our model is able to discover novel and more fine-grained objects than the common categories in COCO. We show that the model achieves competitive quantitative results on LVIS as compared to the supervised and partially supervised methods.



### Robotic Waste Sorter with Agile Manipulation and Quickly Trainable Detector
- **Arxiv ID**: http://arxiv.org/abs/2104.01260v2
- **DOI**: 10.1109/ACCESS.2021.3110795
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01260v2)
- **Published**: 2021-04-02 22:19:34+00:00
- **Updated**: 2021-09-04 07:13:20+00:00
- **Authors**: Takuya Kiyokawa, Hiroki Katayama, Yuya Tatsuta, Jun Takamatsu, Tsukasa Ogasawara
- **Comment**: 15 pages, 18 figures
- **Journal**: None
- **Summary**: Owing to human labor shortages, the automation of labor-intensive manual waste-sorting is needed. The goal of automating waste-sorting is to replace the human role of robust detection and agile manipulation of waste items with robots. To achieve this, we propose three methods. First, we provide a combined manipulation method using graspless push-and-drop and pick-and-release manipulation. Second, we provide a robotic system that can automatically collect object images to quickly train a deep neural-network model. Third, we provide a method to mitigate the differences in the appearance of target objects from two scenes: one for dataset collection and the other for waste sorting in a recycling factory. If differences exist, the performance of a trained waste detector may decrease. We address differences in illumination and background by applying object scaling, histogram matching with histogram equalization, and background synthesis to the source target-object images. Via experiments in an indoor experimental workplace for waste-sorting, we confirm that the proposed methods enable quick collection of the training image sets for three classes of waste items (i.e., aluminum can, glass bottle, and plastic bottle) and detection with higher performance than the methods that do not consider the differences. We also confirm that the proposed method enables the robot quickly manipulate the objects.



### Optical Flow Dataset Synthesis from Unpaired Images
- **Arxiv ID**: http://arxiv.org/abs/2104.02615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.02615v1)
- **Published**: 2021-04-02 22:19:47+00:00
- **Updated**: 2021-04-02 22:19:47+00:00
- **Authors**: Adrian Wälchli, Paolo Favaro
- **Comment**: None
- **Journal**: None
- **Summary**: The estimation of optical flow is an ambiguous task due to the lack of correspondence at occlusions, shadows, reflections, lack of texture and changes in illumination over time. Thus, unsupervised methods face major challenges as they need to tune complex cost functions with several terms designed to handle each of these sources of ambiguity. In contrast, supervised methods avoid these challenges altogether by relying on explicit ground truth optical flow obtained directly from synthetic or real data. In the case of synthetic data, the ground truth provides an exact and explicit description of what optical flow to assign to a given scene. However, the domain gap between synthetic data and real data often limits the ability of a trained network to generalize. In the case of real data, the ground truth is obtained through multiple sensors and additional data processing, which might introduce persistent errors and contaminate it. As a solution to these issues, we introduce a novel method to build a training set of pseudo-real images that can be used to train optical flow in a supervised manner. Our dataset uses two unpaired frames from real data and creates pairs of frames by simulating random warps, occlusions with super-pixels, shadows and illumination changes, and associates them to their corresponding exact optical flow. We thus obtain the benefit of directly training on real data while having access to an exact ground truth. Training with our datasets on the Sintel and KITTI benchmarks is straightforward and yields models on par or with state of the art performance compared to much more sophisticated training approaches.



### A Semantic Segmentation Network for Urban-Scale Building Footprint Extraction Using RGB Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2104.01263v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01263v2)
- **Published**: 2021-04-02 22:32:04+00:00
- **Updated**: 2021-11-19 04:11:44+00:00
- **Authors**: Aatif Jiwani, Shubhrakanti Ganguly, Chao Ding, Nan Zhou, David M. Chan
- **Comment**: 11 pages, 5 figures. Code available at
  https://github.com/aatifjiwani/rgb-footprint-extract/
- **Journal**: None
- **Summary**: Urban areas consume over two-thirds of the world's energy and account for more than 70 percent of global CO2 emissions. As stated in IPCC's Global Warming of 1.5C report, achieving carbon neutrality by 2050 requires a clear understanding of urban geometry. High-quality building footprint generation from satellite images can accelerate this predictive process and empower municipal decision-making at scale. However, previous Deep Learning-based approaches face consequential issues such as scale invariance and defective footprints, partly due to ever-present class-wise imbalance. Additionally, most approaches require supplemental data such as point cloud data, building height information, and multi-band imagery - which has limited availability and are tedious to produce. In this paper, we propose a modified DeeplabV3+ module with a Dilated Res-Net backbone to generate masks of building footprints from three-channel RGB satellite imagery only. Furthermore, we introduce an F-Beta measure in our objective function to help the model account for skewed class distributions and prevent false-positive footprints. In addition to F-Beta, we incorporate an exponentially weighted boundary loss and use a cross-dataset training strategy to further increase the quality of predictions. As a result, we achieve state-of-the-art performances across three public benchmarks and demonstrate that our RGB-only method produces higher quality visual results and is agnostic to the scale, resolution, and urban density of satellite imagery.



### Multi-class motion-based semantic segmentation for ureteroscopy and laser lithotripsy
- **Arxiv ID**: http://arxiv.org/abs/2104.01268v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01268v1)
- **Published**: 2021-04-02 22:47:21+00:00
- **Updated**: 2021-04-02 22:47:21+00:00
- **Authors**: Soumya Gupta, Sharib Ali, Louise Goldsmith, Ben Turney, Jens Rittscher
- **Comment**: 18 pages, 5 figures
- **Journal**: None
- **Summary**: Kidney stones represent a considerable burden for public health-care systems. Ureteroscopy with laser lithotripsy has evolved as the most commonly used technique for the treatment of kidney stones. Automated segmentation of kidney stones and laser fiber is an important initial step to performing any automated quantitative analysis of the stones, particularly stone-size estimation, that helps the surgeon decide if the stone requires more fragmentation. Factors such as turbid fluid inside the cavity, specularities, motion blur due to kidney movements and camera motion, bleeding, and stone debris impact the quality of vision within the kidney and lead to extended operative times. To the best of our knowledge, this is the first attempt made towards multi-class segmentation in ureteroscopy and laser lithotripsy data. We propose an end-to-end CNN-based framework for the segmentation of stones and laser fiber. The proposed approach utilizes two sub-networks: HybResUNet, a version of residual U-Net, that uses residual connections in the encoder path of U-Net and a DVFNet that generates DVF predictions which are then used to prune the prediction maps. We also present ablation studies that combine dilated convolutions, recurrent and residual connections, ASPP and attention gate. We propose a compound loss function that improves our segmentation performance. We have also provided an ablation study to determine the optimal data augmentation strategy. Our qualitative and quantitative results illustrate that our proposed method outperforms SOTA methods such as UNet and DeepLabv3+ showing an improvement of 5.2% and 15.93%, respectively, for the combined mean of DSC and JI in our invivo test dataset. We also show that our proposed model generalizes better on a new clinical dataset showing a mean improvement of 25.4%, 20%, and 11% over UNet, HybResUNet, and DeepLabv3+, respectively, for the same metric.



