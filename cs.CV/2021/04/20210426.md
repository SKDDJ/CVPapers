# Arxiv Papers in cs.CV on 2021-04-26
### Wise-SrNet: A Novel Architecture for Enhancing Image Classification by Learning Spatial Resolution of Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/2104.12294v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.12294v1)
- **Published**: 2021-04-26 00:37:11+00:00
- **Updated**: 2021-04-26 00:37:11+00:00
- **Authors**: Mohammad Rahimzadeh, Soroush Parvin, Elnaz Safi, Mohammad Reza Mohammadi
- **Comment**: The code is shared at
  https://github.com/mr7495/image-classification-spatial
- **Journal**: None
- **Summary**: One of the main challenges since the advancement of convolutional neural networks is how to connect the extracted feature map to the final classification layer. VGG models used two sets of fully connected layers for the classification part of their architectures, which significantly increases the number of models' weights. ResNet and next deep convolutional models used the Global Average Pooling (GAP) layer to compress the feature map and feed it to the classification layer. Although using the GAP layer reduces the computational cost, but also causes losing spatial resolution of the feature map, which results in decreasing learning efficiency. In this paper, we aim to tackle this problem by replacing the GAP layer with a new architecture called Wise-SrNet. It is inspired by the depthwise convolutional idea and is designed for processing spatial resolution and also not increasing computational cost. We have evaluated our method using three different datasets: Intel Image Classification Challenge, MIT Indoors Scenes, and a part of the ImageNet dataset. We investigated the implementation of our architecture on several models of Inception, ResNet and DensNet families. Applying our architecture has revealed a significant effect on increasing convergence speed and accuracy. Our Experiments on images with 224x224 resolution increased the Top-1 accuracy between 2% to 8% on different datasets and models. Running our models on 512x512 resolution images of the MIT Indoors Scenes dataset showed a notable result of improving the Top-1 accuracy within 3% to 26%. We will also demonstrate the GAP layer's disadvantage when the input images are large and the number of classes is not few. In this circumstance, our proposed architecture can do a great help in enhancing classification results. The code is shared at https://github.com/mr7495/image-classification-spatial.



### dualFace:Two-Stage Drawing Guidance for Freehand Portrait Sketching
- **Arxiv ID**: http://arxiv.org/abs/2104.12297v1
- **DOI**: 10.1007/s41095-021-0227-7
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.12297v1)
- **Published**: 2021-04-26 00:56:37+00:00
- **Updated**: 2021-04-26 00:56:37+00:00
- **Authors**: Zhengyu Huang, Yichen Peng, Tomohiro Hibino, Chunqi Zhao, Haoran Xie, Tsukasa Fukusato, Kazunori Miyata
- **Comment**: Accepted in the Journal of Computational Visual Media Conference
  2021. 13 pages, 12 figures
- **Journal**: None
- **Summary**: In this paper, we propose dualFace, a portrait drawing interface to assist users with different levels of drawing skills to complete recognizable and authentic face sketches. dualFace consists of two-stage drawing assistance to provide global and local visual guidance: global guidance, which helps users draw contour lines of portraits (i.e., geometric structure), and local guidance, which helps users draws details of facial parts (which conform to user-drawn contour lines), inspired by traditional artist workflows in portrait drawing. In the stage of global guidance, the user draws several contour lines, and dualFace then searches several relevant images from an internal database and displays the suggested face contour lines over the background of the canvas. In the stage of local guidance, we synthesize detailed portrait images with a deep generative model from user-drawn contour lines, but use the synthesized results as detailed drawing guidance. We conducted a user study to verify the effectiveness of dualFace, and we confirmed that dualFace significantly helps achieve a detailed portrait sketch. see http://www.jaist.ac.jp/~xie/dualface.html



### ODDObjects: A Framework for Multiclass Unsupervised Anomaly Detection on Masked Objects
- **Arxiv ID**: http://arxiv.org/abs/2104.12300v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.12300v1)
- **Published**: 2021-04-26 01:13:28+00:00
- **Updated**: 2021-04-26 01:13:28+00:00
- **Authors**: Ricky Ma
- **Comment**: 11 pages, 15 Postscript figures
- **Journal**: None
- **Summary**: This paper presents a novel framework for unsupervised anomaly detection on masked objects called ODDObjects, which stands for Out-of-Distribution Detection on Objects. ODDObjects is designed to detect anomalies of various categories using unsupervised autoencoders trained on COCO-style datasets. The method utilizes autoencoder-based image reconstruction, where high reconstruction error indicates the possibility of an anomaly. The framework extends previous work on anomaly detection with autoencoders, comparing state-of-the-art models trained on object recognition datasets. Various model architectures were compared, and experimental results show that memory-augmented deep convolutional autoencoders perform the best at detecting out-of-distribution objects.



### Diverse Image Inpainting with Bidirectional and Autoregressive Transformers
- **Arxiv ID**: http://arxiv.org/abs/2104.12335v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12335v3)
- **Published**: 2021-04-26 03:52:27+00:00
- **Updated**: 2021-06-01 04:10:48+00:00
- **Authors**: Yingchen Yu, Fangneng Zhan, Rongliang Wu, Jianxiong Pan, Kaiwen Cui, Shijian Lu, Feiying Ma, Xuansong Xie, Chunyan Miao
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: Image inpainting is an underdetermined inverse problem, which naturally allows diverse contents to fill up the missing or corrupted regions realistically. Prevalent approaches using convolutional neural networks (CNNs) can synthesize visually pleasant contents, but CNNs suffer from limited perception fields for capturing global features. With image-level attention, transformers enable to model long-range dependencies and generate diverse contents with autoregressive modeling of pixel-sequence distributions. However, the unidirectional attention in autoregressive transformers is suboptimal as corrupted image regions may have arbitrary shapes with contexts from any direction. We propose BAT-Fill, an innovative image inpainting framework that introduces a novel bidirectional autoregressive transformer (BAT) for image inpainting. BAT utilizes the transformers to learn autoregressive distributions, which naturally allows the diverse generation of missing contents. In addition, it incorporates the masked language model like BERT, which enables bidirectionally modeling of contextual information of missing regions for better image completion. Extensive experiments over multiple datasets show that BAT-Fill achieves superior diversity and fidelity in image inpainting qualitatively and quantitatively.



### Machine Learning-based Lie Detector applied to a Novel Annotated Game Dataset
- **Arxiv ID**: http://arxiv.org/abs/2104.12345v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2104.12345v2)
- **Published**: 2021-04-26 04:48:42+00:00
- **Updated**: 2021-06-30 04:00:37+00:00
- **Authors**: Nuria Rodriguez-Diaz, Decky Aspandi, Federico Sukno, Xavier Binefa
- **Comment**: None
- **Journal**: None
- **Summary**: Lie detection is considered a concern for everyone in their day to day life given its impact on human interactions. Thus, people normally pay attention to both what their interlocutors are saying and also to their visual appearances, including faces, to try to find any signs that indicate whether the person is telling the truth or not. While automatic lie detection may help us to understand this lying characteristics, current systems are still fairly limited, partly due to lack of adequate datasets to evaluate their performance in realistic scenarios. In this work, we have collected an annotated dataset of facial images, comprising both 2D and 3D information of several participants during a card game that encourages players to lie. Using our collected dataset, We evaluated several types of machine learning-based lie detectors in terms of their generalization, person-specific and cross-domain experiments. Our results show that models based on deep learning achieve the best accuracy, reaching up to 57\% for the generalization task and 63\% when dealing with a single participant. Finally, we also highlight the limitation of the deep learning based lie detector when dealing with cross-domain lie detection tasks.



### Dynamic Image Restoration and Fusion Based on Dynamic Degradation
- **Arxiv ID**: http://arxiv.org/abs/2104.12347v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12347v3)
- **Published**: 2021-04-26 04:59:55+00:00
- **Updated**: 2021-04-30 12:02:03+00:00
- **Authors**: Aiqing Fang, Xinbo Zhao, Jiaqi Yang, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The deep-learning-based image restoration and fusion methods have achieved remarkable results. However, the existing restoration and fusion methods paid little research attention to the robustness problem caused by dynamic degradation. In this paper, we propose a novel dynamic image restoration and fusion neural network, termed as DDRF-Net, which is capable of solving two problems, i.e., static restoration and fusion, dynamic degradation. In order to solve the static fusion problem of existing methods, dynamic convolution is introduced to learn dynamic restoration and fusion weights. In addition, a dynamic degradation kernel is proposed to improve the robustness of image restoration and fusion. Our network framework can effectively combine image degradation with image fusion tasks, provide more detailed information for image fusion tasks through image restoration loss, and optimize image restoration tasks through image fusion loss. Therefore, the stumbling blocks of deep learning in image fusion, e.g., static fusion weight and specifically designed network architecture, are greatly mitigated. Extensive experiments show that our method is more superior compared with the state-of-the-art methods.



### VCGAN: Video Colorization with Hybrid Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2104.12357v2
- **DOI**: 10.1109/TMM.2022.3154600
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.12357v2)
- **Published**: 2021-04-26 05:50:53+00:00
- **Updated**: 2023-05-07 14:22:31+00:00
- **Authors**: Yuzhi Zhao, Lai-Man Po, Wing-Yin Yu, Yasar Abbas Ur Rehman, Mengyang Liu, Yujia Zhang, Weifeng Ou
- **Comment**: accepted by IEEE Transactions on Multimedia (TMM)
- **Journal**: IEEE Transactions on Multimedia, 2022
- **Summary**: We propose a hybrid recurrent Video Colorization with Hybrid Generative Adversarial Network (VCGAN), an improved approach to video colorization using end-to-end learning. The VCGAN addresses two prevalent issues in the video colorization domain: Temporal consistency and unification of colorization network and refinement network into a single architecture. To enhance colorization quality and spatiotemporal consistency, the mainstream of generator in VCGAN is assisted by two additional networks, i.e., global feature extractor and placeholder feature extractor, respectively. The global feature extractor encodes the global semantics of grayscale input to enhance colorization quality, whereas the placeholder feature extractor acts as a feedback connection to encode the semantics of the previous colorized frame in order to maintain spatiotemporal consistency. If changing the input for placeholder feature extractor as grayscale input, the hybrid VCGAN also has the potential to perform image colorization. To improve the consistency of far frames, we propose a dense long-term loss that smooths the temporal disparity of every two remote frames. Trained with colorization and temporal losses jointly, VCGAN strikes a good balance between color vividness and video continuity. Experimental results demonstrate that VCGAN produces higher-quality and temporally more consistent colorful videos than existing approaches.



### Recalibration of Aleatoric and Epistemic Regression Uncertainty in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2104.12376v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.12376v1)
- **Published**: 2021-04-26 07:18:58+00:00
- **Updated**: 2021-04-26 07:18:58+00:00
- **Authors**: Max-Heinrich Laves, Sontje Ihler, Jacob F. Fast, Lüder A. Kahrs, Tobias Ortmaier
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org
- **Journal**: None
- **Summary**: The consideration of predictive uncertainty in medical imaging with deep learning is of utmost importance. We apply estimation of both aleatoric and epistemic uncertainty by variational Bayesian inference with Monte Carlo dropout to regression tasks and show that predictive uncertainty is systematically underestimated. We apply $ \sigma $ scaling with a single scalar value; a simple, yet effective calibration method for both types of uncertainty. The performance of our approach is evaluated on a variety of common medical regression data sets using different state-of-the-art convolutional network architectures. In our experiments, $ \sigma $ scaling is able to reliably recalibrate predictive uncertainty. It is easy to implement and maintains the accuracy. Well-calibrated uncertainty in regression allows robust rejection of unreliable predictions or detection of out-of-distribution samples. Our source code is available at https://github.com/mlaves/well-calibrated-regression-uncertainty



### Delving into Data: Effectively Substitute Training for Black-box Attack
- **Arxiv ID**: http://arxiv.org/abs/2104.12378v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12378v1)
- **Published**: 2021-04-26 07:26:29+00:00
- **Updated**: 2021-04-26 07:26:29+00:00
- **Authors**: Wenxuan Wang, Bangjie Yin, Taiping Yao, Li Zhang, Yanwei Fu, Shouhong Ding, Jilin Li, Feiyue Huang, Xiangyang Xue
- **Comment**: 10 pages, 6 figures, 6 tables, 1 algorithm, To appear in CVPR 2021 as
  a poster paper
- **Journal**: None
- **Summary**: Deep models have shown their vulnerability when processing adversarial samples. As for the black-box attack, without access to the architecture and weights of the attacked model, training a substitute model for adversarial attacks has attracted wide attention. Previous substitute training approaches focus on stealing the knowledge of the target model based on real training data or synthetic data, without exploring what kind of data can further improve the transferability between the substitute and target models. In this paper, we propose a novel perspective substitute training that focuses on designing the distribution of data used in the knowledge stealing process. More specifically, a diverse data generation module is proposed to synthesize large-scale data with wide distribution. And adversarial substitute training strategy is introduced to focus on the data distributed near the decision boundary. The combination of these two modules can further boost the consistency of the substitute model and target model, which greatly improves the effectiveness of adversarial attack. Extensive experiments demonstrate the efficacy of our method against state-of-the-art competitors under non-target and target attack settings. Detailed visualization and analysis are also provided to help understand the advantage of our method.



### Variational Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.12389v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.12389v1)
- **Published**: 2021-04-26 08:06:41+00:00
- **Updated**: 2021-04-26 08:06:41+00:00
- **Authors**: Yuang Zhang, Huanyu He, Jianguo Li, Yuxi Li, John See, Weiyao Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrian detection in a crowd is a challenging task due to a high number of mutually-occluding human instances, which brings ambiguity and optimization difficulties to the current IoU-based ground truth assignment procedure in classical object detection methods. In this paper, we develop a unique perspective of pedestrian detection as a variational inference problem. We formulate a novel and efficient algorithm for pedestrian detection by modeling the dense proposals as a latent variable while proposing a customized Auto Encoding Variational Bayes (AEVB) algorithm. Through the optimization of our proposed algorithm, a classical detector can be fashioned into a variational pedestrian detector. Experiments conducted on CrowdHuman and CityPersons datasets show that the proposed algorithm serves as an efficient solution to handle the dense pedestrian detection problem for the case of single-stage detectors. Our method can also be flexibly applied to two-stage detectors, achieving notable performance enhancement.



### Spherical formulation of geometric motion segmentation constraints in fisheye cameras
- **Arxiv ID**: http://arxiv.org/abs/2104.12404v1
- **DOI**: 10.1109/TITS.2020.3042759
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.12404v1)
- **Published**: 2021-04-26 08:48:12+00:00
- **Updated**: 2021-04-26 08:48:12+00:00
- **Authors**: Letizia Mariotti, Ciaran Eising
- **Comment**: arXiv admin note: text overlap with arXiv:2003.03262
- **Journal**: IEEE Transactions on Intelligent Transportation Systems, Volume
  23, Issue 5, May 2022, pp. 4201 - 4211
- **Summary**: We introduce a visual motion segmentation method employing spherical geometry for fisheye cameras and automoated driving. Three commonly used geometric constraints in pin-hole imagery (the positive height, positive depth and epipolar constraints) are reformulated to spherical coordinates, making them invariant to specific camera configurations as long as the camera calibration is known. A fourth constraint, known as the anti-parallel constraint, is added to resolve motion-parallax ambiguity, to support the detection of moving objects undergoing parallel or near-parallel motion with respect to the host vehicle. A final constraint constraint is described, known as the spherical three-view constraint, is described though not employed in our proposed algorithm. Results are presented and analyzed that demonstrate that the proposal is an effective motion segmentation approach for direct employment on fisheye imagery.



### Model Guided Road Intersection Classification
- **Arxiv ID**: http://arxiv.org/abs/2104.12417v1
- **DOI**: 10.1109/IV48863.2021.9575605
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.12417v1)
- **Published**: 2021-04-26 09:15:28+00:00
- **Updated**: 2021-04-26 09:15:28+00:00
- **Authors**: Augusto Luis Ballardini, Álvaro Hernández, Miguel Ángel Sotelo
- **Comment**: To be presented at the 2021 32nd IEEE Intelligent Vehicles Symposium
  (IV) (IV 2021)
- **Journal**: None
- **Summary**: Understanding complex scenarios from in-vehicle cameras is essential for safely operating autonomous driving systems in densely populated areas. Among these, intersection areas are one of the most critical as they concentrate a considerable number of traffic accidents and fatalities. Detecting and understanding the scene configuration of these usually crowded areas is then of extreme importance for both autonomous vehicles and modern ADAS aimed at preventing road crashes and increasing the safety of vulnerable road users. This work investigates inter-section classification from RGB images using well-consolidate neural network approaches along with a method to enhance the results based on the teacher/student training paradigm. An extensive experimental activity aimed at identifying the best input configuration and evaluating different network parameters on both the well-known KITTI dataset and the new KITTI-360 sequences shows that our method outperforms current state-of-the-art approaches on a per-frame basis and prove the effectiveness of the proposed learning scheme.



### ECLIPSE : Envisioning CLoud Induced Perturbations in Solar Energy
- **Arxiv ID**: http://arxiv.org/abs/2104.12419v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, 68T45, I.4.8; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2104.12419v3)
- **Published**: 2021-04-26 09:19:43+00:00
- **Updated**: 2022-09-06 15:28:45+00:00
- **Authors**: Quentin Paletta, Anthony Hu, Guillaume Arbod, Joan Lasenby
- **Comment**: Manuscript accepted for publication in Applied Energy
- **Journal**: None
- **Summary**: Efficient integration of solar energy into the electricity mix depends on a reliable anticipation of its intermittency. A promising approach to forecast the temporal variability of solar irradiance resulting from the cloud cover dynamics is based on the analysis of sequences of ground-taken sky images or satellite observations. Despite encouraging results, a recurrent limitation of existing deep learning approaches lies in the ubiquitous tendency of reacting to past observations rather than actively anticipating future events. This leads to a frequent temporal lag and limited ability to predict sudden events. To address this challenge, we introduce ECLIPSE, a spatio-temporal neural network architecture that models cloud motion from sky images to not only predict future irradiance levels and associated uncertainties, but also segmented images, which provide richer information on the local irradiance map. We show that ECLIPSE anticipates critical events and reduces temporal delay while generating visually realistic futures. The model characteristics and properties are investigated with an ablation study and a comparative study on the benefits and different ways to integrate auxiliary data into the modelling. The model predictions are also interpreted through an analysis of the principal spatio-temporal components learned during network training.



### Heterogeneous-Agent Trajectory Forecasting Incorporating Class Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2104.12446v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.12446v2)
- **Published**: 2021-04-26 10:28:34+00:00
- **Updated**: 2022-03-03 04:31:07+00:00
- **Authors**: Boris Ivanovic, Kuan-Hui Lee, Pavel Tokmakov, Blake Wulfe, Rowan McAllister, Adrien Gaidon, Marco Pavone
- **Comment**: 15 pages, 15 figures, 6 tables
- **Journal**: None
- **Summary**: Reasoning about the future behavior of other agents is critical to safe robot navigation. The multiplicity of plausible futures is further amplified by the uncertainty inherent to agent state estimation from data, including positions, velocities, and semantic class. Forecasting methods, however, typically neglect class uncertainty, conditioning instead only on the agent's most likely class, even though perception models often return full class distributions. To exploit this information, we present HAICU, a method for heterogeneous-agent trajectory forecasting that explicitly incorporates agents' class probabilities. We additionally present PUP, a new challenging real-world autonomous driving dataset, to investigate the impact of Perceptual Uncertainty in Prediction. It contains challenging crowded scenes with unfiltered agent class probabilities that reflect the long-tail of current state-of-the-art perception systems. We demonstrate that incorporating class probabilities in trajectory forecasting significantly improves performance in the face of uncertainty, and enables new forecasting capabilities such as counterfactual predictions.



### 3D Scene Compression through Entropy Penalized Neural Representation Functions
- **Arxiv ID**: http://arxiv.org/abs/2104.12456v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.12456v1)
- **Published**: 2021-04-26 10:36:47+00:00
- **Updated**: 2021-04-26 10:36:47+00:00
- **Authors**: Thomas Bird, Johannes Ballé, Saurabh Singh, Philip A. Chou
- **Comment**: accepted (in an abridged format) as a contribution to the
  Learning-based Image Coding special session of the Picture Coding Symposium
  2021
- **Journal**: None
- **Summary**: Some forms of novel visual media enable the viewer to explore a 3D scene from arbitrary viewpoints, by interpolating between a discrete set of original views. Compared to 2D imagery, these types of applications require much larger amounts of storage space, which we seek to reduce. Existing approaches for compressing 3D scenes are based on a separation of compression and rendering: each of the original views is compressed using traditional 2D image formats; the receiver decompresses the views and then performs the rendering. We unify these steps by directly compressing an implicit representation of the scene, a function that maps spatial coordinates to a radiance vector field, which can then be queried to render arbitrary viewpoints. The function is implemented as a neural network and jointly trained for reconstruction as well as compressibility, in an end-to-end manner, with the use of an entropy penalty on the parameters. Our method significantly outperforms a state-of-the-art conventional approach for scene compression, achieving simultaneously higher quality reconstructions and lower bitrates. Furthermore, we show that the performance at lower bitrates can be improved by jointly representing multiple scenes using a soft form of parameter sharing.



### Points2Sound: From mono to binaural audio using 3D point cloud scenes
- **Arxiv ID**: http://arxiv.org/abs/2104.12462v3
- **DOI**: 10.1186/s13636-022-00265-4
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2104.12462v3)
- **Published**: 2021-04-26 10:44:01+00:00
- **Updated**: 2023-05-19 12:54:02+00:00
- **Authors**: Francesc Lluís, Vasileios Chatziioannou, Alex Hofmann
- **Comment**: Code, data, and listening examples:
  https://github.com/francesclluis/points2sound
- **Journal**: EURASIP Journal on Audio, Speech, and Music Processing 2022 (1),
  1-15
- **Summary**: For immersive applications, the generation of binaural sound that matches its visual counterpart is crucial to bring meaningful experiences to people in a virtual environment. Recent studies have shown the possibility of using neural networks for synthesizing binaural audio from mono audio by using 2D visual information as guidance. Extending this approach by guiding the audio with 3D visual information and operating in the waveform domain may allow for a more accurate auralization of a virtual audio scene. We propose Points2Sound, a multi-modal deep learning model which generates a binaural version from mono audio using 3D point cloud scenes. Specifically, Points2Sound consists of a vision network and an audio network. The vision network uses 3D sparse convolutions to extract a visual feature from the point cloud scene. Then, the visual feature conditions the audio network, which operates in the waveform domain, to synthesize the binaural version. Results show that 3D visual information can successfully guide multi-modal deep learning models for the task of binaural synthesis. We also investigate how 3D point cloud attributes, learning objectives, different reverberant conditions, and several types of mono mixture signals affect the binaural audio synthesis performance of Points2Sound for the different numbers of sound sources present in the scene.



### Practical Wide-Angle Portraits Correction with Deep Structured Models
- **Arxiv ID**: http://arxiv.org/abs/2104.12464v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12464v3)
- **Published**: 2021-04-26 10:47:35+00:00
- **Updated**: 2021-04-28 06:22:21+00:00
- **Authors**: Jing Tan, Shan Zhao, Pengfei Xiong, Jiangyu Liu, Haoqiang Fan, Shuaicheng Liu
- **Comment**: This work has been accepted to CVPR2021. The project link is
  https://github.com/TanJing94/Deep_Portraits_Correction
- **Journal**: None
- **Summary**: Wide-angle portraits often enjoy expanded views. However, they contain perspective distortions, especially noticeable when capturing group portrait photos, where the background is skewed and faces are stretched. This paper introduces the first deep learning based approach to remove such artifacts from freely-shot photos. Specifically, given a wide-angle portrait as input, we build a cascaded network consisting of a LineNet, a ShapeNet, and a transition module (TM), which corrects perspective distortions on the background, adapts to the stereographic projection on facial regions, and achieves smooth transitions between these two projections, accordingly. To train our network, we build the first perspective portrait dataset with a large diversity in identities, scenes and camera modules. For the quantitative evaluation, we introduce two novel metrics, line consistency and face congruence. Compared to the previous state-of-the-art approach, our method does not require camera distortion parameters. We demonstrate that our approach significantly outperforms the previous state-of-the-art approach both qualitatively and quantitatively.



### GPT2MVS: Generative Pre-trained Transformer-2 for Multi-modal Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/2104.12465v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.12465v1)
- **Published**: 2021-04-26 10:50:37+00:00
- **Updated**: 2021-04-26 10:50:37+00:00
- **Authors**: Jia-Hong Huang, Luka Murn, Marta Mrak, Marcel Worring
- **Comment**: This paper is accepted by ACM International Conference on Multimedia
  Retrieval (ICMR), 2021
- **Journal**: None
- **Summary**: Traditional video summarization methods generate fixed video representations regardless of user interest. Therefore such methods limit users' expectations in content search and exploration scenarios. Multi-modal video summarization is one of the methods utilized to address this problem. When multi-modal video summarization is used to help video exploration, a text-based query is considered as one of the main drivers of video summary generation, as it is user-defined. Thus, encoding the text-based query and the video effectively are both important for the task of multi-modal video summarization. In this work, a new method is proposed that uses a specialized attention network and contextualized word representations to tackle this task. The proposed model consists of a contextualized video summary controller, multi-modal attention mechanisms, an interactive attention network, and a video summary generator. Based on the evaluation of the existing multi-modal video summarization benchmark, experimental results show that the proposed model is effective with the increase of +5.88% in accuracy and +4.06% increase of F1-score, compared with the state-of-the-art method.



### Dynamic VAEs with Generative Replay for Continual Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.12468v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12468v1)
- **Published**: 2021-04-26 10:56:43+00:00
- **Updated**: 2021-04-26 10:56:43+00:00
- **Authors**: Subhankar Ghosh
- **Comment**: 10 pages, 10 figures. arXiv admin note: text overlap with
  arXiv:2102.03778
- **Journal**: None
- **Summary**: Continual zero-shot learning(CZSL) is a new domain to classify objects sequentially the model has not seen during training. It is more suitable than zero-shot and continual learning approaches in real-case scenarios when data may come continually with only attributes for a few classes and attributes and features for other classes. Continual learning(CL) suffers from catastrophic forgetting, and zero-shot learning(ZSL) models cannot classify objects like state-of-the-art supervised classifiers due to lack of actual data(or features) during training. This paper proposes a novel continual zero-shot learning (DVGR-CZSL) model that grows in size with each task and uses generative replay to update itself with previously learned classes to avoid forgetting. We demonstrate our hybrid model(DVGR-CZSL) outperforms the baselines and is effective on several datasets, i.e., CUB, AWA1, AWA2, and aPY. We show our method is superior in task sequentially learning with ZSL(Zero-Shot Learning). We also discuss our results on the SUN dataset.



### Generative modeling of spatio-temporal weather patterns with extreme event conditioning
- **Arxiv ID**: http://arxiv.org/abs/2104.12469v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2104.12469v1)
- **Published**: 2021-04-26 10:58:44+00:00
- **Updated**: 2021-04-26 10:58:44+00:00
- **Authors**: Konstantin Klemmer, Sudipan Saha, Matthias Kahl, Tianlin Xu, Xiao Xiang Zhu
- **Comment**: ICLR'21 Workshop AI: Modeling Oceans and Climate Change (AIMOCC)
- **Journal**: None
- **Summary**: Deep generative models are increasingly used to gain insights in the geospatial data domain, e.g., for climate data. However, most existing approaches work with temporal snapshots or assume 1D time-series; few are able to capture spatio-temporal processes simultaneously. Beyond this, Earth-systems data often exhibit highly irregular and complex patterns, for example caused by extreme weather events. Because of climate change, these phenomena are only increasing in frequency. Here, we proposed a novel GAN-based approach for generating spatio-temporal weather patterns conditioned on detected extreme events. Our approach augments GAN generator and discriminator with an encoded extreme weather event segmentation mask. These segmentation masks can be created from raw input using existing event detection frameworks. As such, our approach is highly modular and can be combined with custom GAN architectures. We highlight the applicability of our proposed approach in experiments with real-world surface radiation and zonal wind data.



### Three-Dimensional Embedded Attentive RNN (3D-EAR) Segmentor for Left Ventricle Delineation from Myocardial Velocity Mapping
- **Arxiv ID**: http://arxiv.org/abs/2104.13214v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T01
- **Links**: [PDF](http://arxiv.org/pdf/2104.13214v1)
- **Published**: 2021-04-26 11:04:43+00:00
- **Updated**: 2021-04-26 11:04:43+00:00
- **Authors**: Mengmeng Kuang, Yinzhe Wu, Diego Alonso-Álvarez, David Firmin, Jennifer Keegan, Peter Gatehouse, Guang Yang
- **Comment**: 8 pages, 4 figures, Functional Imaging and Modeling of the Heart
- **Journal**: None
- **Summary**: Myocardial Velocity Mapping Cardiac MR (MVM-CMR) can be used to measure global and regional myocardial velocities with proved reproducibility. Accurate left ventricle delineation is a prerequisite for robust and reproducible myocardial velocity estimation. Conventional manual segmentation on this dataset can be time-consuming and subjective, and an effective fully automated delineation method is highly in demand. By leveraging recently proposed deep learning-based semantic segmentation approaches, in this study, we propose a novel fully automated framework incorporating a 3D-UNet backbone architecture with Embedded multichannel Attention mechanism and LSTM based Recurrent neural networks (RNN) for the MVM-CMR datasets (dubbed 3D-EAR segmentor). The proposed method also utilises the amalgamation of magnitude and phase images as input to realise an information fusion of this multichannel dataset and exploring the correlations of temporal frames via the embedded RNN. By comparing the baseline model of 3D-UNet and ablation studies with and without embedded attentive LSTM modules and various loss functions, we can demonstrate that the proposed model has outperformed the state-of-the-art baseline models with significant improvement.



### Contextualized Keyword Representations for Multi-modal Retinal Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2104.12471v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.12471v1)
- **Published**: 2021-04-26 11:08:13+00:00
- **Updated**: 2021-04-26 11:08:13+00:00
- **Authors**: Jia-Hong Huang, Ting-Wei Wu, Marcel Worring
- **Comment**: This paper is accepted by ACM International Conference on Multimedia
  Retrieval (ICMR), 2021
- **Journal**: None
- **Summary**: Medical image captioning automatically generates a medical description to describe the content of a given medical image. A traditional medical image captioning model creates a medical description only based on a single medical image input. Hence, an abstract medical description or concept is hard to be generated based on the traditional approach. Such a method limits the effectiveness of medical image captioning. Multi-modal medical image captioning is one of the approaches utilized to address this problem. In multi-modal medical image captioning, textual input, e.g., expert-defined keywords, is considered as one of the main drivers of medical description generation. Thus, encoding the textual input and the medical image effectively are both important for the task of multi-modal medical image captioning. In this work, a new end-to-end deep multi-modal medical image captioning model is proposed. Contextualized keyword representations, textual feature reinforcement, and masked self-attention are used to develop the proposed approach. Based on the evaluation of the existing multi-modal medical image captioning dataset, experimental results show that the proposed model is effective with the increase of +53.2% in BLEU-avg and +18.6% in CIDEr, compared with the state-of-the-art method.



### EigenGAN: Layer-Wise Eigen-Learning for GANs
- **Arxiv ID**: http://arxiv.org/abs/2104.12476v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2104.12476v2)
- **Published**: 2021-04-26 11:14:37+00:00
- **Updated**: 2021-08-09 11:57:55+00:00
- **Authors**: Zhenliang He, Meina Kan, Shiguang Shan
- **Comment**: ICCV 2021. Code: https://github.com/LynnHo/EigenGAN-Tensorflow
- **Journal**: None
- **Summary**: Recent studies on Generative Adversarial Network (GAN) reveal that different layers of a generative CNN hold different semantics of the synthesized images. However, few GAN models have explicit dimensions to control the semantic attributes represented in a specific layer. This paper proposes EigenGAN which is able to unsupervisedly mine interpretable and controllable dimensions from different generator layers. Specifically, EigenGAN embeds one linear subspace with orthogonal basis into each generator layer. Via generative adversarial training to learn a target distribution, these layer-wise subspaces automatically discover a set of "eigen-dimensions" at each layer corresponding to a set of semantic attributes or interpretable variations. By traversing the coefficient of a specific eigen-dimension, the generator can produce samples with continuous changes corresponding to a specific semantic attribute. Taking the human face for example, EigenGAN can discover controllable dimensions for high-level concepts such as pose and gender in the subspace of deep layers, as well as low-level concepts such as hue and color in the subspace of shallow layers. Moreover, in the linear case, we theoretically prove that our algorithm derives the principal components as PCA does. Codes can be found in https://github.com/LynnHo/EigenGAN-Tensorflow.



### A deep learning model for gastric diffuse-type adenocarcinoma classification in whole slide images
- **Arxiv ID**: http://arxiv.org/abs/2104.12478v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.12478v1)
- **Published**: 2021-04-26 11:22:20+00:00
- **Updated**: 2021-04-26 11:22:20+00:00
- **Authors**: Fahdi Kanavati, Masayuki Tsuneki
- **Comment**: None
- **Journal**: None
- **Summary**: Gastric diffuse-type adenocarcinoma represents a disproportionately high percentage of cases of gastric cancers occurring in the young, and its relative incidence seems to be on the rise. Usually it affects the body of the stomach, and presents shorter duration and worse prognosis compared with the differentiated (intestinal) type adenocarcinoma. The main difficulty encountered in the differential diagnosis of gastric adenocarcinomas occurs with the diffuse-type. As the cancer cells of diffuse-type adenocarcinoma are often single and inconspicuous in a background desmoplaia and inflammation, it can often be mistaken for a wide variety of non-neoplastic lesions including gastritis or reactive endothelial cells seen in granulation tissue. In this study we trained deep learning models to classify gastric diffuse-type adenocarcinoma from WSIs. We evaluated the models on five test sets obtained from distinct sources, achieving receiver operator curve (ROC) area under the curves (AUCs) in the range of 0.95-0.99. The highly promising results demonstrate the potential of AI-based computational pathology for aiding pathologists in their diagnostic workflow system.



### Dense Point Prediction: A Simple Baseline for Crowd Counting and Localization
- **Arxiv ID**: http://arxiv.org/abs/2104.12505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12505v1)
- **Published**: 2021-04-26 12:08:08+00:00
- **Updated**: 2021-04-26 12:08:08+00:00
- **Authors**: Yi Wang, Xinyu Hou, Lap-Pui Chau
- **Comment**: 6 pages. Accepted at ICME Workshop 2021
- **Journal**: None
- **Summary**: In this paper, we propose a simple yet effective crowd counting and localization network named SCALNet. Unlike most existing works that separate the counting and localization tasks, we consider those tasks as a pixel-wise dense prediction problem and integrate them into an end-to-end framework. Specifically, for crowd counting, we adopt a counting head supervised by the Mean Square Error (MSE) loss. For crowd localization, the key insight is to recognize the keypoint of people, i.e., the center point of heads. We propose a localization head to distinguish dense crowds trained by two loss functions, i.e., Negative-Suppressed Focal (NSF) loss and False-Positive (FP) loss, which balances the positive/negative examples and handles the false-positive predictions. Experiments on the recent and large-scale benchmark, NWPU-Crowd, show that our approach outperforms the state-of-the-art methods by more than 5% and 10% improvement in crowd localization and counting tasks, respectively. The code is publicly available at https://github.com/WangyiNTU/SCALNet.



### Inner-ear Augmented Metal Artifact Reduction with Simulation-based 3D Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.12510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12510v1)
- **Published**: 2021-04-26 12:22:56+00:00
- **Updated**: 2021-04-26 12:22:56+00:00
- **Authors**: Wang Zihao, Vandersteen Clair, Demarcy Thomas, Gnansia Dan, Raffaelli Charles, Guevara Nicolas, Delingette Herve
- **Comment**: None
- **Journal**: None
- **Summary**: Metal Artifacts creates often difficulties for a high quality visual assessment of post-operative imaging in {c}omputed {t}omography (CT). A vast body of methods have been proposed to tackle this issue, but {these} methods were designed for regular CT scans and their performance is usually insufficient when imaging tiny implants. In the context of post-operative high-resolution {CT} imaging, we propose a 3D metal {artifact} reduction algorithm based on a generative adversarial neural network. It is based on the simulation of physically realistic CT metal artifacts created by cochlea implant electrodes on preoperative images. The generated images serve to train a 3D generative adversarial networks for artifacts reduction. The proposed approach was assessed qualitatively and quantitatively on clinical conventional and cone-beam CT of cochlear implant postoperative images. These experiments show that the proposed method {outperforms other} general metal artifact reduction approaches.



### Visformer: The Vision-friendly Transformer
- **Arxiv ID**: http://arxiv.org/abs/2104.12533v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12533v5)
- **Published**: 2021-04-26 13:13:03+00:00
- **Updated**: 2021-12-18 08:37:49+00:00
- **Authors**: Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: The past year has witnessed the rapid development of applying the Transformer module to vision problems. While some researchers have demonstrated that Transformer-based models enjoy a favorable ability of fitting data, there are still growing number of evidences showing that these models suffer over-fitting especially when the training data is limited. This paper offers an empirical study by performing step-by-step operations to gradually transit a Transformer-based model to a convolution-based model. The results we obtain during the transition process deliver useful messages for improving visual recognition. Based on these observations, we propose a new architecture named Visformer, which is abbreviated from the `Vision-friendly Transformer'. With the same computational complexity, Visformer outperforms both the Transformer-based and convolution-based models in terms of ImageNet classification accuracy, and the advantage becomes more significant when the model complexity is lower or the training set is smaller. The code is available at https://github.com/danczs/Visformer.



### Computer vision in automated parking systems: Design, implementation and challenges
- **Arxiv ID**: http://arxiv.org/abs/2104.12537v1
- **DOI**: 10.1016/j.imavis.2017.07.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12537v1)
- **Published**: 2021-04-26 13:18:02+00:00
- **Updated**: 2021-04-26 13:18:02+00:00
- **Authors**: Markus Heimberger, Jonathan Horgan, Ciaran Hughes, John McDonald, Senthil Yogamani
- **Comment**: None
- **Journal**: Image and Vision Computing, Volume 68, December 2017, Pages 88-101
- **Summary**: Automated driving is an active area of research in both industry and academia. Automated Parking, which is automated driving in a restricted scenario of parking with low speed manoeuvring, is a key enabling product for fully autonomous driving systems. It is also an important milestone from the perspective of a higher end system built from the previous generation driver assistance systems comprising of collision warning, pedestrian detection, etc. In this paper, we discuss the design and implementation of an automated parking system from the perspective of computer vision algorithms. Designing a low-cost system with functional safety is challenging and leads to a large gap between the prototype and the end product, in order to handle all the corner cases. We demonstrate how camera systems are crucial for addressing a range of automated parking use cases and also, to add robustness to systems based on active distance measuring sensors, such as ultrasonics and radar. The key vision modules which realize the parking use cases are 3D reconstruction, parking slot marking recognition, freespace and vehicle/pedestrian detection. We detail the important parking use cases and demonstrate how to combine the vision modules to form a robust parking system. To the best of the authors' knowledge, this is the first detailed discussion of a systemic view of a commercial automated parking system.



### Synthetic 3D Data Generation Pipeline for Geometric Deep Learning in Architecture
- **Arxiv ID**: http://arxiv.org/abs/2104.12564v1
- **DOI**: 10.5194/isprs-archives-XLIII-B2-2021-337-2021
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12564v1)
- **Published**: 2021-04-26 13:32:03+00:00
- **Updated**: 2021-04-26 13:32:03+00:00
- **Authors**: Stanislava Fedorova, Alberto Tono, Meher Shashwat Nigam, Jiayao Zhang, Amirhossein Ahmadnia, Cecilia Bolognesi, Dominik L. Michels
- **Comment**: Project Page:
  https://cdinstitute.github.io/Building-Dataset-Generator/
- **Journal**: None
- **Summary**: With the growing interest in deep learning algorithms and computational design in the architectural field, the need for large, accessible and diverse architectural datasets increases. We decided to tackle this problem by constructing a field-specific synthetic data generation pipeline that generates an arbitrary amount of 3D data along with the associated 2D and 3D annotations. The variety of annotations, the flexibility to customize the generated building and dataset parameters make this framework suitable for multiple deep learning tasks, including geometric deep learning that requires direct 3D supervision. Creating our building data generation pipeline we leveraged architectural knowledge from experts in order to construct a framework that would be modular, extendable and would provide a sufficient amount of class-balanced data samples. Moreover, we purposefully involve the researcher in the dataset customization allowing the introduction of additional building components, material textures, building classes, number and type of annotations as well as the number of views per 3D model sample. In this way, the framework would satisfy different research requirements and would be adaptable to a large variety of tasks. All code and data are made publicly available.



### Mutual Contrastive Learning for Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.12565v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12565v2)
- **Published**: 2021-04-26 13:32:33+00:00
- **Updated**: 2022-01-16 05:44:33+00:00
- **Authors**: Chuanguang Yang, Zhulin An, Linhang Cai, Yongjun Xu
- **Comment**: 9 pages, AAAI-2022
- **Journal**: None
- **Summary**: We present a collaborative learning method called Mutual Contrastive Learning (MCL) for general visual representation learning. The core idea of MCL is to perform mutual interaction and transfer of contrastive distributions among a cohort of networks. A crucial component of MCL is Interactive Contrastive Learning (ICL). Compared with vanilla contrastive learning, ICL can aggregate cross-network embedding information and maximize the lower bound to the mutual information between two networks. This enables each network to learn extra contrastive knowledge from others, leading to better feature representations for visual recognition tasks. We emphasize that the resulting MCL is conceptually simple yet empirically powerful. It is a generic framework that can be applied to both supervised and self-supervised representation learning. Experimental results on image classification and transfer learning to object detection show that MCL can lead to consistent performance gains, demonstrating that MCL can guide the network to generate better feature representations. Code is available at https://github.com/winycg/MCL.



### Learning from Event Cameras with Sparse Spiking Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2104.12579v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12579v1)
- **Published**: 2021-04-26 13:52:01+00:00
- **Updated**: 2021-04-26 13:52:01+00:00
- **Authors**: Loïc Cordone, Benoît Miramond, Sonia Ferrante
- **Comment**: Accepted to the International Joint Conference on Neural Networks
  (IJCNN) 2021
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are now the de facto solution for computer vision problems thanks to their impressive results and ease of learning. These networks are composed of layers of connected units called artificial neurons, loosely modeling the neurons in a biological brain. However, their implementation on conventional hardware (CPU/GPU) results in high power consumption, making their integration on embedded systems difficult. In a car for example, embedded algorithms have very high constraints in term of energy, latency and accuracy. To design more efficient computer vision algorithms, we propose to follow an end-to-end biologically inspired approach using event cameras and spiking neural networks (SNNs). Event cameras output asynchronous and sparse events, providing an incredibly efficient data source, but processing these events with synchronous and dense algorithms such as CNNs does not yield any significant benefits. To address this limitation, we use spiking neural networks (SNNs), which are more biologically realistic neural networks where units communicate using discrete spikes. Due to the nature of their operations, they are hardware friendly and energy-efficient, but training them still remains a challenge. Our method enables the training of sparse spiking convolutional neural networks directly on event data, using the popular deep learning framework PyTorch. The performances in terms of accuracy, sparsity and training time on the popular DVS128 Gesture Dataset make it possible to use this bio-inspired approach for the future embedding of real-time applications on low-power neuromorphic hardware.



### FedDPGAN: Federated Differentially Private Generative Adversarial Networks Framework for the Detection of COVID-19 Pneumonia
- **Arxiv ID**: http://arxiv.org/abs/2104.12581v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.12581v1)
- **Published**: 2021-04-26 13:52:12+00:00
- **Updated**: 2021-04-26 13:52:12+00:00
- **Authors**: Longling Zhang, Bochen Shen, Ahmed Barnawi, Shan Xi, Neeraj Kumar, Yi Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Existing deep learning technologies generally learn the features of chest X-ray data generated by Generative Adversarial Networks (GAN) to diagnose COVID-19 pneumonia. However, the above methods have a critical challenge: data privacy. GAN will leak the semantic information of the training data which can be used to reconstruct the training samples by attackers, thereby this method will leak the privacy of the patient. Furthermore, for this reason that is the limitation of the training data sample, different hospitals jointly train the model through data sharing, which will also cause the privacy leakage. To solve this problem, we adopt the Federated Learning (FL) frame-work which is a new technique being used to protect the data privacy. Under the FL framework and Differentially Private thinking, we propose a FederatedDifferentially Private Generative Adversarial Network (FedDPGAN) to detectCOVID-19 pneumonia for sustainable smart cities. Specifically, we use DP-GAN to privately generate diverse patient data in which differential privacy technology is introduced to make sure the privacy protection of the semantic information of training dataset. Furthermore, we leverage FL to allow hospitals to collaboratively train COVID-19 models without sharing the original data. Under Independent and Identically Distributed (IID) and non-IID settings, The evaluation of the proposed model is on three types of chest X-ray (CXR) images dataset (COVID-19, normal, and normal pneumonia). A large number of the truthful reports make the verification of our model can effectively diagnose COVID-19 without compromising privacy.



### Vision-based Driver Assistance Systems: Survey, Taxonomy and Advances
- **Arxiv ID**: http://arxiv.org/abs/2104.12583v1
- **DOI**: 10.1109/ITSC.2015.329
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.12583v1)
- **Published**: 2021-04-26 13:53:00+00:00
- **Updated**: 2021-04-26 13:53:00+00:00
- **Authors**: Jonathan Horgan, Ciarán Hughes, John McDonald, Senthil Yogamani
- **Comment**: None
- **Journal**: 2015 IEEE 18th International Conference on Intelligent
  Transportation Systems
- **Summary**: Vision-based driver assistance systems is one of the rapidly growing research areas of ITS, due to various factors such as the increased level of safety requirements in automotive, computational power in embedded systems, and desire to get closer to autonomous driving. It is a cross disciplinary area encompassing specialised fields like computer vision, machine learning, robotic navigation, embedded systems, automotive electronics and safety critical software. In this paper, we survey the list of vision based advanced driver assistance systems with a consistent terminology and propose a taxonomy. We also propose an abstract model in an attempt to formalize a top-down view of application development to scale towards autonomous driving system.



### PatchGuard++: Efficient Provable Attack Detection against Adversarial Patches
- **Arxiv ID**: http://arxiv.org/abs/2104.12609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12609v1)
- **Published**: 2021-04-26 14:22:33+00:00
- **Updated**: 2021-04-26 14:22:33+00:00
- **Authors**: Chong Xiang, Prateek Mittal
- **Comment**: ICLR 2021 Workshop on Security and Safety in Machine Learning Systems
- **Journal**: None
- **Summary**: An adversarial patch can arbitrarily manipulate image pixels within a restricted region to induce model misclassification. The threat of this localized attack has gained significant attention because the adversary can mount a physically-realizable attack by attaching patches to the victim object. Recent provably robust defenses generally follow the PatchGuard framework by using CNNs with small receptive fields and secure feature aggregation for robust model predictions. In this paper, we extend PatchGuard to PatchGuard++ for provably detecting the adversarial patch attack to boost both provable robust accuracy and clean accuracy. In PatchGuard++, we first use a CNN with small receptive fields for feature extraction so that the number of features corrupted by the adversarial patch is bounded. Next, we apply masks in the feature space and evaluate predictions on all possible masked feature maps. Finally, we extract a pattern from all masked predictions to catch the adversarial patch attack. We evaluate PatchGuard++ on ImageNette (a 10-class subset of ImageNet), ImageNet, and CIFAR-10 and demonstrate that PatchGuard++ significantly improves the provable robustness and clean performance.



### AWCD: An Efficient Point Cloud Processing Approach via Wasserstein Curvature
- **Arxiv ID**: http://arxiv.org/abs/2105.04402v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.04402v2)
- **Published**: 2021-04-26 14:33:05+00:00
- **Updated**: 2021-05-11 08:04:21+00:00
- **Authors**: Yihao Luo, Ailing Yang, Fupeng Sun, Huafei Sun
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: In this paper, we introduce the adaptive Wasserstein curvature denoising (AWCD), an original processing approach for point cloud data. By collecting curvatures information from Wasserstein distance, AWCD consider more precise structures of data and preserves stability and effectiveness even for data with noise in high density. This paper contains some theoretical analysis about the Wasserstein curvature and the complete algorithm of AWCD. In addition, we design digital experiments to show the denoising effect of AWCD. According to comparison results, we present the advantages of AWCD against traditional algorithms.



### Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Models
- **Arxiv ID**: http://arxiv.org/abs/2104.12623v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.12623v2)
- **Published**: 2021-04-26 14:50:59+00:00
- **Updated**: 2023-02-28 09:37:59+00:00
- **Authors**: Sebastian Szyller, Vasisht Duddu, Tommi Gröndahl, N. Asokan
- **Comment**: 19 pages
- **Journal**: None
- **Summary**: Machine learning models are typically made available to potential client users via inference APIs. Model extraction attacks occur when a malicious client uses information gleaned from queries to the inference API of a victim model $F_V$ to build a surrogate model $F_A$ with comparable functionality. Recent research has shown successful model extraction of image classification, and natural language processing models. In this paper, we show the first model extraction attack against real-world generative adversarial network (GAN) image translation models. We present a framework for conducting such attacks, and show that an adversary can successfully extract functional surrogate models by querying $F_V$ using data from the same domain as the training data for $F_V$. The adversary need not know $F_V$'s architecture or any other information about it beyond its intended task. We evaluate the effectiveness of our attacks using three different instances of two popular categories of image translation: (1) Selfie-to-Anime and (2) Monet-to-Photo (image style transfer), and (3) Super-Resolution (super resolution). Using standard performance metrics for GANs, we show that our attacks are effective. Furthermore, we conducted a large scale (125 participants) user study on Selfie-to-Anime and Monet-to-Photo to show that human perception of the images produced by $F_V$ and $F_A$ can be considered equivalent, within an equivalence bound of Cohen's d = 0.3. Finally, we show that existing defenses against model extraction attacks (watermarking, adversarial examples, poisoning) do not extend to image translation models.



### Analyzing Green View Index and Green View Index best path using Google Street View and deep learning
- **Arxiv ID**: http://arxiv.org/abs/2104.12627v4
- **DOI**: 10.1093/jcde/qwac102
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2104.12627v4)
- **Published**: 2021-04-26 14:53:21+00:00
- **Updated**: 2022-09-22 06:46:36+00:00
- **Authors**: Jiahao Zhang, Anqi Hu
- **Comment**: 14 pages, 14 figures. Accepted by the Journal of Computational Design
  and Engineering. DOI: https://doi.org/10.1093/jcde/qwac102
- **Journal**: None
- **Summary**: As an important part of urban landscape research, analyzing and studying street-level greenery can increase the understanding of a city's greenery, contributing to better urban living environment planning and design. Planning the best path of urban greenery is a means to effectively maximize the use of urban greenery, which plays a positive role in the physical and mental health of urban residents and the path planning of visitors. In this paper, we used Google Street View (GSV) to obtain street view images of Osaka City. The semantic segmentation model is adopted to segment the street view images and analyze the Green View Index (GVI) of Osaka City. Based on the GVI, we take advantage of the adjacency matrix and Floyd-Warshall Algorithm to calculate Green View Index best path, solving the limitations of ArcGIS software. Our analysis not only allows the calculation of specific routes for the GVI best paths but also realizes the visualization and integration of neighborhood urban greenery. By summarizing all the data, we can conduct an intuitive feeling and objective analysis of the street-level greenery in the research area. Based on this, such as urban residents and visitors can maximize the available natural resources for a better life. The dataset and code are available at https://github.com/Jackieam/GVI-Best-Path.



### CompOFA: Compound Once-For-All Networks for Faster Multi-Platform Deployment
- **Arxiv ID**: http://arxiv.org/abs/2104.12642v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.12642v1)
- **Published**: 2021-04-26 15:10:48+00:00
- **Updated**: 2021-04-26 15:10:48+00:00
- **Authors**: Manas Sahni, Shreya Varshini, Alind Khare, Alexey Tumanov
- **Comment**: Published as a conference paper at ICLR 2021
- **Journal**: None
- **Summary**: The emergence of CNNs in mainstream deployment has necessitated methods to design and train efficient architectures tailored to maximize the accuracy under diverse hardware & latency constraints. To scale these resource-intensive tasks with an increasing number of deployment targets, Once-For-All (OFA) proposed an approach to jointly train several models at once with a constant training cost. However, this cost remains as high as 40-50 GPU days and also suffers from a combinatorial explosion of sub-optimal model configurations. We seek to reduce this search space -- and hence the training budget -- by constraining search to models close to the accuracy-latency Pareto frontier. We incorporate insights of compound relationships between model dimensions to build CompOFA, a design space smaller by several orders of magnitude. Through experiments on ImageNet, we demonstrate that even with simple heuristics we can achieve a 2x reduction in training time and 216x speedup in model search/extraction time compared to the state of the art, without loss of Pareto optimality! We also show that this smaller design space is dense enough to support equally accurate models for a similar diversity of hardware and latency targets, while also reducing the complexity of the training and subsequent extraction algorithms.



### CAGAN: Text-To-Image Generation with Combined Attention GANs
- **Arxiv ID**: http://arxiv.org/abs/2104.12663v4
- **DOI**: 10.1007/978-3-030-92659-5_25
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12663v4)
- **Published**: 2021-04-26 15:46:40+00:00
- **Updated**: 2022-01-14 16:16:53+00:00
- **Authors**: Henning Schulze, Dogucan Yaman, Alexander Waibel
- **Comment**: None
- **Journal**: LNCS 13024 (2021) 392-404
- **Summary**: Generating images according to natural language descriptions is a challenging task. Prior research has mainly focused to enhance the quality of generation by investigating the use of spatial attention and/or textual attention thereby neglecting the relationship between channels. In this work, we propose the Combined Attention Generative Adversarial Network (CAGAN) to generate photo-realistic images according to textual descriptions. The proposed CAGAN utilises two attention models: word attention to draw different sub-regions conditioned on related words; and squeeze-and-excitation attention to capture non-linear interaction among channels. With spectral normalisation to stabilise training, our proposed CAGAN improves the state of the art on the IS and FID on the CUB dataset and the FID on the more challenging COCO dataset. Furthermore, we demonstrate that judging a model by a single evaluation metric can be misleading by developing an additional model adding local self-attention which scores a higher IS, outperforming the state of the art on the CUB dataset, but generates unrealistic images through feature repetition.



### Clean Images are Hard to Reblur: Exploiting the Ill-Posed Inverse Task for Dynamic Scene Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2104.12665v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.12665v2)
- **Published**: 2021-04-26 15:49:21+00:00
- **Updated**: 2022-04-02 10:33:59+00:00
- **Authors**: Seungjun Nah, Sanghyun Son, Jaerin Lee, Kyoung Mu Lee
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: The goal of dynamic scene deblurring is to remove the motion blur in a given image. Typical learning-based approaches implement their solutions by minimizing the L1 or L2 distance between the output and the reference sharp image. Recent attempts adopt visual recognition features in training to improve the perceptual quality. However, those features are primarily designed to capture high-level contexts rather than low-level structures such as blurriness. Instead, we propose a more direct way to make images sharper by exploiting the inverse task of deblurring, namely, reblurring. Reblurring amplifies the remaining blur to rebuild the original blur, however, a well-deblurred clean image with zero-magnitude blur is hard to reblur. Thus, we design two types of reblurring loss functions for better deblurring. The supervised reblurring loss at training stage compares the amplified blur between the deblurred and the sharp images. The self-supervised reblurring loss at inference stage inspects if there noticeable blur remains in the deblurred. Our experimental results on large-scale benchmarks and real images demonstrate the effectiveness of the reblurring losses in improving the perceptual quality of the deblurred images in terms of NIQE and LPIPS scores as well as visual sharpness.



### Appearance-based Gaze Estimation With Deep Learning: A Review and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2104.12668v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12668v1)
- **Published**: 2021-04-26 15:53:03+00:00
- **Updated**: 2021-04-26 15:53:03+00:00
- **Authors**: Yihua Cheng, Haofei Wang, Yiwei Bao, Feng Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Gaze estimation reveals where a person is looking. It is an important clue for understanding human intention. The recent development of deep learning has revolutionized many computer vision tasks, the appearance-based gaze estimation is no exception. However, it lacks a guideline for designing deep learning algorithms for gaze estimation tasks. In this paper, we present a comprehensive review of the appearance-based gaze estimation methods with deep learning. We summarize the processing pipeline and discuss these methods from four perspectives: deep feature extraction, deep neural network architecture design, personal calibration as well as device and platform. Since the data pre-processing and post-processing methods are crucial for gaze estimation, we also survey face/eye detection method, data rectification method, 2D/3D gaze conversion method, and gaze origin conversion method. To fairly compare the performance of various gaze estimation approaches, we characterize all the publicly available gaze estimation datasets and collect the code of typical gaze estimation algorithms. We implement these codes and set up a benchmark of converting the results of different methods into the same evaluation metrics. This paper not only serves as a reference to develop deep learning-based gaze estimation methods but also a guideline for future gaze estimation research. Implemented methods and data processing codes are available at http://phi-ai.org/GazeHub.



### Exploiting Explanations for Model Inversion Attacks
- **Arxiv ID**: http://arxiv.org/abs/2104.12669v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.12669v3)
- **Published**: 2021-04-26 15:53:57+00:00
- **Updated**: 2022-03-14 04:17:58+00:00
- **Authors**: Xuejun Zhao, Wencan Zhang, Xiaokui Xiao, Brian Y. Lim
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: The successful deployment of artificial intelligence (AI) in many domains from healthcare to hiring requires their responsible use, particularly in model explanations and privacy. Explainable artificial intelligence (XAI) provides more information to help users to understand model decisions, yet this additional knowledge exposes additional risks for privacy attacks. Hence, providing explanation harms privacy. We study this risk for image-based model inversion attacks and identified several attack architectures with increasing performance to reconstruct private image data from model explanations. We have developed several multi-modal transposed CNN architectures that achieve significantly higher inversion performance than using the target model prediction only. These XAI-aware inversion models were designed to exploit the spatial knowledge in image explanations. To understand which explanations have higher privacy risk, we analyzed how various explanation types and factors influence inversion performance. In spite of some models not providing explanations, we further demonstrate increased inversion performance even for non-explainable target models by exploiting explanations of surrogate models through attention transfer. This method first inverts an explanation from the target prediction, then reconstructs the target image. These threats highlight the urgent and significant privacy risks of explanations and calls attention for new privacy preservation techniques that balance the dual-requirement for AI explainability and privacy.



### Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos
- **Arxiv ID**: http://arxiv.org/abs/2104.12671v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12671v3)
- **Published**: 2021-04-26 15:55:01+00:00
- **Updated**: 2021-09-03 19:04:16+00:00
- **Authors**: Brian Chen, Andrew Rouditchenko, Kevin Duarte, Hilde Kuehne, Samuel Thomas, Angie Boggust, Rameswar Panda, Brian Kingsbury, Rogerio Feris, David Harwath, James Glass, Michael Picheny, Shih-Fu Chang
- **Comment**: To be presented at ICCV 2021
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2021, pp. 8012-8021
- **Summary**: Multimodal self-supervised learning is getting more and more attention as it allows not only to train large networks without human supervision but also to search and retrieve data across various modalities. In this context, this paper proposes a self-supervised training framework that learns a common multimodal embedding space that, in addition to sharing representations across different modalities, enforces a grouping of semantically similar instances. To this end, we extend the concept of instance-level contrastive learning with a multimodal clustering step in the training pipeline to capture semantic similarities across modalities. The resulting embedding space enables retrieval of samples across all modalities, even from unseen datasets and different domains. To evaluate our approach, we train our model on the HowTo100M dataset and evaluate its zero-shot retrieval capabilities in two challenging domains, namely text-to-video retrieval, and temporal action localization, showing state-of-the-art results on four different datasets.



### Joint Representation Learning and Novel Category Discovery on Single- and Multi-modal Data
- **Arxiv ID**: http://arxiv.org/abs/2104.12673v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12673v3)
- **Published**: 2021-04-26 15:56:16+00:00
- **Updated**: 2021-10-14 22:43:27+00:00
- **Authors**: Xuhui Jia, Kai Han, Yukun Zhu, Bradley Green
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: This paper studies the problem of novel category discovery on single- and multi-modal data with labels from different but relevant categories. We present a generic, end-to-end framework to jointly learn a reliable representation and assign clusters to unlabelled data. To avoid over-fitting the learnt embedding to labelled data, we take inspiration from self-supervised representation learning by noise-contrastive estimation and extend it to jointly handle labelled and unlabelled data. In particular, we propose using category discrimination on labelled data and cross-modal discrimination on multi-modal data to augment instance discrimination used in conventional contrastive learning approaches. We further employ Winner-Take-All (WTA) hashing algorithm on the shared representation space to generate pairwise pseudo labels for unlabelled data to better predict cluster assignments. We thoroughly evaluate our framework on large-scale multi-modal video benchmarks Kinetics-400 and VGG-Sound, and image benchmarks CIFAR10, CIFAR100 and ImageNet, obtaining state-of-the-art results.



### Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets
- **Arxiv ID**: http://arxiv.org/abs/2104.12690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12690v1)
- **Published**: 2021-04-26 16:29:32+00:00
- **Updated**: 2021-04-26 16:29:32+00:00
- **Authors**: Yuan-Hong Liao, Amlan Kar, Sanja Fidler
- **Comment**: CVPR 2021 Oral
- **Journal**: None
- **Summary**: Data is the engine of modern computer vision, which necessitates collecting large-scale datasets. This is expensive, and guaranteeing the quality of the labels is a major challenge. In this paper, we investigate efficient annotation strategies for collecting multi-class classification labels for a large collection of images. While methods that exploit learnt models for labeling exist, a surprisingly prevalent approach is to query humans for a fixed number of labels per datum and aggregate them, which is expensive. Building on prior work on online joint probabilistic modeling of human annotations and machine-generated beliefs, we propose modifications and best practices aimed at minimizing human labeling effort. Specifically, we make use of advances in self-supervised learning, view annotation as a semi-supervised learning problem, identify and mitigate pitfalls and ablate several key design choices to propose effective guidelines for labeling. Our analysis is done in a more realistic simulation that involves querying human labelers, which uncovers issues with evaluation using existing worker simulation methods. Simulated experiments on a 125k image subset of the ImageNet100 show that it can be annotated to 80% top-1 accuracy with 0.35 annotations per image on average, a 2.7x and 6.7x improvement over prior work and manual annotation, respectively. Project page: https://fidler-lab.github.io/efficient-annotation-cookbook



### Rich Semantics Improve Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.12709v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12709v2)
- **Published**: 2021-04-26 16:48:27+00:00
- **Updated**: 2021-11-12 18:08:34+00:00
- **Authors**: Mohamed Afham, Salman Khan, Muhammad Haris Khan, Muzammal Naseer, Fahad Shahbaz Khan
- **Comment**: Accepted to 32nd British Machine Vision Conference (BMVC 2021)
- **Journal**: None
- **Summary**: Human learning benefits from multi-modal inputs that often appear as rich semantics (e.g., description of an object's attributes while learning about it). This enables us to learn generalizable concepts from very limited visual examples. However, current few-shot learning (FSL) methods use numerical class labels to denote object classes which do not provide rich semantic meanings about the learned concepts. In this work, we show that by using 'class-level' language descriptions, that can be acquired with minimal annotation cost, we can improve the FSL performance. Given a support set and queries, our main idea is to create a bottleneck visual feature (hybrid prototype) which is then used to generate language descriptions of the classes as an auxiliary task during training. We develop a Transformer based forward and backward encoding mechanism to relate visual and semantic tokens that can encode intricate relationships between the two modalities. Forcing the prototypes to retain semantic information about class description acts as a regularizer on the visual features, improving their generalization to novel classes at inference. Furthermore, this strategy imposes a human prior on the learned representations, ensuring that the model is faithfully relating visual and semantic concepts, thereby improving model interpretability. Our experiments on four datasets and ablation studies show the benefit of effectively modeling rich semantics for FSL.



### 2.5D Visual Relationship Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.12727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12727v1)
- **Published**: 2021-04-26 17:19:10+00:00
- **Updated**: 2021-04-26 17:19:10+00:00
- **Authors**: Yu-Chuan Su, Soravit Changpinyo, Xiangning Chen, Sathish Thoppay, Cho-Jui Hsieh, Lior Shapira, Radu Soricut, Hartwig Adam, Matthew Brown, Ming-Hsuan Yang, Boqing Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Visual 2.5D perception involves understanding the semantics and geometry of a scene through reasoning about object relationships with respect to the viewer in an environment. However, existing works in visual recognition primarily focus on the semantics. To bridge this gap, we study 2.5D visual relationship detection (2.5VRD), in which the goal is to jointly detect objects and predict their relative depth and occlusion relationships. Unlike general VRD, 2.5VRD is egocentric, using the camera's viewpoint as a common reference for all 2.5D relationships. Unlike depth estimation, 2.5VRD is object-centric and not only focuses on depth. To enable progress on this task, we create a new dataset consisting of 220k human-annotated 2.5D relationships among 512K objects from 11K images. We analyze this dataset and conduct extensive experiments including benchmarking multiple state-of-the-art VRD models on this task. Our results show that existing models largely rely on semantic cues and simple heuristics to solve 2.5VRD, motivating further research on models for 2.5D perception. The new dataset is available at https://github.com/google-research-datasets/2.5vrd.



### Vision Transformers with Patch Diversification
- **Arxiv ID**: http://arxiv.org/abs/2104.12753v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.12753v3)
- **Published**: 2021-04-26 17:43:04+00:00
- **Updated**: 2021-06-11 01:35:08+00:00
- **Authors**: Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, Qiang Liu
- **Comment**: preprint
- **Journal**: None
- **Summary**: Vision transformer has demonstrated promising performance on challenging computer vision tasks. However, directly training the vision transformers may yield unstable and sub-optimal results. Recent works propose to improve the performance of the vision transformers by modifying the transformer structures, e.g., incorporating convolution layers. In contrast, we investigate an orthogonal approach to stabilize the vision transformer training without modifying the networks. We observe the instability of the training can be attributed to the significant similarity across the extracted patch representations. More specifically, for deep vision transformers, the self-attention blocks tend to map different patches into similar latent representations, yielding information loss and performance degradation. To alleviate this problem, in this work, we introduce novel loss functions in vision transformer training to explicitly encourage diversity across patch representations for more discriminative feature extraction. We empirically show that our proposed techniques stabilize the training and allow us to train wider and deeper vision transformers. We further show the diversified features significantly benefit the downstream tasks in transfer learning. For semantic segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and ADE20k. Our code is available at https://github.com/ChengyueGongR/PatchVisionTransformer.



### InfographicVQA
- **Arxiv ID**: http://arxiv.org/abs/2104.12756v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2104.12756v2)
- **Published**: 2021-04-26 17:45:54+00:00
- **Updated**: 2021-08-22 04:27:56+00:00
- **Authors**: Minesh Mathew, Viraj Bagal, Rubèn Pérez Tito, Dimosthenis Karatzas, Ernest Valveny, C. V Jawahar
- **Comment**: None
- **Journal**: None
- **Summary**: Infographics are documents designed to effectively communicate information using a combination of textual, graphical and visual elements. In this work, we explore the automatic understanding of infographic images by using Visual Question Answering technique.To this end, we present InfographicVQA, a new dataset that comprises a diverse collection of infographics along with natural language questions and answers annotations. The collected questions require methods to jointly reason over the document layout, textual content, graphical elements, and data visualizations. We curate the dataset with emphasis on questions that require elementary reasoning and basic arithmetic skills. Finally, we evaluate two strong baselines based on state of the art multi-modal VQA models, and establish baseline performance for the new task. The dataset, code and leaderboard will be made available at http://docvqa.org



### MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding
- **Arxiv ID**: http://arxiv.org/abs/2104.12763v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.12763v2)
- **Published**: 2021-04-26 17:55:33+00:00
- **Updated**: 2021-10-12 00:49:54+00:00
- **Authors**: Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, Nicolas Carion
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-modal reasoning systems rely on a pre-trained object detector to extract regions of interest from the image. However, this crucial module is typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This makes it challenging for such systems to capture the long tail of visual concepts expressed in free form text. In this paper we propose MDETR, an end-to-end modulated detector that detects objects in an image conditioned on a raw text query, like a caption or a question. We use a transformer-based architecture to reason jointly over text and image by fusing the two modalities at an early stage of the model. We pre-train the network on 1.3M text-image pairs, mined from pre-existing multi-modal datasets having explicit alignment between phrases in text and objects in the image. We then fine-tune on several downstream tasks such as phrase grounding, referring expression comprehension and segmentation, achieving state-of-the-art results on popular benchmarks. We also investigate the utility of our model as an object detector on a given label set when fine-tuned in a few-shot setting. We show that our pre-training approach provides a way to handle the long tail of object categories which have very few labelled instances. Our approach can be easily extended for visual question answering, achieving competitive performance on GQA and CLEVR. The code and models are available at https://github.com/ashkamath/mdetr.



### HAO: Hardware-aware neural Architecture Optimization for Efficient Inference
- **Arxiv ID**: http://arxiv.org/abs/2104.12766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12766v1)
- **Published**: 2021-04-26 17:59:29+00:00
- **Updated**: 2021-04-26 17:59:29+00:00
- **Authors**: Zhen Dong, Yizhao Gao, Qijing Huang, John Wawrzynek, Hayden K. H. So, Kurt Keutzer
- **Comment**: None
- **Journal**: FCCM 2021
- **Summary**: Automatic algorithm-hardware co-design for DNN has shown great success in improving the performance of DNNs on FPGAs. However, this process remains challenging due to the intractable search space of neural network architectures and hardware accelerator implementation. Differing from existing hardware-aware neural architecture search (NAS) algorithms that rely solely on the expensive learning-based approaches, our work incorporates integer programming into the search algorithm to prune the design space. Given a set of hardware resource constraints, our integer programming formulation directly outputs the optimal accelerator configuration for mapping a DNN subgraph that minimizes latency. We use an accuracy predictor for different DNN subgraphs with different quantization schemes and generate accuracy-latency pareto frontiers. With low computational cost, our algorithm can generate quantized networks that achieve state-of-the-art accuracy and hardware performance on Xilinx Zynq (ZU3EG) FPGA for image classification on ImageNet dataset. The solution searched by our algorithm achieves 72.5% top-1 accuracy on ImageNet at framerate 50, which is 60% faster than MnasNet and 135% faster than FBNet with comparable accuracy.



### Less is more: Selecting informative and diverse subsets with balancing constraints
- **Arxiv ID**: http://arxiv.org/abs/2104.12835v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.12835v2)
- **Published**: 2021-04-26 19:22:27+00:00
- **Updated**: 2021-10-08 16:59:33+00:00
- **Authors**: Srikumar Ramalingam, Daniel Glasner, Kaushal Patel, Raviteja Vemulapalli, Sadeep Jayasumana, Sanjiv Kumar
- **Comment**: Added error bars to the experiments
- **Journal**: None
- **Summary**: Deep learning has yielded extraordinary results in vision and natural language processing, but this achievement comes at a cost. Most models require enormous resources during training, both in terms of computation and in human labeling effort. We show that we can identify informative and diverse subsets of data that lead to deep learning models with similar performance as the ones trained with the original dataset. Prior methods have exploited diversity and uncertainty in submodular objective functions for choosing subsets. In addition to these measures, we show that balancing constraints on predicted class labels and decision boundaries are beneficial. We propose a novel formulation of these constraints using matroids, an algebraic structure that generalizes linear independence in vector spaces, and present an efficient greedy algorithm with constant approximation guarantees. We outperform competing baselines on standard classification datasets such as CIFAR-10, CIFAR-100, ImageNet, as well as long-tailed datasets such as CIFAR-100-LT.



### Multimodal Contrastive Training for Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2104.12836v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12836v1)
- **Published**: 2021-04-26 19:23:36+00:00
- **Updated**: 2021-04-26 19:23:36+00:00
- **Authors**: Xin Yuan, Zhe Lin, Jason Kuen, Jianming Zhang, Yilin Wang, Michael Maire, Ajinkya Kale, Baldo Faieta
- **Comment**: None
- **Journal**: None
- **Summary**: We develop an approach to learning visual representations that embraces multimodal data, driven by a combination of intra- and inter-modal similarity preservation objectives. Unlike existing visual pre-training methods, which solve a proxy prediction task in a single domain, our method exploits intrinsic data properties within each modality and semantic information from cross-modal correlation simultaneously, hence improving the quality of learned visual representations. By including multimodal training in a unified framework with different types of contrastive losses, our method can learn more powerful and generic visual features. We first train our model on COCO and evaluate the learned visual representations on various downstream tasks including image classification, object detection, and instance segmentation. For example, the visual representations pre-trained on COCO by our method achieve state-of-the-art top-1 validation accuracy of $55.3\%$ on ImageNet classification, under the common transfer protocol. We also evaluate our method on the large-scale Stock images dataset and show its effectiveness on multi-label image tagging, and cross-modal retrieval tasks.



### One-dimensional Active Contour Models for Raman Spectrum Baseline Correction
- **Arxiv ID**: http://arxiv.org/abs/2104.12839v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.12839v1)
- **Published**: 2021-04-26 19:30:34+00:00
- **Updated**: 2021-04-26 19:30:34+00:00
- **Authors**: M. Hamed Mozaffari, Li-Lin Tay
- **Comment**: 4 figures, and 9 pages
- **Journal**: None
- **Summary**: Raman spectroscopy is a powerful and non-invasive method for analysis of chemicals and detection of unknown substances. However, Raman signal is so weak that background noise can distort the actual Raman signal. These baseline shifts that exist in the Raman spectrum might deteriorate analytical results. In this paper, a modified version of active contour models in one-dimensional space has been proposed for the baseline correction of Raman spectra. Our technique, inspired by principles of physics and heuristic optimization methods, iteratively deforms an initialized curve toward the desired baseline. The performance of the proposed algorithm was evaluated and compared with similar techniques using simulated Raman spectra. The results showed that the 1D active contour model outperforms many iterative baseline correction methods. The proposed algorithm was successfully applied to experimental Raman spectral data, and the results indicate that the baseline of Raman spectra can be automatically subtracted.



### On the Importance of 3D Surface Information for Remote Sensing Classification Tasks
- **Arxiv ID**: http://arxiv.org/abs/2104.13969v1
- **DOI**: 10.5334/dsj-2021-020
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.13969v1)
- **Published**: 2021-04-26 19:55:51+00:00
- **Updated**: 2021-04-26 19:55:51+00:00
- **Authors**: Jan Petrich, Ryan Sander, Eliza Bradley, Adam Dawood, Shawn Hough
- **Comment**: Accepted to CODATA Data Science Journal
- **Journal**: None
- **Summary**: There has been a surge in remote sensing machine learning applications that operate on data from active or passive sensors as well as multi-sensor combinations (Ma et al. (2019)). Despite this surge, however, there has been relatively little study on the comparative value of 3D surface information for machine learning classification tasks. Adding 3D surface information to RGB imagery can provide crucial geometric information for semantic classes such as buildings, and can thus improve out-of-sample predictive performance. In this paper, we examine in-sample and out-of-sample classification performance of Fully Convolutional Neural Networks (FCNNs) and Support Vector Machines (SVMs) trained with and without 3D normalized digital surface model (nDSM) information. We assess classification performance using multispectral imagery from the International Society for Photogrammetry and Remote Sensing (ISPRS) 2D Semantic Labeling contest and the United States Special Operations Command (USSOCOM) Urban 3D Challenge. We find that providing RGB classifiers with additional 3D nDSM information results in little increase in in-sample classification performance, suggesting that spectral information alone may be sufficient for the given classification tasks. However, we observe that providing these RGB classifiers with additional nDSM information leads to significant gains in out-of-sample predictive performance. Specifically, we observe an average improvement in out-of-sample all-class accuracy of 14.4% on the ISPRS dataset and an average improvement in out-of-sample F1 score of 8.6% on the USSOCOM dataset. In addition, the experiments establish that nDSM information is critical in machine learning and classification settings that face training sample scarcity.



### SGNet: A Super-class Guided Network for Image Classification and Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.12898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.12898v1)
- **Published**: 2021-04-26 22:26:12+00:00
- **Updated**: 2021-04-26 22:26:12+00:00
- **Authors**: Kaidong Li, Nina Y. Wang, Yiju Yang, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Most classification models treat different object classes in parallel and the misclassifications between any two classes are treated equally. In contrast, human beings can exploit high-level information in making a prediction of an unknown object. Inspired by this observation, the paper proposes a super-class guided network (SGNet) to integrate the high-level semantic information into the network so as to increase its performance in inference. SGNet takes two-level class annotations that contain both super-class and finer class labels. The super-classes are higher-level semantic categories that consist of a certain amount of finer classes. A super-class branch (SCB), trained on super-class labels, is introduced to guide finer class prediction. At the inference time, we adopt two different strategies: Two-step inference (TSI) and direct inference (DI). TSI first predicts the super-class and then makes predictions of the corresponding finer class. On the other hand, DI directly generates predictions from the finer class branch (FCB). Extensive experiments have been performed on CIFAR-100 and MS COCO datasets. The experimental results validate the proposed approach and demonstrate its superior performance on image classification and object detection.



