# Arxiv Papers in cs.CV on 2021-08-03
### Rapid Elastic Architecture Search under Specialized Classes and Resource Constraints
- **Arxiv ID**: http://arxiv.org/abs/2108.01224v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.01224v3)
- **Published**: 2021-08-03 00:54:27+00:00
- **Updated**: 2022-03-15 23:37:46+00:00
- **Authors**: Jing Liu, Bohan Zhuang, Mingkui Tan, Xu Liu, Dinh Phung, Yuanqing Li, Jianfei Cai
- **Comment**: Tech report
- **Journal**: None
- **Summary**: In many real-world applications, we often need to handle various deployment scenarios, where the resource constraint and the superclass of interest corresponding to a group of classes are dynamically specified. How to efficiently deploy deep models for diverse deployment scenarios is a new challenge. Previous NAS approaches seek to design architectures for all classes simultaneously, which may not be optimal for some individual superclasses. A straightforward solution is to search an architecture from scratch for each deployment scenario, which however is computation-intensive and impractical. To address this, we present a novel and general framework, called Elastic Architecture Search (EAS), permitting instant specializations at runtime for diverse superclasses with various resource constraints. To this end, we first propose to effectively train an over-parameterized network via a superclass dropout strategy during training. In this way, the resulting model is robust to the subsequent superclasses dropping at inference time. Based on the well-trained over-parameterized network, we then propose an efficient architecture generator to obtain promising architectures within a single forward pass. Experiments on three image classification datasets show that EAS is able to find more compact networks with better performance while remarkably being orders of magnitude faster than state-of-the-art NAS methods, e.g., outperforming OFA (once-for-all) by 1.3% on Top-1 accuracy at a budget around 361M #MAdds on ImageNet-10. More critically, EAS is able to find compact architectures within 0.1 second for 50 deployment scenarios.



### AGAR a microbial colony dataset for deep learning detection
- **Arxiv ID**: http://arxiv.org/abs/2108.01234v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2108.01234v1)
- **Published**: 2021-08-03 01:26:41+00:00
- **Updated**: 2021-08-03 01:26:41+00:00
- **Authors**: Sylwia Majchrowska, Jarosław Pawłowski, Grzegorz Guła, Tomasz Bonus, Agata Hanas, Adam Loch, Agnieszka Pawlak, Justyna Roszkowiak, Tomasz Golan, Zuzanna Drulis-Kawa
- **Comment**: main: 9 pages, 4 figures; supplementary: 17 pages, 23 figures/tables
- **Journal**: None
- **Summary**: The Annotated Germs for Automated Recognition (AGAR) dataset is an image database of microbial colonies cultured on agar plates. It contains 18000 photos of five different microorganisms as single or mixed cultures, taken under diverse lighting conditions with two different cameras. All the images are classified into "countable", "uncountable", and "empty", with the "countable" class labeled by microbiologists with colony location and species identification (336442 colonies in total). This study describes the dataset itself and the process of its development. In the second part, the performance of selected deep neural network architectures for object detection, namely Faster R-CNN and Cascade R-CNN, was evaluated on the AGAR dataset. The results confirmed the great potential of deep learning methods to automate the process of microbe localization and classification based on Petri dish photos. Moreover, AGAR is the first publicly available dataset of this kind and size and will facilitate the future development of machine learning models. The data used in these studies can be found at https://agar.neurosys.com/.



### AcousticFusion: Fusing Sound Source Localization to Visual SLAM in Dynamic Environments
- **Arxiv ID**: http://arxiv.org/abs/2108.01246v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.01246v1)
- **Published**: 2021-08-03 02:10:26+00:00
- **Updated**: 2021-08-03 02:10:26+00:00
- **Authors**: Tianwei Zhang, Huayan Zhang, Xiaofei Li, Junfeng Chen, Tin Lun Lam, Sethu Vijayakumar
- **Comment**: Accepted by IROS-2021
- **Journal**: None
- **Summary**: Dynamic objects in the environment, such as people and other agents, lead to challenges for existing simultaneous localization and mapping (SLAM) approaches. To deal with dynamic environments, computer vision researchers usually apply some learning-based object detectors to remove these dynamic objects. However, these object detectors are computationally too expensive for mobile robot on-board processing. In practical applications, these objects output noisy sounds that can be effectively detected by on-board sound source localization. The directional information of the sound source object can be efficiently obtained by direction of sound arrival (DoA) estimation, but depth estimation is difficult. Therefore, in this paper, we propose a novel audio-visual fusion approach that fuses sound source direction into the RGB-D image and thus removes the effect of dynamic obstacles on the multi-robot SLAM system. Experimental results of multi-robot SLAM in different dynamic environments show that the proposed method uses very small computational resources to obtain very stable self-localization results.



### CanvasVAE: Learning to Generate Vector Graphic Documents
- **Arxiv ID**: http://arxiv.org/abs/2108.01249v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01249v1)
- **Published**: 2021-08-03 02:14:25+00:00
- **Updated**: 2021-08-03 02:14:25+00:00
- **Authors**: Kota Yamaguchi
- **Comment**: to be published in ICCV 2021
- **Journal**: None
- **Summary**: Vector graphic documents present visual elements in a resolution free, compact format and are often seen in creative applications. In this work, we attempt to learn a generative model of vector graphic documents. We define vector graphic documents by a multi-modal set of attributes associated to a canvas and a sequence of visual elements such as shapes, images, or texts, and train variational auto-encoders to learn the representation of the documents. We collect a new dataset of design templates from an online service that features complete document structure including occluded elements. In experiments, we show that our model, named CanvasVAE, constitutes a strong baseline for generative modeling of vector graphic documents.



### SABER: Data-Driven Motion Planner for Autonomously Navigating Heterogeneous Robots
- **Arxiv ID**: http://arxiv.org/abs/2108.01262v1
- **DOI**: 10.1109/LRA.2021.3103054
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.MA, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2108.01262v1)
- **Published**: 2021-08-03 02:56:21+00:00
- **Updated**: 2021-08-03 02:56:21+00:00
- **Authors**: Alexander Schperberg, Stephanie Tsuei, Stefano Soatto, Dennis Hong
- **Comment**: Accepted to IEEE Robotics and Automation Letters (RA-L) 2021.
  Pre-print version. The video link of the paper is:
  https://www.youtube.com/watch?v=EKCCQtN5Z6A
- **Journal**: None
- **Summary**: We present an end-to-end online motion planning framework that uses a data-driven approach to navigate a heterogeneous robot team towards a global goal while avoiding obstacles in uncertain environments. First, we use stochastic model predictive control (SMPC) to calculate control inputs that satisfy robot dynamics, and consider uncertainty during obstacle avoidance with chance constraints. Second, recurrent neural networks are used to provide a quick estimate of future state uncertainty considered in the SMPC finite-time horizon solution, which are trained on uncertainty outputs of various simultaneous localization and mapping algorithms. When two or more robots are in communication range, these uncertainties are then updated using a distributed Kalman filtering approach. Lastly, a Deep Q-learning agent is employed to serve as a high-level path planner, providing the SMPC with target positions that move the robots towards a desired global goal. Our complete methods are demonstrated on a ground and aerial robot simultaneously (code available at: https://github.com/AlexS28/SABER).



### Toward Spatially Unbiased Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2108.01285v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.01285v1)
- **Published**: 2021-08-03 04:13:03+00:00
- **Updated**: 2021-08-03 04:13:03+00:00
- **Authors**: Jooyoung Choi, Jungbeom Lee, Yonghyun Jeong, Sungroh Yoon
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Recent image generation models show remarkable generation performance. However, they mirror strong location preference in datasets, which we call spatial bias. Therefore, generators render poor samples at unseen locations and scales. We argue that the generators rely on their implicit positional encoding to render spatial content. From our observations, the generator's implicit positional encoding is translation-variant, making the generator spatially biased. To address this issue, we propose injecting explicit positional encoding at each scale of the generator. By learning the spatially unbiased generator, we facilitate the robust use of generators in multiple tasks, such as GAN inversion, multi-scale generation, generation of arbitrary sizes and aspect ratios. Furthermore, we show that our method can also be applied to denoising diffusion probabilistic models.



### Deep Rival Penalized Competitive Learning for Low-resolution Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.01286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01286v1)
- **Published**: 2021-08-03 04:14:56+00:00
- **Updated**: 2021-08-03 04:14:56+00:00
- **Authors**: Peiying Li, Shikui Tu, Lei Xu
- **Comment**: 30 pages, 10 figures
- **Journal**: None
- **Summary**: Current face recognition tasks are usually carried out on high-quality face images, but in reality, most face images are captured under unconstrained or poor conditions, e.g., by video surveillance. Existing methods are featured by learning data uncertainty to avoid overfitting the noise, or by adding margins to the angle or cosine space of the normalized softmax loss to penalize the target logit, which enforces intra-class compactness and inter-class discrepancy. In this paper, we propose a deep Rival Penalized Competitive Learning (RPCL) for deep face recognition in low-resolution (LR) images. Inspired by the idea of the RPCL, our method further enforces regulation on the rival logit, which is defined as the largest non-target logit for an input image. Different from existing methods that only consider penalization on the target logit, our method not only strengthens the learning towards the target label, but also enforces a reverse direction, i.e., becoming de-learning, away from the rival label. Comprehensive experiments demonstrate that our method improves the existing state-of-the-art methods to be very robust for LR face recognition.



### Dynamic Feature Regularized Loss for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.01296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01296v1)
- **Published**: 2021-08-03 05:11:00+00:00
- **Updated**: 2021-08-03 05:11:00+00:00
- **Authors**: Bingfeng Zhang, Jimin Xiao, Yao Zhao
- **Comment**: Under Review
- **Journal**: None
- **Summary**: We focus on tackling weakly supervised semantic segmentation with scribble-level annotation. The regularized loss has been proven to be an effective solution for this task. However, most existing regularized losses only leverage static shallow features (color, spatial information) to compute the regularized kernel, which limits its final performance since such static shallow features fail to describe pair-wise pixel relationship in complicated cases. In this paper, we propose a new regularized loss which utilizes both shallow and deep features that are dynamically updated in order to aggregate sufficient information to represent the relationship of different pixels. Moreover, in order to provide accurate deep features, we adopt vision transformer as the backbone and design a feature consistency head to train the pair-wise feature relationship. Unlike most approaches that adopt multi-stage training strategy with many bells and whistles, our approach can be directly trained in an end-to-end manner, in which the feature consistency head and our regularized loss can benefit from each other. Extensive experiments show that our approach achieves new state-of-the-art performances, outperforming other approaches by a significant margin with more than 6\% mIoU increase.



### Skeleton Split Strategies for Spatial Temporal Graph Convolution Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.01309v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.01309v1)
- **Published**: 2021-08-03 05:57:52+00:00
- **Updated**: 2021-08-03 05:57:52+00:00
- **Authors**: Motasem S. Alsawadi, Miguel Rio
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: A skeleton representation of the human body has been proven to be effective for this task. The skeletons are presented in graphs form-like. However, the topology of a graph is not structured like Euclidean-based data. Therefore, a new set of methods to perform the convolution operation upon the skeleton graph is presented. Our proposal is based upon the ST-GCN framework proposed by Yan et al. [1]. In this study, we present an improved set of label mapping methods for the ST-GCN framework. We introduce three split processes (full distance split, connection split, and index split) as an alternative approach for the convolution operation. To evaluate the performance, the experiments presented in this study have been trained using two benchmark datasets: NTU-RGB+D and Kinetics. Our results indicate that all of our split processes outperform the previous partition strategies and are more stable during training without using the edge importance weighting additional training parameter. Therefore, our proposal can provide a more realistic solution for real-time applications centred on daily living recognition systems activities for indoor environments.



### RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2108.01316v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.01316v1)
- **Published**: 2021-08-03 06:30:30+00:00
- **Updated**: 2021-08-03 06:30:30+00:00
- **Authors**: Jiachen Li, Fan Yang, Hengbo Ma, Srikanth Malla, Masayoshi Tomizuka, Chiho Choi
- **Comment**: ICCV 2021 (Project website:
  https://jiachenli94.github.io/publications/RAIN/)
- **Journal**: None
- **Summary**: Motion forecasting plays a significant role in various domains (e.g., autonomous driving, human-robot interaction), which aims to predict future motion sequences given a set of historical observations. However, the observed elements may be of different levels of importance. Some information may be irrelevant or even distracting to the forecasting in certain situations. To address this issue, we propose a generic motion forecasting framework (named RAIN) with dynamic key information selection and ranking based on a hybrid attention mechanism. The general framework is instantiated to handle multi-agent trajectory prediction and human motion forecasting tasks, respectively. In the former task, the model learns to recognize the relations between agents with a graph representation and to determine their relative significance. In the latter task, the model learns to capture the temporal proximity and dependency in long-term human motions. We also propose an effective double-stage training pipeline with an alternating training strategy to optimize the parameters in different modules of the framework. We validate the framework on both synthetic simulations and motion forecasting benchmarks in different domains, demonstrating that our method not only achieves state-of-the-art forecasting performance, but also provides interpretable and reasonable hybrid attention weights.



### Predicting Popularity of Images Over 30 Days
- **Arxiv ID**: http://arxiv.org/abs/2108.01326v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.01326v1)
- **Published**: 2021-08-03 07:04:27+00:00
- **Updated**: 2021-08-03 07:04:27+00:00
- **Authors**: Amartya Dutta, Ferdous Ahmed Barbhuiya
- **Comment**: Accepted in ICIP Image Popularity Prediction Challenge 2020, Won 3rd
  Position in Challenge
- **Journal**: None
- **Summary**: The current work deals with the problem of attempting to predict the popularity of images before even being uploaded. This method is specifically focused on Flickr images. Social features of each image as well as that of the user who had uploaded it, have been recorded. The dataset also includes the engagement score of each image which is the ground truth value of the views obtained by each image over a period of 30 days. The work aims to predict the popularity of images on Flickr over a period of 30 days using the social features of the user and the image, as well as the visual features of the images. The method states that the engagement sequence of an image can be said to depend on two independent quantities, namely scale and shape of an image. Once the shape and scale of an image have been predicted, combining them the predicted sequence of an image over 30 days is obtained. The current work follows a previous work done in the same direction, with certain speculations and suggestions of improvement.



### Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability
- **Arxiv ID**: http://arxiv.org/abs/2108.01335v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.01335v2)
- **Published**: 2021-08-03 07:32:34+00:00
- **Updated**: 2022-10-10 03:54:30+00:00
- **Authors**: Roman Levin, Manli Shu, Eitan Borgnia, Furong Huang, Micah Goldblum, Tom Goldstein
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional saliency maps highlight input features to which neural network predictions are highly sensitive. We take a different approach to saliency, in which we identify and analyze the network parameters, rather than inputs, which are responsible for erroneous decisions. We find that samples which cause similar parameters to malfunction are semantically similar. We also show that pruning the most salient parameters for a wrongly classified sample often improves model behavior. Furthermore, fine-tuning a small number of the most salient parameters on a single sample results in error correction on other samples that are misclassified for similar reasons. Based on our parameter saliency method, we also introduce an input-space saliency technique that reveals how image features cause specific network components to malfunction. Further, we rigorously validate the meaningfulness of our saliency maps on both the dataset and case-study levels.



### I3CL:Intra- and Inter-Instance Collaborative Learning for Arbitrary-shaped Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.01343v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01343v3)
- **Published**: 2021-08-03 07:48:12+00:00
- **Updated**: 2022-04-20 07:04:46+00:00
- **Authors**: Bo Du, Jian Ye, Jing Zhang, Juhua Liu, Dacheng Tao
- **Comment**: IJCV. Code is available at
  https://github.com/ViTAE-Transformer/ViTAE-Transformer-Scene-Text-Detection
- **Journal**: None
- **Summary**: Existing methods for arbitrary-shaped text detection in natural scenes face two critical issues, i.e., 1) fracture detections at the gaps in a text instance; and 2) inaccurate detections of arbitrary-shaped text instances with diverse background context. To address these issues, we propose a novel method named Intra- and Inter-Instance Collaborative Learning (I3CL). Specifically, to address the first issue, we design an effective convolutional module with multiple receptive fields, which is able to collaboratively learn better character and gap feature representations at local and long ranges inside a text instance. To address the second issue, we devise an instance-based transformer module to exploit the dependencies between different text instances and a global context module to exploit the semantic context from the shared background, which are able to collaboratively learn more discriminative text feature representation. In this way, I3CL can effectively exploit the intra- and inter-instance dependencies together in a unified end-to-end trainable framework. Besides, to make full use of the unlabeled data, we design an effective semi-supervised learning method to leverage the pseudo labels via an ensemble strategy. Without bells and whistles, experimental results show that the proposed I3CL sets new state-of-the-art results on three challenging public benchmarks, i.e., an F-measure of 77.5% on ICDAR2019-ArT, 86.9% on Total-Text, and 86.4% on CTW-1500. Notably, our I3CL with the ResNeSt-101 backbone ranked 1st place on the ICDAR2019-ArT leaderboard. The source code will be available at https://github.com/ViTAE-Transformer/ViTAE-Transformer-Scene-Text-Detection.



### Adaptive Affinity Loss and Erroneous Pseudo-Label Refinement for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.01344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01344v1)
- **Published**: 2021-08-03 07:48:33+00:00
- **Updated**: 2021-08-03 07:48:33+00:00
- **Authors**: Xiangrong Zhang, Zelin Peng, Peng Zhu, Tianyang Zhang, Chen Li, Huiyu Zhou, Licheng Jiao
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation has been continuously investigated in the last ten years, and majority of the established technologies are based on supervised models. In recent years, image-level weakly supervised semantic segmentation (WSSS), including single- and multi-stage process, has attracted large attention due to data labeling efficiency. In this paper, we propose to embed affinity learning of multi-stage approaches in a single-stage model. To be specific, we introduce an adaptive affinity loss to thoroughly learn the local pairwise affinity. As such, a deep neural network is used to deliver comprehensive semantic information in the training phase, whilst improving the performance of the final prediction module. On the other hand, considering the existence of errors in the pseudo labels, we propose a novel label reassign loss to mitigate over-fitting. Extensive experiments are conducted on the PASCAL VOC 2012 dataset to evaluate the effectiveness of our proposed approach that outperforms other standard single-stage methods and achieves comparable performance against several multi-stage methods.



### Cycle-Consistent Inverse GAN for Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2108.01361v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01361v2)
- **Published**: 2021-08-03 08:38:16+00:00
- **Updated**: 2021-09-12 12:35:33+00:00
- **Authors**: Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao
- **Comment**: Accepted at ACM MM 2021
- **Journal**: None
- **Summary**: This paper investigates an open research task of text-to-image synthesis for automatically generating or manipulating images from text descriptions. Prevailing methods mainly use the text as conditions for GAN generation, and train different models for the text-guided image generation and manipulation tasks. In this paper, we propose a novel unified framework of Cycle-consistent Inverse GAN (CI-GAN) for both text-to-image generation and text-guided image manipulation tasks. Specifically, we first train a GAN model without text input, aiming to generate images with high diversity and quality. Then we learn a GAN inversion model to convert the images back to the GAN latent space and obtain the inverted latent codes for each image, where we introduce the cycle-consistency training to learn more robust and consistent inverted latent codes. We further uncover the latent space semantics of the trained GAN model, by learning a similarity model between text representations and the latent codes. In the text-guided optimization module, we generate images with the desired semantic attributes by optimizing the inverted latent codes. Extensive experiments on the Recipe1M and CUB datasets validate the efficacy of our proposed framework.



### Robust Compressed Sensing MRI with Deep Generative Priors
- **Arxiv ID**: http://arxiv.org/abs/2108.01368v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2108.01368v2)
- **Published**: 2021-08-03 08:52:06+00:00
- **Updated**: 2021-12-06 16:35:41+00:00
- **Authors**: Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G. Dimakis, Jonathan I. Tamir
- **Comment**: None
- **Journal**: None
- **Summary**: The CSGM framework (Bora-Jalal-Price-Dimakis'17) has shown that deep generative priors can be powerful tools for solving inverse problems. However, to date this framework has been empirically successful only on certain datasets (for example, human faces and MNIST digits), and it is known to perform poorly on out-of-distribution samples. In this paper, we present the first successful application of the CSGM framework on clinical MRI data. We train a generative prior on brain scans from the fastMRI dataset, and show that posterior sampling via Langevin dynamics achieves high quality reconstructions. Furthermore, our experiments and theory show that posterior sampling is robust to changes in the ground-truth distribution and measurement process. Our code and models are available at: \url{https://github.com/utcsilab/csgm-mri-langevin}.



### Classifying action correctness in physical rehabilitation exercises
- **Arxiv ID**: http://arxiv.org/abs/2108.01375v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.01375v1)
- **Published**: 2021-08-03 09:00:05+00:00
- **Updated**: 2021-08-03 09:00:05+00:00
- **Authors**: Alina Miron, Crina Grosan
- **Comment**: IJCAI 2018, ARIAL workshop, 2nd Workshop on AI for Aging,
  Rehabilitation and Independent Assisted Living
- **Journal**: None
- **Summary**: The work in this paper focuses on the role of machine learning in assessing the correctness of a human motion or action. This task proves to be more challenging than the gesture and action recognition ones. We will demonstrate, through a set of experiments on a recent dataset, that machine learning algorithms can produce good results for certain actions, but can also fall into the trap of classifying an incorrect execution of an action as a correct execution of another action.



### MixMicrobleedNet: segmentation of cerebral microbleeds using nnU-Net
- **Arxiv ID**: http://arxiv.org/abs/2108.01389v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.01389v1)
- **Published**: 2021-08-03 09:52:13+00:00
- **Updated**: 2021-08-03 09:52:13+00:00
- **Authors**: Hugo J. Kuijf
- **Comment**: Submitted to the "Where is VALDO?'' challeng, MICCAI 2021
- **Journal**: None
- **Summary**: Cerebral microbleeds are small hypointense lesions visible on magnetic resonance imaging (MRI) with gradient echo, T2*, or susceptibility weighted (SWI) imaging. Assessment of cerebral microbleeds is mostly performed by visual inspection. The past decade has seen the rise of semi-automatic tools to assist with rating and more recently fully automatic tools for microbleed detection. In this work, we explore the use of nnU-Net as a fully automated tool for microbleed segmentation. Data was provided by the ``Where is VALDO?'' challenge of MICCAI 2021. The final method consists of nnU-Net in the ``3D full resolution U-Net'' configuration trained on all data (fold = `all'). No post-processing options of nnU-Net were used. Self-evaluation on the training data showed an estimated Dice of 0.80, false discovery rate of 0.16, and false negative rate of 0.15. Final evaluation on the test set of the VALDO challenge is pending. Visual inspection of the results showed that most of the reported false positives could be an actual microbleed that might have been missed during visual rating. Source code is available at: https://github.com/hjkuijf/MixMicrobleedNet . The docker container hjkuijf/mixmicrobleednet can be pulled from https://hub.docker.com/r/hjkuijf/mixmicrobleednet .



### Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2108.01390v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01390v5)
- **Published**: 2021-08-03 09:56:07+00:00
- **Updated**: 2021-12-06 15:28:49+00:00
- **Authors**: Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke Li, Weiming Dong, Liqing Zhang, Changsheng Xu, Xing Sun
- **Comment**: We propose a novel and effective design for dynamic vision
  transformer to achieve better computational efficiency. The code is available
  at https://github.com/YifanXu74/Evo-ViT
- **Journal**: None
- **Summary**: Vision transformers (ViTs) have recently received explosive popularity, but the huge computational cost is still a severe issue. Since the computation complexity of ViT is quadratic with respect to the input sequence length, a mainstream paradigm for computation reduction is to reduce the number of tokens. Existing designs include structured spatial compression that uses a progressive shrinking pyramid to reduce the computations of large feature maps, and unstructured token pruning that dynamically drops redundant tokens. However, the limitation of existing token pruning lies in two folds: 1) the incomplete spatial structure caused by pruning is not compatible with structured spatial compression that is commonly used in modern deep-narrow transformers; 2) it usually requires a time-consuming pre-training procedure. To tackle the limitations and expand the applicable scenario of token pruning, we present Evo-ViT, a self-motivated slow-fast token evolution approach for vision transformers. Specifically, we conduct unstructured instance-wise token selection by taking advantage of the simple and effective global class attention that is native to vision transformers. Then, we propose to update the selected informative tokens and uninformative tokens with different computation paths, namely, slow-fast updating. Since slow-fast updating mechanism maintains the spatial structure and information flow, Evo-ViT can accelerate vanilla transformers of both flat and deep-narrow structures from the very beginning of the training process. Experimental results demonstrate that our method significantly reduces the computational cost of vision transformers while maintaining comparable performance on image classification.



### AI Based Waste classifier with Thermo-Rapid Composting
- **Arxiv ID**: http://arxiv.org/abs/2108.01394v1
- **DOI**: 10.1109/ICPECTS49113.2020.9337012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01394v1)
- **Published**: 2021-08-03 10:06:19+00:00
- **Updated**: 2021-08-03 10:06:19+00:00
- **Authors**: Saswati kumari behera, Aouthithiye Barathwaj SR Y, Vasundhara L, Saisudha G, Haariharan N C
- **Comment**: 4 pages, 8 figures, conference
- **Journal**: None
- **Summary**: Waste management is a certainly a very complex and difficult process especially in very large cities. It needs immense man power and also uses up other resources such as electricity and fuel. This creates a need to use a novel method with help of latest technologies. Here in this article we present a new waste classification technique using Computer Vision (CV) and deep learning (DL). To further improve waste classification ability, support machine vectors (SVM) are used. We also decompose the degradable waste with help of rapid composting. In this article we have mainly worked on segregation of municipal solid waste (MSW). For this model, we use YOLOv3 (You Only Look Once) a computer vision-based algorithm popularly used to detect objects which is developed based on Convolution Neural Networks (CNNs) which is a machine learning (ML) based tool. They are extensively used to extract features from a data especially image-oriented data. In this article we propose a waste classification technique which will be faster and more efficient. And we decompose the biodegradable waste by Berkley Method of composting (BKC)



### Region-wise Loss for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.01405v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.01405v2)
- **Published**: 2021-08-03 10:38:21+00:00
- **Updated**: 2022-03-29 12:13:23+00:00
- **Authors**: Juan Miguel Valverde, Jussi Tohka
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Region-wise (RW) loss for biomedical image segmentation. Region-wise loss is versatile, can simultaneously account for class imbalance and pixel importance, and it can be easily implemented as the pixel-wise multiplication between the softmax output and a RW map. We show that, under the proposed RW loss framework, certain loss functions, such as Active Contour and Boundary loss, can be reformulated similarly with appropriate RW maps, thus revealing their underlying similarities and a new perspective to understand these loss functions. We investigate the observed optimization instability caused by certain RW maps, such as Boundary loss distance maps, and we introduce a mathematically-grounded principle to avoid such instability. This principle provides excellent adaptability to any dataset and practically ensures convergence without extra regularization terms or optimization tricks. Following this principle, we propose a simple version of boundary distance maps called rectified Region-wise (RRW) maps that, as we demonstrate in our experiments, achieve state-of-the-art performance with similar or better Dice coefficients and Hausdorff distances than Dice, Focal, weighted Cross entropy, and Boundary losses in three distinct segmentation tasks. We quantify the optimization instability provided by Boundary loss distance maps, and we empirically show that our RRW maps are stable to optimize. The code to run all our experiments is publicly available at: https://github.com/jmlipman/RegionWiseLoss.



### HyperColor: A HyperNetwork Approach for Synthesizing Auto-colored 3D Models for Game Scenes Population
- **Arxiv ID**: http://arxiv.org/abs/2108.01411v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2108.01411v1)
- **Published**: 2021-08-03 11:00:08+00:00
- **Updated**: 2021-08-03 11:00:08+00:00
- **Authors**: Ivan Kostiuk, Przemysław Stachura, Sławomir K. Tadeja, Tomasz Trzciński, Przemysław Spurek
- **Comment**: None
- **Journal**: None
- **Summary**: Designing a 3D game scene is a tedious task that often requires a substantial amount of work. Typically, this task involves synthesis, coloring, and placement of 3D models within the game scene. To lessen this workload, we can apply machine learning to automate some aspects of the game scene development. Earlier research has already tackled automated generation of the game scene background with machine learning. However, model auto-coloring remains an underexplored problem. The automatic coloring of a 3D model is a challenging task, especially when dealing with the digital representation of a colorful, multipart object. In such a case, we have to ``understand'' the object's composition and coloring scheme of each part. Existing single-stage methods have their own caveats such as the need for segmentation of the object or generating individual parts that have to be assembled together to yield the final model. We address these limitations by proposing a two-stage training approach to synthesize auto-colored 3D models. In the first stage, we obtain a 3D point cloud representing a 3D object, whilst in the second stage, we assign colors to points within such cloud. Next, by leveraging the so-called triangulation trick, we generate a 3D mesh in which the surfaces are colored based on interpolation of colored points representing vertices of a given mesh triangle. This approach allows us to generate a smooth coloring scheme. Experimental evaluation shows that our two-stage approach gives better results in terms of shape reconstruction and coloring when compared to traditional single-stage techniques.



### Noise-Resistant Deep Metric Learning with Probabilistic Instance Filtering
- **Arxiv ID**: http://arxiv.org/abs/2108.01431v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.01431v2)
- **Published**: 2021-08-03 12:15:25+00:00
- **Updated**: 2021-12-17 09:35:24+00:00
- **Authors**: Chang Liu, Han Yu, Boyang Li, Zhiqi Shen, Zhanning Gao, Peiran Ren, Xuansong Xie, Lizhen Cui, Chunyan Miao
- **Comment**: Under review. Journal version of arXiv:2103.16047
- **Journal**: None
- **Summary**: Noisy labels are commonly found in real-world data, which cause performance degradation of deep neural networks. Cleaning data manually is labour-intensive and time-consuming. Previous research mostly focuses on enhancing classification models against noisy labels, while the robustness of deep metric learning (DML) against noisy labels remains less well-explored. In this paper, we bridge this important gap by proposing Probabilistic Ranking-based Instance Selection with Memory (PRISM) approach for DML. PRISM calculates the probability of a label being clean, and filters out potentially noisy samples. Specifically, we propose a novel method, namely the von Mises-Fisher Distribution Similarity (vMF-Sim), to calculate this probability by estimating a von Mises-Fisher (vMF) distribution for each data class. Compared with the existing average similarity method (AvgSim), vMF-Sim considers the variance of each class in addition to the average similarity. With such a design, the proposed approach can deal with challenging DML situations in which the majority of the samples are noisy. Extensive experiments on both synthetic and real-world noisy dataset show that the proposed approach achieves up to 8.37% higher Precision@1 compared with the best performing state-of-the-art baseline approaches, within reasonable training time.



### Wavelet-Based Network For High Dynamic Range Imaging
- **Arxiv ID**: http://arxiv.org/abs/2108.01434v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.01434v2)
- **Published**: 2021-08-03 12:26:33+00:00
- **Updated**: 2022-02-08 19:30:33+00:00
- **Authors**: Tianhong Dai, Wei Li, Xilei Cao, Jianzhuang Liu, Xu Jia, Ales Leonardis, Youliang Yan, Shanxin Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: High dynamic range (HDR) imaging from multiple low dynamic range (LDR) images has been suffering from ghosting artifacts caused by scene and objects motion. Existing methods, such as optical flow based and end-to-end deep learning based solutions, are error-prone either in detail restoration or ghosting artifacts removal. Comprehensive empirical evidence shows that ghosting artifacts caused by large foreground motion are mainly low-frequency signals and the details are mainly high-frequency signals. In this work, we propose a novel frequency-guided end-to-end deep neural network (FHDRNet) to conduct HDR fusion in the frequency domain, and Discrete Wavelet Transform (DWT) is used to decompose inputs into different frequency bands. The low-frequency signals are used to avoid specific ghosting artifacts, while the high-frequency signals are used for preserving details. Using a U-Net as the backbone, we propose two novel modules: merging module and frequency-guided upsampling module. The merging module applies the attention mechanism to the low-frequency components to deal with the ghost caused by large foreground motion. The frequency-guided upsampling module reconstructs details from multiple frequency-specific components with rich details. In addition, a new RAW dataset is created for training and evaluating multi-frame HDR imaging algorithms in the RAW domain. Extensive experiments are conducted on public datasets and our RAW dataset, showing that the proposed FHDRNet achieves state-of-the-art performance.



### Cross-Modal Analysis of Human Detection for Robotics: An Industrial Case Study
- **Arxiv ID**: http://arxiv.org/abs/2108.01495v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.01495v1)
- **Published**: 2021-08-03 13:33:37+00:00
- **Updated**: 2021-08-03 13:33:37+00:00
- **Authors**: Timm Linder, Narunas Vaskevicius, Robert Schirmer, Kai O. Arras
- **Comment**: Accepted for publication at 2021 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS)
- **Journal**: None
- **Summary**: Advances in sensing and learning algorithms have led to increasingly mature solutions for human detection by robots, particularly in selected use-cases such as pedestrian detection for self-driving cars or close-range person detection in consumer settings. Despite this progress, the simple question "which sensor-algorithm combination is best suited for a person detection task at hand?" remains hard to answer. In this paper, we tackle this issue by conducting a systematic cross-modal analysis of sensor-algorithm combinations typically used in robotics. We compare the performance of state-of-the-art person detectors for 2D range data, 3D lidar, and RGB-D data as well as selected combinations thereof in a challenging industrial use-case.   We further address the related problems of data scarcity in the industrial target domain, and that recent research on human detection in 3D point clouds has mostly focused on autonomous driving scenarios. To leverage these methodological advances for robotics applications, we utilize a simple, yet effective multi-sensor transfer learning strategy by extending a strong image-based RGB-D detector to provide cross-modal supervision for lidar detectors in the form of weak 3D bounding box labels.   Our results show a large variance among the different approaches in terms of detection performance, generalization, frame rates and computational requirements. As our use-case contains difficulties representative for a wide range of service robot applications, we believe that these results point to relevant open challenges for further research and provide valuable support to practitioners for the design of their robot system.



### Boosting Weakly Supervised Object Detection via Learning Bounding Box Adjusters
- **Arxiv ID**: http://arxiv.org/abs/2108.01499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01499v1)
- **Published**: 2021-08-03 13:38:20+00:00
- **Updated**: 2021-08-03 13:38:20+00:00
- **Authors**: Bowen Dong, Zitong Huang, Yuelin Guo, Qilong Wang, Zhenxing Niu, Wangmeng Zuo
- **Comment**: ICCV 2021 (poster)
- **Journal**: None
- **Summary**: Weakly-supervised object detection (WSOD) has emerged as an inspiring recent topic to avoid expensive instance-level object annotations. However, the bounding boxes of most existing WSOD methods are mainly determined by precomputed proposals, thereby being limited in precise object localization. In this paper, we defend the problem setting for improving localization performance by leveraging the bounding box regression knowledge from a well-annotated auxiliary dataset. First, we use the well-annotated auxiliary dataset to explore a series of learnable bounding box adjusters (LBBAs) in a multi-stage training manner, which is class-agnostic. Then, only LBBAs and a weakly-annotated dataset with non-overlapped classes are used for training LBBA-boosted WSOD. As such, our LBBAs are practically more convenient and economical to implement while avoiding the leakage of the auxiliary well-annotated dataset. In particular, we formulate learning bounding box adjusters as a bi-level optimization problem and suggest an EM-like multi-stage training algorithm. Then, a multi-stage scheme is further presented for LBBA-boosted WSOD. Additionally, a masking strategy is adopted to improve proposal classification. Experimental results verify the effectiveness of our method. Our method performs favorably against state-of-the-art WSOD methods and knowledge transfer model with similar problem setting. Code is publicly available at \url{https://github.com/DongSky/lbba_boosted_wsod}.



### SphereFace2: Binary Classification is All You Need for Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.01513v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.01513v3)
- **Published**: 2021-08-03 13:58:45+00:00
- **Updated**: 2022-04-11 03:49:28+00:00
- **Authors**: Yandong Wen, Weiyang Liu, Adrian Weller, Bhiksha Raj, Rita Singh
- **Comment**: ICLR 2022 Spotlight (v3: Updated Appendix)
- **Journal**: None
- **Summary**: State-of-the-art deep face recognition methods are mostly trained with a softmax-based multi-class classification framework. Despite being popular and effective, these methods still have a few shortcomings that limit empirical performance. In this paper, we start by identifying the discrepancy between training and evaluation in the existing multi-class classification framework and then discuss the potential limitations caused by the "competitive" nature of softmax normalization. Motivated by these limitations, we propose a novel binary classification training framework, termed SphereFace2. In contrast to existing methods, SphereFace2 circumvents the softmax normalization, as well as the corresponding closed-set assumption. This effectively bridges the gap between training and evaluation, enabling the representations to be improved individually by each binary classification task. Besides designing a specific well-performing loss function, we summarize a few general principles for this "one-vs-all" binary classification framework so that it can outperform current competitive methods. Our experiments on popular benchmarks demonstrate that SphereFace2 can consistently outperform state-of-the-art deep face recognition methods. The code has been made publicly available.



### Two New Stenosis Detection Methods of Coronary Angiograms
- **Arxiv ID**: http://arxiv.org/abs/2108.01516v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.01516v2)
- **Published**: 2021-08-03 14:01:58+00:00
- **Updated**: 2021-12-14 09:26:39+00:00
- **Authors**: Yaofang Liu, Xinyue Zhang, Wenlong Wan, Shaoyu Liu, Yingdi Liu, Hu Liu, Xueying Zeng, Qing Zhang
- **Comment**: Correspondence should be addressed to Qing Zhang
- **Journal**: None
- **Summary**: Coronary angiography is the "gold standard" for diagnosing coronary artery disease (CAD). At present, the methods for detecting and evaluating coronary artery stenosis cannot satisfy the clinical needs, e.g., there is no prior study of detecting stenoses in prespecified vessel segments, which is necessary in clinical practice. Two vascular stenosis detection methods are proposed to assist the diagnosis. The first one is an automatic method, which can automatically extract the entire coronary artery tree and mark all the possible stenoses. The second one is an interactive method. With this method, the user can choose any vessel segment to do further analysis of its stenoses. Experiments show that the proposed methods are robust for angiograms with various vessel structures. The precision, sensitivity, and $F_1$ score of the automatic stenosis detection method are 0.821, 0.757, and 0.788, respectively. Further investigation proves that the interactive method can provide a more precise outcome of stenosis detection, and our quantitative analysis is closer to reality. The proposed automatic method and interactive method are effective and can complement each other in clinical practice. The first method can be used for preliminary screening, and the second method can be used for further quantitative analysis. We believe the proposed solution is more suitable for the clinical diagnosis of CAD.



### Non-local Graph Convolutional Network for joint Activity Recognition and Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2108.01518v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.01518v1)
- **Published**: 2021-08-03 14:07:10+00:00
- **Updated**: 2021-08-03 14:07:10+00:00
- **Authors**: Dianhao Zhang, Ngo Anh Vien, Mien Van, Sean McLoone
- **Comment**: Accepted to IROS 2021
- **Journal**: None
- **Summary**: 3D skeleton-based motion prediction and activity recognition are two interwoven tasks in human behaviour analysis. In this work, we propose a motion context modeling methodology that provides a new way to combine the advantages of both graph convolutional neural networks and recurrent neural networks for joint human motion prediction and activity recognition. Our approach is based on using an LSTM encoder-decoder and a non-local feature extraction attention mechanism to model the spatial correlation of human skeleton data and temporal correlation among motion frames. The proposed network can easily include two output branches, one for Activity Recognition and one for Future Motion Prediction, which can be jointly trained for enhanced performance. Experimental results on Human 3.6M, CMU Mocap and NTU RGB-D datasets show that our proposed approach provides the best prediction capability among baseline LSTM-based methods, while achieving comparable performance to other state-of-the-art methods.



### Double-Dot Network for Antipodal Grasp Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.01527v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.01527v1)
- **Published**: 2021-08-03 14:21:17+00:00
- **Updated**: 2021-08-03 14:21:17+00:00
- **Authors**: Yao Wang, Yangtao Zheng, Boyang Gao, Di Huang
- **Comment**: Preprint. 8 pages. Accepted at IROS 2021
- **Journal**: None
- **Summary**: This paper proposes a new deep learning approach to antipodal grasp detection, named Double-Dot Network (DD-Net). It follows the recent anchor-free object detection framework, which does not depend on empirically pre-set anchors and thus allows more generalized and flexible prediction on unseen objects. Specifically, unlike the widely used 5-dimensional rectangle, the gripper configuration is defined as a pair of fingertips. An effective CNN architecture is introduced to localize such fingertips, and with the help of auxiliary centers for refinement, it accurately and robustly infers grasp candidates. Additionally, we design a specialized loss function to measure the quality of grasps, and in contrast to the IoU scores of bounding boxes adopted in object detection, it is more consistent to the grasp detection task. Both the simulation and robotic experiments are executed and state of the art accuracies are achieved, showing that DD-Net is superior to the counterparts in handling unseen objects.



### Inference via Sparse Coding in a Hierarchical Vision Model
- **Arxiv ID**: http://arxiv.org/abs/2108.01548v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.01548v3)
- **Published**: 2021-08-03 14:55:33+00:00
- **Updated**: 2022-01-16 18:34:26+00:00
- **Authors**: Joshua Bowren, Luis Sanchez-Giraldo, Odelia Schwartz
- **Comment**: To appear in Journal of Vision (JoV)
- **Journal**: None
- **Summary**: Sparse coding has been incorporated in models of the visual cortex for its computational advantages and connection to biology. But how the level of sparsity contributes to performance on visual tasks is not well understood. In this work, sparse coding has been integrated into an existing hierarchical V2 model (Hosoya and Hyv\"arinen, 2015), but replacing its independent component analysis (ICA) with an explicit sparse coding in which the degree of sparsity can be controlled. After training, the sparse coding basis functions with a higher degree of sparsity resembled qualitatively different structures, such as curves and corners. The contributions of the models were assessed with image classification tasks, specifically tasks associated with mid-level vision including figure-ground classification, texture classification, and angle prediction between two line stimuli. In addition, the models were assessed in comparison to a texture sensitivity measure that has been reported in V2 (Freeman et al., 2013), and a deleted-region inference task. The results from the experiments show that while sparse coding performed worse than ICA at classifying images, only sparse coding was able to better match the texture sensitivity level of V2 and infer deleted image regions, both by increasing the degree of sparsity in sparse coding. Higher degrees of sparsity allowed for inference over larger deleted image regions. The mechanism that allows for this inference capability in sparse coding is described here.



### Domain Adaptor Networks for Hyperspectral Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.01555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01555v1)
- **Published**: 2021-08-03 15:06:39+00:00
- **Updated**: 2021-08-03 15:06:39+00:00
- **Authors**: Gustavo Perez, Subhransu Maji
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of adapting a network trained on three-channel color images to a hyperspectral domain with a large number of channels. To this end, we propose domain adaptor networks that map the input to be compatible with a network trained on large-scale color image datasets such as ImageNet. Adaptors enable learning on small hyperspectral datasets where training a network from scratch may not be effective. We investigate architectures and strategies for training adaptors and evaluate them on a benchmark consisting of multiple hyperspectral datasets. We find that simple schemes such as linear projection or subset selection are often the most effective, but can lead to a loss in performance in some cases. We also propose a novel multi-view adaptor where of the inputs are combined in an intermediate layer of the network in an order invariant manner that provides further improvements. We present extensive experiments by varying the number of training examples in the benchmark to characterize the accuracy and computational trade-offs offered by these adaptors.



### Deep GAN-Based Cross-Spectral Cross-Resolution Iris Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.01569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01569v1)
- **Published**: 2021-08-03 15:30:04+00:00
- **Updated**: 2021-08-03 15:30:04+00:00
- **Authors**: Moktari Mostofa, Salman Mohamadi, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: 20 pages, 12 figures, Accepted to IEEE TRANSACTIONS ON BIOMETRICS,
  BEHAVIOR, AND IDENTITY SCIENCE (T-BIOM). arXiv admin note: text overlap with
  arXiv:2010.11689
- **Journal**: None
- **Summary**: In recent years, cross-spectral iris recognition has emerged as a promising biometric approach to establish the identity of individuals. However, matching iris images acquired at different spectral bands (i.e., matching a visible (VIS) iris probe to a gallery of near-infrared (NIR) iris images or vice versa) shows a significant performance degradation when compared to intraband NIR matching. Hence, in this paper, we have investigated a range of deep convolutional generative adversarial network (DCGAN) architectures to further improve the accuracy of cross-spectral iris recognition methods. Moreover, unlike the existing works in the literature, we introduce a resolution difference into the classical cross-spectral matching problem domain. We have developed two different techniques using the conditional generative adversarial network (cGAN) as a backbone architecture for cross-spectral iris matching. In the first approach, we simultaneously address the cross-resolution and cross-spectral matching problem by training a cGAN that jointly translates cross-resolution as well as cross-spectral tasks to the same resolution and within the same spectrum. In the second approach, we design a coupled generative adversarial network (cpGAN) architecture consisting of a pair of cGAN modules that project the VIS and NIR iris images into a low-dimensional embedding domain to ensure maximum pairwise similarity between the feature vectors from the two iris modalities of the same subject.



### SPG-VTON: Semantic Prediction Guidance for Multi-pose Virtual Try-on
- **Arxiv ID**: http://arxiv.org/abs/2108.01578v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01578v2)
- **Published**: 2021-08-03 15:40:50+00:00
- **Updated**: 2022-10-17 01:32:27+00:00
- **Authors**: Bingwen Hu, Ping Liu, Zhedong Zheng, Mingwu Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Image-based virtual try-on is challenging in fitting a target in-shop clothes into a reference person under diverse human poses. Previous works focus on preserving clothing details ( e.g., texture, logos, patterns ) when transferring desired clothes onto a target person under a fixed pose. However, the performances of existing methods significantly dropped when extending existing methods to multi-pose virtual try-on. In this paper, we propose an end-to-end Semantic Prediction Guidance multi-pose Virtual Try-On Network (SPG-VTON), which could fit the desired clothing into a reference person under arbitrary poses. Concretely, SPG-VTON is composed of three sub-modules. First, a Semantic Prediction Module (SPM) generates the desired semantic map. The predicted semantic map provides more abundant guidance to locate the desired clothes region and produce a coarse try-on image. Second, a Clothes Warping Module (CWM) warps in-shop clothes to the desired shape according to the predicted semantic map and the desired pose. Specifically, we introduce a conductible cycle consistency loss to alleviate the misalignment in the clothes warping process. Third, a Try-on Synthesis Module (TSM) combines the coarse result and the warped clothes to generate the final virtual try-on image, preserving details of the desired clothes and under the desired pose. Besides, we introduce a face identity loss to refine the facial appearance and maintain the identity of the final virtual try-on result at the same time. We evaluate the proposed method on the most massive multi-pose dataset (MPV) and the DeepFashion dataset. The qualitative and quantitative experiments show that SPG-VTON is superior to the state-of-the-art methods and is robust to the data noise, including background and accessory changes, i.e., hats and handbags, showing good scalability to the real-world scenario.



### DuCN: Dual-children Network for Medical Diagnosis and Similar Case Recommendation towards COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2108.01997v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.01997v1)
- **Published**: 2021-08-03 15:46:53+00:00
- **Updated**: 2021-08-03 15:46:53+00:00
- **Authors**: Chengtao Peng, Yunfei Long, Senhua Zhu, Dandan Tu, Bin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Early detection of the coronavirus disease 2019 (COVID-19) helps to treat patients timely and increase the cure rate, thus further suppressing the spread of the disease. In this study, we propose a novel deep learning based detection and similar case recommendation network to help control the epidemic. Our proposed network contains two stages: the first one is a lung region segmentation step and is used to exclude irrelevant factors, and the second is a detection and recommendation stage. Under this framework, in the second stage, we develop a dual-children network (DuCN) based on a pre-trained ResNet-18 to simultaneously realize the disease diagnosis and similar case recommendation. Besides, we employ triplet loss and intrapulmonary distance maps to assist the detection, which helps incorporate tiny differences between two images and is conducive to improving the diagnostic accuracy. For each confirmed COVID-19 case, we give similar cases to provide radiologists with diagnosis and treatment references. We conduct experiments on a large publicly available dataset (CC-CCII) and compare the proposed model with state-of-the-art COVID-19 detection methods. The results show that our proposed model achieves a promising clinical performance.



### Super Neurons
- **Arxiv ID**: http://arxiv.org/abs/2109.01594v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.01594v2)
- **Published**: 2021-08-03 16:17:45+00:00
- **Updated**: 2023-04-15 21:14:09+00:00
- **Authors**: Serkan Kiranyaz, Junaid Malik, Mehmet Yamac, Mert Duman, Ilke Adalioglu, Esin Guldogan, Turker Ince, Moncef Gabbouj
- **Comment**: 25 pages, 13 figures
- **Journal**: None
- **Summary**: Self-Organized Operational Neural Networks (Self-ONNs) have recently been proposed as new-generation neural network models with nonlinear learning units, i.e., the generative neurons that yield an elegant level of diversity; however, like its predecessor, conventional Convolutional Neural Networks (CNNs), they still have a common drawback: localized (fixed) kernel operations. This severely limits the receptive field and information flow between layers and thus brings the necessity for deep and complex models. It is highly desired to improve the receptive field size without increasing the kernel dimensions. This requires a significant upgrade over the generative neurons to achieve the non-localized kernel operations for each connection between consecutive layers. In this article, we present superior (generative) neuron models (or super neurons in short) that allow random or learnable kernel shifts and thus can increase the receptive field size of each connection. The kernel localization process varies among the two super-neuron models. The first model assumes randomly localized kernels within a range and the second one learns (optimizes) the kernel locations during training. An extensive set of comparative evaluations against conventional and deformable convolutional, along with the generative neurons demonstrates that super neurons can empower Self-ONNs to achieve a superior learning and generalization capability with a minimal computational complexity burden.



### Generalized Source-free Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2108.01614v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01614v2)
- **Published**: 2021-08-03 16:34:12+00:00
- **Updated**: 2021-10-08 12:48:47+00:00
- **Authors**: Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, Shangling Jui
- **Comment**: Accepted by ICCV 2021; Update the acknowledgement section
- **Journal**: None
- **Summary**: Domain adaptation (DA) aims to transfer the knowledge learned from a source domain to an unlabeled target domain. Some recent works tackle source-free domain adaptation (SFDA) where only a source pre-trained model is available for adaptation to the target domain. However, those methods do not consider keeping source performance which is of high practical value in real world applications. In this paper, we propose a new domain adaptation paradigm called Generalized Source-free Domain Adaptation (G-SFDA), where the learned model needs to perform well on both the target and source domains, with only access to current unlabeled target data during adaptation. First, we propose local structure clustering (LSC), aiming to cluster the target features with its semantically similar neighbors, which successfully adapts the model to the target domain in the absence of source data. Second, we propose sparse domain attention (SDA), it produces a binary domain specific attention to activate different feature channels for different domains, meanwhile the domain attention will be utilized to regularize the gradient during adaptation to keep source information. In the experiments, for target performance our method is on par with or better than existing DA and SFDA methods, specifically it achieves state-of-the-art performance (85.4%) on VisDA, and our method works well for all domains after adapting to single or multiple target domains. Code is available in https://github.com/Albert0147/G-SFDA.



### Domain Generalization via Gradient Surgery
- **Arxiv ID**: http://arxiv.org/abs/2108.01621v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.01621v2)
- **Published**: 2021-08-03 16:49:25+00:00
- **Updated**: 2021-11-03 13:02:22+00:00
- **Authors**: Lucas Mansilla, Rodrigo Echeveste, Diego H. Milone, Enzo Ferrante
- **Comment**: Accepted for publication at ICCV 2021
- **Journal**: None
- **Summary**: In real-life applications, machine learning models often face scenarios where there is a change in data distribution between training and test domains. When the aim is to make predictions on distributions different from those seen at training, we incur in a domain generalization problem. Methods to address this issue learn a model using data from multiple source domains, and then apply this model to the unseen target domain. Our hypothesis is that when training with multiple domains, conflicting gradients within each mini-batch contain information specific to the individual domains which is irrelevant to the others, including the test domain. If left untouched, such disagreement may degrade generalization performance. In this work, we characterize the conflicting gradients emerging in domain shift scenarios and devise novel gradient agreement strategies based on gradient surgery to alleviate their effect. We validate our approach in image classification tasks with three multi-domain datasets, showing the value of the proposed agreement strategy in enhancing the generalization capability of deep learning models in domain shift scenarios.



### Del-Net: A Single-Stage Network for Mobile Camera ISP
- **Arxiv ID**: http://arxiv.org/abs/2108.01623v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.01623v1)
- **Published**: 2021-08-03 16:51:11+00:00
- **Updated**: 2021-08-03 16:51:11+00:00
- **Authors**: Saumya Gupta, Diplav Srivastava, Umang Chaturvedi, Anurag Jain, Gaurav Khandelwal
- **Comment**: None
- **Journal**: None
- **Summary**: The quality of images captured by smartphones is an important specification since smartphones are becoming ubiquitous as primary capturing devices. The traditional image signal processing (ISP) pipeline in a smartphone camera consists of several image processing steps performed sequentially to reconstruct a high quality sRGB image from the raw sensor data. These steps consist of demosaicing, denoising, white balancing, gamma correction, colour enhancement, etc. Since each of them are performed sequentially using hand-crafted algorithms, the residual error from each processing module accumulates in the final reconstructed signal. Thus, the traditional ISP pipeline has limited reconstruction quality in terms of generalizability across different lighting conditions and associated noise levels while capturing the image. Deep learning methods using convolutional neural networks (CNN) have become popular in solving many image-related tasks such as image denoising, contrast enhancement, super resolution, deblurring, etc. Furthermore, recent approaches for the RAW to sRGB conversion using deep learning methods have also been published, however, their immense complexity in terms of their memory requirement and number of Mult-Adds make them unsuitable for mobile camera ISP. In this paper we propose DelNet - a single end-to-end deep learning model - to learn the entire ISP pipeline within reasonable complexity for smartphone deployment. Del-Net is a multi-scale architecture that uses spatial and channel attention to capture global features like colour, as well as a series of lightweight modified residual attention blocks to help with denoising. For validation, we provide results to show the proposed Del-Net achieves compelling reconstruction quality.



### From augmented microscopy to the topological transformer: a new approach in cell image analysis for Alzheimer's research
- **Arxiv ID**: http://arxiv.org/abs/2108.01625v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, math.AT, math.ST, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/2108.01625v1)
- **Published**: 2021-08-03 16:59:33+00:00
- **Updated**: 2021-08-03 16:59:33+00:00
- **Authors**: Wooseok Jung
- **Comment**: None
- **Journal**: None
- **Summary**: Cell image analysis is crucial in Alzheimer's research to detect the presence of A$\beta$ protein inhibiting cell function. Deep learning speeds up the process by making only low-level data sufficient for fruitful inspection. We first found Unet is most suitable in augmented microscopy by comparing performance in multi-class semantics segmentation. We develop the augmented microscopy method to capture nuclei in a brightfield image and the transformer using Unet model to convert an input image into a sequence of topological information. The performance regarding Intersection-over-Union is consistent concerning the choice of image preprocessing and ground-truth generation. Training model with data of a specific cell type demonstrates transfer learning applies to some extent.   The topological transformer aims to extract persistence silhouettes or landscape signatures containing geometric information of a given image of cells. This feature extraction facilitates studying an image as a collection of one-dimensional data, substantially reducing computational costs. Using the transformer, we attempt grouping cell images by their cell type relying solely on topological features. Performances of the transformers followed by SVM, XGBoost, LGBM, and simple convolutional neural network classifiers are inferior to the conventional image classification. However, since this research initiates a new perspective in biomedical research by combining deep learning and topology for image analysis, we speculate follow-up investigation will reinforce our genuine regime.



### Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.01634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01634v1)
- **Published**: 2021-08-03 17:09:56+00:00
- **Updated**: 2021-08-03 17:09:56+00:00
- **Authors**: Victor Besnier, Andrei Bursuc, David Picard, Alexandre Briot
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we tackle the detection of out-of-distribution (OOD) objects in semantic segmentation. By analyzing the literature, we found that current methods are either accurate or fast but not both which limits their usability in real world applications. To get the best of both aspects, we propose to mitigate the common shortcomings by following four design principles: decoupling the OOD detection from the segmentation task, observing the entire segmentation network instead of just its output, generating training data for the OOD detector by leveraging blind spots in the segmentation network and focusing the generated data on localized regions in the image to simulate OOD objects. Our main contribution is a new OOD detection architecture called ObsNet associated with a dedicated training scheme based on Local Adversarial Attacks (LAA). We validate the soundness of our approach across numerous ablation studies. We also show it obtains top performances both in speed and accuracy when compared to ten recent methods of the literature on three different datasets.



### OncoNet: Weakly Supervised Siamese Network to automate cancer treatment response assessment between longitudinal FDG PET/CT examinations
- **Arxiv ID**: http://arxiv.org/abs/2108.02016v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.02016v1)
- **Published**: 2021-08-03 17:48:05+00:00
- **Updated**: 2021-08-03 17:48:05+00:00
- **Authors**: Anirudh Joshi, Sabri Eyuboglu, Shih-Cheng Huang, Jared Dunnmon, Arjun Soin, Guido Davidzon, Akshay Chaudhari, Matthew P Lungren
- **Comment**: None
- **Journal**: None
- **Summary**: FDG PET/CT imaging is a resource intensive examination critical for managing malignant disease and is particularly important for longitudinal assessment during therapy. Approaches to automate longtudinal analysis present many challenges including lack of available longitudinal datasets, managing complex large multimodal imaging examinations, and need for detailed annotations for traditional supervised machine learning. In this work we develop OncoNet, novel machine learning algorithm that assesses treatment response from a 1,954 pairs of sequential FDG PET/CT exams through weak supervision using the standard uptake values (SUVmax) in associated radiology reports. OncoNet demonstrates an AUROC of 0.86 and 0.84 on internal and external institution test sets respectively for determination of change between scans while also showing strong agreement to clinical scoring systems with a kappa score of 0.8. We also curated a dataset of 1,954 paired FDG PET/CT exams designed for response assessment for the broader machine learning in healthcare research community. Automated assessment of radiographic response from FDG PET/CT with OncoNet could provide clinicians with a valuable tool to rapidly and consistently interpret change over time in longitudinal multi-modal imaging exams.



### Comparison of modern open-source visual SLAM approaches
- **Arxiv ID**: http://arxiv.org/abs/2108.01654v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.01654v2)
- **Published**: 2021-08-03 17:52:30+00:00
- **Updated**: 2023-02-04 07:53:37+00:00
- **Authors**: Dinar Sharafutdinov, Mark Griguletskii, Pavel Kopanev, Mikhail Kurenkov, Gonzalo Ferrer, Aleksey Burkov, Aleksei Gonnochenko, Dzmitry Tsetserukou
- **Comment**: Preprint, 19 pages
- **Journal**: None
- **Summary**: SLAM is one of the most fundamental areas of research in robotics and computer vision. State of the art solutions has advanced significantly in terms of accuracy and stability. Unfortunately, not all the approaches are available as open-source solutions and free to use. The results of some of them are difficult to reproduce, and there is a lack of comparison on common datasets. In our work, we make a comparative analysis of state of the art open-source methods. We assess the algorithms based on accuracy, computational performance, robustness, and fault tolerance. Moreover, we present a comparison of datasets as well as an analysis of algorithms from a practical point of view. The findings of the work raise several crucial questions for SLAM researchers.



### Image Augmentation Using a Task Guided Generative Adversarial Network for Age Estimation on Brain MRI
- **Arxiv ID**: http://arxiv.org/abs/2108.01659v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.01659v1)
- **Published**: 2021-08-03 17:56:50+00:00
- **Updated**: 2021-08-03 17:56:50+00:00
- **Authors**: Ruizhe Li, Matteo Bastiani, Dorothee Auer, Christian Wagner, Xin Chen
- **Comment**: Accepted for publication at 25th Annual Conference on Medical Image
  Understanding and Analysis (MIUA 2021)
- **Journal**: None
- **Summary**: Brain age estimation based on magnetic resonance imaging (MRI) is an active research area in early diagnosis of some neurodegenerative diseases (e.g. Alzheimer, Parkinson, Huntington, etc.) for elderly people or brain underdevelopment for the young group. Deep learning methods have achieved the state-of-the-art performance in many medical image analysis tasks, including brain age estimation. However, the performance and generalisability of the deep learning model are highly dependent on the quantity and quality of the training data set. Both collecting and annotating brain MRI data are extremely time-consuming. In this paper, to overcome the data scarcity problem, we propose a generative adversarial network (GAN) based image synthesis method. Different from the existing GAN-based methods, we integrate a task-guided branch (a regression model for age estimation) to the end of the generator in GAN. By adding a task-guided loss to the conventional GAN loss, the learned low-dimensional latent space and the synthesised images are more task-specific. It helps to boost the performance of the down-stream task by combining the synthesised images and real images for model training. The proposed method was evaluated on a public brain MRI data set for age estimation. Our proposed method outperformed (statistically significant) a deep convolutional neural network based regression model and the GAN-based image synthesis method without the task-guided branch. More importantly, it enables the identification of age-related brain regions in the image space. The code is available on GitHub (https://github.com/ruizhe-l/tgb-gan).



### Uniform Sampling over Episode Difficulty
- **Arxiv ID**: http://arxiv.org/abs/2108.01662v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.01662v2)
- **Published**: 2021-08-03 17:58:54+00:00
- **Updated**: 2022-01-15 20:28:12+00:00
- **Authors**: Sébastien M. R. Arnold, Guneet S. Dhillon, Avinash Ravichandran, Stefano Soatto
- **Comment**: NeurIPS'21 camera ready
- **Journal**: None
- **Summary**: Episodic training is a core ingredient of few-shot learning to train models on tasks with limited labelled data. Despite its success, episodic training remains largely understudied, prompting us to ask the question: what is the best way to sample episodes? In this paper, we first propose a method to approximate episode sampling distributions based on their difficulty. Building on this method, we perform an extensive analysis and find that sampling uniformly over episode difficulty outperforms other sampling schemes, including curriculum and easy-/hard-mining. As the proposed sampling method is algorithm agnostic, we can leverage these insights to improve few-shot learning accuracies across many episodic training algorithms. We demonstrate the efficacy of our method across popular few-shot learning datasets, algorithms, network architectures, and protocols.



### Exploiting BERT For Multimodal Target Sentiment Classification Through Input Space Translation
- **Arxiv ID**: http://arxiv.org/abs/2108.01682v2
- **DOI**: 10.1145/3474085.3475692
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.01682v2)
- **Published**: 2021-08-03 18:02:38+00:00
- **Updated**: 2021-08-05 20:58:17+00:00
- **Authors**: Zaid Khan, Yun Fu
- **Comment**: ACM Multimedia 2021 Oral
- **Journal**: None
- **Summary**: Multimodal target/aspect sentiment classification combines multimodal sentiment analysis and aspect/target sentiment classification. The goal of the task is to combine vision and language to understand the sentiment towards a target entity in a sentence. Twitter is an ideal setting for the task because it is inherently multimodal, highly emotional, and affects real world events. However, multimodal tweets are short and accompanied by complex, possibly irrelevant images. We introduce a two-stream model that translates images in input space using an object-aware transformer followed by a single-pass non-autoregressive text generation approach. We then leverage the translation to construct an auxiliary sentence that provides multimodal information to a language model. Our approach increases the amount of text available to the language model and distills the object-level information in complex images. We achieve state-of-the-art performance on two multimodal Twitter datasets without modifying the internals of the language model to accept multimodal data, demonstrating the effectiveness of our translation. In addition, we explain a failure mode of a popular approach for aspect sentiment analysis when applied to tweets. Our code is available at \textcolor{blue}{\url{https://github.com/codezakh/exploiting-BERT-thru-translation}}.



### Vision Transformer with Progressive Sampling
- **Arxiv ID**: http://arxiv.org/abs/2108.01684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01684v1)
- **Published**: 2021-08-03 18:04:31+00:00
- **Updated**: 2021-08-03 18:04:31+00:00
- **Authors**: Xiaoyu Yue, Shuyang Sun, Zhanghui Kuang, Meng Wei, Philip Torr, Wayne Zhang, Dahua Lin
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Transformers with powerful global relation modeling abilities have been introduced to fundamental computer vision tasks recently. As a typical example, the Vision Transformer (ViT) directly applies a pure transformer architecture on image classification, by simply splitting images into tokens with a fixed length, and employing transformers to learn relations between these tokens. However, such naive tokenization could destruct object structures, assign grids to uninterested regions such as background, and introduce interference signals. To mitigate the above issues, in this paper, we propose an iterative and progressive sampling strategy to locate discriminative regions. At each iteration, embeddings of the current sampling step are fed into a transformer encoder layer, and a group of sampling offsets is predicted to update the sampling locations for the next step. The progressive sampling is differentiable. When combined with the Vision Transformer, the obtained PS-ViT network can adaptively learn where to look. The proposed PS-ViT is both effective and efficient. When trained from scratch on ImageNet, PS-ViT performs 3.8% higher than the vanilla ViT in terms of top-1 accuracy with about $4\times$ fewer parameters and $10\times$ fewer FLOPs. Code is available at https://github.com/yuexy/PS-ViT.



### An Empirical Evaluation of End-to-End Polyphonic Optical Music Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.01769v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2108.01769v1)
- **Published**: 2021-08-03 22:04:40+00:00
- **Updated**: 2021-08-03 22:04:40+00:00
- **Authors**: Sachinda Edirisooriya, Hao-Wen Dong, Julian McAuley, Taylor Berg-Kirkpatrick
- **Comment**: Accepted to ISMIR 2021
- **Journal**: None
- **Summary**: Previous work has shown that neural architectures are able to perform optical music recognition (OMR) on monophonic and homophonic music with high accuracy. However, piano and orchestral scores frequently exhibit polyphonic passages, which add a second dimension to the task. Monophonic and homophonic music can be described as homorhythmic, or having a single musical rhythm. Polyphonic music, on the other hand, can be seen as having multiple rhythmic sequences, or voices, concurrently. We first introduce a workflow for creating large-scale polyphonic datasets suitable for end-to-end recognition from sheet music publicly available on the MuseScore forum. We then propose two novel formulations for end-to-end polyphonic OMR -- one treating the problem as a type of multi-task binary classification, and the other treating it as multi-sequence detection. Building upon the encoder-decoder architecture and an image encoder proposed in past work on end-to-end OMR, we propose two novel decoder models -- FlagDecoder and RNNDecoder -- that correspond to the two formulations. Finally, we compare the empirical performance of these end-to-end approaches to polyphonic OMR and observe a new state-of-the-art performance with our multi-sequence detection decoder, RNNDecoder.



### Solo-learn: A Library of Self-supervised Methods for Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.01775v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01775v4)
- **Published**: 2021-08-03 22:19:55+00:00
- **Updated**: 2022-02-04 11:23:00+00:00
- **Authors**: Victor G. Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, Elisa Ricci
- **Comment**: Accepted to JMLR
- **Journal**: None
- **Summary**: This paper presents solo-learn, a library of self-supervised methods for visual representation learning. Implemented in Python, using Pytorch and Pytorch lightning, the library fits both research and industry needs by featuring distributed training pipelines with mixed-precision, faster data loading via Nvidia DALI, online linear evaluation for better prototyping, and many additional training tricks. Our goal is to provide an easy-to-use library comprising a large amount of Self-supervised Learning (SSL) methods, that can be easily extended and fine-tuned by the community. solo-learn opens up avenues for exploiting large-budget SSL solutions on inexpensive smaller infrastructures and seeks to democratize SSL by making it accessible to all. The source code is available at https://github.com/vturrisi/solo-learn.



### Armour: Generalizable Compact Self-Attention for Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2108.01778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01778v1)
- **Published**: 2021-08-03 22:33:58+00:00
- **Updated**: 2021-08-03 22:33:58+00:00
- **Authors**: Lingchuan Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Attention-based transformer networks have demonstrated promising potential as their applications extend from natural language processing to vision. However, despite the recent improvements, such as sub-quadratic attention approximation and various training enhancements, the compact vision transformers to date using the regular attention still fall short in comparison with its convnet counterparts, in terms of \textit{accuracy,} \textit{model size}, \textit{and} \textit{throughput}. This paper introduces a compact self-attention mechanism that is fundamental and highly generalizable. The proposed method reduces redundancy and improves efficiency on top of the existing attention optimizations. We show its drop-in applicability for both the regular attention mechanism and some most recent variants in vision transformers. As a result, we produced smaller and faster models with the same or better accuracies.



### Weakly Supervised Foreground Learning for Weakly Supervised Localization and Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.01785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01785v1)
- **Published**: 2021-08-03 23:33:51+00:00
- **Updated**: 2021-08-03 23:33:51+00:00
- **Authors**: Chen-Lin Zhang, Yin Li, Jianxin Wu
- **Comment**: Work in progress
- **Journal**: None
- **Summary**: Modern deep learning models require large amounts of accurately annotated data, which is often difficult to satisfy. Hence, weakly supervised tasks, including weakly supervised object localization~(WSOL) and detection~(WSOD), have recently received attention in the computer vision community. In this paper, we motivate and propose the weakly supervised foreground learning (WSFL) task by showing that both WSOL and WSOD can be greatly improved if groundtruth foreground masks are available. More importantly, we propose a complete WSFL pipeline with low computational cost, which generates pseudo boxes, learns foreground masks, and does not need any localization annotations. With the help of foreground masks predicted by our WSFL model, we achieve 72.97% correct localization accuracy on CUB for WSOL, and 55.7% mean average precision on VOC07 for WSOD, thereby establish new state-of-the-art for both tasks. Our WSFL model also shows excellent transfer ability.



