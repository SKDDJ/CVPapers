# Arxiv Papers in cs.CV on 2021-08-15
### Deep Algorithm Unrolling for Biomedical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2108.06637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06637v1)
- **Published**: 2021-08-15 01:06:26+00:00
- **Updated**: 2021-08-15 01:06:26+00:00
- **Authors**: Yuelong Li, Or Bar-Shira, Vishal Monga, Yonina C. Eldar
- **Comment**: None
- **Journal**: None
- **Summary**: In this chapter, we review biomedical applications and breakthroughs via leveraging algorithm unrolling, an important technique that bridges between traditional iterative algorithms and modern deep learning techniques. To provide context, we start by tracing the origin of algorithm unrolling and providing a comprehensive tutorial on how to unroll iterative algorithms into deep networks. We then extensively cover algorithm unrolling in a wide variety of biomedical imaging modalities and delve into several representative recent works in detail. Indeed, there is a rich history of iterative algorithms for biomedical image synthesis, which makes the field ripe for unrolling techniques. In addition, we put algorithm unrolling into a broad perspective, in order to understand why it is particularly effective and discuss recent trends. Finally, we conclude the chapter by discussing open challenges, and suggesting future research directions.



### Few-Shot Fine-Grained Action Recognition via Bidirectional Attention and Contrastive Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.06647v1
- **DOI**: 10.1145/3474085.3475216
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06647v1)
- **Published**: 2021-08-15 02:21:01+00:00
- **Updated**: 2021-08-15 02:21:01+00:00
- **Authors**: Jiahao Wang, Yunhong Wang, Sheng Liu, Annan Li
- **Comment**: Accepted in ACM Multimedia 2021
- **Journal**: None
- **Summary**: Fine-grained action recognition is attracting increasing attention due to the emerging demand of specific action understanding in real-world applications, whereas the data of rare fine-grained categories is very limited. Therefore, we propose the few-shot fine-grained action recognition problem, aiming to recognize novel fine-grained actions with only few samples given for each class. Although progress has been made in coarse-grained actions, existing few-shot recognition methods encounter two issues handling fine-grained actions: the inability to capture subtle action details and the inadequacy in learning from data with low inter-class variance. To tackle the first issue, a human vision inspired bidirectional attention module (BAM) is proposed. Combining top-down task-driven signals with bottom-up salient stimuli, BAM captures subtle action details by accurately highlighting informative spatio-temporal regions. To address the second issue, we introduce contrastive meta-learning (CML). Compared with the widely adopted ProtoNet-based method, CML generates more discriminative video representations for low inter-class variance data, since it makes full use of potential contrastive pairs in each training episode. Furthermore, to fairly compare different models, we establish specific benchmark protocols on two large-scale fine-grained action recognition datasets. Extensive experiments show that our method consistently achieves state-of-the-art performance across evaluated tasks.



### Semi-supervised 3D Object Detection via Adaptive Pseudo-Labeling
- **Arxiv ID**: http://arxiv.org/abs/2108.06649v1
- **DOI**: 10.1109/ICIP42928.2021.9506421
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06649v1)
- **Published**: 2021-08-15 02:58:43+00:00
- **Updated**: 2021-08-15 02:58:43+00:00
- **Authors**: Hongyi Xu, Fengqi Liu, Qianyu Zhou, Jinkun Hao, Zhijie Cao, Zhengyang Feng, Lizhuang Ma
- **Comment**: Accepted at International Conference on Image Processing (ICIP 2021)
- **Journal**: None
- **Summary**: 3D object detection is an important task in computer vision. Most existing methods require a large number of high-quality 3D annotations, which are expensive to collect. Especially for outdoor scenes, the problem becomes more severe due to the sparseness of the point cloud and the complexity of urban scenes. Semi-supervised learning is a promising technique to mitigate the data annotation issue. Inspired by this, we propose a novel semi-supervised framework based on pseudo-labeling for outdoor 3D object detection tasks. We design the Adaptive Class Confidence Selection module (ACCS) to generate high-quality pseudo-labels. Besides, we propose Holistic Point Cloud Augmentation (HPCA) for unlabeled data to improve robustness. Experiments on the KITTI benchmark demonstrate the effectiveness of our method.



### Semantic-embedded Unsupervised Spectral Reconstruction from Single RGB Images in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2108.06659v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06659v2)
- **Published**: 2021-08-15 05:19:44+00:00
- **Updated**: 2021-08-17 03:36:48+00:00
- **Authors**: Zhiyu Zhu, Hui Liu, Junhui Hou, Huanqiang Zeng, Qingfu Zhang
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: This paper investigates the problem of reconstructing hyperspectral (HS) images from single RGB images captured by commercial cameras, \textbf{without} using paired HS and RGB images during training. To tackle this challenge, we propose a new lightweight and end-to-end learning-based framework. Specifically, on the basis of the intrinsic imaging degradation model of RGB images from HS images, we progressively spread the differences between input RGB images and re-projected RGB images from recovered HS images via effective unsupervised camera spectral response function estimation. To enable the learning without paired ground-truth HS images as supervision, we adopt the adversarial learning manner and boost it with a simple yet effective $\mathcal{L}_1$ gradient clipping scheme. Besides, we embed the semantic information of input RGB images to locally regularize the unsupervised learning, which is expected to promote pixels with identical semantics to have consistent spectral signatures. In addition to conducting quantitative experiments over two widely-used datasets for HS image reconstruction from synthetic RGB images, we also evaluate our method by applying recovered HS images from real RGB images to HS-based visual tracking. Extensive results show that our method significantly outperforms state-of-the-art unsupervised methods and even exceeds the latest supervised method under some settings. The source code is public available at https://github.com/zbzhzhy/Unsupervised-Spectral-Reconstruction.



### Contrast Limited Adaptive Histogram Equalization (CLAHE) Approach for Enhancement of the Microstructures of Friction Stir Welded Joints
- **Arxiv ID**: http://arxiv.org/abs/2109.00886v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.00886v1)
- **Published**: 2021-08-15 05:37:32+00:00
- **Updated**: 2021-08-15 05:37:32+00:00
- **Authors**: Akshansh Mishra
- **Comment**: None
- **Journal**: None
- **Summary**: Image processing algorithms are finding various applications in manufacturing and materials industries such as identification of cracks in the fabricated samples, calculating the geometrical properties of the given microstructure, presence of surface defects, etc. The present work deals with the application of Contrast Limited Adaptive Histogram Equalization (CLAHE) algorithm for improving the quality of the microstructure images of the Friction Stir Welded joints. The obtained results showed that the obtained value of quantitative metric features such as Entropy value and RMS Contrast value were high which resulted in enhanced microstructure images.



### HCR-Net: A deep learning based script independent handwritten character recognition network
- **Arxiv ID**: http://arxiv.org/abs/2108.06663v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.06663v3)
- **Published**: 2021-08-15 05:48:07+00:00
- **Updated**: 2023-01-31 19:47:50+00:00
- **Authors**: Vinod Kumar Chauhan, Sukhdeep Singh, Anuj Sharma
- **Comment**: 23 pages (double-column), 6 figures, 16 tables (under review) --
  revised version
- **Journal**: None
- **Summary**: Despite being studied extensively for a few decades, handwritten character recognition (HCR) is still considered a challenging learning problem in pattern recognition, and there is very limited research on script independent models. This is mainly because of similarity in structure of characters, different handwriting styles, noisy datasets, diversity of scripts, focus of the conventional research on handcrafted feature extraction techniques, and unavailability of public datasets and code-repositories to reproduce the results. On the other hand, deep learning has witnessed huge success in different areas of pattern recognition, including HCR, and provides an end-to-end learning. However, deep learning techniques are computationally expensive, need large amount of data for training and have been developed for specific scripts only. To address the above limitations, we have proposed a novel generic deep learning architecture for script independent handwritten character recognition, called HCR-Net. HCR-Net is based on a novel transfer learning approach for HCR, which partly utilizes feature extraction layers of a pre-trained network. Due to transfer learning and image-augmentation, HCR-Net provides faster and computationally efficient training, better performance and better generalizations, and can work with small datasets. HCR-Net is extensively evaluated on 40 publicly available datasets of Bangla, Punjabi, Hindi, English, Swedish, Urdu, Farsi, Tibetan, Kannada, Malayalam, Telugu, Marathi, Nepali and Arabic languages, and established 26 new benchmark results while performed close to the best results in the rest cases. HCR-Net showed performance improvements up to 11% against the existing results and achieved a fast convergence rate showing up to 99% of final performance in the very first epoch. HCR-Net significantly outperformed the state-of-the-art transfer learning techniques...



### CPNet: Cycle Prototype Network for Weakly-supervised 3D Renal Compartments Segmentation on CT Images
- **Arxiv ID**: http://arxiv.org/abs/2108.06669v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.06669v1)
- **Published**: 2021-08-15 06:54:38+00:00
- **Updated**: 2021-08-15 06:54:38+00:00
- **Authors**: Song Wang, Yuting He, Youyong Kong, Xiaomei Zhu, Shaobo Zhang, Pengfei Shao, Jean-Louis Dillenseger, Jean-Louis Coatrieux, Shuo Li, Guanyu Yang
- **Comment**: 24th International Conference on Medical Image Computing and Computer
  Assisted Intervention
- **Journal**: None
- **Summary**: Renal compartment segmentation on CT images targets on extracting the 3D structure of renal compartments from abdominal CTA images and is of great significance to the diagnosis and treatment for kidney diseases. However, due to the unclear compartment boundary, thin compartment structure and large anatomy variation of 3D kidney CT images, deep-learning based renal compartment segmentation is a challenging task. We propose a novel weakly supervised learning framework, Cycle Prototype Network, for 3D renal compartment segmentation. It has three innovations: 1) A Cycle Prototype Learning (CPL) is proposed to learn consistency for generalization. It learns from pseudo labels through the forward process and learns consistency regularization through the reverse process. The two processes make the model robust to noise and label-efficient. 2) We propose a Bayes Weakly Supervised Module (BWSM) based on cross-period prior knowledge. It learns prior knowledge from cross-period unlabeled data and perform error correction automatically, thus generates accurate pseudo labels. 3) We present a Fine Decoding Feature Extractor (FDFE) for fine-grained feature extraction. It combines global morphology information and local detail information to obtain feature maps with sharp detail, so the model will achieve fine segmentation on thin structures. Our model achieves Dice of 79.1% and 78.7% with only four labeled images, achieving a significant improvement by about 20% than typical prototype model PANet.



### Multi-granularity for knowledge distillation
- **Arxiv ID**: http://arxiv.org/abs/2108.06681v1
- **DOI**: 10.1016/j.imavis.2021.104286
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06681v1)
- **Published**: 2021-08-15 07:47:08+00:00
- **Updated**: 2021-08-15 07:47:08+00:00
- **Authors**: Baitan Shao, Ying Chen
- **Comment**: 14 pages, 12 figures
- **Journal**: None
- **Summary**: Considering the fact that students have different abilities to understand the knowledge imparted by teachers, a multi-granularity distillation mechanism is proposed for transferring more understandable knowledge for student networks. A multi-granularity self-analyzing module of the teacher network is designed, which enables the student network to learn knowledge from different teaching patterns. Furthermore, a stable excitation scheme is proposed for robust supervision for the student training. The proposed distillation mechanism can be embedded into different distillation frameworks, which are taken as baselines. Experiments show the mechanism improves the accuracy by 0.58% on average and by 1.08% in the best over the baselines, which makes its performance superior to the state-of-the-arts. It is also exploited that the student's ability of fine-tuning and robustness to noisy inputs can be improved via the proposed mechanism. The code is available at https://github.com/shaoeric/multi-granularity-distillation.



### ST3D++: Denoised Self-training for Unsupervised Domain Adaptation on 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.06682v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.06682v1)
- **Published**: 2021-08-15 07:49:06+00:00
- **Updated**: 2021-08-15 07:49:06+00:00
- **Authors**: Jihan Yang, Shaoshuai Shi, Zhe Wang, Hongsheng Li, Xiaojuan Qi
- **Comment**: arXiv admin note: text overlap with arXiv:2103.05346
- **Journal**: None
- **Summary**: In this paper, we present a self-training method, named ST3D++, with a holistic pseudo label denoising pipeline for unsupervised domain adaptation on 3D object detection. ST3D++ aims at reducing noise in pseudo label generation as well as alleviating the negative impacts of noisy pseudo labels on model training. First, ST3D++ pre-trains the 3D object detector on the labeled source domain with random object scaling (ROS) which is designed to reduce target domain pseudo label noise arising from object scale bias of the source domain. Then, the detector is progressively improved through alternating between generating pseudo labels and training the object detector with pseudo-labeled target domain data. Here, we equip the pseudo label generation process with a hybrid quality-aware triplet memory to improve the quality and stability of generated pseudo labels. Meanwhile, in the model training stage, we propose a source data assisted training strategy and a curriculum data augmentation policy to effectively rectify noisy gradient directions and avoid model over-fitting to noisy pseudo labeled data. These specific designs enable the detector to be trained on meticulously refined pseudo labeled target data with denoised training signals, and thus effectively facilitate adapting an object detector to a target domain without requiring annotations. Finally, our method is assessed on four 3D benchmark datasets (i.e., Waymo, KITTI, Lyft, and nuScenes) for three common categories (i.e., car, pedestrian and bicycle). ST3D++ achieves state-of-the-art performance on all evaluated settings, outperforming the corresponding baseline by a large margin (e.g., 9.6% $\sim$ 38.16% on Waymo $\rightarrow$ KITTI in terms of AP$_{\text{3D}}$), and even surpasses the fully supervised oracle results on the KITTI 3D object detection benchmark with target prior. Code will be available.



### Vector-Decomposed Disentanglement for Domain-Invariant Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.06685v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06685v1)
- **Published**: 2021-08-15 07:58:59+00:00
- **Updated**: 2021-08-15 07:58:59+00:00
- **Authors**: Aming Wu, Rui Liu, Yahong Han, Linchao Zhu, Yi Yang
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: To improve the generalization of detectors, for domain adaptive object detection (DAOD), recent advances mainly explore aligning feature-level distributions between the source and single-target domain, which may neglect the impact of domain-specific information existing in the aligned features. Towards DAOD, it is important to extract domain-invariant object representations. To this end, in this paper, we try to disentangle domain-invariant representations from domain-specific representations. And we propose a novel disentangled method based on vector decomposition. Firstly, an extractor is devised to separate domain-invariant representations from the input, which are used for extracting object proposals. Secondly, domain-specific representations are introduced as the differences between the input and domain-invariant representations. Through the difference operation, the gap between the domain-specific and domain-invariant representations is enlarged, which promotes domain-invariant representations to contain more domain-irrelevant information. In the experiment, we separately evaluate our method on the single- and compound-target case. For the single-target case, experimental results of four domain-shift scenes show our method obtains a significant performance gain over baseline methods. Moreover, for the compound-target case (i.e., the target is a compound of two different domains without domain labels), our method outperforms baseline methods by around 4%, which demonstrates the effectiveness of our method.



### Exploring Temporal Coherence for More General Video Face Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.06693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06693v1)
- **Published**: 2021-08-15 08:45:37+00:00
- **Updated**: 2021-08-15 08:45:37+00:00
- **Authors**: Yinglin Zheng, Jianmin Bao, Dong Chen, Ming Zeng, Fang Wen
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Although current face manipulation techniques achieve impressive performance regarding quality and controllability, they are struggling to generate temporal coherent face videos. In this work, we explore to take full advantage of the temporal coherence for video face forgery detection. To achieve this, we propose a novel end-to-end framework, which consists of two major stages. The first stage is a fully temporal convolution network (FTCN). The key insight of FTCN is to reduce the spatial convolution kernel size to 1, while maintaining the temporal convolution kernel size unchanged. We surprisingly find this special design can benefit the model for extracting the temporal features as well as improve the generalization capability. The second stage is a Temporal Transformer network, which aims to explore the long-term temporal coherence. The proposed framework is general and flexible, which can be directly trained from scratch without any pre-training models or external datasets. Extensive experiments show that our framework outperforms existing methods and remains effective when applied to detect new sorts of face forgery videos.



### U-mesh: Human Correspondence Matching with Mesh Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.06695v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06695v2)
- **Published**: 2021-08-15 08:58:45+00:00
- **Updated**: 2021-08-22 09:56:34+00:00
- **Authors**: Benjamin Groisser, Alon Wolf, Ron Kimmel
- **Comment**: None
- **Journal**: None
- **Summary**: The proliferation of 3D scanning technology has driven a need for methods to interpret geometric data, particularly for human subjects. In this paper we propose an elegant fusion of regression (bottom-up) and generative (top-down) methods to fit a parametric template model to raw scan meshes.   Our first major contribution is an intrinsic convolutional mesh U-net architecture that predicts pointwise correspondence to a template surface. Soft-correspondence is formulated as coordinates in a newly-constructed Cartesian space. Modeling correspondence as Euclidean proximity enables efficient optimization, both for network training and for the next step of the algorithm.   Our second contribution is a generative optimization algorithm that uses the U-net correspondence predictions to guide a parametric Iterative Closest Point registration. By employing pre-trained human surface parametric models we maximally leverage domain-specific prior knowledge.   The pairing of a mesh-convolutional network with generative model fitting enables us to predict correspondence for real human surface scans including occlusions, partialities, and varying genus (e.g. from self-contact). We evaluate the proposed method on the FAUST correspondence challenge where we achieve 20% (33%) improvement over state of the art methods for inter- (intra-) subject correspondence.



### Reference Service Model for Federated Identity Management
- **Arxiv ID**: http://arxiv.org/abs/2108.06701v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.NI, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2108.06701v1)
- **Published**: 2021-08-15 09:37:26+00:00
- **Updated**: 2021-08-15 09:37:26+00:00
- **Authors**: Daniela Pöhn, Peter Hillmann
- **Comment**: None
- **Journal**: EMMSAD 2021
- **Summary**: With the pandemic of COVID-19, people around the world increasingly work from home. Each natural person typically has several digital identities with different associated information. During the last years, various identity and access management approaches have gained attraction, helping for example to access other organization's services within trust boundaries. The resulting heterogeneity creates a high complexity to differentiate between these approaches and scenarios as participating entity; combining them is even harder. Last but not least, various actors have a different understanding or perspective of the terms, like 'service', in this context. Our paper describes a reference service with standard components in generic federated identity management. This is utilized with modern Enterprise Architecture using the framework ArchiMate. The proposed universal federated identity management service model (FIMSM) is applied to describe various federated identity management scenarios in a generic service-oriented way. The presented reference design is approved in multiple aspects and is easily applicable in numerous scenarios.



### Deepfake Representation with Multilinear Regression
- **Arxiv ID**: http://arxiv.org/abs/2108.06702v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.06702v1)
- **Published**: 2021-08-15 09:37:38+00:00
- **Updated**: 2021-08-15 09:37:38+00:00
- **Authors**: Sara Abdali, M. Alex O. Vasilescu, Evangelos E. Papalexakis
- **Comment**: None
- **Journal**: None
- **Summary**: Generative neural network architectures such as GANs, may be used to generate synthetic instances to compensate for the lack of real data. However, they may be employed to create media that may cause social, political or economical upheaval. One emerging media is "Deepfake".Techniques that can discriminate between such media is indispensable. In this paper, we propose a modified multilinear (tensor) method, a combination of linear and multilinear regressions for representing fake and real data. We test our approach by representing Deepfakes with our modified multilinear (tensor) approach and perform SVM classification with encouraging results.



### Temporal Action Segmentation with High-level Complex Activity Labels
- **Arxiv ID**: http://arxiv.org/abs/2108.06706v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06706v3)
- **Published**: 2021-08-15 09:50:42+00:00
- **Updated**: 2022-12-17 08:12:09+00:00
- **Authors**: Guodong Ding, Angela Yao
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: The temporal action segmentation task segments videos temporally and predicts action labels for all frames. Fully supervising such a segmentation model requires dense frame-wise action annotations, which are expensive and tedious to collect.   This work is the first to propose a Constituent Action Discovery (CAD) framework that only requires the video-wise high-level complex activity label as supervision for temporal action segmentation. The proposed approach automatically discovers constituent video actions using an activity classification task. Specifically, we define a finite number of latent action prototypes to construct video-level dual representations with which these prototypes are learned collectively through the activity classification training. This setting endows our approach with the capability to discover potentially shared actions across multiple complex activities.   Due to the lack of action-level supervision, we adopt the Hungarian matching algorithm to relate latent action prototypes to ground truth semantic classes for evaluation. We show that with the high-level supervision, the Hungarian matching can be extended from the existing video and activity levels to the global level. The global-level matching allows for action sharing across activities, which has never been considered in the literature before. Extensive experiments demonstrate that our discovered actions can help perform temporal action segmentation and activity recognition tasks.



### SPG: Unsupervised Domain Adaptation for 3D Object Detection via Semantic Point Generation
- **Arxiv ID**: http://arxiv.org/abs/2108.06709v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06709v1)
- **Published**: 2021-08-15 10:00:39+00:00
- **Updated**: 2021-08-15 10:00:39+00:00
- **Authors**: Qiangeng Xu, Yin Zhou, Weiyue Wang, Charles R. Qi, Dragomir Anguelov
- **Comment**: None
- **Journal**: None
- **Summary**: In autonomous driving, a LiDAR-based object detector should perform reliably at different geographic locations and under various weather conditions. While recent 3D detection research focuses on improving performance within a single domain, our study reveals that the performance of modern detectors can drop drastically cross-domain. In this paper, we investigate unsupervised domain adaptation (UDA) for LiDAR-based 3D object detection. On the Waymo Domain Adaptation dataset, we identify the deteriorating point cloud quality as the root cause of the performance drop. To address this issue, we present Semantic Point Generation (SPG), a general approach to enhance the reliability of LiDAR detectors against domain shifts. Specifically, SPG generates semantic points at the predicted foreground regions and faithfully recovers missing parts of the foreground objects, which are caused by phenomena such as occlusions, low reflectance or weather interference. By merging the semantic points with the original points, we obtain an augmented point cloud, which can be directly consumed by modern LiDAR-based detectors. To validate the wide applicability of SPG, we experiment with two representative detectors, PointPillars and PV-RCNN. On the UDA task, SPG significantly improves both detectors across all object categories of interest and at all difficulty levels. SPG can also benefit object detection in the original domain. On the Waymo Open Dataset and KITTI, SPG improves 3D detection results of these two methods across all categories. Combined with PV-RCNN, SPG achieves state-of-the-art 3D detection results on KITTI.



### Pattern Inversion as a Pattern Recognition Method for Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.10242v1
- **DOI**: 10.1088/1742-6596/2224/1/012002
- **Categories**: **cs.CV**, cs.LG, cs.NE, C.3; I.2; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2108.10242v1)
- **Published**: 2021-08-15 10:25:51+00:00
- **Updated**: 2021-08-15 10:25:51+00:00
- **Authors**: Alexei Mikhailov, Mikhail Karavay
- **Comment**: 9 pages, 12 figures
- **Journal**: None
- **Summary**: Artificial neural networks use a lot of coefficients that take a great deal of computing power for their adjustment, especially if deep learning networks are employed. However, there exist coefficients-free extremely fast indexing-based technologies that work, for instance, in Google search engines, in genome sequencing, etc. The paper discusses the use of indexing-based methods for pattern recognition. It is shown that for pattern recognition applications such indexing methods replace with inverse patterns the fully inverted files, which are typically employed in search engines. Not only such inversion provide automatic feature extraction, which is a distinguishing mark of deep learning, but, unlike deep learning, pattern inversion supports almost instantaneous learning, which is a consequence of absence of coefficients. The paper discusses a pattern inversion formalism that makes use on a novel pattern transform and its application for unsupervised instant learning. Examples demonstrate a view-angle independent recognition of three-dimensional objects, such as cars, against arbitrary background, prediction of remaining useful life of aircraft engines, and other applications. In conclusion, it is noted that, in neurophysiology, the function of the neocortical mini-column has been widely debated since 1957. This paper hypothesize that, mathematically, the cortical mini-column can be described as an inverse pattern, which physically serves as a connection multiplier expanding associations of inputs with relevant pattern classes.



### Enterprise Architecture Model Transformation Engine
- **Arxiv ID**: http://arxiv.org/abs/2108.13169v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.AI, cs.CV, cs.PL, cs.SE, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2108.13169v1)
- **Published**: 2021-08-15 11:10:42+00:00
- **Updated**: 2021-08-15 11:10:42+00:00
- **Authors**: Erik Heiland, Peter Hillmann, Andreas Karcher
- **Comment**: None
- **Journal**: International Conference on Operations Research and Enterprise
  Systems (ICORES) 2021
- **Summary**: With increasing linkage within value chains, the IT systems of different companies are also being connected with each other. This enables the integration of services within the movement of Industry 4.0 in order to improve the quality and performance of the processes. Enterprise architecture models form the basis for this with a better buisness IT-alignment. However, the heterogeneity of the modeling frameworks and description languages makes a concatenation considerably difficult, especially differences in syntax, semantic and relations. Therefore, this paper presents a transformation engine to convert enterprise architecture models between several languages. We developed the first generic translation approach that is free of specific meta-modeling, which is flexible adaptable to arbitrary modeling languages. The transformation process is defined by various pattern matching techniques using a rule-based description language. It uses set theory and first-order logic for an intuitive description as a basis. The concept is practical evaluated using an example in the area of a large German IT-service provider. Anyhow, the approach is applicable between a wide range of enterprise architecture frameworks.



### Audio2Gestures: Generating Diverse Gestures from Speech Audio with Conditional Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2108.06720v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06720v1)
- **Published**: 2021-08-15 11:15:51+00:00
- **Updated**: 2021-08-15 11:15:51+00:00
- **Authors**: Jing Li, Di Kang, Wenjie Pei, Xuefei Zhe, Ying Zhang, Zhenyu He, Linchao Bao
- **Comment**: None
- **Journal**: None
- **Summary**: Generating conversational gestures from speech audio is challenging due to the inherent one-to-many mapping between audio and body motions. Conventional CNNs/RNNs assume one-to-one mapping, and thus tend to predict the average of all possible target motions, resulting in plain/boring motions during inference. In order to overcome this problem, we propose a novel conditional variational autoencoder (VAE) that explicitly models one-to-many audio-to-motion mapping by splitting the cross-modal latent code into shared code and motion-specific code. The shared code mainly models the strong correlation between audio and motion (such as the synchronized audio and motion beats), while the motion-specific code captures diverse motion information independent of the audio. However, splitting the latent code into two parts poses training difficulties for the VAE model. A mapping network facilitating random sampling along with other techniques including relaxed motion loss, bicycle constraint, and diversity loss are designed to better train the VAE. Experiments on both 3D and 2D motion datasets verify that our method generates more realistic and diverse motions than state-of-the-art methods, quantitatively and qualitatively. Finally, we demonstrate that our method can be readily used to generate motion sequences with user-specified motion clips on the timeline. Code and more results are at https://jingli513.github.io/audio2gestures.



### Self-supervised Contrastive Learning of Multi-view Facial Expressions
- **Arxiv ID**: http://arxiv.org/abs/2108.06723v1
- **DOI**: 10.1145/3462244.3479955
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.06723v1)
- **Published**: 2021-08-15 11:23:34+00:00
- **Updated**: 2021-08-15 11:23:34+00:00
- **Authors**: Shuvendu Roy, Ali Etemad
- **Comment**: Accepted by 23rd ACM International Conference on Multimodal
  Interaction (ICMI 2021)
- **Journal**: None
- **Summary**: Facial expression recognition (FER) has emerged as an important component of human-computer interaction systems. Despite recent advancements in FER, performance often drops significantly for non-frontal facial images. We propose Contrastive Learning of Multi-view facial Expressions (CL-MEx) to exploit facial images captured simultaneously from different angles towards FER. CL-MEx is a two-step training framework. In the first step, an encoder network is pre-trained with the proposed self-supervised contrastive loss, where it learns to generate view-invariant embeddings for different views of a subject. The model is then fine-tuned with labeled data in a supervised setting. We demonstrate the performance of the proposed method on two multi-view FER datasets, KDEF and DDCF, where state-of-the-art performances are achieved. Further experiments show the robustness of our method in dealing with challenging angles and reduced amounts of labeled data.



### SOTR: Segmenting Objects with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2108.06747v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06747v2)
- **Published**: 2021-08-15 14:10:11+00:00
- **Updated**: 2021-08-17 04:15:21+00:00
- **Authors**: Ruohao Guo, Dantong Niu, Liao Qu, Zhenbo Li
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Most recent transformer-based models show impressive performance on vision tasks, even better than Convolution Neural Networks (CNN). In this work, we present a novel, flexible, and effective transformer-based model for high-quality instance segmentation. The proposed method, Segmenting Objects with TRansformers (SOTR), simplifies the segmentation pipeline, building on an alternative CNN backbone appended with two parallel subtasks: (1) predicting per-instance category via transformer and (2) dynamically generating segmentation mask with the multi-level upsampling module. SOTR can effectively extract lower-level feature representations and capture long-range context dependencies by Feature Pyramid Network (FPN) and twin transformer, respectively. Meanwhile, compared with the original transformer, the proposed twin transformer is time- and resource-efficient since only a row and a column attention are involved to encode pixels. Moreover, SOTR is easy to be incorporated with various CNN backbones and transformer model variants to make considerable improvements for the segmentation accuracy and training convergence. Extensive experiments show that our SOTR performs well on the MS COCO dataset and surpasses state-of-the-art instance segmentation approaches. We hope our simple but strong framework could serve as a preferment baseline for instance-level recognition. Our code is available at https://github.com/easton-cau/SOTR.



### Learning Open-World Object Proposals without Learning to Classify
- **Arxiv ID**: http://arxiv.org/abs/2108.06753v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06753v1)
- **Published**: 2021-08-15 14:36:02+00:00
- **Updated**: 2021-08-15 14:36:02+00:00
- **Authors**: Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon, Weicheng Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: Object proposals have become an integral preprocessing steps of many vision pipelines including object detection, weakly supervised detection, object discovery, tracking, etc. Compared to the learning-free methods, learning-based proposals have become popular recently due to the growing interest in object detection. The common paradigm is to learn object proposals from data labeled with a set of object regions and their corresponding categories. However, this approach often struggles with novel objects in the open world that are absent in the training set. In this paper, we identify that the problem is that the binary classifiers in existing proposal methods tend to overfit to the training categories. Therefore, we propose a classification-free Object Localization Network (OLN) which estimates the objectness of each region purely by how well the location and shape of a region overlap with any ground-truth object (e.g., centerness and IoU). This simple strategy learns generalizable objectness and outperforms existing proposals on cross-category generalization on COCO, as well as cross-dataset evaluation on RoboNet, Object365, and EpicKitchens. Finally, we demonstrate the merit of OLN for long-tail object detection on large vocabulary dataset, LVIS, where we notice clear improvement in rare and common categories.



### Improving the trustworthiness of image classification models by utilizing bounding-box annotations
- **Arxiv ID**: http://arxiv.org/abs/2108.10131v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.10131v1)
- **Published**: 2021-08-15 15:09:07+00:00
- **Updated**: 2021-08-15 15:09:07+00:00
- **Authors**: Dharma KC, Chicheng Zhang
- **Comment**: 6 pages including references
- **Journal**: NeurIPS workshop on Interpretable Inductive Biases and Physically
  Structured Learning (2020)
- **Summary**: We study utilizing auxiliary information in training data to improve the trustworthiness of machine learning models. Specifically, in the context of image classification, we propose to optimize a training objective that incorporates bounding box information, which is available in many image classification datasets. Preliminary experimental results show that the proposed algorithm achieves better performance in accuracy, robustness, and interpretability compared with baselines.



### A Cascaded Zoom-In Network for Patterned Fabric Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.06760v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06760v2)
- **Published**: 2021-08-15 15:29:26+00:00
- **Updated**: 2021-09-19 17:48:37+00:00
- **Authors**: Zhiwei Zhang
- **Comment**: Technical Report; 12 pages, 15 figures
- **Journal**: None
- **Summary**: Nowadays, Deep Convolutional Neural Networks (DCNNs) are widely used in fabric defect detection, which come with the cost of expensive training and complex model parameters. With the observation that most fabrics are defect free in practice, a two-step Cascaded Zoom-In Network (CZI-Net) is proposed for patterned fabric defect detection. In the CZI-Net, the Aggregated HOG (A-HOG) and SIFT features are used to instead of simple convolution filters for feature extraction. Moreover, in order to extract more distinctive features, the feature representation layer and full connection layer are included in the CZI-Net. In practice, Most defect-free fabrics only involve in the first step of our method and avoid a costive computation in the second step, which makes very fast fabric detection. More importantly, we propose the Locality-constrained Reconstruction Error (LCRE) in the first step and Restrictive Locality-constrained Coding (RLC), Bag-of-Indexes (BoI) methods in the second step. We also analyse the connections between different coding methods and conclude that the index of visual words plays an essential role in the coding methods. In conclusion, experiments based on real-world datasets are implemented and demonstrate that our proposed method is not only computationally simple but also with high detection accuracy.



### Multi-Slice Dense-Sparse Learning for Efficient Liver and Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.06761v1
- **DOI**: 10.1109/EMBC46164.2021.9629698
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.06761v1)
- **Published**: 2021-08-15 15:29:48+00:00
- **Updated**: 2021-08-15 15:29:48+00:00
- **Authors**: Ziyuan Zhao, Zeyu Ma, Yanjie Liu, Zeng Zeng, Pierce KH Chow
- **Comment**: Accepted in 43rd Annual International Conference of the IEEE
  Engineering in Medicine and Biology Society, IEEE EMBC 2021
- **Journal**: 2021 43rd Annual International Conference of the IEEE Engineering
  in Medicine & Biology Society (EMBC)
- **Summary**: Accurate automatic liver and tumor segmentation plays a vital role in treatment planning and disease monitoring. Recently, deep convolutional neural network (DCNNs) has obtained tremendous success in 2D and 3D medical image segmentation. However, 2D DCNNs cannot fully leverage the inter-slice information, while 3D DCNNs are computationally expensive and memory intensive. To address these issues, we first propose a novel dense-sparse training flow from a data perspective, in which, densely adjacent slices and sparsely adjacent slices are extracted as inputs for regularizing DCNNs, thereby improving the model performance. Moreover, we design a 2.5D light-weight nnU-Net from a network perspective, in which, depthwise separable convolutions are adopted to improve the efficiency. Extensive experiments on the LiTS dataset have demonstrated the superiority of the proposed method.



### Two Eyes Are Better Than One: Exploiting Binocular Correlation for Diabetic Retinopathy Severity Grading
- **Arxiv ID**: http://arxiv.org/abs/2108.06763v1
- **DOI**: 10.1109/EMBC46164.2021.9630812
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.06763v1)
- **Published**: 2021-08-15 15:40:02+00:00
- **Updated**: 2021-08-15 15:40:02+00:00
- **Authors**: Peisheng Qian, Ziyuan Zhao, Cong Chen, Zeng Zeng, Xiaoli Li
- **Comment**: Accepted in 43rd Annual International Conference of the IEEE
  Engineering in Medicine and Biology Society, IEEE EMBC 2021
- **Journal**: 2021 43rd Annual International Conference of the IEEE Engineering
  in Medicine & Biology Society (EMBC)
- **Summary**: Diabetic retinopathy (DR) is one of the most common eye conditions among diabetic patients. However, vision loss occurs primarily in the late stages of DR, and the symptoms of visual impairment, ranging from mild to severe, can vary greatly, adding to the burden of diagnosis and treatment in clinical practice. Deep learning methods based on retinal images have achieved remarkable success in automatic DR grading, but most of them neglect that the presence of diabetes usually affects both eyes, and ophthalmologists usually compare both eyes concurrently for DR diagnosis, leaving correlations between left and right eyes unexploited. In this study, simulating the diagnostic process, we propose a two-stream binocular network to capture the subtle correlations between left and right eyes, in which, paired images of eyes are fed into two identical subnetworks separately during training. We design a contrastive grading loss to learn binocular correlation for five-class DR detection, which maximizes inter-class dissimilarity while minimizing the intra-class difference. Experimental results on the EyePACS dataset show the superiority of the proposed binocular model, outperforming monocular methods by a large margin.



### Occlusion-Aware Video Object Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2108.06765v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06765v1)
- **Published**: 2021-08-15 15:46:57+00:00
- **Updated**: 2021-08-15 15:46:57+00:00
- **Authors**: Lei Ke, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Conventional video inpainting is neither object-oriented nor occlusion-aware, making it liable to obvious artifacts when large occluded object regions are inpainted. This paper presents occlusion-aware video object inpainting, which recovers both the complete shape and appearance for occluded objects in videos given their visible mask segmentation.   To facilitate this new research, we construct the first large-scale video object inpainting benchmark YouTube-VOI to provide realistic occlusion scenarios with both occluded and visible object masks available. Our technical contribution VOIN jointly performs video object shape completion and occluded texture generation. In particular, the shape completion module models long-range object coherence while the flow completion module recovers accurate flow with sharp motion boundary, for propagating temporally-consistent texture to the same moving object across frames. For more realistic results, VOIN is optimized using both T-PatchGAN and a new spatio-temporal attention-based multi-class discriminator.   Finally, we compare VOIN and strong baselines on YouTube-VOI. Experimental results clearly demonstrate the efficacy of our method including inpainting complex and dynamic objects. VOIN degrades gracefully with inaccurate input visible mask.



### NPBDREG: Uncertainty Assessment in Diffeomorphic Brain MRI Registration using a Non-parametric Bayesian Deep-Learning Based Approach
- **Arxiv ID**: http://arxiv.org/abs/2108.06771v4
- **DOI**: 10.1016/j.compmedimag.2022.102087
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06771v4)
- **Published**: 2021-08-15 16:00:06+00:00
- **Updated**: 2022-06-09 06:40:18+00:00
- **Authors**: Samah Khawaled, Moti Freiman
- **Comment**: To appear in Computerized Medical Imaging and Graphics, DOI:
  https://doi.org/10.1016/j.compmedimag.2022.102087. This is an extension
  representing a more comprehensive work extending preliminary work presented
  at arXiv:2008.03949
- **Journal**: None
- **Summary**: Quantification of uncertainty in deep-neural-networks (DNN) based image registration algorithms plays a critical role in the deployment of image registration algorithms for clinical applications such as surgical planning, intraoperative guidance, and longitudinal monitoring of disease progression or treatment efficacy as well as in research-oriented processing pipelines. Currently available approaches for uncertainty estimation in DNN-based image registration algorithms may result in sub-optimal clinical decision making due to potentially inaccurate estimation of the uncertainty of the registration stems for the assumed parametric distribution of the registration latent space. We introduce NPBDREG, a fully non-parametric Bayesian framework for uncertainty estimation in DNN-based deformable image registration by combining an Adam optimizer with stochastic gradient Langevin dynamics (SGLD) to characterize the underlying posterior distribution through posterior sampling. Thus, it has the potential to provide uncertainty estimates that are highly correlated with the presence of out of distribution data. We demonstrated the added-value of NPBDREG, compared to the baseline probabilistic VoxelMorph model (PrVXM), on brain MRI image registration using $390$ image pairs from four publicly available databases: MGH10, CMUC12, ISBR18 and LPBA40. The NPBDREG shows a better correlation of the predicted uncertainty with out-of-distribution data ($r>0.95$ vs. $r<0.5$) as well as a 7.3%improvement in the registration accuracy (Dice score, $0.74$ vs. $0.69$, $p \ll 0.01$), and 18% improvement in registration smoothness (percentage of folds in the deformation field, 0.014 vs. 0.017, $p \ll 0.01$). Finally, NPBDREG demonstrated a better generalization capability for data corrupted by a mixed structure noise (Dice score of $0.73$ vs. $0.69$, $p \ll 0.01$) compared to the baseline PrVXM approach.



### Dilated Inception U-Net (DIU-Net) for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.06772v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.06772v1)
- **Published**: 2021-08-15 16:04:09+00:00
- **Updated**: 2021-08-15 16:04:09+00:00
- **Authors**: Daniel E. Cahall, Ghulam Rasool, Nidhal C. Bouaynaya, Hassan M. Fathallah-Shaykh
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) is routinely used for brain tumor diagnosis, treatment planning, and post-treatment surveillance. Recently, various models based on deep neural networks have been proposed for the pixel-level segmentation of tumors in brain MRIs. However, the structural variations, spatial dissimilarities, and intensity inhomogeneity in MRIs make segmentation a challenging task. We propose a new end-to-end brain tumor segmentation architecture based on U-Net that integrates Inception modules and dilated convolutions into its contracting and expanding paths. This allows us to extract local structural as well as global contextual information. We performed segmentation of glioma sub-regions, including tumor core, enhancing tumor, and whole tumor using Brain Tumor Segmentation (BraTS) 2018 dataset. Our proposed model performed significantly better than the state-of-the-art U-Net-based model ($p<0.05$) for tumor core and whole tumor segmentation.



### Online Continual Learning For Visual Food Classification
- **Arxiv ID**: http://arxiv.org/abs/2108.06781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06781v1)
- **Published**: 2021-08-15 17:48:03+00:00
- **Updated**: 2021-08-15 17:48:03+00:00
- **Authors**: Jiangpeng He, Fengqing Zhu
- **Comment**: Accepted paper for LargeFineFoodAI, ICCV 2021
- **Journal**: None
- **Summary**: Food image classification is challenging for real-world applications since existing methods require static datasets for training and are not capable of learning from sequentially available new food images. Online continual learning aims to learn new classes from data stream by using each new data only once without forgetting the previously learned knowledge. However, none of the existing works target food image analysis, which is more difficult to learn incrementally due to its high intra-class variation with the unbalanced and unpredictable characteristics of future food class distribution. In this paper, we address these issues by introducing (1) a novel clustering based exemplar selection algorithm to store the most representative data belonging to each learned food for knowledge replay, and (2) an effective online learning regime using balanced training batch along with the knowledge distillation on augmented exemplars to maintain the model performance on all learned classes. Our method is evaluated on a challenging large scale food image database, Food-1K, by varying the number of newly added food classes. Our results show significant improvements compared with existing state-of-the-art online continual learning methods, showing great potential to achieve lifelong learning for food image classification in real world.



### Deep Adversarially-Enhanced k-Nearest Neighbors
- **Arxiv ID**: http://arxiv.org/abs/2108.06797v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.06797v2)
- **Published**: 2021-08-15 19:18:53+00:00
- **Updated**: 2021-10-07 02:20:13+00:00
- **Authors**: Ren Wang, Tianqi Chen, Alfred Hero
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works have theoretically and empirically shown that deep neural networks (DNNs) have an inherent vulnerability to small perturbations. Applying the Deep k-Nearest Neighbors (DkNN) classifier, we observe a dramatically increasing robustness-accuracy trade-off as the layer goes deeper. In this work, we propose a Deep Adversarially-Enhanced k-Nearest Neighbors (DAEkNN) method which achieves higher robustness than DkNN and mitigates the robustness-accuracy trade-off in deep layers through two key elements. First, DAEkNN is based on an adversarially trained model. Second, DAEkNN makes predictions by leveraging a weighted combination of benign and adversarial training data. Empirically, we find that DAEkNN improves both the robustness and the robustness-accuracy trade-off on MNIST and CIFAR-10 datasets.



### The Marine Debris Dataset for Forward-Looking Sonar Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.06800v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.06800v1)
- **Published**: 2021-08-15 19:29:23+00:00
- **Updated**: 2021-08-15 19:29:23+00:00
- **Authors**: Deepak Singh, Matias Valdenegro-Toro
- **Comment**: OceanVision 2021 ICCV Worshop, Camera Ready, 9 pages, 13 figures, 6
  Tables
- **Journal**: None
- **Summary**: Accurate detection and segmentation of marine debris is important for keeping the water bodies clean. This paper presents a novel dataset for marine debris segmentation collected using a Forward Looking Sonar (FLS). The dataset consists of 1868 FLS images captured using ARIS Explorer 3000 sensor. The objects used to produce this dataset contain typical house-hold marine debris and distractor marine objects (tires, hooks, valves,etc), divided in 11 classes plus a background class. Performance of state of the art semantic segmentation architectures with a variety of encoders have been analyzed on this dataset and presented as baseline results. Since the images are grayscale, no pretrained weights have been used. Comparisons are made using Intersection over Union (IoU). The best performing model is Unet with ResNet34 backbone at 0.7481 mIoU. The dataset is available at https://github.com/mvaldenegro/marine-debris-fls-datasets/



### SSH: A Self-Supervised Framework for Image Harmonization
- **Arxiv ID**: http://arxiv.org/abs/2108.06805v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06805v2)
- **Published**: 2021-08-15 19:51:33+00:00
- **Updated**: 2021-08-17 18:02:53+00:00
- **Authors**: Yifan Jiang, He Zhang, Jianming Zhang, Yilin Wang, Zhe Lin, Kalyan Sunkavalli, Simon Chen, Sohrab Amirghodsi, Sarah Kong, Zhangyang Wang
- **Comment**: Accepted by ICCV'2021
- **Journal**: None
- **Summary**: Image harmonization aims to improve the quality of image compositing by matching the "appearance" (\eg, color tone, brightness and contrast) between foreground and background images. However, collecting large-scale annotated datasets for this task requires complex professional retouching. Instead, we propose a novel Self-Supervised Harmonization framework (SSH) that can be trained using just "free" natural images without being edited. We reformulate the image harmonization problem from a representation fusion perspective, which separately processes the foreground and background examples, to address the background occlusion issue. This framework design allows for a dual data augmentation method, where diverse [foreground, background, pseudo GT] triplets can be generated by cropping an image with perturbations using 3D color lookup tables (LUTs). In addition, we build a real-world harmonization dataset as carefully created by expert users, for evaluation and benchmarking purposes. Our results show that the proposed self-supervised method outperforms previous state-of-the-art methods in terms of reference metrics, visual quality, and subject user study. Code and dataset are available at \url{https://github.com/VITA-Group/SSHarmonization}.



### SCIDA: Self-Correction Integrated Domain Adaptation from Single- to Multi-label Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2108.06810v2
- **DOI**: 10.1109/TGRS.2022.3170357
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06810v2)
- **Published**: 2021-08-15 20:38:02+00:00
- **Updated**: 2021-11-29 23:09:49+00:00
- **Authors**: Tianze Yu, Jianzhe Lin, Lichao Mou, Yuansheng Hua, Xiaoxiang Zhu, Z. Jane Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Most publicly available datasets for image classification are with single labels, while images are inherently multi-labeled in our daily life. Such an annotation gap makes many pre-trained single-label classification models fail in practical scenarios. This annotation issue is more concerned for aerial images: Aerial data collected from sensors naturally cover a relatively large land area with multiple labels, while annotated aerial datasets, which are publicly available (e.g., UCM, AID), are single-labeled. As manually annotating multi-label aerial images would be time/labor-consuming, we propose a novel self-correction integrated domain adaptation (SCIDA) method for automatic multi-label learning. SCIDA is weakly supervised, i.e., automatically learning the multi-label image classification model from using massive, publicly available single-label images. To achieve this goal, we propose a novel Label-Wise self-Correction (LWC) module to better explore underlying label correlations. This module also makes the unsupervised domain adaptation (UDA) from single- to multi-label data possible. For model training, the proposed model only uses single-label information yet requires no prior knowledge of multi-labeled data; and it predicts labels for multi-label aerial images. In our experiments, trained with single-labeled MAI-AID-s and MAI-UCM-s datasets, the proposed model is tested directly on our collected Multi-scene Aerial Image (MAI) dataset.



### Asymmetric Bilateral Motion Estimation for Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2108.06815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06815v1)
- **Published**: 2021-08-15 21:11:35+00:00
- **Updated**: 2021-08-15 21:11:35+00:00
- **Authors**: Junheum Park, Chul Lee, Chang-Su Kim
- **Comment**: Accepted to ICCV2021
- **Journal**: None
- **Summary**: We propose a novel video frame interpolation algorithm based on asymmetric bilateral motion estimation (ABME), which synthesizes an intermediate frame between two input frames. First, we predict symmetric bilateral motion fields to interpolate an anchor frame. Second, we estimate asymmetric bilateral motions fields from the anchor frame to the input frames. Third, we use the asymmetric fields to warp the input frames backward and reconstruct the intermediate frame. Last, to refine the intermediate frame, we develop a new synthesis network that generates a set of dynamic filters and a residual frame using local and global information. Experimental results show that the proposed algorithm achieves excellent performance on various datasets. The source codes and pretrained models are available at https://github.com/JunHeum/ABME.



### Weakly Supervised Temporal Anomaly Segmentation with Dynamic Time Warping
- **Arxiv ID**: http://arxiv.org/abs/2108.06816v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.06816v1)
- **Published**: 2021-08-15 21:22:06+00:00
- **Updated**: 2021-08-15 21:22:06+00:00
- **Authors**: Dongha Lee, Sehun Yu, Hyunjun Ju, Hwanjo Yu
- **Comment**: ICCV 2021. 8 pages, References (2 pages), Appendix (3 pages), 6
  figures
- **Journal**: None
- **Summary**: Most recent studies on detecting and localizing temporal anomalies have mainly employed deep neural networks to learn the normal patterns of temporal data in an unsupervised manner. Unlike them, the goal of our work is to fully utilize instance-level (or weak) anomaly labels, which only indicate whether any anomalous events occurred or not in each instance of temporal data. In this paper, we present WETAS, a novel framework that effectively identifies anomalous temporal segments (i.e., consecutive time points) in an input instance. WETAS learns discriminative features from the instance-level labels so that it infers the sequential order of normal and anomalous segments within each instance, which can be used as a rough segmentation mask. Based on the dynamic time warping (DTW) alignment between the input instance and its segmentation mask, WETAS obtains the result of temporal segmentation, and simultaneously, it further enhances itself by using the mask as additional supervision. Our experiments show that WETAS considerably outperforms other baselines in terms of the localization of temporal anomalies, and also it provides more informative results than point-level detection methods.



### EventHPE: Event-based 3D Human Pose and Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/2108.06819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06819v1)
- **Published**: 2021-08-15 21:40:19+00:00
- **Updated**: 2021-08-15 21:40:19+00:00
- **Authors**: Shihao Zou, Chuan Guo, Xinxin Zuo, Sen Wang, Pengyu Wang, Xiaoqin Hu, Shoushun Chen, Minglun Gong, Li Cheng
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Event camera is an emerging imaging sensor for capturing dynamics of moving objects as events, which motivates our work in estimating 3D human pose and shape from the event signals. Events, on the other hand, have their unique challenges: rather than capturing static body postures, the event signals are best at capturing local motions. This leads us to propose a two-stage deep learning approach, called EventHPE. The first-stage, FlowNet, is trained by unsupervised learning to infer optical flow from events. Both events and optical flow are closely related to human body dynamics, which are fed as input to the ShapeNet in the second stage, to estimate 3D human shapes. To mitigate the discrepancy between image-based flow (optical flow) and shape-based flow (vertices movement of human body shape), a novel flow coherence loss is introduced by exploiting the fact that both flows are originated from the identical human motion. An in-house event-based 3D human dataset is curated that comes with 3D pose and shape annotations, which is by far the largest one to our knowledge. Empirical evaluations on DHP19 dataset and our in-house dataset demonstrate the effectiveness of our approach.



### CONet: Channel Optimization for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.06822v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2108.06822v2)
- **Published**: 2021-08-15 21:48:25+00:00
- **Updated**: 2022-04-07 19:14:42+00:00
- **Authors**: Mahdi S. Hosseini, Jia Shu Zhang, Zhe Liu, Andre Fu, Jingxuan Su, Mathieu Tuli, Sepehr Hosseini, Arsh Kadakia, Haoran Wang, Konstantinos N. Plataniotis
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) has shifted network design from using human intuition to leveraging search algorithms guided by evaluation metrics. We study channel size optimization in convolutional neural networks (CNN) and identify the role it plays in model accuracy and complexity. Current channel size selection methods are generally limited by discrete sample spaces while suffering from manual iteration and simple heuristics. To solve this, we introduce an efficient dynamic scaling algorithm -- CONet -- that automatically optimizes channel sizes across network layers for a given CNN. Two metrics -- "\textit{Rank}" and "\textit{Rank Average Slope}" -- are introduced to identify the information accumulated in training. The algorithm dynamically scales channel sizes up or down over a fixed searching phase. We conduct experiments on CIFAR10/100 and ImageNet datasets and show that CONet can find efficient and accurate architectures searched in ResNet, DARTS, and DARTS+ spaces that outperform their baseline models.   This document supersedes previously published paper in ICCV2021-NeurArch workshop. An additional section is included on manual scaling of channel size in CNNs to numerically validate of the metrics used in searching optimum channel configurations in CNNs.



### Human Pose and Shape Estimation from Single Polarization Images
- **Arxiv ID**: http://arxiv.org/abs/2108.06834v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06834v2)
- **Published**: 2021-08-15 22:56:18+00:00
- **Updated**: 2022-03-22 20:02:03+00:00
- **Authors**: Shihao Zou, Xinxin Zuo, Sen Wang, Yiming Qian, Chuan Guo, Li Cheng
- **Comment**: Accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: This paper focuses on a new problem of estimating human pose and shape from single polarization images. Polarization camera is known to be able to capture the polarization of reflected lights that preserves rich geometric cues of an object surface. Inspired by the recent applications in surface normal reconstruction from polarization images, in this paper, we attempt to estimate human pose and shape from single polarization images by leveraging the polarization-induced geometric cues. A dedicated two-stage pipeline is proposed: given a single polarization image, stage one (Polar2Normal) focuses on the fine detailed human body surface normal estimation; stage two (Polar2Shape) then reconstructs clothed human shape from the polarization image and the estimated surface normal. To empirically validate our approach, a dedicated dataset (PHSPD) is constructed, consisting of over 500K frames with accurate pose and parametric shape annotations. Empirical evaluations on this real-world dataset as well as a synthetic dataset, SURREAL, demonstrate the effectiveness of our approach. It suggests polarization camera as a promising alternative to the more conventional RGB camera for human pose and shape estimation.



