# Arxiv Papers in cs.CV on 2021-08-13
### Non-imaging real-time detection and tracking of fast-moving objects using a single-pixel detector
- **Arxiv ID**: http://arxiv.org/abs/2108.06009v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.06009v3)
- **Published**: 2021-08-13 01:07:51+00:00
- **Updated**: 2022-06-29 14:30:55+00:00
- **Authors**: Fengming Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Detection and tracking of fast-moving objects have widespread utility in many fields. However, fulfilling this demand for fast and efficient detecting and tracking using image-based techniques is problematic, owing to the complex calculations and limited data processing capabilities. To tackle this problem, we propose an image-free method to achieve real-time detection and tracking of fast-moving objects. It employs the Hadamard pattern to illuminate the fast-moving object by a spatial light modulator, in which the resulting light signal is collected by a single-pixel detector. The single-pixel measurement values are directly used to reconstruct the position information without image reconstruction. Furthermore, a new sampling method is used to optimize the pattern projection way for achieving an ultra-low sampling rate. Compared with the state-of-the-art methods, our approach is not only capable of handling real-time detection and tracking, but also it has a small amount of calculation and high efficiency. We experimentally demonstrate that the proposed method, using a 22kHz digital micro-mirror device, can implement a 105fps frame rate at a 1.28% sampling rate when tracked. Our method breaks through the traditional tracking ways, which can implement the object real-time tracking without image reconstruction.



### AGKD-BML: Defense Against Adversarial Attack by Attention Guided Knowledge Distillation and Bi-directional Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.06017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06017v1)
- **Published**: 2021-08-13 01:25:04+00:00
- **Updated**: 2021-08-13 01:25:04+00:00
- **Authors**: Hong Wang, Yuefan Deng, Shinjae Yoo, Haibin Ling, Yuewei Lin
- **Comment**: ICCV 2021 paper
- **Journal**: None
- **Summary**: While deep neural networks have shown impressive performance in many tasks, they are fragile to carefully designed adversarial attacks. We propose a novel adversarial training-based model by Attention Guided Knowledge Distillation and Bi-directional Metric Learning (AGKD-BML). The attention knowledge is obtained from a weight-fixed model trained on a clean dataset, referred to as a teacher model, and transferred to a model that is under training on adversarial examples (AEs), referred to as a student model. In this way, the student model is able to focus on the correct region, as well as correcting the intermediate features corrupted by AEs to eventually improve the model accuracy. Moreover, to efficiently regularize the representation in feature space, we propose a bidirectional metric learning. Specifically, given a clean image, it is first attacked to its most confusing class to get the forward AE. A clean image in the most confusing class is then randomly picked and attacked back to the original class to get the backward AE. A triplet loss is then used to shorten the representation distance between original image and its AE, while enlarge that between the forward and backward AEs. We conduct extensive adversarial robustness experiments on two widely used datasets with different attacks. Our proposed AGKD-BML model consistently outperforms the state-of-the-art approaches. The code of AGKD-BML will be available at: https://github.com/hongw579/AGKD-BML.



### CODEs: Chamfer Out-of-Distribution Examples against Overconfidence Issue
- **Arxiv ID**: http://arxiv.org/abs/2108.06024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06024v1)
- **Published**: 2021-08-13 01:56:10+00:00
- **Updated**: 2021-08-13 01:56:10+00:00
- **Authors**: Keke Tang, Dingruibo Miao, Weilong Peng, Jianpeng Wu, Yawen Shi, Zhaoquan Gu, Zhihong Tian, Wenping Wang
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Overconfident predictions on out-of-distribution (OOD) samples is a thorny issue for deep neural networks. The key to resolve the OOD overconfidence issue inherently is to build a subset of OOD samples and then suppress predictions on them. This paper proposes the Chamfer OOD examples (CODEs), whose distribution is close to that of in-distribution samples, and thus could be utilized to alleviate the OOD overconfidence issue effectively by suppressing predictions on them. To obtain CODEs, we first generate seed OOD examples via slicing&splicing operations on in-distribution samples from different categories, and then feed them to the Chamfer generative adversarial network for distribution transformation, without accessing to any extra data. Training with suppressing predictions on CODEs is validated to alleviate the OOD overconfidence issue largely without hurting classification accuracy, and outperform the state-of-the-art methods. Besides, we demonstrate CODEs are useful for improving OOD detection and classification.



### Track without Appearance: Learn Box and Tracklet Embedding with Local and Global Motion Patterns for Vehicle Tracking
- **Arxiv ID**: http://arxiv.org/abs/2108.06029v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06029v1)
- **Published**: 2021-08-13 02:27:09+00:00
- **Updated**: 2021-08-13 02:27:09+00:00
- **Authors**: Gaoang Wang, Renshu Gu, Zuozhu Liu, Weijie Hu, Mingli Song, Jenq-Neng Hwang
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle tracking is an essential task in the multi-object tracking (MOT) field. A distinct characteristic in vehicle tracking is that the trajectories of vehicles are fairly smooth in both the world coordinate and the image coordinate. Hence, models that capture motion consistencies are of high necessity. However, tracking with the standalone motion-based trackers is quite challenging because targets could get lost easily due to limited information, detection error and occlusion. Leveraging appearance information to assist object re-identification could resolve this challenge to some extent. However, doing so requires extra computation while appearance information is sensitive to occlusion as well. In this paper, we try to explore the significance of motion patterns for vehicle tracking without appearance information. We propose a novel approach that tackles the association issue for long-term tracking with the exclusive fully-exploited motion information. We address the tracklet embedding issue with the proposed reconstruct-to-embed strategy based on deep graph convolutional neural networks (GCN). Comprehensive experiments on the KITTI-car tracking dataset and UA-Detrac dataset show that the proposed method, though without appearance information, could achieve competitive performance with the state-of-the-art (SOTA) trackers. The source code will be available at https://github.com/GaoangW/LGMTracker.



### Progressive Representative Labeling for Deep Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.06070v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06070v1)
- **Published**: 2021-08-13 05:39:50+00:00
- **Updated**: 2021-08-13 05:39:50+00:00
- **Authors**: Xiaopeng Yan, Riquan Chen, Litong Feng, Jingkang Yang, Huabin Zheng, Wayne Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep semi-supervised learning (SSL) has experienced significant attention in recent years, to leverage a huge amount of unlabeled data to improve the performance of deep learning with limited labeled data. Pseudo-labeling is a popular approach to expand the labeled dataset. However, whether there is a more effective way of labeling remains an open problem. In this paper, we propose to label only the most representative samples to expand the labeled set. Representative samples, selected by indegree of corresponding nodes on a directed k-nearest neighbor (kNN) graph, lie in the k-nearest neighborhood of many other samples. We design a graph neural network (GNN) labeler to label them in a progressive learning manner. Aided by the progressive GNN labeler, our deep SSL approach outperforms state-of-the-art methods on several popular SSL benchmarks including CIFAR-10, SVHN, and ILSVRC-2012. Notably, we achieve 72.1% top-1 accuracy, surpassing the previous best result by 3.3%, on the challenging ImageNet benchmark with only $10\%$ labeled data.



### Coupling Model-Driven and Data-Driven Methods for Remote Sensing Image Restoration and Fusion
- **Arxiv ID**: http://arxiv.org/abs/2108.06073v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.06073v1)
- **Published**: 2021-08-13 06:00:31+00:00
- **Updated**: 2021-08-13 06:00:31+00:00
- **Authors**: Huanfeng Shen, Menghui Jiang, Jie Li, Chenxia Zhou, Qiangqiang Yuan, Liangpei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In the fields of image restoration and image fusion, model-driven methods and data-driven methods are the two representative frameworks. However, both approaches have their respective advantages and disadvantages. The model-driven methods consider the imaging mechanism, which is deterministic and theoretically reasonable; however, they cannot easily model complicated nonlinear problems. The data-driven methods have a stronger prior knowledge learning capability for huge data, especially for nonlinear statistical features; however, the interpretability of the networks is poor, and they are over-dependent on training data. In this paper, we systematically investigate the coupling of model-driven and data-driven methods, which has rarely been considered in the remote sensing image restoration and fusion communities. We are the first to summarize the coupling approaches into the following three categories: 1) data-driven and model-driven cascading methods; 2) variational models with embedded learning; and 3) model-constrained network learning methods. The typical existing and potential coupling methods for remote sensing image restoration and fusion are introduced with application examples. This paper also gives some new insights into the potential future directions, in terms of both methods and applications.



### PVT: Point-Voxel Transformer for Point Cloud Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.06076v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2108.06076v4)
- **Published**: 2021-08-13 06:07:57+00:00
- **Updated**: 2022-05-25 06:34:21+00:00
- **Authors**: Cheng Zhang, Haocheng Wan, Xinyi Shen, Zizhao Wu
- **Comment**: 29 pages
- **Journal**: None
- **Summary**: The recently developed pure Transformer architectures have attained promising accuracy on point cloud learning benchmarks compared to convolutional neural networks. However, existing point cloud Transformers are computationally expensive since they waste a significant amount of time on structuring the irregular data. To solve this shortcoming, we present Sparse Window Attention (SWA) module to gather coarse-grained local features from non-empty voxels, which not only bypasses the expensive irregular data structuring and invalid empty voxel computation, but also obtains linear computational complexity with respect to voxel resolution. Meanwhile, to gather fine-grained features about the global shape, we introduce relative attention (RA) module, a more robust self-attention variant for rigid transformations of objects. Equipped with the SWA and RA, we construct our neural architecture called PVT that integrates both modules into a joint framework for point cloud learning. Compared with previous Transformer-based and attention-based models, our method attains top accuracy of 94.0% on classification benchmark and 10x inference speedup on average. Extensive experiments also valid the effectiveness of PVT on part and semantic segmentation benchmarks (86.6% and 69.2% mIoU, respectively).



### A Generative Adversarial Framework for Optimizing Image Matting and Harmonization Simultaneously
- **Arxiv ID**: http://arxiv.org/abs/2108.06087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06087v1)
- **Published**: 2021-08-13 06:48:14+00:00
- **Updated**: 2021-08-13 06:48:14+00:00
- **Authors**: Xuqian Ren, Yifan Liu, Chunlei Song
- **Comment**: Extension for accepted ICIP 2021
- **Journal**: ICIP 2021
- **Summary**: Image matting and image harmonization are two important tasks in image composition. Image matting, aiming to achieve foreground boundary details, and image harmonization, aiming to make the background compatible with the foreground, are both promising yet challenging tasks. Previous works consider optimizing these two tasks separately, which may lead to a sub-optimal solution. We propose to optimize matting and harmonization simultaneously to get better performance on both the two tasks and achieve more natural results. We propose a new Generative Adversarial (GAN) framework which optimizing the matting network and the harmonization network based on a self-attention discriminator. The discriminator is required to distinguish the natural images from different types of fake synthesis images. Extensive experiments on our constructed dataset demonstrate the effectiveness of our proposed method. Our dataset and dataset generating pipeline can be found in \url{https://git.io/HaMaGAN}



### SVC-onGoing: Signature Verification Competition
- **Arxiv ID**: http://arxiv.org/abs/2108.06090v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2108.06090v2)
- **Published**: 2021-08-13 06:51:32+00:00
- **Updated**: 2022-02-22 10:55:48+00:00
- **Authors**: Ruben Tolosana, Ruben Vera-Rodriguez, Carlos Gonzalez-Garcia, Julian Fierrez, Aythami Morales, Javier Ortega-Garcia, Juan Carlos Ruiz-Garcia, Sergio Romero-Tapiador, Santiago Rengifo, Miguel Caruana, Jiajia Jiang, Songxuan Lai, Lianwen Jin, Yecheng Zhu, Javier Galbally, Moises Diaz, Miguel Angel Ferrer, Marta Gomez-Barrero, Ilya Hodashinsky, Konstantin Sarin, Artem Slezkin, Marina Bardamova, Mikhail Svetlakov, Mohammad Saleem, Cintia Lia Szucs, Bence Kovari, Falk Pulsmeyer, Mohamad Wehbi, Dario Zanca, Sumaiya Ahmad, Sarthak Mishra, Suraiya Jabin
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2106.00739
- **Journal**: Pattern Recognition, 2022
- **Summary**: This article presents SVC-onGoing, an on-going competition for on-line signature verification where researchers can easily benchmark their systems against the state of the art in an open common platform using large-scale public databases, such as DeepSignDB and SVC2021_EvalDB, and standard experimental protocols. SVC-onGoing is based on the ICDAR 2021 Competition on On-Line Signature Verification (SVC 2021), which has been extended to allow participants anytime. The goal of SVC-onGoing is to evaluate the limits of on-line signature verification systems on popular scenarios (office/mobile) and writing inputs (stylus/finger) through large-scale public databases. Three different tasks are considered in the competition, simulating realistic scenarios as both random and skilled forgeries are simultaneously considered on each task. The results obtained in SVC-onGoing prove the high potential of deep learning methods in comparison with traditional methods. In particular, the best signature verification system has obtained Equal Error Rate (EER) values of 3.33% (Task 1), 7.41% (Task 2), and 6.04% (Task 3). Future studies in the field should be oriented to improve the performance of signature verification systems on the challenging mobile scenarios of SVC-onGoing in which several mobile devices and the finger are used during the signature acquisition.



### FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.06098v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.06098v3)
- **Published**: 2021-08-13 07:16:40+00:00
- **Updated**: 2023-01-19 08:21:44+00:00
- **Authors**: Nam Hyeon-Woo, Moon Ye-Bin, Tae-Hyun Oh
- **Comment**: Accepted at ICLR 2022
- **Journal**: None
- **Summary**: In this work, we propose a communication-efficient parameterization, FedPara, for federated learning (FL) to overcome the burdens on frequent model uploads and downloads. Our method re-parameterizes weight parameters of layers using low-rank weights followed by the Hadamard product. Compared to the conventional low-rank parameterization, our FedPara method is not restricted to low-rank constraints, and thereby it has a far larger capacity. This property enables to achieve comparable performance while requiring 3 to 10 times lower communication costs than the model with the original layers, which is not achievable by the traditional low-rank methods. The efficiency of our method can be further improved by combining with other efficient FL optimizers. In addition, we extend our method to a personalized FL application, pFedPara, which separates parameters into global and local ones. We show that pFedPara outperforms competing personalized FL methods with more than three times fewer parameters.



### Bi-Temporal Semantic Reasoning for the Semantic Change Detection in HR Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2108.06103v4
- **DOI**: 10.1109/TGRS.2022.3154390
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.06103v4)
- **Published**: 2021-08-13 07:28:09+00:00
- **Updated**: 2022-01-05 14:30:44+00:00
- **Authors**: Lei Ding, Haitao Guo, Sicong Liu, Lichao Mou, Jing Zhang, Lorenzo Bruzzone
- **Comment**: Manuscript to IEEE TGRS
- **Journal**: None
- **Summary**: Semantic change detection (SCD) extends the multi-class change detection (MCD) task to provide not only the change locations but also the detailed land-cover/land-use (LCLU) categories before and after the observation intervals. This fine-grained semantic change information is very useful in many applications. Recent studies indicate that the SCD can be modeled through a triple-branch Convolutional Neural Network (CNN), which contains two temporal branches and a change branch. However, in this architecture, the communications between the temporal branches and the change branch are insufficient. To overcome the limitations in existing methods, we propose a novel CNN architecture for the SCD, where the semantic temporal features are merged in a deep CD unit. Furthermore, we elaborate on this architecture to reason the bi-temporal semantic correlations. The resulting Bi-temporal Semantic Reasoning Network (Bi-SRNet) contains two types of semantic reasoning blocks to reason both single-temporal and cross-temporal semantic correlations, as well as a novel loss function to improve the semantic consistency of change detection results. Experimental results on a benchmark dataset show that the proposed architecture obtains significant accuracy improvements over the existing approaches, while the added designs in the Bi-SRNet further improves the segmentation of both semantic categories and the changed areas. The codes in this paper are accessible at: github.com/ggsDing/Bi-SRNet.



### UMFA: A photorealistic style transfer method based on U-Net and multi-layer feature aggregation
- **Arxiv ID**: http://arxiv.org/abs/2108.06113v1
- **DOI**: 10.1117/1.JEI.30.5.053013
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.06113v1)
- **Published**: 2021-08-13 08:06:29+00:00
- **Updated**: 2021-08-13 08:06:29+00:00
- **Authors**: D. Y. Rao, X. J. Wu, H. Li, J. Kittler, T. Y. Xu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a photorealistic style transfer network to emphasize the natural effect of photorealistic image stylization. In general, distortion of the image content and lacking of details are two typical issues in the style transfer field. To this end, we design a novel framework employing the U-Net structure to maintain the rich spatial clues, with a multi-layer feature aggregation (MFA) method to simultaneously provide the details obtained by the shallow layers in the stylization processing. In particular, an encoder based on the dense block and a decoder form a symmetrical structure of U-Net are jointly staked to realize an effective feature extraction and image reconstruction. Besides, a transfer module based on MFA and "adaptive instance normalization" (AdaIN) is inserted in the skip connection positions to achieve the stylization. Accordingly, the stylized image possesses the texture of a real photo and preserves rich content details without introducing any mask or post-processing steps. The experimental results on public datasets demonstrate that our method achieves a more faithful structural similarity with a lower style loss, reflecting the effectiveness and merit of our approach.



### Effective semantic segmentation in Cataract Surgery: What matters most?
- **Arxiv ID**: http://arxiv.org/abs/2108.06119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06119v1)
- **Published**: 2021-08-13 08:27:54+00:00
- **Updated**: 2021-08-13 08:27:54+00:00
- **Authors**: Theodoros Pissas, Claudio Ravasio, Lyndon Da Cruz, Christos Bergeles
- **Comment**: MICCAI 2021
- **Journal**: None
- **Summary**: Our work proposes neural network design choices that set the state-of-the-art on a challenging public benchmark on cataract surgery, CaDIS. Our methodology achieves strong performance across three semantic segmentation tasks with increasingly granular surgical tool class sets by effectively handling class imbalance, an inherent challenge in any surgical video. We consider and evaluate two conceptually simple data oversampling methods as well as different loss functions. We show significant performance gains across network architectures and tasks especially on the rarest tool classes, thereby presenting an approach for achieving high performance when imbalanced granular datasets are considered. Our code and trained models are available at https://github.com/RViMLab/MICCAI2021_Cataract_semantic_segmentation and qualitative results on unseen surgical video can be found at https://youtu.be/twVIPUj1WZM.



### Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices
- **Arxiv ID**: http://arxiv.org/abs/2108.06128v3
- **DOI**: 10.1109/ACCESS.2022.3140807
- **Categories**: **cs.SD**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.06128v3)
- **Published**: 2021-08-13 09:07:45+00:00
- **Updated**: 2022-01-17 05:30:27+00:00
- **Authors**: Md Mohaimenuzzaman, Christoph Bergmeir, Bernd Meyer
- **Comment**: 13 pages
- **Journal**: in IEEE Access, vol. 10, 2022
- **Summary**: Deep learning has celebrated resounding successes in many application areas of relevance to the Internet of Things (IoT), such as computer vision and machine listening. These technologies must ultimately be brought directly to the edge to fully harness the power of deep learning for the IoT. The obvious challenge is that deep learning techniques can only be implemented on strictly resource-constrained edge devices if the models are radically downsized. This task relies on different model compression techniques, such as network pruning, quantization, and the recent advancement of XNOR-Net. This study examines the suitability of these techniques for audio classification on microcontrollers. We present an application of XNOR-Net for end-to-end raw audio classification and a comprehensive empirical study comparing this approach with pruning-and-quantization methods. We show that raw audio classification with XNOR yields comparable performance to regular full precision networks for small numbers of classes while reducing memory requirements 32-fold and computation requirements 58-fold. However, as the number of classes increases significantly, performance degrades, and pruning-and-quantization based compression techniques take over as the preferred technique being able to satisfy the same space constraints but requiring approximately 8x more computation. We show that these insights are consistent between raw audio classification and image classification using standard benchmark sets. To the best of our knowledge, this is the first study to apply XNOR to end-to-end audio classification and evaluate it in the context of alternative techniques. All codes are publicly available on GitHub.



### Learning Transferable Parameters for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2108.06129v1
- **DOI**: 10.1109/TIP.2022.3184848
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.06129v1)
- **Published**: 2021-08-13 09:09:15+00:00
- **Updated**: 2021-08-13 09:09:15+00:00
- **Authors**: Zhongyi Han, Haoliang Sun, Yilong Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) enables a learning machine to adapt from a labeled source domain to an unlabeled domain under the distribution shift. Thanks to the strong representation ability of deep neural networks, recent remarkable achievements in UDA resort to learning domain-invariant features. Intuitively, the hope is that a good feature representation, together with the hypothesis learned from the source domain, can generalize well to the target domain. However, the learning processes of domain-invariant features and source hypothesis inevitably involve domain-specific information that would degrade the generalizability of UDA models on the target domain. In this paper, motivated by the lottery ticket hypothesis that only partial parameters are essential for generalization, we find that only partial parameters are essential for learning domain-invariant information and generalizing well in UDA. Such parameters are termed transferable parameters. In contrast, the other parameters tend to fit domain-specific details and often fail to generalize, which we term as untransferable parameters. Driven by this insight, we propose Transferable Parameter Learning (TransPar) to reduce the side effect brought by domain-specific information in the learning process and thus enhance the memorization of domain-invariant information. Specifically, according to the distribution discrepancy degree, we divide all parameters into transferable and untransferable ones in each training iteration. We then perform separate updates rules for the two types of parameters. Extensive experiments on image classification and regression tasks (keypoint detection) show that TransPar outperforms prior arts by non-trivial margins. Moreover, experiments demonstrate that TransPar can be integrated into the most popular deep UDA networks and be easily extended to handle any data distribution shift scenarios.



### Full-resolution quality assessment for pansharpening
- **Arxiv ID**: http://arxiv.org/abs/2108.06144v3
- **DOI**: 10.3390/rs14081808
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06144v3)
- **Published**: 2021-08-13 09:35:45+00:00
- **Updated**: 2022-04-04 12:43:36+00:00
- **Authors**: Giuseppe Scarpa, Matteo Ciotola
- **Comment**: None
- **Journal**: Remote Sensing, 14 (2022), 1808
- **Summary**: A reliable quality assessment procedure for pansharpening methods is of critical importance for the development of the related solutions. Unfortunately, the lack of ground-truths to be used as guidance for an objective evaluation has pushed the community to resort to two approaches which can also be jointly applied. Hence, two kinds of indexes can be found in the literature: i) reference-based reduced-resolution indexes aimed to assess the synthesis ability; ii) no-reference subjective quality indexes for full-resolution datasets aimed to assess spectral and spatial consistency. Both reference-based and no-reference indexes present critical shortcomings which motivate the community to explore new solutions. In this work, we propose an alternative no-reference full-resolution assessment framework. On one side we introduce a protocol, namely the reprojection protocol, to take care of the spectral consistency issue. On the other side, a new index of the spatial consistency between the pansharpened image and the panchromatic band at full resolution is also proposed. Experimental results carried out on different datasets/sensors demonstrate the effectiveness of the proposed approach.



### Conditional DETR for Fast Training Convergence
- **Arxiv ID**: http://arxiv.org/abs/2108.06152v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06152v2)
- **Published**: 2021-08-13 10:07:46+00:00
- **Updated**: 2021-08-19 03:03:46+00:00
- **Authors**: Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: The recently-developed DETR approach applies the transformer encoder and decoder architecture to object detection and achieves promising performance. In this paper, we handle the critical issue, slow training convergence, and present a conditional cross-attention mechanism for fast DETR training. Our approach is motivated by that the cross-attention in DETR relies highly on the content embeddings for localizing the four extremities and predicting the box, which increases the need for high-quality content embeddings and thus the training difficulty. Our approach, named conditional DETR, learns a conditional spatial query from the decoder embedding for decoder multi-head cross-attention. The benefit is that through the conditional spatial query, each cross-attention head is able to attend to a band containing a distinct region, e.g., one object extremity or a region inside the object box. This narrows down the spatial range for localizing the distinct regions for object classification and box regression, thus relaxing the dependence on the content embeddings and easing the training. Empirical results show that conditional DETR converges 6.7x faster for the backbones R50 and R101 and 10x faster for stronger backbones DC5-R50 and DC5-R101. Code is available at https://github.com/Atten4Vis/ConditionalDETR.



### EEEA-Net: An Early Exit Evolutionary Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2108.06156v1
- **DOI**: 10.1016/j.engappai.2021.104397
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2108.06156v1)
- **Published**: 2021-08-13 10:23:19+00:00
- **Updated**: 2021-08-13 10:23:19+00:00
- **Authors**: Chakkrit Termritthikun, Yeshi Jamtsho, Jirarat Ieamsaard, Paisarn Muneesawang, Ivan Lee
- **Comment**: Published at Engineering Applications of Artificial Intelligence;
  Code and pretrained models available at
  https://github.com/chakkritte/EEEA-Net
- **Journal**: Engineering Applications of Artificial Intelligence. 2021 Sep
  1;104:104397
- **Summary**: The goals of this research were to search for Convolutional Neural Network (CNN) architectures, suitable for an on-device processor with limited computing resources, performing at substantially lower Network Architecture Search (NAS) costs. A new algorithm entitled an Early Exit Population Initialisation (EE-PI) for Evolutionary Algorithm (EA) was developed to achieve both goals. The EE-PI reduces the total number of parameters in the search process by filtering the models with fewer parameters than the maximum threshold. It will look for a new model to replace those models with parameters more than the threshold. Thereby, reducing the number of parameters, memory usage for model storage and processing time while maintaining the same performance or accuracy. The search time was reduced to 0.52 GPU day. This is a huge and significant achievement compared to the NAS of 4 GPU days achieved using NSGA-Net, 3,150 GPU days by the AmoebaNet model, and the 2,000 GPU days by the NASNet model. As well, Early Exit Evolutionary Algorithm networks (EEEA-Nets) yield network architectures with minimal error and computational cost suitable for a given dataset as a class of network algorithms. Using EEEA-Net on CIFAR-10, CIFAR-100, and ImageNet datasets, our experiments showed that EEEA-Net achieved the lowest error rate among state-of-the-art NAS models, with 2.46% for CIFAR-10, 15.02% for CIFAR-100, and 23.8% for ImageNet dataset. Further, we implemented this image recognition architecture for other tasks, such as object detection, semantic segmentation, and keypoint detection tasks, and, in our experiments, EEEA-Net-C2 outperformed MobileNet-V3 on all of these various tasks. (The algorithm code is available at https://github.com/chakkritte/EEEA-Net).



### Robustness testing of AI systems: A case study for traffic sign recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.06159v1
- **DOI**: 10.1007/978-3-030-79150-6_21
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.06159v1)
- **Published**: 2021-08-13 10:29:09+00:00
- **Updated**: 2021-08-13 10:29:09+00:00
- **Authors**: Christian Berghoff, Pavol Bielik, Matthias Neu, Petar Tsankov, Arndt von Twickel
- **Comment**: 12 pages, 7 figures. The final publication is available at Springer
  via https://doi.org/10.1007/978-3-030-79150-6_21
- **Journal**: In: Maglogiannis I., Macintyre J., Iliadis L. (eds) Artificial
  Intelligence Applications and Innovations. AIAI 2021. IFIP Advances in
  Information and Communication Technology, vol 627. Springer, Cham
- **Summary**: In the last years, AI systems, in particular neural networks, have seen a tremendous increase in performance, and they are now used in a broad range of applications. Unlike classical symbolic AI systems, neural networks are trained using large data sets and their inner structure containing possibly billions of parameters does not lend itself to human interpretation. As a consequence, it is so far not feasible to provide broad guarantees for the correct behaviour of neural networks during operation if they process input data that significantly differ from those seen during training. However, many applications of AI systems are security- or safety-critical, and hence require obtaining statements on the robustness of the systems when facing unexpected events, whether they occur naturally or are induced by an attacker in a targeted way. As a step towards developing robust AI systems for such applications, this paper presents how the robustness of AI systems can be practically examined and which methods and metrics can be used to do so. The robustness testing methodology is described and analysed for the example use case of traffic sign recognition in autonomous driving.



### Caption Generation on Scenes with Seen and Unseen Object Categories
- **Arxiv ID**: http://arxiv.org/abs/2108.06165v2
- **DOI**: 10.1016/j.imavis.2022.104515
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06165v2)
- **Published**: 2021-08-13 10:43:20+00:00
- **Updated**: 2022-07-01 11:47:46+00:00
- **Authors**: Berkan Demirel, Ramazan Gokberk Cinbis
- **Comment**: Accepted for Publication at Image and Vision Computing (IMAVIS)
- **Journal**: None
- **Summary**: Image caption generation is one of the most challenging problems at the intersection of vision and language domains. In this work, we propose a realistic captioning task where the input scenes may incorporate visual objects with no corresponding visual or textual training examples. For this problem, we propose a detection-driven approach that consists of a single-stage generalized zero-shot detection model to recognize and localize instances of both seen and unseen classes, and a template-based captioning model that transforms detections into sentences. To improve the generalized zero-shot detection model, which provides essential information for captioning, we define effective class representations in terms of class-to-class semantic similarities, and leverage their special structure to construct an effective unseen/seen class confidence score calibration mechanism. We also propose a novel evaluation metric that provides additional insights for the captioning outputs by separately measuring the visual and non-visual contents of generated sentences. Our experiments highlight the importance of studying captioning in the proposed zero-shot setting, and verify the effectiveness of the proposed detection-driven zero-shot captioning approach.



### IFR: Iterative Fusion Based Recognizer For Low Quality Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.06166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06166v1)
- **Published**: 2021-08-13 10:45:01+00:00
- **Updated**: 2021-08-13 10:45:01+00:00
- **Authors**: Zhiwei Jia, Shugong Xu, Shiyi Mu, Yue Tao, Shan Cao, Zhiyong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Although recent works based on deep learning have made progress in improving recognition accuracy on scene text recognition, how to handle low-quality text images in end-to-end deep networks remains a research challenge. In this paper, we propose an Iterative Fusion based Recognizer (IFR) for low quality scene text recognition, taking advantage of refined text images input and robust feature representation. IFR contains two branches which focus on scene text recognition and low quality scene text image recovery respectively. We utilize an iterative collaboration between two branches, which can effectively alleviate the impact of low quality input. A feature fusion module is proposed to strengthen the feature representation of the two branches, where the features from the Recognizer are Fused with image Restoration branch, referred to as RRF. Without changing the recognition network structure, extensive quantitative and qualitative experimental results show that the proposed method significantly outperforms the baseline methods in boosting the recognition accuracy of benchmark datasets and low resolution images in TextZoom dataset.



### Evaluating the Robustness of Semantic Segmentation for Autonomous Driving against Real-World Adversarial Patch Attacks
- **Arxiv ID**: http://arxiv.org/abs/2108.06179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06179v1)
- **Published**: 2021-08-13 11:49:09+00:00
- **Updated**: 2021-08-13 11:49:09+00:00
- **Authors**: Federico Nesti, Giulio Rossolini, Saasha Nair, Alessandro Biondi, Giorgio Buttazzo
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning and convolutional neural networks allow achieving impressive performance in computer vision tasks, such as object detection and semantic segmentation (SS). However, recent studies have shown evident weaknesses of such models against adversarial perturbations. In a real-world scenario instead, like autonomous driving, more attention should be devoted to real-world adversarial examples (RWAEs), which are physical objects (e.g., billboards and printable patches) optimized to be adversarial to the entire perception pipeline. This paper presents an in-depth evaluation of the robustness of popular SS models by testing the effects of both digital and real-world adversarial patches. These patches are crafted with powerful attacks enriched with a novel loss function. Firstly, an investigation on the Cityscapes dataset is conducted by extending the Expectation Over Transformation (EOT) paradigm to cope with SS. Then, a novel attack optimization, called scene-specific attack, is proposed. Such an attack leverages the CARLA driving simulator to improve the transferability of the proposed EOT-based attack to a real 3D environment. Finally, a printed physical billboard containing an adversarial patch was tested in an outdoor driving scenario to assess the feasibility of the studied attacks in the real world. Exhaustive experiments revealed that the proposed attack formulations outperform previous work to craft both digital and real-world adversarial patches for SS. At the same time, the experimental results showed how these attacks are notably less effective in the real world, hence questioning the practical relevance of adversarial attacks to SS models for autonomous/assisted driving.



### SPACE: A Simulator for Physical Interactions and Causal Learning in 3D Environments
- **Arxiv ID**: http://arxiv.org/abs/2108.06180v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.06180v1)
- **Published**: 2021-08-13 11:49:46+00:00
- **Updated**: 2021-08-13 11:49:46+00:00
- **Authors**: Jiafei Duan, Samson Yu Bai Jian, Cheston Tan
- **Comment**: Accepted to ICCV 21, Simulation Technology for Embodied AI (SEAI)
  Workshop
- **Journal**: None
- **Summary**: Recent advancements in deep learning, computer vision, and embodied AI have given rise to synthetic causal reasoning video datasets. These datasets facilitate the development of AI algorithms that can reason about physical interactions between objects. However, datasets thus far have primarily focused on elementary physical events such as rolling or falling. There is currently a scarcity of datasets that focus on the physical interactions that humans perform daily with objects in the real world. To address this scarcity, we introduce SPACE: A Simulator for Physical Interactions and Causal Learning in 3D Environments. The SPACE simulator allows us to generate the SPACE dataset, a synthetic video dataset in a 3D environment, to systematically evaluate physics-based models on a range of physical causal reasoning tasks. Inspired by daily object interactions, the SPACE dataset comprises videos depicting three types of physical events: containment, stability and contact. These events make up the vast majority of the basic physical interactions between objects. We then further evaluate it with a state-of-the-art physics-based deep model and show that the SPACE dataset improves the learning of intuitive physics with an approach inspired by curriculum learning. Repository: https://github.com/jiafei1224/SPACE



### Detecting socially interacting groups using f-formation: A survey of taxonomy, methods, datasets, applications, challenges, and future research directions
- **Arxiv ID**: http://arxiv.org/abs/2108.06181v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.HC, cs.LG, cs.RO, Artificial intelligence, Computer vision, Robotics, Machine
  learning, Deep learning, I.2; I.2.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2108.06181v2)
- **Published**: 2021-08-13 11:51:17+00:00
- **Updated**: 2021-10-13 13:28:16+00:00
- **Authors**: Hrishav Bakul Barua, Theint Haythi Mg, Pradip Pramanick, Chayan Sarkar
- **Comment**: Preprint submitted for Review
- **Journal**: None
- **Summary**: Robots in our daily surroundings are increasing day by day. Their usability and acceptability largely depend on their explicit and implicit interaction capability with fellow human beings. As a result, social behavior is one of the most sought-after qualities that a robot can possess. However, there is no specific aspect and/or feature that defines socially acceptable behavior and it largely depends on the situation, application, and society. In this article, we investigate one such social behavior for collocated robots. Imagine a group of people is interacting with each other and we want to join the group. We as human beings do it in a socially acceptable manner, i.e., within the group, we do position ourselves in such a way that we can participate in the group activity without disturbing/obstructing anybody. To possess such a quality, first, a robot needs to determine the formation of the group and then determine a position for itself, which we humans do implicitly. The theory of f-formation can be utilized for this purpose. As the types of formations can be very diverse, detecting the social groups is not a trivial task. In this article, we provide a comprehensive survey of the existing work on social interaction and group detection using f-formation for robotics and other applications. We also put forward a novel holistic survey framework combining all the possible concerns and modules relevant to this problem. We define taxonomies based on methods, camera views, datasets, detection capabilities and scale, evaluation approaches, and application areas. We discuss certain open challenges and limitations in current literature along with possible future research directions based on this framework. In particular, we discuss the existing methods/techniques and their relative merits and demerits, applications, and provide a set of unsolved but relevant problems in this domain.



### CNN-based Two-Stage Parking Slot Detection Using Region-Specific Multi-Scale Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/2108.06185v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06185v1)
- **Published**: 2021-08-13 12:02:02+00:00
- **Updated**: 2021-08-13 12:02:02+00:00
- **Authors**: Quang Huy Bui, Jae Kyu Suhr
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous parking systems start with the detection of available parking slots. Parking slot detection performance has been dramatically improved by deep learning techniques. Deep learning-based object detection methods can be categorized into one-stage and two-stage approaches. Although it is well-known that the two-stage approach outperforms the one-stage approach in general object detection, they have performed similarly in parking slot detection so far. We consider this is because the two-stage approach has not yet been adequately specialized for parking slot detection. Thus, this paper proposes a highly specialized two-stage parking slot detector that uses region-specific multi-scale feature extraction. In the first stage, the proposed method finds the entrance of the parking slot as a region proposal by estimating its center, length, and orientation. The second stage of this method designates specific regions that most contain the desired information and extracts features from them. That is, features for the location and orientation are separately extracted from only the specific regions that most contain the locational and orientational information. In addition, multi-resolution feature maps are utilized to increase both positioning and classification accuracies. A high-resolution feature map is used to extract detailed information (location and orientation), while another low-resolution feature map is used to extract semantic information (type and occupancy). In experiments, the proposed method was quantitatively evaluated with two large-scale public parking slot detection datasets and outperformed previous methods, including both one-stage and two-stage approaches.



### SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.06227v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.06227v4)
- **Published**: 2021-08-13 13:17:58+00:00
- **Updated**: 2022-03-21 16:43:52+00:00
- **Authors**: Chenyu You, Yuan Zhou, Ruihan Zhao, Lawrence Staib, James S. Duncan
- **Comment**: IEEE Transactions on Medical Imaging (IEEE-TMI) 2022
- **Journal**: None
- **Summary**: Automated segmentation in medical image analysis is a challenging task that requires a large amount of manually labeled data. However, most existing learning-based approaches usually suffer from limited manually annotated medical data, which poses a major practical problem for accurate and robust medical image segmentation. In addition, most existing semi-supervised approaches are usually not robust compared with the supervised counterparts, and also lack explicit modeling of geometric structure and semantic information, both of which limit the segmentation accuracy. In this work, we present SimCVD, a simple contrastive distillation framework that significantly advances state-of-the-art voxel-wise representation learning. We first describe an unsupervised training strategy, which takes two views of an input volume and predicts their signed distance maps of object boundaries in a contrastive objective, with only two independent dropout as mask. This simple approach works surprisingly well, performing on the same level as previous fully supervised methods with much less labeled data. We hypothesize that dropout can be viewed as a minimal form of data augmentation and makes the network robust to representation collapse. Then, we propose to perform structural distillation by distilling pair-wise similarities. We evaluate SimCVD on two popular datasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT dataset. The results on the LA dataset demonstrate that, in two types of labeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of 90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to previous best results. Our method can be trained in an end-to-end fashion, showing the promise of utilizing SimCVD as a general framework for downstream tasks, such as medical image synthesis, enhancement, and registration.



### Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2108.06230v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06230v5)
- **Published**: 2021-08-13 13:29:27+00:00
- **Updated**: 2023-01-19 11:58:47+00:00
- **Authors**: Björn Michele, Alexandre Boulch, Gilles Puy, Maxime Bucher, Renaud Marlet
- **Comment**: For the published code, see https://github.com/valeoai/3DGenZ
- **Journal**: Proceedings of the 2021 International Conference on 3D Vision (3DV
  2021), pp. 992-1002
- **Summary**: While there has been a number of studies on Zero-Shot Learning (ZSL) for 2D images, its application to 3D data is still recent and scarce, with just a few methods limited to classification. We present the first generative approach for both ZSL and Generalized ZSL (GZSL) on 3D data, that can handle both classification and, for the first time, semantic segmentation. We show that it reaches or outperforms the state of the art on ModelNet40 classification for both inductive ZSL and inductive GZSL. For semantic segmentation, we created three benchmarks for evaluating this new ZSL task, using S3DIS, ScanNet and SemanticKITTI. Our experiments show that our method outperforms strong baselines, which we additionally propose for this task.



### An Interpretable Algorithm for Uveal Melanoma Subtyping from Whole Slide Cytology Images
- **Arxiv ID**: http://arxiv.org/abs/2108.06246v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.06246v1)
- **Published**: 2021-08-13 13:55:08+00:00
- **Updated**: 2021-08-13 13:55:08+00:00
- **Authors**: Haomin Chen, T. Y. Alvin Liu, Catalina Gomez, Zelia Correa, Mathias Unberath
- **Comment**: Accepted by ICML 2021 workshop of ILHM
- **Journal**: None
- **Summary**: Algorithmic decision support is rapidly becoming a staple of personalized medicine, especially for high-stakes recommendations in which access to certain information can drastically alter the course of treatment, and thus, patient outcome; a prominent example is radiomics for cancer subtyping. Because in these scenarios the stakes are high, it is desirable for decision systems to not only provide recommendations but supply transparent reasoning in support thereof. For learning-based systems, this can be achieved through an interpretable design of the inference pipeline. Herein we describe an automated yet interpretable system for uveal melanoma subtyping with digital cytology images from fine needle aspiration biopsies. Our method embeds every automatically segmented cell of a candidate cytology image as a point in a 2D manifold defined by many representative slides, which enables reasoning about the cell-level composition of the tissue sample, paving the way for interpretable subtyping of the biopsy. Finally, a rule-based slide-level classification algorithm is trained on the partitions of the circularly distorted 2D manifold. This process results in a simple rule set that is evaluated automatically but highly transparent for human verification. On our in house cytology dataset of 88 uveal melanoma patients, the proposed method achieves an accuracy of 87.5% that compares favorably to all competing approaches, including deep "black box" models. The method comes with a user interface to facilitate interaction with cell-level content, which may offer additional insights for pathological assessment.



### Modal-Adaptive Gated Recoding Network for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.06281v2
- **DOI**: 10.1109/LSP.2021.3125268
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06281v2)
- **Published**: 2021-08-13 15:08:21+00:00
- **Updated**: 2021-11-09 14:07:20+00:00
- **Authors**: Jinchao Zhu, Xiaoyu Zhang, Xian Fang, Feng Dong, Qiu Yu
- **Comment**: None
- **Journal**: None
- **Summary**: The multi-modal salient object detection model based on RGB-D information has better robustness in the real world. However, it remains nontrivial to better adaptively balance effective multi-modal information in the feature fusion phase. In this letter, we propose a novel gated recoding network (GRNet) to evaluate the information validity of the two modes, and balance their influence. Our framework is divided into three phases: perception phase, recoding mixing phase and feature integration phase. First, A perception encoder is adopted to extract multi-level single-modal features, which lays the foundation for multi-modal semantic comparative analysis. Then, a modal-adaptive gate unit (MGU) is proposed to suppress the invalid information and transfer the effective modal features to the recoding mixer and the hybrid branch decoder. The recoding mixer is responsible for recoding and mixing the balanced multi-modal information. Finally, the hybrid branch decoder completes the multi-level feature integration under the guidance of an optional edge guidance stream (OEGS). Experiments and analysis on eight popular benchmarks verify that our framework performs favorably against 9 state-of-art methods.



### 3D point cloud segmentation using GIS
- **Arxiv ID**: http://arxiv.org/abs/2108.06306v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06306v1)
- **Published**: 2021-08-13 16:26:51+00:00
- **Updated**: 2021-08-13 16:26:51+00:00
- **Authors**: Chao-Jung Liu, Vladimir Krylov, Rozenn Dahyot
- **Comment**: 8 pages
- **Journal**: IMVIP 2018
- **Summary**: In this paper we propose an approach to perform semantic segmentation of 3D point cloud data by importing the geographic information from a 2D GIS layer (OpenStreetMap). The proposed automatic procedure identifies meaningful units such as buildings and adjusts their locations to achieve best fit between the GIS polygonal perimeters and the point cloud. Our processing pipeline is presented and illustrated by segmenting point cloud data of Trinity College Dublin (Ireland) campus constructed from optical imagery collected by a drone.



### Unsupervised Learning for Target Tracking and Background Subtraction in Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2109.00885v1
- **DOI**: 10.1117/12.2580620
- **Categories**: **cs.CV**, cs.LG, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2109.00885v1)
- **Published**: 2021-08-13 16:36:23+00:00
- **Updated**: 2021-08-13 16:36:23+00:00
- **Authors**: Jonathan S. Kent, Charles C. Wamsley, Davin Flateau, Amber Ferguson
- **Comment**: 10 pages
- **Journal**: Conference: SPIE 2021, Artificial Intelligence and Machine
  Learning for Multi-Domain Operations Applications III. Volume: 11746. Pages:
  117460H. Year: 2021. Organization: International Society for Optics and
  Photonics
- **Summary**: This paper describes an unsupervised machine learning methodology capable of target tracking and background suppression via a novel dual-model approach. ``Jekyll`` produces a video bit-mask describing an estimate of the locations of moving objects, and ``Hyde`` outputs a pseudo-background frame to subtract from the original input image sequence. These models were trained with a custom-modified version of Cross Entropy Loss.   Simulated data were used to compare the performance of Jekyll and Hyde against a more traditional supervised Machine Learning approach. The results from these comparisons show that the unsupervised methods developed are competitive in output quality with supervised techniques, without the associated cost of acquiring labeled training data.



### Towards Efficient Point Cloud Graph Neural Networks Through Architectural Simplification
- **Arxiv ID**: http://arxiv.org/abs/2108.06317v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.06317v1)
- **Published**: 2021-08-13 17:04:54+00:00
- **Updated**: 2021-08-13 17:04:54+00:00
- **Authors**: Shyam A. Tailor, René de Jong, Tiago Azevedo, Matthew Mattina, Partha Maji
- **Comment**: 8 pages. Accepted to the Deep Learning for Geometric Computing
  Workshop at ICCV 2021
- **Journal**: None
- **Summary**: In recent years graph neural network (GNN)-based approaches have become a popular strategy for processing point cloud data, regularly achieving state-of-the-art performance on a variety of tasks. To date, the research community has primarily focused on improving model expressiveness, with secondary thought given to how to design models that can run efficiently on resource constrained mobile devices including smartphones or mixed reality headsets. In this work we make a step towards improving the efficiency of these models by making the observation that these GNN models are heavily limited by the representational power of their first, feature extracting, layer. We find that it is possible to radically simplify these models so long as the feature extraction layer is retained with minimal degradation to model performance; further, we discover that it is possible to improve performance overall on ModelNet40 and S3DIS by improving the design of the feature extractor. Our approach reduces memory consumption by 20$\times$ and latency by up to 9.9$\times$ for graph layers in models such as DGCNN; overall, we achieve speed-ups of up to 4.5$\times$ and peak memory reductions of 72.5%.



### Dual Path Learning for Domain Adaptation of Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.06337v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06337v1)
- **Published**: 2021-08-13 17:59:55+00:00
- **Updated**: 2021-08-13 17:59:55+00:00
- **Authors**: Yiting Cheng, Fangyun Wei, Jianmin Bao, Dong Chen, Fang Wen, Wenqiang Zhang
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Domain adaptation for semantic segmentation enables to alleviate the need for large-scale pixel-wise annotations. Recently, self-supervised learning (SSL) with a combination of image-to-image translation shows great effectiveness in adaptive segmentation. The most common practice is to perform SSL along with image translation to well align a single domain (the source or target). However, in this single-domain paradigm, unavoidable visual inconsistency raised by image translation may affect subsequent learning. In this paper, based on the observation that domain adaptation frameworks performed in the source and target domain are almost complementary in terms of image translation and SSL, we propose a novel dual path learning (DPL) framework to alleviate visual inconsistency. Concretely, DPL contains two complementary and interactive single-domain adaptation pipelines aligned in source and target domain respectively. The inference of DPL is extremely simple, only one segmentation model in the target domain is employed. Novel technologies such as dual path image translation and dual path adaptive segmentation are proposed to make two paths promote each other in an interactive manner. Experiments on GTA5$\rightarrow$Cityscapes and SYNTHIA$\rightarrow$Cityscapes scenarios demonstrate the superiority of our DPL model over the state-of-the-art methods. The code and models are available at: \url{https://github.com/royee182/DPL}



### DensePASS: Dense Panoramic Semantic Segmentation via Unsupervised Domain Adaptation with Attention-Augmented Context Exchange
- **Arxiv ID**: http://arxiv.org/abs/2108.06383v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.06383v1)
- **Published**: 2021-08-13 20:15:46+00:00
- **Updated**: 2021-08-13 20:15:46+00:00
- **Authors**: Chaoxiang Ma, Jiaming Zhang, Kailun Yang, Alina Roitberg, Rainer Stiefelhagen
- **Comment**: Accepted to IEEE ITSC 2021. Dataset and code will be made publicly
  available at https://github.com/chma1024/DensePASS
- **Journal**: None
- **Summary**: Intelligent vehicles clearly benefit from the expanded Field of View (FoV) of the 360-degree sensors, but the vast majority of available semantic segmentation training images are captured with pinhole cameras. In this work, we look at this problem through the lens of domain adaptation and bring panoramic semantic segmentation to a setting, where labelled training data originates from a different distribution of conventional pinhole camera images. First, we formalize the task of unsupervised domain adaptation for panoramic semantic segmentation, where a network trained on labelled examples from the source domain of pinhole camera data is deployed in a different target domain of panoramic images, for which no labels are available. To validate this idea, we collect and publicly release DensePASS - a novel densely annotated dataset for panoramic segmentation under cross-domain conditions, specifically built to study the Pinhole-to-Panoramic transfer and accompanied with pinhole camera training examples obtained from Cityscapes. DensePASS covers both, labelled- and unlabelled 360-degree images, with the labelled data comprising 19 classes which explicitly fit the categories available in the source domain (i.e. pinhole) data. To meet the challenge of domain shift, we leverage the current progress of attention-based mechanisms and build a generic framework for cross-domain panoramic semantic segmentation based on different variants of attention-augmented domain adaptation modules. Our framework facilitates information exchange at local- and global levels when learning the domain correspondences and improves the domain adaptation performance of two standard segmentation networks by 6.05% and 11.26% in Mean IoU.



### Finding Representative Interpretations on Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.06384v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06384v3)
- **Published**: 2021-08-13 20:17:30+00:00
- **Updated**: 2021-08-20 20:02:02+00:00
- **Authors**: Peter Cho-Ho Lam, Lingyang Chu, Maxim Torgonskiy, Jian Pei, Yong Zhang, Lanjun Wang
- **Comment**: Accepted by ICCV 2021 (http://iccv2021.thecvf.com/home) | A python
  notebook of the proposed method can be found in
  https://marketplace.huaweicloud.com/markets/aihub/notebook/detail/?id=8e92ea6c-2f4a-4cff-a89f-bf2905bb7ac0
- **Journal**: None
- **Summary**: Interpreting the decision logic behind effective deep convolutional neural networks (CNN) on images complements the success of deep learning models. However, the existing methods can only interpret some specific decision logic on individual or a small number of images. To facilitate human understandability and generalization ability, it is important to develop representative interpretations that interpret common decision logics of a CNN on a large group of similar images, which reveal the common semantics data contributes to many closely related predictions. In this paper, we develop a novel unsupervised approach to produce a highly representative interpretation for a large number of similar images. We formulate the problem of finding representative interpretations as a co-clustering problem, and convert it into a submodular cost submodular cover problem based on a sample of the linear decision boundaries of a CNN. We also present a visualization and similarity ranking method. Our extensive experiments demonstrate the excellent performance of our method.



### Is Pseudo-Lidar needed for Monocular 3D Object detection?
- **Arxiv ID**: http://arxiv.org/abs/2108.06417v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06417v1)
- **Published**: 2021-08-13 22:22:51+00:00
- **Updated**: 2021-08-13 22:22:51+00:00
- **Authors**: Dennis Park, Rares Ambrus, Vitor Guizilini, Jie Li, Adrien Gaidon
- **Comment**: In Proceedings of the ICCV 2021
- **Journal**: None
- **Summary**: Recent progress in 3D object detection from single images leverages monocular depth estimation as a way to produce 3D pointclouds, turning cameras into pseudo-lidar sensors. These two-stage detectors improve with the accuracy of the intermediate depth estimation network, which can itself be improved without manual labels via large-scale self-supervised learning. However, they tend to suffer from overfitting more than end-to-end methods, are more complex, and the gap with similar lidar-based detectors remains significant. In this work, we propose an end-to-end, single stage, monocular 3D object detector, DD3D, that can benefit from depth pre-training like pseudo-lidar methods, but without their limitations. Our architecture is designed for effective information transfer between depth estimation and 3D detection, allowing us to scale with the amount of unlabeled pre-training data. Our method achieves state-of-the-art results on two challenging benchmarks, with 16.34% and 9.28% AP for Cars and Pedestrians (respectively) on the KITTI-3D benchmark, and 41.5% mAP on NuScenes.



### GeoCLR: Georeference Contrastive Learning for Efficient Seafloor Image Interpretation
- **Arxiv ID**: http://arxiv.org/abs/2108.06421v2
- **DOI**: 10.55417/fr.2022037
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06421v2)
- **Published**: 2021-08-13 22:42:34+00:00
- **Updated**: 2022-06-26 14:15:48+00:00
- **Authors**: Takaki Yamada, Adam Prügel-Bennett, Stefan B. Williams, Oscar Pizarro, Blair Thornton
- **Comment**: 30 pages, 9 figures
- **Journal**: Field Robotics 2 (2022) 1134-1155
- **Summary**: This paper describes Georeference Contrastive Learning of visual Representation (GeoCLR) for efficient training of deep-learning Convolutional Neural Networks (CNNs). The method leverages georeference information by generating a similar image pair using images taken of nearby locations, and contrasting these with an image pair that is far apart. The underlying assumption is that images gathered within a close distance are more likely to have similar visual appearance, where this can be reasonably satisfied in seafloor robotic imaging applications where image footprints are limited to edge lengths of a few metres and are taken so that they overlap along a vehicle's trajectory, whereas seafloor substrates and habitats have patch sizes that are far larger. A key advantage of this method is that it is self-supervised and does not require any human input for CNN training. The method is computationally efficient, where results can be generated between dives during multi-day AUV missions using computational resources that would be accessible during most oceanic field trials. We apply GeoCLR to habitat classification on a dataset that consists of ~86k images gathered using an Autonomous Underwater Vehicle (AUV). We demonstrate how the latent representations generated by GeoCLR can be used to efficiently guide human annotation efforts, where the semi-supervised framework improves classification accuracy by an average of 10.2% compared to the state-of-the-art SimCLR using the same CNN and equivalent number of human annotations for training.



### FrankMocap: A Monocular 3D Whole-Body Pose Estimation System via Regression and Integration
- **Arxiv ID**: http://arxiv.org/abs/2108.06428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.06428v1)
- **Published**: 2021-08-13 23:57:27+00:00
- **Updated**: 2021-08-13 23:57:27+00:00
- **Authors**: Yu Rong, Takaaki Shiratori, Hanbyul Joo
- **Comment**: Accepted to ICCV 2021 Workshops on Assistive Computer Vision and
  Robotics. An updated version of arXiv:2008.08324. Code, models and demo
  videos are available at:https://github.com/facebookresearch/frankmocap
- **Journal**: None
- **Summary**: Most existing monocular 3D pose estimation approaches only focus on a single body part, neglecting the fact that the essential nuance of human motion is conveyed through a concert of subtle movements of face, hands, and body. In this paper, we present FrankMocap, a fast and accurate whole-body 3D pose estimation system that can produce 3D face, hands, and body simultaneously from in-the-wild monocular images. The core idea of FrankMocap is its modular design: We first run 3D pose regression methods for face, hands, and body independently, followed by composing the regression outputs via an integration module. The separate regression modules allow us to take full advantage of their state-of-the-art performances without compromising the original accuracy and reliability in practice. We develop three different integration modules that trade off between latency and accuracy. All of them are capable of providing simple yet effective solutions to unify the separate outputs into seamless whole-body pose estimation results. We quantitatively and qualitatively demonstrate that our modularized system outperforms both the optimization-based and end-to-end methods of estimating whole-body pose.



