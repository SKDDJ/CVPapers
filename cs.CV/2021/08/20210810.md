# Arxiv Papers in cs.CV on 2021-08-10
### Rethinking Architecture Selection in Differentiable NAS
- **Arxiv ID**: http://arxiv.org/abs/2108.04392v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04392v1)
- **Published**: 2021-08-10 00:53:39+00:00
- **Updated**: 2021-08-10 00:53:39+00:00
- **Authors**: Ruochen Wang, Minhao Cheng, Xiangning Chen, Xiaocheng Tang, Cho-Jui Hsieh
- **Comment**: Outstanding Paper Award at ICLR 2021. The code is available at
  https://github.com/ruocwang/darts-pt
- **Journal**: None
- **Summary**: Differentiable Neural Architecture Search is one of the most popular Neural Architecture Search (NAS) methods for its search efficiency and simplicity, accomplished by jointly optimizing the model weight and architecture parameters in a weight-sharing supernet via gradient-based algorithms. At the end of the search phase, the operations with the largest architecture parameters will be selected to form the final architecture, with the implicit assumption that the values of architecture parameters reflect the operation strength. While much has been discussed about the supernet's optimization, the architecture selection process has received little attention. We provide empirical and theoretical analysis to show that the magnitude of architecture parameters does not necessarily indicate how much the operation contributes to the supernet's performance. We propose an alternative perturbation-based architecture selection that directly measures each operation's influence on the supernet. We re-evaluate several differentiable NAS methods with the proposed architecture selection and find that it is able to extract significantly improved architectures from the underlying supernets consistently. Furthermore, we find that several failure modes of DARTS can be greatly alleviated with the proposed selection method, indicating that much of the poor generalization observed in DARTS can be attributed to the failure of magnitude-based architecture selection rather than entirely the optimization of its supernet.



### Stroke Correspondence by Labeling Closed Areas
- **Arxiv ID**: http://arxiv.org/abs/2108.04393v1
- **DOI**: 10.1109/NICOINT52941.2021.00014
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04393v1)
- **Published**: 2021-08-10 00:56:19+00:00
- **Updated**: 2021-08-10 00:56:19+00:00
- **Authors**: Ryoma Miyauchi, Tsukasa Fukusato, Haoran Xie, Kazunori Miyata
- **Comment**: Proceedings of NICOGRAPH International 2021. 9 pages, 11 figures
- **Journal**: None
- **Summary**: Constructing stroke correspondences between keyframes is one of the most important processes in the production pipeline of hand-drawn inbetweening frames. This process requires time-consuming manual work imposing a tremendous burden on the animators. We propose a method to estimate stroke correspondences between raster character images (keyframes) without vectorization processes. First, the proposed system separates the closed areas in each keyframe and estimates the correspondences between closed areas by using the characteristics of shape, depth, and closed area connection. Second, the proposed system estimates stroke correspondences from the estimated closed area correspondences. We demonstrate the effectiveness of our method by performing a user study and comparing the proposed system with conventional approaches.



### On Procedural Adversarial Noise Attack And Defense
- **Arxiv ID**: http://arxiv.org/abs/2108.04409v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04409v2)
- **Published**: 2021-08-10 02:47:01+00:00
- **Updated**: 2021-08-27 03:35:54+00:00
- **Authors**: Jun Yan, Xiaoyang Deng, Huilin Yin, Wancheng Ge
- **Comment**: Remove theoretical analysis and focus on the empirical study
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are vulnerable to adversarial examples which would inveigle neural networks to make prediction errors with small perturbations on the input images. Researchers have been devoted to promoting the research on the universal adversarial perturbations (UAPs) which are gradient-free and have little prior knowledge on data distributions. Procedural adversarial noise attack is a data-free universal perturbation generation method. In this paper, we propose two universal adversarial perturbation (UAP) generation methods based on procedural noise functions: Simplex noise and Worley noise. In our framework, the shading which disturbs visual classification is generated with rendering technology. Without changing the semantic representations, the adversarial examples generated via our methods show superior performance on the attack.



### Semi-supervised classification of radiology images with NoTeacher: A Teacher that is not Mean
- **Arxiv ID**: http://arxiv.org/abs/2108.04423v1
- **DOI**: 10.1016/j.media.2021.102148
- **Categories**: **cs.CV**, cs.AI, cs.LG, 41A05, 41A10, 65D05, 65D17
- **Links**: [PDF](http://arxiv.org/pdf/2108.04423v1)
- **Published**: 2021-08-10 03:08:35+00:00
- **Updated**: 2021-08-10 03:08:35+00:00
- **Authors**: Balagopal Unnikrishnan, Cuong Nguyen, Shafa Balaram, Chao Li, Chuan Sheng Foo, Pavitra Krishnaswamy
- **Comment**: Preprint submitted to Medical Image Analysis. Accepted in June 2021
- **Journal**: None
- **Summary**: Deep learning models achieve strong performance for radiology image classification, but their practical application is bottlenecked by the need for large labeled training datasets. Semi-supervised learning (SSL) approaches leverage small labeled datasets alongside larger unlabeled datasets and offer potential for reducing labeling cost. In this work, we introduce NoTeacher, a novel consistency-based SSL framework which incorporates probabilistic graphical models. Unlike Mean Teacher which maintains a teacher network updated via a temporal ensemble, NoTeacher employs two independent networks, thereby eliminating the need for a teacher network. We demonstrate how NoTeacher can be customized to handle a range of challenges in radiology image classification. Specifically, we describe adaptations for scenarios with 2D and 3D inputs, uni and multi-label classification, and class distribution mismatch between labeled and unlabeled portions of the training data. In realistic empirical evaluations on three public benchmark datasets spanning the workhorse modalities of radiology (X-Ray, CT, MRI), we show that NoTeacher achieves over 90-95% of the fully supervised AUROC with less than 5-15% labeling budget. Further, NoTeacher outperforms established SSL methods with minimal hyperparameter tuning, and has implications as a principled and practical option for semisupervised learning in radiology applications.



### FT-TDR: Frequency-guided Transformer and Top-Down Refinement Network for Blind Face Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2108.04424v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04424v2)
- **Published**: 2021-08-10 03:12:01+00:00
- **Updated**: 2022-01-29 15:45:01+00:00
- **Authors**: Junke Wang, Shaoxiang Chen, Zuxuan Wu, Yu-Gang Jiang
- **Comment**: None
- **Journal**: IEEE Transactions on Multimedia, 2022
- **Summary**: Blind face inpainting refers to the task of reconstructing visual contents without explicitly indicating the corrupted regions in a face image. Inherently, this task faces two challenges: (1) how to detect various mask patterns of different shapes and contents; (2) how to restore visually plausible and pleasing contents in the masked regions. In this paper, we propose a novel two-stage blind face inpainting method named Frequency-guided Transformer and Top-Down Refinement Network (FT-TDR) to tackle these challenges. Specifically, we first use a transformer-based network to detect the corrupted regions to be inpainted as masks by modeling the relation among different patches. We also exploit the frequency modality as complementary information for improved detection results and capture the local contextual incoherence to enhance boundary consistency. Then a top-down refinement network is proposed to hierarchically restore features at different levels and generate contents that are semantically consistent with the unmasked face regions. Extensive experiments demonstrate that our method outperforms current state-of-the-art blind and non-blind face inpainting methods qualitatively and quantitatively.



### Domain-Aware Universal Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2108.04441v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04441v3)
- **Published**: 2021-08-10 04:22:16+00:00
- **Updated**: 2021-08-17 02:00:56+00:00
- **Authors**: Kibeom Hong, Seogkyu Jeon, Huan Yang, Jianlong Fu, Hyeran Byun
- **Comment**: Accepted by ICCV 2021. Code is available at
  https://github.com/Kibeom-Hong/Domain-Aware-Style-Transfer
- **Journal**: None
- **Summary**: Style transfer aims to reproduce content images with the styles from reference images. Existing universal style transfer methods successfully deliver arbitrary styles to original images either in an artistic or a photo-realistic way. However, the range of 'arbitrary style' defined by existing works is bounded in the particular domain due to their structural limitation. Specifically, the degrees of content preservation and stylization are established according to a predefined target domain. As a result, both photo-realistic and artistic models have difficulty in performing the desired style transfer for the other domain. To overcome this limitation, we propose a unified architecture, Domain-aware Style Transfer Networks (DSTN) that transfer not only the style but also the property of domain (i.e., domainness) from a given reference image. To this end, we design a novel domainness indicator that captures the domainness value from the texture and structural features of reference images. Moreover, we introduce a unified framework with domain-aware skip connection to adaptively transfer the stroke and palette to the input contents guided by the domainness indicator. Our extensive experiments validate that our model produces better qualitative results and outperforms previous methods in terms of proxy metrics on both artistic and photo-realistic stylizations.



### SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer
- **Arxiv ID**: http://arxiv.org/abs/2108.04444v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04444v2)
- **Published**: 2021-08-10 04:33:37+00:00
- **Updated**: 2021-10-27 06:35:30+00:00
- **Authors**: Peng Xiang, Xin Wen, Yu-Shen Liu, Yan-Pei Cao, Pengfei Wan, Wen Zheng, Zhizhong Han
- **Comment**: ICCV 2021 (Oral)
- **Journal**: None
- **Summary**: Point cloud completion aims to predict a complete shape in high accuracy from its partial observation. However, previous methods usually suffered from discrete nature of point cloud and unstructured prediction of points in local regions, which makes it hard to reveal fine local geometric details on the complete shape. To resolve this issue, we propose SnowflakeNet with Snowflake Point Deconvolution (SPD) to generate the complete point clouds. The SnowflakeNet models the generation of complete point clouds as the snowflake-like growth of points in 3D space, where the child points are progressively generated by splitting their parent points after each SPD. Our insight of revealing detailed geometry is to introduce skip-transformer in SPD to learn point splitting patterns which can fit local regions the best. Skip-transformer leverages attention mechanism to summarize the splitting patterns used in the previous SPD layer to produce the splitting in the current SPD layer. The locally compact and structured point cloud generated by SPD is able to precisely capture the structure characteristic of 3D shape in local patches, which enables the network to predict highly detailed geometries, such as smooth regions, sharp edges and corners. Our experimental results outperform the state-of-the-art point cloud completion methods under widely used benchmarks. Code will be available at https://github.com/AllenXiangX/SnowflakeNet.



### Method Towards CVPR 2021 Image Matching Challenge
- **Arxiv ID**: http://arxiv.org/abs/2108.04453v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.04453v2)
- **Published**: 2021-08-10 05:25:59+00:00
- **Updated**: 2021-08-11 02:36:26+00:00
- **Authors**: Xiaopeng Bi, Yu Chen, Xinyang Liu, Dehao Zhang, Ran Yan, Zheng Chai, Haotian Zhang, Xiao Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This report describes Megvii-3D team's approach towards CVPR 2021 Image Matching Workshop.



### CPNet: Cross-Parallel Network for Efficient Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.04454v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04454v4)
- **Published**: 2021-08-10 05:29:37+00:00
- **Updated**: 2021-10-27 07:55:34+00:00
- **Authors**: Youngsaeng Jin, Jonghwan Hong, David Han, Hanseok Ko
- **Comment**: 8 pages, 4 figures, The 17th IEEE International Conference on
  Advanced Video and Signal-based Surveillance (AVSS 2021). (Accept)
- **Journal**: None
- **Summary**: Anomaly detection in video streams is a challenging problem because of the scarcity of abnormal events and the difficulty of accurately annotating them. To alleviate these issues, unsupervised learning-based prediction methods have been previously applied. These approaches train the model with only normal events and predict a future frame from a sequence of preceding frames by use of encoder-decoder architectures so that they result in small prediction errors on normal events but large errors on abnormal events. The architecture, however, comes with the computational burden as some anomaly detection tasks require low computational cost without sacrificing performance. In this paper, Cross-Parallel Network (CPNet) for efficient anomaly detection is proposed here to minimize computations without performance drops. It consists of N smaller parallel U-Net, each of which is designed to handle a single input frame, to make the calculations significantly more efficient. Additionally, an inter-network shift module is incorporated to capture temporal relationships among sequential frames to enable more accurate future predictions.The quantitative results show that our model requires less computational cost than the baseline U-Net while delivering equivalent performance in anomaly detection.



### Reference-based Defect Detection Network
- **Arxiv ID**: http://arxiv.org/abs/2108.04456v1
- **DOI**: 10.1109/TIP.2021.3096067
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04456v1)
- **Published**: 2021-08-10 05:44:23+00:00
- **Updated**: 2021-08-10 05:44:23+00:00
- **Authors**: Zhaoyang Zeng, Bei Liu, Jianlong Fu, Hongyang Chao
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, vol. 30, pp. 6637-6647,
  2021
- **Summary**: The defect detection task can be regarded as a realistic scenario of object detection in the computer vision field and it is widely used in the industrial field. Directly applying vanilla object detector to defect detection task can achieve promising results, while there still exists challenging issues that have not been solved. The first issue is the texture shift which means a trained defect detector model will be easily affected by unseen texture, and the second issue is partial visual confusion which indicates that a partial defect box is visually similar with a complete box. To tackle these two problems, we propose a Reference-based Defect Detection Network (RDDN). Specifically, we introduce template reference and context reference to against those two problems, respectively. Template reference can reduce the texture shift from image, feature or region levels, and encourage the detectors to focus more on the defective area as a result. We can use either well-aligned template images or the outputs of a pseudo template generator as template references in this work, and they are jointly trained with detectors by the supervision of normal samples. To solve the partial visual confusion issue, we propose to leverage the carried context information of context reference, which is the concentric bigger box of each region proposal, to perform more accurate region classification and regression. Experiments on two defect detection datasets demonstrate the effectiveness of our proposed approach.



### Method Towards CVPR 2021 SimLocMatch Challenge
- **Arxiv ID**: http://arxiv.org/abs/2108.04466v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04466v2)
- **Published**: 2021-08-10 06:21:42+00:00
- **Updated**: 2021-08-11 02:35:11+00:00
- **Authors**: Xiaopeng Bi, Ran Yan, Zheng Chai, Haotian Zhang, Xiao Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This report describes Megvii-3D team's approach towards SimLocMatch Challenge @ CVPR 2021 Image Matching Workshop.



### SP-GAN: Sphere-Guided 3D Shape Generation and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2108.04476v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04476v1)
- **Published**: 2021-08-10 06:49:45+00:00
- **Updated**: 2021-08-10 06:49:45+00:00
- **Authors**: Ruihui Li, Xianzhi Li, Ka-Hei Hui, Chi-Wing Fu
- **Comment**: SIGGRAPH 2021, website https://liruihui.github.io/publication/SP-GAN/
- **Journal**: ACM Trans. Graph., Vol. 40, No. 4, Article 151. Publication date:
  August 2021
- **Summary**: We present SP-GAN, a new unsupervised sphere-guided generative model for direct synthesis of 3D shapes in the form of point clouds. Compared with existing models, SP-GAN is able to synthesize diverse and high-quality shapes with fine details and promote controllability for part-aware shape generation and manipulation, yet trainable without any parts annotations. In SP-GAN, we incorporate a global prior (uniform points on a sphere) to spatially guide the generative process and attach a local prior (a random latent code) to each sphere point to provide local details. The key insight in our design is to disentangle the complex 3D shape generation task into a global shape modeling and a local structure adjustment, to ease the learning process and enhance the shape generation quality. Also, our model forms an implicit dense correspondence between the sphere points and points in every generated shape, enabling various forms of structure-aware shape manipulations such as part editing, part-wise shape interpolation, and multi-shape part composition, etc., beyond the existing generative models. Experimental results, which include both visual and quantitative evaluations, demonstrate that our model is able to synthesize diverse point clouds with fine details and less noise, as compared with the state-of-the-art models.



### Scalable Reverse Image Search Engine for NASAWorldview
- **Arxiv ID**: http://arxiv.org/abs/2108.04479v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.04479v1)
- **Published**: 2021-08-10 07:03:00+00:00
- **Updated**: 2021-08-10 07:03:00+00:00
- **Authors**: Abhigya Sodani, Michael Levy, Anirudh Koul, Meher Anand Kasam, Siddha Ganju
- **Comment**: 7 pages, Published at COSPAR 2021, 6 figures
- **Journal**: None
- **Summary**: Researchers often spend weeks sifting through decades of unlabeled satellite imagery(on NASA Worldview) in order to develop datasets on which they can start conducting research. We developed an interactive, scalable and fast image similarity search engine (which can take one or more images as the query image) that automatically sifts through the unlabeled dataset reducing dataset generation time from weeks to minutes. In this work, we describe key components of the end to end pipeline. Our similarity search system was created to be able to identify similar images from a potentially petabyte scale database that are similar to an input image, and for this we had to break down each query image into its features, which were generated by a classification layer stripped CNN trained in a supervised manner. To store and search these features efficiently, we had to make several scalability improvements. To improve the speed, reduce the storage, and shrink memory requirements for embedding search, we add a fully connected layer to our CNN make all images into a 128 length vector before entering the classification layers. This helped us compress the size of our image features from 2048 (for ResNet, which was initially tried as our featurizer) to 128 for our new custom model. Additionally, we utilize existing approximate nearest neighbor search libraries to significantly speed up embedding search. Our system currently searches over our entire database of images at 5 seconds per query on a single virtual machine in the cloud. In the future, we would like to incorporate a SimCLR based featurizing model which could be trained without any labelling by a human (since the classification aspect of the model is irrelevant to this use case).



### Self-supervised Consensus Representation Learning for Attributed Graph
- **Arxiv ID**: http://arxiv.org/abs/2108.04822v1
- **DOI**: 10.1145/3474085.3475416
- **Categories**: **cs.SI**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04822v1)
- **Published**: 2021-08-10 07:53:09+00:00
- **Updated**: 2021-08-10 07:53:09+00:00
- **Authors**: Changshu Liu, Liangjian Wen, Zhao Kang, Guangchun Luo, Ling Tian
- **Comment**: Accepted by ACM Multimedia 2021
- **Journal**: None
- **Summary**: Attempting to fully exploit the rich information of topological structure and node features for attributed graph, we introduce self-supervised learning mechanism to graph representation learning and propose a novel Self-supervised Consensus Representation Learning (SCRL) framework. In contrast to most existing works that only explore one graph, our proposed SCRL method treats graph from two perspectives: topology graph and feature graph. We argue that their embeddings should share some common information, which could serve as a supervisory signal. Specifically, we construct the feature graph of node features via k-nearest neighbor algorithm. Then graph convolutional network (GCN) encoders extract features from two graphs respectively. Self-supervised loss is designed to maximize the agreement of the embeddings of the same node in the topology graph and the feature graph. Extensive experiments on real citation networks and social networks demonstrate the superiority of our proposed SCRL over the state-of-the-art methods on semi-supervised node classification task. Meanwhile, compared with its main competitors, SCRL is rather efficient.



### Exploiting Features with Split-and-Share Module
- **Arxiv ID**: http://arxiv.org/abs/2108.04500v2
- **DOI**: 10.3390/electronics11020235
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04500v2)
- **Published**: 2021-08-10 08:11:26+00:00
- **Updated**: 2021-08-11 00:34:07+00:00
- **Authors**: Jaemin Lee, Minseok Seo, Jongchan Park, Dong-Geol Choi
- **Comment**: None
- **Journal**: Electronics 2022
- **Summary**: Deep convolutional neural networks (CNNs) have shown state-of-the-art performances in various computer vision tasks. Advances on CNN architectures have focused mainly on designing convolutional blocks of the feature extractors, but less on the classifiers that exploit extracted features. In this work, we propose Split-and-Share Module (SSM),a classifier that splits a given feature into parts, which are partially shared by multiple sub-classifiers. Our intuition is that the more the features are shared, the more common they will become, and SSM can encourage such structural characteristics in the split features. SSM can be easily integrated into any architecture without bells and whistles. We have extensively validated the efficacy of SSM on ImageNet-1K classification task, andSSM has shown consistent and significant improvements over baseline architectures. In addition, we analyze the effect of SSM using the Grad-CAM visualization.



### TBNet:Two-Stream Boundary-aware Network for Generic Image Manipulation Localization
- **Arxiv ID**: http://arxiv.org/abs/2108.04508v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04508v1)
- **Published**: 2021-08-10 08:22:05+00:00
- **Updated**: 2021-08-10 08:22:05+00:00
- **Authors**: Zan Gao, Chao Sun, Zhiyong Cheng, Weili Guan, Anan Liu, Meng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Finding tampered regions in images is a hot research topic in machine learning and computer vision. Although many image manipulation location algorithms have been proposed, most of them only focus on the RGB images with different color spaces, and the frequency information that contains the potential tampering clues is often ignored. In this work, a novel end-to-end two-stream boundary-aware network (abbreviated as TBNet) is proposed for generic image manipulation localization in which the RGB stream, the frequency stream, and the boundary artifact location are explored in a unified framework. Specifically, we first design an adaptive frequency selection module (AFS) to adaptively select the appropriate frequency to mine inconsistent statistics and eliminate the interference of redundant statistics. Then, an adaptive cross-attention fusion module (ACF) is proposed to adaptively fuse the RGB feature and the frequency feature. Finally, the boundary artifact location network (BAL) is designed to locate the boundary artifacts for which the parameters are jointly updated by the outputs of the ACF, and its results are further fed into the decoder. Thus, the parameters of the RGB stream, the frequency stream, and the boundary artifact location network are jointly optimized, and their latent complementary relationships are fully mined. The results of extensive experiments performed on four public benchmarks of the image manipulation localization task, namely, CASIA1.0, COVER, Carvalho, and In-The-Wild, demonstrate that the proposed TBNet can significantly outperform state-of-the-art generic image manipulation localization methods in terms of both MCC and F1.



### MotionInput v2.0 supporting DirectX: A modular library of open-source gesture-based machine learning and computer vision methods for interacting and controlling existing software with a webcam
- **Arxiv ID**: http://arxiv.org/abs/2108.04357v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.HC, cs.LG, D.2; I.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2108.04357v1)
- **Published**: 2021-08-10 08:23:21+00:00
- **Updated**: 2021-08-10 08:23:21+00:00
- **Authors**: Ashild Kummen, Guanlin Li, Ali Hassan, Teodora Ganeva, Qianying Lu, Robert Shaw, Chenuka Ratwatte, Yang Zou, Lu Han, Emil Almazov, Sheena Visram, Andrew Taylor, Neil J Sebire, Lee Stott, Yvonne Rogers, Graham Roberts, Dean Mohamedally
- **Comment**: 6 pages, 2 figures
- **Journal**: None
- **Summary**: Touchless computer interaction has become an important consideration during the COVID-19 pandemic period. Despite progress in machine learning and computer vision that allows for advanced gesture recognition, an integrated collection of such open-source methods and a user-customisable approach to utilising them in a low-cost solution for touchless interaction in existing software is still missing. In this paper, we introduce the MotionInput v2.0 application. This application utilises published open-source libraries and additional gesture definitions developed to take the video stream from a standard RGB webcam as input. It then maps human motion gestures to input operations for existing applications and games. The user can choose their own preferred way of interacting from a series of motion types, including single and bi-modal hand gesturing, full-body repetitive or extremities-based exercises, head and facial movements, eye tracking, and combinations of the above. We also introduce a series of bespoke gesture recognition classifications as DirectInput triggers, including gestures for idle states, auto calibration, depth capture from a 2D RGB webcam stream and tracking of facial motions such as mouth motions, winking, and head direction with rotation. Three use case areas assisted the development of the modules: creativity software, office and clinical software, and gaming software. A collection of open-source libraries has been integrated and provide a layer of modular gesture mapping on top of existing mouse and keyboard controls in Windows via DirectX. With ease of access to webcams integrated into most laptops and desktop computers, touchless computing becomes more available with MotionInput v2.0, in a federated and locally processed method.



### Multi-domain Collaborative Feature Representation for Robust Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2108.04521v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04521v2)
- **Published**: 2021-08-10 09:01:42+00:00
- **Updated**: 2021-10-08 07:35:59+00:00
- **Authors**: Jiqing Zhang, Kai Zhao, Bo Dong, Yingkai Fu, Yuxin Wang, Xin Yang, Baocai Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Jointly exploiting multiple different yet complementary domain information has been proven to be an effective way to perform robust object tracking. This paper focuses on effectively representing and utilizing complementary features from the frame domain and event domain for boosting object tracking performance in challenge scenarios. Specifically, we propose Common Features Extractor (CFE) to learn potential common representations from the RGB domain and event domain. For learning the unique features of the two domains, we utilize a Unique Extractor for Event (UEE) based on Spiking Neural Networks to extract edge cues in the event domain which may be missed in RGB in some challenging conditions, and a Unique Extractor for RGB (UER) based on Deep Convolutional Neural Networks to extract texture and semantic information in RGB domain. Extensive experiments on standard RGB benchmark and real event tracking dataset demonstrate the effectiveness of the proposed approach. We show our approach outperforms all compared state-of-the-art tracking algorithms and verify event-based data is a powerful cue for tracking in challenging scenes.



### Multigranular Visual-Semantic Embedding for Cloth-Changing Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2108.04527v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04527v1)
- **Published**: 2021-08-10 09:14:44+00:00
- **Updated**: 2021-08-10 09:14:44+00:00
- **Authors**: Zan Gao, Hongwei Wei, Weili Guan, Weizhi Nie, Meng Liu, Meng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Person reidentification (ReID) is a very hot research topic in machine learning and computer vision, and many person ReID approaches have been proposed; however, most of these methods assume that the same person has the same clothes within a short time interval, and thus their visual appearance must be similar. However, in an actual surveillance environment, a given person has a great probability of changing clothes after a long time span, and they also often take different personal belongings with them. When the existing person ReID methods are applied in this type of case, almost all of them fail. To date, only a few works have focused on the cloth-changing person ReID task, but since it is very difficult to extract generalized and robust features for representing people with different clothes, their performances need to be improved. Moreover, visual-semantic information is often ignored. To solve these issues, in this work, a novel multigranular visual-semantic embedding algorithm (MVSE) is proposed for cloth-changing person ReID, where visual semantic information and human attributes are embedded into the network, and the generalized features of human appearance can be well learned to effectively solve the problem of clothing changes. Specifically, to fully represent a person with clothing changes, a multigranular feature representation scheme (MGR) is employed to focus on the unchanged part of the human, and then a cloth desensitization network (CDN) is designed to improve the feature robustness of the approach for the person with different clothing, where different high-level human attributes are fully utilized. Moreover, to further solve the issue of pose changes and occlusion under different camera perspectives, a partially semantically aligned network (PSA) is proposed to obtain the visual-semantic information that is used to align the human attributes.



### Hand Pose Classification Based on Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.04529v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04529v1)
- **Published**: 2021-08-10 09:14:51+00:00
- **Updated**: 2021-08-10 09:14:51+00:00
- **Authors**: Rashmi Bakshi
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, deep learning models are applied to a segment of a robust hand-washing dataset that has been created with the help of 30 volunteers. This work demonstrates the classification of presence of one hand, two hands and no hand in the scene based on transfer learning. The pre-trained model; simplest NN from Keras library is utilized to train the network with 704 images of hand gestures and the predictions are carried out for the input image. Due to the controlled and restricted dataset, 100% accuracy is achieved during the training with correct predictions for the input image. Complete handwashing dataset with dense models such as AlexNet for video classification for hand hygiene stages will be used in the future work.



### ASMR: Learning Attribute-Based Person Search with Adaptive Semantic Margin Regularizer
- **Arxiv ID**: http://arxiv.org/abs/2108.04533v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04533v1)
- **Published**: 2021-08-10 09:19:06+00:00
- **Updated**: 2021-08-10 09:19:06+00:00
- **Authors**: Boseung Jeong, Jicheol Park, Suha Kwak
- **Comment**: ICCV 2021 accepted
- **Journal**: None
- **Summary**: Attribute-based person search is the task of finding person images that are best matched with a set of text attributes given as query. The main challenge of this task is the large modality gap between attributes and images. To reduce the gap, we present a new loss for learning cross-modal embeddings in the context of attribute-based person search. We regard a set of attributes as a category of people sharing the same traits. In a joint embedding space of the two modalities, our loss pulls images close to their person categories for modality alignment. More importantly, it pushes apart a pair of person categories by a margin determined adaptively by their semantic distance, where the distance metric is learned end-to-end so that the loss considers importance of each attribute when relating person categories. Our loss guided by the adaptive semantic margin leads to more discriminative and semantically well-arranged distributions of person images. As a consequence, it enables a simple embedding model to achieve state-of-the-art records on public benchmarks without bells and whistles.



### Learning Multi-Granular Spatio-Temporal Graph Network for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.04536v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04536v1)
- **Published**: 2021-08-10 09:25:07+00:00
- **Updated**: 2021-08-10 09:25:07+00:00
- **Authors**: Tailin Chen, Desen Zhou, Jian Wang, Shidong Wang, Yu Guan, Xuming He, Errui Ding
- **Comment**: Accepted by ACM MM'21
- **Journal**: None
- **Summary**: The task of skeleton-based action recognition remains a core challenge in human-centred scene understanding due to the multiple granularities and large variation in human motion. Existing approaches typically employ a single neural representation for different motion patterns, which has difficulty in capturing fine-grained action classes given limited training data. To address the aforementioned problems, we propose a novel multi-granular spatio-temporal graph network for skeleton-based action classification that jointly models the coarse- and fine-grained skeleton motion patterns. To this end, we develop a dual-head graph network consisting of two interleaved branches, which enables us to extract features at two spatio-temporal resolutions in an effective and efficient manner. Moreover, our network utilises a cross-head communication strategy to mutually enhance the representations of both heads. We conducted extensive experiments on three large-scale datasets, namely NTU RGB+D 60, NTU RGB+D 120, and Kinetics-Skeleton, and achieves the state-of-the-art performance on all the benchmarks, which validates the effectiveness of our method.



### TrUMAn: Trope Understanding in Movies and Animations
- **Arxiv ID**: http://arxiv.org/abs/2108.04542v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04542v3)
- **Published**: 2021-08-10 09:34:14+00:00
- **Updated**: 2021-08-21 05:02:42+00:00
- **Authors**: Hung-Ting Su, Po-Wei Shen, Bing-Chen Tsai, Wen-Feng Cheng, Ke-Jyun Wang, Winston H. Hsu
- **Comment**: CIKM 2021. The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: Understanding and comprehending video content is crucial for many real-world applications such as search and recommendation systems. While recent progress of deep learning has boosted performance on various tasks using visual cues, deep cognition to reason intentions, motivation, or causality remains challenging. Existing datasets that aim to examine video reasoning capability focus on visual signals such as actions, objects, relations, or could be answered utilizing text bias. Observing this, we propose a novel task, along with a new dataset: Trope Understanding in Movies and Animations (TrUMAn), with 2423 videos associated with 132 tropes, intending to evaluate and develop learning systems beyond visual signals. Tropes are frequently used storytelling devices for creative works. By coping with the trope understanding task and enabling the deep cognition skills of machines, data mining applications and algorithms could be taken to the next level. To tackle the challenging TrUMAn dataset, we present a Trope Understanding and Storytelling (TrUSt) with a new Conceptual Storyteller module, which guides the video encoder by performing video storytelling on a latent space. Experimental results demonstrate that state-of-the-art learning systems on existing tasks reach only 12.01% of accuracy with raw input signals. Also, even in the oracle case with human-annotated descriptions, BERT contextual embedding achieves at most 28% of accuracy. Our proposed TrUSt boosts the model performance and reaches 13.94% performance. We also provide detailed analysis to pave the way for future research. TrUMAn is publicly available at:https://www.cmlab.csie.ntu.edu.tw/project/trope



### Known Operator Learning and Hybrid Machine Learning in Medical Imaging -- A Review of the Past, the Present, and the Future
- **Arxiv ID**: http://arxiv.org/abs/2108.04543v1
- **DOI**: 10.1088/2516-1091/ac5b13
- **Categories**: **cs.LG**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2108.04543v1)
- **Published**: 2021-08-10 09:36:10+00:00
- **Updated**: 2021-08-10 09:36:10+00:00
- **Authors**: Andreas Maier, Harald Köstler, Marco Heisig, Patrick Krauss, Seung Hee Yang
- **Comment**: 22 pages, 4 figures, submitted to "Progress in Biomedical
  Engineering"
- **Journal**: Prog. Biomed. Eng. 4 022002 (2022)
- **Summary**: In this article, we perform a review of the state-of-the-art of hybrid machine learning in medical imaging. We start with a short summary of the general developments of the past in machine learning and how general and specialized approaches have been in competition in the past decades. A particular focus will be the theoretical and experimental evidence pro and contra hybrid modelling. Next, we inspect several new developments regarding hybrid machine learning with a particular focus on so-called known operator learning and how hybrid approaches gain more and more momentum across essentially all applications in medical imaging and medical image analysis. As we will point out by numerous examples, hybrid models are taking over in image reconstruction and analysis. Even domains such as physical simulation and scanner and acquisition design are being addressed using machine learning grey box modelling approaches. Towards the end of the article, we will investigate a few future directions and point out relevant areas in which hybrid modelling, meta learning, and other domains will likely be able to drive the state-of-the-art ahead.



### Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2108.04547v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04547v2)
- **Published**: 2021-08-10 09:44:59+00:00
- **Updated**: 2021-08-11 07:11:06+00:00
- **Authors**: Weilun Wang, Wengang Zhou, Jianmin Bao, Dong Chen, Houqiang Li
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Contrastive learning shows great potential in unpaired image-to-image translation, but sometimes the translated results are in poor quality and the contents are not preserved consistently. In this paper, we uncover that the negative examples play a critical role in the performance of contrastive learning for image translation. The negative examples in previous methods are randomly sampled from the patches of different positions in the source image, which are not effective to push the positive examples close to the query examples. To address this issue, we present instance-wise hard Negative Example Generation for Contrastive learning in Unpaired image-to-image Translation (NEGCUT). Specifically, we train a generator to produce negative examples online. The generator is novel from two perspectives: 1) it is instance-wise which means that the generated examples are based on the input image, and 2) it can generate hard negative examples since it is trained with an adversarial loss. With the generator, the performance of unpaired image-to-image translation is significantly improved. Experiments on three benchmark datasets demonstrate that the proposed NEGCUT framework achieves state-of-the-art performance compared to previous methods.



### Understanding Character Recognition using Visual Explanations Derived from the Human Visual System and Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.04558v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04558v2)
- **Published**: 2021-08-10 10:09:37+00:00
- **Updated**: 2021-08-29 16:51:48+00:00
- **Authors**: Chetan Ralekar, Shubham Choudhary, Tapan Kumar Gandhi, Santanu Chaudhury
- **Comment**: None
- **Journal**: None
- **Summary**: Human observers engage in selective information uptake when classifying visual patterns. The same is true of deep neural networks, which currently constitute the best performing artificial vision systems. Our goal is to examine the congruence, or lack thereof, in the information-gathering strategies of the two systems. We have operationalized our investigation as a character recognition task. We have used eye-tracking to assay the spatial distribution of information hotspots for humans via fixation maps and an activation mapping technique for obtaining analogous distributions for deep networks through visualization maps. Qualitative comparison between visualization maps and fixation maps reveals an interesting correlate of congruence. The deep learning model considered similar regions in character, which humans have fixated in the case of correctly classified characters. On the other hand, when the focused regions are different for humans and deep nets, the characters are typically misclassified by the latter. Hence, we propose to use the visual fixation maps obtained from the eye-tracking experiment as a supervisory input to align the model's focus on relevant character regions. We find that such supervision improves the model's performance significantly and does not require any additional parameters. This approach has the potential to find applications in diverse domains such as medical analysis and surveillance in which explainability helps to determine system fidelity.



### Deep Metric Learning for Open World Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.04562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04562v1)
- **Published**: 2021-08-10 10:15:57+00:00
- **Updated**: 2021-08-10 10:15:57+00:00
- **Authors**: Jun Cen, Peng Yun, Junhao Cai, Michael Yu Wang, Ming Liu
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: Classical close-set semantic segmentation networks have limited ability to detect out-of-distribution (OOD) objects, which is important for safety-critical applications such as autonomous driving. Incrementally learning these OOD objects with few annotations is an ideal way to enlarge the knowledge base of the deep learning models. In this paper, we propose an open world semantic segmentation system that includes two modules: (1) an open-set semantic segmentation module to detect both in-distribution and OOD objects. (2) an incremental few-shot learning module to gradually incorporate those OOD objects into its existing knowledge base. This open world semantic segmentation system behaves like a human being, which is able to identify OOD objects and gradually learn them with corresponding supervision. We adopt the Deep Metric Learning Network (DMLNet) with contrastive clustering to implement open-set semantic segmentation. Compared to other open-set semantic segmentation methods, our DMLNet achieves state-of-the-art performance on three challenging open-set semantic segmentation datasets without using additional data or generative models. On this basis, two incremental few-shot learning methods are further proposed to progressively improve the DMLNet with the annotations of OOD objects.



### UniNet: A Unified Scene Understanding Network and Exploring Multi-Task Relationships through the Lens of Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2108.04584v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04584v2)
- **Published**: 2021-08-10 11:00:56+00:00
- **Updated**: 2022-08-12 15:16:40+00:00
- **Authors**: Naresh Kumar Gurulingan, Elahe Arani, Bahram Zonooz
- **Comment**: Accepted at DeepMTL workshop, ICCV 2021
- **Journal**: None
- **Summary**: Scene understanding is crucial for autonomous systems which intend to operate in the real world. Single task vision networks extract information only based on some aspects of the scene. In multi-task learning (MTL), on the other hand, these single tasks are jointly learned, thereby providing an opportunity for tasks to share information and obtain a more comprehensive understanding. To this end, we develop UniNet, a unified scene understanding network that accurately and efficiently infers vital vision tasks including object detection, semantic segmentation, instance segmentation, monocular depth estimation, and monocular instance depth prediction. As these tasks look at different semantic and geometric information, they can either complement or conflict with each other. Therefore, understanding inter-task relationships can provide useful cues to enable complementary information sharing. We evaluate the task relationships in UniNet through the lens of adversarial attacks based on the notion that they can exploit learned biases and task interactions in the neural network. Extensive experiments on the Cityscapes dataset, using untargeted and targeted attacks reveal that semantic tasks strongly interact amongst themselves, and the same holds for geometric tasks. Additionally, we show that the relationship between semantic and geometric tasks is asymmetric and their interaction becomes weaker as we move towards higher-level representations.



### Joint Multi-Object Detection and Tracking with Camera-LiDAR Fusion for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2108.04602v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04602v1)
- **Published**: 2021-08-10 11:17:05+00:00
- **Updated**: 2021-08-10 11:17:05+00:00
- **Authors**: Kemiao Huang, Qi Hao
- **Comment**: accepted by IROS 2021
- **Journal**: None
- **Summary**: Multi-object tracking (MOT) with camera-LiDAR fusion demands accurate results of object detection, affinity computation and data association in real time. This paper presents an efficient multi-modal MOT framework with online joint detection and tracking schemes and robust data association for autonomous driving applications. The novelty of this work includes: (1) development of an end-to-end deep neural network for joint object detection and correlation using 2D and 3D measurements; (2) development of a robust affinity computation module to compute occlusion-aware appearance and motion affinities in 3D space; (3) development of a comprehensive data association module for joint optimization among detection confidences, affinities and start-end probabilities. The experiment results on the KITTI tracking benchmark demonstrate the superior performance of the proposed method in terms of both tracking accuracy and processing speed.



### Relation-aware Compositional Zero-shot Learning for Attribute-Object Pair Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.04603v1
- **DOI**: 10.1109/TMM.2021.3104411
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.04603v1)
- **Published**: 2021-08-10 11:23:03+00:00
- **Updated**: 2021-08-10 11:23:03+00:00
- **Authors**: Ziwei Xu, Guangzhi Wang, Yongkang Wong, Mohan Kankanhalli
- **Comment**: Accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: This paper proposes a novel model for recognizing images with composite attribute-object concepts, notably for composite concepts that are unseen during model training. We aim to explore the three key properties required by the task --- relation-aware, consistent, and decoupled --- to learn rich and robust features for primitive concepts that compose attribute-object pairs. To this end, we propose the Blocked Message Passing Network (BMP-Net). The model consists of two modules. The concept module generates semantically meaningful features for primitive concepts, whereas the visual module extracts visual features for attributes and objects from input images. A message passing mechanism is used in the concept module to capture the relations between primitive concepts. Furthermore, to prevent the model from being biased towards seen composite concepts and reduce the entanglement between attributes and objects, we propose a blocking mechanism that equalizes the information available to the model for both seen and unseen concepts. Extensive experiments and ablation studies on two benchmarks show the efficacy of the proposed model.



### White blood cell subtype detection and classification
- **Arxiv ID**: http://arxiv.org/abs/2108.04614v3
- **DOI**: 10.23919/EECSI53397.2021.9624268
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04614v3)
- **Published**: 2021-08-10 11:55:52+00:00
- **Updated**: 2021-10-21 12:20:12+00:00
- **Authors**: Nalla Praveen, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali Agarwal, M. Syafrullah, Krisna Adiyarta
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning has endless applications in the health care industry. White blood cell classification is one of the interesting and promising area of research. The classification of the white blood cells plays an important part in the medical diagnosis. In practise white blood cell classification is performed by the haematologist by taking a small smear of blood and careful examination under the microscope. The current procedures to identify the white blood cell subtype is more time taking and error-prone. The computer aided detection and diagnosis of the white blood cells tend to avoid the human error and reduce the time taken to classify the white blood cells. In the recent years several deep learning approaches have been developed in the context of classification of the white blood cells that are able to identify but are unable to localize the positions of white blood cells in the blood cell image. Following this, the present research proposes to utilize YOLOv3 object detection technique to localize and classify the white blood cells with bounding boxes. With exhaustive experimental analysis, the proposed work is found to detect the white blood cell with 99.2% accuracy and classify with 90% accuracy.



### Learning Canonical 3D Object Representation for Fine-Grained Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.04628v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04628v1)
- **Published**: 2021-08-10 12:19:34+00:00
- **Updated**: 2021-08-10 12:19:34+00:00
- **Authors**: Sunghun Joung, Seungryong Kim, Minsu Kim, Ig-Jae Kim, Kwanghoon Sohn
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: We propose a novel framework for fine-grained object recognition that learns to recover object variation in 3D space from a single image, trained on an image collection without using any ground-truth 3D annotation. We accomplish this by representing an object as a composition of 3D shape and its appearance, while eliminating the effect of camera viewpoint, in a canonical configuration. Unlike conventional methods modeling spatial variation in 2D images only, our method is capable of reconfiguring the appearance feature in a canonical 3D space, thus enabling the subsequent object classifier to be invariant under 3D geometric variation. Our representation also allows us to go beyond existing methods, by incorporating 3D shape variation as an additional cue for object recognition. To learn the model without ground-truth 3D annotation, we deploy a differentiable renderer in an analysis-by-synthesis framework. By incorporating 3D shape and appearance jointly in a deep representation, our method learns the discriminative representation of the object and achieves competitive performance on fine-grained image recognition and vehicle re-identification. We also demonstrate that the performance of 3D shape reconstruction is improved by learning fine-grained shape deformation in a boosting manner.



### FoodLogoDet-1500: A Dataset for Large-Scale Food Logo Detection via Multi-Scale Feature Decoupling Network
- **Arxiv ID**: http://arxiv.org/abs/2108.04644v1
- **DOI**: 10.1145/3474085.3475289
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04644v1)
- **Published**: 2021-08-10 12:47:04+00:00
- **Updated**: 2021-08-10 12:47:04+00:00
- **Authors**: Qiang Hou, Weiqing Min, Jing Wang, Sujuan Hou, Yuanjie Zheng, Shuqiang Jiang
- **Comment**: This paper has been accepted to ACM MM 2021. The FoodLogoDet-1500,
  see https://github.com/hq03/FoodLogoDet-1500-Dataset
- **Journal**: None
- **Summary**: Food logo detection plays an important role in the multimedia for its wide real-world applications, such as food recommendation of the self-service shop and infringement detection on e-commerce platforms. A large-scale food logo dataset is urgently needed for developing advanced food logo detection algorithms. However, there are no available food logo datasets with food brand information. To support efforts towards food logo detection, we introduce the dataset FoodLogoDet-1500, a new large-scale publicly available food logo dataset, which has 1,500 categories, about 100,000 images and about 150,000 manually annotated food logo objects. We describe the collection and annotation process of FoodLogoDet-1500, analyze its scale and diversity, and compare it with other logo datasets. To the best of our knowledge, FoodLogoDet-1500 is the first largest publicly available high-quality dataset for food logo detection. The challenge of food logo detection lies in the large-scale categories and similarities between food logo categories. For that, we propose a novel food logo detection method Multi-scale Feature Decoupling Network (MFDNet), which decouples classification and regression into two branches and focuses on the classification branch to solve the problem of distinguishing multiple food logo categories. Specifically, we introduce the feature offset module, which utilizes the deformation-learning for optimal classification offset and can effectively obtain the most representative features of classification in detection. In addition, we adopt a balanced feature pyramid in MFDNet, which pays attention to global information, balances the multi-scale feature maps, and enhances feature extraction capability. Comprehensive experiments on FoodLogoDet-1500 and other two benchmark logo datasets demonstrate the effectiveness of the proposed method. The FoodLogoDet-1500 can be found at this https URL.



### DVM-CAR: A large-scale automotive dataset for visual marketing research and applications
- **Arxiv ID**: http://arxiv.org/abs/2109.00881v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00881v3)
- **Published**: 2021-08-10 12:48:58+00:00
- **Updated**: 2023-01-09 15:36:23+00:00
- **Authors**: Jingmin Huang, Bowei Chen, Lan Luo, Shigang Yue, Iadh Ounis
- **Comment**: Proceedings of IEEE International Conference on Big Data, pp.
  4130-4137, 2022
- **Journal**: None
- **Summary**: There is a growing interest in product aesthetics analytics and design. However, the lack of available large-scale data that covers various variables and information is one of the biggest challenges faced by analysts and researchers. In this paper, we present our multidisciplinary initiative of developing a comprehensive automotive dataset from different online sources and formats. Specifically, the created dataset contains 1.4 million images from 899 car models and their corresponding model specifications and sales information over more than ten years in the UK market. Our work makes significant contributions to: (i) research and applications in the automotive industry; (ii) big data creation and sharing; (iii) database design; and (iv) data fusion. Apart from our motivation, technical details and data structure, we further present three simple examples to demonstrate how our data can be used in business research and applications.



### U-Net-and-a-half: Convolutional network for biomedical image segmentation using multiple expert-driven annotations
- **Arxiv ID**: http://arxiv.org/abs/2108.04658v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/2108.04658v1)
- **Published**: 2021-08-10 13:08:39+00:00
- **Updated**: 2021-08-10 13:08:39+00:00
- **Authors**: Yichi Zhang, Jesper Kers, Clarissa A. Cassol, Joris J. Roelofs, Najia Idrees, Alik Farber, Samir Haroon, Kevin P. Daly, Suvranu Ganguli, Vipul C. Chitalia, Vijaya B. Kolachalama
- **Comment**: https://github.com/vkola-lab/unaah
- **Journal**: None
- **Summary**: Development of deep learning systems for biomedical segmentation often requires access to expert-driven, manually annotated datasets. If more than a single expert is involved in the annotation of the same images, then the inter-expert agreement is not necessarily perfect, and no single expert annotation can precisely capture the so-called ground truth of the regions of interest on all images. Also, it is not trivial to generate a reference estimate using annotations from multiple experts. Here we present a deep neural network, defined as U-Net-and-a-half, which can simultaneously learn from annotations performed by multiple experts on the same set of images. U-Net-and-a-half contains a convolutional encoder to generate features from the input images, multiple decoders that allow simultaneous learning from image masks obtained from annotations that were independently generated by multiple experts, and a shared low-dimensional feature space. To demonstrate the applicability of our framework, we used two distinct datasets from digital pathology and radiology, respectively. Specifically, we trained two separate models using pathologist-driven annotations of glomeruli on whole slide images of human kidney biopsies (10 patients), and radiologist-driven annotations of lumen cross-sections of human arteriovenous fistulae obtained from intravascular ultrasound images (10 patients), respectively. The models based on U-Net-and-a-half exceeded the performance of the traditional U-Net models trained on single expert annotations alone, thus expanding the scope of multitask learning in the context of biomedical image segmentation.



### Multi-Camera Trajectory Forecasting with Trajectory Tensors
- **Arxiv ID**: http://arxiv.org/abs/2108.04694v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04694v2)
- **Published**: 2021-08-10 13:53:05+00:00
- **Updated**: 2021-08-24 09:33:07+00:00
- **Authors**: Olly Styles, Tanaya Guha, Victor Sanchez
- **Comment**: To appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (tPAMI)
- **Journal**: None
- **Summary**: We introduce the problem of multi-camera trajectory forecasting (MCTF), which involves predicting the trajectory of a moving object across a network of cameras. While multi-camera setups are widespread for applications such as surveillance and traffic monitoring, existing trajectory forecasting methods typically focus on single-camera trajectory forecasting (SCTF), limiting their use for such applications. Furthermore, using a single camera limits the field-of-view available, making long-term trajectory forecasting impossible. We address these shortcomings of SCTF by developing an MCTF framework that simultaneously uses all estimated relative object locations from several viewpoints and predicts the object's future location in all possible viewpoints. Our framework follows a Which-When-Where approach that predicts in which camera(s) the objects appear and when and where within the camera views they appear. To this end, we propose the concept of trajectory tensors: a new technique to encode trajectories across multiple camera views and the associated uncertainties. We develop several encoder-decoder MCTF models for trajectory tensors and present extensive experiments on our own database (comprising 600 hours of video data from 15 camera views) created particularly for the MCTF task. Results show that our trajectory tensor models outperform coordinate trajectory-based MCTF models and existing SCTF methods adapted for MCTF. Code is available from: https://github.com/olly-styles/Trajectory-Tensors



### BIDCD -- Bosch Industrial Depth Completion Dataset
- **Arxiv ID**: http://arxiv.org/abs/2108.04706v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.04706v2)
- **Published**: 2021-08-10 14:06:49+00:00
- **Updated**: 2021-10-04 05:07:33+00:00
- **Authors**: Adam Botach, Yuri Feldman, Yakov Miron, Yoel Shapiro, Dotan Di Castro
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce BIDCD -- the Bosch Industrial Depth Completion Dataset. BIDCD is a new RGBD dataset of metallic industrial objects, collected with a depth camera mounted on a robotic manipulator. The main purpose of this dataset is to facilitate the training of domain-specific depth completion models, to be used in logistics and manufacturing tasks. We trained a State-of-the-Art depth completion model on this dataset, and report the results, setting an initial benchmark. Further, we propose to use this dataset for learning synthetic-to-depth-camera domain adaptation. Modifying synthetic RGBD data to mimic characteristics of real-world depth acquisition could potentially enhance training on synthetic data. For this end, we trained a Generative Adversarial Network (GAN) on a synthetic industrial dataset and our real-world data. Finally, to address geometric distortions in the generated images, we introduce an auxiliary loss that promotes preservation of the original shape. The BIDCD data is publicly available at https://zenodo.org/communities/bidcd.



### Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2108.04728v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04728v2)
- **Published**: 2021-08-10 14:45:05+00:00
- **Updated**: 2021-10-31 15:32:11+00:00
- **Authors**: Chaoda Zheng, Xu Yan, Jiantao Gao, Weibing Zhao, Wei Zhang, Zhen Li, Shuguang Cui
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Current 3D single object tracking approaches track the target based on a feature comparison between the target template and the search area. However, due to the common occlusion in LiDAR scans, it is non-trivial to conduct accurate feature comparisons on severe sparse and incomplete shapes. In this work, we exploit the ground truth bounding box given in the first frame as a strong cue to enhance the feature description of the target object, enabling a more accurate feature comparison in a simple yet effective way. In particular, we first propose the BoxCloud, an informative and robust representation, to depict an object using the point-to-box relation. We further design an efficient box-aware feature fusion module, which leverages the aforementioned BoxCloud for reliable feature matching and embedding. Integrating the proposed general components into an existing model P2B, we construct a superior box-aware tracker (BAT). Experiments confirm that our proposed BAT outperforms the previous state-of-the-art by a large margin on both KITTI and NuScenes benchmarks, achieving a 15.2% improvement in terms of precision while running ~20% faster.



### Semantics-STGCNN: A Semantics-guided Spatial-Temporal Graph Convolutional Network for Multi-class Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2108.04740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04740v1)
- **Published**: 2021-08-10 15:02:50+00:00
- **Updated**: 2021-08-10 15:02:50+00:00
- **Authors**: Ben A. Rainbow, Qianhui Men, Hubert P. H. Shum
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting the movement trajectories of multiple classes of road users in real-world scenarios is a challenging task due to the diverse trajectory patterns. While recent works of pedestrian trajectory prediction successfully modelled the influence of surrounding neighbours based on the relative distances, they are ineffective on multi-class trajectory prediction. This is because they ignore the impact of the implicit correlations between different types of road users on the trajectory to be predicted - for example, a nearby pedestrian has a different level of influence from a nearby car. In this paper, we propose to introduce class information into a graph convolutional neural network to better predict the trajectory of an individual. We embed the class labels of the surrounding objects into the label adjacency matrix (LAM), which is combined with the velocity-based adjacency matrix (VAM) comprised of the objects' velocity, thereby generating a semantics-guided graph adjacency (SAM). SAM effectively models semantic information with trainable parameters to automatically learn the embedded label features that will contribute to the fixed velocity-based trajectory. Such information of spatial and temporal dependencies is passed to a graph convolutional and temporal convolutional network to estimate the predicted trajectory distributions. We further propose new metrics, known as Average2 Displacement Error (aADE) and Average Final Displacement Error (aFDE), that assess network accuracy more accurately. We call our framework Semantics-STGCNN. It consistently shows superior performance to the state-of-the-arts in existing and the newly proposed metrics.



### SUNet: Symmetric Undistortion Network for Rolling Shutter Correction
- **Arxiv ID**: http://arxiv.org/abs/2108.04775v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.04775v1)
- **Published**: 2021-08-10 16:43:13+00:00
- **Updated**: 2021-08-10 16:43:13+00:00
- **Authors**: Bin Fan, Yuchao Dai, Mingyi He
- **Comment**: Accepted by IEEE International Conference on Computer Vision (ICCV)
  2021
- **Journal**: None
- **Summary**: The vast majority of modern consumer-grade cameras employ a rolling shutter mechanism, leading to image distortions if the camera moves during image acquisition. In this paper, we present a novel deep network to solve the generic rolling shutter correction problem with two consecutive frames. Our pipeline is symmetrically designed to predict the global shutter image corresponding to the intermediate time of these two frames, which is difficult for existing methods because it corresponds to a camera pose that differs most from the two frames. First, two time-symmetric dense undistortion flows are estimated by using well-established principles: pyramidal construction, warping, and cost volume processing. Then, both rolling shutter images are warped into a common global shutter one in the feature space, respectively. Finally, a symmetric consistency constraint is constructed in the image decoder to effectively aggregate the contextual cues of two rolling shutter images, thereby recovering the high-quality global shutter image. Extensive experiments with both synthetic and real data from public benchmarks demonstrate the superiority of our proposed approach over the state-of-the-art methods.



### Meta-repository of screening mammography classifiers
- **Arxiv ID**: http://arxiv.org/abs/2108.04800v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04800v3)
- **Published**: 2021-08-10 17:39:26+00:00
- **Updated**: 2022-01-18 06:22:39+00:00
- **Authors**: Benjamin Stadnick, Jan Witowski, Vishwaesh Rajiv, Jakub Chłędowski, Farah E. Shamout, Kyunghyun Cho, Krzysztof J. Geras
- **Comment**: 17 pages, 2 figures. Meta-repository available at
  https://www.github.com/nyukat/mammography_metarepository ; v3 adds results on
  the CSAW-CC dataset
- **Journal**: None
- **Summary**: Artificial intelligence (AI) is showing promise in improving clinical diagnosis. In breast cancer screening, recent studies show that AI has the potential to improve early cancer diagnosis and reduce unnecessary workup. As the number of proposed models and their complexity grows, it is becoming increasingly difficult to re-implement them. To enable reproducibility of research and to enable comparison between different methods, we release a meta-repository containing models for classification of screening mammograms. This meta-repository creates a framework that enables the evaluation of AI models on any screening mammography data set. At its inception, our meta-repository contains five state-of-the-art models with open-source implementations and cross-platform compatibility. We compare their performance on seven international data sets. Our framework has a flexible design that can be generalized to other medical image analysis tasks. The meta-repository is available at https://www.github.com/nyukat/mammography_metarepository.



### R4Dyn: Exploring Radar for Self-Supervised Monocular Depth Estimation of Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2108.04814v2
- **DOI**: 10.1109/3DV53792.2021.00084
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.04814v2)
- **Published**: 2021-08-10 17:57:03+00:00
- **Updated**: 2021-11-29 18:29:54+00:00
- **Authors**: Stefano Gasperini, Patrick Koch, Vinzenz Dallabetta, Nassir Navab, Benjamin Busam, Federico Tombari
- **Comment**: Accepted at the International Conference on 3D Vision (3DV) 2021
- **Journal**: None
- **Summary**: While self-supervised monocular depth estimation in driving scenarios has achieved comparable performance to supervised approaches, violations of the static world assumption can still lead to erroneous depth predictions of traffic participants, posing a potential safety issue. In this paper, we present R4Dyn, a novel set of techniques to use cost-efficient radar data on top of a self-supervised depth estimation framework. In particular, we show how radar can be used during training as weak supervision signal, as well as an extra input to enhance the estimation robustness at inference time. Since automotive radars are readily available, this allows to collect training data from a variety of existing vehicles. Moreover, by filtering and expanding the signal to make it compatible with learning-based approaches, we address radar inherent issues, such as noise and sparsity. With R4Dyn we are able to overcome a major limitation of self-supervised depth estimation, i.e. the prediction of traffic participants. We substantially improve the estimation on dynamic objects, such as cars by 37% on the challenging nuScenes dataset, hence demonstrating that radar is a valuable additional sensor for monocular depth estimation in autonomous vehicles.



### The Effect of the Loss on Generalization: Empirical Study on Synthetic Lung Nodule Data
- **Arxiv ID**: http://arxiv.org/abs/2108.04815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04815v1)
- **Published**: 2021-08-10 17:58:01+00:00
- **Updated**: 2021-08-10 17:58:01+00:00
- **Authors**: Vasileios Baltatzis, Loic Le Folgoc, Sam Ellis, Octavio E. Martinez Manzanera, Kyriaki-Margarita Bintsi, Arjun Nair, Sujal Desai, Ben Glocker, Julia A. Schnabel
- **Comment**: Accepted at iMIMIC, MICCAI 2021
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are widely used for image classification in a variety of fields, including medical imaging. While most studies deploy cross-entropy as the loss function in such tasks, a growing number of approaches have turned to a family of contrastive learning-based losses. Even though performance metrics such as accuracy, sensitivity and specificity are regularly used for the evaluation of CNN classifiers, the features that these classifiers actually learn are rarely identified and their effect on the classification performance on out-of-distribution test samples is insufficiently explored. In this paper, motivated by the real-world task of lung nodule classification, we investigate the features that a CNN learns when trained and tested on different distributions of a synthetic dataset with controlled modes of variation. We show that different loss functions lead to different features being learned and consequently affect the generalization ability of the classifier on unseen data. This study provides some important insights into the design of deep learning solutions for medical imaging tasks.



### AuraSense: Robot Collision Avoidance by Full Surface Proximity Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.04867v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2108.04867v1)
- **Published**: 2021-08-10 18:37:54+00:00
- **Updated**: 2021-08-10 18:37:54+00:00
- **Authors**: Xiaoran Fan, Riley Simmons-Edler, Daewon Lee, Larry Jackel, Richard Howard, Daniel Lee
- **Comment**: Accepted to IROS 2021
- **Journal**: None
- **Summary**: Perceiving obstacles and avoiding collisions is fundamental to the safe operation of a robot system, particularly when the robot must operate in highly dynamic human environments. Proximity detection using on-robot sensors can be used to avoid or mitigate impending collisions. However, existing proximity sensing methods are orientation and placement dependent, resulting in blind spots even with large numbers of sensors. In this paper, we introduce the phenomenon of the Leaky Surface Wave (LSW), a novel sensing modality, and present AuraSense, a proximity detection system using the LSW. AuraSense is the first system to realize no-dead-spot proximity sensing for robot arms. It requires only a single pair of piezoelectric transducers, and can easily be applied to off-the-shelf robots with minimal modifications. We further introduce a set of signal processing techniques and a lightweight neural network to address the unique challenges in using the LSW for proximity sensing. Finally, we demonstrate a prototype system consisting of a single piezoelectric element pair on a robot manipulator, which validates our design. We conducted several micro benchmark experiments and performed more than 2000 on-robot proximity detection trials with various potential robot arm materials, colliding objects, approach patterns, and robot movement patterns. AuraSense achieves 100% and 95.3% true positive proximity detection rates when the arm approaches static and mobile obstacles respectively, with a true negative rate over 99%, showing the real-world viability of this system.



### MetaPose: Fast 3D Pose from Multiple Views without 3D Supervision
- **Arxiv ID**: http://arxiv.org/abs/2108.04869v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04869v2)
- **Published**: 2021-08-10 18:39:56+00:00
- **Updated**: 2021-11-25 23:16:01+00:00
- **Authors**: Ben Usman, Andrea Tagliasacchi, Kate Saenko, Avneesh Sud
- **Comment**: None
- **Journal**: None
- **Summary**: In the era of deep learning, human pose estimation from multiple cameras with unknown calibration has received little attention to date. We show how to train a neural model to perform this task with high precision and minimal latency overhead. The proposed model takes into account joint location uncertainty due to occlusion from multiple views, and requires only 2D keypoint data for training. Our method outperforms both classical bundle adjustment and weakly-supervised monocular 3D baselines on the well-established Human3.6M dataset, as well as the more challenging in-the-wild Ski-Pose PTZ dataset.



### Differentiable Surface Rendering via Non-Differentiable Sampling
- **Arxiv ID**: http://arxiv.org/abs/2108.04886v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04886v1)
- **Published**: 2021-08-10 19:25:06+00:00
- **Updated**: 2021-08-10 19:25:06+00:00
- **Authors**: Forrester Cole, Kyle Genova, Avneesh Sud, Daniel Vlasic, Zhoutong Zhang
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: We present a method for differentiable rendering of 3D surfaces that supports both explicit and implicit representations, provides derivatives at occlusion boundaries, and is fast and simple to implement. The method first samples the surface using non-differentiable rasterization, then applies differentiable, depth-aware point splatting to produce the final image. Our approach requires no differentiable meshing or rasterization steps, making it efficient for large 3D models and applicable to isosurfaces extracted from implicit surface definitions. We demonstrate the effectiveness of our method for implicit-, mesh-, and parametric-surface-based inverse rendering and neural-network training applications. In particular, we show for the first time efficient, differentiable rendering of an isosurface extracted from a neural radiance field (NeRF), and demonstrate surface-based, rather than volume-based, rendering of a NeRF.



### On the Effect of Pruning on Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2108.04890v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04890v2)
- **Published**: 2021-08-10 19:30:41+00:00
- **Updated**: 2021-11-24 20:43:05+00:00
- **Authors**: Artur Jordao, Helio Pedrini
- **Comment**: Published at International Conference on Computer Vision Workshop
  (ICCVW), 2021
- **Journal**: None
- **Summary**: Pruning is a well-known mechanism for reducing the computational cost of deep convolutional networks. However, studies have shown the potential of pruning as a form of regularization, which reduces overfitting and improves generalization. We demonstrate that this family of strategies provides additional benefits beyond computational performance and generalization. Our analyses reveal that pruning structures (filters and/or layers) from convolutional networks increase not only generalization but also robustness to adversarial images (natural images with content modified). Such achievements are possible since pruning reduces network capacity and provides regularization, which have been proven effective tools against adversarial images. In contrast to promising defense mechanisms that require training with adversarial images and careful regularization, we show that pruning obtains competitive results considering only natural images (e.g., the standard and low-cost training). We confirm these findings on several adversarial attacks and architectures; thus suggesting the potential of pruning as a novel defense mechanism against adversarial images.



### How Self-Supervised Learning Can be Used for Fine-Grained Head Pose Estimation?
- **Arxiv ID**: http://arxiv.org/abs/2108.04893v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04893v6)
- **Published**: 2021-08-10 19:34:45+00:00
- **Updated**: 2022-08-01 08:33:12+00:00
- **Authors**: Mahdi Pourmirzaei, Farzaneh Esmaili, Ebrahim Mousavi, Sasan Karamizadeh, Seyedehsamaneh Shojaeilangari
- **Comment**: None
- **Journal**: None
- **Summary**: The cost of head pose labeling is the main challenge of improving the fine-grained Head Pose Estimation (HPE). Although Self-Supervised Learning (SSL) can be a solution to the lack of huge amounts of labeled data, its efficacy for fine-grained HPE is not yet fully explored. This study aims to assess the usage of SSL in fine-grained HPE based on two scenarios: (1) using SSL for weights pre-training procedure, and (2) leveraging auxiliary SSL losses besides HPE. We design a Hybrid Multi-Task Learning (HMTL) architecture based on the ResNet50 backbone in which both strategies are applied. Our experimental results reveal that the combination of both scenarios is the best for HPE. Together, the average error rate is reduced up to 23.1% for AFLW2000 and 14.2% for BIWI benchmark compared to the baseline. Moreover, it is found that some SSL methods are more suitable for transfer learning, while others may be effective when they are considered as auxiliary tasks incorporated into supervised learning. Finally, it is shown that by using the proposed HMTL architecture, the average error is reduced with different types of initial weights: random, ImageNet and SSL pre-trained weights.



### Interpreting Generative Adversarial Networks for Interactive Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2108.04896v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04896v2)
- **Published**: 2021-08-10 19:42:20+00:00
- **Updated**: 2022-02-02 01:56:18+00:00
- **Authors**: Bolei Zhou
- **Comment**: An invited book chapter on explainable machine learning
- **Journal**: None
- **Summary**: Significant progress has been made by the advances in Generative Adversarial Networks (GANs) for image generation. However, there lacks enough understanding of how a realistic image is generated by the deep representations of GANs from a random vector. This chapter gives a summary of recent works on interpreting deep generative models. The methods are categorized into the supervised, the unsupervised, and the embedding-guided approaches. We will see how the human-understandable concepts that emerge in the learned representation can be identified and used for interactive image generation and editing.



### Depth Infused Binaural Audio Generation using Hierarchical Cross-Modal Attention
- **Arxiv ID**: http://arxiv.org/abs/2108.04906v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.04906v1)
- **Published**: 2021-08-10 20:26:44+00:00
- **Updated**: 2021-08-10 20:26:44+00:00
- **Authors**: Kranti Kumar Parida, Siddharth Srivastava, Neeraj Matiyali, Gaurav Sharma
- **Comment**: Presented at Sight and Sound Workshop, CVPR 2021
- **Journal**: None
- **Summary**: Binaural audio gives the listener the feeling of being in the recording place and enhances the immersive experience if coupled with AR/VR. But the problem with binaural audio recording is that it requires a specialized setup which is not possible to fabricate within handheld devices as compared to traditional mono audio that can be recorded with a single microphone. In order to overcome this drawback, prior works have tried to uplift the mono recorded audio to binaural audio as a post processing step conditioning on the visual input. But all the prior approaches missed other most important information required for the task, i.e. distance of different sound producing objects from the recording setup. In this work, we argue that the depth map of the scene can act as a proxy for encoding distance information of objects in the scene and show that adding depth features along with image features improves the performance both qualitatively and quantitatively. We propose a novel encoder-decoder architecture, where we use a hierarchical attention mechanism to encode the image and depth feature extracted from individual transformer backbone, with audio features at each layer of the decoder.



### FLAME-in-NeRF : Neural control of Radiance Fields for Free View Face Animation
- **Arxiv ID**: http://arxiv.org/abs/2108.04913v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04913v1)
- **Published**: 2021-08-10 20:41:15+00:00
- **Updated**: 2021-08-10 20:41:15+00:00
- **Authors**: ShahRukh Athar, Zhixin Shu, Dimitris Samaras
- **Comment**: version 1.0.0
- **Journal**: None
- **Summary**: This paper presents a neural rendering method for controllable portrait video synthesis. Recent advances in volumetric neural rendering, such as neural radiance fields (NeRF), has enabled the photorealistic novel view synthesis of static scenes with impressive results. However, modeling dynamic and controllable objects as part of a scene with such scene representations is still challenging. In this work, we design a system that enables both novel view synthesis for portrait video, including the human subject and the scene background, and explicit control of the facial expressions through a low-dimensional expression representation. We leverage the expression space of a 3D morphable face model (3DMM) to represent the distribution of human facial expressions, and use it to condition the NeRF volumetric function. Furthermore, we impose a spatial prior brought by 3DMM fitting to guide the network to learn disentangled control for scene appearance and facial actions. We demonstrate the effectiveness of our method on free view synthesis of portrait videos with expression controls. To train a scene, our method only requires a short video of a subject captured by a mobile device.



### Optimal MRI Undersampling Patterns for Ultimate Benefit of Medical Vision Tasks
- **Arxiv ID**: http://arxiv.org/abs/2108.04914v1
- **DOI**: 10.1007/978-3-031-16446-0_73
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04914v1)
- **Published**: 2021-08-10 20:48:47+00:00
- **Updated**: 2021-08-10 20:48:47+00:00
- **Authors**: Artem Razumov, Oleg Y. Rogov, Dmitry V. Dylov
- **Comment**: None
- **Journal**: MICCAI 2022
- **Summary**: To accelerate MRI, the field of compressed sensing is traditionally concerned with optimizing the image quality after a partial undersampling of the measurable $\textit{k}$-space. In our work, we propose to change the focus from the quality of the reconstructed image to the quality of the downstream image analysis outcome. Specifically, we propose to optimize the patterns according to how well a sought-after pathology could be detected or localized in the reconstructed images. We find the optimal undersampling patterns in $\textit{k}$-space that maximize target value functions of interest in commonplace medical vision problems (reconstruction, segmentation, and classification) and propose a new iterative gradient sampling routine universally suitable for these tasks. We validate the proposed MRI acceleration paradigm on three classical medical datasets, demonstrating a noticeable improvement of the target metrics at the high acceleration factors (for the segmentation problem at $\times$16 acceleration, we report up to 12% improvement in Dice score over the other undersampling patterns).



### First Order Locally Orderless Registration
- **Arxiv ID**: http://arxiv.org/abs/2108.04926v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04926v1)
- **Published**: 2021-08-10 21:23:09+00:00
- **Updated**: 2021-08-10 21:23:09+00:00
- **Authors**: Sune Darkner, Jose D Tascon, Francois Lauze
- **Comment**: None
- **Journal**: None
- **Summary**: First Order Locally Orderless Registration (FLOR) is a scale-space framework for image density estimation used for defining image similarity, mainly for Image Registration. The Locally Orderless Registration framework was designed in principle to use zeroth-order information, providing image density estimates over three scales: image scale, intensity scale, and integration scale. We extend it to take first-order information into account and hint at higher-order information. We show how standard similarity measures extend into the framework. We study especially Sum of Squared Differences (SSD) and Normalized Cross-Correlation (NCC) but present the theory of how Normalised Mutual Information (NMI) can be included.



### Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion
- **Arxiv ID**: http://arxiv.org/abs/2108.04927v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04927v2)
- **Published**: 2021-08-10 21:24:05+00:00
- **Updated**: 2021-11-04 17:12:42+00:00
- **Authors**: Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, Gaurav Sukhatme
- **Comment**: Accepted at Novel Ideas in Learning-to-Learn through Interaction
  (NILLI) workshop @ EMNLP 2021
- **Journal**: None
- **Summary**: Language-guided robots performing home and office tasks must navigate in and interact with the world. Grounding language instructions against visual observations and actions to take in an environment is an open challenge. We present Embodied BERT (EmBERT), a transformer-based model which can attend to high-dimensional, multi-modal inputs across long temporal horizons for language-conditioned task completion. Additionally, we bridge the gap between successful object-centric navigation models used for non-interactive agents and the language-guided visual task completion benchmark, ALFRED, by introducing object navigation targets for EmBERT training. We achieve competitive performance on the ALFRED benchmark, and EmBERT marks the first transformer-based model to successfully handle the long-horizon, dense, multi-modal histories of ALFRED, and the first ALFRED model to utilize object-centric navigation targets.



### BERTHop: An Effective Vision-and-Language Model for Chest X-ray Disease Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2108.04938v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2108.04938v1)
- **Published**: 2021-08-10 21:51:25+00:00
- **Updated**: 2021-08-10 21:51:25+00:00
- **Authors**: Masoud Monajatipoor, Mozhdeh Rouhsedaghat, Liunian Harold Li, Aichi Chien, C. -C. Jay Kuo, Fabien Scalzo, Kai-Wei Chang
- **Comment**: 10 pages, 8 figures, Accepted in ICCV workshop
- **Journal**: None
- **Summary**: Vision-and-language(V&L) models take image and text as input and learn to capture the associations between them. Prior studies show that pre-trained V&L models can significantly improve the model performance for downstream tasks such as Visual Question Answering (VQA). However, V&L models are less effective when applied in the medical domain (e.g., on X-ray images and clinical notes) due to the domain gap. In this paper, we investigate the challenges of applying pre-trained V&L models in medical applications. In particular, we identify that the visual representation in general V&L models is not suitable for processing medical data. To overcome this limitation, we propose BERTHop, a transformer-based model based on PixelHop++ and VisualBERT, for better capturing the associations between the two modalities. Experiments on the OpenI dataset, a commonly used thoracic disease diagnosis benchmark, show that BERTHop achieves an average Area Under the Curve (AUC) of 98.12% which is 1.62% higher than state-of-the-art (SOTA) while it is trained on a 9 times smaller dataset.



### An Image-based Generator Architecture for Synthetic Image Refinement
- **Arxiv ID**: http://arxiv.org/abs/2108.04957v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04957v1)
- **Published**: 2021-08-10 22:58:07+00:00
- **Updated**: 2021-08-10 22:58:07+00:00
- **Authors**: Alex Nasser
- **Comment**: None
- **Journal**: None
- **Summary**: Proposed are alternative generator architectures for Boundary Equilibrium Generative Adversarial Networks, motivated by Learning from Simulated and Unsupervised Images through Adversarial Training. It disentangles the need for a noise-based latent space. The generator will operate mainly as a refiner network to gain a photo-realistic presentation of the given synthetic images. It also attempts to resolve the latent space's poorly understood properties by eliminating the need for noise injection and replacing it with an image-based concept. The new flexible and simple generator architecture will also give the power to control the trade-off between restrictive refinement and expressiveness ability. Contrary to other available methods, this architecture will not require a paired or unpaired dataset of real and synthetic images for the training phase. Only a relatively small set of real images would suffice.



### Tracked 3D Ultrasound and Deep Neural Network-based Thyroid Segmentation reduce Interobserver Variability in Thyroid Volumetry
- **Arxiv ID**: http://arxiv.org/abs/2108.10118v1
- **DOI**: 10.1371/journal.pone.0268550
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2108.10118v1)
- **Published**: 2021-08-10 23:28:27+00:00
- **Updated**: 2021-08-10 23:28:27+00:00
- **Authors**: Markus Krönke, Christine Eilers, Desislava Dimova, Melanie Köhler, Gabriel Buschner, Lilit Mirzojan, Lemonia Konstantinidou, Marcus R. Makowski, James Nagarajah, Nassir Navab, Wolfgang Weber, Thomas Wendler
- **Comment**: 7 figures, 19 pages, under review
- **Journal**: None
- **Summary**: Background: Thyroid volumetry is crucial in diagnosis, treatment and monitoring of thyroid diseases. However, conventional thyroid volumetry with 2D ultrasound is highly operator-dependent. This study compares 2D ultrasound and tracked 3D ultrasound with an automatic thyroid segmentation based on a deep neural network regarding inter- and intraobserver variability, time and accuracy. Volume reference was MRI. Methods: 28 healthy volunteers were scanned with 2D and 3D ultrasound as well as by MRI. Three physicians (MD 1, 2, 3) with different levels of experience (6, 4 and 1 a) performed three 2D ultrasound and three tracked 3D ultrasound scans on each volunteer. In the 2D scans the thyroid lobe volumes were calculated with the ellipsoid formula. A convolutional deep neural network (CNN) segmented the 3D thyroid lobes automatically. On MRI (T1 VIBE sequence) the thyroid was manually segmented by an experienced medical doctor. Results: The CNN was trained to obtain a dice score of 0.94. The interobserver variability comparing two MDs showed mean differences for 2D and 3D respectively of 0.58 ml to 0.52 ml (MD1 vs. 2), -1.33 ml to -0.17 ml (MD1 vs. 3) and -1.89 ml to -0.70 ml (MD2 vs. 3). Paired samples t-tests showed significant differences in two comparisons for 2D and none for 3D. Intraobsever variability was similar for 2D and 3D ultrasound. Comparison of ultrasound volumes and MRI volumes by paired samples t-tests showed a significant difference for the 2D volumetry of all MDs, and no significant difference for 3D ultrasound. Acquisition time was significantly shorter for 3D ultrasound. Conclusion: Tracked 3D ultrasound combined with a CNN segmentation significantly reduces interobserver variability in thyroid volumetry and increases the accuracy of the measurements with shorter acquisition times.



