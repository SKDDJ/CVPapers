# Arxiv Papers in cs.CV on 2021-08-31
### Dead Pixel Test Using Effective Receptive Field
- **Arxiv ID**: http://arxiv.org/abs/2108.13576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13576v1)
- **Published**: 2021-08-31 01:42:47+00:00
- **Updated**: 2021-08-31 01:42:47+00:00
- **Authors**: Bum Jun Kim, Hyeyeon Choi, Hyeonah Jang, Dong Gu Lee, Wonseok Jeong, Sang Woo Kim
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Deep neural networks have been used in various fields, but their internal behavior is not well known. In this study, we discuss two counterintuitive behaviors of convolutional neural networks (CNNs). First, we evaluated the size of the receptive field. Previous studies have attempted to increase or control the size of the receptive field. However, we observed that the size of the receptive field does not describe the classification accuracy. The size of the receptive field would be inappropriate for representing superiority in performance because it reflects only depth or kernel size and does not reflect other factors such as width or cardinality. Second, using the effective receptive field, we examined the pixels contributing to the output. Intuitively, each pixel is expected to equally contribute to the final output. However, we found that there exist pixels in a partially dead state with little contribution to the output. We reveal that the reason for this lies in the architecture of CNN and discuss solutions to reduce the phenomenon. Interestingly, for general classification tasks, the existence of dead pixels improves the training of CNNs. However, in a task that captures small perturbation, dead pixels degrade the performance. Therefore, the existence of these dead pixels should be understood and considered in practical applications of CNN.



### From Less to More: Spectral Splitting and Aggregation Network for Hyperspectral Face Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2108.13584v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13584v2)
- **Published**: 2021-08-31 02:13:00+00:00
- **Updated**: 2022-04-12 15:08:30+00:00
- **Authors**: Junjun Jiang, Chenyang Wang, Xianming Liu, Kui Jiang, Jiayi Ma
- **Comment**: 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW)
- **Journal**: None
- **Summary**: High-resolution (HR) hyperspectral face image plays an important role in face related computer vision tasks under uncontrolled conditions, such as low-light environment and spoofing attacks. However, the dense spectral bands of hyperspectral face images come at the cost of limited amount of photons reached a narrow spectral window on average, which greatly reduces the spatial resolution of hyperspectral face images. In this paper, we investigate how to adapt the deep learning techniques to hyperspectral face image super-resolution (HFSR), especially when the training samples are very limited. Benefiting from the amount of spectral bands, in which each band can be seen as an image, we present a spectral splitting and aggregation network (SSANet) for HFSR with limited training samples. In the shallow layers, we split the hyperspectral image into different spectral groups. Then, we gradually aggregate the neighbor bands at deeper layers to exploit spectral correlations. By this spectral splitting and aggregation strategy (SSAS), we can divide the original hyperspectral image into multiple samples (\emph{from less to more}) to support the efficient training of the network and effectively exploit the spectral correlations among spectrum. To cope with the challenge of small training sample size (S3) problem, we propose to expand the training samples by a self-representation model and symmetry-induced augmentation. Experiments show that SSANet can well model the joint correlations of spatial and spectral information. By expanding the training samples, SSANet can effectively alleviate the S3 problem.



### SMAC-Seg: LiDAR Panoptic Segmentation via Sparse Multi-directional Attention Clustering
- **Arxiv ID**: http://arxiv.org/abs/2108.13588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13588v1)
- **Published**: 2021-08-31 02:25:01+00:00
- **Updated**: 2021-08-31 02:25:01+00:00
- **Authors**: Enxu Li, Ryan Razani, Yixuan Xu, Liu Bingbing
- **Comment**: None
- **Journal**: None
- **Summary**: Panoptic segmentation aims to address semantic and instance segmentation simultaneously in a unified framework. However, an efficient solution of panoptic segmentation in applications like autonomous driving is still an open research problem. In this work, we propose a novel LiDAR-based panoptic system, called SMAC-Seg. We present a learnable sparse multi-directional attention clustering to segment multi-scale foreground instances. SMAC-Seg is a real-time clustering-based approach, which removes the complex proposal network to segment instances. Most existing clustering-based methods use the difference of the predicted and ground truth center offset as the only loss to supervise the instance centroid regression. However, this loss function only considers the centroid of the current object, but its relative position with respect to the neighbouring objects is not considered when learning to cluster. Thus, we propose to use a novel centroid-aware repel loss as an additional term to effectively supervise the network to differentiate each object cluster with its neighbours. Our experimental results show that SMAC-Seg achieves state-of-the-art performance among all real-time deployable networks on both large-scale public SemanticKITTI and nuScenes panoptic segmentation datasets.



### AIP: Adversarial Iterative Pruning Based on Knowledge Transfer for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.13591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13591v1)
- **Published**: 2021-08-31 02:38:36+00:00
- **Updated**: 2021-08-31 02:38:36+00:00
- **Authors**: Jingfei Chang, Yang Lu, Ping Xue, Yiqun Xu, Zhen Wei
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: With the increase of structure complexity, convolutional neural networks (CNNs) take a fair amount of computation cost. Meanwhile, existing research reveals the salient parameter redundancy in CNNs. The current pruning methods can compress CNNs with little performance drop, but when the pruning ratio increases, the accuracy loss is more serious. Moreover, some iterative pruning methods are difficult to accurately identify and delete unimportant parameters due to the accuracy drop during pruning. We propose a novel adversarial iterative pruning method (AIP) for CNNs based on knowledge transfer. The original network is regarded as the teacher while the compressed network is the student. We apply attention maps and output features to transfer information from the teacher to the student. Then, a shallow fully-connected network is designed as the discriminator to allow the output of two networks to play an adversarial game, thereby it can quickly recover the pruned accuracy among pruning intervals. Finally, an iterative pruning scheme based on the importance of channels is proposed. We conduct extensive experiments on the image classification tasks CIFAR-10, CIFAR-100, and ILSVRC-2012 to verify our pruning method can achieve efficient compression for CNNs even without accuracy loss. On the ILSVRC-2012, when removing 36.78% parameters and 45.55% floating-point operations (FLOPs) of ResNet-18, the Top-1 accuracy drop are only 0.66%. Our method is superior to some state-of-the-art pruning schemes in terms of compressing rate and accuracy. Moreover, we further demonstrate that AIP has good generalization on the object detection task PASCAL VOC.



### Self-balanced Learning For Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2108.13597v1
- **DOI**: 10.1109/ICIP42928.2021.9506516
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.13597v1)
- **Published**: 2021-08-31 03:17:54+00:00
- **Updated**: 2021-08-31 03:17:54+00:00
- **Authors**: Jin Kim, Jiyoung Lee, Jungin Park, Dongbo Min, Kwanghoon Sohn
- **Comment**: Accepted at International Conference on Image Processing (ICIP) 2021
- **Journal**: ICIP, 2021, pp. 779-783
- **Summary**: Domain generalization aims to learn a prediction model on multi-domain source data such that the model can generalize to a target domain with unknown statistics. Most existing approaches have been developed under the assumption that the source data is well-balanced in terms of both domain and class. However, real-world training data collected with different composition biases often exhibits severe distribution gaps for domain and class, leading to substantial performance degradation. In this paper, we propose a self-balanced domain generalization framework that adaptively learns the weights of losses to alleviate the bias caused by different distributions of the multi-domain source data. The self-balanced scheme is based on an auxiliary reweighting network that iteratively updates the weight of loss conditioned on the domain and class information by leveraging balanced meta data. Experimental results demonstrate the effectiveness of our method overwhelming state-of-the-art works for domain generalization.



### Iterative Filter Adaptive Network for Single Image Defocus Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2108.13610v2
- **DOI**: 10.1109/CVPR46437.2021.00207
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.13610v2)
- **Published**: 2021-08-31 04:27:07+00:00
- **Updated**: 2022-03-28 07:45:06+00:00
- **Authors**: Junyong Lee, Hyeongseok Son, Jaesung Rim, Sunghyun Cho, Seungyong Lee
- **Comment**: CVPR 2021
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR), 2021, pp. 2034-2042
- **Summary**: We propose a novel end-to-end learning-based approach for single image defocus deblurring. The proposed approach is equipped with a novel Iterative Filter Adaptive Network (IFAN) that is specifically designed to handle spatially-varying and large defocus blur. For adaptively handling spatially-varying blur, IFAN predicts pixel-wise deblurring filters, which are applied to defocused features of an input image to generate deblurred features. For effectively managing large blur, IFAN models deblurring filters as stacks of small-sized separable filters. Predicted separable deblurring filters are applied to defocused features using a novel Iterative Adaptive Convolution (IAC) layer. We also propose a training scheme based on defocus disparity estimation and reblurring, which significantly boosts the deblurring quality. We demonstrate that our method achieves state-of-the-art performance both quantitatively and qualitatively on real-world images.



### Segmentation Fault: A Cheap Defense Against Adversarial Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.13617v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.13617v1)
- **Published**: 2021-08-31 04:56:58+00:00
- **Updated**: 2021-08-31 04:56:58+00:00
- **Authors**: Doha Al Bared, Mohamed Nassar
- **Comment**: None
- **Journal**: None
- **Summary**: Recently published attacks against deep neural networks (DNNs) have stressed the importance of methodologies and tools to assess the security risks of using this technology in critical systems. Efficient techniques for detecting adversarial machine learning helps establishing trust and boost the adoption of deep learning in sensitive and security systems. In this paper, we propose a new technique for defending deep neural network classifiers, and convolutional ones in particular. Our defense is cheap in the sense that it requires less computation power despite a small cost to pay in terms of detection accuracy. The work refers to a recently published technique called ML-LOO. We replace the costly pixel by pixel leave-one-out approach of ML-LOO by adopting coarse-grained leave-one-out. We evaluate and compare the efficiency of different segmentation algorithms for this task. Our results show that a large gain in efficiency is possible, even though penalized by a marginal decrease in detection accuracy.



### Spike time displacement based error backpropagation in convolutional spiking neural networks
- **Arxiv ID**: http://arxiv.org/abs/2108.13621v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.13621v1)
- **Published**: 2021-08-31 05:18:59+00:00
- **Updated**: 2021-08-31 05:18:59+00:00
- **Authors**: Maryam Mirsadeghi, Majid Shalchian, Saeed Reza Kheradpisheh, Timothée Masquelier
- **Comment**: None
- **Journal**: None
- **Summary**: We recently proposed the STiDi-BP algorithm, which avoids backward recursive gradient computation, for training multi-layer spiking neural networks (SNNs) with single-spike-based temporal coding. The algorithm employs a linear approximation to compute the derivative of the spike latency with respect to the membrane potential and it uses spiking neurons with piecewise linear postsynaptic potential to reduce the computational cost and the complexity of neural processing. In this paper, we extend the STiDi-BP algorithm to employ it in deeper and convolutional architectures. The evaluation results on the image classification task based on two popular benchmarks, MNIST and Fashion-MNIST datasets with the accuracies of respectively 99.2% and 92.8%, confirm that this algorithm has been applicable in deep SNNs. Another issue we consider is the reduction of memory storage and computational cost. To do so, we consider a convolutional SNN (CSNN) with two sets of weights: real-valued weights that are updated in the backward pass and their signs, binary weights, that are employed in the feedforward process. We evaluate the binary CSNN on two datasets of MNIST and Fashion-MNIST and obtain acceptable performance with a negligible accuracy drop with respect to real-valued weights (about $0.6%$ and $0.8%$ drops, respectively).



### SimulLR: Simultaneous Lip Reading Transducer with Attention-Guided Adaptive Memory
- **Arxiv ID**: http://arxiv.org/abs/2108.13630v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2108.13630v1)
- **Published**: 2021-08-31 05:54:16+00:00
- **Updated**: 2021-08-31 05:54:16+00:00
- **Authors**: Zhijie Lin, Zhou Zhao, Haoyuan Li, Jinglin Liu, Meng Zhang, Xingshan Zeng, Xiaofei He
- **Comment**: ACMMM 2021
- **Journal**: None
- **Summary**: Lip reading, aiming to recognize spoken sentences according to the given video of lip movements without relying on the audio stream, has attracted great interest due to its application in many scenarios. Although prior works that explore lip reading have obtained salient achievements, they are all trained in a non-simultaneous manner where the predictions are generated requiring access to the full video. To breakthrough this constraint, we study the task of simultaneous lip reading and devise SimulLR, a simultaneous lip Reading transducer with attention-guided adaptive memory from three aspects: (1) To address the challenge of monotonic alignments while considering the syntactic structure of the generated sentences under simultaneous setting, we build a transducer-based model and design several effective training strategies including CTC pre-training, model warm-up and curriculum learning to promote the training of the lip reading transducer. (2) To learn better spatio-temporal representations for simultaneous encoder, we construct a truncated 3D convolution and time-restricted self-attention layer to perform the frame-to-frame interaction within a video segment containing fixed number of frames. (3) The history information is always limited due to the storage in real-time scenarios, especially for massive video data. Therefore, we devise a novel attention-guided adaptive memory to organize semantic information of history segments and enhance the visual representations with acceptable computation-aware latency. The experiments show that the SimulLR achieves the translation speedup 9.10$\times$ compared with the state-of-the-art non-simultaneous methods, and also obtains competitive results, which indicates the effectiveness of our proposed methods.



### Module-Power Prediction from PL Measurements using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.13640v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13640v1)
- **Published**: 2021-08-31 06:43:03+00:00
- **Updated**: 2021-08-31 06:43:03+00:00
- **Authors**: Mathis Hoffmann, Johannes Hepp, Bernd Doll, Claudia Buerhop-Lutz, Ian Marius Peters, Christoph Brabec, Andreas Maier, Vincent Christlein
- **Comment**: None
- **Journal**: None
- **Summary**: The individual causes for power loss of photovoltaic modules are investigated for quite some time. Recently, it has been shown that the power loss of a module is, for example, related to the fraction of inactive areas. While these areas can be easily identified from electroluminescense (EL) images, this is much harder for photoluminescence (PL) images. With this work, we close the gap between power regression from EL and PL images. We apply a deep convolutional neural network to predict the module power from PL images with a mean absolute error (MAE) of 4.4% or 11.7WP. Furthermore, we depict that regression maps computed from the embeddings of the trained network can be used to compute the localized power loss. Finally, we show that these regression maps can be used to identify inactive regions in PL images as well.



### Is First Person Vision Challenging for Object Tracking?
- **Arxiv ID**: http://arxiv.org/abs/2108.13665v1
- **DOI**: 10.1109/ICCVW54120.2021.00304
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13665v1)
- **Published**: 2021-08-31 08:06:01+00:00
- **Updated**: 2021-08-31 08:06:01+00:00
- **Authors**: Matteo Dunnhofer, Antonino Furnari, Giovanni Maria Farinella, Christian Micheloni
- **Comment**: IEEE/CVF International Conference on Computer Vision (ICCV) 2021,
  Visual Object Tracking Challenge VOT2021 workshop. arXiv admin note: text
  overlap with arXiv:2011.12263
- **Journal**: None
- **Summary**: Understanding human-object interactions is fundamental in First Person Vision (FPV). Tracking algorithms which follow the objects manipulated by the camera wearer can provide useful cues to effectively model such interactions. Visual tracking solutions available in the computer vision literature have significantly improved their performance in the last years for a large variety of target objects and tracking scenarios. However, despite a few previous attempts to exploit trackers in FPV applications, a methodical analysis of the performance of state-of-the-art trackers in this domain is still missing. In this paper, we fill the gap by presenting the first systematic study of object tracking in FPV. Our study extensively analyses the performance of recent visual trackers and baseline FPV trackers with respect to different aspects and considering a new performance measure. This is achieved through TREK-150, a novel benchmark dataset composed of 150 densely annotated video sequences. Our results show that object tracking in FPV is challenging, which suggests that more research efforts should be devoted to this problem so that tracking could benefit FPV tasks.



### Semi-supervised Image Classification with Grad-CAM Consistency
- **Arxiv ID**: http://arxiv.org/abs/2108.13673v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13673v1)
- **Published**: 2021-08-31 08:26:35+00:00
- **Updated**: 2021-08-31 08:26:35+00:00
- **Authors**: Juyong Lee, Seunghyuk Cho
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: Consistency training, which exploits both supervised and unsupervised learning with different augmentations on image, is an effective method of utilizing unlabeled data in semi-supervised learning (SSL) manner. Here, we present another version of the method with Grad-CAM consistency loss, so it can be utilized in training model with better generalization and adjustability. We show that our method improved the baseline ResNet model with at most 1.44 % and 0.31 $\pm$ 0.59 %p accuracy improvement on average with CIFAR-10 dataset. We conducted ablation study comparing to using only psuedo-label for consistency training. Also, we argue that our method can adjust in different environments when targeted to different units in the model. The code is available: https://github.com/gimme1dollar/gradcam-consistency-semi-sup.



### Attention-based Multi-Reference Learning for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2108.13697v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.13697v1)
- **Published**: 2021-08-31 09:12:26+00:00
- **Updated**: 2021-08-31 09:12:26+00:00
- **Authors**: Marco Pesavento, Marco Volino, Adrian Hilton
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel Attention-based Multi-Reference Super-resolution network (AMRSR) that, given a low-resolution image, learns to adaptively transfer the most similar texture from multiple reference images to the super-resolution output whilst maintaining spatial coherence. The use of multiple reference images together with attention-based sampling is demonstrated to achieve significantly improved performance over state-of-the-art reference super-resolution approaches on multiple benchmark datasets. Reference super-resolution approaches have recently been proposed to overcome the ill-posed problem of image super-resolution by providing additional information from a high-resolution reference image. Multi-reference super-resolution extends this approach by providing a more diverse pool of image features to overcome the inherent information deficit whilst maintaining memory efficiency. A novel hierarchical attention-based sampling approach is introduced to learn the similarity between low-resolution image features and multiple reference images based on a perceptual loss. Ablation demonstrates the contribution of both multi-reference and hierarchical attention-based sampling to overall performance. Perceptual and quantitative ground-truth evaluation demonstrates significant improvement in performance even when the reference images deviate significantly from the target image. The project website can be found at https://marcopesavento.github.io/AMRSR/



### End-to-End Monocular Vanishing Point Detection Exploiting Lane Annotations
- **Arxiv ID**: http://arxiv.org/abs/2108.13699v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13699v1)
- **Published**: 2021-08-31 09:23:33+00:00
- **Updated**: 2021-08-31 09:23:33+00:00
- **Authors**: Hiroto Honda, Motoki Kimura, Takumi Karasawa, Yusuke Uchida
- **Comment**: None
- **Journal**: None
- **Summary**: Vanishing points (VPs) play a vital role in various computer vision tasks, especially for recognizing the 3D scenes from an image. In the real-world scenario of automobile applications, it is costly to manually obtain the external camera parameters when the camera is attached to the vehicle or the attachment is accidentally perturbed. In this paper we introduce a simple but effective end-to-end vanishing point detection. By automatically calculating intersection of the extrapolated lane marker annotations, we obtain geometrically consistent VP labels and mitigate human annotation errors caused by manual VP labeling. With the calculated VP labels we train end-to-end VP Detector via heatmap estimation. The VP Detector realizes higher accuracy than the methods utilizing manual annotation or lane detection, paving the way for accurate online camera calibration.



### SemIE: Semantically-aware Image Extrapolation
- **Arxiv ID**: http://arxiv.org/abs/2108.13702v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.13702v1)
- **Published**: 2021-08-31 09:31:27+00:00
- **Updated**: 2021-08-31 09:31:27+00:00
- **Authors**: Bholeshwar Khurana, Soumya Ranjan Dash, Abhishek Bhatia, Aniruddha Mahapatra, Hrituraj Singh, Kuldeep Kulkarni
- **Comment**: To appear in International Conference on Computer Vision (ICCV) 2021.
  Project URL: https://semie-iccv.github.io
- **Journal**: None
- **Summary**: We propose a semantically-aware novel paradigm to perform image extrapolation that enables the addition of new object instances. All previous methods are limited in their capability of extrapolation to merely extending the already existing objects in the image. However, our proposed approach focuses not only on (i) extending the already present objects but also on (ii) adding new objects in the extended region based on the context. To this end, for a given image, we first obtain an object segmentation map using a state-of-the-art semantic segmentation method. The, thus, obtained segmentation map is fed into a network to compute the extrapolated semantic segmentation and the corresponding panoptic segmentation maps. The input image and the obtained segmentation maps are further utilized to generate the final extrapolated image. We conduct experiments on Cityscapes and ADE20K-bedroom datasets and show that our method outperforms all baselines in terms of FID, and similarity in object co-occurrence statistics.



### Pruning with Compensation: Efficient Channel Pruning for Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.13728v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13728v1)
- **Published**: 2021-08-31 10:17:36+00:00
- **Updated**: 2021-08-31 10:17:36+00:00
- **Authors**: Zhouyang Xie, Yan Fu, Shengzhao Tian, Junlin Zhou, Duanbing Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Channel pruning is a promising technique to compress the parameters of deep convolutional neural networks(DCNN) and to speed up the inference. This paper aims to address the long-standing inefficiency of channel pruning. Most channel pruning methods recover the prediction accuracy by re-training the pruned model from the remaining parameters or random initialization. This re-training process is heavily dependent on the sufficiency of computational resources, training data, and human interference(tuning the training strategy). In this paper, a highly efficient pruning method is proposed to significantly reduce the cost of pruning DCNN. The main contributions of our method include: 1) pruning compensation, a fast and data-efficient substitute of re-training to minimize the post-pruning reconstruction loss of features, 2) compensation-aware pruning(CaP), a novel pruning algorithm to remove redundant or less-weighted channels by minimizing the loss of information, and 3) binary structural search with step constraint to minimize human interference. On benchmarks including CIFAR-10/100 and ImageNet, our method shows competitive pruning performance among the state-of-the-art retraining-based pruning methods and, more importantly, reduces the processing time by 95% and data usage by 90%.



### Deep Learning on Edge TPUs
- **Arxiv ID**: http://arxiv.org/abs/2108.13732v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.13732v2)
- **Published**: 2021-08-31 10:23:37+00:00
- **Updated**: 2022-10-28 20:52:06+00:00
- **Authors**: Yipeng Sun, Andreas M Kist
- **Comment**: 10 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: Computing at the edge is important in remote settings, however, conventional hardware is not optimized for utilizing deep neural networks. The Google Edge TPU is an emerging hardware accelerator that is cost, power and speed efficient, and is available for prototyping and production purposes. Here, I review the Edge TPU platform, the tasks that have been accomplished using the Edge TPU, and which steps are necessary to deploy a model to the Edge TPU hardware. The Edge TPU is not only capable of tackling common computer vision tasks, but also surpasses other hardware accelerators, especially when the entire model can be deployed to the Edge TPU. Co-embedding the Edge TPU in cameras allows a seamless analysis of primary data. In summary, the Edge TPU is a maturing system that has proven its usability across multiple tasks.



### Super-Resolution Appearance Transfer for 4D Human Performances
- **Arxiv ID**: http://arxiv.org/abs/2108.13739v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.13739v1)
- **Published**: 2021-08-31 10:53:11+00:00
- **Updated**: 2021-08-31 10:53:11+00:00
- **Authors**: Marco Pesavento, Marco Volino, Adrian Hilton
- **Comment**: None
- **Journal**: None
- **Summary**: A common problem in the 4D reconstruction of people from multi-view video is the quality of the captured dynamic texture appearance which depends on both the camera resolution and capture volume. Typically the requirement to frame cameras to capture the volume of a dynamic performance ($>50m^3$) results in the person occupying only a small proportion $<$ 10% of the field of view. Even with ultra high-definition 4k video acquisition this results in sampling the person at less-than standard definition 0.5k video resolution resulting in low-quality rendering. In this paper we propose a solution to this problem through super-resolution appearance transfer from a static high-resolution appearance capture rig using digital stills cameras ($> 8k$) to capture the person in a small volume ($<8m^3$). A pipeline is proposed for super-resolution appearance transfer from high-resolution static capture to dynamic video performance capture to produce super-resolution dynamic textures. This addresses two key problems: colour mapping between different camera systems; and dynamic texture map super-resolution using a learnt model. Comparative evaluation demonstrates a significant qualitative and quantitative improvement in rendering the 4D performance capture with super-resolution dynamic texture appearance. The proposed approach reproduces the high-resolution detail of the static capture whilst maintaining the appearance dynamics of the captured video.



### Automatic labelling of urban point clouds using data fusion
- **Arxiv ID**: http://arxiv.org/abs/2108.13757v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2108.13757v2)
- **Published**: 2021-08-31 11:14:22+00:00
- **Updated**: 2021-10-15 14:07:55+00:00
- **Authors**: Daan Bloembergen, Chris Eijgenstein
- **Comment**: 5 pages, 5 figures; minor changes and improved results w.r.t. v1.
  Presented at the 10th Intl. Workshop on Urban Computing at ACM SIGSPATIAL
  2021. Code for this paper is available at
  https://github.com/Amsterdam-AI-Team/Urban_PointCloud_Processing
- **Journal**: None
- **Summary**: In this paper we describe an approach to semi-automatically create a labelled dataset for semantic segmentation of urban street-level point clouds. We use data fusion techniques using public data sources such as elevation data and large-scale topographical maps to automatically label parts of the point cloud, after which only limited human effort is needed to check the results and make amendments where needed. This drastically limits the time needed to create a labelled dataset that is extensive enough to train deep semantic segmentation models. We apply our method to point clouds of the Amsterdam region, and successfully train a RandLA-Net semantic segmentation model on the labelled dataset. These results demonstrate the potential of smart data fusion and semantic segmentation for the future of smart city planning and management.



### Discriminative Semantic Feature Pyramid Network with Guided Anchoring for Logo Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.13775v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13775v2)
- **Published**: 2021-08-31 11:59:00+00:00
- **Updated**: 2023-01-06 07:03:55+00:00
- **Authors**: Baisong Zhang, Weiqing Min, Jing Wang, Sujuan Hou, Qiang Hou, Yuanjie Zheng, Shuqiang Jiang
- **Comment**: We are very sorry that the result of the whole experiment is wrong
  because of the wrong derivation of Equation 3, and we would like to withdraw
  the manuscript to stop the propagation of the mistake
- **Journal**: None
- **Summary**: Recently, logo detection has received more and more attention for its wide applications in the multimedia field, such as intellectual property protection, product brand management, and logo duration monitoring. Unlike general object detection, logo detection is a challenging task, especially for small logo objects and large aspect ratio logo objects in the real-world scenario. In this paper, we propose a novel approach, named Discriminative Semantic Feature Pyramid Network with Guided Anchoring (DSFP-GA), which can address these challenges via aggregating the semantic information and generating different aspect ratio anchor boxes. More specifically, our approach mainly consists of Discriminative Semantic Feature Pyramid (DSFP) and Guided Anchoring (GA). Considering that low-level feature maps that are used to detect small logo objects lack semantic information, we propose the DSFP, which can enrich more discriminative semantic features of low-level feature maps and can achieve better performance on small logo objects. Furthermore, preset anchor boxes are less efficient for detecting large aspect ratio logo objects. We therefore integrate the GA into our method to generate large aspect ratio anchor boxes to mitigate this issue. Extensive experimental results on four benchmarks demonstrate the effectiveness of our proposed DSFP-GA. Moreover, we further conduct visual analysis and ablation studies to illustrate the advantage of our method in detecting small and large aspect logo objects. The code and models can be found at https://github.com/Zhangbaisong/DSFP-GA.



### Self-Calibrating Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2108.13826v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13826v2)
- **Published**: 2021-08-31 13:34:28+00:00
- **Updated**: 2021-09-02 13:26:43+00:00
- **Authors**: Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Animashree Anandkumar, Minsu Cho, Jaesik Park
- **Comment**: Accepted in ICCV21, Project Page:
  https://postech-cvlab.github.io/SCNeRF/
- **Journal**: None
- **Summary**: In this work, we propose a camera self-calibration algorithm for generic cameras with arbitrary non-linear distortions. We jointly learn the geometry of the scene and the accurate camera parameters without any calibration objects. Our camera model consists of a pinhole model, a fourth order radial distortion, and a generic noise model that can learn arbitrary non-linear camera distortions. While traditional self-calibration algorithms mostly rely on geometric constraints, we additionally incorporate photometric consistency. This requires learning the geometry of the scene, and we use Neural Radiance Fields (NeRF). We also propose a new geometric loss function, viz., projected ray distance loss, to incorporate geometric consistency for complex non-linear camera models. We validate our approach on standard real image datasets and demonstrate that our model can learn the camera intrinsics and extrinsics (pose) from scratch without COLMAP initialization. Also, we show that learning accurate camera models in a differentiable manner allows us to improve PSNR over baselines. Our module is an easy-to-use plugin that can be applied to NeRF variants to improve performance. The code and data are currently available at https://github.com/POSTECH-CVLab/SCNeRF.



### PACE: Posthoc Architecture-Agnostic Concept Extractor for Explaining CNNs
- **Arxiv ID**: http://arxiv.org/abs/2108.13828v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.13828v1)
- **Published**: 2021-08-31 13:36:15+00:00
- **Updated**: 2021-08-31 13:36:15+00:00
- **Authors**: Vidhya Kamakshi, Uday Gupta, Narayanan C Krishnan
- **Comment**: Accepted at International Joint Conference on Neural Networks (IJCNN
  2021)
- **Journal**: None
- **Summary**: Deep CNNs, though have achieved the state of the art performance in image classification tasks, remain a black-box to a human using them. There is a growing interest in explaining the working of these deep models to improve their trustworthiness. In this paper, we introduce a Posthoc Architecture-agnostic Concept Extractor (PACE) that automatically extracts smaller sub-regions of the image called concepts relevant to the black-box prediction. PACE tightly integrates the faithfulness of the explanatory framework to the black-box model. To the best of our knowledge, this is the first work that extracts class-specific discriminative concepts in a posthoc manner automatically. The PACE framework is used to generate explanations for two different CNN architectures trained for classifying the AWA2 and Imagenet-Birds datasets. Extensive human subject experiments are conducted to validate the human interpretability and consistency of the explanations extracted by PACE. The results from these experiments suggest that over 72% of the concepts extracted by PACE are human interpretable.



### InSeGAN: A Generative Approach to Segmenting Identical Instances in Depth Images
- **Arxiv ID**: http://arxiv.org/abs/2108.13865v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.13865v2)
- **Published**: 2021-08-31 14:18:40+00:00
- **Updated**: 2022-01-28 17:06:06+00:00
- **Authors**: Anoop Cherian, Goncalo Dias Pais, Siddarth Jain, Tim K. Marks, Alan Sullivan
- **Comment**: Accepted at ICCV 2021. Code & data @
  https://www.merl.com/research/license/InSeGAN
- **Journal**: None
- **Summary**: In this paper, we present InSeGAN, an unsupervised 3D generative adversarial network (GAN) for segmenting (nearly) identical instances of rigid objects in depth images. Using an analysis-by-synthesis approach, we design a novel GAN architecture to synthesize a multiple-instance depth image with independent control over each instance. InSeGAN takes in a set of code vectors (e.g., random noise vectors), each encoding the 3D pose of an object that is represented by a learned implicit object template. The generator has two distinct modules. The first module, the instance feature generator, uses each encoded pose to transform the implicit template into a feature map representation of each object instance. The second module, the depth image renderer, aggregates all of the single-instance feature maps output by the first module and generates a multiple-instance depth image. A discriminator distinguishes the generated multiple-instance depth images from the distribution of true depth images. To use our model for instance segmentation, we propose an instance pose encoder that learns to take in a generated depth image and reproduce the pose code vectors for all of the object instances. To evaluate our approach, we introduce a new synthetic dataset, "Insta-10", consisting of 100,000 depth images, each with 5 instances of an object from one of 10 classes. Our experiments on Insta-10, as well as on real-world noisy depth images, show that InSeGAN achieves state-of-the-art performance, often outperforming prior methods by large margins.



### One-shot domain adaptation for semantic face editing of real world images using StyleALAE
- **Arxiv ID**: http://arxiv.org/abs/2108.13876v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13876v1)
- **Published**: 2021-08-31 14:32:18+00:00
- **Updated**: 2021-08-31 14:32:18+00:00
- **Authors**: Ravi Kiran Reddy, Kumar Shubham, Gopalakrishnan Venkatesh, Sriram Gandikota, Sarthak Khoche, Dinesh Babu Jayagopi, Gopalakrishnan Srinivasaraghavan
- **Comment**: 12 pages, 3 figures
- **Journal**: None
- **Summary**: Semantic face editing of real world facial images is an important application of generative models. Recently, multiple works have explored possible techniques to generate such modifications using the latent structure of pre-trained GAN models. However, such approaches often require training an encoder network and that is typically a time-consuming and resource intensive process. A possible alternative to such a GAN-based architecture can be styleALAE, a latent-space based autoencoder that can generate photo-realistic images of high quality. Unfortunately, the reconstructed image in styleALAE does not preserve the identity of the input facial image. This limits the application of styleALAE for semantic face editing of images with known identities. In our work, we use a recent advancement in one-shot domain adaptation to address this problem. Our work ensures that the identity of the reconstructed image is the same as the given input image. We further generate semantic modifications over the reconstructed image by using the latent space of the pre-trained styleALAE model. Results show that our approach can generate semantic modifications on any real world facial image while preserving the identity.



### Estimation of Air Pollution with Remote Sensing Data: Revealing Greenhouse Gas Emissions from Space
- **Arxiv ID**: http://arxiv.org/abs/2108.13902v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2108.13902v1)
- **Published**: 2021-08-31 14:58:04+00:00
- **Updated**: 2021-08-31 14:58:04+00:00
- **Authors**: Linus Scheibenreif, Michael Mommert, Damian Borth
- **Comment**: for associated codebase, see
  https://www.github.com/HSG-AIML/RemoteSensingNO2Estimation
- **Journal**: None
- **Summary**: Air pollution is a major driver of climate change. Anthropogenic emissions from the burning of fossil fuels for transportation and power generation emit large amounts of problematic air pollutants, including Greenhouse Gases (GHGs). Despite the importance of limiting GHG emissions to mitigate climate change, detailed information about the spatial and temporal distribution of GHG and other air pollutants is difficult to obtain. Existing models for surface-level air pollution rely on extensive land-use datasets which are often locally restricted and temporally static. This work proposes a deep learning approach for the prediction of ambient air pollution that only relies on remote sensing data that is globally available and frequently updated. Combining optical satellite imagery with satellite-based atmospheric column density air pollution measurements enables the scaling of air pollution estimates (in this case NO$_2$) to high spatial resolution (up to $\sim$10m) at arbitrary locations and adds a temporal component to these estimates. The proposed model performs with high accuracy when evaluated against air quality measurements from ground stations (mean absolute error $<$6$~\mu g/m^3$). Our results enable the identification and temporal monitoring of major sources of air pollution and GHGs.



### Simultaneous Nuclear Instance and Layer Segmentation in Oral Epithelial Dysplasia
- **Arxiv ID**: http://arxiv.org/abs/2108.13904v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.13904v2)
- **Published**: 2021-08-31 15:02:39+00:00
- **Updated**: 2021-09-01 17:33:17+00:00
- **Authors**: Adam J. Shephard, Simon Graham, R. M. Saad Bashir, Mostafa Jahanifar, Hanya Mahmood, Syed Ali Khurram, Nasir M. Rajpoot
- **Comment**: 10 pages, 3 figures, conference
- **Journal**: None
- **Summary**: Oral epithelial dysplasia (OED) is a pre-malignant histopathological diagnosis given to lesions of the oral cavity. Predicting OED grade or whether a case will transition to malignancy is critical for early detection and appropriate treatment. OED typically begins in the lower third of the epithelium before progressing upwards with grade severity, thus we have suggested that segmenting intra-epithelial layers, in addition to individual nuclei, may enable researchers to evaluate important layer-specific morphological features for grade/malignancy prediction. We present HoVer-Net+, a deep learning framework to simultaneously segment (and classify) nuclei and (intra-)epithelial layers in H&E stained slides from OED cases. The proposed architecture consists of an encoder branch and four decoder branches for simultaneous instance segmentation of nuclei and semantic segmentation of the epithelial layers. We show that the proposed model achieves the state-of-the-art (SOTA) performance in both tasks, with no additional costs when compared to previous SOTA methods for each task. To the best of our knowledge, ours is the first method for simultaneous nuclear instance segmentation and semantic tissue segmentation, with potential for use in computational pathology for other similar simultaneous tasks and for future studies into malignancy prediction.



### Automatic digital twin data model generation of building energy systems from piping and instrumentation diagrams
- **Arxiv ID**: http://arxiv.org/abs/2108.13912v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13912v1)
- **Published**: 2021-08-31 15:09:39+00:00
- **Updated**: 2021-08-31 15:09:39+00:00
- **Authors**: Florian Stinner, Martin Wiecek, Marc Baranski, Alexander Kümpel, Dirk Müller
- **Comment**: None
- **Journal**: Proceedings of ECOS 2021 - The 34rth International Conference On
  Efficiency, Cost, Optimization, Simulation and Environmental Impact of Energy
  Systems, June 27-July 2, 2021, At: Taormina, Italy
- **Summary**: Buildings directly and indirectly emit a large share of current CO2 emissions. There is a high potential for CO2 savings through modern control methods in building automation systems (BAS) like model predictive control (MPC). For a proper control, MPC needs mathematical models to predict the future behavior of the controlled system. For this purpose, digital twins of the building can be used. However, with current methods in existing buildings, a digital twin set up is usually labor-intensive. Especially connecting the different components of the technical system to an overall digital twin of the building is time-consuming. Piping and instrument diagrams (P&ID) can provide the needed information, but it is necessary to extract the information and provide it in a standardized format to process it further.   In this work, we present an approach to recognize symbols and connections of P&ID from buildings in a completely automated way. There are various standards for graphical representation of symbols in P&ID of building energy systems. Therefore, we use different data sources and standards to generate a holistic training data set. We apply algorithms for symbol recognition, line recognition and derivation of connections to the data sets. Furthermore, the result is exported to a format that provides semantics of building energy systems.   The symbol recognition, line recognition and connection recognition show good results with an average precision of 93.7%, which can be used in further processes like control generation, (distributed) model predictive control or fault detection. Nevertheless, the approach needs further research.



### ScatSimCLR: self-supervised contrastive learning with pretext task regularization for small-scale datasets
- **Arxiv ID**: http://arxiv.org/abs/2108.13939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13939v1)
- **Published**: 2021-08-31 15:58:45+00:00
- **Updated**: 2021-08-31 15:58:45+00:00
- **Authors**: Vitaliy Kinakh, Olga Taran, Svyatoslav Voloshynovskiy
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider a problem of self-supervised learning for small-scale datasets based on contrastive loss between multiple views of the data, which demonstrates the state-of-the-art performance in classification task. Despite the reported results, such factors as the complexity of training requiring complex architectures, the needed number of views produced by data augmentation, and their impact on the classification accuracy are understudied problems. To establish the role of these factors, we consider an architecture of contrastive loss system such as SimCLR, where baseline model is replaced by geometrically invariant "hand-crafted" network ScatNet with small trainable adapter network and argue that the number of parameters of the whole system and the number of views can be considerably reduced while practically preserving the same classification accuracy. In addition, we investigate the impact of regularization strategies using pretext task learning based on an estimation of parameters of augmentation transform such as rotation and jigsaw permutation for both traditional baseline models and ScatNet based models. Finally, we demonstrate that the proposed architecture with pretext task learning regularization achieves the state-of-the-art classification performance with a smaller number of trainable parameters and with reduced number of views.



### A Novel Dataset for Keypoint Detection of quadruped Animals from Images
- **Arxiv ID**: http://arxiv.org/abs/2108.13958v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13958v1)
- **Published**: 2021-08-31 16:40:09+00:00
- **Updated**: 2021-08-31 16:40:09+00:00
- **Authors**: Prianka Banik, Lin Li, Xishuang Dong
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we studied the problem of localizing a generic set of keypoints across multiple quadruped or four-legged animal species from images. Due to the lack of large scale animal keypoint dataset with ground truth annotations, we developed a novel dataset, AwA Pose, for keypoint detection of quadruped animals from images. Our dataset contains significantly more keypoints per animal and has much more diverse animals than the existing datasets for animal keypoint detection. We benchmarked the dataset with a state-of-the-art deep learning model for different keypoint detection tasks, including both seen and unseen animal cases. Experimental results showed the effectiveness of the dataset. We believe that this dataset will help the computer vision community in the design and evaluation of improved models for the generalized quadruped animal keypoint detection problem.



### DepthTrack : Unveiling the Power of RGBD Tracking
- **Arxiv ID**: http://arxiv.org/abs/2108.13962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13962v1)
- **Published**: 2021-08-31 16:42:38+00:00
- **Updated**: 2021-08-31 16:42:38+00:00
- **Authors**: Song Yan, Jinyu Yang, Jani Käpylä, Feng Zheng, Aleš Leonardis, Joni-Kristian Kämäräinen
- **Comment**: Accepted to ICCV2021
- **Journal**: None
- **Summary**: RGBD (RGB plus depth) object tracking is gaining momentum as RGBD sensors have become popular in many application fields such as robotics.However, the best RGBD trackers are extensions of the state-of-the-art deep RGB trackers. They are trained with RGB data and the depth channel is used as a sidekick for subtleties such as occlusion detection. This can be explained by the fact that there are no sufficiently large RGBD datasets to 1) train deep depth trackers and to 2) challenge RGB trackers with sequences for which the depth cue is essential. This work introduces a new RGBD tracking dataset - Depth-Track - that has twice as many sequences (200) and scene types (40) than in the largest existing dataset, and three times more objects (90). In addition, the average length of the sequences (1473), the number of deformable objects (16) and the number of annotated tracking attributes (15) have been increased. Furthermore, by running the SotA RGB and RGBD trackers on DepthTrack, we propose a new RGBD tracking baseline, namely DeT, which reveals that deep RGBD tracking indeed benefits from genuine training data. The code and dataset is available at https://github.com/xiaozai/DeT



### S4-Crowd: Semi-Supervised Learning with Self-Supervised Regularisation for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2108.13969v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.13969v2)
- **Published**: 2021-08-31 16:51:00+00:00
- **Updated**: 2022-02-27 19:05:37+00:00
- **Authors**: Haoran Duan, Yu Guan
- **Comment**: There are some mistakes in the presentation of our methods
- **Journal**: None
- **Summary**: Crowd counting has drawn more attention because of its wide application in smart cities. Recent works achieved promising performance but relied on the supervised paradigm with expensive crowd annotations. To alleviate annotation cost, in this work we proposed a semi-supervised learning framework S4-Crowd, which can leverage both unlabeled/labeled data for robust crowd modelling. In the unsupervised pathway, two self-supervised losses were proposed to simulate the crowd variations such as scale, illumination, etc., based on which and the supervised information pseudo labels were generated and gradually refined. We also proposed a crowd-driven recurrent unit Gated-Crowd-Recurrent-Unit (GCRU), which can preserve discriminant crowd information by extracting second-order statistics, yielding pseudo labels with improved quality. A joint loss including both unsupervised/supervised information was proposed, and a dynamic weighting strategy was employed to balance the importance of the unsupervised loss and supervised loss at different training stages. We conducted extensive experiments on four popular crowd counting datasets in semi-supervised settings. Experimental results suggested the effectiveness of each proposed component in our S4-Crowd framework. Our method also outperformed other state-of-the-art semi-supervised learning approaches on these crowd datasets.



### Detecting Mitosis against Domain Shift using a Fused Detector and Deep Ensemble Classification Model for MIDOG Challenge
- **Arxiv ID**: http://arxiv.org/abs/2108.13983v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.13983v1)
- **Published**: 2021-08-31 17:10:26+00:00
- **Updated**: 2021-08-31 17:10:26+00:00
- **Authors**: Jingtang Liang, Cheng Wang, Yujie Cheng, Zheng Wang, Fang Wang, Liyu Huang, Zhibin Yu, Yubo Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Mitotic figure count is an important marker of tumor proliferation and has been shown to be associated with patients' prognosis. Deep learning based mitotic figure detection methods have been utilized to automatically locate the cell in mitosis using hematoxylin \& eosin (H\&E) stained images. However, the model performance deteriorates due to the large variation of color tone and intensity in H\&E images. In this work, we proposed a two stage mitotic figure detection framework by fusing a detector and a deep ensemble classification model. To alleviate the impact of color variation in H\&E images, we utilize both stain normalization and data augmentation, aiding model to learn color irrelevant features. The proposed model obtains an F1 score of 0.7550 on the preliminary testing set released by the MIDOG challenge.



### OARnet: Automated organs-at-risk delineation in Head and Neck CT images
- **Arxiv ID**: http://arxiv.org/abs/2108.13987v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2108.13987v4)
- **Published**: 2021-08-31 17:16:24+00:00
- **Updated**: 2021-11-23 11:07:43+00:00
- **Authors**: Mumtaz Hussain Soomro, Hamidreza Nourzadeh, Victor Gabriel Leandro Alves, Wookjin Choi, Jeffrey V. Siebers
- **Comment**: Paper is withdrawn, need more work on results and methodology
- **Journal**: None
- **Summary**: A 3D deep learning model (OARnet) is developed and used to delineate 28 H&N OARs on CT images. OARnet utilizes a densely connected network to detect the OAR bounding-box, then delineates the OAR within the box. It reuses information from any layer to subsequent layers and uses skip connections to combine information from different dense block levels to progressively improve delineation accuracy. Training uses up to 28 expert manual delineated (MD) OARs from 165 CTs. Dice similarity coefficient (DSC) and the 95th percentile Hausdorff distance (HD95) with respect to MD is assessed for 70 other CTs. Mean, maximum, and root-mean-square dose differences with respect to MD are assessed for 56 of the 70 CTs. OARnet is compared with UaNet, AnatomyNet, and Multi-Atlas Segmentation (MAS). Wilcoxon signed-rank tests using 95% confidence intervals are used to assess significance. Wilcoxon signed ranked tests show that, compared with UaNet, OARnet improves (p<0.05) the DSC (23/28 OARs) and HD95 (17/28). OARnet outperforms both AnatomyNet and MAS for DSC (28/28) and HD95 (27/28). Compared with UaNet, OARnet improves median DSC up to 0.05 and HD95 up to 1.5mm. Compared with AnatomyNet and MAS, OARnet improves median (DSC, HD95) by up to (0.08, 2.7mm) and (0.17, 6.3mm). Dosimetrically, OARnet outperforms UaNet (Dmax 7/28; Dmean 10/28), AnatomyNet (Dmax 21/28; Dmean 24/28), and MAS (Dmax 22/28; Dmean 21/28). The DenseNet architecture is optimized using a hybrid approach that performs OAR-specific bounding box detection followed by feature recognition. Compared with other auto-delineation methods, OARnet is better than or equal to UaNet for all but one geometric (Temporal Lobe L, HD95) and one dosimetric (Eye L, mean dose) endpoint for the 28 H&N OARs, and is better than or equal to both AnatomyNet and MAS for all OARs.



### RealisticHands: A Hybrid Model for 3D Hand Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2108.13995v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.13995v2)
- **Published**: 2021-08-31 17:40:49+00:00
- **Updated**: 2022-02-01 17:49:10+00:00
- **Authors**: Michael Seeber, Roi Poranne, Marc Polleyfeys, Martin R. Oswald
- **Comment**: International Conference on 3D Vision (3DV) 2021
- **Journal**: None
- **Summary**: Estimating 3D hand meshes from RGB images robustly is a highly desirable task, made challenging due to the numerous degrees of freedom, and issues such as self similarity and occlusions. Previous methods generally either use parametric 3D hand models or follow a model-free approach. While the former can be considered more robust, e.g. to occlusions, they are less expressive. We propose a hybrid approach, utilizing a deep neural network and differential rendering based optimization to demonstrably achieve the best of both worlds. In addition, we explore Virtual Reality (VR) as an application. Most VR headsets are nowadays equipped with multiple cameras, which we can leverage by extending our method to the egocentric stereo domain. This extension proves to be more resilient to the above mentioned issues. Finally, as a use-case, we show that the improved image-model alignment can be used to acquire the user's hand texture, which leads to a more realistic virtual hand representation.



### Working Memory Connections for LSTM
- **Arxiv ID**: http://arxiv.org/abs/2109.00020v1
- **DOI**: 10.1016/j.neunet.2021.08.030
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2109.00020v1)
- **Published**: 2021-08-31 18:01:30+00:00
- **Updated**: 2021-08-31 18:01:30+00:00
- **Authors**: Federico Landi, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara
- **Comment**: Accepted for publication in Neural Networks
- **Journal**: None
- **Summary**: Recurrent Neural Networks with Long Short-Term Memory (LSTM) make use of gating mechanisms to mitigate exploding and vanishing gradients when learning long-term dependencies. For this reason, LSTMs and other gated RNNs are widely adopted, being the standard de facto for many sequence modeling tasks. Although the memory cell inside the LSTM contains essential information, it is not allowed to influence the gating mechanism directly. In this work, we improve the gate potential by including information coming from the internal cell state. The proposed modification, named Working Memory Connection, consists in adding a learnable nonlinear projection of the cell content into the network gates. This modification can fit into the classical LSTM gates without any assumption on the underlying task, being particularly effective when dealing with longer sequences. Previous research effort in this direction, which goes back to the early 2000s, could not bring a consistent improvement over vanilla LSTM. As part of this paper, we identify a key issue tied to previous connections that heavily limits their effectiveness, hence preventing a successful integration of the knowledge coming from the internal cell state. We show through extensive experimental evaluation that Working Memory Connections constantly improve the performance of LSTMs on a variety of tasks. Numerical results suggest that the cell state contains useful information that is worth including in the gate structure.



### DensePose 3D: Lifting Canonical Surface Maps of Articulated Objects to the Third Dimension
- **Arxiv ID**: http://arxiv.org/abs/2109.00033v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2109.00033v1)
- **Published**: 2021-08-31 18:33:55+00:00
- **Updated**: 2021-08-31 18:33:55+00:00
- **Authors**: Roman Shapovalov, David Novotny, Benjamin Graham, Patrick Labatut, Andrea Vedaldi
- **Comment**: Accepted for ICCV 2021
- **Journal**: None
- **Summary**: We tackle the problem of monocular 3D reconstruction of articulated objects like humans and animals. We contribute DensePose 3D, a method that can learn such reconstructions in a weakly supervised fashion from 2D image annotations only. This is in stark contrast with previous deformable reconstruction methods that use parametric models such as SMPL pre-trained on a large dataset of 3D object scans. Because it does not require 3D scans, DensePose 3D can be used for learning a wide range of articulated categories such as different animal species. The method learns, in an end-to-end fashion, a soft partition of a given category-specific 3D template mesh into rigid parts together with a monocular reconstruction network that predicts the part motions such that they reproject correctly onto 2D DensePose-like surface annotations of the object. The decomposition of the object into parts is regularized by expressing part assignments as a combination of the smooth eigenfunctions of the Laplace-Beltrami operator. We show significant improvements compared to state-of-the-art non-rigid structure-from-motion baselines on both synthetic and real data on categories of humans and animals.



### Bio-inspired robot perception coupled with robot-modeled human perception
- **Arxiv ID**: http://arxiv.org/abs/2109.00097v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.00097v1)
- **Published**: 2021-08-31 22:22:55+00:00
- **Updated**: 2021-08-31 22:22:55+00:00
- **Authors**: Tobias Fischer
- **Comment**: Paper accepted to the "Robotics: Science and Systems Pioneers
  Workshop 2021"
- **Journal**: None
- **Summary**: My overarching research goal is to provide robots with perceptional abilities that allow interactions with humans in a human-like manner. To develop these perceptional abilities, I believe that it is useful to study the principles of the human visual system. I use these principles to develop new computer vision algorithms and validate their effectiveness in intelligent robotic systems. I am enthusiastic about this approach as it offers the dual benefit of uncovering principles inherent in the human visual system, as well as applying these principles to its artificial counterpart. Fig. 1 contains a depiction of my research.



### Two-step Domain Adaptation for Mitosis Cell Detection in Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/2109.00109v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.00109v2)
- **Published**: 2021-08-31 23:14:55+00:00
- **Updated**: 2021-09-27 19:26:44+00:00
- **Authors**: Ramin Nateghi, Fattaneh Pourakpour
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a two-step domain shift-invariant mitosis cell detection method based on Faster RCNN and a convolutional neural network (CNN). We generate various domain-shifted versions of existing histopathology images using a stain augmentation technique, enabling our method to effectively learn various stain domains and achieve better generalization. The performance of our method is evaluated on the preliminary test data set of the MIDOG-2021 challenge. The experimental results demonstrate that the proposed mitosis detection method can achieve promising performance for domain-shifted histopathology images.



### CPFN: Cascaded Primitive Fitting Networks for High-Resolution Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2109.00113v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00113v2)
- **Published**: 2021-08-31 23:27:33+00:00
- **Updated**: 2021-09-06 07:06:53+00:00
- **Authors**: Eric-Tuan Lê, Minhyuk Sung, Duygu Ceylan, Radomir Mech, Tamy Boubekeur, Niloy J. Mitra
- **Comment**: ICCV 2021: 15 pages, 8 figures
- **Journal**: ICCV 2021
- **Summary**: Representing human-made objects as a collection of base primitives has a long history in computer vision and reverse engineering. In the case of high-resolution point cloud scans, the challenge is to be able to detect both large primitives as well as those explaining the detailed parts. While the classical RANSAC approach requires case-specific parameter tuning, state-of-the-art networks are limited by memory consumption of their backbone modules such as PointNet++, and hence fail to detect the fine-scale primitives. We present Cascaded Primitive Fitting Networks (CPFN) that relies on an adaptive patch sampling network to assemble detection results of global and local primitive detection networks. As a key enabler, we present a merging formulation that dynamically aggregates the primitives across global and local scales. Our evaluation demonstrates that CPFN improves the state-of-the-art SPFN performance by 13-14% on high-resolution point cloud datasets and specifically improves the detection of fine-scale primitives by 20-22%.



### Uncertainty Quantified Deep Learning for Predicting Dice Coefficient of Digital Histopathology Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.00115v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T07, 54H30, I.2.1; G.3
- **Links**: [PDF](http://arxiv.org/pdf/2109.00115v1)
- **Published**: 2021-08-31 23:38:17+00:00
- **Updated**: 2021-08-31 23:38:17+00:00
- **Authors**: Sambuddha Ghosal, Audrey Xie, Pratik Shah
- **Comment**: Submitted to the 2022 IEEE International Symposium on Biomedical
  Imaging (ISBI) scientific conference
- **Journal**: None
- **Summary**: Deep learning models (DLMs) can achieve state of the art performance in medical image segmentation and classification tasks. However, DLMs that do not provide feedback for their predictions such as Dice coefficients (Dice) have limited deployment potential in real world clinical settings. Uncertainty estimates can increase the trust of these automated systems by identifying predictions that need further review but remain computationally prohibitive to deploy. In this study, we use a DLM with randomly initialized weights and Monte Carlo dropout (MCD) to segment tumors from microscopic Hematoxylin and Eosin (H&E) dye stained prostate core biopsy RGB images. We devise a novel approach that uses multiple clinical region based uncertainties from a single image (instead of the entire image) to predict Dice of the DLM model output by linear models. Image level uncertainty maps were generated and showed correspondence between imperfect model segmentation and high levels of uncertainty associated with specific prostate tissue regions with or without tumors. Results from this study suggest that linear models can learn coefficients of uncertainty quantified deep learning and correlations ((Spearman's correlation (p<0.05)) to predict Dice scores of specific regions of medical images.



### Contrastive Multiview Coding with Electro-optics for SAR Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.00120v1
- **DOI**: 10.1109/LGRS.2021.3109345
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00120v1)
- **Published**: 2021-08-31 23:55:41+00:00
- **Updated**: 2021-08-31 23:55:41+00:00
- **Authors**: Keumgang Cha, Junghoon Seo, Yeji Choi
- **Comment**: To be appeared in IEEE GRSL. DOI to be updated
- **Journal**: None
- **Summary**: In the training of deep learning models, how the model parameters are initialized greatly affects the model performance, sample efficiency, and convergence speed. Representation learning for model initialization has recently been actively studied in the remote sensing field. In particular, the appearance characteristics of the imagery obtained using the a synthetic aperture radar (SAR) sensor are quite different from those of general electro-optical (EO) images, and thus representation learning is even more important in remote sensing domain. Motivated from contrastive multiview coding, we propose multi-modal representation learning for SAR semantic segmentation. Unlike previous studies, our method jointly uses EO imagery, SAR imagery, and a label mask. Several experiments show that our approach is superior to the existing methods in model performance, sample efficiency, and convergence speed.



