# Arxiv Papers in cs.CV on 2021-08-11
### Simple black-box universal adversarial attacks on medical image classification based on deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/2108.04979v1
- **DOI**: 10.3390/a15050144
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04979v1)
- **Published**: 2021-08-11 00:59:34+00:00
- **Updated**: 2021-08-11 00:59:34+00:00
- **Authors**: Kazuki Koga, Kazuhiro Takemoto
- **Comment**: 19 pages, 3 figures, 2 tables
- **Journal**: Algorithms 15(5), 144 (2022)
- **Summary**: Universal adversarial attacks, which hinder most deep neural network (DNN) tasks using only a small single perturbation called a universal adversarial perturbation (UAP), is a realistic security threat to the practical application of a DNN. In particular, such attacks cause serious problems in medical imaging. Given that computer-based systems are generally operated under a black-box condition in which only queries on inputs are allowed and outputs are accessible, the impact of UAPs seems to be limited because well-used algorithms for generating UAPs are limited to a white-box condition in which adversaries can access the model weights and loss gradients. Nevertheless, we demonstrate that UAPs are easily generatable using a relatively small dataset under black-box conditions. In particular, we propose a method for generating UAPs using a simple hill-climbing search based only on DNN outputs and demonstrate the validity of the proposed method using representative DNN-based medical image classifications. Black-box UAPs can be used to conduct both non-targeted and targeted attacks. Overall, the black-box UAPs showed high attack success rates (40% to 90%), although some of them had relatively low success rates because the method only utilizes limited information to generate UAPs. The vulnerability of black-box UAPs was observed in several model architectures. The results indicate that adversaries can also generate UAPs through a simple procedure under the black-box condition to foil or control DNN-based medical image diagnoses, and that UAPs are a more realistic security threat.



### Learning Fair Face Representation With Progressive Cross Transformer
- **Arxiv ID**: http://arxiv.org/abs/2108.04983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04983v1)
- **Published**: 2021-08-11 01:31:14+00:00
- **Updated**: 2021-08-11 01:31:14+00:00
- **Authors**: Yong Li, Yufei Sun, Zhen Cui, Shiguang Shan, Jian Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition (FR) has made extraordinary progress owing to the advancement of deep convolutional neural networks. However, demographic bias among different racial cohorts still challenges the practical face recognition system. The race factor has been proven to be a dilemma for fair FR (FFR) as the subject-related specific attributes induce the classification bias whilst carrying some useful cues for FR. To mitigate racial bias and meantime preserve robust FR, we abstract face identity-related representation as a signal denoising problem and propose a progressive cross transformer (PCT) method for fair face recognition. Originating from the signal decomposition theory, we attempt to decouple face representation into i) identity-related components and ii) noisy/identity-unrelated components induced by race. As an extension of signal subspace decomposition, we formulate face decoupling as a generalized functional expression model to cross-predict face identity and race information. The face expression model is further concretized by designing dual cross-transformers to distill identity-related components and suppress racial noises. In order to refine face representation, we take a progressive face decoupling way to learn identity/race-specific transformations, so that identity-unrelated components induced by race could be better disentangled. We evaluate the proposed PCT on the public fair face recognition benchmarks (BFW, RFW) and verify that PCT is capable of mitigating bias in face recognition while achieving state-of-the-art FR performance. Besides, visualization results also show that the attention maps in PCT can well reveal the race-related/biased facial regions.



### A Transformer-based Math Language Model for Handwritten Math Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.05002v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.05002v1)
- **Published**: 2021-08-11 03:03:48+00:00
- **Updated**: 2021-08-11 03:03:48+00:00
- **Authors**: Huy Quang Ung, Cuong Tuan Nguyen, Hung Tuan Nguyen, Thanh-Nghia Truong, Masaki Nakagawa
- **Comment**: 14 pages, accepted in ICDAR-DIL 2021
- **Journal**: None
- **Summary**: Handwritten mathematical expressions (HMEs) contain ambiguities in their interpretations, even for humans sometimes. Several math symbols are very similar in the writing style, such as dot and comma or 0, O, and o, which is a challenge for HME recognition systems to handle without using contextual information. To address this problem, this paper presents a Transformer-based Math Language Model (TMLM). Based on the self-attention mechanism, the high-level representation of an input token in a sequence of tokens is computed by how it is related to the previous tokens. Thus, TMLM can capture long dependencies and correlations among symbols and relations in a mathematical expression (ME). We trained the proposed language model using a corpus of approximately 70,000 LaTeX sequences provided in CROHME 2016. TMLM achieved the perplexity of 4.42, which outperformed the previous math language models, i.e., the N-gram and recurrent neural network-based language models. In addition, we combine TMLM into a stochastic context-free grammar-based HME recognition system using a weighting parameter to re-rank the top-10 best candidates. The expression rates on the testing sets of CROHME 2016 and CROHME 2019 were improved by 2.97 and 0.83 percentage points, respectively.



### Learning Deep Multimodal Feature Representation with Asymmetric Multi-layer Fusion
- **Arxiv ID**: http://arxiv.org/abs/2108.05009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05009v1)
- **Published**: 2021-08-11 03:42:13+00:00
- **Updated**: 2021-08-11 03:42:13+00:00
- **Authors**: Yikai Wang, Fuchun Sun, Ming Lu, Anbang Yao
- **Comment**: ACMMM 2020 (2020.3)
- **Journal**: None
- **Summary**: We propose a compact and effective framework to fuse multimodal features at multiple layers in a single network. The framework consists of two innovative fusion schemes. Firstly, unlike existing multimodal methods that necessitate individual encoders for different modalities, we verify that multimodal features can be learnt within a shared single network by merely maintaining modality-specific batch normalization layers in the encoder, which also enables implicit fusion via joint feature representation learning. Secondly, we propose a bidirectional multi-layer fusion scheme, where multimodal features can be exploited progressively. To take advantage of such scheme, we introduce two asymmetric fusion operations including channel shuffle and pixel shift, which learn different fused features with respect to different fusion directions. These two operations are parameter-free and strengthen the multimodal feature interactions across channels as well as enhance the spatial feature discrimination within channels. We conduct extensive experiments on semantic segmentation and image translation tasks, based on three publicly available datasets covering diverse modalities. Results indicate that our proposed framework is general, compact and is superior to state-of-the-art fusion frameworks.



### Prototype Completion for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.05010v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05010v1)
- **Published**: 2021-08-11 03:44:00+00:00
- **Updated**: 2021-08-11 03:44:00+00:00
- **Authors**: Baoquan Zhang, Xutao Li, Yunming Ye, Shanshan Feng
- **Comment**: Extended version of 'Prototype Completion with Primitive Knowledge
  for Few-Shot Learning' in CVPR2021. arXiv admin note: substantial text
  overlap with arXiv:2009.04960
- **Journal**: None
- **Summary**: Few-shot learning aims to recognize novel classes with few examples. Pre-training based methods effectively tackle the problem by pre-training a feature extractor and then fine-tuning it through the nearest centroid based meta-learning. However, results show that the fine-tuning step makes marginal improvements. In this paper, 1) we figure out the reason, i.e., in the pre-trained feature space, the base classes already form compact clusters while novel classes spread as groups with large variances, which implies that fine-tuning feature extractor is less meaningful; 2) instead of fine-tuning feature extractor, we focus on estimating more representative prototypes. Consequently, we propose a novel prototype completion based meta-learning framework. This framework first introduces primitive knowledge (i.e., class-level part or attribute annotations) and extracts representative features for seen attributes as priors. Second, a part/attribute transfer network is designed to learn to infer the representative features for unseen attributes as supplementary priors. Finally, a prototype completion network is devised to learn to complete prototypes with these priors. Moreover, to avoid the prototype completion error, we further develop a Gaussian based prototype fusion strategy that fuses the mean-based and completed prototypes by exploiting the unlabeled samples. Extensive experiments show that our method: (i) obtains more accurate prototypes; (ii) achieves superior performance on both inductive and transductive FSL settings.



### Elastic Tactile Simulation Towards Tactile-Visual Perception
- **Arxiv ID**: http://arxiv.org/abs/2108.05013v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.05013v2)
- **Published**: 2021-08-11 03:49:59+00:00
- **Updated**: 2021-08-12 09:54:27+00:00
- **Authors**: Yikai Wang, Wenbing Huang, Bin Fang, Fuchun Sun, Chang Li
- **Comment**: ACMMM 2021 (Oral). Code available at https://github.com/yikaiw/EIP.
  arXiv admin note: substantial text overlap with arXiv:2011.11528
- **Journal**: None
- **Summary**: Tactile sensing plays an important role in robotic perception and manipulation tasks. To overcome the real-world limitations of data collection, simulating tactile response in a virtual environment comes as a desirable direction of robotic research. In this paper, we propose Elastic Interaction of Particles (EIP) for tactile simulation. Most existing works model the tactile sensor as a rigid multi-body, which is incapable of reflecting the elastic property of the tactile sensor as well as characterizing the fine-grained physical interaction between the two objects. By contrast, EIP models the tactile sensor as a group of coordinated particles, and the elastic property is applied to regulate the deformation of particles during contact. With the tactile simulation by EIP, we further propose a tactile-visual perception network that enables information fusion between tactile data and visual images. The perception network is based on a global-to-local fusion mechanism where multi-scale tactile features are aggregated to the corresponding local region of the visual modality with the guidance of tactile positions and directions. The fusion method exhibits superiority regarding the 3D geometric reconstruction task.



### VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows
- **Arxiv ID**: http://arxiv.org/abs/2108.05015v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.05015v3)
- **Published**: 2021-08-11 03:55:12+00:00
- **Updated**: 2022-06-28 12:31:22+00:00
- **Authors**: Xiao Wang, Jianing Li, Lin Zhu, Zhipeng Zhang, Zhe Chen, Xin Li, Yaowei Wang, Yonghong Tian, Feng Wu
- **Comment**: In Peer Review
- **Journal**: None
- **Summary**: Different from visible cameras which record intensity images frame by frame, the biologically inspired event camera produces a stream of asynchronous and sparse events with much lower latency. In practice, the visible cameras can better perceive texture details and slow motion, while event cameras can be free from motion blurs and have a larger dynamic range which enables them to work well under fast motion and low illumination. Therefore, the two sensors can cooperate with each other to achieve more reliable object tracking. In this work, we propose a large-scale Visible-Event benchmark (termed VisEvent) due to the lack of a realistic and scaled dataset for this task. Our dataset consists of 820 video pairs captured under low illumination, high speed, and background clutter scenarios, and it is divided into a training and a testing subset, each of which contains 500 and 320 videos, respectively. Based on VisEvent, we transform the event flows into event images and construct more than 30 baseline methods by extending current single-modality trackers into dual-modality versions. More importantly, we further build a simple but effective tracking algorithm by proposing a cross-modality transformer, to achieve more effective feature fusion between visible and event data. Extensive experiments on the proposed VisEvent dataset, FE108, and two simulated datasets (i.e., OTB-DVS and VOT-DVS), validated the effectiveness of our model. The dataset and source code have been released at our project page: \url{https://sites.google.com/view/viseventtrack/}.



### One-Sided Box Filter for Edge Preserving Image Smoothing
- **Arxiv ID**: http://arxiv.org/abs/2108.05021v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.DS, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.05021v1)
- **Published**: 2021-08-11 04:22:38+00:00
- **Updated**: 2021-08-11 04:22:38+00:00
- **Authors**: Yuanhao Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Image smoothing is a fundamental task in signal processing. For such task, box filter is well-known. However, box filter can not keep some features of the signal, such as edges, corners and the jump in the step function. In this paper, we present a one-sided box filter that can smooth the signal but keep the discontinuous features in the signal. More specifically, we perform box filter on eight one-sided windows, leading to a one-sided box filter that can preserve corners and edges. Our filter inherits the constant $O(1)$ computational complexity of the original box filter with respect to the window size and also the linear $O(N)$ computational complexity with respect to the total number of samples. We performance several experiments to show the efficiency and effectiveness of this filter. We further compare our filter with other the-state-of-the-art edge preserving methods. Our filter can be deployed in a large range of applications where the classical box filter is adopted.



### Learning Oculomotor Behaviors from Scanpath
- **Arxiv ID**: http://arxiv.org/abs/2108.05025v1
- **DOI**: 10.1145/3462244.3479923
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.05025v1)
- **Published**: 2021-08-11 04:38:17+00:00
- **Updated**: 2021-08-11 04:38:17+00:00
- **Authors**: Beibin Li, Nicholas Nuechterlein, Erin Barney, Claire Foster, Minah Kim, Monique Mahony, Adham Atyabi, Li Feng, Quan Wang, Pamela Ventola, Linda Shapiro, Frederick Shic
- **Comment**: Accepted ACM ICMI 2021
- **Journal**: None
- **Summary**: Identifying oculomotor behaviors relevant for eye-tracking applications is a critical but often challenging task. Aiming to automatically learn and extract knowledge from existing eye-tracking data, we develop a novel method that creates rich representations of oculomotor scanpaths to facilitate the learning of downstream tasks. The proposed stimulus-agnostic Oculomotor Behavior Framework (OBF) model learns human oculomotor behaviors from unsupervised and semi-supervised tasks, including reconstruction, predictive coding, fixation identification, and contrastive learning tasks. The resultant pre-trained OBF model can be used in a variety of applications. Our pre-trained model outperforms baseline approaches and traditional scanpath methods in autism spectrum disorder and viewed-stimulus classification tasks. Ablation experiments further show our proposed method could achieve even better results with larger model sizes and more diverse eye-tracking training datasets, supporting the model's potential for future eye-tracking applications. Open source code: http://github.com/BeibinLi/OBF.



### Boosting the Generalization Capability in Cross-Domain Few-shot Learning via Noise-enhanced Supervised Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2108.05028v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.05028v2)
- **Published**: 2021-08-11 04:45:56+00:00
- **Updated**: 2021-09-23 04:56:31+00:00
- **Authors**: Hanwen Liang, Qiong Zhang, Peng Dai, Juwei Lu
- **Comment**: Accepted at ICCV2021
- **Journal**: None
- **Summary**: State of the art (SOTA) few-shot learning (FSL) methods suffer significant performance drop in the presence of domain differences between source and target datasets. The strong discrimination ability on the source dataset does not necessarily translate to high classification accuracy on the target dataset. In this work, we address this cross-domain few-shot learning (CDFSL) problem by boosting the generalization capability of the model. Specifically, we teach the model to capture broader variations of the feature distributions with a novel noise-enhanced supervised autoencoder (NSAE). NSAE trains the model by jointly reconstructing inputs and predicting the labels of inputs as well as their reconstructed pairs. Theoretical analysis based on intra-class correlation (ICC) shows that the feature embeddings learned from NSAE have stronger discrimination and generalization abilities in the target domain. We also take advantage of NSAE structure and propose a two-step fine-tuning procedure that achieves better adaption and improves classification performance in the target domain. Extensive experiments and ablation studies are conducted to demonstrate the effectiveness of the proposed method. Experimental results show that our proposed method consistently outperforms SOTA methods under various conditions.



### Learning Action Completeness from Points for Weakly-supervised Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2108.05029v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05029v1)
- **Published**: 2021-08-11 04:54:39+00:00
- **Updated**: 2021-08-11 04:54:39+00:00
- **Authors**: Pilhyeon Lee, Hyeran Byun
- **Comment**: Accepted by ICCV 2021 (Oral). Code is available at
  https://github.com/Pilhyeon
- **Journal**: None
- **Summary**: We tackle the problem of localizing temporal intervals of actions with only a single frame label for each action instance for training. Owing to label sparsity, existing work fails to learn action completeness, resulting in fragmentary action predictions. In this paper, we propose a novel framework, where dense pseudo-labels are generated to provide completeness guidance for the model. Concretely, we first select pseudo background points to supplement point-level action labels. Then, by taking the points as seeds, we search for the optimal sequence that is likely to contain complete action instances while agreeing with the seeds. To learn completeness from the obtained sequence, we introduce two novel losses that contrast action instances with background ones in terms of action score and feature similarity, respectively. Experimental results demonstrate that our completeness guidance indeed helps the model to locate complete action instances, leading to large performance gains especially under high IoU thresholds. Moreover, we demonstrate the superiority of our method over existing state-of-the-art methods on four benchmarks: THUMOS'14, GTEA, BEOID, and ActivityNet. Notably, our method even performs comparably to recent fully-supervised methods, at the 6 times cheaper annotation cost. Our code is available at https://github.com/Pilhyeon.



### Semi-Supervised Domain Generalizable Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2108.05045v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05045v2)
- **Published**: 2021-08-11 06:08:25+00:00
- **Updated**: 2021-09-09 02:54:03+00:00
- **Authors**: Lingxiao He, Wu Liu, Jian Liang, Kecheng Zheng, Xingyu Liao, Peng Cheng, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Existing person re-identification (re-id) methods are stuck when deployed to a new unseen scenario despite the success in cross-camera person matching. Recent efforts have been substantially devoted to domain adaptive person re-id where extensive unlabeled data in the new scenario are utilized in a transductive learning manner. However, for each scenario, it is required to first collect enough data and then train such a domain adaptive re-id model, thus restricting their practical application. Instead, we aim to explore multiple labeled datasets to learn generalized domain-invariant representations for person re-id, which is expected universally effective for each new-coming re-id scenario. To pursue practicability in real-world systems, we collect all the person re-id datasets (20 datasets) in this field and select the three most frequently used datasets (i.e., Market1501, DukeMTMC, and MSMT17) as unseen target domains. In addition, we develop DataHunter that collects over 300K+ weak annotated images named YouTube-Human from YouTube street-view videos, which joins 17 remaining full labeled datasets to form multiple source domains. On such a large and challenging benchmark called FastHuman (~440K+ labeled images), we further propose a simple yet effective Semi-Supervised Knowledge Distillation (SSKD) framework. SSKD effectively exploits the weakly annotated data by assigning soft pseudo labels to YouTube-Human to improve models' generalization ability. Experiments on several protocols verify the effectiveness of the proposed SSKD framework on domain generalizable person re-id, which is even comparable to supervised learning on the target domains. Lastly, but most importantly, we hope the proposed benchmark FastHuman could bring the next development of domain generalizable person re-id algorithms.



### Rethinking Coarse-to-Fine Approach in Single Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2108.05054v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.05054v2)
- **Published**: 2021-08-11 06:37:01+00:00
- **Updated**: 2021-09-16 09:04:59+00:00
- **Authors**: Sung-Jin Cho, Seo-Won Ji, Jun-Pyo Hong, Seung-Won Jung, Sung-Jea Ko
- **Comment**: Accepted by IEEE International Conference on Computer Vision (ICCV)
  2021
- **Journal**: None
- **Summary**: Coarse-to-fine strategies have been extensively used for the architecture design of single image deblurring networks. Conventional methods typically stack sub-networks with multi-scale input images and gradually improve sharpness of images from the bottom sub-network to the top sub-network, yielding inevitably high computational costs. Toward a fast and accurate deblurring network design, we revisit the coarse-to-fine strategy and present a multi-input multi-output U-net (MIMO-UNet). The MIMO-UNet has three distinct features. First, the single encoder of the MIMO-UNet takes multi-scale input images to ease the difficulty of training. Second, the single decoder of the MIMO-UNet outputs multiple deblurred images with different scales to mimic multi-cascaded U-nets using a single U-shaped network. Last, asymmetric feature fusion is introduced to merge multi-scale features in an efficient manner. Extensive experiments on the GoPro and RealBlur datasets demonstrate that the proposed network outperforms the state-of-the-art methods in terms of both accuracy and computational complexity. Source code is available for research purposes at https://github.com/chosj95/MIMO-UNet.



### Statistical Dependency Guided Contrastive Learning for Multiple Labeling in Prenatal Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2108.05055v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05055v3)
- **Published**: 2021-08-11 06:39:26+00:00
- **Updated**: 2022-05-20 11:42:29+00:00
- **Authors**: Shuangchi He, Zehui Lin, Xin Yang, Chaoyu Chen, Jian Wang, Xue Shuang, Ziwei Deng, Qin Liu, Yan Cao, Xiduo Lu, Ruobing Huang, Nishant Ravikumar, Alejandro Frangi, Yuanji Zhang, Yi Xiong, Dong Ni
- **Comment**: Accepted by MICCAI-MLMI 2021
- **Journal**: None
- **Summary**: Standard plane recognition plays an important role in prenatal ultrasound (US) screening. Automatically recognizing the standard plane along with the corresponding anatomical structures in US image can not only facilitate US image interpretation but also improve diagnostic efficiency. In this study, we build a novel multi-label learning (MLL) scheme to identify multiple standard planes and corresponding anatomical structures of fetus simultaneously. Our contribution is three-fold. First, we represent the class correlation by word embeddings to capture the fine-grained semantic and latent statistical concurrency. Second, we equip the MLL with a graph convolutional network to explore the inner and outer relationship among categories. Third, we propose a novel cluster relabel-based contrastive learning algorithm to encourage the divergence among ambiguous classes. Extensive validation was performed on our large in-house dataset. Our approach reports the highest accuracy as 90.25% for standard planes labeling, 85.59% for planes and structures labeling and mAP as 94.63%. The proposed MLL scheme provides a novel perspective for standard plane recognition and can be easily extended to other medical image classification tasks.



### Towards Top-Down Just Noticeable Difference Estimation of Natural Images
- **Arxiv ID**: http://arxiv.org/abs/2108.05058v2
- **DOI**: 10.1109/TIP.2022.3174398
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.05058v2)
- **Published**: 2021-08-11 06:51:50+00:00
- **Updated**: 2022-05-24 05:29:34+00:00
- **Authors**: Qiuping Jiang, Zhentao Liu, Shiqi Wang, Feng Shao, Weisi Lin
- **Comment**: 16 pages, 16 figures
- **Journal**: IEEE Transactions on Image Processing, 2022
- **Summary**: Just noticeable difference (JND) of natural images refers to the maximum pixel intensity change magnitude that typical human visual system (HVS) cannot perceive. Existing efforts on JND estimation mainly dedicate to modeling the diverse masking effects in either/both spatial or/and frequency domains, and then fusing them into an overall JND estimate. In this work, we turn to a dramatically different way to address this problem with a top-down design philosophy. Instead of explicitly formulating and fusing different masking effects in a bottom-up way, the proposed JND estimation model dedicates to first predicting a critical perceptual lossless (CPL) counterpart of the original image and then calculating the difference map between the original image and the predicted CPL image as the JND map. We conduct subjective experiments to determine the critical points of 500 images and find that the distribution of cumulative normalized KLT coefficient energy values over all 500 images at these critical points can be well characterized by a Weibull distribution. Given a testing image, its corresponding critical point is determined by a simple weighted average scheme where the weights are determined by a fitted Weibull distribution function. The performance of the proposed JND model is evaluated explicitly with direct JND prediction and implicitly with two applications including JND-guided noise injection and JND-guided image compression. Experimental results have demonstrated that our proposed JND model can achieve better performance than several latest JND models. In addition, we also compare the proposed JND model with existing visual difference predicator (VDP) metrics in terms of the capability in distortion detection and discrimination. The results indicate that our JND model also has a good performance in this task.



### MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approach
- **Arxiv ID**: http://arxiv.org/abs/2108.05060v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.05060v2)
- **Published**: 2021-08-11 06:57:04+00:00
- **Updated**: 2021-09-10 08:11:38+00:00
- **Authors**: Falk Heuer, Sven Mantowsky, Syed Saqib Bukhari, Georg Schneider
- **Comment**: Accepted by IEEE International Conference on Computer Vision (ICCV)
  2021
- **Journal**: None
- **Summary**: Multitask learning is a common approach in machine learning, which allows to train multiple objectives with a shared architecture. It has been shown that by training multiple tasks together inference time and compute resources can be saved, while the objectives performance remains on a similar or even higher level. However, in perception related multitask networks only closely related tasks can be found, such as object detection, instance and semantic segmentation or depth estimation. Multitask networks with diverse tasks and their effects with respect to efficiency on one another are not well studied. In this paper we augment the CenterNet anchor-free approach for training multiple diverse perception related tasks together, including the task of object detection and semantic segmentation as well as human pose estimation. We refer to this DNN as Multitask-CenterNet (MCN). Additionally, we study different MCN settings for efficiency. The MCN can perform several tasks at once while maintaining, and in some cases even exceeding, the performance values of its corresponding single task networks. More importantly, the MCN architecture decreases inference time and reduces network size when compared to a composition of single task networks.



### NI-UDA: Graph Adversarial Domain Adaptation from Non-shared-and-Imbalanced Big Data to Small Imbalanced Applications
- **Arxiv ID**: http://arxiv.org/abs/2108.05061v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.05061v2)
- **Published**: 2021-08-11 07:01:13+00:00
- **Updated**: 2021-08-12 01:37:55+00:00
- **Authors**: Guangyi Xiao, Weiwei Xiang, Huan Liu, Hao Chen, Shun Peng, Jingzhi Guo, Zhiguo Gong
- **Comment**: 11 pages, 5 figures, and 8 tables
- **Journal**: None
- **Summary**: We propose a new general Graph Adversarial Domain Adaptation (GADA) based on semantic knowledge reasoning of class structure for solving the problem of unsupervised domain adaptation (UDA) from the big data with non-shared and imbalanced classes to specified small and imbalanced applications (NI-UDA), where non-shared classes mean the label space out of the target domain. Our goal is to leverage priori hierarchy knowledge to enhance domain adversarial aligned feature representation with graph reasoning. In this paper, to address two challenges in NI-UDA, we equip adversarial domain adaptation with Hierarchy Graph Reasoning (HGR) layer and the Source Classifier Filter (SCF). For sparse classes transfer challenge, our HGR layer can aggregate local feature to hierarchy graph nodes by node prediction and enhance domain adversarial aligned feature with hierarchy graph reasoning for sparse classes. Our HGR contributes to learn direct semantic patterns for sparse classes by hierarchy attention in self-attention, non-linear mapping and graph normalization. our SCF is proposed for the challenge of knowledge sharing from non-shared data without negative transfer effect by filtering low-confidence non-shared data in HGR layer. Experiments on two benchmark datasets show our GADA methods consistently improve the state-of-the-art adversarial UDA algorithms, e.g. GADA(HGR) can greatly improve f1 of the MDD by \textbf{7.19\%} and GVB-GD by \textbf{7.89\%} respectively on imbalanced source task in Meal300 dataset. The code is available at https://gadatransfer.wixsite.com/gada.



### Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.05067v2
- **DOI**: 10.1109/TNNLS.2021.3099165
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2108.05067v2)
- **Published**: 2021-08-11 07:12:57+00:00
- **Updated**: 2021-08-18 05:32:00+00:00
- **Authors**: Guangyi Liu, Yinghong Liao, Fuyu Wang, Bin Zhang, Lu Zhang, Xiaodan Liang, Xiang Wan, Shaolin Li, Zhen Li, Shuixing Zhang, Shuguang Cui
- **Comment**: Accepted by IEEE Transactions on Neural Networks and Learning Systems
- **Journal**: None
- **Summary**: Medical imaging technologies, including computed tomography (CT) or chest X-Ray (CXR), are largely employed to facilitate the diagnosis of the COVID-19. Since manual report writing is usually too time-consuming, a more intelligent auxiliary medical system that could generate medical reports automatically and immediately is urgently needed. In this article, we propose to use the medical visual language BERT (Medical-VLBERT) model to identify the abnormality on the COVID-19 scans and generate the medical report automatically based on the detected lesion regions. To produce more accurate medical reports and minimize the visual-and-linguistic differences, this model adopts an alternate learning strategy with two procedures that are knowledge pretraining and transferring. To be more precise, the knowledge pretraining procedure is to memorize the knowledge from medical texts, while the transferring procedure is to utilize the acquired knowledge for professional medical sentences generations through observations of medical images. In practice, for automatic medical report generation on the COVID-19 cases, we constructed a dataset of 368 medical findings in Chinese and 1104 chest CT scans from The First Affiliated Hospital of Jinan University, Guangzhou, China, and The Fifth Affiliated Hospital of Sun Yat-sen University, Zhuhai, China. Besides, to alleviate the insufficiency of the COVID-19 training samples, our model was first trained on the large-scale Chinese CX-CHR dataset and then transferred to the COVID-19 CT dataset for further fine-tuning. The experimental results showed that Medical-VLBERT achieved state-of-the-art performances on terminology prediction and report generation with the Chinese COVID-19 CT dataset and the CX-CHR dataset. The Chinese COVID-19 CT dataset is available at https://covid19ct.github.io/.



### Multi-Source Fusion and Automatic Predictor Selection for Zero-Shot Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.05076v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05076v1)
- **Published**: 2021-08-11 07:37:44+00:00
- **Updated**: 2021-08-11 07:37:44+00:00
- **Authors**: Xiaoqi Zhao, Youwei Pang, Jiaxing Yang, Lihe Zhang, Huchuan Lu
- **Comment**: This work was accepted as ACM MM 2021 oral
- **Journal**: None
- **Summary**: Location and appearance are the key cues for video object segmentation. Many sources such as RGB, depth, optical flow and static saliency can provide useful information about the objects. However, existing approaches only utilize the RGB or RGB and optical flow. In this paper, we propose a novel multi-source fusion network for zero-shot video object segmentation. With the help of interoceptive spatial attention module (ISAM), spatial importance of each source is highlighted. Furthermore, we design a feature purification module (FPM) to filter the inter-source incompatible features. By the ISAM and FPM, the multi-source features are effectively fused. In addition, we put forward an automatic predictor selection network (APS) to select the better prediction of either the static saliency predictor or the moving object predictor in order to prevent over-reliance on the failed results caused by low-quality optical flow maps. Extensive experiments on three challenging public benchmarks (i.e. DAVIS$_{16}$, Youtube-Objects and FBMS) show that the proposed model achieves compelling performance against the state-of-the-arts. The source code will be publicly available at \textcolor{red}{\url{https://github.com/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS}}.



### Mining the Benefits of Two-stage and One-stage HOI Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.05077v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05077v2)
- **Published**: 2021-08-11 07:38:09+00:00
- **Updated**: 2021-10-13 06:09:22+00:00
- **Authors**: Aixi Zhang, Yue Liao, Si Liu, Miao Lu, Yongliang Wang, Chen Gao, Xiaobo Li
- **Comment**: Accepted by NeurIPS 2021
- **Journal**: None
- **Summary**: Two-stage methods have dominated Human-Object Interaction (HOI) detection for several years. Recently, one-stage HOI detection methods have become popular. In this paper, we aim to explore the essential pros and cons of two-stage and one-stage methods. With this as the goal, we find that conventional two-stage methods mainly suffer from positioning positive interactive human-object pairs, while one-stage methods are challenging to make an appropriate trade-off on multi-task learning, i.e., object detection, and interaction classification. Therefore, a core problem is how to take the essence and discard the dregs from the conventional two types of methods. To this end, we propose a novel one-stage framework with disentangling human-object detection and interaction classification in a cascade manner. In detail, we first design a human-object pair generator based on a state-of-the-art one-stage HOI detector by removing the interaction classification module or head and then design a relatively isolated interaction classifier to classify each human-object pair. Two cascade decoders in our proposed framework can focus on one specific task, detection or interaction classification. In terms of the specific implementation, we adopt a transformer-based HOI detector as our base model. The newly introduced disentangling paradigm outperforms existing methods by a large margin, with a significant relative mAP gain of 9.32% on HICO-Det. The source codes are available at https://github.com/YueLiao/CDN.



### FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset
- **Arxiv ID**: http://arxiv.org/abs/2108.05080v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS, I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2108.05080v4)
- **Published**: 2021-08-11 07:49:36+00:00
- **Updated**: 2022-03-01 10:38:07+00:00
- **Authors**: Hasam Khalid, Shahroz Tariq, Minha Kim, Simon S. Woo
- **Comment**: Part of Proceedings of the Neural Information Processing Systems
  Track on Datasets and Benchmarks (NeurIPS Datasets and Benchmarks 2021)
- **Journal**: None
- **Summary**: While the significant advancements have made in the generation of deepfakes using deep learning technologies, its misuse is a well-known issue now. Deepfakes can cause severe security and privacy issues as they can be used to impersonate a person's identity in a video by replacing his/her face with another person's face. Recently, a new problem of generating synthesized human voice of a person is emerging, where AI-based deep learning models can synthesize any person's voice requiring just a few seconds of audio. With the emerging threat of impersonation attacks using deepfake audios and videos, a new generation of deepfake detectors is needed to focus on both video and audio collectively. To develop a competent deepfake detector, a large amount of high-quality data is typically required to capture real-world (or practical) scenarios. Existing deepfake datasets either contain deepfake videos or audios, which are racially biased as well. As a result, it is critical to develop a high-quality video and audio deepfake dataset that can be used to detect both audio and video deepfakes simultaneously. To fill this gap, we propose a novel Audio-Video Deepfake dataset, FakeAVCeleb, which contains not only deepfake videos but also respective synthesized lip-synced fake audios. We generate this dataset using the most popular deepfake generation methods. We selected real YouTube videos of celebrities with four ethnic backgrounds to develop a more realistic multimodal dataset that addresses racial bias, and further help develop multimodal deepfake detectors. We performed several experiments using state-of-the-art detection methods to evaluate our deepfake dataset and demonstrate the challenges and usefulness of our multimodal Audio-Video deepfake dataset.



### Cervical Optical Coherence Tomography Image Classification Based on Contrastive Self-Supervised Texture Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.05081v3
- **DOI**: 10.1002/mp.15630
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2108.05081v3)
- **Published**: 2021-08-11 07:52:59+00:00
- **Updated**: 2022-03-18 00:56:50+00:00
- **Authors**: Kaiyi Chen, Qingbin Wang, Yutao Ma
- **Comment**: 22 pages, 7 figures, and 7 tables
- **Journal**: Medical Physics, 2022, 49 (6), 3638-3653
- **Summary**: Background: Cervical cancer seriously affects the health of the female reproductive system. Optical coherence tomography (OCT) emerged as a non-invasive, high-resolution imaging technology for cervical disease detection. However, OCT image annotation is knowledge-intensive and time-consuming, which impedes the training process of deep-learning-based classification models. Purpose: This study aims to develop a computer-aided diagnosis (CADx) approach to classifying in-vivo cervical OCT images based on self-supervised learning. Methods: In addition to high-level semantic features extracted by a convolutional neural network (CNN), the proposed CADx approach leverages unlabeled cervical OCT images' texture features learned by contrastive texture learning. We conducted ten-fold cross-validation on the OCT image dataset from a multi-center clinical study on 733 patients from China. Results: In a binary classification task for detecting high-risk diseases, including high-grade squamous intraepithelial lesion and cervical cancer, our method achieved an area-under-the-curve value of 0.9798 plus or minus 0.0157 with a sensitivity of 91.17 plus or minus 4.99% and a specificity of 93.96 plus or minus 4.72% for OCT image patches; also, it outperformed two out of four medical experts on the test set. Furthermore, our method achieved a 91.53% sensitivity and 97.37% specificity on an external validation dataset containing 287 3D OCT volumes from 118 Chinese patients in a new hospital using a cross-shaped threshold voting strategy. Conclusions: The proposed contrastive-learning-based CADx method outperformed the end-to-end CNN models and provided better interpretability based on texture features, which holds great potential to be used in the clinical protocol of "see-and-treat."



### Automatic Polyp Segmentation via Multi-scale Subtraction Network
- **Arxiv ID**: http://arxiv.org/abs/2108.05082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05082v1)
- **Published**: 2021-08-11 07:54:07+00:00
- **Updated**: 2021-08-11 07:54:07+00:00
- **Authors**: Xiaoqi Zhao, Lihe Zhang, Huchuan Lu
- **Comment**: This work was accepted by MICCAI 2021
- **Journal**: None
- **Summary**: More than 90\% of colorectal cancer is gradually transformed from colorectal polyps. In clinical practice, precise polyp segmentation provides important information in the early detection of colorectal cancer. Therefore, automatic polyp segmentation techniques are of great importance for both patients and doctors. Most existing methods are based on U-shape structure and use element-wise addition or concatenation to fuse different level features progressively in decoder. However, both the two operations easily generate plenty of redundant information, which will weaken the complementarity between different level features, resulting in inaccurate localization and blurred edges of polyps. To address this challenge, we propose a multi-scale subtraction network (MSNet) to segment polyp from colonoscopy image. Specifically, we first design a subtraction unit (SU) to produce the difference features between adjacent levels in encoder. Then, we pyramidally equip the SUs at different levels with varying receptive fields, thereby obtaining rich multi-scale difference information. In addition, we build a training-free network "LossNet" to comprehensively supervise the polyp-aware features from bottom layer to top layer, which drives the MSNet to capture the detailed and structural cues simultaneously. Extensive experiments on five benchmark datasets demonstrate that our MSNet performs favorably against most state-of-the-art methods under different evaluation metrics. Furthermore, MSNet runs at a real-time speed of $\sim$70fps when processing a $352 \times 352$ image. The source code will be publicly available at \url{https://github.com/Xiaoqi-Zhao-DLUT/MSNet}. \keywords{Colorectal Cancer \and Automatic Polyp Segmentation \and Subtraction \and LossNet.}



### Preventing Catastrophic Forgetting and Distribution Mismatch in Knowledge Distillation via Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2108.05698v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.05698v2)
- **Published**: 2021-08-11 08:11:08+00:00
- **Updated**: 2021-11-05 09:53:15+00:00
- **Authors**: Kuluhan Binici, Nam Trung Pham, Tulika Mitra, Karianto Leman
- **Comment**: Accepted by the 2022 Winter Conference on Applications of Computer
  Vision (WACV 2022)
- **Journal**: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV), 2022, pp. 663-671
- **Summary**: With the increasing popularity of deep learning on edge devices, compressing large neural networks to meet the hardware requirements of resource-constrained devices became a significant research direction. Numerous compression methodologies are currently being used to reduce the memory sizes and energy consumption of neural networks. Knowledge distillation (KD) is among such methodologies and it functions by using data samples to transfer the knowledge captured by a large model (teacher) to a smaller one(student). However, due to various reasons, the original training data might not be accessible at the compression stage. Therefore, data-free model compression is an ongoing research problem that has been addressed by various works. In this paper, we point out that catastrophic forgetting is a problem that can potentially be observed in existing data-free distillation methods. Moreover, the sample generation strategies in some of these methods could result in a mismatch between the synthetic and real data distributions. To prevent such problems, we propose a data-free KD framework that maintains a dynamic collection of generated samples over time. Additionally, we add the constraint of matching the real data distribution in sample generation strategies that target maximum information gain. Our experiments demonstrate that we can improve the accuracy of the student models obtained via KD when compared with state-of-the-art approaches on the SVHN, Fashion MNIST and CIFAR100 datasets.



### Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach
- **Arxiv ID**: http://arxiv.org/abs/2108.05094v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05094v1)
- **Published**: 2021-08-11 08:32:58+00:00
- **Updated**: 2021-08-11 08:32:58+00:00
- **Authors**: Aidan M. Swope, Xander H. Rudelis, Kyle T. Story
- **Comment**: 9 pages, 5 figures. Work completed in 2019 and submitted to ICLR in
  2020. Source code available at:
  https://github.com/descarteslabs/contrastive_sensor_fusion. Data available
  at:
  https://storage.cloud.google.com/public-published-datasets/osm_example_dataset.zip?folder=true&organizationId=272688069953
- **Journal**: None
- **Summary**: In the application of machine learning to remote sensing, labeled data is often scarce or expensive, which impedes the training of powerful models like deep convolutional neural networks. Although unlabeled data is abundant, recent self-supervised learning approaches are ill-suited to the remote sensing domain. In addition, most remote sensing applications currently use only a small subset of the multi-sensor, multi-channel information available, motivating the need for fused multi-sensor representations. We propose a new self-supervised training objective, Contrastive Sensor Fusion, which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. Using a dataset of 47 million unlabeled coterminous image triplets, we train an encoder to produce semantically meaningful representations from any possible combination of channels from the input sensors. These representations outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused. Our code is available at https://storage.cloud.google.com/public-published-datasets/csf_code.zip.



### Tracking Hand Hygiene Gestures with Leap Motion Controller
- **Arxiv ID**: http://arxiv.org/abs/2109.00884v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.00884v1)
- **Published**: 2021-08-11 08:48:39+00:00
- **Updated**: 2021-08-11 08:48:39+00:00
- **Authors**: Rashmi Bakshi, Jane Courtney, Damon Berry, Graham Gavin
- **Comment**: None
- **Journal**: None
- **Summary**: The process of hand washing, according to the WHO, is divided into stages with clearly defined two handed dynamic gestures. In this paper, videos of hand washing experts are segmented and analyzed with the goal of extracting their corresponding features. These features can be further processed in software to classify particular hand movements, determine whether the stages have been successfully completed by the user and also assess the quality of washing. Having identified the important features, a 3D gesture tracker, the Leap Motion Controller (LEAP), was used to track and detect the hand features associated with these stages. With the help of sequential programming and threshold values, the hand features were combined together to detect the initiation and completion of a sample WHO Stage 2 (Rub hands Palm to Palm). The LEAP provides accurate raw positional data for tracking single hand gestures and two hands in separation but suffers from occlusion when hands are in contact. Other than hand hygiene the approaches shown here can be applied in other biomedical applications requiring close hand gesture analysis.



### M3D-VTON: A Monocular-to-3D Virtual Try-On Network
- **Arxiv ID**: http://arxiv.org/abs/2108.05126v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05126v1)
- **Published**: 2021-08-11 10:05:17+00:00
- **Updated**: 2021-08-11 10:05:17+00:00
- **Authors**: Fuwei Zhao, Zhenyu Xie, Michael Kampffmeyer, Haoye Dong, Songfang Han, Tianxiang Zheng, Tao Zhang, Xiaodan Liang
- **Comment**: Accepted at ICCV 2021
- **Journal**: None
- **Summary**: Virtual 3D try-on can provide an intuitive and realistic view for online shopping and has a huge potential commercial value. However, existing 3D virtual try-on methods mainly rely on annotated 3D human shapes and garment templates, which hinders their applications in practical scenarios. 2D virtual try-on approaches provide a faster alternative to manipulate clothed humans, but lack the rich and realistic 3D representation. In this paper, we propose a novel Monocular-to-3D Virtual Try-On Network (M3D-VTON) that builds on the merits of both 2D and 3D approaches. By integrating 2D information efficiently and learning a mapping that lifts the 2D representation to 3D, we make the first attempt to reconstruct a 3D try-on mesh only taking the target clothing and a person image as inputs. The proposed M3D-VTON includes three modules: 1) The Monocular Prediction Module (MPM) that estimates an initial full-body depth map and accomplishes 2D clothes-person alignment through a novel two-stage warping procedure; 2) The Depth Refinement Module (DRM) that refines the initial body depth to produce more detailed pleat and face characteristics; 3) The Texture Fusion Module (TFM) that fuses the warped clothing with the non-target body part to refine the results. We also construct a high-quality synthesized Monocular-to-3D virtual try-on dataset, in which each person image is associated with a front and a back depth map. Extensive experiments demonstrate that the proposed M3D-VTON can manipulate and reconstruct the 3D human body wearing the given clothing with compelling details and is more efficient than other 3D approaches.



### Zero-Shot Day-Night Domain Adaptation with a Physics Prior
- **Arxiv ID**: http://arxiv.org/abs/2108.05137v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05137v2)
- **Published**: 2021-08-11 10:28:56+00:00
- **Updated**: 2021-10-11 14:20:43+00:00
- **Authors**: Attila Lengyel, Sourav Garg, Michael Milford, Jan C. van Gemert
- **Comment**: ICCV 2021 Oral presentation. Code, datasets and supplementary
  material: https://github.com/Attila94/CIConv
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2021, pp. 4399-4409
- **Summary**: We explore the zero-shot setting for day-night domain adaptation. The traditional domain adaptation setting is to train on one domain and adapt to the target domain by exploiting unlabeled data samples from the test set. As gathering relevant test data is expensive and sometimes even impossible, we remove any reliance on test data imagery and instead exploit a visual inductive prior derived from physics-based reflection models for domain adaptation. We cast a number of color invariant edge detectors as trainable layers in a convolutional neural network and evaluate their robustness to illumination changes. We show that the color invariant layer reduces the day-night distribution shift in feature map activations throughout the network. We demonstrate improved performance for zero-shot day to night domain adaptation on both synthetic as well as natural datasets in various tasks, including classification, segmentation and place recognition.



### Mounting Video Metadata on Transformer-based Language Model for Open-ended Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2108.05158v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.05158v1)
- **Published**: 2021-08-11 11:11:43+00:00
- **Updated**: 2021-08-11 11:11:43+00:00
- **Authors**: Donggeon Lee, Seongho Choi, Youwon Jang, Byoung-Tak Zhang
- **Comment**: 5 pages, 1 figure
- **Journal**: None
- **Summary**: Video question answering has recently received a lot of attention from multimodal video researchers. Most video question answering datasets are usually in the form of multiple-choice. But, the model for the multiple-choice task does not infer the answer. Rather it compares the answer candidates for picking the correct answer. Furthermore, it makes it difficult to extend to other tasks. In this paper, we challenge the existing multiple-choice video question answering by changing it to open-ended video question answering. To tackle open-ended question answering, we use the pretrained GPT2 model. The model is fine-tuned with video inputs and subtitles. An ablation study is performed by changing the existing DramaQA dataset to an open-ended question answering, and it shows that performance can be improved using video metadata.



### Efficient Surfel Fusion Using Normalised Information Distance
- **Arxiv ID**: http://arxiv.org/abs/2108.05163v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2108.05163v1)
- **Published**: 2021-08-11 11:28:31+00:00
- **Updated**: 2021-08-11 11:28:31+00:00
- **Authors**: Louis Gallagher, John B. McDonald
- **Comment**: 4 pages, 4 figures, presented at CVPR 2019 Workshop on 3D Scene
  Understanding for Vision, Graphics, and Robotics
- **Journal**: None
- **Summary**: We present a new technique that achieves a significant reduction in the quantity of measurements required for a fusion based dense 3D mapping system to converge to an accurate, de-noised surface reconstruction. This is achieved through the use of a Normalised Information Distance metric, that computes the novelty of the information contained in each incoming frame with respect to the reconstruction, and avoids fusing those frames that exceed a redundancy threshold. This provides a principled approach for opitmising the trade-off between surface reconstruction accuracy and the computational cost of processing frames. The technique builds upon the ElasticFusion (EF) algorithm where we report results of the technique's scalability and the accuracy of the resultant maps by applying it to both the ICL-NUIM and TUM RGB-D datasets. These results demonstrate the capabilities of the approach in performing accurate surface reconstructions whilst utilising a fraction of the frames when compared to the original EF algorithm.



### ProAI: An Efficient Embedded AI Hardware for Automotive Applications -- a Benchmark Study
- **Arxiv ID**: http://arxiv.org/abs/2108.05170v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.05170v2)
- **Published**: 2021-08-11 11:54:05+00:00
- **Updated**: 2021-09-09 13:23:34+00:00
- **Authors**: Sven Mantowsky, Falk Heuer, Syed Saqib Bukhari, Michael Keckeisen, Georg Schneider
- **Comment**: Accepted by IEEE International Conference on Computer Vision (ICCV)
  2021
- **Journal**: None
- **Summary**: Development in the field of Single Board Computers (SBC) have been increasing for several years. They provide a good balance between computing performance and power consumption which is usually required for mobile platforms, like application in vehicles for Advanced Driver Assistance Systems (ADAS) and Autonomous Driving (AD). However, there is an ever-increasing need of more powerful and efficient SBCs which can run power intensive Deep Neural Networks (DNNs) in real-time and can also satisfy necessary functional safety requirements such as Automotive Safety Integrity Level (ASIL). ProAI is being developed by ZF mainly to run powerful and efficient applications such as multitask DNNs and on top of that it also has the required safety certification for AD. In this work, we compare and discuss state of the art SBC on the basis of power intensive multitask DNN architecture called Multitask-CenterNet with respect to performance measures such as, FPS and power efficiency. As an automotive supercomputer, ProAI delivers an excellent combination of performance and efficiency, managing nearly twice the number of FPS per watt than a modern workstation laptop and almost four times compared to the Jetson Nano. Furthermore, it was also shown that there is still power in reserve for further and more complex tasks on the ProAI, based on the CPU and GPU utilization during the benchmark.



### Discriminative Distillation to Reduce Class Confusion in Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.05187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05187v1)
- **Published**: 2021-08-11 12:46:43+00:00
- **Updated**: 2021-08-11 12:46:43+00:00
- **Authors**: Changhong Zhong, Zhiying Cui, Ruixuan Wang, Wei-Shi Zheng
- **Comment**: arXiv admin note: text overlap with arXiv:2104.13614
- **Journal**: None
- **Summary**: Successful continual learning of new knowledge would enable intelligent systems to recognize more and more classes of objects. However, current intelligent systems often fail to correctly recognize previously learned classes of objects when updated to learn new classes. It is widely believed that such downgraded performance is solely due to the catastrophic forgetting of previously learned knowledge. In this study, we argue that the class confusion phenomena may also play a role in downgrading the classification performance during continual learning, i.e., the high similarity between new classes and any previously learned classes would also cause the classifier to make mistakes in recognizing these old classes, even if the knowledge of these old classes is not forgotten. To alleviate the class confusion issue, we propose a discriminative distillation strategy to help the classify well learn the discriminative features between confusing classes during continual learning. Experiments on multiple natural image classification tasks support that the proposed distillation strategy, when combined with existing methods, is effective in further improving continual learning.



### A Real-Time Online Learning Framework for Joint 3D Reconstruction and Semantic Segmentation of Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/2108.05246v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05246v2)
- **Published**: 2021-08-11 14:29:01+00:00
- **Updated**: 2021-12-28 22:37:30+00:00
- **Authors**: Davide Menini, Suryansh Kumar, Martin R. Oswald, Erik Sandstrom, Cristian Sminchisescu, Luc Van Gool
- **Comment**: Accepted for publication at IEEE Robotics and Automation Letters
  (RA-L), 2022. Draft info: 9 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: This paper presents a real-time online vision framework to jointly recover an indoor scene's 3D structure and semantic label. Given noisy depth maps, a camera trajectory, and 2D semantic labels at train time, the proposed deep neural network based approach learns to fuse the depth over frames with suitable semantic labels in the scene space. Our approach exploits the joint volumetric representation of the depth and semantics in the scene feature space to solve this task. For a compelling online fusion of the semantic labels and geometry in real-time, we introduce an efficient vortex pooling block while dropping the use of routing network in online depth fusion to preserve high-frequency surface details. We show that the context information provided by the semantics of the scene helps the depth fusion network learn noise-resistant features. Not only that, it helps overcome the shortcomings of the current online depth fusion method in dealing with thin object structures, thickening artifacts, and false surfaces. Experimental evaluation on the Replica dataset shows that our approach can perform depth fusion at 37 and 10 frames per second with an average reconstruction F-score of 88% and 91%, respectively, depending on the depth map resolution. Moreover, our model shows an average IoU score of 0.515 on the ScanNet 3D semantic benchmark leaderboard.



### Fog Simulation on Real LiDAR Point Clouds for 3D Object Detection in Adverse Weather
- **Arxiv ID**: http://arxiv.org/abs/2108.05249v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.05249v3)
- **Published**: 2021-08-11 14:37:54+00:00
- **Updated**: 2021-08-16 14:21:47+00:00
- **Authors**: Martin Hahner, Christos Sakaridis, Dengxin Dai, Luc Van Gool
- **Comment**: Camera-Ready Version for ICCV 2021
- **Journal**: None
- **Summary**: This work addresses the challenging task of LiDAR-based 3D object detection in foggy weather. Collecting and annotating data in such a scenario is very time, labor and cost intensive. In this paper, we tackle this problem by simulating physically accurate fog into clear-weather scenes, so that the abundant existing real datasets captured in clear weather can be repurposed for our task. Our contributions are twofold: 1) We develop a physically valid fog simulation method that is applicable to any LiDAR dataset. This unleashes the acquisition of large-scale foggy training data at no extra cost. These partially synthetic data can be used to improve the robustness of several perception methods, such as 3D object detection and tracking or simultaneous localization and mapping, on real foggy data. 2) Through extensive experiments with several state-of-the-art detection approaches, we show that our fog simulation can be leveraged to significantly improve the performance for 3D object detection in the presence of fog. Thus, we are the first to provide strong 3D object detection baselines on the Seeing Through Fog dataset. Our code is available at www.trace.ethz.ch/lidar_fog_simulation.



### Improving Single-Image Defocus Deblurring: How Dual-Pixel Images Help Through Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.05251v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05251v2)
- **Published**: 2021-08-11 14:45:15+00:00
- **Updated**: 2022-02-09 15:58:11+00:00
- **Authors**: Abdullah Abuolaim, Mahmoud Afifi, Michael S. Brown
- **Comment**: Published in the Winter Conference on Applications of Computer Vision
  2022 (WACV'22)
- **Journal**: None
- **Summary**: Many camera sensors use a dual-pixel (DP) design that operates as a rudimentary light field providing two sub-aperture views of a scene in a single capture. The DP sensor was developed to improve how cameras perform autofocus. Since the DP sensor's introduction, researchers have found additional uses for the DP data, such as depth estimation, reflection removal, and defocus deblurring. We are interested in the latter task of defocus deblurring. In particular, we propose a single-image deblurring network that incorporates the two sub-aperture views into a multi-task framework. Specifically, we show that jointly learning to predict the two DP views from a single blurry input image improves the network's ability to learn to deblur the image. Our experiments show this multi-task strategy achieves +1dB PSNR improvement over state-of-the-art defocus deblurring methods. In addition, our multi-task framework allows accurate DP-view synthesis (e.g., ~39dB PSNR) from the single input image. These high-quality DP views can be used for other DP-based applications, such as reflection removal. As part of this effort, we have captured a new dataset of 7,059 high-quality images to support our training for the DP-view synthesis task. Our dataset, code, and trained models are publicly available at https://github.com/Abdullah-Abuolaim/multi-task-defocus-deblurring-dual-pixel-nimat.



### Deep Learning Classification of Lake Zooplankton
- **Arxiv ID**: http://arxiv.org/abs/2108.05258v1
- **DOI**: 10.3389/fmicb.2021.746297
- **Categories**: **cs.CV**, cs.LG, q-bio.PE
- **Links**: [PDF](http://arxiv.org/pdf/2108.05258v1)
- **Published**: 2021-08-11 14:57:43+00:00
- **Updated**: 2021-08-11 14:57:43+00:00
- **Authors**: S. P. Kyathanahally, T. Hardeman, E. Merz, T. Kozakiewicz, M. Reyes, P. Isles, F. Pomati, M. Baity-Jesi
- **Comment**: Data and code links will be active/updated after publication
- **Journal**: Front. Microbiol. 12:746297 (2021)
- **Summary**: Plankton are effective indicators of environmental change and ecosystem health in freshwater habitats, but collection of plankton data using manual microscopic methods is extremely labor-intensive and expensive. Automated plankton imaging offers a promising way forward to monitor plankton communities with high frequency and accuracy in real-time. Yet, manual annotation of millions of images proposes a serious challenge to taxonomists. Deep learning classifiers have been successfully applied in various fields and provided encouraging results when used to categorize marine plankton images. Here, we present a set of deep learning models developed for the identification of lake plankton, and study several strategies to obtain optimal performances,which lead to operational prescriptions for users. To this aim, we annotated into 35 classes over 17900 images of zooplankton and large phytoplankton colonies, detected in Lake Greifensee (Switzerland) with the Dual Scripps Plankton Camera. Our best models were based on transfer learning and ensembling, which classified plankton images with 98% accuracy and 93% F1 score. When tested on freely available plankton datasets produced by other automated imaging tools (ZooScan, FlowCytobot and ISIIS), our models performed better than previously used models. Our annotated data, code and classification models are freely available online.



### Learning to Rearrange Voxels in Binary Segmentation Masks for Smooth Manifold Triangulation
- **Arxiv ID**: http://arxiv.org/abs/2108.05269v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CG, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.05269v1)
- **Published**: 2021-08-11 15:11:34+00:00
- **Updated**: 2021-08-11 15:11:34+00:00
- **Authors**: Jianning Li, Antonio Pepe, Christina Gsaxner, Yuan Jin, Jan Egger
- **Comment**: 18 pages, 11 figures, 1 table, 14 references
- **Journal**: None
- **Summary**: Medical images, especially volumetric images, are of high resolution and often exceed the capacity of standard desktop GPUs. As a result, most deep learning-based medical image analysis tasks require the input images to be downsampled, often substantially, before these can be fed to a neural network. However, downsampling can lead to a loss of image quality, which is undesirable especially in reconstruction tasks, where the fine geometric details need to be preserved. In this paper, we propose that high-resolution images can be reconstructed in a coarse-to-fine fashion, where a deep learning algorithm is only responsible for generating a coarse representation of the image, which consumes moderate GPU memory. For producing the high-resolution outcome, we propose two novel methods: learned voxel rearrangement of the coarse output and hierarchical image synthesis. Compared to the coarse output, the high-resolution counterpart allows for smooth surface triangulation, which can be 3D-printed in the highest possible quality. Experiments of this paper are carried out on the dataset of AutoImplant 2021 (https://autoimplant2021.grand-challenge.org/), a MICCAI challenge on cranial implant design. The dataset contains high-resolution skulls that can be viewed as 2D manifolds embedded in a 3D space. Codes associated with this study can be accessed at https://github.com/Jianningli/voxel_rearrangement.



### Instance-weighted Central Similarity for Multi-label Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2108.05274v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05274v5)
- **Published**: 2021-08-11 15:18:18+00:00
- **Updated**: 2022-09-20 09:21:35+00:00
- **Authors**: Zhiwei Zhang, Hanyu Peng
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Deep hashing has been widely applied to large-scale image retrieval by encoding high-dimensional data points into binary codes for efficient retrieval. Compared with pairwise/triplet similarity based hash learning, central similarity based hashing can more efficiently capture the global data distribution. For multi-label image retrieval, however, previous methods only use multiple hash centers with equal weights to generate one centroid as the learning target, which ignores the relationship between the weights of hash centers and the proportion of instance regions in the image. To address the above issue, we propose a two-step alternative optimization approach, Instance-weighted Central Similarity (ICS), to automatically learn the center weight corresponding to a hash code. Firstly, we apply the maximum entropy regularizer to prevent one hash center from dominating the loss function, and compute the center weights via projection gradient descent. Secondly, we update neural network parameters by standard back-propagation with fixed center weights. More importantly, the learned center weights can well reflect the proportion of foreground instances in the image. Our method achieves the state-of-the-art performance on the image retrieval benchmarks, and especially improves the mAP by 1.6%-6.4% on the MS COCO dataset.



### Few-Shot Segmentation with Global and Local Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.05293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05293v1)
- **Published**: 2021-08-11 15:52:22+00:00
- **Updated**: 2021-08-11 15:52:22+00:00
- **Authors**: Weide Liu, Zhonghua Wu, Henghui Ding, Fayao Liu, Jie Lin, Guosheng Lin
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we address the challenging task of few-shot segmentation. Previous few-shot segmentation methods mainly employ the information of support images as guidance for query image segmentation. Although some works propose to build cross-reference between support and query images, their extraction of query information still depends on the support images. We here propose to extract the information from the query itself independently to benefit the few-shot segmentation task. To this end, we first propose a prior extractor to learn the query information from the unlabeled images with our proposed global-local contrastive learning. Then, we extract a set of predetermined priors via this prior extractor. With the obtained priors, we generate the prior region maps for query images, which locate the objects, as guidance to perform cross interaction with support features. In such a way, the extraction of query information is detached from the support branch, overcoming the limitation by support, and could obtain more informative query clues to achieve better interaction. Without bells and whistles, the proposed approach achieves new state-of-the-art performance for the few-shot segmentation task on PASCAL-5$^{i}$ and COCO datasets.



### Hierarchical Conditional Flow: A Unified Framework for Image Super-Resolution and Image Rescaling
- **Arxiv ID**: http://arxiv.org/abs/2108.05301v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.05301v1)
- **Published**: 2021-08-11 16:11:01+00:00
- **Updated**: 2021-08-11 16:11:01+00:00
- **Authors**: Jingyun Liang, Andreas Lugmayr, Kai Zhang, Martin Danelljan, Luc Van Gool, Radu Timofte
- **Comment**: Accepted by ICCV2021. Code: https://github.com/JingyunLiang/HCFlow
- **Journal**: None
- **Summary**: Normalizing flows have recently demonstrated promising results for low-level vision tasks. For image super-resolution (SR), it learns to predict diverse photo-realistic high-resolution (HR) images from the low-resolution (LR) image rather than learning a deterministic mapping. For image rescaling, it achieves high accuracy by jointly modelling the downscaling and upscaling processes. While existing approaches employ specialized techniques for these two tasks, we set out to unify them in a single formulation. In this paper, we propose the hierarchical conditional flow (HCFlow) as a unified framework for image SR and image rescaling. More specifically, HCFlow learns a bijective mapping between HR and LR image pairs by modelling the distribution of the LR image and the rest high-frequency component simultaneously. In particular, the high-frequency component is conditional on the LR image in a hierarchical manner. To further enhance the performance, other losses such as perceptual loss and GAN loss are combined with the commonly used negative log-likelihood loss in training. Extensive experiments on general image SR, face image SR and image rescaling have demonstrated that the proposed HCFlow achieves state-of-the-art performance in terms of both quantitative metrics and visual quality.



### Mutual Affine Network for Spatially Variant Kernel Estimation in Blind Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2108.05302v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05302v1)
- **Published**: 2021-08-11 16:11:17+00:00
- **Updated**: 2021-08-11 16:11:17+00:00
- **Authors**: Jingyun Liang, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte
- **Comment**: Accepted by ICCV2021. Code: https://github.com/JingyunLiang/MANet
- **Journal**: None
- **Summary**: Existing blind image super-resolution (SR) methods mostly assume blur kernels are spatially invariant across the whole image. However, such an assumption is rarely applicable for real images whose blur kernels are usually spatially variant due to factors such as object motion and out-of-focus. Hence, existing blind SR methods would inevitably give rise to poor performance in real applications. To address this issue, this paper proposes a mutual affine network (MANet) for spatially variant kernel estimation. Specifically, MANet has two distinctive features. First, it has a moderate receptive field so as to keep the locality of degradation. Second, it involves a new mutual affine convolution (MAConv) layer that enhances feature expressiveness without increasing receptive field, model size and computation burden. This is made possible through exploiting channel interdependence, which applies each channel split with an affine transformation module whose input are the rest channel splits. Extensive experiments on synthetic and real images show that the proposed MANet not only performs favorably for both spatially variant and invariant kernel estimation, but also leads to state-of-the-art blind SR performance when combined with non-blind SR methods.



### ConvNets vs. Transformers: Whose Visual Representations are More Transferable?
- **Arxiv ID**: http://arxiv.org/abs/2108.05305v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05305v2)
- **Published**: 2021-08-11 16:20:38+00:00
- **Updated**: 2021-08-17 04:53:18+00:00
- **Authors**: Hong-Yu Zhou, Chixiang Lu, Sibei Yang, Yizhou Yu
- **Comment**: Accepted to appear in ICCV workshop on Multi-Task Learning in
  Computer Vision (DeepMTL)
- **Journal**: None
- **Summary**: Vision transformers have attracted much attention from computer vision researchers as they are not restricted to the spatial inductive bias of ConvNets. However, although Transformer-based backbones have achieved much progress on ImageNet classification, it is still unclear whether the learned representations are as transferable as or even more transferable than ConvNets' features. To address this point, we systematically investigate the transfer learning ability of ConvNets and vision transformers in 15 single-task and multi-task performance evaluations. Given the strong correlation between the performance of pre-trained models and transfer learning, we include 2 residual ConvNets (i.e., R-101x3 and R-152x4) and 3 Transformer-based visual backbones (i.e., ViT-B, ViT-L and Swin-B), which have close error rates on ImageNet, that indicate similar transfer learning performance on downstream datasets.   We observe consistent advantages of Transformer-based backbones on 13 downstream tasks (out of 15), including but not limited to fine-grained classification, scene recognition (classification, segmentation and depth estimation), open-domain classification, face recognition, etc. More specifically, we find that two ViT models heavily rely on whole network fine-tuning to achieve performance gains while Swin Transformer does not have such a requirement. Moreover, vision transformers behave more robustly in multi-task learning, i.e., bringing more improvements when managing mutually beneficial tasks and reducing performance losses when tackling irrelevant tasks. We hope our discoveries can facilitate the exploration and exploitation of vision transformers in the future.



### Video Transformer for Deepfake Detection with Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.05307v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05307v1)
- **Published**: 2021-08-11 16:22:56+00:00
- **Updated**: 2021-08-11 16:22:56+00:00
- **Authors**: Sohail A. Khan, Hang Dai
- **Comment**: Accepted at ACM International Conference on Multimedia, October 20 to
  24, 2021, Virtual Event, China
- **Journal**: None
- **Summary**: Face forgery by deepfake is widely spread over the internet and this raises severe societal concerns. In this paper, we propose a novel video transformer with incremental learning for detecting deepfake videos. To better align the input face images, we use a 3D face reconstruction method to generate UV texture from a single input face image. The aligned face image can also provide pose, eyes blink and mouth movement information that cannot be perceived in the UV texture image, so we use both face images and their UV texture maps to extract the image features. We present an incremental learning strategy to fine-tune the proposed model on a smaller amount of data and achieve better deepfake detection performance. The comprehensive experiments on various public deepfake datasets demonstrate that the proposed video transformer model with incremental learning achieves state-of-the-art performance in the deepfake video detection task with enhanced feature learning from the sequenced data.



### A Better Loss for Visual-Textual Grounding
- **Arxiv ID**: http://arxiv.org/abs/2108.05308v2
- **DOI**: 10.1145/3477314.3507047
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.05308v2)
- **Published**: 2021-08-11 16:26:54+00:00
- **Updated**: 2022-02-02 10:57:29+00:00
- **Authors**: Davide Rigoni, Luciano Serafini, Alessandro Sperduti
- **Comment**: None
- **Journal**: None
- **Summary**: Given a textual phrase and an image, the visual grounding problem is the task of locating the content of the image referenced by the sentence. It is a challenging task that has several real-world applications in human-computer interaction, image-text reference resolution, and video-text reference resolution. In the last years, several works have addressed this problem by proposing more and more large and complex models that try to capture visual-textual dependencies better than before. These models are typically constituted by two main components that focus on how to learn useful multi-modal features for grounding and how to improve the predicted bounding box of the visual mention, respectively. Finding the right learning balance between these two sub-tasks is not easy, and the current models are not necessarily optimal with respect to this issue. In this work, we propose a loss function based on bounding boxes classes probabilities that: (i) improves the bounding boxes selection; (ii) improves the bounding boxes coordinates prediction. Our model, although using a simple multi-modal feature fusion component, is able to achieve a higher accuracy than state-of-the-art models on two widely adopted datasets, reaching a better learning balance between the two sub-tasks mentioned above.



### Towards Interpretable Deep Networks for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2108.05312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05312v1)
- **Published**: 2021-08-11 16:43:45+00:00
- **Updated**: 2021-08-11 16:43:45+00:00
- **Authors**: Zunzhi You, Yi-Hsuan Tsai, Wei-Chen Chiu, Guanbin Li
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: Deep networks for Monocular Depth Estimation (MDE) have achieved promising performance recently and it is of great importance to further understand the interpretability of these networks. Existing methods attempt to provide posthoc explanations by investigating visual cues, which may not explore the internal representations learned by deep networks. In this paper, we find that some hidden units of the network are selective to certain ranges of depth, and thus such behavior can be served as a way to interpret the internal representations. Based on our observations, we quantify the interpretability of a deep MDE network by the depth selectivity of its hidden units. Moreover, we then propose a method to train interpretable MDE deep networks without changing their original architectures, by assigning a depth range for each unit to select. Experimental results demonstrate that our method is able to enhance the interpretability of deep MDE networks by largely improving the depth selectivity of their units, while not harming or even improving the depth estimation accuracy. We further provide a comprehensive analysis to show the reliability of selective units, the applicability of our method on different layers, models, and datasets, and a demonstration on analysis of model error. Source code and models are available at https://github.com/youzunzhi/InterpretableMDE .



### Two is a crowd: tracking relations in videos
- **Arxiv ID**: http://arxiv.org/abs/2108.05331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05331v1)
- **Published**: 2021-08-11 17:19:34+00:00
- **Updated**: 2021-08-11 17:19:34+00:00
- **Authors**: Artem Moskalev, Ivan Sosnovik, Arnold Smeulders
- **Comment**: None
- **Journal**: None
- **Summary**: Tracking multiple objects individually differs from tracking groups of related objects. When an object is a part of the group, its trajectory depends on the trajectories of the other group members. Most of the current state-of-the-art trackers follow the approach of tracking each object independently, with the mechanism to handle the overlapping trajectories where necessary. Such an approach does not take inter-object relations into account, which may cause unreliable tracking for the members of the groups, especially in crowded scenarios, where individual cues become unreliable due to occlusions. To overcome these limitations and to extend such trackers to crowded scenes, we propose a plug-in Relation Encoding Module (REM). REM encodes relations between tracked objects by running a message passing over a corresponding spatio-temporal graph, computing relation embeddings for the tracked objects. Our experiments on MOT17 and MOT20 demonstrate that the baseline tracker improves its results after a simple extension with REM. The proposed module allows for tracking severely or even fully occluded objects by utilizing relational cues.



### Person Re-identification via Attention Pyramid
- **Arxiv ID**: http://arxiv.org/abs/2108.05340v1
- **DOI**: 10.1109/TIP.2021.3107211
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.05340v1)
- **Published**: 2021-08-11 17:33:36+00:00
- **Updated**: 2021-08-11 17:33:36+00:00
- **Authors**: Guangyi Chen, Tianpei Gu, Jiwen Lu, Jin-An Bao, Jie Zhou
- **Comment**: Accepted by IEEE Transcations on Image Processing. Code available at
  https://github.com/CHENGY12/APNet
- **Journal**: None
- **Summary**: In this paper, we propose an attention pyramid method for person re-identification. Unlike conventional attention-based methods which only learn a global attention map, our attention pyramid exploits the attention regions in a multi-scale manner because human attention varies with different scales. Our attention pyramid imitates the process of human visual perception which tends to notice the foreground person over the cluttered background, and further focus on the specific color of the shirt with close observation. Specifically, we describe our attention pyramid by a "split-attend-merge-stack" principle. We first split the features into multiple local parts and learn the corresponding attentions. Then, we merge local attentions and stack these merged attentions with the residual connection as an attention pyramid. The proposed attention pyramid is a lightweight plug-and-play module that can be applied to off-the-shelf models. We implement our attention pyramid method in two different attention mechanisms including channel-wise attention and spatial attention. We evaluate our method on four largescale person re-identification benchmarks including Market-1501, DukeMTMC, CUHK03, and MSMT17. Experimental results demonstrate the superiority of our method, which outperforms the state-of-the-art methods by a large margin with limited computational cost.



### An Approach to Partial Observability in Games: Learning to Both Act and Observe
- **Arxiv ID**: http://arxiv.org/abs/2108.05701v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.GT
- **Links**: [PDF](http://arxiv.org/pdf/2108.05701v1)
- **Published**: 2021-08-11 17:45:56+00:00
- **Updated**: 2021-08-11 17:45:56+00:00
- **Authors**: Elizabeth Gilmour, Noah Plotkin, Leslie Smith
- **Comment**: 5 pages, 5 figures, to be published in proceedings of IEEE Conference
  on Games 2021
- **Journal**: None
- **Summary**: Reinforcement learning (RL) is successful at learning to play games where the entire environment is visible. However, RL approaches are challenged in complex games like Starcraft II and in real-world environments where the entire environment is not visible. In these more complex games with more limited visual information, agents must choose where to look and how to optimally use their limited visual information in order to succeed at the game. We verify that with a relatively simple model the agent can learn where to look in scenarios with a limited visual bandwidth. We develop a method for masking part of the environment in Atari games to force the RL agent to learn both where to look and how to play the game in order to study where the RL agent learns to look. In addition, we develop a neural network architecture and method for allowing the agent to choose where to look and what action to take in the Pong game. Further, we analyze the strategies the agent learns to better understand how the RL agent learns to play the game.



### The Pitfalls of Sample Selection: A Case Study on Lung Nodule Classification
- **Arxiv ID**: http://arxiv.org/abs/2108.05386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05386v1)
- **Published**: 2021-08-11 18:07:07+00:00
- **Updated**: 2021-08-11 18:07:07+00:00
- **Authors**: Vasileios Baltatzis, Kyriaki-Margarita Bintsi, Loic Le Folgoc, Octavio E. Martinez Manzanera, Sam Ellis, Arjun Nair, Sujal Desai, Ben Glocker, Julia A. Schnabel
- **Comment**: Accepted at PRIME, MICCAI 2021
- **Journal**: None
- **Summary**: Using publicly available data to determine the performance of methodological contributions is important as it facilitates reproducibility and allows scrutiny of the published results. In lung nodule classification, for example, many works report results on the publicly available LIDC dataset. In theory, this should allow a direct comparison of the performance of proposed methods and assess the impact of individual contributions. When analyzing seven recent works, however, we find that each employs a different data selection process, leading to largely varying total number of samples and ratios between benign and malignant cases. As each subset will have different characteristics with varying difficulty for classification, a direct comparison between the proposed methods is thus not always possible, nor fair. We study the particular effect of truthing when aggregating labels from multiple experts. We show that specific choices can have severe impact on the data distribution where it may be possible to achieve superior performance on one sample distribution but not on another. While we show that we can further improve on the state-of-the-art on one sample selection, we also find that on a more challenging sample selection, on the same database, the more advanced models underperform with respect to very simple baseline methods, highlighting that the selected data distribution may play an even more important role than the model architecture. This raises concerns about the validity of claimed methodological contributions. We believe the community should be aware of these pitfalls and make recommendations on how these can be avoided in future work.



### Voxel-level Importance Maps for Interpretable Brain Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/2108.05388v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.05388v1)
- **Published**: 2021-08-11 18:08:09+00:00
- **Updated**: 2021-08-11 18:08:09+00:00
- **Authors**: Kyriaki-Margarita Bintsi, Vasileios Baltatzis, Alexander Hammers, Daniel Rueckert
- **Comment**: Accepted at iMIMIC, MICCAI 2021
- **Journal**: None
- **Summary**: Brain aging, and more specifically the difference between the chronological and the biological age of a person, may be a promising biomarker for identifying neurodegenerative diseases. For this purpose accurate prediction is important but the localisation of the areas that play a significant role in the prediction is also crucial, in order to gain clinicians' trust and reassurance about the performance of a prediction model. Most interpretability methods are focused on classification tasks and cannot be directly transferred to regression tasks. In this study, we focus on the task of brain age regression from 3D brain Magnetic Resonance (MR) images using a Convolutional Neural Network, termed prediction model. We interpret its predictions by extracting importance maps, which discover the parts of the brain that are the most important for brain age. In order to do so, we assume that voxels that are not useful for the regression are resilient to noise addition. We implement a noise model which aims to add as much noise as possible to the input without harming the performance of the prediction model. We average the importance maps of the subjects and end up with a population-based importance map, which displays the regions of the brain that are influential for the task. We test our method on 13,750 3D brain MR images from the UK Biobank, and our findings are consistent with the existing neuropathology literature, highlighting that the hippocampus and the ventricles are the most relevant regions for brain aging.



### Deep PET/CT fusion with Dempster-Shafer theory for lymphoma segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.05422v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.05422v1)
- **Published**: 2021-08-11 19:24:40+00:00
- **Updated**: 2021-08-11 19:24:40+00:00
- **Authors**: Ling Huang, Thierry Denoeux, David Tonnelet, Pierre Decazes, Su Ruan
- **Comment**: MICCAI 2021 Workshop MLMI
- **Journal**: None
- **Summary**: Lymphoma detection and segmentation from whole-body Positron Emission Tomography/Computed Tomography (PET/CT) volumes are crucial for surgical indication and radiotherapy. Designing automatic segmentation methods capable of effectively exploiting the information from PET and CT as well as resolving their uncertainty remain a challenge. In this paper, we propose an lymphoma segmentation model using an UNet with an evidential PET/CT fusion layer. Single-modality volumes are trained separately to get initial segmentation maps and an evidential fusion layer is proposed to fuse the two pieces of evidence using Dempster-Shafer theory (DST). Moreover, a multi-task loss function is proposed: in addition to the use of the Dice loss for PET and CT segmentation, a loss function based on the concordance between the two segmentation is added to constrain the final segmentation. We evaluate our proposal on a database of polycentric PET/CT volumes of patients treated for lymphoma, delineated by the experts. Our method get accurate segmentation results with Dice score of 0.726, without any user interaction. Quantitative results show that our method is superior to the state-of-the-art methods.



### Learning Bias-Invariant Representation by Cross-Sample Mutual Information Minimization
- **Arxiv ID**: http://arxiv.org/abs/2108.05449v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.05449v2)
- **Published**: 2021-08-11 21:17:02+00:00
- **Updated**: 2021-08-13 01:45:27+00:00
- **Authors**: Wei Zhu, Haitian Zheng, Haofu Liao, Weijian Li, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning algorithms mine knowledge from the training data and thus would likely inherit the dataset's bias information. As a result, the obtained model would generalize poorly and even mislead the decision process in real-life applications. We propose to remove the bias information misused by the target task with a cross-sample adversarial debiasing (CSAD) method. CSAD explicitly extracts target and bias features disentangled from the latent representation generated by a feature extractor and then learns to discover and remove the correlation between the target and bias features. The correlation measurement plays a critical role in adversarial debiasing and is conducted by a cross-sample neural mutual information estimator. Moreover, we propose joint content and local structural representation learning to boost mutual information estimation for better performance. We conduct thorough experiments on publicly available datasets to validate the advantages of the proposed method over state-of-the-art approaches.



### SIDER: Single-Image Neural Optimization for Facial Geometric Detail Recovery
- **Arxiv ID**: http://arxiv.org/abs/2108.05465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.05465v1)
- **Published**: 2021-08-11 22:34:53+00:00
- **Updated**: 2021-08-11 22:34:53+00:00
- **Authors**: Aggelina Chatziagapi, ShahRukh Athar, Francesc Moreno-Noguer, Dimitris Samaras
- **Comment**: version 1.0.0
- **Journal**: None
- **Summary**: We present SIDER(Single-Image neural optimization for facial geometric DEtail Recovery), a novel photometric optimization method that recovers detailed facial geometry from a single image in an unsupervised manner. Inspired by classical techniques of coarse-to-fine optimization and recent advances in implicit neural representations of 3D shape, SIDER combines a geometry prior based on statistical models and Signed Distance Functions (SDFs) to recover facial details from single images. First, it estimates a coarse geometry using a morphable model represented as an SDF. Next, it reconstructs facial geometry details by optimizing a photometric loss with respect to the ground truth image. In contrast to prior work, SIDER does not rely on any dataset priors and does not require additional supervision from multiple views, lighting changes or ground truth 3D shape. Extensive qualitative and quantitative evaluation demonstrates that our method achieves state-of-the-art on facial geometric detail recovery, using only a single in-the-wild image.



