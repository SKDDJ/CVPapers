# Arxiv Papers in cs.CV on 2021-08-27
### Matching Underwater Sonar Images by the Learned Descriptor Based on Style Transfer Method
- **Arxiv ID**: http://arxiv.org/abs/2108.12072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12072v1)
- **Published**: 2021-08-27 00:32:52+00:00
- **Updated**: 2021-08-27 00:32:52+00:00
- **Authors**: Xiaoteng Zhou, Changli Yu, Xin Yuan, Citong Luo
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a method that combines the style transfer technique and the learned descriptor to enhance the matching performances of underwater sonar images. In the field of underwater vision, sonar is currently the most effective long-distance detection sensor, it has excellent performances in map building and target search tasks. However, the traditional image matching algorithms are all developed based on optical images. In order to solve this contradiction, the style transfer method is used to convert the sonar images into optical styles, and at the same time, the learned descriptor with excellent expressiveness for sonar images matching is introduced. Experiments show that this method significantly enhances the matching quality of sonar images. In addition, it also provides new ideas for the preprocessing of underwater sonar images by using the style transfer approach.



### Detection and Continual Learning of Novel Face Presentation Attacks
- **Arxiv ID**: http://arxiv.org/abs/2108.12081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12081v1)
- **Published**: 2021-08-27 01:33:52+00:00
- **Updated**: 2021-08-27 01:33:52+00:00
- **Authors**: Mohammad Rostami, Leonidas Spinoulas, Mohamed Hussein, Joe Mathai, Wael Abd-Almageed
- **Comment**: None
- **Journal**: 2021 International Conference on Computer Vision
- **Summary**: Advances in deep learning, combined with availability of large datasets, have led to impressive improvements in face presentation attack detection research. However, state-of-the-art face antispoofing systems are still vulnerable to novel types of attacks that are never seen during training. Moreover, even if such attacks are correctly detected, these systems lack the ability to adapt to newly encountered attacks. The post-training ability of continually detecting new types of attacks and self-adaptation to identify these attack types, after the initial detection phase, is highly appealing. In this paper, we enable a deep neural network to detect anomalies in the observed input data points as potential new types of attacks by suppressing the confidence-level of the network outside the training samples' distribution. We then use experience replay to update the model to incorporate knowledge about new types of attacks without forgetting the past learned attack types. Experimental results are provided to demonstrate the effectiveness of the proposed method on two benchmark datasets as well as a newly introduced dataset which exhibits a large variety of attack types.



### Deep Denoising Method for Side Scan Sonar Images without High-quality Reference Data
- **Arxiv ID**: http://arxiv.org/abs/2108.12083v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12083v1)
- **Published**: 2021-08-27 01:49:23+00:00
- **Updated**: 2021-08-27 01:49:23+00:00
- **Authors**: Xiaoteng Zhou, Changli Yu, Xin Yuan, Citong Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Subsea images measured by the side scan sonars (SSSs) are necessary visual data in the process of deep-sea exploration by using the autonomous underwater vehicles (AUVs). They could vividly reflect the topography of the seabed, but usually accompanied by complex and severe noise. This paper proposes a deep denoising method for SSS images without high-quality reference data, which uses one single noise SSS image to perform self-supervised denoising. Compared with the classical artificially designed filters, the deep denoising method shows obvious advantages. The denoising experiments are performed on the real seabed SSS images, and the results demonstrate that our proposed method could effectively reduce the noise on the SSS image while minimizing the image quality and detail loss.



### FOVEA: Foveated Image Magnification for Autonomous Navigation
- **Arxiv ID**: http://arxiv.org/abs/2108.12102v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12102v2)
- **Published**: 2021-08-27 03:07:55+00:00
- **Updated**: 2021-10-11 09:38:15+00:00
- **Authors**: Chittesh Thavamani, Mengtian Li, Nicolas Cebron, Deva Ramanan
- **Comment**: ICCV 2021. Code can be found on the project page at
  https://www.cs.cmu.edu/~mengtial/proj/fovea/
- **Journal**: None
- **Summary**: Efficient processing of high-res video streams is safety-critical for many robotics applications such as autonomous driving. To maintain real-time performance, many practical systems downsample the video stream. But this can hurt downstream tasks such as (small) object detection. Instead, we take inspiration from biological vision systems that allocate more foveal "pixels" to salient parts of the scene. We introduce FOVEA, an approach for intelligent downsampling that ensures salient image regions remain "magnified" in the downsampled output. Given a high-res image, FOVEA applies a differentiable resampling layer that outputs a small fixed-size image canvas, which is then processed with a differentiable vision module (e.g., object detection network), whose output is then differentiably backward mapped onto the original image size. The key idea is to resample such that background pixels can make room for salient pixels of interest. In order to ensure the overall pipeline remains efficient, FOVEA makes use of cheap and readily available cues for saliency, including dataset-specific spatial priors or temporal priors computed from object predictions in the recent past. On the autonomous driving datasets Argoverse-HD and BDD100K, our proposed method boosts the detection AP over standard Faster R-CNN, both with and without finetuning. Without any noticeable increase in compute, we improve accuracy on small objects by over 2x without degrading performance on large objects. Finally, FOVEA sets a new record for streaming AP (from 17.8 to 23.0 on a GTX 1080 Ti GPU), a metric designed to capture both accuracy and latency.



### Binocular Mutual Learning for Improving Few-shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2108.12104v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12104v1)
- **Published**: 2021-08-27 03:17:22+00:00
- **Updated**: 2021-08-27 03:17:22+00:00
- **Authors**: Ziqi Zhou, Xi Qiu, Jiangtao Xie, Jianan Wu, Chi Zhang
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Most of the few-shot learning methods learn to transfer knowledge from datasets with abundant labeled data (i.e., the base set). From the perspective of class space on base set, existing methods either focus on utilizing all classes under a global view by normal pretraining, or pay more attention to adopt an episodic manner to train meta-tasks within few classes in a local view. However, the interaction of the two views is rarely explored. As the two views capture complementary information, we naturally think of the compatibility of them for achieving further performance gains. Inspired by the mutual learning paradigm and binocular parallax, we propose a unified framework, namely Binocular Mutual Learning (BML), which achieves the compatibility of the global view and the local view through both intra-view and cross-view modeling. Concretely, the global view learns in the whole class space to capture rich inter-class relationships. Meanwhile, the local view learns in the local class space within each episode, focusing on matching positive pairs correctly. In addition, cross-view mutual interaction further promotes the collaborative learning and the implicit exploration of useful knowledge from each other. During meta-test, binocular embeddings are aggregated together to support decision-making, which greatly improve the accuracy of classification. Extensive experiments conducted on multiple benchmarks including cross-domain validation confirm the effectiveness of our method.



### Recognition Awareness: An Application of Latent Cognizance to Open-Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.12115v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12115v2)
- **Published**: 2021-08-27 04:41:41+00:00
- **Updated**: 2021-10-29 05:41:00+00:00
- **Authors**: Tatpong Katanyukul, Pisit Nakjai
- **Comment**: 27 pages
- **Journal**: None
- **Summary**: This study investigates an application of a new probabilistic interpretation of a softmax output to Open-Set Recognition (OSR). Softmax is a mechanism wildly used in classification and object recognition.   However, a softmax mechanism forces a model to operate under a closed-set paradigm, i.e., to predict an object class out of a set of pre-defined labels.   This characteristic contributes to efficacy in classification, but poses a risk of non-sense prediction in object recognition.   Object recognition is often operated under a dynamic and diverse condition.   A foreign object -- an object of any unprepared class -- can be encountered at any time.   OSR is intended to address an issue of identifying a foreign object in object recognition.   Based on Bayes theorem and the emphasis of conditioning on the context, softmax inference has been re-interpreted.   This re-interpretation has led to a new approach to OSR, called Latent Cognizance (LC). Our investigation employs various scenarios, using Imagenet 2012 dataset as well as fooling and open-set images. The findings support LC hypothesis and show its effectiveness on OSR.



### Densely-Populated Traffic Detection using YOLOv5 and Non-Maximum Suppression Ensembling
- **Arxiv ID**: http://arxiv.org/abs/2108.12118v1
- **DOI**: 10.1007/978-981-16-6636-0_43
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.12118v1)
- **Published**: 2021-08-27 04:58:45+00:00
- **Updated**: 2021-08-27 04:58:45+00:00
- **Authors**: Raian Rahman, Zadid Bin Azad, Md. Bakhtiar Hasan
- **Comment**: 13 pages, 4 figures, conference: International Conference on Big
  Data, IoT and Machine Learning 2021 (BIM 2021)
- **Journal**: None
- **Summary**: Vehicular object detection is the heart of any intelligent traffic system. It is essential for urban traffic management. R-CNN, Fast R-CNN, Faster R-CNN and YOLO were some of the earlier state-of-the-art models. Region based CNN methods have the problem of higher inference time which makes it unrealistic to use the model in real-time. YOLO on the other hand struggles to detect small objects that appear in groups. In this paper, we propose a method that can locate and classify vehicular objects from a given densely crowded image using YOLOv5. The shortcoming of YOLO was solved my ensembling 4 different models. Our proposed model performs well on images taken from both top view and side view of the street in both day and night. The performance of our proposed model was measured on Dhaka AI dataset which contains densely crowded vehicular images. Our experiment shows that our model achieved mAP@0.5 of 0.458 with inference time of 0.75 sec which outperforms other state-of-the-art models on performance. Hence, the model can be implemented in the street for real-time traffic detection which can be used for traffic control and data collection.



### DAE-GAN: Dynamic Aspect-aware GAN for Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2108.12141v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.12141v1)
- **Published**: 2021-08-27 07:20:34+00:00
- **Updated**: 2021-08-27 07:20:34+00:00
- **Authors**: Shulan Ruan, Yong Zhang, Kun Zhang, Yanbo Fan, Fan Tang, Qi Liu, Enhong Chen
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Text-to-image synthesis refers to generating an image from a given text description, the key goal of which lies in photo realism and semantic consistency. Previous methods usually generate an initial image with sentence embedding and then refine it with fine-grained word embedding. Despite the significant progress, the 'aspect' information (e.g., red eyes) contained in the text, referring to several words rather than a word that depicts 'a particular part or feature of something', is often ignored, which is highly helpful for synthesizing image details. How to make better utilization of aspect information in text-to-image synthesis still remains an unresolved challenge. To address this problem, in this paper, we propose a Dynamic Aspect-awarE GAN (DAE-GAN) that represents text information comprehensively from multiple granularities, including sentence-level, word-level, and aspect-level. Moreover, inspired by human learning behaviors, we develop a novel Aspect-aware Dynamic Re-drawer (ADR) for image refinement, in which an Attended Global Refinement (AGR) module and an Aspect-aware Local Refinement (ALR) module are alternately employed. AGR utilizes word-level embedding to globally enhance the previously generated image, while ALR dynamically employs aspect-level embedding to refine image details from a local perspective. Finally, a corresponding matching loss function is designed to ensure the text-image semantic consistency at different levels. Extensive experiments on two well-studied and publicly available datasets (i.e., CUB-200 and COCO) demonstrate the superiority and rationality of our method.



### A Matching Algorithm based on Image Attribute Transfer and Local Features for Underwater Acoustic and Optical Images
- **Arxiv ID**: http://arxiv.org/abs/2108.12151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12151v1)
- **Published**: 2021-08-27 07:50:09+00:00
- **Updated**: 2021-08-27 07:50:09+00:00
- **Authors**: Xiaoteng Zhou, Changli Yu, Xin Yuan, Citong Luo
- **Comment**: None
- **Journal**: None
- **Summary**: In the field of underwater vision research, image matching between the sonar sensors and optical cameras has always been a challenging problem. Due to the difference in the imaging mechanism between them, which are the gray value, texture, contrast, etc. of the acoustic images and the optical images are also variant in local locations, which makes the traditional matching method based on the optical image invalid. Coupled with the difficulties and high costs of underwater data acquisition, it further affects the research process of acousto-optic data fusion technology. In order to maximize the use of underwater sensor data and promote the development of multi-sensor information fusion (MSIF), this study applies the image attribute transfer method based on deep learning approach to solve the problem of acousto-optic image matching, the core of which is to eliminate the imaging differences between them as much as possible. At the same time, the advanced local feature descriptor is introduced to solve the challenging acousto-optic matching problem. Experimental results show that our proposed method could preprocess acousto-optic images effectively and obtain accurate matching results. Additionally, the method is based on the combination of image depth semantic layer, and it could indirectly display the local feature matching relationship between original image pair, which provides a new solution to the underwater multi-sensor image matching problem.



### Anomaly Detection of Defect using Energy of Point Pattern Features within Random Finite Set Framework
- **Arxiv ID**: http://arxiv.org/abs/2108.12159v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.12159v1)
- **Published**: 2021-08-27 08:06:37+00:00
- **Updated**: 2021-08-27 08:06:37+00:00
- **Authors**: Ammar Mansoor Kamoona, Amirali Khodadadian Gostar, Alireza Bab-Hadiashar, Reza Hoseinnezhad
- **Comment**: to be submitted to TII journal, 17pages
- **Journal**: None
- **Summary**: In this paper, we propose an efficient approach for industrial defect detection that is modeled based on anomaly detection using point pattern data. Most recent works use \textit{global features} for feature extraction to summarize image content. However, global features are not robust against lighting and viewpoint changes and do not describe the image's geometrical information to be fully utilized in the manufacturing industry. To the best of our knowledge, we are the first to propose using transfer learning of local/point pattern features to overcome these limitations and capture geometrical information of the image regions. We model these local/point pattern features as a random finite set (RFS). In addition we propose RFS energy, in contrast to RFS likelihood as anomaly score. The similarity distribution of point pattern features of the normal sample has been modeled as a multivariate Gaussian. Parameters learning of the proposed RFS energy does not require any heavy computation. We evaluate the proposed approach on the MVTec AD dataset, a multi-object defect detection dataset. Experimental results show the outstanding performance of our proposed approach compared to the state-of-the-art methods, and the proposed RFS energy outperforms the state-of-the-art in the few shot learning settings.



### LassoLayer: Nonlinear Feature Selection by Switching One-to-one Links
- **Arxiv ID**: http://arxiv.org/abs/2108.12165v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12165v1)
- **Published**: 2021-08-27 08:16:40+00:00
- **Updated**: 2021-08-27 08:16:40+00:00
- **Authors**: Akihito Sudo, Teng Teck Hou, Masaki Yamaguchi, Yoshinori Tone
- **Comment**: None
- **Journal**: None
- **Summary**: Along with the desire to address more complex problems, feature selection methods have gained in importance. Feature selection methods can be classified into wrapper method, filter method, and embedded method. Being a powerful embedded feature selection method, Lasso has attracted the attention of many researchers. However, as a linear approach, the applicability of Lasso has been limited. In this work, we propose LassoLayer that is one-to-one connected and trained by L1 optimization, which work to drop out unnecessary units for prediction. For nonlinear feature selections, we build LassoMLP: the network equipped with LassoLayer as its first layer. Because we can insert LassoLayer in any network structure, it can harness the strength of neural network suitable for tasks where feature selection is needed. We evaluate LassoMLP in feature selection with regression and classification tasks. LassoMLP receives features including considerable numbers of noisy factors that is harmful for overfitting. In the experiments using MNIST dataset, we confirm that LassoMLP outperforms the state-of-the-art method.



### CoCo DistillNet: a Cross-layer Correlation Distillation Network for Pathological Gastric Cancer Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.12173v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12173v2)
- **Published**: 2021-08-27 08:36:00+00:00
- **Updated**: 2021-11-12 13:44:42+00:00
- **Authors**: Wenxuan Zou, Muyi Sun
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep convolutional neural networks have made significant advances in pathology image segmentation. However, pathology image segmentation encounters with a dilemma in which the higher-performance networks generally require more computational resources and storage. This phenomenon limits the employment of high-accuracy networks in real scenes due to the inherent high-resolution of pathological images. To tackle this problem, we propose CoCo DistillNet, a novel Cross-layer Correlation (CoCo) knowledge distillation network for pathological gastric cancer segmentation. Knowledge distillation, a general technique which aims at improving the performance of a compact network through knowledge transfer from a cumbersome network. Concretely, our CoCo DistillNet models the correlations of channel-mixed spatial similarity between different layers and then transfers this knowledge from a pre-trained cumbersome teacher network to a non-trained compact student network. In addition, we also utilize the adversarial learning strategy to further prompt the distilling procedure which is called Adversarial Distillation (AD). Furthermore, to stabilize our training procedure, we make the use of the unsupervised Paraphraser Module (PM) to boost the knowledge paraphrase in the teacher network. As a result, extensive experiments conducted on the Gastric Cancer Segmentation Dataset demonstrate the prominent ability of CoCo DistillNet which achieves state-of-the-art performance.



### Rethinking the Misalignment Problem in Dense Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.12176v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12176v5)
- **Published**: 2021-08-27 08:40:37+00:00
- **Updated**: 2022-05-01 03:49:46+00:00
- **Authors**: Yang Yang, Min Li, Bo Meng, Junxing Ren, Degang Sun, Zihao Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection aims to localize and classify the objects in a given image, and these two tasks are sensitive to different object regions. Therefore, some locations predict high-quality bounding boxes but low classification scores, and some locations are quite the opposite. A misalignment exists between the two tasks, and their features are spatially entangled. In order to solve the misalignment problem, we propose a plug-in Spatial-disentangled and Task-aligned operator (SALT). By predicting two task-aware point sets that are located in each task's sensitive regions, SALT can reassign features from those regions and align them to the corresponding anchor point. Therefore, features for the two tasks are spatially aligned and disentangled. To minimize the difference between the two regression stages, we propose a Self-distillation regression (SDR) loss that can transfer knowledge from the refined regression results to the coarse regression results. On the basis of SALT and SDR loss, we propose SALT-Net, which explicitly exploits task-aligned point-set features for accurate detection results. Extensive experiments on the MS-COCO dataset show that our proposed methods can consistently boost different state-of-the-art dense detectors by $\sim$2 AP. Notably, SALT-Net with Res2Net-101-DCN backbone achieves 53.8 AP on the MS-COCO test-dev.



### MultiSiam: Self-supervised Multi-instance Siamese Representation Learning for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2108.12178v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12178v1)
- **Published**: 2021-08-27 08:47:01+00:00
- **Updated**: 2021-08-27 08:47:01+00:00
- **Authors**: Kai Chen, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Autonomous driving has attracted much attention over the years but turns out to be harder than expected, probably due to the difficulty of labeled data collection for model training. Self-supervised learning (SSL), which leverages unlabeled data only for representation learning, might be a promising way to improve model performance. Existing SSL methods, however, usually rely on the single-centric-object guarantee, which may not be applicable for multi-instance datasets such as street scenes. To alleviate this limitation, we raise two issues to solve: (1) how to define positive samples for cross-view consistency and (2) how to measure similarity in multi-instance circumstances. We first adopt an IoU threshold during random cropping to transfer global-inconsistency to local-consistency. Then, we propose two feature alignment methods to enable 2D feature maps for multi-instance similarity measurement. Additionally, we adopt intra-image clustering with self-attention for further mining intra-image similarity and translation-invariance. Experiments show that, when pre-trained on Waymo dataset, our method called Multi-instance Siamese Network (MultiSiam) remarkably improves generalization ability and achieves state-of-the-art transfer performance on autonomous driving benchmarks, including Cityscapes and BDD100K, while existing SSL counterparts like MoCo, MoCo-v2, and BYOL show significant performance drop. By pre-training on SODA10M, a large-scale autonomous driving dataset, MultiSiam exceeds the ImageNet pre-trained MoCo-v2, demonstrating the potential of domain-specific pre-training. Code will be available at https://github.com/KaiChen1998/MultiSiam.



### TIMo -- A Dataset for Indoor Building Monitoring with a Time-of-Flight Camera
- **Arxiv ID**: http://arxiv.org/abs/2108.12196v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12196v1)
- **Published**: 2021-08-27 09:33:11+00:00
- **Updated**: 2021-08-27 09:33:11+00:00
- **Authors**: Pascal Schneider, Yuriy Anisimov, Raisul Islam, Bruno Mirbach, Jason Rambach, Frédéric Grandidier, Didier Stricker
- **Comment**: None
- **Journal**: None
- **Summary**: We present TIMo (Time-of-flight Indoor Monitoring), a dataset for video-based monitoring of indoor spaces captured using a time-of-flight (ToF) camera. The resulting depth videos feature people performing a set of different predefined actions, for which we provide detailed annotations. Person detection for people counting and anomaly detection are the two targeted applications. Most existing surveillance video datasets provide either grayscale or RGB videos. Depth information, on the other hand, is still a rarity in this class of datasets in spite of being popular and much more common in other research fields within computer vision. Our dataset addresses this gap in the landscape of surveillance video datasets. The recordings took place at two different locations with the ToF camera set up either in a top-down or a tilted perspective on the scene. The dataset is publicly available at https://vizta-tof.kl.dfki.de/timo-dataset-overview/.



### Fast Rule-Based Clutter Detection in Automotive Radar Data
- **Arxiv ID**: http://arxiv.org/abs/2108.12224v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2108.12224v1)
- **Published**: 2021-08-27 11:32:50+00:00
- **Updated**: 2021-08-27 11:32:50+00:00
- **Authors**: Johannes Kopp, Dominik Kellner, Aldi Piroli, Klaus Dietmayer
- **Comment**: To be published in IEEE 24th International Conference on Intelligent
  Transportation Systems (ITSC), Indianapolis, USA, 2021
- **Journal**: None
- **Summary**: Automotive radar sensors output a lot of unwanted clutter or ghost detections, whose position and velocity do not correspond to any real object in the sensor's field of view. This poses a substantial challenge for environment perception methods like object detection or tracking. Especially problematic are clutter detections that occur in groups or at similar locations in multiple consecutive measurements. In this paper, a new algorithm for identifying such erroneous detections is presented. It is mainly based on the modeling of specific commonly occurring wave propagation paths that lead to clutter. In particular, the three effects explicitly covered are reflections at the underbody of a car or truck, signals traveling back and forth between the vehicle on which the sensor is mounted and another object, and multipath propagation via specular reflection. The latter often occurs near guardrails, concrete walls or similar reflective surfaces. Each of these effects is described both theoretically and regarding a method for identifying the corresponding clutter detections. Identification is done by analyzing detections generated from a single sensor measurement only. The final algorithm is evaluated on recordings of real extra-urban traffic. For labeling, a semi-automatic process is employed. The results are promising, both in terms of performance and regarding the very low execution time. Typically, a large part of clutter is found, while only a small ratio of detections corresponding to real objects are falsely classified by the algorithm.



### TE-YOLOF: Tiny and efficient YOLOF for blood cell detection
- **Arxiv ID**: http://arxiv.org/abs/2108.12313v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.12313v1)
- **Published**: 2021-08-27 14:45:27+00:00
- **Updated**: 2021-08-27 14:45:27+00:00
- **Authors**: Fanxin Xu, Xiangkui Li, Hang Yang, Yali Wang, Wei Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: Blood cell detection in microscopic images is an essential branch of medical image processing research. Since disease detection based on manual checking of blood cells is time-consuming and full of errors, testing of blood cells using object detectors with Deep Convolutional Neural Network can be regarded as a feasible solution. In this work, an object detector based on YOLOF has been proposed to detect blood cell objects such as red blood cells, white blood cells and platelets. This object detector is called TE-YOLOF, Tiny and Efficient YOLOF, and it is a One-Stage detector using dilated encoder to extract information from single-level feature maps. For increasing efficiency and flexibility, the EfficientNet Convolutional Neural Network is utilized as the backbone for the proposed object detector. Furthermore, the Depthwise Separable Convolution is applied to enhance the performance and minimize the parameters of the network. In addition, the Mish activation function is employed to increase the precision. Extensive experiments on the BCCD dataset prove the effectiveness of the proposed model, which is more efficient than other existing studies for blood cell detection.



### Classifying Organisms and Artefacts By Their Shapes
- **Arxiv ID**: http://arxiv.org/abs/2109.00920v1
- **DOI**: 10.1098/rsif.2022.0493
- **Categories**: **cs.CV**, stat.ME, 62P99
- **Links**: [PDF](http://arxiv.org/pdf/2109.00920v1)
- **Published**: 2021-08-27 15:44:22+00:00
- **Updated**: 2021-08-27 15:44:22+00:00
- **Authors**: Arianna Salili-James, Anne Mackay, Emilio Rodriguez-Alvarez, Diana Rodriguez-Perez, Thomas Mannack, Timothy A. Rawlings, A. Richard Palmer, Jonathan Todd, Terhi E. Riutta, Cate Macinnis-Ng, Zhitong Han, Megan Davies, Zinnia Thorpe, Stephen Marsland, Armand M. Leroi
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: We often wish to classify objects by their shapes. Indeed, the study of shapes is an important part of many scientific fields such as evolutionary biology, structural biology, image processing, and archaeology. The most widely-used method of shape analysis, Geometric Morphometrics, assumes that that the mathematical space in which shapes are represented is linear. However, it has long been known that shape space is, in fact, rather more complicated, and certainly non-linear. Diffeomorphic methods that take this non-linearity into account, and so give more accurate estimates of the distances among shapes, exist but have rarely been applied to real-world problems. Using a machine classifier, we tested the ability of several of these methods to describe and classify the shapes of a variety of organic and man-made objects. We find that one method, the Square-Root Velocity Function (SRVF), is superior to all others, including a standard Geometric Morphometric method (eigenshapes). We also show that computational shape classifiers outperform human experts, and that the SRVF shortest-path between shapes can be used to estimate the shapes of intermediate steps in evolutionary series. Diffeomorphic shape analysis methods, we conclude, now provide practical and effective solutions to many shape description and classification problems in the natural and human sciences.



### A Pedestrian Detection and Tracking Framework for Autonomous Cars: Efficient Fusion of Camera and LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/2108.12375v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.12375v1)
- **Published**: 2021-08-27 16:16:01+00:00
- **Updated**: 2021-08-27 16:16:01+00:00
- **Authors**: Muhammad Mobaidul Islam, Abdullah Al Redwan Newaz, Ali Karimoddini
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel method for pedestrian detection and tracking by fusing camera and LiDAR sensor data. To deal with the challenges associated with the autonomous driving scenarios, an integrated tracking and detection framework is proposed. The detection phase is performed by converting LiDAR streams to computationally tractable depth images, and then, a deep neural network is developed to identify pedestrian candidates both in RGB and depth images. To provide accurate information, the detection phase is further enhanced by fusing multi-modal sensor information using the Kalman filter. The tracking phase is a combination of the Kalman filter prediction and an optical flow algorithm to track multiple pedestrians in a scene. We evaluate our framework on a real public driving dataset. Experimental results demonstrate that the proposed method achieves significant performance improvement over a baseline method that solely uses image-based pedestrian detection.



### ISNet: Integrate Image-Level and Semantic-Level Context for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.12382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12382v1)
- **Published**: 2021-08-27 16:38:22+00:00
- **Updated**: 2021-08-27 16:38:22+00:00
- **Authors**: Zhenchao Jin, Bin Liu, Qi Chu, Nenghai Yu
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: Co-occurrent visual pattern makes aggregating contextual information a common paradigm to enhance the pixel representation for semantic image segmentation. The existing approaches focus on modeling the context from the perspective of the whole image, i.e., aggregating the image-level contextual information. Despite impressive, these methods weaken the significance of the pixel representations of the same category, i.e., the semantic-level contextual information. To address this, this paper proposes to augment the pixel representations by aggregating the image-level and semantic-level contextual information, respectively. First, an image-level context module is designed to capture the contextual information for each pixel in the whole image. Second, we aggregate the representations of the same category for each pixel where the category regions are learned under the supervision of the ground-truth segmentation. Third, we compute the similarities between each pixel representation and the image-level contextual information, the semantic-level contextual information, respectively. At last, a pixel representation is augmented by weighted aggregating both the image-level contextual information and the semantic-level contextual information with the similarities as the weights. Integrating the image-level and semantic-level context allows this paper to report state-of-the-art accuracy on four benchmarks, i.e., ADE20K, LIP, COCOStuff and Cityscapes.



### DC-GNet: Deep Mesh Relation Capturing Graph Convolution Network for 3D Human Shape Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2108.12384v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2108.12384v1)
- **Published**: 2021-08-27 16:43:32+00:00
- **Updated**: 2021-08-27 16:43:32+00:00
- **Authors**: Shihao Zhou, Mengxi Jiang, Shanshan Cai, Yunqi Lei
- **Comment**: Accepted by ACM MM'21 (oral)
- **Journal**: None
- **Summary**: In this paper, we aim to reconstruct a full 3D human shape from a single image. Previous vertex-level and parameter regression approaches reconstruct 3D human shape based on a pre-defined adjacency matrix to encode positive relations between nodes. The deep topological relations for the surface of the 3D human body are not carefully exploited. Moreover, the performance of most existing approaches often suffer from domain gap when handling more occlusion cases in real-world scenes.   In this work, we propose a Deep Mesh Relation Capturing Graph Convolution Network, DC-GNet, with a shape completion task for 3D human shape reconstruction. Firstly, we propose to capture deep relations within mesh vertices, where an adaptive matrix encoding both positive and negative relations is introduced. Secondly, we propose a shape completion task to learn prior about various kinds of occlusion cases. Our approach encodes mesh structure from more subtle relations between nodes in a more distant region. Furthermore, our shape completion module alleviates the performance degradation issue in the outdoor scene. Extensive experiments on several benchmarks show that our approach outperforms the previous 3D human pose and shape estimation approaches.



### A Novel Hierarchical Light Field Coding Scheme Based on Hybrid Stacked Multiplicative Layers and Fourier Disparity Layers for Glasses-Free 3D Displays
- **Arxiv ID**: http://arxiv.org/abs/2108.12399v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12399v1)
- **Published**: 2021-08-27 17:09:29+00:00
- **Updated**: 2021-08-27 17:09:29+00:00
- **Authors**: Joshitha Ravishankar, Mansi Sharma
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel hierarchical coding scheme for light fields based on transmittance patterns of low-rank multiplicative layers and Fourier disparity layers. The proposed scheme identifies multiplicative layers of light field view subsets optimized using a convolutional neural network for different scanning orders. Our approach exploits the hidden low-rank structure in the multiplicative layers obtained from the subsets of different scanning patterns. The spatial redundancies in the multiplicative layers can be efficiently removed by performing low-rank approximation at different ranks on the Krylov subspace. The intra-view and inter-view redundancies between approximated layers are further removed by HEVC encoding. Next, a Fourier disparity layer representation is constructed from the first subset of the approximated light field based on the chosen hierarchical order. Subsequent view subsets are synthesized by modeling the Fourier disparity layers that iteratively refine the representation with improved accuracy. The critical advantage of the proposed hybrid layered representation and coding scheme is that it utilizes not just spatial and temporal redundancies in light fields but efficiently exploits intrinsic similarities among neighboring sub-aperture images in both horizontal and vertical directions as specified by different predication orders. In addition, the scheme is flexible to realize a range of multiple bitrates at the decoder within a single integrated system. The compression performance of the proposed scheme is analyzed on real light fields. We achieved substantial bitrate savings and maintained good light field reconstruction quality.



### SynthIA: A Synthetic Inversion Approximation for the Stokes Vector Fusing SDO and Hinode into a Virtual Observatory
- **Arxiv ID**: http://arxiv.org/abs/2108.12421v1
- **DOI**: 10.3847/1538-4365/ac42d5
- **Categories**: **astro-ph.IM**, astro-ph.SR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12421v1)
- **Published**: 2021-08-27 17:58:38+00:00
- **Updated**: 2021-08-27 17:58:38+00:00
- **Authors**: Richard E. L. Higgins, David F. Fouhey, Spiro K. Antiochos, Graham Barnes, Mark C. M. Cheung, J. Todd Hoeksema, KD Leka, Yang Liu, Peter W. Schuck, Tamas I. Gombosi
- **Comment**: None
- **Journal**: None
- **Summary**: Both NASA's Solar Dynamics Observatory (SDO) and the JAXA/NASA Hinode mission include spectropolarimetric instruments designed to measure the photospheric magnetic field. SDO's Helioseismic and Magnetic Imager (HMI) emphasizes full-disk high-cadence and good spatial resolution data acquisition while Hinode's Solar Optical Telescope Spectro-Polarimeter (SOT-SP) focuses on high spatial resolution and spectral sampling at the cost of a limited field of view and slower temporal cadence. This work introduces a deep-learning system named SynthIA (Synthetic Inversion Approximation), that can enhance both missions by capturing the best of each instrument's characteristics. We use SynthIA to produce a new magnetogram data product, SynodeP (Synthetic Hinode Pipeline), that mimics magnetograms from the higher spectral resolution Hinode/SOT-SP pipeline, but is derived from full-disk, high-cadence, and lower spectral-resolution SDO/HMI Stokes observations. Results on held-out data show that SynodeP has good agreement with the Hinode/SOT-SP pipeline inversions, including magnetic fill fraction, which is not provided by the current SDO/HMI pipeline. SynodeP further shows a reduction in the magnitude of the 24-hour oscillations present in the SDO/HMI data. To demonstrate SynthIA's generality, we show the use of SDO/AIA data and subsets of the HMI data as inputs, which enables trade-offs between fidelity to the Hinode/SOT-SP inversions, number of observations used, and temporal artifacts. We discuss possible generalizations of SynthIA and its implications for space weather modeling. This work is part of the NASA Heliophysics DRIVE Science Center (SOLSTICE) at the University of Michigan under grant NASA 80NSSC20K0600E, and will be open-sourced.



### High Fidelity Deep Learning-based MRI Reconstruction with Instance-wise Discriminative Feature Matching Loss
- **Arxiv ID**: http://arxiv.org/abs/2108.12460v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2108.12460v1)
- **Published**: 2021-08-27 19:01:59+00:00
- **Updated**: 2021-08-27 19:01:59+00:00
- **Authors**: Ke Wang, Jonathan I Tamir, Alfredo De Goyeneche, Uri Wollner, Rafi Brada, Stella Yu, Michael Lustig
- **Comment**: 35 pages, 13 figures
- **Journal**: None
- **Summary**: Purpose: To improve reconstruction fidelity of fine structures and textures in deep learning (DL) based reconstructions.   Methods: A novel patch-based Unsupervised Feature Loss (UFLoss) is proposed and incorporated into the training of DL-based reconstruction frameworks in order to preserve perceptual similarity and high-order statistics. The UFLoss provides instance-level discrimination by mapping similar instances to similar low-dimensional feature vectors and is trained without any human annotation. By adding an additional loss function on the low-dimensional feature space during training, the reconstruction frameworks from under-sampled or corrupted data can reproduce more realistic images that are closer to the original with finer textures, sharper edges, and improved overall image quality. The performance of the proposed UFLoss is demonstrated on unrolled networks for accelerated 2D and 3D knee MRI reconstruction with retrospective under-sampling. Quantitative metrics including NRMSE, SSIM, and our proposed UFLoss were used to evaluate the performance of the proposed method and compare it with others.   Results: In-vivo experiments indicate that adding the UFLoss encourages sharper edges and more faithful contrasts compared to traditional and learning-based methods with pure l2 loss. More detailed textures can be seen in both 2D and 3D knee MR images. Quantitative results indicate that reconstruction with UFLoss can provide comparable NRMSE and a higher SSIM while achieving a much lower UFLoss value.   Conclusion: We present UFLoss, a patch-based unsupervised learned feature loss, which allows the training of DL-based reconstruction to obtain more detailed texture, finer features, and sharper edges with higher overall image quality under DL-based reconstruction frameworks.



### Learning Inner-Group Relations on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2108.12468v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.12468v1)
- **Published**: 2021-08-27 19:29:05+00:00
- **Updated**: 2021-08-27 19:29:05+00:00
- **Authors**: Haoxi Ran, Wei Zhuo, Jun Liu, Li Lu
- **Comment**: ICCV 2021. arXiv admin note: text overlap with arXiv:2011.14285
- **Journal**: None
- **Summary**: The prevalence of relation networks in computer vision is in stark contrast to underexplored point-based methods. In this paper, we explore the possibilities of local relation operators and survey their feasibility. We propose a scalable and efficient module, called group relation aggregator. The module computes a feature of a group based on the aggregation of the features of the inner-group points weighted by geometric relations and semantic relations. We adopt this module to design our RPNet. We further verify the expandability of RPNet, in terms of both depth and width, on the tasks of classification and segmentation. Surprisingly, empirical results show that wider RPNet fits for classification, while deeper RPNet works better on segmentation. RPNet achieves state-of-the-art for classification and segmentation on challenging benchmarks. We also compare our local aggregator with PointNet++, with around 30% parameters and 50% computation saving. Finally, we conduct experiments to reveal the robustness of RPNet with regard to rigid transformation and noises.



### VisGraphNet: a complex network interpretation of convolutional neural features
- **Arxiv ID**: http://arxiv.org/abs/2108.12490v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12490v1)
- **Published**: 2021-08-27 20:21:04+00:00
- **Updated**: 2021-08-27 20:21:04+00:00
- **Authors**: Joao B. Florindo, Young-Sup Lee, Kyungkoo Jun, Gwanggil Jeon, Marcelo K. Albertini
- **Comment**: None
- **Journal**: None
- **Summary**: Here we propose and investigate the use of visibility graphs to model the feature map of a neural network. The model, initially devised for studies on complex networks, is employed here for the classification of texture images. The work is motivated by an alternative viewpoint provided by these graphs over the original data. The performance of the proposed method is verified in the classification of four benchmark databases, namely, KTHTIPS-2b, FMD, UIUC, and UMD and in a practical problem, which is the identification of plant species using scanned images of their leaves. Our method was competitive with other state-of-the-art approaches, confirming the potential of techniques used for data analysis in different contexts to give more meaningful interpretation to the use of neural networks in texture classification.



### Fractal measures of image local features: an application to texture recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.12491v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12491v1)
- **Published**: 2021-08-27 20:27:28+00:00
- **Updated**: 2021-08-27 20:27:28+00:00
- **Authors**: Pedro M. Silva, Joao B. Florindo
- **Comment**: None
- **Journal**: None
- **Summary**: Here we propose a new method for the classification of texture images combining fractal measures (fractal dimension, multifractal spectrum and lacunarity) with local binary patterns. More specifically we compute the box counting dimension of the local binary codes thresholded at different levels to compose the feature vector. The proposal is assessed in the classification of three benchmark databases: KTHTIPS-2b, UMD and UIUC as well as in a real-world problem, namely the identification of Brazilian plant species (database 1200Tex) using scanned images of their leaves. The proposed method demonstrated to be competitive with other state-of-the-art solutions reported in the literature. Such results confirmed the potential of combining a powerful local coding description with the multiscale information captured by the fractal dimension for texture classification.



### On the impact of using X-ray energy response imagery for object detection via Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.12505v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.12505v1)
- **Published**: 2021-08-27 21:28:28+00:00
- **Updated**: 2021-08-27 21:28:28+00:00
- **Authors**: Neelanjan Bhowmik, Yona Falinie A. Gaus, Toby P. Breckon
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic detection of prohibited items within complex and cluttered X-ray security imagery is essential to maintaining transport security, where prior work on automatic prohibited item detection focus primarily on pseudo-colour (rgb}) X-ray imagery. In this work we study the impact of variant X-ray imagery, i.e., X-ray energy response (high, low}) and effective-z compared to rgb, via the use of deep Convolutional Neural Networks (CNN) for the joint object detection and segmentation task posed within X-ray baggage security screening. We evaluate state-of-the-art CNN architectures (Mask R-CNN, YOLACT, CARAFE and Cascade Mask R-CNN) to explore the transferability of models trained with such 'raw' variant imagery between the varying X-ray security scanners that exhibits differing imaging geometries, image resolutions and material colour profiles. Overall, we observe maximal detection performance using CARAFE, attributable to training using combination of rgb, high, low, and effective-z X-ray imagery, obtaining 0.7 mean Average Precision (mAP) for a six class object detection problem. Our results also exhibit a remarkable degree of generalisation capability in terms of cross-scanner transferability (AP: 0.835/0.611) for a one class object detection problem by combining rgb, high, low, and effective-z imagery.



### Automated Kidney Segmentation by Mask R-CNN in T2-weighted Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/2108.12506v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12506v1)
- **Published**: 2021-08-27 21:34:32+00:00
- **Updated**: 2021-08-27 21:34:32+00:00
- **Authors**: Manu Goyal, Junyu Guo, Lauren Hinojosa, Keith Hulsey, Ivan Pedrosa
- **Comment**: Submitted to SPIE Medical Imaging Conference
- **Journal**: None
- **Summary**: Despite the recent advances of deep learning algorithms in medical imaging, the automatic segmentation algorithms for kidneys in MRI exams are still scarce. Automated segmentation of kidneys in Magnetic Resonance Imaging (MRI) exams are important for enabling radiomics and machine learning analysis of renal disease. In this work, we propose to use the popular Mask R-CNN for the automatic segmentation of kidneys in coronal T2-weighted Fast Spin Eco slices of 100 MRI exams. We propose the morphological operations as post-processing to further improve the performance of Mask R-CNN for this task. With 5-fold cross-validation data, the proposed Mask R-CNN is trained and validated on 70 and 10 MRI exams and then evaluated on the remaining 20 exams in each fold. Our proposed method achieved a dice score of 0.904 and IoU of 0.822.



### Robustness Disparities in Commercial Face Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.12508v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.12508v1)
- **Published**: 2021-08-27 21:37:16+00:00
- **Updated**: 2021-08-27 21:37:16+00:00
- **Authors**: Samuel Dooley, Tom Goldstein, John P. Dickerson
- **Comment**: None
- **Journal**: None
- **Summary**: Facial detection and analysis systems have been deployed by large companies and critiqued by scholars and activists for the past decade. Critiques that focus on system performance analyze disparity of the system's output, i.e., how frequently is a face detected for different Fitzpatrick skin types or perceived genders. However, we focus on the robustness of these system outputs under noisy natural perturbations. We present the first of its kind detailed benchmark of the robustness of three such systems: Amazon Rekognition, Microsoft Azure, and Google Cloud Platform. We use both standard and recently released academic facial datasets to quantitatively analyze trends in robustness for each. Across all the datasets and systems, we generally find that photos of individuals who are older, masculine presenting, of darker skin type, or have dim lighting are more susceptible to errors than their counterparts in other identities.



### SIGN: Spatial-information Incorporated Generative Network for Generalized Zero-shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.12517v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12517v1)
- **Published**: 2021-08-27 22:18:24+00:00
- **Updated**: 2021-08-27 22:18:24+00:00
- **Authors**: Jiaxin Cheng, Soumyaroop Nandi, Prem Natarajan, Wael Abd-Almageed
- **Comment**: Accepted in ICCV 2021
- **Journal**: None
- **Summary**: Unlike conventional zero-shot classification, zero-shot semantic segmentation predicts a class label at the pixel level instead of the image level. When solving zero-shot semantic segmentation problems, the need for pixel-level prediction with surrounding context motivates us to incorporate spatial information using positional encoding. We improve standard positional encoding by introducing the concept of Relative Positional Encoding, which integrates spatial information at the feature level and can handle arbitrary image sizes. Furthermore, while self-training is widely used in zero-shot semantic segmentation to generate pseudo-labels, we propose a new knowledge-distillation-inspired self-training strategy, namely Annealed Self-Training, which can automatically assign different importance to pseudo-labels to improve performance. We systematically study the proposed Relative Positional Encoding and Annealed Self-Training in a comprehensive experimental evaluation, and our empirical results confirm the effectiveness of our method on three benchmark datasets.



### Combining chest X-rays and electronic health record (EHR) data using machine learning to diagnose acute respiratory failure
- **Arxiv ID**: http://arxiv.org/abs/2108.12530v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12530v2)
- **Published**: 2021-08-27 23:37:31+00:00
- **Updated**: 2022-04-20 15:12:56+00:00
- **Authors**: Sarah Jabbour, David Fouhey, Ella Kazerooni, Jenna Wiens, Michael W Sjoding
- **Comment**: None
- **Journal**: None
- **Summary**: Objective: When patients develop acute respiratory failure, accurately identifying the underlying etiology is essential for determining the best treatment. However, differentiating between common medical diagnoses can be challenging in clinical practice. Machine learning models could improve medical diagnosis by aiding in the diagnostic evaluation of these patients. Materials and Methods: Machine learning models were trained to predict the common causes of acute respiratory failure (pneumonia, heart failure, and/or COPD). Models were trained using chest radiographs and clinical data from the electronic health record (EHR) and applied to an internal and external cohort. Results: The internal cohort of 1,618 patients included 508 (31%) with pneumonia, 363 (22%) with heart failure, and 137 (8%) with COPD based on physician chart review. A model combining chest radiographs and EHR data outperformed models based on each modality alone. Models had similar or better performance compared to a randomly selected physician reviewer. For pneumonia, the combined model area under the receiver operating characteristic curve (AUROC) was 0.79 (0.77-0.79), image model AUROC was 0.74 (0.72-0.75), and EHR model AUROC was 0.74 (0.70-0.76). For heart failure, combined: 0.83 (0.77-0.84), image: 0.80 (0.71-0.81), and EHR: 0.79 (0.75-0.82). For COPD, combined: AUROC = 0.88 (0.83-0.91), image: 0.83 (0.77-0.89), and EHR: 0.80 (0.76-0.84). In the external cohort, performance was consistent for heart failure and increased for COPD, but declined slightly for pneumonia. Conclusions: Machine learning models combining chest radiographs and EHR data can accurately differentiate between common causes of acute respiratory failure. Further work is needed to determine how these models could act as a diagnostic aid to clinicians in clinical settings.



