# Arxiv Papers in cs.CV on 2021-08-09
### PASS: Protected Attribute Suppression System for Mitigating Bias in Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.03764v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03764v1)
- **Published**: 2021-08-09 00:39:22+00:00
- **Updated**: 2021-08-09 00:39:22+00:00
- **Authors**: Prithviraj Dhar, Joshua Gleason, Aniket Roy, Carlos D. Castillo, Rama Chellappa
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Face recognition networks encode information about sensitive attributes while being trained for identity classification. Such encoding has two major issues: (a) it makes the face representations susceptible to privacy leakage (b) it appears to contribute to bias in face recognition. However, existing bias mitigation approaches generally require end-to-end training and are unable to achieve high verification accuracy. Therefore, we present a descriptor-based adversarial de-biasing approach called `Protected Attribute Suppression System (PASS)'. PASS can be trained on top of descriptors obtained from any previously trained high-performing network to classify identities and simultaneously reduce encoding of sensitive attributes. This eliminates the need for end-to-end training. As a component of PASS, we present a novel discriminator training strategy that discourages a network from encoding protected attribute information. We show the efficacy of PASS to reduce gender and skintone information in descriptors from SOTA face recognition networks like Arcface. As a result, PASS descriptors outperform existing baselines in reducing gender and skintone bias on the IJB-C dataset, while maintaining a high verification accuracy.



### Multi-Slice Net: A novel light weight framework for COVID-19 Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2108.03786v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.03786v1)
- **Published**: 2021-08-09 02:46:11+00:00
- **Updated**: 2021-08-09 02:46:11+00:00
- **Authors**: Harshala Gammulle, Tharindu Fernando, Sridha Sridharan, Simon Denman, Clinton Fookes
- **Comment**: IEEE International Conference on Autonomous Systems 2021
- **Journal**: None
- **Summary**: This paper presents a novel lightweight COVID-19 diagnosis framework using CT scans. Our system utilises a novel two-stage approach to generate robust and efficient diagnoses across heterogeneous patient level inputs. We use a powerful backbone network as a feature extractor to capture discriminative slice-level features. These features are aggregated by a lightweight network to obtain a patient level diagnosis. The aggregation network is carefully designed to have a small number of trainable parameters while also possessing sufficient capacity to generalise to diverse variations within different CT volumes and to adapt to noise introduced during the data acquisition. We achieve a significant performance increase over the baselines when benchmarked on the SPGC COVID-19 Radiomics Dataset, despite having only 2.5 million trainable parameters and requiring only 0.623 seconds on average to process a single patient's CT volume using an Nvidia-GeForce RTX 2080 GPU.



### Learning Joint Embedding with Modality Alignments for Cross-Modal Retrieval of Recipes and Food Images
- **Arxiv ID**: http://arxiv.org/abs/2108.03788v2
- **DOI**: 10.1145/3459637.3482270
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2108.03788v2)
- **Published**: 2021-08-09 03:11:54+00:00
- **Updated**: 2021-08-18 07:11:14+00:00
- **Authors**: Zhongwei Xie, Ling Liu, Lin Li, Luo Zhong
- **Comment**: accepted by CIKM 2021. arXiv admin note: substantial text overlap
  with arXiv:2108.00705
- **Journal**: None
- **Summary**: This paper presents a three-tier modality alignment approach to learning text-image joint embedding, coined as JEMA, for cross-modal retrieval of cooking recipes and food images. The first tier improves recipe text embedding by optimizing the LSTM networks with term extraction and ranking enhanced sequence patterns, and optimizes the image embedding by combining the ResNeXt-101 image encoder with the category embedding using wideResNet-50 with word2vec. The second tier modality alignment optimizes the textual-visual joint embedding loss function using a double batch-hard triplet loss with soft-margin optimization. The third modality alignment incorporates two types of cross-modality alignments as the auxiliary loss regularizations to further reduce the alignment errors in the joint learning of the two modality-specific embedding functions. The category-based cross-modal alignment aims to align the image category with the recipe category as a loss regularization to the joint embedding. The cross-modal discriminator-based alignment aims to add the visual-textual embedding distribution alignment to further regularize the joint embedding loss. Extensive experiments with the one-million recipes benchmark dataset Recipe1M demonstrate that the proposed JEMA approach outperforms the state-of-the-art cross-modal embedding methods for both image-to-recipe and recipe-to-image retrievals.



### Boundary-aware Graph Reasoning for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.03791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03791v1)
- **Published**: 2021-08-09 03:23:42+00:00
- **Updated**: 2021-08-09 03:23:42+00:00
- **Authors**: Haoteng Tang, Haozhe Jia, Weidong Cai, Heng Huang, Yong Xia, Liang Zhan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a Boundary-aware Graph Reasoning (BGR) module to learn long-range contextual features for semantic segmentation. Rather than directly construct the graph based on the backbone features, our BGR module explores a reasonable way to combine segmentation erroneous regions with the graph construction scenario. Motivated by the fact that most hard-to-segment pixels broadly distribute on boundary regions, our BGR module uses the boundary score map as prior knowledge to intensify the graph node connections and thereby guide the graph reasoning focus on boundary regions. In addition, we employ an efficient graph convolution implementation to reduce the computational cost, which benefits the integration of our BGR module into current segmentation backbones. Extensive experiments on three challenging segmentation benchmarks demonstrate the effectiveness of our proposed BGR module for semantic segmentation.



### Paint Transformer: Feed Forward Neural Painting with Stroke Prediction
- **Arxiv ID**: http://arxiv.org/abs/2108.03798v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03798v2)
- **Published**: 2021-08-09 04:18:58+00:00
- **Updated**: 2021-08-11 13:09:55+00:00
- **Authors**: Songhua Liu, Tianwei Lin, Dongliang He, Fu Li, Ruifeng Deng, Xin Li, Errui Ding, Hao Wang
- **Comment**: Accepted by ICCV 2021 (oral). Codes will be released on
  https://github.com/wzmsltw/PaintTransformer
- **Journal**: None
- **Summary**: Neural painting refers to the procedure of producing a series of strokes for a given image and non-photo-realistically recreating it using neural networks. While reinforcement learning (RL) based agents can generate a stroke sequence step by step for this task, it is not easy to train a stable RL agent. On the other hand, stroke optimization methods search for a set of stroke parameters iteratively in a large search space; such low efficiency significantly limits their prevalence and practicality. Different from previous methods, in this paper, we formulate the task as a set prediction problem and propose a novel Transformer-based framework, dubbed Paint Transformer, to predict the parameters of a stroke set with a feed forward network. This way, our model can generate a set of strokes in parallel and obtain the final painting of size 512 * 512 in near real time. More importantly, since there is no dataset available for training the Paint Transformer, we devise a self-training pipeline such that it can be trained without any off-the-shelf dataset while still achieving excellent generalization capability. Experiments demonstrate that our method achieves better painting performance than previous ones with cheaper training and inference costs. Codes and models are available.



### COVID-view: Diagnosis of COVID-19 using Chest CT
- **Arxiv ID**: http://arxiv.org/abs/2108.03799v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.03799v1)
- **Published**: 2021-08-09 04:19:25+00:00
- **Updated**: 2021-08-09 04:19:25+00:00
- **Authors**: Shreeraj Jadhav, Gaofeng Deng, Marlene Zawin, Arie E. Kaufman
- **Comment**: 11 pages, 10 figures, accepted to IEEE VIS 2021 conference and IEEE
  Transactions on Visualization and Computer Graphics
- **Journal**: None
- **Summary**: Significant work has been done towards deep learning (DL) models for automatic lung and lesion segmentation and classification of COVID-19 on chest CT data. However, comprehensive visualization systems focused on supporting the dual visual+DL diagnosis of COVID-19 are non-existent. We present COVID-view, a visualization application specially tailored for radiologists to diagnose COVID-19 from chest CT data. The system incorporates a complete pipeline of automatic lungs segmentation, localization/ isolation of lung abnormalities, followed by visualization, visual and DL analysis, and measurement/quantification tools. Our system combines the traditional 2D workflow of radiologists with newer 2D and 3D visualization techniques with DL support for a more comprehensive diagnosis. COVID-view incorporates a novel DL model for classifying the patients into positive/negative COVID-19 cases, which acts as a reading aid for the radiologist using COVID-view and provides the attention heatmap as an explainable DL for the model output. We designed and evaluated COVID-view through suggestions, close feedback and conducting case studies of real-world patient data by expert radiologists who have substantial experience diagnosing chest CT scans for COVID-19, pulmonary embolism, and other forms of lung infections. We present requirements and task analysis for the diagnosis of COVID-19 that motivate our design choices and results in a practical system which is capable of handling real-world patient cases.



### PSGR: Pixel-wise Sparse Graph Reasoning for COVID-19 Pneumonia Segmentation in CT Images
- **Arxiv ID**: http://arxiv.org/abs/2108.03809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03809v1)
- **Published**: 2021-08-09 04:58:23+00:00
- **Updated**: 2021-08-09 04:58:23+00:00
- **Authors**: Haozhe Jia, Haoteng Tang, Guixiang Ma, Weidong Cai, Heng Huang, Liang Zhan, Yong Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Automated and accurate segmentation of the infected regions in computed tomography (CT) images is critical for the prediction of the pathological stage and treatment response of COVID-19. Several deep convolutional neural networks (DCNNs) have been designed for this task, whose performance, however, tends to be suppressed by their limited local receptive fields and insufficient global reasoning ability. In this paper, we propose a pixel-wise sparse graph reasoning (PSGR) module and insert it into a segmentation network to enhance the modeling of long-range dependencies for COVID-19 infected region segmentation in CT images. In the PSGR module, a graph is first constructed by projecting each pixel on a node based on the features produced by the segmentation backbone, and then converted into a sparsely-connected graph by keeping only K strongest connections to each uncertain pixel. The long-range information reasoning is performed on the sparsely-connected graph to generate enhanced features. The advantages of this module are two-fold: (1) the pixel-wise mapping strategy not only avoids imprecise pixel-to-node projections but also preserves the inherent information of each pixel for global reasoning; and (2) the sparsely-connected graph construction results in effective information retrieval and reduction of the noise propagation. The proposed solution has been evaluated against four widely-used segmentation models on three public datasets. The results show that the segmentation model equipped with our PSGR module can effectively segment COVID-19 infected regions in CT images, outperforming all other competing models.



### P-WAE: Generalized Patch-Wasserstein Autoencoder for Anomaly Screening
- **Arxiv ID**: http://arxiv.org/abs/2108.03815v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.03815v4)
- **Published**: 2021-08-09 05:31:45+00:00
- **Updated**: 2022-03-09 09:12:48+00:00
- **Authors**: Yurong Chen
- **Comment**: I need to revise the paper
- **Journal**: None
- **Summary**: Anomaly detection plays a pivotal role in numerous real-world scenarios, such as industrial automation and manufacturing intelligence. Recently, variational inference-based anomaly analysis has attracted researchers' and developers' attention. It aims to model the defect-free distribution so that anomalies can be classified as out-of-distribution samples. Nevertheless, there are two disturbing factors that need us to prioritize: (i) the simplistic prior latent distribution inducing limited expressive capability; (ii) the strong probability distance notion results in collapsed features. In this paper, we propose a novel Patch-wise Wasserstein AutoEncoder (P-WAE) architecture to alleviate those challenges. In particular, a patch-wise variational inference model coupled with solving the jigsaw puzzle is designed, which is a simple yet effective way to increase the expressiveness of the latent manifold. This makes using the model on high-dimensional practical data possible. In addition, we leverage a weaker measure, sliced-Wasserstein distance, to achieve the equilibrium between the reconstruction fidelity and generalized representations. Comprehensive experiments, conducted on the MVTec AD dataset, demonstrate the superior performance of our proposed method.



### DistillPose: Lightweight Camera Localization Using Auxiliary Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.03819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03819v1)
- **Published**: 2021-08-09 05:48:24+00:00
- **Updated**: 2021-08-09 05:48:24+00:00
- **Authors**: Yehya Abouelnaga, Mai Bui, Slobodan Ilic
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a lightweight retrieval-based pipeline to predict 6DOF camera poses from RGB images. Our pipeline uses a convolutional neural network (CNN) to encode a query image as a feature vector. A nearest neighbor lookup finds the pose-wise nearest database image. A siamese convolutional neural network regresses the relative pose from the nearest neighboring database image to the query image. The relative pose is then applied to the nearest neighboring absolute pose to obtain the query image's final absolute pose prediction. Our model is a distilled version of NN-Net that reduces its parameters by 98.87%, information retrieval feature vector size by 87.5%, and inference time by 89.18% without a significant decrease in localization accuracy.



### Video Annotation for Visual Tracking via Selection and Refinement
- **Arxiv ID**: http://arxiv.org/abs/2108.03821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03821v1)
- **Published**: 2021-08-09 05:56:47+00:00
- **Updated**: 2021-08-09 05:56:47+00:00
- **Authors**: Kenan Dai, Jie Zhao, Lijun Wang, Dong Wang, Jianhua Li, Huchuan Lu, Xuesheng Qian, Xiaoyun Yang
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: Deep learning based visual trackers entail offline pre-training on large volumes of video datasets with accurate bounding box annotations that are labor-expensive to achieve. We present a new framework to facilitate bounding box annotations for video sequences, which investigates a selection-and-refinement strategy to automatically improve the preliminary annotations generated by tracking algorithms. A temporal assessment network (T-Assess Net) is proposed which is able to capture the temporal coherence of target locations and select reliable tracking results by measuring their quality. Meanwhile, a visual-geometry refinement network (VG-Refine Net) is also designed to further enhance the selected tracking results by considering both target appearance and temporal geometry constraints, allowing inaccurate tracking results to be corrected. The combination of the above two networks provides a principled approach to ensure the quality of automatic video annotation. Experiments on large scale tracking benchmarks demonstrate that our method can deliver highly accurate bounding box annotations and significantly reduce human labor by 94.0%, yielding an effective means to further boost tracking performance with augmented training data.



### Towards to Robust and Generalized Medical Image Segmentation Framework
- **Arxiv ID**: http://arxiv.org/abs/2108.03823v7
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.03823v7)
- **Published**: 2021-08-09 05:58:49+00:00
- **Updated**: 2022-03-20 02:17:03+00:00
- **Authors**: Yurong Chen
- **Comment**: I need revise this paper
- **Journal**: None
- **Summary**: Deep learning-based computer-aided diagnosis is gradually deployed to review and analyze medical images. However, this paradigm is restricted in real-world clinical applications due to the poor robustness and generalization. The issue is more sinister with a lack of training data. In this paper, we address the challenge from the transfer learning point of view. Different from the common setting that transferring knowledge from the natural image domain to the medical image domain, we find the knowledge from the same domain further boosts the model robustness and generalization. Therefore, we propose a novel two-stage framework for robust generalized medical image segmentation. Firstly, an unsupervised tile-wise autoencoder pretraining architecture is proposed to learn local and global knowledge. Secondly, the downstream segmentation model coupled with an auxiliary reconstruction network is designed. The reconstruction branch encourages the model to capture more general semantic features. Experiments of lung segmentation on multi chest X-ray datasets are conducted. Comprehensive results demonstrate the superior robustness of the proposed framework to corruption and high generalization performance on unseen datasets, especially under the scenario of the limited training data.



### AA-RMVSNet: Adaptive Aggregation Recurrent Multi-view Stereo Network
- **Arxiv ID**: http://arxiv.org/abs/2108.03824v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03824v1)
- **Published**: 2021-08-09 06:10:48+00:00
- **Updated**: 2021-08-09 06:10:48+00:00
- **Authors**: Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, Guoping Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel recurrent multi-view stereo network based on long short-term memory (LSTM) with adaptive aggregation, namely AA-RMVSNet. We firstly introduce an intra-view aggregation module to adaptively extract image features by using context-aware convolution and multi-scale aggregation, which efficiently improves the performance on challenging regions, such as thin objects and large low-textured surfaces. To overcome the difficulty of varying occlusion in complex scenes, we propose an inter-view cost volume aggregation module for adaptive pixel-wise view aggregation, which is able to preserve better-matched pairs among all views. The two proposed adaptive aggregation modules are lightweight, effective and complementary regarding improving the accuracy and completeness of 3D reconstruction. Instead of conventional 3D CNNs, we utilize a hybrid network with recurrent structure for cost volume regularization, which allows high-resolution reconstruction and finer hypothetical plane sweep. The proposed network is trained end-to-end and achieves excellent performance on various datasets. It ranks $1^{st}$ among all submissions on Tanks and Temples benchmark and achieves competitive results on DTU dataset, which exhibits strong generalizability and robustness. Implementation of our method is available at https://github.com/QT-Zhu/AA-RMVSNet.



### Weakly-Supervised Spatio-Temporal Anomaly Detection in Surveillance Video
- **Arxiv ID**: http://arxiv.org/abs/2108.03825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03825v1)
- **Published**: 2021-08-09 06:11:14+00:00
- **Updated**: 2021-08-09 06:11:14+00:00
- **Authors**: Jie Wu, Wei Zhang, Guanbin Li, Wenhao Wu, Xiao Tan, Yingying Li, Errui Ding, Liang Lin
- **Comment**: accepted by IJCAI2021
- **Journal**: None
- **Summary**: In this paper, we introduce a novel task, referred to as Weakly-Supervised Spatio-Temporal Anomaly Detection (WSSTAD) in surveillance video. Specifically, given an untrimmed video, WSSTAD aims to localize a spatio-temporal tube (i.e., a sequence of bounding boxes at consecutive times) that encloses the abnormal event, with only coarse video-level annotations as supervision during training. To address this challenging task, we propose a dual-branch network which takes as input the proposals with multi-granularities in both spatial-temporal domains. Each branch employs a relationship reasoning module to capture the correlation between tubes/videolets, which can provide rich contextual information and complex entity relationships for the concept learning of abnormal behaviors. Mutually-guided Progressive Refinement framework is set up to employ dual-path mutual guidance in a recurrent manner, iteratively sharing auxiliary supervision information across branches. It impels the learned concepts of each branch to serve as a guide for its counterpart, which progressively refines the corresponding branch and the whole framework. Furthermore, we contribute two datasets, i.e., ST-UCF-Crime and STRA, consisting of videos containing spatio-temporal abnormal annotations to serve as the benchmarks for WSSTAD. We conduct extensive qualitative and quantitative evaluations to demonstrate the effectiveness of the proposed approach and analyze the key factors that contribute more to handle this task.



### Regularizing Nighttime Weirdness: Efficient Self-supervised Monocular Depth Estimation in the Dark
- **Arxiv ID**: http://arxiv.org/abs/2108.03830v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03830v2)
- **Published**: 2021-08-09 06:24:35+00:00
- **Updated**: 2021-08-13 13:46:00+00:00
- **Authors**: Kun Wang, Zhenyu Zhang, Zhiqiang Yan, Xiang Li, Baobei Xu, Jun Li, Jian Yang
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Monocular depth estimation aims at predicting depth from a single image or video. Recently, self-supervised methods draw much attention since they are free of depth annotations and achieve impressive performance on several daytime benchmarks. However, they produce weird outputs in more challenging nighttime scenarios because of low visibility and varying illuminations, which bring weak textures and break brightness-consistency assumption, respectively. To address these problems, in this paper we propose a novel framework with several improvements: (1) we introduce Priors-Based Regularization to learn distribution knowledge from unpaired depth maps and prevent model from being incorrectly trained; (2) we leverage Mapping-Consistent Image Enhancement module to enhance image visibility and contrast while maintaining brightness consistency; and (3) we present Statistics-Based Mask strategy to tune the number of removed pixels within textureless regions, using dynamic statistics. Experimental results demonstrate the effectiveness of each component. Meanwhile, our framework achieves remarkable improvements and state-of-the-art results on two nighttime datasets.



### Complementary Patch for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.03852v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03852v1)
- **Published**: 2021-08-09 07:50:54+00:00
- **Updated**: 2021-08-09 07:50:54+00:00
- **Authors**: Fei Zhang, Chaochen Gu, Chenyue Zhang, Yuchao Dai
- **Comment**: 10 pages, accepted in ICCV2021
- **Journal**: None
- **Summary**: Weakly Supervised Semantic Segmentation (WSSS) based on image-level labels has been greatly advanced by exploiting the outputs of Class Activation Map (CAM) to generate the pseudo labels for semantic segmentation. However, CAM merely discovers seeds from a small number of regions, which may be insufficient to serve as pseudo masks for semantic segmentation. In this paper, we formulate the expansion of object regions in CAM as an increase in information. From the perspective of information theory, we propose a novel Complementary Patch (CP) Representation and prove that the information of the sum of the CAMs by a pair of input images with complementary hidden (patched) parts, namely CP Pair, is greater than or equal to the information of the baseline CAM. Therefore, a CAM with more information related to object seeds can be obtained by narrowing down the gap between the sum of CAMs generated by the CP Pair and the original CAM. We propose a CP Network (CPN) implemented by a triplet network and three regularization functions. To further improve the quality of the CAMs, we propose a Pixel-Region Correlation Module (PRCM) to augment the contextual information by using object-region relations between the feature maps and the CAMs. Experimental results on the PASCAL VOC 2012 datasets show that our proposed method achieves a new state-of-the-art in WSSS, validating the effectiveness of our CP Representation and CPN.



### GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2108.03857v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2108.03857v2)
- **Published**: 2021-08-09 07:59:04+00:00
- **Updated**: 2021-08-14 15:45:40+00:00
- **Authors**: Sakib Shahriar
- **Comment**: Submitted to Pattern Recognition Letters
- **Journal**: None
- **Summary**: "Art is the lie that enables us to realize the truth." - Pablo Picasso. For centuries, humans have dedicated themselves to producing arts to convey their imagination. The advancement in technology and deep learning in particular, has caught the attention of many researchers trying to investigate whether art generation is possible by computers and algorithms. Using generative adversarial networks (GANs), applications such as synthesizing photorealistic human faces and creating captions automatically from images were realized. This survey takes a comprehensive look at the recent works using GANs for generating visual arts, music, and literary text. A performance comparison and description of the various GAN architecture are also presented. Finally, some of the key challenges in art generation using GANs are highlighted along with recommendations for future work.



### Safe Vessel Navigation Visually Aided by Autonomous Unmanned Aerial Vehicles in Congested Harbors and Waterways
- **Arxiv ID**: http://arxiv.org/abs/2108.03862v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.03862v1)
- **Published**: 2021-08-09 08:15:17+00:00
- **Updated**: 2021-08-09 08:15:17+00:00
- **Authors**: Jonas le Fevre Sejersen, Rui Pimentel de Figueiredo, Erdal Kayacan
- **Comment**: Accepted by "International Conference on Automation Science and
  Engineering" - Case 2021, waiting for publishment
- **Journal**: None
- **Summary**: In the maritime sector, safe vessel navigation is of great importance, particularly in congested harbors and waterways. The focus of this work is to estimate the distance between an object of interest and potential obstacles using a companion UAV. The proposed approach fuses GPS data with long-range aerial images. First, we employ semantic segmentation DNN for discriminating the vessel of interest, water, and potential solid objects using raw image data. The network is trained with both real and images generated and automatically labeled from a realistic AirSim simulation environment. Then, the distances between the extracted vessel and non-water obstacle blobs are computed using a novel GSD estimation algorithm. To the best of our knowledge, this work is the first attempt to detect and estimate distances to unknown objects from long-range visual data captured with conventional RGB cameras and auxiliary absolute positioning systems (e.g. GPS). The simulation results illustrate the accuracy and efficacy of the proposed method for visually aided navigation of vessels assisted by UAV.



### TransForensics: Image Forgery Localization with Dense Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/2108.03871v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03871v1)
- **Published**: 2021-08-09 08:43:26+00:00
- **Updated**: 2021-08-09 08:43:26+00:00
- **Authors**: Jing Hao, Zhixin Zhang, Shicai Yang, Di Xie, Shiliang Pu
- **Comment**: accepted by ICCV2021
- **Journal**: None
- **Summary**: Nowadays advanced image editing tools and technical skills produce tampered images more realistically, which can easily evade image forensic systems and make authenticity verification of images more difficult. To tackle this challenging problem, we introduce TransForensics, a novel image forgery localization method inspired by Transformers. The two major components in our framework are dense self-attention encoders and dense correction modules. The former is to model global context and all pairwise interactions between local patches at different scales, while the latter is used for improving the transparency of the hidden layers and correcting the outputs from different branches. Compared to previous traditional and deep learning methods, TransForensics not only can capture discriminative representations and obtain high-quality mask predictions but is also not limited by tampering types and patch sequence orders. By conducting experiments on main benchmarks, we show that TransForensics outperforms the stateof-the-art methods by a large margin.



### Rain Removal and Illumination Enhancement Done in One Go
- **Arxiv ID**: http://arxiv.org/abs/2108.03873v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.03873v2)
- **Published**: 2021-08-09 08:46:15+00:00
- **Updated**: 2021-10-16 04:36:59+00:00
- **Authors**: Yecong Wan, Yuanshuo Cheng, Mingwen Shao
- **Comment**: In section 5.2 of the paper, the comparison results are unfair due to
  different calculation methods of model speed. Please allow us to correct the
  unfair result
- **Journal**: None
- **Summary**: Rain removal plays an important role in the restoration of degraded images. Recently, data-driven methods have achieved remarkable success. However, these approaches neglect that the appearance of rain is often accompanied by low light conditions, which will further degrade the image quality. Therefore, it is very indispensable to jointly remove the rain and enhance the light for real-world rain image restoration. In this paper, we aim to address this problem from two aspects. First, we proposed a novel entangled network, namely EMNet, which can remove the rain and enhance illumination in one go. Specifically, two encoder-decoder networks interact complementary information through entanglement structure, and parallel rain removal and illumination enhancement. Considering that the encoder-decoder structure is unreliable in preserving spatial details, we employ a detail recovery network to restore the desired fine texture. Second, we present a new synthetic dataset, namely DarkRain, to boost the development of rain image restoration algorithms in practical scenarios. DarkRain not only contains different degrees of rain, but also considers different lighting conditions, and more realistically simulates the rainfall in the real world. EMNet is extensively evaluated on the proposed benchmark and achieves state-of-the-art results. In addition, after a simple transformation, our method outshines existing methods in both rain removal and low-light image enhancement. The source code and dataset will be made publicly available later.



### NeuralMVS: Bridging Multi-View Stereo and Novel View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2108.03880v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03880v2)
- **Published**: 2021-08-09 08:59:24+00:00
- **Updated**: 2022-06-15 12:32:54+00:00
- **Authors**: Radu Alexandru Rosu, Sven Behnke
- **Comment**: Accepted for International Joint Conference on Neural Networks
  (IJCNN) 2022. Code available at https://github.com/AIS-Bonn/neural_mvs
- **Journal**: None
- **Summary**: Multi-View Stereo (MVS) is a core task in 3D computer vision. With the surge of novel deep learning methods, learned MVS has surpassed the accuracy of classical approaches, but still relies on building a memory intensive dense cost volume. Novel View Synthesis (NVS) is a parallel line of research and has recently seen an increase in popularity with Neural Radiance Field (NeRF) models, which optimize a per scene radiance field. However, NeRF methods do not generalize to novel scenes and are slow to train and test. We propose to bridge the gap between these two methodologies with a novel network that can recover 3D scene geometry as a distance function, together with high-resolution color images. Our method uses only a sparse set of images as input and can generalize well to novel scenes. Additionally, we propose a coarse-to-fine sphere tracing approach in order to significantly increase speed. We show on various datasets that our method reaches comparable accuracy to per-scene optimized methods while being able to generalize and running significantly faster. We provide the source code at https://github.com/AIS-Bonn/neural_mvs



### Self-supervised Learning of Occlusion Aware Flow Guided 3D Geometry Perception with Adaptive Cross Weighted Loss from Monocular Videos
- **Arxiv ID**: http://arxiv.org/abs/2108.03893v3
- **DOI**: None
- **Categories**: **cs.CV**, ams.org, I.2.2
- **Links**: [PDF](http://arxiv.org/pdf/2108.03893v3)
- **Published**: 2021-08-09 09:21:24+00:00
- **Updated**: 2021-10-22 07:48:01+00:00
- **Authors**: Jiaojiao Fang, Guizhong Liu
- **Comment**: 7pages,4figures
- **Journal**: None
- **Summary**: Self-supervised deep learning-based 3D scene understanding methods can overcome the difficulty of acquiring the densely labeled ground-truth and have made a lot of advances. However, occlusions and moving objects are still some of the major limitations. In this paper, we explore the learnable occlusion aware optical flow guided self-supervised depth and camera pose estimation by an adaptive cross weighted loss to address the above limitations. Firstly, we explore to train the learnable occlusion mask fused optical flow network by an occlusion-aware photometric loss with the temporally supplemental information and backward-forward consistency of adjacent views. And then, we design an adaptive cross-weighted loss between the depth-pose and optical flow loss of the geometric and photometric error to distinguish the moving objects which violate the static scene assumption. Our method shows promising results on KITTI, Make3D, and Cityscapes datasets under multiple tasks. We also show good generalization ability under a variety of challenging scenarios.



### FIFA: Fast Inference Approximation for Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.03894v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.03894v1)
- **Published**: 2021-08-09 09:23:43+00:00
- **Updated**: 2021-08-09 09:23:43+00:00
- **Authors**: Yaser Souri, Yazan Abu Farha, Fabien Despinoy, Gianpiero Francesca, Juergen Gall
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce FIFA, a fast approximate inference method for action segmentation and alignment. Unlike previous approaches, FIFA does not rely on expensive dynamic programming for inference. Instead, it uses an approximate differentiable energy function that can be minimized using gradient-descent. FIFA is a general approach that can replace exact inference improving its speed by more than 5 times while maintaining its performance. FIFA is an anytime inference algorithm that provides a better speed vs. accuracy trade-off compared to exact inference. We apply FIFA on top of state-of-the-art approaches for weakly supervised action segmentation and alignment as well as fully supervised action segmentation. FIFA achieves state-of-the-art results on most metrics on two action segmentation datasets.



### Unified Regularity Measures for Sample-wise Learning and Generalization
- **Arxiv ID**: http://arxiv.org/abs/2108.03913v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.03913v1)
- **Published**: 2021-08-09 10:11:14+00:00
- **Updated**: 2021-08-09 10:11:14+00:00
- **Authors**: Chi Zhang, Xiaoning Ma, Yu Liu, Le Wang, Yuanqi Su, Yuehu Liu
- **Comment**: 20 pages, 13 figures, 3 tables
- **Journal**: None
- **Summary**: Fundamental machine learning theory shows that different samples contribute unequally both in learning and testing processes. Contemporary studies on DNN imply that such sample difference is rooted on the distribution of intrinsic pattern information, namely sample regularity. Motivated by the recent discovery on network memorization and generalization, we proposed a pair of sample regularity measures for both processes with a formulation-consistent representation. Specifically, cumulative binary training/generalizing loss (CBTL/CBGL), the cumulative number of correct classiffcations of the training/testing sample within training stage, is proposed to quantize the stability in memorization-generalization process; while forgetting/mal-generalizing events, i.e., the mis-classification of previously learned or generalized sample, are utilized to represent the uncertainty of sample regularity with respect to optimization dynamics. Experiments validated the effectiveness and robustness of the proposed approaches for mini-batch SGD optimization. Further applications on training/testing sample selection show the proposed measures sharing the unified computing procedure could benefit for both tasks.



### LatticeNet: Fast Spatio-Temporal Point Cloud Segmentation Using Permutohedral Lattices
- **Arxiv ID**: http://arxiv.org/abs/2108.03917v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.03917v1)
- **Published**: 2021-08-09 10:17:27+00:00
- **Updated**: 2021-08-09 10:17:27+00:00
- **Authors**: Radu Alexandru Rosu, Peer Schütt, Jan Quenzel, Sven Behnke
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1912.05905
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have shown outstanding performance in the task of semantically segmenting images. Applying the same methods on 3D data still poses challenges due to the heavy memory requirements and the lack of structured data. Here, we propose LatticeNet, a novel approach for 3D semantic segmentation, which takes raw point clouds as input. A PointNet describes the local geometry which we embed into a sparse permutohedral lattice. The lattice allows for fast convolutions while keeping a low memory footprint. Further, we introduce DeformSlice, a novel learned data-dependent interpolation for projecting lattice features back onto the point cloud. We present results of 3D segmentation on multiple datasets where our method achieves state-of-the-art performance. We also extend and evaluate our network for instance and dynamic object segmentation.



### Selective Light Field Refocusing for Camera Arrays Using Bokeh Rendering and Superresolution
- **Arxiv ID**: http://arxiv.org/abs/2108.03918v1
- **DOI**: 10.1109/LSP.2018.2885213
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.03918v1)
- **Published**: 2021-08-09 10:19:21+00:00
- **Updated**: 2021-08-09 10:19:21+00:00
- **Authors**: Yingqian Wang, Jungang Yang, Yulan Guo, Chao Xiao, Wei An
- **Comment**: None
- **Journal**: IEEE Signal Processing Letters, volume: 26, issue: 1, pages:
  204-208, 2019
- **Summary**: Camera arrays provide spatial and angular information within a single snapshot. With refocusing methods, focal planes can be altered after exposure. In this letter, we propose a light field refocusing method to improve the imaging quality of camera arrays. In our method, the disparity is first estimated. Then, the unfocused region (bokeh) is rendered by using a depth-based anisotropic filter. Finally, the refocused image is produced by a reconstruction-based superresolution approach where the bokeh image is used as a regularization term. Our method can selectively refocus images with focused region being superresolved and bokeh being aesthetically rendered. Our method also enables postadjustment of depth of field. We conduct experiments on both public and self-developed datasets. Our method achieves superior visual performance with acceptable computational cost as compared to other state-of-the-art methods. Code is available at https://github.com/YingqianWang/Selective-LF-Refocusing.



### FA-GAN: Fused Attentive Generative Adversarial Networks for MRI Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2108.03920v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.03920v1)
- **Published**: 2021-08-09 10:21:39+00:00
- **Updated**: 2021-08-09 10:21:39+00:00
- **Authors**: Mingfeng Jiang, Minghao Zhi, Liying Wei, Xiaocheng Yang, Jucheng Zhang, Yongming Li, Pin Wang, Jiahao Huang, Guang Yang
- **Comment**: 27 pages, 12 figures, accepted by CMIG
- **Journal**: None
- **Summary**: High-resolution magnetic resonance images can provide fine-grained anatomical information, but acquiring such data requires a long scanning time. In this paper, a framework called the Fused Attentive Generative Adversarial Networks(FA-GAN) is proposed to generate the super-resolution MR image from low-resolution magnetic resonance images, which can reduce the scanning time effectively but with high resolution MR images. In the framework of the FA-GAN, the local fusion feature block, consisting of different three-pass networks by using different convolution kernels, is proposed to extract image features at different scales. And the global feature fusion module, including the channel attention module, the self-attention module, and the fusion operation, is designed to enhance the important features of the MR image. Moreover, the spectral normalization process is introduced to make the discriminator network stable. 40 sets of 3D magnetic resonance images (each set of images contains 256 slices) are used to train the network, and 10 sets of images are used to test the proposed method. The experimental results show that the PSNR and SSIM values of the super-resolution magnetic resonance image generated by the proposed FA-GAN method are higher than the state-of-the-art reconstruction methods.



### 3D Human Reconstruction in the Wild with Collaborative Aerial Cameras
- **Arxiv ID**: http://arxiv.org/abs/2108.03936v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.03936v1)
- **Published**: 2021-08-09 11:03:38+00:00
- **Updated**: 2021-08-09 11:03:38+00:00
- **Authors**: Cherie Ho, Andrew Jong, Harry Freeman, Rohan Rao, Rogerio Bonatti, Sebastian Scherer
- **Comment**: 7 pages, 11 figures, IROS 2021
- **Journal**: None
- **Summary**: Aerial vehicles are revolutionizing applications that require capturing the 3D structure of dynamic targets in the wild, such as sports, medicine, and entertainment. The core challenges in developing a motion-capture system that operates in outdoors environments are: (1) 3D inference requires multiple simultaneous viewpoints of the target, (2) occlusion caused by obstacles is frequent when tracking moving targets, and (3) the camera and vehicle state estimation is noisy. We present a real-time aerial system for multi-camera control that can reconstruct human motions in natural environments without the use of special-purpose markers. We develop a multi-robot coordination scheme that maintains the optimal flight formation for target reconstruction quality amongst obstacles. We provide studies evaluating system performance in simulation, and validate real-world performance using two drones while a target performs activities such as jogging and playing soccer. Supplementary video: https://youtu.be/jxt91vx0cns



### TriTransNet: RGB-D Salient Object Detection with a Triplet Transformer Embedding Network
- **Arxiv ID**: http://arxiv.org/abs/2108.03990v1
- **DOI**: 10.1145/3474085.3475601
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03990v1)
- **Published**: 2021-08-09 12:42:56+00:00
- **Updated**: 2021-08-09 12:42:56+00:00
- **Authors**: Zhengyi Liu, Yuan Wang, Zhengzheng Tu, Yun Xiao, Bin Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Salient object detection is the pixel-level dense prediction task which can highlight the prominent object in the scene. Recently U-Net framework is widely used, and continuous convolution and pooling operations generate multi-level features which are complementary with each other. In view of the more contribution of high-level features for the performance, we propose a triplet transformer embedding module to enhance them by learning long-range dependencies across layers. It is the first to use three transformer encoders with shared weights to enhance multi-level features. By further designing scale adjustment module to process the input, devising three-stream decoder to process the output and attaching depth features to color features for the multi-modal fusion, the proposed triplet transformer embedding network (TriTransNet) achieves the state-of-the-art performance in RGB-D salient object detection, and pushes the performance to a new level. Experimental results demonstrate the effectiveness of the proposed modules and the competition of TriTransNet.



### Transductive Few-Shot Classification on the Oblique Manifold
- **Arxiv ID**: http://arxiv.org/abs/2108.04009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04009v1)
- **Published**: 2021-08-09 13:01:03+00:00
- **Updated**: 2021-08-09 13:01:03+00:00
- **Authors**: Guodong Qi, Huimin Yu, Zhaohui Lu, Shuzhao Li
- **Comment**: ICCV2021
- **Journal**: None
- **Summary**: Few-shot learning (FSL) attempts to learn with limited data. In this work, we perform the feature extraction in the Euclidean space and the geodesic distance metric on the Oblique Manifold (OM). Specially, for better feature extraction, we propose a non-parametric Region Self-attention with Spatial Pyramid Pooling (RSSPP), which realizes a trade-off between the generalization and the discriminative ability of the single image feature. Then, we embed the feature to OM as a point. Furthermore, we design an Oblique Distance-based Classifier (ODC) that achieves classification in the tangent spaces which better approximate OM locally by learnable tangency points. Finally, we introduce a new method for parameters initialization and a novel loss function in the transductive settings. Extensive experiments demonstrate the effectiveness of our algorithm and it outperforms state-of-the-art methods on the popular benchmarks: mini-ImageNet, tiered-ImageNet, and Caltech-UCSD Birds-200-2011 (CUB).



### Dynamic Multi-Scale Loss Optimization for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.04014v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04014v1)
- **Published**: 2021-08-09 13:12:41+00:00
- **Updated**: 2021-08-09 13:12:41+00:00
- **Authors**: Yihao Luo, Xiang Cao, Juntao Zhang, Peng Cheng, Tianjiang Wang, Qi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: With the continuous improvement of the performance of object detectors via advanced model architectures, imbalance problems in the training process have received more attention. It is a common paradigm in object detection frameworks to perform multi-scale detection. However, each scale is treated equally during training. In this paper, we carefully study the objective imbalance of multi-scale detector training. We argue that the loss in each scale level is neither equally important nor independent. Different from the existing solutions of setting multi-task weights, we dynamically optimize the loss weight of each scale level in the training process. Specifically, we propose an Adaptive Variance Weighting (AVW) to balance multi-scale loss according to the statistical variance. Then we develop a novel Reinforcement Learning Optimization (RLO) to decide the weighting scheme probabilistically during training. The proposed dynamic methods make better utilization of multi-scale training loss without extra computational complexity and learnable parameters for backpropagation. Experiments show that our approaches can consistently boost the performance over various baseline detectors on Pascal VOC and MS COCO benchmark.



### Deep Learning methods for automatic evaluation of delayed enhancement-MRI. The results of the EMIDEC challenge
- **Arxiv ID**: http://arxiv.org/abs/2108.04016v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04016v2)
- **Published**: 2021-08-09 13:15:25+00:00
- **Updated**: 2021-08-10 14:21:29+00:00
- **Authors**: Alain Lalande, Zhihao Chen, Thibaut Pommier, Thomas Decourselle, Abdul Qayyum, Michel Salomon, Dominique Ginhac, Youssef Skandarani, Arnaud Boucher, Khawla Brahim, Marleen de Bruijne, Robin Camarasa, Teresa M. Correia, Xue Feng, Kibrom B. Girum, Anja Hennemuth, Markus Huellebrand, Raabid Hussain, Matthias Ivantsits, Jun Ma, Craig Meyer, Rishabh Sharma, Jixi Shi, Nikolaos V. Tsekos, Marta Varela, Xiyue Wang, Sen Yang, Hannu Zhang, Yichi Zhang, Yuncheng Zhou, Xiahai Zhuang, Raphael Couturier, Fabrice Meriaudeau
- **Comment**: Submitted to Medical Image Analysis
- **Journal**: None
- **Summary**: A key factor for assessing the state of the heart after myocardial infarction (MI) is to measure whether the myocardium segment is viable after reperfusion or revascularization therapy. Delayed enhancement-MRI or DE-MRI, which is performed several minutes after injection of the contrast agent, provides high contrast between viable and nonviable myocardium and is therefore a method of choice to evaluate the extent of MI. To automatically assess myocardial status, the results of the EMIDEC challenge that focused on this task are presented in this paper. The challenge's main objectives were twofold. First, to evaluate if deep learning methods can distinguish between normal and pathological cases. Second, to automatically calculate the extent of myocardial infarction. The publicly available database consists of 150 exams divided into 50 cases with normal MRI after injection of a contrast agent and 100 cases with myocardial infarction (and then with a hyperenhanced area on DE-MRI), whatever their inclusion in the cardiac emergency department. Along with MRI, clinical characteristics are also provided. The obtained results issued from several works show that the automatic classification of an exam is a reachable task (the best method providing an accuracy of 0.92), and the automatic segmentation of the myocardium is possible. However, the segmentation of the diseased area needs to be improved, mainly due to the small size of these areas and the lack of contrast with the surrounding structures.



### DRINet: A Dual-Representation Iterative Learning Network for Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.04023v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.04023v1)
- **Published**: 2021-08-09 13:23:54+00:00
- **Updated**: 2021-08-09 13:23:54+00:00
- **Authors**: Maosheng Ye, Shuangjie Xu, Tongyi Cao, Qifeng Chen
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: We present a novel and flexible architecture for point cloud segmentation with dual-representation iterative learning. In point cloud processing, different representations have their own pros and cons. Thus, finding suitable ways to represent point cloud data structure while keeping its own internal physical property such as permutation and scale-invariant is a fundamental problem. Therefore, we propose our work, DRINet, which serves as the basic network structure for dual-representation learning with great flexibility at feature transferring and less computation cost, especially for large-scale point clouds. DRINet mainly consists of two modules called Sparse Point-Voxel Feature Extraction and Sparse Voxel-Point Feature Extraction. By utilizing these two modules iteratively, features can be propagated between two different representations. We further propose a novel multi-scale pooling layer for pointwise locality learning to improve context information propagation. Our network achieves state-of-the-art results for point cloud classification and segmentation tasks on several datasets while maintaining high runtime efficiency. For large-scale outdoor scenarios, our method outperforms state-of-the-art methods with a real-time inference speed of 62ms per frame.



### Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2108.04024v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2108.04024v1)
- **Published**: 2021-08-09 13:25:06+00:00
- **Updated**: 2021-08-09 13:25:06+00:00
- **Authors**: Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, Stephen Gould
- **Comment**: ICCV 2021. Dataset, code, and pre-trained models are released at
  https://cuberick-orion.github.io/CIRR/
- **Journal**: None
- **Summary**: We extend the task of composed image retrieval, where an input query consists of an image and short textual description of how to modify the image. Existing methods have only been applied to non-complex images within narrow domains, such as fashion products, thereby limiting the scope of study on in-depth visual reasoning in rich image and language contexts. To address this issue, we collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which consists of over 36,000 pairs of crowd-sourced, open-domain images with human-generated modifying text. To extend current methods to the open-domain, we propose CIRPLANT, a transformer based model that leverages rich pre-trained vision-and-language (V&L) knowledge for modifying visual features conditioned on natural language. Retrieval is then done by nearest neighbor lookup on the modified features. We demonstrate that with a relatively simple architecture, CIRPLANT outperforms existing methods on open-domain images, while matching state-of-the-art accuracy on the existing narrow datasets, such as fashion. Together with the release of CIRR, we believe this work will inspire further research on composed image retrieval.



### Tensor Yard: One-Shot Algorithm of Hardware-Friendly Tensor-Train Decomposition for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.04029v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/2108.04029v1)
- **Published**: 2021-08-09 13:31:04+00:00
- **Updated**: 2021-08-09 13:31:04+00:00
- **Authors**: Anuar Taskynov, Vladimir Korviakov, Ivan Mazurenko, Yepan Xiong
- **Comment**: 10 pages, 5 figures, 1 table
- **Journal**: None
- **Summary**: Nowadays Deep Learning became widely used in many economic, technical and scientific areas of human interest. It is clear that efficiency of solutions based on Deep Neural Networks should consider not only quality metric for the target task, but also latency and constraints of target platform design should be taken into account. In this paper we present novel hardware-friendly Tensor-Train decomposition implementation for Convolutional Neural Networks together with Tensor Yard - one-shot training algorithm which optimizes an order of decomposition of network layers. These ideas allow to accelerate ResNet models on Ascend 310 NPU devices without significant loss of accuracy. For example we accelerate ResNet-101 by 14.6% with drop by 0.5 of top-1 ImageNet accuracy.



### Two-stream Convolutional Networks for Multi-frame Face Anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/2108.04032v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04032v1)
- **Published**: 2021-08-09 13:35:30+00:00
- **Updated**: 2021-08-09 13:35:30+00:00
- **Authors**: Zhuoyi Zhang, Cheng Jiang, Xiya Zhong, Chang Song, Yifeng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Face anti-spoofing is an important task to protect the security of face recognition. Most of previous work either struggle to capture discriminative and generalizable feature or rely on auxiliary information which is unavailable for most of industrial product. Inspired by the video classification work, we propose an efficient two-stream model to capture the key differences between live and spoof faces, which takes multi-frames and RGB difference as input respectively. Feature pyramid modules with two opposite fusion directions and pyramid pooling modules are applied to enhance feature representation. We evaluate the proposed method on the datasets of Siw, Oulu-NPU, CASIA-MFSD and Replay-Attack. The results show that our model achieves the state-of-the-art results on most of datasets' protocol with much less parameter size.



### Detecting Visual Design Principles in Art and Architecture through Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.04048v1
- **DOI**: 10.1016/j.autcon.2021.103826
- **Categories**: **cs.CV**, I.4; I.5; J.5
- **Links**: [PDF](http://arxiv.org/pdf/2108.04048v1)
- **Published**: 2021-08-09 14:00:17+00:00
- **Updated**: 2021-08-09 14:00:17+00:00
- **Authors**: Gozdenur Demir, Asli Cekmis, Vahit Bugra Yesilkaynak, Gozde Unal
- **Comment**: None
- **Journal**: None
- **Summary**: Visual design is associated with the use of some basic design elements and principles. Those are applied by the designers in the various disciplines for aesthetic purposes, relying on an intuitive and subjective process. Thus, numerical analysis of design visuals and disclosure of the aesthetic value embedded in them are considered as hard. However, it has become possible with emerging artificial intelligence technologies. This research aims at a neural network model, which recognizes and classifies the design principles over different domains. The domains include artwork produced since the late 20th century; professional photos; and facade pictures of contemporary buildings. The data collection and curation processes, including the production of computationally-based synthetic dataset, is genuine. The proposed model learns from the knowledge of myriads of original designs, by capturing the underlying shared patterns. It is expected to consolidate design processes by providing an aesthetic evaluation of the visual compositions with objectivity.



### Zero in on Shape: A Generic 2D-3D Instance Similarity Metric learned from Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2108.04091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04091v1)
- **Published**: 2021-08-09 14:44:08+00:00
- **Updated**: 2021-08-09 14:44:08+00:00
- **Authors**: Maciej Janik, Niklas Gard, Anna Hilsmann, Peter Eisert
- **Comment**: Accepted to ICIP 2021
- **Journal**: None
- **Summary**: We present a network architecture which compares RGB images and untextured 3D models by the similarity of the represented shape. Our system is optimised for zero-shot retrieval, meaning it can recognise shapes never shown in training. We use a view-based shape descriptor and a siamese network to learn object geometry from pairs of 3D models and 2D images. Due to scarcity of datasets with exact photograph-mesh correspondences, we train our network with only synthetic data. Our experiments investigate the effect of different qualities and quantities of training data on retrieval accuracy and present insights from bridging the domain gap. We show that increasing the variety of synthetic data improves retrieval accuracy and that our system's performance in zero-shot mode can match that of the instance-aware mode, as far as narrowing down the search to the top 10% of objects.



### Transfer Learning Gaussian Anomaly Detection by Fine-tuning Representations
- **Arxiv ID**: http://arxiv.org/abs/2108.04116v2
- **DOI**: 10.5220/0011063900003209
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04116v2)
- **Published**: 2021-08-09 15:29:04+00:00
- **Updated**: 2022-06-13 08:33:46+00:00
- **Authors**: Oliver Rippel, Arnav Chavan, Chucai Lei, Dorit Merhof
- **Comment**: Camera ready for IMPROVE22 + additional typo fixes
- **Journal**: None
- **Summary**: Current state-of-the-art anomaly detection (AD) methods exploit the powerful representations yielded by large-scale ImageNet training. However, catastrophic forgetting prevents the successful fine-tuning of pre-trained representations on new datasets in the semi-supervised setting, and representations are therefore commonly fixed. In our work, we propose a new method to overcome catastrophic forgetting and thus successfully fine-tune pre-trained representations for AD in the transfer learning setting. Specifically, we induce a multivariate Gaussian distribution for the normal class based on the linkage between generative and discriminative modeling, and use the Mahalanobis distance of normal images to the estimated distribution as training objective. We additionally propose to use augmentations commonly employed for vicinal risk minimization in a validation scheme to detect onset of catastrophic forgetting. Extensive evaluations on the public MVTec dataset reveal that a new state of the art is achieved by our method in the AD task while simultaneously achieving anomaly segmentation performance comparable to prior state of the art. Further, ablation studies demonstrate the importance of the induced Gaussian distribution as well as the robustness of the proposed fine-tuning scheme with respect to the choice of augmentations.



### Manifold-aware Synthesis of High-resolution Diffusion from Structural Imaging
- **Arxiv ID**: http://arxiv.org/abs/2108.04135v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04135v2)
- **Published**: 2021-08-09 16:00:40+00:00
- **Updated**: 2021-08-11 21:30:21+00:00
- **Authors**: Benoit Anctil-Robitaille, Antoine Théberge, Pierre-Marc Jodoin, Maxime Descoteaux, Christian Desrosiers, Hervé Lombaert
- **Comment**: Preprint submitted for review
- **Journal**: None
- **Summary**: The physical and clinical constraints surrounding diffusion-weighted imaging (DWI) often limit the spatial resolution of the produced images to voxels up to 8 times larger than those of T1w images. Thus, the detailed information contained in T1w imagescould help in the synthesis of diffusion images in higher resolution. However, the non-Euclidean nature of diffusion imaging hinders current deep generative models from synthesizing physically plausible images. In this work, we propose the first Riemannian network architecture for the direct generation of diffusion tensors (DT) and diffusion orientation distribution functions (dODFs) from high-resolution T1w images. Our integration of the Log-Euclidean Metric into a learning objective guarantees, unlike standard Euclidean networks, the mathematically-valid synthesis of diffusion. Furthermore, our approach improves the fractional anisotropy mean squared error (FA MSE) between the synthesized diffusion and the ground-truth by more than 23% and the cosine similarity between principal directions by almost 5% when compared to our baselines. We validate our generated diffusion by comparing the resulting tractograms to our expected real data. We observe similar fiber bundles with streamlines having less than 3% difference in length, less than 1% difference in volume, and a visually close shape. While our method is able to generate high-resolution diffusion images from structural inputs in less than 15 seconds, we acknowledge and discuss the limits of diffusion inference solely relying on T1w images. Our results nonetheless suggest a relationship between the high-level geometry of the brain and the overall white matter architecture.



### No-Reference Image Quality Assessment by Hallucinating Pristine Features
- **Arxiv ID**: http://arxiv.org/abs/2108.04165v3
- **DOI**: 10.1109/TIP.2022.3205770
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04165v3)
- **Published**: 2021-08-09 16:48:34+00:00
- **Updated**: 2022-09-02 04:02:02+00:00
- **Authors**: Baoliang Chen, Lingyu Zhu, Chenqi Kong, Hanwei Zhu, Shiqi Wang, Zhu Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a no-reference (NR) image quality assessment (IQA) method via feature level pseudo-reference (PR) hallucination. The proposed quality assessment framework is grounded on the prior models of natural image statistical behaviors and rooted in the view that the perceptually meaningful features could be well exploited to characterize the visual quality. Herein, the PR features from the distorted images are learned by a mutual learning scheme with the pristine reference as the supervision, and the discriminative characteristics of PR features are further ensured with the triplet constraints. Given a distorted image for quality inference, the feature level disentanglement is performed with an invertible neural layer for final quality prediction, leading to the PR and the corresponding distortion features for comparison. The effectiveness of our proposed method is demonstrated on four popular IQA databases, and superior performance on cross-database evaluation also reveals the high generalization capability of our method. The implementation of our method is publicly available on https://github.com/Baoliang93/FPR.



### Using Deep Learning for Visual Decoding and Reconstruction from Brain Activity: A Review
- **Arxiv ID**: http://arxiv.org/abs/2108.04169v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04169v1)
- **Published**: 2021-08-09 16:54:35+00:00
- **Updated**: 2021-08-09 16:54:35+00:00
- **Authors**: Madison Van Horn
- **Comment**: None
- **Journal**: None
- **Summary**: This literature review will discuss the use of deep learning methods for image reconstruction using fMRI data. More specifically, the quality of image reconstruction will be determined by the choice in decoding and reconstruction architectures. I will show that these structures can struggle with adaptability to various input stimuli due to complicated objects in images. Also, the significance of feature representation will be evaluated. This paper will conclude the use of deep learning within visual decoding and reconstruction is highly optimal when using variations of deep neural networks and will provide details of potential future work.



### Distributionally Robust Segmentation of Abnormal Fetal Brain 3D MRI
- **Arxiv ID**: http://arxiv.org/abs/2108.04175v1
- **DOI**: 10.1007/978-3-030-87735-4_25
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04175v1)
- **Published**: 2021-08-09 17:00:21+00:00
- **Updated**: 2021-08-09 17:00:21+00:00
- **Authors**: Lucas Fidon, Michael Aertsen, Nada Mufti, Thomas Deprest, Doaa Emam, Frédéric Guffens, Ernst Schwartz, Michael Ebner, Daniela Prayer, Gregor Kasprian, Anna L. David, Andrew Melbourne, Sébastien Ourselin, Jan Deprest, Georg Langs, Tom Vercauteren
- **Comment**: Accepted at the MICCAI 2021 Perinatal, Preterm and Paediatric Image
  Analysis (PIPPI) workshop. arXiv admin note: substantial text overlap with
  arXiv:2001.02658
- **Journal**: None
- **Summary**: The performance of deep neural networks typically increases with the number of training images. However, not all images have the same importance towards improved performance and robustness. In fetal brain MRI, abnormalities exacerbate the variability of the developing brain anatomy compared to non-pathological cases. A small number of abnormal cases, as is typically available in clinical datasets used for training, are unlikely to fairly represent the rich variability of abnormal developing brains. This leads machine learning systems trained by maximizing the average performance to be biased toward non-pathological cases. This problem was recently referred to as hidden stratification. To be suited for clinical use, automatic segmentation methods need to reliably achieve high-quality segmentation outcomes also for pathological cases. In this paper, we show that the state-of-the-art deep learning pipeline nnU-Net has difficulties to generalize to unseen abnormal cases. To mitigate this problem, we propose to train a deep neural network to minimize a percentile of the distribution of per-volume loss over the dataset. We show that this can be achieved by using Distributionally Robust Optimization (DRO). DRO automatically reweights the training samples with lower performance, encouraging nnU-Net to perform more consistently on all cases. We validated our approach using a dataset of 368 fetal brain T2w MRIs, including 124 MRIs of open spina bifida cases and 51 MRIs of cases with other severe abnormalities of brain development.



### Novel scorpion detection system combining computer vision and fluorescence
- **Arxiv ID**: http://arxiv.org/abs/2108.04177v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04177v1)
- **Published**: 2021-08-09 17:00:52+00:00
- **Updated**: 2021-08-09 17:00:52+00:00
- **Authors**: Francisco Luis Giambelluca, Jorge Osio, Luis A. Giambelluca, Marcelo A. Cappelletti
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, a fully automatic and real-time system for the detection of scorpions was developed using computer vision and deep learning techniques. This system is based on the implementation of a double validation process using the shape features and the fluorescent characteristics of scorpions when exposed to ultraviolet (UV) light. The Haar Cascade Classifier (HCC) and YOLO (You Only Look Once) models have been used and compared as the first mechanism for the scorpion shape detection. The detection of the fluorescence emitted by the scorpions under UV light has been used as a second detection mechanism in order to increase the accuracy and precision of the system. The results obtained show that the system can accurately and reliably detect the presence of scorpions. In addition, values obtained of recall of 100% is essential with the purpose of providing a health security tool. Although the developed system can only be used at night or in dark environment, where the fluorescence emitted by the scorpions can be visualized, the nocturnal activity of scorpions justifies the incorporation of this second validation mechanism.



### Pose is all you need: The pose only group activity recognition system (POGARS)
- **Arxiv ID**: http://arxiv.org/abs/2108.04186v1
- **DOI**: 10.1007/s00138-022-01346-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04186v1)
- **Published**: 2021-08-09 17:16:04+00:00
- **Updated**: 2021-08-09 17:16:04+00:00
- **Authors**: Haritha Thilakarathne, Aiden Nibali, Zhen He, Stuart Morgan
- **Comment**: 12 pages, 7 figures
- **Journal**: Machine.Vision.and.Applications. 33 (2002)
- **Summary**: We introduce a novel deep learning based group activity recognition approach called the Pose Only Group Activity Recognition System (POGARS), designed to use only tracked poses of people to predict the performed group activity. In contrast to existing approaches for group activity recognition, POGARS uses 1D CNNs to learn spatiotemporal dynamics of individuals involved in a group activity and forgo learning features from pixel data. The proposed model uses a spatial and temporal attention mechanism to infer person-wise importance and multi-task learning for simultaneously performing group and individual action classification. Experimental results confirm that POGARS achieves highly competitive results compared to state-of-the-art methods on a widely used public volleyball dataset despite only using tracked pose as input. Further our experiments show by using pose only as input, POGARS has better generalization capabilities compared to methods that use RGB as input.



### Meta Gradient Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2108.04204v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04204v2)
- **Published**: 2021-08-09 17:44:19+00:00
- **Updated**: 2021-08-10 06:22:51+00:00
- **Authors**: Zheng Yuan, Jie Zhang, Yunpei Jia, Chuanqi Tan, Tao Xue, Shiguang Shan
- **Comment**: 13 pages, 2 figures, 12 tables. Accepted by ICCV2021
- **Journal**: None
- **Summary**: In recent years, research on adversarial attacks has become a hot spot. Although current literature on the transfer-based adversarial attack has achieved promising results for improving the transferability to unseen black-box models, it still leaves a long way to go. Inspired by the idea of meta-learning, this paper proposes a novel architecture called Meta Gradient Adversarial Attack (MGAA), which is plug-and-play and can be integrated with any existing gradient-based attack method for improving the cross-model transferability. Specifically, we randomly sample multiple models from a model zoo to compose different tasks and iteratively simulate a white-box attack and a black-box attack in each task. By narrowing the gap between the gradient directions in white-box and black-box attacks, the transferability of adversarial examples on the black-box setting can be improved. Extensive experiments on the CIFAR10 and ImageNet datasets show that our architecture outperforms the state-of-the-art methods for both black-box and white-box attack settings.



### AutoVideo: An Automated Video Action Recognition System
- **Arxiv ID**: http://arxiv.org/abs/2108.04212v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04212v4)
- **Published**: 2021-08-09 17:53:32+00:00
- **Updated**: 2022-07-17 00:17:49+00:00
- **Authors**: Daochen Zha, Zaid Pervaiz Bhat, Yi-Wei Chen, Yicheng Wang, Sirui Ding, Jiaben Chen, Kwei-Herng Lai, Mohammad Qazim Bhat, Anmoll Kumar Jain, Alfredo Costilla Reyes, Na Zou, Xia Hu
- **Comment**: Accepted by IJCAI https://github.com/datamllab/autovideo
- **Journal**: None
- **Summary**: Action recognition is an important task for video understanding with broad applications. However, developing an effective action recognition solution often requires extensive engineering efforts in building and testing different combinations of the modules and their hyperparameters. In this demo, we present AutoVideo, a Python system for automated video action recognition. AutoVideo is featured for 1) highly modular and extendable infrastructure following the standard pipeline language, 2) an exhaustive list of primitives for pipeline construction, 3) data-driven tuners to save the efforts of pipeline tuning, and 4) easy-to-use Graphical User Interface (GUI). AutoVideo is released under MIT license at https://github.com/datamllab/autovideo



### Automated Olfactory Bulb Segmentation on High Resolutional T2-Weighted MRI
- **Arxiv ID**: http://arxiv.org/abs/2108.04267v1
- **DOI**: 10.1016/j.neuroimage.2021.118464
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04267v1)
- **Published**: 2021-08-09 18:03:25+00:00
- **Updated**: 2021-08-09 18:03:25+00:00
- **Authors**: Santiago Estrada, Ran Lu, Kersten Diers, Weiyi Zeng, Philipp Ehses, Tony Stöcker, Monique M. B Breteler, Martin Reuter
- **Comment**: None
- **Journal**: None
- **Summary**: The neuroimage analysis community has neglected the automated segmentation of the olfactory bulb (OB) despite its crucial role in olfactory function. The lack of an automatic processing method for the OB can be explained by its challenging properties. Nonetheless, recent advances in MRI acquisition techniques and resolution have allowed raters to generate more reliable manual annotations. Furthermore, the high accuracy of deep learning methods for solving semantic segmentation problems provides us with an option to reliably assess even small structures. In this work, we introduce a novel, fast, and fully automated deep learning pipeline to accurately segment OB tissue on sub-millimeter T2-weighted (T2w) whole-brain MR images. To this end, we designed a three-stage pipeline: (1) Localization of a region containing both OBs using FastSurferCNN, (2) Segmentation of OB tissue within the localized region through four independent AttFastSurferCNN - a novel deep learning architecture with a self-attention mechanism to improve modeling of contextual information, and (3) Ensemble of the predicted label maps. The OB pipeline exhibits high performance in terms of boundary delineation, OB localization, and volume estimation across a wide range of ages in 203 participants of the Rhineland Study. Moreover, it also generalizes to scans of an independent dataset never encountered during training, the Human Connectome Project (HCP), with different acquisition parameters and demographics, evaluated in 30 cases at the native 0.7mm HCP resolution, and the default 0.8mm pipeline resolution. We extensively validated our pipeline not only with respect to segmentation accuracy but also to known OB volume effects, where it can sensitively replicate age effects.



### Visual SLAM with Graph-Cut Optimized Multi-Plane Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2108.04281v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.04281v2)
- **Published**: 2021-08-09 18:16:08+00:00
- **Updated**: 2022-06-21 18:27:22+00:00
- **Authors**: Fangwen Shu, Yaxu Xie, Jason Rambach, Alain Pagani, Didier Stricker
- **Comment**: accepted to ISMAR 2021 (Poster), v2 fixed some typos and minor errors
- **Journal**: None
- **Summary**: This paper presents a semantic planar SLAM system that improves pose estimation and mapping using cues from an instance planar segmentation network. While the mainstream approaches are using RGB-D sensors, employing a monocular camera with such a system still faces challenges such as robust data association and precise geometric model fitting. In the majority of existing work, geometric model estimation problems such as homography estimation and piece-wise planar reconstruction (PPR) are usually solved by standard (greedy) RANSAC separately and sequentially. However, setting the inlier-outlier threshold is difficult in absence of information about the scene (i.e. the scale). In this work, we revisit these problems and argue that two mentioned geometric models (homographies/3D planes) can be solved by minimizing an energy function that exploits the spatial coherence, i.e. with graph-cut optimization, which also tackles the practical issue when the output of a trained CNN is inaccurate. Moreover, we propose an adaptive parameter setting strategy based on our experiments, and report a comprehensive evaluation on various open-source datasets.



### Learning to Cut by Watching Movies
- **Arxiv ID**: http://arxiv.org/abs/2108.04294v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.04294v3)
- **Published**: 2021-08-09 18:37:17+00:00
- **Updated**: 2021-09-29 11:44:32+00:00
- **Authors**: Alejandro Pardo, Fabian Caba Heilbron, Juan León Alcázar, Ali Thabet, Bernard Ghanem
- **Comment**: Accepted at ICCV2021. Paper website:
  https://alejandropardo.net/publication/learning-to-cut/
- **Journal**: None
- **Summary**: Video content creation keeps growing at an incredible pace; yet, creating engaging stories remains challenging and requires non-trivial video editing expertise. Many video editing components are astonishingly hard to automate primarily due to the lack of raw video materials. This paper focuses on a new task for computational video editing, namely the task of raking cut plausibility. Our key idea is to leverage content that has already been edited to learn fine-grained audiovisual patterns that trigger cuts. To do this, we first collected a data source of more than 10K videos, from which we extract more than 255K cuts. We devise a model that learns to discriminate between real and artificial cuts via contrastive learning. We set up a new task and a set of baselines to benchmark video cut generation. We observe that our proposed model outperforms the baselines by large margins. To demonstrate our model in real-world applications, we conduct human studies in a collection of unedited videos. The results show that our model does a better job at cutting than random and alternative baselines.



### Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development
- **Arxiv ID**: http://arxiv.org/abs/2108.04308v2
- **DOI**: 10.1145/3476058
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2108.04308v2)
- **Published**: 2021-08-09 19:07:58+00:00
- **Updated**: 2021-09-16 23:00:49+00:00
- **Authors**: Morgan Klaus Scheuerman, Emily Denton, Alex Hanna
- **Comment**: CSCW 2021; 37 pages
- **Journal**: Proc. ACM Hum.-Comput. Interact.5, CSCW2, Article 317(October
  2021), 37 pages
- **Summary**: Data is a crucial component of machine learning. The field is reliant on data to train, validate, and test models. With increased technical capabilities, machine learning research has boomed in both academic and industry settings, and one major focus has been on computer vision. Computer vision is a popular domain of machine learning increasingly pertinent to real-world applications, from facial recognition in policing to object detection for autonomous vehicles. Given computer vision's propensity to shape machine learning research and impact human life, we seek to understand disciplinary practices around dataset documentation - how data is collected, curated, annotated, and packaged into datasets for computer vision researchers and practitioners to use for model tuning and development. Specifically, we examine what dataset documentation communicates about the underlying values of vision data and the larger practices and goals of computer vision as a field. To conduct this study, we collected a corpus of about 500 computer vision datasets, from which we sampled 114 dataset publications across different vision tasks. Through both a structured and thematic content analysis, we document a number of values around accepted data practices, what makes desirable data, and the treatment of humans in the dataset construction process. We discuss how computer vision datasets authors value efficiency at the expense of care; universality at the expense of contextuality; impartiality at the expense of positionality; and model work at the expense of data work. Many of the silenced values we identify sit in opposition with social computing practices. We conclude with suggestions on how to better incorporate silenced values into the dataset creation and curation process.



### AnyoneNet: Synchronized Speech and Talking Head Generation for Arbitrary Person
- **Arxiv ID**: http://arxiv.org/abs/2108.04325v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2108.04325v2)
- **Published**: 2021-08-09 19:58:38+00:00
- **Updated**: 2021-08-11 08:19:36+00:00
- **Authors**: Xinsheng Wang, Qicong Xie, Jihua Zhu, Lei Xie, Scharenborg
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically generating videos in which synthesized speech is synchronized with lip movements in a talking head has great potential in many human-computer interaction scenarios. In this paper, we present an automatic method to generate synchronized speech and talking-head videos on the basis of text and a single face image of an arbitrary person as input. In contrast to previous text-driven talking head generation methods, which can only synthesize the voice of a specific person, the proposed method is capable of synthesizing speech for any person that is inaccessible in the training stage. Specifically, the proposed method decomposes the generation of synchronized speech and talking head videos into two stages, i.e., a text-to-speech (TTS) stage and a speech-driven talking head generation stage. The proposed TTS module is a face-conditioned multi-speaker TTS model that gets the speaker identity information from face images instead of speech, which allows us to synthesize a personalized voice on the basis of the input face image. To generate the talking head videos from the face images, a facial landmark-based method that can predict both lip movements and head rotations is proposed. Extensive experiments demonstrate that the proposed method is able to generate synchronized speech and talking head videos for arbitrary persons and non-persons. Synthesized speech shows consistency with the given face regarding to the synthesized voice's timbre and one's appearance in the image, and the proposed landmark-based talking head method outperforms the state-of-the-art landmark-based method on generating natural talking head videos.



### Natural Numerical Networks for Natura 2000 habitats classification by satellite images
- **Arxiv ID**: http://arxiv.org/abs/2108.04327v2
- **DOI**: None
- **Categories**: **math.NA**, cs.AI, cs.CV, cs.CY, cs.LG, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/2108.04327v2)
- **Published**: 2021-08-09 20:03:16+00:00
- **Updated**: 2021-12-22 10:38:41+00:00
- **Authors**: Karol Mikula, Michal Kollar, Aneta A. Ozvat, Martin Ambroz, Lucia Cahojova, Ivan Jarolimek, Jozef Sibik, Maria Sibikova
- **Comment**: None
- **Journal**: None
- **Summary**: Natural numerical networks are introduced as a new classification algorithm based on the numerical solution of nonlinear partial differential equations of forward-backward diffusion type on complete graphs. The proposed natural numerical network is applied to open important environmental and nature conservation task, the automated identification of protected habitats by using satellite images. In the natural numerical network, the forward diffusion causes the movement of points in a feature space toward each other. The opposite effect, keeping the points away from each other, is caused by backward diffusion. This yields the desired classification. The natural numerical network contains a few parameters that are optimized in the learning phase of the method. After learning parameters and optimizing the topology of the network graph, classification necessary for habitat identification is performed. A relevancy map for each habitat is introduced as a tool for validating the classification and finding new Natura 2000 habitat appearances.



### Towards artificially intelligent recycling Improving image processing for waste classification
- **Arxiv ID**: http://arxiv.org/abs/2108.06274v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.06274v1)
- **Published**: 2021-08-09 21:41:48+00:00
- **Updated**: 2021-08-09 21:41:48+00:00
- **Authors**: Youpeng Yu, Ryan Grammenos
- **Comment**: None
- **Journal**: None
- **Summary**: The ever-increasing amount of global refuse is overwhelming the waste and recycling management industries. The need for smart systems for environmental monitoring and the enhancement of recycling processes is thus greater than ever. Amongst these efforts lies IBM's Wastenet project which aims to improve recycling by using artificial intelligence for waste classification. The work reported in this paper builds on this project through the use of transfer learning and data augmentation techniques to ameliorate classification accuracy. Starting with a convolutional neural network (CNN), a systematic approach is followed for selecting appropriate splitting ratios and for tuning multiple training parameters including learning rate schedulers, layers freezing, batch sizes and loss functions, in the context of the given scenario which requires classification of waste into different recycling types. Results are compared and contrasted using 10-fold cross validation and demonstrate that the model developed achieves a 91.21% test accuracy. Subsequently, a range of data augmentation techniques are then incorporated into this work including flipping, rotation, shearing, zooming, and brightness control. Results show that these augmentation techniques further improve the test accuracy of the final model to 95.40%. Unlike other work reported in the field, this paper provides full details regarding the training of the model. Furthermore, the code for this work has been made open-source and we have demonstrated that the model can perform successful real-time classification of recycling waste items using a standard computer webcam.



### Explainable AI and susceptibility to adversarial attacks: a case study in classification of breast ultrasound images
- **Arxiv ID**: http://arxiv.org/abs/2108.04345v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04345v1)
- **Published**: 2021-08-09 23:52:16+00:00
- **Updated**: 2021-08-09 23:52:16+00:00
- **Authors**: Hamza Rasaee, Hassan Rivaz
- **Comment**: 4 pages, 4 figures, Accepted to IEEE IUS 2021
- **Journal**: None
- **Summary**: Ultrasound is a non-invasive imaging modality that can be conveniently used to classify suspicious breast nodules and potentially detect the onset of breast cancer. Recently, Convolutional Neural Networks (CNN) techniques have shown promising results in classifying ultrasound images of the breast into benign or malignant. However, CNN inference acts as a black-box model, and as such, its decision-making is not interpretable. Therefore, increasing effort has been dedicated to explaining this process, most notably through GRAD-CAM and other techniques that provide visual explanations into inner workings of CNNs. In addition to interpretation, these methods provide clinically important information, such as identifying the location for biopsy or treatment. In this work, we analyze how adversarial assaults that are practically undetectable may be devised to alter these importance maps dramatically. Furthermore, we will show that this change in the importance maps can come with or without altering the classification result, rendering them even harder to detect. As such, care must be taken when using these importance maps to shed light on the inner workings of deep learning. Finally, we utilize Multi-Task Learning (MTL) and propose a new network based on ResNet-50 to improve the classification accuracies. Our sensitivity and specificity is comparable to the state of the art results.



### RaftMLP: How Much Can Be Done Without Attention and with Less Spatial Locality?
- **Arxiv ID**: http://arxiv.org/abs/2108.04384v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04384v3)
- **Published**: 2021-08-09 23:55:24+00:00
- **Updated**: 2023-01-12 14:04:15+00:00
- **Authors**: Yuki Tatsunami, Masato Taki
- **Comment**: ACCV2022 accepted
- **Journal**: None
- **Summary**: For the past ten years, CNN has reigned supreme in the world of computer vision, but recently, Transformer has been on the rise. However, the quadratic computational cost of self-attention has become a serious problem in practice applications. There has been much research on architectures without CNN and self-attention in this context. In particular, MLP-Mixer is a simple architecture designed using MLPs and hit an accuracy comparable to the Vision Transformer. However, the only inductive bias in this architecture is the embedding of tokens. This leaves open the possibility of incorporating a non-convolutional (or non-local) inductive bias into the architecture, so we used two simple ideas to incorporate inductive bias into the MLP-Mixer while taking advantage of its ability to capture global correlations. A way is to divide the token-mixing block vertically and horizontally. Another way is to make spatial correlations denser among some channels of token-mixing. With this approach, we were able to improve the accuracy of the MLP-Mixer while reducing its parameters and computational complexity. The small model that is RaftMLP-S is comparable to the state-of-the-art global MLP-based model in terms of parameters and efficiency per calculation. In addition, we tackled the problem of fixed input image resolution for global MLP-based models by utilizing bicubic interpolation. We demonstrated that these models could be applied as the backbone of architectures for downstream tasks such as object detection. However, it did not have significant performance and mentioned the need for MLP-specific architectures for downstream tasks for global MLP-based models. The source code in PyTorch version is available at \url{https://github.com/okojoalg/raft-mlp}.



