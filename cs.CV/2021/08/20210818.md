# Arxiv Papers in cs.CV on 2021-08-18
### Classification of Abnormal Hand Movement for Aiding in Autism Detection: Machine Learning Study
- **Arxiv ID**: http://arxiv.org/abs/2108.07917v6
- **DOI**: 10.2196/33771
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.07917v6)
- **Published**: 2021-08-18 01:00:02+00:00
- **Updated**: 2022-06-06 15:38:49+00:00
- **Authors**: Anish Lakkapragada, Aaron Kline, Onur Cezmi Mutlu, Kelley Paskov, Brianna Chrisman, Nate Stockham, Peter Washington, Dennis Wall
- **Comment**: JMIR Biomedical Engineering '22, To Be Presented at DataBricks
  Conference '22
- **Journal**: None
- **Summary**: A formal autism diagnosis can be an inefficient and lengthy process. Families may wait months or longer before receiving a diagnosis for their child despite evidence that earlier intervention leads to better treatment outcomes. Digital technologies which detect the presence of behaviors related to autism can scale access to pediatric diagnoses. This work aims to demonstrate the feasibility of deep learning technologies for detecting hand flapping from unstructured home videos as a first step towards validating whether models and digital technologies can be leveraged to aid with autism diagnoses. We used the Self-Stimulatory Behavior Dataset (SSBD), which contains 75 videos of hand flapping, head banging, and spinning exhibited by children. From all the hand flapping videos, we extracted 100 positive and control videos of hand flapping, each between 2 to 5 seconds in duration. Utilizing both landmark-driven-approaches and MobileNet V2's pretrained convolutional layers, our highest performing model achieved a testing F1 score of 84% (90% precision and 80% recall) when evaluating with 5-fold cross validation 100 times. This work provides the first step towards developing precise deep learning methods for activity detection of autism-related behaviors.



### Adversarial Relighting Against Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.07920v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.07920v4)
- **Published**: 2021-08-18 01:05:53+00:00
- **Updated**: 2022-08-27 02:39:18+00:00
- **Authors**: Qian Zhang, Qing Guo, Ruijun Gao, Felix Juefei-Xu, Hongkai Yu, Wei Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep face recognition (FR) has achieved significantly high accuracy on several challenging datasets and fosters successful real-world applications, even showing high robustness to the illumination variation that is usually regarded as a main threat to the FR system. However, in the real world, illumination variation caused by diverse lighting conditions cannot be fully covered by the limited face dataset. In this paper, we study the threat of lighting against FR from a new angle, i.e., adversarial attack, and identify a new task, i.e., adversarial relighting. Given a face image, adversarial relighting aims to produce a naturally relighted counterpart while fooling the state-of-the-art deep FR methods. To this end, we first propose the physical modelbased adversarial relighting attack (ARA) denoted as albedoquotient-based adversarial relighting attack (AQ-ARA). It generates natural adversarial light under the physical lighting model and guidance of FR systems and synthesizes adversarially relighted face images. Moreover, we propose the auto-predictive adversarial relighting attack (AP-ARA) by training an adversarial relighting network (ARNet) to automatically predict the adversarial light in a one-step manner according to different input faces, allowing efficiency-sensitive applications. More importantly, we propose to transfer the above digital attacks to physical ARA (PhyARA) through a precise relighting device, making the estimated adversarial lighting condition reproducible in the real world. We validate our methods on three state-of-the-art deep FR methods, i.e., FaceNet, ArcFace, and CosFace, on two public datasets. The extensive and insightful results demonstrate our work can generate realistic adversarial relighted face images fooling face recognition tasks easily, revealing the threat of specific light directions and strengths.



### Calibration Method of the Monocular Omnidirectional Stereo Camera
- **Arxiv ID**: http://arxiv.org/abs/2108.07936v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.07936v1)
- **Published**: 2021-08-18 02:07:58+00:00
- **Updated**: 2021-08-18 02:07:58+00:00
- **Authors**: Ryota Kawamata, Keiichi Betsui, Kazuyoshi Yamazaki, Rei Sakakibara, Takeshi Shimano
- **Comment**: 8 pages, 8 figures, 2 tables, accepted for publication in
  International Journal of Automotive Engineering
- **Journal**: None
- **Summary**: Compact and low-cost devices are needed for autonomous driving to image and measure distances to objects 360-degree around. We have been developing an omnidirectional stereo camera exploiting two hyperbolic mirrors and a single set of a lens and sensor, which makes this camera compact and cost efficient. We establish a new calibration method for this camera considering higher-order radial distortion, detailed tangential distortion, an image sensor tilt, and a lens-mirror offset. Our method reduces the calibration error by 6.0 and 4.3 times for the upper- and lower-view images, respectively. The random error of the distance measurement is 4.9% and the systematic error is 5.7% up to objects 14 meters apart, which is improved almost nine times compared to the conventional method. The remaining distance errors is due to a degraded optical resolution of the prototype, which we plan to make further improvements as future work.



### FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.07938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.07938v1)
- **Published**: 2021-08-18 02:10:26+00:00
- **Updated**: 2021-08-18 02:10:26+00:00
- **Authors**: Chenxu Zhang, Yifan Zhao, Yifei Huang, Ming Zeng, Saifeng Ni, Madhukar Budagavi, Xiaohu Guo
- **Comment**: 10 pages, 9 figures. Accepted by ICCV 2021
- **Journal**: None
- **Summary**: In this paper, we propose a talking face generation method that takes an audio signal as input and a short target video clip as reference, and synthesizes a photo-realistic video of the target face with natural lip motions, head poses, and eye blinks that are in-sync with the input audio signal. We note that the synthetic face attributes include not only explicit ones such as lip motions that have high correlations with speech, but also implicit ones such as head poses and eye blinks that have only weak correlation with the input audio. To model such complicated relationships among different face attributes with input audio, we propose a FACe Implicit Attribute Learning Generative Adversarial Network (FACIAL-GAN), which integrates the phonetics-aware, context-aware, and identity-aware information to synthesize the 3D face animation with realistic motions of lips, head poses, and eye blinks. Then, our Rendering-to-Video network takes the rendered face images and the attention map of eye blinks as input to generate the photo-realistic output video frames. Experimental results and user studies show our method can generate realistic talking face videos with not only synchronized lip motions, but also natural head movements and eye blinks, with better qualities than the results of state-of-the-art methods.



### Object Disparity
- **Arxiv ID**: http://arxiv.org/abs/2108.07939v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.07939v1)
- **Published**: 2021-08-18 02:11:28+00:00
- **Updated**: 2021-08-18 02:11:28+00:00
- **Authors**: Ynjiun Paul Wang
- **Comment**: 10 pages, 13 figures, 7 tables
- **Journal**: None
- **Summary**: Most of stereo vision works are focusing on computing the dense pixel disparity of a given pair of left and right images. A camera pair usually required lens undistortion and stereo calibration to provide an undistorted epipolar line calibrated image pair for accurate dense pixel disparity computation. Due to noise, object occlusion, repetitive or lack of texture and limitation of matching algorithms, the pixel disparity accuracy usually suffers the most at those object boundary areas. Although statistically the total number of pixel disparity errors might be low (under 2% according to the Kitti Vision Benchmark of current top ranking algorithms), the percentage of these disparity errors at object boundaries are very high. This renders the subsequence 3D object distance detection with much lower accuracy than desired. This paper proposed a different approach for solving a 3D object distance detection by detecting object disparity directly without going through a dense pixel disparity computation. An example squeezenet Object Disparity-SSD (OD-SSD) was constructed to demonstrate an efficient object disparity detection with comparable accuracy compared with Kitti dataset pixel disparity ground truth. Further training and testing results with mixed image dataset captured by several different stereo systems may suggest that an OD-SSD might be agnostic to stereo system parameters such as a baseline, FOV, lens distortion, even left/right camera epipolar line misalignment.



### STN PLAD: A Dataset for Multi-Size Power Line Assets Detection in High-Resolution UAV Images
- **Arxiv ID**: http://arxiv.org/abs/2108.07944v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.07944v3)
- **Published**: 2021-08-18 02:26:05+00:00
- **Updated**: 2021-09-02 22:34:20+00:00
- **Authors**: André Luiz Buarque Vieira-e-Silva, Heitor Felix, Thiago de Menezes Chaves, Francisco Paulo Magalhães Simões, Veronica Teichrieb, Michel Mozinho dos Santos, Hemir da Cunha Santiago, Virginia Adélia Cordeiro Sgotti, Henrique Baptista Duffles Teixeira Lott Neto
- **Comment**: Accepted for presentation at SIBGRAPI 2021
- **Journal**: None
- **Summary**: Many power line companies are using UAVs to perform their inspection processes instead of putting their workers at risk by making them climb high voltage power line towers, for instance. A crucial task for the inspection is to detect and classify assets in the power transmission lines. However, public data related to power line assets are scarce, preventing a faster evolution of this area. This work proposes the Power Line Assets Dataset, containing high-resolution and real-world images of multiple high-voltage power line components. It has 2,409 annotated objects divided into five classes: transmission tower, insulator, spacer, tower plate, and Stockbridge damper, which vary in size (resolution), orientation, illumination, angulation, and background. This work also presents an evaluation with popular deep object detection methods, showing considerable room for improvement. The STN PLAD dataset is publicly available at https://github.com/andreluizbvs/PLAD.



### Learning Conditional Knowledge Distillation for Degraded-Reference Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2108.07948v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.07948v1)
- **Published**: 2021-08-18 02:35:08+00:00
- **Updated**: 2021-08-18 02:35:08+00:00
- **Authors**: Heliang Zheng, Huan Yang, Jianlong Fu, Zheng-Jun Zha, Jiebo Luo
- **Comment**: None
- **Journal**: ICCV 2021 Camera Ready Version
- **Summary**: An important scenario for image quality assessment (IQA) is to evaluate image restoration (IR) algorithms. The state-of-the-art approaches adopt a full-reference paradigm that compares restored images with their corresponding pristine-quality images. However, pristine-quality images are usually unavailable in blind image restoration tasks and real-world scenarios. In this paper, we propose a practical solution named degraded-reference IQA (DR-IQA), which exploits the inputs of IR models, degraded images, as references. Specifically, we extract reference information from degraded images by distilling knowledge from pristine-quality images. The distillation is achieved through learning a reference space, where various degraded images are encouraged to share the same feature statistics with pristine-quality images. And the reference space is optimized to capture deep image priors that are useful for quality assessment. Note that pristine-quality images are only used during training. Our work provides a powerful and differentiable metric for blind IRs, especially for GAN-based methods. Extensive experiments show that our results can even be close to the performance of full-reference settings.



### DeepFake MNIST+: A DeepFake Facial Animation Dataset
- **Arxiv ID**: http://arxiv.org/abs/2108.07949v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.07949v1)
- **Published**: 2021-08-18 02:37:17+00:00
- **Updated**: 2021-08-18 02:37:17+00:00
- **Authors**: Jiajun Huang, Xueyu Wang, Bo Du, Pei Du, Chang Xu
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: The DeepFakes, which are the facial manipulation techniques, is the emerging threat to digital society. Various DeepFake detection methods and datasets are proposed for detecting such data, especially for face-swapping. However, recent researches less consider facial animation, which is also important in the DeepFake attack side. It tries to animate a face image with actions provided by a driving video, which also leads to a concern about the security of recent payment systems that reply on liveness detection to authenticate real users via recognising a sequence of user facial actions. However, our experiments show that the existed datasets are not sufficient to develop reliable detection methods. While the current liveness detector cannot defend such videos as the attack. As a response, we propose a new human face animation dataset, called DeepFake MNIST+, generated by a SOTA image animation generator. It includes 10,000 facial animation videos in ten different actions, which can spoof the recent liveness detectors. A baseline detection method and a comprehensive analysis of the method is also included in this paper. In addition, we analyze the proposed dataset's properties and reveal the difficulty and importance of detecting animation datasets under different types of motion and compression quality.



### Self-Supervised Visual Representations Learning by Contrastive Mask Prediction
- **Arxiv ID**: http://arxiv.org/abs/2108.07954v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.07954v1)
- **Published**: 2021-08-18 02:50:33+00:00
- **Updated**: 2021-08-18 02:50:33+00:00
- **Authors**: Yucheng Zhao, Guangting Wang, Chong Luo, Wenjun Zeng, Zheng-Jun Zha
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Advanced self-supervised visual representation learning methods rely on the instance discrimination (ID) pretext task. We point out that the ID task has an implicit semantic consistency (SC) assumption, which may not hold in unconstrained datasets. In this paper, we propose a novel contrastive mask prediction (CMP) task for visual representation learning and design a mask contrast (MaskCo) framework to implement the idea. MaskCo contrasts region-level features instead of view-level features, which makes it possible to identify the positive sample without any assumptions. To solve the domain gap between masked and unmasked features, we design a dedicated mask prediction head in MaskCo. This module is shown to be the key to the success of the CMP. We evaluated MaskCo on training datasets beyond ImageNet and compare its performance with MoCo V2. Results show that MaskCo achieves comparable performance with MoCo V2 using ImageNet training dataset, but demonstrates a stronger performance across a range of downstream tasks when COCO or Conceptual Captions are used for training. MaskCo provides a promising alternative to the ID-based methods for self-supervised learning in the wild.



### WRICNet:A Weighted Rich-scale Inception Coder Network for Multi-Resolution Remote Sensing Image Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.07955v1
- **DOI**: 10.1109/TGRS.2022.3145652
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.07955v1)
- **Published**: 2021-08-18 02:56:11+00:00
- **Updated**: 2021-08-18 02:56:11+00:00
- **Authors**: Yu Jiang, Lei Hu, Yongmei Zhang, Xin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Majority models of remote sensing image changing detection can only get great effect in a specific resolution data set. With the purpose of improving change detection effectiveness of the model in the multi-resolution data set, a weighted rich-scale inception coder network (WRICNet) is proposed in this article, which can make a great fusion of shallow multi-scale features, and deep multi-scale features. The weighted rich-scale inception module of the proposed can obtain shallow multi-scale features, the weighted rich-scale coder module can obtain deep multi-scale features. The weighted scale block assigns appropriate weights to features of different scales, which can strengthen expressive ability of the edge of the changing area. The performance experiments on the multi-resolution data set demonstrate that, compared to the comparative methods, the proposed can further reduce the false alarm outside the change area, and the missed alarm in the change area, besides, the edge of the change area is more accurate. The ablation study of the proposed shows that the training strategy, and improvements of this article can improve the effectiveness of change detection.



### SynFace: Face Recognition with Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2108.07960v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.07960v2)
- **Published**: 2021-08-18 03:41:54+00:00
- **Updated**: 2021-12-03 13:12:40+00:00
- **Authors**: Haibo Qiu, Baosheng Yu, Dihong Gong, Zhifeng Li, Wei Liu, Dacheng Tao
- **Comment**: Accepted by ICCV 2021. Code is available at
  https://github.com/haibo-qiu/SynFace
- **Journal**: None
- **Summary**: With the recent success of deep neural networks, remarkable progress has been achieved on face recognition. However, collecting large-scale real-world training data for face recognition has turned out to be challenging, especially due to the label noise and privacy issues. Meanwhile, existing face recognition datasets are usually collected from web images, lacking detailed annotations on attributes (e.g., pose and expression), so the influences of different attributes on face recognition have been poorly investigated. In this paper, we address the above-mentioned issues in face recognition using synthetic face images, i.e., SynFace. Specifically, we first explore the performance gap between recent state-of-the-art face recognition models trained with synthetic and real face images. We then analyze the underlying causes behind the performance gap, e.g., the poor intra-class variations and the domain gap between synthetic and real face images. Inspired by this, we devise the SynFace with identity mixup (IM) and domain mixup (DM) to mitigate the above performance gap, demonstrating the great potentials of synthetic data for face recognition. Furthermore, with the controllable face synthesis model, we can easily manage different factors of synthetic face generation, including pose, expression, illumination, the number of identities, and samples per identity. Therefore, we also perform a systematically empirical analysis on synthetic face images to provide some insights on how to effectively utilize synthetic data for face recognition.



### A Simple Framework for 3D Lensless Imaging with Programmable Masks
- **Arxiv ID**: http://arxiv.org/abs/2108.07966v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.07966v1)
- **Published**: 2021-08-18 04:05:33+00:00
- **Updated**: 2021-08-18 04:05:33+00:00
- **Authors**: Yucheng Zheng, Yi Hua, Aswin C. Sankaranarayanan, M. Salman Asif
- **Comment**: Supplementary material available at
  https://github.com/CSIPlab/Programmable3Dcam.git
- **Journal**: International Conference on Computer Vision (ICCV) 2021
- **Summary**: Lensless cameras provide a framework to build thin imaging systems by replacing the lens in a conventional camera with an amplitude or phase mask near the sensor. Existing methods for lensless imaging can recover the depth and intensity of the scene, but they require solving computationally-expensive inverse problems. Furthermore, existing methods struggle to recover dense scenes with large depth variations. In this paper, we propose a lensless imaging system that captures a small number of measurements using different patterns on a programmable mask. In this context, we make three contributions. First, we present a fast recovery algorithm to recover textures on a fixed number of depth planes in the scene. Second, we consider the mask design problem, for programmable lensless cameras, and provide a design template for optimizing the mask patterns with the goal of improving depth estimation. Third, we use a refinement network as a post-processing step to identify and remove artifacts in the reconstruction. These modifications are evaluated extensively with experimental results on a lensless camera prototype to showcase the performance benefits of the optimized masks and recovery algorithms over the state of the art.



### Thermal Image Processing via Physics-Inspired Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.07973v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.07973v2)
- **Published**: 2021-08-18 04:57:48+00:00
- **Updated**: 2021-08-25 21:10:02+00:00
- **Authors**: Vishwanath Saragadam, Akshat Dave, Ashok Veeraraghavan, Richard Baraniuk
- **Comment**: Accepted to 2nd ICCV workshop on Learning for Computational Imaging
  (LCI)
- **Journal**: None
- **Summary**: We introduce DeepIR, a new thermal image processing framework that combines physically accurate sensor modeling with deep network-based image representation. Our key enabling observations are that the images captured by thermal sensors can be factored into slowly changing, scene-independent sensor non-uniformities (that can be accurately modeled using physics) and a scene-specific radiance flux (that is well-represented using a deep network-based regularizer). DeepIR requires neither training data nor periodic ground-truth calibration with a known black body target--making it well suited for practical computer vision tasks. We demonstrate the power of going DeepIR by developing new denoising and super-resolution algorithms that exploit multiple images of the scene captured with camera jitter. Simulated and real data experiments demonstrate that DeepIR can perform high-quality non-uniformity correction with as few as three images, achieving a 10dB PSNR improvement over competing approaches.



### Unsupervised Image Generation with Infinite Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.07975v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.07975v1)
- **Published**: 2021-08-18 05:03:19+00:00
- **Updated**: 2021-08-18 05:03:19+00:00
- **Authors**: Hui Ying, He Wang, Tianjia Shao, Yin Yang, Kun Zhou
- **Comment**: 18 pages, 11 figures
- **Journal**: None
- **Summary**: Image generation has been heavily investigated in computer vision, where one core research challenge is to generate images from arbitrarily complex distributions with little supervision. Generative Adversarial Networks (GANs) as an implicit approach have achieved great successes in this direction and therefore been employed widely. However, GANs are known to suffer from issues such as mode collapse, non-structured latent space, being unable to compute likelihoods, etc. In this paper, we propose a new unsupervised non-parametric method named mixture of infinite conditional GANs or MIC-GANs, to tackle several GAN issues together, aiming for image generation with parsimonious prior knowledge. Through comprehensive evaluations across different datasets, we show that MIC-GANs are effective in structuring the latent space and avoiding mode collapse, and outperform state-of-the-art methods. MICGANs are adaptive, versatile, and robust. They offer a promising solution to several well-known GAN issues. Code available: github.com/yinghdb/MICGANs.



### Classification and reconstruction of spatially overlapping phase images using diffractive optical networks
- **Arxiv ID**: http://arxiv.org/abs/2108.07977v1
- **DOI**: 10.1038/s41598-022-12020-y
- **Categories**: **physics.optics**, cs.CV, cs.NE, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2108.07977v1)
- **Published**: 2021-08-18 05:15:05+00:00
- **Updated**: 2021-08-18 05:15:05+00:00
- **Authors**: Deniz Mengu, Muhammed Veli, Yair Rivenson, Aydogan Ozcan
- **Comment**: 30 Pages, 7 Figures, 2 Tables
- **Journal**: Scientific Reports (2022)
- **Summary**: Diffractive optical networks unify wave optics and deep learning to all-optically compute a given machine learning or computational imaging task as the light propagates from the input to the output plane. Here, we report the design of diffractive optical networks for the classification and reconstruction of spatially overlapping, phase-encoded objects. When two different phase-only objects spatially overlap, the individual object functions are perturbed since their phase patterns are summed up. The retrieval of the underlying phase images from solely the overlapping phase distribution presents a challenging problem, the solution of which is generally not unique. We show that through a task-specific training process, passive diffractive networks composed of successive transmissive layers can all-optically and simultaneously classify two different randomly-selected, spatially overlapping phase images at the input. After trained with ~550 million unique combinations of phase-encoded handwritten digits from the MNIST dataset, our blind testing results reveal that the diffractive network achieves an accuracy of >85.8% for all-optical classification of two overlapping phase images of new handwritten digits. In addition to all-optical classification of overlapping phase objects, we also demonstrate the reconstruction of these phase images based on a shallow electronic neural network that uses the highly compressed output of the diffractive network as its input (with e.g., ~20-65 times less number of pixels) to rapidly reconstruct both of the phase images, despite their spatial overlap and related phase ambiguity. The presented phase image classification and reconstruction framework might find applications in e.g., computational imaging, microscopy and quantitative phase imaging fields.



### A New Journey from SDRTV to HDRTV
- **Arxiv ID**: http://arxiv.org/abs/2108.07978v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.07978v2)
- **Published**: 2021-08-18 05:17:08+00:00
- **Updated**: 2021-09-25 16:42:11+00:00
- **Authors**: Xiangyu Chen, Zhengwen Zhang, Jimmy S. Ren, Lynhoo Tian, Yu Qiao, Chao Dong
- **Comment**: Accepted to ICCV
- **Journal**: None
- **Summary**: Nowadays modern displays are capable to render video content with high dynamic range (HDR) and wide color gamut (WCG). However, most available resources are still in standard dynamic range (SDR). Therefore, there is an urgent demand to transform existing SDR-TV contents into their HDR-TV versions. In this paper, we conduct an analysis of SDRTV-to-HDRTV task by modeling the formation of SDRTV/HDRTV content. Base on the analysis, we propose a three-step solution pipeline including adaptive global color mapping, local enhancement and highlight generation. Moreover, the above analysis inspires us to present a lightweight network that utilizes global statistics as guidance to conduct image-adaptive color mapping. In addition, we construct a dataset using HDR videos in HDR10 standard, named HDRTV1K, and select five metrics to evaluate the results of SDRTV-to-HDRTV algorithms. Furthermore, our final results achieve state-of-the-art performance in quantitative comparisons and visual quality. The code and dataset are available at https://github.com/chxy95/HDRTVNet.



### A New Bidirectional Unsupervised Domain Adaptation Segmentation Framework
- **Arxiv ID**: http://arxiv.org/abs/2108.07979v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.07979v1)
- **Published**: 2021-08-18 05:25:11+00:00
- **Updated**: 2021-08-18 05:25:11+00:00
- **Authors**: Munan Ning, Cheng Bian, Dong Wei, Chenglang Yuan, Yaohua Wang, Yang Guo, Kai Ma, Yefeng Zheng
- **Comment**: IPMI 2021
- **Journal**: None
- **Summary**: Domain shift happens in cross-domain scenarios commonly because of the wide gaps between different domains: when applying a deep learning model well-trained in one domain to another target domain, the model usually performs poorly. To tackle this problem, unsupervised domain adaptation (UDA) techniques are proposed to bridge the gap between different domains, for the purpose of improving model performance without annotation in the target domain. Particularly, UDA has a great value for multimodal medical image analysis, where annotation difficulty is a practical concern. However, most existing UDA methods can only achieve satisfactory improvements in one adaptation direction (e.g., MRI to CT), but often perform poorly in the other (CT to MRI), limiting their practical usage. In this paper, we propose a bidirectional UDA (BiUDA) framework based on disentangled representation learning for equally competent two-way UDA performances. This framework employs a unified domain-aware pattern encoder which not only can adaptively encode images in different domains through a domain controller, but also improve model efficiency by eliminating redundant parameters. Furthermore, to avoid distortion of contents and patterns of input images during the adaptation process, a content-pattern consistency loss is introduced. Additionally, for better UDA segmentation performance, a label consistency strategy is proposed to provide extra supervision by recomposing target-domain-styled images and corresponding source-domain annotations. Comparison experiments and ablation studies conducted on two public datasets demonstrate the superiority of our BiUDA framework to current state-of-the-art UDA methods and the effectiveness of its novel designs. By successfully addressing two-way adaptations, our BiUDA framework offers a flexible solution of UDA techniques to the real-world scenario.



### Structured Outdoor Architecture Reconstruction by Exploration and Classification
- **Arxiv ID**: http://arxiv.org/abs/2108.07990v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.07990v1)
- **Published**: 2021-08-18 06:28:26+00:00
- **Updated**: 2021-08-18 06:28:26+00:00
- **Authors**: Fuyang Zhang, Xiang Xu, Nelson Nauata, Yasutaka Furukawa
- **Comment**: 2021 International Conference on Computer Vision (ICCV 2021)
- **Journal**: None
- **Summary**: This paper presents an explore-and-classify framework for structured architectural reconstruction from an aerial image. Starting from a potentially imperfect building reconstruction by an existing algorithm, our approach 1) explores the space of building models by modifying the reconstruction via heuristic actions; 2) learns to classify the correctness of building models while generating classification labels based on the ground-truth, and 3) repeat. At test time, we iterate exploration and classification, seeking for a result with the best classification score. We evaluate the approach using initial reconstructions by two baselines and two state-of-the-art reconstruction algorithms. Qualitative and quantitative evaluations demonstrate that our approach consistently improves the reconstruction quality from every initial reconstruction.



### Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search
- **Arxiv ID**: http://arxiv.org/abs/2108.08910v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2108.08910v2)
- **Published**: 2021-08-18 06:47:31+00:00
- **Updated**: 2023-02-14 05:52:24+00:00
- **Authors**: Zheng Zhan, Yifan Gong, Pu Zhao, Geng Yuan, Wei Niu, Yushu Wu, Tianyun Zhang, Malith Jayaweera, David Kaeli, Bin Ren, Xue Lin, Yanzhi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Though recent years have witnessed remarkable progress in single image super-resolution (SISR) tasks with the prosperous development of deep neural networks (DNNs), the deep learning methods are confronted with the computation and memory consumption issues in practice, especially for resource-limited platforms such as mobile devices. To overcome the challenge and facilitate the real-time deployment of SISR tasks on mobile, we combine neural architecture search with pruning search and propose an automatic search framework that derives sparse super-resolution (SR) models with high image quality while satisfying the real-time inference requirement. To decrease the search cost, we leverage the weight sharing strategy by introducing a supernet and decouple the search problem into three stages, including supernet construction, compiler-aware architecture and pruning search, and compiler-aware pruning ratio search. With the proposed framework, we are the first to achieve real-time SR inference (with only tens of milliseconds per frame) for implementing 720p resolution with competitive image quality (in terms of PSNR and SSIM) on mobile platforms (Samsung Galaxy S20).



### Multi-Anchor Active Domain Adaptation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.08012v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08012v1)
- **Published**: 2021-08-18 07:33:13+00:00
- **Updated**: 2021-08-18 07:33:13+00:00
- **Authors**: Munan Ning, Donghuan Lu, Dong Wei, Cheng Bian, Chenglang Yuan, Shuang Yu, Kai Ma, Yefeng Zheng
- **Comment**: ICCV 2021 Oral
- **Journal**: None
- **Summary**: Unsupervised domain adaption has proven to be an effective approach for alleviating the intensive workload of manual annotation by aligning the synthetic source-domain data and the real-world target-domain samples. Unfortunately, mapping the target-domain distribution to the source-domain unconditionally may distort the essential structural information of the target-domain data. To this end, we firstly propose to introduce a novel multi-anchor based active learning strategy to assist domain adaptation regarding the semantic segmentation task. By innovatively adopting multiple anchors instead of a single centroid, the source domain can be better characterized as a multimodal distribution, thus more representative and complimentary samples are selected from the target domain. With little workload to manually annotate these active samples, the distortion of the target-domain distribution can be effectively alleviated, resulting in a large performance gain. The multi-anchor strategy is additionally employed to model the target-distribution. By regularizing the latent representation of the target samples compact around multiple anchors through a novel soft alignment loss, more precise segmentation can be achieved. Extensive experiments are conducted on public datasets to demonstrate that the proposed approach outperforms state-of-the-art methods significantly, along with thorough ablation study to verify the effectiveness of each component.



### Deep Hybrid Self-Prior for Full 3D Mesh Generation
- **Arxiv ID**: http://arxiv.org/abs/2108.08017v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08017v2)
- **Published**: 2021-08-18 07:44:21+00:00
- **Updated**: 2021-08-24 09:32:23+00:00
- **Authors**: Xingkui Wei, Zhengqing Chen, Yanwei Fu, Zhaopeng Cui, Yinda Zhang
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: We present a deep learning pipeline that leverages network self-prior to recover a full 3D model consisting of both a triangular mesh and a texture map from the colored 3D point cloud. Different from previous methods either exploiting 2D self-prior for image editing or 3D self-prior for pure surface reconstruction, we propose to exploit a novel hybrid 2D-3D self-prior in deep neural networks to significantly improve the geometry quality and produce a high-resolution texture map, which is typically missing from the output of commodity-level 3D scanners. In particular, we first generate an initial mesh using a 3D convolutional neural network with 3D self-prior, and then encode both 3D information and color information in the 2D UV atlas, which is further refined by 2D convolutional neural networks with the self-prior. In this way, both 2D and 3D self-priors are utilized for the mesh and texture recovery. Experiments show that, without the need of any additional training data, our method recovers the 3D textured mesh model of high quality from sparse input, and outperforms the state-of-the-art methods in terms of both the geometry and texture quality.



### RANK-NOSH: Efficient Predictor-Based Architecture Search via Non-Uniform Successive Halving
- **Arxiv ID**: http://arxiv.org/abs/2108.08019v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08019v1)
- **Published**: 2021-08-18 07:45:21+00:00
- **Updated**: 2021-08-18 07:45:21+00:00
- **Authors**: Ruochen Wang, Xiangning Chen, Minhao Cheng, Xiaocheng Tang, Cho-Jui Hsieh
- **Comment**: To Appear in ICCV2021. The code will be released shortly at
  https://github.com/ruocwang
- **Journal**: None
- **Summary**: Predictor-based algorithms have achieved remarkable performance in the Neural Architecture Search (NAS) tasks. However, these methods suffer from high computation costs, as training the performance predictor usually requires training and evaluating hundreds of architectures from scratch. Previous works along this line mainly focus on reducing the number of architectures required to fit the predictor. In this work, we tackle this challenge from a different perspective - improve search efficiency by cutting down the computation budget of architecture training. We propose NOn-uniform Successive Halving (NOSH), a hierarchical scheduling algorithm that terminates the training of underperforming architectures early to avoid wasting budget. To effectively leverage the non-uniform supervision signals produced by NOSH, we formulate predictor-based architecture search as learning to rank with pairwise comparisons. The resulting method - RANK-NOSH, reduces the search budget by ~5x while achieving competitive or even better performance than previous state-of-the-art predictor-based methods on various spaces and datasets.



### Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates
- **Arxiv ID**: http://arxiv.org/abs/2108.08020v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08020v2)
- **Published**: 2021-08-18 07:53:36+00:00
- **Updated**: 2021-11-29 07:13:04+00:00
- **Authors**: Shenhan Qian, Zhi Tu, Yihao Zhi, Wen Liu, Shenghua Gao
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Co-speech gesture generation is to synthesize a gesture sequence that not only looks real but also matches with the input speech audio. Our method generates the movements of a complete upper body, including arms, hands, and the head. Although recent data-driven methods achieve great success, challenges still exist, such as limited variety, poor fidelity, and lack of objective metrics. Motivated by the fact that the speech cannot fully determine the gesture, we design a method that learns a set of gesture template vectors to model the latent conditions, which relieve the ambiguity. For our method, the template vector determines the general appearance of a generated gesture sequence, while the speech audio drives subtle movements of the body, both indispensable for synthesizing a realistic gesture sequence. Due to the intractability of an objective metric for gesture-speech synchronization, we adopt the lip-sync error as a proxy metric to tune and evaluate the synchronization ability of our model. Extensive experiments show the superiority of our method in both objective and subjective evaluations on fidelity and synchronization.



### Variational Attention: Propagating Domain-Specific Knowledge for Multi-Domain Learning in Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2108.08023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08023v1)
- **Published**: 2021-08-18 08:06:37+00:00
- **Updated**: 2021-08-18 08:06:37+00:00
- **Authors**: Binghui Chen, Zhaoyi Yan, Ke Li, Pengyu Li, Biao Wang, Wangmeng Zuo, Lei Zhang
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: In crowd counting, due to the problem of laborious labelling, it is perceived intractability of collecting a new large-scale dataset which has plentiful images with large diversity in density, scene, etc. Thus, for learning a general model, training with data from multiple different datasets might be a remedy and be of great value. In this paper, we resort to the multi-domain joint learning and propose a simple but effective Domain-specific Knowledge Propagating Network (DKPNet)1 for unbiasedly learning the knowledge from multiple diverse data domains at the same time. It is mainly achieved by proposing the novel Variational Attention(VA) technique for explicitly modeling the attention distributions for different domains. And as an extension to VA, Intrinsic Variational Attention(InVA) is proposed to handle the problems of over-lapped domains and sub-domains. Extensive experiments have been conducted to validate the superiority of our DKPNet over several popular datasets, including ShanghaiTech A/B, UCF-QNRF and NWPU.



### Unbiased IoU for Spherical Image Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.08029v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08029v1)
- **Published**: 2021-08-18 08:18:37+00:00
- **Updated**: 2021-08-18 08:18:37+00:00
- **Authors**: Qiang Zhao, Bin Chen, Hang Xu, Yike Ma, Xiaodong Li, Bailan Feng, Chenggang Yan, Feng Dai
- **Comment**: None
- **Journal**: None
- **Summary**: As one of the most fundamental and challenging problems in computer vision, object detection tries to locate object instances and find their categories in natural images. The most important step in the evaluation of object detection algorithm is calculating the intersection-over-union (IoU) between the predicted bounding box and the ground truth one. Although this procedure is well-defined and solved for planar images, it is not easy for spherical image object detection. Existing methods either compute the IoUs based on biased bounding box representations or make excessive approximations, thus would give incorrect results. In this paper, we first identify that spherical rectangles are unbiased bounding boxes for objects in spherical images, and then propose an analytical method for IoU calculation without any approximations. Based on the unbiased representation and calculation, we also present an anchor free object detection algorithm for spherical images. The experiments on two spherical object detection datasets show that the proposed method can achieve better performance than existing methods.



### Adaptive Graph Convolution for Point Cloud Analysis
- **Arxiv ID**: http://arxiv.org/abs/2108.08035v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08035v2)
- **Published**: 2021-08-18 08:38:52+00:00
- **Updated**: 2021-08-19 07:07:34+00:00
- **Authors**: Haoran Zhou, Yidan Feng, Mingsheng Fang, Mingqiang Wei, Jing Qin, Tong Lu
- **Comment**: Camera-ready, to be published in ICCV 2021
- **Journal**: None
- **Summary**: Convolution on 3D point clouds that generalized from 2D grid-like domains is widely researched yet far from perfect. The standard convolution characterises feature correspondences indistinguishably among 3D points, presenting an intrinsic limitation of poor distinctive feature learning. In this paper, we propose Adaptive Graph Convolution (AdaptConv) which generates adaptive kernels for points according to their dynamically learned features. Compared with using a fixed/isotropic kernel, AdaptConv improves the flexibility of point cloud convolutions, effectively and precisely capturing the diverse relations between points from different semantic parts. Unlike popular attentional weight schemes, the proposed AdaptConv implements the adaptiveness inside the convolution operation instead of simply assigning different weights to the neighboring points. Extensive qualitative and quantitative evaluations show that our method outperforms state-of-the-art point cloud classification and segmentation approaches on several benchmark datasets. Our code is available at https://github.com/hrzhou2/AdaptConv-master.



### Few-Shot Batch Incremental Road Object Detection via Detector Fusion
- **Arxiv ID**: http://arxiv.org/abs/2108.08048v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.08048v1)
- **Published**: 2021-08-18 08:57:04+00:00
- **Updated**: 2021-08-18 08:57:04+00:00
- **Authors**: Anuj Tambwekar, Kshitij Agrawal, Anay Majee, Anbumani Subramanian
- **Comment**: accepted in 2nd Autonomous Vehicle Vision Workshop, ICCV2021
- **Journal**: None
- **Summary**: Incremental few-shot learning has emerged as a new and challenging area in deep learning, whose objective is to train deep learning models using very few samples of new class data, and none of the old class data. In this work we tackle the problem of batch incremental few-shot road object detection using data from the India Driving Dataset (IDD). Our approach, DualFusion, combines object detectors in a manner that allows us to learn to detect rare objects with very limited data, all without severely degrading the performance of the detector on the abundant classes. In the IDD OpenSet incremental few-shot detection task, we achieve a mAP50 score of 40.0 on the base classes and an overall mAP50 score of 38.8, both of which are the highest to date. In the COCO batch incremental few-shot detection task, we achieve a novel AP score of 9.9, surpassing the state-of-the-art novel class performance on the same by over 6.6 times.



### Multi-patch Feature Pyramid Network for Weakly Supervised Object Detection in Optical Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2108.08063v1
- **DOI**: 10.1109/TGRS.2021.3106442
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08063v1)
- **Published**: 2021-08-18 09:25:39+00:00
- **Updated**: 2021-08-18 09:25:39+00:00
- **Authors**: Pourya Shamsolmoali, Jocelyn Chanussot, Masoumeh Zareapoor, Huiyu Zhou, Jie Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is a challenging task in remote sensing because objects only occupy a few pixels in the images, and the models are required to simultaneously learn object locations and detection. Even though the established approaches well perform for the objects of regular sizes, they achieve weak performance when analyzing small ones or getting stuck in the local minima (e.g. false object parts). Two possible issues stand in their way. First, the existing methods struggle to perform stably on the detection of small objects because of the complicated background. Second, most of the standard methods used hand-crafted features, and do not work well on the detection of objects parts of which are missing. We here address the above issues and propose a new architecture with a multiple patch feature pyramid network (MPFP-Net). Different from the current models that during training only pursue the most discriminative patches, in MPFPNet the patches are divided into class-affiliated subsets, in which the patches are related and based on the primary loss function, a sequence of smooth loss functions are determined for the subsets to improve the model for collecting small object parts. To enhance the feature representation for patch selection, we introduce an effective method to regularize the residual values and make the fusion transition layers strictly norm-preserving. The network contains bottom-up and crosswise connections to fuse the features of different scales to achieve better accuracy, compared to several state-of-the-art object detection models. Also, the developed architecture is more efficient than the baselines.



### Panoramic Depth Estimation via Supervised and Unsupervised Learning in Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/2108.08076v1
- **DOI**: 10.1364/AO.432534
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.08076v1)
- **Published**: 2021-08-18 09:58:44+00:00
- **Updated**: 2021-08-18 09:58:44+00:00
- **Authors**: Keyang Zhou, Kailun Yang, Kaiwei Wang
- **Comment**: Accepted to Applied Optics. Code will be made publicly available at
  https://github.com/zzzkkkyyy/PADENet
- **Journal**: None
- **Summary**: Depth estimation, as a necessary clue to convert 2D images into the 3D space, has been applied in many machine vision areas. However, to achieve an entire surrounding 360-degree geometric sensing, traditional stereo matching algorithms for depth estimation are limited due to large noise, low accuracy, and strict requirements for multi-camera calibration. In this work, for a unified surrounding perception, we introduce panoramic images to obtain larger field of view. We extend PADENet first appeared in our previous conference work for outdoor scene understanding, to perform panoramic monocular depth estimation with a focus for indoor scenes. At the same time, we improve the training process of the neural network adapted to the characteristics of panoramic images. In addition, we fuse traditional stereo matching algorithm with deep learning methods and further improve the accuracy of depth predictions. With a comprehensive variety of experiments, this research demonstrates the effectiveness of our schemes aiming for indoor scene perception.



### DRDrV3: Complete Lesion Detection in Fundus Images Using Mask R-CNN, Transfer Learning, and LSTM
- **Arxiv ID**: http://arxiv.org/abs/2108.08095v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08095v1)
- **Published**: 2021-08-18 11:36:37+00:00
- **Updated**: 2021-08-18 11:36:37+00:00
- **Authors**: Farzan Shenavarmasouleh, Farid Ghareh Mohammadi, M. Hadi Amini, Thiab Taha, Khaled Rasheed, Hamid R. Arabnia
- **Comment**: The 7th International Conference on Health Informatics & Medical
  Systems (HIMS'21: July 26-29, 2021, USA)
- **Journal**: None
- **Summary**: Medical Imaging is one of the growing fields in the world of computer vision. In this study, we aim to address the Diabetic Retinopathy (DR) problem as one of the open challenges in medical imaging. In this research, we propose a new lesion detection architecture, comprising of two sub-modules, which is an optimal solution to detect and find not only the type of lesions caused by DR, their corresponding bounding boxes, and their masks; but also the severity level of the overall case. Aside from traditional accuracy, we also use two popular evaluation criteria to evaluate the outputs of our models, which are intersection over union (IOU) and mean average precision (mAP). We hypothesize that this new solution enables specialists to detect lesions with high confidence and estimate the severity of the damage with high accuracy.



### Image Collation: Matching illustrations in manuscripts
- **Arxiv ID**: http://arxiv.org/abs/2108.08109v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08109v1)
- **Published**: 2021-08-18 12:12:14+00:00
- **Updated**: 2021-08-18 12:12:14+00:00
- **Authors**: Ryad Kaoua, Xi Shen, Alexandra Durr, Stavros Lazaris, David Picard, Mathieu Aubry
- **Comment**: accepted to ICDAR 2021
- **Journal**: None
- **Summary**: Illustrations are an essential transmission instrument. For an historian, the first step in studying their evolution in a corpus of similar manuscripts is to identify which ones correspond to each other. This image collation task is daunting for manuscripts separated by many lost copies, spreading over centuries, which might have been completely re-organized and greatly modified to adapt to novel knowledge or belief and include hundreds of illustrations. Our contributions in this paper are threefold. First, we introduce the task of illustration collation and a large annotated public dataset to evaluate solutions, including 6 manuscripts of 2 different texts with more than 2 000 illustrations and 1 200 annotated correspondences. Second, we analyze state of the art similarity measures for this task and show that they succeed in simple cases but struggle for large manuscripts when the illustrations have undergone very significant changes and are discriminated only by fine details. Finally, we show clear evidence that significant performance boosts can be expected by exploiting cycle-consistent correspondences. Our code and data are available on http://imagine.enpc.fr/~shenx/ImageCollation.



### Rendering and Tracking the Directional TSDF: Modeling Surface Orientation for Coherent Maps
- **Arxiv ID**: http://arxiv.org/abs/2108.08115v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.08115v1)
- **Published**: 2021-08-18 12:37:15+00:00
- **Updated**: 2021-08-18 12:37:15+00:00
- **Authors**: Malte Splietker, Sven Behnke
- **Comment**: to be published in 10th European Conference on Mobile Robots (ECMR
  2021)
- **Journal**: None
- **Summary**: Dense real-time tracking and mapping from RGB-D images is an important tool for many robotic applications, such as navigation or grasping. The recently presented Directional Truncated Signed Distance Function (DTSDF) is an augmentation of the regular TSDF and shows potential for more coherent maps and improved tracking performance. In this work, we present methods for rendering depth- and color maps from the DTSDF, making it a true drop-in replacement for the regular TSDF in established trackers. We evaluate and show, that our method increases re-usability of mapped scenes. Furthermore, we add color integration which notably improves color-correctness at adjacent surfaces.



### Learning RAW-to-sRGB Mappings with Inaccurately Aligned Supervision
- **Arxiv ID**: http://arxiv.org/abs/2108.08119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08119v1)
- **Published**: 2021-08-18 12:41:36+00:00
- **Updated**: 2021-08-18 12:41:36+00:00
- **Authors**: Zhilu Zhang, Haolin Wang, Ming Liu, Ruohao Wang, Jiawei Zhang, Wangmeng Zuo
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Learning RAW-to-sRGB mapping has drawn increasing attention in recent years, wherein an input raw image is trained to imitate the target sRGB image captured by another camera. However, the severe color inconsistency makes it very challenging to generate well-aligned training pairs of input raw and target sRGB images. While learning with inaccurately aligned supervision is prone to causing pixel shift and producing blurry results. In this paper, we circumvent such issue by presenting a joint learning model for image alignment and RAW-to-sRGB mapping. To diminish the effect of color inconsistency in image alignment, we introduce to use a global color mapping (GCM) module to generate an initial sRGB image given the input raw image, which can keep the spatial location of the pixels unchanged, and the target sRGB image is utilized to guide GCM for converting the color towards it. Then a pre-trained optical flow estimation network (e.g., PWC-Net) is deployed to warp the target sRGB image to align with the GCM output. To alleviate the effect of inaccurately aligned supervision, the warped target sRGB image is leveraged to learn RAW-to-sRGB mapping. When training is done, the GCM module and optical flow network can be detached, thereby bringing no extra computation cost for inference. Experiments show that our method performs favorably against state-of-the-arts on ZRR and SR-RAW datasets. With our joint learning model, a light-weight backbone can achieve better quantitative and qualitative performance on ZRR dataset. Codes are available at https://github.com/cszhilu1998/RAW-to-sRGB.



### Target Adaptive Context Aggregation for Video Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2108.08121v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08121v1)
- **Published**: 2021-08-18 12:46:28+00:00
- **Updated**: 2021-08-18 12:46:28+00:00
- **Authors**: Yao Teng, Limin Wang, Zhifeng Li, Gangshan Wu
- **Comment**: ICCV 2021 camera-ready version
- **Journal**: None
- **Summary**: This paper deals with a challenging task of video scene graph generation (VidSGG), which could serve as a structured video representation for high-level understanding tasks. We present a new {\em detect-to-track} paradigm for this task by decoupling the context modeling for relation prediction from the complicated low-level entity tracking. Specifically, we design an efficient method for frame-level VidSGG, termed as {\em Target Adaptive Context Aggregation Network} (TRACE), with a focus on capturing spatio-temporal context information for relation recognition. Our TRACE framework streamlines the VidSGG pipeline with a modular design, and presents two unique blocks of Hierarchical Relation Tree (HRTree) construction and Target-adaptive Context Aggregation. More specific, our HRTree first provides an adpative structure for organizing possible relation candidates efficiently, and guides context aggregation module to effectively capture spatio-temporal structure information. Then, we obtain a contextualized feature representation for each relation candidate and build a classification head to recognize its relation category. Finally, we provide a simple temporal association strategy to track TRACE detected results to yield the video-level VidSGG. We perform experiments on two VidSGG benchmarks: ImageNet-VidVRD and Action Genome, and the results demonstrate that our TRACE achieves the state-of-the-art performance. The code and models are made available at \url{https://github.com/MCG-NJU/TRACE}.



### Hand Hygiene Video Classification Based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.08127v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08127v1)
- **Published**: 2021-08-18 12:56:07+00:00
- **Updated**: 2021-08-18 12:56:07+00:00
- **Authors**: Rashmi Bakshi
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, an extensive review of literature in the field of gesture recognition carried out along with the implementation of a simple classification system for hand hygiene stages based on deep learning solutions. A subset of robust dataset that consist of handwashing gestures with two hands as well as one-hand gestures such as linear hand movement utilized. A pretrained neural network model, RES Net 50, with image net weights used for the classification of 3 categories: Linear hand movement, rub hands palm to palm and rub hands with fingers interlaced movement. Correct predictions made for the first two classes with > 60% accuracy. A complete dataset along with increased number of classes and training steps will be explored as a future work.



### Statistical analysis of locally parameterized shapes
- **Arxiv ID**: http://arxiv.org/abs/2109.03027v1
- **DOI**: None
- **Categories**: **stat.ME**, cs.CV, q-bio.NC, stat.OT
- **Links**: [PDF](http://arxiv.org/pdf/2109.03027v1)
- **Published**: 2021-08-18 12:59:21+00:00
- **Updated**: 2021-08-18 12:59:21+00:00
- **Authors**: Mohsen Taheri, Jörn Schulz
- **Comment**: 25 pages, 20 figures
- **Journal**: None
- **Summary**: The alignment of shapes has been a crucial step in statistical shape analysis, for example, in calculating mean shape, detecting locational differences between two shape populations, and classification. Procrustes alignment is the most commonly used method and state of the art. In this work, we uncover that alignment might seriously affect the statistical analysis. For example, alignment can induce false shape differences and lead to misleading results and interpretations. We propose a novel hierarchical shape parameterization based on local coordinate systems. The local parameterized shapes are translation and rotation invariant. Thus, the inherent alignment problems from the commonly used global coordinate system for shape representation can be avoided using this parameterization. The new parameterization is also superior for shape deformation and simulation. The method's power is demonstrated on the hypothesis testing of simulated data as well as the left hippocampi of patients with Parkinson's disease and controls.



### Single-DARTS: Towards Stable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2108.08128v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08128v1)
- **Published**: 2021-08-18 13:00:39+00:00
- **Updated**: 2021-08-18 13:00:39+00:00
- **Authors**: Pengfei Hou, Ying Jin, Yukang Chen
- **Comment**: Accepted by ICCV 2021 NeurArch Workshp
- **Journal**: None
- **Summary**: Differentiable architecture search (DARTS) marks a milestone in Neural Architecture Search (NAS), boasting simplicity and small search costs. However, DARTS still suffers from frequent performance collapse, which happens when some operations, such as skip connections, zeroes and poolings, dominate the architecture. In this paper, we are the first to point out that the phenomenon is attributed to bi-level optimization. We propose Single-DARTS which merely uses single-level optimization, updating network weights and architecture parameters simultaneously with the same data batch. Even single-level optimization has been previously attempted, no literature provides a systematic explanation on this essential point. Replacing the bi-level optimization, Single-DARTS obviously alleviates performance collapse as well as enhances the stability of architecture search. Experiment results show that Single-DARTS achieves state-of-the-art performance on mainstream search spaces. For instance, on NAS-Benchmark-201, the searched architectures are nearly optimal ones. We also validate that the single-level optimization framework is much more stable than the bi-level one. We hope that this simple yet effective method will give some insights on differential architecture search. The code is available at https://github.com/PencilAndBike/Single-DARTS.git.



### Temporal Kernel Consistency for Blind Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2108.08305v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08305v1)
- **Published**: 2021-08-18 13:09:38+00:00
- **Updated**: 2021-08-18 13:09:38+00:00
- **Authors**: Lichuan Xiang, Royson Lee, Mohamed S. Abdelfattah, Nicholas D. Lane, Hongkai Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based blind super-resolution (SR) methods have recently achieved unprecedented performance in upscaling frames with unknown degradation. These models are able to accurately estimate the unknown downscaling kernel from a given low-resolution (LR) image in order to leverage the kernel during restoration. Although these approaches have largely been successful, they are predominantly image-based and therefore do not exploit the temporal properties of the kernels across multiple video frames. In this paper, we investigated the temporal properties of the kernels and highlighted its importance in the task of blind video super-resolution. Specifically, we measured the kernel temporal consistency of real-world videos and illustrated how the estimated kernels might change per frame in videos of varying dynamicity of the scene and its objects. With this new insight, we revisited previous popular video SR approaches, and showed that previous assumptions of using a fixed kernel throughout the restoration process can lead to visual artifacts when upscaling real-world videos. In order to counteract this, we tailored existing single-image and video SR techniques to leverage kernel consistency during both kernel estimation and video upscaling processes. Extensive experiments on synthetic and real-world videos show substantial restoration gains quantitatively and qualitatively, achieving the new state-of-the-art in blind video SR and underlining the potential of exploiting kernel temporal consistency.



### Optimising Knee Injury Detection with Spatial Attention and Validating Localisation Ability
- **Arxiv ID**: http://arxiv.org/abs/2108.08136v1
- **DOI**: 10.1007/978-3-030-80432-9_6
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08136v1)
- **Published**: 2021-08-18 13:24:17+00:00
- **Updated**: 2021-08-18 13:24:17+00:00
- **Authors**: Niamh Belton, Ivan Welaratne, Adil Dahlan, Ronan T Hearne, Misgina Tsighe Hagos, Aonghus Lawlor, Kathleen M. Curran
- **Comment**: None
- **Journal**: Medical Image Understanding and Analysis (2021) 71-86
- **Summary**: This work employs a pre-trained, multi-view Convolutional Neural Network (CNN) with a spatial attention block to optimise knee injury detection. An open-source Magnetic Resonance Imaging (MRI) data set with image-level labels was leveraged for this analysis. As MRI data is acquired from three planes, we compare our technique using data from a single-plane and multiple planes (multi-plane). For multi-plane, we investigate various methods of fusing the planes in the network. This analysis resulted in the novel 'MPFuseNet' network and state-of-the-art Area Under the Curve (AUC) scores for detecting Anterior Cruciate Ligament (ACL) tears and Abnormal MRIs, achieving AUC scores of 0.977 and 0.957 respectively. We then developed an objective metric, Penalised Localisation Accuracy (PLA), to validate the model's localisation ability. This metric compares binary masks generated from Grad-Cam output and the radiologist's annotations on a sample of MRIs. We also extracted explainability features in a model-agnostic approach that were then verified as clinically relevant by the radiologist.



### Active Observer Visual Problem-Solving Methods are Dynamically Hypothesized, Deployed and Tested
- **Arxiv ID**: http://arxiv.org/abs/2108.08145v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2108.08145v2)
- **Published**: 2021-08-18 13:33:07+00:00
- **Updated**: 2022-01-24 20:19:59+00:00
- **Authors**: Markus D. Solbach, John K. Tsotsos
- **Comment**: Presented at The Ninth Advances in Cognitive Systems (ACS) Conference
  2021 (arXiv:2201.06134)
- **Journal**: None
- **Summary**: The STAR architecture was designed to test the value of the full Selective Tuning model of visual attention for complex real-world visuospatial tasks and behaviors. However, knowledge of how humans solve such tasks in 3D as active observers is lean. We thus devised a novel experimental setup and examined such behavior. We discovered that humans exhibit a variety of problem-solving strategies whose breadth and complexity are surprising and not easily handled by current methodologies. It is apparent that solution methods are dynamically composed by hypothesizing sequences of actions, testing them, and if they fail, trying different ones. The importance of active observation is striking as is the lack of any learning effect. These results inform our Cognitive Program representation of STAR extending its relevance to real-world tasks.



### Towards Deep and Efficient: A Deep Siamese Self-Attention Fully Efficient Convolutional Network for Change Detection in VHR Images
- **Arxiv ID**: http://arxiv.org/abs/2108.08157v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08157v1)
- **Published**: 2021-08-18 14:02:38+00:00
- **Updated**: 2021-08-18 14:02:38+00:00
- **Authors**: Hongruixuan Chen, Chen Wu, Bo Du
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, FCNs have attracted widespread attention in the CD field. In pursuit of better CD performance, it has become a tendency to design deeper and more complicated FCNs, which inevitably brings about huge numbers of parameters and an unbearable computational burden. With the goal of designing a quite deep architecture to obtain more precise CD results while simultaneously decreasing parameter numbers to improve efficiency, in this work, we present a very deep and efficient CD network, entitled EffCDNet. In EffCDNet, to reduce the numerous parameters associated with deep architecture, an efficient convolution consisting of depth-wise convolution and group convolution with a channel shuffle mechanism is introduced to replace standard convolutional layers. In terms of the specific network architecture, EffCDNet does not use mainstream UNet-like architecture, but rather adopts the architecture with a very deep encoder and a lightweight decoder. In the very deep encoder, two very deep siamese streams stacked by efficient convolution first extract two highly representative and informative feature maps from input image-pairs. Subsequently, an efficient ASPP module is designed to capture multi-scale change information. In the lightweight decoder, a recurrent criss-cross self-attention (RCCA) module is applied to efficiently utilize non-local similar feature representations to enhance discriminability for each pixel, thus effectively separating the changed and unchanged regions. Moreover, to tackle the optimization problem in confused pixels, two novel loss functions based on information entropy are presented. On two challenging CD datasets, our approach outperforms other SOTA FCN-based methods, with only benchmark-level parameter numbers and quite low computational overhead.



### Practical X-ray Gastric Cancer Screening Using Refined Stochastic Data Augmentation and Hard Boundary Box Training
- **Arxiv ID**: http://arxiv.org/abs/2108.08158v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08158v2)
- **Published**: 2021-08-18 14:04:52+00:00
- **Updated**: 2023-03-23 00:44:18+00:00
- **Authors**: Hideaki Okamoto, Takakiyo Nomura, Kazuhito Nabeshima, Jun Hashimoto, Hitoshi Iyatomi
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: In gastric cancer screening, X-rays can be performed by radiographers, allowing them to see far more patients than endoscopy, which can only be performed by physicians. However, due to subsequent diagnostic difficulties, the sensitivity of gastric X-ray is only 85.5%, and little research has been done on automated diagnostic aids that directly target gastric cancer. This paper proposes a practical gastric cancer screening system for X-ray images taken under realistic clinical imaging conditions. Our system not only provides a diagnostic result for each image, but also provides an explanation for the result by displaying candidate cancer areas with bounding boxes. Training object detection models to do this was very expensive in terms of assigning supervised labels, and had the disadvantage of not being able to use negative (i.e., non-cancer) data for training. Our proposal consists of two novel techniques: (1) refined stochastic gastric image augmentation (R-sGAIA) and (2) hard boundary box training (HBBT). The R-sGAIA probabilistically highlights the gastric folds in the X-ray image based on medical knowledge, thus increasing the detection efficiency of gastric cancer. The HBBT is a new, efficient, and versatile training method that can reduce the number of false positive detections by actively using negative samples. The results showed that the proposed R-sGAIA and HBBT significantly improved the F1 score by 5.9% compared to the baseline EfficientDet-D7 + RandAugment (F1: 57.8%, recall: 90.2%, precision: 42.5%). This score is higher than the physician's cancer detection rate, indicating that at least 2 out of 5 areas detected are cancerous, confirming the utility of gastric cancer screening.



### Specificity-preserving RGB-D Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.08162v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08162v2)
- **Published**: 2021-08-18 14:14:22+00:00
- **Updated**: 2022-01-09 02:59:30+00:00
- **Authors**: Tao Zhou, Deng-Ping Fan, Geng Chen, Yi Zhou, Huazhu Fu
- **Comment**: This is an extensive version and has been accepted by Computational
  Visual Media
- **Journal**: None
- **Summary**: Salient object detection (SOD) on RGB and depth images has attracted more and more research interests, due to its effectiveness and the fact that depth cues can now be conveniently captured. Existing RGB-D SOD models usually adopt different fusion strategies to learn a shared representation from the two modalities (\ie, RGB and depth), while few methods explicitly consider how to preserve modality-specific characteristics. In this study, we propose a novel framework, termed SPNet} (Specificity-preserving network), which benefits SOD performance by exploring both the shared information and modality-specific properties (\eg, specificity). Specifically, we propose to adopt two modality-specific networks and a shared learning network to generate individual and shared saliency prediction maps, respectively. To effectively fuse cross-modal features in the shared learning network, we propose a cross-enhanced integration module (CIM) and then propagate the fused feature to the next layer for integrating cross-level information. Moreover, to capture rich complementary multi-modal information for boosting the SOD performance, we propose a multi-modal feature aggregation (MFA) module to integrate the modality-specific features from each individual decoder into the shared decoder. By using a skip connection, the hierarchical features between the encoder and decoder layers can be fully combined. Extensive experiments demonstrate that our~\ours~outperforms cutting-edge approaches on six popular RGB-D SOD and three camouflaged object detection benchmarks. The project is publicly available at: https://github.com/taozh2017/SPNet.



### Generalized and Incremental Few-Shot Learning by Explicit Learning and Calibration without Forgetting
- **Arxiv ID**: http://arxiv.org/abs/2108.08165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08165v1)
- **Published**: 2021-08-18 14:21:43+00:00
- **Updated**: 2021-08-18 14:21:43+00:00
- **Authors**: Anna Kukleva, Hilde Kuehne, Bernt Schiele
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Both generalized and incremental few-shot learning have to deal with three major challenges: learning novel classes from only few samples per class, preventing catastrophic forgetting of base classes, and classifier calibration across novel and base classes. In this work we propose a three-stage framework that allows to explicitly and effectively address these challenges. While the first phase learns base classes with many samples, the second phase learns a calibrated classifier for novel classes from few samples while also preventing catastrophic forgetting. In the final phase, calibration is achieved across all classes. We evaluate the proposed framework on four challenging benchmark datasets for image and video few-shot classification and obtain state-of-the-art results for both generalized and incremental few shot learning.



### Deployment of Deep Neural Networks for Object Detection on Edge AI Devices with Runtime Optimization
- **Arxiv ID**: http://arxiv.org/abs/2108.08166v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.08166v1)
- **Published**: 2021-08-18 14:21:53+00:00
- **Updated**: 2021-08-18 14:21:53+00:00
- **Authors**: Lukas Stäcker, Juncong Fei, Philipp Heidenreich, Frank Bonarens, Jason Rambach, Didier Stricker, Christoph Stiller
- **Comment**: To present in ICCV 2021 (ERCVAD Workshop)
- **Journal**: None
- **Summary**: Deep neural networks have proven increasingly important for automotive scene understanding with new algorithms offering constant improvements of the detection performance. However, there is little emphasis on experiences and needs for deployment in embedded environments. We therefore perform a case study of the deployment of two representative object detection networks on an edge AI platform. In particular, we consider RetinaNet for image-based 2D object detection and PointPillars for LiDAR-based 3D object detection. We describe the modifications necessary to convert the algorithms from a PyTorch training environment to the deployment environment taking into account the available tools. We evaluate the runtime of the deployed DNN using two different libraries, TensorRT and TorchScript. In our experiments, we observe slight advantages of TensorRT for convolutional layers and TorchScript for fully connected layers. We also study the trade-off between runtime and performance, when selecting an optimized setup for deployment, and observe that quantization significantly reduces the runtime while having only little impact on the detection performance.



### Effect of Parameter Optimization on Classical and Learning-based Image Matching Methods
- **Arxiv ID**: http://arxiv.org/abs/2108.08179v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08179v2)
- **Published**: 2021-08-18 14:45:32+00:00
- **Updated**: 2021-08-28 21:16:17+00:00
- **Authors**: Ufuk Efe, Kutalmis Gokalp Ince, A. Aydin Alatan
- **Comment**: 8 pages, 2 figures, 3 tables, ICCV 2021 TradiCV Workshop
- **Journal**: None
- **Summary**: Deep learning-based image matching methods are improved significantly during the recent years. Although these methods are reported to outperform the classical techniques, the performance of the classical methods is not examined in detail. In this study, we compare classical and learning-based methods by employing mutual nearest neighbor search with ratio test and optimizing the ratio test threshold to achieve the best performance on two different performance metrics. After a fair comparison, the experimental results on HPatches dataset reveal that the performance gap between classical and learning-based methods is not that significant. Throughout the experiments, we demonstrated that SuperGlue is the state-of-the-art technique for the image matching problem on HPatches dataset. However, if a single parameter, namely ratio test threshold, is carefully optimized, a well-known traditional method SIFT performs quite close to SuperGlue and even outperforms in terms of mean matching accuracy (MMA) under 1 and 2 pixel thresholds. Moreover, a recent approach, DFM, which only uses pre-trained VGG features as descriptors and ratio test, is shown to outperform most of the well-trained learning-based methods. Therefore, we conclude that the parameters of any classical method should be analyzed carefully before comparing against a learning-based technique.



### ME-PCN: Point Completion Conditioned on Mask Emptiness
- **Arxiv ID**: http://arxiv.org/abs/2108.08187v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08187v2)
- **Published**: 2021-08-18 15:02:27+00:00
- **Updated**: 2021-10-14 14:24:59+00:00
- **Authors**: Bingchen Gong, Yinyu Nie, Yiqun Lin, Xiaoguang Han, Yizhou Yu
- **Comment**: Accepted to ICCV 2021; typos corrected
- **Journal**: None
- **Summary**: Point completion refers to completing the missing geometries of an object from incomplete observations. Main-stream methods predict the missing shapes by decoding a global feature learned from the input point cloud, which often leads to deficient results in preserving topology consistency and surface details. In this work, we present ME-PCN, a point completion network that leverages `emptiness' in 3D shape space. Given a single depth scan, previous methods often encode the occupied partial shapes while ignoring the empty regions (e.g. holes) in depth maps. In contrast, we argue that these `emptiness' clues indicate shape boundaries that can be used to improve topology representation and detail granularity on surfaces. Specifically, our ME-PCN encodes both the occupied point cloud and the neighboring `empty points'. It estimates coarse-grained but complete and reasonable surface points in the first stage, followed by a refinement stage to produce fine-grained surface details. Comprehensive experiments verify that our ME-PCN presents better qualitative and quantitative performance against the state-of-the-art. Besides, we further prove that our `emptiness' design is lightweight and easy to embed in existing methods, which shows consistent effectiveness in improving the CD and EMD scores.



### LSD-StructureNet: Modeling Levels of Structural Detail in 3D Part Hierarchies
- **Arxiv ID**: http://arxiv.org/abs/2108.13459v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2108.13459v2)
- **Published**: 2021-08-18 15:05:06+00:00
- **Updated**: 2021-09-07 15:10:56+00:00
- **Authors**: Dominic Roberts, Ara Danielyan, Hang Chu, Mani Golparvar-Fard, David Forsyth
- **Comment**: accepted by ICCV 2021
- **Journal**: None
- **Summary**: Generative models for 3D shapes represented by hierarchies of parts can generate realistic and diverse sets of outputs. However, existing models suffer from the key practical limitation of modelling shapes holistically and thus cannot perform conditional sampling, i.e. they are not able to generate variants on individual parts of generated shapes without modifying the rest of the shape. This is limiting for applications such as 3D CAD design that involve adjusting created shapes at multiple levels of detail. To address this, we introduce LSD-StructureNet, an augmentation to the StructureNet architecture that enables re-generation of parts situated at arbitrary positions in the hierarchies of its outputs. We achieve this by learning individual, probabilistic conditional decoders for each hierarchy depth. We evaluate LSD-StructureNet on the PartNet dataset, the largest dataset of 3D shapes represented by hierarchies of parts. Our results show that contrarily to existing methods, LSD-StructureNet can perform conditional sampling without impacting inference speed or the realism and diversity of its outputs.



### Masked Face Recognition Challenge: The InsightFace Track Report
- **Arxiv ID**: http://arxiv.org/abs/2108.08191v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08191v1)
- **Published**: 2021-08-18 15:14:44+00:00
- **Updated**: 2021-08-18 15:14:44+00:00
- **Authors**: Jiankang Deng, Jia Guo, Xiang An, Zheng Zhu, Stefanos Zafeiriou
- **Comment**: The WebFace260M Track of ICCV-21 MFR Challenge is still open in
  https://github.com/deepinsight/insightface/tree/master/challenges/iccv21-mfr
- **Journal**: None
- **Summary**: During the COVID-19 coronavirus epidemic, almost everyone wears a facial mask, which poses a huge challenge to deep face recognition. In this workshop, we organize Masked Face Recognition (MFR) challenge and focus on bench-marking deep face recognition methods under the existence of facial masks. In the MFR challenge, there are two main tracks: the InsightFace track and the WebFace260M track. For the InsightFace track, we manually collect a large-scale masked face test set with 7K identities. In addition, we also collect a children test set including 14K identities and a multi-racial test set containing 242K identities. By using these three test sets, we build up an online model testing system, which can give a comprehensive evaluation of face recognition models. To avoid data privacy problems, no test image is released to the public. As the challenge is still under-going, we will keep on updating the top-ranked solutions as well as this report on the arxiv.



### ALLNet: A Hybrid Convolutional Neural Network to Improve Diagnosis of Acute Lymphocytic Leukemia (ALL) in White Blood Cells
- **Arxiv ID**: http://arxiv.org/abs/2108.08195v2
- **DOI**: 10.1109/BIBM52615.2021.9669840
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2108.08195v2)
- **Published**: 2021-08-18 15:24:53+00:00
- **Updated**: 2022-07-28 16:34:15+00:00
- **Authors**: Sai Mattapalli, Rishi Athavale
- **Comment**: 7 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: Due to morphological similarity at the microscopic level, making an accurate and time-sensitive distinction between blood cells affected by Acute Lymphocytic Leukemia (ALL) and their healthy counterparts calls for the usage of machine learning architectures. However, three of the most common models, VGG, ResNet, and Inception, each come with their own set of flaws with room for improvement which demands the need for a superior model. ALLNet, the proposed hybrid convolutional neural network architecture, consists of a combination of the VGG, ResNet, and Inception models. The ALL Challenge dataset of ISBI 2019 (available here) contains 10,691 images of white blood cells which were used to train and test the models. 7,272 of the images in the dataset are of cells with ALL and 3,419 of them are of healthy cells. Of the images, 60% were used to train the model, 20% were used for the cross-validation set, and 20% were used for the test set. ALLNet outperformed the VGG, ResNet, and the Inception models across the board, achieving an accuracy of 92.6567%, a sensitivity of 95.5304%, a specificity of 85.9155%, an AUC score of 0.966347, and an F1 score of 0.94803 in the cross-validation set. In the test set, ALLNet achieved an accuracy of 92.0991%, a sensitivity of 96.5446%, a specificity of 82.8035%, an AUC score of 0.959972, and an F1 score of 0.942963. The utilization of ALLNet in the clinical workspace can better treat the thousands of people suffering from ALL across the world, many of whom are children.



### Overfitting the Data: Compact Neural Video Delivery via Content-aware Feature Modulation
- **Arxiv ID**: http://arxiv.org/abs/2108.08202v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08202v2)
- **Published**: 2021-08-18 15:34:11+00:00
- **Updated**: 2021-09-17 14:24:07+00:00
- **Authors**: Jiaming Liu, Ming Lu, Kaixin Chen, Xiaoqi Li, Shizun Wang, Zhaoqing Wang, Enhua Wu, Yurong Chen, Chuang Zhang, Ming Wu
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Internet video delivery has undergone a tremendous explosion of growth over the past few years. However, the quality of video delivery system greatly depends on the Internet bandwidth. Deep Neural Networks (DNNs) are utilized to improve the quality of video delivery recently. These methods divide a video into chunks, and stream LR video chunks and corresponding content-aware models to the client. The client runs the inference of models to super-resolve the LR chunks. Consequently, a large number of models are streamed in order to deliver a video. In this paper, we first carefully study the relation between models of different chunks, then we tactfully design a joint training framework along with the Content-aware Feature Modulation (CaFM) layer to compress these models for neural video delivery. {\bf With our method, each video chunk only requires less than $1\% $ of original parameters to be streamed, achieving even better SR performance.} We conduct extensive experiments across various SR backbones, video time length, and scaling factors to demonstrate the advantages of our method. Besides, our method can be also viewed as a new approach of video coding. Our primary experiments achieve better video quality compared with the commercial H.264 and H.265 standard under the same storage cost, showing the great potential of the proposed method. Code is available at:\url{https://github.com/Neural-video-delivery/CaFM-Pytorch-ICCV2021}



### An Attention Module for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.08205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08205v1)
- **Published**: 2021-08-18 15:36:18+00:00
- **Updated**: 2021-08-18 15:36:18+00:00
- **Authors**: Zhu Baozhou, Peter Hofstee, Jinho Lee, Zaid Al-Ars
- **Comment**: None
- **Journal**: None
- **Summary**: Attention mechanism has been regarded as an advanced technique to capture long-range feature interactions and to boost the representation capability for convolutional neural networks. However, we found two ignored problems in current attentional activations-based models: the approximation problem and the insufficient capacity problem of the attention maps. To solve the two problems together, we initially propose an attention module for convolutional neural networks by developing an AW-convolution, where the shape of attention maps matches that of the weights rather than the activations. Our proposed attention module is a complementary method to previous attention-based schemes, such as those that apply the attention mechanism to explore the relationship between channel-wise and spatial features. Experiments on several datasets for image classification and object detection tasks show the effectiveness of our proposed attention module. In particular, our proposed attention module achieves 1.00% Top-1 accuracy improvement on ImageNet classification over a ResNet101 baseline and 0.63 COCO-style Average Precision improvement on the COCO object detection on top of a Faster R-CNN baseline with the backbone of ResNet101-FPN. When integrating with the previous attentional activations-based models, our proposed attention module can further increase their Top-1 accuracy on ImageNet classification by up to 0.57% and COCO-style Average Precision on the COCO object detection by up to 0.45. Code and pre-trained models will be publicly available.



### MBRS : Enhancing Robustness of DNN-based Watermarking by Mini-Batch of Real and Simulated JPEG Compression
- **Arxiv ID**: http://arxiv.org/abs/2108.08211v1
- **DOI**: 10.1145/3474085.3475324
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2108.08211v1)
- **Published**: 2021-08-18 15:47:37+00:00
- **Updated**: 2021-08-18 15:47:37+00:00
- **Authors**: Zhaoyang Jia, Han Fang, Weiming Zhang
- **Comment**: 9 pages, 6 figures, received by ACM MM'21
- **Journal**: None
- **Summary**: Based on the powerful feature extraction ability of deep learning architecture, recently, deep-learning based watermarking algorithms have been widely studied. The basic framework of such algorithm is the auto-encoder like end-to-end architecture with an encoder, a noise layer and a decoder. The key to guarantee robustness is the adversarial training with the differential noise layer. However, we found that none of the existing framework can well ensure the robustness against JPEG compression, which is non-differential but is an essential and important image processing operation. To address such limitations, we proposed a novel end-to-end training architecture, which utilizes Mini-Batch of Real and Simulated JPEG compression (MBRS) to enhance the JPEG robustness. Precisely, for different mini-batches, we randomly choose one of real JPEG, simulated JPEG and noise-free layer as the noise layer. Besides, we suggest to utilize the Squeeze-and-Excitation blocks which can learn better feature in embedding and extracting stage, and propose a "message processor" to expand the message in a more appreciate way. Meanwhile, to improve the robustness against crop attack, we propose an additive diffusion block into the network. The extensive experimental results have demonstrated the superior performance of the proposed scheme compared with the state-of-the-art algorithms. Under the JPEG compression with quality factor Q=50, our models achieve a bit error rate less than 0.01% for extracted messages, with PSNR larger than 36 for the encoded images, which shows the well-enhanced robustness against JPEG attack. Besides, under many other distortions such as Gaussian filter, crop, cropout and dropout, the proposed framework also obtains strong robustness. The code implemented by PyTorch \cite{2011torch7} is avaiable in https://github.com/jzyustc/MBRS.



### Confidence Adaptive Regularization for Deep Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2108.08212v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08212v2)
- **Published**: 2021-08-18 15:51:25+00:00
- **Updated**: 2021-09-05 22:07:23+00:00
- **Authors**: Yangdi Lu, Yang Bo, Wenbo He
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies on the memorization effects of deep neural networks on noisy labels show that the networks first fit the correctly-labeled training samples before memorizing the mislabeled samples. Motivated by this early-learning phenomenon, we propose a novel method to prevent memorization of the mislabeled samples. Unlike the existing approaches which use the model output to identify or ignore the mislabeled samples, we introduce an indicator branch to the original model and enable the model to produce a confidence value for each sample. The confidence values are incorporated in our loss function which is learned to assign large confidence values to correctly-labeled samples and small confidence values to mislabeled samples. We also propose an auxiliary regularization term to further improve the robustness of the model. To improve the performance, we gradually correct the noisy labels with a well-designed target estimation strategy. We provide the theoretical analysis and conduct the experiments on synthetic and real-world datasets, demonstrating that our approach achieves comparable results to the state-of-the-art methods.



### X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics
- **Arxiv ID**: http://arxiv.org/abs/2108.08217v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.08217v1)
- **Published**: 2021-08-18 16:05:30+00:00
- **Updated**: 2021-08-18 16:05:30+00:00
- **Authors**: Yehao Li, Yingwei Pan, Jingwen Chen, Ting Yao, Tao Mei
- **Comment**: Accepted by 2021 ACMMM Open Source Software Competition. Source code:
  https://github.com/YehLi/xmodaler
- **Journal**: None
- **Summary**: With the rise and development of deep learning over the past decade, there has been a steady momentum of innovation and breakthroughs that convincingly push the state-of-the-art of cross-modal analytics between vision and language in multimedia field. Nevertheless, there has not been an open-source codebase in support of training and deploying numerous neural network models for cross-modal analytics in a unified and modular fashion. In this work, we propose X-modaler -- a versatile and high-performance codebase that encapsulates the state-of-the-art cross-modal analytics into several general-purpose stages (e.g., pre-processing, encoder, cross-modal interaction, decoder, and decode strategy). Each stage is empowered with the functionality that covers a series of modules widely adopted in state-of-the-arts and allows seamless switching in between. This way naturally enables a flexible implementation of state-of-the-art algorithms for image captioning, video captioning, and vision-language pre-training, aiming to facilitate the rapid development of research community. Meanwhile, since the effective modular designs in several stages (e.g., cross-modal interaction) are shared across different vision-language tasks, X-modaler can be simply extended to power startup prototypes for other tasks in cross-modal analytics, including visual question answering, visual commonsense reasoning, and cross-modal retrieval. X-modaler is an Apache-licensed codebase, and its source codes, sample projects and pre-trained models are available on-line: https://github.com/YehLi/xmodaler.



### A Survey on Open Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.00893v1
- **DOI**: 10.1109/AIKE52691.2021.00013
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00893v1)
- **Published**: 2021-08-18 16:40:03+00:00
- **Updated**: 2021-08-18 16:40:03+00:00
- **Authors**: Atefeh Mahdavi, Marco Carvalho
- **Comment**: 17 pages, 6 figures
- **Journal**: None
- **Summary**: Open Set Recognition (OSR) is about dealing with unknown situations that were not learned by the models during training. In this paper, we provide a survey of existing works about OSR and distinguish their respective advantages and disadvantages to help out new researchers interested in the subject. The categorization of OSR models is provided along with an extensive summary of recent progress. Additionally, the relationships between OSR and its related tasks including multi-class classification and novelty detection are analyzed. It is concluded that OSR can appropriately deal with unknown instances in the real-world where capturing all possible classes in the training data is not practical. Lastly, applications of OSR are highlighted and some new directions for future research topics are suggested.



### Research on Gender-related Fingerprint Features
- **Arxiv ID**: http://arxiv.org/abs/2108.08233v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08233v2)
- **Published**: 2021-08-18 16:54:34+00:00
- **Updated**: 2022-04-18 04:31:02+00:00
- **Authors**: Yong Qi, Yanping Li, Huawei Lin, Jiashu Chen, Huaiguang Lei
- **Comment**: 11 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: Fingerprint is an important biological feature of human body, which contains abundant gender information. At present, the academic research of fingerprint gender characteristics is generally at the level of understanding, while the standardization research is quite limited. In this work, we propose a more robust method, Dense Dilated Convolution ResNet (DDC-ResNet) to extract valid gender information from fingerprints. By replacing the normal convolution operations with the atrous convolution in the backbone, prior knowledge is provided to keep the edge details and the global reception field can be extended. We explored the results in 3 ways: 1) The efficiency of the DDC-ResNet. 6 typical methods of automatic feature extraction coupling with 9 mainstream classifiers are evaluated in our dataset with fair implementation details. Experimental results demonstrate that the combination of our approach outperforms other combinations in terms of average accuracy and separate-gender accuracy. It reaches 96.5% for average and 0.9752 (males)/0.9548 (females) for separate-gender accuracy. 2) The effect of fingers. It is found that the best performance of classifying gender with separate fingers is achieved by the right ring finger. 3) The effect of specific features. Based on the observations of the concentrations of fingerprints visualized by our approach, it can be inferred that loops and whorls (level 1), bifurcations (level 2), as well as line shapes (level 3) are connected with gender. Finally, we will open source the dataset that contains 6000 fingerprint images



### LOKI: Long Term and Key Intentions for Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2108.08236v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.08236v3)
- **Published**: 2021-08-18 16:57:03+00:00
- **Updated**: 2021-09-17 16:38:31+00:00
- **Authors**: Harshayu Girase, Haiming Gang, Srikanth Malla, Jiachen Li, Akira Kanehara, Karttikeya Mangalam, Chiho Choi
- **Comment**: ICCV 2021 (The dataset is available at https://usa.honda-ri.com/loki)
- **Journal**: None
- **Summary**: Recent advances in trajectory prediction have shown that explicit reasoning about agents' intent is important to accurately forecast their motion. However, the current research activities are not directly applicable to intelligent and safety critical systems. This is mainly because very few public datasets are available, and they only consider pedestrian-specific intents for a short temporal horizon from a restricted egocentric view. To this end, we propose LOKI (LOng term and Key Intentions), a novel large-scale dataset that is designed to tackle joint trajectory and intention prediction for heterogeneous traffic agents (pedestrians and vehicles) in an autonomous driving setting. The LOKI dataset is created to discover several factors that may affect intention, including i) agent's own will, ii) social interactions, iii) environmental constraints, and iv) contextual information. We also propose a model that jointly performs trajectory and intention prediction, showing that recurrently reasoning about intention can assist with trajectory prediction. We show our method outperforms state-of-the-art trajectory prediction methods by upto $27\%$ and also provide a baseline for frame-wise intention estimation.



### Scarce Data Driven Deep Learning of Drones via Generalized Data Distribution Space
- **Arxiv ID**: http://arxiv.org/abs/2108.08244v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08244v2)
- **Published**: 2021-08-18 17:07:32+00:00
- **Updated**: 2022-04-07 16:28:36+00:00
- **Authors**: Chen Li, Schyler C. Sun, Zhuangkun Wei, Antonios Tsourdos, Weisi Guo
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Increased drone proliferation in civilian and professional settings has created new threat vectors for airports and national infrastructures. The economic damage for a single major airport from drone incursions is estimated to be millions per day. Due to the lack of diverse drone training data, accurate training of deep learning detection algorithms under scarce data is an open challenge. Existing methods largely rely on collecting diverse and comprehensive experimental drone footage data, artificially induced data augmentation, transfer and meta-learning, as well as physics-informed learning. However, these methods cannot guarantee capturing diverse drone designs and fully understanding the deep feature space of drones. Here, we show how understanding the general distribution of the drone data via a Generative Adversarial Network (GAN) and explaining the missing features using Topological Data Analysis (TDA) - can allow us to acquire missing data to achieve rapid and more accurate learning. We demonstrate our results on a drone image dataset, which contains both real drone images as well as simulated images from computer-aided design. When compared to random data collection (usual practice - discriminator accuracy of 94.67\% after 200 epochs), our proposed GAN-TDA informed data collection method offers a significant 4\% improvement (99.42\% after 200 epochs). We believe that this approach of exploiting general data distribution knowledge form neural networks can be applied to a wide range of scarce data open challenges.



### LIGA-Stereo: Learning LiDAR Geometry Aware Representations for Stereo-based 3D Detector
- **Arxiv ID**: http://arxiv.org/abs/2108.08258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08258v1)
- **Published**: 2021-08-18 17:24:40+00:00
- **Updated**: 2021-08-18 17:24:40+00:00
- **Authors**: Xiaoyang Guo, Shaoshuai Shi, Xiaogang Wang, Hongsheng Li
- **Comment**: ICCV'21
- **Journal**: None
- **Summary**: Stereo-based 3D detection aims at detecting 3D object bounding boxes from stereo images using intermediate depth maps or implicit 3D geometry representations, which provides a low-cost solution for 3D perception. However, its performance is still inferior compared with LiDAR-based detection algorithms. To detect and localize accurate 3D bounding boxes, LiDAR-based models can encode accurate object boundaries and surface normal directions from LiDAR point clouds. However, the detection results of stereo-based detectors are easily affected by the erroneous depth features due to the limitation of stereo matching. To solve the problem, we propose LIGA-Stereo (LiDAR Geometry Aware Stereo Detector) to learn stereo-based 3D detectors under the guidance of high-level geometry-aware representations of LiDAR-based detection models. In addition, we found existing voxel-based stereo detectors failed to learn semantic features effectively from indirect 3D supervisions. We attach an auxiliary 2D detection head to provide direct 2D semantic supervisions. Experiment results show that the above two strategies improved the geometric and semantic representation capabilities. Compared with the state-of-the-art stereo detector, our method has improved the 3D detection performance of cars, pedestrians, cyclists by 10.44%, 5.69%, 5.97% mAP respectively on the official KITTI benchmark. The gap between stereo-based and LiDAR-based 3D detectors is further narrowed.



### Towards Robust Human Trajectory Prediction in Raw Videos
- **Arxiv ID**: http://arxiv.org/abs/2108.08259v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.08259v1)
- **Published**: 2021-08-18 17:27:26+00:00
- **Updated**: 2021-08-18 17:27:26+00:00
- **Authors**: Rui Yu, Zihan Zhou
- **Comment**: 8 pages, 6 figures. Accepted by the 2021 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2021)
- **Journal**: None
- **Summary**: Human trajectory prediction has received increased attention lately due to its importance in applications such as autonomous vehicles and indoor robots. However, most existing methods make predictions based on human-labeled trajectories and ignore the errors and noises in detection and tracking. In this paper, we study the problem of human trajectory forecasting in raw videos, and show that the prediction accuracy can be severely affected by various types of tracking errors. Accordingly, we propose a simple yet effective strategy to correct the tracking failures by enforcing prediction consistency over time. The proposed "re-tracking" algorithm can be applied to any existing tracking and prediction pipelines. Experiments on public benchmark datasets demonstrate that the proposed method can improve both tracking and prediction performance in challenging real-world scenarios. The code and data are available at https://git.io/retracking-prediction.



### End-to-End Urban Driving by Imitating a Reinforcement Learning Coach
- **Arxiv ID**: http://arxiv.org/abs/2108.08265v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.08265v3)
- **Published**: 2021-08-18 17:36:51+00:00
- **Updated**: 2021-10-04 19:42:35+00:00
- **Authors**: Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, Luc Van Gool
- **Comment**: Published at ICCV 2021
- **Journal**: None
- **Summary**: End-to-end approaches to autonomous driving commonly rely on expert demonstrations. Although humans are good drivers, they are not good coaches for end-to-end algorithms that demand dense on-policy supervision. On the contrary, automated experts that leverage privileged information can efficiently generate large scale on-policy and off-policy demonstrations. However, existing automated experts for urban driving make heavy use of hand-crafted rules and perform suboptimally even on driving simulators, where ground-truth information is available. To address these issues, we train a reinforcement learning expert that maps bird's-eye view images to continuous low-level actions. While setting a new performance upper-bound on CARLA, our expert is also a better coach that provides informative supervision signals for imitation learning agents to learn from. Supervised by our reinforcement learning coach, a baseline end-to-end agent with monocular camera-input achieves expert-level performance. Our end-to-end agent achieves a 78% success rate while generalizing to a new town and new weather on the NoCrash-dense benchmark and state-of-the-art performance on the challenging public routes of the CARLA LeaderBoard.



### Stochastic Scene-Aware Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2108.08284v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08284v1)
- **Published**: 2021-08-18 17:56:17+00:00
- **Updated**: 2021-08-18 17:56:17+00:00
- **Authors**: Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, Michael Black
- **Comment**: ICCV2021
- **Journal**: None
- **Summary**: A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as a source of training data. This is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. It is necessary to model this diversity when synthesizing virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our method, called SAMP, for Scene-Aware Motion Prediction, generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train our method, we collected MoCap data covering various sitting, lying down, walking, and running styles. We demonstrate our method on complex indoor scenes and achieve superior performance compared to existing solutions. Our code and data are available for research at https://samp.is.tue.mpg.de.



### Deep Reparametrization of Multi-Frame Super-Resolution and Denoising
- **Arxiv ID**: http://arxiv.org/abs/2108.08286v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08286v1)
- **Published**: 2021-08-18 17:57:02+00:00
- **Updated**: 2021-08-18 17:57:02+00:00
- **Authors**: Goutam Bhat, Martin Danelljan, Fisher Yu, Luc Van Gool, Radu Timofte
- **Comment**: ICCV 2021 Oral
- **Journal**: None
- **Summary**: We propose a deep reparametrization of the maximum a posteriori formulation commonly employed in multi-frame image restoration tasks. Our approach is derived by introducing a learned error metric and a latent representation of the target image, which transforms the MAP objective to a deep feature space. The deep reparametrization allows us to directly model the image formation process in the latent space, and to integrate learned image priors into the prediction. Our approach thereby leverages the advantages of deep learning, while also benefiting from the principled multi-frame fusion provided by the classical MAP formulation. We validate our approach through comprehensive experiments on burst denoising and burst super-resolution datasets. Our approach sets a new state-of-the-art for both tasks, demonstrating the generality and effectiveness of the proposed formulation.



### Pixel-Perfect Structure-from-Motion with Featuremetric Refinement
- **Arxiv ID**: http://arxiv.org/abs/2108.08291v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08291v1)
- **Published**: 2021-08-18 17:58:55+00:00
- **Updated**: 2021-08-18 17:58:55+00:00
- **Authors**: Philipp Lindenberger, Paul-Edouard Sarlin, Viktor Larsson, Marc Pollefeys
- **Comment**: Accepted to ICCV 2021 for oral presentation
- **Journal**: None
- **Summary**: Finding local features that are repeatable across multiple views is a cornerstone of sparse 3D reconstruction. The classical image matching paradigm detects keypoints per-image once and for all, which can yield poorly-localized features and propagate large errors to the final geometry. In this paper, we refine two key steps of structure-from-motion by a direct alignment of low-level image information from multiple views: we first adjust the initial keypoint locations prior to any geometric estimation, and subsequently refine points and camera poses as a post-processing. This refinement is robust to large detection noise and appearance changes, as it optimizes a featuremetric error based on dense features predicted by a neural network. This significantly improves the accuracy of camera poses and scene geometry for a wide range of keypoint detectors, challenging viewing conditions, and off-the-shelf deep features. Our system easily scales to large image collections, enabling pixel-perfect crowd-sourced localization at scale. Our code is publicly available at https://github.com/cvg/pixel-perfect-sfm as an add-on to the popular SfM software COLMAP.



### Real-time Bangla License Plate Recognition System for Low Resource Video-based Applications
- **Arxiv ID**: http://arxiv.org/abs/2108.08339v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.08339v3)
- **Published**: 2021-08-18 18:31:01+00:00
- **Updated**: 2021-11-14 17:41:49+00:00
- **Authors**: Alif Ashrafee, Akib Mohammed Khan, Mohammad Sabik Irbaz, MD Abdullah Al Nasim
- **Comment**: Accepted in IEEE/CVF Winter Conference on Applications of Computer
  Vision - Real-World Surveillance 2022 (IEEE/CVF WACV RWS 2022)
- **Journal**: None
- **Summary**: Automatic License Plate Recognition systems aim to provide a solution for detecting, localizing, and recognizing license plate characters from vehicles appearing in video frames. However, deploying such systems in the real world requires real-time performance in low-resource environments. In our paper, we propose a two-stage detection pipeline paired with Vision API that provides real-time inference speed along with consistently accurate detection and recognition performance. We used a haar-cascade classifier as a filter on top of our backbone MobileNet SSDv2 detection model. This reduces inference time by only focusing on high confidence detections and using them for recognition. We also impose a temporal frame separation strategy to distinguish between multiple vehicle license plates in the same clip. Furthermore, there are no publicly available Bangla license plate datasets, for which we created an image dataset and a video dataset containing license plates in the wild. We trained our models on the image dataset and achieved an AP(0.5) score of 86% and tested our pipeline on the video dataset and observed reasonable detection and recognition performance (82.7% detection rate, and 60.8% OCR F1 score) with real-time processing speed (27.2 frames per second).



### The Multi-Modal Video Reasoning and Analyzing Competition
- **Arxiv ID**: http://arxiv.org/abs/2108.08344v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2108.08344v1)
- **Published**: 2021-08-18 18:40:00+00:00
- **Updated**: 2021-08-18 18:40:00+00:00
- **Authors**: Haoran Peng, He Huang, Li Xu, Tianjiao Li, Jun Liu, Hossein Rahmani, Qiuhong Ke, Zhicheng Guo, Cong Wu, Rongchang Li, Mang Ye, Jiahao Wang, Jiaxu Zhang, Yuanzhong Liu, Tao He, Fuwei Zhang, Xianbin Liu, Tao Lin
- **Comment**: Accepted to ICCV 2021 Workshops
- **Journal**: None
- **Summary**: In this paper, we introduce the Multi-Modal Video Reasoning and Analyzing Competition (MMVRAC) workshop in conjunction with ICCV 2021. This competition is composed of four different tracks, namely, video question answering, skeleton-based action recognition, fisheye video-based action recognition, and person re-identification, which are based on two datasets: SUTD-TrafficQA and UAV-Human. We summarize the top-performing methods submitted by the participants in this competition and show their results achieved in the competition.



### Universal Cross-Domain Retrieval: Generalizing Across Classes and Domains
- **Arxiv ID**: http://arxiv.org/abs/2108.08356v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08356v1)
- **Published**: 2021-08-18 19:21:04+00:00
- **Updated**: 2021-08-18 19:21:04+00:00
- **Authors**: Soumava Paul, Titir Dutta, Soma Biswas
- **Comment**: Accepted at ICCV 2021. 15 pages, 6 figures
- **Journal**: None
- **Summary**: In this work, for the first time, we address the problem of universal cross-domain retrieval, where the test data can belong to classes or domains which are unseen during training. Due to dynamically increasing number of categories and practical constraint of training on every possible domain, which requires large amounts of data, generalizing to both unseen classes and domains is important. Towards that goal, we propose SnMpNet (Semantic Neighbourhood and Mixture Prediction Network), which incorporates two novel losses to account for the unseen classes and domains encountered during testing. Specifically, we introduce a novel Semantic Neighborhood loss to bridge the knowledge gap between seen and unseen classes and ensure that the latent space embedding of the unseen classes is semantically meaningful with respect to its neighboring classes. We also introduce a mix-up based supervision at image-level as well as semantic-level of the data for training with the Mixture Prediction loss, which helps in efficient retrieval when the query belongs to an unseen domain. These losses are incorporated on the SE-ResNet50 backbone to obtain SnMpNet. Extensive experiments on two large-scale datasets, Sketchy Extended and DomainNet, and thorough comparisons with state-of-the-art justify the effectiveness of the proposed model.



### STAR: Noisy Semi-Supervised Transfer Learning for Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/2108.08362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08362v1)
- **Published**: 2021-08-18 19:35:05+00:00
- **Updated**: 2021-08-18 19:35:05+00:00
- **Authors**: Hasib Zunair, Yan Gobeil, Samuel Mercier, A. Ben Hamza
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) has proven to be effective at leveraging large-scale unlabeled data to mitigate the dependency on labeled data in order to learn better models for visual recognition and classification tasks. However, recent SSL methods rely on unlabeled image data at a scale of billions to work well. This becomes infeasible for tasks with relatively fewer unlabeled data in terms of runtime, memory and data acquisition. To address this issue, we propose noisy semi-supervised transfer learning, an efficient SSL approach that integrates transfer learning and self-training with noisy student into a single framework, which is tailored for tasks that can leverage unlabeled image data on a scale of thousands. We evaluate our method on both binary and multi-class classification tasks, where the objective is to identify whether an image displays people practicing sports or the type of sport, as well as to identify the pose from a pool of popular yoga poses. Extensive experiments and ablation studies demonstrate that by leveraging unlabeled data, our proposed framework significantly improves visual classification, especially in multi-class classification settings compared to state-of-the-art methods. Moreover, incorporating transfer learning not only improves classification performance, but also requires 6x less compute time and 5x less memory. We also show that our method boosts robustness of visual classification models, even without specifically optimizing for adversarial robustness.



### Social Fabric: Tubelet Compositions for Video Relation Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.08363v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08363v1)
- **Published**: 2021-08-18 19:38:12+00:00
- **Updated**: 2021-08-18 19:38:12+00:00
- **Authors**: Shuo Chen, Zenglin Shi, Pascal Mettes, Cees G. M. Snoek
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: This paper strives to classify and detect the relationship between object tubelets appearing within a video as a <subject-predicate-object> triplet. Where existing works treat object proposals or tubelets as single entities and model their relations a posteriori, we propose to classify and detect predicates for pairs of object tubelets a priori. We also propose Social Fabric: an encoding that represents a pair of object tubelets as a composition of interaction primitives. These primitives are learned over all relations, resulting in a compact representation able to localize and classify relations from the pool of co-occurring object tubelets across all timespans in a video. The encoding enables our two-stage network. In the first stage, we train Social Fabric to suggest proposals that are likely interacting. We use the Social Fabric in the second stage to simultaneously fine-tune and predict predicate labels for the tubelets. Experiments demonstrate the benefit of early video relation modeling, our encoding and the two-stage architecture, leading to a new state-of-the-art on two benchmarks. We also show how the encoding enables query-by-primitive-example to search for spatio-temporal video relations. Code: https://github.com/shanshuo/Social-Fabric.



### SO-Pose: Exploiting Self-Occlusion for Direct 6D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2108.08367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08367v1)
- **Published**: 2021-08-18 19:49:29+00:00
- **Updated**: 2021-08-18 19:49:29+00:00
- **Authors**: Yan Di, Fabian Manhardt, Gu Wang, Xiangyang Ji, Nassir Navab, Federico Tombari
- **Comment**: ICCV2021
- **Journal**: None
- **Summary**: Directly regressing all 6 degrees-of-freedom (6DoF) for the object pose (e.g. the 3D rotation and translation) in a cluttered environment from a single RGB image is a challenging problem. While end-to-end methods have recently demonstrated promising results at high efficiency, they are still inferior when compared with elaborate P$n$P/RANSAC-based approaches in terms of pose accuracy. In this work, we address this shortcoming by means of a novel reasoning about self-occlusion, in order to establish a two-layer representation for 3D objects which considerably enhances the accuracy of end-to-end 6D pose estimation. Our framework, named SO-Pose, takes a single RGB image as input and respectively generates 2D-3D correspondences as well as self-occlusion information harnessing a shared encoder and two separate decoders. Both outputs are then fused to directly regress the 6DoF pose parameters. Incorporating cross-layer consistencies that align correspondences, self-occlusion and 6D pose, we can further improve accuracy and robustness, surpassing or rivaling all other state-of-the-art approaches on various challenging datasets.



### Quality assessment of image matchers for DSM generation -- a comparative study based on UAV images
- **Arxiv ID**: http://arxiv.org/abs/2108.08369v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08369v1)
- **Published**: 2021-08-18 20:00:05+00:00
- **Updated**: 2021-08-18 20:00:05+00:00
- **Authors**: Rongjun Qin, Armin Gruen, Cive Fraser
- **Comment**: None
- **Journal**: None
- **Summary**: Recently developed automatic dense image matching algorithms are now being implemented for DSM/DTM production, with their pixel-level surface generation capability offering the prospect of partially alleviating the need for manual and semi-automatic stereoscopic measurements. In this paper, five commercial/public software packages for 3D surface generation are evaluated, using 5cm GSD imagery recorded from a UAV. Generated surface models are assessed against point clouds generated from mobile LiDAR and manual stereoscopic measurements. The software packages considered are APS, MICMAC, SURE, Pix4UAV and an SGM implementation from DLR.



### Vis2Mesh: Efficient Mesh Reconstruction from Unstructured Point Clouds of Large Scenes with Learned Virtual View Visibility
- **Arxiv ID**: http://arxiv.org/abs/2108.08378v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08378v1)
- **Published**: 2021-08-18 20:28:16+00:00
- **Updated**: 2021-08-18 20:28:16+00:00
- **Authors**: Shuang Song, Zhaopeng Cui, Rongjun Qin
- **Comment**: ICCV2021
- **Journal**: None
- **Summary**: We present a novel framework for mesh reconstruction from unstructured point clouds by taking advantage of the learned visibility of the 3D points in the virtual views and traditional graph-cut based mesh generation. Specifically, we first propose a three-step network that explicitly employs depth completion for visibility prediction. Then the visibility information of multiple views is aggregated to generate a 3D mesh model by solving an optimization problem considering visibility in which a novel adaptive visibility weighting in surface determination is also introduced to suppress line of sight with a large incident angle. Compared to other learning-based approaches, our pipeline only exercises the learning on a 2D binary classification task, \ie, points visible or not in a view, which is much more generalizable and practically more efficient and capable to deal with a large number of points. Experiments demonstrate that our method with favorable transferability and robustness, and achieve competing performances \wrt state-of-the-art learning-based approaches on small complex objects and outperforms on large indoor and outdoor scenes. Code is available at https://github.com/GDAOSU/vis2mesh.



### Revisiting Binary Local Image Description for Resource Limited Devices
- **Arxiv ID**: http://arxiv.org/abs/2108.08380v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08380v1)
- **Published**: 2021-08-18 20:42:43+00:00
- **Updated**: 2021-08-18 20:42:43+00:00
- **Authors**: Iago Suárez, José M. Buenaposada, Luis Baumela
- **Comment**: None
- **Journal**: None
- **Summary**: The advent of a panoply of resource limited devices opens up new challenges in the design of computer vision algorithms with a clear compromise between accuracy and computational requirements. In this paper we present new binary image descriptors that emerge from the application of triplet ranking loss, hard negative mining and anchor swapping to traditional features based on pixel differences and image gradients. These descriptors, BAD (Box Average Difference) and HashSIFT, establish new operating points in the state-of-the-art's accuracy vs.\ resources trade-off curve. In our experiments we evaluate the accuracy, execution time and energy consumption of the proposed descriptors. We show that BAD bears the fastest descriptor implementation in the literature while HashSIFT approaches in accuracy that of the top deep learning-based descriptors, being computationally more efficient. We have made the source code public.



### GP-S3Net: Graph-based Panoptic Sparse Semantic Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/2108.08401v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08401v1)
- **Published**: 2021-08-18 21:49:58+00:00
- **Updated**: 2021-08-18 21:49:58+00:00
- **Authors**: Ryan Razani, Ran Cheng, Enxu Li, Ehsan Taghavi, Yuan Ren, Liu Bingbing
- **Comment**: None
- **Journal**: None
- **Summary**: Panoptic segmentation as an integrated task of both static environmental understanding and dynamic object identification, has recently begun to receive broad research interest. In this paper, we propose a new computationally efficient LiDAR based panoptic segmentation framework, called GP-S3Net. GP-S3Net is a proposal-free approach in which no object proposals are needed to identify the objects in contrast to conventional two-stage panoptic systems, where a detection network is incorporated for capturing instance information. Our new design consists of a novel instance-level network to process the semantic results by constructing a graph convolutional network to identify objects (foreground), which later on are fused with the background classes. Through the fine-grained clusters of the foreground objects from the semantic segmentation backbone, over-segmentation priors are generated and subsequently processed by 3D sparse convolution to embed each cluster. Each cluster is treated as a node in the graph and its corresponding embedding is used as its node feature. Then a GCNN predicts whether edges exist between each cluster pair. We utilize the instance label to generate ground truth edge labels for each constructed graph in order to supervise the learning. Extensive experiments demonstrate that GP-S3Net outperforms the current state-of-the-art approaches, by a significant margin across available datasets such as, nuScenes and SemanticPOSS, ranking first on the competitive public SemanticKITTI leaderboard upon publication.



