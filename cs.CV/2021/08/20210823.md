# Arxiv Papers in cs.CV on 2021-08-23
### Multi-Expert Adversarial Attack Detection in Person Re-identification Using Context Inconsistency
- **Arxiv ID**: http://arxiv.org/abs/2108.09891v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.09891v2)
- **Published**: 2021-08-23 01:59:09+00:00
- **Updated**: 2022-04-01 01:23:56+00:00
- **Authors**: Xueping Wang, Shasha Li, Min Liu, Yaonan Wang, Amit K. Roy-Chowdhury
- **Comment**: Accepted at IEEE ICCV 2021
- **Journal**: None
- **Summary**: The success of deep neural networks (DNNs) has promoted the widespread applications of person re-identification (ReID). However, ReID systems inherit the vulnerability of DNNs to malicious attacks of visually inconspicuous adversarial perturbations. Detection of adversarial attacks is, therefore, a fundamental requirement for robust ReID systems. In this work, we propose a Multi-Expert Adversarial Attack Detection (MEAAD) approach to achieve this goal by checking context inconsistency, which is suitable for any DNN-based ReID systems. Specifically, three kinds of context inconsistencies caused by adversarial attacks are employed to learn a detector for distinguishing the perturbed examples, i.e., a) the embedding distances between a perturbed query person image and its top-K retrievals are generally larger than those between a benign query image and its top-K retrievals, b) the embedding distances among the top-K retrievals of a perturbed query image are larger than those of a benign query image, c) the top-K retrievals of a benign query image obtained with multiple expert ReID models tend to be consistent, which is not preserved when attacks are present. Extensive experiments on the Market1501 and DukeMTMC-ReID datasets show that, as the first adversarial attack detection approach for ReID, MEAAD effectively detects various adversarial attacks and achieves high ROC-AUC (over 97.5%).



### Improving 3D Object Detection with Channel-wise Transformer
- **Arxiv ID**: http://arxiv.org/abs/2108.10723v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10723v2)
- **Published**: 2021-08-23 02:03:40+00:00
- **Updated**: 2021-09-15 02:21:15+00:00
- **Authors**: Hualian Sheng, Sijia Cai, Yuan Liu, Bing Deng, Jianqiang Huang, Xian-Sheng Hua, Min-Jian Zhao
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: Though 3D object detection from point clouds has achieved rapid progress in recent years, the lack of flexible and high-performance proposal refinement remains a great hurdle for existing state-of-the-art two-stage detectors. Previous works on refining 3D proposals have relied on human-designed components such as keypoints sampling, set abstraction and multi-scale feature fusion to produce powerful 3D object representations. Such methods, however, have limited ability to capture rich contextual dependencies among points. In this paper, we leverage the high-quality region proposal network and a Channel-wise Transformer architecture to constitute our two-stage 3D object detection framework (CT3D) with minimal hand-crafted design. The proposed CT3D simultaneously performs proposal-aware embedding and channel-wise context aggregation for the point features within each proposal. Specifically, CT3D uses proposal's keypoints for spatial contextual modelling and learns attention propagation in the encoding module, mapping the proposal to point embeddings. Next, a new channel-wise decoding module enriches the query-key interaction via channel-wise re-weighting to effectively merge multi-level contexts, which contributes to more accurate object predictions. Extensive experiments demonstrate that our CT3D method has superior performance and excellent scalability. Remarkably, CT3D achieves the AP of 81.77% in the moderate car category on the KITTI test 3D detection benchmark, outperforms state-of-the-art 3D detectors.



### CANet: A Context-Aware Network for Shadow Removal
- **Arxiv ID**: http://arxiv.org/abs/2108.09894v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.09894v1)
- **Published**: 2021-08-23 02:05:52+00:00
- **Updated**: 2021-08-23 02:05:52+00:00
- **Authors**: Zipei Chen, Chengjiang Long, Ling Zhang, Chunxia Xiao
- **Comment**: This paper was accepted to the IEEE International Conference on
  Computer Vision (ICCV), Montreal, Canada, Oct 11-17, 2021
- **Journal**: None
- **Summary**: In this paper, we propose a novel two-stage context-aware network named CANet for shadow removal, in which the contextual information from non-shadow regions is transferred to shadow regions at the embedded feature spaces. At Stage-I, we propose a contextual patch matching (CPM) module to generate a set of potential matching pairs of shadow and non-shadow patches. Combined with the potential contextual relationships between shadow and non-shadow regions, our well-designed contextual feature transfer (CFT) mechanism can transfer contextual information from non-shadow to shadow regions at different scales. With the reconstructed feature maps, we remove shadows at L and A/B channels separately. At Stage-II, we use an encoder-decoder to refine current results and generate the final shadow removal results. We evaluate our proposed CANet on two benchmark datasets and some real-world shadow images with complex scenes. Extensive experimental results strongly demonstrate the efficacy of our proposed CANet and exhibit superior performance to state-of-the-arts.



### Burst Imaging for Light-Constrained Structure-From-Motion
- **Arxiv ID**: http://arxiv.org/abs/2108.09895v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.09895v1)
- **Published**: 2021-08-23 02:12:40+00:00
- **Updated**: 2021-08-23 02:12:40+00:00
- **Authors**: Ahalya Ravendran, Mitch Bryson, Donald G. Dansereau
- **Comment**: 8 pages, 8 figures, 2 tables, for associated project page, see:
  https://roboticimaging.org/Projects/BurstSfM/
- **Journal**: None
- **Summary**: Images captured under extremely low light conditions are noise-limited, which can cause existing robotic vision algorithms to fail. In this paper we develop an image processing technique for aiding 3D reconstruction from images acquired in low light conditions. Our technique, based on burst photography, uses direct methods for image registration within bursts of short exposure time images to improve the robustness and accuracy of feature-based structure-from-motion (SfM). We demonstrate improved SfM performance in challenging light-constrained scenes, including quantitative evaluations that show improved feature performance and camera pose estimates. Additionally, we show that our method converges more frequently to correct reconstructions than the state-of-the-art. Our method is a significant step towards allowing robots to operate in low light conditions, with potential applications to robots operating in environments such as underground mines and night time operation.



### A Weakly Supervised Amodal Segmenter with Boundary Uncertainty Estimation
- **Arxiv ID**: http://arxiv.org/abs/2108.09897v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.09897v2)
- **Published**: 2021-08-23 02:27:29+00:00
- **Updated**: 2021-08-30 02:17:00+00:00
- **Authors**: Khoi Nguyen, Sinisa Todorovic
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: This paper addresses weakly supervised amodal instance segmentation, where the goal is to segment both visible and occluded (amodal) object parts, while training provides only ground-truth visible (modal) segmentations. Following prior work, we use data manipulation to generate occlusions in training images and thus train a segmenter to predict amodal segmentations of the manipulated data. The resulting predictions on training images are taken as the pseudo-ground truth for the standard training of Mask-RCNN, which we use for amodal instance segmentation of test images. For generating the pseudo-ground truth, we specify a new Amodal Segmenter based on Boundary Uncertainty estimation (ASBU) and make two contributions. First, while prior work uses the occluder's mask, our ASBU uses the occlusion boundary as input. Second, ASBU estimates an uncertainty map of the prediction. The estimated uncertainty regularizes learning such that lower segmentation loss is incurred on regions with high uncertainty. ASBU achieves significant performance improvement relative to the state of the art on the COCOA and KINS datasets in three tasks: amodal instance segmentation, amodal completion, and ordering recovery.



### Face Photo-Sketch Recognition Using Bidirectional Collaborative Synthesis Network
- **Arxiv ID**: http://arxiv.org/abs/2108.09898v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.09898v1)
- **Published**: 2021-08-23 02:30:50+00:00
- **Updated**: 2021-08-23 02:30:50+00:00
- **Authors**: Seho Bae, Nizam Ud Din, Hyunkyu Park, Juneho Yi
- **Comment**: None
- **Journal**: None
- **Summary**: This research features a deep-learning based framework to address the problem of matching a given face sketch image against a face photo database. The problem of photo-sketch matching is challenging because 1) there is large modality gap between photo and sketch, and 2) the number of paired training samples is insufficient to train deep learning based networks. To circumvent the problem of large modality gap, our approach is to use an intermediate latent space between the two modalities. We effectively align the distributions of the two modalities in this latent space by employing a bidirectional (photo -> sketch and sketch -> photo) collaborative synthesis network. A StyleGAN-like architecture is utilized to make the intermediate latent space be equipped with rich representation power. To resolve the problem of insufficient training samples, we introduce a three-step training scheme. Extensive evaluation on public composite face sketch database confirms superior performance of our method compared to existing state-of-the-art methods. The proposed methodology can be employed in matching other modality pairs.



### The 2nd Anti-UAV Workshop & Challenge: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2108.09909v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.09909v3)
- **Published**: 2021-08-23 03:27:36+00:00
- **Updated**: 2021-09-29 07:38:39+00:00
- **Authors**: Jian Zhao, Gang Wang, Jianan Li, Lei Jin, Nana Fan, Min Wang, Xiaojuan Wang, Ting Yong, Yafeng Deng, Yandong Guo, Shiming Ge, Guodong Guo
- **Comment**: None
- **Journal**: None
- **Summary**: The 2nd Anti-UAV Workshop \& Challenge aims to encourage research in developing novel and accurate methods for multi-scale object tracking. The Anti-UAV dataset used for the Anti-UAV Challenge has been publicly released. There are two subsets in the dataset, $i.e.$, the test-dev subset and test-challenge subset. Both subsets consist of 140 thermal infrared video sequences, spanning multiple occurrences of multi-scale UAVs. Around 24 participating teams from the globe competed in the 2nd Anti-UAV Challenge. In this paper, we provide a brief summary of the 2nd Anti-UAV Workshop \& Challenge including brief introductions to the top three methods.The submission leaderboard will be reopened for researchers that are interested in the Anti-UAV challenge. The benchmark dataset and other information can be found at: https://anti-uav.github.io/.



### Black-Box Test-Time Shape REFINEment for Single View 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2108.09911v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.09911v1)
- **Published**: 2021-08-23 03:28:47+00:00
- **Updated**: 2021-08-23 03:28:47+00:00
- **Authors**: Brandon Leung, Chih-Hui Ho, Nuno Vasconcelos
- **Comment**: None
- **Journal**: None
- **Summary**: Much recent progress has been made in reconstructing the 3D shape of an object from an image of it, i.e. single view 3D reconstruction. However, it has been suggested that current methods simply adopt a "nearest-neighbor" strategy, instead of genuinely understanding the shape behind the input image. In this paper, we rigorously show that for many state of the art methods, this issue manifests as (1) inconsistencies between coarse reconstructions and input images, and (2) inability to generalize across domains. We thus propose REFINE, a postprocessing mesh refinement step that can be easily integrated into the pipeline of any black-box method in the literature. At test time, REFINE optimizes a network per mesh instance, to encourage consistency between the mesh and the given object view. This, along with a novel combination of regularizing losses, reduces the domain gap and achieves state of the art performance. We believe that this novel paradigm is an important step towards robust, accurate reconstructions, remaining relevant as new reconstruction networks are introduced.



### PR-GCN: A Deep Graph Convolutional Network with Point Refinement for 6D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2108.09916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.09916v1)
- **Published**: 2021-08-23 03:53:34+00:00
- **Updated**: 2021-08-23 03:53:34+00:00
- **Authors**: Guangyuan Zhou, Huiqun Wang, Jiaxin Chen, Di Huang
- **Comment**: None
- **Journal**: None
- **Summary**: RGB-D based 6D pose estimation has recently achieved remarkable progress, but still suffers from two major limitations: (1) ineffective representation of depth data and (2) insufficient integration of different modalities. This paper proposes a novel deep learning approach, namely Graph Convolutional Network with Point Refinement (PR-GCN), to simultaneously address the issues above in a unified way. It first introduces the Point Refinement Network (PRN) to polish 3D point clouds, recovering missing parts with noise removed. Subsequently, the Multi-Modal Fusion Graph Convolutional Network (MMF-GCN) is presented to strengthen RGB-D combination, which captures geometry-aware inter-modality correlation through local information propagation in the graph convolutional network. Extensive experiments are conducted on three widely used benchmarks, and state-of-the-art performance is reached. Besides, it is also shown that the proposed PRN and MMF-GCN modules are well generalized to other frameworks.



### Towards Real-world X-ray Security Inspection: A High-Quality Benchmark and Lateral Inhibition Module for Prohibited Items Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.09917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.09917v1)
- **Published**: 2021-08-23 03:59:23+00:00
- **Updated**: 2021-08-23 03:59:23+00:00
- **Authors**: Renshuai Tao, Yanlu Wei, Xiangjian Jiang, Hainan Li, Haotong Qin, Jiakai Wang, Yuqing Ma, Libo Zhang, Xianglong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Prohibited items detection in X-ray images often plays an important role in protecting public safety, which often deals with color-monotonous and luster-insufficient objects, resulting in unsatisfactory performance. Till now, there have been rare studies touching this topic due to the lack of specialized high-quality datasets. In this work, we first present a High-quality X-ray (HiXray) security inspection image dataset, which contains 102,928 common prohibited items of 8 categories. It is the largest dataset of high quality for prohibited items detection, gathered from the real-world airport security inspection and annotated by professional security inspectors. Besides, for accurate prohibited item detection, we further propose the Lateral Inhibition Module (LIM) inspired by the fact that humans recognize these items by ignoring irrelevant information and focusing on identifiable characteristics, especially when objects are overlapped with each other. Specifically, LIM, the elaborately designed flexible additional module, suppresses the noisy information flowing maximumly by the Bidirectional Propagation (BP) module and activates the most identifiable charismatic, boundary, from four directions by Boundary Activation (BA) module. We evaluate our method extensively on HiXray and OPIXray and the results demonstrate that it outperforms SOTA detection methods.



### SegMix: Co-occurrence Driven Mixup for Semantic Segmentation and Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2108.09929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.09929v1)
- **Published**: 2021-08-23 04:35:48+00:00
- **Updated**: 2021-08-23 04:35:48+00:00
- **Authors**: Md Amirul Islam, Matthew Kowal, Konstantinos G. Derpanis, Neil D. B. Bruce
- **Comment**: Under submission at IJCV (BMVC 2020 Extension). arXiv admin note:
  substantial text overlap with arXiv:2008.05667
- **Journal**: None
- **Summary**: In this paper, we present a strategy for training convolutional neural networks to effectively resolve interference arising from competing hypotheses relating to inter-categorical information throughout the network. The premise is based on the notion of feature binding, which is defined as the process by which activations spread across space and layers in the network are successfully integrated to arrive at a correct inference decision. In our work, this is accomplished for the task of dense image labelling by blending images based on (i) categorical clustering or (ii) the co-occurrence likelihood of categories. We then train a feature binding network which simultaneously segments and separates the blended images. Subsequent feature denoising to suppress noisy activations reveals additional desirable properties and high degrees of successful predictions. Through this process, we reveal a general mechanism, distinct from any prior methods, for boosting the performance of the base segmentation and saliency network while simultaneously increasing robustness to adversarial attacks.



### Modeling Dynamics of Facial Behavior for Mental Health Assessment
- **Arxiv ID**: http://arxiv.org/abs/2108.09934v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.09934v1)
- **Published**: 2021-08-23 05:08:45+00:00
- **Updated**: 2021-08-23 05:08:45+00:00
- **Authors**: Minh Tran, Ellen Bradley, Michelle Matvey, Joshua Woolley, Mohammad Soleymani
- **Comment**: Accepted to FG 2021
- **Journal**: None
- **Summary**: Facial action unit (FAU) intensities are popular descriptors for the analysis of facial behavior. However, FAUs are sparsely represented when only a few are activated at a time. In this study, we explore the possibility of representing the dynamics of facial expressions by adopting algorithms used for word representation in natural language processing. Specifically, we perform clustering on a large dataset of temporal facial expressions with 5.3M frames before applying the Global Vector representation (GloVe) algorithm to learn the embeddings of the facial clusters. We evaluate the usefulness of our learned representations on two downstream tasks: schizophrenia symptom estimation and depression severity regression. These experimental results show the potential effectiveness of our approach for improving the assessment of mental health symptoms over baseline models that use FAU intensities alone.



### Voxel-based Network for Shape Completion by Leveraging Edge Generation
- **Arxiv ID**: http://arxiv.org/abs/2108.09936v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.09936v1)
- **Published**: 2021-08-23 05:10:29+00:00
- **Updated**: 2021-08-23 05:10:29+00:00
- **Authors**: Xiaogang Wang, Marcelo H Ang Jr, Gim Hee Lee
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Deep learning technique has yielded significant improvements in point cloud completion with the aim of completing missing object shapes from partial inputs. However, most existing methods fail to recover realistic structures due to over-smoothing of fine-grained details. In this paper, we develop a voxel-based network for point cloud completion by leveraging edge generation (VE-PCN). We first embed point clouds into regular voxel grids, and then generate complete objects with the help of the hallucinated shape edges. This decoupled architecture together with a multi-scale grid feature learning is able to generate more realistic on-surface details. We evaluate our model on the publicly available completion datasets and show that it outperforms existing state-of-the-art approaches quantitatively and qualitatively. Our source code is available at https://github.com/xiaogangw/VE-PCN.



### Learning Signed Distance Field for Multi-view Surface Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2108.09964v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.09964v1)
- **Published**: 2021-08-23 06:23:50+00:00
- **Updated**: 2021-08-23 06:23:50+00:00
- **Authors**: Jingyang Zhang, Yao Yao, Long Quan
- **Comment**: ICCV 2021 (Oral)
- **Journal**: None
- **Summary**: Recent works on implicit neural representations have shown promising results for multi-view surface reconstruction. However, most approaches are limited to relatively simple geometries and usually require clean object masks for reconstructing complex and concave objects. In this work, we introduce a novel neural surface reconstruction framework that leverages the knowledge of stereo matching and feature consistency to optimize the implicit surface representation. More specifically, we apply a signed distance field (SDF) and a surface light field to represent the scene geometry and appearance respectively. The SDF is directly supervised by geometry from stereo matching, and is refined by optimizing the multi-view feature consistency and the fidelity of rendered images. Our method is able to improve the robustness of geometry estimation and support reconstruction of complex scene topologies. Extensive experiments have been conducted on DTU, EPFL and Tanks and Temples datasets. Compared to previous state-of-the-art methods, our method achieves better mesh reconstruction in wide open scenes without masks as input.



### Exploring the Quality of GAN Generated Images for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2108.09977v1
- **DOI**: 10.1145/3474085.3475547
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.09977v1)
- **Published**: 2021-08-23 07:20:12+00:00
- **Updated**: 2021-08-23 07:20:12+00:00
- **Authors**: Yiqi Jiang, Weihua Chen, Xiuyu Sun, Xiaoyu Shi, Fan Wang, Hao Li
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Recently, GAN based method has demonstrated strong effectiveness in generating augmentation data for person re-identification (ReID), on account of its ability to bridge the gap between domains and enrich the data variety in feature space. However, most of the ReID works pick all the GAN generated data as additional training samples or evaluate the quality of GAN generation at the entire data set level, ignoring the image-level essential feature of data in ReID task. In this paper, we analyze the in-depth characteristics of ReID sample and solve the problem of "What makes a GAN-generated image good for ReID". Specifically, we propose to examine each data sample with id-consistency and diversity constraints by mapping image onto different spaces. With a metric-based sampling method, we demonstrate that not every GAN-generated data is beneficial for augmentation. Models trained with data filtered by our quality evaluation outperform those trained with the full augmentation set by a large margin. Extensive experiments show the effectiveness of our method on both supervised ReID task and unsupervised domain adaptation ReID task.



### TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment
- **Arxiv ID**: http://arxiv.org/abs/2108.09980v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.09980v1)
- **Published**: 2021-08-23 07:24:57+00:00
- **Updated**: 2021-08-23 07:24:57+00:00
- **Authors**: Jianwei Yang, Yonatan Bisk, Jianfeng Gao
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Contrastive learning has been widely used to train transformer-based vision-language models for video-text alignment and multi-modal representation learning. This paper presents a new algorithm called Token-Aware Cascade contrastive learning (TACo) that improves contrastive learning using two novel techniques. The first is the token-aware contrastive loss which is computed by taking into account the syntactic classes of words. This is motivated by the observation that for a video-text pair, the content words in the text, such as nouns and verbs, are more likely to be aligned with the visual contents in the video than the function words. Second, a cascade sampling method is applied to generate a small set of hard negative examples for efficient loss estimation for multi-modal fusion layers. To validate the effectiveness of TACo, in our experiments we finetune pretrained models for a set of downstream tasks including text-video retrieval (YouCook2, MSR-VTT and ActivityNet), video action step localization (CrossTask), video action segmentation (COIN). The results show that our models attain consistent improvements across different experimental settings over previous methods, setting new state-of-the-art on three public text-video retrieval benchmarks of YouCook2, MSR-VTT and ActivityNet.



### Recurrent Video Deblurring with Blur-Invariant Motion Estimation and Pixel Volumes
- **Arxiv ID**: http://arxiv.org/abs/2108.09982v1
- **DOI**: 10.1145/3453720
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.09982v1)
- **Published**: 2021-08-23 07:36:49+00:00
- **Updated**: 2021-08-23 07:36:49+00:00
- **Authors**: Hyeongseok Son, Junyong Lee, Jonghyeop Lee, Sunghyun Cho, Seungyong Lee
- **Comment**: 17 pages, Camera-ready version for ACM Transactions on Graphics (TOG)
  2021
- **Journal**: ACM Transactions on Graphics, Vol. 40, No. 5, Article 185, 2021
- **Summary**: For the success of video deblurring, it is essential to utilize information from neighboring frames. Most state-of-the-art video deblurring methods adopt motion compensation between video frames to aggregate information from multiple frames that can help deblur a target frame. However, the motion compensation methods adopted by previous deblurring methods are not blur-invariant, and consequently, their accuracy is limited for blurry frames with different blur amounts. To alleviate this problem, we propose two novel approaches to deblur videos by effectively aggregating information from multiple video frames. First, we present blur-invariant motion estimation learning to improve motion estimation accuracy between blurry frames. Second, for motion compensation, instead of aligning frames by warping with estimated motions, we use a pixel volume that contains candidate sharp pixels to resolve motion estimation errors. We combine these two processes to propose an effective recurrent video deblurring network that fully exploits deblurred previous frames. Experiments show that our method achieves the state-of-the-art performance both quantitatively and qualitatively compared to recent methods that use deep learning.



### Efficient Medical Image Segmentation Based on Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2108.09987v1
- **DOI**: 10.1109/TMI.2021.3098703
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.09987v1)
- **Published**: 2021-08-23 07:41:10+00:00
- **Updated**: 2021-08-23 07:41:10+00:00
- **Authors**: Dian Qin, Jiajun Bu, Zhe Liu, Xin Shen, Sheng Zhou, Jingjun Gu, Zhijua Wang, Lei Wu, Huifen Dai
- **Comment**: Accepted by IEEE TMI, Code Avalivable
- **Journal**: None
- **Summary**: Recent advances have been made in applying convolutional neural networks to achieve more precise prediction results for medical image segmentation problems. However, the success of existing methods has highly relied on huge computational complexity and massive storage, which is impractical in the real-world scenario. To deal with this problem, we propose an efficient architecture by distilling knowledge from well-trained medical image segmentation networks to train another lightweight network. This architecture empowers the lightweight network to get a significant improvement on segmentation capability while retaining its runtime efficiency. We further devise a novel distillation module tailored for medical image segmentation to transfer semantic region information from teacher to student network. It forces the student network to mimic the extent of difference of representations calculated from different tissue regions. This module avoids the ambiguous boundary problem encountered when dealing with medical imaging but instead encodes the internal information of each semantic region for transferring. Benefited from our module, the lightweight network could receive an improvement of up to 32.6% in our experiment while maintaining its portability in the inference phase. The entire structure has been verified on two widely accepted public CT datasets LiTS17 and KiTS19. We demonstrate that a lightweight network distilled by our method has non-negligible value in the scenario which requires relatively high operating speed and low storage usage.



### Learned Image Coding for Machines: A Content-Adaptive Approach
- **Arxiv ID**: http://arxiv.org/abs/2108.09992v3
- **DOI**: 10.1109/ICME51207.2021.9428224
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.09992v3)
- **Published**: 2021-08-23 07:53:35+00:00
- **Updated**: 2021-10-13 08:51:54+00:00
- **Authors**: Nam Le, Honglei Zhang, Francesco Cricri, Ramin Ghaznavi-Youvalari, Hamed Rezazadegan Tavakoli, Esa Rahtu
- **Comment**: Fig 4 correction
- **Journal**: 2021 IEEE International Conference on Multimedia and Expo (ICME),
  2021, pp. 1-6
- **Summary**: Today, according to the Cisco Annual Internet Report (2018-2023), the fastest-growing category of Internet traffic is machine-to-machine communication. In particular, machine-to-machine communication of images and videos represents a new challenge and opens up new perspectives in the context of data compression. One possible solution approach consists of adapting current human-targeted image and video coding standards to the use case of machine consumption. Another approach consists of developing completely new compression paradigms and architectures for machine-to-machine communications. In this paper, we focus on image compression and present an inference-time content-adaptive finetuning scheme that optimizes the latent representation of an end-to-end learned image codec, aimed at improving the compression efficiency for machine-consumption. The conducted experiments show that our online finetuning brings an average bitrate saving (BD-rate) of -3.66% with respect to our pretrained image codec. In particular, at low bitrate points, our proposed method results in a significant bitrate saving of -9.85%. Overall, our pretrained-and-then-finetuned system achieves -30.54% BD-rate over the state-of-the-art image/video codec Versatile Video Coding (VVC).



### Image coding for machines: an end-to-end learned approach
- **Arxiv ID**: http://arxiv.org/abs/2108.09993v2
- **DOI**: 10.1109/ICASSP39728.2021.9414465
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.09993v2)
- **Published**: 2021-08-23 07:54:42+00:00
- **Updated**: 2021-08-30 13:23:06+00:00
- **Authors**: Nam Le, Honglei Zhang, Francesco Cricri, Ramin Ghaznavi-Youvalari, Esa Rahtu
- **Comment**: Fixed a couple of mistakes since the version accepted in IEEE
  ICASSP2021
- **Journal**: 2021 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP2021), 2021, pp. 1590-1594
- **Summary**: Over recent years, deep learning-based computer vision systems have been applied to images at an ever-increasing pace, oftentimes representing the only type of consumption for those images. Given the dramatic explosion in the number of images generated per day, a question arises: how much better would an image codec targeting machine-consumption perform against state-of-the-art codecs targeting human-consumption? In this paper, we propose an image codec for machines which is neural network (NN) based and end-to-end learned. In particular, we propose a set of training strategies that address the delicate problem of balancing competing loss functions, such as computer vision task losses, image distortion losses, and rate loss. Our experimental results show that our NN-based codec outperforms the state-of-the-art Versa-tile Video Coding (VVC) standard on the object detection and instance segmentation tasks, achieving -37.87% and -32.90% of BD-rate gain, respectively, while being fast thanks to its compact size. To the best of our knowledge, this is the first end-to-end learned machine-targeted image codec.



### BiaSwap: Removing dataset bias with bias-tailored swapping augmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.10008v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.10008v1)
- **Published**: 2021-08-23 08:35:26+00:00
- **Updated**: 2021-08-23 08:35:26+00:00
- **Authors**: Eungyeup Kim, Jihyeon Lee, Jaegul Choo
- **Comment**: Accepted to ICCV'21
- **Journal**: None
- **Summary**: Deep neural networks often make decisions based on the spurious correlations inherent in the dataset, failing to generalize in an unbiased data distribution. Although previous approaches pre-define the type of dataset bias to prevent the network from learning it, recognizing the bias type in the real dataset is often prohibitive. This paper proposes a novel bias-tailored augmentation-based approach, BiaSwap, for learning debiased representation without requiring supervision on the bias type. Assuming that the bias corresponds to the easy-to-learn attributes, we sort the training images based on how much a biased classifier can exploits them as shortcut and divide them into bias-guiding and bias-contrary samples in an unsupervised manner. Afterwards, we integrate the style-transferring module of the image translation model with the class activation maps of such biased classifier, which enables to primarily transfer the bias attributes learned by the classifier. Therefore, given the pair of bias-guiding and bias-contrary, BiaSwap generates the bias-swapped image which contains the bias attributes from the bias-contrary images, while preserving bias-irrelevant ones in the bias-guiding images. Given such augmented images, BiaSwap demonstrates the superiority in debiasing against the existing baselines over both synthetic and real-world datasets. Even without careful supervision on the bias, BiaSwap achieves a remarkable performance on both unbiased and bias-guiding samples, implying the improved generalization capability of the model.



### Deep Relational Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.10026v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.10026v1)
- **Published**: 2021-08-23 09:31:18+00:00
- **Updated**: 2021-08-23 09:31:18+00:00
- **Authors**: Wenzhao Zheng, Borui Zhang, Jiwen Lu, Jie Zhou
- **Comment**: Accepted to ICCV 2021. Source code available at
  https://github.com/zbr17/DRML
- **Journal**: None
- **Summary**: This paper presents a deep relational metric learning (DRML) framework for image clustering and retrieval. Most existing deep metric learning methods learn an embedding space with a general objective of increasing interclass distances and decreasing intraclass distances. However, the conventional losses of metric learning usually suppress intraclass variations which might be helpful to identify samples of unseen classes. To address this problem, we propose to adaptively learn an ensemble of features that characterizes an image from different aspects to model both interclass and intraclass distributions. We further employ a relational module to capture the correlations among each feature in the ensemble and construct a graph to represent an image. We then perform relational inference on the graph to integrate the ensemble and obtain a relation-aware embedding to measure the similarities. Extensive experiments on the widely-used CUB-200-2011, Cars196, and Stanford Online Products datasets demonstrate that our framework improves existing deep metric learning methods and achieves very competitive results.



### Realistic Image Synthesis with Configurable 3D Scene Layouts
- **Arxiv ID**: http://arxiv.org/abs/2108.10031v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10031v2)
- **Published**: 2021-08-23 09:44:56+00:00
- **Updated**: 2021-08-24 05:52:59+00:00
- **Authors**: Jaebong Jeong, Janghun Jo, Jingdong Wang, Sunghyun Cho, Jaesik Park
- **Comment**: paper: 9 pages, supplementary materials: 7 pages
- **Journal**: None
- **Summary**: Recent conditional image synthesis approaches provide high-quality synthesized images. However, it is still challenging to accurately adjust image contents such as the positions and orientations of objects, and synthesized images often have geometrically invalid contents. To provide users with rich controllability on synthesized images in the aspect of 3D geometry, we propose a novel approach to realistic-looking image synthesis based on a configurable 3D scene layout. Our approach takes a 3D scene with semantic class labels as input and trains a 3D scene painting network that synthesizes color values for the input 3D scene. With the trained painting network, realistic-looking images for the input 3D scene can be rendered and manipulated. To train the painting network without 3D color supervision, we exploit an off-the-shelf 2D semantic image synthesis method. In experiments, we show that our approach produces images with geometrically correct structures and supports geometric manipulation such as the change of the viewpoint and object poses as well as manipulation of the painting style.



### Discovering Spatial Relationships by Transformers for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2108.10046v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.10046v2)
- **Published**: 2021-08-23 10:35:38+00:00
- **Updated**: 2021-10-13 06:47:38+00:00
- **Authors**: Cuicui Kang, Karthik Nandakumar
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the rapid increase in the diversity of image data, the problem of domain generalization has received increased attention recently. While domain generalization is a challenging problem, it has achieved great development thanks to the fast development of AI techniques in computer vision. Most of these advanced algorithms are proposed with deep architectures based on convolution neural nets (CNN). However, though CNNs have a strong ability to find the discriminative features, they do a poor job of modeling the relations between different locations in the image due to the response to CNN filters are mostly local. Since these local and global spatial relationships are characterized to distinguish an object under consideration, they play a critical role in improving the generalization ability against the domain gap. In order to get the object parts relationships to gain better domain generalization, this work proposes to use the self attention model. However, the attention models are proposed for sequence, which are not expert in discriminate feature extraction for 2D images. Considering this, we proposed a hybrid architecture to discover the spatial relationships between these local features, and derive a composite representation that encodes both the discriminative features and their relationships to improve the domain generalization. Evaluation on three well-known benchmarks demonstrates the benefits of modeling relationships between the features of an image using the proposed method and achieves state-of-the-art domain generalization performance. More specifically, the proposed algorithm outperforms the state-of-the-art by 2.2% and 3.4% on PACS and Office-Home databases, respectively.



### How Transferable Are Self-supervised Features in Medical Image Classification Tasks?
- **Arxiv ID**: http://arxiv.org/abs/2108.10048v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10048v3)
- **Published**: 2021-08-23 10:39:31+00:00
- **Updated**: 2021-11-30 12:36:27+00:00
- **Authors**: Tuan Truong, Sadegh Mohammadi, Matthias Lenga
- **Comment**: Accepted to Machine Learning for Health (ML4H) (ML4H 2021)
- **Journal**: None
- **Summary**: Transfer learning has become a standard practice to mitigate the lack of labeled data in medical classification tasks. Whereas finetuning a downstream task using supervised ImageNet pretrained features is straightforward and extensively investigated in many works, there is little study on the usefulness of self-supervised pretraining. In this paper, we assess the transferability of ImageNet self-supervisedpretraining by evaluating the performance of models initialized with pretrained features from three self-supervised techniques (SimCLR, SwAV, and DINO) on selected medical classification tasks. The chosen tasks cover tumor detection in sentinel axillary lymph node images, diabetic retinopathy classification in fundus images, and multiple pathological condition classification in chest X-ray images. We demonstrate that self-supervised pretrained models yield richer embeddings than their supervised counterpart, which benefits downstream tasks in view of both linear evaluation and finetuning. For example, in view of linear evaluation at acritically small subset of the data, we see an improvement up to 14.79% in Kappa score in the diabetic retinopathy classification task, 5.4% in AUC in the tumor classification task, 7.03% AUC in the pneumonia detection, and 9.4% in AUC in the detection of pathological conditions in chest X-ray. In addition, we introduce Dynamic Visual Meta-Embedding (DVME) as an end-to-end transfer learning approach that fuses pretrained embeddings from multiple models. We show that the collective representation obtained by DVME leads to a significant improvement in the performance of selected tasks compared to using a single pretrained model approach and can be generalized to any combination of pretrained models.



### ZS-SLR: Zero-Shot Sign Language Recognition from RGB-D Videos
- **Arxiv ID**: http://arxiv.org/abs/2108.10059v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2108.10059v1)
- **Published**: 2021-08-23 10:48:18+00:00
- **Updated**: 2021-08-23 10:48:18+00:00
- **Authors**: Razieh Rastgoo, Kourosh Kiani, Sergio Escalera
- **Comment**: None
- **Journal**: None
- **Summary**: Sign Language Recognition (SLR) is a challenging research area in computer vision. To tackle the annotation bottleneck in SLR, we formulate the problem of Zero-Shot Sign Language Recognition (ZS-SLR) and propose a two-stream model from two input modalities: RGB and Depth videos. To benefit from the vision Transformer capabilities, we use two vision Transformer models, for human detection and visual features representation. We configure a transformer encoder-decoder architecture, as a fast and accurate human detection model, to overcome the challenges of the current human detection models. Considering the human keypoints, the detected human body is segmented into nine parts. A spatio-temporal representation from human body is obtained using a vision Transformer and a LSTM network. A semantic space maps the visual features to the lingual embedding of the class labels via a Bidirectional Encoder Representations from Transformers (BERT) model. We evaluated the proposed model on four datasets, Montalbano II, MSR Daily Activity 3D, CAD-60, and NTU-60, obtaining state-of-the-art results compared to state-of-the-art ZS-SLR models.



### Deep neural networks approach to microbial colony detection -- a comparative analysis
- **Arxiv ID**: http://arxiv.org/abs/2108.10103v2
- **DOI**: 10.1007/978-3-031-11432-8_9
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2108.10103v2)
- **Published**: 2021-08-23 12:06:00+00:00
- **Updated**: 2021-08-24 14:00:12+00:00
- **Authors**: Sylwia Majchrowska, Jarosław Pawłowski, Natalia Czerep, Aleksander Górecki, Jakub Kuciński, Tomasz Golan
- **Comment**: 8 pages, 2 figures, 3 tables
- **Journal**: In: Biele, C., Kacprzyk, J., Kope\'c, W., Owsi\'nski, J.W.,
  Romanowski, A., Sikorski, M. (eds) Digital Interaction and Machine
  Intelligence. MIDI 2021. Lecture Notes in Networks and Systems, vol 440.
  Springer, Cham, pp. 98-106
- **Summary**: Counting microbial colonies is a fundamental task in microbiology and has many applications in numerous industry branches. Despite this, current studies towards automatic microbial counting using artificial intelligence are hardly comparable due to the lack of unified methodology and the availability of large datasets. The recently introduced AGAR dataset is the answer to the second need, but the research carried out is still not exhaustive. To tackle this problem, we compared the performance of three well-known deep learning approaches for object detection on the AGAR dataset, namely two-stage, one-stage and transformer based neural networks. The achieved results may serve as a benchmark for future experiments.



### ODAM: Object Detection, Association, and Mapping using Posed RGB Video
- **Arxiv ID**: http://arxiv.org/abs/2108.10165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10165v1)
- **Published**: 2021-08-23 13:28:10+00:00
- **Updated**: 2021-08-23 13:28:10+00:00
- **Authors**: Kejie Li, Daniel DeTone, Steven Chen, Minh Vo, Ian Reid, Hamid Rezatofighi, Chris Sweeney, Julian Straub, Richard Newcombe
- **Comment**: Accepted in ICCV 2021 as oral
- **Journal**: None
- **Summary**: Localizing objects and estimating their extent in 3D is an important step towards high-level 3D scene understanding, which has many applications in Augmented Reality and Robotics. We present ODAM, a system for 3D Object Detection, Association, and Mapping using posed RGB videos. The proposed system relies on a deep learning front-end to detect 3D objects from a given RGB frame and associate them to a global object-based map using a graph neural network (GNN). Based on these frame-to-model associations, our back-end optimizes object bounding volumes, represented as super-quadrics, under multi-view geometry constraints and the object scale prior. We validate the proposed system on ScanNet where we show a significant improvement over existing RGB-only methods.



### Towards Balanced Learning for Instance Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.10175v1
- **DOI**: 10.1007/s11263-021-01434-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10175v1)
- **Published**: 2021-08-23 13:40:45+00:00
- **Updated**: 2021-08-23 13:40:45+00:00
- **Authors**: Jiangmiao Pang, Kai Chen, Qi Li, Zhihai Xu, Huajun Feng, Jianping Shi, Wanli Ouyang, Dahua Lin
- **Comment**: Accepted by IJCV. Journal extension of paper arXiv:1904.02701
- **Journal**: None
- **Summary**: Instance recognition is rapidly advanced along with the developments of various deep convolutional neural networks. Compared to the architectures of networks, the training process, which is also crucial to the success of detectors, has received relatively less attention. In this work, we carefully revisit the standard training practice of detectors, and find that the detection performance is often limited by the imbalance during the training process, which generally consists in three levels - sample level, feature level, and objective level. To mitigate the adverse effects caused thereby, we propose Libra R-CNN, a simple yet effective framework towards balanced learning for instance recognition. It integrates IoU-balanced sampling, balanced feature pyramid, and objective re-weighting, respectively for reducing the imbalance at sample, feature, and objective level. Extensive experiments conducted on MS COCO, LVIS and Pascal VOC datasets prove the effectiveness of the overall balanced design.



### LivDet 2021 Fingerprint Liveness Detection Competition -- Into the unknown
- **Arxiv ID**: http://arxiv.org/abs/2108.10183v1
- **DOI**: 10.1109/IJCB52358.2021.9484399
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10183v1)
- **Published**: 2021-08-23 13:53:25+00:00
- **Updated**: 2021-08-23 13:53:25+00:00
- **Authors**: Roberto Casula, Marco Micheletto, Giulia Orrù, Rita Delussu, Sara Concas, Andrea Panzino, Gian Luca Marcialis
- **Comment**: Preprint version of a paper accepted at IJCB 2021
- **Journal**: 2021 IEEE International Joint Conference on Biometrics (IJCB),
  2021, pp. 1-6
- **Summary**: The International Fingerprint Liveness Detection Competition is an international biennial competition open to academia and industry with the aim to assess and report advances in Fingerprint Presentation Attack Detection. The proposed "Liveness Detection in Action" and "Fingerprint representation" challenges were aimed to evaluate the impact of a PAD embedded into a verification system, and the effectiveness and compactness of feature sets for mobile applications. Furthermore, we experimented a new spoof fabrication method that has particularly affected the final results. Twenty-three algorithms were submitted to the competition, the maximum number ever achieved by LivDet.



### Diverse Similarity Encoder for Deep GAN Inversion
- **Arxiv ID**: http://arxiv.org/abs/2108.10201v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.10201v2)
- **Published**: 2021-08-23 14:37:58+00:00
- **Updated**: 2022-07-12 11:23:04+00:00
- **Authors**: Cheng Yu, Wenmin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Current deep generative adversarial networks (GANs) can synthesize high-quality (HQ) images, so learning representation with GANs is favorable. GAN inversion is one of emerging approaches that study how to invert images into latent space. Existing GAN encoders can invert images on StyleGAN, but cannot adapt to other deep GANs. We propose a novel approach to address this issue. By evaluating diverse similarity in latent vectors and images, we design an adaptive encoder, named diverse similarity encoder (DSE), that can be expanded to a variety of state-of-the-art GANs. DSE makes GANs reconstruct higher fidelity images from HQ images, no matter whether they are synthesized or real images. DSE has unified convolutional blocks and adapts well to mainstream deep GANs, e.g., PGGAN, StyleGAN, and BigGAN.



### Separable Convolutions for Optimizing 3D Stereo Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.10216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10216v1)
- **Published**: 2021-08-23 14:52:00+00:00
- **Updated**: 2021-08-23 14:52:00+00:00
- **Authors**: Rafia Rahim, Faranak Shamsafar, Andreas Zell
- **Comment**: Accepted at IEEE International Conference on Image Processing, ICIP,
  2021
- **Journal**: None
- **Summary**: Deep learning based 3D stereo networks give superior performance compared to 2D networks and conventional stereo methods. However, this improvement in the performance comes at the cost of increased computational complexity, thus making these networks non-practical for the real-world applications. Specifically, these networks use 3D convolutions as a major work horse to refine and regress disparities. In this work first, we show that these 3D convolutions in stereo networks consume up to 94% of overall network operations and act as a major bottleneck. Next, we propose a set of "plug-&-run" separable convolutions to reduce the number of parameters and operations. When integrated with the existing state of the art stereo networks, these convolutions lead up to 7x reduction in number of operations and up to 3.5x reduction in parameters without compromising their performance. In fact these convolutions lead to improvement in their performance in the majority of cases.



### Deep Bayesian Image Set Classification: A Defence Approach against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2108.10217v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.10217v1)
- **Published**: 2021-08-23 14:52:44+00:00
- **Updated**: 2021-08-23 14:52:44+00:00
- **Authors**: Nima Mirnateghi, Syed Afaq Ali Shah, Mohammed Bennamoun
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has become an integral part of various computer vision systems in recent years due to its outstanding achievements for object recognition, facial recognition, and scene understanding. However, deep neural networks (DNNs) are susceptible to be fooled with nearly high confidence by an adversary. In practice, the vulnerability of deep learning systems against carefully perturbed images, known as adversarial examples, poses a dire security threat in the physical world applications. To address this phenomenon, we present, what to our knowledge, is the first ever image set based adversarial defence approach. Image set classification has shown an exceptional performance for object and face recognition, owing to its intrinsic property of handling appearance variability. We propose a robust deep Bayesian image set classification as a defence framework against a broad range of adversarial attacks. We extensively experiment the performance of the proposed technique with several voting strategies. We further analyse the effects of image size, perturbation magnitude, along with the ratio of perturbed images in each image set. We also evaluate our technique with the recent state-of-the-art defence methods, and single-shot recognition task. The empirical results demonstrate superior performance on CIFAR-10, MNIST, ETH-80, and Tiny ImageNet datasets.



### Fusion of evidential CNN classifiers for image classification
- **Arxiv ID**: http://arxiv.org/abs/2108.10233v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.10233v1)
- **Published**: 2021-08-23 15:12:26+00:00
- **Updated**: 2021-08-23 15:12:26+00:00
- **Authors**: Zheng Tong, Philippe Xu, Thierry Denoeux
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an information-fusion approach based on belief functions to combine convolutional neural networks. In this approach, several pre-trained DS-based CNN architectures extract features from input images and convert them into mass functions on different frames of discernment. A fusion module then aggregates these mass functions using Dempster's rule. An end-to-end learning procedure allows us to fine-tune the overall architecture using a learning set with soft labels, which further improves the classification performance. The effectiveness of this approach is demonstrated experimentally using three benchmark databases.



### SwinIR: Image Restoration Using Swin Transformer
- **Arxiv ID**: http://arxiv.org/abs/2108.10257v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.10257v1)
- **Published**: 2021-08-23 15:55:32+00:00
- **Updated**: 2021-08-23 15:55:32+00:00
- **Authors**: Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte
- **Comment**: Sota results on classical/lightweight/real-world image SR, image
  denoising and JPEG compression artifact reduction. Code:
  https://github.com/JingyunLiang/SwinIR
- **Journal**: None
- **Summary**: Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by $\textbf{up to 0.14$\sim$0.45dB}$, while the total number of parameters can be reduced by $\textbf{up to 67%}$.



### Exploring Biases and Prejudice of Facial Synthesis via Semantic Latent Space
- **Arxiv ID**: http://arxiv.org/abs/2108.10265v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.10265v1)
- **Published**: 2021-08-23 16:09:18+00:00
- **Updated**: 2021-08-23 16:09:18+00:00
- **Authors**: Xuyang Shen, Jo Plested, Sabrina Caldwell, Tom Gedeon
- **Comment**: 8 pages, 11 figures; accepted by IJCNN2021
- **Journal**: None
- **Summary**: Deep learning (DL) models are widely used to provide a more convenient and smarter life. However, biased algorithms will negatively influence us. For instance, groups targeted by biased algorithms will feel unfairly treated and even fearful of negative consequences of these biases. This work targets biased generative models' behaviors, identifying the cause of the biases and eliminating them. We can (as expected) conclude that biased data causes biased predictions of face frontalization models. Varying the proportions of male and female faces in the training data can have a substantial effect on behavior on the test data: we found that the seemingly obvious choice of 50:50 proportions was not the best for this dataset to reduce biased behavior on female faces, which was 71% unbiased as compared to our top unbiased rate of 84%. Failure in generation and generating incorrect gender faces are two behaviors of these models. In addition, only some layers in face frontalization models are vulnerable to biased datasets. Optimizing the skip-connections of the generator in face frontalization models can make models less biased. We conclude that it is likely to be impossible to eliminate all training bias without an unlimited size dataset, and our experiments show that the bias can be reduced and quantified. We believe the next best to a perfect unbiased predictor is one that has minimized the remaining known bias.



### Vogtareuth Rehab Depth Datasets: Benchmark for Marker-less Posture Estimation in Rehabilitation
- **Arxiv ID**: http://arxiv.org/abs/2108.10272v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10272v1)
- **Published**: 2021-08-23 16:18:26+00:00
- **Updated**: 2021-08-23 16:18:26+00:00
- **Authors**: Soubarna Banik, Alejandro Mendoza Garcia, Lorenz Kiwull, Steffen Berweck, Alois Knoll
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Posture estimation using a single depth camera has become a useful tool for analyzing movements in rehabilitation. Recent advances in posture estimation in computer vision research have been possible due to the availability of large-scale pose datasets. However, the complex postures involved in rehabilitation exercises are not represented in the existing benchmark depth datasets. To address this limitation, we propose two rehabilitation-specific pose datasets containing depth images and 2D pose information of patients, both adult and children, performing rehab exercises. We use a state-of-the-art marker-less posture estimation model which is trained on a non-rehab benchmark dataset. We evaluate it on our rehab datasets, and observe that the performance degrades significantly from non-rehab to rehab, highlighting the need for these datasets. We show that our dataset can be used to train pose models to detect rehab-specific complex postures. The datasets will be released for the benefit of the research community.



### ChiNet: Deep Recurrent Convolutional Learning for Multimodal Spacecraft Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2108.10282v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.10282v1)
- **Published**: 2021-08-23 16:48:58+00:00
- **Updated**: 2021-08-23 16:48:58+00:00
- **Authors**: Duarte Rondao, Nabil Aouf, Mark A. Richardson
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: This paper presents an innovative deep learning pipeline which estimates the relative pose of a spacecraft by incorporating the temporal information from a rendezvous sequence. It leverages the performance of long short-term memory (LSTM) units in modelling sequences of data for the processing of features extracted by a convolutional neural network (CNN) backbone. Three distinct training strategies, which follow a coarse-to-fine funnelled approach, are combined to facilitate feature learning and improve end-to-end pose estimation by regression. The capability of CNNs to autonomously ascertain feature representations from images is exploited to fuse thermal infrared data with red-green-blue (RGB) inputs, thus mitigating the effects of artefacts from imaging space objects in the visible wavelength. Each contribution of the proposed framework, dubbed ChiNet, is demonstrated on a synthetic dataset, and the complete pipeline is validated on experimental data.



### Cross-Quality LFW: A Database for Analyzing Cross-Resolution Image Face Recognition in Unconstrained Environments
- **Arxiv ID**: http://arxiv.org/abs/2108.10290v3
- **DOI**: 10.1109/FG52635.2021.9666960
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10290v3)
- **Published**: 2021-08-23 17:04:32+00:00
- **Updated**: 2022-11-25 11:44:07+00:00
- **Authors**: Martin Knoche, Stefan Hörmann, Gerhard Rigoll
- **Comment**: 9 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Real-world face recognition applications often deal with suboptimal image quality or resolution due to different capturing conditions such as various subject-to-camera distances, poor camera settings, or motion blur. This characteristic has an unignorable effect on performance. Recent cross-resolution face recognition approaches used simple, arbitrary, and unrealistic down- and up-scaling techniques to measure robustness against real-world edge-cases in image quality. Thus, we propose a new standardized benchmark dataset and evaluation protocol derived from the famous Labeled Faces in the Wild (LFW). In contrast to previous derivatives, which focus on pose, age, similarity, and adversarial attacks, our Cross-Quality Labeled Faces in the Wild (XQLFW) maximizes the quality difference. It contains only more realistic synthetically degraded images when necessary. Our proposed dataset is then used to further investigate the influence of image quality on several state-of-the-art approaches. With XQLFW, we show that these models perform differently in cross-quality cases, and hence, the generalizing capability is not accurately predicted by their performance on LFW. Additionally, we report baseline accuracy with recent deep learning models explicitly trained for cross-resolution applications and evaluate the susceptibility to image quality. To encourage further research in cross-resolution face recognition and incite the assessment of image quality robustness, we publish the database and code for evaluation.



### PW-MAD: Pixel-wise Supervision for Generalized Face Morphing Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.10291v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10291v2)
- **Published**: 2021-08-23 17:04:51+00:00
- **Updated**: 2021-09-26 16:56:22+00:00
- **Authors**: Naser Damer, Noemie Spiller, Meiling Fang, Fadi Boutros, Florian Kirchbuchner, Arjan Kuijper
- **Comment**: Accepted at the 16th International Symposium on Visual Computing
  (ISVC 2021)
- **Journal**: None
- **Summary**: A face morphing attack image can be verified to multiple identities, making this attack a major vulnerability to processes based on identity verification, such as border checks. Various methods have been proposed to detect face morphing attacks, however, with low generalizability to unexpected post-morphing processes. A major post-morphing process is the print and scan operation performed in many countries when issuing a passport or identity document. In this work, we address this generalization problem by adapting a pixel-wise supervision approach where we train a network to classify each pixel of the image into an attack or not, rather than only having one label for the whole image. Our pixel-wise morphing attack detection (PW-MAD) solution proved to perform more accurately than a set of established baselines. More importantly, PW-MAD shows high generalizability in comparison to related works, when evaluated on unknown re-digitized attacks. Additionally to our PW-MAD approach, we create a new face morphing attack dataset with digital and re-digitized samples, namely the LMA-DRD dataset that is publicly available for research purposes upon request.



### Ranking Models in Unlabeled New Environments
- **Arxiv ID**: http://arxiv.org/abs/2108.10310v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10310v2)
- **Published**: 2021-08-23 17:57:15+00:00
- **Updated**: 2021-09-06 13:20:23+00:00
- **Authors**: Xiaoxiao Sun, Yunzhong Hou, Weijian Deng, Hongdong Li, Liang Zheng
- **Comment**: 13 pages, 10 figures, ICCV2021
- **Journal**: None
- **Summary**: Consider a scenario where we are supplied with a number of ready-to-use models trained on a certain source domain and hope to directly apply the most appropriate ones to different target domains based on the models' relative performance. Ideally we should annotate a validation set for model performance assessment on each new target environment, but such annotations are often very expensive. Under this circumstance, we introduce the problem of ranking models in unlabeled new environments. For this problem, we propose to adopt a proxy dataset that 1) is fully labeled and 2) well reflects the true model rankings in a given target environment, and use the performance rankings on the proxy sets as surrogates. We first select labeled datasets as the proxy. Specifically, datasets that are more similar to the unlabeled target domain are found to better preserve the relative performance rankings. Motivated by this, we further propose to search the proxy set by sampling images from various datasets that have similar distributions as the target. We analyze the problem and its solutions on the person re-identification (re-ID) task, for which sufficient datasets are publicly available, and show that a carefully constructed proxy set effectively captures relative performance ranking in new environments. Code is available at \url{https://github.com/sxzrt/Proxy-Set}.



### Exploring Simple 3D Multi-Object Tracking for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2108.10312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10312v1)
- **Published**: 2021-08-23 17:59:22+00:00
- **Updated**: 2021-08-23 17:59:22+00:00
- **Authors**: Chenxu Luo, Xiaodong Yang, Alan Yuille
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: 3D multi-object tracking in LiDAR point clouds is a key ingredient for self-driving vehicles. Existing methods are predominantly based on the tracking-by-detection pipeline and inevitably require a heuristic matching step for the detection association. In this paper, we present SimTrack to simplify the hand-crafted tracking paradigm by proposing an end-to-end trainable model for joint detection and tracking from raw point clouds. Our key design is to predict the first-appear location of each object in a given snippet to get the tracking identity and then update the location based on motion estimation. In the inference, the heuristic matching step can be completely waived by a simple read-off operation. SimTrack integrates the tracked object association, newborn object detection, and dead track killing in a single unified model. We conduct extensive evaluations on two large-scale datasets: nuScenes and Waymo Open Dataset. Experimental results reveal that our simple approach compares favorably with the state-of-the-art methods while ruling out the heuristic matching rules.



### edge-SR: Super-Resolution For The Masses
- **Arxiv ID**: http://arxiv.org/abs/2108.10335v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2108.10335v2)
- **Published**: 2021-08-23 18:00:19+00:00
- **Updated**: 2021-10-15 15:55:18+00:00
- **Authors**: Pablo Navarrete Michelini, Yunhua Lu, Xingqun Jiang
- **Comment**: In WACV 2022. Code available in https://github.com/pnavarre/eSR
- **Journal**: None
- **Summary**: Classic image scaling (e.g. bicubic) can be seen as one convolutional layer and a single upscaling filter. Its implementation is ubiquitous in all display devices and image processing software. In the last decade deep learning systems have been introduced for the task of image super-resolution (SR), using several convolutional layers and numerous filters. These methods have taken over the benchmarks of image quality for upscaling tasks. Would it be possible to replace classic upscalers with deep learning architectures on edge devices such as display panels, tablets, laptop computers, etc.? On one hand, the current trend in Edge-AI chips shows a promising future in this direction, with rapid development of hardware that can run deep-learning tasks efficiently. On the other hand, in image SR only few architectures have pushed the limit to extreme small sizes that can actually run on edge devices at real-time. We explore possible solutions to this problem with the aim to fill the gap between classic upscalers and small deep learning configurations. As a transition from classic to deep-learning upscaling we propose edge-SR (eSR), a set of one-layer architectures that use interpretable mechanisms to upscale images. Certainly, a one-layer architecture cannot reach the quality of deep learning systems. Nevertheless, we find that for high speed requirements, eSR becomes better at trading-off image quality and runtime performance. Filling the gap between classic and deep-learning architectures for image upscaling is critical for massive adoption of this technology. It is equally important to have an interpretable system that can reveal the inner strategies to solve this problem and guide us to future improvements and better understanding of larger networks.



### Explaining Bayesian Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.10346v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2108.10346v1)
- **Published**: 2021-08-23 18:09:41+00:00
- **Updated**: 2021-08-23 18:09:41+00:00
- **Authors**: Kirill Bykov, Marina M. -C. Höhne, Adelaida Creosteanu, Klaus-Robert Müller, Frederick Klauschen, Shinichi Nakajima, Marius Kloft
- **Comment**: 16 pages, 8 figures
- **Journal**: None
- **Summary**: To make advanced learning machines such as Deep Neural Networks (DNNs) more transparent in decision making, explainable AI (XAI) aims to provide interpretations of DNNs' predictions. These interpretations are usually given in the form of heatmaps, each one illustrating relevant patterns regarding the prediction for a given instance. Bayesian approaches such as Bayesian Neural Networks (BNNs) so far have a limited form of transparency (model transparency) already built-in through their prior weight distribution, but notably, they lack explanations of their predictions for given instances. In this work, we bring together these two perspectives of transparency into a holistic explanation framework for explaining BNNs. Within the Bayesian framework, the network weights follow a probability distribution. Hence, the standard (deterministic) prediction strategy of DNNs extends in BNNs to a predictive distribution, and thus the standard explanation extends to an explanation distribution. Exploiting this view, we uncover that BNNs implicitly employ multiple heterogeneous prediction strategies. While some of these are inherited from standard DNNs, others are revealed to us by considering the inherent uncertainty in BNNs. Our quantitative and qualitative experiments on toy/benchmark data and real-world data from pathology show that the proposed approach of explaining BNNs can lead to more effective and insightful explanations.



### Interpreting Face Inference Models using Hierarchical Network Dissection
- **Arxiv ID**: http://arxiv.org/abs/2108.10360v2
- **DOI**: 10.1007/s11263-022-01603-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10360v2)
- **Published**: 2021-08-23 18:52:47+00:00
- **Updated**: 2022-03-29 03:06:23+00:00
- **Authors**: Divyang Teotia, Agata Lapedriza, Sarah Ostadabbas
- **Comment**: None
- **Journal**: International Journal of Computer Vision (2022)
- **Summary**: This paper presents Hierarchical Network Dissection, a general pipeline to interpret the internal representation of face-centric inference models. Using a probabilistic formulation, our pipeline pairs units of the model with concepts in our "Face Dictionary", a collection of facial concepts with corresponding sample images. Our pipeline is inspired by Network Dissection, a popular interpretability model for object-centric and scene-centric models. However, our formulation allows to deal with two important challenges of face-centric models that Network Dissection cannot address: (1) spacial overlap of concepts: there are different facial concepts that simultaneously occur in the same region of the image, like "nose" (facial part) and "pointy nose" (facial attribute); and (2) global concepts: there are units with affinity to concepts that do not refer to specific locations of the face (e.g. apparent age). We use Hierarchical Network Dissection to dissect different face-centric inference models trained on widely-used facial datasets. The results show models trained for different tasks learned different internal representations. Furthermore, the interpretability results can reveal some biases in the training data and some interesting characteristics of the face-centric inference tasks. Finally, we conduct controlled experiments on biased data to showcase the potential of Hierarchical Network Dissection for bias discovery. The results illustrate how Hierarchical Network Dissection can be used to discover and quantify bias in the training data that is also encoded in the model.



### Marine vessel tracking using a monocular camera
- **Arxiv ID**: http://arxiv.org/abs/2108.10367v1
- **DOI**: 10.5220/0010516000170028
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.10367v1)
- **Published**: 2021-08-23 19:08:20+00:00
- **Updated**: 2021-08-23 19:08:20+00:00
- **Authors**: Tobias Jacob, Raffaele Galliera, Muddasar Ali, Sikha Bagui
- **Comment**: 12 pages, 9 figures, the paper is based on the submission for the AI
  Tracks at Sea challenge made by the same team taking to a 3rd place in the
  competition, included in DeLTA 2021 conference proceedings, published on
  SCITEPRESS Digital Library and available at
  https://www.scitepress.org/PublicationsDetail.aspx?ID=yzZS+b/VkZ4=&t=1
- **Journal**: None
- **Summary**: In this paper, a new technique for camera calibration using only GPS data is presented. A new way of tracking objects that move on a plane in a video is achieved by using the location and size of the bounding box to estimate the distance, achieving an average prediction error of 5.55m per 100m distance from the camera. This solution can be run in real-time at the edge, achieving efficient inference in a low-powered IoT environment while also being able to track multiple different vessels.



### Lightweight Multi-person Total Motion Capture Using Sparse Multi-view Cameras
- **Arxiv ID**: http://arxiv.org/abs/2108.10378v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10378v1)
- **Published**: 2021-08-23 19:23:35+00:00
- **Updated**: 2021-08-23 19:23:35+00:00
- **Authors**: Yuxiang Zhang, Zhe Li, Liang An, Mengcheng Li, Tao Yu, Yebin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-person total motion capture is extremely challenging when it comes to handle severe occlusions, different reconstruction granularities from body to face and hands, drastically changing observation scales and fast body movements. To overcome these challenges above, we contribute a lightweight total motion capture system for multi-person interactive scenarios using only sparse multi-view cameras. By contributing a novel hand and face bootstrapping algorithm, our method is capable of efficient localization and accurate association of the hands and faces even on severe occluded occasions. We leverage both pose regression and keypoints detection methods and further propose a unified two-stage parametric fitting method for achieving pixel-aligned accuracy. Moreover, for extremely self-occluded poses and close interactions, a novel feedback mechanism is proposed to propagate the pixel-aligned reconstructions into the next frame for more accurate association. Overall, we propose the first light-weight total capture system and achieves fast, robust and accurate multi-person total motion capture performance. The results and experiments show that our method achieves more accurate results than existing methods under sparse-view setups.



### CE-Dedup: Cost-Effective Convolutional Neural Nets Training based on Image Deduplication
- **Arxiv ID**: http://arxiv.org/abs/2109.00899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00899v1)
- **Published**: 2021-08-23 19:54:03+00:00
- **Updated**: 2021-08-23 19:54:03+00:00
- **Authors**: Xuan Li, Liqiong Chang, Xue Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Attributed to the ever-increasing large image datasets, Convolutional Neural Networks (CNNs) have become popular for vision-based tasks. It is generally admirable to have larger-sized datasets for higher network training accuracies. However, the impact of dataset quality has not to be involved. It is reasonable to assume the near-duplicate images exist in the datasets. For instance, the Street View House Numbers (SVHN) dataset having cropped house plate digits from 0 to 9 are likely to have repetitive digits from the same/similar house plates. Redundant images may take up a certain portion of the dataset without consciousness. While contributing little to no accuracy improvement for the CNNs training, these duplicated images unnecessarily pose extra resource and computation consumption. To this end, this paper proposes a framework to assess the impact of the near-duplicate images on CNN training performance, called CE-Dedup. Specifically, CE-Dedup associates a hashing-based image deduplication approach with downstream CNNs-based image classification tasks. CE-Dedup balances the tradeoff between a large deduplication ratio and a stable accuracy by adjusting the deduplication threshold. The effectiveness of CE-Dedup is validated through extensive experiments on well-known CNN benchmarks. On one hand, while maintaining the same validation accuracy, CE-Dedup can reduce the dataset size by 23%. On the other hand, when allowing a small validation accuracy drop (by 5%), CE-Dedup can trim the dataset size by 75%.



### Dynamic Network Quantization for Efficient Video Inference
- **Arxiv ID**: http://arxiv.org/abs/2108.10394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10394v1)
- **Published**: 2021-08-23 20:23:57+00:00
- **Updated**: 2021-08-23 20:23:57+00:00
- **Authors**: Ximeng Sun, Rameswar Panda, Chun-Fu Chen, Aude Oliva, Rogerio Feris, Kate Saenko
- **Comment**: ICCV 2021 Camera Ready Version
- **Journal**: None
- **Summary**: Deep convolutional networks have recently achieved great success in video recognition, yet their practical realization remains a challenge due to the large amount of computational resources required to achieve robust recognition. Motivated by the effectiveness of quantization for boosting efficiency, in this paper, we propose a dynamic network quantization framework, that selects optimal precision for each frame conditioned on the input for efficient video recognition. Specifically, given a video clip, we train a very lightweight network in parallel with the recognition network, to produce a dynamic policy indicating which numerical precision to be used per frame in recognizing videos. We train both networks effectively using standard backpropagation with a loss to achieve both competitive performance and resource efficiency required for video recognition. Extensive experiments on four challenging diverse benchmark datasets demonstrate that our proposed approach provides significant savings in computation and memory usage while outperforming the existing state-of-the-art methods.



### Learning Motion Priors for 4D Human Body Capture in 3D Scenes
- **Arxiv ID**: http://arxiv.org/abs/2108.10399v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.10399v1)
- **Published**: 2021-08-23 20:47:09+00:00
- **Updated**: 2021-08-23 20:47:09+00:00
- **Authors**: Siwei Zhang, Yan Zhang, Federica Bogo, Marc Pollefeys, Siyu Tang
- **Comment**: Accepted by ICCV 2021, camera ready version with appendix
- **Journal**: None
- **Summary**: Recovering high-quality 3D human motion in complex scenes from monocular videos is important for many applications, ranging from AR/VR to robotics. However, capturing realistic human-scene interactions, while dealing with occlusions and partial views, is challenging; current approaches are still far from achieving compelling results. We address this problem by proposing LEMO: LEarning human MOtion priors for 4D human body capture. By leveraging the large-scale motion capture dataset AMASS, we introduce a novel motion smoothness prior, which strongly reduces the jitters exhibited by poses recovered over a sequence. Furthermore, to handle contacts and occlusions occurring frequently in body-scene interactions, we design a contact friction term and a contact-aware motion infiller obtained via per-instance self-supervised training. To prove the effectiveness of the proposed motion priors, we combine them into a novel pipeline for 4D human body capture in 3D scenes. With our pipeline, we demonstrate high-quality 4D human body capture, reconstructing smooth motions and physically plausible body-scene interactions. The code and data are available at https://sanweiliti.github.io/LEMO/LEMO.html.



### CoverTheFace: face covering monitoring and demonstrating using deep learning and statistical shape analysis
- **Arxiv ID**: http://arxiv.org/abs/2108.10430v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.10430v1)
- **Published**: 2021-08-23 22:11:07+00:00
- **Updated**: 2021-08-23 22:11:07+00:00
- **Authors**: Yixin Hu, Xingyu Li
- **Comment**: None
- **Journal**: None
- **Summary**: Wearing a mask is a strong protection against the COVID-19 pandemic, even though the vaccine has been successfully developed and is widely available. However, many people wear them incorrectly. This observation prompts us to devise an automated approach to monitor the condition of people wearing masks. Unlike previous studies, our work goes beyond mask detection; it focuses on generating a personalized demonstration on proper mask-wearing, which helps people use masks better through visual demonstration rather than text explanation. The pipeline starts from the detection of face covering. For images where faces are improperly covered, our mask overlay module incorporates statistical shape analysis (SSA) and dense landmark alignment to approximate the geometry of a face and generates corresponding face-covering examples. Our results show that the proposed system successfully identifies images with faces covered properly. Our ablation study on mask overlay suggests that the SSA model helps to address variations in face shapes, orientations, and scales. The final face-covering examples, especially half profile face images, surpass previous arts by a noticeable margin.



### All You Need is Color: Image based Spatial Gene Expression Prediction using Neural Stain Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.10446v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2108.10446v2)
- **Published**: 2021-08-23 23:43:38+00:00
- **Updated**: 2021-08-26 10:45:21+00:00
- **Authors**: Muhammad Dawood, Kim Branson, Nasir M. Rajpoot, Fayyaz ul Amir Afsar Minhas
- **Comment**: 14 pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: "Is it possible to predict expression levels of different genes at a given spatial location in the routine histology image of a tumor section by modeling its stain absorption characteristics?" In this work, we propose a "stain-aware" machine learning approach for prediction of spatial transcriptomic gene expression profiles using digital pathology image of a routine Hematoxylin & Eosin (H&E) histology section. Unlike recent deep learning methods which are used for gene expression prediction, our proposed approach termed Neural Stain Learning (NSL) explicitly models the association of stain absorption characteristics of the tissue with gene expression patterns in spatial transcriptomics by learning a problem-specific stain deconvolution matrix in an end-to-end manner. The proposed method with only 11 trainable weight parameters outperforms both classical regression models with cellular composition and morphological features as well as deep learning methods. We have found that the gene expression predictions from the proposed approach show higher correlations with true expression values obtained through sequencing for a larger set of genes in comparison to other approaches.



### Fast Robust Tensor Principal Component Analysis via Fiber CUR Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2108.10448v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2108.10448v1)
- **Published**: 2021-08-23 23:49:40+00:00
- **Updated**: 2021-08-23 23:49:40+00:00
- **Authors**: HanQin Cai, Zehan Chao, Longxiu Huang, Deanna Needell
- **Comment**: Accepted to Workshop on Robust Subspace Learning and Applications in
  Computer Vision, International Conference on Computer Vision (ICCV) 2021
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision Workshops, pages 189-197, 2021
- **Summary**: We study the problem of tensor robust principal component analysis (TRPCA), which aims to separate an underlying low-multilinear-rank tensor and a sparse outlier tensor from their sum. In this work, we propose a fast non-convex algorithm, coined Robust Tensor CUR (RTCUR), for large-scale TRPCA problems. RTCUR considers a framework of alternating projections and utilizes the recently developed tensor Fiber CUR decomposition to dramatically lower the computational complexity. The performance advantage of RTCUR is empirically verified against the state-of-the-arts on the synthetic datasets and is further demonstrated on the real-world application such as color video background subtraction.



