# Arxiv Papers in cs.CV on 2021-08-28
### Image-to-Graph Convolutional Network for Deformable Shape Reconstruction from a Single Projection Image
- **Arxiv ID**: http://arxiv.org/abs/2108.12533v2
- **DOI**: 10.1007/978-3-030-87202-1_25
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12533v2)
- **Published**: 2021-08-28 00:00:09+00:00
- **Updated**: 2021-08-31 09:50:27+00:00
- **Authors**: M. Nakao, F. Tong, M. Nakamura, T. Matsuda
- **Comment**: This paper will be appeared in MICCAI 2021
- **Journal**: International Conference on Medical Image Computing and Computer
  Assisted Intervention 2021 (MICCAI)
- **Summary**: Shape reconstruction of deformable organs from two-dimensional X-ray images is a key technology for image-guided intervention. In this paper, we propose an image-to-graph convolutional network (IGCN) for deformable shape reconstruction from a single-viewpoint projection image. The IGCN learns relationship between shape/deformation variability and the deep image features based on a deformation mapping scheme. In experiments targeted to the respiratory motion of abdominal organs, we confirmed the proposed framework with a regularized loss function can reconstruct liver shapes from a single digitally reconstructed radiograph with a mean distance error of 3.6mm.



### SeeTheSeams: Localized Detection of Seam Carving based Image Forgery in Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2108.12534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12534v1)
- **Published**: 2021-08-28 00:00:37+00:00
- **Updated**: 2021-08-28 00:00:37+00:00
- **Authors**: Chandrakanth Gudavalli, Erik Rosten, Lakshmanan Nataraj, Shivkumar Chandrasekaran, B. S. Manjunath
- **Comment**: None
- **Journal**: None
- **Summary**: Seam carving is a popular technique for content aware image retargeting. It can be used to deliberately manipulate images, for example, change the GPS locations of a building or insert/remove roads in a satellite image. This paper proposes a novel approach for detecting and localizing seams in such images. While there are methods to detect seam carving based manipulations, this is the first time that robust localization and detection of seam carving forgery is made possible. We also propose a seam localization score (SLS) metric to evaluate the effectiveness of localization. The proposed method is evaluated extensively on a large collection of images from different sources, demonstrating a high level of detection and localization performance across these datasets. The datasets curated during this work will be released to the public.



### High performing ensemble of convolutional neural networks for insect pest image detection
- **Arxiv ID**: http://arxiv.org/abs/2108.12539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12539v1)
- **Published**: 2021-08-28 00:49:11+00:00
- **Updated**: 2021-08-28 00:49:11+00:00
- **Authors**: Loris Nanni, Alessandro Manfe, Gianluca Maguolo, Alessandra Lumini, Sheryl Brahnam
- **Comment**: None
- **Journal**: None
- **Summary**: Pest infestation is a major cause of crop damage and lost revenues worldwide. Automatic identification of invasive insects would greatly speedup the identification of pests and expedite their removal. In this paper, we generate ensembles of CNNs based on different topologies (ResNet50, GoogleNet, ShuffleNet, MobileNetv2, and DenseNet201) altered by random selection from a simple set of data augmentation methods or optimized with different Adam variants for pest identification. Two new Adam algorithms for deep network optimization based on DGrad are proposed that introduce a scaling factor in the learning rate. Sets of the five CNNs that vary in either data augmentation or the type of Adam optimization were trained on both the Deng (SMALL) and the large IP102 pest data sets. Ensembles were compared and evaluated using three performance indicators. The best performing ensemble, which combined the CNNs using the different augmentation methods and the two new Adam variants proposed here, achieved state of the art on both insect data sets: 95.52% on Deng and 73.46% on IP102, a score on Deng that competed with human expert classifications. Additional tests were performed on data sets for medical imagery classification that further validated the robustness and power of the proposed Adam optimization variants. All MATLAB source code is available at https://github.com/LorisNanni/.



### Improving Semi-Supervised and Domain-Adaptive Semantic Segmentation with Self-Supervised Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2108.12545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12545v1)
- **Published**: 2021-08-28 01:33:38+00:00
- **Updated**: 2021-08-28 01:33:38+00:00
- **Authors**: Lukas Hoyer, Dengxin Dai, Qin Wang, Yuhua Chen, Luc Van Gool
- **Comment**: arXiv admin note: text overlap with arXiv:2012.10782
- **Journal**: None
- **Summary**: Training deep networks for semantic segmentation requires large amounts of labeled training data, which presents a major challenge in practice, as labeling segmentation masks is a highly labor-intensive process. To address this issue, we present a framework for semi-supervised and domain-adaptive semantic segmentation, which is enhanced by self-supervised monocular depth estimation (SDE) trained only on unlabeled image sequences.   In particular, we utilize SDE as an auxiliary task comprehensively across the entire learning framework: First, we automatically select the most useful samples to be annotated for semantic segmentation based on the correlation of sample diversity and difficulty between SDE and semantic segmentation. Second, we implement a strong data augmentation by mixing images and labels using the geometry of the scene. Third, we transfer knowledge from features learned during SDE to semantic segmentation by means of transfer and multi-task learning. And fourth, we exploit additional labeled synthetic data with Cross-Domain DepthMix and Matching Geometry Sampling to align synthetic and real data.   We validate the proposed model on the Cityscapes dataset, where all four contributions demonstrate significant performance gains, and achieve state-of-the-art results for semi-supervised semantic segmentation as well as for semi-supervised domain adaptation. In particular, with only 1/30 of the Cityscapes labels, our method achieves 92% of the fully-supervised baseline performance and even 97% when exploiting additional data from GTA. The source code is available at https://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth.



### QACE: Asking Questions to Evaluate an Image Caption
- **Arxiv ID**: http://arxiv.org/abs/2108.12560v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12560v1)
- **Published**: 2021-08-28 03:04:28+00:00
- **Updated**: 2021-08-28 03:04:28+00:00
- **Authors**: Hwanhee Lee, Thomas Scialom, Seunghyun Yoon, Franck Dernoncourt, Kyomin Jung
- **Comment**: EMNLP 2021 Findings
- **Journal**: None
- **Summary**: In this paper, we propose QACE, a new metric based on Question Answering for Caption Evaluation. QACE generates questions on the evaluated caption and checks its content by asking the questions on either the reference caption or the source image. We first develop QACE-Ref that compares the answers of the evaluated caption to its reference, and report competitive results with the state-of-the-art metrics. To go further, we propose QACE-Img, which asks the questions directly on the image, instead of reference. A Visual-QA system is necessary for QACE-Img. Unfortunately, the standard VQA models are framed as a classification among only a few thousand categories. Instead, we propose Visual-T5, an abstractive VQA system. The resulting metric, QACE-Img is multi-modal, reference-less, and explainable. Our experiments show that QACE-Img compares favorably w.r.t. other reference-less metrics. We will release the pre-trained models to compute QACE.



### AMMASurv: Asymmetrical Multi-Modal Attention for Accurate Survival Analysis with Whole Slide Images and Gene Expression Data
- **Arxiv ID**: http://arxiv.org/abs/2108.12565v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.12565v2)
- **Published**: 2021-08-28 04:02:10+00:00
- **Updated**: 2021-12-08 07:48:15+00:00
- **Authors**: Ruoqi Wang, Ziwang Huang, Haitao Wang, Hejun Wu
- **Comment**: 4 pages. Accepted by IEEE BIBM 2021
- **Journal**: None
- **Summary**: The use of multi-modal data such as the combination of whole slide images (WSIs) and gene expression data for survival analysis can lead to more accurate survival predictions. Previous multi-modal survival models are not able to efficiently excavate the intrinsic information within each modality. Moreover, previous methods regard the information from different modalities as similarly important so they cannot flexibly utilize the potential connection between the modalities. To address the above problems, we propose a new asymmetrical multi-modal method, termed as AMMASurv. Different from previous works, AMMASurv can effectively utilize the intrinsic information within every modality and flexibly adapts to the modalities of different importance. Encouraging experimental results demonstrate the superiority of our method over other state-of-the-art methods.



### An implementation of ROS Autonomous Navigation on Parallax Eddie platform
- **Arxiv ID**: http://arxiv.org/abs/2108.12571v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12571v1)
- **Published**: 2021-08-28 04:57:49+00:00
- **Updated**: 2021-08-28 04:57:49+00:00
- **Authors**: Hafiq Anas, Wee Hong Ong
- **Comment**: 12 pages, 23 figures, 9 tables, 24 equations
- **Journal**: None
- **Summary**: This paper presents an implementation of autonomous navigation functionality based on Robot Operating System (ROS) on a wheeled differential drive mobile platform called Eddie robot. ROS is a framework that contains many reusable software stacks as well as visualization and debugging tools that provides an ideal environment for any robotic project development. The main contribution of this paper is the description of the customized hardware and software system setup of Eddie robot to work with an autonomous navigation system in ROS called Navigation Stack and to implement one application use case for autonomous navigation. For this paper, photo taking is chosen to demonstrate a use case of the mobile robot.



### Goal-driven text descriptions for images
- **Arxiv ID**: http://arxiv.org/abs/2108.12575v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2108.12575v1)
- **Published**: 2021-08-28 05:10:38+00:00
- **Updated**: 2021-08-28 05:10:38+00:00
- **Authors**: Ruotian Luo
- **Comment**: Ph.D. thesis
- **Journal**: None
- **Summary**: A big part of achieving Artificial General Intelligence(AGI) is to build a machine that can see and listen like humans. Much work has focused on designing models for image classification, video classification, object detection, pose estimation, speech recognition, etc., and has achieved significant progress in recent years thanks to deep learning. However, understanding the world is not enough. An AI agent also needs to know how to talk, especially how to communicate with a human. While perception (vision, for example) is more common across animal species, the use of complicated language is unique to humans and is one of the most important aspects of intelligence.   In this thesis, we focus on generating textual output given visual input. In Chapter 3, we focus on generating the referring expression, a text description for an object in the image so that a receiver can infer which object is being described. We use a comprehension machine to directly guide the generated referring expressions to be more discriminative. In Chapter 4, we introduce a method that encourages discriminability in image caption generation. We show that more discriminative captioning models generate more descriptive captions. In Chapter 5, we study how training objectives and sampling methods affect the models' ability to generate diverse captions. We find that a popular captioning training strategy will be detrimental to the diversity of generated captions. In Chapter 6, we propose a model that can control the length of generated captions. By changing the desired length, one can influence the style and descriptiveness of the captions. Finally, in Chapter 7, we rank/generate informative image tags according to their information utility. The proposed method better matches what humans think are the most important tags for the images.



### On the Significance of Question Encoder Sequence Model in the Out-of-Distribution Performance in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2108.12585v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12585v2)
- **Published**: 2021-08-28 05:51:27+00:00
- **Updated**: 2021-12-21 05:52:26+00:00
- **Authors**: Gouthaman KV, Anurag Mittal
- **Comment**: None
- **Journal**: None
- **Summary**: Generalizing beyond the experiences has a significant role in developing practical AI systems. It has been shown that current Visual Question Answering (VQA) models are over-dependent on the language-priors (spurious correlations between question-types and their most frequent answers) from the train set and pose poor performance on Out-of-Distribution (OOD) test sets. This conduct limits their generalizability and restricts them from being utilized in real-world situations. This paper shows that the sequence model architecture used in the question-encoder has a significant role in the generalizability of VQA models. To demonstrate this, we performed a detailed analysis of various existing RNN-based and Transformer-based question-encoders, and along, we proposed a novel Graph attention network (GAT)-based question-encoder. Our study found that a better choice of sequence model in the question-encoder improves the generalizability of VQA models even without using any additional relatively complex bias-mitigation approaches.



### Towards Fine-grained Image Classification with Generative Adversarial Networks and Facial Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.00891v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2109.00891v1)
- **Published**: 2021-08-28 06:32:42+00:00
- **Updated**: 2021-08-28 06:32:42+00:00
- **Authors**: Mahdi Darvish, Mahsa Pouramini, Hamid Bahador
- **Comment**: Submitted to International Conference on Machine Vision and Image
  Processing (MVIP), 2022
- **Journal**: None
- **Summary**: Fine-grained classification remains a challenging task because distinguishing categories needs learning complex and local differences. Diversity in the pose, scale, and position of objects in an image makes the problem even more difficult. Although the recent Vision Transformer models achieve high performance, they need an extensive volume of input data. To encounter this problem, we made the best use of GAN-based data augmentation to generate extra dataset instances. Oxford-IIIT Pets was our dataset of choice for this experiment. It consists of 37 breeds of cats and dogs with variations in scale, poses, and lighting, which intensifies the difficulty of the classification task. Furthermore, we enhanced the performance of the recent Generative Adversarial Network (GAN), StyleGAN2-ADA model to generate more realistic images while preventing overfitting to the training set. We did this by training a customized version of MobileNetV2 to predict animal facial landmarks; then, we cropped images accordingly. Lastly, we combined the synthetic images with the original dataset and compared our proposed method with standard GANs augmentation and no augmentation with different subsets of training data. We validated our work by evaluating the accuracy of fine-grained image classification on the recent Vision Transformer (ViT) Model.



### New Pruning Method Based on DenseNet Network for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2108.12604v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12604v4)
- **Published**: 2021-08-28 08:48:31+00:00
- **Updated**: 2021-12-28 04:52:55+00:00
- **Authors**: Rui-Yang Ju, Ting-Yu Lin, Jen-Shiun Chiang
- **Comment**: 5 pages, 3 figures, TAAI 2021
- **Journal**: None
- **Summary**: Deep neural networks have made significant progress in the field of computer vision. Recent studies have shown that depth, width and shortcut connections of neural network architectures play a crucial role in their performance. One of the most advanced neural network architectures, DenseNet, has achieved excellent convergence rates through dense connections. However, it still has obvious shortcomings in the usage of amount of memory. In this paper, we introduce a new type of pruning tool, threshold, which refers to the principle of the threshold voltage in MOSFET. This work employs this method to connect blocks of different depths in different ways to reduce the usage of memory. It is denoted as ThresholdNet. We evaluate ThresholdNet and other different networks on datasets of CIFAR10. Experiments show that HarDNet is twice as fast as DenseNet, and on this basis, ThresholdNet is 10% faster and 10% lower error rate than HarDNet.



### Stagewise Unsupervised Domain Adaptation with Adversarial Self-Training for Road Segmentation of Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2108.12611v1
- **DOI**: 10.1109/TGRS.2021.3104032
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12611v1)
- **Published**: 2021-08-28 09:29:14+00:00
- **Updated**: 2021-08-28 09:29:14+00:00
- **Authors**: Lefei Zhang, Meng Lan, Jing Zhang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Road segmentation from remote sensing images is a challenging task with wide ranges of application potentials. Deep neural networks have advanced this field by leveraging the power of large-scale labeled data, which, however, are extremely expensive and time-consuming to acquire. One solution is to use cheap available data to train a model and deploy it to directly process the data from a specific application domain. Nevertheless, the well-known domain shift (DS) issue prevents the trained model from generalizing well on the target domain. In this paper, we propose a novel stagewise domain adaptation model called RoadDA to address the DS issue in this field. In the first stage, RoadDA adapts the target domain features to align with the source ones via generative adversarial networks (GAN) based inter-domain adaptation. Specifically, a feature pyramid fusion module is devised to avoid information loss of long and thin roads and learn discriminative and robust features. Besides, to address the intra-domain discrepancy in the target domain, in the second stage, we propose an adversarial self-training method. We generate the pseudo labels of target domain using the trained generator and divide it to labeled easy split and unlabeled hard split based on the road confidence scores. The features of hard split are adapted to align with the easy ones using adversarial learning and the intra-domain adaptation process is repeated to progressively improve the segmentation performance. Experiment results on two benchmarks demonstrate that RoadDA can efficiently reduce the domain gap and outperforms state-of-the-art methods.



### Uncertainty-Aware Model Adaptation for Unsupervised Cross-Domain Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.12612v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12612v1)
- **Published**: 2021-08-28 09:37:18+00:00
- **Updated**: 2021-08-28 09:37:18+00:00
- **Authors**: Minjie Cai, Minyi Luo, Xionghu Zhong, Hao Chen
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: This work tackles the unsupervised cross-domain object detection problem which aims to generalize a pre-trained object detector to a new target domain without labels. We propose an uncertainty-aware model adaptation method, which is based on two motivations: 1) the estimation and exploitation of model uncertainty in a new domain is critical for reliable domain adaptation; and 2) the joint alignment of distributions for inputs (feature alignment) and outputs (self-training) is needed. To this end, we compose a Bayesian CNN-based framework for uncertainty estimation in object detection, and propose an algorithm for generation of uncertainty-aware pseudo-labels. We also devise a scheme for joint feature alignment and self-training of the object detection model with uncertainty-aware pseudo-labels. Experiments on multiple cross-domain object detection benchmarks show that our proposed method achieves state-of-the-art performance.



### AP-10K: A Benchmark for Animal Pose Estimation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2108.12617v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12617v2)
- **Published**: 2021-08-28 10:23:34+00:00
- **Updated**: 2021-11-01 05:36:12+00:00
- **Authors**: Hang Yu, Yufei Xu, Jing Zhang, Wei Zhao, Ziyu Guan, Dacheng Tao
- **Comment**: Accepted to NeurIPS 2021 Datasets and Benchmarks Track
- **Journal**: None
- **Summary**: Accurate animal pose estimation is an essential step towards understanding animal behavior, and can potentially benefit many downstream applications, such as wildlife conservation. Previous works only focus on specific animals while ignoring the diversity of animal species, limiting the generalization ability. In this paper, we propose AP-10K, the first large-scale benchmark for mammal animal pose estimation, to facilitate the research in animal pose estimation. AP-10K consists of 10,015 images collected and filtered from 23 animal families and 54 species following the taxonomic rank and high-quality keypoint annotations labeled and checked manually. Based on AP-10K, we benchmark representative pose estimation models on the following three tracks: (1) supervised learning for animal pose estimation, (2) cross-domain transfer learning from human pose estimation to animal pose estimation, and (3) intra- and inter-family domain generalization for unseen animals. The experimental results provide sound empirical evidence on the superiority of learning from diverse animals species in terms of both accuracy and generalization ability. It opens new directions for facilitating future research in animal pose estimation. AP-10k is publicly available at https://github.com/AlexTheBad/AP10K.



### GroupFormer: Group Activity Recognition with Clustered Spatial-Temporal Transformer
- **Arxiv ID**: http://arxiv.org/abs/2108.12630v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12630v1)
- **Published**: 2021-08-28 11:24:36+00:00
- **Updated**: 2021-08-28 11:24:36+00:00
- **Authors**: Shuaicheng Li, Qianggang Cao, Lingbo Liu, Kunlin Yang, Shinan Liu, Jun Hou, Shuai Yi
- **Comment**: Accepted at ICCV2021
- **Journal**: None
- **Summary**: Group activity recognition is a crucial yet challenging problem, whose core lies in fully exploring spatial-temporal interactions among individuals and generating reasonable group representations. However, previous methods either model spatial and temporal information separately, or directly aggregate individual features to form group features. To address these issues, we propose a novel group activity recognition network termed GroupFormer. It captures spatial-temporal contextual information jointly to augment the individual and group representations effectively with a clustered spatial-temporal transformer. Specifically, our GroupFormer has three appealing advantages: (1) A tailor-modified Transformer, Clustered Spatial-Temporal Transformer, is proposed to enhance the individual representation and group representation. (2) It models the spatial and temporal dependencies integrally and utilizes decoders to build the bridge between the spatial and temporal information. (3) A clustered attention mechanism is utilized to dynamically divide individuals into multiple clusters for better learning activity-aware semantic representations. Moreover, experimental results show that the proposed framework outperforms state-of-the-art methods on the Volleyball dataset and Collective Activity dataset. Code is available at https://github.com/xueyee/GroupFormer.



### Self-supervised Neural Networks for Spectral Snapshot Compressive Imaging
- **Arxiv ID**: http://arxiv.org/abs/2108.12654v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12654v1)
- **Published**: 2021-08-28 14:17:38+00:00
- **Updated**: 2021-08-28 14:17:38+00:00
- **Authors**: Ziyi Meng, Zhenming Yu, Kun Xu, Xin Yuan
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: We consider using {\bf\em untrained neural networks} to solve the reconstruction problem of snapshot compressive imaging (SCI), which uses a two-dimensional (2D) detector to capture a high-dimensional (usually 3D) data-cube in a compressed manner. Various SCI systems have been built in recent years to capture data such as high-speed videos, hyperspectral images, and the state-of-the-art reconstruction is obtained by the deep neural networks. However, most of these networks are trained in an end-to-end manner by a large amount of corpus with sometimes simulated ground truth, measurement pairs. In this paper, inspired by the untrained neural networks such as deep image priors (DIP) and deep decoders, we develop a framework by integrating DIP into the plug-and-play regime, leading to a self-supervised network for spectral SCI reconstruction. Extensive synthetic and real data results show that the proposed algorithm without training is capable of achieving competitive results to the training based networks. Furthermore, by integrating the proposed method with a pre-trained deep denoising prior, we have achieved state-of-the-art results. {Our code is available at \url{https://github.com/mengziyi64/CASSI-Self-Supervised}.}



### DenseLiDAR: A Real-Time Pseudo Dense Depth Guided Depth Completion Network
- **Arxiv ID**: http://arxiv.org/abs/2108.12655v1
- **DOI**: 10.1109/LRA.2021.3060396
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12655v1)
- **Published**: 2021-08-28 14:18:29+00:00
- **Updated**: 2021-08-28 14:18:29+00:00
- **Authors**: Jiaqi Gu, Zhiyu Xiang, Yuwen Ye, Lingxuan Wang
- **Comment**: 9 pages, 7 figures, published in IEEE Robotics and Automation Letters
  and Accepted by ICRA2021
- **Journal**: IEEE Robotics and Automation Letters ( Volume: 6, Issue: 2, April
  2021)
- **Summary**: Depth Completion can produce a dense depth map from a sparse input and provide a more complete 3D description of the environment. Despite great progress made in depth completion, the sparsity of the input and low density of the ground truth still make this problem challenging. In this work, we propose DenseLiDAR, a novel real-time pseudo-depth guided depth completion neural network. We exploit dense pseudo-depth map obtained from simple morphological operations to guide the network in three aspects: (1) Constructing a residual structure for the output; (2) Rectifying the sparse input data; (3) Providing dense structural loss for training the network. Thanks to these novel designs, higher performance of the output could be achieved. In addition, two new metrics for better evaluating the quality of the predicted depth map are also presented. Extensive experiments on KITTI depth completion benchmark suggest that our model is able to achieve the state-of-the-art performance at the highest frame rate of 50Hz. The predicted dense depth is further evaluated by several downstream robotic perception or positioning tasks. For the task of 3D object detection, 3~5 percent performance gains on small objects categories are achieved on KITTI 3D object detection dataset. For RGB-D SLAM, higher accuracy on vehicle's trajectory is also obtained in KITTI Odometry dataset. These promising results not only verify the high quality of our depth prediction, but also demonstrate the potential of improving the related downstream tasks by using depth completion results.



### DKM: Differentiable K-Means Clustering Layer for Neural Network Compression
- **Arxiv ID**: http://arxiv.org/abs/2108.12659v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12659v4)
- **Published**: 2021-08-28 14:35:41+00:00
- **Updated**: 2022-02-20 16:47:22+00:00
- **Authors**: Minsik Cho, Keivan A. Vahid, Saurabh Adya, Mohammad Rastegari
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: Deep neural network (DNN) model compression for efficient on-device inference is becoming increasingly important to reduce memory requirements and keep user data on-device. To this end, we propose a novel differentiable k-means clustering layer (DKM) and its application to train-time weight clustering-based DNN model compression. DKM casts k-means clustering as an attention problem and enables joint optimization of the DNN parameters and clustering centroids. Unlike prior works that rely on additional regularizers and parameters, DKM-based compression keeps the original loss function and model architecture fixed. We evaluated DKM-based compression on various DNN models for computer vision and natural language processing (NLP) tasks. Our results demonstrate that DKM delivers superior compression and accuracy trade-off on ImageNet1k and GLUE benchmarks. For example, DKM-based compression can offer 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with 3.3MB model size (29.4x model compression factor). For MobileNet-v1, which is a challenging DNN to compress, DKM delivers 63.9% top-1 ImageNet1k accuracy with 0.72 MB model size (22.4x model compression factor). This result is 6.8% higher top-1accuracy and 33% relatively smaller model size than the current state-of-the-art DNN compression algorithms. Additionally, DKM enables compression of DistilBERT model by 11.8x with minimal (1.1%) accuracy loss on GLUE NLP benchmarks.



### Learning to Track Objects from Unlabeled Videos
- **Arxiv ID**: http://arxiv.org/abs/2108.12711v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12711v1)
- **Published**: 2021-08-28 22:10:06+00:00
- **Updated**: 2021-08-28 22:10:06+00:00
- **Authors**: Jilai Zheng, Chao Ma, Houwen Peng, Xiaokang Yang
- **Comment**: Accpeted to ICCV2021
- **Journal**: None
- **Summary**: In this paper, we propose to learn an Unsupervised Single Object Tracker (USOT) from scratch. We identify that three major challenges, i.e., moving object discovery, rich temporal variation exploitation, and online update, are the central causes of the performance bottleneck of existing unsupervised trackers. To narrow the gap between unsupervised trackers and supervised counterparts, we propose an effective unsupervised learning approach composed of three stages. First, we sample sequentially moving objects with unsupervised optical flow and dynamic programming, instead of random cropping. Second, we train a naive Siamese tracker from scratch using single-frame pairs. Third, we continue training the tracker with a novel cycle memory learning scheme, which is conducted in longer temporal spans and also enables our tracker to update online. Extensive experiments show that the proposed USOT learned from unlabeled videos performs well over the state-of-the-art unsupervised trackers by large margins, and on par with recent supervised deep trackers. Code is available at https://github.com/VISION-SJTU/USOT.



### DeepFake Detection with Inconsistent Head Poses: Reproducibility and Analysis
- **Arxiv ID**: http://arxiv.org/abs/2108.12715v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12715v1)
- **Published**: 2021-08-28 22:56:09+00:00
- **Updated**: 2021-08-28 22:56:09+00:00
- **Authors**: Kevin Lutz, Robert Bassett
- **Comment**: None
- **Journal**: None
- **Summary**: Applications of deep learning to synthetic media generation allow the creation of convincing forgeries, called DeepFakes, with limited technical expertise. DeepFake detection is an increasingly active research area. In this paper, we analyze an existing DeepFake detection technique based on head pose estimation, which can be applied when fake images are generated with an autoencoder-based face swap. Existing literature suggests that this method is an effective DeepFake detector, and its motivating principles are attractively simple. With an eye towards using these principles to develop new DeepFake detectors, we conduct a reproducibility study of the existing method. We conclude that its merits are dramatically overstated, despite its celebrated status. By investigating this discrepancy we uncover a number of important and generalizable insights related to facial landmark detection, identity-agnostic head pose estimation, and algorithmic bias in DeepFake detectors. Our results correct the current literature's perception of state of the art performance for DeepFake detection.



### A Dual Adversarial Calibration Framework for Automatic Fetal Brain Biometry
- **Arxiv ID**: http://arxiv.org/abs/2108.12719v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.12719v1)
- **Published**: 2021-08-28 23:25:59+00:00
- **Updated**: 2021-08-28 23:25:59+00:00
- **Authors**: Yuan Gao, Lok Hin Lee, Richard Droste, Rachel Craik, Sridevi Beriwal, Aris Papageorghiou, Alison Noble
- **Comment**: CVAMD ICCV 2021
- **Journal**: None
- **Summary**: This paper presents a novel approach to automatic fetal brain biometry motivated by needs in low- and medium- income countries. Specifically, we leverage high-end (HE) ultrasound images to build a biometry solution for low-cost (LC) point-of-care ultrasound images. We propose a novel unsupervised domain adaptation approach to train deep models to be invariant to significant image distribution shift between the image types. Our proposed method, which employs a Dual Adversarial Calibration (DAC) framework, consists of adversarial pathways which enforce model invariance to; i) adversarial perturbations in the feature space derived from LC images, and ii) appearance domain discrepancy. Our Dual Adversarial Calibration method estimates transcerebellar diameter and head circumference on images from low-cost ultrasound devices with a mean absolute error (MAE) of 2.43mm and 1.65mm, compared with 7.28 mm and 5.65 mm respectively for SOTA.



