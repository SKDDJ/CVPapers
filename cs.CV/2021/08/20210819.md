# Arxiv Papers in cs.CV on 2021-08-19
### D3D-HOI: Dynamic 3D Human-Object Interactions from Videos
- **Arxiv ID**: http://arxiv.org/abs/2108.08420v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08420v1)
- **Published**: 2021-08-19 00:49:01+00:00
- **Updated**: 2021-08-19 00:49:01+00:00
- **Authors**: Xiang Xu, Hanbyul Joo, Greg Mori, Manolis Savva
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce D3D-HOI: a dataset of monocular videos with ground truth annotations of 3D object pose, shape and part motion during human-object interactions. Our dataset consists of several common articulated objects captured from diverse real-world scenes and camera viewpoints. Each manipulated object (e.g., microwave oven) is represented with a matching 3D parametric model. This data allows us to evaluate the reconstruction quality of articulated objects and establish a benchmark for this challenging task. In particular, we leverage the estimated 3D human pose for more accurate inference of the object spatial layout and dynamics. We evaluate this approach on our dataset, demonstrating that human-object relations can significantly reduce the ambiguity of articulated object reconstructions from challenging real-world videos. Code and dataset are available at https://github.com/facebookresearch/d3d-hoi.



### Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes
- **Arxiv ID**: http://arxiv.org/abs/2108.08421v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08421v1)
- **Published**: 2021-08-19 00:52:10+00:00
- **Updated**: 2021-08-19 00:52:10+00:00
- **Authors**: Mingjun Yin, Shasha Li, Zikui Cai, Chengyu Song, M. Salman Asif, Amit K. Roy-Chowdhury, Srikanth V. Krishnamurthy
- **Comment**: ICCV'21 Accepted
- **Journal**: None
- **Summary**: Vision systems that deploy Deep Neural Networks (DNNs) are known to be vulnerable to adversarial examples. Recent research has shown that checking the intrinsic consistencies in the input data is a promising way to detect adversarial attacks (e.g., by checking the object co-occurrence relationships in complex scenes). However, existing approaches are tied to specific models and do not offer generalizability. Motivated by the observation that language descriptions of natural scene images have already captured the object co-occurrence relationships that can be learned by a language model, we develop a novel approach to perform context consistency checks using such language models. The distinguishing aspect of our approach is that it is independent of the deployed object detector and yet offers very high accuracy in terms of detecting adversarial examples in practical scenes with multiple objects.



### Generating Smooth Pose Sequences for Diverse Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2108.08422v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08422v3)
- **Published**: 2021-08-19 00:58:00+00:00
- **Updated**: 2022-01-13 06:55:24+00:00
- **Authors**: Wei Mao, Miaomiao Liu, Mathieu Salzmann
- **Comment**: ICCV21(oral) (v3: update the threshold for pesudo GT in supplementary
  material)
- **Journal**: None
- **Summary**: Recent progress in stochastic motion prediction, i.e., predicting multiple possible future human motions given a single past pose sequence, has led to producing truly diverse future motions and even providing control over the motion of some body parts. However, to achieve this, the state-of-the-art method requires learning several mappings for diversity and a dedicated model for controllable motion prediction. In this paper, we introduce a unified deep generative network for both diverse and controllable motion prediction. To this end, we leverage the intuition that realistic human motions consist of smooth sequences of valid poses, and that, given limited data, learning a pose prior is much more tractable than a motion one. We therefore design a generator that predicts the motion of different body parts sequentially, and introduce a normalizing flow based pose prior, together with a joint angle loss, to achieve motion realism.Our experiments on two standard benchmark datasets, Human3.6M and HumanEva-I, demonstrate that our approach outperforms the state-of-the-art baselines in terms of both sample diversity and accuracy. The code is available at https://github.com/wei-mao-2019/gsps



### Self-Supervised Video Representation Learning with Meta-Contrastive Network
- **Arxiv ID**: http://arxiv.org/abs/2108.08426v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08426v2)
- **Published**: 2021-08-19 01:21:13+00:00
- **Updated**: 2021-08-23 16:41:06+00:00
- **Authors**: Yuanze Lin, Xun Guo, Yan Lu
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Self-supervised learning has been successfully applied to pre-train video representations, which aims at efficient adaptation from pre-training domain to downstream tasks. Existing approaches merely leverage contrastive loss to learn instance-level discrimination. However, lack of category information will lead to hard-positive problem that constrains the generalization ability of this kind of methods. We find that the multi-task process of meta learning can provide a solution to this problem. In this paper, we propose a Meta-Contrastive Network (MCN), which combines the contrastive learning and meta learning, to enhance the learning ability of existing self-supervised approaches. Our method contains two training stages based on model-agnostic meta learning (MAML), each of which consists of a contrastive branch and a meta branch. Extensive evaluations demonstrate the effectiveness of our method. For two downstream tasks, i.e., video action recognition and video retrieval, MCN outperforms state-of-the-art approaches on UCF101 and HMDB51 datasets. To be more specific, with R(2+1)D backbone, MCN achieves Top-1 accuracies of 84.8% and 54.5% for video action recognition, as well as 52.5% and 23.7% for video retrieval.



### Box-Adapt: Domain-Adaptive Medical Image Segmentation using Bounding BoxSupervision
- **Arxiv ID**: http://arxiv.org/abs/2108.08432v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08432v2)
- **Published**: 2021-08-19 01:51:04+00:00
- **Updated**: 2021-08-28 03:53:55+00:00
- **Authors**: Yanwu Xu, Mingming Gong, Shaoan Xie, Kayhan Batmanghelich
- **Comment**: None
- **Journal**: IJCAI Workshop on Weakly Supervised Representation Learning, 2021
- **Summary**: Deep learning has achieved remarkable success in medicalimage segmentation, but it usually requires a large numberof images labeled with fine-grained segmentation masks, andthe annotation of these masks can be very expensive andtime-consuming. Therefore, recent methods try to use un-supervised domain adaptation (UDA) methods to borrow in-formation from labeled data from other datasets (source do-mains) to a new dataset (target domain). However, due tothe absence of labels in the target domain, the performance ofUDA methods is much worse than that of the fully supervisedmethod. In this paper, we propose a weakly supervised do-main adaptation setting, in which we can partially label newdatasets with bounding boxes, which are easier and cheaperto obtain than segmentation masks. Accordingly, we proposea new weakly-supervised domain adaptation method calledBox-Adapt, which fully explores the fine-grained segmenta-tion mask in the source domain and the weak bounding boxin the target domain. Our Box-Adapt is a two-stage methodthat first performs joint training on the source and target do-mains, and then conducts self-training with the pseudo-labelsof the target domain. We demonstrate the effectiveness of ourmethod in the liver segmentation task. Weakly supervised do-main adaptation



### Semantic Reinforced Attention Learning for Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.08443v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.08443v1)
- **Published**: 2021-08-19 02:14:36+00:00
- **Updated**: 2021-08-19 02:14:36+00:00
- **Authors**: Guohao Peng, Yufeng Yue, Jun Zhang, Zhenyu Wu, Xiaoyu Tang, Danwei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale visual place recognition (VPR) is inherently challenging because not all visual cues in the image are beneficial to the task. In order to highlight the task-relevant visual cues in the feature embedding, the existing attention mechanisms are either based on artificial rules or trained in a thorough data-driven manner. To fill the gap between the two types, we propose a novel Semantic Reinforced Attention Learning Network (SRALNet), in which the inferred attention can benefit from both semantic priors and data-driven fine-tuning. The contribution lies in two-folds. (1) To suppress misleading local features, an interpretable local weighting scheme is proposed based on hierarchical feature distribution. (2) By exploiting the interpretability of the local weighting scheme, a semantic constrained initialization is proposed so that the local attention can be reinforced by semantic priors. Experiments demonstrate that our method outperforms state-of-the-art techniques on city-scale VPR benchmark datasets.



### Medical Image Segmentation with 3D Convolutional Neural Networks: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2108.08467v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T07 (Primary) 68T45 (Secondary), I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2108.08467v3)
- **Published**: 2021-08-19 03:23:08+00:00
- **Updated**: 2022-04-28 11:28:27+00:00
- **Authors**: S Niyas, S J Pawan, M Anand Kumar, Jeny Rajan
- **Comment**: 17 pages, 4 figures
- **Journal**: None
- **Summary**: Computer-aided medical image analysis plays a significant role in assisting medical practitioners for expert clinical diagnosis and deciding the optimal treatment plan. At present, convolutional neural networks (CNN) are the preferred choice for medical image analysis. In addition, with the rapid advancements in three-dimensional (3D) imaging systems and the availability of excellent hardware and software support to process large volumes of data, 3D deep learning methods are gaining popularity in medical image analysis. Here, we present an extensive review of the recently evolved 3D deep learning methods in medical image segmentation. Furthermore, the research gaps and future directions in 3D medical image segmentation are discussed.



### Classification of Diabetic Retinopathy Severity in Fundus Images with DenseNet121 and ResNet50
- **Arxiv ID**: http://arxiv.org/abs/2108.08473v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08473v1)
- **Published**: 2021-08-19 03:28:45+00:00
- **Updated**: 2021-08-19 03:28:45+00:00
- **Authors**: Jonathan Zhang, Bowen Xie, Xin Wu, Rahul Ram, David Liang
- **Comment**: 15 pages, 14 figures; Jonathan Zhang - first author, Rahul Ram and
  David Liang - principal investigators; classifier repository -
  $\url{https://github.com/JZhang-305/Diabetic-Retinopathy-Classifier}$
- **Journal**: None
- **Summary**: In this work, deep learning algorithms are used to classify fundus images in terms of diabetic retinopathy severity. Six different combinations of two model architectures, the Dense Convolutional Network-121 and the Residual Neural Network-50 and three image types, RGB, Green, and High Contrast, were tested to find the highest performing combination. We achieved an average validation loss of 0.17 and a max validation accuracy of 85 percent. By testing out multiple combinations, certain combinations of parameters performed better than others, though minimal variance was found overall. Green filtration was shown to perform the poorest, while amplified contrast appeared to have a negligible effect in comparison to RGB analysis. ResNet50 proved to be less of a robust model as opposed to DenseNet121.



### Image2Lego: Customized LEGO Set Generation from Images
- **Arxiv ID**: http://arxiv.org/abs/2108.08477v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08477v1)
- **Published**: 2021-08-19 03:42:58+00:00
- **Updated**: 2021-08-19 03:42:58+00:00
- **Authors**: Kyle Lennon, Katharina Fransen, Alexander O'Brien, Yumeng Cao, Matthew Beveridge, Yamin Arefeen, Nikhil Singh, Iddo Drori
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: Although LEGO sets have entertained generations of children and adults, the challenge of designing customized builds matching the complexity of real-world or imagined scenes remains too great for the average enthusiast. In order to make this feat possible, we implement a system that generates a LEGO brick model from 2D images. We design a novel solution to this problem that uses an octree-structured autoencoder trained on 3D voxelized models to obtain a feasible latent representation for model reconstruction, and a separate network trained to predict this latent representation from 2D images. LEGO models are obtained by algorithmic conversion of the 3D voxelized model to bricks. We demonstrate first-of-its-kind conversion of photographs to 3D LEGO models. An octree architecture enables the flexibility to produce multiple resolutions to best fit a user's creative vision or design needs. In order to demonstrate the broad applicability of our system, we generate step-by-step building instructions and animations for LEGO models of objects and human faces. Finally, we test these automatically generated LEGO sets by constructing physical builds using real LEGO bricks.



### Learning Anchored Unsigned Distance Functions with Gradient Direction Alignment for Single-view Garment Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2108.08478v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08478v2)
- **Published**: 2021-08-19 03:45:38+00:00
- **Updated**: 2021-10-24 15:17:39+00:00
- **Authors**: Fang Zhao, Wenhao Wang, Shengcai Liao, Ling Shao
- **Comment**: ICCV 2021 (Oral). Code is available at
  https://github.com/zhaofang0627/AnchorUDF
- **Journal**: None
- **Summary**: While single-view 3D reconstruction has made significant progress benefiting from deep shape representations in recent years, garment reconstruction is still not solved well due to open surfaces, diverse topologies and complex geometric details. In this paper, we propose a novel learnable Anchored Unsigned Distance Function (AnchorUDF) representation for 3D garment reconstruction from a single image. AnchorUDF represents 3D shapes by predicting unsigned distance fields (UDFs) to enable open garment surface modeling at arbitrary resolution. To capture diverse garment topologies, AnchorUDF not only computes pixel-aligned local image features of query points, but also leverages a set of anchor points located around the surface to enrich 3D position features for query points, which provides stronger 3D space context for the distance function. Furthermore, in order to obtain more accurate point projection direction at inference, we explicitly align the spatial gradient direction of AnchorUDF with the ground-truth direction to the surface during training. Extensive experiments on two public 3D garment datasets, i.e., MGN and Deep Fashion3D, demonstrate that AnchorUDF achieves the state-of-the-art performance on single-view garment reconstruction.



### VIL-100: A New Dataset and A Baseline Model for Video Instance Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.08482v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08482v1)
- **Published**: 2021-08-19 03:57:39+00:00
- **Updated**: 2021-08-19 03:57:39+00:00
- **Authors**: Yujun Zhang, Lei Zhu, Wei Feng, Huazhu Fu, Mingqian Wang, Qingxia Li, Cheng Li, Song Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Lane detection plays a key role in autonomous driving. While car cameras always take streaming videos on the way, current lane detection works mainly focus on individual images (frames) by ignoring dynamics along the video. In this work, we collect a new video instance lane detection (VIL-100) dataset, which contains 100 videos with in total 10,000 frames, acquired from different real traffic scenarios. All the frames in each video are manually annotated to a high-quality instance-level lane annotation, and a set of frame-level and video-level metrics are included for quantitative performance evaluation. Moreover, we propose a new baseline model, named multi-level memory aggregation network (MMA-Net), for video instance lane detection. In our approach, the representation of current frame is enhanced by attentively aggregating both local and global memory features from other frames. Experiments on the new collected dataset show that the proposed MMA-Net outperforms state-of-the-art lane detection methods and video object segmentation methods. We release our dataset and code at https://github.com/yujun0-0/MMA-Net.



### Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain
- **Arxiv ID**: http://arxiv.org/abs/2108.08487v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08487v1)
- **Published**: 2021-08-19 04:04:41+00:00
- **Updated**: 2021-08-19 04:04:41+00:00
- **Authors**: Guangyao Chen, Peixi Peng, Li Ma, Jia Li, Lin Du, Yonghong Tian
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Recently, the generalization behavior of Convolutional Neural Networks (CNN) is gradually transparent through explanation techniques with the frequency components decomposition. However, the importance of the phase spectrum of the image for a robust vision system is still ignored. In this paper, we notice that the CNN tends to converge at the local optimum which is closely related to the high-frequency components of the training images, while the amplitude spectrum is easily disturbed such as noises or common corruptions. In contrast, more empirical studies found that humans rely on more phase components to achieve robust recognition. This observation leads to more explanations of the CNN's generalization behaviors in both robustness to common perturbations and out-of-distribution detection, and motivates a new perspective on data augmentation designed by re-combing the phase spectrum of the current image and the amplitude spectrum of the distracter image. That is, the generated samples force the CNN to pay more attention to the structured information from phase components and keep robust to the variation of the amplitude. Experiments on several image datasets indicate that the proposed method achieves state-of-the-art performances on multiple generalizations and calibration tasks, including adaptability for common corruptions and surface variations, out-of-distribution detection, and adversarial attack.



### Generative Wind Power Curve Modeling Via Machine Vision: A Self-learning Deep Convolutional Network Based Method
- **Arxiv ID**: http://arxiv.org/abs/2109.00894v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00894v2)
- **Published**: 2021-08-19 05:18:22+00:00
- **Updated**: 2022-05-02 09:29:57+00:00
- **Authors**: Luoxiao Yang, Long Wang, Zijun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper develops a novel self-training U-net (STU-net) based method for the automated WPC model generation without requiring data pre-processing. The self-training (ST) process of STU-net has two steps. First, different from traditional studies regarding the WPC modeling as a curve fitting problem, in this paper, we renovate the WPC modeling formulation from a machine vision aspect. To develop sufficiently diversified training samples, we synthesize supervisory control and data acquisition (SCADA) data based on a set of S-shape functions depicting WPCs. These synthesized SCADA data and WPC functions are visualized as images and paired as training samples(I_x, I_wpc). A U-net is then developed to approximate the model recovering I_wpc from I_x. The developed U-net is applied into observed SCADA data and can successfully generate the I_wpc. Moreover, we develop a pixel mapping and correction process to derive a mathematical form f_wpc representing I_wpcgenerated previously. The proposed STU-net only needs to train once and does not require any data preprocessing in applications. Numerical experiments based on 76 WTs are conducted to validate the superiority of the proposed method by benchmarking against classical WPC modeling methods. To demonstrate the repeatability of the presented research, we release our code at https://github.com/IkeYang/STU-net.



### Understanding and Mitigating Annotation Bias in Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.08504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.08504v1)
- **Published**: 2021-08-19 05:28:07+00:00
- **Updated**: 2021-08-19 05:28:07+00:00
- **Authors**: Yunliang Chen, Jungseock Joo
- **Comment**: To appear in ICCV 2021
- **Journal**: None
- **Summary**: The performance of a computer vision model depends on the size and quality of its training data. Recent studies have unveiled previously-unknown composition biases in common image datasets which then lead to skewed model outputs, and have proposed methods to mitigate these biases. However, most existing works assume that human-generated annotations can be considered gold-standard and unbiased. In this paper, we reveal that this assumption can be problematic, and that special care should be taken to prevent models from learning such annotation biases. We focus on facial expression recognition and compare the label biases between lab-controlled and in-the-wild datasets. We demonstrate that many expression datasets contain significant annotation biases between genders, especially when it comes to the happy and angry expressions, and that traditional methods cannot fully mitigate such biases in trained models. To remove expression annotation bias, we propose an AU-Calibrated Facial Expression Recognition (AUC-FER) framework that utilizes facial action units (AUs) and incorporates the triplet loss into the objective function. Experimental results suggest that the proposed method is more effective in removing expression annotation bias than existing techniques.



### Blindly Assess Quality of In-the-Wild Videos via Quality-aware Pre-training and Motion Perception
- **Arxiv ID**: http://arxiv.org/abs/2108.08505v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08505v2)
- **Published**: 2021-08-19 05:29:19+00:00
- **Updated**: 2022-04-05 15:19:28+00:00
- **Authors**: Bowen Li, Weixia Zhang, Meng Tian, Guangtao Zhai, Xianpei Wang
- **Comment**: Accepted to IEEE TCSVT
- **Journal**: None
- **Summary**: Perceptual quality assessment of the videos acquired in the wilds is of vital importance for quality assurance of video services. The inaccessibility of reference videos with pristine quality and the complexity of authentic distortions pose great challenges for this kind of blind video quality assessment (BVQA) task. Although model-based transfer learning is an effective and efficient paradigm for the BVQA task, it remains to be a challenge to explore what and how to bridge the domain shifts for better video representation. In this work, we propose to transfer knowledge from image quality assessment (IQA) databases with authentic distortions and large-scale action recognition with rich motion patterns. We rely on both groups of data to learn the feature extractor. We train the proposed model on the target VQA databases using a mixed list-wise ranking loss function. Extensive experiments on six databases demonstrate that our method performs very competitively under both individual database and mixed database training settings. We also verify the rationality of each component of the proposed method and explore a simple manner for further improvement.



### Patch-Based Cervical Cancer Segmentation using Distance from Boundary of Tissue
- **Arxiv ID**: http://arxiv.org/abs/2108.08508v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08508v1)
- **Published**: 2021-08-19 05:41:18+00:00
- **Updated**: 2021-08-19 05:41:18+00:00
- **Authors**: Kengo Araki, Mariyo Rokutan-Kurata, Kazuhiro Terada, Akihiko Yoshizawa, Ryoma Bise
- **Comment**: 4 pages, 6 figures, EMBC2021
- **Journal**: None
- **Summary**: Pathological diagnosis is used for examining cancer in detail, and its automation is in demand. To automatically segment each cancer area, a patch-based approach is usually used since a Whole Slide Image (WSI) is huge. However, this approach loses the global information needed to distinguish between classes. In this paper, we utilized the Distance from the Boundary of tissue (DfB), which is global information that can be extracted from the original image. We experimentally applied our method to the three-class classification of cervical cancer, and found that it improved the total performance compared with the conventional method.



### Retrieval and Localization with Observation Constraints
- **Arxiv ID**: http://arxiv.org/abs/2108.08516v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.08516v1)
- **Published**: 2021-08-19 06:14:33+00:00
- **Updated**: 2021-08-19 06:14:33+00:00
- **Authors**: Yuhao Zhou, Huanhuan Fan, Shuang Gao, Yuchen Yang, Xudong Zhang, Jijunnan Li, Yandong Guo
- **Comment**: Accepted by the 2021 International Conference on Robotics and
  Automation (ICRA2021)
- **Journal**: None
- **Summary**: Accurate visual re-localization is very critical to many artificial intelligence applications, such as augmented reality, virtual reality, robotics and autonomous driving. To accomplish this task, we propose an integrated visual re-localization method called RLOCS by combining image retrieval, semantic consistency and geometry verification to achieve accurate estimations. The localization pipeline is designed as a coarse-to-fine paradigm. In the retrieval part, we cascade the architecture of ResNet101-GeM-ArcFace and employ DBSCAN followed by spatial verification to obtain a better initial coarse pose. We design a module called observation constraints, which combines geometry information and semantic consistency for filtering outliers. Comprehensive experiments are conducted on open datasets, including retrieval on R-Oxford5k and R-Paris6k, semantic segmentation on Cityscapes, localization on Aachen Day-Night and InLoc. By creatively modifying separate modules in the total pipeline, our method achieves many performance improvements on the challenging localization benchmarks.



### Few-shot Segmentation with Optimal Transport Matching and Message Flow
- **Arxiv ID**: http://arxiv.org/abs/2108.08518v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08518v2)
- **Published**: 2021-08-19 06:26:11+00:00
- **Updated**: 2022-06-29 15:32:04+00:00
- **Authors**: Weide Liu, Chi Zhang, Henghui Ding, Tzu-Yi Hung, Guosheng Lin
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the challenging task of few-shot segmentation in this work. It is essential for few-shot semantic segmentation to fully utilize the support information. Previous methods typically adopt masked average pooling over the support feature to extract the support clues as a global vector, usually dominated by the salient part and lost certain essential clues. In this work, we argue that every support pixel's information is desired to be transferred to all query pixels and propose a Correspondence Matching Network (CMNet) with an Optimal Transport Matching module to mine out the correspondence between the query and support images. Besides, it is critical to fully utilize both local and global information from the annotated support images. To this end, we propose a Message Flow module to propagate the message along the inner-flow inside the same image and cross-flow between support and query images, which greatly helps enhance the local feature representations. Experiments on PASCAL VOC 2012, MS COCO, and FSS-1000 datasets show that our network achieves new state-of-the-art few-shot segmentation performance.



### Inter-Species Cell Detection: Datasets on pulmonary hemosiderophages in equine, human and feline specimens
- **Arxiv ID**: http://arxiv.org/abs/2108.08529v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08529v1)
- **Published**: 2021-08-19 06:57:33+00:00
- **Updated**: 2021-08-19 06:57:33+00:00
- **Authors**: Christian Marzahl, Jenny Hill, Jason Stayt, Dorothee Bienzle, Lutz Welker, Frauke Wilm, Jörn Voigt, Marc Aubreville, Andreas Maier, Robert Klopfleisch, Katharina Breininger, Christof A. Bertram
- **Comment**: Submitted to SCIENTIFIC DATA
- **Journal**: None
- **Summary**: Pulmonary hemorrhage (P-Hem) occurs among multiple species and can have various causes. Cytology of bronchoalveolarlavage fluid (BALF) using a 5-tier scoring system of alveolar macrophages based on their hemosiderin content is considered the most sensitive diagnostic method. We introduce a novel, fully annotated multi-species P-Hem dataset which consists of 74 cytology whole slide images (WSIs) with equine, feline and human samples. To create this high-quality and high-quantity dataset, we developed an annotation pipeline combining human expertise with deep learning and data visualisation techniques. We applied a deep learning-based object detection approach trained on 17 expertly annotated equine WSIs, to the remaining 39 equine, 12 human and 7 feline WSIs. The resulting annotations were semi-automatically screened for errors on multiple types of specialised annotation maps and finally reviewed by a trained pathologists. Our dataset contains a total of 297,383 hemosiderophages classified into five grades. It is one of the largest publicly availableWSIs datasets with respect to the number of annotations, the scanned area and the number of species covered.



### An Information Theory-inspired Strategy for Automatic Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/2108.08532v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08532v3)
- **Published**: 2021-08-19 07:03:22+00:00
- **Updated**: 2021-12-07 11:58:50+00:00
- **Authors**: Xiawu Zheng, Yuexiao Ma, Teng Xi, Gang Zhang, Errui Ding, Yuchao Li, Jie Chen, Yonghong Tian, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Despite superior performance on many computer vision tasks, deep convolution neural networks are well known to be compressed on devices that have resource constraints. Most existing network pruning methods require laborious human efforts and prohibitive computation resources, especially when the constraints are changed. This practically limits the application of model compression when the model needs to be deployed on a wide range of devices. Besides, existing methods are still challenged by the missing theoretical guidance. In this paper we propose an information theory-inspired strategy for automatic model compression. The principle behind our method is the information bottleneck theory, i.e., the hidden representation should compress information with each other. We thus introduce the normalized Hilbert-Schmidt Independence Criterion (nHSIC) on network activations as a stable and generalized indicator of layer importance. When a certain resource constraint is given, we integrate the HSIC indicator with the constraint to transform the architecture search problem into a linear programming problem with quadratic constraints. Such a problem is easily solved by a convex optimization method with a few seconds. We also provide a rigorous proof to reveal that optimizing the normalized HSIC simultaneously minimizes the mutual information between different layers. Without any search process, our method achieves better compression tradeoffs comparing to the state-of-the-art compression algorithms. For instance, with ResNet-50, we achieve a 45.3%-FLOPs reduction, with a 75.75 top-1 accuracy on ImageNet. Codes are avaliable at https://github.com/MAC-AutoML/ITPruner/tree/master.



### A Unified Objective for Novel Class Discovery
- **Arxiv ID**: http://arxiv.org/abs/2108.08536v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08536v4)
- **Published**: 2021-08-19 07:22:29+00:00
- **Updated**: 2021-09-29 14:00:00+00:00
- **Authors**: Enrico Fini, Enver Sangineto, Stéphane Lathuilière, Zhun Zhong, Moin Nabi, Elisa Ricci
- **Comment**: ICCV 2021 (Oral)
- **Journal**: None
- **Summary**: In this paper, we study the problem of Novel Class Discovery (NCD). NCD aims at inferring novel object categories in an unlabeled set by leveraging from prior knowledge of a labeled set containing different, but related classes. Existing approaches tackle this problem by considering multiple objective functions, usually involving specialized loss terms for the labeled and the unlabeled samples respectively, and often requiring auxiliary regularization terms. In this paper, we depart from this traditional scheme and introduce a UNified Objective function (UNO) for discovering novel classes, with the explicit purpose of favoring synergy between supervised and unsupervised learning. Using a multi-view self-labeling strategy, we generate pseudo-labels that can be treated homogeneously with ground truth labels. This leads to a single classification objective operating on both known and unknown classes. Despite its simplicity, UNO outperforms the state of the art by a significant margin on several benchmarks (~+10% on CIFAR-100 and +8% on ImageNet). The project page is available at: https://ncd-uno.github.io.



### Multi-task Federated Learning for Heterogeneous Pancreas Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.08537v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2108.08537v1)
- **Published**: 2021-08-19 07:24:32+00:00
- **Updated**: 2021-08-19 07:24:32+00:00
- **Authors**: Chen Shen, Pochuan Wang, Holger R. Roth, Dong Yang, Daguang Xu, Masahiro Oda, Weichung Wang, Chiou-Shann Fuh, Po-Ting Chen, Kao-Lang Liu, Wei-Chih Liao, Kensaku Mori
- **Comment**: Accepted by MICCAI DCL Workshop 2021
- **Journal**: None
- **Summary**: Federated learning (FL) for medical image segmentation becomes more challenging in multi-task settings where clients might have different categories of labels represented in their data. For example, one client might have patient data with "healthy'' pancreases only while datasets from other clients may contain cases with pancreatic tumors. The vanilla federated averaging algorithm makes it possible to obtain more generalizable deep learning-based segmentation models representing the training data from multiple institutions without centralizing datasets. However, it might be sub-optimal for the aforementioned multi-task scenarios. In this paper, we investigate heterogeneous optimization methods that show improvements for the automated segmentation of pancreas and pancreatic tumors in abdominal CT images with FL settings.



### Learned Video Compression with Residual Prediction and Loop Filter
- **Arxiv ID**: http://arxiv.org/abs/2108.08551v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.08551v1)
- **Published**: 2021-08-19 08:31:41+00:00
- **Updated**: 2021-08-19 08:31:41+00:00
- **Authors**: Chao Liu, Heming Sun, Jiro Katto, Xiaoyang Zeng, Yibo Fan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a learned video codec with a residual prediction network (RP-Net) and a feature-aided loop filter (LF-Net). For the RP-Net, we exploit the residual of previous multiple frames to further eliminate the redundancy of the current frame residual. For the LF-Net, the features from residual decoding network and the motion compensation network are used to aid the reconstruction quality. To reduce the complexity, a light ResNet structure is used as the backbone for both RP-Net and LF-Net. Experimental results illustrate that we can save about 10% BD-rate compared with previous learned video compression frameworks. Moreover, we can achieve faster coding speed due to the ResNet backbone. This project is available at https://github.com/chaoliu18/RPLVC.



### DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2108.08557v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08557v1)
- **Published**: 2021-08-19 08:46:15+00:00
- **Updated**: 2021-08-19 08:46:15+00:00
- **Authors**: Nicola Garau, Niccolò Bisagno, Piotr Bródka, Nicola Conci
- **Comment**: International Conference on Computer Vision 2021 (ICCV 2021), 8
  pages, 4 figures, 4 tables, accepted for ICCV 2021 oral
- **Journal**: None
- **Summary**: Human Pose Estimation (HPE) aims at retrieving the 3D position of human joints from images or videos. We show that current 3D HPE methods suffer a lack of viewpoint equivariance, namely they tend to fail or perform poorly when dealing with viewpoints unseen at training time. Deep learning methods often rely on either scale-invariant, translation-invariant, or rotation-invariant operations, such as max-pooling. However, the adoption of such procedures does not necessarily improve viewpoint generalization, rather leading to more data-dependent methods. To tackle this issue, we propose a novel capsule autoencoder network with fast Variational Bayes capsule routing, named DECA. By modeling each joint as a capsule entity, combined with the routing algorithm, our approach can preserve the joints' hierarchical and geometrical structure in the feature space, independently from the viewpoint. By achieving viewpoint equivariance, we drastically reduce the network data dependency at training time, resulting in an improved ability to generalize for unseen viewpoints. In the experimental validation, we outperform other methods on depth images from both seen and unseen viewpoints, both top-view, and front-view. In the RGB domain, the same network gives state-of-the-art results on the challenging viewpoint transfer task, also establishing a new framework for top-view HPE. The code can be found at https://github.com/mmlab-cv/DECA.



### Concurrent Discrimination and Alignment for Self-Supervised Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.08562v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08562v1)
- **Published**: 2021-08-19 09:07:41+00:00
- **Updated**: 2021-08-19 09:07:41+00:00
- **Authors**: Anjan Dutta, Massimiliano Mancini, Zeynep Akata
- **Comment**: International Conference on Computer Vision (DeepMTL) 2021
- **Journal**: None
- **Summary**: Existing self-supervised learning methods learn representation by means of pretext tasks which are either (1) discriminating that explicitly specify which features should be separated or (2) aligning that precisely indicate which features should be closed together, but ignore the fact how to jointly and principally define which features to be repelled and which ones to be attracted. In this work, we combine the positive aspects of the discriminating and aligning methods, and design a hybrid method that addresses the above issue. Our method explicitly specifies the repulsion and attraction mechanism respectively by discriminative predictive task and concurrently maximizing mutual information between paired views sharing redundant information. We qualitatively and quantitatively show that our proposed model learns better features that are more effective for the diverse downstream tasks ranging from classification to semantic segmentation. Our experiments on nine established benchmarks show that the proposed model consistently outperforms the existing state-of-the-art results of self-supervised and transfer learning protocol.



### StructDepth: Leveraging the structural regularities for self-supervised indoor depth estimation
- **Arxiv ID**: http://arxiv.org/abs/2108.08574v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08574v1)
- **Published**: 2021-08-19 09:26:13+00:00
- **Updated**: 2021-08-19 09:26:13+00:00
- **Authors**: Boying Li, Yuan Huang, Zeyu Liu, Danping Zou, Wenxian Yu
- **Comment**: Accepted by ICCV2021. Project is in
  https://github.com/SJTU-ViSYS/StructDepth
- **Journal**: None
- **Summary**: Self-supervised monocular depth estimation has achieved impressive performance on outdoor datasets. Its performance however degrades notably in indoor environments because of the lack of textures. Without rich textures, the photometric consistency is too weak to train a good depth network. Inspired by the early works on indoor modeling, we leverage the structural regularities exhibited in indoor scenes, to train a better depth network. Specifically, we adopt two extra supervisory signals for self-supervised training: 1) the Manhattan normal constraint and 2) the co-planar constraint. The Manhattan normal constraint enforces the major surfaces (the floor, ceiling, and walls) to be aligned with dominant directions. The co-planar constraint states that the 3D points be well fitted by a plane if they are located within the same planar region. To generate the supervisory signals, we adopt two components to classify the major surface normal into dominant directions and detect the planar regions on the fly during training. As the predicted depth becomes more accurate after more training epochs, the supervisory signals also improve and in turn feedback to obtain a better depth model. Through extensive experiments on indoor benchmark datasets, the results show that our network outperforms the state-of-the-art methods. The source code is available at https://github.com/SJTU-ViSYS/StructDepth .



### Exploiting Scene Graphs for Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.08584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08584v1)
- **Published**: 2021-08-19 09:40:50+00:00
- **Updated**: 2021-08-19 09:40:50+00:00
- **Authors**: Tao He, Lianli Gao, Jingkuan Song, Yuan-Fang Li
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Human-Object Interaction (HOI) detection is a fundamental visual task aiming at localizing and recognizing interactions between humans and objects. Existing works focus on the visual and linguistic features of humans and objects. However, they do not capitalise on the high-level and semantic relationships present in the image, which provides crucial contextual and detailed relational knowledge for HOI inference. We propose a novel method to exploit this information, through the scene graph, for the Human-Object Interaction (SG2HOI) detection task. Our method, SG2HOI, incorporates the SG information in two ways: (1) we embed a scene graph into a global context clue, serving as the scene-specific environmental context; and (2) we build a relation-aware message-passing module to gather relationships from objects' neighborhood and transfer them into interactions. Empirical evaluation shows that our SG2HOI method outperforms the state-of-the-art methods on two benchmark HOI datasets: V-COCO and HICO-DET. Code will be available at https://github.com/ht014/SG2HOI.



### Progressive and Selective Fusion Network for High Dynamic Range Imaging
- **Arxiv ID**: http://arxiv.org/abs/2108.08585v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08585v1)
- **Published**: 2021-08-19 09:42:03+00:00
- **Updated**: 2021-08-19 09:42:03+00:00
- **Authors**: Qian Ye, Jun Xiao, Kin-man Lam, Takayuki Okatani
- **Comment**: None
- **Journal**: None
- **Summary**: This paper considers the problem of generating an HDR image of a scene from its LDR images. Recent studies employ deep learning and solve the problem in an end-to-end fashion, leading to significant performance improvements. However, it is still hard to generate a good quality image from LDR images of a dynamic scene captured by a hand-held camera, e.g., occlusion due to the large motion of foreground objects, causing ghosting artifacts. The key to success relies on how well we can fuse the input images in their feature space, where we wish to remove the factors leading to low-quality image generation while performing the fundamental computations for HDR image generation, e.g., selecting the best-exposed image/region. We propose a novel method that can better fuse the features based on two ideas. One is multi-step feature fusion; our network gradually fuses the features in a stack of blocks having the same structure. The other is the design of the component block that effectively performs two operations essential to the problem, i.e., comparing and selecting appropriate images/regions. Experimental results show that the proposed method outperforms the previous state-of-the-art methods on the standard benchmark tests.



### 3D Shapes Local Geometry Codes Learning with SDF
- **Arxiv ID**: http://arxiv.org/abs/2108.08593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08593v1)
- **Published**: 2021-08-19 09:56:03+00:00
- **Updated**: 2021-08-19 09:56:03+00:00
- **Authors**: Shun Yao, Fei Yang, Yongmei Cheng, Mikhail G. Mozerov
- **Comment**: DLGC workshop in ICCV 2021
- **Journal**: None
- **Summary**: A signed distance function (SDF) as the 3D shape description is one of the most effective approaches to represent 3D geometry for rendering and reconstruction. Our work is inspired by the state-of-the-art method DeepSDF that learns and analyzes the 3D shape as the iso-surface of its shell and this method has shown promising results especially in the 3D shape reconstruction and compression domain. In this paper, we consider the degeneration problem of reconstruction coming from the capacity decrease of the DeepSDF model, which approximates the SDF with a neural network and a single latent code. We propose Local Geometry Code Learning (LGCL), a model that improves the original DeepSDF results by learning from a local shape geometry of the full 3D shape. We add an extra graph neural network to split the single transmittable latent code into a set of local latent codes distributed on the 3D shape. Mentioned latent codes are used to approximate the SDF in their local regions, which will alleviate the complexity of the approximation compared to the original DeepSDF. Furthermore, we introduce a new geometric loss function to facilitate the training of these local latent codes. Note that other local shape adjusting methods use the 3D voxel representation, which in turn is a problem highly difficult to solve or even is insolvable. In contrast, our architecture is based on graph processing implicitly and performs the learning regression process directly in the latent code space, thus make the proposed architecture more flexible and also simple for realization. Our experiments on 3D shape reconstruction demonstrate that our LGCL method can keep more details with a significantly smaller size of the SDF decoder and outperforms considerably the original DeepSDF method under the most important quantitative metrics.



### Feature Stylization and Domain-aware Contrastive Learning for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2108.08596v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08596v1)
- **Published**: 2021-08-19 10:04:01+00:00
- **Updated**: 2021-08-19 10:04:01+00:00
- **Authors**: Seogkyu Jeon, Kibeom Hong, Pilhyeon Lee, Jewook Lee, Hyeran Byun
- **Comment**: Accepted to ACM MM 2021 (oral)
- **Journal**: None
- **Summary**: Domain generalization aims to enhance the model robustness against domain shift without accessing the target domain. Since the available source domains for training are limited, recent approaches focus on generating samples of novel domains. Nevertheless, they either struggle with the optimization problem when synthesizing abundant domains or cause the distortion of class semantics. To these ends, we propose a novel domain generalization framework where feature statistics are utilized for stylizing original features to ones with novel domain properties. To preserve class information during stylization, we first decompose features into high and low frequency components. Afterward, we stylize the low frequency components with the novel domain styles sampled from the manipulated statistics, while preserving the shape cues in high frequency ones. As the final step, we re-merge both components to synthesize novel domain features. To enhance domain robustness, we utilize the stylized features to maintain the model consistency in terms of features as well as outputs. We achieve the feature consistency with the proposed domain-aware supervised contrastive loss, which ensures domain invariance while increasing class discriminability. Experimental results demonstrate the effectiveness of the proposed feature stylization and the domain-aware contrastive loss. Through quantitative comparisons, we verify the lead of our method upon existing state-of-the-art methods on two benchmarks, PACS and Office-Home.



### Semantic Compositional Learning for Low-shot Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2108.08600v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08600v1)
- **Published**: 2021-08-19 10:13:55+00:00
- **Updated**: 2021-08-19 10:13:55+00:00
- **Authors**: Tao He, Lianli Gao, Jingkuan Song, Jianfei Cai, Yuan-Fang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Scene graphs provide valuable information to many downstream tasks. Many scene graph generation (SGG) models solely use the limited annotated relation triples for training, leading to their underperformance on low-shot (few and zero) scenarios, especially on the rare predicates. To address this problem, we propose a novel semantic compositional learning strategy that makes it possible to construct additional, realistic relation triples with objects from different images. Specifically, our strategy decomposes a relation triple by identifying and removing the unessential component and composes a new relation triple by fusing with a semantically or visually similar object from a visual components dictionary, whilst ensuring the realisticity of the newly composed triple. Notably, our strategy is generic and can be combined with existing SGG models to significantly improve their performance. We performed a comprehensive evaluation on the benchmark dataset Visual Genome. For three recent SGG models, adding our strategy improves their performance by close to 50\%, and all of them substantially exceed the current state-of-the-art.



### Generating Superpixels for High-resolution Images with Decoupled Patch Calibration
- **Arxiv ID**: http://arxiv.org/abs/2108.08607v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08607v2)
- **Published**: 2021-08-19 10:33:05+00:00
- **Updated**: 2021-08-23 14:28:41+00:00
- **Authors**: Yaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu, Yi Yang
- **Comment**: 10 pages, superpixel segmentation. High-resolution Computer Version
- **Journal**: None
- **Summary**: Superpixel segmentation has recently seen important progress benefiting from the advances in differentiable deep learning. However, the very high-resolution superpixel segmentation still remains challenging due to the expensive memory and computation cost, making the current advanced superpixel networks fail to process. In this paper, we devise Patch Calibration Networks (PCNet), aiming to efficiently and accurately implement high-resolution superpixel segmentation. PCNet follows the principle of producing high-resolution output from low-resolution input for saving GPU memory and relieving computation cost. To recall the fine details destroyed by the down-sampling operation, we propose a novel Decoupled Patch Calibration (DPC) branch for collaboratively augment the main superpixel generation branch. In particular, DPC takes a local patch from the high-resolution images and dynamically generates a binary mask to impose the network to focus on region boundaries. By sharing the parameters of DPC and main branches, the fine-detailed knowledge learned from high-resolution patches will be transferred to help calibrate the destroyed information. To the best of our knowledge, we make the first attempt to consider the deep-learning-based superpixel generation for high-resolution cases. To facilitate this research, we build evaluation benchmarks from two public datasets and one new constructed one, covering a wide range of diversities from fine-grained human parts to cityscapes. Extensive experiments demonstrate that our PCNet can not only perform favorably against the state-of-the-arts in the quantitative results but also improve the resolution upper bound from 3K to 5K on 1080Ti GPUs.



### Spatially-Adaptive Image Restoration using Distortion-Guided Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.08617v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08617v1)
- **Published**: 2021-08-19 11:02:25+00:00
- **Updated**: 2021-08-19 11:02:25+00:00
- **Authors**: Kuldeep Purohit, Maitreya Suin, A. N. Rajagopalan, Vishnu Naresh Boddeti
- **Comment**: Accepted at ICCV 2021
- **Journal**: None
- **Summary**: We present a general learning-based solution for restoring images suffering from spatially-varying degradations. Prior approaches are typically degradation-specific and employ the same processing across different images and different pixels within. However, we hypothesize that such spatially rigid processing is suboptimal for simultaneously restoring the degraded pixels as well as reconstructing the clean regions of the image. To overcome this limitation, we propose SPAIR, a network design that harnesses distortion-localization information and dynamically adjusts computation to difficult regions in the image. SPAIR comprises of two components, (1) a localization network that identifies degraded pixels, and (2) a restoration network that exploits knowledge from the localization network in filter and feature domain to selectively and adaptively restore degraded pixels. Our key idea is to exploit the non-uniformity of heavy degradations in spatial-domain and suitably embed this knowledge within distortion-guided modules performing sparse normalization, feature extraction and attention. Our architecture is agnostic to physical formation model and generalizes across several types of spatially-varying degradations. We demonstrate the efficacy of SPAIR individually on four restoration tasks-removal of rain-streaks, raindrops, shadows and motion blur. Extensive qualitative and quantitative comparisons with prior art on 11 benchmark datasets demonstrate that our degradation-agnostic network design offers significant performance gains over state-of-the-art degradation-specific architectures. Code available at https://github.com/human-analysis/spatially-adaptive-image-restoration.



### Reproducible radiomics through automated machine learning validated on twelve clinical applications
- **Arxiv ID**: http://arxiv.org/abs/2108.08618v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08618v2)
- **Published**: 2021-08-19 11:03:54+00:00
- **Updated**: 2022-07-29 13:36:52+00:00
- **Authors**: Martijn P. A. Starmans, Sebastian R. van der Voort, Thomas Phil, Milea J. M. Timbergen, Melissa Vos, Guillaume A. Padmos, Wouter Kessels, David Hanff, Dirk J. Grunhagen, Cornelis Verhoef, Stefan Sleijfer, Martin J. van den Bent, Marion Smits, Roy S. Dwarkasing, Christopher J. Els, Federico Fiduzi, Geert J. L. H. van Leenders, Anela Blazevic, Johannes Hofland, Tessa Brabander, Renza A. H. van Gils, Gaston J. H. Franssen, Richard A. Feelders, Wouter W. de Herder, Florian E. Buisman, Francois E. J. A. Willemssen, Bas Groot Koerkamp, Lindsay Angus, Astrid A. M. van der Veldt, Ana Rajicic, Arlette E. Odink, Mitchell Deen, Jose M. Castillo T., Jifke Veenland, Ivo Schoots, Michel Renckens, Michail Doukas, Rob A. de Man, Jan N. M. IJzermans, Razvan L. Miclea, Peter B. Vermeulen, Esther E. Bron, Maarten G. Thomeer, Jacob J. Visser, Wiro J. Niessen, Stefan Klein
- **Comment**: 33 pages, 4 figures, 4 tables, 2 supplementary figures, 3
  supplementary table, submitted to Medical Image Analysis; revision;
- **Journal**: None
- **Summary**: Radiomics uses quantitative medical imaging features to predict clinical outcomes. Currently, in a new clinical application, finding the optimal radiomics method out of the wide range of available options has to be done manually through a heuristic trial-and-error process. In this study we propose a framework for automatically optimizing the construction of radiomics workflows per application. To this end, we formulate radiomics as a modular workflow and include a large collection of common algorithms for each component. To optimize the workflow per application, we employ automated machine learning using a random search and ensembling. We evaluate our method in twelve different clinical applications, resulting in the following area under the curves: 1) liposarcoma (0.83); 2) desmoid-type fibromatosis (0.82); 3) primary liver tumors (0.80); 4) gastrointestinal stromal tumors (0.77); 5) colorectal liver metastases (0.61); 6) melanoma metastases (0.45); 7) hepatocellular carcinoma (0.75); 8) mesenteric fibrosis (0.80); 9) prostate cancer (0.72); 10) glioma (0.71); 11) Alzheimer's disease (0.87); and 12) head and neck cancer (0.84). We show that our framework has a competitive performance compared human experts, outperforms a radiomics baseline, and performs similar or superior to Bayesian optimization and more advanced ensemble approaches. Concluding, our method fully automatically optimizes the construction of radiomics workflows, thereby streamlining the search for radiomics biomarkers in new applications. To facilitate reproducibility and future research, we publicly release six datasets, the software implementation of our framework, and the code to reproduce this study.



### VolumeFusion: Deep Depth Fusion for 3D Scene Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2108.08623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08623v1)
- **Published**: 2021-08-19 11:33:58+00:00
- **Updated**: 2021-08-19 11:33:58+00:00
- **Authors**: Jaesung Choe, Sunghoon Im, Francois Rameau, Minjun Kang, In So Kweon
- **Comment**: ICCV 2021 Accepted
- **Journal**: None
- **Summary**: To reconstruct a 3D scene from a set of calibrated views, traditional multi-view stereo techniques rely on two distinct stages: local depth maps computation and global depth maps fusion. Recent studies concentrate on deep neural architectures for depth estimation by using conventional depth fusion method or direct 3D reconstruction network by regressing Truncated Signed Distance Function (TSDF). In this paper, we advocate that replicating the traditional two stages framework with deep neural networks improves both the interpretability and the accuracy of the results. As mentioned, our network operates in two steps: 1) the local computation of the local depth maps with a deep MVS technique, and, 2) the depth maps and images' features fusion to build a single TSDF volume. In order to improve the matching performance between images acquired from very different viewpoints (e.g., large-baseline and rotations), we introduce a rotation-invariant 3D convolution kernel called PosedConv. The effectiveness of the proposed architecture is underlined via a large series of experiments conducted on the ScanNet dataset where our approach compares favorably against both traditional and deep learning techniques.



### Spatio-Temporal Interaction Graph Parsing Networks for Human-Object Interaction Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.08633v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.08633v1)
- **Published**: 2021-08-19 11:57:27+00:00
- **Updated**: 2021-08-19 11:57:27+00:00
- **Authors**: Ning Wang, Guangming Zhu, Liang Zhang, Peiyi Shen, Hongsheng Li, Cong Hua
- **Comment**: ACM MM Oral paper
- **Journal**: None
- **Summary**: For a given video-based Human-Object Interaction scene, modeling the spatio-temporal relationship between humans and objects are the important cue to understand the contextual information presented in the video. With the effective spatio-temporal relationship modeling, it is possible not only to uncover contextual information in each frame but also to directly capture inter-time dependencies. It is more critical to capture the position changes of human and objects over the spatio-temporal dimension when their appearance features may not show up significant changes over time. The full use of appearance features, the spatial location and the semantic information are also the key to improve the video-based Human-Object Interaction recognition performance. In this paper, Spatio-Temporal Interaction Graph Parsing Networks (STIGPN) are constructed, which encode the videos with a graph composed of human and object nodes. These nodes are connected by two types of relations: (i) spatial relations modeling the interactions between human and the interacted objects within each frame. (ii) inter-time relations capturing the long range dependencies between human and the interacted objects across frame. With the graph, STIGPN learn spatio-temporal features directly from the whole video-based Human-Object Interaction scenes. Multi-modal features and a multi-stream fusion strategy are used to enhance the reasoning capability of STIGPN. Two Human-Object Interaction video datasets, including CAD-120 and Something-Else, are used to evaluate the proposed architectures, and the state-of-the-art performance demonstrates the superiority of STIGPN.



### Wind Turbine Blade Surface Damage Detection based on Aerial Imagery and VGG16-RCNN Framework
- **Arxiv ID**: http://arxiv.org/abs/2108.08636v2
- **DOI**: None
- **Categories**: **eess.SY**, cs.CV, cs.SY, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08636v2)
- **Published**: 2021-08-19 12:00:13+00:00
- **Updated**: 2022-08-18 11:06:31+00:00
- **Authors**: Juhi Patel, Lagan Sharma, Harsh S. Dhiman
- **Comment**: Introduction/Methodology section needs further review
- **Journal**: None
- **Summary**: In this manuscript, an image analytics based deep learning framework for wind turbine blade surface damage detection is proposed. Turbine blade(s) which carry approximately one-third of a turbine weight are susceptible to damage and can cause sudden malfunction of a grid-connected wind energy conversion system. The surface damage detection of wind turbine blade requires a large dataset so as to detect a type of damage at an early stage. Turbine blade images are captured via aerial imagery. Upon inspection, it is found that the image dataset was limited and hence image augmentation is applied to improve blade image dataset. The approach is modeled as a multi-class supervised learning problem and deep learning methods like Convolutional neural network (CNN), VGG16-RCNN and AlexNet are tested for determining the potential capability of turbine blade surface damage.



### 3DIAS: 3D Shape Reconstruction with Implicit Algebraic Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2108.08653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08653v1)
- **Published**: 2021-08-19 12:34:28+00:00
- **Updated**: 2021-08-19 12:34:28+00:00
- **Authors**: Mohsen Yavartanoo, JaeYoung Chung, Reyhaneh Neshatavar, Kyoung Mu Lee
- **Comment**: Published at ICCV 2021
- **Journal**: None
- **Summary**: 3D Shape representation has substantial effects on 3D shape reconstruction. Primitive-based representations approximate a 3D shape mainly by a set of simple implicit primitives, but the low geometrical complexity of the primitives limits the shape resolution. Moreover, setting a sufficient number of primitives for an arbitrary shape is challenging. To overcome these issues, we propose a constrained implicit algebraic surface as the primitive with few learnable coefficients and higher geometrical complexities and a deep neural network to produce these primitives. Our experiments demonstrate the superiorities of our method in terms of representation power compared to the state-of-the-art methods in single RGB image 3D shape reconstruction. Furthermore, we show that our method can semantically learn segments of 3D shapes in an unsupervised manner. The code is publicly available from https://myavartanoo.github.io/3dias/ .



### Video Relation Detection via Tracklet based Visual Transformer
- **Arxiv ID**: http://arxiv.org/abs/2108.08669v1
- **DOI**: 10.1145/3474085.3479231
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08669v1)
- **Published**: 2021-08-19 13:13:23+00:00
- **Updated**: 2021-08-19 13:13:23+00:00
- **Authors**: Kaifeng Gao, Long Chen, Yifeng Huang, Jun Xiao
- **Comment**: 1st of Video Relation Understanding (VRU) Grand Challenge in ACM
  Multimedia 2021
- **Journal**: None
- **Summary**: Video Visual Relation Detection (VidVRD), has received significant attention of our community over recent years. In this paper, we apply the state-of-the-art video object tracklet detection pipeline MEGA and deepSORT to generate tracklet proposals. Then we perform VidVRD in a tracklet-based manner without any pre-cutting operations. Specifically, we design a tracklet-based visual Transformer. It contains a temporal-aware decoder which performs feature interactions between the tracklets and learnable predicate query embeddings, and finally predicts the relations. Experimental results strongly demonstrate the superiority of our method, which outperforms other methods by a large margin on the Video Relation Understanding (VRU) Grand Challenge in ACM Multimedia 2021. Codes are released at https://github.com/Dawn-LX/VidVRD-tracklets.



### Towards Controllable and Photorealistic Region-wise Image Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2108.08674v1
- **DOI**: 10.1145/3474085.3475206
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08674v1)
- **Published**: 2021-08-19 13:29:45+00:00
- **Updated**: 2021-08-19 13:29:45+00:00
- **Authors**: Ansheng You, Chenglin Zhou, Qixuan Zhang, Lan Xu
- **Comment**: None
- **Journal**: ACMMM 2021
- **Summary**: Adaptive and flexible image editing is a desirable function of modern generative models. In this work, we present a generative model with auto-encoder architecture for per-region style manipulation. We apply a code consistency loss to enforce an explicit disentanglement between content and style latent representations, making the content and style of generated samples consistent with their corresponding content and style references. The model is also constrained by a content alignment loss to ensure the foreground editing will not interfere background contents. As a result, given interested region masks provided by users, our model supports foreground region-wise style transfer. Specially, our model receives no extra annotations such as semantic labels except for self-supervision. Extensive experiments show the effectiveness of the proposed method and exhibit the flexibility of the proposed model for various applications, including region-wise style editing, latent space interpolation, cross-domain style transfer.



### Contrastive Language-Image Pre-training for the Italian Language
- **Arxiv ID**: http://arxiv.org/abs/2108.08688v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08688v1)
- **Published**: 2021-08-19 13:53:47+00:00
- **Updated**: 2021-08-19 13:53:47+00:00
- **Authors**: Federico Bianchi, Giuseppe Attanasio, Raphael Pisoni, Silvia Terragni, Gabriele Sarti, Sri Lakshmi
- **Comment**: None
- **Journal**: None
- **Summary**: CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal model that jointly learns representations of images and texts. The model is trained on a massive amount of English data and shows impressive performance on zero-shot classification tasks. Training the same model on a different language is not trivial, since data in other languages might be not enough and the model needs high-quality translations of the texts to guarantee a good performance. In this paper, we present the first CLIP model for the Italian Language (CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show that CLIP-Italian outperforms the multilingual CLIP model on the tasks of image retrieval and zero-shot classification.



### Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables
- **Arxiv ID**: http://arxiv.org/abs/2108.08697v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08697v1)
- **Published**: 2021-08-19 14:04:59+00:00
- **Updated**: 2021-08-19 14:04:59+00:00
- **Authors**: Tao Wang, Yong Li, Jingyang Peng, Yipeng Ma, Xian Wang, Fenglong Song, Youliang Yan
- **Comment**: Accepted to ICCV2021
- **Journal**: None
- **Summary**: Recently, deep learning-based image enhancement algorithms achieved state-of-the-art (SOTA) performance on several publicly available datasets. However, most existing methods fail to meet practical requirements either for visual perception or for computation efficiency, especially for high-resolution images. In this paper, we propose a novel real-time image enhancer via learnable spatial-aware 3-dimentional lookup tables(3D LUTs), which well considers global scenario and local spatial information. Specifically, we introduce a light weight two-head weight predictor that has two outputs. One is a 1D weight vector used for image-level scenario adaptation, the other is a 3D weight map aimed for pixel-wise category fusion. We learn the spatial-aware 3D LUTs and fuse them according to the aforementioned weights in an end-to-end manner. The fused LUT is then used to transform the source image into the target tone in an efficient way. Extensive results show that our model outperforms SOTA image enhancement methods on public datasets both subjectively and objectively, and that our model only takes about 4ms to process a 4K resolution image on one NVIDIA V100 GPU.



### How to cheat with metrics in single-image HDR reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2108.08713v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08713v1)
- **Published**: 2021-08-19 14:29:15+00:00
- **Updated**: 2021-08-19 14:29:15+00:00
- **Authors**: Gabriel Eilertsen, Saghi Hajisharif, Param Hanji, Apostolia Tsirikoglou, Rafal K. Mantiuk, Jonas Unger
- **Comment**: ICCV 2021 workshop on Learning for Computational Imaging (LCI)
- **Journal**: None
- **Summary**: Single-image high dynamic range (SI-HDR) reconstruction has recently emerged as a problem well-suited for deep learning methods. Each successive technique demonstrates an improvement over existing methods by reporting higher image quality scores. This paper, however, highlights that such improvements in objective metrics do not necessarily translate to visually superior images. The first problem is the use of disparate evaluation conditions in terms of data and metric parameters, calling for a standardized protocol to make it possible to compare between papers. The second problem, which forms the main focus of this paper, is the inherent difficulty in evaluating SI-HDR reconstructions since certain aspects of the reconstruction problem dominate objective differences, thereby introducing a bias. Here, we reproduce a typical evaluation using existing as well as simulated SI-HDR methods to demonstrate how different aspects of the problem affect objective quality metrics. Surprisingly, we found that methods that do not even reconstruct HDR information can compete with state-of-the-art deep learning methods. We show how such results are not representative of the perceived quality and that SI-HDR reconstruction needs better evaluation protocols.



### Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2108.08728v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08728v2)
- **Published**: 2021-08-19 14:53:40+00:00
- **Updated**: 2021-10-26 12:52:43+00:00
- **Authors**: Yongming Rao, Guangyi Chen, Jiwen Lu, Jie Zhou
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Attention mechanism has demonstrated great potential in fine-grained visual recognition tasks. In this paper, we present a counterfactual attention learning method to learn more effective attention based on causal inference. Unlike most existing methods that learn visual attention based on conventional likelihood, we propose to learn the attention with counterfactual causality, which provides a tool to measure the attention quality and a powerful supervisory signal to guide the learning process. Specifically, we analyze the effect of the learned visual attention on network prediction through counterfactual intervention and maximize the effect to encourage the network to learn more useful attention for fine-grained image recognition. Empirically, we evaluate our method on a wide range of fine-grained recognition tasks where attention plays a crucial role, including fine-grained image categorization, person re-identification, and vehicle re-identification. The consistent improvement on all benchmarks demonstrates the effectiveness of our method. Code is available at https://github.com/raoyongming/CAL



### FSNet: A Failure Detection Framework for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.08748v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.08748v2)
- **Published**: 2021-08-19 15:26:52+00:00
- **Updated**: 2021-09-28 00:53:53+00:00
- **Authors**: Quazi Marufur Rahman, Niko Sünderhauf, Peter Corke, Feras Dayoub
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation is an important task that helps autonomous vehicles understand their surroundings and navigate safely. During deployment, even the most mature segmentation models are vulnerable to various external factors that can degrade the segmentation performance with potentially catastrophic consequences for the vehicle and its surroundings. To address this issue, we propose a failure detection framework to identify pixel-level misclassification. We do so by exploiting internal features of the segmentation model and training it simultaneously with a failure detection network. During deployment, the failure detector can flag areas in the image where the segmentation model have failed to segment correctly. We evaluate the proposed approach against state-of-the-art methods and achieve 12.30%, 9.46%, and 9.65% performance improvement in the AUPR-Error metric for Cityscapes, BDD100K, and Mapillary semantic segmentation datasets.



### Category-Level 6D Object Pose Estimation via Cascaded Relation and Recurrent Reconstruction Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.08755v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08755v1)
- **Published**: 2021-08-19 15:46:52+00:00
- **Updated**: 2021-08-19 15:46:52+00:00
- **Authors**: Jiaze Wang, Kai Chen, Qi Dou
- **Comment**: accepted by IROS2021
- **Journal**: None
- **Summary**: Category-level 6D pose estimation, aiming to predict the location and orientation of unseen object instances, is fundamental to many scenarios such as robotic manipulation and augmented reality, yet still remains unsolved. Precisely recovering instance 3D model in the canonical space and accurately matching it with the observation is an essential point when estimating 6D pose for unseen objects. In this paper, we achieve accurate category-level 6D pose estimation via cascaded relation and recurrent reconstruction networks. Specifically, a novel cascaded relation network is dedicated for advanced representation learning to explore the complex and informative relations among instance RGB image, instance point cloud and category shape prior. Furthermore, we design a recurrent reconstruction network for iterative residual refinement to progressively improve the reconstruction and correspondence estimations from coarse to fine. Finally, the instance 6D pose is obtained leveraging the estimated dense correspondences between the instance point cloud and the reconstructed 3D model in the canonical space. We have conducted extensive experiments on two well-acknowledged benchmarks of category-level 6D pose estimation, with significant performance improvement over existing approaches. On the representatively strict evaluation metrics of $3D_{75}$ and $5^{\circ}2 cm$, our method exceeds the latest state-of-the-art SPD by $4.9\%$ and $17.7\%$ on the CAMERA25 dataset, and by $2.7\%$ and $8.5\%$ on the REAL275 dataset. Codes are available at https://wangjiaze.cn/projects/6DPoseEstimation.html.



### Robust outlier detection by de-biasing VAE likelihoods
- **Arxiv ID**: http://arxiv.org/abs/2108.08760v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.10; I.4.8; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2108.08760v3)
- **Published**: 2021-08-19 16:00:58+00:00
- **Updated**: 2022-07-19 12:52:19+00:00
- **Authors**: Kushal Chauhan, Barath Mohan U, Pradeep Shenoy, Manish Gupta, Devarajan Sridharan
- **Comment**: CVPR 2022. 20 pages and 19 figures
- **Journal**: None
- **Summary**: Deep networks often make confident, yet, incorrect, predictions when tested with outlier data that is far removed from their training distributions. Likelihoods computed by deep generative models (DGMs) are a candidate metric for outlier detection with unlabeled data. Yet, previous studies have shown that DGM likelihoods are unreliable and can be easily biased by simple transformations to input data. Here, we examine outlier detection with variational autoencoders (VAEs), among the simplest of DGMs. We propose novel analytical and algorithmic approaches to ameliorate key biases with VAE likelihoods. Our bias corrections are sample-specific, computationally inexpensive, and readily computed for various decoder visible distributions. Next, we show that a well-known image pre-processing technique -- contrast stretching -- extends the effectiveness of bias correction to further improve outlier detection. Our approach achieves state-of-the-art accuracies with nine grayscale and natural image datasets, and demonstrates significant advantages -- both with speed and performance -- over four recent, competing approaches. In summary, lightweight remedies suffice to achieve robust outlier detection with VAEs.



### Learning to Match Features with Seeded Graph Matching Network
- **Arxiv ID**: http://arxiv.org/abs/2108.08771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08771v1)
- **Published**: 2021-08-19 16:25:23+00:00
- **Updated**: 2021-08-19 16:25:23+00:00
- **Authors**: Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, Xuyang Bai, Zeyu Hu, Chiew-Lan Tai, Long Quan
- **Comment**: Accepted by ICCV2021, code to be realeased at
  https://github.com/vdvchen/SGMNet
- **Journal**: None
- **Summary**: Matching local features across images is a fundamental problem in computer vision. Targeting towards high accuracy and efficiency, we propose Seeded Graph Matching Network, a graph neural network with sparse structure to reduce redundant connectivity and learn compact representation. The network consists of 1) Seeding Module, which initializes the matching by generating a small set of reliable matches as seeds. 2) Seeded Graph Neural Network, which utilizes seed matches to pass messages within/across images and predicts assignment costs. Three novel operations are proposed as basic elements for message passing: 1) Attentional Pooling, which aggregates keypoint features within the image to seed matches. 2) Seed Filtering, which enhances seed features and exchanges messages across images. 3) Attentional Unpooling, which propagates seed features back to original keypoints. Experiments show that our method reduces computational and memory complexity significantly compared with typical attention-based networks while competitive or higher performance is achieved.



### MobileCaps: A Lightweight Model for Screening and Severity Analysis of COVID-19 Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2108.08775v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T07, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2108.08775v1)
- **Published**: 2021-08-19 16:33:05+00:00
- **Updated**: 2021-08-19 16:33:05+00:00
- **Authors**: S J Pawan, Rahul Sankar, Amithash M Prabhudev, P A Mahesh, K Prakashini, Sudha Kiran Das, Jeny Rajan
- **Comment**: 14 pages, 6 figures
- **Journal**: None
- **Summary**: The world is going through a challenging phase due to the disastrous effect caused by the COVID-19 pandemic on the healthcare system and the economy. The rate of spreading, post-COVID-19 symptoms, and the occurrence of new strands of COVID-19 have put the healthcare systems in disruption across the globe. Due to this, the task of accurately screening COVID-19 cases has become of utmost priority. Since the virus infects the respiratory system, Chest X-Ray is an imaging modality that is adopted extensively for the initial screening. We have performed a comprehensive study that uses CXR images to identify COVID-19 cases and realized the necessity of having a more generalizable model. We utilize MobileNetV2 architecture as the feature extractor and integrate it into Capsule Networks to construct a fully automated and lightweight model termed as MobileCaps. MobileCaps is trained and evaluated on the publicly available dataset with the model ensembling and Bayesian optimization strategies to efficiently classify CXR images of patients with COVID-19 from non-COVID-19 pneumonia and healthy cases. The proposed model is further evaluated on two additional RT-PCR confirmed datasets to demonstrate the generalizability. We also introduce MobileCaps-S and leverage it for performing severity assessment of CXR images of COVID-19 based on the Radiographic Assessment of Lung Edema (RALE) scoring technique. Our classification model achieved an overall recall of 91.60, 94.60, 92.20, and a precision of 98.50, 88.21, 92.62 for COVID-19, non-COVID-19 pneumonia, and healthy cases, respectively. Further, the severity assessment model attained an R$^2$ coefficient of 70.51. Owing to the fact that the proposed models have fewer trainable parameters than the state-of-the-art models reported in the literature, we believe our models will go a long way in aiding healthcare systems in the battle against the pandemic.



### Causal Attention for Unbiased Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.08782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08782v1)
- **Published**: 2021-08-19 16:45:51+00:00
- **Updated**: 2021-08-19 16:45:51+00:00
- **Authors**: Tan Wang, Chang Zhou, Qianru Sun, Hanwang Zhang
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Attention module does not always help deep models learn causal features that are robust in any confounding context, e.g., a foreground object feature is invariant to different backgrounds. This is because the confounders trick the attention to capture spurious correlations that benefit the prediction when the training and testing data are IID (identical & independent distribution); while harm the prediction when the data are OOD (out-of-distribution). The sole fundamental solution to learn causal attention is by causal intervention, which requires additional annotations of the confounders, e.g., a "dog" model is learned within "grass+dog" and "road+dog" respectively, so the "grass" and "road" contexts will no longer confound the "dog" recognition. However, such annotation is not only prohibitively expensive, but also inherently problematic, as the confounders are elusive in nature. In this paper, we propose a causal attention module (CaaM) that self-annotates the confounders in unsupervised fashion. In particular, multiple CaaMs can be stacked and integrated in conventional attention CNN and self-attention Vision Transformer. In OOD settings, deep models with CaaM outperform those without it significantly; even in IID settings, the attention localization is also improved by CaaM, showing a great potential in applications that require robust visual saliency. Codes are available at \url{https://github.com/Wangt-CN/CaaM}.



### Wisdom of (Binned) Crowds: A Bayesian Stratification Paradigm for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2108.08784v1
- **DOI**: 10.1145/3474085.3475522
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.08784v1)
- **Published**: 2021-08-19 16:50:31+00:00
- **Updated**: 2021-08-19 16:50:31+00:00
- **Authors**: Sravya Vardhani Shivapuja, Mansi Pradeep Khamkar, Divij Bajaj, Ganesh Ramakrishnan, Ravi Kiran Sarvadevabhatla
- **Comment**: Accepted at ACM Multimedia (ACMMM) 2021 . Code, pretrained models and
  interactive visualizations can be viewed at our project page
  https://deepcount.iiit.ac.in/
- **Journal**: None
- **Summary**: Datasets for training crowd counting deep networks are typically heavy-tailed in count distribution and exhibit discontinuities across the count range. As a result, the de facto statistical measures (MSE, MAE) exhibit large variance and tend to be unreliable indicators of performance across the count range. To address these concerns in a holistic manner, we revise processes at various stages of the standard crowd counting pipeline. To enable principled and balanced minibatch sampling, we propose a novel smoothed Bayesian sample stratification approach. We propose a novel cost function which can be readily incorporated into existing crowd counting deep networks to encourage strata-aware optimization. We analyze the performance of representative crowd counting approaches across standard datasets at per strata level and in aggregate. We analyze the performance of crowd counting approaches across standard datasets and demonstrate that our proposed modifications noticeably reduce error standard deviation. Our contributions represent a nuanced, statistically balanced and fine-grained characterization of performance for crowd counting approaches. Code, pretrained models and interactive visualizations can be viewed at our project page https://deepcount.iiit.ac.in/



### Image Inpainting using Partial Convolution
- **Arxiv ID**: http://arxiv.org/abs/2108.08791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08791v1)
- **Published**: 2021-08-19 17:01:27+00:00
- **Updated**: 2021-08-19 17:01:27+00:00
- **Authors**: Harsh Patel, Amey Kulkarni, Shivam Sahni, Udit Vyas
- **Comment**: None
- **Journal**: None
- **Summary**: Image Inpainting is one of the very popular tasks in the field of image processing with broad applications in computer vision. In various practical applications, images are often deteriorated by noise due to the presence of corrupted, lost, or undesirable information. There have been various restoration techniques used in the past with both classical and deep learning approaches for handling such issues. Some traditional methods include image restoration by filling gap pixels using the nearby known pixels or using the moving average over the same. The aim of this paper is to perform image inpainting using robust deep learning methods that use partial convolution layers.



### Neural-GIF: Neural Generalized Implicit Functions for Animating People in Clothing
- **Arxiv ID**: http://arxiv.org/abs/2108.08807v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08807v2)
- **Published**: 2021-08-19 17:25:16+00:00
- **Updated**: 2021-08-20 11:54:18+00:00
- **Authors**: Garvita Tiwari, Nikolaos Sarafianos, Tony Tung, Gerard Pons-Moll
- **Comment**: None
- **Journal**: None
- **Summary**: We present Neural Generalized Implicit Functions(Neural-GIF), to animate people in clothing as a function of the body pose. Given a sequence of scans of a subject in various poses, we learn to animate the character for new poses. Existing methods have relied on template-based representations of the human body (or clothing). However such models usually have fixed and limited resolutions, require difficult data pre-processing steps and cannot be used with complex clothing. We draw inspiration from template-based methods, which factorize motion into articulation and non-rigid deformation, but generalize this concept for implicit shape learning to obtain a more flexible model. We learn to map every point in the space to a canonical space, where a learned deformation field is applied to model non-rigid effects, before evaluating the signed distance field. Our formulation allows the learning of complex and non-rigid deformations of clothing and soft tissue, without computing a template registration as it is common with current approaches. Neural-GIF can be trained on raw 3D scans and reconstructs detailed complex surface geometry and deformations. Moreover, the model can generalize to new poses. We evaluate our method on a variety of characters from different public datasets in diverse clothing styles and show significant improvements over baseline methods, quantitatively and qualitatively. We also extend our model to multiple shape setting. To stimulate further research, we will make the model, code and data publicly available at: https://virtualhumans.mpi-inf.mpg.de/neuralgif/



### Do Vision Transformers See Like Convolutional Neural Networks?
- **Arxiv ID**: http://arxiv.org/abs/2108.08810v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2108.08810v2)
- **Published**: 2021-08-19 17:27:03+00:00
- **Updated**: 2022-03-03 22:04:47+00:00
- **Authors**: Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, Alexey Dosovitskiy
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have so far been the de-facto model for visual data. Recent work has shown that (Vision) Transformer models (ViT) can achieve comparable or even superior performance on image classification tasks. This raises a central question: how are Vision Transformers solving these tasks? Are they acting like convolutional networks, or learning entirely different visual representations? Analyzing the internal representation structure of ViTs and CNNs on image classification benchmarks, we find striking differences between the two architectures, such as ViT having more uniform representations across all layers. We explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connections, which strongly propagate features from lower to higher layers. We study the ramifications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, we study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer.



### Click to Move: Controlling Video Generation with Sparse Motion
- **Arxiv ID**: http://arxiv.org/abs/2108.08815v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.08815v1)
- **Published**: 2021-08-19 17:33:13+00:00
- **Updated**: 2021-08-19 17:33:13+00:00
- **Authors**: Pierfrancesco Ardino, Marco De Nadai, Bruno Lepri, Elisa Ricci, Stéphane Lathuilière
- **Comment**: Accepted by International Conference on Computer Vision (ICCV 2021)
- **Journal**: None
- **Summary**: This paper introduces Click to Move (C2M), a novel framework for video generation where the user can control the motion of the synthesized video through mouse clicks specifying simple object trajectories of the key objects in the scene. Our model receives as input an initial frame, its corresponding segmentation map and the sparse motion vectors encoding the input provided by the user. It outputs a plausible video sequence starting from the given frame and with a motion that is consistent with user input. Notably, our proposed deep architecture incorporates a Graph Convolution Network (GCN) modelling the movements of all the objects in the scene in a holistic manner and effectively combining the sparse user motion information and image features. Experimental results show that C2M outperforms existing methods on two publicly available datasets, thus demonstrating the effectiveness of our GCN framework at modelling object interactions. The source code is publicly available at https://github.com/PierfrancescoArdino/C2M.



### Towards Vivid and Diverse Image Colorization with Generative Color Prior
- **Arxiv ID**: http://arxiv.org/abs/2108.08826v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08826v2)
- **Published**: 2021-08-19 17:49:21+00:00
- **Updated**: 2022-08-08 13:41:17+00:00
- **Authors**: Yanze Wu, Xintao Wang, Yu Li, Honglun Zhang, Xun Zhao, Ying Shan
- **Comment**: ICCV 2021. Codes are available at
  https://github.com/ToTheBeginning/GCP-Colorization
- **Journal**: None
- **Summary**: Colorization has attracted increasing interest in recent years. Classic reference-based methods usually rely on external color images for plausible results. A large image database or online search engine is inevitably required for retrieving such exemplars. Recent deep-learning-based methods could automatically colorize images at a low cost. However, unsatisfactory artifacts and incoherent colors are always accompanied. In this work, we propose GCP-Colorization that leverages the rich and diverse color priors encapsulated in a pretrained Generative Adversarial Networks (GAN) for automatic colorization. Specifically, we first "retrieve" matched features (similar to exemplars) via a GAN encoder and then incorporate these features into the colorization process with feature modulations. Thanks to the powerful generative color prior (GCP) and delicate designs, our GCP-Colorization could produce vivid colors with a single forward pass. Moreover, it is highly convenient to obtain diverse results by modifying GAN latent codes. GCP-Colorization also inherits the merit of interpretable controls of GANs and could attain controllable and smooth transitions by walking through GAN latent space. Extensive experiments and user studies demonstrate that GCP-Colorization achieves superior performance than previous works. Codes are available at https://github.com/ToTheBeginning/GCP-Colorization.



### ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2108.08827v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08827v1)
- **Published**: 2021-08-19 17:50:07+00:00
- **Updated**: 2021-08-19 17:50:07+00:00
- **Authors**: Patrick Esser, Robin Rombach, Andreas Blattmann, Björn Ommer
- **Comment**: None
- **Journal**: None
- **Summary**: Autoregressive models and their sequential factorization of the data likelihood have recently demonstrated great potential for image representation and synthesis. Nevertheless, they incorporate image context in a linear 1D order by attending only to previously synthesized image patches above or to the left. Not only is this unidirectional, sequential bias of attention unnatural for images as it disregards large parts of a scene until synthesis is almost complete. It also processes the entire image on a single scale, thus ignoring more global contextual information up to the gist of the entire scene. As a remedy we incorporate a coarse-to-fine hierarchy of context by combining the autoregressive formulation with a multinomial diffusion process: Whereas a multistage diffusion process successively removes information to coarsen an image, we train a (short) Markov chain to invert this process. In each stage, the resulting autoregressive ImageBART model progressively incorporates context from previous stages in a coarse-to-fine manner. Experiments show greatly improved image modification capabilities over autoregressive models while also providing high-fidelity image generation, both of which are enabled through efficient training in a compressed latent space. Specifically, our approach can take unrestricted, user-provided masks into account to perform local image editing. Thus, in contrast to pure autoregressive models, it can solve free-form image inpainting and, in the case of conditional models, local, text-guided image modification without requiring mask-specific training.



### Fine-grained Semantics-aware Representation Enhancement for Self-supervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2108.08829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08829v1)
- **Published**: 2021-08-19 17:50:51+00:00
- **Updated**: 2021-08-19 17:50:51+00:00
- **Authors**: Hyunyoung Jung, Eunhyeok Park, Sungjoo Yoo
- **Comment**: ICCV 2021 (Oral)
- **Journal**: None
- **Summary**: Self-supervised monocular depth estimation has been widely studied, owing to its practical importance and recent promising improvements. However, most works suffer from limited supervision of photometric consistency, especially in weak texture regions and at object boundaries. To overcome this weakness, we propose novel ideas to improve self-supervised monocular depth estimation by leveraging cross-domain information, especially scene semantics. We focus on incorporating implicit semantic knowledge into geometric representation enhancement and suggest two ideas: a metric learning approach that exploits the semantics-guided local geometry to optimize intermediate depth representations and a novel feature fusion module that judiciously utilizes cross-modality between two heterogeneous feature representations. We comprehensively evaluate our methods on the KITTI dataset and demonstrate that our method outperforms state-of-the-art methods. The source code is available at https://github.com/hyBlue/FSRE-Depth.



### Multi-Object Tracking with Hallucinated and Unlabeled Videos
- **Arxiv ID**: http://arxiv.org/abs/2108.08836v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08836v1)
- **Published**: 2021-08-19 17:57:29+00:00
- **Updated**: 2021-08-19 17:57:29+00:00
- **Authors**: Daniel McKee, Bing Shuai, Andrew Berneshawi, Manchen Wang, Davide Modolo, Svetlana Lazebnik, Joseph Tighe
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore learning end-to-end deep neural trackers without tracking annotations. This is important as large-scale training data is essential for training deep neural trackers while tracking annotations are expensive to acquire. In place of tracking annotations, we first hallucinate videos from images with bounding box annotations using zoom-in/out motion transformations to obtain free tracking labels. We add video simulation augmentations to create a diverse tracking dataset, albeit with simple motion. Next, to tackle harder tracking cases, we mine hard examples across an unlabeled pool of real videos with a tracker trained on our hallucinated video data. For hard example mining, we propose an optimization-based connecting process to first identify and then rectify hard examples from the pool of unlabeled videos. Finally, we train our tracker jointly on hallucinated data and mined hard video examples. Our weakly supervised tracker achieves state-of-the-art performance on the MOT17 and TAO-person datasets. On MOT17, we further demonstrate that the combination of our self-generated data and the existing manually-annotated data leads to additional improvements.



### PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers
- **Arxiv ID**: http://arxiv.org/abs/2108.08839v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08839v1)
- **Published**: 2021-08-19 17:58:56+00:00
- **Updated**: 2021-08-19 17:58:56+00:00
- **Authors**: Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, Jie Zhou
- **Comment**: Accepted to ICCV 2021 (Oral Presentation)
- **Journal**: None
- **Summary**: Point clouds captured in real-world applications are often incomplete due to the limited sensor resolution, single viewpoint, and occlusion. Therefore, recovering the complete point clouds from partial ones becomes an indispensable task in many practical applications. In this paper, we present a new method that reformulates point cloud completion as a set-to-set translation problem and design a new model, called PoinTr that adopts a transformer encoder-decoder architecture for point cloud completion. By representing the point cloud as a set of unordered groups of points with position embeddings, we convert the point cloud to a sequence of point proxies and employ the transformers for point cloud generation. To facilitate transformers to better leverage the inductive bias about 3D geometric structures of point clouds, we further devise a geometry-aware block that models the local geometric relationships explicitly. The migration of transformers enables our model to better learn structural knowledge and preserve detailed information for point cloud completion. Furthermore, we propose two more challenging benchmarks with more diverse incomplete point clouds that can better reflect the real-world scenarios to promote future research. Experimental results show that our method outperforms state-of-the-art methods by a large margin on both the new benchmarks and the existing ones. Code is available at https://github.com/yuxumin/PoinTr



### Graph-to-3D: End-to-End Generation and Manipulation of 3D Scenes Using Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/2108.08841v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08841v1)
- **Published**: 2021-08-19 17:59:07+00:00
- **Updated**: 2021-08-19 17:59:07+00:00
- **Authors**: Helisa Dhamo, Fabian Manhardt, Nassir Navab, Federico Tombari
- **Comment**: accepted to ICCV 2021
- **Journal**: None
- **Summary**: Controllable scene synthesis consists of generating 3D information that satisfy underlying specifications. Thereby, these specifications should be abstract, i.e. allowing easy user interaction, whilst providing enough interface for detailed control. Scene graphs are representations of a scene, composed of objects (nodes) and inter-object relationships (edges), proven to be particularly suited for this task, as they allow for semantic control on the generated content. Previous works tackling this task often rely on synthetic data, and retrieve object meshes, which naturally limits the generation capabilities. To circumvent this issue, we instead propose the first work that directly generates shapes from a scene graph in an end-to-end manner. In addition, we show that the same model supports scene modification, using the respective scene graph as interface. Leveraging Graph Convolutional Networks (GCN) we train a variational Auto-Encoder on top of the object and edge categories, as well as 3D shapes and scene layouts, allowing latter sampling of new scenes and shapes.



### Gravity-Aware Monocular 3D Human-Object Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2108.08844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08844v1)
- **Published**: 2021-08-19 17:59:57+00:00
- **Updated**: 2021-08-19 17:59:57+00:00
- **Authors**: Rishabh Dabral, Soshi Shimada, Arjun Jain, Christian Theobalt, Vladislav Golyanik
- **Comment**: 12 pages, six figures, five tables; project webpage:
  http://4dqv.mpi-inf.mpg.de/GraviCap/
- **Journal**: International Conference on Computer Vision (ICCV) 2021
- **Summary**: This paper proposes GraviCap, i.e., a new approach for joint markerless 3D human motion capture and object trajectory estimation from monocular RGB videos. We focus on scenes with objects partially observed during a free flight. In contrast to existing monocular methods, we can recover scale, object trajectories as well as human bone lengths in meters and the ground plane's orientation, thanks to the awareness of the gravity constraining object motions. Our objective function is parametrised by the object's initial velocity and position, gravity direction and focal length, and jointly optimised for one or several free flight episodes. The proposed human-object interaction constraints ensure geometric consistency of the 3D reconstructions and improved physical plausibility of human poses compared to the unconstrained case. We evaluate GraviCap on a new dataset with ground-truth annotations for persons and different objects undergoing free flights. In the experiments, our approach achieves state-of-the-art accuracy in 3D human motion capture on various metrics. We urge the reader to watch our supplementary video. Both the source code and the dataset are released; see http://4dqv.mpi-inf.mpg.de/GraviCap/.



### Towards A Fairer Landmark Recognition Dataset
- **Arxiv ID**: http://arxiv.org/abs/2108.08874v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08874v2)
- **Published**: 2021-08-19 18:42:22+00:00
- **Updated**: 2022-06-06 15:36:36+00:00
- **Authors**: Zu Kim, André Araujo, Bingyi Cao, Cam Askew, Jack Sim, Mike Green, N'Mah Fodiatu Yilla, Tobias Weyand
- **Comment**: Please cite the full detailed version of the paper instead: Improving
  Fairness in Large-Scale Object Recognition by CrowdSourced Demographic
  Information arXiv:2206.01326
- **Journal**: None
- **Summary**: We introduce a new landmark recognition dataset, which is created with a focus on fair worldwide representation. While previous work proposes to collect as many images as possible from web repositories, we instead argue that such approaches can lead to biased data. To create a more comprehensive and equitable dataset, we start by defining the fair relevance of a landmark to the world population. These relevances are estimated by combining anonymized Google Maps user contribution statistics with the contributors' demographic information. We present a stratification approach and analysis which leads to a much fairer coverage of the world, compared to existing datasets. The resulting datasets are used to evaluate computer vision models as part of the the Google Landmark Recognition and RetrievalChallenges 2021.



### Deep Learning-based Spacecraft Relative Navigation Methods: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2108.08876v2
- **DOI**: 10.1016/j.actaastro.2021.10.025
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2108.08876v2)
- **Published**: 2021-08-19 18:54:19+00:00
- **Updated**: 2021-10-05 09:31:08+00:00
- **Authors**: Jianing Song, Duarte Rondao, Nabil Aouf
- **Comment**: 41 pages; 17 figures; Submitted to Acta Astronautica, under review
- **Journal**: None
- **Summary**: Autonomous spacecraft relative navigation technology has been planned for and applied to many famous space missions. The development of on-board electronics systems has enabled the use of vision-based and LiDAR-based methods to achieve better performances. Meanwhile, deep learning has reached great success in different areas, especially in computer vision, which has also attracted the attention of space researchers. However, spacecraft navigation differs from ground tasks due to high reliability requirements but lack of large datasets. This survey aims to systematically investigate the current deep learning-based autonomous spacecraft relative navigation methods, focusing on concrete orbital applications such as spacecraft rendezvous and landing on small bodies or the Moon. The fundamental characteristics, primary motivations, and contributions of deep learning-based relative navigation algorithms are first summarised from three perspectives of spacecraft rendezvous, asteroid exploration, and terrain navigation. Furthermore, popular visual tracking benchmarks and their respective properties are compared and summarised. Finally, potential applications are discussed, along with expected impediments.



### Signal Injection Attacks against CCD Image Sensors
- **Arxiv ID**: http://arxiv.org/abs/2108.08881v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08881v2)
- **Published**: 2021-08-19 19:05:28+00:00
- **Updated**: 2021-12-13 18:09:37+00:00
- **Authors**: Sebastian Köhler, Richard Baker, Ivan Martinovic
- **Comment**: None
- **Journal**: None
- **Summary**: Since cameras have become a crucial part in many safety-critical systems and applications, such as autonomous vehicles and surveillance, a large body of academic and non-academic work has shown attacks against their main component - the image sensor. However, these attacks are limited to coarse-grained and often suspicious injections because light is used as an attack vector. Furthermore, due to the nature of optical attacks, they require the line-of-sight between the adversary and the target camera.   In this paper, we present a novel post-transducer signal injection attack against CCD image sensors, as they are used in professional, scientific, and even military settings. We show how electromagnetic emanation can be used to manipulate the image information captured by a CCD image sensor with the granularity down to the brightness of individual pixels. We study the feasibility of our attack and then demonstrate its effects in the scenario of automatic barcode scanning. Our results indicate that the injected distortion can disrupt automated vision-based intelligent systems.



### A Deep Learning Based Automatic Defect Analysis Framework for In-situ TEM Ion Irradiations
- **Arxiv ID**: http://arxiv.org/abs/2108.08882v1
- **DOI**: 10.1016/j.commatsci.2021.110560
- **Categories**: **cs.CV**, cond-mat.mtrl-sci
- **Links**: [PDF](http://arxiv.org/pdf/2108.08882v1)
- **Published**: 2021-08-19 19:15:44+00:00
- **Updated**: 2021-08-19 19:15:44+00:00
- **Authors**: Mingren Shen, Guanzhao Li, Dongxia Wu, Yudai Yaguchi, Jack C. Haley, Kevin G. Field, Dane Morgan
- **Comment**: None
- **Journal**: None
- **Summary**: Videos captured using Transmission Electron Microscopy (TEM) can encode details regarding the morphological and temporal evolution of a material by taking snapshots of the microstructure sequentially. However, manual analysis of such video is tedious, error-prone, unreliable, and prohibitively time-consuming if one wishes to analyze a significant fraction of frames for even videos of modest length. In this work, we developed an automated TEM video analysis system for microstructural features based on the advanced object detection model called YOLO and tested the system on an in-situ ion irradiation TEM video of dislocation loops formed in a FeCrAl alloy. The system provides analysis of features observed in TEM including both static and dynamic properties using the YOLO-based defect detection module coupled to a geometry analysis module and a dynamic tracking module. Results show that the system can achieve human comparable performance with an F1 score of 0.89 for fast, consistent, and scalable frame-level defect analysis. This result is obtained on a real but exceptionally clean and stable data set and more challenging data sets may not achieve this performance. The dynamic tracking also enabled evaluation of individual defect evolution like per defect growth rate at a fidelity never before achieved using common human analysis methods. Our work shows that automatically detecting and tracking interesting microstructures and properties contained in TEM videos is viable and opens new doors for evaluating materials dynamics.



### Multi defect detection and analysis of electron microscopy images with deep learning
- **Arxiv ID**: http://arxiv.org/abs/2108.08883v1
- **DOI**: 10.1016/j.commatsci.2021.110576
- **Categories**: **cs.CV**, cond-mat.mtrl-sci
- **Links**: [PDF](http://arxiv.org/pdf/2108.08883v1)
- **Published**: 2021-08-19 19:16:24+00:00
- **Updated**: 2021-08-19 19:16:24+00:00
- **Authors**: Mingren Shen, Guanzhao Li, Dongxia Wu, Yuhan Liu, Jacob Greaves, Wei Hao, Nathaniel J. Krakauer, Leah Krudy, Jacob Perez, Varun Sreenivasan, Bryan Sanchez, Oigimer Torres, Wei Li, Kevin Field, Dane Morgan
- **Comment**: None
- **Journal**: None
- **Summary**: Electron microscopy is widely used to explore defects in crystal structures, but human detecting of defects is often time-consuming, error-prone, and unreliable, and is not scalable to large numbers of images or real-time analysis. In this work, we discuss the application of machine learning approaches to find the location and geometry of different defect clusters in irradiated steels. We show that a deep learning based Faster R-CNN analysis system has a performance comparable to human analysis with relatively small training data sets. This study proves the promising ability to apply deep learning to assist the development of automated microscopy data analysis even when multiple features are present and paves the way for fast, scalable, and reliable analysis systems for massive amounts of modern electron microscopy data.



### Challenges and Solutions for Utilizing Earth Observations in the "Big Data" era
- **Arxiv ID**: http://arxiv.org/abs/2108.08886v1
- **DOI**: 10.5281/zenodo.2391937
- **Categories**: **cs.CY**, cs.CV, eess.IV, 86Axx, 68Pxx, 93Cxx, 94Axx, 97Pxx, 68Qxx, 68Uxx, 68Txx, 91Cxx,
  85-XX, 86-XX,, C.3; E.1; E.2; H.1; H.3
- **Links**: [PDF](http://arxiv.org/pdf/2108.08886v1)
- **Published**: 2021-08-19 19:24:32+00:00
- **Updated**: 2021-08-19 19:24:32+00:00
- **Authors**: Lachezar Filchev, Lyubka Pashova, Vasil Kolev, Stuart Frye
- **Comment**: 6 pages, BigSkyEarth conference: AstroGeoInformatics, Tenerife,
  Spain, December 17-19, 2018
- **Journal**: None
- **Summary**: The ever-growing need of data preservation and their systematic analysis contributing to sustainable development of the society spurred in the past decade,numerous Big Data projects and initiatives are focusing on the Earth Observation (EO). The number of Big Data EO applications has grown extremely worldwide almost simultaneously with other scientific and technological areas of the human knowledge due to the revolutionary technological progress in the space and information technology sciences. The substantial contribution to this development are the space programs of the renowned space agencies, such as NASA, ESA,Roskosmos, JAXA, DLR, INPE, ISRO, CNES etc. A snap-shot of the current Big Data sets from available satellite missions covering the Bulgarian territory is also presented. This short overview of the geoscience Big Data collection with a focus on EO will emphasize to the multiple Vs of EO in order to provide a snapshot on the current state-of-the-art in EO data preservation and manipulation. Main modern approaches for compressing, clustering and modelling EO in the geoinformation science for Big Data analysis, interpretation and visualization for a variety of applications are outlined. Special attention is paid to the contemporary EO data modelling and visualization systems.



### Neural TMDlayer: Modeling Instantaneous flow of features via SDE Generators
- **Arxiv ID**: http://arxiv.org/abs/2108.08891v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08891v1)
- **Published**: 2021-08-19 19:54:04+00:00
- **Updated**: 2021-08-19 19:54:04+00:00
- **Authors**: Zihang Meng, Vikas Singh, Sathya N. Ravi
- **Comment**: None
- **Journal**: None
- **Summary**: We study how stochastic differential equation (SDE) based ideas can inspire new modifications to existing algorithms for a set of problems in computer vision. Loosely speaking, our formulation is related to both explicit and implicit strategies for data augmentation and group equivariance, but is derived from new results in the SDE literature on estimating infinitesimal generators of a class of stochastic processes. If and when there is nominal agreement between the needs of an application/task and the inherent properties and behavior of the types of processes that we can efficiently handle, we obtain a very simple and efficient plug-in layer that can be incorporated within any existing network architecture, with minimal modification and only a few additional parameters. We show promising experiments on a number of vision tasks including few shot learning, point cloud transformers and deep variational segmentation obtaining efficiency or performance improvements.



### Segmentation of Lungs COVID Infected Regions by Attention Mechanism and Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2108.08895v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08895v2)
- **Published**: 2021-08-19 20:15:47+00:00
- **Updated**: 2021-08-25 20:54:21+00:00
- **Authors**: Parham Yazdekhasty, Ali Zindari, Zahra Nabizadeh-ShahreBabak, Pejman Khadivi, Nader Karimi, Shadrokh Samavi
- **Comment**: Figures 2 and 3 by mistake were similar. In this version, they are
  corrected
- **Journal**: None
- **Summary**: Coronavirus has caused hundreds of thousands of deaths. Fatalities could decrease if every patient could get suitable treatment by the healthcare system. Machine learning, especially computer vision methods based on deep learning, can help healthcare professionals diagnose and treat COVID-19 infected cases more efficiently. Hence, infected patients can get better service from the healthcare system and decrease the number of deaths caused by the coronavirus. This research proposes a method for segmenting infected lung regions in a CT image. For this purpose, a convolutional neural network with an attention mechanism is used to detect infected areas with complex patterns. Attention blocks improve the segmentation accuracy by focusing on informative parts of the image. Furthermore, a generative adversarial network generates synthetic images for data augmentation and expansion of small available datasets. Experimental results show the superiority of the proposed method compared to some existing procedures.



### Detection of Illicit Drug Trafficking Events on Instagram: A Deep Multimodal Multilabel Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2108.08920v2
- **DOI**: 10.1145/3459637.3481908
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.08920v2)
- **Published**: 2021-08-19 21:16:21+00:00
- **Updated**: 2021-08-23 02:13:56+00:00
- **Authors**: Chuanbo Hu, Minglei Yin, Bin Liu, Xin Li, Yanfang Ye
- **Comment**: Accepted by CIKM 2021
- **Journal**: None
- **Summary**: Social media such as Instagram and Twitter have become important platforms for marketing and selling illicit drugs. Detection of online illicit drug trafficking has become critical to combat the online trade of illicit drugs. However, the legal status often varies spatially and temporally; even for the same drug, federal and state legislation can have different regulations about its legality. Meanwhile, more drug trafficking events are disguised as a novel form of advertising commenting leading to information heterogeneity. Accordingly, accurate detection of illicit drug trafficking events (IDTEs) from social media has become even more challenging. In this work, we conduct the first systematic study on fine-grained detection of IDTEs on Instagram. We propose to take a deep multimodal multilabel learning (DMML) approach to detect IDTEs and demonstrate its effectiveness on a newly constructed dataset called multimodal IDTE(MM-IDTE). Specifically, our model takes text and image data as the input and combines multimodal information to predict multiple labels of illicit drugs. Inspired by the success of BERT, we have developed a self-supervised multimodal bidirectional transformer by jointly fine-tuning pretrained text and image encoders. We have constructed a large-scale dataset MM-IDTE with manually annotated multiple drug labels to support fine-grained detection of illicit drugs. Extensive experimental results on the MM-IDTE dataset show that the proposed DMML methodology can accurately detect IDTEs even in the presence of special characters and style changes attempting to evade detection.



### Controlled GAN-Based Creature Synthesis via a Challenging Game Art Dataset -- Addressing the Noise-Latent Trade-Off
- **Arxiv ID**: http://arxiv.org/abs/2108.08922v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08922v2)
- **Published**: 2021-08-19 21:31:20+00:00
- **Updated**: 2021-10-20 20:16:25+00:00
- **Authors**: Vaibhav Vavilala, David Forsyth
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: The state-of-the-art StyleGAN2 network supports powerful methods to create and edit art, including generating random images, finding images "like" some query, and modifying content or style. Further, recent advancements enable training with small datasets. We apply these methods to synthesize card art, by training on a novel Yu-Gi-Oh dataset. While noise inputs to StyleGAN2 are essential for good synthesis, we find that coarse-scale noise interferes with latent variables on this dataset because both control long-scale image effects. We observe over-aggressive variation in art with changes in noise and weak content control via latent variable edits. Here, we demonstrate that training a modified StyleGAN2, where coarse-scale noise is suppressed, removes these unwanted effects. We obtain a superior FID; changes in noise result in local exploration of style; and identity control is markedly improved. These results and analysis lead towards a GAN-assisted art synthesis tool for digital artists of all skill levels, which can be used in film, games, or any creative industry for artistic ideation.



### CenterPoly: real-time instance segmentation using bounding polygons
- **Arxiv ID**: http://arxiv.org/abs/2108.08923v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08923v2)
- **Published**: 2021-08-19 21:31:30+00:00
- **Updated**: 2021-09-15 14:01:32+00:00
- **Authors**: Hughes Perreault, Guillaume-Alexandre Bilodeau, Nicolas Saunier, Maguelonne Héritier
- **Comment**: Accepted to the 2nd Autonomous Vehicle Vision Workshop (AVVision)
- **Journal**: None
- **Summary**: We present a novel method, called CenterPoly, for real-time instance segmentation using bounding polygons. We apply it to detect road users in dense urban environments, making it suitable for applications in intelligent transportation systems like automated vehicles. CenterPoly detects objects by their center keypoint while predicting a fixed number of polygon vertices for each object, thus performing detection and segmentation in parallel. Most of the network parameters are shared by the network heads, making it fast and lightweight enough to run at real-time speed. To properly convert mask ground-truth to polygon ground-truth, we designed a vertex selection strategy to facilitate the learning of the polygons. Additionally, to better segment overlapping objects in dense urban scenes, we also train a relative depth branch to determine which instances are closer and which are further, using available weak annotations. We propose several models with different backbones to show the possible speed / accuracy trade-offs. The models were trained and evaluated on Cityscapes, KITTI and IDD and the results are reported on their public benchmark, which are state-of-the-art at real-time speeds. Code is available at https://github.com/hu64/CenterPoly



### Augmenting Implicit Neural Shape Representations with Explicit Deformation Fields
- **Arxiv ID**: http://arxiv.org/abs/2108.08931v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.08931v1)
- **Published**: 2021-08-19 22:07:08+00:00
- **Updated**: 2021-08-19 22:07:08+00:00
- **Authors**: Matan Atzmon, David Novotny, Andrea Vedaldi, Yaron Lipman
- **Comment**: None
- **Journal**: None
- **Summary**: Implicit neural representation is a recent approach to learn shape collections as zero level-sets of neural networks, where each shape is represented by a latent code. So far, the focus has been shape reconstruction, while shape generalization was mostly left to generic encoder-decoder or auto-decoder regularization.   In this paper we advocate deformation-aware regularization for implicit neural representations, aiming at producing plausible deformations as latent code changes. The challenge is that implicit representations do not capture correspondences between different shapes, which makes it difficult to represent and regularize their deformations. Thus, we propose to pair the implicit representation of the shapes with an explicit, piecewise linear deformation field, learned as an auxiliary function. We demonstrate that, by regularizing these deformation fields, we can encourage the implicit neural representation to induce natural deformations in the learned shape space, such as as-rigid-as-possible deformations.



### PatchMatch-RL: Deep MVS with Pixelwise Depth, Normal, and Visibility
- **Arxiv ID**: http://arxiv.org/abs/2108.08943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.08943v1)
- **Published**: 2021-08-19 23:14:48+00:00
- **Updated**: 2021-08-19 23:14:48+00:00
- **Authors**: Jae Yong Lee, Joseph DeGol, Chuhang Zou, Derek Hoiem
- **Comment**: Accepted to ICCV 2021 for oral presentation
- **Journal**: None
- **Summary**: Recent learning-based multi-view stereo (MVS) methods show excellent performance with dense cameras and small depth ranges. However, non-learning based approaches still outperform for scenes with large depth ranges and sparser wide-baseline views, in part due to their PatchMatch optimization over pixelwise estimates of depth, normals, and visibility. In this paper, we propose an end-to-end trainable PatchMatch-based MVS approach that combines advantages of trainable costs and regularizations with pixelwise estimates. To overcome the challenge of the non-differentiable PatchMatch optimization that involves iterative sampling and hard decisions, we use reinforcement learning to minimize expected photometric cost and maximize likelihood of ground truth depth and normals. We incorporate normal estimation by using dilated patch kernels, and propose a recurrent cost regularization that applies beyond frontal plane-sweep algorithms to our pixelwise depth/normal estimates. We evaluate our method on widely used MVS benchmarks, ETH3D and Tanks and Temples (TnT), and compare to other state of the art learning based MVS models. On ETH3D, our method outperforms other recent learning-based approaches and performs comparably on advanced TnT.



