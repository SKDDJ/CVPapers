# Arxiv Papers in cs.CV on 2021-08-24
### Spatio-temporal-spectral-angular observation model that integrates observations from UAV and mobile mapping vehicle for better urban mapping
- **Arxiv ID**: http://arxiv.org/abs/2109.00900v2
- **DOI**: 10.1080/10095020.2021.1961567
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00900v2)
- **Published**: 2021-08-24 02:58:12+00:00
- **Updated**: 2021-09-05 14:23:16+00:00
- **Authors**: Zhenfeng Shao, Gui Cheng, Deren Li, Xiao Huang, Zhipeng Lu, Jian Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In a complex urban scene, observation from a single sensor unavoidably leads to voids in observations, failing to describe urban objects in a comprehensive manner. In this paper, we propose a spatio-temporal-spectral-angular observation model to integrate observations from UAV and mobile mapping vehicle platform, realizing a joint, coordinated observation operation from both air and ground. We develop a multi-source remote sensing data acquisition system to effectively acquire multi-angle data of complex urban scenes. Multi-source data fusion solves the missing data problem caused by occlusion and achieves accurate, rapid, and complete collection of holographic spatial and temporal information in complex urban scenes. We carried out an experiment on Baisha Town, Chongqing, China and obtained multi-sensor, multi-angle data from UAV and mobile mapping vehicle. We first extracted the point cloud from UAV and then integrated the UAV and mobile mapping vehicle point cloud. The integrated results combined both the characteristic of UAV and mobile mapping vehicle point cloud, confirming the practicability of the proposed joint data acquisition platform and the effectiveness of spatio-temporal-spectral-angular observation model. Compared with the observation from UAV or mobile mapping vehicle alone, the integrated system provides an effective data acquisition solution towards comprehensive urban monitoring.



### ParamCrop: Parametric Cubic Cropping for Video Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.10501v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10501v3)
- **Published**: 2021-08-24 03:18:12+00:00
- **Updated**: 2021-11-23 06:59:10+00:00
- **Authors**: Zhiwu Qing, Ziyuan Huang, Shiwei Zhang, Mingqian Tang, Changxin Gao, Marcelo H. Ang Jr, Rong Jin, Nong Sang
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: The central idea of contrastive learning is to discriminate between different instances and force different views from the same instance to share the same representation. To avoid trivial solutions, augmentation plays an important role in generating different views, among which random cropping is shown to be effective for the model to learn a generalized and robust representation. Commonly used random crop operation keeps the distribution of the difference between two views unchanged along the training process. In this work, we show that adaptively controlling the disparity between two augmented views along the training process enhances the quality of the learned representation. Specifically, we present a parametric cubic cropping operation, ParamCrop, for video contrastive learning, which automatically crops a 3D cubic by differentiable 3D affine transformations. ParamCrop is trained simultaneously with the video backbone using an adversarial objective and learns an optimal cropping strategy from the data. The visualizations show that ParamCrop adaptively controls the center distance and the IoU between two augmented views, and the learned change in the disparity along the training process is beneficial to learning a strong representation. Extensive ablation studies demonstrate the effectiveness of the proposed ParamCrop on multiple contrastive learning frameworks and video backbones. Codes and models will be available.



### Small Object Detection Based on Modified FSSD and Model Compression
- **Arxiv ID**: http://arxiv.org/abs/2108.10503v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.10503v1)
- **Published**: 2021-08-24 03:20:32+00:00
- **Updated**: 2021-08-24 03:20:32+00:00
- **Authors**: Qingcai Wang, Hao Zhang, Xianggong Hong, Qinqin Zhou
- **Comment**: None
- **Journal**: IEEE ICSIP,2021
- **Summary**: Small objects have relatively low resolution, the unobvious visual features which are difficult to be extracted, so the existing object detection methods cannot effectively detect small objects, and the detection speed and stability are poor. Thus, this paper proposes a small object detection algorithm based on FSSD, meanwhile, in order to reduce the computational cost and storage space, pruning is carried out to achieve model compression. Firstly, the semantic information contained in the features of different layers can be used to detect different scale objects, and the feature fusion method is improved to obtain more information beneficial to small objects; secondly, batch normalization layer is introduced to accelerate the training of neural network and make the model sparse; finally, the model is pruned by scaling factor to get the corresponding compressed model. The experimental results show that the average accuracy (mAP) of the algorithm can reach 80.4% on PASCAL VOC and the speed is 59.5 FPS on GTX1080ti. After pruning, the compressed model can reach 79.9% mAP, and 79.5 FPS in detection speed. On MS COCO, the best detection accuracy (APs) is 12.1%, and the overall detection accuracy is 49.8% AP when IoU is 0.5. The algorithm can not only improve the detection accuracy of small objects, but also greatly improves the detection speed, which reaches a balance between speed and accuracy.



### Real-Time Monocular Human Depth Estimation and Segmentation on Embedded Systems
- **Arxiv ID**: http://arxiv.org/abs/2108.10506v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.10506v1)
- **Published**: 2021-08-24 03:26:08+00:00
- **Updated**: 2021-08-24 03:26:08+00:00
- **Authors**: Shan An, Fangru Zhou, Mei Yang, Haogang Zhu, Changhong Fu, Konstantinos A. Tsintotas
- **Comment**: Accepted by IROS 2021
- **Journal**: None
- **Summary**: Estimating a scene's depth to achieve collision avoidance against moving pedestrians is a crucial and fundamental problem in the robotic field. This paper proposes a novel, low complexity network architecture for fast and accurate human depth estimation and segmentation in indoor environments, aiming to applications for resource-constrained platforms (including battery-powered aerial, micro-aerial, and ground vehicles) with a monocular camera being the primary perception module. Following the encoder-decoder structure, the proposed framework consists of two branches, one for depth prediction and another for semantic segmentation. Moreover, network structure optimization is employed to improve its forward inference speed. Exhaustive experiments on three self-generated datasets prove our pipeline's capability to execute in real-time, achieving higher frame rates than contemporary state-of-the-art frameworks (114.6 frames per second on an NVIDIA Jetson Nano GPU with TensorRT) while maintaining comparable accuracy.



### ARShoe: Real-Time Augmented Reality Shoe Try-on System on Smartphones
- **Arxiv ID**: http://arxiv.org/abs/2108.10515v1
- **DOI**: 10.1145/3474085.3481537
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10515v1)
- **Published**: 2021-08-24 03:54:45+00:00
- **Updated**: 2021-08-24 03:54:45+00:00
- **Authors**: Shan An, Guangfu Che, Jinghao Guo, Haogang Zhu, Junjie Ye, Fangru Zhou, Zhaoqi Zhu, Dong Wei, Aishan Liu, Wei Zhang
- **Comment**: Accepted by ACM Multimedia 2021
- **Journal**: None
- **Summary**: Virtual try-on technology enables users to try various fashion items using augmented reality and provides a convenient online shopping experience. However, most previous works focus on the virtual try-on for clothes while neglecting that for shoes, which is also a promising task. To this concern, this work proposes a real-time augmented reality virtual shoe try-on system for smartphones, namely ARShoe. Specifically, ARShoe adopts a novel multi-branch network to realize pose estimation and segmentation simultaneously. A solution to generate realistic 3D shoe model occlusion during the try-on process is presented. To achieve a smooth and stable try-on effect, this work further develop a novel stabilization method. Moreover, for training and evaluation, we construct the very first large-scale foot benchmark with multiple virtual shoe try-on task-related labels annotated. Exhaustive experiments on our newly constructed benchmark demonstrate the satisfying performance of ARShoe. Practical tests on common smartphones validate the real-time performance and stabilization of the proposed approach.



### Improving Object Detection by Label Assignment Distillation
- **Arxiv ID**: http://arxiv.org/abs/2108.10520v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10520v3)
- **Published**: 2021-08-24 04:37:48+00:00
- **Updated**: 2021-10-19 19:34:06+00:00
- **Authors**: Chuong H. Nguyen, Thuy C. Nguyen, Tuan N. Tang, Nam L. H. Phan
- **Comment**: To appear in WACV 2022
- **Journal**: None
- **Summary**: Label assignment in object detection aims to assign targets, foreground or background, to sampled regions in an image. Unlike labeling for image classification, this problem is not well defined due to the object's bounding box. In this paper, we investigate the problem from a perspective of distillation, hence we call Label Assignment Distillation (LAD). Our initial motivation is very simple, we use a teacher network to generate labels for the student. This can be achieved in two ways: either using the teacher's prediction as the direct targets (soft label), or through the hard labels dynamically assigned by the teacher (LAD). Our experiments reveal that: (i) LAD is more effective than soft-label, but they are complementary. (ii) Using LAD, a smaller teacher can also improve a larger student significantly, while soft-label can't. We then introduce Co-learning LAD, in which two networks simultaneously learn from scratch and the role of teacher and student are dynamically interchanged. Using PAA-ResNet50 as a teacher, our LAD techniques can improve detectors PAA-ResNet101 and PAA-ResNeXt101 to $46 \rm AP$ and $47.5\rm AP$ on the COCO test-dev set. With a stronger teacher PAA-SwinB, we improve the students PAA-ResNet50 to $43.7\rm AP$ by only 1x schedule training and standard setting, and PAA-ResNet101 to $47.9\rm AP$, significantly surpassing the current methods. Our source code and checkpoints are released at https://git.io/JrDZo.



### ShapeConv: Shape-aware Convolutional Layer for Indoor RGB-D Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.10528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10528v1)
- **Published**: 2021-08-24 05:36:16+00:00
- **Updated**: 2021-08-24 05:36:16+00:00
- **Authors**: Jinming Cao, Hanchao Leng, Dani Lischinski, Danny Cohen-Or, Changhe Tu, Yangyan Li
- **Comment**: ICCV2021
- **Journal**: None
- **Summary**: RGB-D semantic segmentation has attracted increasing attention over the past few years. Existing methods mostly employ homogeneous convolution operators to consume the RGB and depth features, ignoring their intrinsic differences. In fact, the RGB values capture the photometric appearance properties in the projected image space, while the depth feature encodes both the shape of a local geometry as well as the base (whereabout) of it in a larger context. Compared with the base, the shape probably is more inherent and has a stronger connection to the semantics, and thus is more critical for segmentation accuracy. Inspired by this observation, we introduce a Shape-aware Convolutional layer (ShapeConv) for processing the depth feature, where the depth feature is firstly decomposed into a shape-component and a base-component, next two learnable weights are introduced to cooperate with them independently, and finally a convolution is applied on the re-weighted combination of these two components. ShapeConv is model-agnostic and can be easily integrated into most CNNs to replace vanilla convolutional layers for semantic segmentation. Extensive experiments on three challenging indoor RGB-D semantic segmentation benchmarks, i.e., NYU-Dv2(-13,-40), SUN RGB-D, and SID, demonstrate the effectiveness of our ShapeConv when employing it over five popular architectures. Moreover, the performance of CNNs with ShapeConv is boosted without introducing any computation and memory increase in the inference phase. The reason is that the learnt weights for balancing the importance between the shape and base components in ShapeConv become constants in the inference phase, and thus can be fused into the following convolution, resulting in a network that is identical to one with vanilla convolutional layers.



### Unsupervised Depth Completion with Calibrated Backprojection Layers
- **Arxiv ID**: http://arxiv.org/abs/2108.10531v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.10531v2)
- **Published**: 2021-08-24 05:41:59+00:00
- **Updated**: 2021-10-10 21:56:55+00:00
- **Authors**: Alex Wong, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deep neural network architecture to infer dense depth from an image and a sparse point cloud. It is trained using a video stream and corresponding synchronized sparse point cloud, as obtained from a LIDAR or other range sensor, along with the intrinsic calibration parameters of the camera. At inference time, the calibration of the camera, which can be different than the one used for training, is fed as an input to the network along with the sparse point cloud and a single image. A Calibrated Backprojection Layer backprojects each pixel in the image to three-dimensional space using the calibration matrix and a depth feature descriptor. The resulting 3D positional encoding is concatenated with the image descriptor and the previous layer output to yield the input to the next layer of the encoder. A decoder, exploiting skip-connections, produces a dense depth map. The resulting Calibrated Backprojection Network, or KBNet, is trained without supervision by minimizing the photometric reprojection error. KBNet imputes missing depth value based on the training set, rather than on generic regularization. We test KBNet on public depth completion benchmarks, where it outperforms the state of the art by 30.5% indoor and 8.8% outdoor when the same camera is used for training and testing. When the test camera is different, the improvement reaches 62%. Code available at: https://github.com/alexklwong/calibrated-backprojection-network.



### Making Person Search Enjoy the Merits of Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2108.10536v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10536v2)
- **Published**: 2021-08-24 06:00:13+00:00
- **Updated**: 2021-11-14 05:31:19+00:00
- **Authors**: Chuang Liu, Hua Yang, Qin Zhou, Shibao Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Person search is an extended task of person re-identification (Re-ID). However, most existing one-step person search works have not studied how to employ existing advanced Re-ID models to boost the one-step person search performance due to the integration of person detection and Re-ID. To address this issue, we propose a faster and stronger one-step person search framework, the Teacher-guided Disentangling Networks (TDN), to make the one-step person search enjoy the merits of the existing Re-ID researches. The proposed TDN can significantly boost the person search performance by transferring the advanced person Re-ID knowledge to the person search model. In the proposed TDN, for better knowledge transfer from the Re-ID teacher model to the one-step person search model, we design a strong one-step person search base framework by partially disentangling the two subtasks. Besides, we propose a Knowledge Transfer Bridge module to bridge the scale gap caused by different input formats between the Re-ID model and one-step person search model. During testing, we further propose the Ranking with Context Persons strategy to exploit the context information in panoramic images for better retrieval. Experiments on two public person search datasets demonstrate the favorable performance of the proposed method.



### Joint Learning Architecture for Multiple Object Tracking and Trajectory Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2108.10543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10543v1)
- **Published**: 2021-08-24 06:53:01+00:00
- **Updated**: 2021-08-24 06:53:01+00:00
- **Authors**: Oluwafunmilola Kesa, Olly Styles, Victor Sanchez
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a joint learning architecture (JLA) for multiple object tracking (MOT) and trajectory forecasting in which the goal is to predict objects' current and future trajectories simultaneously. Motion prediction is widely used in several state of the art MOT methods to refine predictions in the form of bounding boxes. Typically, a Kalman Filter provides short-term estimations to help trackers correctly predict objects' locations in the current frame. However, the Kalman Filter-based approaches cannot predict non-linear trajectories. We propose to jointly train a tracking and trajectory forecasting model and use the predicted trajectory forecasts for short-term motion estimates in lieu of linear motion prediction methods such as the Kalman filter. We evaluate our JLA on the MOTChallenge benchmark. Evaluations result show that JLA performs better for short-term motion prediction and reduces ID switches by 33%, 31%, and 47% in the MOT16, MOT17, and MOT20 datasets, respectively, in comparison to FairMOT.



### StyleAugment: Learning Texture De-biased Representations by Style Augmentation without Pre-defined Textures
- **Arxiv ID**: http://arxiv.org/abs/2108.10549v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10549v1)
- **Published**: 2021-08-24 07:17:02+00:00
- **Updated**: 2021-08-24 07:17:02+00:00
- **Authors**: Sanghyuk Chun, Song Park
- **Comment**: 10 pages, 2 figures, 4 tables
- **Journal**: None
- **Summary**: Recent powerful vision classifiers are biased towards textures, while shape information is overlooked by the models. A simple attempt by augmenting training images using the artistic style transfer method, called Stylized ImageNet, can reduce the texture bias. However, Stylized ImageNet approach has two drawbacks in fidelity and diversity. First, the generated images show low image quality due to the significant semantic gap betweeen natural images and artistic paintings. Also, Stylized ImageNet training samples are pre-computed before training, resulting in showing the lack of diversity for each sample. We propose a StyleAugment by augmenting styles from the mini-batch. StyleAugment does not rely on the pre-defined style references, but generates augmented images on-the-fly by natural images in the mini-batch for the references. Hence, StyleAugment let the model observe abundant confounding cues for each image by on-the-fly the augmentation strategy, while the augmented images are more realistic than artistic style transferred images. We validate the effectiveness of StyleAugment in the ImageNet dataset with robustness benchmarks, such as texture de-biased accuracy, corruption robustness, natural adversarial samples, and occlusion robustness. StyleAugment shows better generalization performances than previous unsupervised de-biasing methods and state-of-the-art data augmentation methods in our experiments.



### A generative adversarial approach to facilitate archival-quality histopathologic diagnoses from frozen tissue sections
- **Arxiv ID**: http://arxiv.org/abs/2108.10550v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2108.10550v1)
- **Published**: 2021-08-24 07:20:53+00:00
- **Updated**: 2021-08-24 07:20:53+00:00
- **Authors**: Kianoush Falahkheirkhah, Tao Guo, Michael Hwang, Pheroze Tamboli, Christopher G Wood, Jose A Karam, Kanishka Sircar, Rohit Bhargava
- **Comment**: 24 pages, 6 figures, and 3 tables
- **Journal**: None
- **Summary**: In clinical diagnostics and research involving histopathology, formalin fixed paraffin embedded (FFPE) tissue is almost universally favored for its superb image quality. However, tissue processing time (more than 24 hours) can slow decision-making. In contrast, fresh frozen (FF) processing (less than 1 hour) can yield rapid information but diagnostic accuracy is suboptimal due to lack of clearing, morphologic deformation and more frequent artifacts. Here, we bridge this gap using artificial intelligence. We synthesize FFPE-like images ,virtual FFPE, from FF images using a generative adversarial network (GAN) from 98 paired kidney samples derived from 40 patients. Five board-certified pathologists evaluated the results in a blinded test. Image quality of the virtual FFPE data was assessed to be high and showed a close resemblance to real FFPE images. Clinical assessments of disease on the virtual FFPE images showed a higher inter-observer agreement compared to FF images. The nearly instantaneously generated virtual FFPE images can not only reduce time to information but can facilitate more precise diagnosis from routine FF images without extraneous costs and effort.



### Lossless Image Compression Using a Multi-Scale Progressive Statistical Model
- **Arxiv ID**: http://arxiv.org/abs/2108.10551v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.10551v1)
- **Published**: 2021-08-24 07:33:13+00:00
- **Updated**: 2021-08-24 07:33:13+00:00
- **Authors**: Honglei Zhang, Francesco Cricri, Hamed R. Tavakoli, Nannan Zou, Emre Aksu, Miska M. Hannuksela
- **Comment**: Accepted ACCV 2020
- **Journal**: None
- **Summary**: Lossless image compression is an important technique for image storage and transmission when information loss is not allowed. With the fast development of deep learning techniques, deep neural networks have been used in this field to achieve a higher compression rate. Methods based on pixel-wise autoregressive statistical models have shown good performance. However, the sequential processing way prevents these methods to be used in practice. Recently, multi-scale autoregressive models have been proposed to address this limitation. Multi-scale approaches can use parallel computing systems efficiently and build practical systems. Nevertheless, these approaches sacrifice compression performance in exchange for speed. In this paper, we propose a multi-scale progressive statistical model that takes advantage of the pixel-wise approach and the multi-scale approach. We developed a flexible mechanism where the processing order of the pixels can be adjusted easily. Our proposed method outperforms the state-of-the-art lossless image compression methods on two large benchmark datasets by a significant margin without degrading the inference speed dramatically.



### E-RAFT: Dense Optical Flow from Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/2108.10552v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10552v3)
- **Published**: 2021-08-24 07:39:08+00:00
- **Updated**: 2021-10-21 09:46:49+00:00
- **Authors**: Mathias Gehrig, Mario Millhäusler, Daniel Gehrig, Davide Scaramuzza
- **Comment**: International Conference on 3D Vision (3DV)
- **Journal**: None
- **Summary**: We propose to incorporate feature correlation and sequential processing into dense optical flow estimation from event cameras. Modern frame-based optical flow methods heavily rely on matching costs computed from feature correlation. In contrast, there exists no optical flow method for event cameras that explicitly computes matching costs. Instead, learning-based approaches using events usually resort to the U-Net architecture to estimate optical flow sparsely. Our key finding is that the introduction of correlation features significantly improves results compared to previous methods that solely rely on convolution layers. Compared to the state-of-the-art, our proposed approach computes dense optical flow and reduces the end-point error by 23% on MVSEC. Furthermore, we show that all existing optical flow methods developed so far for event cameras have been evaluated on datasets with very small displacement fields with a maximum flow magnitude of 10 pixels. Based on this observation, we introduce a new real-world dataset that exhibits displacement fields with magnitudes up to 210 pixels and 3 times higher camera resolution. Our proposed approach reduces the end-point error on this dataset by 66%.



### Auto-Parsing Network for Image Captioning and Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2108.10568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10568v1)
- **Published**: 2021-08-24 08:14:35+00:00
- **Updated**: 2021-08-24 08:14:35+00:00
- **Authors**: Xu Yang, Chongyang Gao, Hanwang Zhang, Jianfei Cai
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an Auto-Parsing Network (APN) to discover and exploit the input data's hidden tree structures for improving the effectiveness of the Transformer-based vision-language systems. Specifically, we impose a Probabilistic Graphical Model (PGM) parameterized by the attention operations on each self-attention layer to incorporate sparse assumption. We use this PGM to softly segment an input sequence into a few clusters where each cluster can be treated as the parent of the inside entities. By stacking these PGM constrained self-attention layers, the clusters in a lower layer compose into a new sequence, and the PGM in a higher layer will further segment this sequence. Iteratively, a sparse tree can be implicitly parsed, and this tree's hierarchical knowledge is incorporated into the transformed embeddings, which can be used for solving the target vision-language tasks. Specifically, we showcase that our APN can strengthen Transformer based networks in two major vision-language tasks: Captioning and Visual Question Answering. Also, a PGM probability-based parsing algorithm is developed by which we can discover what the hidden structure of input is during the inference.



### Support-Set Based Cross-Supervision for Video Grounding
- **Arxiv ID**: http://arxiv.org/abs/2108.10576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10576v1)
- **Published**: 2021-08-24 08:25:26+00:00
- **Updated**: 2021-08-24 08:25:26+00:00
- **Authors**: Xinpeng Ding, Nannan Wang, Shiwei Zhang, De Cheng, Xiaomeng Li, Ziyuan Huang, Mingqian Tang, Xinbo Gao
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Current approaches for video grounding propose kinds of complex architectures to capture the video-text relations, and have achieved impressive improvements. However, it is hard to learn the complicated multi-modal relations by only architecture designing in fact. In this paper, we introduce a novel Support-set Based Cross-Supervision (Sscs) module which can improve existing methods during training phase without extra inference cost. The proposed Sscs module contains two main components, i.e., discriminative contrastive objective and generative caption objective. The contrastive objective aims to learn effective representations by contrastive learning, while the caption objective can train a powerful video encoder supervised by texts. Due to the co-existence of some visual entities in both ground-truth and background intervals, i.e., mutual exclusion, naively contrastive learning is unsuitable to video grounding. We address the problem by boosting the cross-supervision with the support-set concept, which collects visual information from the whole video and eliminates the mutual exclusion of entities. Combined with the original objectives, Sscs can enhance the abilities of multi-modal relation modeling for existing approaches. We extensively evaluate Sscs on three challenging datasets, and show that our method can improve current state-of-the-art methods by large margins, especially 6.35% in terms of R1@0.5 on Charades-STA.



### Making Higher Order MOT Scalable: An Efficient Approximate Solver for Lifted Disjoint Paths
- **Arxiv ID**: http://arxiv.org/abs/2108.10606v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DM
- **Links**: [PDF](http://arxiv.org/pdf/2108.10606v1)
- **Published**: 2021-08-24 09:45:30+00:00
- **Updated**: 2021-08-24 09:45:30+00:00
- **Authors**: Andrea Hornakova, Timo Kaiser, Paul Swoboda, Michal Rolinek, Bodo Rosenhahn, Roberto Henschel
- **Comment**: ICCV 2021. Short version published at CVPR 2021 RVSU workshop
  https://omnomnom.vision.rwth-aachen.de/data/RobMOTS/workshop/papers/9/CameraReady/paper_V3.pdf
  . Implementation available at https://github.com/LPMP/LPMP and
  https://github.com/TimoK93/ApLift
- **Journal**: None
- **Summary**: We present an efficient approximate message passing solver for the lifted disjoint paths problem (LDP), a natural but NP-hard model for multiple object tracking (MOT). Our tracker scales to very large instances that come from long and crowded MOT sequences. Our approximate solver enables us to process the MOT15/16/17 benchmarks without sacrificing solution quality and allows for solving MOT20, which has been out of reach up to now for LDP solvers due to its size and complexity. On all these four standard MOT benchmarks we achieve performance comparable or better than current state-of-the-art methods including a tracker based on an optimal LDP solver.



### Occlusion-robust Visual Markerless Bone Tracking for Computer-Assisted Orthopaedic Surgery
- **Arxiv ID**: http://arxiv.org/abs/2108.10608v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, J.3; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2108.10608v1)
- **Published**: 2021-08-24 09:49:08+00:00
- **Updated**: 2021-08-24 09:49:08+00:00
- **Authors**: Xue Hu, Anh Nguyen, Ferdinando Rodriguez y Baena
- **Comment**: This work has been submitted to the IEEE for possible publication
- **Journal**: None
- **Summary**: Conventional computer-assisted orthopaedic navigation systems rely on the tracking of dedicated optical markers for patient poses, which makes the surgical workflow more invasive, tedious, and expensive. Visual tracking has recently been proposed to measure the target anatomy in a markerless and effortless way, but the existing methods fail under real-world occlusion caused by intraoperative interventions. Furthermore, such methods are hardware-specific and not accurate enough for surgical applications. In this paper, we propose a RGB-D sensing-based markerless tracking method that is robust against occlusion. We design a new segmentation network that features dynamic region-of-interest prediction and robust 3D point cloud segmentation. As it is expensive to collect large-scale training data with occlusion instances, we also propose a new method to create synthetic RGB-D images for network training. Experimental results show that our proposed markerless tracking method outperforms recent state-of-the-art approaches by a large margin, especially when an occlusion exists. Furthermore, our method generalises well to new cameras and new target models, including a cadaver, without the need for network retraining. In practice, by using a high-quality commercial RGB-D camera, our proposed visual tracking method achieves an accuracy of 1-2 degress and 2-4 mm on a model knee, which meets the standard for clinical applications.



### ProtoMIL: Multiple Instance Learning with Prototypical Parts for Whole-Slide Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2108.10612v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.10612v2)
- **Published**: 2021-08-24 10:02:31+00:00
- **Updated**: 2022-09-06 14:10:20+00:00
- **Authors**: Dawid Rymarczyk, Adam Pardyl, Jarosław Kraus, Aneta Kaczyńska, Marek Skomorowski, Bartosz Zieliński
- **Comment**: Accepted to ECML PKDD 2022
- **Journal**: None
- **Summary**: Multiple Instance Learning (MIL) gains popularity in many real-life machine learning applications due to its weakly supervised nature. However, the corresponding effort on explaining MIL lags behind, and it is usually limited to presenting instances of a bag that are crucial for a particular prediction. In this paper, we fill this gap by introducing ProtoMIL, a novel self-explainable MIL method inspired by the case-based reasoning process that operates on visual prototypes. Thanks to incorporating prototypical features into objects description, ProtoMIL unprecedentedly joins the model accuracy and fine-grained interpretability, which we present with the experiments on five recognized MIL datasets.



### Image-free single-pixel segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.10617v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.10617v1)
- **Published**: 2021-08-24 10:06:53+00:00
- **Updated**: 2021-08-24 10:06:53+00:00
- **Authors**: Haiyan Liu, Liheng Bian, Jun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The existing segmentation techniques require high-fidelity images as input to perform semantic segmentation. Since the segmentation results contain most of edge information that is much less than the acquired images, the throughput gap leads to both hardware and software waste. In this letter, we report an image-free single-pixel segmentation technique. The technique combines structured illumination and single-pixel detection together, to efficiently samples and multiplexes scene's segmentation information into compressed one-dimensional measurements. The illumination patterns are optimized together with the subsequent reconstruction neural network, which directly infers segmentation maps from the single-pixel measurements. The end-to-end encoding-and-decoding learning framework enables optimized illumination with corresponding network, which provides both high acquisition and segmentation efficiency. Both simulation and experimental results validate that accurate segmentation can be achieved using two-order-of-magnitude less input data. When the sampling ratio is 1%, the Dice coefficient reaches above 80% and the pixel accuracy reaches above 96%. We envision that this image-free segmentation technique can be widely applied in various resource-limited platforms such as UAV and unmanned vehicle that require real-time sensing.



### Improving Generalization of Batch Whitening by Convolutional Unit Optimization
- **Arxiv ID**: http://arxiv.org/abs/2108.10629v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2108.10629v2)
- **Published**: 2021-08-24 10:27:57+00:00
- **Updated**: 2021-11-02 06:52:23+00:00
- **Authors**: Yooshin Cho, Hanbyel Cho, Youngsoo Kim, Junmo Kim
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Batch Whitening is a technique that accelerates and stabilizes training by transforming input features to have a zero mean (Centering) and a unit variance (Scaling), and by removing linear correlation between channels (Decorrelation). In commonly used structures, which are empirically optimized with Batch Normalization, the normalization layer appears between convolution and activation function. Following Batch Whitening studies have employed the same structure without further analysis; even Batch Whitening was analyzed on the premise that the input of a linear layer is whitened. To bridge the gap, we propose a new Convolutional Unit that is in line with the theory, and our method generally improves the performance of Batch Whitening. Moreover, we show the inefficacy of the original Convolutional Unit by investigating rank and correlation of features. As our method is employable off-the-shelf whitening modules, we use Iterative Normalization (IterNorm), the state-of-the-art whitening module, and obtain significantly improved performance on five image classification datasets: CIFAR-10, CIFAR-100, CUB-200-2011, Stanford Dogs, and ImageNet. Notably, we verify that our method improves stability and performance of whitening when using large learning rate, group size, and iteration number.



### Full-Velocity Radar Returns by Radar-Camera Fusion
- **Arxiv ID**: http://arxiv.org/abs/2108.10637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10637v1)
- **Published**: 2021-08-24 10:42:16+00:00
- **Updated**: 2021-08-24 10:42:16+00:00
- **Authors**: Yunfei Long, Daniel Morris, Xiaoming Liu, Marcos Castro, Punarjay Chakravarty, Praveen Narayanan
- **Comment**: International Conference on Computer Vision, 2021
- **Journal**: None
- **Summary**: A distinctive feature of Doppler radar is the measurement of velocity in the radial direction for radar points. However, the missing tangential velocity component hampers object velocity estimation as well as temporal integration of radar sweeps in dynamic scenes. Recognizing that fusing camera with radar provides complementary information to radar, in this paper we present a closed-form solution for the point-wise, full-velocity estimate of Doppler returns using the corresponding optical flow from camera images. Additionally, we address the association problem between radar returns and camera images with a neural network that is trained to estimate radar-camera correspondences. Experimental results on the nuScenes dataset verify the validity of the method and show significant improvements over the state-of-the-art in velocity estimation and accumulation of radar points.



### Temporal Knowledge Consistency for Unsupervised Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.10668v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10668v1)
- **Published**: 2021-08-24 12:14:13+00:00
- **Updated**: 2021-08-24 12:14:13+00:00
- **Authors**: Weixin Feng, Yuanjiang Wang, Lihua Ma, Ye Yuan, Chi Zhang
- **Comment**: To appear in ICCV 2021
- **Journal**: ICCV 2021
- **Summary**: The instance discrimination paradigm has become dominant in unsupervised learning. It always adopts a teacher-student framework, in which the teacher provides embedded knowledge as a supervision signal for the student. The student learns meaningful representations by enforcing instance spatial consistency with the views from the teacher. However, the outputs of the teacher can vary dramatically on the same instance during different training stages, introducing unexpected noise and leading to catastrophic forgetting caused by inconsistent objectives. In this paper, we first integrate instance temporal consistency into current instance discrimination paradigms, and propose a novel and strong algorithm named Temporal Knowledge Consistency (TKC). Specifically, our TKC dynamically ensembles the knowledge of temporal teachers and adaptively selects useful information according to its importance to learning instance temporal consistency. Experimental result shows that TKC can learn better visual representations on both ResNet and AlexNet on linear evaluation protocol while transfer well to downstream tasks. All experiments suggest the good effectiveness and generalization of our method.



### Spatio-Temporal Self-Attention Network for Video Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/2108.10696v2
- **DOI**: 10.1109/TMM.2021.3139743
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10696v2)
- **Published**: 2021-08-24 12:52:47+00:00
- **Updated**: 2022-01-17 14:45:41+00:00
- **Authors**: Ziqiang Wang, Zhi Liu, Gongyang Li, Yang Wang, Tianhong Zhang, Lihua Xu, Jijun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: 3D convolutional neural networks have achieved promising results for video tasks in computer vision, including video saliency prediction that is explored in this paper. However, 3D convolution encodes visual representation merely on fixed local spacetime according to its kernel size, while human attention is always attracted by relational visual features at different time. To overcome this limitation, we propose a novel Spatio-Temporal Self-Attention 3D Network (STSANet) for video saliency prediction, in which multiple Spatio-Temporal Self-Attention (STSA) modules are employed at different levels of 3D convolutional backbone to directly capture long-range relations between spatio-temporal features of different time steps. Besides, we propose an Attentional Multi-Scale Fusion (AMSF) module to integrate multi-level features with the perception of context in semantic and spatio-temporal subspaces. Extensive experiments demonstrate the contributions of key components of our method, and the results on DHF1K, Hollywood-2, UCF, and DIEM benchmark datasets clearly prove the superiority of the proposed model compared with all state-of-the-art models.



### MCUa: Multi-level Context and Uncertainty aware Dynamic Deep Ensemble for Breast Cancer Histology Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2108.10709v1
- **DOI**: 10.1109/TBME.2021.3107446
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.10709v1)
- **Published**: 2021-08-24 13:18:57+00:00
- **Updated**: 2021-08-24 13:18:57+00:00
- **Authors**: Zakaria Senousy, Mohammed M. Abdelsamea, Mohamed Medhat Gaber, Moloud Abdar, U Rajendra Acharya, Abbas Khosravi, Saeid Nahavandi
- **Comment**: accepted by IEEE Transactions on Biomedical Engineering
- **Journal**: IEEE Transactions on Biomedical Engineering 2021
- **Summary**: Breast histology image classification is a crucial step in the early diagnosis of breast cancer. In breast pathological diagnosis, Convolutional Neural Networks (CNNs) have demonstrated great success using digitized histology slides. However, tissue classification is still challenging due to the high visual variability of the large-sized digitized samples and the lack of contextual information. In this paper, we propose a novel CNN, called Multi-level Context and Uncertainty aware (MCUa) dynamic deep learning ensemble model.MCUamodel consists of several multi-level context-aware models to learn the spatial dependency between image patches in a layer-wise fashion. It exploits the high sensitivity to the multi-level contextual information using an uncertainty quantification component to accomplish a novel dynamic ensemble model.MCUamodelhas achieved a high accuracy of 98.11% on a breast cancer histology image dataset. Experimental results show the superior effectiveness of the proposed solution compared to the state-of-the-art histology classification models.



### PocketNet: Extreme Lightweight Face Recognition Network using Neural Architecture Search and Multi-Step Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2108.10710v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10710v2)
- **Published**: 2021-08-24 13:19:08+00:00
- **Updated**: 2021-12-13 15:16:22+00:00
- **Authors**: Fadi Boutros, Patrick Siebke, Marcel Klemt, Naser Damer, Florian Kirchbuchner, Arjan Kuijper
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have rapidly become the mainstream method for face recognition (FR). However, this limits the deployment of such models that contain an extremely large number of parameters to embedded and low-end devices. In this work, we present an extremely lightweight and accurate FR solution, namely PocketNet. We utilize neural architecture search to develop a new family of lightweight face-specific architectures. We additionally propose a novel training paradigm based on knowledge distillation (KD), the multi-step KD, where the knowledge is distilled from the teacher model to the student model at different stages of the training maturity. We conduct a detailed ablation study proving both, the sanity of using NAS for the specific task of FR rather than general object classification, and the benefits of our proposed multi-step KD. We present an extensive experimental evaluation and comparisons with the state-of-the-art (SOTA) compact FR models on nine different benchmarks including large-scale evaluation benchmarks such as IJB-B, IJB-C, and MegaFace. PocketNets have consistently advanced the SOTA FR performance on nine mainstream benchmarks when considering the same level of model compactness. With 0.92M parameters, our smallest network PocketNetS-128 achieved very competitive results to recent SOTA compacted models that contain up to 4M parameters.



### Pen Spinning Hand Movement Analysis Using MediaPipe Hands
- **Arxiv ID**: http://arxiv.org/abs/2108.10716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10716v1)
- **Published**: 2021-08-24 13:27:28+00:00
- **Updated**: 2021-08-24 13:27:28+00:00
- **Authors**: Tung-Lin Wu, Taishi Senda
- **Comment**: None
- **Journal**: None
- **Summary**: We challenged to get data about hand movement in pen spinning using MediaPipe Hands and OpenCV. The purpose is to create a system that can be used to objectively evaluate the performance of pen spinning competitions. Evaluation of execution, smoothness, and control in competitions are quite difficult and often with subjectivity. Therefore, we aimed to fully automate the process by using objective numerical values for evaluation. Uncertainty still exists in MediaPipe's skeletal recognition, and it tends to be more difficult to recognize in brightly colored backgrounds. However, we could improve the recognition accuracy by changing the saturation and brightness in the program. Furthermore, automatic detection and adjustment of brightness is now possible. As the next step to systematize the evaluation of pen spinning using objective numerical values, we adopted "hand movements". We were able to visualize the ups and downs of the hand movements by calculating the standard deviation and L2 norm of the hand's coordinates in each frame. The results of hand movements are quite accurate, and we feel that it is a big step toward our goal. In the future, we would like to make great efforts to fully automate the grading of pen spinning.



### DeepPanoContext: Panoramic 3D Scene Understanding with Holistic Scene Context Graph and Relation-based Optimization
- **Arxiv ID**: http://arxiv.org/abs/2108.10743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10743v1)
- **Published**: 2021-08-24 13:55:29+00:00
- **Updated**: 2021-08-24 13:55:29+00:00
- **Authors**: Cheng Zhang, Zhaopeng Cui, Cai Chen, Shuaicheng Liu, Bing Zeng, Hujun Bao, Yinda Zhang
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Panorama images have a much larger field-of-view thus naturally encode enriched scene context information compared to standard perspective images, which however is not well exploited in the previous scene understanding methods. In this paper, we propose a novel method for panoramic 3D scene understanding which recovers the 3D room layout and the shape, pose, position, and semantic category for each object from a single full-view panorama image. In order to fully utilize the rich context information, we design a novel graph neural network based context model to predict the relationship among objects and room layout, and a differentiable relationship-based optimization module to optimize object arrangement with well-designed objective functions on-the-fly. Realizing the existing data are either with incomplete ground truth or overly-simplified scene, we present a new synthetic dataset with good diversity in room layout and furniture placement, and realistic image quality for total panoramic 3D scene understanding. Experiments demonstrate that our method outperforms existing methods on panoramic scene understanding in terms of both geometry accuracy and object arrangement. Code is available at https://chengzhag.github.io/publication/dpc.



### DU-GAN: Generative Adversarial Networks with Dual-Domain U-Net Based Discriminators for Low-Dose CT Denoising
- **Arxiv ID**: http://arxiv.org/abs/2108.10772v2
- **DOI**: 10.1109/TIM.2021.3128703
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2108.10772v2)
- **Published**: 2021-08-24 14:37:46+00:00
- **Updated**: 2021-11-07 13:18:51+00:00
- **Authors**: Zhizhong Huang, Junping Zhang, Yi Zhang, Hongming Shan
- **Comment**: Accepted by IEEE Transactions on Instrumentation and Measurement
- **Journal**: IEEE Transactions on Instrumentation and Measurement, 4500512,
  2022
- **Summary**: LDCT has drawn major attention in the medical imaging field due to the potential health risks of CT-associated X-ray radiation to patients. Reducing the radiation dose, however, decreases the quality of the reconstructed images, which consequently compromises the diagnostic performance. Various deep learning techniques have been introduced to improve the image quality of LDCT images through denoising. GANs-based denoising methods usually leverage an additional classification network, i.e. discriminator, to learn the most discriminate difference between the denoised and normal-dose images and, hence, regularize the denoising model accordingly; it often focuses either on the global structure or local details. To better regularize the LDCT denoising model, this paper proposes a novel method, termed DU-GAN, which leverages U-Net based discriminators in the GANs framework to learn both global and local difference between the denoised and normal-dose images in both image and gradient domains. The merit of such a U-Net based discriminator is that it can not only provide the per-pixel feedback to the denoising network through the outputs of the U-Net but also focus on the global structure in a semantic level through the middle layer of the U-Net. In addition to the adversarial training in the image domain, we also apply another U-Net based discriminator in the image gradient domain to alleviate the artifacts caused by photon starvation and enhance the edge of the denoised CT images. Furthermore, the CutMix technique enables the per-pixel outputs of the U-Net based discriminator to provide radiologists with a confidence map to visualize the uncertainty of the denoised results, facilitating the LDCT-based screening and diagnosis. Extensive experiments on the simulated and real-world datasets demonstrate superior performance over recently published methods both qualitatively and quantitatively.



### A Benchmark for Spray from Nearby Cutting Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2108.10800v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.10800v1)
- **Published**: 2021-08-24 15:40:09+00:00
- **Updated**: 2021-08-24 15:40:09+00:00
- **Authors**: Stefanie Walz, Mario Bijelic, Florian Kraus, Werner Ritter, Martin Simon, Igor Doric
- **Comment**: None
- **Journal**: None
- **Summary**: Current driver assistance systems and autonomous driving stacks are limited to well-defined environment conditions and geo fenced areas. To increase driving safety in adverse weather conditions, broadening the application spectrum of autonomous driving and driver assistance systems is necessary. In order to enable this development, reproducible benchmarking methods are required to quantify the expected distortions. In this publication, a testing methodology for disturbances from spray is presented. It introduces a novel lightweight and configurable spray setup alongside an evaluation scheme to assess the disturbances caused by spray. The analysis covers an automotive RGB camera and two different LiDAR systems, as well as downstream detection algorithms based on YOLOv3 and PV-RCNN. In a common scenario of a closely cutting vehicle, it is visible that the distortions are severely affecting the perception stack up to four seconds showing the necessity of benchmarking the influences of spray.



### Reconcile Prediction Consistency for Balanced Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.10809v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10809v2)
- **Published**: 2021-08-24 15:52:11+00:00
- **Updated**: 2021-08-27 01:42:28+00:00
- **Authors**: Keyang Wang, Lei Zhang
- **Comment**: To appear in ICCV 2021
- **Journal**: None
- **Summary**: Classification and regression are two pillars of object detectors. In most CNN-based detectors, these two pillars are optimized independently. Without direct interactions between them, the classification loss and the regression loss can not be optimized synchronously toward the optimal direction in the training phase. This clearly leads to lots of inconsistent predictions with high classification score but low localization accuracy or low classification score but high localization accuracy in the inference phase, especially for the objects of irregular shape and occlusion, which severely hurts the detection performance of existing detectors after NMS. To reconcile prediction consistency for balanced object detection, we propose a Harmonic loss to harmonize the optimization of classification branch and localization branch. The Harmonic loss enables these two branches to supervise and promote each other during training, thereby producing consistent predictions with high co-occurrence of top classification and localization in the inference phase. Furthermore, in order to prevent the localization loss from being dominated by outliers during training phase, a Harmonic IoU loss is proposed to harmonize the weight of the localization loss of different IoU-level samples. Comprehensive experiments on benchmarks PASCAL VOC and MS COCO demonstrate the generality and effectiveness of our model for facilitating existing object detectors to state-of-the-art accuracy.



### Next-generation perception system for automated defects detection in composite laminates via polarized computational imaging
- **Arxiv ID**: http://arxiv.org/abs/2108.10819v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.10819v1)
- **Published**: 2021-08-24 16:09:48+00:00
- **Updated**: 2021-08-24 16:09:48+00:00
- **Authors**: Yuqi Ding, Jinwei Ye, Corina Barbalata, James Oubre, Chandler Lemoine, Jacob Agostinho, Genevieve Palardy
- **Comment**: None
- **Journal**: None
- **Summary**: Finishing operations on large-scale composite components like wind turbine blades, including trimming and sanding, often require multiple workers and part repositioning. In the composites manufacturing industry, automation of such processes is challenging, as manufactured part geometry may be inconsistent and task completion is based on human judgment and experience. Implementing a mobile, collaborative robotic system capable of performing finishing tasks in dynamic and uncertain environments would improve quality and lower manufacturing costs. To complete the given tasks, the collaborative robotic team must properly understand the environment and detect irregularities in the manufactured parts. In this paper, we describe the initial implementation and demonstration of a polarized computational imaging system to identify defects in composite laminates. As the polarimetric images are highly relevant to the surface micro-geometry, they can be used to detect surface defects that are not visible in conventional color images. The proposed vision system successfully identifies defect types and surface characteristics (e.g., pinholes, voids, scratches, resin flash) for different glass fiber and carbon fiber laminates.



### LLVIP: A Visible-infrared Paired Dataset for Low-light Vision
- **Arxiv ID**: http://arxiv.org/abs/2108.10831v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.10831v4)
- **Published**: 2021-08-24 16:29:17+00:00
- **Updated**: 2023-06-14 12:14:17+00:00
- **Authors**: Xinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, Shengjie Liu, Wenli Zhou
- **Comment**: 10 pages, 11 figures, ICCV workshop
- **Journal**: None
- **Summary**: It is very challenging for various visual tasks such as image fusion, pedestrian detection and image-to-image translation in low light conditions due to the loss of effective target areas. In this case, infrared and visible images can be used together to provide both rich detail information and effective target areas. In this paper, we present LLVIP, a visible-infrared paired dataset for low-light vision. This dataset contains 30976 images, or 15488 pairs, most of which were taken at very dark scenes, and all of the images are strictly aligned in time and space. Pedestrians in the dataset are labeled. We compare the dataset with other visible-infrared datasets and evaluate the performance of some popular visual algorithms including image fusion, pedestrian detection and image-to-image translation on the dataset. The experimental results demonstrate the complementary effect of fusion on image information, and find the deficiency of existing algorithms of the three visual tasks in very low-light conditions. We believe the LLVIP dataset will contribute to the community of computer vision by promoting image fusion, pedestrian detection and image-to-image translation in very low-light applications. The dataset is being released in https://bupt-ai-cz.github.io/LLVIP. Raw data is also provided for further research such as image registration.



### Meta Self-Learning for Multi-Source Domain Adaptation: A Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2108.10840v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.10840v1)
- **Published**: 2021-08-24 17:07:34+00:00
- **Updated**: 2021-08-24 17:07:34+00:00
- **Authors**: Shuhao Qiu, Chuang Zhu, Wenli Zhou
- **Comment**: 10 pages, 4 figures, ICCV workshop
- **Journal**: None
- **Summary**: In recent years, deep learning-based methods have shown promising results in computer vision area. However, a common deep learning model requires a large amount of labeled data, which is labor-intensive to collect and label. What's more, the model can be ruined due to the domain shift between training data and testing data. Text recognition is a broadly studied field in computer vision and suffers from the same problems noted above due to the diversity of fonts and complicated backgrounds. In this paper, we focus on the text recognition problem and mainly make three contributions toward these problems. First, we collect a multi-source domain adaptation dataset for text recognition, including five different domains with over five million images, which is the first multi-domain text recognition dataset to our best knowledge. Secondly, we propose a new method called Meta Self-Learning, which combines the self-learning method with the meta-learning paradigm and achieves a better recognition result under the scene of multi-domain adaptation. Thirdly, extensive experiments are conducted on the dataset to provide a benchmark and also show the effectiveness of our method. The code of our work and dataset are available soon at https://bupt-ai-cz.github.io/Meta-SelfLearning/.



### imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose
- **Arxiv ID**: http://arxiv.org/abs/2108.10842v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.10842v1)
- **Published**: 2021-08-24 17:08:28+00:00
- **Updated**: 2021-08-24 17:08:28+00:00
- **Authors**: Thiemo Alldieck, Hongyi Xu, Cristian Sminchisescu
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: We present imGHUM, the first holistic generative model of 3D human shape and articulated pose, represented as a signed distance function. In contrast to prior work, we model the full human body implicitly as a function zero-level-set and without the use of an explicit template mesh. We propose a novel network architecture and a learning paradigm, which make it possible to learn a detailed implicit generative model of human pose, shape, and semantics, on par with state-of-the-art mesh-based models. Our model features desired detail for human models, such as articulated pose including hand motion and facial expressions, a broad spectrum of shape variations, and can be queried at arbitrary resolutions and spatial locations. Additionally, our model has attached spatial semantics making it straightforward to establish correspondences between different shape instances, thus enabling applications that are difficult to tackle using classical implicit representations. In extensive experiments, we demonstrate the model accuracy and its applicability to current research problems.



### Bridging Unsupervised and Supervised Depth from Focus via All-in-Focus Supervision
- **Arxiv ID**: http://arxiv.org/abs/2108.10843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10843v1)
- **Published**: 2021-08-24 17:09:13+00:00
- **Updated**: 2021-08-24 17:09:13+00:00
- **Authors**: Ning-Hsu Wang, Ren Wang, Yu-Lun Liu, Yu-Hao Huang, Yu-Lin Chang, Chia-Ping Chen, Kevin Jou
- **Comment**: ICCV 2021. Project page: https://albert100121.github.io/AiFDepthNet/
  Code: https://github.com/albert100121/AiFDepthNet
- **Journal**: None
- **Summary**: Depth estimation is a long-lasting yet important task in computer vision. Most of the previous works try to estimate depth from input images and assume images are all-in-focus (AiF), which is less common in real-world applications. On the other hand, a few works take defocus blur into account and consider it as another cue for depth estimation. In this paper, we propose a method to estimate not only a depth map but an AiF image from a set of images with different focus positions (known as a focal stack). We design a shared architecture to exploit the relationship between depth and AiF estimation. As a result, the proposed method can be trained either supervisedly with ground truth depth, or \emph{unsupervisedly} with AiF images as supervisory signals. We show in various experiments that our method outperforms the state-of-the-art methods both quantitatively and qualitatively, and also has higher efficiency in inference time.



### Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density
- **Arxiv ID**: http://arxiv.org/abs/2108.10860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10860v1)
- **Published**: 2021-08-24 17:41:45+00:00
- **Updated**: 2021-08-24 17:41:45+00:00
- **Authors**: Kuniaki Saito, Donghyun Kim, Piotr Teterwak, Stan Sclaroff, Trevor Darrell, Kate Saenko
- **Comment**: ICCV2021
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) methods can dramatically improve generalization on unlabeled target domains. However, optimal hyper-parameter selection is critical to achieving high accuracy and avoiding negative transfer. Supervised hyper-parameter validation is not possible without labeled target data, which raises the question: How can we validate unsupervised adaptation techniques in a realistic way? We first empirically analyze existing criteria and demonstrate that they are not very effective for tuning hyper-parameters. Intuitively, a well-trained source classifier should embed target samples of the same class nearby, forming dense neighborhoods in feature space. Based on this assumption, we propose a novel unsupervised validation criterion that measures the density of soft neighborhoods by computing the entropy of the similarity distribution between points. Our criterion is simpler than competing validation methods, yet more effective; it can tune hyper-parameters and the number of training iterations in both image classification and semantic segmentation models. The code used for the paper will be available at \url{https://github.com/VisionLearningGroup/SND}.



### DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras
- **Arxiv ID**: http://arxiv.org/abs/2108.10869v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10869v2)
- **Published**: 2021-08-24 17:50:10+00:00
- **Updated**: 2022-02-02 19:28:32+00:00
- **Authors**: Zachary Teed, Jia Deng
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce DROID-SLAM, a new deep learning based SLAM system. DROID-SLAM consists of recurrent iterative updates of camera pose and pixelwise depth through a Dense Bundle Adjustment layer. DROID-SLAM is accurate, achieving large improvements over prior work, and robust, suffering from substantially fewer catastrophic failures. Despite training on monocular video, it can leverage stereo or RGB-D video to achieve improved performance at test time. The URL to our open source code is https://github.com/princeton-vl/DROID-SLAM.



### A QuadTree Image Representation for Computational Pathology
- **Arxiv ID**: http://arxiv.org/abs/2108.10873v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.10873v1)
- **Published**: 2021-08-24 17:53:19+00:00
- **Updated**: 2021-08-24 17:53:19+00:00
- **Authors**: Rob Jewsbury, Abhir Bhalerao, Nasir Rajpoot
- **Comment**: 11 pages, 5 figures, accepted to CDPath ICCV 2021
- **Journal**: None
- **Summary**: The field of computational pathology presents many challenges for computer vision algorithms due to the sheer size of pathology images. Histopathology images are large and need to be split up into image tiles or patches so modern convolutional neural networks (CNNs) can process them. In this work, we present a method to generate an interpretable image representation of computational pathology images using quadtrees and a pipeline to use these representations for highly accurate downstream classification. To the best of our knowledge, this is the first attempt to use quadtrees for pathology image data. We show it is highly accurate, able to achieve as good results as the currently widely adopted tissue mask patch extraction methods all while using over 38% less data.



### Are socially-aware trajectory prediction models really socially-aware?
- **Arxiv ID**: http://arxiv.org/abs/2108.10879v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10879v2)
- **Published**: 2021-08-24 17:59:09+00:00
- **Updated**: 2022-02-11 17:21:56+00:00
- **Authors**: Saeed Saadatnejad, Mohammadhossein Bahari, Pedram Khorsandi, Mohammad Saneian, Seyed-Mohsen Moosavi-Dezfooli, Alexandre Alahi
- **Comment**: None
- **Journal**: None
- **Summary**: Our field has recently witnessed an arms race of neural network-based trajectory predictors. While these predictors are at the core of many applications such as autonomous navigation or pedestrian flow simulations, their adversarial robustness has not been carefully studied. In this paper, we introduce a socially-attended attack to assess the social understanding of prediction models in terms of collision avoidance. An attack is a small yet carefully-crafted perturbations to fail predictors. Technically, we define collision as a failure mode of the output, and propose hard- and soft-attention mechanisms to guide our attack. Thanks to our attack, we shed light on the limitations of the current models in terms of their social understanding. We demonstrate the strengths of our method on the recent trajectory prediction models. Finally, we show that our attack can be employed to increase the social understanding of state-of-the-art models. The code is available online: https://s-attack.github.io/



### SimVLM: Simple Visual Language Model Pretraining with Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/2108.10904v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.10904v3)
- **Published**: 2021-08-24 18:14:00+00:00
- **Updated**: 2022-05-15 23:20:46+00:00
- **Authors**: Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, Yuan Cao
- **Comment**: Published at ICLR 2022
- **Journal**: None
- **Summary**: With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE (+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.



### Correcting inter-scan motion artefacts in quantitative R1 mapping at 7T
- **Arxiv ID**: http://arxiv.org/abs/2108.10943v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.10943v2)
- **Published**: 2021-08-24 20:30:23+00:00
- **Updated**: 2021-12-07 23:02:28+00:00
- **Authors**: Yaël Balbastre, Ali Aghaeifar, Nadège Corbin, Mikael Brudfors, John Ashburner, Martina F. Callaghan
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Inter-scan motion is a substantial source of error in $R_1$ estimation, and can be expected to increase at 7T where $B_1$ fields are more inhomogeneous. The established correction scheme does not translate to 7T since it requires a body coil reference. Here we introduce two alternatives that outperform the established method. Since they compute relative sensitivities they do not require body coil images.   Theory: The proposed methods use coil-combined magnitude images to obtain the relative coil sensitivities. The first method efficiently computes the relative sensitivities via a simple ratio; the second by fitting a more sophisticated generative model.   Methods: $R_1$ maps were computed using the variable flip angle (VFA) approach. Multiple datasets were acquired at 3T and 7T, with and without motion between the acquisition of the VFA volumes. $R_1$ maps were constructed without correction, with the proposed corrections, and (at 3T) with the previously established correction scheme.   Results: At 3T, the proposed methods outperform the baseline method. Inter-scan motion artefacts were also reduced at 7T. However, reproducibility only converged on that of the no motion condition if position-specific transmit field effects were also incorporated.   Conclusion: The proposed methods simplify inter-scan motion correction of $R_1$ maps and are applicable at both 3T and 7T, where a body coil is typically not available. The open-source code for all methods is made publicly available.



### Field-Guide-Inspired Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.10967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10967v1)
- **Published**: 2021-08-24 21:36:05+00:00
- **Updated**: 2021-08-24 21:36:05+00:00
- **Authors**: Utkarsh Mall, Bharath Hariharan, Kavita Bala
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Modern recognition systems require large amounts of supervision to achieve accuracy. Adapting to new domains requires significant data from experts, which is onerous and can become too expensive. Zero-shot learning requires an annotated set of attributes for a novel category. Annotating the full set of attributes for a novel category proves to be a tedious and expensive task in deployment. This is especially the case when the recognition domain is an expert domain. We introduce a new field-guide-inspired approach to zero-shot annotation where the learner model interactively asks for the most useful attributes that define a class. We evaluate our method on classification benchmarks with attribute annotations like CUB, SUN, and AWA2 and show that our model achieves the performance of a model with full annotations at the cost of a significantly fewer number of annotations. Since the time of experts is precious, decreasing annotation cost can be very valuable for real-world deployment.



### Real-time Indian Sign Language (ISL) Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.10970v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2108.10970v1)
- **Published**: 2021-08-24 21:49:21+00:00
- **Updated**: 2021-08-24 21:49:21+00:00
- **Authors**: Kartik Shenoy, Tejas Dastane, Varun Rao, Devendra Vyavaharkar
- **Comment**: 9 pages
- **Journal**: 9th International Conference on Communication and Network
  Technology 2018
- **Summary**: This paper presents a system which can recognise hand poses & gestures from the Indian Sign Language (ISL) in real-time using grid-based features. This system attempts to bridge the communication gap between the hearing and speech impaired and the rest of the society. The existing solutions either provide relatively low accuracy or do not work in real-time. This system provides good results on both the parameters. It can identify 33 hand poses and some gestures from the ISL. Sign Language is captured from a smartphone camera and its frames are transmitted to a remote server for processing. The use of any external hardware (such as gloves or the Microsoft Kinect sensor) is avoided, making it user-friendly. Techniques such as Face detection, Object stabilisation and Skin Colour Segmentation are used for hand detection and tracking. The image is further subjected to a Grid-based Feature Extraction technique which represents the hand's pose in the form of a Feature Vector. Hand poses are then classified using the k-Nearest Neighbours algorithm. On the other hand, for gesture classification, the motion and intermediate hand poses observation sequences are fed to Hidden Markov Model chains corresponding to the 12 pre-selected gestures defined in ISL. Using this methodology, the system is able to achieve an accuracy of 99.7% for static hand poses, and an accuracy of 97.23% for gesture recognition.



### An Effective Pixel-Wise Approach for Skin Colour Segmentation Using Pixel Neighbourhood Technique
- **Arxiv ID**: http://arxiv.org/abs/2108.10971v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10971v1)
- **Published**: 2021-08-24 21:54:38+00:00
- **Updated**: 2021-08-24 21:54:38+00:00
- **Authors**: Tejas Dastane, Varun Rao, Kartik Shenoy, Devendra Vyavaharkar
- **Comment**: 5 pages
- **Journal**: International Journal on Recent and Innovation Trends in Computing
  and Communication 2018, Volume: 6, Issue: 3, pp. 182-186
- **Summary**: This paper presents a novel technique for skin colour segmentation that overcomes the limitations faced by existing techniques such as Colour Range Thresholding. Skin colour segmentation is affected by the varied skin colours and surrounding lighting conditions, leading to poorskin segmentation for many techniques. We propose a new two stage Pixel Neighbourhood technique that classifies any pixel as skin or non-skin based on its neighbourhood pixels. The first step calculates the probability of each pixel being skin by passing HSV values of the pixel to a Deep Neural Network model. In the next step, it calculates the likeliness of pixel being skin using these probabilities of neighbouring pixels. This technique performs skin colour segmentation better than the existing techniques.



### Domain Adaptation for Real-World Single View 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2108.10972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10972v1)
- **Published**: 2021-08-24 22:02:27+00:00
- **Updated**: 2021-08-24 22:02:27+00:00
- **Authors**: Brandon Leung, Siddharth Singh, Arik Horodniceanu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based object reconstruction algorithms have shown remarkable improvements over classical methods. However, supervised learning based methods perform poorly when the training data and the test data have different distributions. Indeed, most current works perform satisfactorily on the synthetic ShapeNet dataset, but dramatically fail in when presented with real world images. To address this issue, unsupervised domain adaptation can be used transfer knowledge from the labeled synthetic source domain and learn a classifier for the unlabeled real target domain. To tackle this challenge of single view 3D reconstruction in the real domain, we experiment with a variety of domain adaptation techniques inspired by the maximum mean discrepancy (MMD) loss, Deep CORAL, and the domain adversarial neural network (DANN). From these findings, we additionally propose a novel architecture which takes advantage of the fact that in this setting, target domain data is unsupervised with regards to the 3D model but supervised for class labels. We base our framework off a recent network called pix2vox. Results are performed with ShapeNet as the source domain and domains within the Object Dataset Domain Suite (ODDS) dataset as the target, which is a real world multiview, multidomain image dataset. The domains in ODDS vary in difficulty, allowing us to assess notions of domain gap size. Our results are the first in the multiview reconstruction literature using this dataset.



### NeRP: Implicit Neural Representation Learning with Prior Embedding for Sparsely Sampled Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2108.10991v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.10991v2)
- **Published**: 2021-08-24 23:19:34+00:00
- **Updated**: 2023-01-13 23:24:43+00:00
- **Authors**: Liyue Shen, John Pauly, Lei Xing
- **Comment**: Code available at https://github.com/liyues/NeRP
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems (TNNLS),
  2022
- **Summary**: Image reconstruction is an inverse problem that solves for a computational image based on sampled sensor measurement. Sparsely sampled image reconstruction poses addition challenges due to limited measurements. In this work, we propose an implicit Neural Representation learning methodology with Prior embedding (NeRP) to reconstruct a computational image from sparsely sampled measurements. The method differs fundamentally from previous deep learning-based image reconstruction approaches in that NeRP exploits the internal information in an image prior, and the physics of the sparsely sampled measurements to produce a representation of the unknown subject. No large-scale data is required to train the NeRP except for a prior image and sparsely sampled measurements. In addition, we demonstrate that NeRP is a general methodology that generalizes to different imaging modalities such as CT and MRI. We also show that NeRP can robustly capture the subtle yet significant image changes required for assessing tumor progression.



### OOWL500: Overcoming Dataset Collection Bias in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2108.10992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.10992v1)
- **Published**: 2021-08-24 23:22:53+00:00
- **Updated**: 2021-08-24 23:22:53+00:00
- **Authors**: Brandon Leung, Chih-Hui Ho, Amir Persekian, David Orozco, Yen Chang, Erik Sandstrom, Bo Liu, Nuno Vasconcelos
- **Comment**: None
- **Journal**: None
- **Summary**: The hypothesis that image datasets gathered online "in the wild" can produce biased object recognizers, e.g. preferring professional photography or certain viewing angles, is studied. A new "in the lab" data collection infrastructure is proposed consisting of a drone which captures images as it circles around objects. Crucially, the control provided by this setup and the natural camera shake inherent to flight mitigate many biases. It's inexpensive and easily replicable nature may also potentially lead to a scalable data collection effort by the vision community. The procedure's usefulness is demonstrated by creating a dataset of Objects Obtained With fLight (OOWL). Denoted as OOWL500, it contains 120,000 images of 500 objects and is the largest "in the lab" image dataset available when both number of classes and objects per class are considered. Furthermore, it has enabled several of new insights on object recognition. First, a novel adversarial attack strategy is proposed, where image perturbations are defined in terms of semantic properties such as camera shake and pose. Indeed, experiments have shown that ImageNet has considerable amounts of pose and professional photography bias. Second, it is used to show that the augmentation of in the wild datasets, such as ImageNet, with in the lab data, such as OOWL500, can significantly decrease these biases, leading to object recognizers of improved generalization. Third, the dataset is used to study questions on "best procedures" for dataset collection. It is revealed that data augmentation with synthetic images does not suffice to eliminate in the wild datasets biases, and that camera shake and pose diversity play a more important role in object recognition robustness than previously thought.



