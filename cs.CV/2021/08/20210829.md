# Arxiv Papers in cs.CV on 2021-08-29
### Variational voxelwise rs-fMRI representation learning: Evaluation of sex, age, and neuropsychiatric signatures
- **Arxiv ID**: http://arxiv.org/abs/2108.12756v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12756v1)
- **Published**: 2021-08-29 05:27:32+00:00
- **Updated**: 2021-08-29 05:27:32+00:00
- **Authors**: Eloy Geenjaar, Tonya White, Vince Calhoun
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to apply non-linear representation learning to voxelwise rs-fMRI data. Learning the non-linear representations is done using a variational autoencoder (VAE). The VAE is trained on voxelwise rs-fMRI data and performs non-linear dimensionality reduction that retains meaningful information. The retention of information in the model's representations is evaluated using downstream age regression and sex classification tasks. The results on these tasks are highly encouraging and a linear regressor trained with the representations of our unsupervised model performs almost as well as a supervised neural network, trained specifically for age regression on the same dataset. The model is also evaluated with a schizophrenia diagnosis prediction task, to assess its feasibility as a dimensionality reduction method for neuropsychiatric datasets. These results highlight the potential for pre-training on a larger set of individuals who do not have mental illness, to improve the downstream neuropsychiatric task results. The pre-trained model is fine-tuned for a variable number of epochs on a schizophrenia dataset and we find that fine-tuning for 1 epoch yields the best results. This work therefore not only opens up non-linear dimensionality reduction for voxelwise rs-fMRI data but also shows that pre-training a deep learning model on voxelwise rs-fMRI datasets greatly increases performance even on smaller datasets. It also opens up the ability to look at the distribution of rs-fMRI time series in the latent space of the VAE for heterogeneous neuropsychiatric disorders like schizophrenia in future work. This can be complemented with the generative aspect of the model that allows us to reconstruct points from the model's latent space back into brain space and obtain an improved understanding of the relation that the VAE learns between subjects, timepoints, and a subject's characteristics.



### Calibrating Class Activation Maps for Long-Tailed Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.12757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12757v1)
- **Published**: 2021-08-29 05:45:03+00:00
- **Updated**: 2021-08-29 05:45:03+00:00
- **Authors**: Chi Zhang, Guosheng Lin, Lvlong Lai, Henghui Ding, Qingyao Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world visual recognition problems often exhibit long-tailed distributions, where the amount of data for learning in different categories shows significant imbalance. Standard classification models learned on such data distribution often make biased predictions towards the head classes while generalizing poorly to the tail classes. In this paper, we present two effective modifications of CNNs to improve network learning from long-tailed distribution. First, we present a Class Activation Map Calibration (CAMC) module to improve the learning and prediction of network classifiers, by enforcing network prediction based on important image regions. The proposed CAMC module highlights the correlated image regions across data and reinforces the representations in these areas to obtain a better global representation for classification. Furthermore, we investigate the use of normalized classifiers for representation learning in long-tailed problems. Our empirical study demonstrates that by simply scaling the outputs of the classifier with an appropriate scalar, we can effectively improve the classification accuracy on tail classes without losing the accuracy of head classes. We conduct extensive experiments to validate the effectiveness of our design and we set new state-of-the-art performance on five benchmarks, including ImageNet-LT, Places-LT, iNaturalist 2018, CIFAR10-LT, and CIFAR100-LT.



### CrossedWires: A Dataset of Syntactically Equivalent but Semantically Disparate Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2108.12768v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12768v1)
- **Published**: 2021-08-29 07:27:16+00:00
- **Updated**: 2021-08-29 07:27:16+00:00
- **Authors**: Max Zvyagin, Thomas Brettin, Arvind Ramanathan, Sumit Kumar Jha
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: The training of neural networks using different deep learning frameworks may lead to drastically differing accuracy levels despite the use of the same neural network architecture and identical training hyperparameters such as learning rate and choice of optimization algorithms. Currently, our ability to build standardized deep learning models is limited by the availability of a suite of neural network and corresponding training hyperparameter benchmarks that expose differences between existing deep learning frameworks. In this paper, we present a living dataset of models and hyperparameters, called CrossedWires, that exposes semantic differences between two popular deep learning frameworks: PyTorch and Tensorflow. The CrossedWires dataset currently consists of models trained on CIFAR10 images using three different computer vision architectures: VGG16, ResNet50 and DenseNet121 across a large hyperparameter space. Using hyperparameter optimization, each of the three models was trained on 400 sets of hyperparameters suggested by the HyperSpace search algorithm. The CrossedWires dataset includes PyTorch and Tensforflow models with test accuracies as different as 0.681 on syntactically equivalent models and identical hyperparameter choices. The 340 GB dataset and benchmarks presented here include the performance statistics, training curves, and model weights for all 1200 hyperparameter choices, resulting in 2400 total models. The CrossedWires dataset provides an opportunity to study semantic differences between syntactically equivalent models across popular deep learning frameworks. Further, the insights obtained from this study can enable the development of algorithms and tools that improve reliability and reproducibility of deep learning frameworks. The dataset is freely available at https://github.com/maxzvyagin/crossedwires through a Python API and direct download link.



### RPR-Net: A Point Cloud-based Rotation-aware Large Scale Place Recognition Network
- **Arxiv ID**: http://arxiv.org/abs/2108.12790v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12790v3)
- **Published**: 2021-08-29 09:10:56+00:00
- **Updated**: 2022-08-28 04:07:03+00:00
- **Authors**: Zhaoxin Fan, Zhenbo Song, Wenping Zhang, Hongyan Liu, Jun He, Xiaoyong Du
- **Comment**: Accept to ECCV 2022 AVVision Workshop
- **Journal**: None
- **Summary**: Point cloud-based large scale place recognition is an important but challenging task for many applications such as Simultaneous Localization and Mapping (SLAM). Taking the task as a point cloud retrieval problem, previous methods have made delightful achievements. However, how to deal with catastrophic collapse caused by rotation problems is still under-explored. In this paper, to tackle the issue, we propose a novel Point Cloud-based Rotation-aware Large Scale Place Recognition Network (RPR-Net). In particular, to solve the problem, we propose to learn rotation-invariant features in three steps. First, we design three kinds of novel Rotation-Invariant Features (RIFs), which are low-level features that can hold the rotation-invariant property. Second, using these RIFs, we design an attentive module to learn rotation-invariant kernels. Third, we apply these kernels to previous point cloud features to generate new features, which is the well-known SO(3) mapping process. By doing so, high-level scene-specific rotation-invariant features can be learned. We call the above process an Attentive Rotation-Invariant Convolution (ARIConv). To achieve the place recognition goal, we build RPR-Net, which takes ARIConv as a basic unit to construct a dense network architecture. Then, powerful global descriptors used for retrieval-based place recognition can be sufficiently extracted from RPR-Net. Experimental results on prevalent datasets show that our method achieves comparable results to existing state-of-the-art place recognition models and significantly outperforms other rotation-invariant baseline models when solving rotation problems.



### Airplane Type Identification Based on Mask RCNN and Drone Images
- **Arxiv ID**: http://arxiv.org/abs/2108.12811v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12811v1)
- **Published**: 2021-08-29 10:25:13+00:00
- **Updated**: 2021-08-29 10:25:13+00:00
- **Authors**: W. T Alshaibani, Mustafa Helvaci, Ibraheem Shayea, Sawsan A. Saad, Azizul Azizan, Fitri Yakub
- **Comment**: 14 page
- **Journal**: None
- **Summary**: For dealing with traffic bottlenecks at airports, aircraft object detection is insufficient. Every airport generally has a variety of planes with various physical and technological requirements as well as diverse service requirements. Detecting the presence of new planes will not address all traffic congestion issues. Identifying the type of airplane, on the other hand, will entirely fix the problem because it will offer important information about the plane's technical specifications (i.e., the time it needs to be served and its appropriate place in the airport). Several studies have provided various contributions to address airport traffic jams; however, their ultimate goal was to determine the existence of airplane objects. This paper provides a practical approach to identify the type of airplane in airports depending on the results provided by the airplane detection process using mask region convolution neural network. The key feature employed to identify the type of airplane is the surface area calculated based on the results of airplane detection. The surface area is used to assess the estimated cabin length which is considered as an additional key feature for identifying the airplane type. The length of any detected plane may be calculated by measuring the distance between the detected plane's two furthest points. The suggested approach's performance is assessed using average accuracies and a confusion matrix. The findings show that this method is dependable. This method will greatly aid in the management of airport traffic congestion.



### A survey on IQA
- **Arxiv ID**: http://arxiv.org/abs/2109.00347v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.00347v2)
- **Published**: 2021-08-29 10:52:27+00:00
- **Updated**: 2022-01-11 14:28:22+00:00
- **Authors**: Lanjiang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Image quality assessment(IQA) is of increasing importance for image-based applications. Its purpose is to establish a model that can replace humans for accurately evaluating image quality. According to whether the reference image is complete and available, image quality evaluation can be divided into three categories: full-reference(FR), reduced-reference(RR), and non-reference(NR) image quality assessment. Due to the vigorous development of deep learning and the widespread attention of researchers, several non-reference image quality assessment methods based on deep learning have been proposed in recent years, and some have exceeded the performance of reduced -reference or even full-reference image quality assessment models. This article will review the concepts and metrics of image quality assessment and also video quality assessment, briefly introduce some methods of full-reference and semi-reference image quality assessment, and focus on the non-reference image quality assessment methods based on deep learning. Then introduce the commonly used synthetic database and real-world database. Finally, summarize and present challenges.



### Airplane Detection Based on Mask Region Convolution Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2108.12817v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12817v1)
- **Published**: 2021-08-29 10:55:18+00:00
- **Updated**: 2021-08-29 10:55:18+00:00
- **Authors**: W. T. Alshaibani, Mustafa Helvaci, Ibraheem Shayea, Hafizal Mohamad
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: Addressing airport traffic jams is one of the most crucial and challenging tasks in the remote sensing field, especially for the busiest airports. Several solutions have been employed to address this problem depending on the airplane detection process. The most effective solutions are through the use of satellite images with deep learning techniques. Such solutions, however, are significantly costly and require satellites and modern complicated technology which may not be available in most countries worldwide. This paper provides a universal, low cost and fast solution for airplane detection in airports. This paper recommends the use of drones instead of satellites to feed the system with drone images using a proposed deep learning model. Drone images are employed as the dataset to train and evaluate a mask region convolution neural network (RCNN) model. The Mask RCNN model applies faster RCNN as its base configuration with critical modifications on its head neural network constructions. The model detects whether or not an airplane is present and includes mask estimations to approximate surface area and length, which will help future works identify the airplane type. This solution can be easily implemented globally as it is a low-cost and fast solution for airplane detection at airports. The evaluation process reveals promising results according to Microsoft Common Objects in Context (COCO) metrics.



### MEDIC: A Multi-Task Learning Dataset for Disaster Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2108.12828v4
- **DOI**: 10.1007/s00521-022-07717-0
- **Categories**: **cs.CV**, cs.CY, cs.LG, cs.SI, 68T50, I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2108.12828v4)
- **Published**: 2021-08-29 11:55:50+00:00
- **Updated**: 2022-06-08 19:39:41+00:00
- **Authors**: Firoj Alam, Tanvirul Alam, Md. Arid Hasan, Abul Hasnat, Muhammad Imran, Ferda Ofli
- **Comment**: Multi-task Learning, Social media images, Image Classification,
  Natural disasters, Crisis Informatics, Deep learning, Dataset
- **Journal**: Neural Computing and Applications 35, 2609-2632 (2023)
- **Summary**: Recent research in disaster informatics demonstrates a practical and important use case of artificial intelligence to save human lives and suffering during natural disasters based on social media contents (text and images). While notable progress has been made using texts, research on exploiting the images remains relatively under-explored. To advance image-based approaches, we propose MEDIC (Available at: https://crisisnlp.qcri.org/medic/index.html), which is the largest social media image classification dataset for humanitarian response consisting of 71,198 images to address four different tasks in a multi-task learning setup. This is the first dataset of its kind: social media images, disaster response, and multi-task learning research. An important property of this dataset is its high potential to facilitate research on multi-task learning, which recently receives much interest from the machine learning community and has shown remarkable results in terms of memory, inference speed, performance, and generalization capability. Therefore, the proposed dataset is an important resource for advancing image-based disaster management and multi-task machine learning research. We experiment with different deep learning architectures and report promising results, which are above the majority baselines for all tasks. Along with the dataset, we also release all relevant scripts (https://github.com/firojalam/medic).



### Zero-shot Natural Language Video Localization
- **Arxiv ID**: http://arxiv.org/abs/2110.00428v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.00428v1)
- **Published**: 2021-08-29 13:21:50+00:00
- **Updated**: 2021-08-29 13:21:50+00:00
- **Authors**: Jinwoo Nam, Daechul Ahn, Dongyeop Kang, Seong Jong Ha, Jonghyun Choi
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Understanding videos to localize moments with natural language often requires large expensive annotated video regions paired with language queries. To eliminate the annotation costs, we make a first attempt to train a natural language video localization model in zero-shot manner. Inspired by unsupervised image captioning setup, we merely require random text corpora, unlabeled video collections, and an off-the-shelf object detector to train a model. With the unpaired data, we propose to generate pseudo-supervision of candidate temporal regions and corresponding query sentences, and develop a simple NLVL model to train with the pseudo-supervision. Our empirical validations show that the proposed pseudo-supervised method outperforms several baseline approaches and a number of methods using stronger supervision on Charades-STA and ActivityNet-Captions.



### Rethinking Deep Image Prior for Denoising
- **Arxiv ID**: http://arxiv.org/abs/2108.12841v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12841v1)
- **Published**: 2021-08-29 13:34:31+00:00
- **Updated**: 2021-08-29 13:34:31+00:00
- **Authors**: Yeonsik Jo, Se Young Chun, Jonghyun Choi
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Deep image prior (DIP) serves as a good inductive bias for diverse inverse problems. Among them, denoising is known to be particularly challenging for the DIP due to noise fitting with the requirement of an early stopping. To address the issue, we first analyze the DIP by the notion of effective degrees of freedom (DF) to monitor the optimization progress and propose a principled stopping criterion before fitting to noise without access of a paired ground truth image for Gaussian noise. We also propose the `stochastic temporal ensemble (STE)' method for incorporating techniques to further improve DIP's performance for denoising. We additionally extend our method to Poisson noise. Our empirical validations show that given a single noisy image, our method denoises the image while preserving rich textual details. Further, our approach outperforms prior arts in LPIPS by large margins with comparable PSNR and SSIM on seven different datasets.



### DASHA: Decentralized Autofocusing System with Hierarchical Agents
- **Arxiv ID**: http://arxiv.org/abs/2108.12842v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12842v2)
- **Published**: 2021-08-29 13:45:15+00:00
- **Updated**: 2022-02-02 11:52:08+00:00
- **Authors**: Anna Anikina, Oleg Y. Rogov, Dmitry V. Dylov
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art object detection models are frequently trained offline using available datasets, such as ImageNet: large and overly diverse data that are unbalanced and hard to cluster semantically. This kind of training drops the object detection performance should the change in illumination, in the environmental conditions (e.g., rain), or in the lens positioning (out-of-focus blur) occur. We propose a decentralized hierarchical multi-agent deep reinforcement learning approach for intelligently controlling the camera and the lens focusing settings, leading to a significant improvement beyond the capacity of the popular detection models (YOLO, Faster R-CNN, and Retina are considered). The algorithm relies on the latent representation of the camera's stream and, thus, it is the first method to allow a completely no-reference tuning of the camera, where the system trains itself to auto-focus itself.



### Flow-Guided Video Inpainting with Scene Templates
- **Arxiv ID**: http://arxiv.org/abs/2108.12845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.12845v1)
- **Published**: 2021-08-29 13:49:13+00:00
- **Updated**: 2021-08-29 13:49:13+00:00
- **Authors**: Dong Lao, Peihao Zhu, Peter Wonka, Ganesh Sundaramoorthi
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of filling in missing spatio-temporal regions of a video. We provide a novel flow-based solution by introducing a generative model of images in relation to the scene (without missing regions) and mappings from the scene to images. We use the model to jointly infer the scene template, a 2D representation of the scene, and the mappings. This ensures consistency of the frame-to-frame flows generated to the underlying scene, reducing geometric distortions in flow based inpainting. The template is mapped to the missing regions in the video by a new L2-L1 interpolation scheme, creating crisp inpaintings and reducing common blur and distortion artifacts. We show on two benchmark datasets that our approach out-performs state-of-the-art quantitatively and in user studies.



### Non-Parametric Neural Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2108.12847v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2108.12847v1)
- **Published**: 2021-08-29 13:56:24+00:00
- **Updated**: 2021-08-29 13:56:24+00:00
- **Authors**: Nicholas Kolkin
- **Comment**: PhD thesis
- **Journal**: None
- **Summary**: It seems easy to imagine a photograph of the Eiffel Tower painted in the style of Vincent van Gogh's 'The Starry Night', but upon introspection it is difficult to precisely define what this would entail. What visual elements must an image contain to represent the 'content' of the Eiffel Tower? What visual elements of 'The Starry Night' are caused by van Gogh's 'style' rather than his decision to depict a village under the night sky? Precisely defining 'content' and 'style' is a central challenge of designing algorithms for artistic style transfer, algorithms which can recreate photographs using an artwork's style. My efforts defining these terms, and designing style transfer algorithms themselves, are the focus of this thesis. I will begin by proposing novel definitions of style and content based on optimal transport and self-similarity, and demonstrating how a style transfer algorithm based on these definitions generates outputs with improved visual quality. Then I will describe how the traditional texture-based definition of style can be expanded to include elements of geometry and proportion by jointly optimizing a keypoint-guided deformation field alongside the stylized output's pixels. Finally I will describe a framework inspired by both modern neural style transfer algorithms and traditional patch-based synthesis approaches which is fast, general, and offers state-of-the-art visual quality.



### Differentiable Convolution Search for Point Cloud Processing
- **Arxiv ID**: http://arxiv.org/abs/2108.12856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12856v1)
- **Published**: 2021-08-29 14:42:03+00:00
- **Updated**: 2021-08-29 14:42:03+00:00
- **Authors**: Xing Nie, Yongcheng Liu, Shaohong Chen, Jianlong Chang, Chunlei Huo, Gaofeng Meng, Qi Tian, Weiming Hu, Chunhong Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Exploiting convolutional neural networks for point cloud processing is quite challenging, due to the inherent irregular distribution and discrete shape representation of point clouds. To address these problems, many handcrafted convolution variants have sprung up in recent years. Though with elaborate design, these variants could be far from optimal in sufficiently capturing diverse shapes formed by discrete points. In this paper, we propose PointSeaConv, i.e., a novel differential convolution search paradigm on point clouds. It can work in a purely data-driven manner and thus is capable of auto-creating a group of suitable convolutions for geometric shape modeling. We also propose a joint optimization framework for simultaneous search of internal convolution and external architecture, and introduce epsilon-greedy algorithm to alleviate the effect of discretization error. As a result, PointSeaNet, a deep network that is sufficient to capture geometric shapes at both convolution level and architecture level, can be searched out for point cloud processing. Extensive experiments strongly evidence that our proposed PointSeaNet surpasses current handcrafted deep models on challenging benchmarks across multiple tasks with remarkable margins.



### Edge-Cloud Collaborated Object Detection via Difficult-Case Discriminator
- **Arxiv ID**: http://arxiv.org/abs/2108.12858v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.12858v1)
- **Published**: 2021-08-29 14:55:14+00:00
- **Updated**: 2021-08-29 14:55:14+00:00
- **Authors**: Zhiqiang Cao, Zhijun Li, Pan Heng, Yongrui Chen, Daqi Xie, Jie Liu
- **Comment**: 10 pages infocom2022
- **Journal**: None
- **Summary**: As one of the basic tasks of computer vision, object detection has been widely used in many intelligent applications. However, object detection algorithms are usually heavyweight in computation, hindering their implementations on resource-constrained edge devices. Current edge-cloud collaboration methods, such as CNN partition over Edge-cloud devices, are not suitable for object detection since the huge data size of the intermediate results will introduce extravagant communication costs. To address this challenge, we propose a small-big model framework that deploys a big model in the cloud and a small model on the edge devices. Upon receiving data, the edge device operates a difficult-case discriminator to classify the images into easy cases and difficult cases according to the specific semantics of the images. The easy cases will be processed locally at the edge, and the difficult cases will be uploaded to the cloud. Experimental results on the VOC, COCO, HELMET datasets using two different object detection algorithms demonstrate that the small-big model system can detect 94.01%-97.84% of objects with only about 50% images uploaded to the cloud when using SSD. In addition, the small-big model averagely reaches 91.22%- 92.52% end-to-end mAP of the scheme that uploading all images to the cloud.



### MBDF-Net: Multi-Branch Deep Fusion Network for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.12863v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12863v1)
- **Published**: 2021-08-29 15:40:15+00:00
- **Updated**: 2021-08-29 15:40:15+00:00
- **Authors**: Xun Tan, Xingyu Chen, Guowei Zhang, Jishiyu Ding, Xuguang Lan
- **Comment**: None
- **Journal**: None
- **Summary**: Point clouds and images could provide complementary information when representing 3D objects. Fusing the two kinds of data usually helps to improve the detection results. However, it is challenging to fuse the two data modalities, due to their different characteristics and the interference from the non-interest areas. To solve this problem, we propose a Multi-Branch Deep Fusion Network (MBDF-Net) for 3D object detection. The proposed detector has two stages. In the first stage, our multi-branch feature extraction network utilizes Adaptive Attention Fusion (AAF) modules to produce cross-modal fusion features from single-modal semantic features. In the second stage, we use a region of interest (RoI) -pooled fusion module to generate enhanced local features for refinement. A novel attention-based hybrid sampling strategy is also proposed for selecting key points in the downsampling process. We evaluate our approach on two widely used benchmark datasets including KITTI and SUN-RGBD. The experimental results demonstrate the advantages of our method over state-of-the-art approaches.



### Partial Domain Adaptation without Domain Alignment
- **Arxiv ID**: http://arxiv.org/abs/2108.12867v2
- **DOI**: 10.1109/TPAMI.2022.3228937
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.12867v2)
- **Published**: 2021-08-29 15:57:20+00:00
- **Updated**: 2022-12-12 00:41:45+00:00
- **Authors**: Weikai Li, Songcan Chen
- **Comment**: 10 pages. IEEE Transactions on Pattern Analysis and Machine
  Intelligence,2022
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) aims to transfer knowledge from a well-labeled source domain to a different but related unlabeled target domain with identical label space. Currently, the main workhorse for solving UDA is domain alignment, which has proven successful. However, it is often difficult to find an appropriate source domain with identical label space. A more practical scenario is so-called partial domain adaptation (PDA) in which the source label set or space subsumes the target one. Unfortunately, in PDA, due to the existence of the irrelevant categories in the source domain, it is quite hard to obtain a perfect alignment, thus resulting in mode collapse and negative transfer. Although several efforts have been made by down-weighting the irrelevant source categories, the strategies used tend to be burdensome and risky since exactly which irrelevant categories are unknown. These challenges motivate us to find a relatively simpler alternative to solve PDA. To achieve this, we first provide a thorough theoretical analysis, which illustrates that the target risk is bounded by both model smoothness and between-domain discrepancy. Considering the difficulty of perfect alignment in solving PDA, we turn to focus on the model smoothness while discard the riskier domain alignment to enhance the adaptability of the model. Specifically, we instantiate the model smoothness as a quite simple intra-domain structure preserving (IDSP). To our best knowledge, this is the first naive attempt to address the PDA without domain alignment. Finally, our empirical results on multiple benchmark datasets demonstrate that IDSP is not only superior to the PDA SOTAs by a significant margin on some benchmarks (e.g., +10% on Cl->Rw and +8% on Ar->Rw ), but also complementary to domain alignment in the standard UDA



### A Multimodal Framework for Video Ads Understanding
- **Arxiv ID**: http://arxiv.org/abs/2108.12868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12868v1)
- **Published**: 2021-08-29 16:06:00+00:00
- **Updated**: 2021-08-29 16:06:00+00:00
- **Authors**: Zejia Weng, Lingchen Meng, Rui Wang, Zuxuan Wu, Yu-Gang Jiang
- **Comment**: 4 pages; 2 figures; ACM MM 2021 workshop; Tencent Advertising
  Algorithm Competition ACM Multimedia 2021 Grand Challenge
- **Journal**: None
- **Summary**: There is a growing trend in placing video advertisements on social platforms for online marketing, which demands automatic approaches to understand the contents of advertisements effectively. Taking the 2021 TAAC competition as an opportunity, we developed a multimodal system to improve the ability of structured analysis of advertising video content. In our framework, we break down the video structuring analysis problem into two tasks, i.e., scene segmentation and multi-modal tagging. In scene segmentation, we build upon a temporal convolution module for temporal modeling to predict whether adjacent frames belong to the same scene. In multi-modal tagging, we first compute clip-level visual features by aggregating frame-level features with NeXt-SoftDBoF. The visual features are further complemented with textual features that are derived using a global-local attention mechanism to extract useful information from OCR (Optical Character Recognition) and ASR (Audio Speech Recognition) outputs. Our solution achieved a score of 0.2470 measured in consideration of localization and prediction accuracy, ranking fourth in the 2021 TAAC final leaderboard.



### Solving Viewing Graph Optimization for Simultaneous Position and Rotation Registration
- **Arxiv ID**: http://arxiv.org/abs/2108.12876v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12876v1)
- **Published**: 2021-08-29 16:52:18+00:00
- **Updated**: 2021-08-29 16:52:18+00:00
- **Authors**: Seyed-Mahdi Nasiri, Reshad Hosseini, Hadi Moradi
- **Comment**: None
- **Journal**: None
- **Summary**: A viewing graph is a set of unknown camera poses, as the vertices, and the observed relative motions, as the edges. Solving the viewing graph is an essential step in a Structure-from-Motion procedure, where a set of relative motions is obtained from a collection of 2D images. Almost all methods in the literature solve for the rotations separately, through rotation averaging process, and use them for solving the positions. Obtaining positions is the challenging part because the translation observations only tell the direction of the motions. It becomes more challenging when the set of edges comprises pairwise translation observations between either near and far cameras. In this paper an iterative method is proposed that overcomes these issues. Also a method is proposed which obtains the rotations and positions simultaneously. Experimental results show the-state-of-the-art performance of the proposed methods.



### Ideals and Virtual Realities
- **Arxiv ID**: http://arxiv.org/abs/2109.00926v1
- **DOI**: None
- **Categories**: **physics.ed-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.00926v1)
- **Published**: 2021-08-29 18:08:13+00:00
- **Updated**: 2021-08-29 18:08:13+00:00
- **Authors**: E. Canessa, L. Tenze
- **Comment**: 9 pages, 4 figures, To appear iJET 2021
- **Journal**: None
- **Summary**: A main step for world progress is to keep sharing ever-present Ideals for science and education within today Virtual Realities. On-line education is transforming human society to new levels in the way people teach and learn during the ongoing SARS-CoV-2 pandemic. There is an increasing interest in having more and more reliable, fast and simple apps to communicate and also to record, assemble and distribute videos and lectures in the fields of Physics & Maths still using traditional didactic methods. We describe here how to accurately reproduce chalkboard classes for the popular YouTube video platform using OpenEyA-YT. The audience can thus be expanded over continents to help mitigate the effects of physical isolation.



### Autonomous Curiosity for Real-Time Training Onboard Robotic Agents
- **Arxiv ID**: http://arxiv.org/abs/2109.00927v1
- **DOI**: 10.1109/WACV.2019.00163
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.00927v1)
- **Published**: 2021-08-29 19:48:19+00:00
- **Updated**: 2021-08-29 19:48:19+00:00
- **Authors**: Ervin Teng, Bob Iannucci
- **Comment**: 10 pages, 9 figures. Accepted in IEEE Winter Conference on
  Applications of Computer Vision (WACV), 2019. arXiv admin note: text overlap
  with arXiv:1902.01569
- **Journal**: Proceedings of the 2019 IEEE Winter Conference on Applications of
  Computer Vision (WACV), 1486 - 1495
- **Summary**: Learning requires both study and curiosity. A good learner is not only good at extracting information from the data given to it, but also skilled at finding the right new information to learn from. This is especially true when a human operator is required to provide the ground truth - such a source should only be queried sparingly. In this work, we address the problem of curiosity as it relates to online, real-time, human-in-the-loop training of an object detection algorithm onboard a robotic platform, one where motion produces new views of the subject. We propose a deep reinforcement learning approach that decides when to ask the human user for ground truth, and when to move. Through a series of experiments, we demonstrate that our agent learns a movement and request policy that is at least 3x more effective at using human user interactions to train an object detector than untrained approaches, and is generalizable to a variety of subjects and environments.



### Layout-to-Image Translation with Double Pooling Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.12900v1
- **DOI**: 10.1109/TIP.2021.3109531
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12900v1)
- **Published**: 2021-08-29 19:55:14+00:00
- **Updated**: 2021-08-29 19:55:14+00:00
- **Authors**: Hao Tang, Nicu Sebe
- **Comment**: Accepted to TIP
- **Journal**: None
- **Summary**: In this paper, we address the task of layout-to-image translation, which aims to translate an input semantic layout to a realistic image. One open challenge widely observed in existing methods is the lack of effective semantic constraints during the image translation process, leading to models that cannot preserve the semantic information and ignore the semantic dependencies within the same object. To address this issue, we propose a novel Double Pooing GAN (DPGAN) for generating photo-realistic and semantically-consistent results from the input layout. We also propose a novel Double Pooling Module (DPM), which consists of the Square-shape Pooling Module (SPM) and the Rectangle-shape Pooling Module (RPM). Specifically, SPM aims to capture short-range semantic dependencies of the input layout with different spatial scales, while RPM aims to capture long-range semantic dependencies from both horizontal and vertical directions. We then effectively fuse both outputs of SPM and RPM to further enlarge the receptive field of our generator. Extensive experiments on five popular datasets show that the proposed DPGAN achieves better results than state-of-the-art methods. Finally, both SPM and SPM are general and can be seamlessly integrated into any GAN-based architectures to strengthen the feature representation. The code is available at https://github.com/Ha0Tang/DPGAN.



### Lipschitz Continuity Guided Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2108.12905v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12905v1)
- **Published**: 2021-08-29 20:19:34+00:00
- **Updated**: 2021-08-29 20:19:34+00:00
- **Authors**: Yuzhang Shang, Bin Duan, Ziliang Zong, Liqiang Nie, Yan Yan
- **Comment**: This work has been accepted by ICCV 2021
- **Journal**: None
- **Summary**: Knowledge distillation has become one of the most important model compression techniques by distilling knowledge from larger teacher networks to smaller student ones. Although great success has been achieved by prior distillation methods via delicately designing various types of knowledge, they overlook the functional properties of neural networks, which makes the process of applying those techniques to new tasks unreliable and non-trivial. To alleviate such problem, in this paper, we initially leverage Lipschitz continuity to better represent the functional characteristic of neural networks and guide the knowledge distillation process. In particular, we propose a novel Lipschitz Continuity Guided Knowledge Distillation framework to faithfully distill knowledge by minimizing the distance between two neural networks' Lipschitz constants, which enables teacher networks to better regularize student networks and improve the corresponding performance. We derive an explainable approximation algorithm with an explicit theoretical derivation to address the NP-hard problem of calculating the Lipschitz constant. Experimental results have shown that our method outperforms other benchmarks over several knowledge distillation tasks (e.g., classification, segmentation and object detection) on CIFAR-100, ImageNet, and PASCAL VOC datasets.



### NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.12931v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.12931v1)
- **Published**: 2021-08-29 22:43:52+00:00
- **Updated**: 2021-08-29 22:43:52+00:00
- **Authors**: Haekyu Park, Nilaksh Das, Rahul Duggal, Austin P. Wright, Omar Shaikh, Fred Hohman, Duen Horng Chau
- **Comment**: Accepted to IEEE VIS'21
- **Journal**: None
- **Summary**: Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present NeuroCartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. NeuroCartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting "dog faces" of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting "dog face" and "dog tail" are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs' relationships, in time linear to the number of neurons instead of quadratic time. NeuroCartography scales to large data, such as the ImageNet dataset with 1.2M images. The system's tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The NeuroCartography visualization runs in modern browsers and is open-sourced.



