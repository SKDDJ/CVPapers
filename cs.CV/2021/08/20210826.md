# Arxiv Papers in cs.CV on 2021-08-26
### ChessMix: Spatial Context Data Augmentation for Remote Sensing Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.11535v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.11535v1)
- **Published**: 2021-08-26 01:01:43+00:00
- **Updated**: 2021-08-26 01:01:43+00:00
- **Authors**: Matheus Barros Pereira, Jefersson Alex dos Santos
- **Comment**: None
- **Journal**: None
- **Summary**: Labeling semantic segmentation datasets is a costly and laborious process if compared with tasks like image classification and object detection. This is especially true for remote sensing applications that not only work with extremely high spatial resolution data but also commonly require the knowledge of experts of the area to perform the manual labeling. Data augmentation techniques help to improve deep learning models under the circumstance of few and imbalanced labeled samples. In this work, we propose a novel data augmentation method focused on exploring the spatial context of remote sensing semantic segmentation. This method, ChessMix, creates new synthetic images from the existing training set by mixing transformed mini-patches across the dataset in a chessboard-like grid. ChessMix prioritizes patches with more examples of the rarest classes to alleviate the imbalance problems. The results in three diverse well-known remote sensing datasets show that this is a promising approach that helps to improve the networks' performance, working especially well in datasets with few available data. The results also show that ChessMix is capable of improving the segmentation of objects with few labeled pixels when compared to the most common data augmentation methods widely used.



### TPH-YOLOv5: Improved YOLOv5 Based on Transformer Prediction Head for Object Detection on Drone-captured Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2108.11539v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2108.11539v1)
- **Published**: 2021-08-26 01:24:15+00:00
- **Updated**: 2021-08-26 01:24:15+00:00
- **Authors**: Xingkui Zhu, Shuchang Lyu, Xu Wang, Qi Zhao
- **Comment**: 8 pages,9 figures, VisDrone 2021 ICCV workshop
- **Journal**: None
- **Summary**: Object detection on drone-captured scenarios is a recent popular task. As drones always navigate in different altitudes, the object scale varies violently, which burdens the optimization of networks. Moreover, high-speed and low-altitude flight bring in the motion blur on the densely packed objects, which leads to great challenge of object distinction. To solve the two issues mentioned above, we propose TPH-YOLOv5. Based on YOLOv5, we add one more prediction head to detect different-scale objects. Then we replace the original prediction heads with Transformer Prediction Heads (TPH) to explore the prediction potential with self-attention mechanism. We also integrate convolutional block attention model (CBAM) to find attention region on scenarios with dense objects. To achieve more improvement of our proposed TPH-YOLOv5, we provide bags of useful strategies such as data augmentation, multiscale testing, multi-model integration and utilizing extra classifier. Extensive experiments on dataset VisDrone2021 show that TPH-YOLOv5 have good performance with impressive interpretability on drone-captured scenarios. On DET-test-challenge dataset, the AP result of TPH-YOLOv5 are 39.18%, which is better than previous SOTA method (DPNetV3) by 1.81%. On VisDrone Challenge 2021, TPHYOLOv5 wins 5th place and achieves well-matched results with 1st place model (AP 39.43%). Compared to baseline model (YOLOv5), TPH-YOLOv5 improves about 7%, which is encouraging and competitive.



### Vision-Language Navigation: A Survey and Taxonomy
- **Arxiv ID**: http://arxiv.org/abs/2108.11544v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.11544v3)
- **Published**: 2021-08-26 01:51:18+00:00
- **Updated**: 2022-04-02 02:12:14+00:00
- **Authors**: Wansen Wu, Tao Chang, Xinmeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-Language Navigation (VLN) tasks require an agent to follow human language instructions to navigate in previously unseen environments. This challenging field involving problems in natural language processing, computer vision, robotics, etc., has spawn many excellent works focusing on various VLN tasks. This paper provides a comprehensive survey and an insightful taxonomy of these tasks based on the different characteristics of language instructions in these tasks. Depending on whether the navigation instructions are given for once or multiple times, this paper divides the tasks into two categories, i.e., single-turn and multi-turn tasks. For single-turn tasks, we further subdivide them into goal-oriented and route-oriented based on whether the instructions designate a single goal location or specify a sequence of multiple locations. For multi-turn tasks, we subdivide them into passive and interactive tasks based on whether the agent is allowed to question the instruction or not. These tasks require different capabilities of the agent and entail various model designs. We identify progress made on the tasks and look into the limitations of existing VLN models and task settings. Finally, we discuss several open issues of VLN and point out some opportunities in the future, i.e., incorporating knowledge with VLN models and implementing them in the real physical world.



### The Surprising Effectiveness of Visual Odometry Techniques for Embodied PointGoal Navigation
- **Arxiv ID**: http://arxiv.org/abs/2108.11550v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.11550v1)
- **Published**: 2021-08-26 02:12:49+00:00
- **Updated**: 2021-08-26 02:12:49+00:00
- **Authors**: Xiaoming Zhao, Harsh Agrawal, Dhruv Batra, Alexander Schwing
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: It is fundamental for personal robots to reliably navigate to a specified goal. To study this task, PointGoal navigation has been introduced in simulated Embodied AI environments. Recent advances solve this PointGoal navigation task with near-perfect accuracy (99.6% success) in photo-realistically simulated environments, assuming noiseless egocentric vision, noiseless actuation, and most importantly, perfect localization. However, under realistic noise models for visual sensors and actuation, and without access to a "GPS and Compass sensor," the 99.6%-success agents for PointGoal navigation only succeed with 0.3%. In this work, we demonstrate the surprising effectiveness of visual odometry for the task of PointGoal navigation in this realistic setting, i.e., with realistic noise models for perception and actuation and without access to GPS and Compass sensors. We show that integrating visual odometry techniques into navigation policies improves the state-of-the-art on the popular Habitat PointNav benchmark by a large margin, improving success from 64.5% to 71.7% while executing 6.4 times faster.



### XCI-Sketch: Extraction of Color Information from Images for Generation of Colored Outlines and Sketches
- **Arxiv ID**: http://arxiv.org/abs/2108.11554v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.11554v2)
- **Published**: 2021-08-26 02:27:55+00:00
- **Updated**: 2022-01-07 13:10:09+00:00
- **Authors**: V Manushree, Sameer Saxena, Parna Chowdhury, Manisimha Varma, Harsh Rathod, Ankita Ghosh, Sahil Khose
- **Comment**: ML for Creativity and Design workshop at NeurIPS 2021
- **Journal**: None
- **Summary**: Sketches are a medium to convey a visual scene from an individual's creative perspective. The addition of color substantially enhances the overall expressivity of a sketch. This paper proposes two methods to mimic human-drawn colored sketches by utilizing the Contour Drawing Dataset. Our first approach renders colored outline sketches by applying image processing techniques aided by k-means color clustering. The second method uses a generative adversarial network to develop a model that can generate colored sketches from previously unobserved images. We assess the results obtained through quantitative and qualitative evaluations.



### Identity-aware Graph Memory Network for Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.11559v1
- **DOI**: 10.1145/3474085.3475503
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11559v1)
- **Published**: 2021-08-26 02:34:55+00:00
- **Updated**: 2021-08-26 02:34:55+00:00
- **Authors**: Jingcheng Ni, Jie Qin, Di Huang
- **Comment**: Accepted by ACM MM2021
- **Journal**: None
- **Summary**: Action detection plays an important role in high-level video understanding and media interpretation. Many existing studies fulfill this spatio-temporal localization by modeling the context, capturing the relationship of actors, objects, and scenes conveyed in the video. However, they often universally treat all the actors without considering the consistency and distinctness between individuals, leaving much room for improvement. In this paper, we explicitly highlight the identity information of the actors in terms of both long-term and short-term context through a graph memory network, namely identity-aware graph memory network (IGMN). Specifically, we propose the hierarchical graph neural network (HGNN) to comprehensively conduct long-term relation modeling within the same identity as well as between different ones. Regarding short-term context, we develop a dual attention module (DAM) to generate identity-aware constraint to reduce the influence of interference by the actors of different identities. Extensive experiments on the challenging AVA dataset demonstrate the effectiveness of our method, which achieves state-of-the-art results on AVA v2.1 and v2.2.



### NeighCNN: A CNN based SAR Speckle Reduction using Feature preserving Loss Function
- **Arxiv ID**: http://arxiv.org/abs/2108.11573v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11573v1)
- **Published**: 2021-08-26 04:20:07+00:00
- **Updated**: 2021-08-26 04:20:07+00:00
- **Authors**: Praveen Ravirathinam, Darshan Agrawal, J. Jennifer Ranjani
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Coherent imaging systems like synthetic aperture radar are susceptible to multiplicative noise that makes applications like automatic target recognition challenging. In this paper, NeighCNN, a deep learning-based speckle reduction algorithm that handles multiplicative noise with relatively simple convolutional neural network architecture, is proposed. We have designed a loss function which is an unique combination of weighted sum of Euclidean, neighbourhood, and perceptual loss for training the deep network. Euclidean and neighbourhood losses take pixel-level information into account, whereas perceptual loss considers high-level semantic features between two images. Various synthetic, as well as real SAR images, are used for testing the NeighCNN architecture, and the results verify the noise removal and edge preservation abilities of the proposed architecture. Performance metrics like peak-signal-to-noise ratio, structural similarity index, and universal image quality index are used for evaluating the efficiency of the proposed architecture on synthetic images.



### Shifted Chunk Transformer for Spatio-Temporal Representational Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.11575v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11575v5)
- **Published**: 2021-08-26 04:34:33+00:00
- **Updated**: 2021-10-29 03:02:26+00:00
- **Authors**: Xuefan Zha, Wentao Zhu, Tingxun Lv, Sen Yang, Ji Liu
- **Comment**: 15 pages, 3 figures
- **Journal**: None
- **Summary**: Spatio-temporal representational learning has been widely adopted in various fields such as action recognition, video object segmentation, and action anticipation. Previous spatio-temporal representational learning approaches primarily employ ConvNets or sequential models,e.g., LSTM, to learn the intra-frame and inter-frame features. Recently, Transformer models have successfully dominated the study of natural language processing (NLP), image classification, etc. However, the pure-Transformer based spatio-temporal learning can be prohibitively costly on memory and computation to extract fine-grained features from a tiny patch. To tackle the training difficulty and enhance the spatio-temporal learning, we construct a shifted chunk Transformer with pure self-attention blocks. Leveraging the recent efficient Transformer design in NLP, this shifted chunk Transformer can learn hierarchical spatio-temporal features from a local tiny patch to a global video clip. Our shifted self-attention can also effectively model complicated inter-frame variances. Furthermore, we build a clip encoder based on Transformer to model long-term temporal dependencies. We conduct thorough ablation studies to validate each component and hyper-parameters in our shifted chunk Transformer, and it outperforms previous state-of-the-art approaches on Kinetics-400, Kinetics-600, UCF101, and HMDB51.



### Unsupervised Dense Deformation Embedding Network for Template-Free Shape Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2108.11609v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2108.11609v1)
- **Published**: 2021-08-26 07:07:19+00:00
- **Updated**: 2021-08-26 07:07:19+00:00
- **Authors**: Ronghan Chen, Yang Cong, Jiahua Dong
- **Comment**: 15 pages, 18 figures. Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Shape correspondence from 3D deformation learning has attracted appealing academy interests recently. Nevertheless, current deep learning based methods require the supervision of dense annotations to learn per-point translations, which severely overparameterize the deformation process. Moreover, they fail to capture local geometric details of original shape via global feature embedding. To address these challenges, we develop a new Unsupervised Dense Deformation Embedding Network (i.e., UD^2E-Net), which learns to predict deformations between non-rigid shapes from dense local features. Since it is non-trivial to match deformation-variant local features for deformation prediction, we develop an Extrinsic-Intrinsic Autoencoder to frst encode extrinsic geometric features from source into intrinsic coordinates in a shared canonical shape, with which the decoder then synthesizes corresponding target features. Moreover, a bounded maximum mean discrepancy loss is developed to mitigate the distribution divergence between the synthesized and original features. To learn natural deformation without dense supervision, we introduce a coarse parameterized deformation graph, for which a novel trace and propagation algorithm is proposed to improve both the quality and effciency of the deformation. Our UD^2E-Net outperforms state-of-the-art unsupervised methods by 24% on Faust Inter challenge and even supervised methods by 13% on Faust Intra challenge.



### Few-shot Visual Relationship Co-localization
- **Arxiv ID**: http://arxiv.org/abs/2108.11618v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.11618v1)
- **Published**: 2021-08-26 07:19:57+00:00
- **Updated**: 2021-08-26 07:19:57+00:00
- **Authors**: Revant Teotia, Vaibhav Mishra, Mayank Maheshwari, Anand Mishra
- **Comment**: Accepted in ICCV 2021
- **Journal**: None
- **Summary**: In this paper, given a small bag of images, each containing a common but latent predicate, we are interested in localizing visual subject-object pairs connected via the common predicate in each of the images. We refer to this novel problem as visual relationship co-localization or VRC as an abbreviation. VRC is a challenging task, even more so than the well-studied object co-localization task. This becomes further challenging when using just a few images, the model has to learn to co-localize visual subject-object pairs connected via unseen predicates. To solve VRC, we propose an optimization framework to select a common visual relationship in each image of the bag. The goal of the optimization framework is to find the optimal solution by learning visual relationship similarity across images in a few-shot setting. To obtain robust visual relationship representation, we utilize a simple yet effective technique that learns relationship embedding as a translation vector from visual subject to visual object in a shared space. Further, to learn visual relationship similarity, we utilize a proven meta-learning technique commonly used for few-shot classification tasks. Finally, to tackle the combinatorial complexity challenge arising from an exponential number of feasible solutions, we use a greedy approximation inference algorithm that selects approximately the best solution.   We extensively evaluate our proposed framework on variations of bag sizes obtained from two challenging public datasets, namely VrR-VG and VG-150, and achieve impressive visual co-localization performance.



### Web Image Context Extraction with Graph Neural Networks and Sentence Embeddings on the DOM tree
- **Arxiv ID**: http://arxiv.org/abs/2108.11629v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11629v1)
- **Published**: 2021-08-26 07:49:28+00:00
- **Updated**: 2021-08-26 07:49:28+00:00
- **Authors**: Chen Dang, Hicham Randrianarivo, Raphaël Fournier-S'Niehotta, Nicolas Audebert
- **Comment**: None
- **Journal**: GEM: Graph Embedding and Mining - ECML/PKDD Workshops, Sep 2021,
  Bilbao, Spain
- **Summary**: Web Image Context Extraction (WICE) consists in obtaining the textual information describing an image using the content of the surrounding webpage. A common preprocessing step before performing WICE is to render the content of the webpage. When done at a large scale (e.g., for search engine indexation), it may become very computationally costly (up to several seconds per page). To avoid this cost, we introduce a novel WICE approach that combines Graph Neural Networks (GNNs) and Natural Language Processing models. Our method relies on a graph model containing both node types and text as features. The model is fed through several blocks of GNNs to extract the textual context. Since no labeled WICE dataset with ground truth exists, we train and evaluate the GNNs on a proxy task that consists in finding the semantically closest text to the image caption. We then interpret importance weights to find the most relevant text nodes and define them as the image context. Thanks to GNNs, our model is able to encode both structural and semantic information from the webpage. We show that our approach gives promising results to help address the large-scale WICE problem using only HTML data.



### SketchLattice: Latticed Representation for Sketch Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2108.11636v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11636v1)
- **Published**: 2021-08-26 08:02:21+00:00
- **Updated**: 2021-08-26 08:02:21+00:00
- **Authors**: Yonggang Qi, Guoyao Su, Pinaki Nath Chowdhury, Mingkang Li, Yi-Zhe Song
- **Comment**: accepted to ICCV 2021
- **Journal**: None
- **Summary**: The key challenge in designing a sketch representation lies with handling the abstract and iconic nature of sketches. Existing work predominantly utilizes either, (i) a pixelative format that treats sketches as natural images employing off-the-shelf CNN-based networks, or (ii) an elaborately designed vector format that leverages the structural information of drawing orders using sequential RNN-based methods. While the pixelative format lacks intuitive exploitation of structural cues, sketches in vector format are absent in most cases limiting their practical usage. Hence, in this paper, we propose a lattice structured sketch representation that not only removes the bottleneck of requiring vector data but also preserves the structural cues that vector data provides. Essentially, sketch lattice is a set of points sampled from the pixelative format of the sketch using a lattice graph. We show that our lattice structure is particularly amenable to structural changes that largely benefits sketch abstraction modeling for generation tasks. Our lattice representation could be effectively encoded using a graph model, that uses significantly fewer model parameters (13.5 times lesser) than existing state-of-the-art. Extensive experiments demonstrate the effectiveness of sketch lattice for sketch manipulation, including sketch healing and image-to-sketch synthesis.



### StackMix and Blot Augmentations for Handwritten Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.11667v1
- **DOI**: None
- **Categories**: **cs.CV**, 68-04, I.7.5; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2108.11667v1)
- **Published**: 2021-08-26 09:28:22+00:00
- **Updated**: 2021-08-26 09:28:22+00:00
- **Authors**: Alex Shonenkov, Denis Karachev, Maxim Novopoltsev, Mark Potanin, Denis Dimitrov
- **Comment**: 17 pages, 9 figures
- **Journal**: None
- **Summary**: This paper proposes a handwritten text recognition(HTR) system that outperforms current state-of-the-artmethods. The comparison was carried out on three of themost frequently used in HTR task datasets, namely Ben-tham, IAM, and Saint Gall. In addition, the results on tworecently presented datasets, Peter the Greats manuscriptsand HKR Dataset, are provided.The paper describes the architecture of the neural net-work and two ways of increasing the volume of train-ing data: augmentation that simulates strikethrough text(HandWritten Blots) and a new text generation method(StackMix), which proved to be very effective in HTR tasks.StackMix can also be applied to the standalone task of gen-erating handwritten text based on printed text.



### A Robust Loss for Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2108.11682v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11682v1)
- **Published**: 2021-08-26 09:56:47+00:00
- **Updated**: 2021-08-26 09:56:47+00:00
- **Authors**: Zhi Deng, Yuxin Yao, Bailin Deng, Juyong Zhang
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: The performance of surface registration relies heavily on the metric used for the alignment error between the source and target shapes. Traditionally, such a metric is based on the point-to-point or point-to-plane distance from the points on the source surface to their closest points on the target surface, which is susceptible to failure due to instability of the closest-point correspondence. In this paper, we propose a novel metric based on the intersection points between the two shapes and a random straight line, which does not assume a specific correspondence. We verify the effectiveness of this metric by extensive experiments, including its direct optimization for a single registration problem as well as unsupervised learning for a set of registration problems. The results demonstrate that the algorithms utilizing our proposed metric outperforms the state-of-the-art optimization-based and unsupervised learning-based methods.



### Improving the Reliability of Semantic Segmentation of Medical Images by Uncertainty Modeling with Bayesian Deep Networks and Curriculum Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.11693v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2108.11693v1)
- **Published**: 2021-08-26 10:24:02+00:00
- **Updated**: 2021-08-26 10:24:02+00:00
- **Authors**: Sora Iwamoto, Bisser Raytchev, Toru Tamaki, Kazufumi Kaneda
- **Comment**: MICCAI UNSURE2021 Workshop
- **Journal**: None
- **Summary**: In this paper we propose a novel method which leverages the uncertainty measures provided by Bayesian deep networks through curriculum learning so that the uncertainty estimates are fed back to the system to resample the training data more densely in areas where uncertainty is high. We show in the concrete setting of a semantic segmentation task (iPS cell colony segmentation) that the proposed system is able to increase significantly the reliability of the model.



### PoissonSeg: Semi-Supervised Few-Shot Medical Image Segmentation via Poisson Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.11694v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.11694v2)
- **Published**: 2021-08-26 10:24:04+00:00
- **Updated**: 2021-10-24 03:01:40+00:00
- **Authors**: Xiaoang Shen, Guokai Zhang, Huilin Lai, Jihao Luo, Jianwei Lu, Ye Luo
- **Comment**: Accepted by 2021 IEEE International Conference on Bioinformatics and
  Biomedicine (BIBM 2021)
- **Journal**: None
- **Summary**: The application of deep learning to medical image segmentation has been hampered due to the lack of abundant pixel-level annotated data. Few-shot Semantic Segmentation (FSS) is a promising strategy for breaking the deadlock. However, a high-performing FSS model still requires sufficient pixel-level annotated classes for training to avoid overfitting, which leads to its performance bottleneck in medical image segmentation due to the unmet need for annotations. Thus, semi-supervised FSS for medical images is accordingly proposed to utilize unlabeled data for further performance improvement. Nevertheless, existing semi-supervised FSS methods has two obvious defects: (1) neglecting the relationship between the labeled and unlabeled data; (2) using unlabeled data directly for end-to-end training leads to degenerated representation learning. To address these problems, we propose a novel semi-supervised FSS framework for medical image segmentation. The proposed framework employs Poisson learning for modeling data relationship and propagating supervision signals, and Spatial Consistency Calibration for encouraging the model to learn more coherent representations. In this process, unlabeled samples do not involve in end-to-end training, but provide supervisory information for query image segmentation through graph-based learning. We conduct extensive experiments on three medical image segmentation datasets (i.e. ISIC skin lesion segmentation, abdominal organs segmentation for MRI and abdominal organs segmentation for CT) to demonstrate the state-of-the-art performance and broad applicability of the proposed framework.



### PAENet: A Progressive Attention-Enhanced Network for 3D to 2D Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.11695v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11695v5)
- **Published**: 2021-08-26 10:27:25+00:00
- **Updated**: 2021-12-16 13:08:23+00:00
- **Authors**: Zhuojie Wu, Zijian Wang, Wenxuan Zou, Fan Ji, Hao Dang, Wanting Zhou, Muyi Sun
- **Comment**: Accepted by BIBM 2021
- **Journal**: None
- **Summary**: 3D to 2D retinal vessel segmentation is a challenging problem in Optical Coherence Tomography Angiography (OCTA) images. Accurate retinal vessel segmentation is important for the diagnosis and prevention of ophthalmic diseases. However, making full use of the 3D data of OCTA volumes is a vital factor for obtaining satisfactory segmentation results. In this paper, we propose a Progressive Attention-Enhanced Network (PAENet) based on attention mechanisms to extract rich feature representation. Specifically, the framework consists of two main parts, the three-dimensional feature learning path and the two-dimensional segmentation path. In the three-dimensional feature learning path, we design a novel Adaptive Pooling Module (APM) and propose a new Quadruple Attention Module (QAM). The APM captures dependencies along the projection direction of volumes and learns a series of pooling coefficients for feature fusion, which efficiently reduces feature dimension. In addition, the QAM reweights the features by capturing four-group cross-dimension dependencies, which makes maximum use of 4D feature tensors. In the two-dimensional segmentation path, to acquire more detailed information, we propose a Feature Fusion Module (FFM) to inject 3D information into the 2D path. Meanwhile, we adopt the Polarized Self-Attention (PSA) block to model the semantic interdependencies in spatial and channel dimensions respectively. Experimentally, our extensive experiments on the OCTA-500 dataset show that our proposed algorithm achieves state-of-the-art performance compared with previous methods.



### Glimpse-Attend-and-Explore: Self-Attention for Active Visual Exploration
- **Arxiv ID**: http://arxiv.org/abs/2108.11717v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11717v1)
- **Published**: 2021-08-26 11:41:03+00:00
- **Updated**: 2021-08-26 11:41:03+00:00
- **Authors**: Soroush Seifi, Abhishek Jha, Tinne Tuytelaars
- **Comment**: None
- **Journal**: None
- **Summary**: Active visual exploration aims to assist an agent with a limited field of view to understand its environment based on partial observations made by choosing the best viewing directions in the scene. Recent methods have tried to address this problem either by using reinforcement learning, which is difficult to train, or by uncertainty maps, which are task-specific and can only be implemented for dense prediction tasks. In this paper, we propose the Glimpse-Attend-and-Explore model which: (a) employs self-attention to guide the visual exploration instead of task-specific uncertainty maps; (b) can be used for both dense and sparse prediction tasks; and (c) uses a contrastive stream to further improve the representations learned. Unlike previous works, we show the application of our model on multiple tasks like reconstruction, segmentation and classification. Our model provides encouraging results while being less dependent on dataset bias in driving the exploration. We further perform an ablation study to investigate the features and attention learned by our model. Finally, we show that our self-attention module learns to attend different regions of the scene by minimizing the loss on the downstream task. Code: https://github.com/soroushseifi/glimpse-attend-explore.



### Benchmarking high-fidelity pedestrian tracking systems for research, real-time monitoring and crowd control
- **Arxiv ID**: http://arxiv.org/abs/2108.11719v1
- **DOI**: 10.17815/CD.2021.134
- **Categories**: **physics.soc-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11719v1)
- **Published**: 2021-08-26 11:45:26+00:00
- **Updated**: 2021-08-26 11:45:26+00:00
- **Authors**: Caspar A. S. Pouw, Joris Willems, Frank van Schadewijk, Jasmin Thurau, Federico Toschi, Alessandro Corbetta
- **Comment**: None
- **Journal**: Collective Dynamics. v. 6 p. 1-22, 2022
- **Summary**: High-fidelity pedestrian tracking in real-life conditions has been an important tool in fundamental crowd dynamics research allowing to quantify statistics of relevant observables including walking velocities, mutual distances and body orientations. As this technology advances, it is becoming increasingly useful also in society. In fact, continued urbanization is overwhelming existing pedestrian infrastructures such as transportation hubs and stations, generating an urgent need for real-time highly-accurate usage data, aiming both at flow monitoring and dynamics understanding. To successfully employ pedestrian tracking techniques in research and technology, it is crucial to validate and benchmark them for accuracy. This is not only necessary to guarantee data quality, but also to identify systematic errors.   In this contribution, we present and discuss a benchmark suite, towards an open standard in the community, for privacy-respectful pedestrian tracking techniques. The suite is technology-independent and is applicable to academic and commercial pedestrian tracking systems, operating both in lab environments and real-life conditions. The benchmark suite consists of 5 tests addressing specific aspects of pedestrian tracking quality, including accurate crowd flux estimation, density estimation, position detection and trajectory accuracy. The output of the tests are quality factors expressed as single numbers. We provide the benchmark results for two tracking systems, both operating in real-life, one commercial, and the other based on overhead depth-maps developed at TU Eindhoven. We discuss the results on the basis of the quality factors and report on the typical sensor and algorithmic performance. This enables us to highlight the current state-of-the-art, its limitations and provide installation recommendations, with specific attention to multi-sensor setups and data stitching.



### Segmentation of Shoulder Muscle MRI Using a New Region and Edge based Deep Auto-Encoder
- **Arxiv ID**: http://arxiv.org/abs/2108.11720v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11720v1)
- **Published**: 2021-08-26 11:51:43+00:00
- **Updated**: 2021-08-26 11:51:43+00:00
- **Authors**: Saddam Hussain Khan, Asifullah Khan, Yeon Soo Lee, Mehdi Hassan, Woong Kyo jeong
- **Comment**: Pages: 23, 8 Figures, 2 Tables
- **Journal**: None
- **Summary**: Automatic segmentation of shoulder muscle MRI is challenging due to the high variation in muscle size, shape, texture, and spatial position of tears. Manual segmentation of tear and muscle portion is hard, time-consuming, and subjective to pathological expertise. This work proposes a new Region and Edge-based Deep Auto-Encoder (RE-DAE) for shoulder muscle MRI segmentation. The proposed RE-DAE harmoniously employs average and max-pooling operation in the encoder and decoder blocks of the Convolutional Neural Network (CNN). Region-based segmentation incorporated in the Deep Auto-Encoder (DAE) encourages the network to extract smooth and homogenous regions. In contrast, edge-based segmentation tries to learn the boundary and anatomical information. These two concepts, systematically combined in a DAE, generate a discriminative and sparse hybrid feature space (exploiting both region homogeneity and boundaries). Moreover, the concept of static attention is exploited in the proposed RE-DAE that helps in effectively learning the tear region. The performances of the proposed MRI segmentation based DAE architectures have been tested using a 3D MRI shoulder muscle dataset using the hold-out cross-validation technique. The MRI data has been collected from the Korea University Anam Hospital, Seoul, South Korea. Experimental comparisons have been conducted by employing innovative custom-made and existing pre-trained CNN architectures both using transfer learning and fine-tuning. Objective evaluation on the muscle datasets using the proposed SA-RE-DAE showed a dice similarity of 85.58% and 87.07%, an accuracy of 81.57% and 95.58% for tear and muscle regions, respectively. The high visual quality and the objective result suggest that the proposed SA-RE-DAE is able to correctly segment tear and muscle regions in shoulder muscle MRI for better clinical decisions.



### Learning to Diversify for Single Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2108.11726v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11726v3)
- **Published**: 2021-08-26 12:04:32+00:00
- **Updated**: 2023-03-22 09:13:00+00:00
- **Authors**: Zijian Wang, Yadan Luo, Ruihong Qiu, Zi Huang, Mahsa Baktashmotlagh
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Domain generalization (DG) aims to generalize a model trained on multiple source (i.e., training) domains to a distributionally different target (i.e., test) domain. In contrast to the conventional DG that strictly requires the availability of multiple source domains, this paper considers a more realistic yet challenging scenario, namely Single Domain Generalization (Single-DG), where only one source domain is available for training. In this scenario, the limited diversity may jeopardize the model generalization on unseen target domains. To tackle this problem, we propose a style-complement module to enhance the generalization power of the model by synthesizing images from diverse distributions that are complementary to the source ones. More specifically, we adopt a tractable upper bound of mutual information (MI) between the generated and source samples and perform a two-step optimization iteratively: (1) by minimizing the MI upper bound approximation for each sample pair, the generated images are forced to be diversified from the source samples; (2) subsequently, we maximize the MI between the samples from the same semantic category, which assists the network to learn discriminative features from diverse-styled images. Extensive experiments on three benchmark datasets demonstrate the superiority of our approach, which surpasses the state-of-the-art single-DG methods by up to 25.14%.



### An Underwater Image Semantic Segmentation Method Focusing on Boundaries and a Real Underwater Scene Semantic Segmentation Dataset
- **Arxiv ID**: http://arxiv.org/abs/2108.11727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11727v1)
- **Published**: 2021-08-26 12:05:08+00:00
- **Updated**: 2021-08-26 12:05:08+00:00
- **Authors**: Zhiwei Ma, Haojie Li, Zhihui Wang, Dan Yu, Tianyi Wang, Yingshuang Gu, Xin Fan, Zhongxuan Luo
- **Comment**: 8 pages, 7 figures, accepted by ChinaMM2021
- **Journal**: None
- **Summary**: With the development of underwater object grabbing technology, underwater object recognition and segmentation of high accuracy has become a challenge. The existing underwater object detection technology can only give the general position of an object, unable to give more detailed information such as the outline of the object, which seriously affects the grabbing efficiency. To address this problem, we label and establish the first underwater semantic segmentation dataset of real scene(DUT-USEG:DUT Underwater Segmentation Dataset). The DUT- USEG dataset includes 6617 images, 1487 of which have semantic segmentation and instance segmentation annotations, and the remaining 5130 images have object detection box annotations. Based on this dataset, we propose a semi-supervised underwater semantic segmentation network focusing on the boundaries(US-Net: Underwater Segmentation Network). By designing a pseudo label generator and a boundary detection subnetwork, this network realizes the fine learning of boundaries between underwater objects and background, and improves the segmentation effect of boundary areas. Experiments show that the proposed method improves by 6.7% in three categories of holothurian, echinus, starfish in DUT-USEG dataset, and achieves state-of-the-art results. The DUT- USEG dataset will be released at https://github.com/baxiyi/DUT-USEG.



### Deep learning based dictionary learning and tomographic image reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2108.11730v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE, eess.IV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2108.11730v1)
- **Published**: 2021-08-26 12:10:17+00:00
- **Updated**: 2021-08-26 12:10:17+00:00
- **Authors**: Jevgenija Rudzusika, Thomas Koehler, Ozan Öktem
- **Comment**: 34 pages, 5 figures
- **Journal**: None
- **Summary**: This work presents an approach for image reconstruction in clinical low-dose tomography that combines principles from sparse signal processing with ideas from deep learning. First, we describe sparse signal representation in terms of dictionaries from a statistical perspective and interpret dictionary learning as a process of aligning distribution that arises from a generative model with empirical distribution of true signals. As a result we can see that sparse coding with learned dictionaries resembles a specific variational autoencoder, where the decoder is a linear function and the encoder is a sparse coding algorithm. Next, we show that dictionary learning can also benefit from computational advancements introduced in the context of deep learning, such as parallelism and as stochastic optimization. Finally, we show that regularization by dictionaries achieves competitive performance in computed tomography (CT) reconstruction comparing to state-of-the-art model based and data driven approaches.



### Spatio-Temporal Dynamic Inference Network for Group Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.11743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11743v1)
- **Published**: 2021-08-26 12:40:20+00:00
- **Updated**: 2021-08-26 12:40:20+00:00
- **Authors**: Hangjie Yuan, Dong Ni, Mang Wang
- **Comment**: Accepted to ICCV2021
- **Journal**: None
- **Summary**: Group activity recognition aims to understand the activity performed by a group of people. In order to solve it, modeling complex spatio-temporal interactions is the key. Previous methods are limited in reasoning on a predefined graph, which ignores the inherent person-specific interaction context. Moreover, they adopt inference schemes that are computationally expensive and easily result in the over-smoothing problem. In this paper, we manage to achieve spatio-temporal person-specific inferences by proposing Dynamic Inference Network (DIN), which composes of Dynamic Relation (DR) module and Dynamic Walk (DW) module. We firstly propose to initialize interaction fields on a primary spatio-temporal graph. Within each interaction field, we apply DR to predict the relation matrix and DW to predict the dynamic walk offsets in a joint-processing manner, thus forming a person-specific interaction graph. By updating features on the specific graph, a person can possess a global-level interaction field with a local initialization. Experiments indicate both modules' effectiveness. Moreover, DIN achieves significant improvement compared to previous state-of-the-art methods on two popular datasets under the same setting, while costing much less computation overhead of the reasoning module.



### Physical Adversarial Attacks on an Aerial Imagery Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2108.11765v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11765v3)
- **Published**: 2021-08-26 12:53:41+00:00
- **Updated**: 2021-10-20 05:49:31+00:00
- **Authors**: Andrew Du, Bo Chen, Tat-Jun Chin, Yee Wei Law, Michele Sasdelli, Ramesh Rajasegaran, Dillon Campbell
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have become essential for processing the vast amounts of aerial imagery collected using earth-observing satellite platforms. However, DNNs are vulnerable towards adversarial examples, and it is expected that this weakness also plagues DNNs for aerial imagery. In this work, we demonstrate one of the first efforts on physical adversarial attacks on aerial imagery, whereby adversarial patches were optimised, fabricated and installed on or near target objects (cars) to significantly reduce the efficacy of an object detector applied on overhead images. Physical adversarial attacks on aerial images, particularly those captured from satellite platforms, are challenged by atmospheric factors (lighting, weather, seasons) and the distance between the observer and target. To investigate the effects of these challenges, we devised novel experiments and metrics to evaluate the efficacy of physical adversarial attacks against object detectors in aerial scenes. Our results indicate the palpable threat posed by physical adversarial attacks towards DNNs for processing satellite imagery.



### A Comparison of Deep Saliency Map Generators on Multispectral Data in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.11767v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11767v1)
- **Published**: 2021-08-26 12:56:49+00:00
- **Updated**: 2021-08-26 12:56:49+00:00
- **Authors**: Jens Bayer, David Münch, Michael Arens
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: Deep neural networks, especially convolutional deep neural networks, are state-of-the-art methods to classify, segment or even generate images, movies, or sounds. However, these methods lack of a good semantic understanding of what happens internally. The question, why a COVID-19 detector has classified a stack of lung-ct images as positive, is sometimes more interesting than the overall specificity and sensitivity. Especially when human domain expert knowledge disagrees with the given output. This way, human domain experts could also be advised to reconsider their choice, regarding the information pointed out by the system. In addition, the deep learning model can be controlled, and a present dataset bias can be found. Currently, most explainable AI methods in the computer vision domain are purely used on image classification, where the images are ordinary images in the visible spectrum. As a result, there is no comparison on how the methods behave with multimodal image data, as well as most methods have not been investigated on how they behave when used for object detection. This work tries to close the gaps. Firstly, investigating three saliency map generator methods on how their maps differ across the different spectra. This is achieved via accurate and systematic training. Secondly, we examine how they behave when used for object detection. As a practical problem, we chose object detection in the infrared and visual spectrum for autonomous driving. The dataset used in this work is the Multispectral Object Detection Dataset, where each scene is available in the FIR, MIR and NIR as well as visual spectrum. The results show that there are differences between the infrared and visual activation maps. Further, an advanced training with both, the infrared and visual data not only improves the network's output, it also leads to more focused spots in the saliency maps.



### Cross-category Video Highlight Detection via Set-based Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.11770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11770v1)
- **Published**: 2021-08-26 13:06:47+00:00
- **Updated**: 2021-08-26 13:06:47+00:00
- **Authors**: Minghao Xu, Hang Wang, Bingbing Ni, Riheng Zhu, Zhenbang Sun, Changhu Wang
- **Comment**: Accepted as poster presentation at International Conference on
  Computer Vision (ICCV), 2021
- **Journal**: None
- **Summary**: Autonomous highlight detection is crucial for enhancing the efficiency of video browsing on social media platforms. To attain this goal in a data-driven way, one may often face the situation where highlight annotations are not available on the target video category used in practice, while the supervision on another video category (named as source video category) is achievable. In such a situation, one can derive an effective highlight detector on target video category by transferring the highlight knowledge acquired from source video category to the target one. We call this problem cross-category video highlight detection, which has been rarely studied in previous works. For tackling such practical problem, we propose a Dual-Learner-based Video Highlight Detection (DL-VHD) framework. Under this framework, we first design a Set-based Learning module (SL-module) to improve the conventional pair-based learning by assessing the highlight extent of a video segment under a broader context. Based on such learning manner, we introduce two different learners to acquire the basic distinction of target category videos and the characteristics of highlight moments on source video category, respectively. These two types of highlight knowledge are further consolidated via knowledge distillation. Extensive experiments on three benchmark datasets demonstrate the superiority of the proposed SL-module, and the DL-VHD method outperforms five typical Unsupervised Domain Adaptation (UDA) algorithms on various cross-category highlight detection tasks. Our code is available at https://github.com/ChrisAllenMing/Cross_Category_Video_Highlight .



### ICM-3D: Instantiated Category Modeling for 3D Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.11771v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.11771v1)
- **Published**: 2021-08-26 13:08:37+00:00
- **Updated**: 2021-08-26 13:08:37+00:00
- **Authors**: Ruihang Chu, Yukang Chen, Tao Kong, Lu Qi, Lei Li
- **Comment**: IEEE Robotics and Automation Letters (RA-L). Preprint Version.
  Accepted August, 2021
- **Journal**: None
- **Summary**: Separating 3D point clouds into individual instances is an important task for 3D vision. It is challenging due to the unknown and varying number of instances in a scene. Existing deep learning based works focus on a two-step pipeline: first learn a feature embedding and then cluster the points. Such a two-step pipeline leads to disconnected intermediate objectives. In this paper, we propose an integrated reformulation of 3D instance segmentation as a per-point classification problem. We propose ICM-3D, a single-step method to segment 3D instances via instantiated categorization. The augmented category information is automatically constructed from 3D spatial positions. We conduct extensive experiments to verify the effectiveness of ICM-3D and show that it obtains inspiring performance across multiple frameworks, backbones and benchmarks.



### Multi-Modulation Network for Audio-Visual Event Localization
- **Arxiv ID**: http://arxiv.org/abs/2108.11773v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11773v2)
- **Published**: 2021-08-26 13:11:48+00:00
- **Updated**: 2021-08-30 13:11:02+00:00
- **Authors**: Hao Wang, Zheng-Jun Zha, Liang Li, Xuejin Chen, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of localizing audio-visual events that are both audible and visible in a video. Existing works focus on encoding and aligning audio and visual features at the segment level while neglecting informative correlation between segments of the two modalities and between multi-scale event proposals. We propose a novel MultiModulation Network (M2N) to learn the above correlation and leverage it as semantic guidance to modulate the related auditory, visual, and fused features. In particular, during feature encoding, we propose cross-modal normalization and intra-modal normalization. The former modulates the features of two modalities by establishing and exploiting the cross-modal relationship. The latter modulates the features of a single modality with the event-relevant semantic guidance of the same modality. In the fusion stage,we propose a multi-scale proposal modulating module and a multi-alignment segment modulating module to introduce multi-scale event proposals and enable dense matching between cross-modal segments. With the auditory, visual, and fused features modulated by the correlation information regarding audio-visual events, M2N performs accurate event localization. Extensive experiments conducted on the AVE dataset demonstrate that our proposed method outperforms the state of the art in both supervised event localization and cross-modality localization.



### Quadratic mutual information regularization in real-time deep CNN models
- **Arxiv ID**: http://arxiv.org/abs/2108.11774v1
- **DOI**: 10.1109/MLSP49062.2020.9231566
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11774v1)
- **Published**: 2021-08-26 13:14:24+00:00
- **Updated**: 2021-08-26 13:14:24+00:00
- **Authors**: Maria Tzelepi, Anastasios Tefas
- **Comment**: Accepted at MLSP 2020
- **Journal**: None
- **Summary**: In this paper, regularized lightweight deep convolutional neural network models, capable of effectively operating in real-time on devices with restricted computational power for high-resolution video input are proposed. Furthermore, a novel regularization method motivated by the Quadratic Mutual Information, in order to improve the generalization ability of the utilized models is proposed. Extensive experiments on various binary classification problems involved in autonomous systems are performed, indicating the effectiveness of the proposed models as well as of the proposed regularizer.



### A Hierarchical Assessment of Adversarial Severity
- **Arxiv ID**: http://arxiv.org/abs/2108.11785v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11785v1)
- **Published**: 2021-08-26 13:29:17+00:00
- **Updated**: 2021-08-26 13:29:17+00:00
- **Authors**: Guillaume Jeanneret, Juan C Perez, Pablo Arbelaez
- **Comment**: To appear on the ICCV2021 Workshop on Adversarial Robustness in the
  Real World
- **Journal**: None
- **Summary**: Adversarial Robustness is a growing field that evidences the brittleness of neural networks. Although the literature on adversarial robustness is vast, a dimension is missing in these studies: assessing how severe the mistakes are. We call this notion "Adversarial Severity" since it quantifies the downstream impact of adversarial corruptions by computing the semantic error between the misclassification and the proper label. We propose to study the effects of adversarial noise by measuring the Robustness and Severity into a large-scale dataset: iNaturalist-H. Our contributions are: (i) we introduce novel Hierarchical Attacks that harness the rich structured space of labels to create adversarial examples. (ii) These attacks allow us to benchmark the Adversarial Robustness and Severity of classification models. (iii) We enhance the traditional adversarial training with a simple yet effective Hierarchical Curriculum Training to learn these nodes gradually within the hierarchical tree. We perform extensive experiments showing that hierarchical defenses allow deep models to boost the adversarial Robustness by 1.85% and reduce the severity of all attacks by 0.17, on average.



### Ensemble CNN and Uncertainty Modeling to Improve Automatic Identification/Segmentation of Multiple Sclerosis Lesions in Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/2108.11791v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11791v3)
- **Published**: 2021-08-26 13:48:06+00:00
- **Updated**: 2022-06-20 15:11:54+00:00
- **Authors**: Giuseppe Placidi, Luigi Cinque, Daniela Iacoviello, Filippo Mignosi, Matteo Polsinelli
- **Comment**: None
- **Journal**: None
- **Summary**: To date, several automated strategies for identification/segmentation of Multiple Sclerosis (MS) lesions with the use of Magnetic Resonance Imaging (MRI) have been presented, but they are outperformed by human experts, from whom they act very differently. This is mainly due to: the ambiguity originated by MRI instabilities; peculiar variability of MS; non specificity of MRI regarding MS. Physicians partially manage the uncertainty generated by ambiguity relying on radiological/clinical/anatomical background and experience. To emulate human diagnosis, we present an automated framework for identification/segmentation of MS lesions from MRI based on three pivotal concepts: 1. the modelling of uncertainty; 2. the proposal of two, separately trained, CNN, one optimized for lesions and the other for lesions with respect to the environment surrounding them, respectively repeated for axial, coronal and sagittal directions; 3. the definition of an ensemble classifier to merge the information collected by different CNN. The proposed framework is trained, validated and tested on the 2016 MSSEG benchmark public data set from a single imaging modality, the FLuid-Attenuated Inversion Recovery (FLAIR). The comparison with the ground-truth and with each of 7 human raters, proves that there is no significant difference between the automated and the human raters.



### State of the Art: Image Hashing
- **Arxiv ID**: http://arxiv.org/abs/2108.11794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11794v1)
- **Published**: 2021-08-26 13:53:30+00:00
- **Updated**: 2021-08-26 13:53:30+00:00
- **Authors**: Rubel Biswas, Pablo Blanco-Medina
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Perceptual image hashing methods are often applied in various objectives, such as image retrieval, finding duplicate or near-duplicate images, and finding similar images from large-scale image content. The main challenge in image hashing techniques is robust feature extraction, which generates the same or similar hashes in images that are visually identical. In this article, we present a short review of the state-of-the-art traditional perceptual hashing and deep learning-based perceptual hashing methods, identifying the best approaches.



### Efficient training of lightweight neural networks using Online Self-Acquired Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2108.11798v1
- **DOI**: 10.1109/ICME51207.2021.9428147
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11798v1)
- **Published**: 2021-08-26 14:01:04+00:00
- **Updated**: 2021-08-26 14:01:04+00:00
- **Authors**: Maria Tzelepi, Anastasios Tefas
- **Comment**: Accepted at ICME 2021
- **Journal**: None
- **Summary**: Knowledge Distillation has been established as a highly promising approach for training compact and faster models by transferring knowledge from heavyweight and powerful models. However, KD in its conventional version constitutes an enduring, computationally and memory demanding process. In this paper, Online Self-Acquired Knowledge Distillation (OSAKD) is proposed, aiming to improve the performance of any deep neural model in an online manner. We utilize k-nn non-parametric density estimation technique for estimating the unknown probability distributions of the data samples in the output feature space. This allows us for directly estimating the posterior class probabilities of the data samples, and we use them as soft labels that encode explicit information about the similarities of the data with the classes, negligibly affecting the computational cost. The experimental evaluation on four datasets validates the effectiveness of proposed method.



### Unsupervised domain adaptation for clinician pose estimation and instance segmentation in the operating room
- **Arxiv ID**: http://arxiv.org/abs/2108.11801v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11801v4)
- **Published**: 2021-08-26 14:07:43+00:00
- **Updated**: 2022-06-30 09:54:48+00:00
- **Authors**: Vinkle Srivastav, Afshin Gangi, Nicolas Padoy
- **Comment**: Accepted at Elsevier Journal of Medical Image Analysis. Code is
  available at https://github.com/CAMMA-public/HPE-AdaptOR. Supplementary video
  is available at https://youtu.be/gqwPu9-nfGs
- **Journal**: None
- **Summary**: The fine-grained localization of clinicians in the operating room (OR) is a key component to design the new generation of OR support systems. Computer vision models for person pixel-based segmentation and body-keypoints detection are needed to better understand the clinical activities and the spatial layout of the OR. This is challenging, not only because OR images are very different from traditional vision datasets, but also because data and annotations are hard to collect and generate in the OR due to privacy concerns. To address these concerns, we first study how joint person pose estimation and instance segmentation can be performed on low resolutions images with downsampling factors from 1x to 12x. Second, to address the domain shift and the lack of annotations, we propose a novel unsupervised domain adaptation method, called AdaptOR, to adapt a model from an in-the-wild labeled source domain to a statistically different unlabeled target domain. We propose to exploit explicit geometric constraints on the different augmentations of the unlabeled target domain image to generate accurate pseudo labels and use these pseudo labels to train the model on high- and low-resolution OR images in a self-training framework. Furthermore, we propose disentangled feature normalization to handle the statistically different source and target domain data. Extensive experimental results with detailed ablation studies on the two OR datasets MVOR+ and TUM-OR-test show the effectiveness of our approach against strongly constructed baselines, especially on the low-resolution privacy-preserving OR images. Finally, we show the generality of our method as a semi-supervised learning (SSL) method on the large-scale COCO dataset, where we achieve comparable results with as few as 1% of labeled supervision against a model trained with 100% labeled supervision.



### Mining Contextual Information Beyond Image for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.11819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11819v1)
- **Published**: 2021-08-26 14:34:23+00:00
- **Updated**: 2021-08-26 14:34:23+00:00
- **Authors**: Zhenchao Jin, Tao Gong, Dongdong Yu, Qi Chu, Jian Wang, Changhu Wang, Jie Shao
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: This paper studies the context aggregation problem in semantic image segmentation. The existing researches focus on improving the pixel representations by aggregating the contextual information within individual images. Though impressive, these methods neglect the significance of the representations of the pixels of the corresponding class beyond the input image. To address this, this paper proposes to mine the contextual information beyond individual images to further augment the pixel representations. We first set up a feature memory module, which is updated dynamically during training, to store the dataset-level representations of various categories. Then, we learn class probability distribution of each pixel representation under the supervision of the ground-truth segmentation. At last, the representation of each pixel is augmented by aggregating the dataset-level representations based on the corresponding class probability distribution. Furthermore, by utilizing the stored dataset-level representations, we also propose a representation consistent learning strategy to make the classification head better address intra-class compactness and inter-class dispersion. The proposed method could be effortlessly incorporated into existing segmentation frameworks (e.g., FCN, PSPNet, OCRNet and DeepLabV3) and brings consistent performance improvements. Mining contextual information beyond image allows us to report state-of-the-art performance on various benchmarks: ADE20K, LIP, Cityscapes and COCO-Stuff.



### State of the Art: Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2108.11821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11821v1)
- **Published**: 2021-08-26 14:37:29+00:00
- **Updated**: 2021-08-26 14:37:29+00:00
- **Authors**: Rubel Biswas, Pablo Blanco-Medina
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Working with Child Sexual Exploitation Material (CSEM) in forensic applications might be benefited from the progress in automatic face recognition. However, discriminative parts of a face in CSEM, i.e., mostly the eyes, could be often occluded to difficult the victim's identification. Most of the face recognition approaches cannot deal with such kind of occlusions, resulting in inaccurate face recognition results. This document presents a short review face recognition methods for images with natural and eye occlude faces. The purpose is to select the best baseline approach for solving automatic face recognition of occluded faces.



### Fast and Flexible Human Pose Estimation with HyperPose
- **Arxiv ID**: http://arxiv.org/abs/2108.11826v2
- **DOI**: 10.1145/3474085.3478325
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11826v2)
- **Published**: 2021-08-26 14:51:24+00:00
- **Updated**: 2022-10-26 08:05:49+00:00
- **Authors**: Yixiao Guo, Jiawei Liu, Guo Li, Luo Mai, Hao Dong
- **Comment**: 4 pages, 1 figure. Published in ACM Multimedia
- **Journal**: Proceedings of the 29th ACM International Conference on
  Multimedia, 2021, 3763-3766
- **Summary**: Estimating human pose is an important yet challenging task in multimedia applications. Existing pose estimation libraries target reproducing standard pose estimation algorithms. When it comes to customising these algorithms for real-world applications, none of the existing libraries can offer both the flexibility of developing custom pose estimation algorithms and the high-performance of executing these algorithms on commodity devices. In this paper, we introduce Hyperpose, a novel flexible and high-performance pose estimation library. Hyperpose provides expressive Python APIs that enable developers to easily customise pose estimation algorithms for their applications. It further provides a model inference engine highly optimised for real-time pose estimation. This engine can dynamically dispatch carefully designed pose estimation tasks to CPUs and GPUs, thus automatically achieving high utilisation of hardware resources irrespective of deployment environments. Extensive evaluation results show that Hyperpose can achieve up to 3.1x~7.3x higher pose estimation throughput compared to state-of-the-art pose estimation libraries without compromising estimation accuracy. By 2021, Hyperpose has received over 1000 stars on GitHub and attracted users from both industry and academy.



### Geometry Based Machining Feature Retrieval with Inductive Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.11838v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T07, I.4; I.2; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2108.11838v2)
- **Published**: 2021-08-26 15:08:42+00:00
- **Updated**: 2021-11-15 13:02:46+00:00
- **Authors**: N S Kamal, Barathi Ganesh HB, Sajith Variyar VV, Sowmya V, Soman KP
- **Comment**: Submitted to 9th International Conference on Frontiers of Intelligent
  Computing: Theory and Applications (FICTA 2021)
- **Journal**: None
- **Summary**: Manufacturing industries have widely adopted the reuse of machine parts as a method to reduce costs and as a sustainable manufacturing practice. Identification of reusable features from the design of the parts and finding their similar features from the database is an important part of this process. In this project, with the help of fully convolutional geometric features, we are able to extract and learn the high level semantic features from CAD models with inductive transfer learning. The extracted features are then compared with that of other CAD models from the database using Frobenius norm and identical features are retrieved. Later we passed the extracted features to a deep convolutional neural network with a spatial pyramid pooling layer and the performance of the feature retrieval increased significantly. It was evident from the results that the model could effectively capture the geometrical elements from machining features.



### Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.11845v9
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.11845v9)
- **Published**: 2021-08-26 15:14:38+00:00
- **Updated**: 2022-05-31 03:16:02+00:00
- **Authors**: Bin Liu
- **Comment**: This paper has been accepted by 2022 International Conference on
  Pattern Recognition and Machine Learning (PRML 2022)
- **Journal**: None
- **Summary**: In this paper, we are concerned with image classification with deep convolutional neural networks (CNNs). We focus on the following question: given a set of candidate CNN models, how to select the right one with the best generalization property for the current task? Current model selection methods all require access to a batch of labeled data for computing a pre-specified performance metric, such as the cross-entropy loss, the classification error rate and the negative log-likelihood. In many practical cases, labels are not available in time as labeling itself is a time-consuming and expensive task. To this end, we propose an approach to CNN model selection using only unlabeled data. We develop this method based on a principle termed consistent relative confidence. Experimental results on benchmark datasets demonstrate the effectiveness and efficiency of our method.



### Self-supervised Multi-scale Consistency for Weakly Supervised Segmentation Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.11900v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11900v1)
- **Published**: 2021-08-26 16:43:59+00:00
- **Updated**: 2021-08-26 16:43:59+00:00
- **Authors**: Gabriele Valvano, Andrea Leo, Sotirios A. Tsaftaris
- **Comment**: Accepted at Domain Adaptation and Representation Transfer (DART) 2021
- **Journal**: None
- **Summary**: Collecting large-scale medical datasets with fine-grained annotations is time-consuming and requires experts. For this reason, weakly supervised learning aims at optimising machine learning models using weaker forms of annotations, such as scribbles, which are easier and faster to collect. Unfortunately, training with weak labels is challenging and needs regularisation. Herein, we introduce a novel self-supervised multi-scale consistency loss, which, coupled with an attention mechanism, encourages the segmentor to learn multi-scale relationships between objects and improves performance. We show state-of-the-art performance on several medical and non-medical datasets. The code used for the experiments is available at https://vios-s.github.io/multiscale-pyag.



### Stop Throwing Away Discriminators! Re-using Adversaries for Test-Time Training
- **Arxiv ID**: http://arxiv.org/abs/2108.12280v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12280v1)
- **Published**: 2021-08-26 16:51:28+00:00
- **Updated**: 2021-08-26 16:51:28+00:00
- **Authors**: Gabriele Valvano, Andrea Leo, Sotirios A. Tsaftaris
- **Comment**: Accepted at: Domain Adaptation and Representation Transfer (DART)
  2021
- **Journal**: None
- **Summary**: Thanks to their ability to learn data distributions without requiring paired data, Generative Adversarial Networks (GANs) have become an integral part of many computer vision methods, including those developed for medical image segmentation. These methods jointly train a segmentor and an adversarial mask discriminator, which provides a data-driven shape prior. At inference, the discriminator is discarded, and only the segmentor is used to predict label maps on test images. But should we discard the discriminator? Here, we argue that the life cycle of adversarial discriminators should not end after training. On the contrary, training stable GANs produces powerful shape priors that we can use to correct segmentor mistakes at inference. To achieve this, we develop stable mask discriminators that do not overfit or catastrophically forget. At test time, we fine-tune the segmentor on each individual test instance until it satisfies the learned shape prior. Our method is simple to implement and increases model performance. Moreover, it opens new directions for re-using mask discriminators at inference. We release the code used for the experiments at https://vios-s.github.io/adversarial-test-time-training.



### Similar Scenes arouse Similar Emotions: Parallel Data Augmentation for Stylized Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2108.11912v1
- **DOI**: 10.1145/3474085.3475662
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2108.11912v1)
- **Published**: 2021-08-26 17:08:58+00:00
- **Updated**: 2021-08-26 17:08:58+00:00
- **Authors**: Guodun Li, Yuchen Zhai, Zehao Lin, Yin Zhang
- **Comment**: Accepted at ACM Multimedia (ACMMM) 2021
- **Journal**: None
- **Summary**: Stylized image captioning systems aim to generate a caption not only semantically related to a given image but also consistent with a given style description. One of the biggest challenges with this task is the lack of sufficient paired stylized data. Many studies focus on unsupervised approaches, without considering from the perspective of data augmentation. We begin with the observation that people may recall similar emotions when they are in similar scenes, and often express similar emotions with similar style phrases, which underpins our data augmentation idea. In this paper, we propose a novel Extract-Retrieve-Generate data augmentation framework to extract style phrases from small-scale stylized sentences and graft them to large-scale factual captions. First, we design the emotional signal extractor to extract style phrases from small-scale stylized sentences. Second, we construct the plugable multi-modal scene retriever to retrieve scenes represented with pairs of an image and its stylized caption, which are similar to the query image or caption in the large-scale factual data. In the end, based on the style phrases of similar scenes and the factual description of the current scene, we build the emotion-aware caption generator to generate fluent and diversified stylized captions for the current scene. Extensive experimental results show that our framework can alleviate the data scarcity problem effectively. It also significantly boosts the performance of several existing image captioning models in both supervised and unsupervised settings, which outperforms the state-of-the-art stylized image captioning methods in terms of both sentence relevance and stylishness by a substantial margin.



### User-Centric Semi-Automated Infographics Authoring and Recommendation
- **Arxiv ID**: http://arxiv.org/abs/2108.11914v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11914v2)
- **Published**: 2021-08-26 17:09:59+00:00
- **Updated**: 2021-08-27 18:10:29+00:00
- **Authors**: Anjul Tyagi, Jian Zhao, Pushkar Patel, Swasti Khurana, Klaus Mueller
- **Comment**: None
- **Journal**: None
- **Summary**: Designing infographics can be a tedious process for non-experts and time-consuming even for professional designers. Based on the literature and a formative study, we propose a flexible framework for automated and semi-automated infographics design. This framework captures the main design components in infographics and streamlines the generation workflow into three steps, allowing users to control and optimize each aspect independently. Based on the framework, we also propose an interactive tool, \name{}, for assisting novice designers with creating high-quality infographics from an input in a markdown format by offering recommendations of different design components of infographics. Simultaneously, more experienced designers can provide custom designs and layout ideas to the tool using a canvas to control the automated generation process partially. As part of our work, we also contribute an individual visual group (VG) and connection designs dataset (in SVG), along with a 1k complete infographic image dataset with segmented VGs. This dataset plays a crucial role in diversifying the infographic designs created by our framework. We evaluate our approach with a comparison against similar tools, a user study with novice and expert designers, and a case study. Results confirm that our framework and \name{} excel in creating customized infographics and exploring a large variety of designs.



### Reiterative Domain Aware Multi-Target Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2109.00919v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00919v2)
- **Published**: 2021-08-26 17:12:25+00:00
- **Updated**: 2022-01-31 21:51:37+00:00
- **Authors**: Sudipan Saha, Shan Zhao, Nasrullah Sheikh, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Most domain adaptation methods focus on single-source-single-target adaptation settings. Multi-target domain adaptation is a powerful extension in which a single classifier is learned for multiple unlabeled target domains. To build a multi-target classifier, it is important to have: a feature extractor that generalizes well across domains; and effective aggregation of features from the labeled source and different unlabeled target domains. Towards the first, we use the recently popular Transformer as a feature extraction backbone. Towards the second, we use a co-teaching-based approach using a dual-classifier head, one of which is based on the graph neural network. The proposed approach uses a sequential adaptation strategy that adapts one domain at a time starting from the target domains that are more similar to the source, assuming that the network finds it easier to adapt to such target domains. After adapting on each target, samples with a softmax-based confidence score greater than a threshold are added to the pseudo-source, thus aggregating knowledge from different domains. However, softmax is not entirely trustworthy as a confidence score and may generate a high score for unreliable samples if trained for many iterations. To mitigate this effect, we adopt a reiterative approach, where we reduce target adaptation iterations, however, reiterate multiple times over the target domains. The experimental evaluation on the Office-Home, Office-31 and DomainNet datasets shows significant improvement over the existing methods. We have achieved 10.7$\%$ average improvement in Office-Home dataset over the state-of-art methods.



### Re-using Adversarial Mask Discriminators for Test-time Training under Distribution Shifts
- **Arxiv ID**: http://arxiv.org/abs/2108.11926v4
- **DOI**: 10.59275/j.melba.2022-bd5e
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11926v4)
- **Published**: 2021-08-26 17:31:46+00:00
- **Updated**: 2022-05-27 14:17:34+00:00
- **Authors**: Gabriele Valvano, Andrea Leo, Sotirios A. Tsaftaris
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://www.melba-journal.org/papers/2022:014.html
- **Journal**: Journal of Machine Learning for Biomedical Imaging (MELBA), 2022
- **Summary**: Thanks to their ability to learn flexible data-driven losses, Generative Adversarial Networks (GANs) are an integral part of many semi- and weakly-supervised methods for medical image segmentation. GANs jointly optimise a generator and an adversarial discriminator on a set of training data. After training is complete, the discriminator is usually discarded, and only the generator is used for inference. But should we discard discriminators? In this work, we argue that training stable discriminators produces expressive loss functions that we can re-use at inference to detect and \textit{correct} segmentation mistakes. First, we identify key challenges and suggest possible solutions to make discriminators re-usable at inference. Then, we show that we can combine discriminators with image reconstruction costs (via decoders) to endow a causal perspective to test-time training and further improve the model. Our method is simple and improves the test-time performance of pre-trained GANs. Moreover, we show that it is compatible with standard post-processing techniques and it has the potential to be used for Online Continual Learning. With our work, we open new research avenues for re-using adversarial discriminators at inference. Our code is available at https://vios-s.github.io/adversarial-test-time-training.



### Semantically Coherent Out-of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.11941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11941v1)
- **Published**: 2021-08-26 17:53:32+00:00
- **Updated**: 2021-08-26 17:53:32+00:00
- **Authors**: Jingkang Yang, Haoqi Wang, Litong Feng, Xiaopeng Yan, Huabin Zheng, Wayne Zhang, Ziwei Liu
- **Comment**: 15 pages, 7 figures. Accepted by ICCV-2021. Project page:
  https://jingkang50.github.io/projects/scood
- **Journal**: None
- **Summary**: Current out-of-distribution (OOD) detection benchmarks are commonly built by defining one dataset as in-distribution (ID) and all others as OOD. However, these benchmarks unfortunately introduce some unwanted and impractical goals, e.g., to perfectly distinguish CIFAR dogs from ImageNet dogs, even though they have the same semantics and negligible covariate shifts. These unrealistic goals will result in an extremely narrow range of model capabilities, greatly limiting their use in real applications. To overcome these drawbacks, we re-design the benchmarks and propose the semantically coherent out-of-distribution detection (SC-OOD). On the SC-OOD benchmarks, existing methods suffer from large performance degradation, suggesting that they are extremely sensitive to low-level discrepancy between data sources while ignoring their inherent semantics. To develop an effective SC-OOD detection approach, we leverage an external unlabeled set and design a concise framework featured by unsupervised dual grouping (UDG) for the joint modeling of ID and OOD data. The proposed UDG can not only enrich the semantic knowledge of the model by exploiting unlabeled data in an unsupervised manner, but also distinguish ID/OOD samples to enhance ID classification and OOD detection tasks simultaneously. Extensive experiments demonstrate that our approach achieves state-of-the-art performance on SC-OOD benchmarks. Code and benchmarks are provided on our project page: https://jingkang50.github.io/projects/scood.



### Probabilistic Modeling for Human Mesh Recovery
- **Arxiv ID**: http://arxiv.org/abs/2108.11944v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11944v1)
- **Published**: 2021-08-26 17:55:11+00:00
- **Updated**: 2021-08-26 17:55:11+00:00
- **Authors**: Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman, Kostas Daniilidis
- **Comment**: ICCV 2021. Project page:
  https://www.seas.upenn.edu/~nkolot/projects/prohmr
- **Journal**: None
- **Summary**: This paper focuses on the problem of 3D human reconstruction from 2D evidence. Although this is an inherently ambiguous problem, the majority of recent works avoid the uncertainty modeling and typically regress a single estimate for a given input. In contrast to that, in this work, we propose to embrace the reconstruction ambiguity and we recast the problem as learning a mapping from the input to a distribution of plausible 3D poses. Our approach is based on the normalizing flows model and offers a series of advantages. For conventional applications, where a single 3D estimate is required, our formulation allows for efficient mode computation. Using the mode leads to performance that is comparable with the state of the art among deterministic unimodal regression models. Simultaneously, since we have access to the likelihood of each sample, we demonstrate that our model is useful in a series of downstream tasks, where we leverage the probabilistic nature of the prediction as a tool for more accurate estimation. These tasks include reconstruction from multiple uncalibrated views, as well as human model fitting, where our model acts as a powerful image-based prior for mesh recovery. Our results validate the importance of probabilistic modeling, and indicate state-of-the-art performance across a variety of settings. Code and models are available at: https://www.seas.upenn.edu/~nkolot/projects/prohmr.



### SASRA: Semantically-aware Spatio-temporal Reasoning Agent for Vision-and-Language Navigation in Continuous Environments
- **Arxiv ID**: http://arxiv.org/abs/2108.11945v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11945v1)
- **Published**: 2021-08-26 17:57:02+00:00
- **Updated**: 2021-08-26 17:57:02+00:00
- **Authors**: Muhammad Zubair Irshad, Niluthpol Chowdhury Mithun, Zachary Seymour, Han-Pang Chiu, Supun Samarasekera, Rakesh Kumar
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: This paper presents a novel approach for the Vision-and-Language Navigation (VLN) task in continuous 3D environments, which requires an autonomous agent to follow natural language instructions in unseen environments. Existing end-to-end learning-based VLN methods struggle at this task as they focus mostly on utilizing raw visual observations and lack the semantic spatio-temporal reasoning capabilities which is crucial in generalizing to new environments. In this regard, we present a hybrid transformer-recurrence model which focuses on combining classical semantic mapping techniques with a learning-based method. Our method creates a temporal semantic memory by building a top-down local ego-centric semantic map and performs cross-modal grounding to align map and language modalities to enable effective learning of VLN policy. Empirical results in a photo-realistic long-horizon simulation environment show that the proposed approach outperforms a variety of state-of-the-art methods and baselines with over 22% relative improvement in SPL in prior unseen environments.



### LocTex: Learning Data-Efficient Visual Representations from Localized Textual Supervision
- **Arxiv ID**: http://arxiv.org/abs/2108.11950v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2108.11950v1)
- **Published**: 2021-08-26 17:59:07+00:00
- **Updated**: 2021-08-26 17:59:07+00:00
- **Authors**: Zhijian Liu, Simon Stent, Jie Li, John Gideon, Song Han
- **Comment**: ICCV 2021. Project page: https://loctex.mit.edu/
- **Journal**: None
- **Summary**: Computer vision tasks such as object detection and semantic/instance segmentation rely on the painstaking annotation of large training datasets. In this paper, we propose LocTex that takes advantage of the low-cost localized textual annotations (i.e., captions and synchronized mouse-over gestures) to reduce the annotation effort. We introduce a contrastive pre-training framework between images and captions and propose to supervise the cross-modal attention map with rendered mouse traces to provide coarse localization signals. Our learned visual features capture rich semantics (from free-form captions) and accurate localization (from mouse traces), which are very effective when transferred to various downstream vision tasks. Compared with ImageNet supervised pre-training, LocTex can reduce the size of the pre-training dataset by 10x or the target dataset by 2x while achieving comparable or even improved performance on COCO instance segmentation. When provided with the same amount of annotations, LocTex achieves around 4% higher accuracy than the previous state-of-the-art "vision+language" pre-training approach on the task of PASCAL VOC image classification.



### Learning Cross-modal Contrastive Features for Video Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2108.11974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11974v1)
- **Published**: 2021-08-26 18:14:18+00:00
- **Updated**: 2021-08-26 18:14:18+00:00
- **Authors**: Donghyun Kim, Yi-Hsuan Tsai, Bingbing Zhuang, Xiang Yu, Stan Sclaroff, Kate Saenko, Manmohan Chandraker
- **Comment**: Accepted in ICCV'21
- **Journal**: None
- **Summary**: Learning transferable and domain adaptive feature representations from videos is important for video-relevant tasks such as action recognition. Existing video domain adaptation methods mainly rely on adversarial feature alignment, which has been derived from the RGB image space. However, video data is usually associated with multi-modal information, e.g., RGB and optical flow, and thus it remains a challenge to design a better method that considers the cross-modal inputs under the cross-domain adaptation setting. To this end, we propose a unified framework for video domain adaptation, which simultaneously regularizes cross-modal and cross-domain feature representations. Specifically, we treat each modality in a domain as a view and leverage the contrastive learning technique with properly designed sampling strategies. As a result, our objectives regularize feature spaces, which originally lack the connection across modalities or have less alignment across domains. We conduct experiments on domain adaptive action recognition benchmark datasets, i.e., UCF, HMDB, and EPIC-Kitchens, and demonstrate the effectiveness of our components against state-of-the-art algorithms.



### Evaluating Transformer-based Semantic Segmentation Networks for Pathological Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.11993v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11993v2)
- **Published**: 2021-08-26 18:46:43+00:00
- **Updated**: 2021-09-22 15:18:55+00:00
- **Authors**: Cam Nguyen, Zuhayr Asad, Yuankai Huo
- **Comment**: None
- **Journal**: None
- **Summary**: Histopathology has played an essential role in cancer diagnosis. With the rapid advances in convolutional neural networks (CNN). Various CNN-based automated pathological image segmentation approaches have been developed in computer-assisted pathological image analysis. In the past few years, Transformer neural networks (Transformer) have shown the unique merit of capturing the global long-distance dependencies across the entire image as a new deep learning paradigm. Such merit is appealing for exploring spatially heterogeneous pathological images. However, there have been very few, if any, studies that have systematically evaluated the current Transformer-based approaches in pathological image segmentation. To assess the performance of Transformer segmentation models on whole slide images (WSI), we quantitatively evaluated six prevalent transformer-based models on tumor segmentation, using the widely used PAIP liver histopathological dataset. For a more comprehensive analysis, we also compare the transformer-based models with six major traditional CNN-based models. The results show that the Transformer-based models exhibit a general superior performance over the CNN-based models. In particular, Segmenter, Swin-Transformer and TransUNet-all transformer-based-came out as the best performers among the twelve evaluated models.



### Drop-DTW: Aligning Common Signal Between Sequences While Dropping Outliers
- **Arxiv ID**: http://arxiv.org/abs/2108.11996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11996v1)
- **Published**: 2021-08-26 18:52:35+00:00
- **Updated**: 2021-08-26 18:52:35+00:00
- **Authors**: Nikita Dvornik, Isma Hadji, Konstantinos G. Derpanis, Animesh Garg, Allan D. Jepson
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we consider the problem of sequence-to-sequence alignment for signals containing outliers. Assuming the absence of outliers, the standard Dynamic Time Warping (DTW) algorithm efficiently computes the optimal alignment between two (generally) variable-length sequences. While DTW is robust to temporal shifts and dilations of the signal, it fails to align sequences in a meaningful way in the presence of outliers that can be arbitrarily interspersed in the sequences. To address this problem, we introduce Drop-DTW, a novel algorithm that aligns the common signal between the sequences while automatically dropping the outlier elements from the matching. The entire procedure is implemented as a single dynamic program that is efficient and fully differentiable. In our experiments, we show that Drop-DTW is a robust similarity measure for sequence retrieval and demonstrate its effectiveness as a training loss on diverse applications. With Drop-DTW, we address temporal step localization on instructional videos, representation learning from noisy videos, and cross-modal representation learning for audio-visual retrieval and localization. In all applications, we take a weakly- or unsupervised approach and demonstrate state-of-the-art results under these settings.



### Learning Disentangled Representations in the Imaging Domain
- **Arxiv ID**: http://arxiv.org/abs/2108.12043v6
- **DOI**: 10.1016/j.media.2022.102516
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.12043v6)
- **Published**: 2021-08-26 21:44:10+00:00
- **Updated**: 2022-07-29 12:18:34+00:00
- **Authors**: Xiao Liu, Pedro Sanchez, Spyridon Thermos, Alison Q. O'Neil, Sotirios A. Tsaftaris
- **Comment**: Accepted by Medical Image Analysis. This paper follows a tutorial
  style but also surveys a considerable (more than 260 citations) number of
  works
- **Journal**: None
- **Summary**: Disentangled representation learning has been proposed as an approach to learning general representations even in the absence of, or with limited, supervision. A good general representation can be fine-tuned for new target tasks using modest amounts of data, or used directly in unseen domains achieving remarkable performance in the corresponding task. This alleviation of the data and annotation requirements offers tantalising prospects for applications in computer vision and healthcare. In this tutorial paper, we motivate the need for disentangled representations, revisit key concepts, and describe practical building blocks and criteria for learning such representations. We survey applications in medical imaging emphasising choices made in exemplar key works, and then discuss links to computer vision applications. We conclude by presenting limitations, challenges, and opportunities.



### Ultrafast Focus Detection for Automated Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2108.12050v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12050v3)
- **Published**: 2021-08-26 22:24:41+00:00
- **Updated**: 2022-02-23 03:33:24+00:00
- **Authors**: Maksim Levental, Ryan Chard, Kyle Chard, Ian Foster, Gregg A. Wildenberg
- **Comment**: None
- **Journal**: None
- **Summary**: Technological advancements in modern scientific instruments, such as scanning electron microscopes (SEMs), have significantly increased data acquisition rates and image resolutions enabling new questions to be explored; however, the resulting data volumes and velocities, combined with automated experiments, are quickly overwhelming scientists as there remain crucial steps that require human intervention, for example reviewing image focus. We present a fast out-of-focus detection algorithm for electron microscopy images collected serially and demonstrate that it can be used to provide near-real-time quality control for neuroscience workflows. Our technique, \textit{Multi-scale Histologic Feature Detection}, adapts classical computer vision techniques and is based on detecting various fine-grained histologic features. We exploit the inherent parallelism in the technique to employ GPU primitives in order to accelerate characterization. We show that our method can detect of out-of-focus conditions within just 20ms. To make these capabilities generally available, we deploy our feature detector as an on-demand service and show that it can be used to determine the degree of focus in approximately 230ms, enabling near-real-time use.



### Continual learning under domain transfer with sparse synaptic bursting
- **Arxiv ID**: http://arxiv.org/abs/2108.12056v8
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12056v8)
- **Published**: 2021-08-26 22:53:27+00:00
- **Updated**: 2022-09-20 15:58:57+00:00
- **Authors**: Shawn L. Beaulieu, Jeff Clune, Nick Cheney
- **Comment**: None
- **Journal**: None
- **Summary**: Existing machines are functionally specific tools that were made for easy prediction and control. Tomorrow's machines may be closer to biological systems in their mutability, resilience, and autonomy. But first they must be capable of learning and retaining new information without being exposed to it arbitrarily often. Past efforts to engineer such systems have sought to build or regulate artificial neural networks using disjoint sets of weights that are uniquely sensitive to specific tasks or inputs. This has not yet enabled continual learning over long sequences of previously unseen data without corrupting existing knowledge: a problem known as catastrophic forgetting. In this paper, we introduce a system that can learn sequentially over previously unseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This is done by controlling the activity of weights in a convolutional neural network on the basis of inputs using top-down regulation generated by a second feed-forward neural network. We find that our method learns continually under domain transfer with sparse bursts of activity in weights that are recycled across tasks, rather than by maintaining task-specific modules. Sparse synaptic bursting is found to balance activity and suppression such that new functions can be learned without corrupting extant knowledge, thus mirroring the balance of order and disorder in systems at the edge of chaos. This behavior emerges during a prior pre-training (or 'meta-learning') phase in which regulated synapses are selectively disinhibited, or grown, from an initial state of uniform suppression through prediction error minimization.



### Predicting Stable Configurations for Semantic Placement of Novel Objects
- **Arxiv ID**: http://arxiv.org/abs/2108.12062v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12062v1)
- **Published**: 2021-08-26 23:05:05+00:00
- **Updated**: 2021-08-26 23:05:05+00:00
- **Authors**: Chris Paxton, Chris Xie, Tucker Hermans, Dieter Fox
- **Comment**: None
- **Journal**: None
- **Summary**: Human environments contain numerous objects configured in a variety of arrangements. Our goal is to enable robots to repose previously unseen objects according to learned semantic relationships in novel environments. We break this problem down into two parts: (1) finding physically valid locations for the objects and (2) determining if those poses satisfy learned, high-level semantic relationships. We build our models and training from the ground up to be tightly integrated with our proposed planning algorithm for semantic placement of unknown objects. We train our models purely in simulation, with no fine-tuning needed for use in the real world. Our approach enables motion planning for semantic rearrangement of unknown objects in scenes with varying geometry from only RGB-D sensing. Our experiments through a set of simulated ablations demonstrate that using a relational classifier alone is not sufficient for reliable planning. We further demonstrate the ability of our planner to generate and execute diverse manipulation plans through a set of real-world experiments with a variety of objects.



### An Automatic Image Content Retrieval Method for better Mobile Device Display User Experiences
- **Arxiv ID**: http://arxiv.org/abs/2108.12068v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2108.12068v1)
- **Published**: 2021-08-26 23:44:34+00:00
- **Updated**: 2021-08-26 23:44:34+00:00
- **Authors**: Alessandro Bruno
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: A growing number of commercially available mobile phones come with integrated high-resolution digital cameras. That enables a new class of dedicated applications to image analysis such as mobile visual search, image cropping, object detection, content-based image retrieval, image classification. In this paper, a new mobile application for image content retrieval and classification for mobile device display is proposed to enrich the visual experience of users. The mobile application can extract a certain number of images based on the content of an image with visual saliency methods aiming at detecting the most critical regions in a given image from a perceptual viewpoint. First, the most critical areas from a perceptual perspective are extracted using the local maxima of a 2D saliency function. Next, a salient region is cropped using the bounding box centred on the local maxima of the thresholded Saliency Map of the image. Then, each image crop feds into an Image Classification system based on SVM and SIFT descriptors to detect the class of object present in the image. ImageNet repository was used as the reference for semantic category classification. Android platform was used to implement the mobile application on a client-server architecture. A mobile client sends the photo taken by the camera to the server, which processes the image and returns the results (image contents such as image crops and related target classes) to the mobile client. The application was run on thousands of pictures and showed encouraging results towards a better user visual experience with mobile displays.



