# Arxiv Papers in cs.CV on 2021-08-25
### Wanderlust: Online Continual Object Detection in the Real World
- **Arxiv ID**: http://arxiv.org/abs/2108.11005v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.11005v2)
- **Published**: 2021-08-25 01:08:35+00:00
- **Updated**: 2021-09-07 20:14:00+00:00
- **Authors**: Jianren Wang, Xin Wang, Yue Shang-Guan, Abhinav Gupta
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Online continual learning from data streams in dynamic environments is a critical direction in the computer vision field. However, realistic benchmarks and fundamental studies in this line are still missing. To bridge the gap, we present a new online continual object detection benchmark with an egocentric video dataset, Objects Around Krishna (OAK). OAK adopts the KrishnaCAM videos, an ego-centric video stream collected over nine months by a graduate student. OAK provides exhaustive bounding box annotations of 80 video snippets (~17.5 hours) for 105 object categories in outdoor scenes. The emergence of new object categories in our benchmark follows a pattern similar to what a single person might see in their day-to-day life. The dataset also captures the natural distribution shifts as the person travels to different places. These egocentric long-running videos provide a realistic playground for continual learning algorithms, especially in online embodied settings. We also introduce new evaluation metrics to evaluate the model performance and catastrophic forgetting and provide baseline studies for online continual object detection. We believe this benchmark will pose new exciting challenges for learning from non-stationary data in continual learning. The OAK dataset and the associated benchmark are released at https://oakdata.github.io/.



### iDARTS: Improving DARTS by Node Normalization and Decorrelation Discretization
- **Arxiv ID**: http://arxiv.org/abs/2108.11014v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11014v1)
- **Published**: 2021-08-25 02:23:30+00:00
- **Updated**: 2021-08-25 02:23:30+00:00
- **Authors**: Huiqun Wang, Ruijie Yang, Di Huang, Yunhong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Differentiable ARchiTecture Search (DARTS) uses a continuous relaxation of network representation and dramatically accelerates Neural Architecture Search (NAS) by almost thousands of times in GPU-day. However, the searching process of DARTS is unstable, which suffers severe degradation when training epochs become large, thus limiting its application. In this paper, we claim that this degradation issue is caused by the imbalanced norms between different nodes and the highly correlated outputs from various operations. We then propose an improved version of DARTS, namely iDARTS, to deal with the two problems. In the training phase, it introduces node normalization to maintain the norm balance. In the discretization phase, the continuous architecture is approximated based on the similarity between the outputs of the node and the decorrelated operations rather than the values of the architecture parameters. Extensive evaluation is conducted on CIFAR-10 and ImageNet, and the error rates of 2.25\% and 24.7\% are reported within 0.2 and 1.9 GPU-day for architecture search respectively, which shows its effectiveness. Additional analysis also reveals that iDARTS has the advantage in robustness and generalization over other DARTS-based counterparts.



### A Scaling Law for Synthetic-to-Real Transfer: How Much Is Your Pre-training Effective?
- **Arxiv ID**: http://arxiv.org/abs/2108.11018v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11018v3)
- **Published**: 2021-08-25 02:29:28+00:00
- **Updated**: 2021-10-08 20:57:04+00:00
- **Authors**: Hiroaki Mikami, Kenji Fukumizu, Shogo Murai, Shuji Suzuki, Yuta Kikuchi, Taiji Suzuki, Shin-ichi Maeda, Kohei Hayashi
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic-to-real transfer learning is a framework in which a synthetically generated dataset is used to pre-train a model to improve its performance on real vision tasks. The most significant advantage of using synthetic images is that the ground-truth labels are automatically available, enabling unlimited expansion of the data size without human cost. However, synthetic data may have a huge domain gap, in which case increasing the data size does not improve the performance. How can we know that? In this study, we derive a simple scaling law that predicts the performance from the amount of pre-training data. By estimating the parameters of the law, we can judge whether we should increase the data or change the setting of image synthesis. Further, we analyze the theory of transfer learning by considering learning dynamics and confirm that the derived generalization bound is consistent with our empirical findings. We empirically validated our scaling law on various experimental settings of benchmark tasks, model sizes, and complexities of synthetic images.



### Layer-wise Customized Weak Segmentation Block and AIoU Loss for Accurate Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.11021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11021v1)
- **Published**: 2021-08-25 02:42:28+00:00
- **Updated**: 2021-08-25 02:42:28+00:00
- **Authors**: Keyang Wang, Lei Zhang, Wenli Song, Qinghai Lang, Lingyun Qin
- **Comment**: To appear in IEEE International Conference on Image Processing 2021
- **Journal**: None
- **Summary**: The anchor-based detectors handle the problem of scale variation by building the feature pyramid and directly setting different scales of anchors on each cell in different layers. However, it is difficult for box-wise anchors to guide the adaptive learning of scale-specific features in each layer because there is no one-to-one correspondence between box-wise anchors and pixel-level features. In order to alleviate the problem, in this paper, we propose a scale-customized weak segmentation (SCWS) block at the pixel level for scale customized object feature learning in each layer. By integrating the SCWS blocks into the single-shot detector, a scale-aware object detector (SCOD) is constructed to detect objects of different sizes naturally and accurately. Furthermore, the standard location loss neglects the fact that the hard and easy samples may be seriously imbalanced. A forthcoming problem is that it is unable to get more accurate bounding boxes due to the imbalance. To address this problem, an adaptive IoU (AIoU) loss via a simple yet effective squeeze operation is specified in our SCOD. Extensive experiments on PASCAL VOC and MS COCO demonstrate the superiority of our SCOD.



### EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.11023v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.11023v1)
- **Published**: 2021-08-25 03:00:45+00:00
- **Updated**: 2021-08-25 03:00:45+00:00
- **Authors**: Hongbin Liu, Jinyuan Jia, Wenjie Qu, Neil Zhenqiang Gong
- **Comment**: To appear in ACM Conference on Computer and Communications Security
  (CCS), 2021
- **Journal**: None
- **Summary**: Given a set of unlabeled images or (image, text) pairs, contrastive learning aims to pre-train an image encoder that can be used as a feature extractor for many downstream tasks. In this work, we propose EncoderMI, the first membership inference method against image encoders pre-trained by contrastive learning. In particular, given an input and a black-box access to an image encoder, EncoderMI aims to infer whether the input is in the training dataset of the image encoder. EncoderMI can be used 1) by a data owner to audit whether its (public) data was used to pre-train an image encoder without its authorization or 2) by an attacker to compromise privacy of the training data when it is private/sensitive. Our EncoderMI exploits the overfitting of the image encoder towards its training data. In particular, an overfitted image encoder is more likely to output more (or less) similar feature vectors for two augmented versions of an input in (or not in) its training dataset. We evaluate EncoderMI on image encoders pre-trained on multiple datasets by ourselves as well as the Contrastive Language-Image Pre-training (CLIP) image encoder, which is pre-trained on 400 million (image, text) pairs collected from the Internet and released by OpenAI. Our results show that EncoderMI can achieve high accuracy, precision, and recall. We also explore a countermeasure against EncoderMI via preventing overfitting through early stopping. Our results show that it achieves trade-offs between accuracy of EncoderMI and utility of the image encoder, i.e., it can reduce the accuracy of EncoderMI, but it also incurs classification accuracy loss of the downstream classifiers built based on the image encoder.



### Improving Visual Quality of Unrestricted Adversarial Examples with Wavelet-VAE
- **Arxiv ID**: http://arxiv.org/abs/2108.11032v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.11032v1)
- **Published**: 2021-08-25 03:45:54+00:00
- **Updated**: 2021-08-25 03:45:54+00:00
- **Authors**: Wenzhao Xiang, Chang Liu, Shibao Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional adversarial examples are typically generated by adding perturbation noise to the input image within a small matrix norm. In practice, un-restricted adversarial attack has raised great concern and presented a new threat to the AI safety. In this paper, we propose a wavelet-VAE structure to reconstruct an input image and generate adversarial examples by modifying the latent code. Different from perturbation-based attack, the modifications of the proposed method are not limited but imperceptible to human eyes. Experiments show that our method can generate high quality adversarial examples on ImageNet dataset.



### NGC: A Unified Framework for Learning with Open-World Noisy Data
- **Arxiv ID**: http://arxiv.org/abs/2108.11035v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11035v1)
- **Published**: 2021-08-25 04:04:46+00:00
- **Updated**: 2021-08-25 04:04:46+00:00
- **Authors**: Zhi-Fan Wu, Tong Wei, Jianwen Jiang, Chaojie Mao, Mingqian Tang, Yu-Feng Li
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: The existence of noisy data is prevalent in both the training and testing phases of machine learning systems, which inevitably leads to the degradation of model performance. There have been plenty of works concentrated on learning with in-distribution (IND) noisy labels in the last decade, i.e., some training samples are assigned incorrect labels that do not correspond to their true classes. Nonetheless, in real application scenarios, it is necessary to consider the influence of out-of-distribution (OOD) samples, i.e., samples that do not belong to any known classes, which has not been sufficiently explored yet. To remedy this, we study a new problem setup, namely Learning with Open-world Noisy Data (LOND). The goal of LOND is to simultaneously learn a classifier and an OOD detector from datasets with mixed IND and OOD noise. In this paper, we propose a new graph-based framework, namely Noisy Graph Cleaning (NGC), which collects clean samples by leveraging geometric structure of data and model predictive confidence. Without any additional training effort, NGC can detect and reject the OOD samples based on the learned class prototypes directly in testing phase. We conduct experiments on multiple benchmarks with different types of noise and the results demonstrate the superior performance of our method against state of the arts.



### Localization Uncertainty-Based Attention for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.11042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11042v1)
- **Published**: 2021-08-25 04:32:39+00:00
- **Updated**: 2021-08-25 04:32:39+00:00
- **Authors**: Sanghun Park, Kunhee Kim, Eunseop Lee, Daijin Kim
- **Comment**: ICIP2021
- **Journal**: None
- **Summary**: Object detection has been applied in a wide variety of real world scenarios, so detection algorithms must provide confidence in the results to ensure that appropriate decisions can be made based on their results. Accordingly, several studies have investigated the probabilistic confidence of bounding box regression. However, such approaches have been restricted to anchor-based detectors, which use box confidence values as additional screening scores during non-maximum suppression (NMS) procedures. In this paper, we propose a more efficient uncertainty-aware dense detector (UADET) that predicts four-directional localization uncertainties via Gaussian modeling. Furthermore, a simple uncertainty attention module (UAM) that exploits box confidence maps is proposed to improve performance through feature refinement. Experiments using the MS COCO benchmark show that our UADET consistently surpasses baseline FCOS, and that our best model, ResNext-64x4d-101-DCN, obtains a single model, single-scale AP of 48.3% on COCO test-dev, thus achieving the state-of-the-art among various object detectors.



### Memory-Augmented Non-Local Attention for Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2108.11048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11048v1)
- **Published**: 2021-08-25 05:12:14+00:00
- **Updated**: 2021-08-25 05:12:14+00:00
- **Authors**: Jiyang Yu, Jingen Liu, Liefeng Bo, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel video super-resolution method that aims at generating high-fidelity high-resolution (HR) videos from low-resolution (LR) ones. Previous methods predominantly leverage temporal neighbor frames to assist the super-resolution of the current frame. Those methods achieve limited performance as they suffer from the challenge in spatial frame alignment and the lack of useful information from similar LR neighbor frames. In contrast, we devise a cross-frame non-local attention mechanism that allows video super-resolution without frame alignment, leading to be more robust to large motions in the video. In addition, to acquire the information beyond neighbor frames, we design a novel memory-augmented attention module to memorize general video details during the super-resolution training. Experimental results indicate that our method can achieve superior performance on large motion videos comparing to the state-of-the-art methods without aligning frames. Our source code will be released.



### Understanding of Kernels in CNN Models by Suppressing Irrelevant Visual Features in Images
- **Arxiv ID**: http://arxiv.org/abs/2108.11054v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11054v1)
- **Published**: 2021-08-25 05:48:44+00:00
- **Updated**: 2021-08-25 05:48:44+00:00
- **Authors**: Jia-Xin Zhuang, Wanying Tao, Jianfei Xing, Wei Shi, Ruixuan Wang, Wei-shi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have shown their superior performance in various vision tasks. However, the lack of precisely interpreting kernels in convolutional neural networks (CNNs) is becoming one main obstacle to wide applications of deep learning models in real scenarios. Although existing interpretation methods may find certain visual patterns which are associated with the activation of a specific kernel, those visual patterns may not be specific or comprehensive enough for interpretation of a specific activation of kernel of interest. In this paper, a simple yet effective optimization method is proposed to interpret the activation of any kernel of interest in CNN models. The basic idea is to simultaneously preserve the activation of the specific kernel and suppress the activation of all other kernels at the same layer. In this way, only visual information relevant to the activation of the specific kernel is remained in the input. Consistent visual information from multiple modified inputs would help users understand what kind of features are specifically associated with specific kernel. Comprehensive evaluation shows that the proposed method can help better interpret activation of specific kernels than widely used methods, even when two kernels have very similar activation regions from the same input image.



### Normal Learning in Videos with Attention Prototype Network
- **Arxiv ID**: http://arxiv.org/abs/2108.11055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11055v1)
- **Published**: 2021-08-25 05:51:58+00:00
- **Updated**: 2021-08-25 05:51:58+00:00
- **Authors**: Chao Hu, Fan Wu, Weijie Wu, Weibin Qiu, Shengxin Lai
- **Comment**: None
- **Journal**: None
- **Summary**: Frame reconstruction (current or future frame) based on Auto-Encoder (AE) is a popular method for video anomaly detection. With models trained on the normal data, the reconstruction errors of anomalous scenes are usually much larger than those of normal ones. Previous methods introduced the memory bank into AE, for encoding diverse normal patterns across the training videos. However, they are memory consuming and cannot cope with unseen new scenarios in the testing data. In this work, we propose a self-attention prototype unit (APU) to encode the normal latent space as prototypes in real time, free from extra memory cost. In addition, we introduce circulative attention mechanism to our backbone to form a novel feature extracting learner, namely Circulative Attention Unit (CAU). It enables the fast adaption capability on new scenes by only consuming a few iterations of update. Extensive experiments are conducted on various benchmarks. The superior performance over the state-of-the-art demonstrates the effectiveness of our method. Our code is available at https://github.com/huchao-AI/APN/.



### Learning Class-level Prototypes for Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.11072v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.11072v1)
- **Published**: 2021-08-25 06:33:52+00:00
- **Updated**: 2021-08-25 06:33:52+00:00
- **Authors**: Minglei Yuan, Wenhai Wang, Tao Wang, Chunhao Cai, Qian Xu, Tong Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning aims to recognize new categories using very few labeled samples. Although few-shot learning has witnessed promising development in recent years, most existing methods adopt an average operation to calculate prototypes, thus limited by the outlier samples. In this work, we propose a simple yet effective framework for few-shot classification, which can learn to generate preferable prototypes from few support data, with the help of an episodic prototype generator module. The generated prototype is meant to be close to a certain \textit{\targetproto{}} and is less influenced by outlier samples. Extensive experiments demonstrate the effectiveness of this module, and our approach gets a significant raise over baseline models, and get a competitive result compared to previous methods on \textit{mini}ImageNet, \textit{tiered}ImageNet, and cross-domain (\textit{mini}ImageNet $\rightarrow$ CUB-200-2011) datasets.



### Heredity-aware Child Face Image Generation with Latent Space Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2108.11080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11080v1)
- **Published**: 2021-08-25 06:59:43+00:00
- **Updated**: 2021-08-25 06:59:43+00:00
- **Authors**: Xiao Cui, Wengang Zhou, Yang Hu, Weilun Wang, Houqiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks have been widely used in image synthesis in recent years and the quality of the generated image has been greatly improved. However, the flexibility to control and decouple facial attributes (e.g., eyes, nose, mouth) is still limited. In this paper, we propose a novel approach, called ChildGAN, to generate a child's image according to the images of parents with heredity prior. The main idea is to disentangle the latent space of a pre-trained generation model and precisely control the face attributes of child images with clear semantics. We use distances between face landmarks as pseudo labels to figure out the most influential semantic vectors of the corresponding face attributes by calculating the gradient of latent vectors to pseudo labels. Furthermore, we disentangle the semantic vectors by weighting irrelevant features and orthogonalizing them with Schmidt Orthogonalization. Finally, we fuse the latent vector of the parents by leveraging the disentangled semantic vectors under the guidance of biological genetic laws. Extensive experiments demonstrate that our approach outperforms the existing methods with encouraging results.



### 3D Face Recognition: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2108.11082v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2108.11082v1)
- **Published**: 2021-08-25 07:00:59+00:00
- **Updated**: 2021-08-25 07:00:59+00:00
- **Authors**: Yaping Jing, Xuequan Lu, Shang Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition is one of the most studied research topics in the community. In recent years, the research on face recognition has shifted to using 3D facial surfaces, as more discriminating features can be represented by the 3D geometric information. This survey focuses on reviewing the 3D face recognition techniques developed in the past ten years which are generally categorized into conventional methods and deep learning methods. The categorized techniques are evaluated using detailed descriptions of the representative works. The advantages and disadvantages of the techniques are summarized in terms of accuracy, complexity and robustness to face variation (expression, pose and occlusions, etc). The main contribution of this survey is that it comprehensively covers both conventional methods and deep learning methods on 3D face recognition. In addition, a review of available 3D face databases is provided, along with the discussion of future research challenges and directions.



### Transformer for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2108.11084v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11084v3)
- **Published**: 2021-08-25 07:05:30+00:00
- **Updated**: 2022-04-22 05:56:50+00:00
- **Authors**: Zhisheng Lu, Juncheng Li, Hong Liu, Chaoyan Huang, Linlin Zhang, Tieyong Zeng
- **Comment**: Accepted by CVPR workshop 2022
- **Journal**: None
- **Summary**: Single image super-resolution (SISR) has witnessed great strides with the development of deep learning. However, most existing studies focus on building more complex networks with a massive number of layers. Recently, more and more researchers start to explore the application of Transformer in computer vision tasks. However, the heavy computational cost and high GPU memory occupation of the vision Transformer cannot be ignored. In this paper, we propose a novel Efficient Super-Resolution Transformer (ESRT) for SISR. ESRT is a hybrid model, which consists of a Lightweight CNN Backbone (LCB) and a Lightweight Transformer Backbone (LTB). Among them, LCB can dynamically adjust the size of the feature map to extract deep features with a low computational cost. LTB is composed of a series of Efficient Transformers (ET), which occupies a small GPU memory occupation, thanks to the specially designed Efficient Multi-Head Attention (EMHA). Extensive experiments show that ESRT achieves competitive results with low computational costs. Compared with the original Transformer which occupies 16,057M GPU memory, ESRT only occupies 4,191M GPU memory. All codes are available at https://github.com/luissen/ESRT.



### A Framework for Learning Ante-hoc Explainable Models via Concepts
- **Arxiv ID**: http://arxiv.org/abs/2108.11761v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11761v2)
- **Published**: 2021-08-25 07:09:57+00:00
- **Updated**: 2021-11-30 22:07:35+00:00
- **Authors**: Anirban Sarkar, Deepak Vijaykeerthy, Anindya Sarkar, Vineeth N Balasubramanian
- **Comment**: 16 pages, 15 figures
- **Journal**: None
- **Summary**: Self-explaining deep models are designed to learn the latent concept-based explanations implicitly during training, which eliminates the requirement of any post-hoc explanation generation technique. In this work, we propose one such model that appends an explanation generation module on top of any basic network and jointly trains the whole module that shows high predictive performance and generates meaningful explanations in terms of concepts. Our training strategy is suitable for unsupervised concept learning with much lesser parameter space requirements compared to baseline methods. Our proposed model also has provision for leveraging self-supervision on concepts to extract better explanations. However, with full concept supervision, we achieve the best predictive performance compared to recently proposed concept-based explainable models. We report both qualitative and quantitative results with our method, which shows better performance than recently proposed concept-based explainability methods. We reported exhaustive results with two datasets without ground truth concepts, i.e., CIFAR10, ImageNet, and two datasets with ground truth concepts, i.e., AwA2, CUB-200, to show the effectiveness of our method for both cases. To the best of our knowledge, we are the first ante-hoc explanation generation method to show results with a large-scale dataset such as ImageNet.



### Learning From Long-Tailed Data With Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2108.11096v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.11096v2)
- **Published**: 2021-08-25 07:45:40+00:00
- **Updated**: 2021-09-12 17:06:20+00:00
- **Authors**: Shyamgopal Karthik, Jérome Revaud, Boris Chidlovskii
- **Comment**: None
- **Journal**: None
- **Summary**: Class imbalance and noisy labels are the norm rather than the exception in many large-scale classification datasets. Nevertheless, most works in machine learning typically assume balanced and clean data. There have been some recent attempts to tackle, on one side, the problem of learning from noisy labels and, on the other side, learning from long-tailed data. Each group of methods make simplifying assumptions about the other. Due to this separation, the proposed solutions often underperform when both assumptions are violated. In this work, we present a simple two-stage approach based on recent advances in self-supervised learning to treat both challenges simultaneously. It consists of, first, task-agnostic self-supervised pre-training, followed by task-specific fine-tuning using an appropriate loss. Most significantly, we find that self-supervised learning approaches are effectively able to cope with severe class imbalance. In addition, the resulting learned representations are also remarkably robust to label noise, when fine-tuned with an imbalance- and noise-resistant loss function. We validate our claims with experiments on CIFAR-10 and CIFAR-100 augmented with synthetic imbalance and noise, as well as the large-scale inherently noisy Clothing-1M dataset.



### Monocular Depth Estimation Primed by Salient Point Detection and Normalized Hessian Loss
- **Arxiv ID**: http://arxiv.org/abs/2108.11098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11098v1)
- **Published**: 2021-08-25 07:51:09+00:00
- **Updated**: 2021-08-25 07:51:09+00:00
- **Authors**: Lam Huynh, Matteo Pedone, Phong Nguyen, Jiri Matas, Esa Rahtu, Janne Heikkila
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Deep neural networks have recently thrived on single image depth estimation. That being said, current developments on this topic highlight an apparent compromise between accuracy and network size. This work proposes an accurate and lightweight framework for monocular depth estimation based on a self-attention mechanism stemming from salient point detection. Specifically, we utilize a sparse set of keypoints to train a FuSaNet model that consists of two major components: Fusion-Net and Saliency-Net. In addition, we introduce a normalized Hessian loss term invariant to scaling and shear along the depth direction, which is shown to substantially improve the accuracy. The proposed method achieves state-of-the-art results on NYU-Depth-v2 and KITTI while using 3.1-38.4 times smaller model in terms of the number of parameters than baseline approaches. Experiments on the SUN-RGBD further demonstrate the generalizability of the proposed method.



### Multi-Attributed and Structured Text-to-Face Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2108.11100v1
- **DOI**: 10.1109/TEMSMET51618.2020.9557583
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11100v1)
- **Published**: 2021-08-25 07:52:21+00:00
- **Updated**: 2021-08-25 07:52:21+00:00
- **Authors**: Rohan Wadhawan, Tanuj Drall, Shubham Singh, Shampa Chakraverty
- **Comment**: Accepted by IEEE TEMSMET 2020, Camera Ready Version
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have revolutionized image synthesis through many applications like face generation, photograph editing, and image super-resolution. Image synthesis using GANs has predominantly been uni-modal, with few approaches that can synthesize images from text or other data modes. Text-to-image synthesis, especially text-to-face synthesis, has promising use cases of robust face-generation from eye witness accounts and augmentation of the reading experience with visual cues. However, only a couple of datasets provide consolidated face data and textual descriptions for text-to-face synthesis. Moreover, these textual annotations are less extensive and descriptive, which reduces the diversity of faces generated from it. This paper empirically proves that increasing the number of facial attributes in each textual description helps GANs generate more diverse and real-looking faces. To prove this, we propose a new methodology that focuses on using structured textual descriptions. We also consolidate a Multi-Attributed and Structured Text-to-face (MAST) dataset consisting of high-quality images with structured textual annotations and make it available to researchers to experiment and build upon. Lastly, we report benchmark Frechet's Inception Distance (FID), Facial Semantic Similarity (FSS), and Facial Semantic Distance (FSD) scores for the MAST dataset.



### Detecting Small Objects in Thermal Images Using Single-Shot Detector
- **Arxiv ID**: http://arxiv.org/abs/2108.11101v1
- **DOI**: 10.3103/S0146411621020097
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11101v1)
- **Published**: 2021-08-25 07:54:36+00:00
- **Updated**: 2021-08-25 07:54:36+00:00
- **Authors**: Hao Zhang, Xianggong Hong, Li Zhu
- **Comment**: None
- **Journal**: Automatic Control and Computer Sciences,2021
- **Summary**: SSD (Single Shot Multibox Detector) is one of the most successful object detectors for its high accuracy and fast speed. However, the features from shallow layer (mainly Conv4_3) of SSD lack semantic information, resulting in poor performance in small objects. In this paper, we proposed DDSSD (Dilation and Deconvolution Single Shot Multibox Detector), an enhanced SSD with a novel feature fusion module which can improve the performance over SSD for small object detection. In the feature fusion module, dilation convolution module is utilized to enlarge the receptive field of features from shallow layer and deconvolution module is adopted to increase the size of feature maps from high layer. Our network achieves 79.7% mAP on PASCAL VOC2007 test and 28.3% mmAP on MS COCO test-dev at 41 FPS with only 300x300 input using a single Nvidia 1080 GPU. Especially, for small objects, DDSSD achieves 10.5% on MS COCO and 22.8% on FLIR thermal dataset, outperforming a lot of state-of-the-art object detection algorithms in both aspects of accuracy and speed.



### Lightweight Monocular Depth with a Novel Neural Architecture Search Method
- **Arxiv ID**: http://arxiv.org/abs/2108.11105v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11105v1)
- **Published**: 2021-08-25 08:06:28+00:00
- **Updated**: 2021-08-25 08:06:28+00:00
- **Authors**: Lam Huynh, Phong Nguyen, Jiri Matas, Esa Rahtu, Janne Heikkila
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: This paper presents a novel neural architecture search method, called LiDNAS, for generating lightweight monocular depth estimation models. Unlike previous neural architecture search (NAS) approaches, where finding optimized networks are computationally highly demanding, the introduced novel Assisted Tabu Search leads to efficient architecture exploration. Moreover, we construct the search space on a pre-defined backbone network to balance layer diversity and search space size. The LiDNAS method outperforms the state-of-the-art NAS approach, proposed for disparity and depth estimation, in terms of search efficiency and output model performance. The LiDNAS optimized models achieve results superior to compact depth estimation state-of-the-art on NYU-Depth-v2, KITTI, and ScanNet, while being 7%-500% more compact in size, i.e the number of model parameters.



### TransFER: Learning Relation-aware Facial Expression Representations with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2108.11116v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11116v1)
- **Published**: 2021-08-25 08:28:34+00:00
- **Updated**: 2021-08-25 08:28:34+00:00
- **Authors**: Fanglei Xue, Qiangchang Wang, Guodong Guo
- **Comment**: Camera-ready, ICCV 2021
- **Journal**: None
- **Summary**: Facial expression recognition (FER) has received increasing interest in computer vision. We propose the TransFER model which can learn rich relation-aware local representations. It mainly consists of three components: Multi-Attention Dropping (MAD), ViT-FER, and Multi-head Self-Attention Dropping (MSAD). First, local patches play an important role in distinguishing various expressions, however, few existing works can locate discriminative and diverse local patches. This can cause serious problems when some patches are invisible due to pose variations or viewpoint changes. To address this issue, the MAD is proposed to randomly drop an attention map. Consequently, models are pushed to explore diverse local patches adaptively. Second, to build rich relations between different local patches, the Vision Transformers (ViT) are used in FER, called ViT-FER. Since the global scope is used to reinforce each local patch, a better representation is obtained to boost the FER performance. Thirdly, the multi-head self-attention allows ViT to jointly attend to features from different information subspaces at different positions. Given no explicit guidance, however, multiple self-attentions may extract similar relations. To address this, the MSAD is proposed to randomly drop one self-attention module. As a result, models are forced to learn rich relations among diverse local patches. Our proposed TransFER model outperforms the state-of-the-art methods on several FER benchmarks, showing its effectiveness and usefulness.



### GlassNet: Label Decoupling-based Three-stream Neural Network for Robust Image Glass Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.11117v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11117v2)
- **Published**: 2021-08-25 08:33:49+00:00
- **Updated**: 2022-01-10 16:37:31+00:00
- **Authors**: C. Zheng, D. Shi, X. Yan, D. Liang, M. wei, X. Yang, Y. Guo, H. Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the existing object detection methods generate poor glass detection results, due to the fact that the transparent glass shares the same appearance with arbitrary objects behind it in an image. Different from traditional deep learning-based wisdoms that simply use the object boundary as auxiliary supervision, we exploit label decoupling to decompose the original labeled ground-truth (GT) map into an interior-diffusion map and a boundary-diffusion map. The GT map in collaboration with the two newly generated maps breaks the imbalanced distribution of the object boundary, leading to improved glass detection quality. We have three key contributions to solve the transparent glass detection problem: (1) We propose a three-stream neural network (call GlassNet for short) to fully absorb beneficial features in the three maps. (2) We design a multi-scale interactive dilation module to explore a wider range of contextual information. (3) We develop an attention-based boundary-aware feature Mosaic module to integrate multi-modal information. Extensive experiments on the benchmark dataset exhibit clear improvements of our method over SOTAs, in terms of both the overall glass detection accuracy and boundary clearness.



### Product-oriented Machine Translation with Cross-modal Cross-lingual Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2108.11119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11119v1)
- **Published**: 2021-08-25 08:36:01+00:00
- **Updated**: 2021-08-25 08:36:01+00:00
- **Authors**: Yuqing Song, Shizhe Chen, Qin Jin, Wei Luo, Jun Xie, Fei Huang
- **Comment**: Accepted as Oral by ACMMM 2021
- **Journal**: None
- **Summary**: Translating e-commercial product descriptions, a.k.a product-oriented machine translation (PMT), is essential to serve e-shoppers all over the world. However, due to the domain specialty, the PMT task is more challenging than traditional machine translation problems. Firstly, there are many specialized jargons in the product description, which are ambiguous to translate without the product image. Secondly, product descriptions are related to the image in more complicated ways than standard image descriptions, involving various visual aspects such as objects, shapes, colors or even subjective styles. Moreover, existing PMT datasets are small in scale to support the research. In this paper, we first construct a large-scale bilingual product description dataset called Fashion-MMT, which contains over 114k noisy and 40k manually cleaned description translations with multiple product images. To effectively learn semantic alignments among product images and bilingual texts in translation, we design a unified product-oriented cross-modal cross-lingual model (\upoc~) for pre-training and fine-tuning. Experiments on the Fashion-MMT and Multi30k datasets show that our model significantly outperforms the state-of-the-art models even pre-trained on the same dataset. It is also shown to benefit more from large-scale noisy data to improve the translation quality. We will release the dataset and codes at https://github.com/syuqings/Fashion-MMT.



### AutoShape: Real-Time Shape-Aware Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.11127v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11127v1)
- **Published**: 2021-08-25 08:50:06+00:00
- **Updated**: 2021-08-25 08:50:06+00:00
- **Authors**: Zongdai Liu, Dingfu Zhou, Feixiang Lu, Jin Fang, Liangjun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Existing deep learning-based approaches for monocular 3D object detection in autonomous driving often model the object as a rotated 3D cuboid while the object's geometric shape has been ignored. In this work, we propose an approach for incorporating the shape-aware 2D/3D constraints into the 3D detection framework. Specifically, we employ the deep neural network to learn distinguished 2D keypoints in the 2D image domain and regress their corresponding 3D coordinates in the local 3D object coordinate first. Then the 2D/3D geometric constraints are built by these correspondences for each object to boost the detection performance. For generating the ground truth of 2D/3D keypoints, an automatic model-fitting approach has been proposed by fitting the deformed 3D object model and the object mask in the 2D image. The proposed framework has been verified on the public KITTI dataset and the experimental results demonstrate that by using additional geometrical constraints the detection performance has been significantly improved as compared to the baseline method. More importantly, the proposed framework achieves state-of-the-art performance with real time. Data and code will be available at https://github.com/zongdai/AutoShape



### Semantic Scene Segmentation for Robotics Applications
- **Arxiv ID**: http://arxiv.org/abs/2108.11128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11128v1)
- **Published**: 2021-08-25 08:55:20+00:00
- **Updated**: 2021-08-25 08:55:20+00:00
- **Authors**: Maria Tzelepi, Anastasios Tefas
- **Comment**: Accepted at IISA 2021
- **Journal**: None
- **Summary**: Semantic scene segmentation plays a critical role in a wide range of robotics applications, e.g., autonomous navigation. These applications are accompanied by specific computational restrictions, e.g., operation on low-power GPUs, at sufficient speed, and also for high-resolution input. Existing state-of-the-art segmentation models provide evaluation results under different setups and mainly considering high-power GPUs. In this paper, we investigate the behavior of the most successful semantic scene segmentation models, in terms of deployment (inference) speed, under various setups (GPUs, input sizes, etc.) in the context of robotics applications. The target of this work is to provide a comparative study of current state-of-the-art segmentation models so as to select the most compliant with the robotics applications requirements.



### A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games
- **Arxiv ID**: http://arxiv.org/abs/2108.11149v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11149v2)
- **Published**: 2021-08-25 10:09:28+00:00
- **Updated**: 2021-08-26 11:18:50+00:00
- **Authors**: Henrik Biermann, Jonas Theiner, Manuel Bassek, Dominik Raabe, Daniel Memmert, Ralph Ewerth
- **Comment**: None
- **Journal**: None
- **Summary**: The automatic detection of events in complex sports games like soccer and handball using positional or video data is of large interest in research and industry. One requirement is a fundamental understanding of underlying concepts, i.e., events that occur on the pitch. Previous work often deals only with so-called low-level events based on well-defined rules such as free kicks, free throws, or goals. High-level events, such as passes, are less frequently approached due to a lack of consistent definitions. This introduces a level of ambiguity that necessities careful validation when regarding event annotations. Yet, this validation step is usually neglected as the majority of studies adopt annotations from commercial providers on private datasets of unknown quality and focuses on soccer only. To address these issues, we present (1) a universal taxonomy that covers a wide range of low and high-level events for invasion games and is exemplarily refined to soccer and handball, and (2) release two multi-modal datasets comprising video and positional data with gold-standard annotations to foster research in fine-grained and ball-centered event spotting. Experiments on human performance demonstrate the robustness of the proposed taxonomy, and that disagreements and ambiguities in the annotation increase with the complexity of the event. An I3D model for video classification is adopted for event spotting and reveals the potential for benchmarking. Datasets are available at: https://github.com/mm4spa/eigd



### Duo-SegNet: Adversarial Dual-Views for Semi-Supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.11154v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11154v1)
- **Published**: 2021-08-25 10:16:12+00:00
- **Updated**: 2021-08-25 10:16:12+00:00
- **Authors**: Himashi Peiris, Zhaolin Chen, Gary Egan, Mehrtash Harandi
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation of images is a long-standing challenge in medical AI. This is mainly due to the fact that training a neural network to perform image segmentation requires a significant number of pixel-level annotated data, which is often unavailable. To address this issue, we propose a semi-supervised image segmentation technique based on the concept of multi-view learning. In contrast to the previous art, we introduce an adversarial form of dual-view training and employ a critic to formulate the learning problem in multi-view training as a min-max problem. Thorough quantitative and qualitative evaluations on several datasets indicate that our proposed method outperforms state-of-the-art medical image segmentation algorithms consistently and comfortably. The code is publicly available at https://github.com/himashi92/Duo-SegNet



### Adversarially Robust One-class Novelty Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.11168v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11168v2)
- **Published**: 2021-08-25 10:41:29+00:00
- **Updated**: 2022-10-03 20:29:44+00:00
- **Authors**: Shao-Yuan Lo, Poojan Oza, Vishal M. Patel
- **Comment**: Accepted in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI), 2022
- **Journal**: None
- **Summary**: One-class novelty detectors are trained with examples of a particular class and are tasked with identifying whether a query example belongs to the same known class. Most recent advances adopt a deep auto-encoder style architecture to compute novelty scores for detecting novel class data. Deep networks have shown to be vulnerable to adversarial attacks, yet little focus is devoted to studying the adversarial robustness of deep novelty detectors. In this paper, we first show that existing novelty detectors are susceptible to adversarial examples. We further demonstrate that commonly-used defense approaches for classification tasks have limited effectiveness in one-class novelty detection. Hence, we need a defense specifically designed for novelty detection. To this end, we propose a defense strategy that manipulates the latent space of novelty detectors to improve the robustness against adversarial examples. The proposed method, referred to as Principal Latent Space (PrincipaLS), learns the incrementally-trained cascade principal components in the latent space to robustify novelty detectors. PrincipaLS can purify latent space against adversarial examples and constrain latent space to exclusively model the known class distribution. We conduct extensive experiments on eight attacks, five datasets and seven novelty detectors, showing that PrincipaLS consistently enhances the adversarial robustness of novelty detection models. Code is available at https://github.com/shaoyuanlo/PrincipaLS



### Superpixel-guided Discriminative Low-rank Representation of Hyperspectral Images for Classification
- **Arxiv ID**: http://arxiv.org/abs/2108.11172v2
- **DOI**: 10.1109/TIP.2021.3120675
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.11172v2)
- **Published**: 2021-08-25 10:47:26+00:00
- **Updated**: 2021-10-25 14:42:34+00:00
- **Authors**: Shujun Yang, Junhui Hou, Yuheng Jia, Shaohui Mei, Qian Du
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel classification scheme for the remotely sensed hyperspectral image (HSI), namely SP-DLRR, by comprehensively exploring its unique characteristics, including the local spatial information and low-rankness. SP-DLRR is mainly composed of two modules, i.e., the classification-guided superpixel segmentation and the discriminative low-rank representation, which are iteratively conducted. Specifically, by utilizing the local spatial information and incorporating the predictions from a typical classifier, the first module segments pixels of an input HSI (or its restoration generated by the second module) into superpixels. According to the resulting superpixels, the pixels of the input HSI are then grouped into clusters and fed into our novel discriminative low-rank representation model with an effective numerical solution. Such a model is capable of increasing the intra-class similarity by suppressing the spectral variations locally while promoting the inter-class discriminability globally, leading to a restored HSI with more discriminative pixels. Experimental results on three benchmark datasets demonstrate the significant superiority of SP-DLRR over state-of-the-art methods, especially for the case with an extremely limited number of training pixels.



### Recall@k Surrogate Loss with Large Batches and Similarity Mixup
- **Arxiv ID**: http://arxiv.org/abs/2108.11179v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11179v2)
- **Published**: 2021-08-25 11:09:11+00:00
- **Updated**: 2022-03-25 09:06:25+00:00
- **Authors**: Yash Patel, Giorgos Tolias, Jiri Matas
- **Comment**: CVPR 2022 camera-ready version
- **Journal**: None
- **Summary**: This work focuses on learning deep visual representation models for retrieval by exploring the interplay between a new loss function, the batch size, and a new regularization approach. Direct optimization, by gradient descent, of an evaluation metric, is not possible when it is non-differentiable, which is the case for recall in retrieval. A differentiable surrogate loss for the recall is proposed in this work. Using an implementation that sidesteps the hardware constraints of the GPU memory, the method trains with a very large batch size, which is essential for metrics computed on the entire retrieval database. It is assisted by an efficient mixup regularization approach that operates on pairwise scalar similarities and virtually increases the batch size further. The suggested method achieves state-of-the-art performance in several image retrieval benchmarks when used for deep metric learning. For instance-level recognition, the method outperforms similar approaches that train using an approximation of average precision.



### Anomaly Detection in Medical Imaging -- A Mini Review
- **Arxiv ID**: http://arxiv.org/abs/2108.11986v1
- **DOI**: 10.1007/978-3-658-36295-9_5
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.11986v1)
- **Published**: 2021-08-25 11:45:40+00:00
- **Updated**: 2021-08-25 11:45:40+00:00
- **Authors**: Maximilian E. Tschuchnig, Michael Gadermayr
- **Comment**: Conference: iDSC2021
- **Journal**: iDSC2021
- **Summary**: The increasing digitization of medical imaging enables machine learning based improvements in detecting, visualizing and segmenting lesions, easing the workload for medical experts. However, supervised machine learning requires reliable labelled data, which is is often difficult or impossible to collect or at least time consuming and thereby costly. Therefore methods requiring only partly labeled data (semi-supervised) or no labeling at all (unsupervised methods) have been applied more regularly. Anomaly detection is one possible methodology that is able to leverage semi-supervised and unsupervised methods to handle medical imaging tasks like classification and segmentation. This paper uses a semi-exhaustive literature review of relevant anomaly detection papers in medical imaging to cluster into applications, highlight important results, establish lessons learned and give further advice on how to approach anomaly detection in medical imaging. The qualitative analysis is based on google scholar and 4 different search terms, resulting in 120 different analysed papers. The main results showed that the current research is mostly motivated by reducing the need for labelled data. Also, the successful and substantial amount of research in the brain MRI domain shows the potential for applications in further domains like OCT and chest X-ray.



### Lizard: A Large-Scale Dataset for Colonic Nuclear Instance Segmentation and Classification
- **Arxiv ID**: http://arxiv.org/abs/2108.11195v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.11195v2)
- **Published**: 2021-08-25 11:58:52+00:00
- **Updated**: 2021-11-29 11:16:00+00:00
- **Authors**: Simon Graham, Mostafa Jahanifar, Ayesha Azam, Mohammed Nimir, Yee-Wah Tsang, Katherine Dodd, Emily Hero, Harvir Sahota, Atisha Tank, Ksenija Benes, Noorul Wahab, Fayyaz Minhas, Shan E Ahmed Raza, Hesham El Daly, Kishore Gopalakrishnan, David Snead, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: The development of deep segmentation models for computational pathology (CPath) can help foster the investigation of interpretable morphological biomarkers. Yet, there is a major bottleneck in the success of such approaches because supervised deep learning models require an abundance of accurately labelled data. This issue is exacerbated in the field of CPath because the generation of detailed annotations usually demands the input of a pathologist to be able to distinguish between different tissue constructs and nuclei. Manually labelling nuclei may not be a feasible approach for collecting large-scale annotated datasets, especially when a single image region can contain thousands of different cells. However, solely relying on automatic generation of annotations will limit the accuracy and reliability of ground truth. Therefore, to help overcome the above challenges, we propose a multi-stage annotation pipeline to enable the collection of large-scale datasets for histology image analysis, with pathologist-in-the-loop refinement steps. Using this pipeline, we generate the largest known nuclear instance segmentation and classification dataset, containing nearly half a million labelled nuclei in H&E stained colon tissue. We have released the dataset and encourage the research community to utilise it to drive forward the development of downstream cell-based models in CPath.



### Multi-domain semantic segmentation with overlapping labels
- **Arxiv ID**: http://arxiv.org/abs/2108.11224v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11224v2)
- **Published**: 2021-08-25 13:25:41+00:00
- **Updated**: 2021-11-02 17:25:44+00:00
- **Authors**: Petra Bevandić, Marin Oršić, Ivan Grubišić, Josip Šarić, Siniša Šegvić
- **Comment**: 18 pages, 8 figures, 11 tables
- **Journal**: None
- **Summary**: Deep supervised models have an unprecedented capacity to absorb large quantities of training data. Hence, training on many datasets becomes a method of choice towards graceful degradation in unusual scenes. Unfortunately, different datasets often use incompatible labels. For instance, the Cityscapes road class subsumes all driving surfaces, while Vistas defines separate classes for road markings, manholes etc. We address this challenge by proposing a principled method for seamless learning on datasets with overlapping classes based on partial labels and probabilistic loss. Our method achieves competitive within-dataset and cross-dataset generalization, as well as ability to learn visual concepts which are not separately labeled in any of the training datasets. Experiments reveal competitive or state-of-the-art performance on two multi-domain dataset collections and on the WildDash 2 benchmark.



### Cell Multi-Bernoulli (Cell-MB) Sensor Control for Multi-object Search-While-Tracking (SWT)
- **Arxiv ID**: http://arxiv.org/abs/2108.11236v2
- **DOI**: 10.1109/TPAMI.2022.3223856
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11236v2)
- **Published**: 2021-08-25 13:49:14+00:00
- **Updated**: 2022-07-12 02:00:58+00:00
- **Authors**: Keith A. LeGrand, Pingping Zhu, Silvia Ferrari
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  Vol. 45, No. 6, 01 June 2023, 7195 - 7207
- **Summary**: Information-driven control can be used to develop intelligent sensors that can optimize their measurement value based on environmental feedback. In object tracking applications, sensor actions are chosen based on the expected reduction in uncertainty also known as information gain. Random finite set (RFS) theory provides a formalism for quantifying and estimating information gain in multi-object tracking problems. However, estimating information gain in these applications remains computationally challenging. This paper presents a new tractable approximation of the RFS expected information gain applicable to sensor control for multi-object search and tracking. Unlike existing RFS approaches, the information gain approximation presented in this paper considers the contributions of non-ideal noisy measurements, missed detections, false alarms, and object appearance/disappearance. The effectiveness of the information-driven sensor control is demonstrated through two multi-vehicle search-while-tracking experiments using real video data from remote terrestrial and satellite sensors.



### Fiducial marker recovery and detection from severely truncated data in navigation assisted spine surgery
- **Arxiv ID**: http://arxiv.org/abs/2108.13844v3
- **DOI**: 10.1002/mp.15617
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.13844v3)
- **Published**: 2021-08-25 13:52:58+00:00
- **Updated**: 2021-09-28 11:46:43+00:00
- **Authors**: Fuxin Fan, Björn Kreher, Holger Keil, Andreas Maier, Yixing Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Fiducial markers are commonly used in navigation assisted minimally invasive spine surgery (MISS) and they help transfer image coordinates into real world coordinates. In practice, these markers might be located outside the field-of-view (FOV), due to the limited detector sizes of C-arm cone-beam computed tomography (CBCT) systems used in intraoperative surgeries. As a consequence, reconstructed markers in CBCT volumes suffer from artifacts and have distorted shapes, which sets an obstacle for navigation. In this work, we propose two fiducial marker detection methods: direct detection from distorted markers (direct method) and detection after marker recovery (recovery method). For direct detection from distorted markers in reconstructed volumes, an efficient automatic marker detection method using two neural networks and a conventional circle detection algorithm is proposed. For marker recovery, a task-specific learning strategy is proposed to recover markers from severely truncated data. Afterwards, a conventional marker detection algorithm is applied for position detection. The two methods are evaluated on simulated data and real data, both achieving a marker registration error smaller than 0.2 mm. Our experiments demonstrate that the direct method is capable of detecting distorted markers accurately and the recovery method with task-specific learning has high robustness and generalizability on various data sets.



### Multiscale Spatio-Temporal Graph Neural Networks for 3D Skeleton-Based Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2108.11244v1
- **DOI**: 10.1109/TIP.2021.3108708
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.11244v1)
- **Published**: 2021-08-25 14:05:37+00:00
- **Updated**: 2021-08-25 14:05:37+00:00
- **Authors**: Maosen Li, Siheng Chen, Yangheng Zhao, Ya Zhang, Yanfeng Wang, Qi Tian
- **Comment**: Accepted by IEEE-TIP
- **Journal**: None
- **Summary**: We propose a multiscale spatio-temporal graph neural network (MST-GNN) to predict the future 3D skeleton-based human poses in an action-category-agnostic manner. The core of MST-GNN is a multiscale spatio-temporal graph that explicitly models the relations in motions at various spatial and temporal scales. Different from many previous hierarchical structures, our multiscale spatio-temporal graph is built in a data-adaptive fashion, which captures nonphysical, yet motion-based relations. The key module of MST-GNN is a multiscale spatio-temporal graph computational unit (MST-GCU) based on the trainable graph structure. MST-GCU embeds underlying features at individual scales and then fuses features across scales to obtain a comprehensive representation. The overall architecture of MST-GNN follows an encoder-decoder framework, where the encoder consists of a sequence of MST-GCUs to learn the spatial and temporal features of motions, and the decoder uses a graph-based attention gate recurrent unit (GA-GRU) to generate future poses. Extensive experiments are conducted to show that the proposed MST-GNN outperforms state-of-the-art methods in both short and long-term motion prediction on the datasets of Human 3.6M, CMU Mocap and 3DPW, where MST-GNN outperforms previous works by 5.33% and 3.67% of mean angle errors in average for short-term and long-term prediction on Human 3.6M, and by 11.84% and 4.71% of mean angle errors for short-term and long-term prediction on CMU Mocap, and by 1.13% of mean angle errors on 3DPW in average, respectively. We further investigate the learned multiscale graphs for interpretability.



### Generalize then Adapt: Source-Free Domain Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.11249v1
- **DOI**: 10.1109/ICCV48922.2021.00696
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.11249v1)
- **Published**: 2021-08-25 14:18:59+00:00
- **Updated**: 2021-08-25 14:18:59+00:00
- **Authors**: Jogendra Nath Kundu, Akshay Kulkarni, Amit Singh, Varun Jampani, R. Venkatesh Babu
- **Comment**: ICCV 2021. Project page: http://sites.google.com/view/sfdaseg
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (DA) has gained substantial interest in semantic segmentation. However, almost all prior arts assume concurrent access to both labeled source and unlabeled target, making them unsuitable for scenarios demanding source-free adaptation. In this work, we enable source-free DA by partitioning the task into two: a) source-only domain generalization and b) source-free target adaptation. Towards the former, we provide theoretical insights to develop a multi-head framework trained with a virtually extended multi-source dataset, aiming to balance generalization and specificity. Towards the latter, we utilize the multi-head framework to extract reliable target pseudo-labels for self-training. Additionally, we introduce a novel conditional prior-enforcing auto-encoder that discourages spatial irregularities, thereby enhancing the pseudo-label quality. Experiments on the standard GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes benchmarks show our superiority even against the non-source-free prior-arts. Further, we show our compatibility with online adaptation enabling deployment in a sequentially changing environment.



### YOLOP: You Only Look Once for Panoptic Driving Perception
- **Arxiv ID**: http://arxiv.org/abs/2108.11250v7
- **DOI**: 10.1007/s11633-022-1339-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11250v7)
- **Published**: 2021-08-25 14:19:42+00:00
- **Updated**: 2022-03-26 15:39:42+00:00
- **Authors**: Dong Wu, Manwen Liao, Weitian Zhang, Xinggang Wang, Xiang Bai, Wenqing Cheng, Wenyu Liu
- **Comment**: None
- **Journal**: [J]. Machine Intelligence Research, 2022: 1-13
- **Summary**: A panoptic driving perception system is an essential part of autonomous driving. A high-precision and real-time perception system can assist the vehicle in making the reasonable decision while driving. We present a panoptic driving perception network (YOLOP) to perform traffic object detection, drivable area segmentation and lane detection simultaneously. It is composed of one encoder for feature extraction and three decoders to handle the specific tasks. Our model performs extremely well on the challenging BDD100K dataset, achieving state-of-the-art on all three tasks in terms of accuracy and speed. Besides, we verify the effectiveness of our multi-task learning model for joint training via ablative studies. To our best knowledge, this is the first work that can process these three visual perception tasks simultaneously in real-time on an embedded device Jetson TX2(23 FPS) and maintain excellent accuracy. To facilitate further research, the source codes and pre-trained models are released at https://github.com/hustvl/YOLOP.



### Deep few-shot learning for bi-temporal building change detection
- **Arxiv ID**: http://arxiv.org/abs/2108.11262v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11262v2)
- **Published**: 2021-08-25 14:38:21+00:00
- **Updated**: 2021-10-05 10:48:15+00:00
- **Authors**: Mehdi Khoshboresh-Masouleh, Reza Shah-Hosseini
- **Comment**: 5 pages, 3 figures
- **Journal**: ISPRS-International Archives of the Photogrammetry, Remote Sensing
  and Spatial Information Sciences - 2021
- **Summary**: In real-world applications (e.g., change detection), annotating images is very expensive. To build effective deep learning models in these applications, deep few-shot learning methods have been developed and prove to be a robust approach in small training data. The analysis of building change detection from high spatial resolution remote sensing observations is important research in photogrammetry, computer vision, and remote sensing nowadays, which can be widely used in a variety of real-world applications, such as map updating. As manual high resolution image interpretation is expensive and time-consuming, building change detection methods are of high interest. The interest in developing building change detection approaches from optical remote sensing images is rapidly increasing due to larger coverages, and lower costs of optical images. In this study, we focus on building change detection analysis on a small set of building change from different regions that sit in several cities. In this paper, a new deep few-shot learning method is proposed for building change detection using Monte Carlo dropout and remote sensing observations. The setup is based on a small dataset, including bitemporal optical images labeled for building change detection.



### Multi-task learning from fixed-wing UAV images for 2D/3D city modeling
- **Arxiv ID**: http://arxiv.org/abs/2109.00918v1
- **DOI**: 10.5194/isprs-archives-XLIV-M-3-2021-1-2021
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.00918v1)
- **Published**: 2021-08-25 14:45:42+00:00
- **Updated**: 2021-08-25 14:45:42+00:00
- **Authors**: Mohammad R. Bayanlou, Mehdi Khoshboresh-Masouleh
- **Comment**: None
- **Journal**: The International Archives of the Photogrammetry, Remote Sensing
  and Spatial Information Sciences, Volume XLIV-M-3-2021, 2021, pp.1-5
- **Summary**: Single-task learning in artificial neural networks will be able to learn the model very well, and the benefits brought by transferring knowledge thus become limited. In this regard, when the number of tasks increases (e.g., semantic segmentation, panoptic segmentation, monocular depth estimation, and 3D point cloud), duplicate information may exist across tasks, and the improvement becomes less significant. Multi-task learning has emerged as a solution to knowledge-transfer issues and is an approach to scene understanding which involves multiple related tasks each with potentially limited training data. Multi-task learning improves generalization by leveraging the domain-specific information contained in the training data of related tasks. In urban management applications such as infrastructure development, traffic monitoring, smart 3D cities, and change detection, automated multi-task data analysis for scene understanding based on the semantic, instance, and panoptic annotation, as well as monocular depth estimation, is required to generate precise urban models. In this study, a common framework for the performance assessment of multi-task learning methods from fixed-wing UAV images for 2D/3D city modeling is presented.



### Domain Adversarial RetinaNet as a Reference Algorithm for the MItosis DOmain Generalization Challenge
- **Arxiv ID**: http://arxiv.org/abs/2108.11269v3
- **DOI**: 10.1007/978-3-030-97281-3
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11269v3)
- **Published**: 2021-08-25 14:49:11+00:00
- **Updated**: 2022-03-15 17:09:50+00:00
- **Authors**: Frauke Wilm, Christian Marzahl, Katharina Breininger, Marc Aubreville
- **Comment**: This is the long version of the original pre-print. Due to a bug in
  our automatic threshold computation the detection threshold of our model
  changed from 0.62 to 0.64. This value was not optimized on any other images
  but the validation split of the MIDOG training set. 9 pages, 4 figures, 1
  table
- **Journal**: None
- **Summary**: Assessing the Mitotic Count has a known high degree of intra- and inter-rater variability. Computer-aided systems have proven to decrease this variability and reduce labeling time. These systems, however, are generally highly dependent on their training domain and show poor applicability to unseen domains. In histopathology, these domain shifts can result from various sources, including different slide scanning systems used to digitize histologic samples. The MItosis DOmain Generalization challenge focused on this specific domain shift for the task of mitotic figure detection. This work presents a mitotic figure detection algorithm developed as a baseline for the challenge, based on domain adversarial training. On the challenge's test set, the algorithm scored an F$_1$ score of 0.7183. The corresponding network weights and code for implementing the network are made publicly available.



### Measurement of Hybrid Rocket Solid Fuel Regression Rate for a Slab Burner using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.11276v1
- **DOI**: 10.1016/j.actaastro.2021.09.046
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11276v1)
- **Published**: 2021-08-25 14:57:23+00:00
- **Updated**: 2021-08-25 14:57:23+00:00
- **Authors**: Gabriel Surina III, Georgios Georgalis, Siddhant S. Aphale, Abani Patra, Paul E. DesJardin
- **Comment**: None
- **Journal**: None
- **Summary**: This study presents an imaging-based deep learning tool to measure the fuel regression rate in a 2D slab burner experiment for hybrid rocket fuels. The slab burner experiment is designed to verify mechanistic models of reacting boundary layer combustion in hybrid rockets by the measurement of fuel regression rates. A DSLR camera with a high intensity flash is used to capture images throughout the burn and the images are then used to find the fuel boundary to calculate the regression rate. A U-net convolutional neural network architecture is explored to segment the fuel from the experimental images. A Monte-Carlo Dropout process is used to quantify the regression rate uncertainty produced from the network. The U-net computed regression rates are compared with values from other techniques from literature and show error less than 10%. An oxidizer flux dependency study is performed and shows the U-net predictions of regression rates are accurate and independent of the oxidizer flux, when the images in the training set are not over-saturated. Training with monochrome images is explored and is not successful at predicting the fuel regression rate from images with high noise. The network is superior at filtering out noise introduced by soot, pitting, and wax deposition on the chamber glass as well as the flame when compared to traditional image processing techniques, such as threshold binary conversion and spatial filtering. U-net consistently provides low error image segmentations to allow accurate computation of the regression rate of the fuel.



### Automatic Feature Highlighting in Noisy RES Data With CycleGAN
- **Arxiv ID**: http://arxiv.org/abs/2108.11283v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/2108.11283v1)
- **Published**: 2021-08-25 15:03:47+00:00
- **Updated**: 2021-08-25 15:03:47+00:00
- **Authors**: Nicholas Khami, Omar Imtiaz, Akif Abidi, Akash Aedavelli, Alan Goff, Jesse R. Pisel, Michael J. Pyrcz
- **Comment**: 12 pages, 8 figures, 1 table
- **Journal**: None
- **Summary**: Radio echo sounding (RES) is a common technique used in subsurface glacial imaging, which provides insight into the underlying rock and ice. However, systematic noise is introduced into the data during collection, complicating interpretation of the results. Researchers most often use a combination of manual interpretation and filtering techniques to denoise data; however, these processes are time intensive and inconsistent. Fully Convolutional Networks have been proposed as an automated alternative to identify layer boundaries in radargrams. However, they require high-quality manually processed training data and struggle to interpolate data in noisy samples (Varshney et al. 2020).   Herein, the authors propose a GAN based model to interpolate layer boundaries through noise and highlight layers in two-dimensional glacial RES data. In real-world noisy images, filtering often results in loss of data such that interpolating layer boundaries is nearly impossible. Furthermore, traditional machine learning approaches are not suited to this task because of the lack of paired data, so we employ an unpaired image-to-image translation model. For this model, we create a synthetic dataset to represent the domain of images with clear, highlighted layers and use an existing real-world RES dataset as our noisy domain.   We implement a CycleGAN trained on these two domains to highlight layers in noisy images that can interpolate effectively without significant loss of structure or fidelity. Though the current implementation is not a perfect solution, the model clearly highlights layers in noisy data and allows researchers to determine layer size and position without mathematical filtering, manual processing, or ground-truth images for training. This is significant because clean images generated by our model enable subsurface researchers to determine glacial layer thickness more efficiently.



### Fully Non-Homogeneous Atmospheric Scattering Modeling with Convolutional Neural Networks for Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2108.11292v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11292v1)
- **Published**: 2021-08-25 15:27:44+00:00
- **Updated**: 2021-08-25 15:27:44+00:00
- **Authors**: Cong Wang, Yan Huang, Yuexian Zou, Yong Xu
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: In recent years, single image dehazing models (SIDM) based on atmospheric scattering model (ASM) have achieved remarkable results. However, it is noted that ASM-based SIDM degrades its performance in dehazing real world hazy images due to the limited modelling ability of ASM where the atmospheric light factor (ALF) and the angular scattering coefficient (ASC) are assumed as constants for one image. Obviously, the hazy images taken in real world cannot always satisfy this assumption. Such generating modelling mismatch between the real-world images and ASM sets up the upper bound of trained ASM-based SIDM for dehazing. Bearing this in mind, in this study, a new fully non-homogeneous atmospheric scattering model (FNH-ASM) is proposed for well modeling the hazy images under complex conditions where ALF and ASC are pixel dependent. However, FNH-ASM brings difficulty in practical application. In FNH-ASM based SIDM, the estimation bias of parameters at different positions lead to different distortion of dehazing result. Hence, in order to reduce the influence of parameter estimation bias on dehazing results, two new cost sensitive loss functions, beta-Loss and D-Loss, are innovatively developed for limiting the parameter bias of sensitive positions that have a greater impact on the dehazing result. In the end, based on FNH-ASM, an end-to-end CNN-based dehazing network, FNHD-Net, is developed, which applies beta-Loss and D-Loss. Experimental results demonstrate the effectiveness and superiority of our proposed FNHD-Net for dehazing on both synthetic and real-world images. And the performance improvement of our method increases more obviously in dense and heterogeneous haze scenes.



### Availability Attacks Against Neural Network Certifiers Based on Backdoors
- **Arxiv ID**: http://arxiv.org/abs/2108.11299v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11299v4)
- **Published**: 2021-08-25 15:49:10+00:00
- **Updated**: 2022-10-02 16:58:47+00:00
- **Authors**: Tobias Lorenz, Marta Kwiatkowska, Mario Fritz
- **Comment**: None
- **Journal**: None
- **Summary**: To achieve reliable, robust, and safe AI systems it is important to implement fallback strategies when AI predictions cannot be trusted. Certifiers for neural networks are a reliable way to check the robustness of these predictions. They guarantee for some predictions that a certain class of manipulations or attacks could not have changed the outcome. For the remaining predictions without guarantees, the method abstains from making a prediction and a fallback strategy needs to be invoked, which typically incurs additional costs, can require a human operator, or even fail to provide any prediction. While this is a key concept towards safe and secure AI, we show for the first time that this approach comes with its own security risks, as such fallback strategies can be deliberately triggered by an adversary. Using training-time attacks, the adversary can significantly reduce the certified robustness of the model, making it unavailable. This transfers the main system load onto the fallback, reducing the overall system's integrity and availability. We design two novel backdoor attacks which show the practical relevance of these threats. For example, adding 1% poisoned data during training is sufficient to reduce certified robustness by up to 95 percentage points. Our extensive experiments across multiple datasets, model architectures, and certifiers demonstrate the wide applicability of these attacks. A first investigation into potential defenses shows that current approaches are insufficient to mitigate the issue, highlighting the need for new, more specific solutions.



### CSG-Stump: A Learning Friendly CSG-Like Representation for Interpretable Shape Parsing
- **Arxiv ID**: http://arxiv.org/abs/2108.11305v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.11305v1)
- **Published**: 2021-08-25 16:08:39+00:00
- **Updated**: 2021-08-25 16:08:39+00:00
- **Authors**: Daxuan Ren, Jianmin Zheng, Jianfei Cai, Jiatong Li, Haiyong Jiang, Zhongang Cai, Junzhe Zhang, Liang Pan, Mingyuan Zhang, Haiyu Zhao, Shuai Yi
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Generating an interpretable and compact representation of 3D shapes from point clouds is an important and challenging problem. This paper presents CSG-Stump Net, an unsupervised end-to-end network for learning shapes from point clouds and discovering the underlying constituent modeling primitives and operations as well. At the core is a three-level structure called {\em CSG-Stump}, consisting of a complement layer at the bottom, an intersection layer in the middle, and a union layer at the top. CSG-Stump is proven to be equivalent to CSG in terms of representation, therefore inheriting the interpretable, compact and editable nature of CSG while freeing from CSG's complex tree structures. Particularly, the CSG-Stump has a simple and regular structure, allowing neural networks to give outputs of a constant dimensionality, which makes itself deep-learning friendly. Due to these characteristics of CSG-Stump, CSG-Stump Net achieves superior results compared to previous CSG-based methods and generates much more appealing shapes, as confirmed by extensive experiments. Project page: https://kimren227.github.io/projects/CSGStump/



### Multi-Task Self-Training for Learning General Representations
- **Arxiv ID**: http://arxiv.org/abs/2108.11353v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11353v1)
- **Published**: 2021-08-25 17:20:50+00:00
- **Updated**: 2021-08-25 17:20:50+00:00
- **Authors**: Golnaz Ghiasi, Barret Zoph, Ekin D. Cubuk, Quoc V. Le, Tsung-Yi Lin
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Despite the fast progress in training specialized models for various tasks, learning a single general model that works well for many tasks is still challenging for computer vision. Here we introduce multi-task self-training (MuST), which harnesses the knowledge in independent specialized teacher models (e.g., ImageNet model on classification) to train a single general student model. Our approach has three steps. First, we train specialized teachers independently on labeled datasets. We then use the specialized teachers to label an unlabeled dataset to create a multi-task pseudo labeled dataset. Finally, the dataset, which now contains pseudo labels from teacher models trained on different datasets/tasks, is then used to train a student model with multi-task learning. We evaluate the feature representations of the student model on 6 vision tasks including image recognition (classification, detection, segmentation)and 3D geometry estimation (depth and surface normal estimation). MuST is scalable with unlabeled or partially labeled datasets and outperforms both specialized supervised models and self-supervised models when training on large scale datasets. Lastly, we show MuST can improve upon already strong checkpoints trained with billions of examples. The results suggest self-training is a promising direction to aggregate labeled and unlabeled training data for learning general feature representations.



### Blind Image Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2108.11364v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11364v3)
- **Published**: 2021-08-25 17:37:19+00:00
- **Updated**: 2022-07-18 17:43:29+00:00
- **Authors**: Junlin Han, Weihao Li, Pengfei Fang, Chunyi Sun, Jie Hong, Mohammad Ali Armin, Lars Petersson, Hongdong Li
- **Comment**: ECCV 2022. Project page:
  https://junlinhan.github.io/projects/BID.html. Code:
  https://github.com/JunlinHan/BID
- **Journal**: None
- **Summary**: We propose and study a novel task named Blind Image Decomposition (BID), which requires separating a superimposed image into constituent underlying images in a blind setting, that is, both the source components involved in mixing as well as the mixing mechanism are unknown. For example, rain may consist of multiple components, such as rain streaks, raindrops, snow, and haze. Rainy images can be treated as an arbitrary combination of these components, some of them or all of them. How to decompose superimposed images, like rainy images, into distinct source components is a crucial step toward real-world vision systems. To facilitate research on this new task, we construct multiple benchmark datasets, including mixed image decomposition across multiple domains, real-scenario deraining, and joint shadow/reflection/watermark removal. Moreover, we propose a simple yet general Blind Image Decomposition Network (BIDeN) to serve as a strong baseline for future work. Experimental results demonstrate the tenability of our benchmarks and the effectiveness of BIDeN.



### CDCGen: Cross-Domain Conditional Generation via Normalizing Flows and Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2108.11368v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.11368v1)
- **Published**: 2021-08-25 17:55:14+00:00
- **Updated**: 2021-08-25 17:55:14+00:00
- **Authors**: Hari Prasanna Das, Ryan Tran, Japjot Singh, Yu-Wen Lin, Costas J. Spanos
- **Comment**: Workshop on Machine Learning for Data: Automated Creation,Privacy,
  Bias, In 38th International Conference on Machine Learning (ICML) 2021
- **Journal**: None
- **Summary**: How to generate conditional synthetic data for a domain without utilizing information about its labels/attributes? Our work presents a solution to the above question. We propose a transfer learning-based framework utilizing normalizing flows, coupled with both maximum-likelihood and adversarial training. We model a source domain (labels available) and a target domain (labels unavailable) with individual normalizing flows, and perform domain alignment to a common latent space using adversarial discriminators. Due to the invertible property of flow models, the mapping has exact cycle consistency. We also learn the joint distribution of the data samples and attributes in the source domain by employing an encoder to map attributes to the latent space via adversarial training. During the synthesis phase, given any combination of attributes, our method can generate synthetic samples conditioned on them in the target domain. Empirical studies confirm the effectiveness of our method on benchmarked datasets. We envision our method to be particularly useful for synthetic data generation in label-scarce systems by generating non-trivial augmentations via attribute transformations. These synthetic samples will introduce more entropy into the label-scarce domain than their geometric and photometric transformation counterparts, helpful for robust downstream tasks.



### PGTRNet: Two-phase Weakly Supervised Object Detection with Pseudo Ground Truth Refinement
- **Arxiv ID**: http://arxiv.org/abs/2108.11439v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11439v2)
- **Published**: 2021-08-25 19:20:49+00:00
- **Updated**: 2022-03-17 19:00:16+00:00
- **Authors**: Jun Wang, Hefeng Zhou, Xiaohan Yu
- **Comment**: This paper was accepted by ICASSP2022. arXiv admin note: substantial
  text overlap with arXiv:2104.00231
- **Journal**: None
- **Summary**: Current state-of-the-art weakly supervised object detection (WSOD) studies mainly follow a two-stage training strategy which integrates a fully supervised detector (FSD) with a pure WSOD model. There are two main problems hindering the performance of the two-phase WSOD approaches, i.e., insufficient learning problem and strict reliance between the FSD and the pseudo ground truth (PGT) generated by the WSOD model. This paper proposes pseudo ground truth refinement network (PGTRNet), a simple yet effective method without introducing any extra learnable parameters, to cope with these problems. PGTRNet utilizes multiple bounding boxes to establish the PGT, mitigating the insufficient learning problem. Besides, we propose a novel online PGT refinement approach to steadily improve the quality of PGT by fully taking advantage of the power of FSD during the second-phase training, decoupling the first and second-phase models. Elaborate experiments are conducted on the PASCAL VOC 2007 benchmark to verify the effectiveness of our methods. Experimental results demonstrate that PGTRNet boosts the backbone model by 2.1% mAP and achieves the state-of-the-art performance.



### Design and Scaffolded Training of an Efficient DNN Operator for Computer Vision on the Edge
- **Arxiv ID**: http://arxiv.org/abs/2108.11441v1
- **DOI**: None
- **Categories**: **cs.AR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.11441v1)
- **Published**: 2021-08-25 19:22:25+00:00
- **Updated**: 2021-08-25 19:22:25+00:00
- **Authors**: Vinod Ganesan, Pratyush Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Massively parallel systolic arrays and resource-efficient depthwise separable convolutions are two promising techniques to accelerate DNN inference on the edge. Interestingly, their combination is inefficient: Computational patterns of depthwise separable convolutions do not exhibit a rhythmic systolic flow and lack sufficient data reuse to saturate systolic arrays. We formally analyse this inefficiency and propose an efficient operator, an optimal hardware dataflow, and a superior training methodology towards alleviating this. The efficient operator, called FuSeConv, is a drop-in replacement for depthwise separable convolutions. FuSeConv factorizes convolution fully along their spatial and depth dimensions. The resultant computation efficiently maps to systolic arrays. The optimal dataflow, called Spatial-Tiled Output Stationary (ST-OS), maximizes the efficiency of FuSeConv on systolic arrays. It maps independent convolutions to rows of the array to maximize resource utilization with negligible VLSI overheads. Neural Operator Scaffolding (NOS) scaffolds the training of FuSeConv by distilling knowledge from the expensive depthwise separable convolutions. This bridges the accuracy gap between FuSeConv networks and baselines. Additionally, NOS can be combined with Neural Architecture Search (NAS) to trade-off latency and accuracy. The HW/SW co-design of FuSeConv with ST-OS achieves a significant speedup of 4.1-9.25X with state-of-the-art efficient networks for ImageNet. The parameter efficiency of FuSeConv and its significant out-performance over depthwise separable convolutions on systolic arrays illustrates their promise as a strong solution on the edge. Training FuSeConv networks with NOS achieves accuracy comparable to the baselines. Further, by combining NOS with NAS, we design networks that define state-of-the-art models improving on both accuracy and latency on systolic arrays.



### A Riemannian Framework for Analysis of Human Body Surface
- **Arxiv ID**: http://arxiv.org/abs/2108.11449v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11449v2)
- **Published**: 2021-08-25 19:46:14+00:00
- **Updated**: 2021-10-23 19:02:01+00:00
- **Authors**: Emery Pierson, Mohamed Daoudi, Alice-Barbara Tumpach
- **Comment**: IEEE Workshop on Applications of Computer Vision (WACV) 2022,
  accepted
- **Journal**: None
- **Summary**: We propose a novel framework for comparing 3D human shapes under the change of shape and pose. This problem is challenging since 3D human shapes vary significantly across subjects and body postures. We solve this problem by using a Riemannian approach. Our core contribution is the mapping of the human body surface to the space of metrics and normals. We equip this space with a family of Riemannian metrics, called Ebin (or DeWitt) metrics. We treat a human body surface as a point in a "shape space" equipped with a family of Riemannian metrics. The family of metrics is invariant under rigid motions and reparametrizations; hence it induces a metric on the "shape space" of surfaces. Using the alignment of human bodies with a given template, we show that this family of metrics allows us to distinguish the changes in shape and pose. The proposed framework has several advantages. First, we define a family of metrics with desired invariance properties for the comparison of human shape. Second, we present an efficient framework to compute geodesic paths between human shape given the chosen metric. Third, this framework provides some basic tools for statistical shape analysis of human body surfaces. Finally, we demonstrate the utility of the proposed framework in pose and shape retrieval of human body.



### Reducing Label Effort: Self-Supervised meets Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.11458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11458v1)
- **Published**: 2021-08-25 20:04:44+00:00
- **Updated**: 2021-08-25 20:04:44+00:00
- **Authors**: Javad Zolfaghari Bengar, Joost van de Weijer, Bartlomiej Twardowski, Bogdan Raducanu
- **Comment**: Accepted at ICCVW 2021 (Interactive Labeling and Data Augmentation
  for Vision)
- **Journal**: None
- **Summary**: Active learning is a paradigm aimed at reducing the annotation effort by training the model on actively selected informative and/or representative samples. Another paradigm to reduce the annotation effort is self-training that learns from a large amount of unlabeled data in an unsupervised way and fine-tunes on few labeled samples. Recent developments in self-training have achieved very impressive results rivaling supervised learning on some datasets. The current work focuses on whether the two paradigms can benefit from each other. We studied object recognition datasets including CIFAR10, CIFAR100 and Tiny ImageNet with several labeling budgets for the evaluations. Our experiments reveal that self-training is remarkably more efficient than active learning at reducing the labeling effort, that for a low labeling budget, active learning offers no benefit to self-training, and finally that the combination of active learning and self-training is fruitful when the labeling budget is high. The performance gap between active learning trained either with self-training or from scratch diminishes as we approach to the point where almost half of the dataset is labeled.



### Lifelong Infinite Mixture Model Based on Knowledge-Driven Dirichlet Process
- **Arxiv ID**: http://arxiv.org/abs/2108.12278v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.12278v1)
- **Published**: 2021-08-25 21:06:20+00:00
- **Updated**: 2021-08-25 21:06:20+00:00
- **Authors**: Fei Ye, Adrian G. Bors
- **Comment**: Accepted by International Conference on Computer Vision (ICCV 2021)
- **Journal**: None
- **Summary**: Recent research efforts in lifelong learning propose to grow a mixture of models to adapt to an increasing number of tasks. The proposed methodology shows promising results in overcoming catastrophic forgetting. However, the theory behind these successful models is still not well understood. In this paper, we perform the theoretical analysis for lifelong learning models by deriving the risk bounds based on the discrepancy distance between the probabilistic representation of data generated by the model and that corresponding to the target dataset. Inspired by the theoretical analysis, we introduce a new lifelong learning approach, namely the Lifelong Infinite Mixture (LIMix) model, which can automatically expand its network architectures or choose an appropriate component to adapt its parameters for learning a new task, while preserving its previously learnt information. We propose to incorporate the knowledge by means of Dirichlet processes by using a gating mechanism which computes the dependence between the knowledge learnt previously and stored in each component, and a new set of data. Besides, we train a compact Student model which can accumulate cross-domain representations over time and make quick inferences. The code is available at https://github.com/dtuzi123/Lifelong-infinite-mixture-model.



### Improving Object Detection and Attribute Recognition by Feature Entanglement Reduction
- **Arxiv ID**: http://arxiv.org/abs/2108.11501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11501v1)
- **Published**: 2021-08-25 22:27:06+00:00
- **Updated**: 2021-08-25 22:27:06+00:00
- **Authors**: Zhaoheng Zheng, Arka Sadhu, Ram Nevatia
- **Comment**: Camera-ready for ICIP 2021
- **Journal**: None
- **Summary**: We explore object detection with two attributes: color and material. The task aims to simultaneously detect objects and infer their color and material. A straight-forward approach is to add attribute heads at the very end of a usual object detection pipeline. However, we observe that the two goals are in conflict: Object detection should be attribute-independent and attributes be largely object-independent. Features computed by a standard detection network entangle the category and attribute features; we disentangle them by the use of a two-stream model where the category and attribute features are computed independently but the classification heads share Regions of Interest (RoIs). Compared with a traditional single-stream model, our model shows significant improvements over VG-20, a subset of Visual Genome, on both supervised and attribute transfer tasks.



### Maneuver Identification Challenge
- **Arxiv ID**: http://arxiv.org/abs/2108.11503v1
- **DOI**: 10.1109/HPEC49654.2021.9622788
- **Categories**: **cs.AI**, cs.CV, cs.DC, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/2108.11503v1)
- **Published**: 2021-08-25 22:41:45+00:00
- **Updated**: 2021-08-25 22:41:45+00:00
- **Authors**: Kaira Samuel, Vijay Gadepally, David Jacobs, Michael Jones, Kyle McAlpin, Kyle Palko, Ben Paulk, Sid Samsi, Ho Chit Siu, Charles Yee, Jeremy Kepner
- **Comment**: 7 pages, 8 figures, 1 table, 33 references, accepted to IEEE HPEC
  2021
- **Journal**: None
- **Summary**: AI algorithms that identify maneuvers from trajectory data could play an important role in improving flight safety and pilot training. AI challenges allow diverse teams to work together to solve hard problems and are an effective tool for developing AI solutions. AI challenges are also a key driver of AI computational requirements. The Maneuver Identification Challenge hosted at maneuver-id.mit.edu provides thousands of trajectories collected from pilots practicing in flight simulators, descriptions of maneuvers, and examples of these maneuvers performed by experienced pilots. Each trajectory consists of positions, velocities, and aircraft orientations normalized to a common coordinate system. Construction of the data set required significant data architecture to transform flight simulator logs into AI ready data, which included using a supercomputer for deduplication and data conditioning. There are three proposed challenges. The first challenge is separating physically plausible (good) trajectories from unfeasible (bad) trajectories. Human labeled good and bad trajectories are provided to aid in this task. Subsequent challenges are to label trajectories with their intended maneuvers and to assess the quality of those maneuvers.



### Generalized Real-World Super-Resolution through Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2108.11505v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.11505v1)
- **Published**: 2021-08-25 22:43:20+00:00
- **Updated**: 2021-08-25 22:43:20+00:00
- **Authors**: Angela Castillo, María Escobar, Juan C. Pérez, Andrés Romero, Radu Timofte, Luc Van Gool, Pablo Arbeláez
- **Comment**: ICCV Workshops, 2021
- **Journal**: None
- **Summary**: Real-world Super-Resolution (SR) has been traditionally tackled by first learning a specific degradation model that resembles the noise and corruption artifacts in low-resolution imagery. Thus, current methods lack generalization and lose their accuracy when tested on unseen types of corruption. In contrast to the traditional proposal, we present Robust Super-Resolution (RSR), a method that leverages the generalization capability of adversarial attacks to tackle real-world SR. Our novel framework poses a paradigm shift in the development of real-world SR methods. Instead of learning a dataset-specific degradation, we employ adversarial attacks to create difficult examples that target the model's weaknesses. Afterward, we use these adversarial examples during training to improve our model's capacity to process noisy inputs. We perform extensive experimentation on synthetic and real-world images and empirically demonstrate that our RSR method generalizes well across datasets without re-training for specific noise priors. By using a single robust model, we outperform state-of-the-art specialized methods on real-world benchmarks.



### Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/2108.11510v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.11510v1)
- **Published**: 2021-08-25 23:01:48+00:00
- **Updated**: 2021-08-25 23:01:48+00:00
- **Authors**: Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki, Khoa Luu, Marios Savvides
- **Comment**: None
- **Journal**: None
- **Summary**: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision



### Robust High-Resolution Video Matting with Temporal Guidance
- **Arxiv ID**: http://arxiv.org/abs/2108.11515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.11515v1)
- **Published**: 2021-08-25 23:48:15+00:00
- **Updated**: 2021-08-25 23:48:15+00:00
- **Authors**: Shanchuan Lin, Linjie Yang, Imran Saleemi, Soumyadip Sengupta
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a robust, real-time, high-resolution human video matting method that achieves new state-of-the-art performance. Our method is much lighter than previous approaches and can process 4K at 76 FPS and HD at 104 FPS on an Nvidia GTX 1080Ti GPU. Unlike most existing methods that perform video matting frame-by-frame as independent images, our method uses a recurrent architecture to exploit temporal information in videos and achieves significant improvements in temporal coherence and matting quality. Furthermore, we propose a novel training strategy that enforces our network on both matting and segmentation objectives. This significantly improves our model's robustness. Our method does not require any auxiliary inputs such as a trimap or a pre-captured background image, so it can be widely applied to existing human matting applications.



