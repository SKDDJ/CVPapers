# Arxiv Papers in cs.CV on 2021-08-08
### Disentangled High Quality Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.03551v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03551v2)
- **Published**: 2021-08-08 02:14:15+00:00
- **Updated**: 2021-09-02 01:17:21+00:00
- **Authors**: Lv Tang, Bo Li, Shouhong Ding, Mofei Song
- **Comment**: Full version of ICCV2021
- **Journal**: None
- **Summary**: Aiming at discovering and locating most distinctive objects from visual scenes, salient object detection (SOD) plays an essential role in various computer vision systems. Coming to the era of high resolution, SOD methods are facing new challenges. The major limitation of previous methods is that they try to identify the salient regions and estimate the accurate objects boundaries simultaneously with a single regression task at low-resolution. This practice ignores the inherent difference between the two difficult problems, resulting in poor detection quality. In this paper, we propose a novel deep learning framework for high-resolution SOD task, which disentangles the task into a low-resolution saliency classification network (LRSCN) and a high-resolution refinement network (HRRN). As a pixel-wise classification task, LRSCN is designed to capture sufficient semantics at low-resolution to identify the definite salient, background and uncertain image regions. HRRN is a regression task, which aims at accurately refining the saliency value of pixels in the uncertain region to preserve a clear object boundary at high-resolution with limited GPU memory. It is worth noting that by introducing uncertainty into the training process, our HRRN can well address the high-resolution refinement task without using any high-resolution training data. Extensive experiments on high-resolution saliency datasets as well as some widely used saliency benchmarks show that the proposed method achieves superior performance compared to the state-of-the-art methods.



### Self-Adversarial Disentangling for Specific Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2108.03553v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03553v4)
- **Published**: 2021-08-08 02:36:45+00:00
- **Updated**: 2022-12-06 02:31:55+00:00
- **Authors**: Qianyu Zhou, Qiqi Gu, Jiangmiao Pang, Xuequan Lu, Lizhuang Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation aims to bridge the domain shifts between the source and the target domain. These shifts may span different dimensions such as fog, rainfall, etc. However, recent methods typically do not consider explicit prior knowledge about the domain shifts on a specific dimension, thus leading to less desired adaptation performance. In this paper, we study a practical setting called Specific Domain Adaptation (SDA) that aligns the source and target domains in a demanded-specific dimension. Within this setting, we observe the intra-domain gap induced by different domainness (i.e., numerical magnitudes of domain shifts in this dimension) is crucial when adapting to a specific domain. To address the problem, we propose a novel Self-Adversarial Disentangling (SAD) framework. In particular, given a specific dimension, we first enrich the source domain by introducing a domainness creator with providing additional supervisory signals. Guided by the created domainness, we design a self-adversarial regularizer and two loss functions to jointly disentangle the latent representations into domainness-specific and domainness-invariant features, thus mitigating the intra-domain gap. Our method can be easily taken as a plug-and-play framework and does not introduce any extra costs in the inference time. We achieve consistent improvements over state-of-the-art methods in both object detection and semantic segmentation.



### Rapid Automated Analysis of Skull Base Tumor Specimens Using Intraoperative Optical Imaging and Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2108.03555v2
- **DOI**: 10.1227/neu.0000000000001929
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.03555v2)
- **Published**: 2021-08-08 02:49:29+00:00
- **Updated**: 2022-06-19 06:10:55+00:00
- **Authors**: Cheng Jiang, Abhishek Bhattacharya, Joseph Linzey, Rushikesh S. Joshi, Sung Jik Cha, Sudharsan Srinivasan, Daniel Alber, Akhil Kondepudi, Esteban Urias, Balaji Pandian, Wajd Al-Holou, Steve Sullivan, B. Gregory Thompson, Jason Heth, Chris Freudiger, Siri Khalsa, Donato Pacione, John G. Golfinos, Sandra Camelo-Piragua, Daniel A. Orringer, Honglak Lee, Todd Hollon
- **Comment**: Published as journal article
- **Journal**: Neurosurgery 90 (6), 758-767, 2022
- **Summary**: Background: Accurate diagnosis of skull base tumors is essential for providing personalized surgical treatment strategies. Intraoperative diagnosis can be challenging due to tumor diversity and lack of intraoperative pathology resources.   Objective: To develop an independent and parallel intraoperative pathology workflow that can provide rapid and accurate skull base tumor diagnoses using label-free optical imaging and artificial intelligence.   Method: We used a fiber laser-based, label-free, non-consumptive, high-resolution microscopy method ($<$ 60 sec per 1 $\times$ 1 mm$^\text{2}$), called stimulated Raman histology (SRH), to image a consecutive, multicenter cohort of skull base tumor patients. SRH images were then used to train a convolutional neural network (CNN) model using three representation learning strategies: cross-entropy, self-supervised contrastive learning, and supervised contrastive learning. Our trained CNN models were tested on a held-out, multicenter SRH dataset.   Results: SRH was able to image the diagnostic features of both benign and malignant skull base tumors. Of the three representation learning strategies, supervised contrastive learning most effectively learned the distinctive and diagnostic SRH image features for each of the skull base tumor types. In our multicenter testing set, cross-entropy achieved an overall diagnostic accuracy of 91.5%, self-supervised contrastive learning 83.9%, and supervised contrastive learning 96.6%. Our trained model was able to identify tumor-normal margins and detect regions of microscopic tumor infiltration in whole-slide SRH images.   Conclusion: SRH with trained artificial intelligence models can provide rapid and accurate intraoperative analysis of skull base tumor specimens to inform surgical decision-making.



### Context-Aware Mixup for Domain Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.03557v3
- **DOI**: 10.1109/TCSVT.2022.3206476
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03557v3)
- **Published**: 2021-08-08 03:00:22+00:00
- **Updated**: 2022-09-11 13:29:04+00:00
- **Authors**: Qianyu Zhou, Zhengyang Feng, Qiqi Gu, Jiangmiao Pang, Guangliang Cheng, Xuequan Lu, Jianping Shi, Lizhuang Ma
- **Comment**: Accepted to IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT)
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) aims to adapt a model of the labeled source domain to an unlabeled target domain. Existing UDA-based semantic segmentation approaches always reduce the domain shifts in pixel level, feature level, and output level. However, almost all of them largely neglect the contextual dependency, which is generally shared across different domains, leading to less-desired performance. In this paper, we propose a novel Context-Aware Mixup (CAMix) framework for domain adaptive semantic segmentation, which exploits this important clue of context-dependency as explicit prior knowledge in a fully end-to-end trainable manner for enhancing the adaptability toward the target domain. Firstly, we present a contextual mask generation strategy by leveraging the accumulated spatial distributions and prior contextual relationships. The generated contextual mask is critical in this work and will guide the context-aware domain mixup on three different levels. Besides, provided the context knowledge, we introduce a significance-reweighted consistency loss to penalize the inconsistency between the mixed student prediction and the mixed teacher prediction, which alleviates the negative transfer of the adaptation, e.g., early performance degradation. Extensive experiments and analysis demonstrate the effectiveness of our method against the state-of-the-art approaches on widely-used UDA benchmarks.



### LeafMask: Towards Greater Accuracy on Leaf Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.03568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03568v1)
- **Published**: 2021-08-08 04:57:18+00:00
- **Updated**: 2021-08-08 04:57:18+00:00
- **Authors**: Ruohao Guo, Liao Qu, Dantong Niu, Zhenbo Li, Jun Yue
- **Comment**: None
- **Journal**: ICCV 2021 workshop, CVPPA
- **Summary**: Leaf segmentation is the most direct and effective way for high-throughput plant phenotype data analysis and quantitative researches of complex traits. Currently, the primary goal of plant phenotyping is to raise the accuracy of the autonomous phenotypic measurement. In this work, we present the LeafMask neural network, a new end-to-end model to delineate each leaf region and count the number of leaves, with two main components: 1) the mask assembly module merging position-sensitive bases of each predicted box after non-maximum suppression (NMS) and corresponding coefficients to generate original masks; 2) the mask refining module elaborating leaf boundaries from the mask assembly module by the point selection strategy and predictor. In addition, we also design a novel and flexible multi-scale attention module for the dual attention-guided mask (DAG-Mask) branch to effectively enhance information expression and produce more accurate bases. Our main contribution is to generate the final improved masks by combining the mask assembly module with the mask refining module under the anchor-free instance segmentation paradigm. We validate our LeafMask through extensive experiments on Leaf Segmentation Challenge (LSC) dataset. Our proposed model achieves the 90.09% BestDice score outperforming other state-of-the-art approaches.



### Expressive Power and Loss Surfaces of Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2108.03579v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2108.03579v3)
- **Published**: 2021-08-08 06:28:09+00:00
- **Updated**: 2021-11-20 22:04:53+00:00
- **Authors**: Simant Dube
- **Comment**: 27 Pages, Color Illustrations, Excerpt of and based on an AI book by
  the author
- **Journal**: None
- **Summary**: The goals of this paper are two-fold. The first goal is to serve as an expository tutorial on the working of deep learning models which emphasizes geometrical intuition about the reasons for success of deep learning. The second goal is to complement the current results on the expressive power of deep learning models and their loss surfaces with novel insights and results. In particular, we describe how deep neural networks carve out manifolds especially when the multiplication neurons are introduced. Multiplication is used in dot products and the attention mechanism and it is employed in capsule networks and self-attention based transformers. We also describe how random polynomial, random matrix, spin glass and computational complexity perspectives on the loss surfaces are interconnected.



### Visible Watermark Removal via Self-calibrated Localization and Background Refinement
- **Arxiv ID**: http://arxiv.org/abs/2108.03581v1
- **DOI**: 10.1145/3474085.3475592
- **Categories**: **cs.CV**, eess.IV, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2108.03581v1)
- **Published**: 2021-08-08 06:43:55+00:00
- **Updated**: 2021-08-08 06:43:55+00:00
- **Authors**: Jing Liang, Li Niu, Fengjun Guo, Teng Long, Liqing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Superimposing visible watermarks on images provides a powerful weapon to cope with the copyright issue. Watermark removal techniques, which can strengthen the robustness of visible watermarks in an adversarial way, have attracted increasing research interest. Modern watermark removal methods perform watermark localization and background restoration simultaneously, which could be viewed as a multi-task learning problem. However, existing approaches suffer from incomplete detected watermark and degraded texture quality of restored background. Therefore, we design a two-stage multi-task network to address the above issues. The coarse stage consists of a watermark branch and a background branch, in which the watermark branch self-calibrates the roughly estimated mask and passes the calibrated mask to background branch to reconstruct the watermarked area. In the refinement stage, we integrate multi-level features to improve the texture quality of watermarked area. Extensive experiments on two datasets demonstrate the effectiveness of our proposed method.



### Deep Transfer Learning for Identifications of Slope Surface Cracks
- **Arxiv ID**: http://arxiv.org/abs/2108.04235v1
- **DOI**: 10.3390/app112311193
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04235v1)
- **Published**: 2021-08-08 06:45:54+00:00
- **Updated**: 2021-08-08 06:45:54+00:00
- **Authors**: Yuting Yang, Gang Mei
- **Comment**: None
- **Journal**: Appl. Sci. 2021, 11(23), 11193
- **Summary**: Geohazards such as landslides have caused great losses to the safety of people's lives and property, which is often accompanied with surface cracks. If such surface cracks could be identified in time, it is of great significance for the monitoring and early warning of geohazards. Currently, the most common method for crack identification is manual detection, which is with low efficiency and accuracy. In this paper, a deep transfer learning framework is proposed to effectively and efficiently identify slope surface cracks for the sake of fast monitoring and early warning of geohazards such as landslides. The essential idea is to employ transfer learning by training (a) the large sample dataset of concrete cracks and (b) the small sample dataset of soil and rock masses cracks. In the proposed framework, (1) pretrained cracks identification models are constructed based on the large sample dataset of concrete cracks; (2) refined cracks identification models are further constructed based on the small sample dataset of soil and rock masses cracks. The proposed framework could be applied to conduct UAV surveys on high-steep slopes to realize the monitoring and early warning of landslides to ensure the safety of people's lives and property.



### An optical biomimetic eyes with interested object imaging
- **Arxiv ID**: http://arxiv.org/abs/2108.04236v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2108.04236v1)
- **Published**: 2021-08-08 08:11:14+00:00
- **Updated**: 2021-08-08 08:11:14+00:00
- **Authors**: Jun Li, Shimei Chen, Shangyuan Wang, Miao Lei, Xiaofang Dai, Chuangxue Liang, Kunyuan Xu, Shuxin Lin, Yuhui Li, Yuer Fan, Ting Zhong
- **Comment**: 19pages,7 figures,3 tables
- **Journal**: None
- **Summary**: We presented an optical system to perform imaging interested objects in complex scenes, like the creature easy see the interested prey in the hunt for complex environments. It utilized Deep-learning network to learn the interested objects's vision features and designed the corresponding "imaging matrices", furthermore the learned matrixes act as the measurement matrix to complete compressive imaging with a single-pixel camera, finally we can using the compressed image data to only image the interested objects without the rest objects and backgrounds of the scenes with the previous Deep-learning network. Our results demonstrate that no matter interested object is single feature or rich details, the interference can be successfully filtered out and this idea can be applied in some common applications that effectively improve the performance. This bio-inspired optical system can act as the creature eye to achieve success on interested-based object imaging, object detection, object recognition and object tracking, etc.



### ZiGAN: Fine-grained Chinese Calligraphy Font Generation via a Few-shot Style Transfer Approach
- **Arxiv ID**: http://arxiv.org/abs/2108.03596v1
- **DOI**: 10.1145/3474085.3475225
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03596v1)
- **Published**: 2021-08-08 09:50:20+00:00
- **Updated**: 2021-08-08 09:50:20+00:00
- **Authors**: Qi Wen, Shuang Li, Bingfeng Han, Yi Yuan
- **Comment**: Accepted at ACM MM 2021
- **Journal**: None
- **Summary**: Chinese character style transfer is a very challenging problem because of the complexity of the glyph shapes or underlying structures and large numbers of existed characters, when comparing with English letters. Moreover, the handwriting of calligraphy masters has a more irregular stroke and is difficult to obtain in real-world scenarios. Recently, several GAN-based methods have been proposed for font synthesis, but some of them require numerous reference data and the other part of them have cumbersome preprocessing steps to divide the character into different parts to be learned and transferred separately. In this paper, we propose a simple but powerful end-to-end Chinese calligraphy font generation framework ZiGAN, which does not require any manual operation or redundant preprocessing to generate fine-grained target-style characters with few-shot references. To be specific, a few paired samples from different character styles are leveraged to attain a fine-grained correlation between structures underlying different glyphs. To capture valuable style knowledge in target and strengthen the coarse-grained understanding of character content, we utilize multiple unpaired samples to align the feature distributions belonging to different character styles. By doing so, only a few target Chinese calligraphy characters are needed to generated expected style transferred characters. Experiments demonstrate that our method has a state-of-the-art generalization ability in few-shot Chinese character style transfer.



### Understanding the computational demands underlying visual reasoning
- **Arxiv ID**: http://arxiv.org/abs/2108.03603v2
- **DOI**: 10.1162/neco_a_01485
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.03603v2)
- **Published**: 2021-08-08 10:46:53+00:00
- **Updated**: 2021-12-09 04:57:02+00:00
- **Authors**: Mohit Vaishnav, Remi Cadene, Andrea Alamia, Drew Linsley, Rufin VanRullen, Thomas Serre
- **Comment**: 26 pages, 16 figures
- **Journal**: Neural Computation, 2022
- **Summary**: Visual understanding requires comprehending complex visual relations between objects within a scene. Here, we seek to characterize the computational demands for abstract visual reasoning. We do this by systematically assessing the ability of modern deep convolutional neural networks (CNNs) to learn to solve the "Synthetic Visual Reasoning Test" (SVRT) challenge, a collection of twenty-three visual reasoning problems. Our analysis reveals a novel taxonomy of visual reasoning tasks, which can be primarily explained by both the type of relations (same-different vs. spatial-relation judgments) and the number of relations used to compose the underlying rules. Prior cognitive neuroscience work suggests that attention plays a key role in humans' visual reasoning ability. To test this hypothesis, we extended the CNNs with spatial and feature-based attention mechanisms. In a second series of experiments, we evaluated the ability of these attention networks to learn to solve the SVRT challenge and found the resulting architectures to be much more efficient at solving the hardest of these visual reasoning tasks. Most importantly, the corresponding improvements on individual tasks partially explained our novel taxonomy. Overall, this work provides an granular computational account of visual reasoning and yields testable neuroscience predictions regarding the differential need for feature-based vs. spatial attention depending on the type of visual reasoning problem.



### Triplet Contrastive Learning for Brain Tumor Classification
- **Arxiv ID**: http://arxiv.org/abs/2108.03611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03611v1)
- **Published**: 2021-08-08 11:26:34+00:00
- **Updated**: 2021-08-08 11:26:34+00:00
- **Authors**: Tian Yu Liu, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Brain tumor is a common and fatal form of cancer which affects both adults and children. The classification of brain tumors into different types is hence a crucial task, as it greatly influences the treatment that physicians will prescribe. In light of this, medical imaging techniques, especially those applying deep convolutional networks followed by a classification layer, have been developed to make possible computer-aided classification of brain tumor types. In this paper, we present a novel approach of directly learning deep embeddings for brain tumor types, which can be used for downstream tasks such as classification. Along with using triplet loss variants, our approach applies contrastive learning to performing unsupervised pre-training, combined with a rare-case data augmentation module to effectively ameliorate the lack of data problem in the brain tumor imaging analysis domain. We evaluate our method on an extensive brain tumor dataset which consists of 27 different tumor classes, out of which 13 are defined as rare. With a common encoder during all the experiments, we compare our approach with a baseline classification-layer based model, and the results well prove the effectiveness of our approach across all measured metrics.



### Towards real-world navigation with deep differentiable planners
- **Arxiv ID**: http://arxiv.org/abs/2108.05713v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.05713v2)
- **Published**: 2021-08-08 11:29:16+00:00
- **Updated**: 2022-06-02 15:22:32+00:00
- **Authors**: Shu Ishida, João F. Henriques
- **Comment**: Published in CVPR 2022 (Conference on Computer Vision and Pattern
  Recognition)
- **Journal**: None
- **Summary**: We train embodied neural networks to plan and navigate unseen complex 3D environments, emphasising real-world deployment. Rather than requiring prior knowledge of the agent or environment, the planner learns to model the state transitions and rewards. To avoid the potentially hazardous trial-and-error of reinforcement learning, we focus on differentiable planners such as Value Iteration Networks (VIN), which are trained offline from safe expert demonstrations. Although they work well in small simulations, we address two major limitations that hinder their deployment. First, we observed that current differentiable planners struggle to plan long-term in environments with a high branching complexity. While they should ideally learn to assign low rewards to obstacles to avoid collisions, we posit that the constraints imposed on the network are not strong enough to guarantee the network to learn sufficiently large penalties for every possible collision. We thus impose a structural constraint on the value iteration, which explicitly learns to model any impossible actions. Secondly, we extend the model to work with a limited perspective camera under translation and rotation, which is crucial for real robot deployment. Many VIN-like planners assume a 360 degrees or overhead view without rotation. In contrast, our method uses a memory-efficient lattice map to aggregate CNN embeddings of partial observations, and models the rotational dynamics explicitly using a 3D state-space grid (translation and rotation). Our proposals significantly improve semantic navigation and exploration on several 2D and 3D environments, succeeding in settings that are otherwise challenging for this class of methods. As far as we know, we are the first to successfully perform differentiable planning on the difficult Active Vision Dataset, consisting of real images captured from a robot.



### An EM Framework for Online Incremental Learning of Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.03613v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.03613v1)
- **Published**: 2021-08-08 11:30:09+00:00
- **Updated**: 2021-08-08 11:30:09+00:00
- **Authors**: Shipeng Yan, Jiale Zhou, Jiangwei Xie, Songyang Zhang, Xuming He
- **Comment**: Accepted by ACM MM'21
- **Journal**: None
- **Summary**: Incremental learning of semantic segmentation has emerged as a promising strategy for visual scene interpretation in the open- world setting. However, it remains challenging to acquire novel classes in an online fashion for the segmentation task, mainly due to its continuously-evolving semantic label space, partial pixelwise ground-truth annotations, and constrained data availability. To ad- dress this, we propose an incremental learning strategy that can fast adapt deep segmentation models without catastrophic forgetting, using a streaming input data with pixel annotations on the novel classes only. To this end, we develop a uni ed learning strategy based on the Expectation-Maximization (EM) framework, which integrates an iterative relabeling strategy that lls in the missing labels and a rehearsal-based incremental learning step that balances the stability-plasticity of the model. Moreover, our EM algorithm adopts an adaptive sampling method to select informative train- ing data and a class-balancing training strategy in the incremental model updates, both improving the e cacy of model learning. We validate our approach on the PASCAL VOC 2012 and ADE20K datasets, and the results demonstrate its superior performance over the existing incremental methods.



### Monte Carlo DropBlock for Modelling Uncertainty in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.03614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03614v1)
- **Published**: 2021-08-08 11:34:37+00:00
- **Updated**: 2021-08-08 11:34:37+00:00
- **Authors**: Kumari Deepshikha, Sai Harsha Yelleni, P. K. Srijith, C Krishna Mohan
- **Comment**: None
- **Journal**: None
- **Summary**: With the advancements made in deep learning, computer vision problems like object detection and segmentation have seen a great improvement in performance. However, in many real-world applications such as autonomous driving vehicles, the risk associated with incorrect predictions of objects is very high. Standard deep learning models for object detection such as YOLO models are often overconfident in their predictions and do not take into account the uncertainty in predictions on out-of-distribution data. In this work, we propose an efficient and effective approach to model uncertainty in object detection and segmentation tasks using Monte-Carlo DropBlock (MC-DropBlock) based inference. The proposed approach applies drop-block during training time and test time on the convolutional layer of the deep learning models such as YOLO. We show that this leads to a Bayesian convolutional neural network capable of capturing the epistemic uncertainty in the model. Additionally, we capture the aleatoric uncertainty using a Gaussian likelihood. We demonstrate the effectiveness of the proposed approach on modeling uncertainty in object detection and segmentation tasks using out-of-distribution experiments. Experimental results show that MC-DropBlock improves the generalization, calibration, and uncertainty modeling capabilities of YOLO models in object detection and segmentation.



### MPI: Multi-receptive and Parallel Integration for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.03618v1
- **DOI**: 10.1049/ipr2.12324
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03618v1)
- **Published**: 2021-08-08 12:01:44+00:00
- **Updated**: 2021-08-08 12:01:44+00:00
- **Authors**: Han Sun, Jun Cen, Ningzhong Liu, Dong Liang, Huiyu Zhou
- **Comment**: 10 pages, 10 figures, accepted by IET Image Processing, code:
  https://github.com/NuaaCJ/MPI
- **Journal**: IET Image Processing, 2021
- **Summary**: The semantic representation of deep features is essential for image context understanding, and effective fusion of features with different semantic representations can significantly improve the model's performance on salient object detection. In this paper, a novel method called MPI is proposed for salient object detection. Firstly, a multi-receptive enhancement module (MRE) is designed to effectively expand the receptive fields of features from different layers and generate features with different receptive fields. MRE can enhance the semantic representation and improve the model's perception of the image context, which enables the model to locate the salient object accurately. Secondly, in order to reduce the reuse of redundant information in the complex top-down fusion method and weaken the differences between semantic features, a relatively simple but effective parallel fusion strategy (PFS) is proposed. It allows multi-scale features to better interact with each other, thus improving the overall performance of the model. Experimental results on multiple datasets demonstrate that the proposed method outperforms state-of-the-art methods under different evaluation metrics.



### Learning an Augmented RGB Representation with Cross-Modal Knowledge Distillation for Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.03619v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03619v1)
- **Published**: 2021-08-08 12:04:14+00:00
- **Updated**: 2021-08-08 12:04:14+00:00
- **Authors**: Rui Dai, Srijan Das, Francois Bremond
- **Comment**: None
- **Journal**: None
- **Summary**: In video understanding, most cross-modal knowledge distillation (KD) methods are tailored for classification tasks, focusing on the discriminative representation of the trimmed videos. However, action detection requires not only categorizing actions, but also localizing them in untrimmed videos. Therefore, transferring knowledge pertaining to temporal relations is critical for this task which is missing in the previous cross-modal KD frameworks. To this end, we aim at learning an augmented RGB representation for action detection, taking advantage of additional modalities at training time through KD. We propose a KD framework consisting of two levels of distillation. On one hand, atomic-level distillation encourages the RGB student to learn the sub-representation of the actions from the teacher in a contrastive manner. On the other hand, sequence-level distillation encourages the student to learn the temporal knowledge from the teacher, which consists of transferring the Global Contextual Relations and the Action Boundary Saliency. The result is an Augmented-RGB stream that can achieve competitive performance as the two-stream network while using only RGB at inference time. Extensive experimental analysis shows that our proposed distillation framework is generic and outperforms other popular cross-modal distillation methods in action detection task.



### WideCaps: A Wide Attention based Capsule Network for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2108.03627v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T07, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2108.03627v2)
- **Published**: 2021-08-08 13:09:40+00:00
- **Updated**: 2021-08-14 01:02:49+00:00
- **Authors**: S J Pawan, Rishi Sharma, Hemanth Sai Ram Reddy, M Vani, Jeny Rajan
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: The capsule network is a distinct and promising segment of the neural network family that drew attention due to its unique ability to maintain the equivariance property by preserving the spatial relationship amongst the features. The capsule network has attained unprecedented success over image classification tasks with datasets such as MNIST and affNIST by encoding the characteristic features into the capsules and building the parse-tree structure. However, on the datasets involving complex foreground and background regions such as CIFAR-10, the performance of the capsule network is sub-optimal due to its naive data routing policy and incompetence towards extracting complex features. This paper proposes a new design strategy for capsule network architecture for efficiently dealing with complex images. The proposed method incorporates wide bottleneck residual modules and the Squeeze and Excitation attention blocks upheld by the modified FM routing algorithm to address the defined problem. A wide bottleneck residual module facilitates extracting complex features followed by the squeeze and excitation attention block to enable channel-wise attention by suppressing the trivial features. This setup allows channel inter-dependencies at almost no computational cost, thereby enhancing the representation ability of capsules on complex images. We extensively evaluate the performance of the proposed model on three publicly available datasets, namely CIFAR-10, Fashion MNIST, and SVHN, to outperform the top-5 performance on CIFAR-10 and Fashion MNIST with highly competitive performance on the SVHN dataset.



### Anchor-free 3D Single Stage Detector with Mask-Guided Attention for Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2108.03634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03634v1)
- **Published**: 2021-08-08 13:42:13+00:00
- **Updated**: 2021-08-08 13:42:13+00:00
- **Authors**: Jiale Li, Hang Dai, Ling Shao, Yong Ding
- **Comment**: This is a pre-print of our paper published in proceedings of the 29th
  ACM International Conference on Multimedia (MM'21)
- **Journal**: None
- **Summary**: Most of the existing single-stage and two-stage 3D object detectors are anchor-based methods, while the efficient but challenging anchor-free single-stage 3D object detection is not well investigated. Recent studies on 2D object detection show that the anchor-free methods also are of great potential. However, the unordered and sparse properties of point clouds prevent us from directly leveraging the advanced 2D methods on 3D point clouds. We overcome this by converting the voxel-based sparse 3D feature volumes into the sparse 2D feature maps. We propose an attentive module to fit the sparse feature maps to dense mostly on the object regions through the deformable convolution tower and the supervised mask-guided attention. By directly regressing the 3D bounding box from the enhanced and dense feature maps, we construct a novel single-stage 3D detector for point clouds in an anchor-free manner. We propose an IoU-based detection confidence re-calibration scheme to improve the correlation between the detection confidence score and the accuracy of the bounding box regression. Our code is publicly available at \url{https://github.com/jialeli1/MGAF-3DSSD}.



### Efficient Light Field Reconstruction via Spatio-Angular Dense Network
- **Arxiv ID**: http://arxiv.org/abs/2108.03635v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.03635v1)
- **Published**: 2021-08-08 13:50:51+00:00
- **Updated**: 2021-08-08 13:50:51+00:00
- **Authors**: Zexi Hu, Henry Wing Fung Yeung, Xiaoming Chen, Yuk Ying Chung, Haisheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: As an image sensing instrument, light field images can supply extra angular information compared with monocular images and have facilitated a wide range of measurement applications. Light field image capturing devices usually suffer from the inherent trade-off between the angular and spatial resolutions. To tackle this problem, several methods, such as light field reconstruction and light field super-resolution, have been proposed but leaving two problems unaddressed, namely domain asymmetry and efficient information flow. In this paper, we propose an end-to-end Spatio-Angular Dense Network (SADenseNet) for light field reconstruction with two novel components, namely correlation blocks and spatio-angular dense skip connections to address them. The former performs effective modeling of the correlation information in a way that conforms with the domain asymmetry. And the latter consists of three kinds of connections enhancing the information flow within two domains. Extensive experiments on both real-world and synthetic datasets have been conducted to demonstrate that the proposed SADenseNet's state-of-the-art performance at significantly reduced costs in memory and computation. The qualitative results show that the reconstructed light field images are sharp with correct details and can serve as pre-processing to improve the accuracy of related measurement applications.



### Saliency-Associated Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2108.03637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03637v1)
- **Published**: 2021-08-08 13:54:09+00:00
- **Updated**: 2021-08-08 13:54:09+00:00
- **Authors**: Zikun Zhou, Wenjie Pei, Xin Li, Hongpeng Wang, Feng Zheng, Zhenyu He
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Most existing trackers based on deep learning perform tracking in a holistic strategy, which aims to learn deep representations of the whole target for localizing the target. It is arduous for such methods to track targets with various appearance variations. To address this limitation, another type of methods adopts a part-based tracking strategy which divides the target into equal patches and tracks all these patches in parallel. The target state is inferred by summarizing the tracking results of these patches. A potential limitation of such trackers is that not all patches are equally informative for tracking. Some patches that are not discriminative may have adverse effects. In this paper, we propose to track the salient local parts of the target that are discriminative for tracking. In particular, we propose a fine-grained saliency mining module to capture the local saliencies. Further, we design a saliency-association modeling module to associate the captured saliencies together to learn effective correlation representations between the exemplar and the search image for state estimation. Extensive experiments on five diverse datasets demonstrate that the proposed method performs favorably against state-of-the-art trackers.



### Image reconstruction in light-sheet microscopy: spatially varying deconvolution and mixed noise
- **Arxiv ID**: http://arxiv.org/abs/2108.03642v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2108.03642v1)
- **Published**: 2021-08-08 14:14:35+00:00
- **Updated**: 2021-08-08 14:14:35+00:00
- **Authors**: Bogdan Toader, Jerome Boulanger, Yury Korolev, Martin O. Lenz, James Manton, Carola-Bibiane Schonlieb, Leila Muresan
- **Comment**: 34 pages, 13 figures
- **Journal**: None
- **Summary**: We study the problem of deconvolution for light-sheet microscopy, where the data is corrupted by spatially varying blur and a combination of Poisson and Gaussian noise. The spatial variation of the point spread function (PSF) of a light-sheet microscope is determined by the interaction between the excitation sheet and the detection objective PSF. First, we introduce a model of the image formation process that incorporates this interaction, therefore capturing the main characteristics of this imaging modality. Then, we formulate a variational model that accounts for the combination of Poisson and Gaussian noise through a data fidelity term consisting of the infimal convolution of the single noise fidelities, first introduced in L. Calatroni et al. "Infimal convolution of data discrepancies for mixed noise removal", SIAM Journal on Imaging Sciences 10.3 (2017), 1196-1233. We establish convergence rates in a Bregman distance under a source condition for the infimal convolution fidelity and a discrepancy principle for choosing the value of the regularisation parameter. The inverse problem is solved by applying the primal-dual hybrid gradient (PDHG) algorithm in a novel way. Finally, numerical experiments performed on both simulated and real data show superior reconstruction results in comparison with other methods.



### AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2108.03647v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03647v2)
- **Published**: 2021-08-08 14:26:25+00:00
- **Updated**: 2021-08-11 13:14:49+00:00
- **Authors**: Songhua Liu, Tianwei Lin, Dongliang He, Fu Li, Meiling Wang, Xin Li, Zhengxing Sun, Qian Li, Errui Ding
- **Comment**: Accepted by ICCV 2021. Codes will be released on
  https://github.com/wzmsltw/AdaAttN
- **Journal**: None
- **Summary**: Fast arbitrary neural style transfer has attracted widespread attention from academic, industrial and art communities due to its flexibility in enabling various applications. Existing solutions either attentively fuse deep style feature into deep content feature without considering feature distributions, or adaptively normalize deep content feature according to the style such that their global statistics are matched. Although effective, leaving shallow feature unexplored and without locally considering feature statistics, they are prone to unnatural output with unpleasing local distortions. To alleviate this problem, in this paper, we propose a novel attention and normalization module, named Adaptive Attention Normalization (AdaAttN), to adaptively perform attentive normalization on per-point basis. Specifically, spatial attention score is learnt from both shallow and deep features of content and style images. Then per-point weighted statistics are calculated by regarding a style feature point as a distribution of attention-weighted output of all style feature points. Finally, the content feature is normalized so that they demonstrate the same local feature statistics as the calculated per-point weighted style feature statistics. Besides, a novel local feature loss is derived based on AdaAttN to enhance local visual quality. We also extend AdaAttN to be ready for video style transfer with slight modifications. Experiments demonstrate that our method achieves state-of-the-art arbitrary image/video style transfer. Codes and models are available.



### From Voxel to Point: IoU-guided 3D Object Detection for Point Cloud with Voxel-to-Point Decoder
- **Arxiv ID**: http://arxiv.org/abs/2108.03648v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03648v1)
- **Published**: 2021-08-08 14:30:13+00:00
- **Updated**: 2021-08-08 14:30:13+00:00
- **Authors**: Jiale Li, Hang Dai, Ling Shao, Yong Ding
- **Comment**: This is a pre-print of our paper published in proceedings of the 29th
  ACM International Conference on Multimedia (MM'21)
- **Journal**: None
- **Summary**: In this paper, we present an Intersection-over-Union (IoU) guided two-stage 3D object detector with a voxel-to-point decoder. To preserve the necessary information from all raw points and maintain the high box recall in voxel based Region Proposal Network (RPN), we propose a residual voxel-to-point decoder to extract the point features in addition to the map-view features from the voxel based RPN. We use a 3D Region of Interest (RoI) alignment to crop and align the features with the proposal boxes for accurately perceiving the object position. The RoI-Aligned features are finally aggregated with the corner geometry embeddings that can provide the potentially missing corner information in the box refinement stage. We propose a simple and efficient method to align the estimated IoUs to the refined proposal boxes as a more relevant localization confidence. The comprehensive experiments on KITTI and Waymo Open Dataset demonstrate that our method achieves significant improvements with novel architectures against the existing methods. The code is available on Github URL\footnote{\url{https://github.com/jialeli1/From-Voxel-to-Point}}.



### Joint Depth and Normal Estimation from Real-world Time-of-flight Raw Data
- **Arxiv ID**: http://arxiv.org/abs/2108.03649v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.03649v1)
- **Published**: 2021-08-08 14:34:50+00:00
- **Updated**: 2021-08-08 14:34:50+00:00
- **Authors**: Rongrong Gao, Na Fan, Changlin Li, Wentao Liu, Qifeng Chen
- **Comment**: IROS 2021
- **Journal**: None
- **Summary**: We present a novel approach to joint depth and normal estimation for time-of-flight (ToF) sensors. Our model learns to predict the high-quality depth and normal maps jointly from ToF raw sensor data. To achieve this, we meticulously constructed the first large-scale dataset (named ToF-100) with paired raw ToF data and ground-truth high-resolution depth maps provided by an industrial depth camera. In addition, we also design a simple but effective framework for joint depth and normal estimation, applying a robust Chamfer loss via jittering to improve the performance of our model. Our experiments demonstrate that our proposed method can efficiently reconstruct high-resolution depth and normal maps and significantly outperforms state-of-the-art approaches. Our code and data will be available at \url{https://github.com/hkustVisionRr/JointlyDepthNormalEstimation}



### Skeleton-Contrastive 3D Action Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.03656v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03656v1)
- **Published**: 2021-08-08 14:44:59+00:00
- **Updated**: 2021-08-08 14:44:59+00:00
- **Authors**: Fida Mohammad Thoker, Hazel Doughty, Cees G. M. Snoek
- **Comment**: Accepted in ACM Multimedia 2021
- **Journal**: None
- **Summary**: This paper strives for self-supervised learning of a feature space suitable for skeleton-based action recognition. Our proposal is built upon learning invariances to input skeleton representations and various skeleton augmentations via a noise contrastive estimation. In particular, we propose inter-skeleton contrastive learning, which learns from multiple different input skeleton representations in a cross-contrastive manner. In addition, we contribute several skeleton-specific spatial and temporal augmentations which further encourage the model to learn the spatio-temporal dynamics of skeleton data. By learning similarities between different skeleton representations as well as augmented views of the same sequence, the network is encouraged to learn higher-level semantics of the skeleton data than when only using the augmented views. Our approach achieves state-of-the-art performance for self-supervised learning from skeleton data on the challenging PKU and NTU datasets with multiple downstream tasks, including action recognition, action retrieval and semi-supervised learning. Code is available at https://github.com/fmthoker/skeleton-contrast.



### One-Shot Object Affordance Detection in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2108.03658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03658v1)
- **Published**: 2021-08-08 14:53:10+00:00
- **Updated**: 2021-08-08 14:53:10+00:00
- **Authors**: Wei Zhai, Hongchen Luo, Jing Zhang, Yang Cao, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Affordance detection refers to identifying the potential action possibilities of objects in an image, which is a crucial ability for robot perception and manipulation. To empower robots with this ability in unseen scenarios, we first study the challenging one-shot affordance detection problem in this paper, i.e., given a support image that depicts the action purpose, all objects in a scene with the common affordance should be detected. To this end, we devise a One-Shot Affordance Detection Network (OSAD-Net) that firstly estimates the human action purpose and then transfers it to help detect the common affordance from all candidate images. Through collaboration learning, OSAD-Net can capture the common characteristics between objects having the same underlying affordance and learn a good adaptation capability for perceiving unseen affordances. Besides, we build a large-scale Purpose-driven Affordance Dataset v2 (PADv2) by collecting and labeling 30k images from 39 affordance and 103 object categories. With complex scenes and rich annotations, our PADv2 dataset can be used as a test bed to benchmark affordance detection methods and may also facilitate downstream vision tasks, such as scene understanding, action recognition, and robot manipulation. Specifically, we conducted comprehensive experiments on PADv2 dataset by including 11 advanced models from several related research fields. Experimental results demonstrate the superiority of our model over previous representative ones in terms of both objective metrics and visual quality. The benchmark suite is available at https://github.com/lhc1224/OSAD Net.



### Discriminative Latent Semantic Graph for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/2108.03662v2
- **DOI**: 10.1145/3474085.3475519
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03662v2)
- **Published**: 2021-08-08 15:11:20+00:00
- **Updated**: 2021-08-10 13:55:55+00:00
- **Authors**: Yang Bai, Junyan Wang, Yang Long, Bingzhang Hu, Yang Song, Maurice Pagnucco, Yu Guan
- **Comment**: accepted by ACM MM 2021
- **Journal**: None
- **Summary**: Video captioning aims to automatically generate natural language sentences that can describe the visual contents of a given video. Existing generative models like encoder-decoder frameworks cannot explicitly explore the object-level interactions and frame-level information from complex spatio-temporal data to generate semantic-rich captions. Our main contribution is to identify three key problems in a joint framework for future video summarization tasks. 1) Enhanced Object Proposal: we propose a novel Conditional Graph that can fuse spatio-temporal information into latent object proposal. 2) Visual Knowledge: Latent Proposal Aggregation is proposed to dynamically extract visual words with higher semantic levels. 3) Sentence Validation: A novel Discriminative Language Validator is proposed to verify generated captions so that key semantic concepts can be effectively preserved. Our experiments on two public datasets (MVSD and MSR-VTT) manifest significant improvements over state-of-the-art approaches on all metrics, especially for BLEU-4 and CIDEr. Our code is available at https://github.com/baiyang4/D-LSG-Video-Caption.



### TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation
- **Arxiv ID**: http://arxiv.org/abs/2108.04238v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2108.04238v2)
- **Published**: 2021-08-08 15:27:14+00:00
- **Updated**: 2021-08-26 02:16:50+00:00
- **Authors**: Cong Wang, Haocheng Han, Caleb Chen Cao
- **Comment**: Additional experiments are required
- **Journal**: None
- **Summary**: Explanation of AI, as well as fairness of algorithms' decisions and the transparency of the decision model, are becoming more and more important. And it is crucial to design effective and human-friendly techniques when opening the black-box model. Counterfactual conforms to the human way of thinking and provides a human-friendly explanation, and its corresponding explanation algorithm refers to a strategic alternation of a given data point so that its model output is "counter-facted", i.e. the prediction is reverted. In this paper, we adapt counterfactual explanation over fine-grained image classification problem. We demonstrated an adaptive method that could give a counterfactual explanation by showing the composed counterfactual feature map using top-down layer searching algorithm (TDLS). We have proved that our TDLS algorithm could provide more flexible counterfactual visual explanation in an efficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end, we discussed several applicable scenarios of counterfactual visual explanations.



### RECALL: Replay-based Continual Learning in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.03673v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03673v2)
- **Published**: 2021-08-08 15:57:00+00:00
- **Updated**: 2021-09-19 14:44:08+00:00
- **Authors**: Andrea Maracani, Umberto Michieli, Marco Toldo, Pietro Zanuttigh
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Deep networks allow to obtain outstanding results in semantic segmentation, however they need to be trained in a single shot with a large amount of data. Continual learning settings where new classes are learned in incremental steps and previous training data is no longer available are challenging due to the catastrophic forgetting phenomenon. Existing approaches typically fail when several incremental steps are performed or in presence of a distribution shift of the background class. We tackle these issues by recreating no longer available data for the old classes and outlining a content inpainting scheme on the background class. We propose two sources for replay data. The first resorts to a generative adversarial network to sample from the class space of past learning steps. The second relies on web-crawled data to retrieve images containing examples of old classes from online databases. In both scenarios no samples of past steps are stored, thus avoiding privacy concerns. Replay data are then blended with new samples during the incremental steps. Our approach, RECALL, outperforms state-of-the-art methods.



### AMDet: A Tool for Mitotic Cell Detection in Histopathology Slides
- **Arxiv ID**: http://arxiv.org/abs/2108.03676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03676v1)
- **Published**: 2021-08-08 16:06:28+00:00
- **Updated**: 2021-08-08 16:06:28+00:00
- **Authors**: Walt Williams, Jimmy Hall
- **Comment**: None
- **Journal**: None
- **Summary**: Breast Cancer is the most prevalent cancer in the world. The World Health Organization reports that the disease still affects a significant portion of the developing world citing increased mortality rates in the majority of low to middle income countries. The most popular protocol pathologists use for diagnosing breast cancer is the Nottingham grading system which grades the proliferation of tumors based on 3 major criteria, the most important of them being mitotic cell count. The way in which pathologists evaluate mitotic cell count is to subjectively and qualitatively analyze cells present in stained slides of tissue and make a decision on its mitotic state i.e. is it mitotic or not? This process is extremely inefficient and tiring for pathologists and so an efficient, accurate, and fully automated tool to aid with the diagnosis is extremely desirable. Fortunately, creating such a tool is made significantly easier with the AutoML tool available from Microsoft Azure, however to the best of our knowledge the AutoML tool has never been formally evaluated for use in mitotic cell detection in histopathology images. This paper serves as an evaluation of the AutoML tool for this purpose and will provide a first look on how the tool handles this challenging problem. All code is available athttps://github.com/WaltAFWilliams/AMDet



### Joint Inductive and Transductive Learning for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.03679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03679v1)
- **Published**: 2021-08-08 16:25:48+00:00
- **Updated**: 2021-08-08 16:25:48+00:00
- **Authors**: Yunyao Mao, Ning Wang, Wengang Zhou, Houqiang Li
- **Comment**: To appear in ICCV 2021
- **Journal**: None
- **Summary**: Semi-supervised video object segmentation is a task of segmenting the target object in a video sequence given only a mask annotation in the first frame. The limited information available makes it an extremely challenging task. Most previous best-performing methods adopt matching-based transductive reasoning or online inductive learning. Nevertheless, they are either less discriminative for similar instances or insufficient in the utilization of spatio-temporal information. In this work, we propose to integrate transductive and inductive learning into a unified framework to exploit the complementarity between them for accurate and robust video object segmentation. The proposed approach consists of two functional branches. The transduction branch adopts a lightweight transformer architecture to aggregate rich spatio-temporal cues while the induction branch performs online inductive learning to obtain discriminative target information. To bridge these two diverse branches, a two-head label encoder is introduced to learn the suitable target prior for each of them. The generated mask encodings are further forced to be disentangled to better retain their complementarity. Extensive experiments on several prevalent benchmarks show that, without the need of synthetic training data, the proposed approach sets a series of new state-of-the-art records. Code is available at https://github.com/maoyunyao/JOINT.



### Enhanced Invertible Encoding for Learned Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2108.03690v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.03690v1)
- **Published**: 2021-08-08 17:32:10+00:00
- **Updated**: 2021-08-08 17:32:10+00:00
- **Authors**: Yueqi Xie, Ka Leong Cheng, Qifeng Chen
- **Comment**: Accepted to ACM Multimedia 2021 as Oral
- **Journal**: None
- **Summary**: Although deep learning based image compression methods have achieved promising progress these days, the performance of these methods still cannot match the latest compression standard Versatile Video Coding (VVC). Most of the recent developments focus on designing a more accurate and flexible entropy model that can better parameterize the distributions of the latent features. However, few efforts are devoted to structuring a better transformation between the image space and the latent feature space. In this paper, instead of employing previous autoencoder style networks to build this transformation, we propose an enhanced Invertible Encoding Network with invertible neural networks (INNs) to largely mitigate the information loss problem for better compression. Experimental results on the Kodak, CLIC, and Tecnick datasets show that our method outperforms the existing learned image compression methods and compression standards, including VVC (VTM 12.1), especially for high-resolution images. Our source code is available at https://github.com/xyq7/InvCompress.



### Alignment of Tractography Streamlines using Deformation Transfer via Parallel Transport
- **Arxiv ID**: http://arxiv.org/abs/2108.03697v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, math.DG, stat.AP, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/2108.03697v1)
- **Published**: 2021-08-08 17:58:30+00:00
- **Updated**: 2021-08-08 17:58:30+00:00
- **Authors**: Andrew Lizarraga, David Lee, Antoni Kubicki, Ashish Sahib, Elvis Nunez, Katherine Narr, Shantanu H. Joshi
- **Comment**: None
- **Journal**: None
- **Summary**: We present a geometric framework for aligning white matter fiber tracts. By registering fiber tracts between brains, one expects to see overlap of anatomical structures that often provide meaningful comparisons across subjects. However, the geometry of white matter tracts is highly heterogeneous, and finding direct tract-correspondence across multiple individuals remains a challenging problem. We present a novel deformation metric between tracts that allows one to compare tracts while simultaneously obtaining a registration. To accomplish this, fiber tracts are represented by an intrinsic mean along with the deformation fields represented by tangent vectors from the mean. In this setting, one can determine a parallel transport between tracts and then register corresponding tangent vectors. We present the results of bundle alignment on a population of 43 healthy adult subjects.



### BIGRoC: Boosting Image Generation via a Robust Classifier
- **Arxiv ID**: http://arxiv.org/abs/2108.03702v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03702v4)
- **Published**: 2021-08-08 18:05:44+00:00
- **Updated**: 2023-02-01 11:07:15+00:00
- **Authors**: Roy Ganz, Michael Elad
- **Comment**: None
- **Journal**: Transactions on machine learning research, 2023
- **Summary**: The interest of the machine learning community in image synthesis has grown significantly in recent years, with the introduction of a wide range of deep generative models and means for training them. In this work, we propose a general model-agnostic technique for improving the image quality and the distribution fidelity of generated images obtained by any generative model. Our method, termed BIGRoC (Boosting Image Generation via a Robust Classifier), is based on a post-processing procedure via the guidance of a given robust classifier and without a need for additional training of the generative model. Given a synthesized image, we propose to update it through projected gradient steps over the robust classifier to refine its recognition. We demonstrate this post-processing algorithm on various image synthesis methods and show a significant quantitative and qualitative improvement on CIFAR-10 and ImageNet. Surprisingly, although BIGRoC is the first model agnostic among refinement approaches and requires much less information, it outperforms competitive methods. Specifically, BIGRoC improves the image synthesis best performing diffusion model on ImageNet 128x128 by 14.81%, attaining an FID score of 2.53, and on 256x256 by 7.87%, achieving an FID of 3.63. Moreover, we conduct an opinion survey, according to which humans significantly prefer our method's outputs.



### OVIS: Open-Vocabulary Visual Instance Search via Visual-Semantic Aligned Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.03704v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.03704v1)
- **Published**: 2021-08-08 18:13:53+00:00
- **Updated**: 2021-08-08 18:13:53+00:00
- **Authors**: Sheng Liu, Kevin Lin, Lijuan Wang, Junsong Yuan, Zicheng Liu
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We introduce the task of open-vocabulary visual instance search (OVIS). Given an arbitrary textual search query, Open-vocabulary Visual Instance Search (OVIS) aims to return a ranked list of visual instances, i.e., image patches, that satisfies the search intent from an image database. The term "open vocabulary" means that there are neither restrictions to the visual instance to be searched nor restrictions to the word that can be used to compose the textual search query. We propose to address such a search challenge via visual-semantic aligned representation learning (ViSA). ViSA leverages massive image-caption pairs as weak image-level (not instance-level) supervision to learn a rich cross-modal semantic space where the representations of visual instances (not images) and those of textual queries are aligned, thus allowing us to measure the similarities between any visual instance and an arbitrary textual query. To evaluate the performance of ViSA, we build two datasets named OVIS40 and OVIS1600 and also introduce a pipeline for error analysis. Through extensive experiments on the two datasets, we demonstrate ViSA's ability to search for visual instances in images not available during training given a wide range of textual queries including those composed of uncommon words. Experimental results show that ViSA achieves an mAP@50 of 21.9% on OVIS40 under the most challenging setting and achieves an mAP@6 of 14.9% on OVIS1600 dataset.



### Hierarchical View Predictor: Unsupervised 3D Global Feature Learning through Hierarchical Prediction among Unordered Views
- **Arxiv ID**: http://arxiv.org/abs/2108.03743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03743v1)
- **Published**: 2021-08-08 22:07:10+00:00
- **Updated**: 2021-08-08 22:07:10+00:00
- **Authors**: Zhizhong Han, Xiyang Wang, Yu-Shen Liu, Matthias Zwicker
- **Comment**: To appear at ACMMM 2021
- **Journal**: None
- **Summary**: Unsupervised learning of global features for 3D shape analysis is an important research challenge because it avoids manual effort for supervised information collection. In this paper, we propose a view-based deep learning model called Hierarchical View Predictor (HVP) to learn 3D shape features from unordered views in an unsupervised manner. To mine highly discriminative information from unordered views, HVP performs a novel hierarchical view prediction over a view pair, and aggregates the knowledge learned from the predictions in all view pairs into a global feature. In a view pair, we pose hierarchical view prediction as the task of hierarchically predicting a set of image patches in a current view from its complementary set of patches, and in addition, completing the current view and its opposite from any one of the two sets of patches. Hierarchical prediction, in patches to patches, patches to view and view to view, facilitates HVP to effectively learn the structure of 3D shapes from the correlation between patches in the same view and the correlation between a pair of complementary views. In addition, the employed implicit aggregation over all view pairs enables HVP to learn global features from unordered views. Our results show that HVP can outperform state-of-the-art methods under large-scale 3D shape benchmarks in shape classification and retrieval.



### Unsupervised Learning of Fine Structure Generation for 3D Point Clouds by 2D Projection Matching
- **Arxiv ID**: http://arxiv.org/abs/2108.03746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.03746v1)
- **Published**: 2021-08-08 22:15:31+00:00
- **Updated**: 2021-08-08 22:15:31+00:00
- **Authors**: Chen Chao, Zhizhong Han, Yu-Shen Liu, Matthias Zwicker
- **Comment**: To appear at ICCV 2021. Our code, data and models are available at
  https://github.com/chenchao15/2D\_projection\_matching
- **Journal**: None
- **Summary**: Learning to generate 3D point clouds without 3D supervision is an important but challenging problem. Current solutions leverage various differentiable renderers to project the generated 3D point clouds onto a 2D image plane, and train deep neural networks using the per-pixel difference with 2D ground truth images. However, these solutions are still struggling to fully recover fine structures of 3D shapes, such as thin tubes or planes. To resolve this issue, we propose an unsupervised approach for 3D point cloud generation with fine structures. Specifically, we cast 3D point cloud learning as a 2D projection matching problem. Rather than using entire 2D silhouette images as a regular pixel supervision, we introduce structure adaptive sampling to randomly sample 2D points within the silhouettes as an irregular point supervision, which alleviates the consistency issue of sampling from different view angles. Our method pushes the neural network to generate a 3D point cloud whose 2D projections match the irregular point supervision from different view angles. Our 2D projection matching approach enables the neural network to learn more accurate structure information than using the per-pixel difference, especially for fine and thin 3D structures. Our method can recover fine 3D structures from 2D silhouette images at different resolutions, and is robust to different sampling methods and point number in irregular point supervision. Our method outperforms others under widely used benchmarks. Our code, data and models are available at https://github.com/chenchao15/2D\_projection\_matching.



