# Arxiv Papers in cs.CV on 2021-09-04
### Seam Carving Detection and Localization using Two-Stage Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.01764v1
- **DOI**: 10.1007/978-981-16-0289-4_29
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01764v1)
- **Published**: 2021-09-04 01:49:08+00:00
- **Updated**: 2021-09-04 01:49:08+00:00
- **Authors**: Lakshmanan Nataraj, Chandrakanth Gudavalli, Tajuddin Manhar Mohammed, Shivkumar Chandrasekaran, B. S. Manjunath
- **Comment**: None
- **Journal**: None
- **Summary**: Seam carving is a method to resize an image in a content aware fashion. However, this method can also be used to carve out objects from images. In this paper, we propose a two-step method to detect and localize seam carved images. First, we build a detector to detect small patches in an image that has been seam carved. Next, we compute a heatmap on an image based on the patch detector's output. Using these heatmaps, we build another detector to detect if a whole image is seam carved or not. Our experimental results show that our approach is effective in detecting and localizing seam carved images.



### To be Critical: Self-Calibrated Weakly Supervised Learning for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.01770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01770v1)
- **Published**: 2021-09-04 02:45:22+00:00
- **Updated**: 2021-09-04 02:45:22+00:00
- **Authors**: Yongri Piao, Jian Wang, Miao Zhang, Zhengxuan Ma, Huchuan Lu
- **Comment**: In the manuscript
- **Journal**: None
- **Summary**: Weakly-supervised salient object detection (WSOD) aims to develop saliency models using image-level annotations. Despite of the success of previous works, explorations on an effective training strategy for the saliency network and accurate matches between image-level annotations and salient objects are still inadequate. In this work, 1) we propose a self-calibrated training strategy by explicitly establishing a mutual calibration loop between pseudo labels and network predictions, liberating the saliency network from error-prone propagation caused by pseudo labels. 2) we prove that even a much smaller dataset (merely 1.8% of ImageNet) with well-matched annotations can facilitate models to achieve better performance as well as generalizability. This sheds new light on the development of WSOD and encourages more contributions to the community. Comprehensive experiments demonstrate that our method outperforms all the existing WSOD methods by adopting the self-calibrated strategy only. Steady improvements are further achieved by training on the proposed dataset. Additionally, our method achieves 94.7% of the performance of fully-supervised methods on average. And what is more, the fully supervised models adopting our predicted results as "ground truths" achieve successful results (95.6% for BASNet and 97.3% for ITSD on F-measure), while costing only 0.32% of labeling time for pixel-level annotation.



### Semantics-Guided Contrastive Network for Zero-Shot Object detection
- **Arxiv ID**: http://arxiv.org/abs/2109.06062v2
- **DOI**: 10.1109/TPAMI.2021.3140070
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06062v2)
- **Published**: 2021-09-04 03:32:15+00:00
- **Updated**: 2021-12-31 21:54:40+00:00
- **Authors**: Caixia Yan, Xiaojun Chang, Minnan Luo, Huan Liu, Xiaoqin Zhang, Qinghua Zheng
- **Comment**: 15 pages, Accepted
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2021
- **Summary**: Zero-shot object detection (ZSD), the task that extends conventional detection models to detecting objects from unseen categories, has emerged as a new challenge in computer vision. Most existing approaches tackle the ZSD task with a strict mapping-transfer strategy, which may lead to suboptimal ZSD results: 1) the learning process of those models ignores the available unseen class information, and thus can be easily biased towards the seen categories; 2) the original visual feature space is not well-structured and lack of discriminative information. To address these issues, we develop a novel Semantics-Guided Contrastive Network for ZSD, named ContrastZSD, a detection framework that first brings contrastive learning mechanism into the realm of zero-shot detection. Particularly, ContrastZSD incorporates two semantics-guided contrastive learning subnets that contrast between region-category and region-region pairs respectively. The pairwise contrastive tasks take advantage of additional supervision signals derived from both ground truth label and pre-defined class similarity distribution. Under the guidance of those explicit semantic supervision, the model can learn more knowledge about unseen categories to avoid the bias problem to seen concepts, while optimizing the data structure of visual features to be more discriminative for better visual-semantic alignment. Extensive experiments are conducted on two popular benchmarks for ZSD, i.e., PASCAL VOC and MS COCO. Results show that our method outperforms the previous state-of-the-art on both ZSD and generalized ZSD tasks.



### Real-World Adversarial Examples involving Makeup Application
- **Arxiv ID**: http://arxiv.org/abs/2109.03329v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.03329v1)
- **Published**: 2021-09-04 05:29:28+00:00
- **Updated**: 2021-09-04 05:29:28+00:00
- **Authors**: Chang-Sheng Lin, Chia-Yi Hsu, Pin-Yu Chen, Chia-Mu Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have developed rapidly and have achieved outstanding performance in several tasks, such as image classification and natural language processing. However, recent studies have indicated that both digital and physical adversarial examples can fool neural networks. Face-recognition systems are used in various applications that involve security threats from physical adversarial examples. Herein, we propose a physical adversarial attack with the use of full-face makeup. The presence of makeup on the human face is a reasonable possibility, which possibly increases the imperceptibility of attacks. In our attack framework, we combine the cycle-adversarial generative network (cycle-GAN) and a victimized classifier. The Cycle-GAN is used to generate adversarial makeup, and the architecture of the victimized classifier is VGG 16. Our experimental results show that our attack can effectively overcome manual errors in makeup application, such as color and position-related errors. We also demonstrate that the approaches used to train the models can influence physical attacks; the adversarial perturbations crafted from the pre-trained model are affected by the corresponding training data.



### PR-Net: Preference Reasoning for Personalized Video Highlight Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.01799v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01799v1)
- **Published**: 2021-09-04 06:12:13+00:00
- **Updated**: 2021-09-04 06:12:13+00:00
- **Authors**: Runnan Chen, Penghao Zhou, Wenzhe Wang, Nenglun Chen, Pai Peng, Xing Sun, Wenping Wang
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Personalized video highlight detection aims to shorten a long video to interesting moments according to a user's preference, which has recently raised the community's attention. Current methods regard the user's history as holistic information to predict the user's preference but negating the inherent diversity of the user's interests, resulting in vague preference representation. In this paper, we propose a simple yet efficient preference reasoning framework (PR-Net) to explicitly take the diverse interests into account for frame-level highlight prediction. Specifically, distinct user-specific preferences for each input query frame are produced, presented as the similarity weighted sum of history highlights to the corresponding query frame. Next, distinct comprehensive preferences are formed by the user-specific preferences and a learnable generic preference for more overall highlight measurement. Lastly, the degree of highlight and non-highlight for each query frame is calculated as semantic similarity to its comprehensive and non-highlight preferences, respectively. Besides, to alleviate the ambiguity due to the incomplete annotation, a new bi-directional contrastive loss is proposed to ensure a compact and differentiable metric space. In this way, our method significantly outperforms state-of-the-art methods with a relative improvement of 12% in mean accuracy precision.



### A Comprehensive Approach for UAV Small Object Detection with Simulation-based Transfer Learning and Adaptive Fusion
- **Arxiv ID**: http://arxiv.org/abs/2109.01800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01800v1)
- **Published**: 2021-09-04 06:27:13+00:00
- **Updated**: 2021-09-04 06:27:13+00:00
- **Authors**: Chen Rui, Guo Youwei, Zheng Huafei, Jiang Hongyu
- **Comment**: None
- **Journal**: None
- **Summary**: Precisely detection of Unmanned Aerial Vehicles(UAVs) plays a critical role in UAV defense systems. Deep learning is widely adopted for UAV object detection whereas researches on this topic are limited by the amount of dataset and small scale of UAV. To tackle these problems, a novel comprehensive approach that combines transfer learning based on simulation data and adaptive fusion is proposed. Firstly, the open-source plugin AirSim proposed by Microsoft is used to generate mass realistic simulation data. Secondly, transfer learning is applied to obtain a pre-trained YOLOv5 model on the simulated dataset and fine-tuned model on the real-world dataset. Finally, an adaptive fusion mechanism is proposed to further improve small object detection performance. Experiment results demonstrate the effectiveness of simulation-based transfer learning which leads to a 2.7% performance increase on UAV object detection. Furthermore, with transfer learning and adaptive fusion mechanism, 7.1% improvement is achieved compared to the original YOLO v5 model.



### Dual Transfer Learning for Event-based End-task Prediction via Pluggable Event to Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2109.01801v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01801v3)
- **Published**: 2021-09-04 06:49:09+00:00
- **Updated**: 2021-11-24 09:18:51+00:00
- **Authors**: Lin Wang, Yujeong Chae, Kuk-Jin Yoon
- **Comment**: ICCV 2021 (updated references in this version)
- **Journal**: None
- **Summary**: Event cameras are novel sensors that perceive the per-pixel intensity changes and output asynchronous event streams with high dynamic range and less motion blur. It has been shown that events alone can be used for end-task learning, e.g., semantic segmentation, based on encoder-decoder-like networks. However, as events are sparse and mostly reflect edge information, it is difficult to recover original details merely relying on the decoder. Moreover, most methods resort to pixel-wise loss alone for supervision, which might be insufficient to fully exploit the visual details from sparse events, thus leading to less optimal performance. In this paper, we propose a simple yet flexible two-stream framework named Dual Transfer Learning (DTL) to effectively enhance the performance on the end-tasks without adding extra inference cost. The proposed approach consists of three parts: event to end-task learning (EEL) branch, event to image translation (EIT) branch, and transfer learning (TL) module that simultaneously explores the feature-level affinity information and pixel-level knowledge from the EIT branch to improve the EEL branch. This simple yet novel method leads to strong representation learning from events and is evidenced by the significant performance boost on the end-tasks such as semantic segmentation and depth estimation.



### Stimuli-Aware Visual Emotion Analysis
- **Arxiv ID**: http://arxiv.org/abs/2109.01812v1
- **DOI**: 10.1109/TIP.2021.3106813
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.01812v1)
- **Published**: 2021-09-04 08:14:52+00:00
- **Updated**: 2021-09-04 08:14:52+00:00
- **Authors**: Jingyuan Yang, Jie Li, Xiumei Wang, Yuxuan Ding, Xinbo Gao
- **Comment**: Accepted by TIP
- **Journal**: in IEEE Transactions on Image Processing, vol. 30, pp. 7432-7445,
  2021
- **Summary**: Visual emotion analysis (VEA) has attracted great attention recently, due to the increasing tendency of expressing and understanding emotions through images on social networks. Different from traditional vision tasks, VEA is inherently more challenging since it involves a much higher level of complexity and ambiguity in human cognitive process. Most of the existing methods adopt deep learning techniques to extract general features from the whole image, disregarding the specific features evoked by various emotional stimuli. Inspired by the \textit{Stimuli-Organism-Response (S-O-R)} emotion model in psychological theory, we proposed a stimuli-aware VEA method consisting of three stages, namely stimuli selection (S), feature extraction (O) and emotion prediction (R). First, specific emotional stimuli (i.e., color, object, face) are selected from images by employing the off-the-shelf tools. To the best of our knowledge, it is the first time to introduce stimuli selection process into VEA in an end-to-end network. Then, we design three specific networks, i.e., Global-Net, Semantic-Net and Expression-Net, to extract distinct emotional features from different stimuli simultaneously. Finally, benefiting from the inherent structure of Mikel's wheel, we design a novel hierarchical cross-entropy loss to distinguish hard false examples from easy ones in an emotion-specific manner. Experiments demonstrate that the proposed method consistently outperforms the state-of-the-art approaches on four public visual emotion datasets. Ablation study and visualizations further prove the validity and interpretability of our method.



### RiWNet: A moving object instance segmentation Network being Robust in adverse Weather conditions
- **Arxiv ID**: http://arxiv.org/abs/2109.01820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01820v1)
- **Published**: 2021-09-04 08:55:36+00:00
- **Updated**: 2021-09-04 08:55:36+00:00
- **Authors**: Chenjie Wang, Chengyuan Li, Bin Luo, Wei Wang, Jun Liu
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: Segmenting each moving object instance in a scene is essential for many applications. But like many other computer vision tasks, this task performs well in optimal weather, but then adverse weather tends to fail. To be robust in weather conditions, the usual way is to train network in data of given weather pattern or to fuse multiple sensors. We focus on a new possibility, that is, to improve its resilience to weather interference through the network's structural design. First, we propose a novel FPN structure called RiWFPN with a progressive top-down interaction and attention refinement module. RiWFPN can directly replace other FPN structures to improve the robustness of the network in non-optimal weather conditions. Then we extend SOLOV2 to capture temporal information in video to learn motion information, and propose a moving object instance segmentation network with RiWFPN called RiWNet. Finally, in order to verify the effect of moving instance segmentation in different weather disturbances, we propose a VKTTI-moving dataset which is a moving instance segmentation dataset based on the VKTTI dataset, taking into account different weather scenes such as rain, fog, sunset, morning as well as overcast. The experiment proves how RiWFPN improves the network's resilience to adverse weather effects compared to other FPN structures. We compare RiWNet to several other state-of-the-art methods in some challenging datasets, and RiWNet shows better performance especially under adverse weather conditions.



### Multi-modal Representation Learning for Video Advertisement Content Structuring
- **Arxiv ID**: http://arxiv.org/abs/2109.06637v1
- **DOI**: 10.1145/3474085.3479218
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2109.06637v1)
- **Published**: 2021-09-04 09:08:29+00:00
- **Updated**: 2021-09-04 09:08:29+00:00
- **Authors**: Daya Guo, Zhaoyang Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: Video advertisement content structuring aims to segment a given video advertisement and label each segment on various dimensions, such as presentation form, scene, and style. Different from real-life videos, video advertisements contain sufficient and useful multi-modal content like caption and speech, which provides crucial video semantics and would enhance the structuring process. In this paper, we propose a multi-modal encoder to learn multi-modal representation from video advertisements by interacting between video-audio and text. Based on multi-modal representation, we then apply Boundary-Matching Network to generate temporal proposals. To make the proposals more accurate, we refine generated proposals by scene-guided alignment and re-ranking. Finally, we incorporate proposal located embeddings into the introduced multi-modal encoder to capture temporal relationships between local features of each proposal and global features of the whole video for classification. Experimental results show that our method achieves significantly improvement compared with several baselines and Rank 1 on the task of Multi-modal Ads Video Understanding in ACM Multimedia 2021 Grand Challenge. Ablation study further shows that leveraging multi-modal content like caption and speech in video advertisements significantly improve the performance.



### Multi-View Spatial-Temporal Graph Convolutional Networks with Domain Generalization for Sleep Stage Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.01824v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01824v1)
- **Published**: 2021-09-04 09:19:27+00:00
- **Updated**: 2021-09-04 09:19:27+00:00
- **Authors**: Ziyu Jia, Youfang Lin, Jing Wang, Xiaojun Ning, Yuanlai He, Ronghao Zhou, Yuhan Zhou, Li-wei H. Lehman
- **Comment**: Accepted by IEEE Transactions on Neural Systems and Rehabilitation
  Engineering(TNSRE)
- **Journal**: None
- **Summary**: Sleep stage classification is essential for sleep assessment and disease diagnosis. Although previous attempts to classify sleep stages have achieved high classification performance, several challenges remain open: 1) How to effectively utilize time-varying spatial and temporal features from multi-channel brain signals remains challenging. Prior works have not been able to fully utilize the spatial topological information among brain regions. 2) Due to the many differences found in individual biological signals, how to overcome the differences of subjects and improve the generalization of deep neural networks is important. 3) Most deep learning methods ignore the interpretability of the model to the brain. To address the above challenges, we propose a multi-view spatial-temporal graph convolutional networks (MSTGCN) with domain generalization for sleep stage classification. Specifically, we construct two brain view graphs for MSTGCN based on the functional connectivity and physical distance proximity of the brain regions. The MSTGCN consists of graph convolutions for extracting spatial features and temporal convolutions for capturing the transition rules among sleep stages. In addition, attention mechanism is employed for capturing the most relevant spatial-temporal information for sleep stage classification. Finally, domain generalization and MSTGCN are integrated into a unified framework to extract subject-invariant sleep features. Experiments on two public datasets demonstrate that the proposed model outperforms the state-of-the-art baselines.



### GOHOME: Graph-Oriented Heatmap Output for future Motion Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.01827v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.01827v4)
- **Published**: 2021-09-04 09:34:57+00:00
- **Updated**: 2021-09-21 13:33:23+00:00
- **Authors**: Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, Fabien Moutarde
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose GOHOME, a method leveraging graph representations of the High Definition Map and sparse projections to generate a heatmap output representing the future position probability distribution for a given agent in a traffic scene. This heatmap output yields an unconstrained 2D grid representation of agent future possible locations, allowing inherent multimodality and a measure of the uncertainty of the prediction. Our graph-oriented model avoids the high computation burden of representing the surrounding context as squared images and processing it with classical CNNs, but focuses instead only on the most probable lanes where the agent could end up in the immediate future. GOHOME reaches 2$nd$ on Argoverse Motion Forecasting Benchmark on the MissRate$_6$ metric while achieving significant speed-up and memory burden diminution compared to Argoverse 1$^{st}$ place method HOME. We also highlight that heatmap output enables multimodal ensembling and improve 1$^{st}$ place MissRate$_6$ by more than 15$\%$ with our best ensemble on Argoverse. Finally, we evaluate and reach state-of-the-art performance on the other trajectory prediction datasets nuScenes and Interaction, demonstrating the generalizability of our method.



### OCTAVA: an open-source toolbox for quantitative analysis of optical coherence tomography angiography images
- **Arxiv ID**: http://arxiv.org/abs/2109.01835v1
- **DOI**: 10.1371/journal.pone.0261052
- **Categories**: **eess.IV**, cs.CV, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/2109.01835v1)
- **Published**: 2021-09-04 10:11:42+00:00
- **Updated**: 2021-09-04 10:11:42+00:00
- **Authors**: Gavrielle R. Untracht, Rolando Matos, Nikolaos Dikaios, Mariam Bapir, Abdullah K. Durrani, Teemapron Butsabong, Paola Campagnolo, David D. Sampson, Christian Heiss, Danuta M. Sampson
- **Comment**: 37 pages, 9 figures
- **Journal**: None
- **Summary**: Optical coherence tomography angiography (OCTA) performs non-invasive visualization and characterization of microvasculature in research and clinical applications mainly in ophthalmology and dermatology. A wide variety of instruments, imaging protocols, processing methods and metrics have been used to describe the microvasculature, such that comparing different study outcomes is currently not feasible. With the goal of contributing to standardization of OCTA data analysis, we report a user-friendly, open-source toolbox, OCTAVA (OCTA Vascular Analyzer), to automate the pre-processing, segmentation, and quantitative analysis of en face OCTA maximum intensity projection images in a standardized workflow. We present each analysis step, including optimization of filtering and choice of segmentation algorithm, and definition of metrics. We perform quantitative analysis of OCTA images from different commercial and non-commercial instruments and samples and show OCTAVA can accurately and reproducibly determine metrics for characterization of microvasculature. Wide adoption could enable studies and aggregation of data on a scale sufficient to develop reliable microvascular biomarkers for early detection, and to guide treatment, of microvascular disease.



### RAMA: A Rapid Multicut Algorithm on GPU
- **Arxiv ID**: http://arxiv.org/abs/2109.01838v3
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, cs.DS, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01838v3)
- **Published**: 2021-09-04 10:33:59+00:00
- **Updated**: 2022-03-11 15:28:06+00:00
- **Authors**: Ahmed Abbas, Paul Swoboda
- **Comment**: Published in CVPR 2022
- **Journal**: None
- **Summary**: We propose a highly parallel primal-dual algorithm for the multicut (a.k.a. correlation clustering) problem, a classical graph clustering problem widely used in machine learning and computer vision. Our algorithm consists of three steps executed recursively: (1) Finding conflicted cycles that correspond to violated inequalities of the underlying multicut relaxation, (2) Performing message passing between the edges and cycles to optimize the Lagrange relaxation coming from the found violated cycles producing reduced costs and (3) Contracting edges with high reduced costs through matrix-matrix multiplications. Our algorithm produces primal solutions and lower bounds that estimate the distance to optimum. We implement our algorithm on GPUs and show resulting one to two orders-of-magnitudes improvements in execution speed without sacrificing solution quality compared to traditional sequential algorithms that run on CPUs. We can solve very large scale benchmark problems with up to $\mathcal{O}(10^8)$ variables in a few seconds with small primal-dual gaps. Our code is available at https://github.com/pawelswoboda/RAMA.



### Towards Expressive Communication with Internet Memes: A New Multimodal Conversation Dataset and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2109.01839v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01839v1)
- **Published**: 2021-09-04 10:39:52+00:00
- **Updated**: 2021-09-04 10:39:52+00:00
- **Authors**: Zhengcong Fei, Zekang Li, Jinchao Zhang, Yang Feng, Jie Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: As a kind of new expression elements, Internet memes are popular and extensively used in online chatting scenarios since they manage to make dialogues vivid, moving, and interesting. However, most current dialogue researches focus on text-only dialogue tasks. In this paper, we propose a new task named as \textbf{M}eme incorporated \textbf{O}pen-domain \textbf{D}ialogue (MOD). Compared to previous dialogue tasks, MOD is much more challenging since it requires the model to understand the multimodal elements as well as the emotions behind them. To facilitate the MOD research, we construct a large-scale open-domain multimodal dialogue dataset incorporating abundant Internet memes into utterances. The dataset consists of $\sim$45K Chinese conversations with $\sim$606K utterances. Each conversation contains about $13$ utterances with about $4$ Internet memes on average and each utterance equipped with an Internet meme is annotated with the corresponding emotion. In addition, we present a simple and effective method, which utilizes a unified generation network to solve the MOD task. Experimental results demonstrate that our method trained on the proposed corpus is able to achieve expressive communication including texts and memes. The corpus and models have been publicly available at https://github.com/lizekang/DSTC10-MOD.



### A Privacy-Preserving Image Retrieval Scheme Using A Codebook Generated From Independent Plain-Image Dataset
- **Arxiv ID**: http://arxiv.org/abs/2109.01841v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2109.01841v1)
- **Published**: 2021-09-04 11:00:00+00:00
- **Updated**: 2021-09-04 11:00:00+00:00
- **Authors**: Kenta Iida, Hitoshi Kiya
- **Comment**: This paper will be presented at APSIPA ASC 2021. arXiv admin note:
  text overlap with arXiv:2011.00270
- **Journal**: None
- **Summary**: In this paper, we propose a privacy-preserving image-retrieval scheme using a codebook generated by using a plain-image dataset. Encryption-then-compression (EtC) images, which were proposed for EtC systems, have been used in conventional privacy-preserving image-retrieval schemes, in which a codebook is generated from EtC images uploaded by image owners, and extended SIMPLE descriptors are then calculated as image descriptors by using the codebook. In contrast, in the proposed scheme, a codebook is generated from a dataset independent of uploaded images. The use of an independent dataset enables us not only to use a codebook that does not require recalculation but also to constantly provide a high retrieval accuracy. In an experiment, the proposed scheme is demonstrated to maintain a high retrieval performance, even if codebooks are generated from a plain image dataset independent of image owners' encrypted images.



### On robustness of generative representations against catastrophic forgetting
- **Arxiv ID**: http://arxiv.org/abs/2109.01844v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01844v1)
- **Published**: 2021-09-04 11:33:24+00:00
- **Updated**: 2021-09-04 11:33:24+00:00
- **Authors**: Wojciech Masarczyk, Kamil Deja, Tomasz Trzciński
- **Comment**: None
- **Journal**: None
- **Summary**: Catastrophic forgetting of previously learned knowledge while learning new tasks is a widely observed limitation of contemporary neural networks. Although many continual learning methods are proposed to mitigate this drawback, the main question remains unanswered: what is the root cause of catastrophic forgetting? In this work, we aim at answering this question by posing and validating a set of research hypotheses related to the specificity of representations built internally by neural models. More specifically, we design a set of empirical evaluations that compare the robustness of representations in discriminative and generative models against catastrophic forgetting. We observe that representations learned by discriminative models are more prone to catastrophic forgetting than their generative counterparts, which sheds new light on the advantages of developing generative models for continual learning. Finally, our work opens new research pathways and possibilities to adopt generative models in continual learning beyond mere replay mechanisms.



### Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering
- **Arxiv ID**: http://arxiv.org/abs/2109.01847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01847v1)
- **Published**: 2021-09-04 11:37:18+00:00
- **Updated**: 2021-09-04 11:37:18+00:00
- **Authors**: Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, Zhaopeng Cui
- **Comment**: Accepted to ICCV 2021. Project Page:
  https://zju3dv.github.io/object_nerf
- **Journal**: None
- **Summary**: Implicit neural rendering techniques have shown promising results for novel view synthesis. However, existing methods usually encode the entire scene as a whole, which is generally not aware of the object identity and limits the ability to the high-level editing tasks such as moving or adding furniture. In this paper, we present a novel neural scene rendering system, which learns an object-compositional neural radiance field and produces realistic rendering with editing capability for a clustered and real-world scene. Specifically, we design a novel two-pathway architecture, in which the scene branch encodes the scene geometry and appearance, and the object branch encodes each standalone object conditioned on learnable object activation codes. To survive the training in heavily cluttered scenes, we propose a scene-guided training strategy to solve the 3D space ambiguity in the occluded regions and learn sharp boundaries for each object. Extensive experiments demonstrate that our system not only achieves competitive performance for static scene novel-view synthesis, but also produces realistic rendering for object-level editing.



### Predicting isocitrate dehydrogenase mutation status in glioma using structural brain networks and graph neural networks
- **Arxiv ID**: http://arxiv.org/abs/2109.01854v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2109.01854v2)
- **Published**: 2021-09-04 12:19:33+00:00
- **Updated**: 2021-09-10 08:24:42+00:00
- **Authors**: Yiran Wei, Yonghao Li, Xi Chen, Carola-Bibiane Schönlieb, Chao Li, Stephen J. Price
- **Comment**: Accepted by Brain Lesion Workshop
- **Journal**: None
- **Summary**: Glioma is a common malignant brain tumor with distinct survival among patients. The isocitrate dehydrogenase (IDH) gene mutation provides critical diagnostic and prognostic value for glioma. It is of crucial significance to non-invasively predict IDH mutation based on pre-treatment MRI. Machine learning/deep learning models show reasonable performance in predicting IDH mutation using MRI. However, most models neglect the systematic brain alterations caused by tumor invasion, where widespread infiltration along white matter tracts is a hallmark of glioma. Structural brain network provides an effective tool to characterize brain organisation, which could be captured by the graph neural networks (GNN) to more accurately predict IDH mutation.   Here we propose a method to predict IDH mutation using GNN, based on the structural brain network of patients. Specifically, we firstly construct a network template of healthy subjects, consisting of atlases of edges (white matter tracts) and nodes (cortical/subcortical brain regions) to provide regions of interest (ROIs). Next, we employ autoencoders to extract the latent multi-modal MRI features from the ROIs of edges and nodes in patients, to train a GNN architecture for predicting IDH mutation. The results show that the proposed method outperforms the baseline models using the 3D-CNN and 3D-DenseNet. In addition, model interpretation suggests its ability to identify the tracts infiltrated by tumor, corresponding to clinical prior knowledge. In conclusion, integrating brain networks with GNN offers a new avenue to study brain lesions using computational neuroscience and computer vision approaches.



### Spatiotemporal Inconsistency Learning for DeepFake Video Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.01860v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01860v3)
- **Published**: 2021-09-04 13:05:37+00:00
- **Updated**: 2021-10-11 05:15:08+00:00
- **Authors**: Zhihao Gu, Yang Chen, Taiping Yao, Shouhong Ding, Jilin Li, Feiyue Huang, Lizhuang Ma
- **Comment**: To appear in ACM MM 2021
- **Journal**: None
- **Summary**: The rapid development of facial manipulation techniques has aroused public concerns in recent years. Following the success of deep learning, existing methods always formulate DeepFake video detection as a binary classification problem and develop frame-based and video-based solutions. However, little attention has been paid to capturing the spatial-temporal inconsistency in forged videos. To address this issue, we term this task as a Spatial-Temporal Inconsistency Learning (STIL) process and instantiate it into a novel STIL block, which consists of a Spatial Inconsistency Module (SIM), a Temporal Inconsistency Module (TIM), and an Information Supplement Module (ISM). Specifically, we present a novel temporal modeling paradigm in TIM by exploiting the temporal difference over adjacent frames along with both horizontal and vertical directions. And the ISM simultaneously utilizes the spatial information from SIM and temporal information from TIM to establish a more comprehensive spatial-temporal representation. Moreover, our STIL block is flexible and could be plugged into existing 2D CNNs. Extensive experiments and visualizations are presented to demonstrate the effectiveness of our method against the state-of-the-art competitors.



### Robust Mitosis Detection Using a Cascade Mask-RCNN Approach With Domain-Specific Residual Cycle-GAN Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.01878v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01878v2)
- **Published**: 2021-09-04 14:37:31+00:00
- **Updated**: 2021-09-28 20:30:16+00:00
- **Authors**: Gauthier Roy, Jules Dedieu, Capucine Bertrand, Alireza Moshayedi, Ali Mammadov, Stéphanie Petit, Saima Ben Hadj, Rutger H. J. Fick
- **Comment**: Gauthier Roy and Jules Dedieu contributed equally to this work
- **Journal**: None
- **Summary**: For the MIDOG mitosis detection challenge, we created a cascade algorithm consisting of a Mask-RCNN detector, followed by a classification ensemble consisting of ResNet50 and DenseNet201 to refine detected mitotic candidates. The MIDOG training data consists of 200 frames originating from four scanners, three of which are annotated for mitotic instances with centroid annotations. Our main algorithmic choices are as follows: first, to enhance the generalizability of our detector and classification networks, we use a state-of-the-art residual Cycle-GAN to transform each scanner domain to every other scanner domain. During training, we then randomly load, for each image, one of the four domains. In this way, our networks can learn from the fourth non-annotated scanner domain even if we don't have annotations for it. Second, for training the detector network, rather than using centroid-based fixed-size bounding boxes, we create mitosis-specific bounding boxes. We do this by manually annotating a small selection of mitoses, training a Mask-RCNN on this small dataset, and applying it to the rest of the data to obtain full annotations. We trained the follow-up classification ensemble using only the challenge-provided positive and hard-negative examples. On the preliminary test set, the algorithm scores an F1 score of 0.7578, putting us as the second-place team on the leaderboard.



### Moving Object Detection for Event-based Vision using k-means Clustering
- **Arxiv ID**: http://arxiv.org/abs/2109.01879v4
- **DOI**: 10.1109/UPCON52273.2021.9667636
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01879v4)
- **Published**: 2021-09-04 14:43:14+00:00
- **Updated**: 2022-01-11 21:03:51+00:00
- **Authors**: Anindya Mondal, Mayukhmali Das
- **Comment**: Nine pages, five figures, Published in 2021 IEEE 8th Uttar Pradesh
  Section International Conference on Electrical, Electronics and Computer
  Engineering (UPCON)
- **Journal**: None
- **Summary**: Moving object detection is important in computer vision. Event-based cameras are bio-inspired cameras that work by mimicking the working of the human eye. These cameras have multiple advantages over conventional frame-based cameras, like reduced latency, HDR, reduced motion blur during high motion, low power consumption, etc. In spite of these advantages, event-based cameras are noise-sensitive and have low resolution. Moreover, the task of moving object detection in these cameras is difficult, as event-based sensors lack useful visual features like texture and color. In this paper, we investigate the application of the k-means clustering technique in detecting moving objects in event-based data.



### Deep learning facilitates fully automated brain image registration of optoacoustic tomography and magnetic resonance imaging
- **Arxiv ID**: http://arxiv.org/abs/2109.01880v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01880v1)
- **Published**: 2021-09-04 14:50:44+00:00
- **Updated**: 2021-09-04 14:50:44+00:00
- **Authors**: Yexing Hu, Berkan Lafci, Artur Luzgin, Hao Wang, Jan Klohs, Xose Luis Dean-Ben, Ruiqing Ni, Daniel Razansky, Wuwei Ren
- **Comment**: 15 pages, 5 figures
- **Journal**: None
- **Summary**: Multi-spectral optoacoustic tomography (MSOT) is an emerging optical imaging method providing multiplex molecular and functional information from the rodent brain. It can be greatly augmented by magnetic resonance imaging (MRI) that offers excellent soft-tissue contrast and high-resolution brain anatomy. Nevertheless, registration of multi-modal images remains challenging, chiefly due to the entirely different image contrast rendered by these modalities. Previously reported registration algorithms mostly relied on manual user-dependent brain segmentation, which compromised data interpretation and accurate quantification. Here we propose a fully automated registration method for MSOT-MRI multimodal imaging empowered by deep learning. The automated workflow includes neural network-based image segmentation to generate suitable masks, which are subsequently registered using an additional neural network. Performance of the algorithm is showcased with datasets acquired by cross-sectional MSOT and high-field MRI preclinical scanners. The automated registration method is further validated with manual and half-automated registration, demonstrating its robustness and accuracy.



### Weakly supervised semantic segmentation of tomographic images in the diagnosis of stroke
- **Arxiv ID**: http://arxiv.org/abs/2109.01887v1
- **DOI**: 10.1088/1742-6596/2099/1/012021
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01887v1)
- **Published**: 2021-09-04 15:24:38+00:00
- **Updated**: 2021-09-04 15:24:38+00:00
- **Authors**: Anna Dobshik, Andrey Tulupov, Vladimir Berikov
- **Comment**: 6 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: This paper presents an automatic algorithm for the segmentation of areas affected by an acute stroke on the non-contrast computed tomography brain images. The proposed algorithm is designed for learning in a weakly supervised scenario when some images are labeled accurately, and some images are labeled inaccurately. Wrong labels appear as a result of inaccuracy made by a radiologist in the process of manual annotation of computed tomography images. We propose methods for solving the segmentation problem in the case of inaccurately labeled training data. We use the U-Net neural network architecture with several modifications. Experiments on real computed tomography scans show that the proposed methods increase the segmentation accuracy.



### Fast Image-Anomaly Mitigation for Autonomous Mobile Robots
- **Arxiv ID**: http://arxiv.org/abs/2109.01889v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.01889v1)
- **Published**: 2021-09-04 15:39:42+00:00
- **Updated**: 2021-09-04 15:39:42+00:00
- **Authors**: Gianmario Fumagalli, Yannick Huber, Marcin Dymczyk, Roland Siegwart, Renaud Dubé
- **Comment**: Published on 2021 International Conference on Intelligent Robots and
  Systems (IROS)
- **Journal**: None
- **Summary**: Camera anomalies like rain or dust can severelydegrade image quality and its related tasks, such as localizationand segmentation. In this work we address this importantissue by implementing a pre-processing step that can effectivelymitigate such artifacts in a real-time fashion, thus supportingthe deployment of autonomous systems with limited computecapabilities. We propose a shallow generator with aggregation,trained in an adversarial setting to solve the ill-posed problemof reconstructing the occluded regions. We add an enhancer tofurther preserve high-frequency details and image colorization.We also produce one of the largest publicly available datasets1to train our architecture and use realistic synthetic raindrops toobtain an improved initialization of the model. We benchmarkour framework on existing datasets and on our own imagesobtaining state-of-the-art results while enabling real-time per-formance, with up to 40x faster inference time than existingapproaches.



### On the Out-of-distribution Generalization of Probabilistic Image Modelling
- **Arxiv ID**: http://arxiv.org/abs/2109.02639v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2109.02639v5)
- **Published**: 2021-09-04 17:00:37+00:00
- **Updated**: 2021-12-10 10:39:42+00:00
- **Authors**: Mingtian Zhang, Andi Zhang, Steven McDonagh
- **Comment**: None
- **Journal**: None
- **Summary**: Out-of-distribution (OOD) detection and lossless compression constitute two problems that can be solved by the training of probabilistic models on a first dataset with subsequent likelihood evaluation on a second dataset, where data distributions differ. By defining the generalization of probabilistic models in terms of likelihood we show that, in the case of image models, the OOD generalization ability is dominated by local features. This motivates our proposal of a Local Autoregressive model that exclusively models local image features towards improving OOD performance. We apply the proposed model to OOD detection tasks and achieve state-of-the-art unsupervised OOD detection performance without the introduction of additional data. Additionally, we employ our model to build a new lossless image compressor: NeLLoC (Neural Local Lossless Compressor) and report state-of-the-art compression rates and model size.



### Robust fine-tuning of zero-shot models
- **Arxiv ID**: http://arxiv.org/abs/2109.01903v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01903v3)
- **Published**: 2021-09-04 17:11:28+00:00
- **Updated**: 2022-06-21 21:50:28+00:00
- **Authors**: Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo-Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, Ludwig Schmidt
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Large pre-trained models such as CLIP or ALIGN offer consistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning methods substantially improve accuracy on a given target distribution, they often reduce robustness to distribution shifts. We address this tension by introducing a simple and effective method for improving robustness while fine-tuning: ensembling the weights of the zero-shot and fine-tuned models (WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy improvements under distribution shift, while preserving high accuracy on the target distribution. On ImageNet and five derived distribution shifts, WiSE-FT improves accuracy under distribution shift by 4 to 6 percentage points (pp) over prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves similarly large robustness gains (2 to 23 pp) on a diverse set of six further distribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard fine-tuning on seven commonly used transfer learning datasets. These improvements come at no additional computational cost during fine-tuning or inference.



### Sparse Spatial Attention Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.01915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01915v1)
- **Published**: 2021-09-04 18:41:05+00:00
- **Updated**: 2021-09-04 18:41:05+00:00
- **Authors**: Mengyu Liu, Hujun Yin
- **Comment**: None
- **Journal**: None
- **Summary**: The spatial attention mechanism captures long-range dependencies by aggregating global contextual information to each query location, which is beneficial for semantic segmentation. In this paper, we present a sparse spatial attention network (SSANet) to improve the efficiency of the spatial attention mechanism without sacrificing the performance. Specifically, a sparse non-local (SNL) block is proposed to sample a subset of key and value elements for each query element to capture long-range relations adaptively and generate a sparse affinity matrix to aggregate contextual information efficiently. Experimental results show that the proposed approach outperforms other context aggregation methods and achieves state-of-the-art performance on the Cityscapes, PASCAL Context and ADE20K datasets.



### Audio-Visual Transformer Based Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2109.01926v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01926v1)
- **Published**: 2021-09-04 20:25:35+00:00
- **Updated**: 2021-09-04 20:25:35+00:00
- **Authors**: Usman Sajid, Xiangyu Chen, Hasan Sajid, Taejoon Kim, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd estimation is a very challenging problem. The most recent study tries to exploit auditory information to aid the visual models, however, the performance is limited due to the lack of an effective approach for feature extraction and integration. The paper proposes a new audiovisual multi-task network to address the critical challenges in crowd counting by effectively utilizing both visual and audio inputs for better modalities association and productive feature extraction. The proposed network introduces the notion of auxiliary and explicit image patch-importance ranking (PIR) and patch-wise crowd estimate (PCE) information to produce a third (run-time) modality. These modalities (audio, visual, run-time) undergo a transformer-inspired cross-modality co-attention mechanism to finally output the crowd estimate. To acquire rich visual features, we propose a multi-branch structure with transformer-style fusion in-between. Extensive experimental evaluations show that the proposed scheme outperforms the state-of-the-art networks under all evaluation settings with up to 33.8% improvement. We also analyze and compare the vision-only variant of our network and empirically demonstrate its superiority over previous approaches.



### ISyNet: Convolutional Neural Networks design for AI accelerator
- **Arxiv ID**: http://arxiv.org/abs/2109.01932v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01932v2)
- **Published**: 2021-09-04 20:57:05+00:00
- **Updated**: 2022-08-18 19:15:51+00:00
- **Authors**: Alexey Letunovskiy, Vladimir Korviakov, Vladimir Polovnikov, Anastasiia Kargapoltseva, Ivan Mazurenko, Yepan Xiong
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: In recent years Deep Learning reached significant results in many practical problems, such as computer vision, natural language processing, speech recognition and many others. For many years the main goal of the research was to improve the quality of models, even if the complexity was impractically high. However, for the production solutions, which often require real-time work, the latency of the model plays a very important role. Current state-of-the-art architectures are found with neural architecture search (NAS) taking model complexity into account. However, designing of the search space suitable for specific hardware is still a challenging task. To address this problem we propose a measure of hardware efficiency of neural architecture search space - matrix efficiency measure (MEM); a search space comprising of hardware-efficient operations; a latency-aware scaling method; and ISyNet - a set of architectures designed to be fast on the specialized neural processing unit (NPU) hardware and accurate at the same time. We show the advantage of the designed architectures for the NPU devices on ImageNet and the generalization ability for the downstream classification and detection tasks.



### Weakly Supervised Relative Spatial Reasoning for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2109.01934v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01934v1)
- **Published**: 2021-09-04 21:29:06+00:00
- **Updated**: 2021-09-04 21:29:06+00:00
- **Authors**: Pratyay Banerjee, Tejas Gokhale, Yezhou Yang, Chitta Baral
- **Comment**: Accepted to ICCV 2021. PaperId : ICCV2021-10857 Copyright transferred
  to IEEE ICCV. DOI will be updated later
- **Journal**: None
- **Summary**: Vision-and-language (V\&L) reasoning necessitates perception of visual concepts such as objects and actions, understanding semantics and language grounding, and reasoning about the interplay between the two modalities. One crucial aspect of visual reasoning is spatial understanding, which involves understanding relative locations of objects, i.e.\ implicitly learning the geometry of the scene. In this work, we evaluate the faithfulness of V\&L models to such geometric understanding, by formulating the prediction of pair-wise relative locations of objects as a classification as well as a regression task. Our findings suggest that state-of-the-art transformer-based V\&L models lack sufficient abilities to excel at this task. Motivated by this, we design two objectives as proxies for 3D spatial reasoning (SR) -- object centroid estimation, and relative position estimation, and train V\&L with weak supervision from off-the-shelf depth estimators. This leads to considerable improvements in accuracy for the "GQA" visual question answering challenge (in fully supervised, few-shot, and O.O.D settings) as well as improvements in relative spatial reasoning. Code and data will be released \href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.



### Utilizing Adversarial Targeted Attacks to Boost Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2109.01945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01945v1)
- **Published**: 2021-09-04 22:30:49+00:00
- **Updated**: 2021-09-04 22:30:49+00:00
- **Authors**: Uriya Pesso, Koby Bibas, Meir Feder
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks have been shown to be highly effective at degrading the performance of deep neural networks (DNNs). The most prominent defense is adversarial training, a method for learning a robust model. Nevertheless, adversarial training does not make DNNs immune to adversarial perturbations. We propose a novel solution by adopting the recently suggested Predictive Normalized Maximum Likelihood. Specifically, our defense performs adversarial targeted attacks according to different hypotheses, where each hypothesis assumes a specific label for the test sample. Then, by comparing the hypothesis probabilities, we predict the label. Our refinement process corresponds to recent findings of the adversarial subspace properties. We extensively evaluate our approach on 16 adversarial attack benchmarks using ResNet-50, WideResNet-28, and a2-layer ConvNet trained with ImageNet, CIFAR10, and MNIST, showing a significant improvement of up to 5.7%, 3.7%, and 0.6% respectively.



### LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation
- **Arxiv ID**: http://arxiv.org/abs/2109.04993v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.04993v2)
- **Published**: 2021-09-04 22:48:46+00:00
- **Updated**: 2021-10-19 16:43:54+00:00
- **Authors**: Mohammad Abuzar Shaikh, Zhanghexuan Ji, Dana Moukheiber, Yan Shen, Sargur Srihari, Mingchen Gao
- **Comment**: 14 pages, 10 Figures, 5 Tables
- **Journal**: None
- **Summary**: Pre-training visual and textual representations from large-scale image-text pairs is becoming a standard approach for many downstream vision-language tasks. The transformer-based models learn inter and intra-modal attention through a list of self-supervised learning tasks. This paper proposes LAViTeR, a novel architecture for visual and textual representation learning. The main module, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks, GAN-based image synthesis and Image Captioning. We also propose a new evaluation metric measuring the similarity between the learnt visual and textual embedding. The experimental results on two public datasets, CUB and MS-COCO, demonstrate superior visual and textual representation alignment in the joint feature embedding space



### Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment
- **Arxiv ID**: http://arxiv.org/abs/2109.01949v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01949v1)
- **Published**: 2021-09-04 22:58:35+00:00
- **Updated**: 2021-09-04 22:58:35+00:00
- **Authors**: Zhanghexuan Ji, Mohammad Abuzar Shaikh, Dana Moukheiber, Sargur Srihari, Yifan Peng, Mingchen Gao
- **Comment**: 10 Pages, 1 Figure, 3 Tables, Accepted in 12th Machine Learning in
  Medical Imaging (MLMI 2021) workshop
- **Journal**: None
- **Summary**: Self-supervised learning provides an opportunity to explore unlabeled chest X-rays and their associated free-text reports accumulated in clinical routine without manual supervision. This paper proposes a Joint Image Text Representation Learning Network (JoImTeRNet) for pre-training on chest X-ray images and their radiology reports. The model was pre-trained on both the global image-sentence level and the local image region-word level for visual-textual matching. Both are bidirectionally constrained on Cross-Entropy based and ranking-based Triplet Matching Losses. The region-word matching is calculated using the attention mechanism without direct supervision about their mapping. The pre-trained multi-modal representation learning paves the way for downstream tasks concerning image and/or text encoding. We demonstrate the representation learning quality by cross-modality retrievals and multi-label classifications on two datasets: OpenI-IU and MIMIC-CXR



