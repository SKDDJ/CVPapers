# Arxiv Papers in cs.CV on 2021-09-18
### Small Lesion Segmentation in Brain MRIs with Subpixel Embedding
- **Arxiv ID**: http://arxiv.org/abs/2109.08791v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.08791v1)
- **Published**: 2021-09-18 00:21:17+00:00
- **Updated**: 2021-09-18 00:21:17+00:00
- **Authors**: Alex Wong, Allison Chen, Yangchao Wu, Safa Cicek, Alexandre Tiard, Byung-Woo Hong, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method to segment MRI scans of the human brain into ischemic stroke lesion and normal tissues. We propose a neural network architecture in the form of a standard encoder-decoder where predictions are guided by a spatial expansion embedding network. Our embedding network learns features that can resolve detailed structures in the brain without the need for high-resolution training images, which are often unavailable and expensive to acquire. Alternatively, the encoder-decoder learns global structures by means of striding and max pooling. Our embedding network complements the encoder-decoder architecture by guiding the decoder with fine-grained details lost to spatial downsampling during the encoder stage. Unlike previous works, our decoder outputs at 2 times the input resolution, where a single pixel in the input resolution is predicted by four neighboring subpixels in our output. To obtain the output at the original scale, we propose a learnable downsampler (as opposed to hand-crafted ones e.g. bilinear) that combines subpixel predictions. Our approach improves the baseline architecture by approximately 11.7% and achieves the state of the art on the ATLAS public benchmark dataset with a smaller memory footprint and faster runtime than the best competing method. Our source code has been made available at: https://github.com/alexklwong/subpixel-embedding-segmentation.



### The Report on China-Spain Joint Clinical Testing for Rapid COVID-19 Risk Screening by Eye-region Manifestations
- **Arxiv ID**: http://arxiv.org/abs/2109.08807v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2109.08807v1)
- **Published**: 2021-09-18 02:28:01+00:00
- **Updated**: 2021-09-18 02:28:01+00:00
- **Authors**: Yanwei Fu, Feng Li, Paula boned Fustel, Lei Zhao, Lijie Jia, Haojie Zheng, Qiang Sun, Shisong Rong, Haicheng Tang, Xiangyang Xue, Li Yang, Hong Li, Jiao Xie Wenxuan Wang, Yuan Li, Wei Wang, Yantao Pei, Jianmin Wang, Xiuqi Wu, Yanhua Zheng, Hongxia Tian, Mengwei Gu
- **Comment**: None
- **Journal**: None
- **Summary**: Background: The worldwide surge in coronavirus cases has led to the COVID-19 testing demand surge. Rapid, accurate, and cost-effective COVID-19 screening tests working at a population level are in imperative demand globally.   Methods: Based on the eye symptoms of COVID-19, we developed and tested a COVID-19 rapid prescreening model using the eye-region images captured in China and Spain with cellphone cameras. The convolutional neural networks (CNNs)-based model was trained on these eye images to complete binary classification task of identifying the COVID-19 cases. The performance was measured using area under receiver-operating-characteristic curve (AUC), sensitivity, specificity, accuracy, and F1. The application programming interface was open access.   Findings: The multicenter study included 2436 pictures corresponding to 657 subjects (155 COVID-19 infection, 23.6%) in development dataset (train and validation) and 2138 pictures corresponding to 478 subjects (64 COVID-19 infections, 13.4%) in test dataset. The image-level performance of COVID-19 prescreening model in the China-Spain multicenter study achieved an AUC of 0.913 (95% CI, 0.898-0.927), with a sensitivity of 0.695 (95% CI, 0.643-0.748), a specificity of 0.904 (95% CI, 0.891 -0.919), an accuracy of 0.875(0.861-0.889), and a F1 of 0.611(0.568-0.655).   Interpretation: The CNN-based model for COVID-19 rapid prescreening has reliable specificity and sensitivity. This system provides a low-cost, fully self-performed, non-invasive, real-time feedback solution for continuous surveillance and large-scale rapid prescreening for COVID-19.   Funding: This project is supported by Aimomics (Shanghai) Intelligent



### HYouTube: Video Harmonization Dataset
- **Arxiv ID**: http://arxiv.org/abs/2109.08809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.08809v1)
- **Published**: 2021-09-18 02:43:21+00:00
- **Updated**: 2021-09-18 02:43:21+00:00
- **Authors**: Xinyuan Lu, Shengyuan Huang, Li Niu, Wenyan Cong, Liqing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Video composition aims to generate a composite video by combining the foreground of one video with the background of another video, but the inserted foreground may be incompatible with the background in terms of color and illumination. Video harmonization aims to adjust the foreground of a composite video to make it compatible with the background. So far, video harmonization has only received limited attention and there is no public dataset for video harmonization. In this work, we construct a new video harmonization dataset HYouTube by adjusting the foreground of real videos to create synthetic composite videos. Considering the domain gap between real composite videos and synthetic composite videos, we additionally create 100 real composite videos via copy-and-paste. Datasets are available at https://github.com/bcmi/Video-Harmonization-Dataset-HYouTube.



### Homogeneous and Heterogeneous Relational Graph for Visible-infrared Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2109.08811v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.08811v2)
- **Published**: 2021-09-18 02:51:16+00:00
- **Updated**: 2021-12-02 13:34:52+00:00
- **Authors**: Yujian Feng, Feng Chen, Jian Yu, Yimu Ji, Fei Wu, Shangdong Liu, Xiao-Yuan Jing
- **Comment**: None
- **Journal**: None
- **Summary**: Visible-infrared person re-identification (VI Re-ID) aims to match person images between the visible and infrared modalities. Existing VI Re-ID methods mainly focus on extracting homogeneous structural relationships in an image, i.e. the relations between local features, while ignoring the heterogeneous correlation of local features in different modalities. The heterogeneous structured relationship is crucial to learn effective identity representations and perform cross-modality matching. In this paper, we model the homogenous structural relationship by a modality-specific graph within individual modality and then mine the heterogeneous structural correlation with the modality-specific graph of visible and infrared modality. First, the homogeneous structured graph (HOSG) mines one-vs.-rest relation between an arbitrary node (local feature) and all the rest nodes within a visible or infrared image to learn effective identity representation. Second, to find cross-modality identity-consistent correspondence, the heterogeneous graph alignment module (HGAM) further measures the relational edge strength between local node features of two modalities with routing search way. Third, we propose the cross-modality cross-correlation (CMCC) loss to extract the modality invariance of feature representations of visible and infrared graphs. CMCC computes the mutual information between modalities and expels semantic redundancy. Extensive experiments on SYSU-MM01 and RegDB datasets demonstrate that our method outperforms state-of-the-arts with a gain of 13.73\% and 9.45\% Rank1/mAP. The code is available at https://github.com/fegnyujian/Homogeneous-and-Heterogeneous-Relational-Graph.



### Learning to Regrasp by Learning to Place
- **Arxiv ID**: http://arxiv.org/abs/2109.08817v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.08817v2)
- **Published**: 2021-09-18 03:07:06+00:00
- **Updated**: 2021-11-17 18:28:57+00:00
- **Authors**: Shuo Cheng, Kaichun Mo, Lin Shao
- **Comment**: Accepted to Conference on Robot Learning (CoRL) 2021
- **Journal**: None
- **Summary**: In this paper, we explore whether a robot can learn to regrasp a diverse set of objects to achieve various desired grasp poses. Regrasping is needed whenever a robot's current grasp pose fails to perform desired manipulation tasks. Endowing robots with such an ability has applications in many domains such as manufacturing or domestic services. Yet, it is a challenging task due to the large diversity of geometry in everyday objects and the high dimensionality of the state and action space. In this paper, we propose a system for robots to take partial point clouds of an object and the supporting environment as inputs and output a sequence of pick-and-place operations to transform an initial object grasp pose to the desired object grasp poses. The key technique includes a neural stable placement predictor and a regrasp graph-based solution through leveraging and changing the surrounding environment. We introduce a new and challenging synthetic dataset for learning and evaluating the proposed approach. We demonstrate the effectiveness of our proposed system with both simulator and real-world experiments. More videos and visualization examples are available on our project webpage.



### Self-Adaptive Partial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2109.08829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.08829v1)
- **Published**: 2021-09-18 04:25:33+00:00
- **Updated**: 2021-09-18 04:25:33+00:00
- **Authors**: Jian Hu, Hongya Tuo, Shizhao Zhang, Chao Wang, Haowen Zhong, Zhikang Zou, Zhongliang Jing, Henry Leung, Ruping Zou
- **Comment**: 10 pages, 14 figures
- **Journal**: None
- **Summary**: Partial Domain adaptation (PDA) aims to solve a more practical cross-domain learning problem that assumes target label space is a subset of source label space. However, the mismatched label space causes significant negative transfer. A traditional solution is using soft weights to increase weights of source shared domain and reduce those of source outlier domain. But it still learns features of outliers and leads to negative immigration. The other mainstream idea is to distinguish source domain into shared and outlier parts by hard binary weights, while it is unavailable to correct the tangled shared and outlier classes. In this paper, we propose an end-to-end Self-Adaptive Partial Domain Adaptation(SAPDA) Network. Class weights evaluation mechanism is introduced to dynamically self-rectify the weights of shared, outlier and confused classes, thus the higher confidence samples have the more sufficient weights. Meanwhile it can eliminate the negative transfer caused by the mismatching of label space greatly. Moreover, our strategy can efficiently measure the transferability of samples in a broader sense, so that our method can achieve competitive results on unsupervised DA task likewise. A large number of experiments on multiple benchmarks have demonstrated the effectiveness of our SAPDA.



### Proposing a System Level Machine Learning Hybrid Architecture and Approach for a Comprehensive Autism Spectrum Disorder Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2110.03775v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2110.03775v1)
- **Published**: 2021-09-18 04:33:09+00:00
- **Updated**: 2021-09-18 04:33:09+00:00
- **Authors**: Ryan Liu, Spencer He
- **Comment**: None
- **Journal**: None
- **Summary**: Autism Spectrum Disorder (ASD) is a severe neuropsychiatric disorder that affects intellectual development, social behavior, and facial features, and the number of cases is still significantly increasing. Due to the variety of symptoms ASD displays, the diagnosis process remains challenging, with numerous misdiagnoses as well as lengthy and expensive diagnoses. Fortunately, if ASD is diagnosed and treated early, then the patient will have a much higher chance of developing normally. For an ASD diagnosis, machine learning algorithms can analyze both social behavior and facial features accurately and efficiently, providing an ASD diagnosis in a drastically shorter amount of time than through current clinical diagnosis processes. Therefore, we propose to develop a hybrid architecture fully utilizing both social behavior and facial feature data to improve the accuracy of diagnosing ASD. We first developed a Linear Support Vector Machine for the social behavior based module, which analyzes Autism Diagnostic Observation Schedule (ADOS) social behavior data. For the facial feature based module, a DenseNet model was utilized to analyze facial feature image data. Finally, we implemented our hybrid model by incorporating different features of the Support Vector Machine and the DenseNet into one model. Our results show that the highest accuracy of 87% for ASD diagnosis has been achieved by our proposed hybrid model. The pros and cons of each module will be discussed in this paper.



### SpeechNAS: Towards Better Trade-off between Latency and Accuracy for Large-Scale Speaker Verification
- **Arxiv ID**: http://arxiv.org/abs/2109.08839v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CL, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2109.08839v1)
- **Published**: 2021-09-18 05:31:27+00:00
- **Updated**: 2021-09-18 05:31:27+00:00
- **Authors**: Wentao Zhu, Tianlong Kong, Shun Lu, Jixiang Li, Dawei Zhang, Feng Deng, Xiaorui Wang, Sen Yang, Ji Liu
- **Comment**: 8 pages, 3 figures, 3 tables. Accepted by ASRU2021
- **Journal**: None
- **Summary**: Recently, x-vector has been a successful and popular approach for speaker verification, which employs a time delay neural network (TDNN) and statistics pooling to extract speaker characterizing embedding from variable-length utterances. Improvement upon the x-vector has been an active research area, and enormous neural networks have been elaborately designed based on the x-vector, eg, extended TDNN (E-TDNN), factorized TDNN (F-TDNN), and densely connected TDNN (D-TDNN). In this work, we try to identify the optimal architectures from a TDNN based search space employing neural architecture search (NAS), named SpeechNAS. Leveraging the recent advances in the speaker recognition, such as high-order statistics pooling, multi-branch mechanism, D-TDNN and angular additive margin softmax (AAM) loss with a minimum hyper-spherical energy (MHE), SpeechNAS automatically discovers five network architectures, from SpeechNAS-1 to SpeechNAS-5, of various numbers of parameters and GFLOPs on the large-scale text-independent speaker recognition dataset VoxCeleb1. Our derived best neural network achieves an equal error rate (EER) of 1.02% on the standard test set of VoxCeleb1, which surpasses previous TDNN based state-of-the-art approaches by a large margin. Code and trained weights are in https://github.com/wentaozhu/speechnas.git



### Memory Regulation and Alignment toward Generalizer RGB-Infrared Person
- **Arxiv ID**: http://arxiv.org/abs/2109.08843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.08843v1)
- **Published**: 2021-09-18 05:55:06+00:00
- **Updated**: 2021-09-18 05:55:06+00:00
- **Authors**: Feng Chen, Fei Wu, Qi Wu, Zhiguo Wan
- **Comment**: None
- **Journal**: None
- **Summary**: The domain shift, coming from unneglectable modality gap and non-overlapped identity classes between training and test sets, is a major issue of RGB-Infrared person re-identification. A key to tackle the inherent issue -- domain shift -- is to enforce the data distributions of the two domains to be similar. However, RGB-IR ReID always demands discriminative features, leading to over-rely feature sensitivity of seen classes, \textit{e.g.}, via attention-based feature alignment or metric learning. Therefore, predicting the unseen query category from predefined training classes may not be accurate and leads to a sub-optimal adversarial gradient. In this paper, we uncover it in a more explainable way and propose a novel multi-granularity memory regulation and alignment module (MG-MRA) to solve this issue. By explicitly incorporating a latent variable attribute, from fine-grained to coarse semantic granularity, into intermediate features, our method could alleviate the over-confidence of the model about discriminative features of seen classes. Moreover, instead of matching discriminative features by traversing nearest neighbor, sparse attributes, \textit{i.e.}, global structural pattern, are recollected with respect to features and assigned to measure pair-wise image similarity in hashing. Extensive experiments on RegDB \cite{RegDB} and SYSU-MM01 \cite{SYSU} show the superiority of the proposed method that outperforms existing state-of-the-art methods. Our code is available in https://github.com/Chenfeng1271/MGMRA.



### Towards High-Quality Temporal Action Detection with Sparse Proposals
- **Arxiv ID**: http://arxiv.org/abs/2109.08847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.08847v1)
- **Published**: 2021-09-18 06:15:19+00:00
- **Updated**: 2021-09-18 06:15:19+00:00
- **Authors**: Jiannan Wu, Peize Sun, Shoufa Chen, Jiewen Yang, Zihao Qi, Lan Ma, Ping Luo
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Temporal Action Detection (TAD) is an essential and challenging topic in video understanding, aiming to localize the temporal segments containing human action instances and predict the action categories. The previous works greatly rely upon dense candidates either by designing varying anchors or enumerating all the combinations of boundaries on video sequences; therefore, they are related to complicated pipelines and sensitive hand-crafted designs. Recently, with the resurgence of Transformer, query-based methods have tended to become the rising solutions for their simplicity and flexibility. However, there still exists a performance gap between query-based methods and well-established methods. In this paper, we identify the main challenge lies in the large variants of action duration and the ambiguous boundaries for short action instances; nevertheless, quadratic-computational global attention prevents query-based methods to build multi-scale feature maps. Towards high-quality temporal action detection, we introduce Sparse Proposals to interact with the hierarchical features. In our method, named SP-TAD, each proposal attends to a local segment feature in the temporal feature pyramid. The local interaction enables utilization of high-resolution features to preserve action instances details. Extensive experiments demonstrate the effectiveness of our method, especially under high tIoU thresholds. E.g., we achieve the state-of-the-art performance on THUMOS14 (45.7% on mAP@0.6, 33.4% on mAP@0.7 and 53.5% on mAP@Avg) and competitive results on ActivityNet-1.3 (32.99% on mAP@Avg). Code will be made available at https://github.com/wjn922/SP-TAD.



### Domain Composition and Attention for Unseen-Domain Generalizable Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.08852v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.08852v1)
- **Published**: 2021-09-18 06:42:47+00:00
- **Updated**: 2021-09-18 06:42:47+00:00
- **Authors**: Ran Gu, Jingyang Zhang, Rui Huang, Wenhui Lei, Guotai Wang, Shaoting Zhang
- **Comment**: Accepted by MICCAI 2021
- **Journal**: None
- **Summary**: Domain generalizable model is attracting increasing attention in medical image analysis since data is commonly acquired from different institutes with various imaging protocols and scanners. To tackle this challenging domain generalization problem, we propose a Domain Composition and Attention-based network (DCA-Net) to improve the ability of domain representation and generalization. First, we present a domain composition method that represents one certain domain by a linear combination of a set of basis representations (i.e., a representation bank). Second, a novel plug-and-play parallel domain preceptor is proposed to learn these basis representations and we introduce a divergence constraint function to encourage the basis representations to be as divergent as possible. Then, a domain attention module is proposed to learn the linear combination coefficients of the basis representations. The result of linear combination is used to calibrate the feature maps of an input image, which enables the model to generalize to different and even unseen domains. We validate our method on public prostate MRI dataset acquired from six different institutions with apparent domain shift. Experimental results show that our proposed model can generalize well on different and even unseen domains and it outperforms state-of-the-art methods on the multi-domain prostate segmentation task.



### A survey on deep learning approaches for breast cancer diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2109.08853v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.08853v2)
- **Published**: 2021-09-18 06:43:34+00:00
- **Updated**: 2022-02-13 21:40:18+00:00
- **Authors**: Timothy Kwong, Samaneh Mazaheri
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has introduced several learning-based methods to recognize breast tumours and presents high applicability in breast cancer diagnostics. It has presented itself as a practical installment in Computer-Aided Diagnostic (CAD) systems to further assist radiologists in diagnostics for different modalities. A deep learning network trained on images provided by hospitals or public databases can perform classification, detection, and segmentation of lesion types. Significant progress has been made in recognizing tumours on 2D images but recognizing 3D images remains a frontier so far. The interconnection of deep learning networks between different fields of study help propels discoveries for more efficient, accurate, and robust networks. In this review paper, the following topics will be explored: (i) theory and application of deep learning, (ii) progress of 2D, 2.5D, and 3D CNN approaches in breast tumour recognition from a performance metric perspective, and (iii) challenges faced in CNN approaches.



### Modern Evolution Strategies for Creativity: Fitting Concrete Images and Abstract Concepts
- **Arxiv ID**: http://arxiv.org/abs/2109.08857v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.08857v2)
- **Published**: 2021-09-18 07:04:41+00:00
- **Updated**: 2022-01-28 07:41:24+00:00
- **Authors**: Yingtao Tian, David Ha
- **Comment**: None
- **Journal**: None
- **Summary**: Evolutionary algorithms have been used in the digital art scene since the 1970s. A popular application of genetic algorithms is to optimize the procedural placement of vector graphic primitives to resemble a given painting. In recent years, deep learning-based approaches have also been proposed to generate procedural drawings, which can be optimized using gradient descent. In this work, we revisit the use of evolutionary algorithms for computational creativity. We find that modern evolution strategies (ES) algorithms, when tasked with the placement of shapes, offer large improvements in both quality and efficiency compared to traditional genetic algorithms, and even comparable to gradient-based methods. We demonstrate that ES is also well suited at optimizing the placement of shapes to fit the CLIP model, and can produce diverse, distinct geometric abstractions that are aligned with human interpretation of language. Videos and demo: https://es-clip.github.io/



### V-SlowFast Network for Efficient Visual Sound Separation
- **Arxiv ID**: http://arxiv.org/abs/2109.08867v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2109.08867v2)
- **Published**: 2021-09-18 07:44:31+00:00
- **Updated**: 2021-09-21 04:25:58+00:00
- **Authors**: Lingyu Zhu, Esa Rahtu
- **Comment**: total 21 pages: main paper 8 pages, references 3 pages, supplementary
  material 10 pages
- **Journal**: None
- **Summary**: The objective of this paper is to perform visual sound separation: i) we study visual sound separation on spectrograms of different temporal resolutions; ii) we propose a new light yet efficient three-stream framework V-SlowFast that operates on Visual frame, Slow spectrogram, and Fast spectrogram. The Slow spectrogram captures the coarse temporal resolution while the Fast spectrogram contains the fine-grained temporal resolution; iii) we introduce two contrastive objectives to encourage the network to learn discriminative visual features for separating sounds; iv) we propose an audio-visual global attention module for audio and visual feature fusion; v) the introduced V-SlowFast model outperforms previous state-of-the-art in single-frame based visual sound separation on small- and large-scale datasets: MUSIC-21, AVE, and VGG-Sound. We also propose a small V-SlowFast architecture variant, which achieves 74.2% reduction in the number of model parameters and 81.4% reduction in GMACs compared to the previous multi-stage models. Project page: https://ly-zhu.github.io/V-SlowFast



### Clean-label Backdoor Attack against Deep Hashing based Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2109.08868v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.08868v2)
- **Published**: 2021-09-18 07:56:59+00:00
- **Updated**: 2022-11-03 12:32:41+00:00
- **Authors**: Kuofeng Gao, Jiawang Bai, Bin Chen, Dongxian Wu, Shu-Tao Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Deep hashing has become a popular method in large-scale image retrieval due to its computational and storage efficiency. However, recent works raise the security concerns of deep hashing. Although existing works focus on the vulnerability of deep hashing in terms of adversarial perturbations, we identify a more pressing threat, backdoor attack, when the attacker has access to the training data. A backdoored deep hashing model behaves normally on original query images, while returning the images with the target label when the trigger presents, which makes the attack hard to be detected. In this paper, we uncover this security concern by utilizing clean-label data poisoning. To the best of our knowledge, this is the first attempt at the backdoor attack against deep hashing models. To craft the poisoned images, we first generate the targeted adversarial patch as the backdoor trigger. Furthermore, we propose the confusing perturbations to disturb the hashing code learning, such that the hashing model can learn more about the trigger. The confusing perturbations are imperceptible and generated by dispersing the images with the target label in the Hamming space. We have conducted extensive experiments to verify the efficacy of our backdoor attack under various settings. For instance, it can achieve 63% targeted mean average precision on ImageNet under 48 bits code length with only 40 poisoned images.



### FastHyMix: Fast and Parameter-free Hyperspectral Image Mixed Noise Removal
- **Arxiv ID**: http://arxiv.org/abs/2109.08879v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.08879v1)
- **Published**: 2021-09-18 08:35:45+00:00
- **Updated**: 2021-09-18 08:35:45+00:00
- **Authors**: Lina Zhuang, Michael K. Ng
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral imaging with high spectral resolution plays an important role in finding objects, identifying materials, or detecting processes. The decrease of the widths of spectral bands leads to a decrease in the signal-to-noise ratio (SNR) of measurements. The decreased SNR reduces the reliability of measured features or information extracted from HSIs. Furthermore, the image degradations linked with various mechanisms also result in different types of noise, such as Gaussian noise, impulse noise, deadlines, and stripes. This paper introduces a fast and parameter-free hyperspectral image mixed noise removal method (termed FastHyMix), which characterizes the complex distribution of mixed noise by using a Gaussian mixture model and exploits two main characteristics of hyperspectral data, namely low-rankness in the spectral domain and high correlation in the spatial domain. The Gaussian mixture model enables us to make a good estimation of Gaussian noise intensity and the location of sparse noise. The proposed method takes advantage of the low-rankness using subspace representation and the spatial correlation of HSIs by adding a powerful deep image prior, which is extracted from a neural denoising network. An exhaustive array of experiments and comparisons with state-of-the-art denoisers were carried out. The experimental results show significant improvement in both synthetic and real datasets. A MATLAB demo of this work will be available at https://github.com/LinaZhuang for the sake of reproducibility.



### Computational Imaging and Artificial Intelligence: The Next Revolution of Mobile Vision
- **Arxiv ID**: http://arxiv.org/abs/2109.08880v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.08880v1)
- **Published**: 2021-09-18 08:47:08+00:00
- **Updated**: 2021-09-18 08:47:08+00:00
- **Authors**: Jinli Suo, Weihang Zhang, Jin Gong, Xin Yuan, David J. Brady, Qionghai Dai
- **Comment**: None
- **Journal**: None
- **Summary**: Signal capture stands in the forefront to perceive and understand the environment and thus imaging plays the pivotal role in mobile vision. Recent explosive progresses in Artificial Intelligence (AI) have shown great potential to develop advanced mobile platforms with new imaging devices. Traditional imaging systems based on the "capturing images first and processing afterwards" mechanism cannot meet this unprecedented demand. Differently, Computational Imaging (CI) systems are designed to capture high-dimensional data in an encoded manner to provide more information for mobile vision systems.Thanks to AI, CI can now be used in real systems by integrating deep learning algorithms into the mobile vision platform to achieve the closed loop of intelligent acquisition, processing and decision making, thus leading to the next revolution of mobile vision.Starting from the history of mobile vision using digital cameras, this work first introduces the advances of CI in diverse applications and then conducts a comprehensive review of current research topics combining CI and AI. Motivated by the fact that most existing studies only loosely connect CI and AI (usually using AI to improve the performance of CI and only limited works have deeply connected them), in this work, we propose a framework to deeply integrate CI and AI by using the example of self-driving vehicles with high-speed communication, edge computing and traffic planning. Finally, we outlook the future of CI plus AI by investigating new materials, brain science and new computing techniques to shed light on new directions of mobile vision systems.



### S$^3$VAADA: Submodular Subset Selection for Virtual Adversarial Active Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2109.08901v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.08901v1)
- **Published**: 2021-09-18 10:53:57+00:00
- **Updated**: 2021-09-18 10:53:57+00:00
- **Authors**: Harsh Rangwani, Arihant Jain, Sumukh K Aithal, R. Venkatesh Babu
- **Comment**: ICCV 2021. Project page:
  http://sites.google.com/iisc.ac.in/s3vaada-iccv2021
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (DA) methods have focused on achieving maximal performance through aligning features from source and target domains without using labeled data in the target domain. Whereas, in the real-world scenario's it might be feasible to get labels for a small proportion of target data. In these scenarios, it is important to select maximally-informative samples to label and find an effective way to combine them with the existing knowledge from source data. Towards achieving this, we propose S$^3$VAADA which i) introduces a novel submodular criterion to select a maximally informative subset to label and ii) enhances a cluster-based DA procedure through novel improvements to effectively utilize all the available data for improving generalization on target. Our approach consistently outperforms the competing state-of-the-art approaches on datasets with varying degrees of domain shifts.



### MetaMedSeg: Volumetric Meta-learning for Few-Shot Organ Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.09734v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09734v1)
- **Published**: 2021-09-18 11:13:45+00:00
- **Updated**: 2021-09-18 11:13:45+00:00
- **Authors**: Anastasia Makarevich, Azade Farshad, Vasileios Belagiannis, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: The lack of sufficient annotated image data is a common issue in medical image segmentation. For some organs and densities, the annotation may be scarce, leading to poor model training convergence, while other organs have plenty of annotated data. In this work, we present MetaMedSeg, a gradient-based meta-learning algorithm that redefines the meta-learning task for the volumetric medical data with the goal to capture the variety between the slices. We also explore different weighting schemes for gradients aggregation, arguing that different tasks might have different complexity, and hence, contribute differently to the initialization. We propose an importance-aware weighting scheme to train our model. In the experiments, we present an evaluation of the medical decathlon dataset by extracting 2D slices from CT and MRI volumes of different organs and performing semantic segmentation. The results show that our proposed volumetric task definition leads to up to 30% improvement in terms of IoU compared to related baselines. The proposed update rule is also shown to improve the performance for complex scenarios where the data distribution of the target organ is very different from the source organs.



### Measuring the rogue wave pattern triggered from Gaussian perturbations by deep learning
- **Arxiv ID**: http://arxiv.org/abs/2109.08909v2
- **DOI**: 10.1103/PhysRevE.105.054202
- **Categories**: **cs.CV**, cs.NA, eess.IV, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2109.08909v2)
- **Published**: 2021-09-18 11:39:02+00:00
- **Updated**: 2021-10-09 05:48:06+00:00
- **Authors**: Liwen Zou, XinHang Luo, Delu Zeng, Liming Ling, Li-Chen Zhao
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Weak Gaussian perturbations on a plane wave background could trigger lots of rogue waves, due to modulational instability. Numerical simulations showed that these rogue waves seemed to have similar unit structure. However, to the best of our knowledge, there is no relative result to prove that these rogue waves have the similar patterns for different perturbations, partly due to that it is hard to measure the rogue wave pattern automatically. In this work, we address these problems from the perspective of computer vision via using deep neural networks. We propose a Rogue Wave Detection Network (RWD-Net) model to automatically and accurately detect RWs on the images, which directly indicates they have the similar computer vision patterns. For this purpose, we herein meanwhile have designed the related dataset, termed as Rogue Wave Dataset-$10$K (RWD-$10$K), which has $10,191$ RW images with bounding box annotations for each RW unit. In our detection experiments, we get $99.29\%$ average precision on the test splits of the RWD-$10$K dataset. Finally, we derive our novel metric, the density of RW units (DRW), to characterize the evolution of Gaussian perturbations and obtain the statistical results on them.



### Unsupervised Domain Adaptation for Semantic Segmentation via Low-level Edge Information Transfer
- **Arxiv ID**: http://arxiv.org/abs/2109.08912v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.08912v1)
- **Published**: 2021-09-18 11:51:31+00:00
- **Updated**: 2021-09-18 11:51:31+00:00
- **Authors**: Hongruixuan Chen, Chen Wu, Yonghao Xu, Bo Du
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation for semantic segmentation aims to make models trained on synthetic data (source domain) adapt to real images (target domain). Previous feature-level adversarial learning methods only consider adapting models on the high-level semantic features. However, the large domain gap between source and target domains in the high-level semantic features makes accurate adaptation difficult. In this paper, we present the first attempt at explicitly using low-level edge information, which has a small inter-domain gap, to guide the transfer of semantic information. To this end, a semantic-edge domain adaptation architecture is proposed, which uses an independent edge stream to process edge information, thereby generating high-quality semantic boundaries over the target domain. Then, an edge consistency loss is presented to align target semantic predictions with produced semantic boundaries. Moreover, we further propose two entropy reweighting methods for semantic adversarial learning and self-supervised learning, respectively, which can further enhance the adaptation performance of our architecture. Comprehensive experiments on two UDA benchmark datasets demonstrate the superiority of our architecture compared with state-of-the-art methods.



### Edge Prior Augmented Networks for Motion Deblurring on Naturally Blurry Images
- **Arxiv ID**: http://arxiv.org/abs/2109.08915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.08915v1)
- **Published**: 2021-09-18 12:00:27+00:00
- **Updated**: 2021-09-18 12:00:27+00:00
- **Authors**: Yuedong Chen, Junjia Huang, Jianfeng Wang, Xiaohua Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Motion deblurring has witnessed rapid development in recent years, and most of the recent methods address it by using deep learning techniques, with the help of different kinds of prior knowledge. Concerning that deblurring is essentially expected to improve the image sharpness, edge information can serve as an important prior. However, the edge has not yet been seriously taken into consideration in previous methods when designing deep models. To this end, we present a novel framework that incorporates edge prior knowledge into deep models, termed Edge Prior Augmented Networks (EPAN). EPAN has a content-based main branch and an edge-based auxiliary branch, which are constructed as a Content Deblurring Net (CDN) and an Edge Enhancement Net (EEN), respectively. EEN is designed to augment CDN in the deblurring process via an attentive fusion mechanism, where edge features are mapped as spatial masks to guide content features in a feature-based hierarchical manner. An edge-guided loss function is proposed to further regulate the optimization of EPAN by enforcing the focus on edge areas. Besides, we design a dual-camera-based image capturing setting to build a new dataset, Real Object Motion Blur (ROMB), with paired sharp and naturally blurry images of fast-moving cars, so as to better train motion deblurring models and benchmark the capability of motion deblurring algorithms in practice. Extensive experiments on the proposed ROMB and other existing datasets demonstrate that EPAN outperforms state-of-the-art approaches qualitatively and quantitatively.



### Underwater Image Enhancement Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2109.08916v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.08916v1)
- **Published**: 2021-09-18 12:01:14+00:00
- **Updated**: 2021-09-18 12:01:14+00:00
- **Authors**: Anushka Yadav, Mayank Upadhyay, Ghanapriya Singh
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposes a method for underwater image enhancement using the principle of histogram equalization. Since underwater images have a global strong dominant colour, their colourfulness and contrast are often degraded. Before applying the histogram equalisation technique on the image, the image is converted from coloured image to a gray scale image for further operations. Histogram equalization is a technique for adjusting image intensities to enhance contrast. The colours of the image are retained using a convolutional neural network model which is trained by the datasets of underwater images to give better results.



### A Studious Approach to Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.08924v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.08924v1)
- **Published**: 2021-09-18 12:48:47+00:00
- **Updated**: 2021-09-18 12:48:47+00:00
- **Authors**: Sahil Khose, Shruti Jain, V Manushree
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of learning from few labeled examples while using large amounts of unlabeled data has been approached by various semi-supervised methods. Although these methods can achieve superior performance, the models are often not deployable due to the large number of parameters. This paper is an ablation study of distillation in a semi-supervised setting, which not just reduces the number of parameters of the model but can achieve this while improving the performance over the baseline supervised model and making it better at generalizing. After the supervised pretraining, the network is used as a teacher model, and a student network is trained over the soft labels that the teacher model generates over the entire unlabeled data. We find that the fewer the labels, the more this approach benefits from a smaller student network. This brings forward the potential of distillation as an effective solution to enhance performance in semi-supervised computer vision tasks while maintaining deployability.



### UNetFormer: A UNet-like Transformer for Efficient Semantic Segmentation of Remote Sensing Urban Scene Imagery
- **Arxiv ID**: http://arxiv.org/abs/2109.08937v4
- **DOI**: 10.1016/j.isprsjprs.2022.06.008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.08937v4)
- **Published**: 2021-09-18 13:55:38+00:00
- **Updated**: 2022-06-26 14:15:18+00:00
- **Authors**: Libo Wang, Rui Li, Ce Zhang, Shenghui Fang, Chenxi Duan, Xiaoliang Meng, Peter M. Atkinson
- **Comment**: Accepted by ISPRS
- **Journal**: journal = {ISPRS Journal of Photogrammetry and Remote
  Sensing},volume = {190},pages = {196-214},year = {2022},issn = {0924-2716},
- **Summary**: Semantic segmentation of remotely sensed urban scene images is required in a wide range of practical applications, such as land cover mapping, urban change detection, environmental protection, and economic assessment.Driven by rapid developments in deep learning technologies, the convolutional neural network (CNN) has dominated semantic segmentation for many years. CNN adopts hierarchical feature representation, demonstrating strong capabilities for local information extraction. However, the local property of the convolution layer limits the network from capturing the global context. Recently, as a hot topic in the domain of computer vision, Transformer has demonstrated its great potential in global information modelling, boosting many vision-related tasks such as image classification, object detection, and particularly semantic segmentation. In this paper, we propose a Transformer-based decoder and construct a UNet-like Transformer (UNetFormer) for real-time urban scene segmentation. For efficient segmentation, the UNetFormer selects the lightweight ResNet18 as the encoder and develops an efficient global-local attention mechanism to model both global and local information in the decoder. Extensive experiments reveal that our method not only runs faster but also produces higher accuracy compared with state-of-the-art lightweight models. Specifically, the proposed UNetFormer achieved 67.8% and 52.4% mIoU on the UAVid and LoveDA datasets, respectively, while the inference speed can achieve up to 322.4 FPS with a 512x512 input on a single NVIDIA GTX 3090 GPU. In further exploration, the proposed Transformer-based decoder combined with a Swin Transformer encoder also achieves the state-of-the-art result (91.3% F1 and 84.1% mIoU) on the Vaihingen dataset. The source code will be freely available at https://github.com/WangLibo1995/GeoSeg.



### Violence Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/2109.08941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.08941v1)
- **Published**: 2021-09-18 14:33:40+00:00
- **Updated**: 2021-09-18 14:33:40+00:00
- **Authors**: Praveen Tirupattur, Christian Schulze, Andreas Dengel
- **Comment**: None
- **Journal**: None
- **Summary**: In the recent years, there has been a tremendous increase in the amount of video content uploaded to social networking and video sharing websites like Facebook and Youtube. As of result of this, the risk of children getting exposed to adult and violent content on the web also increased. To address this issue, an approach to automatically detect violent content in videos is proposed in this work. Here, a novel attempt is made also to detect the category of violence present in a video. A system which can automatically detect violence from both Hollywood movies and videos from the web is extremely useful not only in parental control but also for applications related to movie ratings, video surveillance, genre classification and so on.   Here, both audio and visual features are used to detect violence. MFCC features are used as audio cues. Blood, Motion, and SentiBank features are used as visual cues. Binary SVM classifiers are trained on each of these features to detect violence. Late fusion using a weighted sum of classification scores is performed to get final classification scores for each of the violence class target by the system. To determine optimal weights for each of the violence classes an approach based on grid search is employed. Publicly available datasets, mainly Violent Scene Detection (VSD), are used for classifier training, weight calculation, and testing. The performance of the system is evaluated on two classification tasks, Multi-Class classification, and Binary Classification. The results obtained for Binary Classification are better than the baseline results from MediaEval-2014.



### iWave3D: End-to-end Brain Image Compression with Trainable 3-D Wavelet Transform
- **Arxiv ID**: http://arxiv.org/abs/2109.08942v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2109.08942v2)
- **Published**: 2021-09-18 14:38:59+00:00
- **Updated**: 2021-10-10 01:21:50+00:00
- **Authors**: Dongmei Xue, Haichuan Ma, Li Li, Dong Liu, Zhiwei Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid development of whole brain imaging technology, a large number of brain images have been produced, which puts forward a great demand for efficient brain image compression methods. At present, the most commonly used compression methods are all based on 3-D wavelet transform, such as JP3D. However, traditional 3-D wavelet transforms are designed manually with certain assumptions on the signal, but brain images are not as ideal as assumed. What's more, they are not directly optimized for compression task. In order to solve these problems, we propose a trainable 3-D wavelet transform based on the lifting scheme, in which the predict and update steps are replaced by 3-D convolutional neural networks. Then the proposed transform is embedded into an end-to-end compression scheme called iWave3D, which is trained with a large amount of brain images to directly minimize the rate-distortion loss. Experimental results demonstrate that our method outperforms JP3D significantly by 2.012 dB in terms of average BD-PSNR.



### Manifold-preserved GANs
- **Arxiv ID**: http://arxiv.org/abs/2109.08955v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.08955v1)
- **Published**: 2021-09-18 15:46:51+00:00
- **Updated**: 2021-09-18 15:46:51+00:00
- **Authors**: Haozhe Liu, Hanbang Liang, Xianxu Hou, Haoqian Wu, Feng Liu, Linlin Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have been widely adopted in various fields. However, existing GANs generally are not able to preserve the manifold of data space, mainly due to the simple representation of discriminator for the real/generated data. To address such open challenges, this paper proposes Manifold-preserved GANs (MaF-GANs), which generalize Wasserstein GANs into high-dimensional form. Specifically, to improve the representation of data, the discriminator in MaF-GANs is designed to map data into a high-dimensional manifold. Furthermore, to stabilize the training of MaF-GANs, an operation with precise and universal solution for any K-Lipschitz continuity, called Topological Consistency is proposed. The effectiveness of the proposed method is justified by both theoretical analysis and empirical results. When adopting DCGAN as the backbone on CelebA (256*256), the proposed method achieved 12.43 FID, which outperforms the state-of-the-art model like Realness GAN (23.51 FID) by a large margin. Code will be made publicly available.



### SDTP: Semantic-aware Decoupled Transformer Pyramid for Dense Image Prediction
- **Arxiv ID**: http://arxiv.org/abs/2109.08963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.08963v1)
- **Published**: 2021-09-18 16:29:14+00:00
- **Updated**: 2021-09-18 16:29:14+00:00
- **Authors**: Zekun Li, Yufan Liu, Bing Li, Weiming Hu, Kebin Wu, Pei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Although transformer has achieved great progress on computer vision tasks, the scale variation in dense image prediction is still the key challenge. Few effective multi-scale techniques are applied in transformer and there are two main limitations in the current methods. On one hand, self-attention module in vanilla transformer fails to sufficiently exploit the diversity of semantic information because of its rigid mechanism. On the other hand, it is hard to build attention and interaction among different levels due to the heavy computational burden. To alleviate this problem, we first revisit multi-scale problem in dense prediction, verifying the significance of diverse semantic representation and multi-scale interaction, and exploring the adaptation of transformer to pyramidal structure. Inspired by these findings, we propose a novel Semantic-aware Decoupled Transformer Pyramid (SDTP) for dense image prediction, consisting of Intra-level Semantic Promotion (ISP), Cross-level Decoupled Interaction (CDI) and Attention Refinement Function (ARF). ISP explores the semantic diversity in different receptive space. CDI builds the global attention and interaction among different levels in decoupled space which also solves the problem of heavy computation. Besides, ARF is further added to refine the attention in transformer. Experimental results demonstrate the validity and generality of the proposed method, which outperforms the state-of-the-art by a significant margin in dense image prediction tasks. Furthermore, the proposed components are all plug-and-play, which can be embedded in other methods.



### Atrial Fibrillation: A Medical and Technological Review
- **Arxiv ID**: http://arxiv.org/abs/2109.08974v1
- **DOI**: 10.13140/RG.2.2.35263.64160
- **Categories**: **cs.LG**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2109.08974v1)
- **Published**: 2021-09-18 17:25:55+00:00
- **Updated**: 2021-09-18 17:25:55+00:00
- **Authors**: Samayan Bhattacharya, Sk Shahnawaz
- **Comment**: 11 pages, 1 figure
- **Journal**: None
- **Summary**: Atrial Fibrillation (AF) is the most common type of arrhythmia (Greek a-, loss + rhythmos, rhythm = loss of rhythm) leading to hospitalization in the United States. Though sometimes AF is asymptomatic, it increases the risk of stroke and heart failure in patients, in addition to lowering the health-related quality of life (HRQOL). AF-related care costs the healthcare system between $6.0 to $26 billion each year. Early detection of AF and clinical attention can help improve symptoms and HRQOL of the patient, as well as bring down the cost of care. However, the prevalent paradigm of AF detection depends on electrocardiogram (ECG) recorded at a single point in time and does not shed light on the relation of the symptoms with heart rhythm or AF. In the recent decade, due to the democratization of health monitors and the advent of high-performing computers, Machine Learning algorithms have been proven effective in identifying AF, from the ECG of patients. This paper provides an overview of the symptoms of AF, its diagnosis, and future prospects for research in the field.



### AirLoop: Lifelong Loop Closure Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.08975v3
- **DOI**: 10.1109/ICRA46639.2022.9811658
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.08975v3)
- **Published**: 2021-09-18 17:28:47+00:00
- **Updated**: 2022-03-09 03:49:51+00:00
- **Authors**: Dasong Gao, Chen Wang, Sebastian Scherer
- **Comment**: None
- **Journal**: 2022 International Conference on Robotics and Automation (ICRA)
- **Summary**: Loop closure detection is an important building block that ensures the accuracy and robustness of simultaneous localization and mapping (SLAM) systems. Due to their generalization ability, CNN-based approaches have received increasing attention. Although they normally benefit from training on datasets that are diverse and reflective of the environments, new environments often emerge after the model is deployed. It is therefore desirable to incorporate the data newly collected during operation for incremental learning. Nevertheless, simply finetuning the model on new data is infeasible since it may cause the model's performance on previously learned data to degrade over time, which is also known as the problem of catastrophic forgetting. In this paper, we present AirLoop, a method that leverages techniques from lifelong learning to minimize forgetting when training loop closure detection models incrementally. We experimentally demonstrate the effectiveness of AirLoop on TartanAir, Nordland, and RobotCar datasets. To the best of our knowledge, AirLoop is one of the first works to achieve lifelong learning of deep loop closure detectors.



### Human Recognition based on Retinal Bifurcations and Modified Correlation Function
- **Arxiv ID**: http://arxiv.org/abs/2109.08977v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.08977v1)
- **Published**: 2021-09-18 18:01:45+00:00
- **Updated**: 2021-09-18 18:01:45+00:00
- **Authors**: Amin Dehghani
- **Comment**: 5 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: Nowadays high security is an important issue for most of the secure places and recent advances increase the needs of high-security systems. Therefore, needs to high security for controlling and permitting the allowable people to enter the high secure places, increases and extends the use of conventional recognition methods. Therefore, a novel identification method using retinal images is proposed in this paper. For this purpose, new mathematical functions are applied on corners and bifurcations. To evaluate the proposed method we use 40 retinal images from the DRIVE database, 20 normal retinal image from STARE database and 140 normal retinal images from local collected database and the accuracy rate is 99.34 percent.



### Random Multi-Channel Image Synthesis for Multiplexed Immunofluorescence Imaging
- **Arxiv ID**: http://arxiv.org/abs/2109.09004v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.09004v1)
- **Published**: 2021-09-18 20:18:11+00:00
- **Updated**: 2021-09-18 20:18:11+00:00
- **Authors**: Shunxing Bao, Yucheng Tang, Ho Hin Lee, Riqiang Gao, Sophie Chiron, Ilwoo Lyu, Lori A. Coburn, Keith T. Wilson, Joseph T. Roland, Bennett A. Landman, Yuankai Huo
- **Comment**: Accepted at the third MICCAI workshop on Computational Pathology
  (COMPAY 2021)
- **Journal**: None
- **Summary**: Multiplex immunofluorescence (MxIF) is an emerging imaging technique that produces the high sensitivity and specificity of single-cell mapping. With a tenet of 'seeing is believing', MxIF enables iterative staining and imaging extensive antibodies, which provides comprehensive biomarkers to segment and group different cells on a single tissue section. However, considerable depletion of the scarce tissue is inevitable from extensive rounds of staining and bleaching ('missing tissue'). Moreover, the immunofluorescence (IF) imaging can globally fail for particular rounds ('missing stain''). In this work, we focus on the 'missing stain' issue. It would be appealing to develop digital image synthesis approaches to restore missing stain images without losing more tissue physically. Herein, we aim to develop image synthesis approaches for eleven MxIF structural molecular markers (i.e., epithelial and stromal) on real samples. We propose a novel multi-channel high-resolution image synthesis approach, called pixN2N-HD, to tackle possible missing stain scenarios via a high-resolution generative adversarial network (GAN). Our contribution is three-fold: (1) a single deep network framework is proposed to tackle missing stain in MxIF; (2) the proposed 'N-to-N' strategy reduces theoretical four years of computational time to 20 hours when covering all possible missing stains scenarios, with up to five missing stains (e.g., '(N-1)-to-1', '(N-2)-to-2'); and (3) this work is the first comprehensive experimental study of investigating cross-stain synthesis in MxIF. Our results elucidate a promising direction of advancing MxIF imaging with deep image synthesis.



### The Unreasonable Effectiveness of the Final Batch Normalization Layer
- **Arxiv ID**: http://arxiv.org/abs/2109.09016v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2109.09016v1)
- **Published**: 2021-09-18 21:19:31+00:00
- **Updated**: 2021-09-18 21:19:31+00:00
- **Authors**: Veysel Kocaman, Ofer M. Shir, Thomas Baeck
- **Comment**: Accepted for the 16th International Symposium on Visual Computing
  (ISVC 2021). arXiv admin note: substantial text overlap with arXiv:2011.06319
- **Journal**: None
- **Summary**: Early-stage disease indications are rarely recorded in real-world domains, such as Agriculture and Healthcare, and yet, their accurate identification is critical in that point of time. In this type of highly imbalanced classification problems, which encompass complex features, deep learning (DL) is much needed because of its strong detection capabilities. At the same time, DL is observed in practice to favor majority over minority classes and consequently suffer from inaccurate detection of the targeted early-stage indications. In this work, we extend the study done by Kocaman et al., 2020, showing that the final BN layer, when placed before the softmax output layer, has a considerable impact in highly imbalanced image classification problems as well as undermines the role of the softmax outputs as an uncertainty measure. This current study addresses additional hypotheses and reports on the following findings: (i) the performance gain after adding the final BN layer in highly imbalanced settings could still be achieved after removing this additional BN layer in inference; (ii) there is a certain threshold for the imbalance ratio upon which the progress gained by the final BN layer reaches its peak; (iii) the batch size also plays a role and affects the outcome of the final BN application; (iv) the impact of the BN application is also reproducible on other datasets and when utilizing much simpler neural architectures; (v) the reported BN effect occurs only per a single majority class and multiple minority classes i.e., no improvements are evident when there are two majority classes; and finally, (vi) utilizing this BN layer with sigmoid activation has almost no impact when dealing with a strongly imbalanced image classification tasks.



