# Arxiv Papers in cs.CV on 2021-09-22
### The First Vision For Vitals (V4V) Challenge for Non-Contact Video-Based Physiological Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.10471v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10471v1)
- **Published**: 2021-09-22 01:21:33+00:00
- **Updated**: 2021-09-22 01:21:33+00:00
- **Authors**: Ambareesh Revanur, Zhihua Li, Umur A. Ciftci, Lijun Yin, Laszlo A. Jeni
- **Comment**: ICCVw'21. V4V Dataset and Challenge: https://vision4vitals.github.io/
- **Journal**: None
- **Summary**: Telehealth has the potential to offset the high demand for help during public health emergencies, such as the COVID-19 pandemic. Remote Photoplethysmography (rPPG) - the problem of non-invasively estimating blood volume variations in the microvascular tissue from video - would be well suited for these situations. Over the past few years a number of research groups have made rapid advances in remote PPG methods for estimating heart rate from digital video and obtained impressive results. How these various methods compare in naturalistic conditions, where spontaneous behavior, facial expressions, and illumination changes are present, is relatively unknown. To enable comparisons among alternative methods, the 1st Vision for Vitals Challenge (V4V) presented a novel dataset containing high-resolution videos time-locked with varied physiological signals from a diverse population. In this paper, we outline the evaluation protocol, the data used, and the results. V4V is to be held in conjunction with the 2021 International Conference on Computer Vision.



### Rotor Localization and Phase Mapping of Cardiac Excitation Waves using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.10472v2
- **DOI**: 10.3389/fphys.2021.782176
- **Categories**: **physics.med-ph**, cs.CV, eess.IV, physics.bio-ph
- **Links**: [PDF](http://arxiv.org/pdf/2109.10472v2)
- **Published**: 2021-09-22 01:22:18+00:00
- **Updated**: 2021-11-09 02:37:20+00:00
- **Authors**: Jan Lebert, Namita Ravi, Flavio Fenton, Jan Christoph
- **Comment**: None
- **Journal**: Front. Physiol. 12 (2021) 782176
- **Summary**: The analysis of electrical impulse phenomena in cardiac muscle tissue is important for the diagnosis of heart rhythm disorders and other cardiac pathophysiology. Cardiac mapping techniques acquire local temporal measurements and combine them to visualize the spread of electrophysiological wave phenomena across the heart surface. However, low spatial resolution, sparse measurement locations, noise and other artifacts make it challenging to accurately visualize spatio-temporal activity. For instance, electro-anatomical catheter mapping is severely limited by the sparsity of the measurements, and optical mapping is prone to noise and motion artifacts. In the past, several approaches have been proposed to obtain more reliable maps from noisy or sparse mapping data. Here, we demonstrate that deep learning can be used to compute phase maps and detect phase singularities in optical mapping videos of ventricular fibrillation, as well as in very noisy, low-resolution and extremely sparse simulated data of reentrant wave chaos mimicking catheter mapping data. The deep learning approach learns to directly associate phase maps and the positions of phase singularities with short spatio-temporal sequences of electrical data. We tested several neural network architectures, based on a convolutional neural network with an encoding and decoding structure, to predict phase maps or rotor core positions either directly or indirectly via the prediction of phase maps and a subsequent classical calculation of phase singularities. Predictions can be performed across different data, with models being trained on one species and then successfully applied to another, or being trained solely on simulated data and then applied to experimental data. Future uses may include the analysis of optical mapping studies in basic cardiovascular research, as well as the mapping of atrial fibrillation in the clinical setting.



### MVM3Det: A Novel Method for Multi-view Monocular 3D Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.10473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10473v1)
- **Published**: 2021-09-22 01:31:00+00:00
- **Updated**: 2021-09-22 01:31:00+00:00
- **Authors**: Li Haoran, Duan Zicheng, Ma Mingjun, Chen Yaran, Li Jiaqi, Zhao Dongbin
- **Comment**: 7 pages, 3 figures, submitted to ICRA 2022
- **Journal**: None
- **Summary**: Monocular 3D object detection encounters occlusion problems in many application scenarios, such as traffic monitoring, pedestrian monitoring, etc., which leads to serious false negative. Multi-view object detection effectively solves this problem by combining data from different perspectives. However, due to label confusion and feature confusion, the orientation estimation of multi-view 3D object detection is intractable, which is important for object tracking and intention prediction. In this paper, we propose a novel multi-view 3D object detection method named MVM3Det which simultaneously estimates the 3D position and orientation of the object according to the multi-view monocular information. The method consists of two parts: 1) Position proposal network, which integrates the features from different perspectives into consistent global features through feature orthogonal transformation to estimate the position. 2) Multi-branch orientation estimation network, which introduces feature perspective pooling to overcome the two confusion problems during the orientation estimation. In addition, we present a first dataset for multi-view 3D object detection named MVM3D. Comparing with State-Of-The-Art (SOTA) methods on our dataset and public dataset WildTrack, our method achieves very competitive results.



### Rapid detection and recognition of whole brain activity in a freely behaving Caenorhabditis elegans
- **Arxiv ID**: http://arxiv.org/abs/2109.10474v4
- **DOI**: 10.1371/journal.pcbi.1010594
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10474v4)
- **Published**: 2021-09-22 01:33:54+00:00
- **Updated**: 2022-09-15 09:13:11+00:00
- **Authors**: Yuxiang Wu, Shang Wu, Xin Wang, Chengtian Lang, Quanshi Zhang, Quan Wen, Tianqi Xu
- **Comment**: None
- **Journal**: PLOS Computational Biology 18(10): e1010594, 2022
- **Summary**: Advanced volumetric imaging methods and genetically encoded activity indicators have permitted a comprehensive characterization of whole brain activity at single neuron resolution in \textit{Caenorhabditis elegans}. The constant motion and deformation of the nematode nervous system, however, impose a great challenge for consistent identification of densely packed neurons in a behaving animal. Here, we propose a cascade solution for long-term and rapid recognition of head ganglion neurons in a freely moving \textit{C. elegans}. First, potential neuronal regions from a stack of fluorescence images are detected by a deep learning algorithm. Second, 2-dimensional neuronal regions are fused into 3-dimensional neuron entities. Third, by exploiting the neuronal density distribution surrounding a neuron and relative positional information between neurons, a multi-class artificial neural network transforms engineered neuronal feature vectors into digital neuronal identities. With a small number of training samples, our bottom-up approach is able to process each volume - $1024 \times 1024 \times 18$ in voxels - in less than 1 second and achieves an accuracy of $91\%$ in neuronal detection and above $80\%$ in neuronal tracking over a long video recording. Our work represents a step towards rapid and fully automated algorithms for decoding whole brain activity underlying naturalistic behaviors.



### Generating Compositional Color Representations from Text
- **Arxiv ID**: http://arxiv.org/abs/2109.10477v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10477v1)
- **Published**: 2021-09-22 01:37:13+00:00
- **Updated**: 2021-09-22 01:37:13+00:00
- **Authors**: Paridhi Maheshwari, Nihal Jain, Praneetha Vaddamanu, Dhananjay Raut, Shraiysh Vaishay, Vishwa Vinay
- **Comment**: Accepted as a full paper at CIKM 2021
- **Journal**: None
- **Summary**: We consider the cross-modal task of producing color representations for text phrases. Motivated by the fact that a significant fraction of user queries on an image search engine follow an (attribute, object) structure, we propose a generative adversarial network that generates color profiles for such bigrams. We design our pipeline to learn composition - the ability to combine seen attributes and objects to unseen pairs. We propose a novel dataset curation pipeline from existing public sources. We describe how a set of phrases of interest can be compiled using a graph propagation technique, and then mapped to images. While this dataset is specialized for our investigations on color, the method can be extended to other visual dimensions where composition is of interest. We provide detailed ablation studies that test the behavior of our GAN architecture with loss functions from the contrastive learning literature. We show that the generative model achieves lower Frechet Inception Distance than discriminative ones, and therefore predicts color profiles that better match those from real images. Finally, we demonstrate improved performance in image retrieval and classification, indicating the crucial role that color plays in these downstream tasks.



### AI in Osteoporosis
- **Arxiv ID**: http://arxiv.org/abs/2109.10478v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10478v1)
- **Published**: 2021-09-22 01:37:30+00:00
- **Updated**: 2021-09-22 01:37:30+00:00
- **Authors**: Sokratis Makrogiannis, Keni Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: In this chapter we explore and evaluate methods for trabecular bone characterization and osteoporosis diagnosis with increased interest in sparse approximations. We first describe texture representation and classification techniques, patch-based methods such as Bag of Keypoints, and more recent deep neural networks. Then we introduce the concept of sparse representations for pattern recognition and we detail integrative sparse analysis methods and classifier decision fusion methods. We report cross-validation results on osteoporosis datasets of bone radiographs and compare the results produced by the different categories of methods. We conclude that advances in the AI and machine learning fields have enabled the development of methods that can be used as diagnostic tools in clinical settings.



### Single Image Dehazing with An Independent Detail-Recovery Network
- **Arxiv ID**: http://arxiv.org/abs/2109.10492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10492v1)
- **Published**: 2021-09-22 02:49:43+00:00
- **Updated**: 2021-09-22 02:49:43+00:00
- **Authors**: Yan Li, De Cheng, Jiande Sun, Dingwen Zhang, Nannan Wang, Xinbo Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Single image dehazing is a prerequisite which affects the performance of many computer vision tasks and has attracted increasing attention in recent years. However, most existing dehazing methods emphasize more on haze removal but less on the detail recovery of the dehazed images. In this paper, we propose a single image dehazing method with an independent Detail Recovery Network (DRN), which considers capturing the details from the input image over a separate network and then integrates them into a coarse dehazed image. The overall network consists of two independent networks, named DRN and the dehazing network respectively. Specifically, the DRN aims to recover the dehazed image details through local and global branches respectively. The local branch can obtain local detail information through the convolution layer and the global branch can capture more global information by the Smooth Dilated Convolution (SDC). The detail feature map is fused into the coarse dehazed image to obtain the dehazed image with rich image details. Besides, we integrate the DRN, the physical-model-based dehazing network and the reconstruction loss into an end-to-end joint learning framework. Extensive experiments on the public image dehazing datasets (RESIDE-Indoor, RESIDE-Outdoor and the TrainA-TestA) illustrate the effectiveness of the modules in the proposed method and show that our method outperforms the state-of-the-art dehazing methods both quantitatively and qualitatively. The code is released in https://github.com/YanLi-LY/Dehazing-DRN.



### Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2109.10498v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10498v3)
- **Published**: 2021-09-22 03:12:32+00:00
- **Updated**: 2021-12-07 12:39:38+00:00
- **Authors**: Suncheng Xiang, Guanjie You, Mengyuan Guan, Hao Chen, Binjie Yan, Ting Liu, Yuzhuo Fu
- **Comment**: 21 pages with supplementary material
- **Journal**: None
- **Summary**: Person re-identification (re-ID) plays an important role in applications such as public security and video surveillance. Recently, learning from synthetic data, which benefits from the popularity of synthetic data engine, has attracted great attention from the public eyes. However, existing datasets are limited in quantity, diversity and realisticity, and cannot be efficiently used for re-ID problem. To address this challenge, we manually construct a large-scale person dataset named FineGPR with fine-grained attribute annotations. Moreover, aiming to fully exploit the potential of FineGPR and promote the efficient training from millions of synthetic data, we propose an attribute analysis pipeline called AOST, which dynamically learns attribute distribution in real domain, then eliminates the gap between synthetic and real-world data and thus is freely deployed to new scenarios. Experiments conducted on benchmarks demonstrate that FineGPR with AOST outperforms (or is on par with) existing real and synthetic datasets, which suggests its feasibility for re-ID task and proves the proverbial less-is-more principle. Our synthetic FineGPR dataset is publicly available at https://github.com/JeremyXSC/FineGPR.



### Joint Optical Neuroimaging Denoising with Semantic Tasks
- **Arxiv ID**: http://arxiv.org/abs/2109.10499v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10499v1)
- **Published**: 2021-09-22 03:21:29+00:00
- **Updated**: 2021-09-22 03:21:29+00:00
- **Authors**: Tianfang Zhu, Yue Guan, Anan Li
- **Comment**: None
- **Journal**: None
- **Summary**: Optical neuroimaging is a vital tool for understanding the brain structure and the connection between regions and nuclei. However, the image noise introduced in the sample preparation and the imaging system hinders the extraction of the possible knowlege from the dataset, thus denoising for the optical neuroimaging is usually necessary. The supervised denoisng methods often outperform the unsupervised ones, but the training of the supervised denoising models needs the corresponding clean labels, which is not always avaiable due to the high labeling cost. On the other hand, those semantic labels, such as the located soma positions, the reconstructed neuronal fibers, and the nuclei segmentation result, are generally available and accumulated from everyday neuroscience research. This work connects a supervised denoising and a semantic segmentation model together to form a end-to-end model, which can make use of the semantic labels while still provides a denoised image as an intermediate product. We use both the supervised and the self-supervised models for the denoising and introduce a new cost term for the joint denoising and the segmentation setup. We test the proposed approach on both the synthetic data and the real-world data, including the optical neuroimaing dataset and the electron microscope dataset. The result shows that the joint denoising result outperforms the one using the denoising method alone and the joint model benefits the segmentation and other downstream task as well.



### KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2109.10504v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10504v3)
- **Published**: 2021-09-22 03:38:05+00:00
- **Updated**: 2022-08-07 18:27:10+00:00
- **Authors**: Yongfei Liu, Chenfei Wu, Shao-yen Tseng, Vasudev Lal, Xuming He, Nan Duan
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised vision-and-language pretraining (VLP) aims to learn transferable multi-modal representations from large-scale image-text data and to achieve strong performances on a broad scope of vision-language tasks after finetuning. Previous mainstream VLP approaches typically adopt a two-step strategy relying on external object detectors to encode images in a multi-modal Transformer framework, which suffer from restrictive object concept space, limited image context and inefficient computation. In this paper, we propose an object-aware end-to-end VLP framework, which directly feeds image grid features from CNNs into the Transformer and learns the multi-modal representations jointly. More importantly, we propose to perform object knowledge distillation to facilitate learning cross-modal alignment at different semantic levels. To achieve that, we design two novel pretext tasks by taking object features and their semantic labels from external detectors as supervision: 1.) Object-guided masked vision modeling task focuses on enforcing object-aware representation learning in the multi-modal Transformer; 2.) Phrase-region alignment task aims to improve cross-modal alignment by utilizing the similarities between noun phrases and object labels in the linguistic space. Extensive experiments on a wide range of vision-language tasks demonstrate the efficacy of our proposed framework, and we achieve competitive or superior performances over the existing pretraining strategies.



### NudgeSeg: Zero-Shot Object Segmentation by Repeated Physical Interaction
- **Arxiv ID**: http://arxiv.org/abs/2109.13859v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.13859v1)
- **Published**: 2021-09-22 05:17:09+00:00
- **Updated**: 2021-09-22 05:17:09+00:00
- **Authors**: Chahat Deep Singh, Nitin J. Sanket, Chethan M. Parameshwara, Cornelia Fermüller, Yiannis Aloimonos
- **Comment**: 8 Pages, 7 Figures, 3 Tables
- **Journal**: IEEE International Conference on Robots and Systems (IROS) 2021
- **Summary**: Recent advances in object segmentation have demonstrated that deep neural networks excel at object segmentation for specific classes in color and depth images. However, their performance is dictated by the number of classes and objects used for training, thereby hindering generalization to never seen objects or zero-shot samples. To exacerbate the problem further, object segmentation using image frames rely on recognition and pattern matching cues. Instead, we utilize the 'active' nature of a robot and their ability to 'interact' with the environment to induce additional geometric constraints for segmenting zero-shot samples.   In this paper, we present the first framework to segment unknown objects in a cluttered scene by repeatedly 'nudging' at the objects and moving them to obtain additional motion cues at every step using only a monochrome monocular camera. We call our framework NudgeSeg. These motion cues are used to refine the segmentation masks. We successfully test our approach to segment novel objects in various cluttered scenes and provide an extensive study with image and motion segmentation methods. We show an impressive average detection rate of over 86% on zero-shot objects.



### Incorporating Data Uncertainty in Object Tracking Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2109.10521v2
- **DOI**: None
- **Categories**: **eess.SY**, cs.CV, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/2109.10521v2)
- **Published**: 2021-09-22 05:30:46+00:00
- **Updated**: 2021-11-02 20:35:20+00:00
- **Authors**: Anish Muthali, Forrest Laine, Claire Tomlin
- **Comment**: For associated video, see https://youtu.be/S21EvaAynRg
- **Journal**: None
- **Summary**: Methodologies for incorporating the uncertainties characteristic of data-driven object detectors into object tracking algorithms are explored. Object tracking methods rely on measurement error models, typically in the form of measurement noise, false positive rates, and missed detection rates. Each of these quantities, in general, can be dependent on object or measurement location. However, for detections generated from neural-network processed camera inputs, these measurement error statistics are not sufficient to represent the primary source of errors, namely a dissimilarity between run-time sensor input and the training data upon which the detector was trained. To this end, we investigate incorporating data uncertainty into object tracking methods such as to improve the ability to track objects, and particularly those which out-of-distribution w.r.t. training data. The proposed methodologies are validated on an object tracking benchmark as well on experiments with a real autonomous aircraft.



### A Method For Adding Motion-Blur on Arbitrary Objects By using Auto-Segmentation and Color Compensation Techniques
- **Arxiv ID**: http://arxiv.org/abs/2109.10524v1
- **DOI**: 10.1109/ICIP42928.2021.9506443
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10524v1)
- **Published**: 2021-09-22 05:52:27+00:00
- **Updated**: 2021-09-22 05:52:27+00:00
- **Authors**: Michihiro Mikamo, Ryo Furukawa, Hiroshi Kawasaki
- **Comment**: This paper was accepted at ICIP 2021
- **Journal**: 2021 IEEE International Conference on Image Processing (ICIP)
- **Summary**: When dynamic objects are captured by a camera, motion blur inevitably occurs. Such a blur is sometimes considered as just a noise, however, it sometimes gives an important effect to add dynamism in the scene for photographs or videos. Unlike the similar effects, such as defocus blur, which is now easily controlled even by smartphones, motion blur is still uncontrollable and makes undesired effects on photographs. In this paper, an unified framework to add motion blur on per-object basis is proposed. In the method, multiple frames are captured without motion blur and they are accumulated to create motion blur on target objects. To capture images without motion blur, shutter speed must be short, however, it makes captured images dark, and thus, a sensor gain should be increased to compensate it. Since a sensor gain causes a severe noise on image, we propose a color compensation algorithm based on non-linear filtering technique for solution. Another contribution is that our technique can be used to make HDR images for fast moving objects by using multi-exposure images. In the experiments, effectiveness of the method is confirmed by ablation study using several data sets.



### Hierarchical Multimodal Transformer to Summarize Videos
- **Arxiv ID**: http://arxiv.org/abs/2109.10559v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.10559v1)
- **Published**: 2021-09-22 07:38:59+00:00
- **Updated**: 2021-09-22 07:38:59+00:00
- **Authors**: Bin Zhao, Maoguo Gong, Xuelong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Although video summarization has achieved tremendous success benefiting from Recurrent Neural Networks (RNN), RNN-based methods neglect the global dependencies and multi-hop relationships among video frames, which limits the performance. Transformer is an effective model to deal with this problem, and surpasses RNN-based methods in several sequence modeling tasks, such as machine translation, video captioning, \emph{etc}. Motivated by the great success of transformer and the natural structure of video (frame-shot-video), a hierarchical transformer is developed for video summarization, which can capture the dependencies among frame and shots, and summarize the video by exploiting the scene information formed by shots. Furthermore, we argue that both the audio and visual information are essential for the video summarization task. To integrate the two kinds of information, they are encoded in a two-stream scheme, and a multimodal fusion mechanism is developed based on the hierarchical transformer. In this paper, the proposed method is denoted as Hierarchical Multimodal Transformer (HMT). Practically, extensive experiments show that HMT surpasses most of the traditional, RNN-based and attention-based video summarization methods.



### Improving 360 Monocular Depth Estimation via Non-local Dense Prediction Transformer and Joint Supervised and Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.10563v3
- **DOI**: 10.1609/aaai.v36i3.20231
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10563v3)
- **Published**: 2021-09-22 07:45:48+00:00
- **Updated**: 2021-12-30 03:35:18+00:00
- **Authors**: Ilwi Yun, Hyuk-Jae Lee, Chae Eun Rhee
- **Comment**: 14 pages, Accepted to AAAI22, conference preprint version
- **Journal**: None
- **Summary**: Due to difficulties in acquiring ground truth depth of equirectangular (360) images, the quality and quantity of equirectangular depth data today is insufficient to represent the various scenes in the world. Therefore, 360 depth estimation studies, which relied solely on supervised learning, are destined to produce unsatisfactory results. Although self-supervised learning methods focusing on equirectangular images (EIs) are introduced, they often have incorrect or non-unique solutions, causing unstable performance. In this paper, we propose 360 monocular depth estimation methods which improve on the areas that limited previous studies. First, we introduce a self-supervised 360 depth learning method that only utilizes gravity-aligned videos, which has the potential to eliminate the needs for depth data during the training procedure. Second, we propose a joint learning scheme realized by combining supervised and self-supervised learning. The weakness of each learning is compensated, thus leading to more accurate depth estimation. Third, we propose a non-local fusion block, which can further retain the global information encoded by vision transformer when reconstructing the depths. With the proposed methods, we successfully apply the transformer to 360 depth estimations, to the best of our knowledge, which has not been tried before. On several benchmarks, our approach achieves significant improvements over previous works and establishes a state of the art.



### Domain Generalization for Vision-based Driving Trajectory Generation
- **Arxiv ID**: http://arxiv.org/abs/2109.13858v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.13858v1)
- **Published**: 2021-09-22 07:49:07+00:00
- **Updated**: 2021-09-22 07:49:07+00:00
- **Authors**: Yunkai Wang, Dongkun Zhang, Yuxiang Cui, Zexi Chen, Wei Jing, Junbo Chen, Rong Xiong, Yue Wang
- **Comment**: None
- **Journal**: None
- **Summary**: One of the challenges in vision-based driving trajectory generation is dealing with out-of-distribution scenarios. In this paper, we propose a domain generalization method for vision-based driving trajectory generation for autonomous vehicles in urban environments, which can be seen as a solution to extend the Invariant Risk Minimization (IRM) method in complex problems. We leverage an adversarial learning approach to train a trajectory generator as the decoder. Based on the pre-trained decoder, we infer the latent variables corresponding to the trajectories, and pre-train the encoder by regressing the inferred latent variable. Finally, we fix the decoder but fine-tune the encoder with the final trajectory loss. We compare our proposed method with the state-of-the-art trajectory generation method and some recent domain generalization methods on both datasets and simulation, demonstrating that our method has better generalization ability.



### Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation
- **Arxiv ID**: http://arxiv.org/abs/2109.10595v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10595v2)
- **Published**: 2021-09-22 08:47:43+00:00
- **Updated**: 2021-09-24 06:26:47+00:00
- **Authors**: Yuanxun Lu, Jinxiang Chai, Xun Cao
- **Comment**: SIGGRAPH Asia 2021, 17 pages, 16 figures
- **Journal**: None
- **Summary**: To the best of our knowledge, we first present a live system that generates personalized photorealistic talking-head animation only driven by audio signals at over 30 fps. Our system contains three stages. The first stage is a deep neural network that extracts deep audio features along with a manifold projection to project the features to the target person's speech space. In the second stage, we learn facial dynamics and motions from the projected audio features. The predicted motions include head poses and upper body motions, where the former is generated by an autoregressive probabilistic model which models the head pose distribution of the target person. Upper body motions are deduced from head poses. In the final stage, we generate conditional feature maps from previous predictions and send them with a candidate image set to an image-to-image translation network to synthesize photorealistic renderings. Our method generalizes well to wild audio and successfully synthesizes high-fidelity personalized facial details, e.g., wrinkles, teeth. Our method also allows explicit control of head poses. Extensive qualitative and quantitative evaluations, along with user studies, demonstrate the superiority of our method over state-of-the-art techniques.



### Efficient Context-Aware Network for Abdominal Multi-organ Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.10601v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10601v4)
- **Published**: 2021-09-22 09:05:59+00:00
- **Updated**: 2021-10-29 07:36:43+00:00
- **Authors**: Fan Zhang, Yu Wang, Hua Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The contextual information, presented in abdominal CT scan, is relative consistent. In order to make full use of the overall 3D context, we develop a whole-volume-based coarse-to-fine framework for efficient and effective abdominal multi-organ segmentation. We propose a new efficientSegNet network, which is composed of basic encoder, slim decoder and efficient context block. For the decoder module, anisotropic convolution with a k*k*1 intra-slice convolution and a 1*1*k inter-slice convolution, is designed to reduce the computation burden. For the context block, we propose strip pooling module to capture anisotropic and long-range contextual information, which exists in abdominal scene. Quantitative evaluation on the FLARE2021 validation cases, this method achieves the average dice similarity coefficient (DSC) of 0.895 and average normalized surface distance (NSD) of 0.775. This method won the 1st place on the 2021-MICCAI-FLARE challenge. Codes and models are available at https://github.com/Shanghai-Aitrox-Technology/EfficientSegmentation.



### LDC-VAE: A Latent Distribution Consistency Approach to Variational AutoEncoders
- **Arxiv ID**: http://arxiv.org/abs/2109.10640v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2109.10640v2)
- **Published**: 2021-09-22 10:34:40+00:00
- **Updated**: 2022-05-08 07:47:39+00:00
- **Authors**: Xiaoyu Chen, Chen Gong, Qiang He, Xinwen Hou, Yu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Variational autoencoders (VAEs), as an important aspect of generative models, have received a lot of research interests and reached many successful applications. However, it is always a challenge to achieve the consistency between the learned latent distribution and the prior latent distribution when optimizing the evidence lower bound (ELBO), and finally leads to an unsatisfactory performance in data generation. In this paper, we propose a latent distribution consistency approach to avoid such substantial inconsistency between the posterior and prior latent distributions in ELBO optimizing. We name our method as latent distribution consistency VAE (LDC-VAE). We achieve this purpose by assuming the real posterior distribution in latent space as a Gibbs form, and approximating it by using our encoder. However, there is no analytical solution for such Gibbs posterior in approximation, and traditional approximation ways are time consuming, such as using the iterative sampling-based MCMC. To address this problem, we use the Stein Variational Gradient Descent (SVGD) to approximate the Gibbs posterior. Meanwhile, we use the SVGD to train a sampler net which can obtain efficient samples from the Gibbs posterior. Comparative studies on the popular image generation datasets show that our method has achieved comparable or even better performance than several powerful improvements of VAEs.



### Uncertainty-Aware Training for Cardiac Resynchronisation Therapy Response Prediction
- **Arxiv ID**: http://arxiv.org/abs/2109.10641v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10641v1)
- **Published**: 2021-09-22 10:37:50+00:00
- **Updated**: 2021-09-22 10:37:50+00:00
- **Authors**: Tareen Dawood, Chen Chen, Robin Andlauer, Baldeep S. Sidhu, Bram Ruijsink, Justin Gould, Bradley Porter, Mark Elliott, Vishal Mehta, C. Aldo Rinaldi, Esther Puyol-Antón, Reza Razavi, Andrew P. King
- **Comment**: STACOM 2021 Workshop
- **Journal**: None
- **Summary**: Evaluation of predictive deep learning (DL) models beyond conventional performance metrics has become increasingly important for applications in sensitive environments like healthcare. Such models might have the capability to encode and analyse large sets of data but they often lack comprehensive interpretability methods, preventing clinical trust in predictive outcomes. Quantifying uncertainty of a prediction is one way to provide such interpretability and promote trust. However, relatively little attention has been paid to how to include such requirements into the training of the model. In this paper we: (i) quantify the data (aleatoric) and model (epistemic) uncertainty of a DL model for Cardiac Resynchronisation Therapy response prediction from cardiac magnetic resonance images, and (ii) propose and perform a preliminary investigation of an uncertainty-aware loss function that can be used to retrain an existing DL image-based classification model to encourage confidence in correct predictions and reduce confidence in incorrect predictions. Our initial results are promising, showing a significant increase in the (epistemic) confidence of true positive predictions, with some evidence of a reduction in false negative confidence.



### Caption Enriched Samples for Improving Hateful Memes Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.10649v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.10649v1)
- **Published**: 2021-09-22 10:57:51+00:00
- **Updated**: 2021-09-22 10:57:51+00:00
- **Authors**: Efrat Blaier, Itzik Malkiel, Lior Wolf
- **Comment**: EMNLP 2021
- **Journal**: None
- **Summary**: The recently introduced hateful meme challenge demonstrates the difficulty of determining whether a meme is hateful or not. Specifically, both unimodal language models and multimodal vision-language models cannot reach the human level of performance. Motivated by the need to model the contrast between the image content and the overlayed text, we suggest applying an off-the-shelf image captioning tool in order to capture the first. We demonstrate that the incorporation of such automatic captions during fine-tuning improves the results for various unimodal and multimodal models. Moreover, in the unimodal case, continuing the pre-training of language models on augmented and original caption pairs, is highly beneficial to the classification accuracy.



### Vehicle Behavior Prediction and Generalization Using Imbalanced Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/2109.10656v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10656v1)
- **Published**: 2021-09-22 11:21:20+00:00
- **Updated**: 2021-09-22 11:21:20+00:00
- **Authors**: Theodor Westny, Erik Frisk, Björn Olofsson
- **Comment**: Accepted for 2021 IEEE 24th International Conference on Intelligent
  Transportation Systems (ITSC)
- **Journal**: None
- **Summary**: The use of learning-based methods for vehicle behavior prediction is a promising research topic. However, many publicly available data sets suffer from class distribution skews which limits learning performance if not addressed. This paper proposes an interaction-aware prediction model consisting of an LSTM autoencoder and SVM classifier. Additionally, an imbalanced learning technique, the multiclass balancing ensemble is proposed. Evaluations show that the method enhances model performance, resulting in improved classification accuracy. Good generalization properties of learned models are important and therefore a generalization study is done where models are evaluated on unseen traffic data with dissimilar traffic behavior stemming from different road configurations. This is realized by using two distinct highway traffic recordings, the publicly available NGSIM US-101 and I80 data sets. Moreover, methods for encoding structural and static features into the learning process for improved generalization are evaluated. The resulting methods show substantial improvements in classification as well as generalization performance.



### TACTIC: Joint Rate-Distortion-Accuracy Optimisation for Low Bitrate Compression
- **Arxiv ID**: http://arxiv.org/abs/2109.10658v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, eess.IV, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2109.10658v1)
- **Published**: 2021-09-22 11:27:57+00:00
- **Updated**: 2021-09-22 11:27:57+00:00
- **Authors**: Nikolina Kubiak, Simon Hadfield
- **Comment**: None
- **Journal**: None
- **Summary**: We present TACTIC: Task-Aware Compression Through Intelligent Coding. Our lossy compression model learns based on the rate-distortion-accuracy trade-off for a specific task. By considering what information is important for the follow-on problem, the system trades off visual fidelity for good task performance at a low bitrate. When compared against JPEG at the same bitrate, our approach is able to improve the accuracy of ImageNet subset classification by 4.5%. We also demonstrate the applicability of our approach to other problems, providing a 3.4% accuracy and 4.9% mean IoU improvements in performance over task-agnostic compression for semantic segmentation.



### A deep neural network for multi-species fish detection using multiple acoustic cameras
- **Arxiv ID**: http://arxiv.org/abs/2109.10664v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10664v1)
- **Published**: 2021-09-22 11:47:24+00:00
- **Updated**: 2021-09-22 11:47:24+00:00
- **Authors**: Guglielmo Fernandez Garcia, François Martignac, Marie Nevoux, Laurent Beaulaton, Thomas Corpetti
- **Comment**: None
- **Journal**: None
- **Summary**: Underwater acoustic cameras are high potential devices for many applications in ecology, notably for fisheries management and monitoring. However how to extract such data into high value information without a time-consuming entire dataset reading by an operator is still a challenge. Moreover the analysis of acoustic imaging, due to its low signal-to-noise ratio, is a perfect training ground for experimenting with new approaches, especially concerning Deep Learning techniques. We present hereby a novel approach that takes advantage of both CNN (Convolutional Neural Network) and classical CV (Computer Vision) techniques, able to detect a generic class ''fish'' in acoustic video streams. The pipeline pre-treats the acoustic images to extract 2 features, in order to localise the signals and improve the detection performances. To ensure the performances from an ecological point of view, we propose also a two-step validation, one to validate the results of the trainings and one to test the method on a real-world scenario. The YOLOv3-based model was trained with data of fish from multiple species recorded by the two common acoustic cameras, DIDSON and ARIS, including species of high ecological interest, as Atlantic salmon or European eels. The model we developed provides satisfying results detecting almost 80% of fish and minimizing the false positive rate, however the model is much less efficient for eel detections on ARIS videos. The first CNN pipeline for fish monitoring exploiting video data from two models of acoustic cameras satisfies most of the required features. Many challenges are still present, such as the automation of fish species identification through a multiclass model. 1 However the results point a new solution for dealing with complex data, such as sonar data, which can also be reapplied in other cases where the signal-to-noise ratio is a challenge.



### Self-Training Based Unsupervised Cross-Modality Domain Adaptation for Vestibular Schwannoma and Cochlea Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.10674v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10674v1)
- **Published**: 2021-09-22 12:04:41+00:00
- **Updated**: 2021-09-22 12:04:41+00:00
- **Authors**: Hyungseob Shin, Hyeongyu Kim, Sewon Kim, Yohan Jun, Taejoon Eo, Dosik Hwang
- **Comment**: 6 pages, 5 figures, MICCAI 2021 Cross-Modality Domain Adaptation for
  Medical Image Segmentation Challenge
- **Journal**: None
- **Summary**: With the advances of deep learning, many medical image segmentation studies achieve human-level performance when in fully supervised condition. However, it is extremely expensive to acquire annotation on every data in medical fields, especially on magnetic resonance images (MRI) that comprise many different contrasts. Unsupervised methods can alleviate this problem; however, the performance drop is inevitable compared to fully supervised methods. In this work, we propose a self-training based unsupervised-learning framework that performs automatic segmentation of Vestibular Schwannoma (VS) and cochlea on high-resolution T2 scans. Our method consists of 4 main stages: 1) VS-preserving contrast conversion from contrast-enhanced T1 scan to high-resolution T2 scan, 2) training segmentation on generated T2 scans with annotations on T1 scans, and 3) Inferring pseudo-labels on non-annotated real T2 scans, and 4) boosting the generalizability of VS and cochlea segmentation by training with combined data (i.e., real T2 scans with pseudo-labels and generated T2 scans with true annotations). Our method showed mean Dice score and Average Symmetric Surface Distance (ASSD) of 0.8570 (0.0705) and 0.4970 (0.3391) for VS, 0.8446 (0.0211) and 0.1513 (0.0314) for Cochlea on CrossMoDA2021 challenge validation phase leaderboard, outperforming most other approaches.



### Natural Language Video Localization with Learnable Moment Proposals
- **Arxiv ID**: http://arxiv.org/abs/2109.10678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10678v1)
- **Published**: 2021-09-22 12:18:58+00:00
- **Updated**: 2021-09-22 12:18:58+00:00
- **Authors**: Shaoning Xiao, Long Chen, Jian Shao, Yueting Zhuang, Jun Xiao
- **Comment**: emnlp21. arXiv admin note: text overlap with arXiv:2103.08109
- **Journal**: None
- **Summary**: Given an untrimmed video and a natural language query, Natural Language Video Localization (NLVL) aims to identify the video moment described by the query. To address this task, existing methods can be roughly grouped into two groups: 1) propose-and-rank models first define a set of hand-designed moment candidates and then find out the best-matching one. 2) proposal-free models directly predict two temporal boundaries of the referential moment from frames. Currently, almost all the propose-and-rank methods have inferior performance than proposal-free counterparts. In this paper, we argue that propose-and-rank approach is underestimated due to the predefined manners: 1) Hand-designed rules are hard to guarantee the complete coverage of targeted segments. 2) Densely sampled candidate moments cause redundant computation and degrade the performance of ranking process. To this end, we propose a novel model termed LPNet (Learnable Proposal Network for NLVL) with a fixed set of learnable moment proposals. The position and length of these proposals are dynamically adjusted during training process. Moreover, a boundary-aware loss has been proposed to leverage frame-level information and further improve the performance. Extensive ablations on two challenging NLVL benchmarks have demonstrated the effectiveness of LPNet over existing state-of-the-art methods.



### Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers
- **Arxiv ID**: http://arxiv.org/abs/2109.10686v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10686v2)
- **Published**: 2021-09-22 12:29:15+00:00
- **Updated**: 2022-01-30 16:42:46+00:00
- **Authors**: Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, Donald Metzler
- **Comment**: ICLR 2022 + Updated Checkpoint Release
- **Journal**: None
- **Summary**: There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both financial and/or environmental impact. The goal of this paper is to present scaling insights from pretraining and finetuning Transformers. While Kaplan et al. presents a comprehensive study of the scaling behaviour of Transformer language models, the scope is only on the upstream (pretraining) loss. Therefore, it is still unclear if these set of findings transfer to downstream task within the context of the pretrain-finetune paradigm. The key findings of this paper are as follows: (1) we show that aside from only the model size, model shape matters for downstream fine-tuning, (2) scaling protocols operate differently at different compute regions, (3) widely adopted T5-base and T5-large sizes are Pareto-inefficient. To this end, we present improved scaling protocols whereby our redesigned models achieve similar downstream fine-tuning quality while having 50\% fewer parameters and training 40\% faster compared to the widely adopted T5-base model. We publicly release over 100 pretrained checkpoints of different T5 configurations to facilitate future research and analysis.



### Differentiable Surface Triangulation
- **Arxiv ID**: http://arxiv.org/abs/2109.10695v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2109.10695v1)
- **Published**: 2021-09-22 12:42:43+00:00
- **Updated**: 2021-09-22 12:42:43+00:00
- **Authors**: Marie-Julie Rakotosaona, Noam Aigerman, Niloy Mitra, Maks Ovsjanikov, Paul Guerrero
- **Comment**: None
- **Journal**: None
- **Summary**: Triangle meshes remain the most popular data representation for surface geometry. This ubiquitous representation is essentially a hybrid one that decouples continuous vertex locations from the discrete topological triangulation. Unfortunately, the combinatorial nature of the triangulation prevents taking derivatives over the space of possible meshings of any given surface. As a result, to date, mesh processing and optimization techniques have been unable to truly take advantage of modular gradient descent components of modern optimization frameworks. In this work, we present a differentiable surface triangulation that enables optimization for any per-vertex or per-face differentiable objective function over the space of underlying surface triangulations. Our method builds on the result that any 2D triangulation can be achieved by a suitably perturbed weighted Delaunay triangulation. We translate this result into a computational algorithm by proposing a soft relaxation of the classical weighted Delaunay triangulation and optimizing over vertex weights and vertex locations. We extend the algorithm to 3D by decomposing shapes into developable sets and differentiably meshing each set with suitable boundary constraints. We demonstrate the efficacy of our method on various planar and surface meshes on a range of difficult-to-optimize objective functions. Our code can be found online: https://github.com/mrakotosaon/diff-surface-triangulation.



### A Quantitative Comparison of Epistemic Uncertainty Maps Applied to Multi-Class Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.10702v1
- **DOI**: 10.59275/j.melba.2021-ec49
- **Categories**: **cs.CV**, eess.IV, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2109.10702v1)
- **Published**: 2021-09-22 12:48:19+00:00
- **Updated**: 2021-09-22 12:48:19+00:00
- **Authors**: Robin Camarasa, Daniel Bos, Jeroen Hendrikse, Paul Nederkoorn, M. Eline Kooi, Aad van der Lugt, Marleen de Bruijne
- **Comment**: 39 pages, 22 figures, to be published in Journal of Machine Learning
  for Biomedical Imaging for the Special Issue: Uncertainty for Safe
  Utilization of Machine Learning in Medical Imaging (UNSURE) 2020
- **Journal**: None
- **Summary**: Uncertainty assessment has gained rapid interest in medical image analysis. A popular technique to compute epistemic uncertainty is the Monte-Carlo (MC) dropout technique. From a network with MC dropout and a single input, multiple outputs can be sampled. Various methods can be used to obtain epistemic uncertainty maps from those multiple outputs. In the case of multi-class segmentation, the number of methods is even larger as epistemic uncertainty can be computed voxelwise per class or voxelwise per image. This paper highlights a systematic approach to define and quantitatively compare those methods in two different contexts: class-specific epistemic uncertainty maps (one value per image, voxel and class) and combined epistemic uncertainty maps (one value per image and voxel). We applied this quantitative analysis to a multi-class segmentation of the carotid artery lumen and vessel wall, on a multi-center, multi-scanner, multi-sequence dataset of (MR) images. We validated our analysis over 144 sets of hyperparameters of a model. Our main analysis considers the relationship between the order of the voxels sorted according to their epistemic uncertainty values and the misclassification of the prediction. Under this consideration, the comparison of combined uncertainty maps reveals that the multi-class entropy and the multi-class mutual information statistically out-perform the other combined uncertainty maps under study. In a class-specific scenario, the one-versus-all entropy statistically out-performs the class-wise entropy, the class-wise variance and the one versus all mutual information. The class-wise entropy statistically out-performs the other class-specific uncertainty maps in terms of calibration. We made a python package available to reproduce our analysis on different data and tasks.



### Animal inspired Application of a Variant of Mel Spectrogram for Seismic Data Processing
- **Arxiv ID**: http://arxiv.org/abs/2109.10733v1
- **DOI**: 10.13140/RG.2.2.30040.42247
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10733v1)
- **Published**: 2021-09-22 13:41:57+00:00
- **Updated**: 2021-09-22 13:41:57+00:00
- **Authors**: Samayan Bhattacharya, Sk Shahnawaz
- **Comment**: 6 pages, 5 figures, 1 table
- **Journal**: None
- **Summary**: Predicting disaster events from seismic data is of paramount importance and can save thousands of lives, especially in earthquake-prone areas and habitations around volcanic craters. The drastic rise in the number of seismic monitoring stations in recent years has allowed the collection of a huge quantity of data, outpacing the capacity of seismologists. Due to the complex nature of the seismological data, it is often difficult for seismologists to detect subtle patterns with major implications. Machine learning algorithms have been demonstrated to be effective in classification and prediction tasks for seismic data. It has been widely known that some animals can sense disasters like earthquakes from seismic signals well before the disaster strikes. Mel spectrogram has been widely used for speech recognition as it scales the actual frequencies according to human hearing. In this paper, we propose a variant of the Mel spectrogram to scale the raw frequencies of seismic data to the hearing of such animals that can sense disasters from seismic signals. We are using a Computer vision algorithm along with clustering that allows for the classification of unlabelled seismic data.



### DyStyle: Dynamic Neural Network for Multi-Attribute-Conditioned Style Editing
- **Arxiv ID**: http://arxiv.org/abs/2109.10737v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10737v3)
- **Published**: 2021-09-22 13:50:51+00:00
- **Updated**: 2022-09-28 08:25:03+00:00
- **Authors**: Bingchuan Li, Shaofei Cai, Wei Liu, Peng Zhang, Qian He, Miao Hua, Zili Yi
- **Comment**: Accepted to WACV 2023, 19 pages, 20 figures
- **Journal**: None
- **Summary**: The semantic controllability of StyleGAN is enhanced by unremitting research. Although the existing weak supervision methods work well in manipulating the style codes along one attribute, the accuracy of manipulating multiple attributes is neglected. Multi-attribute representations are prone to entanglement in the StyleGAN latent space, while sequential editing leads to error accumulation. To address these limitations, we design a Dynamic Style Manipulation Network (DyStyle) whose structure and parameters vary by input samples, to perform nonlinear and adaptive manipulation of latent codes for flexible and precise attribute control. In order to efficient and stable optimization of the DyStyle network, we propose a Dynamic Multi-Attribute Contrastive Learning (DmaCL) method: including dynamic multi-attribute contrastor and dynamic multi-attribute contrastive loss, which simultaneously disentangle a variety of attributes from the generative image and latent space of model. As a result, our approach demonstrates fine-grained disentangled edits along multiple numeric and binary attributes. Qualitative and quantitative comparisons with existing style manipulation methods verify the superiority of our method in terms of the multi-attribute control accuracy and identity preservation without compromising photorealism.



### Early Lane Change Prediction for Automated Driving Systems Using Multi-Task Attention-based Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.10742v2
- **DOI**: 10.1109/TIV.2022.3161785
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10742v2)
- **Published**: 2021-09-22 13:59:27+00:00
- **Updated**: 2022-03-29 13:53:31+00:00
- **Authors**: Sajjad Mozaffari, Eduardo Arnold, Mehrdad Dianati, Saber Fallah
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: Lane change (LC) is one of the safety-critical manoeuvres in highway driving according to various road accident records. Thus, reliably predicting such manoeuvre in advance is critical for the safe and comfortable operation of automated driving systems. The majority of previous studies rely on detecting a manoeuvre that has been already started, rather than predicting the manoeuvre in advance. Furthermore, most of the previous works do not estimate the key timings of the manoeuvre (e.g., crossing time), which can actually yield more useful information for the decision making in the ego vehicle. To address these shortcomings, this paper proposes a novel multi-task model to simultaneously estimate the likelihood of LC manoeuvres and the time-to-lane-change (TTLC). In both tasks, an attention-based convolutional neural network (CNN) is used as a shared feature extractor from a bird's eye view representation of the driving environment. The spatial attention used in the CNN model improves the feature extraction process by focusing on the most relevant areas of the surrounding environment. In addition, two novel curriculum learning schemes are employed to train the proposed approach. The extensive evaluation and comparative analysis of the proposed method in existing benchmark datasets show that the proposed method outperforms state-of-the-art LC prediction models, particularly considering long-term prediction performance.



### FaceEraser: Removing Facial Parts for Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/2109.10760v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10760v2)
- **Published**: 2021-09-22 14:30:12+00:00
- **Updated**: 2021-10-22 05:02:58+00:00
- **Authors**: Miao Hua, Lijie Liu, Ziyang Cheng, Qian He, Bingchuan Li, Zili Yi
- **Comment**: 18 pages, 15 figures. ICCV 2021, Fifth Workshop on Computer Vision
  for AR/VR
- **Journal**: None
- **Summary**: Our task is to remove all facial parts (e.g., eyebrows, eyes, mouth and nose), and then impose visual elements onto the ``blank'' face for augmented reality. Conventional object removal methods rely on image inpainting techniques (e.g., EdgeConnect, HiFill) that are trained in a self-supervised manner with randomly manipulated image pairs. Specifically, given a set of natural images, randomly masked images are used as inputs and the raw images are treated as ground truths. Whereas, this technique does not satisfy the requirements of facial parts removal, as it is hard to obtain ``ground-truth'' images with real ``blank'' faces. To address this issue, we propose a novel data generation technique to produce paired training data that well mimic the ``blank'' faces. In the mean time, we propose a novel network architecture for improved inpainting quality for our task. Finally, we demonstrate various face-oriented augmented reality applications on top of our facial parts removal model. The source codes are released at \href{https://github.com/duxingren14/FaceEraser}{duxingren14/FaceEraser} on github for research purposes.



### HybridSDF: Combining Deep Implicit Shapes and Geometric Primitives for 3D Shape Representation and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2109.10767v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10767v4)
- **Published**: 2021-09-22 14:45:19+00:00
- **Updated**: 2022-09-08 09:31:07+00:00
- **Authors**: Subeesh Vasu, Nicolas Talabot, Artem Lukoianov, Pierre Baqué, Jonathan Donier, Pascal Fua
- **Comment**: 18 pages, 21 figures, 3DV 2022
- **Journal**: None
- **Summary**: Deep implicit surfaces excel at modeling generic shapes but do not always capture the regularities present in manufactured objects, which is something simple geometric primitives are particularly good at. In this paper, we propose a representation combining latent and explicit parameters that can be decoded into a set of deep implicit and geometric shapes that are consistent with each other. As a result, we can effectively model both complex and highly regular shapes that coexist in manufactured objects. This enables our approach to manipulate 3D shapes in an efficient and precise manner.



### Deep Variational Clustering Framework for Self-labeling of Large-scale Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2109.10777v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10777v1)
- **Published**: 2021-09-22 15:01:52+00:00
- **Updated**: 2021-09-22 15:01:52+00:00
- **Authors**: Farzin Soleymani, Mohammad Eslami, Tobias Elze, Bernd Bischl, Mina Rezaei
- **Comment**: arXiv admin note: text overlap with arXiv:2109.05232
- **Journal**: None
- **Summary**: We propose a Deep Variational Clustering (DVC) framework for unsupervised representation learning and clustering of large-scale medical images. DVC simultaneously learns the multivariate Gaussian posterior through the probabilistic convolutional encoder and the likelihood distribution with the probabilistic convolutional decoder; and optimizes cluster labels assignment. Here, the learned multivariate Gaussian posterior captures the latent distribution of a large set of unlabeled images. Then, we perform unsupervised clustering on top of the variational latent space using a clustering loss. In this approach, the probabilistic decoder helps to prevent the distortion of data points in the latent space and to preserve the local structure of data generating distribution. The training process can be considered as a self-training process to refine the latent space and simultaneously optimizing cluster assignments iteratively. We evaluated our proposed framework on three public datasets that represented different medical imaging modalities. Our experimental results show that our proposed framework generalizes better across different datasets. It achieves compelling results on several medical imaging benchmarks. Thus, our approach offers potential advantages over conventional deep unsupervised learning in real-world applications. The source code of the method and all the experiments are available publicly at: https://github.com/csfarzin/DVC



### Label Cleaning Multiple Instance Learning: Refining Coarse Annotations on Single Whole-Slide Images
- **Arxiv ID**: http://arxiv.org/abs/2109.10778v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10778v2)
- **Published**: 2021-09-22 15:06:06+00:00
- **Updated**: 2022-06-07 22:16:48+00:00
- **Authors**: Zhenzhen Wang, Carla Saoud, Sintawat Wangsiricharoen, Aaron W. James, Aleksander S. Popel, Jeremias Sulam
- **Comment**: None
- **Journal**: None
- **Summary**: Annotating cancerous regions in whole-slide images (WSIs) of pathology samples plays a critical role in clinical diagnosis, biomedical research, and machine learning algorithms development. However, generating exhaustive and accurate annotations is labor-intensive, challenging, and costly. Drawing only coarse and approximate annotations is a much easier task, less costly, and it alleviates pathologists' workload. In this paper, we study the problem of refining these approximate annotations in digital pathology to obtain more accurate ones. Some previous works have explored obtaining machine learning models from these inaccurate annotations, but few of them tackle the refinement problem where the mislabeled regions should be explicitly identified and corrected, and all of them require a -- often very large -- number of training samples. We present a method, named Label Cleaning Multiple Instance Learning (LC-MIL), to refine coarse annotations on a single WSI without the need of external training data. Patches cropped from a WSI with inaccurate labels are processed jointly within a multiple instance learning framework, mitigating their impact on the predictive model and refining the segmentation. Our experiments on a heterogeneous WSI set with breast cancer lymph node metastasis, liver cancer, and colorectal cancer samples show that LC-MIL significantly refines the coarse annotations, outperforming state-of-the-art alternatives, even while learning from a single slide. Moreover, we demonstrate how real annotations drawn by pathologists can be efficiently refined and improved by the proposed approach. All these results demonstrate that LC-MIL is a promising, light-weight tool to provide fine-grained annotations from coarsely annotated pathology sets.



### Neural network relief: a pruning algorithm based on neural activity
- **Arxiv ID**: http://arxiv.org/abs/2109.10795v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10795v2)
- **Published**: 2021-09-22 15:33:49+00:00
- **Updated**: 2022-06-27 13:02:20+00:00
- **Authors**: Aleksandr Dekhovich, David M. J. Tax, Marcel H. F. Sluiter, Miguel A. Bessa
- **Comment**: None
- **Journal**: None
- **Summary**: Current deep neural networks (DNNs) are overparameterized and use most of their neuronal connections during inference for each task. The human brain, however, developed specialized regions for different tasks and performs inference with a small fraction of its neuronal connections. We propose an iterative pruning strategy introducing a simple importance-score metric that deactivates unimportant connections, tackling overparameterization in DNNs and modulating the firing patterns. The aim is to find the smallest number of connections that is still capable of solving a given task with comparable accuracy, i.e. a simpler subnetwork. We achieve comparable performance for LeNet architectures on MNIST, and significantly higher parameter compression than state-of-the-art algorithms for VGG and ResNet architectures on CIFAR-10/100 and Tiny-ImageNet. Our approach also performs well for the two different optimizers considered -- Adam and SGD. The algorithm is not designed to minimize FLOPs when considering current hardware and software implementations, although it performs reasonably when compared to the state of the art.



### KOHTD: Kazakh Offline Handwritten Text Dataset
- **Arxiv ID**: http://arxiv.org/abs/2110.04075v1
- **DOI**: 10.1016/j.image.2022.116827
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.04075v1)
- **Published**: 2021-09-22 16:19:38+00:00
- **Updated**: 2021-09-22 16:19:38+00:00
- **Authors**: Nazgul Toiganbayeva, Mahmoud Kasem, Galymzhan Abdimanap, Kairat Bostanbekov, Abdelrahman Abdallah, Anel Alimova, Daniyar Nurseitov
- **Comment**: None
- **Journal**: Signal Processing: Image Communication, Volume 108, October 2022
- **Summary**: Despite the transition to digital information exchange, many documents, such as invoices, taxes, memos and questionnaires, historical data, and answers to exam questions, still require handwritten inputs. In this regard, there is a need to implement Handwritten Text Recognition (HTR) which is an automatic way to decrypt records using a computer. Handwriting recognition is challenging because of the virtually infinite number of ways a person can write the same message. For this proposal we introduce Kazakh handwritten text recognition research, a comprehensive dataset of Kazakh handwritten texts is necessary. This is particularly true given the lack of a dataset for handwritten Kazakh text. In this paper, we proposed our extensive Kazakh offline Handwritten Text dataset (KOHTD), which has 3000 handwritten exam papers and more than 140335 segmented images and there are approximately 922010 symbols. It can serve researchers in the field of handwriting recognition tasks by using deep and machine learning. We used a variety of popular text recognition methods for word and line recognition in our studies, including CTC-based and attention-based methods. The findings demonstrate KOHTD's diversity. Also, we proposed a Genetic Algorithm (GA) for line and word segmentation based on random enumeration of a parameter. The dataset and GA code are available at https://github.com/abdoelsayed2016/KOHTD.



### DVC-P: Deep Video Compression with Perceptual Optimizations
- **Arxiv ID**: http://arxiv.org/abs/2109.10849v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2109.10849v2)
- **Published**: 2021-09-22 17:20:13+00:00
- **Updated**: 2021-10-08 16:10:48+00:00
- **Authors**: Saiping Zhang, Marta Mrak, Luis Herranz, Marc Górriz, Shuai Wan, Fuzheng Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have witnessed the significant development of learning-based video compression methods, which aim at optimizing objective or perceptual quality and bit rates. In this paper, we introduce deep video compression with perceptual optimizations (DVC-P), which aims at increasing perceptual quality of decoded videos. Our proposed DVC-P is based on Deep Video Compression (DVC) network, but improves it with perceptual optimizations. Specifically, a discriminator network and a mixed loss are employed to help our network trade off among distortion, perception and rate. Furthermore, nearest-neighbor interpolation is used to eliminate checkerboard artifacts which can appear in sequences encoded with DVC frameworks. Thanks to these two improvements, the perceptual quality of decoded sequences is improved. Experimental results demonstrate that, compared with the baseline DVC, our proposed method can generate videos with higher perceptual quality achieving 12.27% reduction in a perceptual BD-rate equivalent, on average.



### Pix2seq: A Language Modeling Framework for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.10852v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10852v2)
- **Published**: 2021-09-22 17:26:36+00:00
- **Updated**: 2022-03-27 14:44:00+00:00
- **Authors**: Ting Chen, Saurabh Saxena, Lala Li, David J. Fleet, Geoffrey Hinton
- **Comment**: ICLR'22. Code and pretrained models at
  https://github.com/google-research/pix2seq
- **Journal**: None
- **Summary**: We present Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.



### The CAMELS Multifield Dataset: Learning the Universe's Fundamental Parameters with Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2109.10915v1
- **DOI**: 10.3847/1538-4365/ac5ab0
- **Categories**: **cs.LG**, astro-ph.CO, astro-ph.GA, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10915v1)
- **Published**: 2021-09-22 18:00:01+00:00
- **Updated**: 2021-09-22 18:00:01+00:00
- **Authors**: Francisco Villaescusa-Navarro, Shy Genel, Daniel Angles-Alcazar, Leander Thiele, Romeel Dave, Desika Narayanan, Andrina Nicola, Yin Li, Pablo Villanueva-Domingo, Benjamin Wandelt, David N. Spergel, Rachel S. Somerville, Jose Manuel Zorrilla Matilla, Faizan G. Mohammad, Sultan Hassan, Helen Shao, Digvijay Wadekar, Michael Eickenberg, Kaze W. K. Wong, Gabriella Contardo, Yongseok Jo, Emily Moser, Erwin T. Lau, Luis Fernando Machado Poletti Valle, Lucia A. Perez, Daisuke Nagai, Nicholas Battaglia, Mark Vogelsberger
- **Comment**: 17 pages, 1 figure. Third paper of a series of four. Hundreds of
  thousands of labeled 2D maps and 3D grids from thousands of simulated
  universes publicly available at
  https://camels-multifield-dataset.readthedocs.io
- **Journal**: None
- **Summary**: We present the Cosmology and Astrophysics with MachinE Learning Simulations (CAMELS) Multifield Dataset, CMD, a collection of hundreds of thousands of 2D maps and 3D grids containing many different properties of cosmic gas, dark matter, and stars from 2,000 distinct simulated universes at several cosmic times. The 2D maps and 3D grids represent cosmic regions that span $\sim$100 million light years and have been generated from thousands of state-of-the-art hydrodynamic and gravity-only N-body simulations from the CAMELS project. Designed to train machine learning models, CMD is the largest dataset of its kind containing more than 70 Terabytes of data. In this paper we describe CMD in detail and outline a few of its applications. We focus our attention on one such task, parameter inference, formulating the problems we face as a challenge to the community. We release all data and provide further technical details at https://camels-multifield-dataset.readthedocs.io.



### T6D-Direct: Transformers for Multi-Object 6D Pose Direct Regression
- **Arxiv ID**: http://arxiv.org/abs/2109.10948v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10948v1)
- **Published**: 2021-09-22 18:13:33+00:00
- **Updated**: 2021-09-22 18:13:33+00:00
- **Authors**: Arash Amini, Arul Selvam Periyasamy, Sven Behnke
- **Comment**: Accepted for DAGM German Conference on Pattern Recognition (GCPR),
  Bonn, Germany, September 2021
- **Journal**: None
- **Summary**: 6D pose estimation is the task of predicting the translation and orientation of objects in a given input image, which is a crucial prerequisite for many robotics and augmented reality applications. Lately, the Transformer Network architecture, equipped with a multi-head self-attention mechanism, is emerging to achieve state-of-the-art results in many computer vision tasks. DETR, a Transformer-based model, formulated object detection as a set prediction problem and achieved impressive results without standard components like region of interest pooling, non-maximal suppression, and bounding box proposals. In this work, we propose T6D-Direct, a real-time single-stage direct method with a transformer-based architecture built on DETR to perform 6D multi-object pose direct estimation. We evaluate the performance of our method on the YCB-Video dataset. Our method achieves the fastest inference time, and the pose estimation accuracy is comparable to state-of-the-art methods.



### Learning Contrastive Representation for Semantic Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2109.10967v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10967v2)
- **Published**: 2021-09-22 18:34:14+00:00
- **Updated**: 2022-03-10 01:48:15+00:00
- **Authors**: Taihong Xiao, Sifei Liu, Shalini De Mello, Zhiding Yu, Jan Kautz, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: International Journal of Computer Vision 2022
- **Summary**: Dense correspondence across semantically related images has been extensively studied, but still faces two challenges: 1) large variations in appearance, scale and pose exist even for objects from the same category, and 2) labeling pixel-level dense correspondences is labor intensive and infeasible to scale. Most existing approaches focus on designing various matching approaches with fully-supervised ImageNet pretrained networks. On the other hand, while a variety of self-supervised approaches are proposed to explicitly measure image-level similarities, correspondence matching the pixel level remains under-explored. In this work, we propose a multi-level contrastive learning approach for semantic matching, which does not rely on any ImageNet pretrained model. We show that image-level contrastive learning is a key component to encourage the convolutional features to find correspondence between similar objects, while the performance can be further enhanced by regularizing cross-instance cycle-consistency at intermediate feature levels. Experimental results on the PF-PASCAL, PF-WILLOW, and SPair-71k benchmark datasets demonstrate that our method performs favorably against the state-of-the-art approaches. The source code and trained models will be made available to the public.



### An Efficient and Scalable Collection of Fly-inspired Voting Units for Visual Place Recognition in Changing Environments
- **Arxiv ID**: http://arxiv.org/abs/2109.10986v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10986v1)
- **Published**: 2021-09-22 19:01:20+00:00
- **Updated**: 2021-09-22 19:01:20+00:00
- **Authors**: Bruno Arcanjo, Bruno Ferrarini, Michael Milford, Klaus D. McDonald-Maier, Shoaib Ehsan
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art visual place recognition performance is currently being achieved utilizing deep learning based approaches. Despite the recent efforts in designing lightweight convolutional neural network based models, these can still be too expensive for the most hardware restricted robot applications. Low-overhead VPR techniques would not only enable platforms equipped with low-end, cheap hardware but also reduce computation on more powerful systems, allowing these resources to be allocated for other navigation tasks. In this work, our goal is to provide an algorithm of extreme compactness and efficiency while achieving state-of-the-art robustness to appearance changes and small point-of-view variations. Our first contribution is DrosoNet, an exceptionally compact model inspired by the odor processing abilities of the fruit fly, Drosophyla melanogaster. Our second and main contribution is a voting mechanism that leverages multiple small and efficient classifiers to achieve more robust and consistent VPR compared to a single one. We use DrosoNet as the baseline classifier for the voting mechanism and evaluate our models on five benchmark datasets, assessing moderate to extreme appearance changes and small to moderate viewpoint variations. We then compare the proposed algorithms to state-of-the-art methods, both in terms of precision-recall AUC results and computational efficiency.



### A Benchmark Comparison of Visual Place Recognition Techniques for Resource-Constrained Embedded Platforms
- **Arxiv ID**: http://arxiv.org/abs/2109.11002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.11002v1)
- **Published**: 2021-09-22 19:45:57+00:00
- **Updated**: 2021-09-22 19:45:57+00:00
- **Authors**: Rose Power, Mubariz Zaffar, Bruno Ferrarini, Michael Milford, Klaus McDonald-Maier, Shoaib Ehsan
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Place Recognition (VPR) has been a subject of significant research over the last 15 to 20 years. VPR is a fundamental task for autonomous navigation as it enables self-localization within an environment. Although robots are often equipped with resource-constrained hardware, the computational requirements of and effects on VPR techniques have received little attention. In this work, we present a hardware-focused benchmark evaluation of a number of state-of-the-art VPR techniques on public datasets. We consider popular single board computers, including ODroid, UP and Raspberry Pi 3, in addition to a commodity desktop and laptop for reference. We present our analysis based on several key metrics, including place-matching accuracy, image encoding time, descriptor matching time and memory needs. Key questions addressed include: (1) How does the performance accuracy of a VPR technique change with processor architecture? (2) How does power consumption vary for different VPR techniques and embedded platforms? (3) How much does descriptor size matter in comparison to today's embedded platforms' storage? (4) How does the performance of a high-end platform relate to an on-board low-end embedded platform for VPR? The extensive analysis and results in this work serve not only as a benchmark for the VPR community, but also provide useful insights for real-world adoption of VPR applications.



### The Impact of Domain Shift on Left and Right Ventricle Segmentation in Short Axis Cardiac MR Images
- **Arxiv ID**: http://arxiv.org/abs/2109.13230v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.13230v1)
- **Published**: 2021-09-22 19:55:15+00:00
- **Updated**: 2021-09-22 19:55:15+00:00
- **Authors**: Devran Ugurlu, Esther Puyol-Anton, Bram Ruijsink, Alistair Young, Ines Machado, Kerstin Hammernik, Andrew P. King, Julia A. Schnabel
- **Comment**: Accepted to STACOM 2021
- **Journal**: None
- **Summary**: Domain shift refers to the difference in the data distribution of two datasets, normally between the training set and the test set for machine learning algorithms. Domain shift is a serious problem for generalization of machine learning models and it is well-established that a domain shift between the training and test sets may cause a drastic drop in the model's performance. In medical imaging, there can be many sources of domain shift such as different scanners or scan protocols, different pathologies in the patient population, anatomical differences in the patient population (e.g. men vs women) etc. Therefore, in order to train models that have good generalization performance, it is important to be aware of the domain shift problem, its potential causes and to devise ways to address it. In this paper, we study the effect of domain shift on left and right ventricle blood pool segmentation in short axis cardiac MR images. Our dataset contains short axis images from 4 different MR scanners and 3 different pathology groups. The training is performed with nnUNet. The results show that scanner differences cause a greater drop in performance compared to changing the pathology group, and that the impact of domain shift is greater on right ventricle segmentation compared to left ventricle segmentation. Increasing the number of training subjects increased cross-scanner performance more than in-scanner performance at small training set sizes, but this difference in improvement decreased with larger training set sizes. Training models using data from multiple scanners improved cross-domain performance.



### Cross-Modal Coherence for Text-to-Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2109.11047v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.11047v2)
- **Published**: 2021-09-22 21:31:27+00:00
- **Updated**: 2022-04-15 13:30:20+00:00
- **Authors**: Malihe Alikhani, Fangda Han, Hareesh Ravi, Mubbasir Kapadia, Vladimir Pavlovic, Matthew Stone
- **Comment**: This paper is published in AAAI-2022
- **Journal**: None
- **Summary**: Common image-text joint understanding techniques presume that images and the associated text can universally be characterized by a single implicit model. However, co-occurring images and text can be related in qualitatively different ways, and explicitly modeling it could improve the performance of current joint understanding models. In this paper, we train a Cross-Modal Coherence Modelfor text-to-image retrieval task. Our analysis shows that models trained with image--text coherence relations can retrieve images originally paired with target text more often than coherence-agnostic models. We also show via human evaluation that images retrieved by the proposed coherence-aware model are preferred over a coherence-agnostic baseline by a huge margin. Our findings provide insights into the ways that different modalities communicate and the role of coherence relations in capturing commonsense inferences in text and imagery.



### Towards practical object detection for weed spraying in precision agriculture
- **Arxiv ID**: http://arxiv.org/abs/2109.11048v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.11048v1)
- **Published**: 2021-09-22 21:32:39+00:00
- **Updated**: 2021-09-22 21:32:39+00:00
- **Authors**: Adrian Salazar-Gomez, Madeleine Darbyshire, Junfeng Gao, Elizabeth I Sklar, Simon Parsons
- **Comment**: 7 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: The evolution of smaller, faster processors and cheaper digital storage mechanisms across the last 4-5 decades has vastly increased the opportunity to integrate intelligent technologies in a wide range of practical environments to address a broad spectrum of tasks. One exciting application domain for such technologies is precision agriculture, where the ability to integrate on-board machine vision with data-driven actuation means that farmers can make decisions about crop care and harvesting at the level of the individual plant rather than the whole field. This makes sense both economically and environmentally. However, the key driver for this capability is fast and robust machine vision -- typically driven by machine learning (ML) solutions and dependent on accurate modelling. One critical challenge is that the bulk of ML-based vision research considers only metrics that evaluate the accuracy of object detection and do not assess practical factors. This paper introduces three metrics that highlight different aspects relevant for real-world deployment of precision weeding and demonstrates their utility through experimental results.



### Learning to Downsample for Segmentation of Ultra-High Resolution Images
- **Arxiv ID**: http://arxiv.org/abs/2109.11071v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2109.11071v2)
- **Published**: 2021-09-22 23:04:59+00:00
- **Updated**: 2022-03-16 01:30:27+00:00
- **Authors**: Chen Jin, Ryutaro Tanno, Thomy Mertzanidou, Eleftheria Panagiotaki, Daniel C. Alexander
- **Comment**: 19 pages, 17 figures
- **Journal**: International Conference on Learning Representations, 2022
- **Summary**: Many computer vision systems require low-cost segmentation algorithms based on deep learning, either because of the enormous size of input images or limited computational budget. Common solutions uniformly downsample the input images to meet memory constraints, assuming all pixels are equally informative. In this work, we demonstrate that this assumption can harm the segmentation performance because the segmentation difficulty varies spatially. We combat this problem by introducing a learnable downsampling module, which can be optimised together with the given segmentation model in an end-to-end fashion. We formulate the problem of training such downsampling module as optimisation of sampling density distributions over the input images given their low-resolution views. To defend against degenerate solutions (e.g. over-sampling trivial regions like the backgrounds), we propose a regularisation term that encourages the sampling locations to concentrate around the object boundaries. We find the downsampling module learns to sample more densely at difficult locations, thereby improving the segmentation performance. Our experiments on benchmarks of high-resolution street view, aerial and medical images demonstrate substantial improvements in terms of efficiency-and-accuracy trade-off compared to both uniform downsampling and two recent advanced downsampling techniques.



### 3N-GAN: Semi-Supervised Classification of X-Ray Images with a 3-Player Adversarial Framework
- **Arxiv ID**: http://arxiv.org/abs/2109.13862v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.13862v1)
- **Published**: 2021-09-22 23:18:59+00:00
- **Updated**: 2021-09-22 23:18:59+00:00
- **Authors**: Shafin Haque, Ayaan Haque
- **Comment**: 5 pages, 2 figures; authors contributed equally
- **Journal**: None
- **Summary**: The success of deep learning for medical imaging tasks, such as classification, is heavily reliant on the availability of large-scale datasets. However, acquiring datasets with large quantities of labeled data is challenging, as labeling is expensive and time-consuming. Semi-supervised learning (SSL) is a growing alternative to fully-supervised learning, but requires unlabeled samples for training. In medical imaging, many datasets lack unlabeled data entirely, so SSL can't be conventionally utilized. We propose 3N-GAN, or 3 Network Generative Adversarial Networks, to perform semi-supervised classification of medical images in fully-supervised settings. We incorporate a classifier into the adversarial relationship such that the generator trains adversarially against both the classifier and discriminator. Our preliminary results show improved classification performance and GAN generations over various algorithms. Our work can seamlessly integrate with numerous other medical imaging model architectures and SSL methods for greater performance.



### A Novel Factor Graph-Based Optimization Technique for Stereo Correspondence Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.11077v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.11077v1)
- **Published**: 2021-09-22 23:30:33+00:00
- **Updated**: 2021-09-22 23:30:33+00:00
- **Authors**: Hanieh Shabanian, Madhusudhanan Balasubramanian
- **Comment**: 25 pages, 5 figures, 6 tables
- **Journal**: None
- **Summary**: Dense disparities among multiple views is essential for estimating the 3D architecture of a scene based on the geometrical relationship among the scene and the views or cameras. Scenes with larger extents of heterogeneous textures, differing scene illumination among the multiple views and with occluding objects affect the accuracy of the estimated disparities. Markov random fields (MRF) based methods for disparity estimation address these limitations using spatial dependencies among the observations and among the disparity estimates. These methods, however, are limited by spatially fixed and smaller neighborhood systems or cliques. In this work, we present a new factor graph-based probabilistic graphical model for disparity estimation that allows a larger and a spatially variable neighborhood structure determined based on the local scene characteristics. We evaluated our method using the Middlebury benchmark stereo datasets and the Middlebury evaluation dataset version 3.0 and compared its performance with recent state-of-the-art disparity estimation algorithms. The new factor graph-based method provided disparity estimates with higher accuracy when compared to the recent non-learning- and learning-based disparity estimation algorithms. In addition to disparity estimation, our factor graph formulation can be useful for obtaining maximum a posteriori solution to optimization problems with complex and variable dependency structures as well as for other dense estimation problems such as optical flow estimation.



