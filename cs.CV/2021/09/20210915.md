# Arxiv Papers in cs.CV on 2021-09-15
### Seeking an Optimal Approach for Computer-Aided Pulmonary Embolism Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.07029v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.07029v1)
- **Published**: 2021-09-15 00:21:23+00:00
- **Updated**: 2021-09-15 00:21:23+00:00
- **Authors**: Nahid Ul Islam, Shiv Gehlot, Zongwei Zhou, Michael B Gotway, Jianming Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Pulmonary embolism (PE) represents a thrombus ("blood clot"), usually originating from a lower extremity vein, that travels to the blood vessels in the lung, causing vascular obstruction and in some patients, death. This disorder is commonly diagnosed using CT pulmonary angiography (CTPA). Deep learning holds great promise for the computer-aided CTPA diagnosis (CAD) of PE. However, numerous competing methods for a given task in the deep learning literature exist, causing great confusion regarding the development of a CAD PE system. To address this confusion, we present a comprehensive analysis of competing deep learning methods applicable to PE diagnosis using CTPA at the both image and exam levels. At the image level, we compare convolutional neural networks (CNNs) with vision transformers, and contrast self-supervised learning (SSL) with supervised learning, followed by an evaluation of transfer learning compared with training from scratch. At the exam level, we focus on comparing conventional classification (CC) with multiple instance learning (MIL). Our extensive experiments consistently show: (1) transfer learning consistently boosts performance despite differences between natural images and CT scans, (2) transfer learning with SSL surpasses its supervised counterparts; (3) CNNs outperform vision transformers, which otherwise show satisfactory performance; and (4) CC is, surprisingly, superior to MIL. Compared with the state of the art, our optimal approach provides an AUC gain of 0.2\% and 1.05\% for image-level and exam-level, respectively.



### PnP-DETR: Towards Efficient Visual Analysis with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2109.07036v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07036v4)
- **Published**: 2021-09-15 01:10:30+00:00
- **Updated**: 2022-03-02 04:39:26+00:00
- **Authors**: Tao Wang, Li Yuan, Yunpeng Chen, Jiashi Feng, Shuicheng Yan
- **Comment**: accepted by ICCV 2021
- **Journal**: None
- **Summary**: Recently, DETR pioneered the solution of vision tasks with transformers, it directly translates the image feature map into the object detection result. Though effective, translating the full feature map can be costly due to redundant computation on some area like the background. In this work, we encapsulate the idea of reducing spatial redundancy into a novel poll and pool (PnP) sampling module, with which we build an end-to-end PnP-DETR architecture that adaptively allocates its computation spatially to be more efficient. Concretely, the PnP module abstracts the image feature map into fine foreground object feature vectors and a small number of coarse background contextual feature vectors. The transformer models information interaction within the fine-coarse feature space and translates the features into the detection result. Moreover, the PnP-augmented model can instantly achieve various desired trade-offs between performance and computation with a single model by varying the sampled feature length, without requiring to train multiple models as existing methods. Thus it offers greater flexibility for deployment in diverse scenarios with varying computation constraint. We further validate the generalizability of the PnP module on panoptic segmentation and the recent transformer-based image recognition model ViT and show consistent efficiency gain. We believe our method makes a step for efficient visual analysis with transformers, wherein spatial redundancy is commonly observed. Code will be available at \url{https://github.com/twangnh/pnp-detr}.



### Uncertainty Quantification in Medical Image Segmentation with Multi-decoder U-Net
- **Arxiv ID**: http://arxiv.org/abs/2109.07045v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.07045v1)
- **Published**: 2021-09-15 01:46:29+00:00
- **Updated**: 2021-09-15 01:46:29+00:00
- **Authors**: Yanwu Yang, Xutao Guo, Yiwei Pan, Pengcheng Shi, Haiyan Lv, Ting Ma
- **Comment**: MICCAI_QUBIQ challenge, conference, Uncertainty qualification
- **Journal**: None
- **Summary**: Accurate medical image segmentation is crucial for diagnosis and analysis. However, the models without calibrated uncertainty estimates might lead to errors in downstream analysis and exhibit low levels of robustness. Estimating the uncertainty in the measurement is vital to making definite, informed conclusions. Especially, it is difficult to make accurate predictions on ambiguous areas and focus boundaries for both models and radiologists, even harder to reach a consensus with multiple annotations. In this work, the uncertainty under these areas is studied, which introduces significant information with anatomical structure and is as important as segmentation performance. We exploit the medical image segmentation uncertainty quantification by measuring segmentation performance with multiple annotations in a supervised learning manner and propose a U-Net based architecture with multiple decoders, where the image representation is encoded with the same encoder, and segmentation referring to each annotation is estimated with multiple decoders. Nevertheless, a cross-loss function is proposed for bridging the gap between different branches. The proposed architecture is trained in an end-to-end manner and able to improve predictive uncertainty estimates. The model achieves comparable performance with fewer parameters to the integrated training model that ranked the runner-up in the MICCAI-QUBIQ 2020 challenge.



### Image Synthesis via Semantic Composition
- **Arxiv ID**: http://arxiv.org/abs/2109.07053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07053v1)
- **Published**: 2021-09-15 02:26:07+00:00
- **Updated**: 2021-09-15 02:26:07+00:00
- **Authors**: Yi Wang, Lu Qi, Ying-Cong Chen, Xiangyu Zhang, Jiaya Jia
- **Comment**: Project page is at https://shepnerd.github.io/scg/. Accepted to ICCV
  2021
- **Journal**: None
- **Summary**: In this paper, we present a novel approach to synthesize realistic images based on their semantic layouts. It hypothesizes that for objects with similar appearance, they share similar representation. Our method establishes dependencies between regions according to their appearance correlation, yielding both spatially variant and associated representations. Conditioning on these features, we propose a dynamic weighted network constructed by spatially conditional computation (with both convolution and normalization). More than preserving semantic distinctions, the given dynamic network strengthens semantic relevance, benefiting global structure and detail synthesis. We demonstrate that our method gives the compelling generation performance qualitatively and quantitatively with extensive experiments on benchmarks.



### F-CAM: Full Resolution Class Activation Maps via Guided Parametric Upscaling
- **Arxiv ID**: http://arxiv.org/abs/2109.07069v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.07069v2)
- **Published**: 2021-09-15 04:45:20+00:00
- **Updated**: 2021-10-21 02:37:53+00:00
- **Authors**: Soufiane Belharbi, Aydin Sarraf, Marco Pedersoli, Ismail Ben Ayed, Luke McCaffrey, Eric Granger
- **Comment**: 23pages, WACV 2022
- **Journal**: None
- **Summary**: Class Activation Mapping (CAM) methods have recently gained much attention for weakly-supervised object localization (WSOL) tasks. They allow for CNN visualization and interpretation without training on fully annotated image datasets. CAM methods are typically integrated within off-the-shelf CNN backbones, such as ResNet50. Due to convolution and pooling operations, these backbones yield low resolution CAMs with a down-scaling factor of up to 32, contributing to inaccurate localizations. Interpolation is required to restore full size CAMs, yet it does not consider the statistical properties of objects, such as color and texture, leading to activations with inconsistent boundaries, and inaccurate localizations. As an alternative, we introduce a generic method for parametric upscaling of CAMs that allows constructing accurate full resolution CAMs (F-CAMs). In particular, we propose a trainable decoding architecture that can be connected to any CNN classifier to produce highly accurate CAM localizations. Given an original low resolution CAM, foreground and background pixels are randomly sampled to fine-tune the decoder. Additional priors such as image statistics and size constraints are also considered to expand and refine object boundaries. Extensive experiments, over three CNN backbones and six WSOL baselines on the CUB-200-2011 and OpenImages datasets, indicate that our F-CAM method yields a significant improvement in CAM localization accuracy. F-CAM performance is competitive with state-of-art WSOL methods, yet it requires fewer computations during inference.



### DSOR: A Scalable Statistical Filter for Removing Falling Snow from LiDAR Point Clouds in Severe Winter Weather
- **Arxiv ID**: http://arxiv.org/abs/2109.07078v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.07078v2)
- **Published**: 2021-09-15 05:02:29+00:00
- **Updated**: 2021-10-30 15:55:34+00:00
- **Authors**: Akhil Kurup, Jeremy Bos
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: For autonomous vehicles to viably replace human drivers they must contend with inclement weather. Falling rain and snow introduce noise in LiDAR returns resulting in both false positive and false negative object detections. In this article we introduce the Winter Adverse Driving dataSet (WADS) collected in the snow belt region of Michigan's Upper Peninsula. WADS is the first multi-modal dataset featuring dense point-wise labeled sequential LiDAR scans collected in severe winter weather; weather that would cause an experienced driver to alter their driving behavior. We have labelled and will make available over 7 GB or 3.6 billion labelled LiDAR points out of over 26 TB of total LiDAR and camera data collected. We also present the Dynamic Statistical Outlier Removal (DSOR) filter, a statistical PCL-based filter capable or removing snow with a higher recall than the state of the art snow de-noising filter while being 28\% faster. Further, the DSOR filter is shown to have a lower time complexity compared to the state of the art resulting in an improved scalability.   Our labeled dataset and DSOR filter will be made available at https://bitbucket.org/autonomymtu/dsor_filter



### FSER: Deep Convolutional Neural Networks for Speech Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.07916v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2109.07916v1)
- **Published**: 2021-09-15 05:03:24+00:00
- **Updated**: 2021-09-15 05:03:24+00:00
- **Authors**: Bonaventure F. P. Dossou, Yeno K. S. Gbenou
- **Comment**: ABAW Workshop, ICCV 2021
- **Journal**: None
- **Summary**: Using mel-spectrograms over conventional MFCCs features, we assess the abilities of convolutional neural networks to accurately recognize and classify emotions from speech data. We introduce FSER, a speech emotion recognition model trained on four valid speech databases, achieving a high-classification accuracy of 95,05\%, over 8 different emotion classes: anger, anxiety, calm, disgust, happiness, neutral, sadness, surprise. On each benchmark dataset, FSER outperforms the best models introduced so far, achieving a state-of-the-art performance. We show that FSER stays reliable, independently of the language, sex identity, and any other external factor. Additionally, we describe how FSER could potentially be used to improve mental and emotional health care and how our analysis and findings serve as guidelines and benchmarks for further works in the same direction.



### Complementary Feature Enhanced Network with Vision Transformer for Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2109.07100v3
- **DOI**: None
- **Categories**: **cs.CV**, 68U10 (Primary) 94A08, 54H30 (Secondary), I.4.3; I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2109.07100v3)
- **Published**: 2021-09-15 06:13:22+00:00
- **Updated**: 2022-01-05 03:05:45+00:00
- **Authors**: Dong Zhao, Jia Li, Hongyu Li, Long Xu
- **Comment**: 12 pages,10 figures
- **Journal**: None
- **Summary**: Conventional CNNs-based dehazing models suffer from two essential issues: the dehazing framework (limited in interpretability) and the convolution layers (content-independent and ineffective to learn long-range dependency information). In this paper, firstly, we propose a new complementary feature enhanced framework, in which the complementary features are learned by several complementary subtasks and then together serve to boost the performance of the primary task. One of the prominent advantages of the new framework is that the purposively chosen complementary tasks can focus on learning weakly dependent complementary features, avoiding repetitive and ineffective learning of the networks. We design a new dehazing network based on such a framework. Specifically, we select the intrinsic image decomposition as the complementary tasks, where the reflectance and shading prediction subtasks are used to extract the color-wise and texture-wise complementary features. To effectively aggregate these complementary features, we propose a complementary features selection module (CFSM) to select the more useful features for image dehazing. Furthermore, we introduce a new version of vision transformer block, named Hybrid Local-Global Vision Transformer (HyLoG-ViT), and incorporate it within our dehazing networks. The HyLoG-ViT block consists of the local and the global vision transformer paths used to capture local and global dependencies. As a result, the HyLoG-ViT introduces locality in the networks and captures the global and long-range dependencies. Extensive experiments on homogeneous, non-homogeneous, and nighttime dehazing tasks reveal that the proposed dehazing network can achieve comparable or even better performance than CNNs-based dehazing models.



### Anchor DETR: Query Design for Transformer-Based Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.07107v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07107v2)
- **Published**: 2021-09-15 06:31:55+00:00
- **Updated**: 2022-01-04 08:20:42+00:00
- **Authors**: Yingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun
- **Comment**: Accepted to AAAI 2022
- **Journal**: None
- **Summary**: In this paper, we propose a novel query design for the transformer-based object detection. In previous transformer-based detectors, the object queries are a set of learned embeddings. However, each learned embedding does not have an explicit physical meaning and we cannot explain where it will focus on. It is difficult to optimize as the prediction slot of each object query does not have a specific mode. In other words, each object query will not focus on a specific region. To solved these problems, in our query design, object queries are based on anchor points, which are widely used in CNN-based detectors. So each object query focuses on the objects near the anchor point. Moreover, our query design can predict multiple objects at one position to solve the difficulty: "one region, multiple objects". In addition, we design an attention variant, which can reduce the memory cost while achieving similar or better performance than the standard attention in DETR. Thanks to the query design and the attention variant, the proposed detector that we called Anchor DETR, can achieve better performance and run faster than the DETR with 10$\times$ fewer training epochs. For example, it achieves 44.2 AP with 19 FPS on the MSCOCO dataset when using the ResNet50-DC5 feature for training 50 epochs. Extensive experiments on the MSCOCO benchmark prove the effectiveness of the proposed methods. Code is available at \url{https://github.com/megvii-research/AnchorDETR}.



### Patch-based Medical Image Segmentation using Matrix Product State Tensor Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.07138v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07138v2)
- **Published**: 2021-09-15 07:54:05+00:00
- **Updated**: 2022-02-23 14:01:56+00:00
- **Authors**: Raghavendra Selvan, Erik B Dam, Søren Alexander Flensborg, Jens Petersen
- **Comment**: Journal extension of our preliminary conference work "Segmenting
  two-dimensional structures with strided tensor networks", Selvan et al. 2021,
  available at arXiv:2102.06900. 24 pages, 12 figures. Accepted to be published
  at the Journal of Machine Learning for Biomedical Imaging, to be updated at
  https://www.melba-journal.org/papers/2022:005.html
- **Journal**: Journal of Machine Learning for Biomedical Imaging. 2022:005. pp
  1-24
- **Summary**: Tensor networks are efficient factorisations of high-dimensional tensors into a network of lower-order tensors. They have been most commonly used to model entanglement in quantum many-body systems and more recently are witnessing increased applications in supervised machine learning. In this work, we formulate image segmentation in a supervised setting with tensor networks. The key idea is to first lift the pixels in image patches to exponentially high-dimensional feature spaces and using a linear decision hyper-plane to classify the input pixels into foreground and background classes. The high-dimensional linear model itself is approximated using the matrix product state (MPS) tensor network. The MPS is weight-shared between the non-overlapping image patches resulting in our strided tensor network model. The performance of the proposed model is evaluated on three 2D- and one 3D- biomedical imaging datasets. The performance of the proposed tensor network segmentation model is compared with relevant baseline methods. In the 2D experiments, the tensor network model yields competitive performance compared to the baseline methods while being more resource efficient.



### Neural Architecture Search in operational context: a remote sensing case-study
- **Arxiv ID**: http://arxiv.org/abs/2109.08028v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2109.08028v1)
- **Published**: 2021-09-15 08:18:12+00:00
- **Updated**: 2021-09-15 08:18:12+00:00
- **Authors**: Anthony Cazasnoves, Pierre-Antoine Ganaye, Kévin Sanchis, Tugdual Ceillier
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has become in recent years a cornerstone tool fueling key innovations in the industry, such as autonomous driving. To attain good performances, the neural network architecture used for a given application must be chosen with care. These architectures are often handcrafted and therefore prone to human biases and sub-optimal selection. Neural Architecture Search (NAS) is a framework introduced to mitigate such risks by jointly optimizing the network architectures and its weights. Albeit its novelty, it was applied on complex tasks with significant results - e.g. semantic image segmentation. In this technical paper, we aim to evaluate its ability to tackle a challenging operational task: semantic segmentation of objects of interest in satellite imagery. Designing a NAS framework is not trivial and has strong dependencies to hardware constraints. We therefore motivate our NAS approach selection and provide corresponding implementation details. We also present novel ideas to carry out other such use-case studies.



### Resolution-robust Large Mask Inpainting with Fourier Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2109.07161v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.07161v2)
- **Published**: 2021-09-15 08:54:29+00:00
- **Updated**: 2021-11-11 01:58:26+00:00
- **Authors**: Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, Victor Lempitsky
- **Comment**: Winter Conference on Applications of Computer Vision (WACV 2022)
- **Journal**: None
- **Summary**: Modern image inpainting systems, despite the significant progress, often struggle with large missing areas, complex geometric structures, and high-resolution images. We find that one of the main reasons for that is the lack of an effective receptive field in both the inpainting network and the loss function. To alleviate this issue, we propose a new method called large mask inpainting (LaMa). LaMa is based on i) a new inpainting network architecture that uses fast Fourier convolutions (FFCs), which have the image-wide receptive field; ii) a high receptive field perceptual loss; iii) large training masks, which unlocks the potential of the first two components. Our inpainting network improves the state-of-the-art across a range of datasets and achieves excellent performance even in challenging scenarios, e.g. completion of periodic structures. Our model generalizes surprisingly well to resolutions that are higher than those seen at train time, and achieves this at lower parameter&time costs than the competitive baselines. The code is available at \url{https://github.com/saic-mdal/lama}.



### MISSFormer: An Effective Medical Image Segmentation Transformer
- **Arxiv ID**: http://arxiv.org/abs/2109.07162v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07162v2)
- **Published**: 2021-09-15 08:56:00+00:00
- **Updated**: 2021-12-19 02:50:37+00:00
- **Authors**: Xiaohong Huang, Zhifang Deng, Dandan Li, Xueguang Yuan
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: The CNN-based methods have achieved impressive results in medical image segmentation, but they failed to capture the long-range dependencies due to the inherent locality of the convolution operation. Transformer-based methods are recently popular in vision tasks because of their capacity for long-range dependencies and promising performance. However, it lacks in modeling local context. In this paper, taking medical image segmentation as an example, we present MISSFormer, an effective and powerful Medical Image Segmentation tranSFormer. MISSFormer is a hierarchical encoder-decoder network with two appealing designs: 1) A feed-forward network is redesigned with the proposed Enhanced Transformer Block, which enhances the long-range dependencies and supplements the local context, making the feature more discriminative. 2) We proposed Enhanced Transformer Context Bridge, different from previous methods of modeling only global information, the proposed context bridge with the enhanced transformer block extracts the long-range dependencies and local context of multi-scale features generated by our hierarchical transformer encoder. Driven by these two designs, the MISSFormer shows a solid capacity to capture more discriminative dependencies and context in medical image segmentation. The experiments on multi-organ and cardiac segmentation tasks demonstrate the superiority, effectiveness and robustness of our MISSFormer, the experimental results of MISSFormer trained from scratch even outperform state-of-the-art methods pre-trained on ImageNet. The core designs can be generalized to other visual segmentation tasks. The code has been released on Github: https://github.com/ZhifangDeng/MISSFormer



### 3D Annotation Of Arbitrary Objects In The Wild
- **Arxiv ID**: http://arxiv.org/abs/2109.07165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07165v1)
- **Published**: 2021-09-15 09:00:56+00:00
- **Updated**: 2021-09-15 09:00:56+00:00
- **Authors**: Kenneth Blomqvist, Julius Hietala
- **Comment**: 6 pages, 4 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
- **Journal**: None
- **Summary**: Recent years have produced a variety of learning based methods in the context of computer vision and robotics. Most of the recently proposed methods are based on deep learning, which require very large amounts of data compared to traditional methods. The performance of the deep learning methods are largely dependent on the data distribution they were trained on, and it is important to use data from the robot's actual operating domain during training. Therefore, it is not possible to rely on pre-built, generic datasets when deploying robots in real environments, creating a need for efficient data collection and annotation in the specific operating conditions the robots will operate in. The challenge is then: how do we reduce the cost of obtaining such datasets to a point where we can easily deploy our robots in new conditions, environments and to support new sensors? As an answer to this question, we propose a data annotation pipeline based on SLAM, 3D reconstruction, and 3D-to-2D geometry. The pipeline allows creating 3D and 2D bounding boxes, along with per-pixel annotations of arbitrary objects without needing accurate 3D models of the objects prior to data collection and annotation. Our results showcase almost 90% Intersection-over-Union (IoU) agreement on both semantic segmentation and 2D bounding box detection across a variety of objects and scenes, while speeding up the annotation process by several orders of magnitude compared to traditional manual annotation.



### FCA: Learning a 3D Full-coverage Vehicle Camouflage for Multi-view Physical Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2109.07193v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68-06, I.4.0; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2109.07193v3)
- **Published**: 2021-09-15 10:17:12+00:00
- **Updated**: 2021-12-12 02:33:39+00:00
- **Authors**: Donghua Wang, Tingsong Jiang, Jialiang Sun, Weien Zhou, Xiaoya Zhang, Zhiqiang Gong, Wen Yao, Xiaoqian Chen
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Physical adversarial attacks in object detection have attracted increasing attention. However, most previous works focus on hiding the objects from the detector by generating an individual adversarial patch, which only covers the planar part of the vehicle's surface and fails to attack the detector in physical scenarios for multi-view, long-distance and partially occluded objects. To bridge the gap between digital attacks and physical attacks, we exploit the full 3D vehicle surface to propose a robust Full-coverage Camouflage Attack (FCA) to fool detectors. Specifically, we first try rendering the nonplanar camouflage texture over the full vehicle surface. To mimic the real-world environment conditions, we then introduce a transformation function to transfer the rendered camouflaged vehicle into a photo realistic scenario. Finally, we design an efficient loss function to optimize the camouflage texture. Experiments show that the full-coverage camouflage attack can not only outperform state-of-the-art methods under various test cases but also generalize to different environments, vehicles, and object detectors. The code of FCA will be available at: https://idrl-lab.github.io/Full-coverage-camouflage-adversarial-attack/.



### Progressive Hard-case Mining across Pyramid Levels for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.07217v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07217v2)
- **Published**: 2021-09-15 11:12:03+00:00
- **Updated**: 2022-03-30 01:38:54+00:00
- **Authors**: Binghong Wu, Yehui Yang, Dalu Yang, Junde Wu, Xiaorong Wang, Haifeng Huang, Lei Wang, Yanwu Xu
- **Comment**: None
- **Journal**: None
- **Summary**: In object detection, multi-level prediction (e.g., FPN) and reweighting skills (e.g., focal loss) have drastically improved one-stage detector performance. However, the synergy between these two techniques is not fully explored in a unified framework. We find that, during training, the one-stage detector's optimization is not only restricted to the static hard-case mining loss (gradient drift) but also suffered from the diverse positive samples' proportions split by different pyramid levels (level discrepancy). Under this concern, we propose Hierarchical Progressive Focus (HPF) consisting of two key designs: 1) progressive focus, a more flexible hard-case mining setting calculated adaptive to the convergence progress, 2) hierarchical sampling, automatically generating a set of progressive focus for level-specific target optimization. Based on focal loss with ATSS-R50, our approach achieves 40.5 AP, surpassing the state-of-the-art QFL (Quality Focal Loss, 39.9 AP) and VFL (Varifocal Loss, 40.1 AP). Our best model achieves 55.1 AP on COCO test-dev, obtaining excellent results with only a typical training setting. Moreover, as a plug-and-play scheme, HPF can cooperate well with recent advances, providing a stable performance improvement on nine mainstream detectors.



### Automatic Plane Adjustment of Orthopedic Intra-operative Flat Panel Detector CT-Volumes
- **Arxiv ID**: http://arxiv.org/abs/2109.10731v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2109.10731v1)
- **Published**: 2021-09-15 11:25:14+00:00
- **Updated**: 2021-09-15 11:25:14+00:00
- **Authors**: Celia Martin Vicario, Florian Kordon, Felix Denzinger, Jan Siad El Barbari, Maxim Privalov, Jochen Franke, Sarina Thomas, Lisa Kausch, Andreas Maier, Holger Kunze
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose   3D acquisitions are often acquired to assess the result in orthopedic trauma surgery. With a mobile C-Arm system, these acquisitions can be performed intra-operatively. That reduces the number of required revision surgeries. However, due to the operation room setup, the acquisitions typically cannot be performed such that the acquired volumes are aligned to the anatomical regions. Thus, the multiplanar reconstructed (MPR) planes need to be adjusted manually during the review of the volume. In this paper, we present a detailed study of multi-task learning (MTL) regression networks to estimate the parameters of the MPR planes.   Approach   First, various mathematical descriptions for rotation, including Euler angle, quaternion, and matrix representation, are revised. Then, three different MTL network architectures based on the PoseNet are compared with a single task learning network.   Results   Using a matrix description rather than the Euler angle description, the accuracy of the regressed normals improves from $7.7^{\circ}$ to $7.3^{\circ}$ in the mean value for single anatomies. The multi-head approach improves the regression of the plane position from $7.4mm$ to $6.1mm$, while the orientation does not benefit from this approach.   Conclusions   The results show that a multi-head approach can lead to slightly better results than the individual tasks networks. The most important benefit of the MTL approach is that it is a single network for standard plane regression for all body regions with a reduced number of stored parameters.



### Navigation-Oriented Scene Understanding for Robotic Autonomy: Learning to Segment Driveability in Egocentric Images
- **Arxiv ID**: http://arxiv.org/abs/2109.07245v2
- **DOI**: 10.1109/LRA.2022.3144491
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.07245v2)
- **Published**: 2021-09-15 12:25:56+00:00
- **Updated**: 2022-01-23 13:14:52+00:00
- **Authors**: Galadrielle Humblot-Renaux, Letizia Marchegiani, Thomas B. Moeslund, Rikke Gade
- **Comment**: Accepted in Robotics and Automation Letters (RA-L 2022).
  Supplementary video available at https://youtu.be/q_XfjUDO39Y
- **Journal**: Robotics and Automation Letters 7(2) (2022) 2913-2920
- **Summary**: This work tackles scene understanding for outdoor robotic navigation, solely relying on images captured by an on-board camera. Conventional visual scene understanding interprets the environment based on specific descriptive categories. However, such a representation is not directly interpretable for decision-making and constrains robot operation to a specific domain. Thus, we propose to segment egocentric images directly in terms of how a robot can navigate in them, and tailor the learning problem to an autonomous navigation task. Building around an image segmentation network, we present a generic affordance consisting of 3 driveability levels which can broadly apply to both urban and off-road scenes. By encoding these levels with soft ordinal labels, we incorporate inter-class distances during learning which improves segmentation compared to standard "hard" one-hot labelling. In addition, we propose a navigation-oriented pixel-wise loss weighting method which assigns higher importance to safety-critical areas. We evaluate our approach on large-scale public image segmentation datasets ranging from sunny city streets to snowy forest trails. In a cross-dataset generalization experiment, we show that our affordance learning scheme can be applied across a diverse mix of datasets and improves driveability estimation in unseen environments compared to general-purpose, single-dataset segmentation.



### RGB-D Saliency Detection via Cascaded Mutual Information Minimization
- **Arxiv ID**: http://arxiv.org/abs/2109.07246v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07246v2)
- **Published**: 2021-09-15 12:31:27+00:00
- **Updated**: 2022-01-06 00:28:43+00:00
- **Authors**: Jing Zhang, Deng-Ping Fan, Yuchao Dai, Xin Yu, Yiran Zhong, Nick Barnes, Ling Shao
- **Comment**: Accepted as ICCV2021 paper
- **Journal**: None
- **Summary**: Existing RGB-D saliency detection models do not explicitly encourage RGB and depth to achieve effective multi-modal learning. In this paper, we introduce a novel multi-stage cascaded learning framework via mutual information minimization to "explicitly" model the multi-modal information between RGB image and depth data. Specifically, we first map the feature of each mode to a lower dimensional feature vector, and adopt mutual information minimization as a regularizer to reduce the redundancy between appearance features from RGB and geometric features from depth. We then perform multi-stage cascaded learning to impose the mutual information minimization constraint at every stage of the network. Extensive experiments on benchmark RGB-D saliency datasets illustrate the effectiveness of our framework. Further, to prosper the development of this field, we contribute the largest (7x larger than NJU2K) dataset, which contains 15,625 image pairs with high quality polygon-/scribble-/object-/instance-/rank-level annotations. Based on these rich labels, we additionally construct four new benchmarks with strong baselines and observe some interesting phenomena, which can motivate future model design. Source code and dataset are available at "https://github.com/JingZhang617/cascaded_rgbd_sod".



### Towards Precise Pruning Points Detection using Semantic-Instance-Aware Plant Models for Grapevine Winter Pruning Automation
- **Arxiv ID**: http://arxiv.org/abs/2109.07247v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.07247v1)
- **Published**: 2021-09-15 12:32:05+00:00
- **Updated**: 2021-09-15 12:32:05+00:00
- **Authors**: Miguel Fernandes, Antonello Scaldaferri, Paolo Guadagna, Giuseppe Fiameni, Tao Teng, Matteo Gatti, Stefano Poni, Claudio Semini, Darwin Caldwell, Fei Chen
- **Comment**: arXiv admin note: text overlap with arXiv:2106.04208
- **Journal**: None
- **Summary**: Grapevine winter pruning is a complex task, that requires skilled workers to execute it correctly. The complexity makes it time consuming. It is an operation that requires about 80-120 hours per hectare annually, making an automated robotic system that helps in speeding up the process a crucial tool in large-size vineyards. We will describe (a) a novel expert annotated dataset for grapevine segmentation, (b) a state of the art neural network implementation and (c) generation of pruning points following agronomic rules, leveraging the simplified structure of the plant. With this approach, we are able to generate a set of pruning points on the canes, paving the way towards a correct automation of grapevine winter pruning.



### Integrating Sensing and Communication in Cellular Networks via NR Sidelink
- **Arxiv ID**: http://arxiv.org/abs/2109.07253v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.07253v1)
- **Published**: 2021-09-15 12:41:31+00:00
- **Updated**: 2021-09-15 12:41:31+00:00
- **Authors**: Dariush Salami, Ramin Hasibi, Stefano Savazzi, Tom Michoel, Stephan Sigg
- **Comment**: The paper is submitted to JSAC and it is still under review
- **Journal**: None
- **Summary**: RF-sensing, the analysis and interpretation of movement or environment-induced patterns in received electromagnetic signals, has been actively investigated for more than a decade. Since electromagnetic signals, through cellular communication systems, are omnipresent, RF sensing has the potential to become a universal sensing mechanism with applications in smart home, retail, localization, gesture recognition, intrusion detection, etc. Specifically, existing cellular network installations might be dual-used for both communication and sensing. Such communications and sensing convergence is envisioned for future communication networks. We propose the use of NR-sidelink direct device-to-device communication to achieve device-initiated,flexible sensing capabilities in beyond 5G cellular communication systems. In this article, we specifically investigate a common issue related to sidelink-based RF-sensing, which is its angle and rotation dependence. In particular, we discuss transformations of mmWave point-cloud data which achieve rotational invariance, as well as distributed processing based on such rotational invariant inputs, at angle and distance diverse devices. To process the distributed data, we propose a graph based encoder to capture spatio-temporal features of the data and propose four approaches for multi-angle learning. The approaches are compared on a newly recorded and openly available dataset comprising 15 subjects, performing 21 gestures which are recorded from 8 angles.



### Distract Your Attention: Multi-head Cross Attention Network for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.07270v6
- **DOI**: 10.3390/biomimetics8020199
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07270v6)
- **Published**: 2021-09-15 13:15:54+00:00
- **Updated**: 2023-05-22 03:19:49+00:00
- **Authors**: Zhengyao Wen, Wenzhong Lin, Tao Wang, Ge Xu
- **Comment**: Paper appears in Biomimetics 2023, 8(2), 199
- **Journal**: None
- **Summary**: We present a novel facial expression recognition network, called Distract your Attention Network (DAN). Our method is based on two key observations. Firstly, multiple classes share inherently similar underlying facial appearance, and their differences could be subtle. Secondly, facial expressions exhibit themselves through multiple facial regions simultaneously, and the recognition requires a holistic approach by encoding high-order interactions among local features. To address these issues, we propose our DAN with three key components: Feature Clustering Network (FCN), Multi-head cross Attention Network (MAN), and Attention Fusion Network (AFN). The FCN extracts robust features by adopting a large-margin learning objective to maximize class separability. In addition, the MAN instantiates a number of attention heads to simultaneously attend to multiple facial areas and build attention maps on these regions. Further, the AFN distracts these attentions to multiple locations before fusing the attention maps to a comprehensive one. Extensive experiments on three public datasets (including AffectNet, RAF-DB, and SFEW 2.0) verified that the proposed method consistently achieves state-of-the-art facial expression recognition performance. Code will be made available at https://github.com/yaoing/DAN.



### New Perspective on Progressive GANs Distillation for One-class Novelty Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.07295v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07295v3)
- **Published**: 2021-09-15 13:45:30+00:00
- **Updated**: 2023-06-30 15:32:05+00:00
- **Authors**: Zhiwei Zhang, Yu Dong, Hanyu Peng, Shifeng Chen
- **Comment**: withdraw original draft
- **Journal**: None
- **Summary**: One-class novelty detection is conducted to identify anomalous instances, with different distributions from the expected normal instances. In this paper, the Generative Adversarial Network based on the Encoder-Decoder-Encoder scheme (EDE-GAN) achieves state-of-the-art performance. The two factors bellow serve the above purpose: 1) The EDE-GAN calculates the distance between two latent vectors as the anomaly score, which is unlike the previous methods by utilizing the reconstruction error between images. 2) The model obtains best results when the batch size is set to 1. To illustrate their superiority, we design a new GAN architecture, and compare performances according to different batch sizes. Moreover, with experimentation leads to discovery, our result implies there is also evidence of just how beneficial constraint on the latent space are when engaging in model training. In an attempt to learn compact and fast models, we present a new technology, Progressive Knowledge Distillation with GANs (P-KDGAN), which connects two standard GANs through the designed distillation loss. Two-step progressive learning continuously augments the performance of student GANs with improved results over single-step approach. Our experimental results on CIFAR-10, MNIST, and FMNIST datasets illustrate that P-KDGAN improves the performance of the student GAN by 2.44%, 1.77%, and 1.73% when compressing the computationat ratios of 24.45:1, 311.11:1, and 700:1, respectively.



### FFAVOD: Feature Fusion Architecture for Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.07298v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07298v1)
- **Published**: 2021-09-15 13:53:21+00:00
- **Updated**: 2021-09-15 13:53:21+00:00
- **Authors**: Hughes Perreault, Guillaume-Alexandre Bilodeau, Nicolas Saunier, Maguelonne Héritier
- **Comment**: Accepted for publication in Pattern Recognition Letters
- **Journal**: None
- **Summary**: A significant amount of redundancy exists between consecutive frames of a video. Object detectors typically produce detections for one image at a time, without any capabilities for taking advantage of this redundancy. Meanwhile, many applications for object detection work with videos, including intelligent transportation systems, advanced driver assistance systems and video surveillance. Our work aims at taking advantage of the similarity between video frames to produce better detections. We propose FFAVOD, standing for feature fusion architecture for video object detection. We first introduce a novel video object detection architecture that allows a network to share feature maps between nearby frames. Second, we propose a feature fusion module that learns to merge feature maps to enhance them. We show that using the proposed architecture and the fusion module can improve the performance of three base object detectors on two object detection benchmarks containing sequences of moving road users. Additionally, to further increase performance, we propose an improvement to the SpotNet attention module. Using our architecture on the improved SpotNet detector, we obtain the state-of-the-art performance on the UA-DETRAC public benchmark as well as on the UAVDT dataset. Code is available at https://github.com/hu64/FFAVOD.



### Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2109.08029v3
- **DOI**: 10.1016/j.eswa.2022.118669
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.08029v3)
- **Published**: 2021-09-15 14:11:29+00:00
- **Updated**: 2022-03-25 08:11:17+00:00
- **Authors**: Ander Salaberria, Gorka Azkune, Oier Lopez de Lacalle, Aitor Soroa, Eneko Agirre
- **Comment**: Under review. 25 pages with 4 figures
- **Journal**: Expert Systems with Applications, Volume 212, 2023, 118669
- **Summary**: Integrating outside knowledge for reasoning in visio-linguistic tasks such as visual question answering (VQA) is an open problem. Given that pretrained language models have been shown to include world knowledge, we propose to use a unimodal (text-only) train and inference procedure based on automatic off-the-shelf captioning of images and pretrained language models. Our results on a visual question answering task which requires external knowledge (OK-VQA) show that our text-only model outperforms pretrained multimodal (image-text) models of comparable number of parameters. In contrast, our model is less effective in a standard VQA task (VQA 2.0) confirming that our text-only method is specially effective for tasks requiring external knowledge. In addition, we show that increasing the language model's size improves notably its performance, yielding results comparable to the state-of-the-art with our largest model, significantly outperforming current multimodal systems, even though augmented with external knowledge. Our qualitative analysis on OK-VQA reveals that automatic captions often fail to capture relevant information in the images, which seems to be balanced by the better inference ability of the text-only language models. Our work opens up possibilities to further improve inference in visio-linguistic tasks



### MD-CSDNetwork: Multi-Domain Cross Stitched Network for Deepfake Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.07311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07311v1)
- **Published**: 2021-09-15 14:11:53+00:00
- **Updated**: 2021-09-15 14:11:53+00:00
- **Authors**: Aayushi Agarwal, Akshay Agarwal, Sayan Sinha, Mayank Vatsa, Richa Singh
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: The rapid progress in the ease of creating and spreading ultra-realistic media over social platforms calls for an urgent need to develop a generalizable deepfake detection technique. It has been observed that current deepfake generation methods leave discriminative artifacts in the frequency spectrum of fake images and videos. Inspired by this observation, in this paper, we present a novel approach, termed as MD-CSDNetwork, for combining the features in the spatial and frequency domains to mine a shared discriminative representation for classifying \textit{deepfakes}. MD-CSDNetwork is a novel cross-stitched network with two parallel branches carrying the spatial and frequency information, respectively. We hypothesize that these multi-domain input data streams can be considered as related supervisory signals. The supervision from both branches ensures better performance and generalization. Further, the concept of cross-stitch connections is utilized where they are inserted between the two branches to learn an optimal combination of domain-specific and shared representations from other domains automatically. Extensive experiments are conducted on the popular benchmark dataset namely FaceForeniscs++ for forgery classification. We report improvements over all the manipulation types in FaceForensics++ dataset and comparable results with state-of-the-art methods for cross-database evaluation on the Celeb-DF dataset and the Deepfake Detection Dataset.



### DeFungi: Direct Mycological Examination of Microscopic Fungi Images
- **Arxiv ID**: http://arxiv.org/abs/2109.07322v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.07322v1)
- **Published**: 2021-09-15 14:25:28+00:00
- **Updated**: 2021-09-15 14:25:28+00:00
- **Authors**: Camilo Javier Pineda Sopo, Farshid Hajati, Soheila Gheisari
- **Comment**: None
- **Journal**: None
- **Summary**: Traditionally, diagnosis and treatment of fungal infections in humans depend heavily on face-to-face consultations or examinations made by specialized laboratory scientists known as mycologists. In many cases, such as the recent mucormycosis spread in the COVID-19 pandemic, an initial treatment can be safely suggested to the patient during the earliest stage of the mycological diagnostic process by performing a direct examination of biopsies or samples through a microscope. Computer-aided diagnosis systems using deep learning models have been trained and used for the late mycological diagnostic stages. However, there are no reference literature works made for the early stages. A mycological laboratory in Colombia donated the images used for the development of this research work. They were manually labelled into five classes and curated with a subject matter expert assistance. The images were later cropped and patched with automated code routines to produce the final dataset. This paper presents experimental results classifying five fungi types using two different deep learning approaches and three different convolutional neural network models, VGG16, Inception V3, and ResNet50. The first approach benchmarks the classification performance for the models trained from scratch, while the second approach benchmarks the classification performance using pre-trained models based on the ImageNet dataset. Using k-fold cross-validation testing on the 5-class dataset, the best performing model trained from scratch was Inception V3, reporting 73.2% accuracy. Also, the best performing model using transfer learning was VGG16 reporting 85.04%. The statistics provided by the two approaches create an initial point of reference to encourage future research works to improve classification performance. Furthermore, the dataset built is published in Kaggle and GitHub to foster future research.



### PointManifoldCut: Point-wise Augmentation in the Manifold for Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2109.07324v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/2109.07324v2)
- **Published**: 2021-09-15 14:31:42+00:00
- **Updated**: 2021-12-13 14:29:02+00:00
- **Authors**: Tianfang Zhu, Yue Guan, Anan Li
- **Comment**: None
- **Journal**: None
- **Summary**: Mixed-based point cloud augmentation is a popular solution to the problem of limited availability of large-scale public datasets. But the mismatch between mixed points and corresponding semantic labels hinders the further application in point-wise tasks such as part segmentation. This paper proposes a point cloud augmentation approach, PointManifoldCut(PMC), which replaces the neural network embedded points, rather than the Euclidean space coordinates. This approach takes the advantage that points at the higher levels of the neural network are already trained to embed its neighbors relations and mixing these representation will not mingle the relation between itself and its label. We set up a spatial transform module after PointManifoldCut operation to align the new instances in the embedded space. The effects of different hidden layers and methods of replacing points are also discussed in this paper. The experiments show that our proposed approach can enhance the performance of point cloud classification as well as segmentation networks, and brings them additional robustness to attacks and geometric transformations. The code of this paper is available at: https://github.com/fun0515/PointManifoldCut.



### S3LAM: Structured Scene SLAM
- **Arxiv ID**: http://arxiv.org/abs/2109.07339v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.07339v2)
- **Published**: 2021-09-15 14:47:42+00:00
- **Updated**: 2022-03-01 23:54:49+00:00
- **Authors**: Mathieu Gonzalez, Eric Marchand, Amine Kacete, Jérôme Royan
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new SLAM system that uses the semantic segmentation of objects and structures in the scene. Semantic information is relevant as it contains high level information which may make SLAM more accurate and robust. Our contribution is twofold: i) A new SLAM system based on ORB-SLAM2 that creates a semantic map made of clusters of points corresponding to objects instances and structures in the scene. ii) A modification of the classical Bundle Adjustment formulation to constrain each cluster using geometrical priors, which improves both camera localization and reconstruction and enables a better understanding of the scene. We evaluate our approach on sequences from several public datasets and show that it improves camera pose estimation with respect to state of the art.



### Learning Dynamical Human-Joint Affinity for 3D Pose Estimation in Videos
- **Arxiv ID**: http://arxiv.org/abs/2109.07353v1
- **DOI**: 10.1109/TIP.2021.3109517
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07353v1)
- **Published**: 2021-09-15 15:06:19+00:00
- **Updated**: 2021-09-15 15:06:19+00:00
- **Authors**: Junhao Zhang, Yali Wang, Zhipeng Zhou, Tianyu Luan, Zhe Wang, Yu Qiao
- **Comment**: Accepted by IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Graph Convolution Network (GCN) has been successfully used for 3D human pose estimation in videos. However, it is often built on the fixed human-joint affinity, according to human skeleton. This may reduce adaptation capacity of GCN to tackle complex spatio-temporal pose variations in videos. To alleviate this problem, we propose a novel Dynamical Graph Network (DG-Net), which can dynamically identify human-joint affinity, and estimate 3D pose by adaptively learning spatial/temporal joint relations from videos. Different from traditional graph convolution, we introduce Dynamical Spatial/Temporal Graph convolution (DSG/DTG) to discover spatial/temporal human-joint affinity for each video exemplar, depending on spatial distance/temporal movement similarity between human joints in this video. Hence, they can effectively understand which joints are spatially closer and/or have consistent motion, for reducing depth ambiguity and/or motion uncertainty when lifting 2D pose to 3D pose. We conduct extensive experiments on three popular benchmarks, e.g., Human3.6M, HumanEva-I, and MPI-INF-3DHP, where DG-Net outperforms a number of recent SOTA approaches with fewer input frames and model size.



### Direct and Sparse Deformable Tracking
- **Arxiv ID**: http://arxiv.org/abs/2109.07370v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07370v1)
- **Published**: 2021-09-15 15:28:10+00:00
- **Updated**: 2021-09-15 15:28:10+00:00
- **Authors**: Jose Lamarca, Juan J. Gomez Rodriguez, Juan D. Tardos, J. M. M. Montiel
- **Comment**: 8 pages, 5 figures, submitted to RAL with ICRA
- **Journal**: None
- **Summary**: Deformable Monocular SLAM algorithms recover the localization of a camera in an unknown deformable environment. Current approaches use a template-based deformable tracking to recover the camera pose and the deformation of the map. These template-based methods use an underlying global deformation model. In this paper, we introduce a novel deformable camera tracking method with a local deformation model for each point. Each map point is defined as a single textured surfel that moves independently of the other map points. Thanks to a direct photometric error cost function, we can track the position and orientation of the surfel without an explicit global deformation model. In our experiments, we validate the proposed system and observe that our local deformation model estimates more accurately and robustly the targeted deformations of the map in both laboratory-controlled experiments and in-body scenarios undergoing non-isometric deformations, with changing topology or discontinuities.



### A Unified Framework for Biphasic Facial Age Translation with Noisy-Semantic Guided Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.07373v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.07373v1)
- **Published**: 2021-09-15 15:30:35+00:00
- **Updated**: 2021-09-15 15:30:35+00:00
- **Authors**: Muyi Sun, Jian Wang, Yunfan Liu, Qi Li, Zhenan Sun
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Biphasic facial age translation aims at predicting the appearance of the input face at any age. Facial age translation has received considerable research attention in the last decade due to its practical value in cross-age face recognition and various entertainment applications. However, most existing methods model age changes between holistic images, regardless of the human face structure and the age-changing patterns of individual facial components. Consequently, the lack of semantic supervision will cause infidelity of generated faces in detail. To this end, we propose a unified framework for biphasic facial age translation with noisy-semantic guided generative adversarial networks. Structurally, we project the class-aware noisy semantic layouts to soft latent maps for the following injection operation on the individual facial parts. In particular, we introduce two sub-networks, ProjectionNet and ConstraintNet. ProjectionNet introduces the low-level structural semantic information with noise map and produces soft latent maps. ConstraintNet disentangles the high-level spatial features to constrain the soft latent maps, which endows more age-related context into the soft latent maps. Specifically, attention mechanism is employed in ConstraintNet for feature disentanglement. Meanwhile, in order to mine the strongest mapping ability of the network, we embed two types of learning strategies in the training procedure, supervised self-driven generation and unsupervised condition-driven cycle-consistent generation. As a result, extensive experiments conducted on MORPH and CACD datasets demonstrate the prominent ability of our proposed method which achieves state-of-the-art performance.



### Semi-supervised Contrastive Learning for Label-efficient Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.07407v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07407v2)
- **Published**: 2021-09-15 16:23:48+00:00
- **Updated**: 2021-09-28 19:44:04+00:00
- **Authors**: Xinrong Hu, Dewen Zeng, Xiaowei Xu, Yiyu Shi
- **Comment**: 9 pages, accepted to MICCAI 2021
- **Journal**: None
- **Summary**: The success of deep learning methods in medical image segmentation tasks heavily depends on a large amount of labeled data to supervise the training. On the other hand, the annotation of biomedical images requires domain knowledge and can be laborious. Recently, contrastive learning has demonstrated great potential in learning latent representation of images even without any label. Existing works have explored its application to biomedical image segmentation where only a small portion of data is labeled, through a pre-training phase based on self-supervised contrastive learning without using any labels followed by a supervised fine-tuning phase on the labeled portion of data only. In this paper, we establish that by including the limited label in formation in the pre-training phase, it is possible to boost the performance of contrastive learning. We propose a supervised local contrastive loss that leverages limited pixel-wise annotation to force pixels with the same label to gather around in the embedding space. Such loss needs pixel-wise computation which can be expensive for large images, and we further propose two strategies, downsampling and block division, to address the issue. We evaluate our methods on two public biomedical image datasets of different modalities. With different amounts of labeled data, our methods consistently outperform the state-of-the-art contrast-based methods and other semi-supervised learning techniques.



### A Wide-area, Low-latency, and Power-efficient 6-DoF Pose Tracking System for Rigid Objects
- **Arxiv ID**: http://arxiv.org/abs/2109.07428v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.SY, eess.SP, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2109.07428v2)
- **Published**: 2021-09-15 17:01:37+00:00
- **Updated**: 2022-01-10 22:37:16+00:00
- **Authors**: Young-Ho Kim, Ankur Kapoor, Tommaso Mansi, Ali Kamen
- **Comment**: None
- **Journal**: None
- **Summary**: Position sensitive detectors (PSDs) offer possibility to track single active marker's two (or three) degrees of freedom (DoF) position with a high accuracy, while having a fast response time with high update frequency and low latency, all using a very simple signal processing circuit. However they are not particularly suitable for 6-DoF object pose tracking system due to lack of orientation measurement, limited tracking range, and sensitivity to environmental variation. We propose a novel 6-DoF pose tracking system for a rigid object tracking requiring a single active marker. The proposed system uses a stereo-based PSD pair and multiple Inertial Measurement Units (IMUs). This is done based on a practical approach to identify and control the power of Infrared-Light Emitting Diode (IR-LED) active markers, with an aim to increase the tracking work space and reduce the power consumption. Our proposed tracking system is validated with three different work space sizes and for static and dynamic positional accuracy using robotic arm manipulator with three different dynamic motion patterns. The results show that the static position root-mean-square (RMS) error is 0.6mm. The dynamic position RMS error is 0.7-0.9mm. The orientation RMS error is between 0.04 and 0.9 degree at varied dynamic motion. Overall, our proposed tracking system is capable of tracking a rigid object pose with sub-millimeter accuracy at the mid range of the work space and sub-degree accuracy for all work space under a lab setting.



### Contact-Aware Retargeting of Skinned Motion
- **Arxiv ID**: http://arxiv.org/abs/2109.07431v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2109.07431v1)
- **Published**: 2021-09-15 17:05:02+00:00
- **Updated**: 2021-09-15 17:05:02+00:00
- **Authors**: Ruben Villegas, Duygu Ceylan, Aaron Hertzmann, Jimei Yang, Jun Saito
- **Comment**: International Conference on Computer Vision (ICCV)
- **Journal**: None
- **Summary**: This paper introduces a motion retargeting method that preserves self-contacts and prevents interpenetration. Self-contacts, such as when hands touch each other or the torso or the head, are important attributes of human body language and dynamics, yet existing methods do not model or preserve these contacts. Likewise, interpenetration, such as a hand passing into the torso, are a typical artifact of motion estimation methods. The input to our method is a human motion sequence and a target skeleton and character geometry. The method identifies self-contacts and ground contacts in the input motion, and optimizes the motion to apply to the output skeleton, while preserving these contacts and reducing interpenetration. We introduce a novel geometry-conditioned recurrent network with an encoder-space optimization strategy that achieves efficient retargeting while satisfying contact constraints. In experiments, our results quantitatively outperform previous methods and we conduct a user study where our retargeted motions are rated as higher-quality than those produced by recent works. We also show our method generalizes to motion estimated from human videos where we improve over previous works that produce noticeable interpenetration.



### Neural Human Performer: Learning Generalizable Radiance Fields for Human Performance Rendering
- **Arxiv ID**: http://arxiv.org/abs/2109.07448v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2109.07448v1)
- **Published**: 2021-09-15 17:32:46+00:00
- **Updated**: 2021-09-15 17:32:46+00:00
- **Authors**: Youngjoong Kwon, Dahun Kim, Duygu Ceylan, Henry Fuchs
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we aim at synthesizing a free-viewpoint video of an arbitrary human performance using sparse multi-view cameras. Recently, several works have addressed this problem by learning person-specific neural radiance fields (NeRF) to capture the appearance of a particular human. In parallel, some work proposed to use pixel-aligned features to generalize radiance fields to arbitrary new scenes and objects. Adopting such generalization approaches to humans, however, is highly challenging due to the heavy occlusions and dynamic articulations of body parts. To tackle this, we propose Neural Human Performer, a novel approach that learns generalizable neural radiance fields based on a parametric human body model for robust performance capture. Specifically, we first introduce a temporal transformer that aggregates tracked visual features based on the skeletal body motion over time. Moreover, a multi-view transformer is proposed to perform cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that our method significantly outperforms recent generalizable NeRF methods on unseen identities and poses. The video results and code are available at https://youngjoongunc.github.io/nhp.



### Deep Bregman Divergence for Contrastive Learning of Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2109.07455v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.07455v2)
- **Published**: 2021-09-15 17:44:40+00:00
- **Updated**: 2021-11-22 23:22:00+00:00
- **Authors**: Mina Rezaei, Farzin Soleymani, Bernd Bischl, Shekoofeh Azizi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Bregman divergence measures divergence of data points using neural networks which is beyond Euclidean distance and capable of capturing divergence over distributions. In this paper, we propose deep Bregman divergences for contrastive learning of visual representation where we aim to enhance contrastive loss used in self-supervised learning by training additional networks based on functional Bregman divergence. In contrast to the conventional contrastive learning methods which are solely based on divergences between single points, our framework can capture the divergence between distributions which improves the quality of learned representation. We show the combination of conventional contrastive loss and our proposed divergence loss outperforms baseline and most of the previous methods for self-supervised and semi-supervised learning on multiple classifications and object detection tasks and datasets. Moreover, the learned representations generalize well when transferred to the other datasets and tasks. The source code and our models are available in supplementary and will be released with paper.



### Sign-MAML: Efficient Model-Agnostic Meta-Learning by SignSGD
- **Arxiv ID**: http://arxiv.org/abs/2109.07497v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.07497v2)
- **Published**: 2021-09-15 18:01:55+00:00
- **Updated**: 2021-12-20 21:36:48+00:00
- **Authors**: Chen Fan, Parikshit Ram, Sijia Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new computationally-efficient first-order algorithm for Model-Agnostic Meta-Learning (MAML). The key enabling technique is to interpret MAML as a bilevel optimization (BLO) problem and leverage the sign-based SGD(signSGD) as a lower-level optimizer of BLO. We show that MAML, through the lens of signSGD-oriented BLO, naturally yields an alternating optimization scheme that just requires first-order gradients of a learned meta-model. We term the resulting MAML algorithm Sign-MAML. Compared to the conventional first-order MAML (FO-MAML) algorithm, Sign-MAML is theoretically-grounded as it does not impose any assumption on the absence of second-order derivatives during meta training. In practice, we show that Sign-MAML outperforms FO-MAML in various few-shot image classification tasks, and compared to MAML, it achieves a much more graceful tradeoff between classification accuracy and computation efficiency.



### Federated Contrastive Learning for Decentralized Unlabeled Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2109.07504v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.07504v1)
- **Published**: 2021-09-15 18:08:31+00:00
- **Updated**: 2021-09-15 18:08:31+00:00
- **Authors**: Nanqing Dong, Irina Voiculescu
- **Comment**: Accepted by MICCAI 2021
- **Journal**: None
- **Summary**: A label-efficient paradigm in computer vision is based on self-supervised contrastive pre-training on unlabeled data followed by fine-tuning with a small number of labels. Making practical use of a federated computing environment in the clinical domain and learning on medical images poses specific challenges. In this work, we propose FedMoCo, a robust federated contrastive learning (FCL) framework, which makes efficient use of decentralized unlabeled medical data. FedMoCo has two novel modules: metadata transfer, an inter-node statistical data augmentation module, and self-adaptive aggregation, an aggregation module based on representational similarity analysis. To the best of our knowledge, this is the first FCL work on medical images. Our experiments show that FedMoCo can consistently outperform FedAvg, a seminal federated learning framework, in extracting meaningful representations for downstream tasks. We further show that FedMoCo can substantially reduce the amount of labeled data required in a downstream task, such as COVID-19 detection, to achieve a reasonable performance.



### Learning to Aggregate and Refine Noisy Labels for Visual Sentiment Analysis
- **Arxiv ID**: http://arxiv.org/abs/2109.07509v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.07509v2)
- **Published**: 2021-09-15 18:18:28+00:00
- **Updated**: 2022-01-27 03:27:16+00:00
- **Authors**: Wei Zhu, Zihe Zheng, Haitian Zheng, Hanjia Lyu, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Visual sentiment analysis has received increasing attention in recent years. However, the dataset's quality is a concern because the sentiment labels are crowd-sourcing, subjective, and prone to mistakes, and poses a severe threat to the data-driven models, especially the deep neural networks. The deep models would generalize poorly on the testing cases when trained to over-fit the training samples with noisy sentiment labels. Inspired by the recent progress on learning with noisy labels, we propose a robust learning method to perform robust visual sentiment analysis. Our method relies on external memory to aggregate and filters noisy labels during training. The memory is composed of the prototypes with corresponding labels, which can be updated online. The learned prototypes and their labels can be regarded as denoising features and labels for the local regions and can guide the training process to prevent the model from overfitting the noisy cases. We establish a benchmark for visual sentiment analysis with label noise using publicly available datasets. The experiment results of the proposed benchmark settings comprehensively show the effectiveness of our method.



### Pose Transformers (POTR): Human Motion Prediction with Non-Autoregressive Transformers
- **Arxiv ID**: http://arxiv.org/abs/2109.07531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07531v1)
- **Published**: 2021-09-15 18:55:15+00:00
- **Updated**: 2021-09-15 18:55:15+00:00
- **Authors**: Angel Martínez-González, Michael Villamizar, Jean-Marc Odobez
- **Comment**: Accepted in ICCV-W
- **Journal**: None
- **Summary**: We propose to leverage Transformer architectures for non-autoregressive human motion prediction. Our approach decodes elements in parallel from a query sequence, instead of conditioning on previous predictions such as instate-of-the-art RNN-based approaches. In such a way our approach is less computational intensive and potentially avoids error accumulation to long term elements in the sequence. In that context, our contributions are fourfold: (i) we frame human motion prediction as a sequence-to-sequence problem and propose a non-autoregressive Transformer to infer the sequences of poses in parallel; (ii) we propose to decode sequences of 3D poses from a query sequence generated in advance with elements from the input sequence;(iii) we propose to perform skeleton-based activity classification from the encoder memory, in the hope that identifying the activity can improve predictions;(iv) we show that despite its simplicity, our approach achieves competitive results in two public datasets, although surprisingly more for short term predictions rather than for long term ones.



### RAFT-Stereo: Multilevel Recurrent Field Transforms for Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2109.07547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07547v1)
- **Published**: 2021-09-15 19:27:31+00:00
- **Updated**: 2021-09-15 19:27:31+00:00
- **Authors**: Lahav Lipson, Zachary Teed, Jia Deng
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce RAFT-Stereo, a new deep architecture for rectified stereo based on the optical flow network RAFT. We introduce multi-level convolutional GRUs, which more efficiently propagate information across the image. A modified version of RAFT-Stereo can perform accurate real-time inference. RAFT-stereo ranks first on the Middlebury leaderboard, outperforming the next best method on 1px error by 29% and outperforms all published work on the ETH3D two-view stereo benchmark. Code is available at https://github.com/princeton-vl/RAFT-Stereo.



### Learning the Regularization in DCE-MR Image Reconstruction for Functional Imaging of Kidneys
- **Arxiv ID**: http://arxiv.org/abs/2109.07548v2
- **DOI**: 10.1109/ACCESS.2021.3139854
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.07548v2)
- **Published**: 2021-09-15 19:27:53+00:00
- **Updated**: 2021-12-30 16:55:50+00:00
- **Authors**: Aziz Koçanaoğulları, Cemre Ariyurek, Onur Afacan, Sila Kurugol
- **Comment**: None
- **Journal**: None
- **Summary**: Kidney DCE-MRI aims at both qualitative assessment of kidney anatomy and quantitative assessment of kidney function by estimating the tracer kinetic (TK) model parameters. Accurate estimation of TK model parameters requires an accurate measurement of the arterial input function (AIF) with high temporal resolution. Accelerated imaging is used to achieve high temporal resolution, which yields under-sampling artifacts in the reconstructed images. Compressed sensing (CS) methods offer a variety of reconstruction options. Most commonly, sparsity of temporal differences is encouraged for regularization to reduce artifacts. Increasing regularization in CS methods removes the ambient artifacts but also over-smooths the signal temporally which reduces the parameter estimation accuracy. In this work, we propose a single image trained deep neural network to reduce MRI under-sampling artifacts without reducing the accuracy of functional imaging markers. Instead of regularizing with a penalty term in optimization, we promote regularization by generating images from a lower dimensional representation. In this manuscript we motivate and explain the lower dimensional input design. We compare our approach to CS reconstructions with multiple regularization weights. Proposed approach results in kidney biomarkers that are highly correlated with the ground truth markers estimated using the CS reconstruction which was optimized for functional analysis. At the same time, the proposed approach reduces the artifacts in the reconstructed images.



### A Pathology Deep Learning System Capable of Triage of Melanoma Specimens Utilizing Dermatopathologist Consensus as Ground Truth
- **Arxiv ID**: http://arxiv.org/abs/2109.07554v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.07554v1)
- **Published**: 2021-09-15 19:48:51+00:00
- **Updated**: 2021-09-15 19:48:51+00:00
- **Authors**: Sivaramakrishnan Sankarapandian, Saul Kohn, Vaughn Spurrier, Sean Grullon, Rajath E. Soans, Kameswari D. Ayyagari, Ramachandra V. Chamarthi, Kiran Motaparthi, Jason B. Lee, Wonwoo Shon, Michael Bonham, Julianna D. Ianni
- **Comment**: Accepted at ICCV 2021 CDpath workshop
- **Journal**: None
- **Summary**: Although melanoma occurs more rarely than several other skin cancers, patients' long term survival rate is extremely low if the diagnosis is missed. Diagnosis is complicated by a high discordance rate among pathologists when distinguishing between melanoma and benign melanocytic lesions. A tool that allows pathology labs to sort and prioritize melanoma cases in their workflow could improve turnaround time by prioritizing challenging cases and routing them directly to the appropriate subspecialist. We present a pathology deep learning system (PDLS) that performs hierarchical classification of digitized whole slide image (WSI) specimens into six classes defined by their morphological characteristics, including classification of "Melanocytic Suspect" specimens likely representing melanoma or severe dysplastic nevi. We trained the system on 7,685 images from a single lab (the reference lab), including the the largest set of triple-concordant melanocytic specimens compiled to date, and tested the system on 5,099 images from two distinct validation labs. We achieved Area Underneath the ROC Curve (AUC) values of 0.93 classifying Melanocytic Suspect specimens on the reference lab, 0.95 on the first validation lab, and 0.82 on the second validation lab. We demonstrate that the PDLS is capable of automatically sorting and triaging skin specimens with high sensitivity to Melanocytic Suspect cases and that a pathologist would only need between 30% and 60% of the caseload to address all melanoma specimens.



### Hybrid ICP
- **Arxiv ID**: http://arxiv.org/abs/2109.07559v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.07559v1)
- **Published**: 2021-09-15 20:17:11+00:00
- **Updated**: 2021-09-15 20:17:11+00:00
- **Authors**: Kamil Dreczkowski, Edward Johns
- **Comment**: Published at IROS 2021. Webpage and video:
  https://www.robot-learning.uk/hybrid-icp
- **Journal**: None
- **Summary**: ICP algorithms typically involve a fixed choice of data association method and a fixed choice of error metric. In this paper, we propose Hybrid ICP, a novel and flexible ICP variant which dynamically optimises both the data association method and error metric based on the live image of an object and the current ICP estimate. We show that when used for object pose estimation, Hybrid ICP is more accurate and more robust to noise than other commonly used ICP variants. We also consider the setting where ICP is applied sequentially with a moving camera, and we study the trade-off between the accuracy of each ICP estimate and the number of ICP estimates available within a fixed amount of time.



### A Framework for Multisensory Foresight for Embodied Agents
- **Arxiv ID**: http://arxiv.org/abs/2109.07561v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.07561v1)
- **Published**: 2021-09-15 20:20:04+00:00
- **Updated**: 2021-09-15 20:20:04+00:00
- **Authors**: Xiaohui Chen, Ramtin Hosseini, Karen Panetta, Jivko Sinapov
- **Comment**: ICRA 2021
- **Journal**: None
- **Summary**: Predicting future sensory states is crucial for learning agents such as robots, drones, and autonomous vehicles. In this paper, we couple multiple sensory modalities with exploratory actions and propose a predictive neural network architecture to address this problem. Most existing approaches rely on large, manually annotated datasets, or only use visual data as a single modality. In contrast, the unsupervised method presented here uses multi-modal perceptions for predicting future visual frames. As a result, the proposed model is more comprehensive and can better capture the spatio-temporal dynamics of the environment, leading to more accurate visual frame prediction. The other novelty of our framework is the use of sub-networks dedicated to anticipating future haptic, audio, and tactile signals. The framework was tested and validated with a dataset containing 4 sensory modalities (vision, haptic, audio, and tactile) on a humanoid robot performing 9 behaviors multiple times on a large set of objects. While the visual information is the dominant modality, utilizing the additional non-visual modalities improves the accuracy of predictions.



### Predicting 3D shapes, masks, and properties of materials, liquids, and objects inside transparent containers, using the TransProteus CGI dataset
- **Arxiv ID**: http://arxiv.org/abs/2109.07577v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07577v2)
- **Published**: 2021-09-15 21:16:36+00:00
- **Updated**: 2021-12-20 21:12:50+00:00
- **Authors**: Sagi Eppel, Haoping Xu, Yi Ru Wang, Alan Aspuru-Guzik
- **Comment**: None
- **Journal**: None
- **Summary**: We present TransProteus, a dataset, and methods for predicting the 3D structure, masks, and properties of materials, liquids, and objects inside transparent vessels from a single image without prior knowledge of the image source and camera parameters. Manipulating materials in transparent containers is essential in many fields and depends heavily on vision. This work supplies a new procedurally generated dataset consisting of 50k images of liquids and solid objects inside transparent containers. The image annotations include 3D models, material properties (color/transparency/roughness...), and segmentation masks for the vessel and its content. The synthetic (CGI) part of the dataset was procedurally generated using 13k different objects, 500 different environments (HDRI), and 1450 material textures (PBR) combined with simulated liquids and procedurally generated vessels. In addition, we supply 104 real-world images of objects inside transparent vessels with depth maps of both the vessel and its content. We propose a camera agnostic method that predicts 3D models from an image as an XYZ map. This allows the trained net to predict the 3D model as a map with XYZ coordinates per pixel without prior knowledge of the image source. To calculate the training loss, we use the distance between pairs of points inside the 3D model instead of the absolute XYZ coordinates. This makes the loss function translation invariant. We use this to predict 3D models of vessels and their content from a single image. Finally, we demonstrate a net that uses a single image to predict the material properties of the vessel content and surface.



### UCP-Net: Unstructured Contour Points for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.07592v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07592v1)
- **Published**: 2021-09-15 22:03:37+00:00
- **Updated**: 2021-09-15 22:03:37+00:00
- **Authors**: Camille Dupont, Yanis Ouakrim, Quoc Cuong Pham
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of interactive segmentation is to assist users in producing segmentation masks as fast and as accurately as possible. Interactions have to be simple and intuitive and the number of interactions required to produce a satisfactory segmentation mask should be as low as possible. In this paper, we propose a novel approach to interactive segmentation based on unconstrained contour clicks for initial segmentation and segmentation refinement. Our method is class-agnostic and produces accurate segmentation masks (IoU > 85%) for a lower number of user interactions than state-of-the-art methods on popular segmentation datasets (COCO MVal, SBD and Berkeley).



### Partner-Assisted Learning for Few-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.07607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07607v1)
- **Published**: 2021-09-15 22:46:19+00:00
- **Updated**: 2021-09-15 22:46:19+00:00
- **Authors**: Jiawei Ma, Hanchen Xie, Guangxing Han, Shih-Fu Chang, Aram Galstyan, Wael Abd-Almageed
- **Comment**: ICCV2021 Camera Ready
- **Journal**: None
- **Summary**: Few-shot Learning has been studied to mimic human visual capabilities and learn effective models without the need of exhaustive human annotation. Even though the idea of meta-learning for adaptation has dominated the few-shot learning methods, how to train a feature extractor is still a challenge. In this paper, we focus on the design of training strategy to obtain an elemental representation such that the prototype of each novel class can be estimated from a few labeled samples. We propose a two-stage training scheme, Partner-Assisted Learning (PAL), which first trains a partner encoder to model pair-wise similarities and extract features serving as soft-anchors, and then trains a main encoder by aligning its outputs with soft-anchors while attempting to maximize classification performance. Two alignment constraints from logit-level and feature-level are designed individually. For each few-shot task, we perform prototype classification. Our method consistently outperforms the state-of-the-art method on four benchmarks. Detailed ablation studies of PAL are provided to justify the selection of each component involved in training.



