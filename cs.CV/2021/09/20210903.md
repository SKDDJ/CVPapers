# Arxiv Papers in cs.CV on 2021-09-03
### Global-Local Transformer for Brain Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.01663v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01663v1)
- **Published**: 2021-09-03 01:39:58+00:00
- **Updated**: 2021-09-03 01:39:58+00:00
- **Authors**: Sheng He, P. Ellen Grant, Yangming Ou
- **Comment**: To appear: IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Deep learning can provide rapid brain age estimation based on brain magnetic resonance imaging (MRI). However, most studies use one neural network to extract the global information from the whole input image, ignoring the local fine-grained details. In this paper, we propose a global-local transformer, which consists of a global-pathway to extract the global-context information from the whole input image and a local-pathway to extract the local fine-grained details from local patches. The fine-grained information from the local patches are fused with the global-context information by the attention mechanism, inspired by the transformer, to estimate the brain age. We evaluate the proposed method on 8 public datasets with 8,379 healthy brain MRIs with the age range of 0-97 years. 6 datasets are used for cross-validation and 2 datasets are used for evaluating the generality. Comparing with other state-of-the-art methods, the proposed global-local transformer reduces the mean absolute error of the estimated ages to 2.70 years and increases the correlation coefficient of the estimated age and the chronological age to 0.9853. In addition, our proposed method provides regional information of which local patches are most informative for brain age estimation. Our source code is available on: \url{https://github.com/shengfly/global-local-transformer}.



### LATFormer: Locality-Aware Point-View Fusion Transformer for 3D Shape Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.01291v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01291v2)
- **Published**: 2021-09-03 03:23:27+00:00
- **Updated**: 2023-08-25 15:02:38+00:00
- **Authors**: Xinwei He, Silin Cheng, Dingkang Liang, Song Bai, Xi Wang, Yingying Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, 3D shape understanding has achieved significant progress due to the advances of deep learning models on various data formats like images, voxels, and point clouds. Among them, point clouds and multi-view images are two complementary modalities of 3D objects and learning representations by fusing both of them has been proven to be fairly effective. While prior works typically focus on exploiting global features of the two modalities, herein we argue that more discriminative features can be derived by modeling ``where to fuse''. To investigate this, we propose a novel Locality-Aware Point-View Fusion Transformer (LATFormer) for 3D shape retrieval and classification. The core component of LATFormer is a module named Locality-Aware Fusion (LAF) which integrates the local features of correlated regions across the two modalities based on the co-occurrence scores. We further propose to filter out scores with low values to obtain salient local co-occurring regions, which reduces redundancy for the fusion process. In our LATFormer, we utilize the LAF module to fuse the multi-scale features of the two modalities both bidirectionally and hierarchically to obtain more informative features. Comprehensive experiments on four popular 3D shape benchmarks covering 3D object retrieval and classification validate its effectiveness.



### Information Symmetry Matters: A Modal-Alternating Propagation Network for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.01295v1
- **DOI**: 10.1109/TIP.2022.3143005
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.01295v1)
- **Published**: 2021-09-03 03:43:53+00:00
- **Updated**: 2021-09-03 03:43:53+00:00
- **Authors**: Zhong Ji, Zhishen Hou, Xiyao Liu, Yanwei Pang, Jungong Han
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic information provides intra-class consistency and inter-class discriminability beyond visual concepts, which has been employed in Few-Shot Learning (FSL) to achieve further gains. However, semantic information is only available for labeled samples but absent for unlabeled samples, in which the embeddings are rectified unilaterally by guiding the few labeled samples with semantics. Therefore, it is inevitable to bring a cross-modal bias between semantic-guided samples and nonsemantic-guided samples, which results in an information asymmetry problem. To address this problem, we propose a Modal-Alternating Propagation Network (MAP-Net) to supplement the absent semantic information of unlabeled samples, which builds information symmetry among all samples in both visual and semantic modalities. Specifically, the MAP-Net transfers the neighbor information by the graph propagation to generate the pseudo-semantics for unlabeled samples guided by the completed visual relationships and rectify the feature embeddings. In addition, due to the large discrepancy between visual and semantic modalities, we design a Relation Guidance (RG) strategy to guide the visual relation vectors via semantics so that the propagated information is more beneficial. Extensive experimental results on three semantic-labeled datasets, i.e., Caltech-UCSD-Birds 200-2011, SUN Attribute Database, and Oxford 102 Flower, have demonstrated that our proposed method achieves promising performance and outperforms the state-of-the-art approaches, which indicates the necessity of information symmetry.



### Self-Taught Cross-Domain Few-Shot Learning with Weakly Supervised Object Localization and Task-Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2109.01302v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.01302v1)
- **Published**: 2021-09-03 04:23:07+00:00
- **Updated**: 2021-09-03 04:23:07+00:00
- **Authors**: Xiyao Liu, Zhong Ji, Yanwei Pang, Zhongfei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The domain shift between the source and target domain is the main challenge in Cross-Domain Few-Shot Learning (CD-FSL). However, the target domain is absolutely unknown during the training on the source domain, which results in lacking directed guidance for target tasks. We observe that since there are similar backgrounds in target domains, it can apply self-labeled samples as prior tasks to transfer knowledge onto target tasks. To this end, we propose a task-expansion-decomposition framework for CD-FSL, called Self-Taught (ST) approach, which alleviates the problem of non-target guidance by constructing task-oriented metric spaces. Specifically, Weakly Supervised Object Localization (WSOL) and self-supervised technologies are employed to enrich task-oriented samples by exchanging and rotating the discriminative regions, which generates a more abundant task set. Then these tasks are decomposed into several tasks to finish the task of few-shot recognition and rotation classification. It helps to transfer the source knowledge onto the target tasks and focus on discriminative regions. We conduct extensive experiments under the cross-domain setting including 8 target domains: CUB, Cars, Places, Plantae, CropDieases, EuroSAT, ISIC, and ChestX. Experimental results demonstrate that the proposed ST approach is applicable to various metric-based models, and provides promising improvements in CD-FSL.



### Self-supervised Pseudo Multi-class Pre-training for Unsupervised Anomaly Detection and Segmentation in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2109.01303v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01303v3)
- **Published**: 2021-09-03 04:25:57+00:00
- **Updated**: 2023-08-14 21:32:45+00:00
- **Authors**: Yu Tian, Fengbei Liu, Guansong Pang, Yuanhong Chen, Yuyuan Liu, Johan W. Verjans, Rajvinder Singh, Gustavo Carneiro
- **Comment**: Accepted to Medical Image Analysis
- **Journal**: None
- **Summary**: Unsupervised anomaly detection (UAD) methods are trained with normal (or healthy) images only, but during testing, they are able to classify normal and abnormal (or disease) images. UAD is an important medical image analysis (MIA) method to be applied in disease screening problems because the training sets available for those problems usually contain only normal images. However, the exclusive reliance on normal images may result in the learning of ineffective low-dimensional image representations that are not sensitive enough to detect and segment unseen abnormal lesions of varying size, appearance, and shape. Pre-training UAD methods with self-supervised learning, based on computer vision techniques, can mitigate this challenge, but they are sub-optimal because they do not explore domain knowledge for designing the pretext tasks, and their contrastive learning losses do not try to cluster the normal training images, which may result in a sparse distribution of normal images that is ineffective for anomaly detection. In this paper, we propose a new self-supervised pre-training method for MIA UAD applications, named Pseudo Multi-class Strong Augmentation via Contrastive Learning (PMSACL). PMSACL consists of a novel optimisation method that contrasts a normal image class from multiple pseudo classes of synthesised abnormal images, with each class enforced to form a dense cluster in the feature space. In the experiments, we show that our PMSACL pre-training improves the accuracy of SOTA UAD methods on many MIA benchmarks using colonoscopy, fundus screening and Covid-19 Chest X-ray datasets. The code is made publicly available via https://github.com/tianyu0207/PMSACL.



### Video Pose Distillation for Few-Shot, Fine-Grained Sports Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.01305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01305v1)
- **Published**: 2021-09-03 04:36:12+00:00
- **Updated**: 2021-09-03 04:36:12+00:00
- **Authors**: James Hong, Matthew Fisher, Michaël Gharbi, Kayvon Fatahalian
- **Comment**: ICCV 2021 (poster)
- **Journal**: None
- **Summary**: Human pose is a useful feature for fine-grained sports action understanding. However, pose estimators are often unreliable when run on sports video due to domain shift and factors such as motion blur and occlusions. This leads to poor accuracy when downstream tasks, such as action recognition, depend on pose. End-to-end learning circumvents pose, but requires more labels to generalize.   We introduce Video Pose Distillation (VPD), a weakly-supervised technique to learn features for new video domains, such as individual sports that challenge pose estimation. Under VPD, a student network learns to extract robust pose features from RGB frames in the sports video, such that, whenever pose is considered reliable, the features match the output of a pretrained teacher pose detector. Our strategy retains the best of both pose and end-to-end worlds, exploiting the rich visual patterns in raw video frames, while learning features that agree with the athletes' pose and motion in the target video domain to avoid over-fitting to patterns unrelated to athletes' motion.   VPD features improve performance on few-shot, fine-grained action recognition, retrieval, and detection tasks in four real-world sports video datasets, without requiring additional ground-truth pose annotations.



### Unsupervised multi-latent space reinforcement learning framework for video summarization in ultrasound imaging
- **Arxiv ID**: http://arxiv.org/abs/2109.01309v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01309v1)
- **Published**: 2021-09-03 04:50:35+00:00
- **Updated**: 2021-09-03 04:50:35+00:00
- **Authors**: Roshan P Mathews, Mahesh Raveendranatha Panicker, Abhilash R Hareendranathan, Yale Tung Chen, Jacob L Jaremko, Brian Buchanan, Kiran Vishnu Narayan, Kesavadas C, Greeta Mathews
- **Comment**: 24 pages, submitted to Elsevier Medical Image Analysis for review
- **Journal**: None
- **Summary**: The COVID-19 pandemic has highlighted the need for a tool to speed up triage in ultrasound scans and provide clinicians with fast access to relevant information. The proposed video-summarization technique is a step in this direction that provides clinicians access to relevant key-frames from a given ultrasound scan (such as lung ultrasound) while reducing resource, storage and bandwidth requirements. We propose a new unsupervised reinforcement learning (RL) framework with novel rewards that facilitates unsupervised learning avoiding tedious and impractical manual labelling for summarizing ultrasound videos to enhance its utility as a triage tool in the emergency department (ED) and for use in telemedicine. Using an attention ensemble of encoders, the high dimensional image is projected into a low dimensional latent space in terms of: a) reduced distance with a normal or abnormal class (classifier encoder), b) following a topology of landmarks (segmentation encoder), and c) the distance or topology agnostic latent representation (convolutional autoencoders). The decoder is implemented using a bi-directional long-short term memory (Bi-LSTM) which utilizes the latent space representation from the encoder. Our new paradigm for video summarization is capable of delivering classification labels and segmentation of key landmarks for each of the summarized keyframes. Validation is performed on lung ultrasound (LUS) dataset, that typically represent potential use cases in telemedicine and ED triage acquired from different medical centers across geographies (India, Spain and Canada).



### Semantic Segmentation on VSPW Dataset through Aggregation of Transformer Models
- **Arxiv ID**: http://arxiv.org/abs/2109.01316v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01316v1)
- **Published**: 2021-09-03 05:20:08+00:00
- **Updated**: 2021-09-03 05:20:08+00:00
- **Authors**: Zixuan Chen, Junhong Zou, Xiaotao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation is an important task in computer vision, from which some important usage scenarios are derived, such as autonomous driving, scene parsing, etc. Due to the emphasis on the task of video semantic segmentation, we participated in this competition. In this report, we briefly introduce the solutions of team 'BetterThing' for the ICCV2021 - Video Scene Parsing in the Wild Challenge. Transformer is used as the backbone for extracting video frame features, and the final result is the aggregation of the output of two Transformer models, SWIN and VOLO. This solution achieves 57.3% mIoU, which is ranked 3rd place in the Video Scene Parsing in the Wild Challenge.



### Analysis of MRI Biomarkers for Brain Cancer Survival Prediction
- **Arxiv ID**: http://arxiv.org/abs/2109.02785v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02785v1)
- **Published**: 2021-09-03 05:35:47+00:00
- **Updated**: 2021-09-03 05:35:47+00:00
- **Authors**: Subhashis Banerjee, Sushmita Mitra, Lawrence O. Hall
- **Comment**: None
- **Journal**: None
- **Summary**: Prediction of Overall Survival (OS) of brain cancer patients from multi-modal MRI is a challenging field of research. Most of the existing literature on survival prediction is based on Radiomic features, which does not consider either non-biological factors or the functional neurological status of the patient(s). Besides, the selection of an appropriate cut-off for survival and the presence of censored data create further problems. Application of deep learning models for OS prediction is also limited due to the lack of large annotated publicly available datasets. In this scenario we analyse the potential of two novel neuroimaging feature families, extracted from brain parcellation atlases and spatial habitats, along with classical radiomic and geometric features; to study their combined predictive power for analysing overall survival. A cross validation strategy with grid search is proposed to simultaneously select and evaluate the most predictive feature subset based on its predictive power. A Cox Proportional Hazard (CoxPH) model is employed for univariate feature selection, followed by the prediction of patient-specific survival functions by three multivariate parsimonious models viz. Coxnet, Random survival forests (RSF) and Survival SVM (SSVM). The brain cancer MRI data used for this research was taken from two open-access collections TCGA-GBM and TCGA-LGG available from The Cancer Imaging Archive (TCIA). Corresponding survival data for each patient was downloaded from The Cancer Genome Atlas (TCGA). A high cross validation $C-index$ score of $0.82\pm.10$ was achieved using RSF with the best $24$ selected features. Age was found to be the most important biological predictor. There were $9$, $6$, $6$ and $2$ features selected from the parcellation, habitat, radiomic and region-based feature groups respectively.



### Exploring Separable Attention for Multi-Contrast MR Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2109.01664v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01664v2)
- **Published**: 2021-09-03 05:53:07+00:00
- **Updated**: 2022-08-22 00:38:30+00:00
- **Authors**: Chun-Mei Feng, Yunlu Yan, Kai Yu, Yong Xu, Ling Shao, Huazhu Fu
- **Comment**: arXiv admin note: text overlap with arXiv:2105.08949
  https://github.com/chunmeifeng/SANet
- **Journal**: None
- **Summary**: Super-resolving the Magnetic Resonance (MR) image of a target contrast under the guidance of the corresponding auxiliary contrast, which provides additional anatomical information, is a new and effective solution for fast MR imaging. However, current multi-contrast super-resolution (SR) methods tend to concatenate different contrasts directly, ignoring their relationships in different clues, e.g., in the high-intensity and low-intensity regions. In this study, we propose a separable attention network (comprising high-intensity priority attention and low-intensity separation attention), named SANet. Our SANet could explore the areas of high-intensity and low-intensity regions in the "forward" and "reverse" directions with the help of the auxiliary contrast, while learning clearer anatomical structure and edge information for the SR of a target-contrast MR image. SANet provides three appealing benefits: (1) It is the first model to explore a separable attention mechanism that uses the auxiliary contrast to predict the high-intensity and low-intensity regions regions, diverting more attention to refining any uncertain details between these regions and correcting the fine areas in the reconstructed results. (2) A multi-stage integration module is proposed to learn the response of multi-contrast fusion at multiple stages, get the dependency between the fused representations, and boost their representation ability. (3) Extensive experiments with various state-of-the-art multi-contrast SR methods on fastMRI and clinical \textit{in vivo} datasets demonstrate the superiority of our model.



### Access Control Using Spatially Invariant Permutation of Feature Maps for Semantic Segmentation Models
- **Arxiv ID**: http://arxiv.org/abs/2109.01332v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01332v1)
- **Published**: 2021-09-03 06:23:42+00:00
- **Updated**: 2021-09-03 06:23:42+00:00
- **Authors**: Hiroki Ito, MaungMaung AprilPyone, Hitoshi Kiya
- **Comment**: To appear in 13th Asia Pacific Signal and Information Processing
  Association Annual Summit and Conference (APSIPA ASC 2021)
- **Journal**: None
- **Summary**: In this paper, we propose an access control method that uses the spatially invariant permutation of feature maps with a secret key for protecting semantic segmentation models. Segmentation models are trained and tested by permuting selected feature maps with a secret key. The proposed method allows rightful users with the correct key not only to access a model to full capacity but also to degrade the performance for unauthorized users. Conventional access control methods have focused only on image classification tasks, and these methods have never been applied to semantic segmentation tasks. In an experiment, the protected models were demonstrated to allow rightful users to obtain almost the same performance as that of non-protected models but also to be robust against access by unauthorized users without a key. In addition, a conventional method with block-wise transformations was also verified to have degraded performance under semantic segmentation models.



### Complementary Calibration: Boosting General Continual Learning with Collaborative Distillation and Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2109.02426v2
- **DOI**: 10.1109/TIP.2022.3230457
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02426v2)
- **Published**: 2021-09-03 06:35:27+00:00
- **Updated**: 2022-12-29 03:03:09+00:00
- **Authors**: Zhong Ji, Jin Li, Qiang Wang, Zhongfei Zhang
- **Comment**: Paper is available at https://ieeexplore.ieee.org/document/10002397.
  Code is available at https://github.com/lijincm/CoCa
- **Journal**: IEEE Transactions on Image Processing 2023
- **Summary**: General Continual Learning (GCL) aims at learning from non independent and identically distributed stream data without catastrophic forgetting of the old tasks that don't rely on task boundaries during both training and testing stages. We reveal that the relation and feature deviations are crucial problems for catastrophic forgetting, in which relation deviation refers to the deficiency of the relationship among all classes in knowledge distillation, and feature deviation refers to indiscriminative feature representations. To this end, we propose a Complementary Calibration (CoCa) framework by mining the complementary model's outputs and features to alleviate the two deviations in the process of GCL. Specifically, we propose a new collaborative distillation approach for addressing the relation deviation. It distills model's outputs by utilizing ensemble dark knowledge of new model's outputs and reserved outputs, which maintains the performance of old tasks as well as balancing the relationship among all classes. Furthermore, we explore a collaborative self-supervision idea to leverage pretext tasks and supervised contrastive learning for addressing the feature deviation problem by learning complete and discriminative features for all classes. Extensive experiments on four popular datasets show that our CoCa framework achieves superior performance against state-of-the-art methods. Code is available at https://github.com/lijincm/CoCa.



### Dual-Camera Super-Resolution with Aligned Attention Modules
- **Arxiv ID**: http://arxiv.org/abs/2109.01349v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01349v2)
- **Published**: 2021-09-03 07:17:31+00:00
- **Updated**: 2021-09-06 11:35:34+00:00
- **Authors**: Tengfei Wang, Jiaxin Xie, Wenxiu Sun, Qiong Yan, Qifeng Chen
- **Comment**: Accepted to ICCV 2021 (oral)
- **Journal**: None
- **Summary**: We present a novel approach to reference-based super-resolution (RefSR) with the focus on dual-camera super-resolution (DCSR), which utilizes reference images for high-quality and high-fidelity results. Our proposed method generalizes the standard patch-based feature matching with spatial alignment operations. We further explore the dual-camera super-resolution that is one promising application of RefSR, and build a dataset that consists of 146 image pairs from the main and telephoto cameras in a smartphone. To bridge the domain gaps between real-world images and the training images, we propose a self-supervised domain adaptation strategy for real-world images. Extensive experiments on our dataset and a public benchmark demonstrate clear improvement achieved by our method over state of the art in both quantitative evaluation and visual comparisons.



### Spatially varying white balancing for mixed and non-uniform illuminants
- **Arxiv ID**: http://arxiv.org/abs/2109.01350v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01350v1)
- **Published**: 2021-09-03 07:26:11+00:00
- **Updated**: 2021-09-03 07:26:11+00:00
- **Authors**: Teruaki Akazawa, Yuma Kinoshita, Hitoshi Kiya
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel white balance adjustment, called "spatially varying white balancing," for single, mixed, and non-uniform illuminants. By using n diagonal matrices along with a weight, the proposed method can reduce lighting effects on all spatially varying colors in an image under such illumination conditions. In contrast, conventional white balance adjustments do not consider the correcting of all colors except under a single illuminant. Also, multi-color balance adjustments can map multiple colors into corresponding ground truth colors, although they may cause the rank deficiency problem to occur as a non-diagonal matrix is used, unlike white balancing. In an experiment, the effectiveness of the proposed method is shown under mixed and non-uniform illuminants, compared with conventional white and multi-color balancing. Moreover, under a single illuminant, the proposed method has almost the same performance as the conventional white balancing.



### MitoVis: A Visually-guided Interactive Intelligent System for Neuronal Mitochondria Analysis
- **Arxiv ID**: http://arxiv.org/abs/2109.01351v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01351v1)
- **Published**: 2021-09-03 07:31:59+00:00
- **Updated**: 2021-09-03 07:31:59+00:00
- **Authors**: JunYoung Choi, Hakjun Lee, Suyeon Kim, Seok-Kyu Kwon, Won-Ki Jeong
- **Comment**: None
- **Journal**: None
- **Summary**: Neurons have a polarized structure, including dendrites and axons, and compartment-specific functions can be affected by dwelling mitochondria. It is known that the morphology of mitochondria is closely related to the functions of neurons and neurodegenerative diseases. Even though several deep learning methods have been developed to automatically analyze the morphology of mitochondria, the application of existing methods to actual analysis still encounters several difficulties. Since the performance of pre-trained deep learning model may vary depending on the target data, re-training of the model is often required. Besides, even though deep learning has shown superior performance under a constrained setup, there are always errors that need to be corrected by humans in real analysis. To address these issues, we introduce MitoVis, a novel visualization system for end-to-end data processing and interactive analysis of the morphology of neuronal mitochondria. MitoVis enables interactive fine-tuning of a pre-trained neural network model without the domain knowledge of machine learning, which allows neuroscientists to easily leverage deep learning in their research. MitoVis also provides novel visual guides and interactive proofreading functions so that the users can quickly identify and correct errors in the result with minimal effort. We demonstrate the usefulness and efficacy of the system via a case study conducted by a neuroscientist on a real analysis scenario. The result shows that MitoVis allows up to 15x faster analysis with similar accuracy compared to the fully manual analysis method.



### Edge-featured Graph Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2109.01356v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01356v1)
- **Published**: 2021-09-03 07:53:18+00:00
- **Updated**: 2021-09-03 07:53:18+00:00
- **Authors**: Shaofei Cai, Liang Li, Xinzhe Han, Zheng-jun Zha, Qingming Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Graph neural networks (GNNs) have been successfully applied to learning representation on graphs in many relational tasks. Recently, researchers study neural architecture search (NAS) to reduce the dependence of human expertise and explore better GNN architectures, but they over-emphasize entity features and ignore latent relation information concealed in the edges. To solve this problem, we incorporate edge features into graph search space and propose Edge-featured Graph Neural Architecture Search to find the optimal GNN architecture. Specifically, we design rich entity and edge updating operations to learn high-order representations, which convey more generic message passing mechanisms. Moreover, the architecture topology in our search space allows to explore complex feature dependence of both entities and edges, which can be efficiently optimized by differentiable search strategy. Experiments at three graph tasks on six datasets show EGNAS can search better GNNs with higher performance than current state-of-the-art human-designed and searched-based GNNs.



### CAM-loss: Towards Learning Spatially Discriminative Feature Representations
- **Arxiv ID**: http://arxiv.org/abs/2109.01359v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01359v2)
- **Published**: 2021-09-03 08:04:17+00:00
- **Updated**: 2022-03-08 13:15:06+00:00
- **Authors**: Chaofei Wang, Jiayu Xiao, Yizeng Han, Qisen Yang, Shiji Song, Gao Huang
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: The backbone of traditional CNN classifier is generally considered as a feature extractor, followed by a linear layer which performs the classification. We propose a novel loss function, termed as CAM-loss, to constrain the embedded feature maps with the class activation maps (CAMs) which indicate the spatially discriminative regions of an image for particular categories. CAM-loss drives the backbone to express the features of target category and suppress the features of non-target categories or background, so as to obtain more discriminative feature representations. It can be simply applied in any CNN architecture with neglectable additional parameters and calculations. Experimental results show that CAM-loss is applicable to a variety of network structures and can be combined with mainstream regularization methods to improve the performance of image classification. The strong generalization ability of CAM-loss is validated in the transfer learning and few shot learning tasks. Based on CAM-loss, we also propose a novel CAAM-CAM matching knowledge distillation method. This method directly uses the CAM generated by the teacher network to supervise the CAAM generated by the student network, which effectively improves the accuracy and convergence rate of the student network.



### Deep Learning for Fitness
- **Arxiv ID**: http://arxiv.org/abs/2109.01376v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01376v1)
- **Published**: 2021-09-03 08:42:07+00:00
- **Updated**: 2021-09-03 08:42:07+00:00
- **Authors**: Mahendran N
- **Comment**: 6 pages, 3 figures, 2 tables. Rejected by a TradiCV 2021
- **Journal**: None
- **Summary**: We present Fitness tutor, an application for maintaining correct posture during workout exercises or doing yoga. Current work on fitness focuses on suggesting food supplements, accessing workouts, workout wearables does a great job in improving the fitness. Meanwhile, the current situation is making difficult to monitor workouts by trainee. Inspired by healthcare innovations like robotic surgery, we design a novel application Fitness tutor which can guide the workouts using pose estimation. Pose estimation can be deployed on the reference image for gathering data and guide the user with the data. This allow Fitness tutor to guide the workouts (both exercise and yoga) in remote conditions with a single reference posture as image. We use posenet model in tensorflow with p5js for developing skeleton. Fitness tutor is an application of pose estimation model in bringing a realtime teaching experience in fitness. Our experiments shows that it can leverage potential of pose estimation models by providing guidance in realtime.



### Segmentation of turbulent computational fluid dynamics simulations with unsupervised ensemble learning
- **Arxiv ID**: http://arxiv.org/abs/2109.01381v1
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.HE, cs.LG, physics.plasm-ph
- **Links**: [PDF](http://arxiv.org/pdf/2109.01381v1)
- **Published**: 2021-09-03 08:52:38+00:00
- **Updated**: 2021-09-03 08:52:38+00:00
- **Authors**: Maarja Bussov, Joonas Nättilä
- **Comment**: 15 pages, 8 figures. Accepted to Signal Processing: Image
  Communication. Code available from a repository:
  https://github.com/mkruuse/segmenting-turbulent-simulations-with-ensemble-learning
- **Journal**: None
- **Summary**: Computer vision and machine learning tools offer an exciting new way for automatically analyzing and categorizing information from complex computer simulations. Here we design an ensemble machine learning framework that can independently and robustly categorize and dissect simulation data output contents of turbulent flow patterns into distinct structure catalogues. The segmentation is performed using an unsupervised clustering algorithm, which segments physical structures by grouping together similar pixels in simulation images. The accuracy and robustness of the resulting segment region boundaries are enhanced by combining information from multiple simultaneously-evaluated clustering operations. The stacking of object segmentation evaluations is performed using image mask combination operations. This statistically-combined ensemble (SCE) of different cluster masks allows us to construct cluster reliability metrics for each pixel and for the associated segments without any prior user input. By comparing the similarity of different cluster occurrences in the ensemble, we can also assess the optimal number of clusters needed to describe the data. Furthermore, by relying on ensemble-averaged spatial segment region boundaries, the SCE method enables reconstruction of more accurate and robust region of interest (ROI) boundaries for the different image data clusters. We apply the SCE algorithm to 2-dimensional simulation data snapshots of magnetically-dominated fully-kinetic turbulent plasma flows where accurate ROI boundaries are needed for geometrical measurements of intermittent flow structures known as current sheets.



### Occlusion-Invariant Rotation-Equivariant Semi-Supervised Depth Based Cross-View Gait Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.01397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01397v1)
- **Published**: 2021-09-03 09:39:05+00:00
- **Updated**: 2021-09-03 09:39:05+00:00
- **Authors**: Xiao Gu, Jianxin Yang, Hanxiao Zhang, Jianing Qiu, Frank Po Wen Lo, Yao Guo, Guang-Zhong Yang, Benny Lo
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate estimation of three-dimensional human skeletons from depth images can provide important metrics for healthcare applications, especially for biomechanical gait analysis. However, there exist inherent problems associated with depth images captured from a single view. The collected data is greatly affected by occlusions where only partial surface data can be recorded. Furthermore, depth images of human body exhibit heterogeneous characteristics with viewpoint changes, and the estimated poses under local coordinate systems are expected to go through equivariant rotations. Most existing pose estimation models are sensitive to both issues. To address this, we propose a novel approach for cross-view generalization with an occlusion-invariant semi-supervised learning framework built upon a novel rotation-equivariant backbone. Our model was trained with real-world data from a single view and unlabelled synthetic data from multiple views. It can generalize well on the real-world data from all the other unseen views. Our approach has shown superior performance on gait analysis on our ICL-Gait dataset compared to other state-of-the-arts and it can produce more convincing keypoints on ITOP dataset, than its provided "ground truth".



### CX-ToM: Counterfactual Explanations with Theory-of-Mind for Enhancing Human Trust in Image Recognition Models
- **Arxiv ID**: http://arxiv.org/abs/2109.01401v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01401v3)
- **Published**: 2021-09-03 09:46:20+00:00
- **Updated**: 2021-12-02 22:51:12+00:00
- **Authors**: Arjun R. Akula, Keze Wang, Changsong Liu, Sari Saba-Sadiya, Hongjing Lu, Sinisa Todorovic, Joyce Chai, Song-Chun Zhu
- **Comment**: Accepted by iScience Cell Press Journal 2021. arXiv admin note: text
  overlap with arXiv:1909.06907
- **Journal**: None
- **Summary**: We propose CX-ToM, short for counterfactual explanations with theory-of mind, a new explainable AI (XAI) framework for explaining decisions made by a deep convolutional neural network (CNN). In contrast to the current methods in XAI that generate explanations as a single shot response, we pose explanation as an iterative communication process, i.e. dialog, between the machine and human user. More concretely, our CX-ToM framework generates sequence of explanations in a dialog by mediating the differences between the minds of machine and human user. To do this, we use Theory of Mind (ToM) which helps us in explicitly modeling human's intention, machine's mind as inferred by the human as well as human's mind as inferred by the machine. Moreover, most state-of-the-art XAI frameworks provide attention (or heat map) based explanations. In our work, we show that these attention based explanations are not sufficient for increasing human trust in the underlying CNN model. In CX-ToM, we instead use counterfactual explanations called fault-lines which we define as follows: given an input image I for which a CNN classification model M predicts class c_pred, a fault-line identifies the minimal semantic-level features (e.g., stripes on zebra, pointed ears of dog), referred to as explainable concepts, that need to be added to or deleted from I in order to alter the classification category of I by M to another specified class c_alt. We argue that, due to the iterative, conceptual and counterfactual nature of CX-ToM explanations, our framework is practical and more natural for both expert and non-expert users to understand the internal workings of complex deep learning models. Extensive quantitative and qualitative experiments verify our hypotheses, demonstrating that our CX-ToM significantly outperforms the state-of-the-art explainable AI models.



### Deep Learning Approach for Hyperspectral Image Demosaicking, Spectral Correction and High-resolution RGB Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2109.01403v1
- **DOI**: 10.1080/21681163.2021.1997646
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01403v1)
- **Published**: 2021-09-03 09:50:03+00:00
- **Updated**: 2021-09-03 09:50:03+00:00
- **Authors**: Peichao Li, Michael Ebner, Philip Noonan, Conor Horgan, Anisha Bahl, Sebastien Ourselin, Jonathan Shapey, Tom Vercauteren
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral imaging is one of the most promising techniques for intraoperative tissue characterisation. Snapshot mosaic cameras, which can capture hyperspectral data in a single exposure, have the potential to make a real-time hyperspectral imaging system for surgical decision-making possible. However, optimal exploitation of the captured data requires solving an ill-posed demosaicking problem and applying additional spectral corrections to recover spatial and spectral information of the image. In this work, we propose a deep learning-based image demosaicking algorithm for snapshot hyperspectral images using supervised learning methods. Due to the lack of publicly available medical images acquired with snapshot mosaic cameras, a synthetic image generation approach is proposed to simulate snapshot images from existing medical image datasets captured by high-resolution, but slow, hyperspectral imaging devices. Image reconstruction is achieved using convolutional neural networks for hyperspectral image super-resolution, followed by cross-talk and leakage correction using a sensor-specific calibration matrix. The resulting demosaicked images are evaluated both quantitatively and qualitatively, showing clear improvements in image quality compared to a baseline demosaicking method using linear interpolation. Moreover, the fast processing time of~45\,ms of our algorithm to obtain super-resolved RGB or oxygenation saturation maps per image frame for a state-of-the-art snapshot mosaic camera demonstrates the potential for its seamless integration into real-time surgical hyperspectral imaging applications.



### Automatic Foot Ulcer Segmentation Using an Ensemble of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.01408v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01408v2)
- **Published**: 2021-09-03 09:55:04+00:00
- **Updated**: 2022-06-17 11:09:19+00:00
- **Authors**: Amirreza Mahbod, Gerald Schaefer, Rupert Ecker, Isabella Ellinger
- **Comment**: Accepted for the 26th International Conference on Pattern Recognition
  (ICPR 2022)
- **Journal**: None
- **Summary**: Foot ulcer is a common complication of diabetes mellitus and, associated with substantial morbidity and mortality, remains a major risk factor for lower leg amputations. Extracting accurate morphological features from foot wounds is crucial for appropriate treatment. Although visual inspection by a medical professional is the common approach for diagnosis, this is subjective and error-prone, and computer-aided approaches thus provide an interesting alternative. Deep learning-based methods, and in particular convolutional neural networks (CNNs), have shown excellent performance for various tasks in medical image analysis including medical image segmentation.   In this paper, we propose an ensemble approach based on two encoder-decoder-based CNN models, namely LinkNet and U-Net, to perform foot ulcer segmentation. To deal with a limited number of available training samples, we use pre-trained weights (EfficientNetB1 for the LinkNet model and EfficientNetB2 for the U-Net model) and perform further pre-training using the Medetec dataset while also applying a number of morphological-based and colour-based augmentation techniques. To boost the segmentation performance, we incorporate five-fold cross-validation, test time augmentation and result fusion.   Applied on the publicly available chronic wound dataset and the MICCAI 2021 Foot Ulcer Segmentation (FUSeg) Challenge, our method achieves state-of-the-art performance with data-based Dice scores of 92.07% and 88.80%, respectively, and is the top ranked method in the FUSeg challenge leaderboard. The Dockerised guidelines, inference codes and saved trained models are publicly available at https://github.com/masih4/Foot_Ulcer_Segmentation.



### How Reliable Are Out-of-Distribution Generalization Methods for Medical Image Segmentation?
- **Arxiv ID**: http://arxiv.org/abs/2109.01668v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01668v1)
- **Published**: 2021-09-03 10:15:44+00:00
- **Updated**: 2021-09-03 10:15:44+00:00
- **Authors**: Antoine Sanner, Camila Gonzalez, Anirban Mukhopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: The recent achievements of Deep Learning rely on the test data being similar in distribution to the training data. In an ideal case, Deep Learning models would achieve Out-of-Distribution (OoD) Generalization, i.e. reliably make predictions on out-of-distribution data. Yet in practice, models usually fail to generalize well when facing a shift in distribution. Several methods were thereby designed to improve the robustness of the features learned by a model through Regularization- or Domain-Prediction-based schemes. Segmenting medical images such as MRIs of the hippocampus is essential for the diagnosis and treatment of neuropsychiatric disorders. But these brain images often suffer from distribution shift due to the patient's age and various pathologies affecting the shape of the organ. In this work, we evaluate OoD Generalization solutions for the problem of hippocampus segmentation in MR data using both fully- and semi-supervised training. We find that no method performs reliably in all experiments. Only the V-REx loss stands out as it remains easy to tune, while it outperforms a standard U-Net in most cases.



### Ghost Loss to Question the Reliability of Training Data
- **Arxiv ID**: http://arxiv.org/abs/2109.01504v1
- **DOI**: 10.1109/ACCESS.2020.2978283
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01504v1)
- **Published**: 2021-09-03 13:24:39+00:00
- **Updated**: 2021-09-03 13:24:39+00:00
- **Authors**: Adrien Deliège, Anthony Cioppa, Marc Van Droogenbroeck
- **Comment**: This is the authors' preprint version of a paper published in IEEE
  Access in 2020. Please cite it as follows: A. Deli\`ege, A. Cioppa and M. Van
  Droogenbroeck, "Ghost Loss to Question the Reliability of Training Data", in
  IEEE Access, vol. 8, pp. 44774-44782, 2020, doi: 10.1109/ACCESS.2020.2978283
- **Journal**: in IEEE Access, vol. 8, pp. 44774-44782, 2020
- **Summary**: Supervised image classification problems rely on training data assumed to have been correctly annotated; this assumption underpins most works in the field of deep learning. In consequence, during its training, a network is forced to match the label provided by the annotator and is not given the flexibility to choose an alternative to inconsistencies that it might be able to detect. Therefore, erroneously labeled training images may end up ``correctly'' classified in classes which they do not actually belong to. This may reduce the performances of the network and thus incite to build more complex networks without even checking the quality of the training data. In this work, we question the reliability of the annotated datasets. For that purpose, we introduce the notion of ghost loss, which can be seen as a regular loss that is zeroed out for some predicted values in a deterministic way and that allows the network to choose an alternative to the given label without being penalized. After a proof of concept experiment, we use the ghost loss principle to detect confusing images and erroneously labeled images in well-known training datasets (MNIST, Fashion-MNIST, SVHN, CIFAR10) and we provide a new tool, called sanity matrix, for summarizing these confusions.



### Safety-aware Motion Prediction with Unseen Vehicles for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2109.01510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01510v1)
- **Published**: 2021-09-03 13:33:33+00:00
- **Updated**: 2021-09-03 13:33:33+00:00
- **Authors**: Xuanchi Ren, Tao Yang, Li Erran Li, Alexandre Alahi, Qifeng Chen
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Motion prediction of vehicles is critical but challenging due to the uncertainties in complex environments and the limited visibility caused by occlusions and limited sensor ranges. In this paper, we study a new task, safety-aware motion prediction with unseen vehicles for autonomous driving. Unlike the existing trajectory prediction task for seen vehicles, we aim at predicting an occupancy map that indicates the earliest time when each location can be occupied by either seen and unseen vehicles. The ability to predict unseen vehicles is critical for safety in autonomous driving. To tackle this challenging task, we propose a safety-aware deep learning model with three new loss functions to predict the earliest occupancy map. Experiments on the large-scale autonomous driving nuScenes dataset show that our proposed model significantly outperforms the state-of-the-art baselines on the safety-aware motion prediction task. To the best of our knowledge, our approach is the first one that can predict the existence of unseen vehicles in most cases. Project page at {\url{https://github.com/xrenaa/Safety-Aware-Motion-Prediction}}.



### Automated detection of COVID-19 cases from chest X-ray images using deep neural network and XGBoost
- **Arxiv ID**: http://arxiv.org/abs/2109.02428v1
- **DOI**: 10.1016/j.radi.2022.03.011
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02428v1)
- **Published**: 2021-09-03 13:41:13+00:00
- **Updated**: 2021-09-03 13:41:13+00:00
- **Authors**: Hamid Nasiri, Sharif Hasani
- **Comment**: Radiography, 2022
- **Journal**: None
- **Summary**: In late 2019 and after COVID-19 pandemic in the world, many researchers and scholars have tried to provide methods for detection of COVID-19 cases. Accordingly, this study focused on identifying COVID-19 cases from chest X-ray images. In this paper, a novel approach to diagnosing coronavirus disease from X-ray images was proposed. In the proposed method, DenseNet169 deep neural network was used to extract the features of X-ray images taken from the patients' chest and the extracted features were then given as input to the Extreme Gradient Boosting (XGBoost) algorithm so that it could perform the classification task. Evaluation of the proposed approach and its comparison with the methods presented in recent years revealed that the proposed method was more accurate and faster than the existing ones and had an acceptable performance in detection of COVID-19 cases from X-ray images.



### UnDeepLIO: Unsupervised Deep Lidar-Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/2109.01533v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.01533v1)
- **Published**: 2021-09-03 13:56:55+00:00
- **Updated**: 2021-09-03 13:56:55+00:00
- **Authors**: Yiming Tu, Jin Xie
- **Comment**: 14 pages, accepted by ACPR2021
- **Journal**: None
- **Summary**: Extensive research efforts have been dedicated to deep learning based odometry. Nonetheless, few efforts are made on the unsupervised deep lidar odometry. In this paper, we design a novel framework for unsupervised lidar odometry with the IMU, which is never used in other deep methods. First, a pair of siamese LSTMs are used to obtain the initial pose from the linear acceleration and angular velocity of IMU. With the initial pose, we perform the rigid transform on the current frame and align it closer to the last frame. Then, we extract vertex and normal features from the transformed point clouds and its normals. Next a two-branches attention modules are proposed to estimate residual rotation and translation from the extracted vertex and normal features, respectively. Finally, our model outputs the sum of initial and residual poses as the final pose. For unsupervised training, we introduce an unsupervised loss function which is employed on the voxelized point clouds. The proposed approach is evaluated on the KITTI odometry estimation benchmark and achieves comparable performances against other state-of-the-art methods.



### Model-Based Parameter Optimization for Ground Texture Based Localization Methods
- **Arxiv ID**: http://arxiv.org/abs/2109.01559v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.01559v1)
- **Published**: 2021-09-03 14:29:36+00:00
- **Updated**: 2021-09-03 14:29:36+00:00
- **Authors**: Jan Fabian Schmid, Stephan F. Simon, Rudolf Mester
- **Comment**: None
- **Journal**: None
- **Summary**: A promising approach to accurate positioning of robots is ground texture based localization. It is based on the observation that visual features of ground images enable fingerprint-like place recognition. We tackle the issue of efficient parametrization of such methods, deriving a prediction model for localization performance, which requires only a small collection of sample images of an application area. In a first step, we examine whether the model can predict the effects of changing one of the most important parameters of feature-based localization methods: the number of extracted features. We examine two localization methods, and in both cases our evaluation shows that the predictions are sufficiently accurate. Since this model can be used to find suitable values for any parameter, we then present a holistic parameter optimization framework, which finds suitable texture-specific parameter configurations, using only the model to evaluate the considered parameter configurations.



### Ordinal Pooling
- **Arxiv ID**: http://arxiv.org/abs/2109.01561v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01561v1)
- **Published**: 2021-09-03 14:33:02+00:00
- **Updated**: 2021-09-03 14:33:02+00:00
- **Authors**: Adrien Deliège, Maxime Istasse, Ashwani Kumar, Christophe De Vleeschouwer, Marc Van Droogenbroeck
- **Comment**: This is the authors' preprint version of a paper published at BMVC
  2019. Please cite it as follows: A. Deli\`ege, M. Istasse, A. Kumar, C. De
  Vleeschouwer and M. Van Droogenbroeck, "Ordinal Pooling", in British Machine
  Vision Conference, 2019
- **Journal**: A. Deliege, M. Istasse, A. Kumar, C. De Vleeschouwer and M. Van
  Droogenbroeck, "Ordinal Pooling", in British Machine Vision Conference, 2019
- **Summary**: In the framework of convolutional neural networks, downsampling is often performed with an average-pooling, where all the activations are treated equally, or with a max-pooling operation that only retains an element with maximum activation while discarding the others. Both of these operations are restrictive and have previously been shown to be sub-optimal. To address this issue, a novel pooling scheme, named\emph{ ordinal pooling}, is introduced in this work. Ordinal pooling rearranges all the elements of a pooling region in a sequence and assigns a different weight to each element based upon its order in the sequence. These weights are used to compute the pooling operation as a weighted sum of the rearranged elements of the pooling region. They are learned via a standard gradient-based training, allowing to learn a behavior anywhere in the spectrum of average-pooling to max-pooling in a differentiable manner. Our experiments suggest that it is advantageous for the networks to perform different types of pooling operations within a pooling layer and that a hybrid behavior between average- and max-pooling is often beneficial. More importantly, they also demonstrate that ordinal pooling leads to consistent improvements in the accuracy over average- or max-pooling operations while speeding up the training and alleviating the issue of the choice of the pooling operations and activation functions to be used in the networks. In particular, ordinal pooling mainly helps on lightweight or quantized deep learning architectures, as typically considered e.g. for embedded applications.



### Deep Metric Learning for Ground Images
- **Arxiv ID**: http://arxiv.org/abs/2109.01569v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.01569v1)
- **Published**: 2021-09-03 14:43:59+00:00
- **Updated**: 2021-09-03 14:43:59+00:00
- **Authors**: Raaghav Radhakrishnan, Jan Fabian Schmid, Randolf Scholz, Lars Schmidt-Thieme
- **Comment**: None
- **Journal**: None
- **Summary**: Ground texture based localization methods are potential prospects for low-cost, high-accuracy self-localization solutions for robots. These methods estimate the pose of a given query image, i.e. the current observation of the ground from a downward-facing camera, in respect to a set of reference images whose poses are known in the application area. In this work, we deal with the initial localization task, in which we have no prior knowledge about the current robot positioning. In this situation, the localization method would have to consider all available reference images. However, in order to reduce computational effort and the risk of receiving a wrong result, we would like to consider only those reference images that are actually overlapping with the query image. For this purpose, we propose a deep metric learning approach that retrieves the most similar reference images to the query image. In contrast to existing approaches to image retrieval for ground images, our approach achieves significantly better recall performance and improves the localization performance of a state-of-the-art ground texture based localization method.



### Using Topological Framework for the Design of Activation Function and Model Pruning in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.01572v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01572v1)
- **Published**: 2021-09-03 14:58:17+00:00
- **Updated**: 2021-09-03 14:58:17+00:00
- **Authors**: Yogesh Kochar, Sunil Kumar Vengalil, Neelam Sinha
- **Comment**: None
- **Journal**: None
- **Summary**: Success of deep neural networks in diverse tasks across domains of computer vision, speech recognition and natural language processing, has necessitated understanding the dynamics of training process and also working of trained models. Two independent contributions of this paper are 1) Novel activation function for faster training convergence 2) Systematic pruning of filters of models trained irrespective of activation function. We analyze the topological transformation of the space of training samples as it gets transformed by each successive layer during training, by changing the activation function. The impact of changing activation function on the convergence during training is reported for the task of binary classification. A novel activation function aimed at faster convergence for classification tasks is proposed. Here, Betti numbers are used to quantify topological complexity of data. Results of experiments on popular synthetic binary classification datasets with large Betti numbers(>150) using MLPs are reported. Results show that the proposed activation function results in faster convergence requiring fewer epochs by a factor of 1.5 to 2, since Betti numbers reduce faster across layers with the proposed activation function. The proposed methodology was verified on benchmark image datasets: fashion MNIST, CIFAR-10 and cat-vs-dog images, using CNNs. Based on empirical results, we propose a novel method for pruning a trained model. The trained model was pruned by eliminating filters that transform data to a topological space with large Betti numbers. All filters with Betti numbers greater than 300 were removed from each layer without significant reduction in accuracy. This resulted in faster prediction time and reduced memory size of the model.



### 3D Human Shape Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2109.01587v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01587v1)
- **Published**: 2021-09-03 15:51:30+00:00
- **Updated**: 2021-09-03 15:51:30+00:00
- **Authors**: Joao Regateiro, Edmond Boyer
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of modifying/replacing the shape style of a real moving character with those of an arbitrary static real source character. Traditional solutions follow a pose transfer strategy, from the moving character to the source character shape, that relies on skeletal pose parametrization. In this paper, we explore an alternative approach that transfers the source shape style onto the moving character. The expected benefit is to avoid the inherently difficult pose to shape conversion required with skeletal parametrization applied on real characters. To this purpose, we consider image style transfer techniques and investigate how to adapt them to 3D human shapes. Adaptive Instance Normalisation (AdaIN) and SPADE architectures have been demonstrated to efficiently and accurately transfer the style of an image onto another while preserving the original image structure. Where AdaIN contributes with a module to perform style transfer through the statistics of the subjects and SPADE contribute with a residual block architecture to refine the quality of the style transfer. We demonstrate that these approaches are extendable to the 3D shape domain by proposing a convolutional neural network that applies the same principle of preserving the shape structure (shape pose) while transferring the style of a new subject shape. The generated results are supervised through a discriminator module to evaluate the realism of the shape, whilst enforcing the decoder to synthesise plausible shapes and improve the style transfer for unseen subjects. Our experiments demonstrate an average of $\approx 56\%$ qualitative and quantitative improvements over the baseline in shape transfer through optimization-based and learning-based methods.



### Neural Human Deformation Transfer
- **Arxiv ID**: http://arxiv.org/abs/2109.01588v4
- **DOI**: 10.1109/3DV53792.2021.00064
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01588v4)
- **Published**: 2021-09-03 15:51:30+00:00
- **Updated**: 2021-11-08 15:55:03+00:00
- **Authors**: Jean Basset, Adnane Boukhayma, Stefanie Wuhrer, Franck Multon, Edmond Boyer
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of human deformation transfer, where the goal is to retarget poses between different characters. Traditional methods that tackle this problem require a clear definition of the pose, and use this definition to transfer poses between characters. In this work, we take a different approach and transform the identity of a character into a new identity without modifying the character's pose. This offers the advantage of not having to define equivalences between 3D human poses, which is not straightforward as poses tend to change depending on the identity of the character performing them, and as their meaning is highly contextual. To achieve the deformation transfer, we propose a neural encoder-decoder architecture where only identity information is encoded and where the decoder is conditioned on the pose. We use pose independent representations, such as isometry-invariant shape characteristics, to represent identity features. Our model uses these features to supervise the prediction of offsets from the deformed pose to the result of the transfer. We show experimentally that our method outperforms state-of-the-art methods both quantitatively and qualitatively, and generalises better to poses not seen during training. We also introduce a fine-tuning step that allows to obtain competitive results for extreme identities, and allows to transfer simple clothing.



### Representing Shape Collections with Alignment-Aware Linear Models
- **Arxiv ID**: http://arxiv.org/abs/2109.01605v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01605v2)
- **Published**: 2021-09-03 16:28:34+00:00
- **Updated**: 2021-12-17 11:00:34+00:00
- **Authors**: Romain Loiseau, Tom Monnier, Mathieu Aubry, Loïc Landrieu
- **Comment**: Accepted to 3DV 2021. 17 pages, 10 figures. Code and data are
  available at: https://romainloiseau.github.io/deep-linear-shapes
- **Journal**: None
- **Summary**: In this paper, we revisit the classical representation of 3D point clouds as linear shape models. Our key insight is to leverage deep learning to represent a collection of shapes as affine transformations of low-dimensional linear shape models. Each linear model is characterized by a shape prototype, a low-dimensional shape basis and two neural networks. The networks take as input a point cloud and predict the coordinates of a shape in the linear basis and the affine transformation which best approximate the input. Both linear models and neural networks are learned end-to-end using a single reconstruction loss. The main advantage of our approach is that, in contrast to many recent deep approaches which learn feature-based complex shape representations, our model is explicit and every operation occurs in 3D space. As a result, our linear shape models can be easily visualized and annotated, and failure cases can be visually understood. While our main goal is to introduce a compact and interpretable representation of shape collections, we show it leads to state of the art results for few-shot segmentation.



### Wildfire smoke plume segmentation using geostationary satellite imagery
- **Arxiv ID**: http://arxiv.org/abs/2109.01637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01637v1)
- **Published**: 2021-09-03 17:29:58+00:00
- **Updated**: 2021-09-03 17:29:58+00:00
- **Authors**: Jeff Wen, Marshall Burke
- **Comment**: None
- **Journal**: None
- **Summary**: Wildfires have increased in frequency and severity over the past two decades, especially in the Western United States. Beyond physical infrastructure damage caused by these wildfire events, researchers have increasingly identified harmful impacts of particulate matter generated by wildfire smoke on respiratory, cardiovascular, and cognitive health. This inference is difficult due to the spatial and temporal uncertainty regarding how much particulate matter is specifically attributable to wildfire smoke. One factor contributing to this challenge is the reliance on manually drawn smoke plume annotations, which are often noisy representations limited to the United States. This work uses deep convolutional neural networks to segment smoke plumes from geostationary satellite imagery. We compare the performance of predicted plume segmentations versus the noisy annotations using causal inference methods to estimate the amount of variation each explains in Environmental Protection Agency (EPA) measured surface level particulate matter <2.5um in diameter ($\textrm{PM}_{2.5}$).



### ReLaX: Retinal Layer Attribution for Guided Explanations of Automated Optical Coherence Tomography Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.02436v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02436v3)
- **Published**: 2021-09-03 17:59:34+00:00
- **Updated**: 2022-10-01 16:40:58+00:00
- **Authors**: Evan Wen, Rebecca Sorenson, Max Ehrlich
- **Comment**: ECCV 2022 Medical Computer Vision Workshop
- **Journal**: None
- **Summary**: 30 million Optical Coherence Tomography (OCT) imaging tests are issued annually to diagnose various retinal diseases, but accurate diagnosis of OCT scans requires trained eye care professionals who are still prone to making errors. With better systems for diagnosis, many cases of vision loss caused by retinal disease could be entirely avoided. In this work, we present ReLaX, a novel deep learning framework for explainable, accurate classification of retinal pathologies which achieves state-of-the-art accuracy. Furthermore, we emphasize producing both qualitative and quantitative explanations of the model's decisions. While previous works use pixel-level attribution methods for generating model explanations, our work uses a novel retinal layer attribution method for producing rich qualitative and quantitative model explanations. ReLaX determines the importance of each retinal layer by combining heatmaps with an OCT segmentation model. Our work is the first to produce detailed quantitative explanations of a model's predictions in this way. The combination of accuracy and interpretability can be clinically applied for accessible, high-quality patient care.



### Weakly Supervised Few-Shot Segmentation Via Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.01693v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01693v1)
- **Published**: 2021-09-03 18:20:26+00:00
- **Updated**: 2021-09-03 18:20:26+00:00
- **Authors**: Pedro H. T. Gama, Hugo Oliveira, José Marcato Junior, Jefersson A. dos Santos
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation is a classic computer vision task with multiple applications, which includes medical and remote sensing image analysis. Despite recent advances with deep-based approaches, labeling samples (pixels) for training models is laborious and, in some cases, unfeasible. In this paper, we present two novel meta learning methods, named WeaSeL and ProtoSeg, for the few-shot semantic segmentation task with sparse annotations. We conducted extensive evaluation of the proposed methods in different applications (12 datasets) in medical imaging and agricultural remote sensing, which are very distinct fields of knowledge and usually subject to data scarcity. The results demonstrated the potential of our method, achieving suitable results for segmenting both coffee/orange crops and anatomical parts of the human body in comparison with full dense annotation.



### Revisiting 3D ResNets for Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.01696v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01696v1)
- **Published**: 2021-09-03 18:27:52+00:00
- **Updated**: 2021-09-03 18:27:52+00:00
- **Authors**: Xianzhi Du, Yeqing Li, Yin Cui, Rui Qian, Jing Li, Irwan Bello
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: A recent work from Bello shows that training and scaling strategies may be more significant than model architectures for visual recognition. This short note studies effective training and scaling strategies for video recognition models. We propose a simple scaling strategy for 3D ResNets, in combination with improved training strategies and minor architectural changes. The resulting models, termed 3D ResNet-RS, attain competitive performance of 81.0 on Kinetics-400 and 83.8 on Kinetics-600 without pre-training. When pre-trained on a large Web Video Text dataset, our best model achieves 83.5 and 84.3 on Kinetics-400 and Kinetics-600. The proposed scaling rule is further evaluated in a self-supervised setup using contrastive learning, demonstrating improved performance. Code is available at: https://github.com/tensorflow/models/tree/master/official.



### IMG2SMI: Translating Molecular Structure Images to Simplified Molecular-input Line-entry System
- **Arxiv ID**: http://arxiv.org/abs/2109.04202v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.04202v1)
- **Published**: 2021-09-03 19:57:07+00:00
- **Updated**: 2021-09-03 19:57:07+00:00
- **Authors**: Daniel Campos, Heng Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Like many scientific fields, new chemistry literature has grown at a staggering pace, with thousands of papers released every month. A large portion of chemistry literature focuses on new molecules and reactions between molecules. Most vital information is conveyed through 2-D images of molecules, representing the underlying molecules or reactions described. In order to ensure reproducible and machine-readable molecule representations, text-based molecule descriptors like SMILES and SELFIES were created. These text-based molecule representations provide molecule generation but are unfortunately rarely present in published literature. In the absence of molecule descriptors, the generation of molecule descriptors from the 2-D images present in the literature is necessary to understand chemistry literature at scale. Successful methods such as Optical Structure Recognition Application (OSRA), and ChemSchematicResolver are able to extract the locations of molecules structures in chemistry papers and infer molecular descriptions and reactions. While effective, existing systems expect chemists to correct outputs, making them unsuitable for unsupervised large-scale data mining. Leveraging the task formulation of image captioning introduced by DECIMER, we introduce IMG2SMI, a model which leverages Deep Residual Networks for image feature extraction and an encoder-decoder Transformer layers for molecule description generation. Unlike previous Neural Network-based systems, IMG2SMI builds around the task of molecule description generation, which enables IMG2SMI to outperform OSRA-based systems by 163% in molecule similarity prediction as measured by the molecular MACCS Fingerprint Tanimoto Similarity. Additionally, to facilitate further research on this task, we release a new molecule prediction dataset. including 81 million molecules for molecule description generation



### Towards Accurate Alignment in Real-time 3D Hand-Mesh Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2109.01723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01723v1)
- **Published**: 2021-09-03 20:42:01+00:00
- **Updated**: 2021-09-03 20:42:01+00:00
- **Authors**: Xiao Tang, Tianyu Wang, Chi-Wing Fu
- **Comment**: None
- **Journal**: None
- **Summary**: 3D hand-mesh reconstruction from RGB images facilitates many applications, including augmented reality (AR). However, this requires not only real-time speed and accurate hand pose and shape but also plausible mesh-image alignment. While existing works already achieve promising results, meeting all three requirements is very challenging. This paper presents a novel pipeline by decoupling the hand-mesh reconstruction task into three stages: a joint stage to predict hand joints and segmentation; a mesh stage to predict a rough hand mesh; and a refine stage to fine-tune it with an offset mesh for mesh-image alignment. With careful design in the network structure and in the loss functions, we can promote high-quality finger-level mesh-image alignment and drive the models together to deliver real-time predictions. Extensive quantitative and qualitative results on benchmark datasets demonstrate that the quality of our results outperforms the state-of-the-art methods on hand-mesh/pose precision and hand-image alignment. In the end, we also showcase several real-time AR scenarios.



### Navigating the Mise-en-Page: Interpretive Machine Learning Approaches to the Visual Layouts of Multi-Ethnic Periodicals
- **Arxiv ID**: http://arxiv.org/abs/2109.01732v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2109.01732v1)
- **Published**: 2021-09-03 21:10:38+00:00
- **Updated**: 2021-09-03 21:10:38+00:00
- **Authors**: Benjamin Charles Germain Lee, Joshua Ortiz Baco, Sarah H. Salter, Jim Casey
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: This paper presents a computational method of analysis that draws from machine learning, library science, and literary studies to map the visual layouts of multi-ethnic newspapers from the late 19th and early 20th century United States. This work departs from prior approaches to newspapers that focus on individual pieces of textual and visual content. Our method combines Chronicling America's MARC data and the Newspaper Navigator machine learning dataset to identify the visual patterns of newspaper page layouts. By analyzing high-dimensional visual similarity, we aim to better understand how editors spoke and protested through the layout of their papers.



### F3S: Free Flow Fever Screening
- **Arxiv ID**: http://arxiv.org/abs/2109.01733v1
- **DOI**: 10.1109/SMARTCOMP52413.2021.00060
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01733v1)
- **Published**: 2021-09-03 21:13:15+00:00
- **Updated**: 2021-09-03 21:13:15+00:00
- **Authors**: Kunal Rao, Giuseppe Coviello, Min Feng, Biplob Debnath, Wang-Pin Hsiung, Murugan Sankaradas, Yi Yang, Oliver Po, Utsav Drolia, Srimat Chakradhar
- **Comment**: None
- **Journal**: None
- **Summary**: Identification of people with elevated body temperature can reduce or dramatically slow down the spread of infectious diseases like COVID-19. We present a novel fever-screening system, F3S, that uses edge machine learning techniques to accurately measure core body temperatures of multiple individuals in a free-flow setting. F3S performs real-time sensor fusion of visual camera with thermal camera data streams to detect elevated body temperature, and it has several unique features: (a) visual and thermal streams represent very different modalities, and we dynamically associate semantically-equivalent regions across visual and thermal frames by using a new, dynamic alignment technique that analyzes content and context in real-time, (b) we track people through occlusions, identify the eye (inner canthus), forehead, face and head regions where possible, and provide an accurate temperature reading by using a prioritized refinement algorithm, and (c) we robustly detect elevated body temperature even in the presence of personal protective equipment like masks, or sunglasses or hats, all of which can be affected by hot weather and lead to spurious temperature readings. F3S has been deployed at over a dozen large commercial establishments, providing contact-less, free-flow, real-time fever screening for thousands of employees and customers in indoors and outdoor settings.



### A realistic approach to generate masked faces applied on two novel masked face recognition data sets
- **Arxiv ID**: http://arxiv.org/abs/2109.01745v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01745v5)
- **Published**: 2021-09-03 22:33:55+00:00
- **Updated**: 2021-10-25 14:56:56+00:00
- **Authors**: Tudor Mare, Georgian Duta, Mariana-Iuliana Georgescu, Adrian Sandru, Bogdan Alexe, Marius Popescu, Radu Tudor Ionescu
- **Comment**: Accepted at NeurIPS 2021
- **Journal**: None
- **Summary**: The COVID-19 pandemic raises the problem of adapting face recognition systems to the new reality, where people may wear surgical masks to cover their noses and mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for training these systems were released before the pandemic, so they now seem unsuited due to the lack of examples of people wearing masks. We propose a method for enhancing data sets containing faces without masks by creating synthetic masks and overlaying them on faces in the original images. Our method relies on SparkAR Studio, a developer program made by Facebook that is used to create Instagram face filters. In our approach, we use 9 masks of different colors, shapes and fabrics. We employ our method to generate a number of 445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254 (96.8%) masks for the CelebA data set, releasing the mask images at https://github.com/securifai/masked_faces. We show that our method produces significantly more realistic training examples of masks overlaid on faces by asking volunteers to qualitatively compare it to other methods or data sets designed for the same task. We also demonstrate the usefulness of our method by evaluating state-of-the-art face recognition systems (FaceNet, VGG-face, ArcFace) trained on our enhanced data sets and showing that they outperform equivalent systems trained on original data sets (containing faces without masks) or competing data sets (containing masks generated by related methods), when the test benchmarks contain masked faces.



### CodeNeRF: Disentangled Neural Radiance Fields for Object Categories
- **Arxiv ID**: http://arxiv.org/abs/2109.01750v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01750v1)
- **Published**: 2021-09-03 23:02:45+00:00
- **Updated**: 2021-09-03 23:02:45+00:00
- **Authors**: Wonbong Jang, Lourdes Agapito
- **Comment**: 10 pages, 15 figures, ICCV 2021
- **Journal**: None
- **Summary**: CodeNeRF is an implicit 3D neural representation that learns the variation of object shapes and textures across a category and can be trained, from a set of posed images, to synthesize novel views of unseen objects. Unlike the original NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture by learning separate embeddings. At test time, given a single unposed image of an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and appearance codes via optimization. Unseen objects can be reconstructed from a single image, and then rendered from new viewpoints or their shape and texture edited by varying the latent codes. We conduct experiments on the SRN benchmark, which show that CodeNeRF generalises well to unseen objects and achieves on-par performance with methods that require known camera pose at test time. Our results on real-world images demonstrate that CodeNeRF can bridge the sim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}



