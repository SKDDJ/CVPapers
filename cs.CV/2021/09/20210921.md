# Arxiv Papers in cs.CV on 2021-09-21
### AirDOS: Dynamic SLAM benefits from Articulated Objects
- **Arxiv ID**: http://arxiv.org/abs/2109.09903v3
- **DOI**: 10.1109/ICRA46639.2022.9811667
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.09903v3)
- **Published**: 2021-09-21 01:23:48+00:00
- **Updated**: 2022-08-24 16:17:06+00:00
- **Authors**: Yuheng Qiu, Chen Wang, Wenshan Wang, Mina Henein, Sebastian Scherer
- **Comment**: None
- **Journal**: 2022 International Conference on Robotics and Automation (ICRA)
- **Summary**: Dynamic Object-aware SLAM (DOS) exploits object-level information to enable robust motion estimation in dynamic environments. Existing methods mainly focus on identifying and excluding dynamic objects from the optimization. In this paper, we show that feature-based visual SLAM systems can also benefit from the presence of dynamic articulated objects by taking advantage of two observations: (1) The 3D structure of each rigid part of articulated object remains consistent over time; (2) The points on the same rigid part follow the same motion. In particular, we present AirDOS, a dynamic object-aware system that introduces rigidity and motion constraints to model articulated objects. By jointly optimizing the camera pose, object motion, and the object 3D structure, we can rectify the camera pose estimation, preventing tracking loss, and generate 4D spatio-temporal maps for both dynamic objects and static scenes. Experiments show that our algorithm improves the robustness of visual SLAM algorithms in challenging crowded urban environments. To the best of our knowledge, AirDOS is the first dynamic object-aware SLAM system demonstrating that camera pose estimation can be improved by incorporating dynamic articulated objects.



### Physics-based Human Motion Estimation and Synthesis from Videos
- **Arxiv ID**: http://arxiv.org/abs/2109.09913v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09913v2)
- **Published**: 2021-09-21 01:57:54+00:00
- **Updated**: 2022-08-11 23:19:04+00:00
- **Authors**: Kevin Xie, Tingwu Wang, Umar Iqbal, Yunrong Guo, Sanja Fidler, Florian Shkurti
- **Comment**: To appear in ICCV 2021
- **Journal**: None
- **Summary**: Human motion synthesis is an important problem with applications in graphics, gaming and simulation environments for robotics. Existing methods require accurate motion capture data for training, which is costly to obtain. Instead, we propose a framework for training generative models of physically plausible human motion directly from monocular RGB videos, which are much more widely available. At the core of our method is a novel optimization formulation that corrects imperfect image-based pose estimations by enforcing physics constraints and reasons about contacts in a differentiable way. This optimization yields corrected 3D poses and motions, as well as their corresponding contact forces. Results show that our physically-corrected motions significantly outperform prior work on pose estimation. We can then use these to train a generative model to synthesize future motion. We demonstrate both qualitatively and quantitatively improved motion estimation, synthesis quality and physical plausibility achieved by our method on the Human3.6m dataset~\cite{h36m_pami} as compared to prior kinematic and physics-based methods. By enabling learning of motion synthesis from video, our method paves the way for large-scale, realistic and diverse motion synthesis. Project page: \url{https://nv-tlabs.github.io/publication/iccv_2021_physics/}



### Introduce the Result Into Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/2109.13860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13860v1)
- **Published**: 2021-09-21 02:16:00+00:00
- **Updated**: 2021-09-21 02:16:00+00:00
- **Authors**: Chengcheng Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional self-attention mechanisms in convolutional networks tend to use only the output of the previous layer as input to the attention network, such as SENet, CBAM, etc. In this paper, we propose a new attention modification method that tries to get the output of the classification network in advance and use it as a part of the input of the attention network. We used the auxiliary classifier proposed in GoogLeNet to obtain the results in advance and pass them into attention networks. we added this mechanism to SE-ResNet for our experiments and achieved a classification accuracy improvement of at most 1.94% on cifar100.



### Survey: Transformer based Video-Language Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2109.09920v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09920v1)
- **Published**: 2021-09-21 02:36:06+00:00
- **Updated**: 2021-09-21 02:36:06+00:00
- **Authors**: Ludan Ruan, Qin Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by the success of transformer-based pre-training methods on natural language tasks and further computer vision tasks, researchers have begun to apply transformer to video processing. This survey aims to give a comprehensive overview on transformer-based pre-training methods for Video-Language learning. We first briefly introduce the transformer tructure as the background knowledge, including attention mechanism, position encoding etc. We then describe the typical paradigm of pre-training & fine-tuning on Video-Language processing in terms of proxy tasks, downstream tasks and commonly used video datasets. Next, we categorize transformer models into Single-Stream and Multi-Stream structures, highlight their innovations and compare their performances. Finally, we analyze and discuss the current challenges and possible future research directions for Video-Language pre-training.



### AutoPhoto: Aesthetic Photo Capture using Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.09923v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.09923v1)
- **Published**: 2021-09-21 02:52:34+00:00
- **Updated**: 2021-09-21 02:52:34+00:00
- **Authors**: Hadi AlZayer, Hubert Lin, Kavita Bala
- **Comment**: Accepted to IROS 2021
- **Journal**: None
- **Summary**: The process of capturing a well-composed photo is difficult and it takes years of experience to master. We propose a novel pipeline for an autonomous agent to automatically capture an aesthetic photograph by navigating within a local region in a scene. Instead of classical optimization over heuristics such as the rule-of-thirds, we adopt a data-driven aesthetics estimator to assess photo quality. A reinforcement learning framework is used to optimize the model with respect to the learned aesthetics metric. We train our model in simulation with indoor scenes, and we demonstrate that our system can capture aesthetic photos in both simulation and real world environments on a ground robot. To our knowledge, this is the first system that can automatically explore an environment to capture an aesthetic photo with respect to a learned aesthetic estimator.



### Robust Extrinsic Symmetry Estimation in 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2109.09927v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09927v2)
- **Published**: 2021-09-21 03:09:51+00:00
- **Updated**: 2023-07-02 06:25:17+00:00
- **Authors**: Rajendra Nagar
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting the reflection symmetry plane of an object represented by a 3D point cloud is a fundamental problem in 3D computer vision and geometry processing due to its various applications, such as compression, object detection, robotic grasping, 3D surface reconstruction, etc. There exist several efficient approaches for solving this problem for clean 3D point clouds. However, it is a challenging problem to solve in the presence of outliers and missing parts. The existing methods try to overcome this challenge mostly by voting-based techniques but do not work efficiently. In this work, we proposed a statistical estimator-based approach for the plane of reflection symmetry that is robust to outliers and missing parts. We pose the problem of finding the optimal estimator for the reflection symmetry as an optimization problem on a 2-Sphere that quickly converges to the global solution for an approximate initialization. We further adapt the heat kernel signature for symmetry invariant matching of mirror symmetric points. This approach helps us to decouple the chicken-and-egg problem of finding the optimal symmetry plane and correspondences between the reflective symmetric points. The proposed approach achieves comparable mean ground-truth error and 4.5\% increment in the F-score as compared to the state-of-the-art approaches on the benchmark dataset.



### MESSFN : a Multi-level and Enhanced Spectral-Spatial Fusion Network for Pan-sharpening
- **Arxiv ID**: http://arxiv.org/abs/2109.09937v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.09937v1)
- **Published**: 2021-09-21 03:38:52+00:00
- **Updated**: 2021-09-21 03:38:52+00:00
- **Authors**: Yuan Yuan, Yi Sun, Yuanlin Zhang
- **Comment**: 37 pages, 14 figures
- **Journal**: None
- **Summary**: Dominant pan-sharpening frameworks simply concatenate the MS stream and the PAN stream once at a specific level. This way of fusion neglects the multi-level spectral-spatial correlation between the two streams, which is vital to improving the fusion performance. In consideration of this, we propose a Multi-level and Enhanced Spectral-Spatial Fusion Network (MESSFN) with the following innovations: First, to fully exploit and strengthen the above correlation, a Hierarchical Multi-level Fusion Architecture (HMFA) is carefully designed. A novel Spectral-Spatial (SS) stream is established to hierarchically derive and fuse the multi-level prior spectral and spatial expertise from the MS stream and the PAN stream. This helps the SS stream master a joint spectral-spatial representation in the hierarchical network for better modeling the fusion relationship. Second, to provide superior expertise, consequently, based on the intrinsic characteristics of the MS image and the PAN image, two feature extraction blocks are specially developed. In the MS stream, a Residual Spectral Attention Block (RSAB) is proposed to mine the potential spectral correlations between different spectra of the MS image through adjacent cross-spectrum interaction. While in the PAN stream, a Residual Multi-scale Spatial Attention Block (RMSAB) is proposed to capture multi-scale information and reconstruct precise high-frequency details from the PAN image through an improved spatial attention-based inception structure. The spectral and spatial feature representations are enhanced. Extensive experiments on two datasets demonstrate that the proposed network is competitive with or better than state-of-the-art methods. Our code can be found in github.



### IgNet. A Super-precise Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2109.09939v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09939v1)
- **Published**: 2021-09-21 03:41:54+00:00
- **Updated**: 2021-09-21 03:41:54+00:00
- **Authors**: Igor Mackarov
- **Comment**: 16 pages, 8 figures
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) are known to be an effective means to detect and analyze images. Their power is essentially based on the ability to extract out images common features. There exist, however, images involving unique, irregular features or details. Such is a collection of unusual children drawings reflecting the kids imagination and individuality. These drawings were analyzed by means of a CNN constructed by means of Keras-TensorFlow. The same problem - on a significantly higher level - was solved with newly developed family of networks called IgNet that is described in this paper. It proved able to learn by 100 % all the categorical characteristics of the drawings. In the case of a regression task (learning the young artists ages) IgNet performed with an error of no more than 0.4 %. The principles are discussed of IgNet design that made it possible to reach such substantial results with rather simple network topology.



### Multi-Domain Few-Shot Learning and Dataset for Agricultural Applications
- **Arxiv ID**: http://arxiv.org/abs/2109.09952v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09952v1)
- **Published**: 2021-09-21 04:20:18+00:00
- **Updated**: 2021-09-21 04:20:18+00:00
- **Authors**: Sai Vidyaranya Nuthalapati, Anirudh Tunga
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic classification of pests and plants (both healthy and diseased) is of paramount importance in agriculture to improve yield. Conventional deep learning models based on convolutional neural networks require thousands of labeled examples per category. In this work we propose a method to learn from a few samples to automatically classify different pests, plants, and their diseases, using Few-Shot Learning (FSL). We learn a feature extractor to generate embeddings and then update the embeddings using Transformers. Using Mahalanobis distance, a class-covariance-based metric, we then calculate the similarity of the transformed embeddings with the embedding of the image to be classified. Using our proposed architecture, we conduct extensive experiments on multiple datasets showing the effectiveness of our proposed model. We conduct 42 experiments in total to comprehensively analyze the model and it achieves up to 14% and 24% performance gains on few-shot image classification benchmarks on two datasets.   We also compile a new FSL dataset containing images of healthy and diseased plants taken in real-world settings. Using our proposed architecture which has been shown to outperform several existing FSL architectures in agriculture, we provide strong baselines on our newly proposed dataset.



### Mutual Consistency Learning for Semi-supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.09960v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.09960v4)
- **Published**: 2021-09-21 04:47:42+00:00
- **Updated**: 2022-07-04 05:40:37+00:00
- **Authors**: Yicheng Wu, Zongyuan Ge, Donghao Zhang, Minfeng Xu, Lei Zhang, Yong Xia, Jianfei Cai
- **Comment**: Accepted by Medical Image Analysis
- **Journal**: None
- **Summary**: In this paper, we propose a novel mutual consistency network (MC-Net+) to effectively exploit the unlabeled data for semi-supervised medical image segmentation. The MC-Net+ model is motivated by the observation that deep models trained with limited annotations are prone to output highly uncertain and easily mis-classified predictions in the ambiguous regions (e.g., adhesive edges or thin branches) for medical image segmentation. Leveraging these challenging samples can make the semi-supervised segmentation model training more effective. Therefore, our proposed MC-Net+ model consists of two new designs. First, the model contains one shared encoder and multiple slightly different decoders (i.e., using different up-sampling strategies). The statistical discrepancy of multiple decoders' outputs is computed to denote the model's uncertainty, which indicates the unlabeled hard regions. Second, we apply a novel mutual consistency constraint between one decoder's probability output and other decoders' soft pseudo labels. In this way, we minimize the discrepancy of multiple outputs (i.e., the model uncertainty) during training and force the model to generate invariant results in such challenging regions, aiming at regularizing the model training. We compared the segmentation results of our MC-Net+ model with five state-of-the-art semi-supervised approaches on three public medical datasets. Extension experiments with two standard semi-supervised settings demonstrate the superior performance of our model over other methods, which sets a new state of the art for semi-supervised medical image segmentation. Our code is released publicly at https://github.com/ycwu1997/MC-Net.



### An Ultra-Fast Method for Simulation of Realistic Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2109.10353v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10353v1)
- **Published**: 2021-09-21 05:03:41+00:00
- **Updated**: 2021-09-21 05:03:41+00:00
- **Authors**: Mostafa Sharifzadeh, Habib Benali, Hassan Rivaz
- **Comment**: arXiv admin note: text overlap with arXiv:2109.09969
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have attracted a rapidly growing interest in a variety of different processing tasks in the medical ultrasound community. However, the performance of CNNs is highly reliant on both the amount and fidelity of the training data. Therefore, scarce data is almost always a concern, particularly in the medical field, where clinical data is not easily accessible. The utilization of synthetic data is a popular approach to address this challenge. However, but simulating a large number of images using packages such as Field II is time-consuming, and the distribution of simulated images is far from that of the real images. Herein, we introduce a novel ultra-fast ultrasound image simulation method based on the Fourier transform and evaluate its performance in a lesion segmentation task. We demonstrate that data augmentation using the images generated by the proposed method substantially outperforms Field II in terms of Dice similarity coefficient, while the simulation is almost 36000 times faster (both on CPU).



### Multi-Source Video Domain Adaptation with Temporal Attentive Moment Alignment
- **Arxiv ID**: http://arxiv.org/abs/2109.09964v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09964v2)
- **Published**: 2021-09-21 05:08:05+00:00
- **Updated**: 2021-09-26 04:37:39+00:00
- **Authors**: Yuecong Xu, Jianfei Yang, Haozhi Cao, Keyu Wu, Min Wu, Rui Zhao, Zhenghua Chen
- **Comment**: 10 pages, 5 figures, Dataset to be available at
  https://xuyu0010.github.io/msvda.html
- **Journal**: None
- **Summary**: Multi-Source Domain Adaptation (MSDA) is a more practical domain adaptation scenario in real-world scenarios. It relaxes the assumption in conventional Unsupervised Domain Adaptation (UDA) that source data are sampled from a single domain and match a uniform data distribution. MSDA is more difficult due to the existence of different domain shifts between distinct domain pairs. When considering videos, the negative transfer would be provoked by spatial-temporal features and can be formulated into a more challenging Multi-Source Video Domain Adaptation (MSVDA) problem. In this paper, we address the MSVDA problem by proposing a novel Temporal Attentive Moment Alignment Network (TAMAN) which aims for effective feature transfer by dynamically aligning both spatial and temporal feature moments. TAMAN further constructs robust global temporal features by attending to dominant domain-invariant local temporal features with high local classification confidence and low disparity between global and local feature discrepancies. To facilitate future research on the MSVDA problem, we introduce comprehensive benchmarks, covering extensive MSVDA scenarios. Empirical results demonstrate a superior performance of the proposed TAMAN across multiple MSVDA benchmarks.



### Towards the Classification of Error-Related Potentials using Riemannian Geometry
- **Arxiv ID**: http://arxiv.org/abs/2109.13085v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GL, cs.HC, I.5.4; J.3; J.m
- **Links**: [PDF](http://arxiv.org/pdf/2109.13085v1)
- **Published**: 2021-09-21 06:42:47+00:00
- **Updated**: 2021-09-21 06:42:47+00:00
- **Authors**: Yichen Tang, Jerry J. Zhang, Paul M. Corballis, Luke E. Hallum
- **Comment**: 4 pages, 3 figures, 1 table, submitted to and accepted by the 43rd
  Annual International Conference of the IEEE Engineering in Medicine and
  Biology Society (EMBC), this is the accepted version
- **Journal**: None
- **Summary**: The error-related potential (ErrP) is an event-related potential (ERP) evoked by an experimental participant's recognition of an error during task performance. ErrPs, originally described by cognitive psychologists, have been adopted for use in brain-computer interfaces (BCIs) for the detection and correction of errors, and the online refinement of decoding algorithms. Riemannian geometry-based feature extraction and classification is a new approach to BCI which shows good performance in a range of experimental paradigms, but has yet to be applied to the classification of ErrPs. Here, we describe an experiment that elicited ErrPs in seven normal participants performing a visual discrimination task. Audio feedback was provided on each trial. We used multi-channel electroencephalogram (EEG) recordings to classify ErrPs (success/failure), comparing a Riemannian geometry-based method to a traditional approach that computes time-point features. Overall, the Riemannian approach outperformed the traditional approach (78.2% versus 75.9% accuracy, p < 0.05); this difference was statistically significant (p < 0.05) in three of seven participants. These results indicate that the Riemannian approach better captured the features from feedback-elicited ErrPs, and may have application in BCI for error detection and correction.



### Automated segmentation and extraction of posterior eye segment using OCT scans
- **Arxiv ID**: http://arxiv.org/abs/2109.10000v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10000v2)
- **Published**: 2021-09-21 07:03:23+00:00
- **Updated**: 2021-10-18 13:47:51+00:00
- **Authors**: Bilal Hassan, Taimur Hassan, Ramsha Ahmed, Shiyin Qin, Naoufel Werghi
- **Comment**: Accepted in 2021 IEEE International Conference on Robotics and
  Automation in Industry (ICRAI)
- **Journal**: None
- **Summary**: This paper proposes an automated method for the segmentation and extraction of the posterior segment of the human eye, including the vitreous, retina, choroid, and sclera compartments, using multi-vendor optical coherence tomography (OCT) scans. The proposed method works in two phases. First extracts the retinal pigment epithelium (RPE) layer by applying the adaptive thresholding technique to identify the retina-choroid junction. Then, it exploits the structure tensor guided approach to extract the inner limiting membrane (ILM) and the choroidal stroma (CS) layers, locating the vitreous-retina and choroid-sclera junctions in the candidate OCT scan. Furthermore, these three junction boundaries are utilized to conduct posterior eye compartmentalization effectively for both healthy and disease eye OCT scans. The proposed framework is evaluated over 1000 OCT scans, where it obtained the mean intersection over union (IoU) and mean Dice similarity coefficient (DSC) scores of 0.874 and 0.930, respectively.



### Unsupervised Abstract Reasoning for Raven's Problem Matrices
- **Arxiv ID**: http://arxiv.org/abs/2109.10011v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10011v1)
- **Published**: 2021-09-21 07:44:58+00:00
- **Updated**: 2021-09-21 07:44:58+00:00
- **Authors**: Tao Zhuo, Qiang Huang, Mohan Kankanhalli
- **Comment**: Accepted by TIP
- **Journal**: None
- **Summary**: Raven's Progressive Matrices (RPM) is highly correlated with human intelligence, and it has been widely used to measure the abstract reasoning ability of humans. In this paper, to study the abstract reasoning capability of deep neural networks, we propose the first unsupervised learning method for solving RPM problems. Since the ground truth labels are not allowed, we design a pseudo target based on the prior constraints of the RPM formulation to approximate the ground truth label, which effectively converts the unsupervised learning strategy into a supervised one. However, the correct answer is wrongly labelled by the pseudo target, and thus the noisy contrast will lead to inaccurate model training. To alleviate this issue, we propose to improve the model performance with negative answers. Moreover, we develop a decentralization method to adapt the feature representation to different RPM problems. Extensive experiments on three datasets demonstrate that our method even outperforms some of the supervised approaches. Our code is available at https://github.com/visiontao/ncd.



### Self-Supervised Action-Space Prediction for Automated Driving
- **Arxiv ID**: http://arxiv.org/abs/2109.10024v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10024v1)
- **Published**: 2021-09-21 08:27:56+00:00
- **Updated**: 2021-09-21 08:27:56+00:00
- **Authors**: Faris Janjoš, Maxim Dolgov, J. Marius Zöllner
- **Comment**: None
- **Journal**: None
- **Summary**: Making informed driving decisions requires reliable prediction of other vehicles' trajectories. In this paper, we present a novel learned multi-modal trajectory prediction architecture for automated driving. It achieves kinematically feasible predictions by casting the learning problem into the space of accelerations and steering angles -- by performing action-space prediction, we can leverage valuable model knowledge. Additionally, the dimensionality of the action manifold is lower than that of the state manifold, whose intrinsically correlated states are more difficult to capture in a learned manner. For the purpose of action-space prediction, we present the simple Feed-Forward Action-Space Prediction (FFW-ASP) architecture. Then, we build on this notion and introduce the novel Self-Supervised Action-Space Prediction (SSP-ASP) architecture that outputs future environment context features in addition to trajectories. A key element in the self-supervised architecture is that, based on an observed action history and past context features, future context features are predicted prior to future trajectories. The proposed methods are evaluated on real-world datasets containing urban intersections and roundabouts, and show accurate predictions, outperforming state-of-the-art for kinematically feasible predictions in several prediction metrics.



### VPN: Video Provenance Network for Robust Content Attribution
- **Arxiv ID**: http://arxiv.org/abs/2109.10038v1
- **DOI**: 10.1145/3485441.3485650
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10038v1)
- **Published**: 2021-09-21 09:07:05+00:00
- **Updated**: 2021-09-21 09:07:05+00:00
- **Authors**: Alexander Black, Tu Bui, Simon Jenni, Vishy Swaminathan, John Collomosse
- **Comment**: CVMP2021 camera-ready version
- **Journal**: None
- **Summary**: We present VPN - a content attribution method for recovering provenance information from videos shared online. Platforms, and users, often transform video into different quality, codecs, sizes, shapes, etc. or slightly edit its content such as adding text or emoji, as they are redistributed online. We learn a robust search embedding for matching such video, invariant to these transformations, using full-length or truncated video queries. Once matched against a trusted database of video clips, associated information on the provenance of the clip is presented to the user. We use an inverted index to match temporal chunks of video using late-fusion to combine both visual and audio features. In both cases, features are extracted via a deep neural network trained using contrastive learning on a dataset of original and augmented video clips. We demonstrate high accuracy recall over a corpus of 100,000 videos.



### Single Person Pose Estimation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2109.10056v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10056v1)
- **Published**: 2021-09-21 09:53:15+00:00
- **Updated**: 2021-09-21 09:53:15+00:00
- **Authors**: Feng Zhang, Xiatian Zhu, Chen Wang
- **Comment**: 16 pages, 3 figures
- **Journal**: None
- **Summary**: Human pose estimation in unconstrained images and videos is a fundamental computer vision task. To illustrate the evolutionary path in technique, in this survey we summarize representative human pose methods in a structured taxonomy, with a particular focus on deep learning models and single-person image setting. Specifically, we examine and survey all the components of a typical human pose estimation pipeline, including data augmentation, model architecture and backbone, supervision representation, post-processing, standard datasets, evaluation metrics. To envisage the future directions, we finally discuss the key unsolved problems and potential trends for human pose estimation.



### LOTR: Face Landmark Localization Using Localization Transformer
- **Arxiv ID**: http://arxiv.org/abs/2109.10057v3
- **DOI**: 10.1109/ACCESS.2022.3149380
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10057v3)
- **Published**: 2021-09-21 09:54:27+00:00
- **Updated**: 2022-02-22 07:53:32+00:00
- **Authors**: Ukrit Watchareeruetai, Benjaphan Sommana, Sanjana Jain, Pavit Noinongyao, Ankush Ganguly, Aubin Samacoits, Samuel W. F. Earp, Nakarin Sritrakool
- **Comment**: Accepted for publication in IEEE Access
- **Journal**: IEEE Access 2022
- **Summary**: This paper presents a novel Transformer-based facial landmark localization network named Localization Transformer (LOTR). The proposed framework is a direct coordinate regression approach leveraging a Transformer network to better utilize the spatial information in the feature map. An LOTR model consists of three main modules: 1) a visual backbone that converts an input image into a feature map, 2) a Transformer module that improves the feature representation from the visual backbone, and 3) a landmark prediction head that directly predicts the landmark coordinates from the Transformer's representation. Given cropped-and-aligned face images, the proposed LOTR can be trained end-to-end without requiring any post-processing steps. This paper also introduces the smooth-Wing loss function, which addresses the gradient discontinuity of the Wing loss, leading to better convergence than standard loss functions such as L1, L2, and Wing loss. Experimental results on the JD landmark dataset provided by the First Grand Challenge of 106-Point Facial Landmark Localization indicate the superiority of LOTR over the existing methods on the leaderboard and two recent heatmap-based approaches. On the WFLW dataset, the proposed LOTR framework demonstrates promising results compared with several state-of-the-art methods. Additionally, we report the improvement in state-of-the-art face recognition performance when using our proposed LOTRs for face alignment.



### DS-Net++: Dynamic Weight Slicing for Efficient Inference in CNNs and Transformers
- **Arxiv ID**: http://arxiv.org/abs/2109.10060v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10060v1)
- **Published**: 2021-09-21 09:57:21+00:00
- **Updated**: 2021-09-21 09:57:21+00:00
- **Authors**: Changlin Li, Guangrun Wang, Bing Wang, Xiaodan Liang, Zhihui Li, Xiaojun Chang
- **Comment**: Extension of the CVPR 2021 oral paper
  (https://openaccess.thecvf.com/content/CVPR2021/html/Li_Dynamic_Slimmable_Network_CVPR_2021_paper.html)
- **Journal**: None
- **Summary**: Dynamic networks have shown their promising capability in reducing theoretical computation complexity by adapting their architectures to the input during inference. However, their practical runtime usually lags behind the theoretical acceleration due to inefficient sparsity. Here, we explore a hardware-efficient dynamic inference regime, named dynamic weight slicing, which adaptively slice a part of network parameters for inputs with diverse difficulty levels, while keeping parameters stored statically and contiguously in hardware to prevent the extra burden of sparse computation. Based on this scheme, we present dynamic slimmable network (DS-Net) and dynamic slice-able network (DS-Net++) by input-dependently adjusting filter numbers of CNNs and multiple dimensions in both CNNs and transformers, respectively. To ensure sub-network generality and routing fairness, we propose a disentangled two-stage optimization scheme with training techniques such as in-place bootstrapping (IB), multi-view consistency (MvCo) and sandwich gate sparsification (SGS) to train supernet and gate separately. Extensive experiments on 4 datasets and 3 different network architectures demonstrate our method consistently outperforms state-of-the-art static and dynamic model compression methods by a large margin (up to 6.6%). Typically, DS-Net++ achieves 2-4x computation reduction and 1.62x real-world acceleration over MobileNet, ResNet-50 and Vision Transformer, with minimal accuracy drops (0.1-0.3%) on ImageNet. Code release: https://github.com/changlin31/DS-Net



### Scale-aware direct monocular odometry
- **Arxiv ID**: http://arxiv.org/abs/2109.10077v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10077v2)
- **Published**: 2021-09-21 10:30:15+00:00
- **Updated**: 2022-07-22 12:55:09+00:00
- **Authors**: Carlos Campos, Juan D. Tardós
- **Comment**: This paper has been accepted for publication in the IROS2022
  conference
- **Journal**: None
- **Summary**: We present a generic framework for scale-aware direct monocular odometry based on depth prediction from a deep neural network. In contrast with previous methods where depth information is only partially exploited, we formulate a novel depth prediction residual which allows us to incorporate multi-view depth information. In addition, we propose to use a truncated robust cost function which prevents considering inconsistent depth estimations. The photometric and depth-prediction measurements are integrated into a tightly-coupled optimization leading to a scale-aware monocular system which does not accumulate scale drift. Our proposal does not particularize for a concrete neural network, being able to work along with the vast majority of the existing depth prediction solutions. We demonstrate the validity and generality of our proposal evaluating it on the KITTI odometry dataset, using two publicly available neural networks and comparing it with similar approaches and the state-of-the-art for monocular and stereo SLAM. Experiments show that our proposal largely outperforms classic monocular SLAM, being 5 to 9 times more precise, beating similar approaches and having an accuracy which is closer to that of stereo systems.



### Learning Interpretable Concept Groups in CNNs
- **Arxiv ID**: http://arxiv.org/abs/2109.10078v1
- **DOI**: 10.24963/ijcai.2021/147
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10078v1)
- **Published**: 2021-09-21 10:32:04+00:00
- **Updated**: 2021-09-21 10:32:04+00:00
- **Authors**: Saurabh Varshneya, Antoine Ledent, Robert A. Vandermeulen, Yunwen Lei, Matthias Enders, Damian Borth, Marius Kloft
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel training methodology -- Concept Group Learning (CGL) -- that encourages training of interpretable CNN filters by partitioning filters in each layer into concept groups, each of which is trained to learn a single visual concept. We achieve this through a novel regularization strategy that forces filters in the same group to be active in similar image regions for a given layer. We additionally use a regularizer to encourage a sparse weighting of the concept groups in each layer so that a few concept groups can have greater importance than others. We quantitatively evaluate CGL's model interpretability using standard interpretability evaluation techniques and find that our method increases interpretability scores in most cases. Qualitatively we compare the image regions that are most active under filters learned using CGL versus filters learned without CGL and find that CGL activation regions more strongly concentrate around semantically relevant features.



### PDFNet: Pointwise Dense Flow Network for Urban-Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.10083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10083v1)
- **Published**: 2021-09-21 10:39:46+00:00
- **Updated**: 2021-09-21 10:39:46+00:00
- **Authors**: Venkata Satya Sai Ajay Daliparthi
- **Comment**: Technical report - 24 Pages
- **Journal**: None
- **Summary**: In recent years, using a deep convolutional neural network (CNN) as a feature encoder (or backbone) is the most commonly observed architectural pattern in several computer vision methods, and semantic segmentation is no exception. The two major drawbacks of this architectural pattern are: (i) the networks often fail to capture small classes such as wall, fence, pole, traffic light, traffic sign, and bicycle, which are crucial for autonomous vehicles to make accurate decisions. (ii) due to the arbitrarily increasing depth, the networks require massive labeled data and additional regularization techniques to converge and to prevent the risk of over-fitting, respectively. While regularization techniques come at minimal cost, the collection of labeled data is an expensive and laborious process. In this work, we address these two drawbacks by proposing a novel lightweight architecture named point-wise dense flow network (PDFNet). In PDFNet, we employ dense, residual, and multiple shortcut connections to allow a smooth gradient flow to all parts of the network. The extensive experiments on Cityscapes and CamVid benchmarks demonstrate that our method significantly outperforms baselines in capturing small classes and in few-data regimes. Moreover, our method achieves considerable performance in classifying out-of-the training distribution samples, evaluated on Cityscapes to KITTI dataset.



### Bayesian Confidence Calibration for Epistemic Uncertainty Modelling
- **Arxiv ID**: http://arxiv.org/abs/2109.10092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10092v1)
- **Published**: 2021-09-21 10:53:16+00:00
- **Updated**: 2021-09-21 10:53:16+00:00
- **Authors**: Fabian Küppers, Jan Kronenberger, Jonas Schneider, Anselm Haselhoff
- **Comment**: Published at 32nd IEEE Intelligent Vehicles Symposium (IV) 2021
- **Journal**: None
- **Summary**: Modern neural networks have found to be miscalibrated in terms of confidence calibration, i.e., their predicted confidence scores do not reflect the observed accuracy or precision. Recent work has introduced methods for post-hoc confidence calibration for classification as well as for object detection to address this issue. Especially in safety critical applications, it is crucial to obtain a reliable self-assessment of a model. But what if the calibration method itself is uncertain, e.g., due to an insufficient knowledge base?   We introduce Bayesian confidence calibration - a framework to obtain calibrated confidence estimates in conjunction with an uncertainty of the calibration method. Commonly, Bayesian neural networks (BNN) are used to indicate a network's uncertainty about a certain prediction. BNNs are interpreted as neural networks that use distributions instead of weights for inference. We transfer this idea of using distributions to confidence calibration. For this purpose, we use stochastic variational inference to build a calibration mapping that outputs a probability distribution rather than a single calibrated estimate. Using this approach, we achieve state-of-the-art calibration performance for object detection calibration. Finally, we show that this additional type of uncertainty can be used as a sufficient criterion for covariate shift detection. All code is open source and available at https://github.com/EFS-OpenSource/calibration-framework.



### StereOBJ-1M: Large-scale Stereo Image Dataset for 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.10115v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.10115v3)
- **Published**: 2021-09-21 11:56:38+00:00
- **Updated**: 2022-03-14 18:35:24+00:00
- **Authors**: Xingyu Liu, Shun Iwase, Kris M. Kitani
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: We present a large-scale stereo RGB image object pose estimation dataset named the $\textbf{StereOBJ-1M}$ dataset. The dataset is designed to address challenging cases such as object transparency, translucency, and specular reflection, in addition to the common challenges of occlusion, symmetry, and variations in illumination and environments. In order to collect data of sufficient scale for modern deep learning models, we propose a novel method for efficiently annotating pose data in a multi-view fashion that allows data capturing in complex and flexible environments. Fully annotated with 6D object poses, our dataset contains over 393K frames and over 1.5M annotations of 18 objects recorded in 182 scenes constructed in 11 different environments. The 18 objects include 8 symmetric objects, 7 transparent objects, and 8 reflective objects. We benchmark two state-of-the-art pose estimation frameworks on StereOBJ-1M as baselines for future work. We also propose a novel object-level pose optimization method for computing 6D pose from keypoint predictions in multiple images. Project website: https://sites.google.com/view/stereobj-1m.



### Survey on Semantic Stereo Matching / Semantic Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.10123v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10123v1)
- **Published**: 2021-09-21 12:11:56+00:00
- **Updated**: 2021-09-21 12:11:56+00:00
- **Authors**: Viny Saajan Victor, Peter Neigel
- **Comment**: None
- **Journal**: None
- **Summary**: Stereo matching is one of the widely used techniques for inferring depth from stereo images owing to its robustness and speed. It has become one of the major topics of research since it finds its applications in autonomous driving, robotic navigation, 3D reconstruction, and many other fields. Finding pixel correspondences in non-textured, occluded and reflective areas is the major challenge in stereo matching. Recent developments have shown that semantic cues from image segmentation can be used to improve the results of stereo matching. Many deep neural network architectures have been proposed to leverage the advantages of semantic segmentation in stereo matching. This paper aims to give a comparison among the state of art networks both in terms of accuracy and in terms of speed which are of higher importance in real-time applications.



### KDFNet: Learning Keypoint Distance Field for 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.10127v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.10127v1)
- **Published**: 2021-09-21 12:17:24+00:00
- **Updated**: 2021-09-21 12:17:24+00:00
- **Authors**: Xingyu Liu, Shun Iwase, Kris M. Kitani
- **Comment**: IROS 2021
- **Journal**: None
- **Summary**: We present KDFNet, a novel method for 6D object pose estimation from RGB images. To handle occlusion, many recent works have proposed to localize 2D keypoints through pixel-wise voting and solve a Perspective-n-Point (PnP) problem for pose estimation, which achieves leading performance. However, such voting process is direction-based and cannot handle long and thin objects where the direction intersections cannot be robustly found. To address this problem, we propose a novel continuous representation called Keypoint Distance Field (KDF) for projected 2D keypoint locations. Formulated as a 2D array, each element of the KDF stores the 2D Euclidean distance between the corresponding image pixel and a specified projected 2D keypoint. We use a fully convolutional neural network to regress the KDF for each keypoint. Using this KDF encoding of projected object keypoint locations, we propose to use a distance-based voting scheme to localize the keypoints by calculating circle intersections in a RANSAC fashion. We validate the design choices of our framework by extensive ablation experiments. Our proposed method achieves state-of-the-art performance on Occlusion LINEMOD dataset with an average ADD(-S) accuracy of 50.3% and TOD dataset mug subset with an average ADD accuracy of 75.72%. Extensive experiments and visualizations demonstrate that the proposed method is able to robustly estimate the 6D pose in challenging scenarios including occlusion.



### Self-supervised Representation Learning for Reliable Robotic Monitoring of Fruit Anomalies
- **Arxiv ID**: http://arxiv.org/abs/2109.10135v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10135v2)
- **Published**: 2021-09-21 12:41:02+00:00
- **Updated**: 2022-03-24 00:50:17+00:00
- **Authors**: Taeyeong Choi, Owen Would, Adrian Salazar-Gomez, Grzegorz Cielniak
- **Comment**: Accepted to ICRA2022; Codes and data are all available online
- **Journal**: None
- **Summary**: Data augmentation can be a simple yet powerful tool for autonomous robots to fully utilise available data for selfsupervised identification of atypical scenes or objects. State-of-the-art augmentation methods arbitrarily embed "structural" peculiarity on typical images so that classifying these artefacts can provide guidance for learning representations for the detection of anomalous visual signals. In this paper, however, we argue that learning such structure-sensitive representations can be a suboptimal approach to some classes of anomaly (e.g., unhealthy fruits) which could be better recognised by a different type of visual element such as "colour". We thus propose Channel Randomisation as a novel data augmentation method for restricting neural networks to learn encoding of "colour irregularity" whilst predicting channel-randomised images to ultimately build reliable fruit-monitoring robots identifying atypical fruit qualities. Our experiments show that (1) this colour-based alternative can better learn representations for consistently accurate identification of fruit anomalies in various fruit species, and also, (2) unlike other methods, the validation accuracy can be utilised as a criterion for early stopping of training in practice due to positive correlation between the performance in the self-supervised colour-differentiation task and the subsequent detection rate of actual anomalous fruits. Also, the proposed approach is evaluated on a new agricultural dataset, Riseholme-2021, consisting of 3.5K strawberry images gathered by a mobile robot, which we share online to encourage active agri-robotics research.



### 3D Point Cloud Completion with Geometric-Aware Adversarial Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.10161v1
- **DOI**: 10.1109/ICPR56361.2022.9956045
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10161v1)
- **Published**: 2021-09-21 13:16:46+00:00
- **Updated**: 2021-09-21 13:16:46+00:00
- **Authors**: Mengxi Wu, Hao Huang, Yi Fang
- **Comment**: 11 page, 5 figures
- **Journal**: None
- **Summary**: With the popularity of 3D sensors in self-driving and other robotics applications, extensive research has focused on designing novel neural network architectures for accurate 3D point cloud completion. However, unlike in point cloud classification and reconstruction, the role of adversarial samples in3D point cloud completion has seldom been explored. In this work, we show that training with adversarial samples can improve the performance of neural networks on 3D point cloud completion tasks. We propose a novel approach to generate adversarial samples that benefit both the performance of clean and adversarial samples. In contrast to the PGD-k attack, our method generates adversarial samples that keep the geometric features in clean samples and contain few outliers. In particular, we use principal directions to constrain the adversarial perturbations for each input point. The gradient components in the mean direction of principal directions are taken as adversarial perturbations. In addition, we also investigate the effect of using the minimum curvature direction. Besides, we adopt attack strength accumulation and auxiliary Batch Normalization layers method to speed up the training process and alleviate the distribution mismatch between clean and adversarial samples. Experimental results show that training with the adversarial samples crafted by our method effectively enhances the performance of PCN on the ShapeNet dataset.



### Oriented Object Detection in Aerial Images Based on Area Ratio of Parallelogram
- **Arxiv ID**: http://arxiv.org/abs/2109.10187v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.10187v5)
- **Published**: 2021-09-21 14:13:36+00:00
- **Updated**: 2021-11-08 07:14:44+00:00
- **Authors**: Xinyi Yu, Mi Lin, Jiangping Lu, Linlin Ou
- **Comment**: None
- **Journal**: None
- **Summary**: Oriented object detection is a challenging task in aerial images since the objects in aerial images are displayed in arbitrary directions and are frequently densely packed. The mainstream detectors describe rotating objects using a five-parament or eight-parament representations, which suffer from representation ambiguity for orientated object definition. In this paper, we propose a novel representation method based on area ratio of parallelogram, called ARP. Specifically, ARP regresses the minimum bounding rectangle of the oriented object and three area ratios. Three area ratios include the area ratio of a directed object to the smallest circumscribed rectangle and two parallelograms to the minimum circumscribed rectangle. It simplifies offset learning and eliminates the issue of angular periodicity or label point sequences for oriented objects. To further remedy the confusion issue of nearly horizontal objects, the area ratio between the object and its minimal circumscribed rectangle is employed to guide the selection of horizontal or oriented detection for each object. Moreover, the rotated efficient Intersection over Union (R-EIoU) loss with horizontal bounding box and three area ratios are designed to optimize the bounding box regression for rotating objects. Experimental results on remote sensing datasets, including HRSC2016, DOTA, and UCAS-AOD, show that our method achieves superior detection performance than many state-of-the-art approaches.



### Does Vision-and-Language Pretraining Improve Lexical Grounding?
- **Arxiv ID**: http://arxiv.org/abs/2109.10246v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10246v1)
- **Published**: 2021-09-21 15:12:39+00:00
- **Updated**: 2021-09-21 15:12:39+00:00
- **Authors**: Tian Yun, Chen Sun, Ellie Pavlick
- **Comment**: Camera ready for Findings of EMNLP 2021
- **Journal**: None
- **Summary**: Linguistic representations derived from text alone have been criticized for their lack of grounding, i.e., connecting words to their meanings in the physical world. Vision-and-Language (VL) models, trained jointly on text and image or video data, have been offered as a response to such criticisms. However, while VL pretraining has shown success on multimodal tasks such as visual question answering, it is not yet known how the internal linguistic representations themselves compare to their text-only counterparts. This paper compares the semantic representations learned via VL vs. text-only pretraining for two recent VL models using a suite of analyses (clustering, probing, and performance on a commonsense question answering task) in a language-only setting. We find that the multimodal models fail to significantly outperform the text-only variants, suggesting that future work is required if multimodal pretraining is to be pursued as a means of improving NLP in general.



### Skeleton-Graph: Long-Term 3D Motion Prediction From 2D Observations Using Deep Spatio-Temporal Graph CNNs
- **Arxiv ID**: http://arxiv.org/abs/2109.10257v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.10257v2)
- **Published**: 2021-09-21 15:33:40+00:00
- **Updated**: 2021-09-27 03:22:31+00:00
- **Authors**: Abduallah Mohamed, Huancheng Chen, Zhangyang Wang, Christian Claudel
- **Comment**: To appear in the The ROAD Challenge: Event Detection for Situation
  Awareness in Autonomous Driving ICCV 2021 Workshop
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV) Workshops, 2021
- **Summary**: Several applications such as autonomous driving, augmented reality and virtual reality require a precise prediction of the 3D human pose. Recently, a new problem was introduced in the field to predict the 3D human poses from observed 2D poses. We propose Skeleton-Graph, a deep spatio-temporal graph CNN model that predicts the future 3D skeleton poses in a single pass from the 2D ones. Unlike prior works, Skeleton-Graph focuses on modeling the interaction between the skeleton joints by exploiting their spatial configuration. This is being achieved by formulating the problem as a graph structure while learning a suitable graph adjacency kernel. By the design, Skeleton-Graph predicts the future 3D poses without divergence in the long-term, unlike prior works. We also introduce a new metric that measures the divergence of predictions in the long term. Our results show an FDE improvement of at least 27% and an ADE of 4% on both the GTA-IM and PROX datasets respectively in comparison with prior works. Also, we are 88% and 93% less divergence on the long-term motion prediction in comparison with prior works on both GTA-IM and PROX datasets. Code is available at https://github.com/abduallahmohamed/Skeleton-Graph.git



### Comparison of single and multitask learning for predicting cognitive decline based on MRI data
- **Arxiv ID**: http://arxiv.org/abs/2109.10266v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10266v1)
- **Published**: 2021-09-21 15:46:42+00:00
- **Updated**: 2021-09-21 15:46:42+00:00
- **Authors**: Vandad Imani, Mithilesh Prakash, Marzieh Zare, Jussi Tohka
- **Comment**: None
- **Journal**: None
- **Summary**: The Alzheimer's Disease Assessment Scale-Cognitive subscale (ADAS-Cog) is a neuropsychological tool that has been designed to assess the severity of cognitive symptoms of dementia. Personalized prediction of the changes in ADAS-Cog scores could help in timing therapeutic interventions in dementia and at-risk populations. In the present work, we compared single and multitask learning approaches to predict the changes in ADAS-Cog scores based on T1-weighted anatomical magnetic resonance imaging (MRI). In contrast to most machine learning-based prediction methods ADAS-Cog changes, we stratified the subjects based on their baseline diagnoses and evaluated the prediction performances in each group. Our experiments indicated a positive relationship between the predicted and observed ADAS-Cog score changes in each diagnostic group, suggesting that T1-weighted MRI has a predictive value for evaluating cognitive decline in the entire AD continuum. We further studied whether correction of the differences in the magnetic field strength of MRI would improve the ADAS-Cog score prediction. The partial least square-based domain adaptation slightly improved the prediction performance, but the improvement was marginal. In summary, this study demonstrated that ADAS-Cog change could be, to some extent, predicted based on anatomical MRI. Based on this study, the recommended method for learning the predictive models is a single-task regularized linear regression due to its simplicity and good performance. It appears important to combine the training data across all subject groups for the most effective predictive models.



### SemCal: Semantic LiDAR-Camera Calibration using Neural MutualInformation Estimator
- **Arxiv ID**: http://arxiv.org/abs/2109.10270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10270v1)
- **Published**: 2021-09-21 15:51:24+00:00
- **Updated**: 2021-09-21 15:51:24+00:00
- **Authors**: Peng Jiang, Philip Osteen, Srikanth Saripalli
- **Comment**: 7 pages, 10 figures, accepted by The 2021 IEEE International
  Conference on Multisensor Fusion and Integration (MFI 2021). arXiv admin
  note: substantial text overlap with arXiv:2104.12023
- **Journal**: None
- **Summary**: This paper proposes SemCal: an automatic, targetless, extrinsic calibration algorithm for a LiDAR and camera system using semantic information. We leverage a neural information estimator to estimate the mutual information (MI) of semantic information extracted from each sensor measurement, facilitating semantic-level data association. By using a matrix exponential formulation of the $se(3)$ transformation and a kernel-based sampling method to sample from camera measurement based on LiDAR projected points, we can formulate the LiDAR-Camera calibration problem as a novel differentiable objective function that supports gradient-based optimization methods. We also introduce a semantic-based initial calibration method using 2D MI-based image registration and Perspective-n-Point (PnP) solver. To evaluate performance, we demonstrate the robustness of our method and quantitatively analyze the accuracy using a synthetic dataset. We also evaluate our algorithm qualitatively on an urban dataset (KITTI360) and an off-road dataset (RELLIS-3D) benchmark datasets using both hand-annotated ground truth labels as well as labels predicted by the state-of-the-art deep learning models, showing improvement over recent comparable calibration approaches.



### TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models
- **Arxiv ID**: http://arxiv.org/abs/2109.10282v5
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10282v5)
- **Published**: 2021-09-21 16:01:56+00:00
- **Updated**: 2022-09-06 15:32:48+00:00
- **Authors**: Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei
- **Comment**: Work in Progress
- **Journal**: None
- **Summary**: Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at \url{https://aka.ms/trocr}.



### Finding Facial Forgery Artifacts with Parts-Based Detectors
- **Arxiv ID**: http://arxiv.org/abs/2109.10688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10688v1)
- **Published**: 2021-09-21 16:18:45+00:00
- **Updated**: 2021-09-21 16:18:45+00:00
- **Authors**: Steven Schwarcz, Rama Chellappa
- **Comment**: Accepted into the CVPR Workshop on Media Forensics 2021
- **Journal**: None
- **Summary**: Manipulated videos, especially those where the identity of an individual has been modified using deep neural networks, are becoming an increasingly relevant threat in the modern day. In this paper, we seek to develop a generalizable, explainable solution to detecting these manipulated videos. To achieve this, we design a series of forgery detection systems that each focus on one individual part of the face. These parts-based detection systems, which can be combined and used together in a single architecture, meet all of our desired criteria - they generalize effectively between datasets and give us valuable insights into what the network is looking at when making its decision. We thus use these detectors to perform detailed empirical analysis on the FaceForensics++, Celeb-DF, and Facebook Deepfake Detection Challenge datasets, examining not just what the detectors find but also collecting and analyzing useful related statistics on the datasets themselves.



### Learning PAC-Bayes Priors for Probabilistic Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.10304v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10304v1)
- **Published**: 2021-09-21 16:27:42+00:00
- **Updated**: 2021-09-21 16:27:42+00:00
- **Authors**: Maria Perez-Ortiz, Omar Rivasplata, Benjamin Guedj, Matthew Gleeson, Jingyu Zhang, John Shawe-Taylor, Miroslaw Bober, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works have investigated deep learning models trained by optimising PAC-Bayes bounds, with priors that are learnt on subsets of the data. This combination has been shown to lead not only to accurate classifiers, but also to remarkably tight risk certificates, bearing promise towards self-certified learning (i.e. use all the data to learn a predictor and certify its quality). In this work, we empirically investigate the role of the prior. We experiment on 6 datasets with different strategies and amounts of data to learn data-dependent PAC-Bayes priors, and we compare them in terms of their effect on test performance of the learnt predictors and tightness of their risk certificate. We ask what is the optimal amount of data which should be allocated for building the prior and show that the optimum may be dataset dependent. We demonstrate that using a small percentage of the prior-building data for validation of the prior leads to promising results. We include a comparison of underparameterised and overparameterised models, along with an empirical study of different training objectives and regularisation strategies to learn the prior distribution.



### CondNet: Conditional Classifier for Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.10322v1
- **DOI**: 10.1109/LSP.2021.3070472
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.10322v1)
- **Published**: 2021-09-21 17:19:09+00:00
- **Updated**: 2021-09-21 17:19:09+00:00
- **Authors**: Changqian Yu, Yuanjie Shao, Changxin Gao, Nong Sang
- **Comment**: Accepted to IEEE SPL. 4 pages, 3 figures, 4 tables
- **Journal**: None
- **Summary**: The fully convolutional network (FCN) has achieved tremendous success in dense visual recognition tasks, such as scene segmentation. The last layer of FCN is typically a global classifier (1x1 convolution) to recognize each pixel to a semantic label. We empirically show that this global classifier, ignoring the intra-class distinction, may lead to sub-optimal results.   In this work, we present a conditional classifier to replace the traditional global classifier, where the kernels of the classifier are generated dynamically conditioned on the input. The main advantages of the new classifier consist of: (i) it attends on the intra-class distinction, leading to stronger dense recognition capability; (ii) the conditional classifier is simple and flexible to be integrated into almost arbitrary FCN architectures to improve the prediction. Extensive experiments demonstrate that the proposed classifier performs favourably against the traditional classifier on the FCN architecture. The framework equipped with the conditional classifier (called CondNet) achieves new state-of-the-art performances on two datasets. The code and models are available at https://git.io/CondNet.



### Data-driven controllers and the need for perception systems in underwater manipulation
- **Arxiv ID**: http://arxiv.org/abs/2109.10327v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.10327v1)
- **Published**: 2021-09-21 17:25:10+00:00
- **Updated**: 2021-09-21 17:25:10+00:00
- **Authors**: James P. Oubre, Ignacio Carlucho, Corina Barbalata
- **Comment**: None
- **Journal**: None
- **Summary**: The underwater environment poses a complex problem for developing autonomous capabilities for Underwater Vehicle Manipulator Systems (UVMSs). The modeling of UVMSs is a complicated and costly process due to the highly nonlinear dynamics and the presence of unknown hydrodynamical effects. This is aggravated in tasks where the manipulation of objects is necessary, as this may not only introduce external disturbances that can lead to a fast degradation of the control system performance, but also requires the coordinating with a vision system for the correct grasping and operation of the object. In this article, we introduce a control strategy for UVMSs working with unknown payloads. The proposed control strategy is based on a data-driven optimal controller. We present a number of experimental results showing the benefits of the proposed strategy. Furthermore, we include a discussion regarding the visual perception requirements for the UVMS in order to achieve full autonomy in underwater manipulation tasks of unknown payloads.



### Homography augumented momentum constrastive learning for SAR image retrieval
- **Arxiv ID**: http://arxiv.org/abs/2109.10329v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10329v1)
- **Published**: 2021-09-21 17:27:07+00:00
- **Updated**: 2021-09-21 17:27:07+00:00
- **Authors**: Seonho Park, Maciej Rysz, Kathleen M. Dipple, Panos M. Pardalos
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based image retrieval has been emphasized in computer vision. Representation embedding extracted by deep neural networks (DNNs) not only aims at containing semantic information of the image, but also can manage large-scale image retrieval tasks. In this work, we propose a deep learning-based image retrieval approach using homography transformation augmented contrastive learning to perform large-scale synthetic aperture radar (SAR) image search tasks. Moreover, we propose a training method for the DNNs induced by contrastive learning that does not require any labeling procedure. This may enable tractability of large-scale datasets with relative ease. Finally, we verify the performance of the proposed method by conducting experiments on the polarimetric SAR image datasets.



### Robust marginalization of baryonic effects for cosmological inference at the field level
- **Arxiv ID**: http://arxiv.org/abs/2109.10360v1
- **DOI**: None
- **Categories**: **astro-ph.CO**, astro-ph.GA, astro-ph.IM, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10360v1)
- **Published**: 2021-09-21 18:00:01+00:00
- **Updated**: 2021-09-21 18:00:01+00:00
- **Authors**: Francisco Villaescusa-Navarro, Shy Genel, Daniel Angles-Alcazar, David N. Spergel, Yin Li, Benjamin Wandelt, Leander Thiele, Andrina Nicola, Jose Manuel Zorrilla Matilla, Helen Shao, Sultan Hassan, Desika Narayanan, Romeel Dave, Mark Vogelsberger
- **Comment**: 7 pages, 4 figures. Second paper of a series of four. The 2D maps,
  codes, and network weights used in this paper are publicly available at
  https://camels-multifield-dataset.readthedocs.io
- **Journal**: None
- **Summary**: We train neural networks to perform likelihood-free inference from $(25\,h^{-1}{\rm Mpc})^2$ 2D maps containing the total mass surface density from thousands of hydrodynamic simulations of the CAMELS project. We show that the networks can extract information beyond one-point functions and power spectra from all resolved scales ($\gtrsim 100\,h^{-1}{\rm kpc}$) while performing a robust marginalization over baryonic physics at the field level: the model can infer the value of $\Omega_{\rm m} (\pm 4\%)$ and $\sigma_8 (\pm 2.5\%)$ from simulations completely different to the ones used to train it.



### Coast Sargassum Level Estimation from Smartphone Pictures
- **Arxiv ID**: http://arxiv.org/abs/2109.10390v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.10390v1)
- **Published**: 2021-09-21 18:21:45+00:00
- **Updated**: 2021-09-21 18:21:45+00:00
- **Authors**: Uriarte-Arcia Abril Valeria, Vasquez-Gomez Juan Irving, Taud Hind, Garcia-Floriano Andres, Ventura-Molina Elias
- **Comment**: Under preparation for submission
- **Journal**: None
- **Summary**: Since 2011, significant and atypical arrival of two species of surface dwelling algae, Sargassum natans and Sargassum Fluitans, have been detected in the Mexican Caribbean. This massive accumulation of algae has had a great environmental and economic impact. Therefore, for the government, ecologists, and local businesses, it is important to keep track of the amount of sargassum that arrives on the Caribbean coast. High-resolution satellite imagery is expensive or may be time delayed. Therefore, we propose to estimate the amount of sargassum based on ground-level smartphone photographs. From the computer vision perspective, the problem is quite difficult since no information about the 3D world is provided, in consequence, we have to model it as a classification problem, where a set of five labels define the amount. For this purpose, we have built a dataset with more than one thousand examples from public forums such as Facebook or Instagram and we have tested several state-of-the-art convolutional networks. As a result, the VGG network trained under fine-tuning showed the best performance. Even though the reached accuracy could be improved with more examples, the current prediction distribution is narrow, so the predictions are adequate for keeping a record and taking quick ecological actions.



### Towards a Real-Time Facial Analysis System
- **Arxiv ID**: http://arxiv.org/abs/2109.10393v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10393v1)
- **Published**: 2021-09-21 18:27:15+00:00
- **Updated**: 2021-09-21 18:27:15+00:00
- **Authors**: Bishwo Adhikari, Xingyang Ni, Esa Rahtu, Heikki Huttunen
- **Comment**: Accepted in IEEE MMSP 2021
- **Journal**: None
- **Summary**: Facial analysis is an active research area in computer vision, with many practical applications. Most of the existing studies focus on addressing one specific task and maximizing its performance. For a complete facial analysis system, one needs to solve these tasks efficiently to ensure a smooth experience. In this work, we present a system-level design of a real-time facial analysis system. With a collection of deep neural networks for object detection, classification, and regression, the system recognizes age, gender, facial expression, and facial similarity for each person that appears in the camera view. We investigate the parallelization and interplay of individual tasks. Results on common off-the-shelf architecture show that the system's accuracy is comparable to the state-of-the-art methods, and the recognition speed satisfies real-time requirements. Moreover, we propose a multitask network for jointly predicting the first three attributes, i.e., age, gender, and facial expression. Source code and trained models are available at https://github.com/mahehu/TUT-live-age-estimator.



### Segmentation with mixed supervision: Confidence maximization helps knowledge distillation
- **Arxiv ID**: http://arxiv.org/abs/2109.10902v5
- **DOI**: 10.1016/j.media.2022.102670
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.10902v5)
- **Published**: 2021-09-21 20:06:13+00:00
- **Updated**: 2022-11-24 04:22:25+00:00
- **Authors**: Bingyuan Liu, Christian Desrosiers, Ismail Ben Ayed, Jose Dolz
- **Comment**: To be published at Medical Image Analysis (Volume 83, January 2023).
  Code: https://github.com/by-liu/ConfKD. Note: this article is a journal
  extension of our paper in IPMI 2021 arXiv:2012.08051
- **Journal**: None
- **Summary**: Despite achieving promising results in a breadth of medical image segmentation tasks, deep neural networks require large training datasets with pixel-wise annotations. Obtaining these curated datasets is a cumbersome process which limits the applicability in scenarios. Mixed supervision is an appealing alternative for mitigating this obstacle. In this work, we propose a dual-branch architecture, where the upper branch (teacher) receives strong annotations, while the bottom one (student) is driven by limited supervision and guided by the upper branch. Combined with a standard cross-entropy loss over the labeled pixels, our novel formulation integrates two important terms: (i) a Shannon entropy loss defined over the less-supervised images, which encourages confident student predictions in the bottom branch; and (ii) a KL divergence term, which transfers the knowledge (i.e., predictions) of the strongly supervised branch to the less-supervised branch and guides the entropy (student-confidence) term to avoid trivial solutions. We show that the synergy between the entropy and KL divergence yields substantial improvements in performance. We also discuss an interesting link between Shannon-entropy minimization and standard pseudo-mask generation, and argue that the former should be preferred over the latter for leveraging information from unlabeled pixels. We evaluate the effectiveness of the proposed formulation through a series of quantitative and qualitative experiments using two publicly available datasets. Results demonstrate that our method significantly outperforms other strategies for semantic segmentation within a mixed-supervision framework, as well as recent semi-supervised approaches. Our code is publicly available: https://github.com/by-liu/ConfKD.



