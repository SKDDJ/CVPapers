# Arxiv Papers in cs.CV on 2021-09-08
### Resolving gas bubbles ascending in liquid metal from low-SNR neutron radiography images
- **Arxiv ID**: http://arxiv.org/abs/2109.04883v2
- **DOI**: 10.3390/app11209710
- **Categories**: **physics.flu-dyn**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.04883v2)
- **Published**: 2021-09-08 00:35:29+00:00
- **Updated**: 2021-09-13 18:40:37+00:00
- **Authors**: Mihails Birjukovs, Pavel Trtik, Anders Kaestner, Jan Hovind, Martins Klevs, Dariusz Jakub Gawryluk, Knud Thomsen, Andris Jakovics
- **Comment**: None
- **Journal**: Applied Sciences 2021, 11(20), 9710
- **Summary**: We demonstrate a new image processing methodology for resolving gas bubbles travelling through liquid metal from dynamic neutron radiography images with intrinsically low signal-to-noise ratio. Image pre-processing, denoising and bubble segmentation are described in detail, with practical recommendations. Experimental validation is presented - stationary and moving reference bodies with neutron-transparent cavities are radiographed with imaging conditions similar to the cases with bubbles in liquid metal. The new methods are applied to our experimental data from previous and recent imaging campaigns, and the performance of the methods proposed in this paper is compared against our previously developed methods. Significant improvements are observed as well as the capacity to reliably extract physically meaningful information from measurements performed under highly adverse imaging conditions. The showcased image processing solution and separate elements thereof are readily extendable beyond the present application, and have been made open-source.



### RoadAtlas: Intelligent Platform for Automated Road Defect Detection and Asset Management
- **Arxiv ID**: http://arxiv.org/abs/2109.03385v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2109.03385v2)
- **Published**: 2021-09-08 01:26:15+00:00
- **Updated**: 2021-09-09 00:31:44+00:00
- **Authors**: Zhuoxiao Chen, Yiyun Zhang, Yadan Luo, Zijian Wang, Jinjiang Zhong, Anthony Southon
- **Comment**: Demonstration slides attached. To view attachments, please download
  the file listed under "Ancillary files"
- **Journal**: None
- **Summary**: With the rapid development of intelligent detection algorithms based on deep learning, much progress has been made in automatic road defect recognition and road marking parsing. This can effectively address the issue of an expensive and time-consuming process for professional inspectors to review the street manually. Towards this goal, we present RoadAtlas, a novel end-to-end integrated system that can support 1) road defect detection, 2) road marking parsing, 3) a web-based dashboard for presenting and inputting data by users, and 4) a backend containing a well-structured database and developed APIs.



### Learning to Discriminate Information for Online Action Detection: Analysis and Application
- **Arxiv ID**: http://arxiv.org/abs/2109.03393v3
- **DOI**: 10.1109/TPAMI.2022.3204808
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03393v3)
- **Published**: 2021-09-08 01:51:51+00:00
- **Updated**: 2022-11-18 05:37:41+00:00
- **Authors**: Sumin Lee, Hyunjun Eun, Jinyoung Moon, Seokeon Choi, Yoonhyung Kim, Chanho Jung, Changick Kim
- **Comment**: To appear in TPAMI. arXiv admin note: substantial text overlap with
  arXiv:1912.04461
- **Journal**: None
- **Summary**: Online action detection, which aims to identify an ongoing action from a streaming video, is an important subject in real-world applications. For this task, previous methods use recurrent neural networks for modeling temporal relations in an input sequence. However, these methods overlook the fact that the input image sequence includes not only the action of interest but background and irrelevant actions. This would induce recurrent units to accumulate unnecessary information for encoding features on the action of interest. To overcome this problem, we propose a novel recurrent unit, named Information Discrimination Unit (IDU), which explicitly discriminates the information relevancy between an ongoing action and others to decide whether to accumulate the input information. This enables learning more discriminative representations for identifying an ongoing action. In this paper, we further present a new recurrent unit, called Information Integration Unit (IIU), for action anticipation. Our IIU exploits the outputs from IDU as pseudo action labels as well as RGB frames to learn enriched features of observed actions effectively. In experiments on TVSeries and THUMOS-14, the proposed methods outperform state-of-the-art methods by a significant margin in online action detection and action anticipation. Moreover, we demonstrate the effectiveness of the proposed units by conducting comprehensive ablation studies.



### Master Face Attacks on Face Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/2109.03398v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03398v1)
- **Published**: 2021-09-08 02:11:35+00:00
- **Updated**: 2021-09-08 02:11:35+00:00
- **Authors**: Huy H. Nguyen, SÃ©bastien Marcel, Junichi Yamagishi, Isao Echizen
- **Comment**: This paper is an extension of the IJCB paper published in 2019
  (Generating Master Faces for Use in Performing Wolf Attacks on Face
  Recognition Systems) and its first version was initially submitted to T-BIOM
  journal on Dec 25, 2020
- **Journal**: None
- **Summary**: Face authentication is now widely used, especially on mobile devices, rather than authentication using a personal identification number or an unlock pattern, due to its convenience. It has thus become a tempting target for attackers using a presentation attack. Traditional presentation attacks use facial images or videos of the victim. Previous work has proven the existence of master faces, i.e., faces that match multiple enrolled templates in face recognition systems, and their existence extends the ability of presentation attacks. In this paper, we perform an extensive study on latent variable evolution (LVE), a method commonly used to generate master faces. We run an LVE algorithm for various scenarios and with more than one database and/or face recognition system to study the properties of the master faces and to understand in which conditions strong master faces could be generated. Moreover, through analysis, we hypothesize that master faces come from some dense areas in the embedding spaces of the face recognition systems. Last but not least, simulated presentation attacks using generated master faces generally preserve the false-matching ability of their original digital forms, thus demonstrating that the existence of master faces poses an actual threat.



### GTT-Net: Learned Generalized Trajectory Triangulation
- **Arxiv ID**: http://arxiv.org/abs/2109.03408v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03408v1)
- **Published**: 2021-09-08 03:01:25+00:00
- **Updated**: 2021-09-08 03:01:25+00:00
- **Authors**: Xiangyu Xu, Enrique Dunn
- **Comment**: None
- **Journal**: None
- **Summary**: We present GTT-Net, a supervised learning framework for the reconstruction of sparse dynamic 3D geometry. We build on a graph-theoretic formulation of the generalized trajectory triangulation problem, where non-concurrent multi-view imaging geometry is known but global image sequencing is not provided. GTT-Net learns pairwise affinities modeling the spatio-temporal relationships among our input observations and leverages them to determine 3D geometry estimates. Experiments reconstructing 3D motion-capture sequences show GTT-Net outperforms the state of the art in terms of accuracy and robustness. Within the context of articulated motion reconstruction, our proposed architecture is 1) able to learn and enforce semantic 3D motion priors for shared training and test domains, while being 2) able to generalize its performance across different training and test domains. Moreover, GTT-Net provides a computationally streamlined framework for trajectory triangulation with applications to multi-instance reconstruction and event segmentation.



### YouRefIt: Embodied Reference Understanding with Language and Gesture
- **Arxiv ID**: http://arxiv.org/abs/2109.03413v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03413v2)
- **Published**: 2021-09-08 03:27:32+00:00
- **Updated**: 2021-09-15 06:56:58+00:00
- **Authors**: Yixin Chen, Qing Li, Deqian Kong, Yik Lun Kei, Song-Chun Zhu, Tao Gao, Yixin Zhu, Siyuan Huang
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: We study the understanding of embodied reference: One agent uses both language and gesture to refer to an object to another agent in a shared physical environment. Of note, this new visual task requires understanding multimodal cues with perspective-taking to identify which object is being referred to. To tackle this problem, we introduce YouRefIt, a new crowd-sourced dataset of embodied reference collected in various physical scenes; the dataset contains 4,195 unique reference clips in 432 indoor scenes. To the best of our knowledge, this is the first embodied reference dataset that allows us to study referring expressions in daily physical scenes to understand referential behavior, human communication, and human-robot interaction. We further devise two benchmarks for image-based and video-based embodied reference understanding. Comprehensive baselines and extensive experiments provide the very first result of machine perception on how the referring expressions and gestures affect the embodied reference understanding. Our results provide essential evidence that gestural cues are as critical as language cues in understanding the embodied reference.



### RGB-D Salient Object Detection with Ubiquitous Target Awareness
- **Arxiv ID**: http://arxiv.org/abs/2109.03425v1
- **DOI**: 10.1109/TIP.2021.3108412
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03425v1)
- **Published**: 2021-09-08 04:27:29+00:00
- **Updated**: 2021-09-08 04:27:29+00:00
- **Authors**: Yifan Zhao, Jiawei Zhao, Jia Li, Xiaowu Chen
- **Comment**: 15 pages, 13 figures, Accepted by IEEE Transactions on Image
  Processing (2021). arXiv admin note: text overlap with arXiv:2006.00269
- **Journal**: None
- **Summary**: Conventional RGB-D salient object detection methods aim to leverage depth as complementary information to find the salient regions in both modalities. However, the salient object detection results heavily rely on the quality of captured depth data which sometimes are unavailable. In this work, we make the first attempt to solve the RGB-D salient object detection problem with a novel depth-awareness framework. This framework only relies on RGB data in the testing phase, utilizing captured depth data as supervision for representation learning. To construct our framework as well as achieving accurate salient detection results, we propose a Ubiquitous Target Awareness (UTA) network to solve three important challenges in RGB-D SOD task: 1) a depth awareness module to excavate depth information and to mine ambiguous regions via adaptive depth-error weights, 2) a spatial-aware cross-modal interaction and a channel-aware cross-level interaction, exploiting the low-level boundary cues and amplifying high-level salient channels, and 3) a gated multi-scale predictor module to perceive the object saliency in different contextual scales. Besides its high performance, our proposed UTA network is depth-free for inference and runs in real-time with 43 FPS. Experimental evidence demonstrates that our proposed network not only surpasses the state-of-the-art methods on five public RGB-D SOD benchmarks by a large margin, but also verifies its extensibility on five public RGB SOD benchmarks.



### Mask is All You Need: Rethinking Mask R-CNN for Dense and Arbitrary-Shaped Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.03426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03426v1)
- **Published**: 2021-09-08 04:32:29+00:00
- **Updated**: 2021-09-08 04:32:29+00:00
- **Authors**: Xugong Qin, Yu Zhou, Youhui Guo, Dayan Wu, Zhihong Tian, Ning Jiang, Hongbin Wang, Weiping Wang
- **Comment**: Accepted by ACM MM 2021
- **Journal**: None
- **Summary**: Due to the large success in object detection and instance segmentation, Mask R-CNN attracts great attention and is widely adopted as a strong baseline for arbitrary-shaped scene text detection and spotting. However, two issues remain to be settled. The first is dense text case, which is easy to be neglected but quite practical. There may exist multiple instances in one proposal, which makes it difficult for the mask head to distinguish different instances and degrades the performance. In this work, we argue that the performance degradation results from the learning confusion issue in the mask head. We propose to use an MLP decoder instead of the "deconv-conv" decoder in the mask head, which alleviates the issue and promotes robustness significantly. And we propose instance-aware mask learning in which the mask head learns to predict the shape of the whole instance rather than classify each pixel to text or non-text. With instance-aware mask learning, the mask branch can learn separated and compact masks. The second is that due to large variations in scale and aspect ratio, RPN needs complicated anchor settings, making it hard to maintain and transfer across different datasets. To settle this issue, we propose an adaptive label assignment in which all instances especially those with extreme aspect ratios are guaranteed to be associated with enough anchors. Equipped with these components, the proposed method named MAYOR achieves state-of-the-art performance on five benchmarks including DAST1500, MSRA-TD500, ICDAR2015, CTW1500, and Total-Text.



### SSEGEP: Small SEGment Emphasized Performance evaluation metric for medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.03435v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.03435v1)
- **Published**: 2021-09-08 05:05:49+00:00
- **Updated**: 2021-09-08 05:05:49+00:00
- **Authors**: Ammu R, Neelam Sinha
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic image segmentation is a critical component of medical image analysis, and hence quantifying segmentation performance is crucial. Challenges in medical image segmentation are mainly due to spatial variations of regions to be segmented and imbalance in distribution of classes. Commonly used metrics treat all detected pixels, indiscriminately. However, pixels in smaller segments must be treated differently from pixels in larger segments, as detection of smaller ones aid in early treatment of associated disease and are also easier to miss. To address this, we propose a novel evaluation metric for segmentation performance, emphasizing smaller segments, by assigning higher weightage to smaller segment pixels. Weighted false positives are also considered in deriving the new metric named, "SSEGEP"(Small SEGment Emphasized Performance evaluation metric), (range : 0(Bad) to 1(Good)). The experiments were performed on diverse anatomies(eye, liver, pancreas and breast) from publicly available datasets to show applicability of the proposed metric across different imaging techniques. Mean opinion score (MOS) and statistical significance testing is used to quantify the relevance of proposed approach. Across 33 fundus images, where the largest exudate is 1.41%, and the smallest is 0.0002% of the image, the proposed metric is 30% closer to MOS, as compared to Dice Similarity Coefficient (DSC). Statistical significance testing resulted in promising p-value of order 10^{-18} with SSEGEP for hepatic tumor compared to DSC. The proposed metric is found to perform better for the images having multiple segments for a single label.



### Unfolding Taylor's Approximations for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2109.03442v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03442v1)
- **Published**: 2021-09-08 05:47:22+00:00
- **Updated**: 2021-09-08 05:47:22+00:00
- **Authors**: Man Zhou, Zeyu Xiao, Xueyang Fu, Aiping Liu, Gang Yang, Zhiwei Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning provides a new avenue for image restoration, which demands a delicate balance between fine-grained details and high-level contextualized information during recovering the latent clear image. In practice, however, existing methods empirically construct encapsulated end-to-end mapping networks without deepening into the rationality, and neglect the intrinsic prior knowledge of restoration task. To solve the above problems, inspired by Taylor's Approximations, we unfold Taylor's Formula to construct a novel framework for image restoration. We find the main part and the derivative part of Taylor's Approximations take the same effect as the two competing goals of high-level contextualized information and spatial details of image restoration respectively. Specifically, our framework consists of two steps, correspondingly responsible for the mapping and derivative functions. The former first learns the high-level contextualized information and the later combines it with the degraded input to progressively recover local high-order spatial details. Our proposed framework is orthogonal to existing methods and thus can be easily integrated with them for further improvement, and extensive experiments demonstrate the effectiveness and scalability of our proposed framework.



### Toward Real-World Super-Resolution via Adaptive Downsampling Models
- **Arxiv ID**: http://arxiv.org/abs/2109.03444v1
- **DOI**: 10.1109/TPAMI.2021.3106790
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.03444v1)
- **Published**: 2021-09-08 06:00:32+00:00
- **Updated**: 2021-09-08 06:00:32+00:00
- **Authors**: Sanghyun Son, Jaeha Kim, Wei-Sheng Lai, Ming-Husan Yang, Kyoung Mu Lee
- **Comment**: Accepted at TPAMI
- **Journal**: None
- **Summary**: Most image super-resolution (SR) methods are developed on synthetic low-resolution (LR) and high-resolution (HR) image pairs that are constructed by a predetermined operation, e.g., bicubic downsampling. As existing methods typically learn an inverse mapping of the specific function, they produce blurry results when applied to real-world images whose exact formulation is different and unknown. Therefore, several methods attempt to synthesize much more diverse LR samples or learn a realistic downsampling model. However, due to restrictive assumptions on the downsampling process, they are still biased and less generalizable. This study proposes a novel method to simulate an unknown downsampling process without imposing restrictive prior knowledge. We propose a generalizable low-frequency loss (LFL) in the adversarial training framework to imitate the distribution of target LR images without using any paired examples. Furthermore, we design an adaptive data loss (ADL) for the downsampler, which can be adaptively learned and updated from the data during the training loops. Extensive experiments validate that our downsampling model can facilitate existing SR methods to perform more accurate reconstructions on various synthetic and real-world examples than the conventional approaches.



### Which and Where to Focus: A Simple yet Accurate Framework for Arbitrary-Shaped Nearby Text Detection in Scene Images
- **Arxiv ID**: http://arxiv.org/abs/2109.03451v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03451v1)
- **Published**: 2021-09-08 06:25:37+00:00
- **Updated**: 2021-09-08 06:25:37+00:00
- **Authors**: Youhui Guo, Yu Zhou, Xugong Qin, Weiping Wang
- **Comment**: Accepted by ICANN 2021
- **Journal**: None
- **Summary**: Scene text detection has drawn the close attention of researchers. Though many methods have been proposed for horizontal and oriented texts, previous methods may not perform well when dealing with arbitrary-shaped texts such as curved texts. In particular, confusion problem arises in the case of nearby text instances. In this paper, we propose a simple yet effective method for accurate arbitrary-shaped nearby scene text detection. Firstly, a One-to-Many Training Scheme (OMTS) is designed to eliminate confusion and enable the proposals to learn more appropriate groundtruths in the case of nearby text instances. Secondly, we propose a Proposal Feature Attention Module (PFAM) to exploit more effective features for each proposal, which can better adapt to arbitrary-shaped text instances. Finally, we propose a baseline that is based on Faster R-CNN and outputs the curve representation directly. Equipped with PFAM and OMTS, the detector can achieve state-of-the-art or competitive performance on several challenging benchmarks.



### Recalibrating the KITTI Dataset Camera Setup for Improved Odometry Accuracy
- **Arxiv ID**: http://arxiv.org/abs/2109.03462v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.03462v1)
- **Published**: 2021-09-08 07:18:59+00:00
- **Updated**: 2021-09-08 07:18:59+00:00
- **Authors**: Igor CviÅ¡iÄ, Ivan MarkoviÄ, Ivan PetroviÄ
- **Comment**: None
- **Journal**: European Conference on Mobile Robots (ECMR) 2021
- **Summary**: Over the last decade, one of the most relevant public datasets for evaluating odometry accuracy is the KITTI dataset. Beside the quality and rich sensor setup, its success is also due to the online evaluation tool, which enables researchers to benchmark and compare algorithms. The results are evaluated on the test subset solely, without any knowledge about the ground truth, yielding unbiased, overfit free and therefore relevant validation for robot localization based on cameras, 3D laser or combination of both. However, as any sensor setup, it requires prior calibration and rectified stereo images are provided, introducing dependence on the default calibration parameters. Given that, a natural question arises if a better set of calibration parameters can be found that would yield higher odometry accuracy. In this paper, we propose a new approach for one shot calibration of the KITTI dataset multiple camera setup. The approach yields better calibration parameters, both in the sense of lower calibration reprojection errors and lower visual odometry error. We conducted experiments where we show for three different odometry algorithms, namely SOFT2, ORB-SLAM2 and VISO2, that odometry accuracy is significantly improved with the proposed calibration parameters. Moreover, our odometry, SOFT2, in conjunction with the proposed calibration method achieved the highest accuracy on the official KITTI scoreboard with 0.53% translational and 0.0009 deg/m rotational error, outperforming even 3D laser-based methods.



### Level Set Binocular Stereo with Occlusions
- **Arxiv ID**: http://arxiv.org/abs/2109.03464v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.03464v1)
- **Published**: 2021-09-08 07:22:25+00:00
- **Updated**: 2021-09-08 07:22:25+00:00
- **Authors**: Jialiang Wang, Todd Zickler
- **Comment**: extended journal version of arXiv:2006.16094
- **Journal**: None
- **Summary**: Localizing stereo boundaries and predicting nearby disparities are difficult because stereo boundaries induce occluded regions where matching cues are absent. Most modern computer vision algorithms treat occlusions secondarily (e.g., via left-right consistency checks after matching) or rely on high-level cues to improve nearby disparities (e.g., via deep networks and large training sets). They ignore the geometry of stereo occlusions, which dictates that the spatial extent of occlusion must equal the amplitude of the disparity jump that causes it. This paper introduces an energy and level-set optimizer that improves boundaries by encoding occlusion geometry. Our model applies to two-layer, figure-ground scenes, and it can be implemented cooperatively using messages that pass predominantly between parents and children in an undecimated hierarchy of multi-scale image patches. In a small collection of figure-ground scenes curated from Middlebury and Falling Things stereo datasets, our model provides more accurate boundaries than previous occlusion-handling stereo techniques. This suggests new directions for creating cooperative stereo systems that incorporate occlusion cues in a human-like manner.



### Cross-Site Severity Assessment of COVID-19 from CT Images via Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2109.03478v1
- **DOI**: 10.1109/TMI.2021.3104474
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.03478v1)
- **Published**: 2021-09-08 07:56:51+00:00
- **Updated**: 2021-09-08 07:56:51+00:00
- **Authors**: Geng-Xin Xu, Chen Liu, Jun Liu, Zhongxiang Ding, Feng Shi, Man Guo, Wei Zhao, Xiaoming Li, Ying Wei, Yaozong Gao, Chuan-Xian Ren, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Early and accurate severity assessment of Coronavirus disease 2019 (COVID-19) based on computed tomography (CT) images offers a great help to the estimation of intensive care unit event and the clinical decision of treatment planning. To augment the labeled data and improve the generalization ability of the classification model, it is necessary to aggregate data from multiple sites. This task faces several challenges including class imbalance between mild and severe infections, domain distribution discrepancy between sites, and presence of heterogeneous features. In this paper, we propose a novel domain adaptation (DA) method with two components to address these problems. The first component is a stochastic class-balanced boosting sampling strategy that overcomes the imbalanced learning problem and improves the classification performance on poorly-predicted classes. The second component is a representation learning that guarantees three properties: 1) domain-transferability by prototype triplet loss, 2) discriminant by conditional maximum mean discrepancy loss, and 3) completeness by multi-view reconstruction loss. Particularly, we propose a domain translator and align the heterogeneous data to the estimated class prototypes (i.e., class centers) in a hyper-sphere manifold. Experiments on cross-site severity assessment of COVID-19 from CT images show that the proposed method can effectively tackle the imbalanced learning problem and outperform recent DA approaches.



### Energy-Efficient Mobile Robot Control via Run-time Monitoring of Environmental Complexity and Computing Workload
- **Arxiv ID**: http://arxiv.org/abs/2109.04285v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.04285v1)
- **Published**: 2021-09-08 08:11:44+00:00
- **Updated**: 2021-09-08 08:11:44+00:00
- **Authors**: Sherif A. S. Mohamed, Mohammad-Hashem Haghbayan, Antonio Miele, Onur Mutlu, Juha Plosila
- **Comment**: Accepted to be published on 2021 International Conference on
  Intelligent Robots and Systems (IROS)
- **Journal**: None
- **Summary**: We propose an energy-efficient controller to minimize the energy consumption of a mobile robot by dynamically manipulating the mechanical and computational actuators of the robot. The mobile robot performs real-time vision-based applications based on an event-based camera. The actuators of the controller are CPU voltage/frequency for the computation part and motor voltage for the mechanical part. We show that independently considering speed control of the robot and voltage/frequency control of the CPU does not necessarily result in an energy-efficient solution. In fact, to obtain the highest efficiency, the computation and mechanical parts should be controlled together in synergy. We propose a fast hill-climbing optimization algorithm to allow the controller to find the best CPU/motor configuration at run-time and whenever the mobile robot is facing a new environment during its travel. Experimental results on a robot with Brushless DC Motors, Jetson TX2 board as the computing unit, and a DAVIS-346 event-based camera show that the proposed control algorithm can save battery energy by an average of 50.5%, 41%, and 30%, in low-complexity, medium-complexity, and high-complexity environments, over baselines.



### Pose-guided Inter- and Intra-part Relational Transformer for Occluded Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2109.03483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03483v1)
- **Published**: 2021-09-08 08:12:28+00:00
- **Updated**: 2021-09-08 08:12:28+00:00
- **Authors**: Zhongxing Ma, Yifan Zhao, Jia Li
- **Comment**: 10 pages, 6 figures, Accepted by ACM MM 2021
- **Journal**: None
- **Summary**: Person Re-Identification (Re-Id) in occlusion scenarios is a challenging problem because a pedestrian can be partially occluded. The use of local information for feature extraction and matching is still necessary. Therefore, we propose a Pose-guided inter-and intra-part relational transformer (Pirt) for occluded person Re-Id, which builds part-aware long-term correlations by introducing transformers. In our framework, we firstly develop a pose-guided feature extraction module with regional grouping and mask construction for robust feature representations. The positions of a pedestrian in the image under surveillance scenarios are relatively fixed, hence we propose an intra-part and inter-part relational transformer. The intra-part module creates local relations with mask-guided features, while the inter-part relationship builds correlations with transformers, to develop cross relationships between part nodes. With the collaborative learning inter- and intra-part relationships, experiments reveal that our proposed Pirt model achieves a new state of the art on the public occluded dataset, and further extensions on standard non-occluded person Re-Id datasets also reveal our comparable performances.



### Shuffled Patch-Wise Supervision for Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.03484v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.03484v2)
- **Published**: 2021-09-08 08:14:13+00:00
- **Updated**: 2021-09-09 12:55:06+00:00
- **Authors**: Alperen KantarcÄ±, Hasan Dertli, HazÄ±m Kemal Ekenel
- **Comment**: Accepted to 20th International Conference of the Biometrics Special
  Interest Group (BIOSIG 2021) as Oral paper
- **Journal**: None
- **Summary**: Face anti-spoofing is essential to prevent false facial verification by using a photo, video, mask, or a different substitute for an authorized person's face. Most of the state-of-the-art presentation attack detection (PAD) systems suffer from overfitting, where they achieve near-perfect scores on a single dataset but fail on a different dataset with more realistic data. This problem drives researchers to develop models that perform well under real-world conditions. This is an especially challenging problem for frame-based presentation attack detection systems that use convolutional neural networks (CNN). To this end, we propose a new PAD approach, which combines pixel-wise binary supervision with patch-based CNN. We believe that training a CNN with face patches allows the model to distinguish spoofs without learning background or dataset-specific traces. We tested the proposed method both on the standard benchmark datasets -- Replay-Mobile, OULU-NPU -- and on a real-world dataset. The proposed approach shows its superiority on challenging experimental setups. Namely, it achieves higher performance on OULU-NPU protocol 3, 4 and on inter-dataset real-world experiments.



### FaceCook: Face Generation Based on Linear Scaling Factors
- **Arxiv ID**: http://arxiv.org/abs/2109.03492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03492v1)
- **Published**: 2021-09-08 08:31:40+00:00
- **Updated**: 2021-09-08 08:31:40+00:00
- **Authors**: Tianren Wang, Can Peng, Teng Zhang, Brian Lovell
- **Comment**: None
- **Journal**: None
- **Summary**: With the excellent disentanglement properties of state-of-the-art generative models, image editing has been the dominant approach to control the attributes of synthesised face images. However, these edited results often suffer from artifacts or incorrect feature rendering, especially when there is a large discrepancy between the image to be edited and the desired feature set. Therefore, we propose a new approach to mapping the latent vectors of the generative model to the scaling factors through solving a set of multivariate linear equations. The coefficients of the equations are the eigenvectors of the weight parameters of the pre-trained model, which form the basis of a hyper coordinate system. The qualitative and quantitative results both show that the proposed method outperforms the baseline in terms of image diversity. In addition, the method is much more time-efficient because you can obtain synthesised images with desirable features directly from the latent vectors, rather than the former process of editing randomly generated images requiring many processing steps.



### Temporal RoI Align for Video Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.03495v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03495v2)
- **Published**: 2021-09-08 08:35:21+00:00
- **Updated**: 2021-09-11 01:30:58+00:00
- **Authors**: Tao Gong, Kai Chen, Xinjiang Wang, Qi Chu, Feng Zhu, Dahua Lin, Nenghai Yu, Huamin Feng
- **Comment**: Accpeted by AAAI 2021
- **Journal**: None
- **Summary**: Video object detection is challenging in the presence of appearance deterioration in certain video frames. Therefore, it is a natural choice to aggregate temporal information from other frames of the same video into the current frame. However, RoI Align, as one of the most core procedures of video detectors, still remains extracting features from a single-frame feature map for proposals, making the extracted RoI features lack temporal information from videos. In this work, considering the features of the same object instance are highly similar among frames in a video, a novel Temporal RoI Align operator is proposed to extract features from other frames feature maps for current frame proposals by utilizing feature similarity. The proposed Temporal RoI Align operator can extract temporal information from the entire video for proposals. We integrate it into single-frame video detectors and other state-of-the-art video detectors, and conduct quantitative experiments to demonstrate that the proposed Temporal RoI Align operator can consistently and significantly boost the performance. Besides, the proposed Temporal RoI Align can also be applied into video instance segmentation. Codes are available at https://github.com/open-mmlab/mmtracking



### Elastic Significant Bit Quantization and Acceleration for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.03513v2
- **DOI**: 10.1109/TPDS.2021.3129615
- **Categories**: **cs.CV**, eess.SP, B.2.4.a; I.2.6.g; I.5.1.d; I.5.4.b
- **Links**: [PDF](http://arxiv.org/pdf/2109.03513v2)
- **Published**: 2021-09-08 09:13:59+00:00
- **Updated**: 2021-11-17 07:45:42+00:00
- **Authors**: Cheng Gong, Ye Lu, Kunpeng Xie, Zongming Jin, Tao Li, Yanzhi Wang
- **Comment**: 15 pages, 14 figures
- **Journal**: IEEE Transactions on Parallel and Distributed Systems, 2021
- **Summary**: Quantization has been proven to be a vital method for improving the inference efficiency of deep neural networks (DNNs). However, it is still challenging to strike a good balance between accuracy and efficiency while quantizing DNN weights or activation values from high-precision formats to their quantized counterparts. We propose a new method called elastic significant bit quantization (ESB) that controls the number of significant bits of quantized values to obtain better inference accuracy with fewer resources. We design a unified mathematical formula to constrain the quantized values of the ESB with a flexible number of significant bits. We also introduce a distribution difference aligner (DDA) to quantitatively align the distributions between the full-precision weight or activation values and quantized values. Consequently, ESB is suitable for various bell-shaped distributions of weights and activation of DNNs, thus maintaining a high inference accuracy. Benefitting from fewer significant bits of quantized values, ESB can reduce the multiplication complexity. We implement ESB as an accelerator and quantitatively evaluate its efficiency on FPGAs. Extensive experimental results illustrate that ESB quantization consistently outperforms state-of-the-art methods and achieves average accuracy improvements of 4.78%, 1.92%, and 3.56% over AlexNet, ResNet18, and MobileNetV2, respectively. Furthermore, ESB as an accelerator can achieve 10.95 GOPS peak performance of 1k LUTs without DSPs on the Xilinx ZCU102 FPGA platform. Compared with CPU, GPU, and state-of-the-art accelerators on FPGAs, the ESB accelerator can improve the energy efficiency by up to 65x, 11x, and 26x, respectively.



### Time Alignment using Lip Images for Frame-based Electrolaryngeal Voice Conversion
- **Arxiv ID**: http://arxiv.org/abs/2109.03551v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CL, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2109.03551v1)
- **Published**: 2021-09-08 11:24:09+00:00
- **Updated**: 2021-09-08 11:24:09+00:00
- **Authors**: Yi-Syuan Liou, Wen-Chin Huang, Ming-Chi Yen, Shu-Wei Tsai, Yu-Huai Peng, Tomoki Toda, Yu Tsao, Hsin-Min Wang
- **Comment**: Accepted to APSIPA ASC 2021
- **Journal**: None
- **Summary**: Voice conversion (VC) is an effective approach to electrolaryngeal (EL) speech enhancement, a task that aims to improve the quality of the artificial voice from an electrolarynx device. In frame-based VC methods, time alignment needs to be performed prior to model training, and the dynamic time warping (DTW) algorithm is widely adopted to compute the best time alignment between each utterance pair. The validity is based on the assumption that the same phonemes of the speakers have similar features and can be mapped by measuring a pre-defined distance between speech frames of the source and the target. However, the special characteristics of the EL speech can break the assumption, resulting in a sub-optimal DTW alignment. In this work, we propose to use lip images for time alignment, as we assume that the lip movements of laryngectomee remain normal compared to healthy people. We investigate two naive lip representations and distance metrics, and experimental results demonstrate that the proposed method can significantly outperform the audio-only alignment in terms of objective and subjective evaluations.



### LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2109.03569v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2109.03569v2)
- **Published**: 2021-09-08 12:06:31+00:00
- **Updated**: 2022-11-25 13:12:08+00:00
- **Authors**: Florent Bartoccioni, Ãloi Zablocki, Patrick PÃ©rez, Matthieu Cord, Karteek Alahari
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-based depth estimation is a key feature in autonomous systems, which often relies on a single camera or several independent ones. In such a monocular setup, dense depth is obtained with either additional input from one or several expensive LiDARs, e.g., with 64 beams, or camera-only methods, which suffer from scale-ambiguity and infinite-depth problems. In this paper, we propose a new alternative of densely estimating metric depth by combining a monocular camera with a light-weight LiDAR, e.g., with 4 beams, typical of today's automotive-grade mass-produced laser scanners. Inspired by recent self-supervised methods, we introduce a novel framework, called LiDARTouch, to estimate dense depth maps from monocular images with the help of ``touches'' of LiDAR, i.e., without the need for dense ground-truth depth. In our setup, the minimal LiDAR input contributes on three different levels: as an additional model's input, in a self-supervised LiDAR reconstruction objective function, and to estimate changes of pose (a key component of self-supervised depth estimation architectures). Our LiDARTouch framework achieves new state of the art in self-supervised depth estimation on the KITTI dataset, thus supporting our choices of integrating the very sparse LiDAR signal with other visual features. Moreover, we show that the use of a few-beam LiDAR alleviates scale ambiguity and infinite-depth issues that camera-only methods suffer from. We also demonstrate that methods from the fully-supervised depth-completion literature can be adapted to a self-supervised regime with a minimal LiDAR signal.



### Deriving Explanation of Deep Visual Saliency Models
- **Arxiv ID**: http://arxiv.org/abs/2109.03575v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.03575v1)
- **Published**: 2021-09-08 12:22:32+00:00
- **Updated**: 2021-09-08 12:22:32+00:00
- **Authors**: Sai Phani Kumar Malladi, Jayanta Mukhopadhyay, Chaker Larabi, Santanu Chaudhury
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have shown their profound impact on achieving human level performance in visual saliency prediction. However, it is still unclear how they learn the task and what it means in terms of understanding human visual system. In this work, we develop a technique to derive explainable saliency models from their corresponding deep neural architecture based saliency models by applying human perception theories and the conventional concepts of saliency. This technique helps us understand the learning pattern of the deep network at its intermediate layers through their activation maps. Initially, we consider two state-of-the-art deep saliency models, namely UNISAL and MSI-Net for our interpretation. We use a set of biologically plausible log-gabor filters for identifying and reconstructing the activation maps of them using our explainable saliency model. The final saliency map is generated using these reconstructed activation maps. We also build our own deep saliency model named cross-concatenated multi-scale residual block based network (CMRNet) for saliency prediction. Then, we evaluate and compare the performance of the explainable models derived from UNISAL, MSI-Net and CMRNet on three benchmark datasets with other state-of-the-art methods. Hence, we propose that this approach of explainability can be applied to any deep visual saliency model for interpretation which makes it a generic one.



### Matching in the Dark: A Dataset for Matching Image Pairs of Low-light Scenes
- **Arxiv ID**: http://arxiv.org/abs/2109.03585v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T40, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2109.03585v2)
- **Published**: 2021-09-08 12:30:04+00:00
- **Updated**: 2021-09-14 06:30:53+00:00
- **Authors**: Wenzheng Song, Masanori Suganuma, Xing Liu, Noriyuki Shimobayashi, Daisuke Maruta, Takayuki Okatani
- **Comment**: 15 pages, 14 figures, ICCV2021
- **Journal**: None
- **Summary**: This paper considers matching images of low-light scenes, aiming to widen the frontier of SfM and visual SLAM applications. Recent image sensors can record the brightness of scenes with more than eight-bit precision, available in their RAW-format image. We are interested in making full use of such high-precision information to match extremely low-light scene images that conventional methods cannot handle. For extreme low-light scenes, even if some of their brightness information exists in the RAW format images' low bits, the standard raw image processing on cameras fails to utilize them properly. As was recently shown by Chen et al., CNNs can learn to produce images with a natural appearance from such RAW-format images. To consider if and how well we can utilize such information stored in RAW-format images for image matching, we have created a new dataset named MID (matching in the dark). Using it, we experimentally evaluated combinations of eight image-enhancing methods and eleven image matching methods consisting of classical/neural local descriptors and classical/neural initial point-matching methods. The results show the advantage of using the RAW-format images and the strengths and weaknesses of the above component methods. They also imply there is room for further research.



### Identification of Social-Media Platform of Videos through the Use of Shared Features
- **Arxiv ID**: http://arxiv.org/abs/2109.03598v1
- **DOI**: 10.3390/jimaging7080140
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03598v1)
- **Published**: 2021-09-08 12:49:40+00:00
- **Updated**: 2021-09-08 12:49:40+00:00
- **Authors**: Luca Maiano, Irene Amerini, Lorenzo Ricciardi Celsi, Aris Anagnostopoulos
- **Comment**: None
- **Journal**: Journal of Imaging 2021
- **Summary**: Videos have become a powerful tool for spreading illegal content such as military propaganda, revenge porn, or bullying through social networks. To counter these illegal activities, it has become essential to try new methods to verify the origin of videos from these platforms. However, collecting datasets large enough to train neural networks for this task has become difficult because of the privacy regulations that have been enacted in recent years. To mitigate this limitation, in this work we propose two different solutions based on transfer learning and multitask learning to determine whether a video has been uploaded from or downloaded to a specific social platform through the use of shared features with images trained on the same task. By transferring features from the shallowest to the deepest levels of the network from the image task to videos, we measure the amount of information shared between these two tasks. Then, we introduce a model based on multitask learning, which learns from both tasks simultaneously. The promising experimental results show, in particular, the effectiveness of the multitask approach. According to our knowledge, this is the first work that addresses the problem of social media platform identification of videos through the use of shared features.



### Tactile Image-to-Image Disentanglement of Contact Geometry from Motion-Induced Shear
- **Arxiv ID**: http://arxiv.org/abs/2109.03615v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.03615v1)
- **Published**: 2021-09-08 13:03:08+00:00
- **Updated**: 2021-09-08 13:03:08+00:00
- **Authors**: Anupam K. Gupta, Laurence Aitchison, Nathan F. Lepora
- **Comment**: 15 pages, 6 figure, under review CORL 2021
- **Journal**: None
- **Summary**: Robotic touch, particularly when using soft optical tactile sensors, suffers from distortion caused by motion-dependent shear. The manner in which the sensor contacts a stimulus is entangled with the tactile information about the geometry of the stimulus. In this work, we propose a supervised convolutional deep neural network model that learns to disentangle, in the latent space, the components of sensor deformations caused by contact geometry from those due to sliding-induced shear. The approach is validated by reconstructing unsheared tactile images from sheared images and showing they match unsheared tactile images collected with no sliding motion. In addition, the unsheared tactile images give a faithful reconstruction of the contact geometry that is not possible from the sheared data, and robust estimation of the contact pose that can be used for servo control sliding around various 2D shapes. Finally, the contact geometry reconstruction in conjunction with servo control sliding were used for faithful full object reconstruction of various 2D shapes. The methods have broad applicability to deep learning models for robots with a shear-sensitive sense of touch.



### Learning Local-Global Contextual Adaptation for Multi-Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.03622v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03622v2)
- **Published**: 2021-09-08 13:15:01+00:00
- **Updated**: 2022-03-02 07:42:34+00:00
- **Authors**: Nan Xue, Tianfu Wu, Gui-Song Xia, Liangpei Zhang
- **Comment**: To appear in CVPR 2022
- **Journal**: None
- **Summary**: This paper studies the problem of multi-person pose estimation in a bottom-up fashion. With a new and strong observation that the localization issue of the center-offset formulation can be remedied in a local-window search scheme in an ideal situation, we propose a multi-person pose estimation approach, dubbed as LOGO-CAP, by learning the LOcal-GlObal Contextual Adaptation for human Pose. Specifically, our approach learns the keypoint attraction maps (KAMs) from the local keypoints expansion maps (KEMs) in small local windows in the first step, which are subsequently treated as dynamic convolutional kernels on the keypoints-focused global heatmaps for contextual adaptation, achieving accurate multi-person pose estimation. Our method is end-to-end trainable with near real-time inference speed in a single forward pass, obtaining state-of-the-art performance on the COCO keypoint benchmark for bottom-up human pose estimation. With the COCO trained model, our method also outperforms prior arts by a large margin on the challenging OCHuman dataset.



### On Recognizing Occluded Faces in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2109.03672v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03672v2)
- **Published**: 2021-09-08 14:20:10+00:00
- **Updated**: 2021-09-11 21:06:01+00:00
- **Authors**: Mustafa Ekrem ErakÄ±n, UÄur Demir, HazÄ±m Kemal Ekenel
- **Comment**: Accepted to 20th International Conference of the Biometrics Special
  Interest Group (BIOSIG 2021) as Poster paper
- **Journal**: None
- **Summary**: Facial appearance variations due to occlusion has been one of the main challenges for face recognition systems. To facilitate further research in this area, it is necessary and important to have occluded face datasets collected from real-world, as synthetically generated occluded faces cannot represent the nature of the problem. In this paper, we present the Real World Occluded Faces (ROF) dataset, that contains faces with both upper face occlusion, due to sunglasses, and lower face occlusion, due to masks. We propose two evaluation protocols for this dataset. Benchmark experiments on the dataset have shown that no matter how powerful the deep face representation models are, their performance degrades significantly when they are tested on real-world occluded faces. It is observed that the performance drop is far less when the models are tested on synthetically generated occluded faces. The ROF dataset and the associated evaluation protocols are publicly available at the following link https://github.com/ekremerakin/RealWorldOccludedFaces.



### Unsupervised clothing change adaptive person ReID
- **Arxiv ID**: http://arxiv.org/abs/2109.03702v2
- **DOI**: 10.1109/LSP.2021.3134195
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03702v2)
- **Published**: 2021-09-08 15:08:10+00:00
- **Updated**: 2021-09-14 14:42:00+00:00
- **Authors**: Ziyue Zhang, Shuai Jiang, Congzhentao Huang, Richard YiDa Xu
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Clothing changes and lack of data labels are both crucial challenges in person ReID. For the former challenge, people may occur multiple times at different locations wearing different clothing. However, most of the current person ReID research works focus on the benchmarks in which a person's clothing is kept the same all the time. For the last challenge, some researchers try to make model learn information from a labeled dataset as a source to an unlabeled dataset. Whereas purely unsupervised training is less used. In this paper, we aim to solve both problems at the same time. We design a novel unsupervised model, Sync-Person-Cloud ReID, to solve the unsupervised clothing change person ReID problem. We developer a purely unsupervised clothing change person ReID pipeline with person sync augmentation operation and same person feature restriction. The person sync augmentation is to supply additional same person resources. These same person's resources can be used as part supervised input by same person feature restriction. The extensive experiments on clothing change ReID datasets show the out-performance of our methods.



### Disentangling Alzheimer's disease neurodegeneration from typical brain aging using machine learning
- **Arxiv ID**: http://arxiv.org/abs/2109.03723v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.med-ph, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2109.03723v1)
- **Published**: 2021-09-08 15:33:29+00:00
- **Updated**: 2021-09-08 15:33:29+00:00
- **Authors**: Gyujoon Hwang, Ahmed Abdulkadir, Guray Erus, Mohamad Habes, Raymond Pomponio, Haochang Shou, Jimit Doshi, Elizabeth Mamourian, Tanweer Rashid, Murat Bilgel, Yong Fan, Aristeidis Sotiras, Dhivya Srinivasan, John C. Morris, Daniel Marcus, Marilyn S. Albert, Nick R. Bryan, Susan M. Resnick, Ilya M. Nasrallah, Christos Davatzikos, David A. Wolk
- **Comment**: 4 figures, 3 tables
- **Journal**: None
- **Summary**: Neuroimaging biomarkers that distinguish between typical brain aging and Alzheimer's disease (AD) are valuable for determining how much each contributes to cognitive decline. Machine learning models can derive multi-variate brain change patterns related to the two processes, including the SPARE-AD (Spatial Patterns of Atrophy for Recognition of Alzheimer's Disease) and SPARE-BA (of Brain Aging) investigated herein. However, substantial overlap between brain regions affected in the two processes confounds measuring them independently. We present a methodology toward disentangling the two. T1-weighted MRI images of 4,054 participants (48-95 years) with AD, mild cognitive impairment (MCI), or cognitively normal (CN) diagnoses from the iSTAGING (Imaging-based coordinate SysTem for AGIng and NeurodeGenerative diseases) consortium were analyzed. First, a subset of AD patients and CN adults were selected based purely on clinical diagnoses to train SPARE-BA1 (regression of age using CN individuals) and SPARE-AD1 (classification of CN versus AD). Second, analogous groups were selected based on clinical and molecular markers to train SPARE-BA2 and SPARE-AD2: amyloid-positive (A+) AD continuum group (consisting of A+AD, A+MCI, and A+ and tau-positive CN individuals) and amyloid-negative (A-) CN group. Finally, the combined group of the AD continuum and A-/CN individuals was used to train SPARE-BA3, with the intention to estimate brain age regardless of AD-related brain changes. Disentangled SPARE models derived brain patterns that were more specific to the two types of the brain changes. Correlation between the SPARE-BA and SPARE-AD was significantly reduced. Correlation of disentangled SPARE-AD was non-inferior to the molecular measurements and to the number of APOE4 alleles, but was less to AD-related psychometric test scores, suggesting contribution of advanced brain aging to these scores.



### Axial multi-layer perceptron architecture for automatic segmentation of choroid plexus in multiple sclerosis
- **Arxiv ID**: http://arxiv.org/abs/2109.03778v2
- **DOI**: 10.1117/12.2612912
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.NC, q-bio.QM, I.2
- **Links**: [PDF](http://arxiv.org/pdf/2109.03778v2)
- **Published**: 2021-09-08 16:55:23+00:00
- **Updated**: 2021-11-21 18:58:12+00:00
- **Authors**: Marius Schmidt-Mengin, Vito A. G. Ricigliano, Benedetta Bodini, Emanuele Morena, Annalisa Colombi, Mariem Hamzaoui, Arya Yazdan Panah, Bruno Stankoff, Olivier Colliot
- **Comment**: None
- **Journal**: Proc. SPIE Medical Imaging 2022
- **Summary**: Choroid plexuses (CP) are structures of the ventricles of the brain which produce most of the cerebrospinal fluid (CSF). Several postmortem and in vivo studies have pointed towards their role in the inflammatory process in multiple sclerosis (MS). Automatic segmentation of CP from MRI thus has high value for studying their characteristics in large cohorts of patients. To the best of our knowledge, the only freely available tool for CP segmentation is FreeSurfer but its accuracy for this specific structure is poor. In this paper, we propose to automatically segment CP from non-contrast enhanced T1-weighted MRI. To that end, we introduce a new model called "Axial-MLP" based on an assembly of Axial multi-layer perceptrons (MLPs). This is inspired by recent works which showed that the self-attention layers of Transformers can be replaced with MLPs. This approach is systematically compared with a standard 3D U-Net, nnU-Net, Freesurfer and FastSurfer. For our experiments, we make use of a dataset of 141 subjects (44 controls and 97 patients with MS). We show that all the tested deep learning (DL) methods outperform FreeSurfer (Dice around 0.7 for DL vs 0.33 for FreeSurfer). Axial-MLP is competitive with U-Nets even though it is slightly less accurate. The conclusions of our paper are two-fold: 1) the studied deep learning methods could be useful tools to study CP in large cohorts of MS patients; 2)~Axial-MLP is a potentially viable alternative to convolutional neural networks for such tasks, although it could benefit from further improvements.



### Egocentric View Hand Action Recognition by Leveraging Hand Surface and Hand Grasp Type
- **Arxiv ID**: http://arxiv.org/abs/2109.03783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03783v1)
- **Published**: 2021-09-08 17:12:02+00:00
- **Updated**: 2021-09-08 17:12:02+00:00
- **Authors**: Sangpil Kim, Jihyun Bae, Hyunggun Chi, Sunghee Hong, Byoung Soo Koh, Karthik Ramani
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a multi-stage framework that uses mean curvature on a hand surface and focuses on learning interaction between hand and object by analyzing hand grasp type for hand action recognition in egocentric videos. The proposed method does not require 3D information of objects including 6D object poses which are difficult to annotate for learning an object's behavior while it interacts with hands. Instead, the framework synthesizes the mean curvature of the hand mesh model to encode the hand surface geometry in 3D space. Additionally, our method learns the hand grasp type which is highly correlated with the hand action. From our experiment, we notice that using hand grasp type and mean curvature of hand increases the performance of the hand action recognition.



### FIDNet: LiDAR Point Cloud Semantic Segmentation with Fully Interpolation Decoding
- **Arxiv ID**: http://arxiv.org/abs/2109.03787v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.03787v1)
- **Published**: 2021-09-08 17:20:09+00:00
- **Updated**: 2021-09-08 17:20:09+00:00
- **Authors**: Yiming Zhao, Lin Bai, Xinming Huang
- **Comment**: Accepted by IROS'21, code link:
  https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI
- **Journal**: None
- **Summary**: Projecting the point cloud on the 2D spherical range image transforms the LiDAR semantic segmentation to a 2D segmentation task on the range image. However, the LiDAR range image is still naturally different from the regular 2D RGB image; for example, each position on the range image encodes the unique geometry information. In this paper, we propose a new projection-based LiDAR semantic segmentation pipeline that consists of a novel network structure and an efficient post-processing step. In our network structure, we design a FID (fully interpolation decoding) module that directly upsamples the multi-resolution feature maps using bilinear interpolation. Inspired by the 3D distance interpolation used in PointNet++, we argue this FID module is a 2D version distance interpolation on $(\theta, \phi)$ space. As a parameter-free decoding module, the FID largely reduces the model complexity by maintaining good performance. Besides the network structure, we empirically find that our model predictions have clear boundaries between different semantic classes. This makes us rethink whether the widely used K-nearest-neighbor post-processing is still necessary for our pipeline. Then, we realize the many-to-one mapping causes the blurring effect that some points are mapped into the same pixel and share the same label. Therefore, we propose to process those occluded points by assigning the nearest predicted label to them. This NLA (nearest label assignment) post-processing step shows a better performance than KNN with faster inference speed in the ablation study. On the SemanticKITTI dataset, our pipeline achieves the best performance among all projection-based methods with $64 \times 2048$ resolution and all point-wise solutions. With a ResNet-34 as the backbone, both the training and testing of our model can be finished on a single RTX 2080 Ti with 11G memory. The code is released.



### Adaptive Few-Shot Learning PoC Ultrasound COVID-19 Diagnostic System
- **Arxiv ID**: http://arxiv.org/abs/2109.03793v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.03793v1)
- **Published**: 2021-09-08 17:29:17+00:00
- **Updated**: 2021-09-08 17:29:17+00:00
- **Authors**: Michael Karnes, Shehan Perera, Srikar Adhikari, Alper Yilmaz
- **Comment**: Biomedical Circuits and Systems Conference (BioCAS) 2021
- **Journal**: None
- **Summary**: This paper presents a novel ultrasound imaging point-of-care (PoC) COVID-19 diagnostic system. The adaptive visual diagnostics utilize few-shot learning (FSL) to generate encoded disease state models that are stored and classified using a dictionary of knowns. The novel vocabulary based feature processing of the pipeline adapts the knowledge of a pretrained deep neural network to compress the ultrasound images into discrimative descriptions. The computational efficiency of the FSL approach enables high diagnostic deep learning performance in PoC settings, where training data is limited and the annotation process is not strictly controlled. The algorithm performance is evaluated on the open source COVID-19 POCUS Dataset to validate the system's ability to distinguish COVID-19, pneumonia, and healthy disease states. The results of the empirical analyses demonstrate the appropriate efficiency and accuracy for scalable PoC use. The code for this work will be made publicly available on GitHub upon acceptance.



### Digitize-PID: Automatic Digitization of Piping and Instrumentation Diagrams
- **Arxiv ID**: http://arxiv.org/abs/2109.03794v1
- **DOI**: 10.1007/978-3-030-75015-2_17
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03794v1)
- **Published**: 2021-09-08 17:32:49+00:00
- **Updated**: 2021-09-08 17:32:49+00:00
- **Authors**: Shubham Paliwal, Arushi Jain, Monika Sharma, Lovekesh Vig
- **Comment**: 13 pages
- **Journal**: Trends and Applications in Knowledge Discovery and Data Mining.
  168-180, PAKDD 2021
- **Summary**: Digitization of scanned Piping and Instrumentation diagrams(P&ID), widely used in manufacturing or mechanical industries such as oil and gas over several decades, has become a critical bottleneck in dynamic inventory management and creation of smart P&IDs that are compatible with the latest CAD tools. Historically, P&ID sheets have been manually generated at the design stage, before being scanned and stored as PDFs. Current digitization initiatives involve manual processing and are consequently very time consuming, labour intensive and error-prone.Thanks to advances in image processing, machine and deep learning techniques there are emerging works on P&ID digitization. However, existing solutions face several challenges owing to the variation in the scale, size and noise in the P&IDs, sheer complexity and crowdedness within drawings, domain knowledge required to interpret the drawings. This motivates our current solution called Digitize-PID which comprises of an end-to-end pipeline for detection of core components from P&IDs like pipes, symbols and textual information, followed by their association with each other and eventually, the validation and correction of output data based on inherent domain knowledge. A novel and efficient kernel-based line detection and a two-step method for detection of complex symbols based on a fine-grained deep recognition technique is presented in the paper. In addition, we have created an annotated synthetic dataset, Dataset-P&ID, of 500 P&IDs by incorporating different types of noise and complex symbols which is made available for public use (currently there exists no public P&ID dataset). We evaluate our proposed method on this synthetic dataset and a real-world anonymized private dataset of 12 P&ID sheets. Results show that Digitize-PID outperforms the existing state-of-the-art for P&ID digitization.



### Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic Segmentation and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2109.03805v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.03805v3)
- **Published**: 2021-09-08 17:45:37+00:00
- **Updated**: 2021-12-23 19:16:51+00:00
- **Authors**: Whye Kit Fong, Rohit Mohan, Juana Valeria Hurtado, Lubing Zhou, Holger Caesar, Oscar Beijbom, Abhinav Valada
- **Comment**: The benchmark is available at https://www.nuscenes.org
- **Journal**: None
- **Summary**: Panoptic scene understanding and tracking of dynamic agents are essential for robots and automated vehicles to navigate in urban environments. As LiDARs provide accurate illumination-independent geometric depictions of the scene, performing these tasks using LiDAR point clouds provides reliable predictions. However, existing datasets lack diversity in the type of urban scenes and have a limited number of dynamic object instances which hinders both learning of these tasks as well as credible benchmarking of the developed methods. In this paper, we introduce the large-scale Panoptic nuScenes benchmark dataset that extends our popular nuScenes dataset with point-wise groundtruth annotations for semantic segmentation, panoptic segmentation, and panoptic tracking tasks. To facilitate comparison, we provide several strong baselines for each of these tasks on our proposed dataset. Moreover, we analyze the drawbacks of the existing metrics for panoptic tracking and propose the novel instance-centric PAT metric that addresses the concerns. We present exhaustive experiments that demonstrate the utility of Panoptic nuScenes compared to existing datasets and make the online evaluation server available at nuScenes.org. We believe that this extension will accelerate the research of novel methods for scene understanding of dynamic urban environments.



### Scaled ReLU Matters for Training Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2109.03810v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03810v2)
- **Published**: 2021-09-08 17:57:58+00:00
- **Updated**: 2022-01-12 01:01:35+00:00
- **Authors**: Pichao Wang, Xue Wang, Hao Luo, Jingkai Zhou, Zhipeng Zhou, Fan Wang, Hao Li, Rong Jin
- **Comment**: Accepted by AAAI2022
- **Journal**: None
- **Summary**: Vision transformers (ViTs) have been an alternative design paradigm to convolutional neural networks (CNNs). However, the training of ViTs is much harder than CNNs, as it is sensitive to the training parameters, such as learning rate, optimizer and warmup epoch. The reasons for training difficulty are empirically analysed in ~\cite{xiao2021early}, and the authors conjecture that the issue lies with the \textit{patchify-stem} of ViT models and propose that early convolutions help transformers see better. In this paper, we further investigate this problem and extend the above conclusion: only early convolutions do not help for stable training, but the scaled ReLU operation in the \textit{convolutional stem} (\textit{conv-stem}) matters. We verify, both theoretically and empirically, that scaled ReLU in \textit{conv-stem} not only improves training stabilization, but also increases the diversity of patch tokens, thus boosting peak performance with a large margin via adding few parameters and flops. In addition, extensive experiments are conducted to demonstrate that previous ViTs are far from being well trained, further showing that ViTs have great potential to be a better substitute of CNNs.



### fastMRI+: Clinical Pathology Annotations for Knee and Brain Fully Sampled Multi-Coil MRI Data
- **Arxiv ID**: http://arxiv.org/abs/2109.03812v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2109.03812v2)
- **Published**: 2021-09-08 17:58:31+00:00
- **Updated**: 2021-09-14 01:33:11+00:00
- **Authors**: Ruiyang Zhao, Burhaneddin Yaman, Yuxin Zhang, Russell Stewart, Austin Dixon, Florian Knoll, Zhengnan Huang, Yvonne W. Lui, Michael S. Hansen, Matthew P. Lungren
- **Comment**: None
- **Journal**: None
- **Summary**: Improving speed and image quality of Magnetic Resonance Imaging (MRI) via novel reconstruction approaches remains one of the highest impact applications for deep learning in medical imaging. The fastMRI dataset, unique in that it contains large volumes of raw MRI data, has enabled significant advances in accelerating MRI using deep learning-based reconstruction methods. While the impact of the fastMRI dataset on the field of medical imaging is unquestioned, the dataset currently lacks clinical expert pathology annotations, critical to addressing clinically relevant reconstruction frameworks and exploring important questions regarding rendering of specific pathology using such novel approaches. This work introduces fastMRI+, which consists of 16154 subspecialist expert bounding box annotations and 13 study-level labels for 22 different pathology categories on the fastMRI knee dataset, and 7570 subspecialist expert bounding box annotations and 643 study-level labels for 30 different pathology categories for the fastMRI brain dataset. The fastMRI+ dataset is open access and aims to support further research and advancement of medical imaging in MRI reconstruction and beyond.



### Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2109.03814v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03814v4)
- **Published**: 2021-09-08 17:59:12+00:00
- **Updated**: 2022-03-18 05:31:21+00:00
- **Authors**: Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo, Tong Lu
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Panoptic segmentation involves a combination of joint semantic segmentation and instance segmentation, where image contents are divided into two types: things and stuff. We present Panoptic SegFormer, a general framework for panoptic segmentation with transformers. It contains three innovative components: an efficient deeply-supervised mask decoder, a query decoupling strategy, and an improved post-processing method. We also use Deformable DETR to efficiently process multi-scale features, which is a fast and efficient version of DETR. Specifically, we supervise the attention modules in the mask decoder in a layer-wise manner. This deep supervision strategy lets the attention modules quickly focus on meaningful semantic regions. It improves performance and reduces the number of required training epochs by half compared to Deformable DETR. Our query decoupling strategy decouples the responsibilities of the query set and avoids mutual interference between things and stuff. In addition, our post-processing strategy improves performance without additional costs by jointly considering classification and segmentation qualities to resolve conflicting mask overlaps. Our approach increases the accuracy 6.2\% PQ over the baseline DETR model. Panoptic SegFormer achieves state-of-the-art results on COCO test-dev with 56.2\% PQ. It also shows stronger zero-shot robustness over existing methods. The code is released at \url{https://github.com/zhiqi-li/Panoptic-SegFormer}.



### OSSR-PID: One-Shot Symbol Recognition in P&ID Sheets using Path Sampling and GCN
- **Arxiv ID**: http://arxiv.org/abs/2109.03849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03849v1)
- **Published**: 2021-09-08 18:05:27+00:00
- **Updated**: 2021-09-08 18:05:27+00:00
- **Authors**: Shubham Paliwal, Monika Sharma, Lovekesh Vig
- **Comment**: None
- **Journal**: International Joint Conference on Neural Network (IJCNN), 2021
- **Summary**: Piping and Instrumentation Diagrams (P&ID) are ubiquitous in several manufacturing, oil and gas enterprises for representing engineering schematics and equipment layout. There is an urgent need to extract and digitize information from P&IDs without the cost of annotating a varying set of symbols for each new use case. A robust one-shot learning approach for symbol recognition i.e., localization followed by classification, would therefore go a long way towards this goal. Our method works by sampling pixels sequentially along the different contour boundaries in the image. These sampled points form paths which are used in the prototypical line diagram to construct a graph that captures the structure of the contours. Subsequently, the prototypical graphs are fed into a Dynamic Graph Convolutional Neural Network (DGCNN) which is trained to classify graphs into one of the given symbol classes. Further, we append embeddings from a Resnet-34 network which is trained on symbol images containing sampled points to make the classification network more robust. Since, many symbols in P&ID are structurally very similar to each other, we utilize Arcface loss during DGCNN training which helps in maximizing symbol class separability by producing highly discriminative embeddings. The images consist of components attached on the pipeline (straight line). The sampled points segregated around the symbol regions are used for the classification task. The proposed pipeline, named OSSR-PID, is fast and gives outstanding performance for recognition of symbols on a synthetic dataset of 100 P&ID diagrams. We also compare our method against prior-work on a real-world private dataset of 12 P&ID sheets and obtain comparable/superior results. Remarkably, it is able to achieve such excellent performance using only one prototypical example per symbol.



### Automated LoD-2 Model Reconstruction from Very-HighResolution Satellite-derived Digital Surface Model and Orthophoto
- **Arxiv ID**: http://arxiv.org/abs/2109.03876v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03876v1)
- **Published**: 2021-09-08 19:03:09+00:00
- **Updated**: 2021-09-08 19:03:09+00:00
- **Authors**: Shengxi Gui, Rongjun Qin
- **Comment**: 16 figures, 31 pages
- **Journal**: None
- **Summary**: In this paper, we propose a model-driven method that reconstructs LoD-2 building models following a "decomposition-optimization-fitting" paradigm. The proposed method starts building detection results through a deep learning-based detector and vectorizes individual segments into polygons using a "three-step" polygon extraction method, followed by a novel grid-based decomposition method that decomposes the complex and irregularly shaped building polygons to tightly combined elementary building rectangles ready to fit elementary building models. We have optionally introduced OpenStreetMap (OSM) and Graph-Cut (GC) labeling to further refine the orientation of 2D building rectangle. The 3D modeling step takes building-specific parameters such as hip lines, as well as non-rigid and regularized transformations to optimize the flexibility for using a minimal set of elementary models. Finally, roof type of building models s refined and adjacent building models in one building segment are merged into the complex polygonal model. Our proposed method has addressed a few technical caveats over existing methods, resulting in practically high-quality results, based on our evaluation and comparative study on a diverse set of experimental datasets of cities with different urban patterns.



### SORNet: Spatial Object-Centric Representations for Sequential Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2109.03891v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.03891v3)
- **Published**: 2021-09-08 19:36:29+00:00
- **Updated**: 2022-09-14 02:33:44+00:00
- **Authors**: Wentao Yuan, Chris Paxton, Karthik Desingh, Dieter Fox
- **Comment**: CoRL 2021 Best Systems Paper Finalist; Code and data available at
  https://github.com/wentaoyuan/sornet
- **Journal**: None
- **Summary**: Sequential manipulation tasks require a robot to perceive the state of an environment and plan a sequence of actions leading to a desired goal state. In such tasks, the ability to reason about spatial relations among object entities from raw sensor inputs is crucial in order to determine when a task has been completed and which actions can be executed. In this work, we propose SORNet (Spatial Object-Centric Representation Network), a framework for learning object-centric representations from RGB images conditioned on a set of object queries, represented as image patches called canonical object views. With only a single canonical view per object and no annotation, SORNet generalizes zero-shot to object entities whose shape and texture are both unseen during training. We evaluate SORNet on various spatial reasoning tasks such as spatial relation classification and relative direction regression in complex tabletop manipulation scenarios and show that SORNet significantly outperforms baselines including state-of-the-art representation learning techniques. We also demonstrate the application of the representation learned by SORNet on visual-servoing and task planning for sequential manipulation on a real robot.



### Improving Building Segmentation for Off-Nadir Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2109.03961v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03961v1)
- **Published**: 2021-09-08 22:55:16+00:00
- **Updated**: 2021-09-08 22:55:16+00:00
- **Authors**: Hanxiang Hao, Sriram Baireddy, Kevin LaTourette, Latisha Konz, Moses Chan, Mary L. Comer, Edward J. Delp
- **Comment**: This is an extended version of our ACM SIGSPATIAL'21 conference paper
- **Journal**: None
- **Summary**: Automatic building segmentation is an important task for satellite imagery analysis and scene understanding. Most existing segmentation methods focus on the case where the images are taken from directly overhead (i.e., low off-nadir/viewing angle). These methods often fail to provide accurate results on satellite images with larger off-nadir angles due to the higher noise level and lower spatial resolution. In this paper, we propose a method that is able to provide accurate building segmentation for satellite imagery captured from a large range of off-nadir angles. Based on Bayesian deep learning, we explicitly design our method to learn the data noise via aleatoric and epistemic uncertainty modeling. Satellite image metadata (e.g., off-nadir angle and ground sample distance) is also used in our model to further improve the result. We show that with uncertainty modeling and metadata injection, our method achieves better performance than the baseline method, especially for noisy images taken from large off-nadir angles.



