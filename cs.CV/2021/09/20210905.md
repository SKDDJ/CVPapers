# Arxiv Papers in cs.CV on 2021-09-05
### Deep Saliency Prior for Reducing Visual Distraction
- **Arxiv ID**: http://arxiv.org/abs/2109.01980v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01980v1)
- **Published**: 2021-09-05 03:19:21+00:00
- **Updated**: 2021-09-05 03:19:21+00:00
- **Authors**: Kfir Aberman, Junfeng He, Yossi Gandelsman, Inbar Mosseri, David E. Jacobs, Kai Kohlhoff, Yael Pritch, Michael Rubinstein
- **Comment**: https://deep-saliency-prior.github.io/
- **Journal**: None
- **Summary**: Using only a model that was trained to predict where people look at images, and no additional training data, we can produce a range of powerful editing effects for reducing distraction in images. Given an image and a mask specifying the region to edit, we backpropagate through a state-of-the-art saliency model to parameterize a differentiable editing operator, such that the saliency within the masked region is reduced. We demonstrate several operators, including: a recoloring operator, which learns to apply a color transform that camouflages and blends distractors into their surroundings; a warping operator, which warps less salient image regions to cover distractors, gradually collapsing objects into themselves and effectively removing them (an effect akin to inpainting); a GAN operator, which uses a semantic prior to fully replace image regions with plausible, less salient alternatives. The resulting effects are consistent with cognitive research on the human visual system (e.g., since color mismatch is salient, the recoloring operator learns to harmonize objects' colors with their surrounding to reduce their saliency), and, importantly, are all achieved solely through the guidance of the pretrained saliency model, with no additional supervision. We present results on a variety of natural images and conduct a perceptual study to evaluate and validate the changes in viewers' eye-gaze between the original images and our edited results.



### Image Compression with Recurrent Neural Network and Generalized Divisive Normalization
- **Arxiv ID**: http://arxiv.org/abs/2109.01999v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2109.01999v1)
- **Published**: 2021-09-05 05:31:55+00:00
- **Updated**: 2021-09-05 05:31:55+00:00
- **Authors**: Khawar Islam, L. Minh Dang, Sujin Lee, Hyeonjoon Moon
- **Comment**: Accpeted at IEEE CVPR Workshop
- **Journal**: None
- **Summary**: Image compression is a method to remove spatial redundancy between adjacent pixels and reconstruct a high-quality image. In the past few years, deep learning has gained huge attention from the research community and produced promising image reconstruction results. Therefore, recent methods focused on developing deeper and more complex networks, which significantly increased network complexity. In this paper, two effective novel blocks are developed: analysis and synthesis block that employs the convolution layer and Generalized Divisive Normalization (GDN) in the variable-rate encoder and decoder side. Our network utilizes a pixel RNN approach for quantization. Furthermore, to improve the whole network, we encode a residual image using LSTM cells to reduce unnecessary information. Experimental results demonstrated that the proposed variable-rate framework with novel blocks outperforms existing methods and standard image codecs, such as George's ~\cite{002} and JPEG in terms of image similarity. The project page along with code and models are available at https://khawar512.github.io/cvpr/



### Cross-token Modeling with Conditional Computation
- **Arxiv ID**: http://arxiv.org/abs/2109.02008v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02008v3)
- **Published**: 2021-09-05 06:43:08+00:00
- **Updated**: 2022-01-14 08:06:11+00:00
- **Authors**: Yuxuan Lou, Fuzhao Xue, Zangwei Zheng, Yang You
- **Comment**: None
- **Journal**: None
- **Summary**: Mixture-of-Experts (MoE), a conditional computation architecture, achieved promising performance by scaling local module (i.e. feed-forward network) of transformer. However, scaling the cross-token module (i.e. self-attention) is challenging due to the unstable training. This work proposes Sparse-MLP, an all-MLP model which applies sparsely-activated MLPs to cross-token modeling. Specifically, in each Sparse block of our all-MLP model, we apply two stages of MoE layers: one with MLP experts mixing information within channels along image patch dimension, the other with MLP experts mixing information within patches along the channel dimension. In addition, by proposing importance-score routing strategy for MoE and redesigning the image representation shape, we further improve our model's computational efficiency. Experimentally, we are more computation-efficient than Vision Transformers with comparable accuracy. Also, our models can outperform MLP-Mixer by 2.5\% on ImageNet Top-1 accuracy with fewer parameters and computational cost. On downstream tasks, i.e. Cifar10 and Cifar100, our models can still achieve better performance than baselines.



### Navigational Path-Planning For All-Terrain Autonomous Agricultural Robot
- **Arxiv ID**: http://arxiv.org/abs/2109.02015v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02015v2)
- **Published**: 2021-09-05 07:29:13+00:00
- **Updated**: 2021-09-07 04:53:19+00:00
- **Authors**: Vedant Ghodke, Jyoti Madake
- **Comment**: 8 pages, 23 figures, 1 table
- **Journal**: None
- **Summary**: The shortage of workforce and increasing cost of maintenance has forced many farm industrialists to shift towards automated and mechanized approaches. The key component for autonomous systems is the path planning techniques used. Coverage path planning (CPP) algorithm is used for navigating over farmlands to perform various agricultural operations such as seeding, ploughing, or spraying pesticides and fertilizers. This report paper compares novel algorithms for autonomous navigation of farmlands. For reduction of navigational constraints, a high-resolution grid map representation is taken into consideration specific to Indian environments. The free space is covered by distinguishing the grid cells as covered, unexplored, partially explored and presence of an obstacle. The performance of the compared algorithms is evaluated with metrics such as time efficiency, space efficiency, accuracy, and robustness to changes in the environment. Robotic Operating System (ROS), Dassault Systemes Experience Platform (3DS Experience), MATLAB along Python were used for the simulation of the compared algorithms. The results proved the applicability of the algorithms for autonomous field navigation and feasibility with robotic path planning.



### Data Efficient Masked Language Modeling for Vision and Language
- **Arxiv ID**: http://arxiv.org/abs/2109.02040v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02040v1)
- **Published**: 2021-09-05 11:27:53+00:00
- **Updated**: 2021-09-05 11:27:53+00:00
- **Authors**: Yonatan Bitton, Gabriel Stanovsky, Michael Elhadad, Roy Schwartz
- **Comment**: Accepted to Findings of EMNLP 2021
- **Journal**: None
- **Summary**: Masked language modeling (MLM) is one of the key sub-tasks in vision-language pretraining. In the cross-modal setting, tokens in the sentence are masked at random, and the model predicts the masked tokens given the image and the text. In this paper, we observe several key disadvantages of MLM in this setting. First, as captions tend to be short, in a third of the sentences no token is sampled. Second, the majority of masked tokens are stop-words and punctuation, leading to under-utilization of the image. We investigate a range of alternative masking strategies specific to the cross-modal setting that address these shortcomings, aiming for better fusion of text and image in the learned representation. When pre-training the LXMERT model, our alternative masking strategies consistently improve over the original masking strategy on three downstream tasks, especially in low resource settings. Further, our pre-training approach substantially outperforms the baseline model on a prompt-based probing task designed to elicit image objects. These results and our analysis indicate that our method allows for better utilization of the training data.



### Sensor Data Augmentation by Resampling for Contrastive Learning in Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.02054v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02054v2)
- **Published**: 2021-09-05 12:18:53+00:00
- **Updated**: 2022-03-23 07:10:20+00:00
- **Authors**: Jinqiang Wang, Tao Zhu, Jingyuan Gan, Liming Chen, Huansheng Ning, Yaping Wan
- **Comment**: 13 pages,5 figures
- **Journal**: None
- **Summary**: While deep learning has contributed to the advancement of sensor-based Human Activity Recognition (HAR), it is usually a costly and challenging supervised task with the needs of a large amount of labeled data. To alleviate this issue, contrastive learning has been applied for sensor-based HAR. Data augmentation is an essential part of contrastive learning and has a significant impact on the performance of downstream tasks. However, current popular augmentation methods do not achieve competitive performance in contrastive learning for sensor-based HAR. Motivated by this issue, we propose a new sensor data augmentation method by resampling, which simulates more realistic activity data by varying the sampling frequency to maximize the coverage of the sampling space. In addition, we extend MoCo, a popular contrastive learning framework, to MoCoHAR for HAR. The resampling augmentation method will be evaluated on two contrastive learning frameworks, SimCLRHAR and MoCoHAR, using UCI-HAR, MotionSensor, and USC-HAD datasets. The experiment results show that the resampling augmentation method outperforms all state-of-the-art methods under a small amount of labeled data, on SimCLRHAR and MoCoHAR, with mean F1-score as the evaluation metric. The results also demonstrate that not all data augmentation methods have positive effects in the contrastive learning framework.



### Hierarchical Object-to-Zone Graph for Object Navigation
- **Arxiv ID**: http://arxiv.org/abs/2109.02066v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02066v2)
- **Published**: 2021-09-05 13:02:17+00:00
- **Updated**: 2021-09-09 08:59:23+00:00
- **Authors**: Sixian Zhang, Xinhang Song, Yubing Bai, Weijie Li, Yakui Chu, Shuqiang Jiang
- **Comment**: Accepted by ICCV21
- **Journal**: None
- **Summary**: The goal of object navigation is to reach the expected objects according to visual information in the unseen environments. Previous works usually implement deep models to train an agent to predict actions in real-time. However, in the unseen environment, when the target object is not in egocentric view, the agent may not be able to make wise decisions due to the lack of guidance. In this paper, we propose a hierarchical object-to-zone (HOZ) graph to guide the agent in a coarse-to-fine manner, and an online-learning mechanism is also proposed to update HOZ according to the real-time observation in new environments. In particular, the HOZ graph is composed of scene nodes, zone nodes and object nodes. With the pre-learned HOZ graph, the real-time observation and the target goal, the agent can constantly plan an optimal path from zone to zone. In the estimated path, the next potential zone is regarded as sub-goal, which is also fed into the deep reinforcement learning model for action prediction. Our methods are evaluated on the AI2-Thor simulator. In addition to widely used evaluation metrics SR and SPL, we also propose a new evaluation metric of SAE that focuses on the effective action rate. Experimental results demonstrate the effectiveness and efficiency of our proposed method.



### Fusformer: A Transformer-based Fusion Approach for Hyperspectral Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2109.02079v1
- **DOI**: 10.1109/LGRS.2022.3194257
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02079v1)
- **Published**: 2021-09-05 14:00:34+00:00
- **Updated**: 2021-09-05 14:00:34+00:00
- **Authors**: Jin-Fan Hu, Ting-Zhu Huang, Liang-Jian Deng
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral image has become increasingly crucial due to its abundant spectral information. However, It has poor spatial resolution with the limitation of the current imaging mechanism. Nowadays, many convolutional neural networks have been proposed for the hyperspectral image super-resolution problem. However, convolutional neural network (CNN) based methods only consider the local information instead of the global one with the limited kernel size of receptive field in the convolution operation. In this paper, we design a network based on the transformer for fusing the low-resolution hyperspectral images and high-resolution multispectral images to obtain the high-resolution hyperspectral images. Thanks to the representing ability of the transformer, our approach is able to explore the intrinsic relationships of features globally. Furthermore, considering the LR-HSIs hold the main spectral structure, the network focuses on the spatial detail estimation releasing from the burden of reconstructing the whole data. It reduces the mapping space of the proposed network, which enhances the final performance. Various experiments and quality indexes show our approach's superiority compared with other state-of-the-art methods.



### Deep Person Generation: A Survey from the Perspective of Face, Pose and Cloth Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2109.02081v2
- **DOI**: 10.1145/3575656
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02081v2)
- **Published**: 2021-09-05 14:15:24+00:00
- **Updated**: 2023-08-21 14:36:56+00:00
- **Authors**: Tong Sha, Wei Zhang, Tong Shen, Zhoujun Li, Tao Mei
- **Comment**: None
- **Journal**: ACM Computing Surveys, 2023, 55(12): 1-37
- **Summary**: Deep person generation has attracted extensive research attention due to its wide applications in virtual agents, video conferencing, online shopping and art/movie production. With the advancement of deep learning, visual appearances (face, pose, cloth) of a person image can be easily generated or manipulated on demand. In this survey, we first summarize the scope of person generation, and then systematically review recent progress and technical trends in deep person generation, covering three major tasks: talking-head generation (face), pose-guided person generation (pose) and garment-oriented person generation (cloth). More than two hundred papers are covered for a thorough overview, and the milestone works are highlighted to witness the major technical breakthrough. Based on these fundamental tasks, a number of applications are investigated, e.g., virtual fitting, digital human, generative data augmentation. We hope this survey could shed some light on the future prospects of deep person generation, and provide a helpful foundation for full applications towards digital human.



### (M)SLAe-Net: Multi-Scale Multi-Level Attention embedded Network for Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.02084v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02084v1)
- **Published**: 2021-09-05 14:29:00+00:00
- **Updated**: 2021-09-05 14:29:00+00:00
- **Authors**: Shreshth Saini, Geetika Agrawal
- **Comment**: 5 pages, 4 figures, Accepted and Presented in 9TH IEEE INTERNATIONAL
  CONFERENCE ON HEALTHCARE INFORMATICS (IEEE-ICHI 2021), Victoria, British
  Columbia, Canada
- **Journal**: None
- **Summary**: Segmentation plays a crucial role in diagnosis. Studying the retinal vasculatures from fundus images help identify early signs of many crucial illnesses such as diabetic retinopathy. Due to the varying shape, size, and patterns of retinal vessels, along with artefacts and noises in fundus images, no one-stage method can accurately segment retinal vessels. In this work, we propose a multi-scale, multi-level attention embedded CNN architecture ((M)SLAe-Net) to address the issue of multi-stage processing for robust and precise segmentation of retinal vessels. We do this by extracting features at multiple scales and multiple levels of the network, enabling our model to holistically extracts the local and global features. Multi-scale features are extracted using our novel dynamic dilated pyramid pooling (D-DPP) module. We also aggregate the features from all the network levels. These effectively resolved the issues of varying shapes and artefacts and hence the need for multiple stages. To assist in better pixel-level classification, we use the Squeeze and Attention(SA) module, a smartly adapted version of the Squeeze and Excitation(SE) module for segmentation tasks in our network to facilitate pixel-group attention. Our unique network design and novel D-DPP module with efficient task-specific loss function for thin vessels enabled our model for better cross data performance. Exhaustive experimental results on DRIVE, STARE, HRF, and CHASE-DB1 show the superiority of our method.



### Timbre Transfer with Variational Auto Encoding and Cycle-Consistent Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.02096v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.AI, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2109.02096v2)
- **Published**: 2021-09-05 15:06:53+00:00
- **Updated**: 2021-10-10 16:22:00+00:00
- **Authors**: Russell Sammut Bonnici, Charalampos Saitis, Martin Benning
- **Comment**: 12 pages, 3 main figures, 4 tables
- **Journal**: None
- **Summary**: This research project investigates the application of deep learning to timbre transfer, where the timbre of a source audio can be converted to the timbre of a target audio with minimal loss in quality. The adopted approach combines Variational Autoencoders with Generative Adversarial Networks to construct meaningful representations of the source audio and produce realistic generations of the target audio and is applied to the Flickr 8k Audio dataset for transferring the vocal timbre between speakers and the URMP dataset for transferring the musical timbre between instruments. Furthermore, variations of the adopted approach are trained, and generalised performance is compared using the metrics SSIM (Structural Similarity Index) and FAD (Frech\'et Audio Distance). It was found that a many-to-many approach supersedes a one-to-one approach in terms of reconstructive capabilities, and that the adoption of a basic over a bottleneck residual block design is more suitable for enriching content information about a latent space. It was also found that the decision on whether cyclic loss takes on a variational autoencoder or vanilla autoencoder approach does not have a significant impact on reconstructive and adversarial translation aspects of the model.



### Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss
- **Arxiv ID**: http://arxiv.org/abs/2109.02100v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02100v1)
- **Published**: 2021-09-05 15:15:07+00:00
- **Updated**: 2021-09-05 15:15:07+00:00
- **Authors**: Jung Hyun Lee, Jihun Yun, Sung Ju Hwang, Eunho Yang
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Network quantization, which aims to reduce the bit-lengths of the network weights and activations, has emerged for their deployments to resource-limited devices. Although recent studies have successfully discretized a full-precision network, they still incur large quantization errors after training, thus giving rise to a significant performance gap between a full-precision network and its quantized counterpart. In this work, we propose a novel quantization method for neural networks, Cluster-Promoting Quantization (CPQ) that finds the optimal quantization grids while naturally encouraging the underlying full-precision weights to gather around those quantization grids cohesively during training. This property of CPQ is thanks to our two main ingredients that enable differentiable quantization: i) the use of the categorical distribution designed by a specific probabilistic parametrization in the forward pass and ii) our proposed multi-class straight-through estimator (STE) in the backward pass. Since our second component, multi-class STE, is intrinsically biased, we additionally propose a new bit-drop technique, DropBits, that revises the standard dropout regularization to randomly drop bits instead of neurons. As a natural extension of DropBits, we further introduce the way of learning heterogeneous quantization levels to find proper bit-length for each layer by imposing an additional regularization on DropBits. We experimentally validate our method on various benchmark datasets and network architectures, and also support a new hypothesis for quantization: learning heterogeneous quantization levels outperforms the case using the same but fixed quantization levels from scratch.



### Recognition of COVID-19 Disease Utilizing X-Ray Imaging of the Chest Using CNN
- **Arxiv ID**: http://arxiv.org/abs/2109.02103v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02103v1)
- **Published**: 2021-09-05 15:31:24+00:00
- **Updated**: 2021-09-05 15:31:24+00:00
- **Authors**: Md Gulzar Hussain, Ye Shiren
- **Comment**: Accepted and Presented in International Conference on Computing,
  Electronics & Communications Engineering (iCCECE '21)
- **Journal**: None
- **Summary**: Since this COVID-19 pandemic thrives, the utilization of X-Ray images of the Chest (CXR) as a complementary screening technique to RT-PCR testing grows to its clinical use for respiratory complaints. Many new deep learning approaches have developed as a consequence. The goal of this research is to assess the convolutional neural networks (CNNs) to diagnosis COVID-19 utisizing X-ray images of chest. The performance of CNN with one, three, and four convolution layers has been evaluated in this research. A dataset of 13,808 CXR photographs are used in this research. When evaluated on X-ray images with three splits of the dataset, our preliminary experimental results show that the CNN model with three convolution layers can reliably detect with 96 percent accuracy (precision being 96 percent). This fact indicates the commitment of our suggested model for reliable screening of COVID-19.



### Light Field-Based Underwater 3D Reconstruction Via Angular Resampling
- **Arxiv ID**: http://arxiv.org/abs/2109.02116v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02116v3)
- **Published**: 2021-09-05 16:23:39+00:00
- **Updated**: 2022-04-05 00:37:11+00:00
- **Authors**: Yuqi Ding, Zhang Chen, Yu Ji, Jingyi Yu, Jinwei Ye
- **Comment**: This submission need to be withdrawn due to one of the authors do not
  agree posting the paper on arxiv
- **Journal**: None
- **Summary**: Recovering 3D geometry of underwater scenes is challenging because of non-linear refraction of light at the water-air interface caused by the camera housing. We present a light field-based approach that leverages properties of angular samples for high-quality underwater 3D reconstruction from a single viewpoint. Specifically, we resample the light field image to angular patches. As underwater scenes exhibit weak view-dependent specularity, an angular patch tends to have uniform intensity when sampled at the correct depth. We thus impose this angular uniformity as a constraint for depth estimation. For efficient angular resampling, we design a fast approximation algorithm based on multivariate polynomial regression to approximate nonlinear refraction paths. We further develop a light field calibration algorithm that estimates the water-air interface geometry along with the camera parameters. Comprehensive experiments on synthetic and real data show our method produces state-of-the-art reconstruction on static and dynamic underwater scenes.



### Identification of Driver Phone Usage Violations via State-of-the-Art Object Detection with Tracking
- **Arxiv ID**: http://arxiv.org/abs/2109.02119v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02119v3)
- **Published**: 2021-09-05 16:37:03+00:00
- **Updated**: 2021-10-08 10:50:23+00:00
- **Authors**: Steven Carrell, Amir Atapour-Abarghouei
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: The use of mobiles phones when driving have been a major factor when it comes to road traffic incidents and the process of capturing such violations can be a laborious task. Advancements in both modern object detection frameworks and high-performance hardware has paved the way for a more automated approach when it comes to video surveillance. In this work, we propose a custom-trained state-of-the-art object detector to work with roadside cameras to capture driver phone usage without the need for human intervention. The proposed approach also addresses the issues caused by windscreen glare and introduces the steps required to remedy this. Twelve pre-trained models are fine-tuned with our custom dataset using four popular object detection methods: YOLO, SSD, Faster R-CNN, and CenterNet. Out of all the object detectors tested, the YOLO yields the highest accuracy levels of up to 96% (AP10) and frame rates of up to ~30 FPS. DeepSort object tracking algorithm is also integrated into the best-performing model to collect records of only the unique violations, and enable the proposed approach to count the number of vehicles. The proposed automated system will collect the output images of the identified violations, timestamps of each violation, and total vehicle count. Data can be accessed via a purpose-built user interface.



### Stochastic Neural Radiance Fields: Quantifying Uncertainty in Implicit 3D Representations
- **Arxiv ID**: http://arxiv.org/abs/2109.02123v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02123v3)
- **Published**: 2021-09-05 16:56:43+00:00
- **Updated**: 2021-09-28 08:39:47+00:00
- **Authors**: Jianxiong Shen, Adria Ruiz, Antonio Agudo, Francesc Moreno-Noguer
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Radiance Fields (NeRF) has become a popular framework for learning implicit 3D representations and addressing different tasks such as novel-view synthesis or depth-map estimation. However, in downstream applications where decisions need to be made based on automatic predictions, it is critical to leverage the confidence associated with the model estimations. Whereas uncertainty quantification is a long-standing problem in Machine Learning, it has been largely overlooked in the recent NeRF literature. In this context, we propose Stochastic Neural Radiance Fields (S-NeRF), a generalization of standard NeRF that learns a probability distribution over all the possible radiance fields modeling the scene. This distribution allows to quantify the uncertainty associated with the scene information provided by the model. S-NeRF optimization is posed as a Bayesian learning problem which is efficiently addressed using the Variational Inference framework. Exhaustive experiments over benchmark datasets demonstrate that S-NeRF is able to provide more reliable predictions and confidence values than generic approaches previously proposed for uncertainty estimation in other domains.



### Efficient Action Recognition Using Confidence Distillation
- **Arxiv ID**: http://arxiv.org/abs/2109.02137v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.02137v2)
- **Published**: 2021-09-05 18:25:49+00:00
- **Updated**: 2022-08-16 16:57:08+00:00
- **Authors**: Shervin Manzuri Shalmani, Fei Chiang, Rong Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Modern neural networks are powerful predictive models. However, when it comes to recognizing that they may be wrong about their predictions, they perform poorly. For example, for one of the most common activation functions, the ReLU and its variants, even a well-calibrated model can produce incorrect but high confidence predictions. In the related task of action recognition, most current classification methods are based on clip-level classifiers that densely sample a given video for non-overlapping, same-sized clips and aggregate the results using an aggregation function - typically averaging - to achieve video level predictions. While this approach has shown to be effective, it is sub-optimal in recognition accuracy and has a high computational overhead. To mitigate both these issues, we propose the confidence distillation framework to teach a representation of uncertainty of the teacher to the student sampler and divide the task of full video prediction between the student and the teacher models. We conduct extensive experiments on three action recognition datasets and demonstrate that our framework achieves significant improvements in action recognition accuracy (up to 20%) and computational efficiency (more than 40%).



### Spatial Domain Feature Extraction Methods for Unconstrained Handwritten Malayalam Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.02153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02153v1)
- **Published**: 2021-09-05 19:21:38+00:00
- **Updated**: 2021-09-05 19:21:38+00:00
- **Authors**: Jomy John
- **Comment**: 10 Pages, 5 figures
- **Journal**: Two day International Conference on MACHINE LEARNING AND NEURAL
  INFORMATION SYSTEMS (ICMLNIS-2021)18-19 March 2021,A-3,Page 2,
  ISBN-13:978-93-90614-27-1
- **Summary**: Handwritten character recognition is an active research challenge,especially for Indian scripts. This paper deals with handwritten Malayalam, with a complete set of basic characters, vowel and consonant signs and compound characters that may be present in the script. Spatial domain features suitable for recognition are chosen in this work. For classification, k-NN, SVM and ELM are employed



### Robust Attentive Deep Neural Network for Exposing GAN-generated Faces
- **Arxiv ID**: http://arxiv.org/abs/2109.02167v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02167v2)
- **Published**: 2021-09-05 21:22:39+00:00
- **Updated**: 2022-02-12 04:48:00+00:00
- **Authors**: Hui Guo, Shu Hu, Xin Wang, Ming-Ching Chang, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: GAN-based techniques that generate and synthesize realistic faces have caused severe social concerns and security problems. Existing methods for detecting GAN-generated faces can perform well on limited public datasets. However, images from existing public datasets do not represent real-world scenarios well enough in terms of view variations and data distributions (where real faces largely outnumber synthetic faces). The state-of-the-art methods do not generalize well in real-world problems and lack the interpretability of detection results. Performance of existing GAN-face detection models degrades significantly when facing imbalanced data distributions. To address these shortcomings, we propose a robust, attentive, end-to-end network that can spot GAN-generated faces by analyzing their eye inconsistencies. Specifically, our model learns to identify inconsistent eye components by localizing and comparing the iris artifacts between the two eyes automatically. Our deep network addresses the imbalance learning issues by considering the AUC loss and the traditional cross-entropy loss jointly. Comprehensive evaluations of the FFHQ dataset in terms of both balanced and imbalanced scenarios demonstrate the superiority of the proposed method.



### Right Ventricular Segmentation from Short- and Long-Axis MRIs via Information Transition
- **Arxiv ID**: http://arxiv.org/abs/2109.02171v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02171v1)
- **Published**: 2021-09-05 21:39:27+00:00
- **Updated**: 2021-09-05 21:39:27+00:00
- **Authors**: Lei Li, Wangbin Ding, Liqun Huang, Xiahai Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: Right ventricular (RV) segmentation from magnetic resonance imaging (MRI) is a crucial step for cardiac morphology and function analysis. However, automatic RV segmentation from MRI is still challenging, mainly due to the heterogeneous intensity, the complex variable shapes, and the unclear RV boundary. Moreover, current methods for the RV segmentation tend to suffer from performance degradation at the basal and apical slices of MRI. In this work, we propose an automatic RV segmentation framework, where the information from long-axis (LA) views is utilized to assist the segmentation of short-axis (SA) views via information transition. Specifically, we employed the transformed segmentation from LA views as a prior information, to extract the ROI from SA views for better segmentation. The information transition aims to remove the surrounding ambiguous regions in the SA views. %, such as the tricuspid valve regions. We tested our model on a public dataset with 360 multi-center, multi-vendor and multi-disease subjects that consist of both LA and SA MRIs. Our experimental results show that including LA views can be effective to improve the accuracy of the SA segmentation. Our model is publicly available at https://github.com/NanYoMy/MMs-2.



### Multi-Agent Variational Occlusion Inference Using People as Sensors
- **Arxiv ID**: http://arxiv.org/abs/2109.02173v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.MA, I.2.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2109.02173v3)
- **Published**: 2021-09-05 21:56:54+00:00
- **Updated**: 2022-03-02 20:37:01+00:00
- **Authors**: Masha Itkina, Ye-Ji Mun, Katherine Driggs-Campbell, Mykel J. Kochenderfer
- **Comment**: 12 pages, 9 figures, International Conference on Robotics and
  Automation (ICRA) 2022
- **Journal**: None
- **Summary**: Autonomous vehicles must reason about spatial occlusions in urban environments to ensure safety without being overly cautious. Prior work explored occlusion inference from observed social behaviors of road agents, hence treating people as sensors. Inferring occupancy from agent behaviors is an inherently multimodal problem; a driver may behave similarly for different occupancy patterns ahead of them (e.g., a driver may move at constant speed in traffic or on an open road). Past work, however, does not account for this multimodality, thus neglecting to model this source of aleatoric uncertainty in the relationship between driver behaviors and their environment. We propose an occlusion inference method that characterizes observed behaviors of human agents as sensor measurements, and fuses them with those from a standard sensor suite. To capture the aleatoric uncertainty, we train a conditional variational autoencoder with a discrete latent space to learn a multimodal mapping from observed driver trajectories to an occupancy grid representation of the view ahead of the driver. Our method handles multi-agent scenarios, combining measurements from multiple observed drivers using evidential theory to solve the sensor fusion problem. Our approach is validated on a cluttered, real-world intersection, outperforming baselines and demonstrating real-time capable performance. Our code is available at https://github.com/sisl/MultiAgentVariationalOcclusionInference .



### Square Root Marginalization for Sliding-Window Bundle Adjustment
- **Arxiv ID**: http://arxiv.org/abs/2109.02182v1
- **DOI**: 10.1109/ICCV48922.2021.01301
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02182v1)
- **Published**: 2021-09-05 23:22:38+00:00
- **Updated**: 2021-09-05 23:22:38+00:00
- **Authors**: Nikolaus Demmel, David Schubert, Christiane Sommer, Daniel Cremers, Vladyslav Usenko
- **Comment**: to be published at ICCV 2021; camera ready version
- **Journal**: None
- **Summary**: In this paper we propose a novel square root sliding-window bundle adjustment suitable for real-time odometry applications. The square root formulation pervades three major aspects of our optimization-based sliding-window estimator: for bundle adjustment we eliminate landmark variables with nullspace projection; to store the marginalization prior we employ a matrix square root of the Hessian; and when marginalizing old poses we avoid forming normal equations and update the square root prior directly with a specialized QR decomposition. We show that the proposed square root marginalization is algebraically equivalent to the conventional use of Schur complement (SC) on the Hessian. Moreover, it elegantly deals with rank-deficient Jacobians producing a prior equivalent to SC with Moore-Penrose inverse. Our evaluation of visual and visual-inertial odometry on real-world datasets demonstrates that the proposed estimator is 36% faster than the baseline. It furthermore shows that in single precision, conventional Hessian-based marginalization leads to numeric failures and reduced accuracy. We analyse numeric properties of the marginalization prior to explain why our square root form does not suffer from the same effect and therefore entails superior performance.



