# Arxiv Papers in cs.CV on 2021-09-06
### Parsing Table Structures in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2109.02199v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02199v1)
- **Published**: 2021-09-06 01:05:48+00:00
- **Updated**: 2021-09-06 01:05:48+00:00
- **Authors**: Rujiao Long, Wen Wang, Nan Xue, Feiyu Gao, Zhibo Yang, Yongpan Wang, Gui-Song Xia
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: This paper tackles the problem of table structure parsing (TSP) from images in the wild. In contrast to existing studies that mainly focus on parsing well-aligned tabular images with simple layouts from scanned PDF documents, we aim to establish a practical table structure parsing system for real-world scenarios where tabular input images are taken or scanned with severe deformation, bending or occlusions. For designing such a system, we propose an approach named Cycle-CenterNet on the top of CenterNet with a novel cycle-pairing module to simultaneously detect and group tabular cells into structured tables. In the cycle-pairing module, a new pairing loss function is proposed for the network training. Alongside with our Cycle-CenterNet, we also present a large-scale dataset, named Wired Table in the Wild (WTW), which includes well-annotated structure parsing of multiple style tables in several scenes like the photo, scanning files, web pages, \emph{etc.}. In experiments, we demonstrate that our Cycle-CenterNet consistently achieves the best accuracy of table structure parsing on the new WTW dataset by 24.6\% absolute improvement evaluated by the TEDS metric. A more comprehensive experimental analysis also validates the advantages of our proposed methods for the TSP task.



### Learning Fine-Grained Motion Embedding for Landscape Animation
- **Arxiv ID**: http://arxiv.org/abs/2109.02216v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02216v2)
- **Published**: 2021-09-06 02:47:11+00:00
- **Updated**: 2021-09-13 03:20:22+00:00
- **Authors**: Hongwei Xue, Bei Liu, Huan Yang, Jianlong Fu, Houqiang Li, Jiebo Luo
- **Comment**: Accepted by ACM Multimedia 2021 as an oral paper
- **Journal**: None
- **Summary**: In this paper we focus on landscape animation, which aims to generate time-lapse videos from a single landscape image. Motion is crucial for landscape animation as it determines how objects move in videos. Existing methods are able to generate appealing videos by learning motion from real time-lapse videos. However, current methods suffer from inaccurate motion generation, which leads to unrealistic video results. To tackle this problem, we propose a model named FGLA to generate high-quality and realistic videos by learning Fine-Grained motion embedding for Landscape Animation. Our model consists of two parts: (1) a motion encoder which embeds time-lapse motion in a fine-grained way. (2) a motion generator which generates realistic motion to animate input images. To train and evaluate on diverse time-lapse videos, we build the largest high-resolution Time-lapse video dataset with Diverse scenes, namely Time-lapse-D, which includes 16,874 video clips with over 10 million frames. Quantitative and qualitative experimental results demonstrate the superiority of our method. In particular, our method achieves relative improvements by 19% on LIPIS and 5.6% on FVD compared with state-of-the-art methods on our dataset. A user study carried out with 700 human subjects shows that our approach visually outperforms existing methods by a large margin.



### Reconstructing High-resolution Turbulent Flows Using Physics-Guided Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.03327v1
- **DOI**: None
- **Categories**: **physics.flu-dyn**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.03327v1)
- **Published**: 2021-09-06 03:01:24+00:00
- **Updated**: 2021-09-06 03:01:24+00:00
- **Authors**: Shengyu Chen, Shervin Sammak, Peyman Givi, Joseph P. Yurko1, Xiaowei Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Large eddy simulation (LES) is an alternative that is computationally less demanding, but is unable to capture all of the scales of turbulent transport accurately. Our goal in this work is to build a new data-driven methodology based on super-resolution techniques to reconstruct DNS data from LES predictions. We leverage the underlying physical relationships to regularize the relationships amongst different physical variables. We also introduce a hierarchical generative process and a reverse degradation process to fully explore the correspondence between DNS and LES data. We demonstrate the effectiveness of our method through a single-snapshot experiment and a cross-time experiment. The results confirm that our method can better reconstruct high-resolution DNS data over space and over time in terms of pixel-wise reconstruction error and structural similarity. Visual comparisons show that our method performs much better in capturing fine-level flow dynamics.



### Reasoning Graph Networks for Kinship Verification: from Star-shaped to Hierarchical
- **Arxiv ID**: http://arxiv.org/abs/2109.02219v1
- **DOI**: 10.1109/TIP.2021.3077111
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02219v1)
- **Published**: 2021-09-06 03:16:56+00:00
- **Updated**: 2021-09-06 03:16:56+00:00
- **Authors**: Wanhua Li, Jiwen Lu, Abudukelimu Wuerkaixi, Jianjiang Feng, Jie Zhou
- **Comment**: Accepted by IEEE Transactions on Image Processing (TIP)
- **Journal**: IEEE Transactions on Image Processing, vol. 30, pp. 4947-4961,
  2021
- **Summary**: In this paper, we investigate the problem of facial kinship verification by learning hierarchical reasoning graph networks. Conventional methods usually focus on learning discriminative features for each facial image of a paired sample and neglect how to fuse the obtained two facial image features and reason about the relations between them. To address this, we propose a Star-shaped Reasoning Graph Network (S-RGN). Our S-RGN first constructs a star-shaped graph where each surrounding node encodes the information of comparisons in a feature dimension and the central node is employed as the bridge for the interaction of surrounding nodes. Then we perform relational reasoning on this star graph with iterative message passing. The proposed S-RGN uses only one central node to analyze and process information from all surrounding nodes, which limits its reasoning capacity. We further develop a Hierarchical Reasoning Graph Network (H-RGN) to exploit more powerful and flexible capacity. More specifically, our H-RGN introduces a set of latent reasoning nodes and constructs a hierarchical graph with them. Then bottom-up comparative information abstraction and top-down comprehensive signal propagation are iteratively performed on the hierarchical graph to update the node features. Extensive experimental results on four widely used kinship databases show that the proposed methods achieve very competitive results.



### GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization
- **Arxiv ID**: http://arxiv.org/abs/2109.02220v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02220v2)
- **Published**: 2021-09-06 03:17:10+00:00
- **Updated**: 2021-09-08 07:51:17+00:00
- **Authors**: Yi Guo, Huan Yuan, Jianchao Tan, Zhangyang Wang, Sen Yang, Ji Liu
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Model compression techniques are recently gaining explosive attention for obtaining efficient AI models for various real-time applications. Channel pruning is one important compression strategy and is widely used in slimming various DNNs. Previous gate-based or importance-based pruning methods aim to remove channels whose importance is smallest. However, it remains unclear what criteria the channel importance should be measured on, leading to various channel selection heuristics. Some other sampling-based pruning methods deploy sampling strategies to train sub-nets, which often causes the training instability and the compressed model's degraded performance. In view of the research gaps, we present a new module named Gates with Differentiable Polarization (GDP), inspired by principled optimization ideas. GDP can be plugged before convolutional layers without bells and whistles, to control the on-and-off of each channel or whole layer block. During the training process, the polarization effect will drive a subset of gates to smoothly decrease to exact zero, while other gates gradually stay away from zero by a large margin. When training terminates, those zero-gated channels can be painlessly removed, while other non-zero gates can be absorbed into the succeeding convolution kernel, causing completely no interruption to training nor damage to the trained model. Experiments conducted over CIFAR-10 and ImageNet datasets show that the proposed GDP algorithm achieves the state-of-the-art performance on various benchmark DNNs at a broad range of pruning ratios. We also apply GDP to DeepLabV3Plus-ResNet50 on the challenging Pascal VOC segmentation task, whose test performance sees no drop (even slightly improved) with over 60% FLOPs saving.



### GeneAnnotator: A Semi-automatic Annotation Tool for Visual Scene Graph
- **Arxiv ID**: http://arxiv.org/abs/2109.02226v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02226v1)
- **Published**: 2021-09-06 03:37:11+00:00
- **Updated**: 2021-09-06 03:37:11+00:00
- **Authors**: Zhixuan Zhang, Chi Zhang, Zhenning Niu, Le Wang, Yuehu Liu
- **Comment**: 5 pages, 2 figures, 2 tables
- **Journal**: None
- **Summary**: In this manuscript, we introduce a semi-automatic scene graph annotation tool for images, the GeneAnnotator. This software allows human annotators to describe the existing relationships between participators in the visual scene in the form of directed graphs, hence enabling the learning and reasoning on visual relationships, e.g., image captioning, VQA and scene graph generation, etc. The annotations for certain image datasets could either be merged in a single VG150 data-format file to support most existing models for scene graph learning or transformed into a separated annotation file for each single image to build customized datasets. Moreover, GeneAnnotator provides a rule-based relationship recommending algorithm to reduce the heavy annotation workload. With GeneAnnotator, we propose Traffic Genome, a comprehensive scene graph dataset with 1000 diverse traffic images, which in return validates the effectiveness of the proposed software for scene graph annotation. The project source code, with usage examples and sample data is available at https://github.com/Milomilo0320/A-Semi-automatic-Annotation-Software-for-Scene-Graph, under the Apache open-source license.



### Learning to Generate Scene Graph from Natural Language Supervision
- **Arxiv ID**: http://arxiv.org/abs/2109.02227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02227v1)
- **Published**: 2021-09-06 03:38:52+00:00
- **Updated**: 2021-09-06 03:38:52+00:00
- **Authors**: Yiwu Zhong, Jing Shi, Jianwei Yang, Chenliang Xu, Yin Li
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Learning from image-text data has demonstrated recent success for many recognition tasks, yet is currently limited to visual features or individual visual concepts such as objects. In this paper, we propose one of the first methods that learn from image-sentence pairs to extract a graphical representation of localized objects and their relationships within an image, known as scene graph. To bridge the gap between images and texts, we leverage an off-the-shelf object detector to identify and localize object instances, match labels of detected regions to concepts parsed from captions, and thus create "pseudo" labels for learning scene graph. Further, we design a Transformer-based model to predict these "pseudo" labels via a masked token prediction task. Learning from only image-sentence pairs, our model achieves 30% relative gain over a latest method trained with human-annotated unlocalized scene graphs. Our model also shows strong results for weakly and fully supervised scene graph generation. In addition, we explore an open-vocabulary setting for detecting scene graphs, and present the first result for open-set scene graph generation. Our code is available at https://github.com/YiwuZhong/SGG_from_NLS.



### Image recognition via Vietoris-Rips complex
- **Arxiv ID**: http://arxiv.org/abs/2109.02231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02231v1)
- **Published**: 2021-09-06 03:51:10+00:00
- **Updated**: 2021-09-06 03:51:10+00:00
- **Authors**: Yasuhiko Asao, Jumpei Nagase, Ryotaro Sakamoto, Shiro Takagi
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Extracting informative features from images has been of capital importance in computer vision. In this paper, we propose a way to extract such features from images by a method based on algebraic topology. To that end, we construct a weighted graph from an image, which extracts local information of an image. By considering this weighted graph as a pseudo-metric space, we construct a Vietoris-Rips complex with a parameter $\varepsilon$ by a well-known process of algebraic topology. We can extract information of complexity of the image and can detect a sub-image with a relatively high concentration of information from this Vietoris-Rips complex. The parameter $\varepsilon$ of the Vietoris-Rips complex produces robustness to noise. We empirically show that the extracted feature captures well images' characteristics.



### Self-supervised Product Quantization for Deep Unsupervised Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2109.02244v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02244v2)
- **Published**: 2021-09-06 05:02:34+00:00
- **Updated**: 2022-01-12 06:18:26+00:00
- **Authors**: Young Kyun Jang, Nam Ik Cho
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Supervised deep learning-based hash and vector quantization are enabling fast and large-scale image retrieval systems. By fully exploiting label annotations, they are achieving outstanding retrieval performances compared to the conventional methods. However, it is painstaking to assign labels precisely for a vast amount of training data, and also, the annotation process is error-prone. To tackle these issues, we propose the first deep unsupervised image retrieval method dubbed Self-supervised Product Quantization (SPQ) network, which is label-free and trained in a self-supervised manner. We design a Cross Quantized Contrastive learning strategy that jointly learns codewords and deep visual descriptors by comparing individually transformed images (views). Our method analyzes the image contents to extract descriptive features, allowing us to understand image representations for accurate retrieval. By conducting extensive experiments on benchmarks, we demonstrate that the proposed method yields state-of-the-art results even without supervised pretraining.



### Deep Convolutional Generative Modeling for Artificial Microstructure Development of Aluminum-Silicon Alloy
- **Arxiv ID**: http://arxiv.org/abs/2109.06635v1
- **DOI**: 10.35940/ijdm.A1603.051121
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.06635v1)
- **Published**: 2021-09-06 05:59:06+00:00
- **Updated**: 2021-09-06 05:59:06+00:00
- **Authors**: Akshansh Mishra, Tarushi Pathak
- **Comment**: None
- **Journal**: Indian Journal of Data Mining (2021)
- **Summary**: Machine learning which is a sub-domain of an Artificial Intelligence which is finding various applications in manufacturing and material science sectors. In the present study, Deep Generative Modeling which a type of unsupervised machine learning technique has been adapted for the constructing the artificial microstructure of Aluminium-Silicon alloy. Deep Generative Adversarial Networks has been used for developing the artificial microstructure of the given microstructure image dataset. The results obtained showed that the developed models had learnt to replicate the lining near the certain images of the microstructures.



### Generative Models Improve Radiomics Performance in Different Tasks and Different Datasets: An Experimental Study
- **Arxiv ID**: http://arxiv.org/abs/2109.02252v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02252v1)
- **Published**: 2021-09-06 06:01:21+00:00
- **Updated**: 2021-09-06 06:01:21+00:00
- **Authors**: Junhua Chen, Inigo Bermejo, Andre Dekker, Leonard Wee
- **Comment**: None
- **Journal**: None
- **Summary**: Radiomics is an active area of research focusing on high throughput feature extraction from medical images with a wide array of applications in clinical practice, such as clinical decision support in oncology. However, noise in low dose computed tomography (CT) scans can impair the accurate extraction of radiomic features. In this article, we investigate the possibility of using deep learning generative models to improve the performance of radiomics from low dose CTs. We used two datasets of low dose CT scans -NSCLC Radiogenomics and LIDC-IDRI - as test datasets for two tasks - pre-treatment survival prediction and lung cancer diagnosis. We used encoder-decoder networks and conditional generative adversarial networks (CGANs) trained in a previous study as generative models to transform low dose CT images into full dose CT images. Radiomic features extracted from the original and improved CT scans were used to build two classifiers - a support vector machine (SVM) and a deep attention based multiple instance learning model - for survival prediction and lung cancer diagnosis respectively. Finally, we compared the performance of the models derived from the original and improved CT scans. Encoder-decoder networks and CGANs improved the area under the curve (AUC) of survival prediction from 0.52 to 0.57 (p-value<0.01). On the other hand, Encoder-decoder network and CGAN can improve the AUC of lung cancer diagnosis from 0.84 to 0.88 and 0.89 respectively (p-value<0.01). Moreover, there are no statistically significant differences in improving AUC by using encoder-decoder network and CGAN (p-value=0.34) when networks trained at 75 and 100 epochs. Generative models can improve the performance of low dose CT-based radiomics in different tasks. Hence, denoising using generative models seems to be a necessary pre-processing step for calculating radiomic features from low dose CTs.



### CTRL-C: Camera calibration TRansformer with Line-Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.02259v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02259v1)
- **Published**: 2021-09-06 06:30:38+00:00
- **Updated**: 2021-09-06 06:30:38+00:00
- **Authors**: Jinwoo Lee, Hyunsung Go, Hyunjoon Lee, Sunghyun Cho, Minhyuk Sung, Junho Kim
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Single image camera calibration is the task of estimating the camera parameters from a single input image, such as the vanishing points, focal length, and horizon line. In this work, we propose Camera calibration TRansformer with Line-Classification (CTRL-C), an end-to-end neural network-based approach to single image camera calibration, which directly estimates the camera parameters from an image and a set of line segments. Our network adopts the transformer architecture to capture the global structure of an image with multi-modal inputs in an end-to-end manner. We also propose an auxiliary task of line classification to train the network to extract the global geometric information from lines effectively. Our experiments demonstrate that CTRL-C outperforms the previous state-of-the-art methods on the Google Street View and SUN360 benchmark datasets.



### Exploiting Spatial-Temporal Semantic Consistency for Video Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/2109.02281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02281v1)
- **Published**: 2021-09-06 08:24:38+00:00
- **Updated**: 2021-09-06 08:24:38+00:00
- **Authors**: Xingjian He, Weining Wang, Zhiyong Xu, Hao Wang, Jie Jiang, Jing Liu
- **Comment**: 1st Place technical report for "The 1st Video Scene Parsing in the
  Wild Challenge" at ICCV2021
- **Journal**: None
- **Summary**: Compared with image scene parsing, video scene parsing introduces temporal information, which can effectively improve the consistency and accuracy of prediction. In this paper, we propose a Spatial-Temporal Semantic Consistency method to capture class-exclusive context information. Specifically, we design a spatial-temporal consistency loss to constrain the semantic consistency in spatial and temporal dimensions. In addition, we adopt an pseudo-labeling strategy to enrich the training dataset. We obtain the scores of 59.84% and 58.85% mIoU on development (test part 1) and testing set of VSPW, respectively. And our method wins the 1st place on VSPW challenge at ICCV2021.



### Does Melania Trump have a body double from the perspective of automatic face recognition?
- **Arxiv ID**: http://arxiv.org/abs/2109.02283v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02283v1)
- **Published**: 2021-09-06 08:28:43+00:00
- **Updated**: 2021-09-06 08:28:43+00:00
- **Authors**: Khawla Mallat, Fabiola Becerra-Riera, Annette Morales-González, Heydi Méndez-Vázquez, Jean-Luc Dugelay
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore whether automatic face recognition can help in verifying widespread misinformation on social media, particularly conspiracy theories that are based on the existence of body doubles. The conspiracy theory addressed in this paper is the case of the Melania Trump body double. We employed four different state-of-the-art descriptors for face recognition to verify the integrity of the claim of the studied conspiracy theory. In addition, we assessed the impact of different image quality metrics on the variation of face recognition results. Two sets of image quality metrics were considered: acquisition-related metrics and subject-related metrics.



### Toward Realistic Single-View 3D Object Reconstruction with Unsupervised Learning from Multiple Images
- **Arxiv ID**: http://arxiv.org/abs/2109.02288v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02288v2)
- **Published**: 2021-09-06 08:34:04+00:00
- **Updated**: 2021-09-07 08:05:42+00:00
- **Authors**: Long-Nhat Ho, Anh Tuan Tran, Quynh Phung, Minh Hoai
- **Comment**: Accepted to the main ICCV 2021 conference
- **Journal**: None
- **Summary**: Recovering the 3D structure of an object from a single image is a challenging task due to its ill-posed nature. One approach is to utilize the plentiful photos of the same object category to learn a strong 3D shape prior for the object. This approach has successfully been demonstrated by a recent work of Wu et al. (2020), which obtained impressive 3D reconstruction networks with unsupervised learning. However, their algorithm is only applicable to symmetric objects. In this paper, we eliminate the symmetry requirement with a novel unsupervised algorithm that can learn a 3D reconstruction network from a multi-image dataset. Our algorithm is more general and covers the symmetry-required scenario as a special case. Besides, we employ a novel albedo loss that improves the reconstructed details and realisticity. Our method surpasses the previous work in both quality and robustness, as shown in experiments on datasets of various structures, including single-view, multi-view, image-collection, and video sets.



### Monitoring Indoor Activity of Daily Living Using Thermal Imaging: A Case Study
- **Arxiv ID**: http://arxiv.org/abs/2109.08672v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.08672v1)
- **Published**: 2021-09-06 08:55:09+00:00
- **Updated**: 2021-09-06 08:55:09+00:00
- **Authors**: Hassan M. Ahmed, Bessam Abdulrazak
- **Comment**: None
- **Journal**: None
- **Summary**: Monitoring indoor activities of daily living (ADLs) of a person is neither an easy nor an accurate process. It is subjected to dependency on sensor type, power supply stability, and connectivity stability without mentioning artifacts introduced by the person himself. Multiple challenges have to be overcome in this field, such as; monitoring the precise spatial location of the person, and estimating vital signs like an individuals average temperature. Privacy is another domain of the problem to be thought of with care. Identifying the persons posture without a camera is another challenge. Posture identification assists in the persons fall detection. Thermal imaging could be a proper solution for most of the mentioned challenges. It provides monitoring both the persons average temperature and spatial location while maintaining privacy. In this research, we propose an IoT system for monitoring an indoor ADL using thermal sensor array (TSA). Three classes of ADLs are introduced, which are daily activity, sleeping activity and no-activity respectively. Estimating person average temperature using TSAs is introduced as well in this paper. Results have shown that the three activity classes can be identified as well as the persons average temperature during day and night. The persons spatial location can be determined while his/her privacy is maintained as well.



### Encoder-decoder with Multi-level Attention for 3D Human Shape and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.02303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02303v1)
- **Published**: 2021-09-06 09:06:17+00:00
- **Updated**: 2021-09-06 09:06:17+00:00
- **Authors**: Ziniu Wan, Zhengjia Li, Maoqing Tian, Jianbo Liu, Shuai Yi, Hongsheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: 3D human shape and pose estimation is the essential task for human motion analysis, which is widely used in many 3D applications. However, existing methods cannot simultaneously capture the relations at multiple levels, including spatial-temporal level and human joint level. Therefore they fail to make accurate predictions in some hard scenarios when there is cluttered background, occlusion, or extreme pose. To this end, we propose Multi-level Attention Encoder-Decoder Network (MAED), including a Spatial-Temporal Encoder (STE) and a Kinematic Topology Decoder (KTD) to model multi-level attentions in a unified framework. STE consists of a series of cascaded blocks based on Multi-Head Self-Attention, and each block uses two parallel branches to learn spatial and temporal attention respectively. Meanwhile, KTD aims at modeling the joint level attention. It regards pose estimation as a top-down hierarchical process similar to SMPL kinematic tree. With the training set of 3DPW, MAED outperforms previous state-of-the-art methods by 6.2, 7.2, and 2.4 mm of PA-MPJPE on the three widely used benchmarks 3DPW, MPI-INF-3DHP, and Human3.6M respectively. Our code is available at https://github.com/ziniuwan/maed.



### Automatic Segmentation of the Optic Nerve Head Region in Optical Coherence Tomography: A Methodological Review
- **Arxiv ID**: http://arxiv.org/abs/2109.02322v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2109.02322v1)
- **Published**: 2021-09-06 09:45:57+00:00
- **Updated**: 2021-09-06 09:45:57+00:00
- **Authors**: Rita Marques, Danilo Andrade De Jesus, João Barbosa Breda, Jan Van Eijgen, Ingeborg Stalmans, Theo van Walsum, Stefan Klein, Pedro G. Vaz, Luisa Sánchez Brea
- **Comment**: None
- **Journal**: None
- **Summary**: The optic nerve head represents the intraocular section of the optic nerve (ONH), which is prone to damage by intraocular pressure. The advent of optical coherence tomography (OCT) has enabled the evaluation of novel optic nerve head parameters, namely the depth and curvature of the lamina cribrosa (LC). Together with the Bruch's membrane opening minimum-rim-width, these seem to be promising optic nerve head parameters for diagnosis and monitoring of retinal diseases such as glaucoma. Nonetheless, these optical coherence tomography derived biomarkers are mostly extracted through manual segmentation, which is time-consuming and prone to bias, thus limiting their usability in clinical practice. The automatic segmentation of optic nerve head in OCT scans could further improve the current clinical management of glaucoma and other diseases.   This review summarizes the current state-of-the-art in automatic segmentation of the ONH in OCT. PubMed and Scopus were used to perform a systematic review. Additional works from other databases (IEEE, Google Scholar and ARVO IOVS) were also included, resulting in a total of 27 reviewed studies.   For each algorithm, the methods, the size and type of dataset used for validation, and the respective results were carefully analyzed. The results show that deep learning-based algorithms provide the highest accuracy, sensitivity and specificity for segmenting the different structures of the ONH including the LC. However, a lack of consensus regarding the definition of segmented regions, extracted parameters and validation approaches has been observed, highlighting the importance and need of standardized methodologies for ONH segmentation.



### Automated Cardiac Resting Phase Detection Targeted on the Right Coronary Artery
- **Arxiv ID**: http://arxiv.org/abs/2109.02342v2
- **DOI**: 10.59275/j.melba.2023-afe2
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2109.02342v2)
- **Published**: 2021-09-06 10:29:52+00:00
- **Updated**: 2023-02-01 00:15:36+00:00
- **Authors**: Seung Su Yoon, Elisabeth Preuhs, Michaela Schmidt, Christoph Forman, Teodora Chitiboi, Puneet Sharma, Juliano Lara Fernandes, Christoph Tillmanns, Jens Wetzl, Andreas Maier
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2023:001
- **Journal**: Machine.Learning.for.Biomedical.Imaging. 2 (2023)
- **Summary**: Static cardiac imaging such as late gadolinium enhancement, mapping, or 3-D coronary angiography require prior information, e.g., the phase during a cardiac cycle with least motion, called resting phase (RP). The purpose of this work is to propose a fully automated framework that allows the detection of the right coronary artery (RCA) RP within CINE series. The proposed prototype system consists of three main steps. First, the localization of the regions of interest (ROI) is performed. Second, the cropped ROI series are taken for tracking motions over all time points. Third, the output motion values are used to classify RPs. In this work, we focused on the detection of the area with the outer edge of the cross-section of the RCA as our target. The proposed framework was evaluated on 102 clinically acquired dataset at 1.5T and 3T. The automatically classified RPs were compared with the reference RPs annotated manually by a expert for testing the robustness and feasibility of the framework. The predicted RCA RPs showed high agreement with the experts annotated RPs with 92.7% accuracy, 90.5% sensitivity and 95.0% specificity for the unseen study dataset. The mean absolute difference of the start and end RP was 13.6 $\pm$ 18.6 ms for the validation study dataset (n=102). In this work, automated RP detection has been introduced by the proposed framework and demonstrated feasibility, robustness, and applicability for static imaging acquisitions.



### Information Theory-Guided Heuristic Progressive Multi-View Coding
- **Arxiv ID**: http://arxiv.org/abs/2109.02344v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02344v2)
- **Published**: 2021-09-06 10:32:24+00:00
- **Updated**: 2023-08-22 03:55:42+00:00
- **Authors**: Jiangmeng Li, Wenwen Qiang, Hang Gao, Bing Su, Farid Razzak, Jie Hu, Changwen Zheng, Hui Xiong
- **Comment**: We have uploaded a new version of this paper in arXiv:2308.10522, so
  that we have to withdrawal this paper
- **Journal**: None
- **Summary**: Multi-view representation learning captures comprehensive information from multiple views of a shared context. Recent works intuitively apply contrastive learning (CL) to learn representations, regarded as a pairwise manner, which is still scalable: view-specific noise is not filtered in learning view-shared representations; the fake negative pairs, where the negative terms are actually within the same class as the positive, and the real negative pairs are coequally treated; and evenly measuring the similarities between terms might interfere with optimization. Importantly, few works research the theoretical framework of generalized self-supervised multi-view learning, especially for more than two views. To this end, we rethink the existing multi-view learning paradigm from the information theoretical perspective and then propose a novel information theoretical framework for generalized multi-view learning. Guided by it, we build a multi-view coding method with a three-tier progressive architecture, namely Information theory-guided heuristic Progressive Multi-view Coding (IPMC). In the distribution-tier, IPMC aligns the distribution between views to reduce view-specific noise. In the set-tier, IPMC builds self-adjusted pools for contrasting, which utilizes a view filter to adaptively modify the pools. Lastly, in the instance-tier, we adopt a designed unified loss to learn discriminative representations and reduce the gradient interference. Theoretically and empirically, we demonstrate the superiority of IPMC over state-of-the-art methods.



### Tensor Normalization and Full Distribution Training
- **Arxiv ID**: http://arxiv.org/abs/2109.02345v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02345v1)
- **Published**: 2021-09-06 10:33:17+00:00
- **Updated**: 2021-09-06 10:33:17+00:00
- **Authors**: Wolfgang Fuhl
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we introduce pixel wise tensor normalization, which is inserted after rectifier linear units and, together with batch normalization, provides a significant improvement in the accuracy of modern deep neural networks. In addition, this work deals with the robustness of networks. We show that the factorized superposition of images from the training set and the reformulation of the multi class problem into a multi-label problem yields significantly more robust networks. The reformulation and the adjustment of the multi class log loss also improves the results compared to the overlay with only one class as label. https://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p=%2FTNandFDT&mode=list



### F3: Fair and Federated Face Attribute Classification with Heterogeneous Data
- **Arxiv ID**: http://arxiv.org/abs/2109.02351v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2109.02351v3)
- **Published**: 2021-09-06 10:44:16+00:00
- **Updated**: 2022-06-24 04:33:30+00:00
- **Authors**: Samhita Kanaparthy, Manisha Padala, Sankarshan Damle, Ravi Kiran Sarvadevabhatla, Sujit Gujar
- **Comment**: This paper is accepted as 2-page extended abstract at CODS-COMAD 2022
  with title "Fair Federated Learning for Heterogeneous Face Data"
- **Journal**: None
- **Summary**: Fairness across different demographic groups is an essential criterion for face-related tasks, Face Attribute Classification (FAC) being a prominent example. Apart from this trend, Federated Learning (FL) is increasingly gaining traction as a scalable paradigm for distributed training. Existing FL approaches require data homogeneity to ensure fairness. However, this assumption is too restrictive in real-world settings. We propose F3, a novel FL framework for fair FAC under data heterogeneity. F3 adopts multiple heuristics to improve fairness across different demographic groups without requiring data homogeneity assumption. We demonstrate the efficacy of F3 by reporting empirically observed fairness measures and accuracy guarantees on popular face datasets. Our results suggest that F3 strikes a practical balance between accuracy and fairness for FAC.



### Fighting Selection Bias in Statistical Learning: Application to Visual Recognition from Biased Image Databases
- **Arxiv ID**: http://arxiv.org/abs/2109.02357v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2109.02357v2)
- **Published**: 2021-09-06 10:56:58+00:00
- **Updated**: 2022-11-01 15:11:58+00:00
- **Authors**: Stephan Clémençon, Pierre Laforgue, Robin Vogel
- **Comment**: None
- **Journal**: None
- **Summary**: In practice, and especially when training deep neural networks, visual recognition rules are often learned based on various sources of information. On the other hand, the recent deployment of facial recognition systems with uneven performances on different population segments has highlighted the representativeness issues induced by a naive aggregation of the datasets. In this paper, we show how biasing models can remedy these problems. Based on the (approximate) knowledge of the biasing mechanisms at work, our approach consists in reweighting the observations, so as to form a nearly debiased estimator of the target distribution. One key condition is that the supports of the biased distributions must partly overlap, and cover the support of the target distribution. In order to meet this requirement in practice, we propose to use a low dimensional image representation, shared across the image databases. Finally, we provide numerical experiments highlighting the relevance of our approach.



### Comparing the Machine Readability of Traffic Sign Pictograms in Austria and Germany
- **Arxiv ID**: http://arxiv.org/abs/2109.02362v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02362v1)
- **Published**: 2021-09-06 11:01:14+00:00
- **Updated**: 2021-09-06 11:01:14+00:00
- **Authors**: Alexander Maletzky, Stefan Thumfart, Christoph Wruß
- **Comment**: None
- **Journal**: None
- **Summary**: We compare the machine readability of pictograms found on Austrian and German traffic signs. To that end, we train classification models on synthetic data sets and evaluate their classification accuracy in a controlled setting. In particular, we focus on differences between currently deployed pictograms in the two countries, and a set of new pictograms designed to increase human readability. Besides other results, we find that machine-learning models generalize poorly to data sets with pictogram designs they have not been trained on. We conclude that manufacturers of advanced driver-assistance systems (ADAS) must take special care to properly address small visual differences between current and newly designed traffic sign pictograms, as well as between pictograms from different countries.



### Point-Based Neural Rendering with Per-View Optimization
- **Arxiv ID**: http://arxiv.org/abs/2109.02369v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2109.02369v2)
- **Published**: 2021-09-06 11:19:31+00:00
- **Updated**: 2021-09-08 08:46:43+00:00
- **Authors**: Georgios Kopanas, Julien Philip, Thomas Leimkühler, George Drettakis
- **Comment**: https://repo-sam.inria.fr/fungraph/differentiable-multi-view/
- **Journal**: In Computer Graphics Forum, Vol. 10. Wiley Online Library, 29-43
  (2021)
- **Summary**: There has recently been great interest in neural rendering methods. Some approaches use 3D geometry reconstructed with Multi-View Stereo (MVS) but cannot recover from the errors of this process, while others directly learn a volumetric neural representation, but suffer from expensive training and inference. We introduce a general approach that is initialized with MVS, but allows further optimization of scene properties in the space of input views, including depth and reprojected features, resulting in improved novel-view synthesis. A key element of our approach is our new differentiable point-based pipeline, based on bi-directional Elliptical Weighted Average splatting, a probabilistic depth test and effective camera selection. We use these elements together in our neural renderer, that outperforms all previous methods both in quality and speed in almost all scenes we tested. Our pipeline can be applied to multi-view harmonization and stylization in addition to novel-view synthesis.



### Improved RAMEN: Towards Domain Generalization for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2109.02370v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02370v1)
- **Published**: 2021-09-06 11:19:57+00:00
- **Updated**: 2021-09-06 11:19:57+00:00
- **Authors**: Bhanuka Manesha Samarasekara Vitharana Gamage, Lim Chern Hong
- **Comment**: 11 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: Currently nearing human-level performance, Visual Question Answering (VQA) is an emerging area in artificial intelligence.   Established as a multi-disciplinary field in machine learning, both computer vision and natural language processing communities are working together to achieve state-of-the-art (SOTA) performance.   However, there is a gap between the SOTA results and real world applications.   This is due to the lack of model generalisation.   The RAMEN model \cite{Shrestha2019} aimed to achieve domain generalization by obtaining the highest score across two main types of VQA datasets.   This study provides two major improvements to the early/late fusion module and aggregation module of the RAMEN architecture, with the objective of further strengthening domain generalization.   Vector operations based fusion strategies are introduced for the fusion module and the transformer architecture is introduced for the aggregation module.   Improvements of up to five VQA datasets from the experiments conducted are evident.   Following the results, this study analyses the effects of both the improvements on the domain generalization problem.   The code is available on GitHub though the following link \url{https://github.com/bhanukaManesha/ramen}.



### Robust Event Detection based on Spatio-Temporal Latent Action Unit using Skeletal Information
- **Arxiv ID**: http://arxiv.org/abs/2109.02376v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, I.5.1; I.5.2; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/2109.02376v2)
- **Published**: 2021-09-06 11:45:45+00:00
- **Updated**: 2021-10-01 10:03:22+00:00
- **Authors**: Hao Xing, Yuxuan Xue, Mingchuan Zhou, Darius Burschka
- **Comment**: 2021 IROS
- **Journal**: None
- **Summary**: This paper propose a novel dictionary learning approach to detect event action using skeletal information extracted from RGBD video. The event action is represented as several latent atoms and composed of latent spatial and temporal attributes. We perform the method at the example of fall event detection. The skeleton frames are clustered by an initial K-means method. Each skeleton frame is assigned with a varying weight parameter and fed into our Gradual Online Dictionary Learning (GODL) algorithm. During the training process, outlier frames will be gradually filtered by reducing the weight that is inversely proportional to a cost. In order to strictly distinguish the event action from similar actions and robustly acquire its action unit, we build a latent unit temporal structure for each sub-action. We evaluate the proposed method on parts of the NTURGB+D dataset, which includes 209 fall videos, 405 ground-lift videos, 420 sit-down videos, and 280 videos of 46 otheractions. We present the experimental validation of the achieved accuracy, recall and precision. Our approach achieves the bestperformance on precision and accuracy of human fall event detection, compared with other existing dictionary learning methods. With increasing noise ratio, our method remains the highest accuracy and the lowest variance.



### Less is More: Lighter and Faster Deep Neural Architecture for Tomato Leaf Disease Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.02394v2
- **DOI**: 10.1109/ACCESS.2022.3187203
- **Categories**: **cs.CV**, cs.LG, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2109.02394v2)
- **Published**: 2021-09-06 12:14:02+00:00
- **Updated**: 2022-07-04 08:42:39+00:00
- **Authors**: Sabbir Ahmed, Md. Bakhtiar Hasan, Tasnim Ahmed, Redwan Karim Sony, Md. Hasanul Kabir
- **Comment**: 18 pages, 13 figures, 5 tables, Accepted in IEEE Access
- **Journal**: None
- **Summary**: To ensure global food security and the overall profit of stakeholders, the importance of correctly detecting and classifying plant diseases is paramount. In this connection, the emergence of deep learning-based image classification has introduced a substantial number of solutions. However, the applicability of these solutions in low-end devices requires fast, accurate, and computationally inexpensive systems. This work proposes a lightweight transfer learning-based approach for detecting diseases from tomato leaves. It utilizes an effective preprocessing method to enhance the leaf images with illumination correction for improved classification. Our system extracts features using a combined model consisting of a pretrained MobileNetV2 architecture and a classifier network for effective prediction. Traditional augmentation approaches are replaced by runtime augmentation to avoid data leakage and address the class imbalance issue. Evaluation on tomato leaf images from the PlantVillage dataset shows that the proposed architecture achieves 99.30% accuracy with a model size of 9.60MB and 4.87M floating-point operations, making it a suitable choice for real-life applications in low-end devices. Our codes and models are available at https://github.com/redwankarimsony/project-tomato.



### A Decoupled Uncertainty Model for MRI Segmentation Quality Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.02413v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02413v1)
- **Published**: 2021-09-06 12:54:44+00:00
- **Updated**: 2021-09-06 12:54:44+00:00
- **Authors**: Richard Shaw, Carole H. Sudre, Sebastien Ourselin, M. Jorge Cardoso, Hugh G. Pemberton
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org. arXiv admin note:
  substantial text overlap with arXiv:2001.11927
- **Journal**: None
- **Summary**: Quality control (QC) of MR images is essential to ensure that downstream analyses such as segmentation can be performed successfully. Currently, QC is predominantly performed visually and subjectively, at significant time and operator cost. We aim to automate the process using a probabilistic network that estimates segmentation uncertainty through a heteroscedastic noise model, providing a measure of task-specific quality. By augmenting training images with k-space artefacts, we propose a novel CNN architecture to decouple sources of uncertainty related to the task and different k-space artefacts in a self-supervised manner. This enables the prediction of separate uncertainties for different types of data degradation. While the uncertainty predictions reflect the presence and severity of artefacts, the network provides more robust and generalisable segmentation predictions given the quality of the data. We show that models trained with artefact augmentation provide informative measures of uncertainty on both simulated artefacts and problematic real-world images identified by human raters, both qualitatively and quantitatively in the form of error bars on volume measurements. Relating artefact uncertainty to segmentation Dice scores, we observe that our uncertainty predictions provide a better estimate of MRI quality from the point of view of the task (gray matter segmentation) compared to commonly used metrics of quality including signal-to-noise ratio (SNR) and contrast-to-noise ratio (CNR), hence providing a real-time quality metric indicative of segmentation quality.



### Evaluation of Convolutional Neural Networks for COVID-19 Classification on Chest X-Rays
- **Arxiv ID**: http://arxiv.org/abs/2109.02415v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02415v1)
- **Published**: 2021-09-06 12:57:55+00:00
- **Updated**: 2021-09-06 12:57:55+00:00
- **Authors**: Felipe André Zeiser, Cristiano André da Costa, Gabriel de Oliveira Ramos, Henrique Bohn, Ismael Santos, Rodrigo da Rosa Righi
- **Comment**: None
- **Journal**: None
- **Summary**: Early identification of patients with COVID-19 is essential to enable adequate treatment and to reduce the burden on the health system. The gold standard for COVID-19 detection is the use of RT-PCR tests. However, due to the high demand for tests, these can take days or even weeks in some regions of Brazil. Thus, an alternative for detecting COVID-19 is the analysis of Digital Chest X-rays (XR). Changes due to COVID-19 can be detected in XR, even in asymptomatic patients. In this context, models based on deep learning have great potential to be used as support systems for diagnosis or as screening tools. In this paper, we propose the evaluation of convolutional neural networks to identify pneumonia due to COVID-19 in XR. The proposed methodology consists of a preprocessing step of the XR, data augmentation, and classification by the convolutional architectures DenseNet121, InceptionResNetV2, InceptionV3, MovileNetV2, ResNet50, and VGG16 pre-trained with the ImageNet dataset. The obtained results demonstrate that the VGG16 architecture obtained superior performance in the classification of XR for the evaluation metrics using the methodology proposed in this article. The obtained results for our methodology demonstrate that the VGG16 architecture presented a superior performance in the classification of XR, with an Accuracy of 85.11%, Sensitivity of 85.25%, Specificity of $85.16%, F1-score of $85.03%, and an AUC of 0.9758.



### Dual camera snapshot hyperspectral imaging system via physics informed learning
- **Arxiv ID**: http://arxiv.org/abs/2109.02643v2
- **DOI**: 10.1016/j.optlaseng.2022.107023
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2109.02643v2)
- **Published**: 2021-09-06 13:39:54+00:00
- **Updated**: 2021-11-17 09:26:46+00:00
- **Authors**: Hui Xie, Zhuang Zhao, Jing Han, Yi Zhang, Lianfa Bai, Jun Lu
- **Comment**: None
- **Journal**: None
- **Summary**: We consider using the system's optical imaging process with convolutional neural networks (CNNs) to solve the snapshot hyperspectral imaging reconstruction problem, which uses a dual-camera system to capture the three-dimensional hyperspectral images (HSIs) in a compressed way. Various methods using CNNs have been developed in recent years to reconstruct HSIs, but most of the supervised deep learning methods aimed to fit a brute-force mapping relationship between the captured compressed image and standard HSIs. Thus, the learned mapping would be invalid when the observation data deviate from the training data. Especially, we usually don't have ground truth in real-life scenarios. In this paper, we present a self-supervised dual-camera equipment with an untrained physics-informed CNNs framework. Extensive simulation and experimental results show that our method without training can be adapted to a wide imaging environment with good performance. Furthermore, compared with the training-based methods, our system can be constantly fine-tuned and self-improved in real-life scenarios.



### Active Learning for Automated Visual Inspection of Manufactured Products
- **Arxiv ID**: http://arxiv.org/abs/2109.02469v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02469v1)
- **Published**: 2021-09-06 13:44:25+00:00
- **Updated**: 2021-09-06 13:44:25+00:00
- **Authors**: Elena Trajkova, Jože M. Rožanec, Paulien Dam, Blaž Fortuna, Dunja Mladenić
- **Comment**: None
- **Journal**: None
- **Summary**: Quality control is a key activity performed by manufacturing enterprises to ensure products meet quality standards and avoid potential damage to the brand's reputation. The decreased cost of sensors and connectivity enabled an increasing digitalization of manufacturing. In addition, artificial intelligence enables higher degrees of automation, reducing overall costs and time required for defect inspection. In this research, we compare three active learning approaches and five machine learning algorithms applied to visual defect inspection with real-world data provided by Philips Consumer Lifestyle BV. Our results show that active learning reduces the data labeling effort without detriment to the models' performance.



### Voxel Transformer for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.02497v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02497v2)
- **Published**: 2021-09-06 14:10:22+00:00
- **Updated**: 2021-09-13 13:28:39+00:00
- **Authors**: Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang, Hang Xu, Chunjing Xu
- **Comment**: To appear at ICCV 2021
- **Journal**: None
- **Summary**: We present Voxel Transformer (VoTr), a novel and effective voxel-based Transformer backbone for 3D object detection from point clouds. Conventional 3D convolutional backbones in voxel-based 3D detectors cannot efficiently capture large context information, which is crucial for object recognition and localization, owing to the limited receptive fields. In this paper, we resolve the problem by introducing a Transformer-based architecture that enables long-range relationships between voxels by self-attention. Given the fact that non-empty voxels are naturally sparse but numerous, directly applying standard Transformer on voxels is non-trivial. To this end, we propose the sparse voxel module and the submanifold voxel module, which can operate on the empty and non-empty voxel positions effectively. To further enlarge the attention range while maintaining comparable computational overhead to the convolutional counterparts, we propose two attention mechanisms for multi-head attention in those two modules: Local Attention and Dilated Attention, and we further propose Fast Voxel Query to accelerate the querying process in multi-head attention. VoTr contains a series of sparse and submanifold voxel modules and can be applied in most voxel-based detectors. Our proposed VoTr shows consistent improvement over the convolutional baselines while maintaining computational efficiency on the KITTI dataset and the Waymo Open dataset.



### Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.02499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02499v1)
- **Published**: 2021-09-06 14:17:51+00:00
- **Updated**: 2021-09-06 14:17:51+00:00
- **Authors**: Jiageng Mao, Minzhe Niu, Haoyue Bai, Xiaodan Liang, Hang Xu, Chunjing Xu
- **Comment**: To appear at ICCV 2021
- **Journal**: None
- **Summary**: We present a flexible and high-performance framework, named Pyramid R-CNN, for two-stage 3D object detection from point clouds. Current approaches generally rely on the points or voxels of interest for RoI feature extraction on the second stage, but cannot effectively handle the sparsity and non-uniform distribution of those points, and this may result in failures in detecting objects that are far away. To resolve the problems, we propose a novel second-stage module, named pyramid RoI head, to adaptively learn the features from the sparse points of interest. The pyramid RoI head consists of three key components. Firstly, we propose the RoI-grid Pyramid, which mitigates the sparsity problem by extensively collecting points of interest for each RoI in a pyramid manner. Secondly, we propose RoI-grid Attention, a new operation that can encode richer information from sparse points by incorporating conventional attention-based and graph-based point operators into a unified formulation. Thirdly, we propose the Density-Aware Radius Prediction (DARP) module, which can adapt to different point density levels by dynamically adjusting the focusing range of RoIs. Combining the three components, our pyramid RoI head is robust to the sparse and imbalanced circumstances, and can be applied upon various 3D backbones to consistently boost the detection performance. Extensive experiments show that Pyramid R-CNN outperforms the state-of-the-art 3D detection models by a large margin on both the KITTI dataset and the Waymo Open dataset.



### Image In painting Applied to Art Completing Escher's Print Gallery
- **Arxiv ID**: http://arxiv.org/abs/2109.02536v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.02536v1)
- **Published**: 2021-09-06 15:24:38+00:00
- **Updated**: 2021-09-06 15:24:38+00:00
- **Authors**: Lucia Cipolina-Kun, Simone Caenazzo, Gaston Mazzei, Aditya Srinivas Menon
- **Comment**: This submission has been removed by arXiv administrators due to a
  copyright claim by a third party
- **Journal**: None
- **Summary**: This extended abstract presents the first stages of a research on in-painting suited for art reconstruction. We introduce M.C Eschers Print Gallery lithography as a use case example. This artwork presents a void on its center and additionally, it follows a challenging mathematical structure that needs to be preserved by the in-painting method. We present our work so far and our future line of research.



### 3D Human Texture Estimation from a Single Image with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2109.02563v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2109.02563v1)
- **Published**: 2021-09-06 16:00:20+00:00
- **Updated**: 2021-09-06 16:00:20+00:00
- **Authors**: Xiangyu Xu, Chen Change Loy
- **Comment**: ICCV 2021 Oral, Project: https://www.mmlab-ntu.com/project/texformer,
  Code: https://github.com/xuxy09/Texformer
- **Journal**: IEEE International Conference on Computer Vision, 2021
- **Summary**: We propose a Transformer-based framework for 3D human texture estimation from a single image. The proposed Transformer is able to effectively exploit the global information of the input image, overcoming the limitations of existing methods that are solely based on convolutional neural networks. In addition, we also propose a mask-fusion strategy to combine the advantages of the RGB-based and texture-flow-based models. We further introduce a part-style loss to help reconstruct high-fidelity colors without introducing unpleasant artifacts. Extensive experiments demonstrate the effectiveness of the proposed method against state-of-the-art 3D human texture estimation approaches both quantitatively and qualitatively.



### Ultra-high Resolution Image Segmentation via Locality-aware Context Fusion and Alternating Local Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2109.02580v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02580v3)
- **Published**: 2021-09-06 16:26:05+00:00
- **Updated**: 2022-12-06 09:44:06+00:00
- **Authors**: Wenxi Liu, Qi Li, Xindai Lin, Weixiang Yang, Shengfeng He, Yuanlong Yu
- **Comment**: Extension of ICCV 2021 "From Contexts to Locality: Ultra-high
  Resolution Image Segmentation via Locality-aware Contextual Correlation"
- **Journal**: None
- **Summary**: Ultra-high resolution image segmentation has raised increasing interests in recent years due to its realistic applications. In this paper, we innovate the widely used high-resolution image segmentation pipeline, in which an ultra-high resolution image is partitioned into regular patches for local segmentation and then the local results are merged into a high-resolution semantic mask. In particular, we introduce a novel locality-aware context fusion based segmentation model to process local patches, where the relevance between local patch and its various contexts are jointly and complementarily utilized to handle the semantic regions with large variations. Additionally, we present the alternating local enhancement module that restricts the negative impact of redundant information introduced from the contexts, and thus is endowed with the ability of fixing the locality-aware features to produce refined results. Furthermore, in comprehensive experiments, we demonstrate that our model outperforms other state-of-the-art methods in public benchmarks. Our released codes are available at: https://github.com/liqiokkk/FCtL.



### Class Semantics-based Attention for Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.02613v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02613v1)
- **Published**: 2021-09-06 17:22:46+00:00
- **Updated**: 2021-09-06 17:22:46+00:00
- **Authors**: Deepak Sridhar, Niamul Quader, Srikanth Muralidharan, Yaoxin Li, Peng Dai, Juwei Lu
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Action localization networks are often structured as a feature encoder sub-network and a localization sub-network, where the feature encoder learns to transform an input video to features that are useful for the localization sub-network to generate reliable action proposals. While some of the encoded features may be more useful for generating action proposals, prior action localization approaches do not include any attention mechanism that enables the localization sub-network to attend more to the more important features. In this paper, we propose a novel attention mechanism, the Class Semantics-based Attention (CSA), that learns from the temporal distribution of semantics of action classes present in an input video to find the importance scores of the encoded features, which are used to provide attention to the more useful encoded features. We demonstrate on two popular action detection datasets that incorporating our novel attention mechanism provides considerable performance gains on competitive action detection models (e.g., around 6.2% improvement over BMN action detection baseline to obtain 47.5% mAP on the THUMOS-14 dataset), and a new state-of-the-art of 36.25% mAP on the ActivityNet v1.3 dataset. Further, the CSA localization model family which includes BMN-CSA, was part of the second-placed submission at the 2021 ActivityNet action localization challenge. Our attention mechanism outperforms prior self-attention modules such as the squeeze-and-excitation in action detection task. We also observe that our attention mechanism is complementary to such self-attention modules in that performance improvements are seen when both are used together.



### The Animation Transformer: Visual Correspondence via Segment Matching
- **Arxiv ID**: http://arxiv.org/abs/2109.02614v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2109.02614v2)
- **Published**: 2021-09-06 17:23:40+00:00
- **Updated**: 2021-09-08 01:45:07+00:00
- **Authors**: Evan Casey, Víctor Pérez, Zhuoru Li, Harry Teitelman, Nick Boyajian, Tim Pulver, Mike Manh, William Grisaitis
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Visual correspondence is a fundamental building block on the way to building assistive tools for hand-drawn animation. However, while a large body of work has focused on learning visual correspondences at the pixel-level, few approaches have emerged to learn correspondence at the level of line enclosures (segments) that naturally occur in hand-drawn animation. Exploiting this structure in animation has numerous benefits: it avoids the intractable memory complexity of attending to individual pixels in high resolution images and enables the use of real-world animation datasets that contain correspondence information at the level of per-segment colors. To that end, we propose the Animation Transformer (AnT) which uses a transformer-based architecture to learn the spatial and visual relationships between segments across a sequence of images. AnT enables practical ML-assisted colorization for professional animation workflows and is publicly accessible as a creative tool in Cadmium.



### Bridging the Gap between Events and Frames through Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2109.02618v2
- **DOI**: 10.1109/LRA.2022.3145053
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02618v2)
- **Published**: 2021-09-06 17:31:37+00:00
- **Updated**: 2022-02-03 18:36:32+00:00
- **Authors**: Nico Messikommer, Daniel Gehrig, Mathias Gehrig, Davide Scaramuzza
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters (RA-L), 2022
- **Summary**: Reliable perception during fast motion maneuvers or in high dynamic range environments is crucial for robotic systems. Since event cameras are robust to these challenging conditions, they have great potential to increase the reliability of robot vision. However, event-based vision has been held back by the shortage of labeled datasets due to the novelty of event cameras. To overcome this drawback, we propose a task transfer method to train models directly with labeled images and unlabeled event data. Compared to previous approaches, (i) our method transfers from single images to events instead of high frame rate videos, and (ii) does not rely on paired sensor data. To achieve this, we leverage the generative event model to split event features into content and motion features. This split enables efficient matching between latent spaces for events and images, which is crucial for successful task transfer. Thus, our approach unlocks the vast amount of existing image datasets for the training of event-based neural networks. Our task transfer method consistently outperforms methods targeting Unsupervised Domain Adaptation for object detection by 0.26 mAP (increase by 93%) and classification by 2.7% accuracy.



### ERA: Entity Relationship Aware Video Summarization with Wasserstein GAN
- **Arxiv ID**: http://arxiv.org/abs/2109.02625v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02625v1)
- **Published**: 2021-09-06 17:46:59+00:00
- **Updated**: 2021-09-06 17:46:59+00:00
- **Authors**: Guande Wu, Jianzhe Lin, Claudio T. Silva
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Video summarization aims to simplify large scale video browsing by generating concise, short summaries that diver from but well represent the original video. Due to the scarcity of video annotations, recent progress for video summarization concentrates on unsupervised methods, among which the GAN based methods are most prevalent. This type of methods includes a summarizer and a discriminator. The summarized video from the summarizer will be assumed as the final output, only if the video reconstructed from this summary cannot be discriminated from the original one by the discriminator. The primary problems of this GAN based methods are two folds. First, the summarized video in this way is a subset of original video with low redundancy and contains high priority events/entities. This summarization criterion is not enough. Second, the training of the GAN framework is not stable. This paper proposes a novel Entity relationship Aware video summarization method (ERA) to address the above problems. To be more specific, we introduce an Adversarial Spatio Temporal network to construct the relationship among entities, which we think should also be given high priority in the summarization. The GAN training problem is solved by introducing the Wasserstein GAN and two newly proposed video patch/score sum losses. In addition, the score sum loss can also relieve the model sensitivity to the varying video lengths, which is an inherent problem for most current video analysis tasks. Our method substantially lifts the performance on the target benchmark datasets and exceeds the current leaderboard Rank 1 state of the art CSNet (2.1% F1 score increase on TVSum and 3.1% F1 score increase on SumMe). We hope our straightforward yet effective approach will shed some light on the future research of unsupervised video summarization.



### Rethinking Crowdsourcing Annotation: Partial Annotation with Salient Labels for Multi-Label Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.02688v1
- **DOI**: 10.1109/TGRS.2022.3191735
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02688v1)
- **Published**: 2021-09-06 18:28:53+00:00
- **Updated**: 2021-09-06 18:28:53+00:00
- **Authors**: Jianzhe Lin, Tianze Yu, Z. Jane Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Annotated images are required for both supervised model training and evaluation in image classification. Manually annotating images is arduous and expensive, especially for multi-labeled images. A recent trend for conducting such laboursome annotation tasks is through crowdsourcing, where images are annotated by volunteers or paid workers online (e.g., workers of Amazon Mechanical Turk) from scratch. However, the quality of crowdsourcing image annotations cannot be guaranteed, and incompleteness and incorrectness are two major concerns for crowdsourcing annotations. To address such concerns, we have a rethinking of crowdsourcing annotations: Our simple hypothesis is that if the annotators only partially annotate multi-label images with salient labels they are confident in, there will be fewer annotation errors and annotators will spend less time on uncertain labels. As a pleasant surprise, with the same annotation budget, we show a multi-label image classifier supervised by images with salient annotations can outperform models supervised by fully annotated images. Our method contributions are 2-fold: An active learning way is proposed to acquire salient labels for multi-label images; and a novel Adaptive Temperature Associated Model (ATAM) specifically using partial annotations is proposed for multi-label image classification. We conduct experiments on practical crowdsourcing data, the Open Street Map (OSM) dataset and benchmark dataset COCO 2014. When compared with state-of-the-art classification methods trained on fully annotated images, the proposed ATAM can achieve higher accuracy. The proposed idea is promising for crowdsourcing data annotation. Our code will be publicly available.



### Improving Transferability of Domain Adaptation Networks Through Domain Alignment Layers
- **Arxiv ID**: http://arxiv.org/abs/2109.02693v2
- **DOI**: 10.1109/SIBGRAPI54419.2021.00031
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02693v2)
- **Published**: 2021-09-06 18:41:19+00:00
- **Updated**: 2022-08-24 14:07:20+00:00
- **Authors**: Lucas Fernando Alvarenga e Silva, Daniel Carlos Guimarães Pedronette, Fábio Augusto Faria, João Paulo Papa, Jurandy Almeida
- **Comment**: None
- **Journal**: in 2021 34th SIBGRAPI Conference on Graphics, Patterns and Images
  (SIBGRAPI), 2021, pp. 168-175
- **Summary**: Deep learning (DL) has been the primary approach used in various computer vision tasks due to its relevant results achieved on many tasks. However, on real-world scenarios with partially or no labeled data, DL methods are also prone to the well-known domain shift problem. Multi-source unsupervised domain adaptation (MSDA) aims at learning a predictor for an unlabeled domain by assigning weak knowledge from a bag of source models. However, most works conduct domain adaptation leveraging only the extracted features and reducing their domain shift from the perspective of loss function designs. In this paper, we argue that it is not sufficient to handle domain shift only based on domain-level features, but it is also essential to align such information on the feature space. Unlike previous works, we focus on the network design and propose to embed Multi-Source version of DomaIn Alignment Layers (MS-DIAL) at different levels of the predictor. These layers are designed to match the feature distributions between different domains and can be easily applied to various MSDA methods. To show the robustness of our approach, we conducted an extensive experimental evaluation considering two challenging scenarios: digit recognition and object classification. The experimental results indicated that our approach can improve state-of-the-art MSDA methods, yielding relative gains of up to +30.64% on their classification accuracies.



### Intelligent Motion Planning for a Cost-effective Object Follower Mobile Robotic System with Obstacle Avoidance
- **Arxiv ID**: http://arxiv.org/abs/2109.02700v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, 68T40, I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2109.02700v1)
- **Published**: 2021-09-06 19:19:47+00:00
- **Updated**: 2021-09-06 19:19:47+00:00
- **Authors**: Sai Nikhil Gona, Prithvi Raj Bandhakavi
- **Comment**: 24 pages, 27 figures
- **Journal**: None
- **Summary**: There are few industries which use manually controlled robots for carrying material and this cannot be used all the time in all the places. So, it is very tranquil to have robots which can follow a specific human by following the unique coloured object held by that person. So, we propose a robotic system which uses robot vision and deep learning to get the required linear and angular velocities which are {\nu} and {\omega}, respectively. Which in turn makes the robot to avoid obstacles when following the unique coloured object held by the human. The novel methodology that we are proposing is accurate in detecting the position of the unique coloured object in any kind of lighting and tells us the horizontal pixel value where the robot is present and also tells if the object is close to or far from the robot. Moreover, the artificial neural networks that we have used in this problem gave us a meagre error in linear and angular velocity prediction and the PI controller which was used to control the linear and angular velocities, which in turn controls the position of the robot gave us impressive results and this methodology outperforms all other methodologies.



### Crash Report Data Analysis for Creating Scenario-Wise, Spatio-Temporal Attention Guidance to Support Computer Vision-based Perception of Fatal Crash Risks
- **Arxiv ID**: http://arxiv.org/abs/2109.02710v1
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02710v1)
- **Published**: 2021-09-06 19:43:37+00:00
- **Updated**: 2021-09-06 19:43:37+00:00
- **Authors**: Yu Li, Muhammad Monjurul Karim, Ruwen Qin
- **Comment**: 20 pages, 14 figures, submitted and accepted by Accident Analysis &
  Prevention
- **Journal**: journal={Accident Analysis \& Prevention}, volume={151},
  pages={105962}, year={2021}, publisher={Elsevier}
- **Summary**: Reducing traffic fatalities and serious injuries is a top priority of the US Department of Transportation. The computer vision (CV)-based crash anticipation in the near-crash phase is receiving growing attention. The ability to perceive fatal crash risks earlier is also critical because it will improve the reliability of crash anticipation. Yet, annotated image data for training a reliable AI model for the early visual perception of crash risks are not abundant. The Fatality Analysis Reporting System contains big data of fatal crashes. It is a reliable data source for learning the relationship between driving scene characteristics and fatal crashes to compensate for the limitation of CV. Therefore, this paper develops a data analytics model, named scenario-wise, Spatio-temporal attention guidance, from fatal crash report data, which can estimate the relevance of detected objects to fatal crashes from their environment and context information. First, the paper identifies five sparse variables that allow for decomposing the 5-year fatal crash dataset to develop scenario-wise attention guidance. Then, exploratory analysis of location- and time-related variables of the crash report data suggests reducing fatal crashes to spatially defined groups. The group's temporal pattern is an indicator of the similarity of fatal crashes in the group. Hierarchical clustering and K-means clustering merge the spatially defined groups into six clusters according to the similarity of their temporal patterns. After that, association rule mining discovers the statistical relationship between the temporal information of driving scenes with crash features, for each cluster. The paper shows how the developed attention guidance supports the design and implementation of a preliminary CV model that can identify objects of a possibility to involve in fatal crashes from their environment and context information.



### Graph Attention Layer Evolves Semantic Segmentation for Road Pothole Detection: A Benchmark and Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2109.02711v1
- **DOI**: 10.1109/TIP.2021.3112316
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.02711v1)
- **Published**: 2021-09-06 19:44:50+00:00
- **Updated**: 2021-09-06 19:44:50+00:00
- **Authors**: Rui Fan, Hengli Wang, Yuan Wang, Ming Liu, Ioannis Pitas
- **Comment**: accepted as a regular paper to IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Existing road pothole detection approaches can be classified as computer vision-based or machine learning-based. The former approaches typically employ 2-D image analysis/understanding or 3-D point cloud modeling and segmentation algorithms to detect road potholes from vision sensor data. The latter approaches generally address road pothole detection using convolutional neural networks (CNNs) in an end-to-end manner. However, road potholes are not necessarily ubiquitous and it is challenging to prepare a large well-annotated dataset for CNN training. In this regard, while computer vision-based methods were the mainstream research trend in the past decade, machine learning-based methods were merely discussed. Recently, we published the first stereo vision-based road pothole detection dataset and a novel disparity transformation algorithm, whereby the damaged and undamaged road areas can be highly distinguished. However, there are no benchmarks currently available for state-of-the-art (SoTA) CNNs trained using either disparity images or transformed disparity images. Therefore, in this paper, we first discuss the SoTA CNNs designed for semantic segmentation and evaluate their performance for road pothole detection with extensive experiments. Additionally, inspired by graph neural network (GNN), we propose a novel CNN layer, referred to as graph attention layer (GAL), which can be easily deployed in any existing CNN to optimize image feature representations for semantic segmentation. Our experiments compare GAL-DeepLabv3+, our best-performing implementation, with nine SoTA CNNs on three modalities of training data: RGB images, disparity images, and transformed disparity images. The experimental results suggest that our proposed GAL-DeepLabv3+ achieves the best overall pothole detection accuracy on all training data modalities.



### Vision Transformers For Weeds and Crops Classification Of High Resolution UAV Images
- **Arxiv ID**: http://arxiv.org/abs/2109.02716v2
- **DOI**: 10.3390/rs14030592
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02716v2)
- **Published**: 2021-09-06 19:58:54+00:00
- **Updated**: 2021-10-22 08:34:08+00:00
- **Authors**: Reenul Reedha, Eric Dericquebourg, Raphael Canals, Adel Hafiane
- **Comment**: Compared to the last version of the article, we have added some
  experiments, that is the evaluation of the performances by varying the number
  of samples in the test and train sets
- **Journal**: None
- **Summary**: Crop and weed monitoring is an important challenge for agriculture and food production nowadays. Thanks to recent advances in data acquisition and computation technologies, agriculture is evolving to a more smart and precision farming to meet with the high yield and high quality crop production. Classification and recognition in Unmanned Aerial Vehicles (UAV) images are important phases for crop monitoring. Advances in deep learning models relying on Convolutional Neural Network (CNN) have achieved high performances in image classification in the agricultural domain. Despite the success of this architecture, CNN still faces many challenges such as high computation cost, the need of large labelled datasets, ... Natural language processing's transformer architecture can be an alternative approach to deal with CNN's limitations. Making use of the self-attention paradigm, Vision Transformer (ViT) models can achieve competitive or better results without applying any convolution operations. In this paper, we adopt the self-attention mechanism via the ViT models for plant classification of weeds and crops: red beet, off-type beet (green leaves), parsley and spinach. Our experiments show that with small set of labelled training data, ViT models perform better compared to state-of-the-art CNN-based models EfficientNet and ResNet, with a top accuracy of 99.8\% achieved by the ViT model.



### Automatic Landmarks Correspondence Detection in Medical Images with an Application to Deformable Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2109.02722v2
- **DOI**: 10.1117/1.JMI.10.1.014007
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02722v2)
- **Published**: 2021-09-06 20:16:27+00:00
- **Updated**: 2023-02-21 14:00:34+00:00
- **Authors**: Monika Grewal, Jan Wiersma, Henrike Westerveld, Peter A. N. Bosman, Tanja Alderliesten
- **Comment**: accepted paper in the Journal of Medical Imaging
- **Journal**: Journal of Medical Imaging, 10, 2023, 014007
- **Summary**: Purpose: Deformable Image Registration (DIR) can benefit from additional guidance using corresponding landmarks in the images. However, the benefits thereof are largely understudied, especially due to the lack of automatic landmark detection methods for three-dimensional (3D) medical images.   Approach: We present a Deep Convolutional Neural Network (DCNN), called DCNN-Match, that learns to predict landmark correspondences in 3D images in a self-supervised manner. We trained DCNN-Match on pairs of Computed Tomography (CT) scans containing simulated deformations. We explored five variants of DCNN-Match that use different loss functions and assessed their effect on the spatial density of predicted landmarks and the associated matching errors. We also tested DCNN-Match variants in combination with the open-source registration software Elastix to assess the impact of predicted landmarks in providing additional guidance to DIR.   Results: We tested our approach on lower-abdominal CT scans from cervical cancer patients: 121 pairs containing simulated deformations and 11 pairs demonstrating clinical deformations. The results showed significant improvement in DIR performance when landmark correspondences predicted by DCNN-Match were used in the case of simulated (p = $0e^0$) as well as clinical deformations (p = 0.030). We also observed that the spatial density of the automatic landmarks with respect to the underlying deformation affect the extent of improvement in DIR. Finally, DCNN-Match was found to generalize to Magnetic Resonance Imaging (MRI) scans without requiring retraining, indicating easy applicability to other datasets.   Conclusions: DCNN-Match learns to predict landmark correspondences in 3D medical images in a self-supervised manner, which can improve DIR performance.



### Improving Dietary Assessment Via Integrated Hierarchy Food Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.02736v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02736v1)
- **Published**: 2021-09-06 20:59:58+00:00
- **Updated**: 2021-09-06 20:59:58+00:00
- **Authors**: Runyu Mao, Jiangpeng He, Luotao Lin, Zeman Shao, Heather A. Eicher-Miller, Fengqing Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Image-based dietary assessment refers to the process of determining what someone eats and how much energy and nutrients are consumed from visual data. Food classification is the first and most crucial step. Existing methods focus on improving accuracy measured by the rate of correct classification based on visual information alone, which is very challenging due to the high complexity and inter-class similarity of foods. Further, accuracy in food classification is conceptual as description of a food can always be improved. In this work, we introduce a new food classification framework to improve the quality of predictions by integrating the information from multiple domains while maintaining the classification accuracy. We apply a multi-task network based on a hierarchical structure that uses both visual and nutrition domain specific information to cluster similar foods. Our method is validated on the modified VIPER-FoodNet (VFN) food image dataset by including associated energy and nutrient information. We achieve comparable classification accuracy with existing methods that use visual information only, but with less error in terms of energy and nutrient values for the wrong predictions.



### Single-Camera 3D Head Fitting for Mixed Reality Clinical Applications
- **Arxiv ID**: http://arxiv.org/abs/2109.02740v2
- **DOI**: 10.1016/j.cviu.2022.103384
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02740v2)
- **Published**: 2021-09-06 21:03:52+00:00
- **Updated**: 2022-03-07 15:29:58+00:00
- **Authors**: Tejas Mane, Aylar Bayramova, Kostas Daniilidis, Philippos Mordohai, Elena Bernardis
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of estimating the shape of a person's head, defined as the geometry of the complete head surface, from a video taken with a single moving camera, and determining the alignment of the fitted 3D head for all video frames, irrespective of the person's pose. 3D head reconstructions commonly tend to focus on perfecting the face reconstruction, leaving the scalp to a statistical approximation. Our goal is to reconstruct the head model of each person to enable future mixed reality applications. To do this, we recover a dense 3D reconstruction and camera information via structure-from-motion and multi-view stereo. These are then used in a new two-stage fitting process to recover the 3D head shape by iteratively fitting a 3D morphable model of the head with the dense reconstruction in canonical space and fitting it to each person's head, using both traditional facial landmarks and scalp features extracted from the head's segmentation mask. Our approach recovers consistent geometry for varying head shapes, from videos taken by different people, with different smartphones, and in a variety of environments from living rooms to outdoor spaces.



### WhyAct: Identifying Action Reasons in Lifestyle Vlogs
- **Arxiv ID**: http://arxiv.org/abs/2109.02747v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.02747v2)
- **Published**: 2021-09-06 21:26:47+00:00
- **Updated**: 2021-09-09 16:17:02+00:00
- **Authors**: Oana Ignat, Santiago Castro, Hanwen Miao, Weiji Li, Rada Mihalcea
- **Comment**: Accepted at EMNLP 2021
- **Journal**: None
- **Summary**: We aim to automatically identify human action reasons in online videos. We focus on the widespread genre of lifestyle vlogs, in which people perform actions while verbally describing them. We introduce and make publicly available the WhyAct dataset, consisting of 1,077 visual actions manually annotated with their reasons. We describe a multimodal model that leverages visual and textual information to automatically infer the reasons corresponding to an action presented in the video.



### Zero-Shot Out-of-Distribution Detection Based on the Pre-trained Model CLIP
- **Arxiv ID**: http://arxiv.org/abs/2109.02748v3
- **DOI**: 10.1609/aaai.v36i6.20610
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02748v3)
- **Published**: 2021-09-06 21:27:43+00:00
- **Updated**: 2022-03-22 17:53:38+00:00
- **Authors**: Sepideh Esmaeilpour, Bing Liu, Eric Robertson, Lei Shu
- **Comment**: None
- **Journal**: None
- **Summary**: In an out-of-distribution (OOD) detection problem, samples of known classes(also called in-distribution classes) are used to train a special classifier. In testing, the classifier can (1) classify the test samples of known classes to their respective classes and also (2) detect samples that do not belong to any of the known classes (i.e., they belong to some unknown or OOD classes). This paper studies the problem of zero-shot out-of-distribution(OOD) detection, which still performs the same two tasks in testing but has no training except using the given known class names. This paper proposes a novel yet simple method (called ZOC) to solve the problem. ZOC builds on top of the recent advances in zero-shot classification through multi-modal representation learning. It first extends the pre-trained language-vision model CLIP by training a text-based image description generator on top of CLIP. In testing, it uses the extended model to generate candidate unknown class names for each test sample and computes a confidence score based on both the known class names and candidate unknown class names for zero-shot OOD detection. Experimental results on 5 benchmark datasets for OOD detection demonstrate that ZOC outperforms the baselines by a large margin.



### Pano3D: A Holistic Benchmark and a Solid Baseline for $360^o$ Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.02749v1
- **DOI**: 10.1109/CVPRW53098.2021.00413
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02749v1)
- **Published**: 2021-09-06 21:30:03+00:00
- **Updated**: 2021-09-06 21:30:03+00:00
- **Authors**: Georgios Albanis, Nikolaos Zioulis, Petros Drakoulis, Vasileios Gkitsas, Vladimiros Sterzentsenko, Federico Alvarez, Dimitrios Zarpalas, Petros Daras
- **Comment**: Presented at the OmniCV CVPR 2021 workshop. Code, models, data and
  demo at https://vcl3d.github.io/Pano3D/
- **Journal**: None
- **Summary**: Pano3D is a new benchmark for depth estimation from spherical panoramas. It aims to assess performance across all depth estimation traits, the primary direct depth estimation performance targeting precision and accuracy, and also the secondary traits, boundary preservation, and smoothness. Moreover, Pano3D moves beyond typical intra-dataset evaluation to inter-dataset performance assessment. By disentangling the capacity to generalize to unseen data into different test splits, Pano3D represents a holistic benchmark for $360^o$ depth estimation. We use it as a basis for an extended analysis seeking to offer insights into classical choices for depth estimation. This results in a solid baseline for panoramic depth that follow-up works can build upon to steer future progress.



### Training Deep Networks from Zero to Hero: avoiding pitfalls and going beyond
- **Arxiv ID**: http://arxiv.org/abs/2109.02752v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02752v2)
- **Published**: 2021-09-06 21:31:42+00:00
- **Updated**: 2021-10-13 12:51:50+00:00
- **Authors**: Moacir Antonelli Ponti, Fernando Pereira dos Santos, Leo Sampaio Ferraz Ribeiro, Gabriel Biscaro Cavallari
- **Comment**: Extended version of SIBGRAPI 2021 Tutorial Paper
- **Journal**: None
- **Summary**: Training deep neural networks may be challenging in real world data. Using models as black-boxes, even with transfer learning, can result in poor generalization or inconclusive results when it comes to small datasets or specific applications. This tutorial covers the basic steps as well as more recent options to improve models, in particular, but not restricted to, supervised learning. It can be particularly useful in datasets that are not as well-prepared as those in challenges, and also under scarce annotation and/or small data. We describe basic procedures: as data preparation, optimization and transfer learning, but also recent architectural choices such as use of transformer modules, alternative convolutional layers, activation functions, wide and deep networks, as well as training procedures including as curriculum, contrastive and self-supervised learning.



### STRIVE: Scene Text Replacement In Videos
- **Arxiv ID**: http://arxiv.org/abs/2109.02762v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02762v1)
- **Published**: 2021-09-06 22:21:40+00:00
- **Updated**: 2021-09-06 22:21:40+00:00
- **Authors**: Vijay Kumar B G, Jeyasri Subramanian, Varnith Chordia, Eugene Bart, Shaobo Fang, Kelly Guan, Raja Bala
- **Comment**: ICCV 2021, Project Page:
  https://striveiccv2021.github.io/STRIVE-ICCV2021/
- **Journal**: None
- **Summary**: We propose replacing scene text in videos using deep style transfer and learned photometric transformations.Building on recent progress on still image text replacement,we present extensions that alter text while preserving the appearance and motion characteristics of the original video.Compared to the problem of still image text replacement,our method addresses additional challenges introduced by video, namely effects induced by changing lighting, motion blur, diverse variations in camera-object pose over time,and preservation of temporal consistency. We parse the problem into three steps. First, the text in all frames is normalized to a frontal pose using a spatio-temporal trans-former network. Second, the text is replaced in a single reference frame using a state-of-art still-image text replacement method. Finally, the new text is transferred from the reference to remaining frames using a novel learned image transformation network that captures lighting and blur effects in a temporally consistent manner. Results on synthetic and challenging real videos show realistic text trans-fer, competitive quantitative and qualitative performance,and superior inference speed relative to alternatives. We introduce new synthetic and real-world datasets with paired text objects. To the best of our knowledge this is the first attempt at deep video text replacement.



### Binaural SoundNet: Predicting Semantics, Depth and Motion with Binaural Sounds
- **Arxiv ID**: http://arxiv.org/abs/2109.02763v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2109.02763v2)
- **Published**: 2021-09-06 22:24:00+00:00
- **Updated**: 2022-02-27 13:30:29+00:00
- **Authors**: Dengxin Dai, Arun Balajee Vasudevan, Jiri Matas, Luc Van Gool
- **Comment**: Accepted by TPAMI. arXiv admin note: substantial text overlap with
  arXiv:2003.04210
- **Journal**: None
- **Summary**: Humans can robustly recognize and localize objects by using visual and/or auditory cues. While machines are able to do the same with visual data already, less work has been done with sounds. This work develops an approach for scene understanding purely based on binaural sounds. The considered tasks include predicting the semantic masks of sound-making objects, the motion of sound-making objects, and the depth map of the scene. To this aim, we propose a novel sensor setup and record a new audio-visual dataset of street scenes with eight professional binaural microphones and a 360-degree camera. The co-existence of visual and audio cues is leveraged for supervision transfer. In particular, we employ a cross-modal distillation framework that consists of multiple vision teacher methods and a sound student method -- the student method is trained to generate the same results as the teacher methods do. This way, the auditory system can be trained without using human annotations. To further boost the performance, we propose another novel auxiliary task, coined Spatial Sound Super-Resolution, to increase the directional resolution of sounds. We then formulate the four tasks into one end-to-end trainable multi-tasking network aiming to boost the overall performance. Experimental results show that 1) our method achieves good results for all four tasks, 2) the four tasks are mutually beneficial -- training them together achieves the best performance, 3) the number and orientation of microphones are both important, and 4) features learned from the standard spectrogram and features obtained by the classic signal processing pipeline are complementary for auditory perception tasks. The data and code are released.



### Robustness and Generalization via Generative Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2109.02765v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02765v1)
- **Published**: 2021-09-06 22:34:04+00:00
- **Updated**: 2021-09-06 22:34:04+00:00
- **Authors**: Omid Poursaeed, Tianxing Jiang, Harry Yang, Serge Belongie, SerNam Lim
- **Comment**: ICCV 2021. arXiv admin note: substantial text overlap with
  arXiv:1911.09058
- **Journal**: None
- **Summary**: While deep neural networks have achieved remarkable success in various computer vision tasks, they often fail to generalize to new domains and subtle variations of input images. Several defenses have been proposed to improve the robustness against these variations. However, current defenses can only withstand the specific attack used in training, and the models often remain vulnerable to other input variations. Moreover, these methods often degrade performance of the model on clean images and do not generalize to out-of-domain samples. In this paper we present Generative Adversarial Training, an approach to simultaneously improve the model's generalization to the test set and out-of-domain samples as well as its robustness to unseen adversarial attacks. Instead of altering a low-level pre-defined aspect of images, we generate a spectrum of low-level, mid-level and high-level changes using generative models with a disentangled latent space. Adversarial training with these examples enable the model to withstand a wide range of attacks by observing a variety of input alterations during training. We show that our approach not only improves performance of the model on clean images and out-of-domain samples but also makes it robust against unforeseen attacks and outperforms prior work. We validate effectiveness of our method by demonstrating results on various tasks such as classification, segmentation and object detection.



### Deep SIMBAD: Active Landmark-based Self-localization Using Ranking -based Scene Descriptor
- **Arxiv ID**: http://arxiv.org/abs/2109.02786v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02786v1)
- **Published**: 2021-09-06 23:51:27+00:00
- **Updated**: 2021-09-06 23:51:27+00:00
- **Authors**: Tanaka Kanji
- **Comment**: 6 pages, 7 figures, a preprint
- **Journal**: None
- **Summary**: Landmark-based robot self-localization has recently garnered interest as a highly-compressive domain-invariant approach for performing visual place recognition (VPR) across domains (e.g., time of day, weather, and season). However, landmark-based self-localization can be an ill-posed problem for a passive observer (e.g., manual robot control), as many viewpoints may not provide an effective landmark view. In this study, we consider an active self-localization task by an active observer and present a novel reinforcement learning (RL)-based next-best-view (NBV) planner. Our contributions are as follows. (1) SIMBAD-based VPR: We formulate the problem of landmark-based compact scene description as SIMBAD (similarity-based pattern recognition) and further present its deep learning extension. (2) VPR-to-NBV knowledge transfer: We address the challenge of RL under uncertainty (i.e., active self-localization) by transferring the state recognition ability of VPR to the NBV. (3) NNQL-based NBV: We regard the available VPR as the experience database by adapting nearest-neighbor approximation of Q-learning (NNQL). The result shows an extremely compact data structure that compresses both the VPR and NBV into a single incremental inverted index. Experiments using the public NCLT dataset validated the effectiveness of the proposed approach.



