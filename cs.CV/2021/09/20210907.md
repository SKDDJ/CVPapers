# Arxiv Papers in cs.CV on 2021-09-07
### Deep Collaborative Multi-Modal Learning for Unsupervised Kinship Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.02804v1
- **DOI**: 10.1109/TIFS.2021.3098165
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02804v1)
- **Published**: 2021-09-07 01:34:51+00:00
- **Updated**: 2021-09-07 01:34:51+00:00
- **Authors**: Guan-Nan Dong, Chi-Man Pun, Zheng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Kinship verification is a long-standing research challenge in computer vision. The visual differences presented to the face have a significant effect on the recognition capabilities of the kinship systems. We argue that aggregating multiple visual knowledge can better describe the characteristics of the subject for precise kinship identification. Typically, the age-invariant features can represent more natural facial details. Such age-related transformations are essential for face recognition due to the biological effects of aging. However, the existing methods mainly focus on employing the single-view image features for kinship identification, while more meaningful visual properties such as race and age are directly ignored in the feature learning step. To this end, we propose a novel deep collaborative multi-modal learning (DCML) to integrate the underlying information presented in facial properties in an adaptive manner to strengthen the facial details for effective unsupervised kinship verification. Specifically, we construct a well-designed adaptive feature fusion mechanism, which can jointly leverage the complementary properties from different visual perspectives to produce composite features and draw greater attention to the most informative components of spatial feature maps. Particularly, an adaptive weighting strategy is developed based on a novel attention mechanism, which can enhance the dependencies between different properties by decreasing the information redundancy in channels in a self-adaptive manner. To validate the effectiveness of the proposed method, extensive experimental evaluations conducted on four widely-used datasets show that our DCML method is always superior to some state-of-the-art kinship verification methods.



### Kinship Verification Based on Cross-Generation Feature Interaction Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.02809v1
- **DOI**: 10.1109/TIP.2021.3104192
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02809v1)
- **Published**: 2021-09-07 01:50:50+00:00
- **Updated**: 2021-09-07 01:50:50+00:00
- **Authors**: Guan-Nan Dong, Chi-Man Pun, Zheng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Kinship verification from facial images has been recognized as an emerging yet challenging technique in many potential computer vision applications. In this paper, we propose a novel cross-generation feature interaction learning (CFIL) framework for robust kinship verification. Particularly, an effective collaborative weighting strategy is constructed to explore the characteristics of cross-generation relations by corporately extracting features of both parents and children image pairs. Specifically, we take parents and children as a whole to extract the expressive local and non-local features. Different from the traditional works measuring similarity by distance, we interpolate the similarity calculations as the interior auxiliary weights into the deep CNN architecture to learn the whole and natural features. These similarity weights not only involve corresponding single points but also excavate the multiple relationships cross points, where local and non-local features are calculated by using these two kinds of distance measurements. Importantly, instead of separately conducting similarity computation and feature extraction, we integrate similarity learning and feature extraction into one unified learning process. The integrated representations deduced from local and non-local features can comprehensively express the informative semantics embedded in images and preserve abundant correlation knowledge from image pairs. Extensive experiments demonstrate the efficiency and superiority of the proposed model compared to some state-of-the-art kinship verification methods.



### Few-shot Learning via Dependency Maximization and Instance Discriminant Analysis
- **Arxiv ID**: http://arxiv.org/abs/2109.02820v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02820v1)
- **Published**: 2021-09-07 02:19:01+00:00
- **Updated**: 2021-09-07 02:19:01+00:00
- **Authors**: Zejiang Hou, Sun-Yuan Kung
- **Comment**: None
- **Journal**: None
- **Summary**: We study the few-shot learning (FSL) problem, where a model learns to recognize new objects with extremely few labeled training data per category. Most of previous FSL approaches resort to the meta-learning paradigm, where the model accumulates inductive bias through learning many training tasks so as to solve a new unseen few-shot task. In contrast, we propose a simple approach to exploit unlabeled data accompanying the few-shot task for improving few-shot performance. Firstly, we propose a Dependency Maximization method based on the Hilbert-Schmidt norm of the cross-covariance operator, which maximizes the statistical dependency between the embedded feature of those unlabeled data and their label predictions, together with the supervised loss over the support set. We then use the obtained model to infer the pseudo-labels for those unlabeled data. Furthermore, we propose anInstance Discriminant Analysis to evaluate the credibility of each pseudo-labeled example and select the most faithful ones into an augmented support set to retrain the model as in the first step. We iterate the above process until the pseudo-labels for the unlabeled data becomes stable. Following the standard transductive and semi-supervised FSL setting, our experiments show that the proposed method out-performs previous state-of-the-art methods on four widely used benchmarks, including mini-ImageNet, tiered-ImageNet, CUB, and CIFARFS.



### CIM: Class-Irrelevant Mapping for Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.02840v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02840v1)
- **Published**: 2021-09-07 03:26:24+00:00
- **Updated**: 2021-09-07 03:26:24+00:00
- **Authors**: Shuai Shao, Lei Xing, Yixin Chen, Yan-Jiang Wang, Bao-Di Liu, Yicong Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot classification (FSC) is one of the most concerned hot issues in recent years. The general setting consists of two phases: (1) Pre-train a feature extraction model (FEM) with base data (has large amounts of labeled samples). (2) Use the FEM to extract the features of novel data (with few labeled samples and totally different categories from base data), then classify them with the to-be-designed classifier. The adaptability of pre-trained FEM to novel data determines the accuracy of novel features, thereby affecting the final classification performances. To this end, how to appraise the pre-trained FEM is the most crucial focus in the FSC community. It sounds like traditional Class Activate Mapping (CAM) based methods can achieve this by overlaying weighted feature maps. However, due to the particularity of FSC (e.g., there is no backpropagation when using the pre-trained FEM to extract novel features), we cannot activate the feature map with the novel classes. To address this challenge, we propose a simple, flexible method, dubbed as Class-Irrelevant Mapping (CIM). Specifically, first, we introduce dictionary learning theory and view the channels of the feature map as the bases in a dictionary. Then we utilize the feature map to fit the feature vector of an image to achieve the corresponding channel weights. Finally, we overlap the weighted feature map for visualization to appraise the ability of pre-trained FEM on novel data. For fair use of CIM in evaluating different models, we propose a new measurement index, called Feature Localization Accuracy (FLA). In experiments, we first compare our CIM with CAM in regular tasks and achieve outstanding performances. Next, we use our CIM to appraise several classical FSC frameworks without considering the classification results and discuss them.



### Hierarchical Graph Convolutional Skeleton Transformer for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.02860v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.02860v4)
- **Published**: 2021-09-07 04:32:10+00:00
- **Updated**: 2022-01-10 11:02:07+00:00
- **Authors**: Ruwen Bai, Min Li, Bo Meng, Fengfa Li, Miao Jiang, Junxing Ren, Degang Sun
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: Graph convolutional networks (GCNs) have emerged as dominant methods for skeleton-based action recognition.   However, they still suffer from two problems, namely, neighborhood constraints and entangled spatiotemporal feature representations.   Most studies have focused on improving the design of graph topology to solve the first problem but they have yet to fully explore the latter.   In this work, we design a disentangled spatiotemporal transformer (DSTT) block to overcome the above limitations of GCNs in three steps: (i) feature disentanglement for spatiotemporal decomposition;(ii) global spatiotemporal attention for capturing correlations in the global context; and (iii) local information enhancement for utilizing more local information.   Thereon, we propose a novel architecture, named Hierarchical Graph Convolutional skeleton Transformer (HGCT), to employ the complementary advantages of GCN (i.e., local topology, temporal dynamics and hierarchy) and Transformer (i.e., global context and dynamic attention).   HGCT is lightweight and computationally efficient.   Quantitative analysis demonstrates the superiority and good interpretability of HGCT.



### Quantum-Classical Hybrid Machine Learning for Image Classification (ICCAD Special Session Paper)
- **Arxiv ID**: http://arxiv.org/abs/2109.02862v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02862v3)
- **Published**: 2021-09-07 04:37:38+00:00
- **Updated**: 2021-09-17 18:45:23+00:00
- **Authors**: Mahabubul Alam, Satwik Kundu, Rasit Onur Topaloglu, Swaroop Ghosh
- **Comment**: None
- **Journal**: None
- **Summary**: Image classification is a major application domain for conventional deep learning (DL). Quantum machine learning (QML) has the potential to revolutionize image classification. In any typical DL-based image classification, we use convolutional neural network (CNN) to extract features from the image and multi-layer perceptron network (MLP) to create the actual decision boundaries. On one hand, QML models can be useful in both of these tasks. Convolution with parameterized quantum circuits (Quanvolution) can extract rich features from the images. On the other hand, quantum neural network (QNN) models can create complex decision boundaries. Therefore, Quanvolution and QNN can be used to create an end-to-end QML model for image classification. Alternatively, we can extract image features separately using classical dimension reduction techniques such as, Principal Components Analysis (PCA) or Convolutional Autoencoder (CAE) and use the extracted features to train a QNN. We review two proposals on quantum-classical hybrid ML models for image classification namely, Quanvolutional Neural Network and dimension reduction using a classical algorithm followed by QNN. Particularly, we make a case for trainable filters in Quanvolution and CAE-based feature extraction for image datasets (instead of dimension reduction using linear transformations such as, PCA). We discuss various design choices, potential opportunities, and drawbacks of these models. We also release a Python-based framework to create and explore these hybrid models with a variety of design choices.



### Journalistic Guidelines Aware News Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2109.02865v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02865v2)
- **Published**: 2021-09-07 04:49:50+00:00
- **Updated**: 2021-09-10 18:16:21+00:00
- **Authors**: Xuewen Yang, Svebor Karaman, Joel Tetreault, Alex Jaimes
- **Comment**: None
- **Journal**: EMNLP 2021
- **Summary**: The task of news article image captioning aims to generate descriptive and informative captions for news article images. Unlike conventional image captions that simply describe the content of the image in general terms, news image captions follow journalistic guidelines and rely heavily on named entities to describe the image content, often drawing context from the whole article they are associated with. In this work, we propose a new approach to this task, motivated by caption guidelines that journalists follow. Our approach, Journalistic Guidelines Aware News Image Captioning (JoGANIC), leverages the structure of captions to improve the generation quality and guide our representation design. Experimental results, including detailed ablation studies, on two large-scale publicly available datasets show that JoGANIC substantially outperforms state-of-the-art methods both on caption generation and named entity related metrics.



### DeepFakes: Detecting Forged and Synthetic Media Content Using Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.02874v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2109.02874v1)
- **Published**: 2021-09-07 05:19:36+00:00
- **Updated**: 2021-09-07 05:19:36+00:00
- **Authors**: Sm Zobaed, Md Fazle Rabby, Md Istiaq Hossain, Ekram Hossain, Sazib Hasan, Asif Karim, Khan Md. Hasib
- **Comment**: A preprint version
- **Journal**: None
- **Summary**: The rapid advancement in deep learning makes the differentiation of authentic and manipulated facial images and video clips unprecedentedly harder. The underlying technology of manipulating facial appearances through deep generative approaches, enunciated as DeepFake that have emerged recently by promoting a vast number of malicious face manipulation applications. Subsequently, the need of other sort of techniques that can assess the integrity of digital visual content is indisputable to reduce the impact of the creations of DeepFake. A large body of research that are performed on DeepFake creation and detection create a scope of pushing each other beyond the current status. This study presents challenges, research trends, and directions related to DeepFake creation and detection techniques by reviewing the notable research in the DeepFake domain to facilitate the development of more robust approaches that could deal with the more advance DeepFake in the future.



### MRI Reconstruction Using Deep Energy-Based Model
- **Arxiv ID**: http://arxiv.org/abs/2109.03237v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.03237v2)
- **Published**: 2021-09-07 05:24:55+00:00
- **Updated**: 2021-09-09 12:32:30+00:00
- **Authors**: Yu Guan, Zongjiang Tu, Shanshan Wang, Qiegen Liu, Yuhao Wang, Dong Liang
- **Comment**: 36 pages, 9 figures
- **Journal**: None
- **Summary**: Purpose: Although recent deep energy-based generative models (EBMs) have shown encouraging results in many image generation tasks, how to take advantage of the self-adversarial cogitation in deep EBMs to boost the performance of Magnetic Resonance Imaging (MRI) reconstruction is still desired.   Methods: With the successful application of deep learning in a wide range of MRI reconstruction, a line of emerging research involves formulating an optimization-based reconstruction method in the space of a generative model. Leveraging this, a novel regularization strategy is introduced in this article which takes advantage of self-adversarial cogitation of the deep energy-based model. More precisely, we advocate for alternative learning a more powerful energy-based model with maximum likelihood estimation to obtain the deep energy-based information, represented as image prior. Simultaneously, implicit inference with Langevin dynamics is a unique property of re-construction. In contrast to other generative models for reconstruction, the proposed method utilizes deep energy-based information as the image prior in reconstruction to improve the quality of image.   Results: Experiment results that imply the proposed technique can obtain remarkable performance in terms of high reconstruction accuracy that is competitive with state-of-the-art methods, and does not suffer from mode collapse.   Conclusion: Algorithmically, an iterative approach was presented to strengthen EBM training with the gradient of energy network. The robustness and the reproducibility of the algorithm were also experimentally validated. More importantly, the proposed reconstruction framework can be generalized for most MRI reconstruction scenarios.



### Fine-grained Hand Gesture Recognition in Multi-viewpoint Hand Hygiene
- **Arxiv ID**: http://arxiv.org/abs/2109.02917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02917v1)
- **Published**: 2021-09-07 08:14:15+00:00
- **Updated**: 2021-09-07 08:14:15+00:00
- **Authors**: Huy Q. Vo, Tuong Do, Vi C. Pham, Duy Nguyen, An T. Duong, Quang D. Tran
- **Comment**: 6 pages, accepted for oral in IEEE SMC 2021
- **Journal**: None
- **Summary**: This paper contributes a new high-quality dataset for hand gesture recognition in hand hygiene systems, named "MFH". Generally, current datasets are not focused on: (i) fine-grained actions; and (ii) data mismatch between different viewpoints, which are available under realistic settings. To address the aforementioned issues, the MFH dataset is proposed to contain a total of 731147 samples obtained by different camera views in 6 non-overlapping locations. Additionally, each sample belongs to one of seven steps introduced by the World Health Organization (WHO). As a minor contribution, inspired by advances in fine-grained image recognition and distribution adaptation, this paper recommends using the self-supervised learning method to handle these preceding problems. The extensive experiments on the benchmarking MFH dataset show that the introduced method yields competitive performance in both the Accuracy and the Macro F1-score. The code and the MFH dataset are available at https://github.com/willogy-team/hand-gesture-recognition-smc2021.



### FDA: Feature Decomposition and Aggregation for Robust Airway Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.02920v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02920v1)
- **Published**: 2021-09-07 08:16:51+00:00
- **Updated**: 2021-09-07 08:16:51+00:00
- **Authors**: Minghui Zhang, Xin Yu, Hanxiao Zhang, Hao Zheng, Weihao Yu, Hong Pan, Xiangran Cai, Yun Gu
- **Comment**: Accepted at MICCAI2021-DART
- **Journal**: None
- **Summary**: 3D Convolutional Neural Networks (CNNs) have been widely adopted for airway segmentation. The performance of 3D CNNs is greatly influenced by the dataset while the public airway datasets are mainly clean CT scans with coarse annotation, thus difficult to be generalized to noisy CT scans (e.g. COVID-19 CT scans). In this work, we proposed a new dual-stream network to address the variability between the clean domain and noisy domain, which utilizes the clean CT scans and a small amount of labeled noisy CT scans for airway segmentation. We designed two different encoders to extract the transferable clean features and the unique noisy features separately, followed by two independent decoders. Further on, the transferable features are refined by the channel-wise feature recalibration and Signed Distance Map (SDM) regression. The feature recalibration module emphasizes critical features and the SDM pays more attention to the bronchi, which is beneficial to extracting the transferable topological features robust to the coarse labels. Extensive experimental results demonstrated the obvious improvement brought by our proposed method. Compared to other state-of-the-art transfer learning methods, our method accurately segmented more bronchi in the noisy CT scans.



### Learning to Combine the Modalities of Language and Video for Temporal Moment Localization
- **Arxiv ID**: http://arxiv.org/abs/2109.02925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02925v1)
- **Published**: 2021-09-07 08:25:45+00:00
- **Updated**: 2021-09-07 08:25:45+00:00
- **Authors**: Jungkyoo Shin, Jinyoung Moon
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal moment localization aims to retrieve the best video segment matching a moment specified by a query. The existing methods generate the visual and semantic embeddings independently and fuse them without full consideration of the long-term temporal relationship between them. To address these shortcomings, we introduce a novel recurrent unit, cross-modal long short-term memory (CM-LSTM), by mimicking the human cognitive process of localizing temporal moments that focuses on the part of a video segment related to the part of a query, and accumulates the contextual information across the entire video recurrently. In addition, we devise a two-stream attention mechanism for both attended and unattended video features by the input query to prevent necessary visual information from being neglected. To obtain more precise boundaries, we propose a two-stream attentive cross-modal interaction network (TACI) that generates two 2D proposal maps obtained globally from the integrated contextual features, which are generated by using CM-LSTM, and locally from boundary score sequences and then combines them into a final 2D map in an end-to-end manner. On the TML benchmark dataset, ActivityNet-Captions, the TACI outperform state-of-the-art TML methods with R@1 of 45.50% and 27.23% for IoU@0.5 and IoU@0.7, respectively. In addition, we show that the revised state-of-the-arts methods by replacing the original LSTM with our CM-LSTM achieve performance gains.



### Brand Label Albedo Extraction of eCommerce Products using Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2109.02929v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.02929v2)
- **Published**: 2021-09-07 08:30:15+00:00
- **Updated**: 2021-10-07 08:45:50+00:00
- **Authors**: Suman Sapkota, Manish Juneja, Laurynas Keleras, Pranav Kotwal, Binod Bhattarai
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: In this paper we present our solution to extract albedo of branded labels for e-commerce products. To this end, we generate a large-scale photo-realistic synthetic data set for albedo extraction followed by training a generative model to translate images with diverse lighting conditions to albedo. We performed an extensive evaluation to test the generalisation of our method to in-the-wild images. From the experimental results, we observe that our solution generalises well compared to the existing method both in the unseen rendered images as well as in the wild image.



### Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization
- **Arxiv ID**: http://arxiv.org/abs/2109.02934v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02934v3)
- **Published**: 2021-09-07 08:36:09+00:00
- **Updated**: 2022-06-01 14:37:01+00:00
- **Authors**: Alexandre Rame, Corentin Dancette, Matthieu Cord
- **Comment**: 31 pages, 14 tables, 7 figures
- **Journal**: ICML 2022
- **Summary**: Learning robust models that generalize well under changes in the data distribution is critical for real-world applications. To this end, there has been a growing surge of interest to learn simultaneously from multiple training domains - while enforcing different types of invariance across those domains. Yet, all existing approaches fail to show systematic benefits under controlled evaluation protocols. In this paper, we introduce a new regularization - named Fishr - that enforces domain invariance in the space of the gradients of the loss: specifically, the domain-level variances of gradients are matched across training domains. Our approach is based on the close relations between the gradient covariance, the Fisher Information and the Hessian of the loss: in particular, we show that Fishr eventually aligns the domain-level loss landscapes locally around the final weights. Extensive experiments demonstrate the effectiveness of Fishr for out-of-distribution generalization. Notably, Fishr improves the state of the art on the DomainBed benchmark and performs consistently better than Empirical Risk Minimization. Our code is available at https://github.com/alexrame/fishr.



### Sensor-Augmented Egocentric-Video Captioning with Dynamic Modal Attention
- **Arxiv ID**: http://arxiv.org/abs/2109.02955v1
- **DOI**: 10.1145/3474085.3475557
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02955v1)
- **Published**: 2021-09-07 09:22:09+00:00
- **Updated**: 2021-09-07 09:22:09+00:00
- **Authors**: Katsuyuki Nakamura, Hiroki Ohashi, Mitsuhiro Okada
- **Comment**: Accepted to ACM Multimedia (ACMMM) 2021
- **Journal**: None
- **Summary**: Automatically describing video, or video captioning, has been widely studied in the multimedia field. This paper proposes a new task of sensor-augmented egocentric-video captioning, a newly constructed dataset for it called MMAC Captions, and a method for the newly proposed task that effectively utilizes multi-modal data of video and motion sensors, or inertial measurement units (IMUs). While conventional video captioning tasks have difficulty in dealing with detailed descriptions of human activities due to the limited view of a fixed camera, egocentric vision has greater potential to be used for generating the finer-grained descriptions of human activities on the basis of a much closer view. In addition, we utilize wearable-sensor data as auxiliary information to mitigate the inherent problems in egocentric vision: motion blur, self-occlusion, and out-of-camera-range activities. We propose a method for effectively utilizing the sensor data in combination with the video data on the basis of an attention mechanism that dynamically determines the modality that requires more attention, taking the contextual information into account. We compared the proposed sensor-fusion method with strong baselines on the MMAC Captions dataset and found that using sensor data as supplementary information to the egocentric-video data was beneficial, and that our proposed method outperformed the strong baselines, demonstrating the effectiveness of the proposed method.



### CovarianceNet: Conditional Generative Model for Correct Covariance Prediction in Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2109.02965v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.02965v1)
- **Published**: 2021-09-07 09:38:24+00:00
- **Updated**: 2021-09-07 09:38:24+00:00
- **Authors**: Aleksey Postnikov, Aleksander Gamayunov, Gonzalo Ferrer
- **Comment**: None
- **Journal**: None
- **Summary**: The correct characterization of uncertainty when predicting human motion is equally important as the accuracy of this prediction. We present a new method to correctly predict the uncertainty associated with the predicted distribution of future trajectories. Our approach, CovariaceNet, is based on a Conditional Generative Model with Gaussian latent variables in order to predict the parameters of a bi-variate Gaussian distribution. The combination of CovarianceNet with a motion prediction model results in a hybrid approach that outputs a uni-modal distribution. We will show how some state of the art methods in motion prediction become overconfident when predicting uncertainty, according to our proposed metric and validated in the ETH data-set \cite{pellegrini2009you}. CovarianceNet correctly predicts uncertainty, which makes our method suitable for applications that use predicted distributions, e.g., planning or decision making.



### Efficient ADMM-based Algorithms for Convolutional Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/2109.02969v1
- **DOI**: 10.1109/LSP.2021.3135196
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.02969v1)
- **Published**: 2021-09-07 09:49:10+00:00
- **Updated**: 2021-09-07 09:49:10+00:00
- **Authors**: Farshad G. Veshki, Sergiy A. Vorobyov
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional sparse coding improves on the standard sparse approximation by incorporating a global shift-invariant model. The most efficient convolutional sparse coding methods are based on the alternating direction method of multipliers and the convolution theorem. The only major difference between these methods is how they approach a convolutional least-squares fitting subproblem. This letter presents a solution to this subproblem, which improves the efficiency of the state-of-the-art algorithms. We also use the same approach for developing an efficient convolutional dictionary learning method. Furthermore, we propose a novel algorithm for convolutional sparse coding with a constraint on the approximation error.



### Unpaired Deep Image Deraining Using Dual Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.02973v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02973v4)
- **Published**: 2021-09-07 10:00:45+00:00
- **Updated**: 2022-03-24 05:29:25+00:00
- **Authors**: Xiang Chen, Jinshan Pan, Kui Jiang, Yufeng Li, Yufeng Huang, Caihua Kong, Longgang Dai, Zhentao Fan
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Learning single image deraining (SID) networks from an unpaired set of clean and rainy images is practical and valuable as acquiring paired real-world data is almost infeasible. However, without the paired data as the supervision, learning a SID network is challenging. Moreover, simply using existing unpaired learning methods (e.g., unpaired adversarial learning and cycle-consistency constraints) in the SID task is insufficient to learn the underlying relationship from rainy inputs to clean outputs as there exists significant domain gap between the rainy and clean images. In this paper, we develop an effective unpaired SID adversarial framework which explores mutual properties of the unpaired exemplars by a dual contrastive learning manner in a deep feature space, named as DCD-GAN. The proposed method mainly consists of two cooperative branches: Bidirectional Translation Branch (BTB) and Contrastive Guidance Branch (CGB). Specifically, BTB exploits full advantage of the circulatory architecture of adversarial consistency to generate abundant exemplar pairs and excavates latent feature distributions between two domains by equipping it with bidirectional mapping. Simultaneously, CGB implicitly constrains the embeddings of different exemplars in the deep feature space by encouraging the similar feature distributions closer while pushing the dissimilar further away, in order to better facilitate rain removal and help image restoration. Extensive experiments demonstrate that our method performs favorably against existing unpaired deraining approaches on both synthetic and real-world datasets, and generates comparable results against several fully-supervised or semi-supervised models.



### FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2109.02974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02974v1)
- **Published**: 2021-09-07 10:13:29+00:00
- **Updated**: 2021-09-07 10:13:29+00:00
- **Authors**: Rui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, Lewei Lu, Wenxiu Sun, Xiaogang Wang, Jifeng Dai, Hongsheng Li
- **Comment**: To appear at ICCV 2021
- **Journal**: None
- **Summary**: Transformer, as a strong and flexible architecture for modelling long-range relations, has been widely explored in vision tasks. However, when used in video inpainting that requires fine-grained representation, existed method still suffers from yielding blurry edges in detail due to the hard patch splitting. Here we aim to tackle this problem by proposing FuseFormer, a Transformer model designed for video inpainting via fine-grained feature fusion based on novel Soft Split and Soft Composition operations. The soft split divides feature map into many patches with given overlapping interval. On the contrary, the soft composition operates by stitching different patches into a whole feature map where pixels in overlapping regions are summed up. These two modules are first used in tokenization before Transformer layers and de-tokenization after Transformer layers, for effective mapping between tokens and features. Therefore, sub-patch level information interaction is enabled for more effective feature propagation between neighboring patches, resulting in synthesizing vivid content for hole regions in videos. Moreover, in FuseFormer, we elaborately insert the soft composition and soft split into the feed-forward network, enabling the 1D linear layers to have the capability of modelling 2D structure. And, the sub-patch level feature fusion ability is further enhanced. In both quantitative and qualitative evaluations, our proposed FuseFormer surpasses state-of-the-art methods. We also conduct detailed analysis to examine its superiority.



### Grassmannian Graph-attentional Landmark Selection for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2109.02990v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.02990v1)
- **Published**: 2021-09-07 10:50:28+00:00
- **Updated**: 2021-09-07 10:50:28+00:00
- **Authors**: Bin Sun, Shaofan Wang, Dehui Kong, Jinghua Li, Baocai Yin
- **Comment**: MTAP-R1,27 pages with 6 figures
- **Journal**: None
- **Summary**: Domain adaptation aims to leverage information from the source domain to improve the classification performance in the target domain. It mainly utilizes two schemes: sample reweighting and feature matching. While the first scheme allocates different weights to individual samples, the second scheme matches the feature of two domains using global structural statistics. The two schemes are complementary with each other, which are expected to jointly work for robust domain adaptation. Several methods combine the two schemes, but the underlying relationship of samples is insufficiently analyzed due to the neglect of the hierarchy of samples and the geometric properties between samples. To better combine the advantages of the two schemes, we propose a Grassmannian graph-attentional landmark selection (GGLS) framework for domain adaptation. GGLS presents a landmark selection scheme using attention-induced neighbors of the graphical structure of samples and performs distribution adaptation and knowledge adaptation over Grassmann manifold. the former treats the landmarks of each sample differently, and the latter avoids feature distortion and achieves better geometric properties. Experimental results on different real-world cross-domain visual recognition tasks demonstrate that GGLS provides better classification accuracies compared with state-of-the-art domain adaptation methods.



### Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors
- **Arxiv ID**: http://arxiv.org/abs/2109.02993v1
- **DOI**: 10.1145/3476099.3484315
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS, eess.IV, I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2109.02993v1)
- **Published**: 2021-09-07 11:00:20+00:00
- **Updated**: 2021-09-07 11:00:20+00:00
- **Authors**: Hasam Khalid, Minha Kim, Shahroz Tariq, Simon S. Woo
- **Comment**: 2 Figures, 2 Tables, Accepted for publication at the 1st Workshop on
  Synthetic Multimedia - Audiovisual Deepfake Generation and Detection (ADGD
  '21) at ACM MM 2021
- **Journal**: None
- **Summary**: Significant advancements made in the generation of deepfakes have caused security and privacy issues. Attackers can easily impersonate a person's identity in an image by replacing his face with the target person's face. Moreover, a new domain of cloning human voices using deep-learning technologies is also emerging. Now, an attacker can generate realistic cloned voices of humans using only a few seconds of audio of the target person. With the emerging threat of potential harm deepfakes can cause, researchers have proposed deepfake detection methods. However, they only focus on detecting a single modality, i.e., either video or audio. On the other hand, to develop a good deepfake detector that can cope with the recent advancements in deepfake generation, we need to have a detector that can detect deepfakes of multiple modalities, i.e., videos and audios. To build such a detector, we need a dataset that contains video and respective audio deepfakes. We were able to find a most recent deepfake dataset, Audio-Video Multimodal Deepfake Detection Dataset (FakeAVCeleb), that contains not only deepfake videos but synthesized fake audios as well. We used this multimodal deepfake dataset and performed detailed baseline experiments using state-of-the-art unimodal, ensemble-based, and multimodal detection methods to evaluate it. We conclude through detailed experimentation that unimodals, addressing only a single modality, video or audio, do not perform well compared to ensemble-based methods. Whereas purely multimodal-based baselines provide the worst performance.



### Knowledge Distillation Using Hierarchical Self-Supervision Augmented Distribution
- **Arxiv ID**: http://arxiv.org/abs/2109.03075v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03075v2)
- **Published**: 2021-09-07 13:29:32+00:00
- **Updated**: 2022-07-23 09:58:08+00:00
- **Authors**: Chuanguang Yang, Zhulin An, Linhang Cai, Yongjun Xu
- **Comment**: 15 pages, Accepted by IEEE Transactions on Neural Networks and
  Learning Systems 2022
- **Journal**: None
- **Summary**: Knowledge distillation (KD) is an effective framework that aims to transfer meaningful information from a large teacher to a smaller student. Generally, KD often involves how to define and transfer knowledge. Previous KD methods often focus on mining various forms of knowledge, for example, feature maps and refined information. However, the knowledge is derived from the primary supervised task and thus is highly task-specific. Motivated by the recent success of self-supervised representation learning, we propose an auxiliary self-supervision augmented task to guide networks to learn more meaningful features. Therefore, we can derive soft self-supervision augmented distributions as richer dark knowledge from this task for KD. Unlike previous knowledge, this distribution encodes joint knowledge from supervised and self-supervised feature learning. Beyond knowledge exploration, we propose to append several auxiliary branches at various hidden layers, to fully take advantage of hierarchical feature maps. Each auxiliary branch is guided to learn self-supervision augmented task and distill this distribution from teacher to student. Overall, we call our KD method as Hierarchical Self-Supervision Augmented Knowledge Distillation (HSSAKD). Experiments on standard image classification show that both offline and online HSSAKD achieves state-of-the-art performance in the field of KD. Further transfer experiments on object detection further verify that HSSAKD can guide the network to learn better features. The code is available at https://github.com/winycg/HSAKD.



### Support Vector Machine for Handwritten Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.03081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03081v1)
- **Published**: 2021-09-07 13:36:12+00:00
- **Updated**: 2021-09-07 13:36:12+00:00
- **Authors**: Jomy John
- **Comment**: 9 pages, KKTM Cognizance - A Multidisciplinary Journal, March 2016,
  ISSN:2456-4168
- **Journal**: None
- **Summary**: Handwriting recognition has been one of the most fascinating and challenging research areas in field of image processing and pattern recognition. It contributes enormously to the improvement of automation process. In this paper, a system for recognition of unconstrained handwritten Malayalam characters is proposed. A database of 10,000 character samples of 44 basic Malayalam characters is used in this work. A discriminate feature set of 64 local and 4 global features are used to train and test SVM classifier and achieved 92.24% accuracy



### Perceptual Learned Video Compression with Recurrent Conditional GAN
- **Arxiv ID**: http://arxiv.org/abs/2109.03082v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.03082v5)
- **Published**: 2021-09-07 13:36:57+00:00
- **Updated**: 2022-04-30 21:29:50+00:00
- **Authors**: Ren Yang, Radu Timofte, Luc Van Gool
- **Comment**: IJCAI 2022 camera ready
- **Journal**: None
- **Summary**: This paper proposes a Perceptual Learned Video Compression (PLVC) approach with recurrent conditional GAN. We employ the recurrent auto-encoder-based compression network as the generator, and most importantly, we propose a recurrent conditional discriminator, which judges on raw vs. compressed video conditioned on both spatial and temporal features, including the latent representation, temporal motion and hidden states in recurrent cells. This way, the adversarial training pushes the generated video to be not only spatially photo-realistic but also temporally consistent with the groundtruth and coherent among video frames. The experimental results show that the learned PLVC model compresses video with good perceptual quality at low bit-rate, and that it outperforms the official HEVC test model (HM 16.20) and the existing learned video compression approaches for several perceptual quality metrics and user studies. The codes will be released at the project page: https://github.com/RenYang-home/PLVC.



### Improving Phenotype Prediction using Long-Range Spatio-Temporal Dynamics of Functional Connectivity
- **Arxiv ID**: http://arxiv.org/abs/2109.03115v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.03115v1)
- **Published**: 2021-09-07 14:23:34+00:00
- **Updated**: 2021-09-07 14:23:34+00:00
- **Authors**: Simon Dahan, Logan Z. J. Williams, Daniel Rueckert, Emma C. Robinson
- **Comment**: MLCN 2021
- **Journal**: None
- **Summary**: The study of functional brain connectivity (FC) is important for understanding the underlying mechanisms of many psychiatric disorders. Many recent analyses adopt graph convolutional networks, to study non-linear interactions between functionally-correlated states. However, although patterns of brain activation are known to be hierarchically organised in both space and time, many methods have failed to extract powerful spatio-temporal features. To overcome those challenges, and improve understanding of long-range functional dynamics, we translate an approach, from the domain of skeleton-based action recognition, designed to model interactions across space and time. We evaluate this approach using the Human Connectome Project (HCP) dataset on sex classification and fluid intelligence prediction. To account for subject topographic variability of functional organisation, we modelled functional connectomes using multi-resolution dual-regressed (subject-specific) ICA nodes. Results show a prediction accuracy of 94.4% for sex classification (an increase of 6.2% compared to other methods), and an improvement of correlation with fluid intelligence of 0.325 vs 0.144, relative to a baseline model that encodes space and time separately. Results suggest that explicit encoding of spatio-temporal dynamics of brain functional activity may improve the precision with which behavioural and cognitive phenotypes may be predicted in the future.



### Smart Traffic Monitoring System using Computer Vision and Edge Computing
- **Arxiv ID**: http://arxiv.org/abs/2109.03141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03141v1)
- **Published**: 2021-09-07 15:15:55+00:00
- **Updated**: 2021-09-07 15:15:55+00:00
- **Authors**: Guanxiong Liu, Hang Shi, Abbas Kiani, Abdallah Khreishah, Jo Young Lee, Nirwan Ansari, Chengjun Liu, Mustafa Yousef
- **Comment**: None
- **Journal**: None
- **Summary**: Traffic management systems capture tremendous video data and leverage advances in video processing to detect and monitor traffic incidents. The collected data are traditionally forwarded to the traffic management center (TMC) for in-depth analysis and may thus exacerbate the network paths to the TMC. To alleviate such bottlenecks, we propose to utilize edge computing by equipping edge nodes that are close to cameras with computing resources (e.g. cloudlets). A cloudlet, with limited computing resources as compared to TMC, provides limited video processing capabilities. In this paper, we focus on two common traffic monitoring tasks, congestion detection, and speed detection, and propose a two-tier edge computing based model that takes into account of both the limited computing capability in cloudlets and the unstable network condition to the TMC. Our solution utilizes two algorithms for each task, one implemented at the edge and the other one at the TMC, which are designed with the consideration of different computing resources. While the TMC provides strong computation power, the video quality it receives depends on the underlying network conditions. On the other hand, the edge processes very high-quality video but with limited computing resources. Our model captures this trade-off. We evaluate the performance of the proposed two-tier model as well as the traffic monitoring algorithms via test-bed experiments under different weather as well as network conditions and show that our proposed hybrid edge-cloud solution outperforms both the cloud-only and edge-only solutions.



### PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System
- **Arxiv ID**: http://arxiv.org/abs/2109.03144v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03144v2)
- **Published**: 2021-09-07 15:24:40+00:00
- **Updated**: 2021-10-12 13:36:26+00:00
- **Authors**: Yuning Du, Chenxia Li, Ruoyu Guo, Cheng Cui, Weiwei Liu, Jun Zhou, Bin Lu, Yehua Yang, Qiwen Liu, Xiaoguang Hu, Dianhai Yu, Yanjun Ma
- **Comment**: 8 pages, 9 figures, 5 tables
- **Journal**: None
- **Summary**: Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.



### Fair Comparison: Quantifying Variance in Resultsfor Fine-grained Visual Categorization
- **Arxiv ID**: http://arxiv.org/abs/2109.03156v2
- **DOI**: 10.1109/WACV48630.2021.00335
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03156v2)
- **Published**: 2021-09-07 15:47:27+00:00
- **Updated**: 2021-09-08 01:23:28+00:00
- **Authors**: Matthew Gwilliam, Adam Teuscher, Connor Anderson, Ryan Farrell
- **Comment**: Accepted at WACV 2021; 8 pages text, 2 pages bib, 12 figures
- **Journal**: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV), January, 2021, pages 3309-3318
- **Summary**: For the task of image classification, researchers work arduously to develop the next state-of-the-art (SOTA) model, each bench-marking their own performance against that of their predecessors and of their peers. Unfortunately, the metric used most frequently to describe a model's performance, average categorization accuracy, is often used in isolation. As the number of classes increases, such as in fine-grained visual categorization (FGVC), the amount of information conveyed by average accuracy alone dwindles. While its most glaring weakness is its failure to describe the model's performance on a class-by-class basis, average accuracy also fails to describe how performance may vary from one trained model of the same architecture, on the same dataset, to another (both averaged across all categories and at the per-class level). We first demonstrate the magnitude of these variations across models and across class distributions based on attributes of the data, comparing results on different visual domains and different per-class image distributions, including long-tailed distributions and few-shot subsets. We then analyze the impact various FGVC methods have on overall and per-class variance. From this analysis, we both highlight the importance of reporting and comparing methods based on information beyond overall accuracy, as well as point out techniques that mitigate variance in FGVC results.



### nnFormer: Interleaved Transformer for Volumetric Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.03201v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03201v6)
- **Published**: 2021-09-07 17:08:24+00:00
- **Updated**: 2022-02-04 06:53:37+00:00
- **Authors**: Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Lequan Yu, Liansheng Wang, Yizhou Yu
- **Comment**: Journal version: more experiments and thorough comparison with
  nnUNet. Codes and models of nnFormer are available at https://git.io/JSf3i
- **Journal**: None
- **Summary**: Transformer, the model of choice for natural language processing, has drawn scant attention from the medical imaging community. Given the ability to exploit long-term dependencies, transformers are promising to help atypical convolutional neural networks to overcome their inherent shortcomings of spatial inductive bias. However, most of recently proposed transformer-based segmentation approaches simply treated transformers as assisted modules to help encode global context into convolutional representations. To address this issue, we introduce nnFormer, a 3D transformer for volumetric medical image segmentation. nnFormer not only exploits the combination of interleaved convolution and self-attention operations, but also introduces local and global volume-based self-attention mechanism to learn volume representations. Moreover, nnFormer proposes to use skip attention to replace the traditional concatenation/summation operations in skip connections in U-Net like architecture. Experiments show that nnFormer significantly outperforms previous transformer-based counterparts by large margins on three public datasets. Compared to nnUNet, nnFormer produces significantly lower HD95 and comparable DSC results. Furthermore, we show that nnFormer and nnUNet are highly complementary to each other in model ensembling.



### Learning Fast Sample Re-weighting Without Reward Data
- **Arxiv ID**: http://arxiv.org/abs/2109.03216v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.03216v1)
- **Published**: 2021-09-07 17:30:56+00:00
- **Updated**: 2021-09-07 17:30:56+00:00
- **Authors**: Zizhao Zhang, Tomas Pfister
- **Comment**: ICCV2021
- **Journal**: None
- **Summary**: Training sample re-weighting is an effective approach for tackling data biases such as imbalanced and corrupted labels. Recent methods develop learning-based algorithms to learn sample re-weighting strategies jointly with model training based on the frameworks of reinforcement learning and meta learning. However, depending on additional unbiased reward data is limiting their general applicability. Furthermore, existing learning-based sample re-weighting methods require nested optimizations of models and weighting parameters, which requires expensive second-order computation. This paper addresses these two problems and presents a novel learning-based fast sample re-weighting (FSR) method that does not require additional reward data. The method is based on two key ideas: learning from history to build proxy reward data and feature sharing to reduce the optimization cost. Our experiments show the proposed method achieves competitive results compared to state of the arts on label noise robustness and long-tailed recognition, and does so while achieving significantly improved training efficiency. The source code is publicly available at https://github.com/google-research/google-research/tree/master/ieg.



### Rendezvous: Attention Mechanisms for the Recognition of Surgical Action Triplets in Endoscopic Videos
- **Arxiv ID**: http://arxiv.org/abs/2109.03223v2
- **DOI**: 10.1016/j.media.2022.102433
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03223v2)
- **Published**: 2021-09-07 17:52:52+00:00
- **Updated**: 2022-03-03 18:53:35+00:00
- **Authors**: Chinedu Innocent Nwoye, Tong Yu, Cristians Gonzalez, Barbara Seeliger, Pietro Mascagni, Didier Mutter, Jacques Marescaux, Nicolas Padoy
- **Comment**: 21 pages, 11 figures, 19 tables, 1 video. Accepted at Elsevier
  Journal of Medical Image Analysis. Supplementary video available at:
  https://youtu.be/d_yHdJtCa98
- **Journal**: Medical Image Analysis (2022) 102433
- **Summary**: Out of all existing frameworks for surgical workflow analysis in endoscopic videos, action triplet recognition stands out as the only one aiming to provide truly fine-grained and comprehensive information on surgical activities. This information, presented as <instrument, verb, target> combinations, is highly challenging to be accurately identified. Triplet components can be difficult to recognize individually; in this task, it requires not only performing recognition simultaneously for all three triplet components, but also correctly establishing the data association between them. To achieve this task, we introduce our new model, the Rendezvous (RDV), which recognizes triplets directly from surgical videos by leveraging attention at two different levels. We first introduce a new form of spatial attention to capture individual action triplet components in a scene; called Class Activation Guided Attention Mechanism (CAGAM). This technique focuses on the recognition of verbs and targets using activations resulting from instruments. To solve the association problem, our RDV model adds a new form of semantic attention inspired by Transformer networks; called Multi-Head of Mixed Attention (MHMA). This technique uses several cross and self attentions to effectively capture relationships between instruments, verbs, and targets. We also introduce CholecT50 - a dataset of 50 endoscopic videos in which every frame has been annotated with labels from 100 triplet classes. Our proposed RDV model significantly improves the triplet prediction mean AP by over 9% compared to the state-of-the-art methods on this dataset.



### Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets
- **Arxiv ID**: http://arxiv.org/abs/2109.03229v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03229v4)
- **Published**: 2021-09-07 17:57:15+00:00
- **Updated**: 2021-10-05 02:18:14+00:00
- **Authors**: Matthew Gwilliam, Srinidhi Hegde, Lade Tinubu, Alex Hanson
- **Comment**: Accepted as an oral paper to the Human-centric Trustworthy Computer
  Vision (HTCV 2021) workshop at ICCV 2021; 17 pages, 9 figures, 6 tables
- **Journal**: None
- **Summary**: Many existing works have made great strides towards reducing racial bias in face recognition. However, most of these methods attempt to rectify bias that manifests in models during training instead of directly addressing a major source of the bias, the dataset itself. Exceptions to this are BUPT-Balancedface/RFW and Fairface, but these works assume that primarily training on a single race or not racially balancing the dataset are inherently disadvantageous. We demonstrate that these assumptions are not necessarily valid. In our experiments, training on only African faces induced less bias than training on a balanced distribution of faces and distributions skewed to include more African faces produced more equitable models. We additionally notice that adding more images of existing identities to a dataset in place of adding new identities can lead to accuracy boosts across racial categories. Our code is available at https://github.com/j-alex-hanson/rethinking-race-face-datasets.



### Self-supervised Tumor Segmentation through Layer Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2109.03230v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03230v3)
- **Published**: 2021-09-07 17:58:35+00:00
- **Updated**: 2021-12-21 12:52:26+00:00
- **Authors**: Xiaoman Zhang, Weidi Xie, Chaoqin Huang, Yanfeng Wang, Ya Zhang, Xin Chen, Qi Tian
- **Comment**: Project webpage: https://xiaoman-zhang.github.io/Layer-Decomposition/
- **Journal**: None
- **Summary**: In this paper, we target self-supervised representation learning for zero-shot tumor segmentation. We make the following contributions: First, we advocate a zero-shot setting, where models from pre-training should be directly applicable for the downstream task, without using any manual annotations. Second, we take inspiration from "layer-decomposition", and innovate on the training regime with simulated tumor data. Third, we conduct extensive ablation studies to analyse the critical components in data simulation, and validate the necessity of different proxy tasks. We demonstrate that, with sufficient texture randomization in simulation, model trained on synthetic data can effortlessly generalise to segment real tumor data. Forth, our approach achieves superior results for zero-shot tumor segmentation on different downstream datasets, BraTS2018 for brain tumor segmentation and LiTS2017 for liver tumor segmentation. While evaluating the model transferability for tumor segmentation under a low-annotation regime, the proposed approach also outperforms all existing self-supervised approaches, opening up the usage of self-supervised learning in practical scenarios.



### Simple Video Generation using Neural ODEs
- **Arxiv ID**: http://arxiv.org/abs/2109.03292v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.03292v1)
- **Published**: 2021-09-07 19:03:33+00:00
- **Updated**: 2021-09-07 19:03:33+00:00
- **Authors**: David Kanaa, Vikram Voleti, Samira Ebrahimi Kahou, Christopher Pal
- **Comment**: 8 pages, 4 figures, NeurIPS 2019 workshop
- **Journal**: NeurIPS 2019 Workshop
- **Summary**: Despite having been studied to a great extent, the task of conditional generation of sequences of frames, or videos, remains extremely challenging. It is a common belief that a key step towards solving this task resides in modelling accurately both spatial and temporal information in video signals. A promising direction to do so has been to learn latent variable models that predict the future in latent space and project back to pixels, as suggested in recent literature. Following this line of work and building on top of a family of models introduced in prior work, Neural ODE, we investigate an approach that models time-continuous dynamics over a continuous latent space with a differential equation with respect to time. The intuition behind this approach is that these trajectories in latent space could then be extrapolated to generate video frames beyond the time steps for which the model is trained. We show that our approach yields promising results in the task of future frame prediction on the Moving MNIST dataset with 1 and 2 digits.



### Self-Supervised Representation Learning using Visual Field Expansion on Digital Pathology
- **Arxiv ID**: http://arxiv.org/abs/2109.03299v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.03299v1)
- **Published**: 2021-09-07 19:20:01+00:00
- **Updated**: 2021-09-07 19:20:01+00:00
- **Authors**: Joseph Boyd, Mykola Liashuha, Eric Deutsch, Nikos Paragios, Stergios Christodoulidis, Maria Vakalopoulou
- **Comment**: None
- **Journal**: None
- **Summary**: The examination of histopathology images is considered to be the gold standard for the diagnosis and stratification of cancer patients. A key challenge in the analysis of such images is their size, which can run into the gigapixels and can require tedious screening by clinicians. With the recent advances in computational medicine, automatic tools have been proposed to assist clinicians in their everyday practice. Such tools typically process these large images by slicing them into tiles that can then be encoded and utilized for different clinical models. In this study, we propose a novel generative framework that can learn powerful representations for such tiles by learning to plausibly expand their visual field. In particular, we developed a progressively grown generative model with the objective of visual field expansion. Thus trained, our model learns to generate different tissue types with fine details, while simultaneously learning powerful representations that can be used for different clinical endpoints, all in a self-supervised way. To evaluate the performance of our model, we conducted classification experiments on CAMELYON17 and CRC benchmark datasets, comparing favorably to other self-supervised and pre-trained strategies that are commonly used in digital pathology. Our code is available at https://github.com/jcboyd/cdpath21-gan.



### Generatively Augmented Neural Network Watchdog for Image Classification Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.06168v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.06168v1)
- **Published**: 2021-09-07 19:53:20+00:00
- **Updated**: 2021-09-07 19:53:20+00:00
- **Authors**: Justin M. Bui, Glauco A. Amigo, Robert J. Marks II
- **Comment**: 9 Pages, 22 Figures
- **Journal**: None
- **Summary**: The identification of out-of-distribution data is vital to the deployment of classification networks. For example, a generic neural network that has been trained to differentiate between images of dogs and cats can only classify an input as either a dog or a cat. If a picture of a car or a kumquat were to be supplied to this classifier, the result would still be either a dog or a cat. In order to mitigate this, techniques such as the neural network watchdog have been developed. The compression of the image input into the latent layer of the autoencoder defines the region of in-distribution in the image space. This in-distribution set of input data has a corresponding boundary in the image space. The watchdog assesses whether inputs are in inside or outside this boundary. This paper demonstrates how to sharpen this boundary using generative network training data augmentation thereby bettering the discrimination and overall performance of the watchdog.



### Melatect: A Machine Learning Model Approach For Identifying Malignant Melanoma in Skin Growths
- **Arxiv ID**: http://arxiv.org/abs/2109.03310v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.03310v3)
- **Published**: 2021-09-07 20:05:08+00:00
- **Updated**: 2021-09-21 23:08:27+00:00
- **Authors**: Vidushi Meel, Asritha Bodepudi
- **Comment**: 7 Pages, Preprint
- **Journal**: None
- **Summary**: Malignant melanoma is a common skin cancer that is mostly curable before metastasis -when growths spawn in organs away from the original site. Melanoma is the most dangerous type of skin cancer if left untreated due to the high risk of metastasis. This paper presents Melatect, a machine learning (ML) model embedded in an iOS app that identifies potential malignant melanoma. Melatect accurately classifies lesions as malignant or benign over 96.6% of the time with no apparent bias or overfitting. Using the Melatect app, users have the ability to take pictures of skin lesions (moles) and subsequently receive a mole classification. The Melatect app provides a convenient way to get free advice on lesions and track these lesions over time. A recursive computer image analysis algorithm and modified MLOps pipeline was developed to create a model that performs at a higher accuracy than existing models. Our training dataset included 18,400 images of benign and malignant lesions, including 18,000 from the International Skin Imaging Collaboration (ISIC) archive, as well as 400 images gathered from local dermatologists; these images were augmented using DeepAugment, an AutoML tool, to 54,054 images.



### Multi-Branch Deep Radial Basis Function Networks for Facial Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.03336v1
- **DOI**: 10.1007/s00521-021-06420-w
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.03336v1)
- **Published**: 2021-09-07 21:05:56+00:00
- **Updated**: 2021-09-07 21:05:56+00:00
- **Authors**: Fernanda Hernández-Luquin, Hugo Jair Escalante
- **Comment**: Neural Comput & Applic (2021)
- **Journal**: None
- **Summary**: Emotion recognition (ER) from facial images is one of the landmark tasks in affective computing with major developments in the last decade. Initial efforts on ER relied on handcrafted features that were used to characterize facial images and then feed to standard predictive models. Recent methodologies comprise end-to-end trainable deep learning methods that simultaneously learn both, features and predictive model. Perhaps the most successful models are based on convolutional neural networks (CNNs). While these models have excelled at this task, they still fail at capturing local patterns that could emerge in the learning process. We hypothesize these patterns could be captured by variants based on locally weighted learning. Specifically, in this paper we propose a CNN based architecture enhanced with multiple branches formed by radial basis function (RBF) units that aims at exploiting local information at the final stage of the learning process. Intuitively, these RBF units capture local patterns shared by similar instances using an intermediate representation, then the outputs of the RBFs are feed to a softmax layer that exploits this information to improve the predictive performance of the model. This feature could be particularly advantageous in ER as cultural / ethnicity differences may be identified by the local units. We evaluate the proposed method in several ER datasets and show the proposed methodology achieves state-of-the-art in some of them, even when we adopt a pre-trained VGG-Face model as backbone. We show it is the incorporation of local information what makes the proposed model competitive.



### Certifiably Optimal Outlier-Robust Geometric Perception: Semidefinite Relaxations and Scalable Global Optimization
- **Arxiv ID**: http://arxiv.org/abs/2109.03349v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2109.03349v2)
- **Published**: 2021-09-07 21:42:16+00:00
- **Updated**: 2022-05-29 23:37:55+00:00
- **Authors**: Heng Yang, Luca Carlone
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2022
- **Summary**: We propose the first general and scalable framework to design certifiable algorithms for robust geometric perception in the presence of outliers. Our first contribution is to show that estimation using common robust costs, such as truncated least squares (TLS), maximum consensus, Geman-McClure, Tukey's biweight, among others, can be reformulated as polynomial optimization problems (POPs). By focusing on the TLS cost, our second contribution is to exploit sparsity in the POP and propose a sparse semidefinite programming (SDP) relaxation that is much smaller than the standard Lasserre's hierarchy while preserving empirical exactness, i.e., the SDP recovers the optimizer of the nonconvex POP with an optimality certificate. Our third contribution is to solve the SDP relaxations at an unprecedented scale and accuracy by presenting STRIDE, a solver that blends global descent on the convex SDP with fast local search on the nonconvex POP. Our fourth contribution is an evaluation of the proposed framework on six geometric perception problems including single and multiple rotation averaging, point cloud and mesh registration, absolute pose estimation, and category-level object pose and shape estimation. Our experiments demonstrate that (i) our sparse SDP relaxation is empirically exact with up to 60%-90% outliers across applications; (ii) while still being far from real-time, STRIDE is up to 100 times faster than existing SDP solvers on medium-scale problems, and is the only solver that can solve large-scale SDPs with hundreds of thousands of constraints to high accuracy; (iii) STRIDE safeguards existing fast heuristics for robust estimation (e.g., RANSAC or Graduated Non-Convexity), i.e., it certifies global optimality if the heuristic estimates are optimal, or detects and allows escaping local optima when the heuristic estimates are suboptimal.



### Capturing the objects of vision with neural networks
- **Arxiv ID**: http://arxiv.org/abs/2109.03351v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.03351v1)
- **Published**: 2021-09-07 21:49:53+00:00
- **Updated**: 2021-09-07 21:49:53+00:00
- **Authors**: Benjamin Peters, Nikolaus Kriegeskorte
- **Comment**: 25 pages, 5 figures
- **Journal**: None
- **Summary**: Human visual perception carves a scene at its physical joints, decomposing the world into objects, which are selectively attended, tracked, and predicted as we engage our surroundings. Object representations emancipate perception from the sensory input, enabling us to keep in mind that which is out of sight and to use perceptual content as a basis for action and symbolic cognition. Human behavioral studies have documented how object representations emerge through grouping, amodal completion, proto-objects, and object files. Deep neural network (DNN) models of visual object recognition, by contrast, remain largely tethered to the sensory input, despite achieving human-level performance at labeling objects. Here, we review related work in both fields and examine how these fields can help each other. The cognitive literature provides a starting point for the development of new experimental tasks that reveal mechanisms of human object perception and serve as benchmarks driving development of deep neural network models that will put the object into object recognition.



