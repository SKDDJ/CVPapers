# Arxiv Papers in cs.CV on 2021-09-24
### Feasibility study of urban flood mapping using traffic signs for route optimization
- **Arxiv ID**: http://arxiv.org/abs/2109.11712v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10, I.5.4, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2109.11712v1)
- **Published**: 2021-09-24 02:13:23+00:00
- **Updated**: 2021-09-24 02:13:23+00:00
- **Authors**: Bahareh Alizadeh, Diya Li, Zhe Zhang, Amir H. Behzadan
- **Comment**: URL: https://do.tu-berlin.de/handle/11303/13226
- **Journal**: EG-ICE 2021 Workshop on Intelligent Computing in Engineering
  (2021) 572-581
- **Summary**: Water events are the most frequent and costliest climate disasters around the world. In the U.S., an estimated 127 million people who live in coastal areas are at risk of substantial home damage from hurricanes or flooding. In flood emergency management, timely and effective spatial decision-making and intelligent routing depend on flood depth information at a fine spatiotemporal scale. In this paper, crowdsourcing is utilized to collect photos of submerged stop signs, and pair each photo with a pre-flood photo taken at the same location. Each photo pair is then analyzed using deep neural network and image processing to estimate the depth of floodwater in the location of the photo. Generated point-by-point depth data is converted to a flood inundation map and used by an A* search algorithm to determine an optimal flood-free path connecting points of interest. Results provide crucial information to rescue teams and evacuees by enabling effective wayfinding during flooding events.



### Training Automatic View Planner for Cardiac MR Imaging via Self-Supervision by Spatial Relationship between Views
- **Arxiv ID**: http://arxiv.org/abs/2109.11715v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.11715v1)
- **Published**: 2021-09-24 02:25:22+00:00
- **Updated**: 2021-09-24 02:25:22+00:00
- **Authors**: Dong Wei, Kai Ma, Yefeng Zheng
- **Comment**: Accepted by MICCAI 2021
- **Journal**: None
- **Summary**: View planning for the acquisition of cardiac magnetic resonance imaging (CMR) requires acquaintance with the cardiac anatomy and remains a challenging task in clinical practice. Existing approaches to its automation relied either on an additional volumetric image not typically acquired in clinic routine, or on laborious manual annotations of cardiac structural landmarks. This work presents a clinic-compatible and annotation-free system for automatic CMR view planning. The system mines the spatial relationship -- more specifically, locates and exploits the intersecting lines -- between the source and target views, and trains deep networks to regress heatmaps defined by these intersecting lines. As the spatial relationship is self-contained in properly stored data, e.g., in the DICOM format, the need for manual annotation is eliminated. Then, a multi-view planning strategy is proposed to aggregate information from the predicted heatmaps for all the source views of a target view, for a globally optimal prescription. The multi-view aggregation mimics the similar strategy practiced by skilled human prescribers. Experimental results on 181 clinical CMR exams show that our system achieves superior accuracy to existing approaches including conventional atlas-based and newer deep learning based ones, in prescribing four standard CMR views. The mean angle difference and point-to-plane distance evaluated against the ground truth planes are 5.98 degrees and 3.48 mm, respectively.



### A 3D Mesh-based Lifting-and-Projection Network for Human Pose Transfer
- **Arxiv ID**: http://arxiv.org/abs/2109.11719v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.11719v1)
- **Published**: 2021-09-24 03:03:02+00:00
- **Updated**: 2021-09-24 03:03:02+00:00
- **Authors**: Jinxiang Liu, Yangheng Zhao, Siheng Chen, Ya Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Human pose transfer has typically been modeled as a 2D image-to-image translation problem. This formulation ignores the human body shape prior in 3D space and inevitably causes implausible artifacts, especially when facing occlusion. To address this issue, we propose a lifting-and-projection framework to perform pose transfer in the 3D mesh space. The core of our framework is a foreground generation module, that consists of two novel networks: a lifting-and-projection network (LPNet) and an appearance detail compensating network (ADCNet). To leverage the human body shape prior, LPNet exploits the topological information of the body mesh to learn an expressive visual representation for the target person in the 3D mesh space. To preserve texture details, ADCNet is further introduced to enhance the feature produced by LPNet with the source foreground image. Such design of the foreground generation module enables the model to better handle difficult cases such as those with occlusions. Experiments on the iPER and Fashion datasets empirically demonstrate that the proposed lifting-and-projection framework is effective and outperforms the existing image-to-image-based and mesh-based methods on human pose transfer task in both self-transfer and cross-transfer settings.



### Holistic Semi-Supervised Approaches for EEG Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.11732v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.11732v2)
- **Published**: 2021-09-24 03:58:13+00:00
- **Updated**: 2022-02-10 23:35:13+00:00
- **Authors**: Guangyi Zhang, Ali Etemad
- **Comment**: Accepted by IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP 2022)
- **Journal**: None
- **Summary**: Recently, supervised methods, which often require substantial amounts of class labels, have achieved promising results for EEG representation learning. However, labeling EEG data is a challenging task. More recently, holistic semi-supervised learning approaches, which only require few output labels, have shown promising results in the field of computer vision. These methods, however, have not yet been adapted for EEG learning. In this paper, we adapt three state-of-the-art holistic semi-supervised approaches, namely MixMatch, FixMatch, and AdaMatch, as well as five classical semi-supervised methods for EEG learning. We perform rigorous experiments with all 8 methods on two public EEG-based emotion recognition datasets, namely SEED and SEED-IV. The experiments with different amounts of limited labeled samples show that the holistic approaches achieve strong results even when only 1 labeled sample is used per class. Further experiments show that in most cases, AdaMatch is the most effective method, followed by MixMatch and FixMatch.



### Unaligned Image-to-Image Translation by Learning to Reweight
- **Arxiv ID**: http://arxiv.org/abs/2109.11736v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.11736v1)
- **Published**: 2021-09-24 04:08:22+00:00
- **Updated**: 2021-09-24 04:08:22+00:00
- **Authors**: Shaoan Xie, Mingming Gong, Yanwu Xu, Kun Zhang
- **Comment**: ICCV2021
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation aims at learning the mapping from the source to target domain without using paired images for training. An essential yet restrictive assumption for unsupervised image translation is that the two domains are aligned, e.g., for the selfie2anime task, the anime (selfie) domain must contain only anime (selfie) face images that can be translated to some images in the other domain. Collecting aligned domains can be laborious and needs lots of attention. In this paper, we consider the task of image translation between two unaligned domains, which may arise for various possible reasons. To solve this problem, we propose to select images based on importance reweighting and develop a method to learn the weights and perform translation simultaneously and automatically. We compare the proposed method with state-of-the-art image translation approaches and present qualitative and quantitative results on different tasks with unaligned domains. Extensive empirical evidence demonstrates the usefulness of the proposed problem formulation and the superiority of our method.



### Multi-View Video-Based 3D Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.11747v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.11747v1)
- **Published**: 2021-09-24 05:20:41+00:00
- **Updated**: 2021-09-24 05:20:41+00:00
- **Authors**: Leyla Khaleghi, Alireza Sepas Moghaddam, Joshua Marshall, Ali Etemad
- **Comment**: 14 pages, 15 figures
- **Journal**: None
- **Summary**: Hand pose estimation (HPE) can be used for a variety of human-computer interaction applications such as gesture-based control for physical or virtual/augmented reality devices. Recent works have shown that videos or multi-view images carry rich information regarding the hand, allowing for the development of more robust HPE systems. In this paper, we present the Multi-View Video-Based 3D Hand (MuViHand) dataset, consisting of multi-view videos of the hand along with ground-truth 3D pose labels. Our dataset includes more than 402,000 synthetic hand images available in 4,560 videos. The videos have been simultaneously captured from six different angles with complex backgrounds and random levels of dynamic lighting. The data has been captured from 10 distinct animated subjects using 12 cameras in a semi-circle topology where six tracking cameras only focus on the hand and the other six fixed cameras capture the entire body. Next, we implement MuViHandNet, a neural pipeline consisting of image encoders for obtaining visual embeddings of the hand, recurrent learners to learn both temporal and angular sequential information, and graph networks with U-Net architectures to estimate the final 3D pose information. We perform extensive experiments and show the challenging nature of this new dataset as well as the effectiveness of our proposed method. Ablation studies show the added value of each component in MuViHandNet, as well as the benefit of having temporal and sequential information in the dataset.



### Fine-Grained Image Generation from Bangla Text Description using Attentional Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2109.11749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.11749v1)
- **Published**: 2021-09-24 05:31:01+00:00
- **Updated**: 2021-09-24 05:31:01+00:00
- **Authors**: Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin
- **Comment**: 6 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: Generating fine-grained, realistic images from text has many applications in the visual and semantic realm. Considering that, we propose Bangla Attentional Generative Adversarial Network (AttnGAN) that allows intensified, multi-stage processing for high-resolution Bangla text-to-image generation. Our model can integrate the most specific details at different sub-regions of the image. We distinctively concentrate on the relevant words in the natural language description. This framework has achieved a better inception score on the CUB dataset. For the first time, a fine-grained image is generated from Bangla text using attentional GAN. Bangla has achieved 7th position among 100 most spoken languages. This inspires us to explicitly focus on this language, which will ensure the inevitable need of many people. Moreover, Bangla has a more complex syntactic structure and less natural language processing resource that validates our work more.



### Quantifying point cloud realism through adversarially learned latent representations
- **Arxiv ID**: http://arxiv.org/abs/2109.11775v1
- **DOI**: 10.1007/978-3-030-92659-5_44
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.11775v1)
- **Published**: 2021-09-24 07:17:27+00:00
- **Updated**: 2021-09-24 07:17:27+00:00
- **Authors**: Larissa T. Triess, David Peter, Stefan A. Baur, J. Marius Zöllner
- **Comment**: 2021 German Conference on Pattern Recognition (GCPR). Project Page:
  http://ltriess.github.io/lidar-metric
- **Journal**: 2021 German Conference on Pattern Recognition (GCPR)
- **Summary**: Judging the quality of samples synthesized by generative models can be tedious and time consuming, especially for complex data structures, such as point clouds. This paper presents a novel approach to quantify the realism of local regions in LiDAR point clouds. Relevant features are learned from real-world and synthetic point clouds by training on a proxy classification task. Inspired by fair networks, we use an adversarial technique to discourage the encoding of dataset-specific information. The resulting metric can assign a quality score to samples without requiring any task specific annotations.   In a series of experiments, we confirm the soundness of our metric by applying it in controllable task setups and on unseen data. Additional experiments show reliable interpolation capabilities of the metric between data with varying degree of realism. As one important application, we demonstrate how the local realism score can be used for anomaly detection in point clouds.



### Dense Contrastive Visual-Linguistic Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2109.11778v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.11778v1)
- **Published**: 2021-09-24 07:20:13+00:00
- **Updated**: 2021-09-24 07:20:13+00:00
- **Authors**: Lei Shi, Kai Shuang, Shijie Geng, Peng Gao, Zuohui Fu, Gerard de Melo, Yunpeng Chen, Sen Su
- **Comment**: Accepted by ACM Multimedia 2021. arXiv admin note: text overlap with
  arXiv:2007.13135
- **Journal**: None
- **Summary**: Inspired by the success of BERT, several multimodal representation learning approaches have been proposed that jointly represent image and text. These approaches achieve superior performance by capturing high-level semantic information from large-scale multimodal pretraining. In particular, LXMERT and UNITER adopt visual region feature regression and label classification as pretext tasks. However, they tend to suffer from the problems of noisy labels and sparse semantic annotations, based on the visual features having been pretrained on a crowdsourced dataset with limited and inconsistent semantic labeling. To overcome these issues, we propose unbiased Dense Contrastive Visual-Linguistic Pretraining (DCVLP), which replaces the region regression and classification with cross-modality region contrastive learning that requires no annotations. Two data augmentation strategies (Mask Perturbation and Intra-/Inter-Adversarial Perturbation) are developed to improve the quality of negative samples used in contrastive learning. Overall, DCVLP allows cross-modality dense region contrastive learning in a self-supervised setting independent of any object annotations. We compare our method against prior visual-linguistic pretraining frameworks to validate the superiority of dense contrastive learning on multimodal representation learning.



### CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2109.11797v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.11797v3)
- **Published**: 2021-09-24 08:07:29+00:00
- **Updated**: 2022-05-20 07:05:41+00:00
- **Authors**: Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun
- **Comment**: Work in progress
- **Journal**: None
- **Summary**: Pre-Trained Vision-Language Models (VL-PTMs) have shown promising capabilities in grounding natural language in image data, facilitating a broad variety of cross-modal tasks. However, we note that there exists a significant gap between the objective forms of model pre-training and fine-tuning, resulting in a need for large amounts of labeled data to stimulate the visual grounding capability of VL-PTMs for downstream tasks. To address the challenge, we present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt Tuning), a novel paradigm for tuning VL-PTMs, which reformulates visual grounding into a fill-in-the-blank problem with color-based co-referential markers in image and text, maximally mitigating the gap. In this way, CPT enables strong few-shot and even zero-shot visual grounding capabilities of VL-PTMs. Comprehensive experimental results show that the prompt-tuned VL-PTMs outperform their fine-tuned counterparts by a large margin (e.g., 17.3% absolute accuracy improvement, and 73.8% relative standard deviation reduction on average with one shot in RefCOCO evaluation). We make the data and code for this paper publicly available at https://github.com/thunlp/CPT.



### Adversarial Domain Feature Adaptation for Bronchoscopic Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.11798v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.11798v1)
- **Published**: 2021-09-24 08:11:34+00:00
- **Updated**: 2021-09-24 08:11:34+00:00
- **Authors**: Mert Asim Karaoglu, Nikolas Brasch, Marijn Stollenga, Wolfgang Wein, Nassir Navab, Federico Tombari, Alexander Ladikos
- **Comment**: None
- **Journal**: None
- **Summary**: Depth estimation from monocular images is an important task in localization and 3D reconstruction pipelines for bronchoscopic navigation. Various supervised and self-supervised deep learning-based approaches have proven themselves on this task for natural images. However, the lack of labeled data and the bronchial tissue's feature-scarce texture make the utilization of these methods ineffective on bronchoscopic scenes. In this work, we propose an alternative domain-adaptive approach. Our novel two-step structure first trains a depth estimation network with labeled synthetic images in a supervised manner; then adopts an unsupervised adversarial domain feature adaptation scheme to improve the performance on real images. The results of our experiments show that the proposed method improves the network's performance on real images by a considerable margin and can be employed in 3D reconstruction pipelines.



### SIM2REALVIZ: Visualizing the Sim2Real Gap in Robot Ego-Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.11801v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.11801v2)
- **Published**: 2021-09-24 08:17:33+00:00
- **Updated**: 2021-12-03 10:02:35+00:00
- **Authors**: Theo Jaunet, Guillaume Bono, Romain Vuillemot, Christian Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: The Robotics community has started to heavily rely on increasingly realistic 3D simulators for large-scale training of robots on massive amounts of data. But once robots are deployed in the real world, the simulation gap, as well as changes in the real world (e.g. lights, objects displacements) lead to errors. In this paper, we introduce Sim2RealViz, a visual analytics tool to assist experts in understanding and reducing this gap for robot ego-pose estimation tasks, i.e. the estimation of a robot's position using trained models. Sim2RealViz displays details of a given model and the performance of its instances in both simulation and real-world. Experts can identify environment differences that impact model predictions at a given location and explore through direct interactions with the model hypothesis to fix it. We detail the design of the tool, and case studies related to the exploit of the regression to the mean bias and how it can be addressed, and how models are perturbed by the vanish of landmarks such as bikes.



### A Multi-stage Transfer Learning Framework for Diabetic Retinopathy Grading on Small Data
- **Arxiv ID**: http://arxiv.org/abs/2109.11806v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.11806v2)
- **Published**: 2021-09-24 08:39:13+00:00
- **Updated**: 2023-05-24 10:01:24+00:00
- **Authors**: Lei Shi, Bin Wang, Junxing Zhang
- **Comment**: 6 pages,Accepted by IEEE ICC2023
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) is one of the major blindness-causing diseases currently known. Automatic grading of DR using deep learning methods not only speeds up the diagnosis of the disease but also reduces the rate of misdiagnosis. However,problems such as insufficient samples and imbalanced class distribution in small DR datasets have constrained the improvement of grading performance. In this paper, we apply the idea of multi-stage transfer learning into the grading task of DR. The new transfer learning technique utilizes multiple datasets with different scales to enable the model to learn more feature representation information. Meanwhile, to cope with the imbalanced problem of small DR datasets, we present a class-balanced loss function in our work and adopt a simple and easy-to-implement training method for it. The experimental results on IDRiD dataset show that our method can effectively improve the grading performance on small data, obtaining scores of 0.7961 and 0.8763 in terms of accuracy and quadratic weighted kappa, respectively. Our method also outperforms several state-of-the-art methods.



### MODNet-V: Improving Portrait Video Matting via Background Restoration
- **Arxiv ID**: http://arxiv.org/abs/2109.11818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.11818v1)
- **Published**: 2021-09-24 09:03:39+00:00
- **Updated**: 2021-09-24 09:03:39+00:00
- **Authors**: Jiayu Sun, Zhanghan Ke, Lihe Zhang, Huchuan Lu, Rynson W. H. Lau
- **Comment**: None
- **Journal**: None
- **Summary**: To address the challenging portrait video matting problem more precisely, existing works typically apply some matting priors that require additional user efforts to obtain, such as annotated trimaps or background images. In this work, we observe that instead of asking the user to explicitly provide a background image, we may recover it from the input video itself. To this end, we first propose a novel background restoration module (BRM) to recover the background image dynamically from the input video. BRM is extremely lightweight and can be easily integrated into existing matting models. By combining BRM with a recent image matting model, MODNet, we then present MODNet-V for portrait video matting. Benefited from the strong background prior provided by BRM, MODNet-V has only 1/3 of the parameters of MODNet but achieves comparable or even better performances. Our design allows MODNet-V to be trained in an end-to-end manner on a single NVIDIA 3090 GPU. Finally, we introduce a new patch refinement module (PRM) to adapt MODNet-V for high-resolution videos while keeping MODNet-V lightweight and fast.



### GSIP: Green Semantic Segmentation of Large-Scale Indoor Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2109.11835v2
- **DOI**: 10.1016/j.patrec.2022.10.014
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.11835v2)
- **Published**: 2021-09-24 09:26:53+00:00
- **Updated**: 2022-11-14 05:03:44+00:00
- **Authors**: Min Zhang, Pranav Kadam, Shan Liu, C. -C. Jay Kuo
- **Comment**: 10 pages, 3 figures
- **Journal**: Pattern Recognition Letters, Volume 164, 2022, Pages 9-15
- **Summary**: An efficient solution to semantic segmentation of large-scale indoor scene point clouds is proposed in this work. It is named GSIP (Green Segmentation of Indoor Point clouds) and its performance is evaluated on a representative large-scale benchmark -- the Stanford 3D Indoor Segmentation (S3DIS) dataset. GSIP has two novel components: 1) a room-style data pre-processing method that selects a proper subset of points for further processing, and 2) a new feature extractor which is extended from PointHop. For the former, sampled points of each room form an input unit. For the latter, the weaknesses of PointHop's feature extraction when extending it to large-scale point clouds are identified and fixed with a simpler processing pipeline. As compared with PointNet, which is a pioneering deep-learning-based solution, GSIP is green since it has significantly lower computational complexity and a much smaller model size. Furthermore, experiments show that GSIP outperforms PointNet in segmentation performance for the S3DIS dataset.



### Frequency Pooling: Shift-Equivalent and Anti-Aliasing Downsampling
- **Arxiv ID**: http://arxiv.org/abs/2109.11839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.11839v1)
- **Published**: 2021-09-24 09:32:10+00:00
- **Updated**: 2021-09-24 09:32:10+00:00
- **Authors**: Zhendong Zhang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Convolution utilizes a shift-equivalent prior of images, thus leading to great success in image processing tasks. However, commonly used poolings in convolutional neural networks (CNNs), such as max-pooling, average-pooling, and strided-convolution, are not shift-equivalent. Thus, the shift-equivalence of CNNs is destroyed when convolutions and poolings are stacked. Moreover, anti-aliasing is another essential property of poolings from the perspective of signal processing. However, recent poolings are neither shift-equivalent nor anti-aliasing. To address this issue, we propose a new pooling method that is shift-equivalent and anti-aliasing, named frequency pooling. Frequency pooling first transforms the features into the frequency domain, and then removes the frequency components beyond the Nyquist frequency. Finally, it transforms the features back to the spatial domain. We prove that frequency pooling is shift-equivalent and anti-aliasing based on the property of Fourier transform and Nyquist frequency. Experiments on image classification show that frequency pooling improves accuracy and robustness with respect to the shifts of CNNs.



### Learnable Triangulation for Deep Learning-based 3D Reconstruction of Objects of Arbitrary Topology from Single RGB Images
- **Arxiv ID**: http://arxiv.org/abs/2109.11844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.11844v1)
- **Published**: 2021-09-24 09:44:22+00:00
- **Updated**: 2021-09-24 09:44:22+00:00
- **Authors**: Tarek Ben Charrada, Hedi Tabia, Aladine Chetouani, Hamid Laga
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel deep reinforcement learning-based approach for 3D object reconstruction from monocular images. Prior works that use mesh representations are template based. Thus, they are limited to the reconstruction of objects that have the same topology as the template. Methods that use volumetric grids as intermediate representations are computationally expensive, which limits their application in real-time scenarios. In this paper, we propose a novel end-to-end method that reconstructs 3D objects of arbitrary topology from a monocular image. It is composed of of (1) a Vertex Generation Network (VGN), which predicts the initial 3D locations of the object's vertices from an input RGB image, (2) a differentiable triangulation layer, which learns in a non-supervised manner, using a novel reinforcement learning algorithm, the best triangulation of the object's vertices, and finally, (3) a hierarchical mesh refinement network that uses graph convolutions to refine the initial mesh. Our key contribution is the learnable triangulation process, which recovers in an unsupervised manner the topology of the input shape. Our experiments on ShapeNet and Pix3D benchmarks show that the proposed method outperforms the state-of-the-art in terms of visual quality, reconstruction accuracy, and computational time.



### How to find a good image-text embedding for remote sensing visual question answering?
- **Arxiv ID**: http://arxiv.org/abs/2109.11848v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.11848v1)
- **Published**: 2021-09-24 09:48:28+00:00
- **Updated**: 2021-09-24 09:48:28+00:00
- **Authors**: Christel Chappuis, Sylvain Lobry, Benjamin Kellenberger, Bertrand Le Saux, Devis Tuia
- **Comment**: 10 pages, 4 figures, presented in the MACLEAN workshop during ECML
  PKDD 2021
- **Journal**: None
- **Summary**: Visual question answering (VQA) has recently been introduced to remote sensing to make information extraction from overhead imagery more accessible to everyone. VQA considers a question (in natural language, therefore easy to formulate) about an image and aims at providing an answer through a model based on computer vision and natural language processing methods. As such, a VQA model needs to jointly consider visual and textual features, which is frequently done through a fusion step. In this work, we study three different fusion methodologies in the context of VQA for remote sensing and analyse the gains in accuracy with respect to the model complexity. Our findings indicate that more complex fusion mechanisms yield an improved performance, yet that seeking a trade-of between model complexity and performance is worthwhile in practice.



### Training dataset generation for bridge game registration
- **Arxiv ID**: http://arxiv.org/abs/2109.11861v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.11861v1)
- **Published**: 2021-09-24 10:09:36+00:00
- **Updated**: 2021-09-24 10:09:36+00:00
- **Authors**: Piotr Wzorek, Tomasz Kryjak
- **Comment**: Submitted to Zeszyty Studenckiego Towarzystwa Naukowego, ISSN
  1732-0925
- **Journal**: None
- **Summary**: This paper presents a method for automatic generation of a training dataset for a deep convolutional neural network used for playing card detection. The solution allows to skip the time-consuming processes of manual image collecting and labelling recognised objects. The YOLOv4 network trained on the generated dataset achieved an efficiency of 99.8% in the cards detection task. The proposed method is a part of a project that aims to automate the process of broadcasting duplicate bridge competitions using a vision system and neural networks.



### Catadioptric Stereo on a Smartphone
- **Arxiv ID**: http://arxiv.org/abs/2109.11872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.11872v1)
- **Published**: 2021-09-24 10:37:03+00:00
- **Updated**: 2021-09-24 10:37:03+00:00
- **Authors**: Kristijan Bartol, David Bojanić, Tomislav Petković, Tomislav Pribanić
- **Comment**: None
- **Journal**: None
- **Summary**: We present a 3D printed adapter with planar mirrors for stereo reconstruction using front and back smartphone camera. The adapter presents a practical and low-cost solution for enabling any smartphone to be used as a stereo camera, which is currently only possible using high-end phones with expensive 3D sensors. Using the prototype version of the adapter, we experiment with parameters like the angles between cameras and mirrors and the distance to each camera (the stereo baseline). We find the most convenient configuration and calibrate the stereo pair. Based on the presented preliminary analysis, we identify possible improvements in the current design. To demonstrate the working prototype, we reconstruct a 3D human pose using 2D keypoint detections from the stereo pair and evaluate extracted body lengths. The result shows that the adapter can be used for anthropometric measurement of several body segments.



### Localizing Infinity-shaped fishes: Sketch-guided object localization in the wild
- **Arxiv ID**: http://arxiv.org/abs/2109.11874v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2109.11874v1)
- **Published**: 2021-09-24 10:39:43+00:00
- **Updated**: 2021-09-24 10:39:43+00:00
- **Authors**: Pau Riba, Sounak Dey, Ali Furkan Biten, Josep Llados
- **Comment**: Under Review
- **Journal**: None
- **Summary**: This work investigates the problem of sketch-guided object localization (SGOL), where human sketches are used as queries to conduct the object localization in natural images. In this cross-modal setting, we first contribute with a tough-to-beat baseline that without any specific SGOL training is able to outperform the previous works on a fixed set of classes. The baseline is useful to analyze the performance of SGOL approaches based on available simple yet powerful methods. We advance prior arts by proposing a sketch-conditioned DETR (DEtection TRansformer) architecture which avoids a hard classification and alleviates the domain gap between sketches and images to localize object instances. Although the main goal of SGOL is focused on object detection, we explored its natural extension to sketch-guided instance segmentation. This novel task allows to move towards identifying the objects at pixel level, which is of key importance in several applications. We experimentally demonstrate that our model and its variants significantly advance over previous state-of-the-art results. All training and testing code of our model will be released to facilitate future research{{https://github.com/priba/sgol_wild}}.



### Learning-based Noise Component Map Estimation for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2109.11877v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.11877v1)
- **Published**: 2021-09-24 10:47:55+00:00
- **Updated**: 2021-09-24 10:47:55+00:00
- **Authors**: Sheyda Ghanbaralizadeh Bahnemiri, Mykola Ponomarenko, Karen Egiazarian
- **Comment**: 5 pages, submitted to IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: A problem of image denoising when images are corrupted by a non-stationary noise is considered in this paper. Since in practice no a priori information on noise is available, noise statistics should be pre-estimated for image denoising. In this paper, deep convolutional neural network (CNN) based method for estimation of a map of local, patch-wise, standard deviations of noise (so-called sigma-map) is proposed. It achieves the state-of-the-art performance in accuracy of estimation of sigma-map for the case of non-stationary noise, as well as estimation of noise variance for the case of additive white Gaussian noise. Extensive experiments on image denoising using estimated sigma-maps demonstrate that our method outperforms recent CNN-based blind image denoising methods by up to 6 dB in PSNR, as well as other state-of-the-art methods based on sigma-map estimation by up to 0.5 dB, providing same time better usage flexibility. Comparison with the ideal case, when denoising is applied using ground-truth sigma-map, shows that a difference of corresponding PSNR values for most of noise levels is within 0.1-0.2 dB and does not exceeds 0.6 dB.



### Tackling Inter-Class Similarity and Intra-Class Variance for Microscopic Image-based Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.11891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.11891v1)
- **Published**: 2021-09-24 11:17:02+00:00
- **Updated**: 2021-09-24 11:17:02+00:00
- **Authors**: Aishwarya Venkataramanan, Martin Laviale, Cécile Figus, Philippe Usseglio-Polatera, Cédric Pradalier
- **Comment**: 13th International Conference on Computer Vision Systems (2021)
- **Journal**: None
- **Summary**: Automatic classification of aquatic microorganisms is based on the morphological features extracted from individual images. The current works on their classification do not consider the inter-class similarity and intra-class variance that causes misclassification. We are particularly interested in the case where variance within a class occurs due to discrete visual changes in microscopic images. In this paper, we propose to account for it by partitioning the classes with high variance based on the visual features. Our algorithm automatically decides the optimal number of sub-classes to be created and consider each of them as a separate class for training. This way, the network learns finer-grained visual features. Our experiments on two databases of freshwater benthic diatoms and marine plankton show that our method can outperform the state-of-the-art approaches for classification of these aquatic microorganisms.



### RSDet++: Point-based Modulated Loss for More Accurate Rotated Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.11906v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.11906v1)
- **Published**: 2021-09-24 11:57:53+00:00
- **Updated**: 2021-09-24 11:57:53+00:00
- **Authors**: Wen Qian, Xue Yang, Silong Peng, Junchi Yan, Xiujuan Zhang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1911.08299
- **Journal**: None
- **Summary**: We classify the discontinuity of loss in both five-param and eight-param rotated object detection methods as rotation sensitivity error (RSE) which will result in performance degeneration. We introduce a novel modulated rotation loss to alleviate the problem and propose a rotation sensitivity detection network (RSDet) which is consists of an eight-param single-stage rotated object detector and the modulated rotation loss. Our proposed RSDet has several advantages: 1) it reformulates the rotated object detection problem as predicting the corners of objects while most previous methods employ a five-para-based regression method with different measurement units. 2) modulated rotation loss achieves consistent improvement on both five-param and eight-param rotated object detection methods by solving the discontinuity of loss. To further improve the accuracy of our method on objects smaller than 10 pixels, we introduce a novel RSDet++ which is consists of a point-based anchor-free rotated object detector and a modulated rotation loss. Extensive experiments demonstrate the effectiveness of both RSDet and RSDet++, which achieve competitive results on rotated object detection in the challenging benchmarks DOTA1.0, DOTA1.5, and DOTA2.0. We hope the proposed method can provide a new perspective for designing algorithms to solve rotated object detection and pay more attention to tiny objects. The codes and models are available at: https://github.com/yangxue0827/RotationDetection.



### Towards Autonomous Visual Navigation in Arable Fields
- **Arxiv ID**: http://arxiv.org/abs/2109.11936v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.11936v3)
- **Published**: 2021-09-24 12:54:42+00:00
- **Updated**: 2022-07-01 15:53:15+00:00
- **Authors**: Alireza Ahmadi, Michael Halstead, Chris McCool
- **Comment**: None
- **Journal**: IROS 2022 Kyoto Japan
- **Summary**: Autonomous navigation of a robot in agricultural fields is essential for every task from crop monitoring to weed management and fertilizer application. Many current approaches rely on accurate GPS, however, such technology is expensive and also prone to failure (e.g. through lack of coverage). As such, autonomous navigation through sensors that can interpret their environment (such as cameras) is important to achieve the goal of autonomy in agriculture. In this paper, we introduce a purely vision-based navigation scheme that is able to reliably guide the robot through row-crop fields without manual intervention. Independent of any global localization or mapping, this approach is able to accurately follow the crop-rows and switch between the rows, only using onboard cameras. With the help of a novel crop-row detection and a novel crop-row switching technique, our navigation scheme can be deployed in a wide range of fields with different canopy types in various growth stages with limited parameter tuning, creating a crop agnostic navigation approach. We have extensively evaluated our approach in three different fields under various illumination conditions using our agricultural robotic platform (BonnBot-I). For navigation, our approach is evaluated on five crop types and achieves an average navigation accuracy of 3.82cm relative to manual teleoperation.



### Two-Stage Mesh Deep Learning for Automated Tooth Segmentation and Landmark Localization on 3D Intraoral Scans
- **Arxiv ID**: http://arxiv.org/abs/2109.11941v4
- **DOI**: 10.1109/TMI.2022.3180343
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.11941v4)
- **Published**: 2021-09-24 13:00:26+00:00
- **Updated**: 2022-06-02 17:41:16+00:00
- **Authors**: Tai-Hsien Wu, Chunfeng Lian, Sanghee Lee, Matthew Pastewait, Christian Piers, Jie Liu, Fang Wang, Li Wang, Chiung-Ying Chiu, Wenchi Wang, Christina Jackson, Wei-Lun Chao, Dinggang Shen, Ching-Chang Ko
- **Comment**: 9 pages, 8 figures, accepted by IEEE TMI
- **Journal**: None
- **Summary**: Accurately segmenting teeth and identifying the corresponding anatomical landmarks on dental mesh models are essential in computer-aided orthodontic treatment. Manually performing these two tasks is time-consuming, tedious, and, more importantly, highly dependent on orthodontists' experiences due to the abnormality and large-scale variance of patients' teeth. Some machine learning-based methods have been designed and applied in the orthodontic field to automatically segment dental meshes (e.g., intraoral scans). In contrast, the number of studies on tooth landmark localization is still limited. This paper proposes a two-stage framework based on mesh deep learning (called TS-MDL) for joint tooth labeling and landmark identification on raw intraoral scans. Our TS-MDL first adopts an end-to-end \emph{i}MeshSegNet method (i.e., a variant of the existing MeshSegNet with both improved accuracy and efficiency) to label each tooth on the downsampled scan. Guided by the segmentation outputs, our TS-MDL further selects each tooth's region of interest (ROI) on the original mesh to construct a light-weight variant of the pioneering PointNet (i.e., PointNet-Reg) for regressing the corresponding landmark heatmaps. Our TS-MDL was evaluated on a real-clinical dataset, showing promising segmentation and localization performance. Specifically, \emph{i}MeshSegNet in the first stage of TS-MDL reached an averaged Dice similarity coefficient (DSC) at \textcolor[rgb]{0,0,0}{$0.964\pm0.054$}, significantly outperforming the original MeshSegNet. In the second stage, PointNet-Reg achieved a mean absolute error (MAE) of $0.597\pm0.761 \, mm$ in distances between the prediction and ground truth for $66$ landmarks, which is superior compared with other networks for landmark detection. All these results suggest the potential usage of our TS-MDL in orthodontics.



### Visual Scene Graphs for Audio Source Separation
- **Arxiv ID**: http://arxiv.org/abs/2109.11955v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2109.11955v1)
- **Published**: 2021-09-24 13:40:51+00:00
- **Updated**: 2021-09-24 13:40:51+00:00
- **Authors**: Moitreya Chatterjee, Jonathan Le Roux, Narendra Ahuja, Anoop Cherian
- **Comment**: Accepted at ICCV 2021
- **Journal**: None
- **Summary**: State-of-the-art approaches for visually-guided audio source separation typically assume sources that have characteristic sounds, such as musical instruments. These approaches often ignore the visual context of these sound sources or avoid modeling object interactions that may be useful to better characterize the sources, especially when the same object class may produce varied sounds from distinct interactions. To address this challenging problem, we propose Audio Visual Scene Graph Segmenter (AVSGS), a novel deep learning model that embeds the visual structure of the scene as a graph and segments this graph into subgraphs, each subgraph being associated with a unique sound obtained by co-segmenting the audio spectrogram. At its core, AVSGS uses a recursive neural network that emits mutually-orthogonal sub-graph embeddings of the visual graph using multi-head attention. These embeddings are used for conditioning an audio encoder-decoder towards source separation. Our pipeline is trained end-to-end via a self-supervised task consisting of separating audio sources using the visual graph from artificially mixed sounds. In this paper, we also introduce an "in the wild'' video dataset for sound source separation that contains multiple non-musical sources, which we call Audio Separation in the Wild (ASIW). This dataset is adapted from the AudioCaps dataset, and provides a challenging, natural, and daily-life setting for source separation. Thorough experiments on the proposed ASIW and the standard MUSIC datasets demonstrate state-of-the-art sound separation performance of our method against recent prior approaches.



### Quantitative Matching of Forensic Evidence Fragments Utilizing 3D Microscopy Analysis of Fracture Surface Replicas
- **Arxiv ID**: http://arxiv.org/abs/2109.11972v1
- **DOI**: 10.1111/1556-4029.15012
- **Categories**: **eess.IV**, cs.CV, stat.AP, 82D35, 62H30, 62H35, 62P20, 62P25, 62P30, 62P35, I.4.9; J.2; K.4.2
- **Links**: [PDF](http://arxiv.org/pdf/2109.11972v1)
- **Published**: 2021-09-24 13:59:52+00:00
- **Updated**: 2021-09-24 13:59:52+00:00
- **Authors**: Bishoy Dawood, Carlos Llosa-Vite, Geoffrey Z. Thompson, Barbara K. Lograsso, Lauren K. Claytor, John Vanderkolk, William Meeker, Ranjan Maitra, Ashraf Bastawros
- **Comment**: 25 pages, 9 figures
- **Journal**: Journal of Forensic Sciences, 2022
- **Summary**: Fractured surfaces carry unique details that can provide an accurate quantitative comparison to support comparative forensic analysis of those fractured surfaces. In this study, a statistical analysis comparison protocol was applied to a set of 3D topological images of fractured surface pairs and their replicas to provide confidence in the quantitative statistical comparison between fractured items and their replicas. A set of 10 fractured stainless steel samples was fractured from the same metal rod under controlled conditions and were cast using a standard forensic casting technique. Six 3D topological maps with 50% overlap were acquired for each fractured pair. Spectral analysis was utilized to identify the correlation between topological surface features at different length scales of the surface topology. We selected two frequency bands over the critical wavelength (which is greater than two-grain diameters) for statistical comparison. Our statistical model utilized a matrix-variate-$t$ distribution that accounts for the image-overlap to model the match and non-match population densities. A decision rule was developed to identify the probability of matched and unmatched pairs of surfaces. The proposed methodology correctly classified the fractured steel surfaces and their replicas with a posterior probability of match exceeding 99.96%. Moreover, the replication technique shows the potential to accurately replicate fracture surface topological details with a wavelength greater than 20$\mu$m, which far exceeds the range for comparison of most metallic alloys of 50-200$\mu$m. The developed framework establishes the basis of forensic comparison of fractured articles and their replicas while providing a reliable quantitative statistical forensic comparison, utilizing fracture mechanics-based analysis of the fracture surface topology.



### Identifying Women with Mammographically-Occult Breast Cancer Leveraging GAN-Simulated Mammograms
- **Arxiv ID**: http://arxiv.org/abs/2109.12113v1
- **DOI**: 10.1109/TMI.2021.3108949
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.12113v1)
- **Published**: 2021-09-24 14:33:54+00:00
- **Updated**: 2021-09-24 14:33:54+00:00
- **Authors**: Juhun Lee, Robert M. Nishikawa
- **Comment**: This article has been accepted for publication in IEEE Transactions
  on Medical Imaging. This is the author's version which has not been fully
  edited and content may change prior to final publication. Citation
  information: DOI 10.1109/TMI.2021.3108949. \c{opyright} 2021 IEEE. Personal
  use is permitted, but republication/redistribution requires IEEE permission
- **Journal**: None
- **Summary**: Our objective is to show the feasibility of using simulated mammograms to detect mammographically-occult (MO) cancer in women with dense breasts and a normal screening mammogram who could be triaged for additional screening with magnetic resonance imaging (MRI) or ultrasound. We developed a Conditional Generative Adversarial Network (CGAN) to simulate a mammogram with normal appearance using the opposite mammogram as the condition. We used a Convolutional Neural Network (CNN) trained on Radon Cumulative Distribution Transform (RCDT) processed mammograms to detect MO cancer. For training CGAN, we used screening mammograms of 1366 women. For MO cancer detection, we used screening mammograms of 333 women (97 MO cancer) with dense breasts. We simulated the right mammogram for normal controls and the cancer side for MO cancer cases. We created two RCDT images, one from a real mammogram pair and another from a real-simulated mammogram pair. We finetuned a VGG16 on resulting RCDT images to classify the women with MO cancer. We compared the classification performance of the CNN trained on fused RCDT images, CNN_{Fused} to that of trained only on real RCDT images, CNN_{Real}, and to that of trained only on simulated RCDT images, CNN_{Simulated}. The test AUC for CNN_{Fused} was 0.77 with a 95% confidence interval (95CI) of [0.71, 0.83], which was statistically better (p-value < 0.02) than the CNN_{Real} AUC of 0.70 with a 95CI of [0.64, 0.77] and CNN_{Simulated} AUC of 0.68 with a 95CI of [0.62, 0.75]. It showed that CGAN simulated mammograms can help MO cancer detection.



### From images in the wild to video-informed image classification
- **Arxiv ID**: http://arxiv.org/abs/2109.12040v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.12040v1)
- **Published**: 2021-09-24 15:53:37+00:00
- **Updated**: 2021-09-24 15:53:37+00:00
- **Authors**: Marc Böhlen, Varun Chandola, Wawan Sujarwo, Raunaq Jain
- **Comment**: 6 pages, 7 figures, supplemental materials
- **Journal**: ICMLA 2021
- **Summary**: Image classifiers work effectively when applied on structured images, yet they often fail when applied on images with very high visual complexity. This paper describes experiments applying state-of-the-art object classifiers toward a unique set of images in the wild with high visual complexity collected on the island of Bali. The text describes differences between actual images in the wild and images from Imagenet, and then discusses a novel approach combining informational cues particular to video with an ensemble of imperfect classifiers in order to improve classification results on video sourced images of plants in the wild.



### DeepStroke: An Efficient Stroke Screening Framework for Emergency Rooms with Multimodal Adversarial Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.12065v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.12065v2)
- **Published**: 2021-09-24 16:46:13+00:00
- **Updated**: 2022-06-27 18:02:49+00:00
- **Authors**: Tongan Cai, Haomiao Ni, Mingli Yu, Xiaolei Huang, Kelvin Wong, John Volpi, James Z. Wang, Stephen T. C. Wong
- **Comment**: None
- **Journal**: None
- **Summary**: In an emergency room (ER) setting, stroke triage or screening is a common challenge. A quick CT is usually done instead of MRI due to MRI's slow throughput and high cost. Clinical tests are commonly referred to during the process, but the misdiagnosis rate remains high. We propose a novel multimodal deep learning framework, DeepStroke, to achieve computer-aided stroke presence assessment by recognizing patterns of minor facial muscles incoordination and speech inability for patients with suspicion of stroke in an acute setting. Our proposed DeepStroke takes one-minute facial video data and audio data readily available during stroke triage for local facial paralysis detection and global speech disorder analysis. Transfer learning was adopted to reduce face-attribute biases and improve generalizability. We leverage a multi-modal lateral fusion to combine the low- and high-level features and provide mutual regularization for joint training. Novel adversarial training is introduced to obtain identity-free and stroke-discriminative features. Experiments on our video-audio dataset with actual ER patients show that DeepStroke outperforms state-of-the-art models and achieves better performance than both a triage team and ER doctors, attaining a 10.94% higher sensitivity and maintaining 7.37% higher accuracy than traditional stroke triage when specificity is aligned. Meanwhile, each assessment can be completed in less than six minutes, demonstrating the framework's great potential for clinical translation.



### Zero-shot Object Detection Through Vision-Language Embedding Alignment
- **Arxiv ID**: http://arxiv.org/abs/2109.12066v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.12066v2)
- **Published**: 2021-09-24 16:46:36+00:00
- **Updated**: 2022-08-26 03:54:26+00:00
- **Authors**: Johnathan Xie, Shuai Zheng
- **Comment**: Code: https://github.com/Johnathan-Xie/ZSD-YOLO
- **Journal**: None
- **Summary**: Recent approaches have shown that training deep neural networks directly on large-scale image-text pair collections enables zero-shot transfer on various recognition tasks. One central issue is how this can be generalized to object detection, which involves the non-semantic task of localization as well as semantic task of classification. To solve this problem, we introduce a vision-language embedding alignment method that transfers the generalization capabilities of a pretrained model such as CLIP to an object detector like YOLOv5. We formulate a loss function that allows us to align the image and text embeddings from the pretrained model CLIP with the modified semantic prediction head from the detector. With this method, we are able to train an object detector that achieves state-of-the-art performance on the COCO, ILSVRC, and Visual Genome zero-shot detection benchmarks. During inference, our model can be adapted to detect any number of object classes without additional training. We also find that standard object detection scaling can transfer well to our method and find consistent improvements across various scales of YOLOv5 models and the YOLOv3 model. Lastly, we develop a self-labeling method that provides a significant score improvement without needing extra images nor labels.



### CLIPort: What and Where Pathways for Robotic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2109.12098v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.12098v1)
- **Published**: 2021-09-24 17:44:28+00:00
- **Updated**: 2021-09-24 17:44:28+00:00
- **Authors**: Mohit Shridhar, Lucas Manuelli, Dieter Fox
- **Comment**: CoRL 2021. Project Website: https://cliport.github.io/
- **Journal**: None
- **Summary**: How can we imbue robots with the ability to manipulate objects precisely but also to reason about them in terms of abstract concepts? Recent works in manipulation have shown that end-to-end networks can learn dexterous skills that require precise spatial reasoning, but these methods often fail to generalize to new goals or quickly learn transferable concepts across tasks. In parallel, there has been great progress in learning generalizable semantic representations for vision and language by training on large-scale internet data, however these representations lack the spatial understanding necessary for fine-grained manipulation. To this end, we propose a framework that combines the best of both worlds: a two-stream architecture with semantic and spatial pathways for vision-based manipulation. Specifically, we present CLIPort, a language-conditioned imitation-learning agent that combines the broad semantic understanding (what) of CLIP [1] with the spatial precision (where) of Transporter [2]. Our end-to-end framework is capable of solving a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, all without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures. Experiments in simulated and real-world settings show that our approach is data efficient in few-shot settings and generalizes effectively to seen and unseen semantic concepts. We even learn one multi-task policy for 10 simulated and 9 real-world tasks that is better or comparable to single-task policies.



### Use of the Deep Learning Approach to Measure Alveolar Bone Level
- **Arxiv ID**: http://arxiv.org/abs/2109.12115v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.12115v1)
- **Published**: 2021-09-24 17:48:27+00:00
- **Updated**: 2021-09-24 17:48:27+00:00
- **Authors**: Chun-Teh Lee, Tanjida Kabir, Jiman Nelson, Sally Sheng, Hsiu-Wan Meng, Thomas E. Van Dyke, Muhammad F. Walji, Xiaoqian Jiang, Shayan Shams
- **Comment**: Word count: 3485; Number of figures: 4; tables: 2; references: 34
- **Journal**: None
- **Summary**: Abstract:   Aim: The goal was to use a Deep Convolutional Neural Network to measure the radiographic alveolar bone level to aid periodontal diagnosis.   Material and methods: A Deep Learning (DL) model was developed by integrating three segmentation networks (bone area, tooth, cementoenamel junction) and image analysis to measure the radiographic bone level and assign radiographic bone loss (RBL) stages. The percentage of RBL was calculated to determine the stage of RBL for each tooth. A provisional periodontal diagnosis was assigned using the 2018 periodontitis classification. RBL percentage, staging, and presumptive diagnosis were compared to the measurements and diagnoses made by the independent examiners.   Results: The average Dice Similarity Coefficient (DSC) for segmentation was over 0.91. There was no significant difference in RBL percentage measurements determined by DL and examiners (p=0.65). The Area Under the Receiver Operating Characteristics Curve of RBL stage assignment for stage I, II and III was 0.89, 0.90 and 0.90, respectively. The accuracy of the case diagnosis was 0.85.   Conclusion: The proposed DL model provides reliable RBL measurements and image-based periodontal diagnosis using periapical radiographic images. However, this model has to be further optimized and validated by a larger number of images to facilitate its application.



### ImplicitVol: Sensorless 3D Ultrasound Reconstruction with Deep Implicit Representation
- **Arxiv ID**: http://arxiv.org/abs/2109.12108v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.12108v1)
- **Published**: 2021-09-24 17:59:18+00:00
- **Updated**: 2021-09-24 17:59:18+00:00
- **Authors**: Pak-Hei Yeung, Linde Hesse, Moska Aliasi, Monique Haak, the INTERGROWTH-21st Consortium, Weidi Xie, Ana I. L. Namburete
- **Comment**: None
- **Journal**: None
- **Summary**: The objective of this work is to achieve sensorless reconstruction of a 3D volume from a set of 2D freehand ultrasound images with deep implicit representation. In contrast to the conventional way that represents a 3D volume as a discrete voxel grid, we do so by parameterizing it as the zero level-set of a continuous function, i.e. implicitly representing the 3D volume as a mapping from the spatial coordinates to the corresponding intensity values. Our proposed model, termed as ImplicitVol, takes a set of 2D scans and their estimated locations in 3D as input, jointly refining the estimated 3D locations and learning a full reconstruction of the 3D volume. When testing on real 2D ultrasound images, novel cross-sectional views that are sampled from ImplicitVol show significantly better visual quality than those sampled from existing reconstruction approaches, outperforming them by over 30% (NCC and SSIM), between the output and ground-truth on the 3D volume testing data. The code will be made publicly available.



### Automatic Map Update Using Dashcam Videos
- **Arxiv ID**: http://arxiv.org/abs/2109.12131v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.12131v2)
- **Published**: 2021-09-24 18:00:57+00:00
- **Updated**: 2022-01-18 21:59:21+00:00
- **Authors**: Aziza Zhanabatyrova, Clayton Souza Leite, Yu Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous driving requires 3D maps that provide accurate and up-to-date information about semantic landmarks. Due to the wider availability and lower cost of cameras compared with laser scanners, vision-based mapping solutions, especially the ones using crowdsourced visual data, have attracted much attention from academia and industry. However, previous works have mainly focused on creating 3D point clouds, leaving automatic change detection as open issues. We propose in this paper a pipeline for initiating and updating 3D maps with dashcam videos, with a focus on automatic change detection based on comparison of metadata (e.g., the types and locations of traffic signs). To improve the performance of metadata generation, which depends on the accuracy of 3D object detection and localization, we introduce a novel deep learning-based pixel-wise 3D localization algorithm. The algorithm, trained directly with SfM point cloud data, can locate objects detected from 2D images in a 3D space with high accuracy by estimating not only depth from monocular images but also lateral and height distances. In addition, we also propose a point clustering and thresholding algorithm to improve the robustness of the system to errors. We have performed experiments on two distinct areas - a campus and a residential area - with different types of cameras, lighting, and weather conditions. The changes were detected with 85% and 100% accuracy in the campus and residential areas, respectively. The errors in the campus area were mainly due to traffic signs seen from a far distance to the vehicle and intended for pedestrians and cyclists only. We also conducted cause analysis of the detection and localization errors to measure the impact from the performance of the background technology in use.



### Attentive Contractive Flow: Improved Contractive Flows with Lipschitz-constrained Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/2109.12135v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.12135v3)
- **Published**: 2021-09-24 18:02:49+00:00
- **Updated**: 2021-12-07 18:11:47+00:00
- **Authors**: Avideep Mukherjee, Badri Narayan Patro, Sahil Sidheekh, Maneesh Singh, Vinay P. Namboodiri
- **Comment**: None
- **Journal**: None
- **Summary**: Normalizing flows provide an elegant method for obtaining tractable density estimates from distributions by using invertible transformations. The main challenge is to improve the expressivity of the models while keeping the invertibility constraints intact. We propose to do so via the incorporation of localized self-attention. However, conventional self-attention mechanisms don't satisfy the requirements to obtain invertible flows and can't be naively incorporated into normalizing flows. To address this, we introduce a novel approach called Attentive Contractive Flow (ACF) which utilizes a special category of flow-based generative models - contractive flows. We demonstrate that ACF can be introduced into a variety of state of the art flow models in a plug-and-play manner. This is demonstrated to not only improve the representation power of these models (improving on the bits per dim metric), but also to results in significantly faster convergence in training them. Qualitative results, including interpolations between test images, demonstrate that samples are more realistic and capture local correlations in the data well. We evaluate the results further by performing perturbation analysis using AWGN demonstrating that ACF models (especially the dot-product variant) show better and more consistent resilience to additive noise.



### Deep Neural Networks for Blind Image Quality Assessment: Addressing the Data Challenge
- **Arxiv ID**: http://arxiv.org/abs/2109.12161v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.12161v1)
- **Published**: 2021-09-24 19:48:52+00:00
- **Updated**: 2021-09-24 19:48:52+00:00
- **Authors**: Shahrukh Athar, Zhongling Wang, Zhou Wang
- **Comment**: 22 pages, 7 figures, 16 tables
- **Journal**: None
- **Summary**: The enormous space and diversity of natural images is usually represented by a few small-scale human-rated image quality assessment (IQA) datasets. This casts great challenges to deep neural network (DNN) based blind IQA (BIQA), which requires large-scale training data that is representative of the natural image distribution. It is extremely difficult to create human-rated IQA datasets composed of millions of images due to constraints of subjective testing. While a number of efforts have focused on design innovations to enhance the performance of DNN based BIQA, attempts to address the scarcity of labeled IQA data remain surprisingly missing. To address this data challenge, we construct so far the largest IQA database, namely Waterloo Exploration-II, which contains 3,570 pristine reference and around 3.45 million singly and multiply distorted images. Since subjective testing for such a large dataset is nearly impossible, we develop a novel mechanism that synthetically assigns perceptual quality labels to the distorted images. We construct a DNN-based BIQA model called EONSS, train it on Waterloo Exploration-II, and test it on nine subject-rated IQA datasets, without any retraining or fine-tuning. The results show that with a straightforward DNN architecture, EONSS is able to outperform the very state-of-the-art in BIQA, both in terms of quality prediction performance and execution speed. This study strongly supports the view that the quantity and quality of meaningfully annotated training data, rather than a sophisticated network architecture or training strategy, is the dominating factor that determines the performance of DNN-based BIQA models. (Note: Since this is an ongoing project, the final versions of Waterloo Exploration-II database, quality annotations, and EONSS, will be made publicly available in the future when it culminates.)



### Unsupervised Cross-Modality Domain Adaptation for Segmenting Vestibular Schwannoma and Cochlea with Data Augmentation and Model Ensemble
- **Arxiv ID**: http://arxiv.org/abs/2109.12169v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.12169v4)
- **Published**: 2021-09-24 20:10:05+00:00
- **Updated**: 2022-08-24 14:19:19+00:00
- **Authors**: Hao Li, Dewei Hu, Qibang Zhu, Kathleen E. Larson, Huahong Zhang, Ipek Oguz
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic resonance images (MRIs) are widely used to quantify vestibular schwannoma and the cochlea. Recently, deep learning methods have shown state-of-the-art performance for segmenting these structures. However, training segmentation models may require manual labels in target domain, which is expensive and time-consuming. To overcome this problem, domain adaptation is an effective way to leverage information from source domain to obtain accurate segmentations without requiring manual labels in target domain. In this paper, we propose an unsupervised learning framework to segment the VS and cochlea. Our framework leverages information from contrast-enhanced T1-weighted (ceT1-w) MRIs and its labels, and produces segmentations for T2-weighted MRIs without any labels in the target domain. We first applied a generator to achieve image-to-image translation. Next, we ensemble outputs from an ensemble of different models to obtain final segmentations. To cope with MRIs from different sites/scanners, we applied various 'online' augmentations during training to better capture the geometric variability and the variability in image appearance and quality. Our method is easy to build and produces promising segmentations, with a mean Dice score of 0.7930 and 0.7432 for VS and cochlea respectively in the validation set.



### MLIM: Vision-and-Language Model Pre-training with Masked Language and Image Modeling
- **Arxiv ID**: http://arxiv.org/abs/2109.12178v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.12178v1)
- **Published**: 2021-09-24 20:25:40+00:00
- **Updated**: 2021-09-24 20:25:40+00:00
- **Authors**: Tarik Arici, Mehmet Saygin Seyfioglu, Tal Neiman, Yi Xu, Son Train, Trishul Chilimbi, Belinda Zeng, Ismail Tutar
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-and-Language Pre-training (VLP) improves model performance for downstream tasks that require image and text inputs. Current VLP approaches differ on (i) model architecture (especially image embedders), (ii) loss functions, and (iii) masking policies. Image embedders are either deep models like ResNet or linear projections that directly feed image-pixels into the transformer. Typically, in addition to the Masked Language Modeling (MLM) loss, alignment-based objectives are used for cross-modality interaction, and RoI feature regression and classification tasks for Masked Image-Region Modeling (MIRM). Both alignment and MIRM objectives mostly do not have ground truth. Alignment-based objectives require pairings of image and text and heuristic objective functions. MIRM relies on object detectors. Masking policies either do not take advantage of multi-modality or are strictly coupled with alignments generated by other models. In this paper, we present Masked Language and Image Modeling (MLIM) for VLP. MLIM uses two loss functions: Masked Language Modeling (MLM) loss and image reconstruction (RECON) loss. We propose Modality Aware Masking (MAM) to boost cross-modality interaction and take advantage of MLM and RECON losses that separately capture text and image reconstruction quality. Using MLM + RECON tasks coupled with MAM, we present a simplified VLP methodology and show that it has better downstream task performance on a proprietary e-commerce multi-modal dataset.



### NanoBatch Privacy: Enabling fast Differentially Private learning on the IPU
- **Arxiv ID**: http://arxiv.org/abs/2109.12191v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.DC, 68T07, I.4.0; I.2.11
- **Links**: [PDF](http://arxiv.org/pdf/2109.12191v2)
- **Published**: 2021-09-24 20:59:04+00:00
- **Updated**: 2022-06-02 22:55:16+00:00
- **Authors**: Edward H. Lee, Mario Michael Krell, Alexander Tsyplikhin, Victoria Rege, Errol Colak, Kristen W. Yeom
- **Comment**: None
- **Journal**: None
- **Summary**: Differentially private SGD (DPSGD) has recently shown promise in deep learning. However, compared to non-private SGD, the DPSGD algorithm places computational overheads that can undo the benefit of batching in GPUs. Micro-batching is a common method to alleviate this and is fully supported in the TensorFlow Privacy library (TFDP). However, it degrades accuracy. We propose NanoBatch Privacy, a lightweight add-on to TFDP to be used on Graphcore IPUs by leveraging batch size of 1 (without microbatching) and gradient accumulation. This allows us to achieve large total batch sizes with minimal impacts to throughput. Second, we illustrate using Cifar-10 how larger batch sizes are not necessarily optimal from a privacy versus utility perspective. On ImageNet, we achieve more than 15x speedup over TFDP versus 8x A100s and significant speedups even across libraries such as Opacus. We also provide two extensions: 1) DPSGD for pipelined models and 2) per-layer clipping that is 15x faster than the Opacus implementation on 8x A100s. Finally as an application case study, we apply NanoBatch training for use on private Covid-19 chest CT prediction.



### An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog
- **Arxiv ID**: http://arxiv.org/abs/2109.12212v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2109.12212v2)
- **Published**: 2021-09-24 21:48:27+00:00
- **Updated**: 2021-09-29 23:02:44+00:00
- **Authors**: Xingyao Wang, David Jurgens
- **Comment**: Findings of EMNLP 2021; 30 pages
- **Journal**: None
- **Summary**: Online conversations include more than just text. Increasingly, image-based responses such as memes and animated gifs serve as culturally recognized and often humorous responses in conversation. However, while NLP has broadened to multimodal models, conversational dialog systems have largely focused only on generating text replies. Here, we introduce a new dataset of 1.56M text-gif conversation turns and introduce a new multimodal conversational model Pepe the King Prawn for selecting gif-based replies. We demonstrate that our model produces relevant and high-quality gif responses and, in a large randomized control trial of multiple models replying to real users, we show that our model replies with gifs that are significantly better received by the community.



### Ground material classification for UAV-based photogrammetric 3D data A 2D-3D Hybrid Approach
- **Arxiv ID**: http://arxiv.org/abs/2109.12221v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.12221v2)
- **Published**: 2021-09-24 22:29:26+00:00
- **Updated**: 2022-10-15 06:28:40+00:00
- **Authors**: Meida Chen, Andrew Feng, Yu Hou, Kyle McCullough, Pratusha Bhuvana Prasad, Lucio Soibelman
- **Comment**: None
- **Journal**: Interservice/Industry Training, Simulation, and Education
  Conference (I/ITSEC) 2021
- **Summary**: In recent years, photogrammetry has been widely used in many areas to create photorealistic 3D virtual data representing the physical environment. The innovation of small unmanned aerial vehicles (sUAVs) has provided additional high-resolution imaging capabilities with low cost for mapping a relatively large area of interest. These cutting-edge technologies have caught the US Army and Navy's attention for the purpose of rapid 3D battlefield reconstruction, virtual training, and simulations. Our previous works have demonstrated the importance of information extraction from the derived photogrammetric data to create semantic-rich virtual environments (Chen et al., 2019). For example, an increase of simulation realism and fidelity was achieved by segmenting and replacing photogrammetric trees with game-ready tree models. In this work, we further investigated the semantic information extraction problem and focused on the ground material segmentation and object detection tasks. The main innovation of this work was that we leveraged both the original 2D images and the derived 3D photogrammetric data to overcome the challenges faced when using each individual data source. For ground material segmentation, we utilized an existing convolutional neural network architecture (i.e., 3DMV) which was originally designed for segmenting RGB-D sensed indoor data. We improved its performance for outdoor photogrammetric data by introducing a depth pooling layer in the architecture to take into consideration the distance between the source images and the reconstructed terrain model. To test the performance of our improved 3DMV, a ground truth ground material database was created using data from the One World Terrain (OWT) data repository. Finally, a workflow for importing the segmented ground materials into a virtual simulation scene was introduced, and visual results are reported in this paper.



### Bringing Generalization to Deep Multi-View Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.12227v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.12227v4)
- **Published**: 2021-09-24 23:02:27+00:00
- **Updated**: 2022-03-13 18:22:27+00:00
- **Authors**: Jeet Vora, Swetanjal Dutta, Kanishk Jain, Shyamgopal Karthik, Vineet Gandhi
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-view Detection (MVD) is highly effective for occlusion reasoning in a crowded environment. While recent works using deep learning have made significant advances in the field, they have overlooked the generalization aspect, which makes them impractical for real-world deployment. The key novelty of our work is to formalize three critical forms of generalization and propose experiments to evaluate them: generalization with i) a varying number of cameras, ii) varying camera positions, and finally, iii) to new scenes. We find that existing state-of-the-art models show poor generalization by overfitting to a single scene and camera configuration. To address the concerns: (a) we propose a novel Generalized MVD (GMVD) dataset, assimilating diverse scenes with changing daytime, camera configurations, varying number of cameras, and (b) we discuss the properties essential to bring generalization to MVD and propose a barebones model to incorporate them. We perform a comprehensive set of experiments on the WildTrack, MultiViewX, and the GMVD datasets to motivate the necessity to evaluate the generalization abilities of MVD methods and to demonstrate the efficacy of the proposed approach. The code and the proposed dataset can be found at https://github.com/jeetv/GMVD



