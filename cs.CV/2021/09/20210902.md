# Arxiv Papers in cs.CV on 2021-09-02
### Variable Augmented Network for Invertible Modality Synthesis-Fusion
- **Arxiv ID**: http://arxiv.org/abs/2109.00670v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.00670v1)
- **Published**: 2021-09-02 02:16:09+00:00
- **Updated**: 2021-09-02 02:16:09+00:00
- **Authors**: Yuhao Wang, Ruirui Liu, Zihao Li, Cailian Yang, Qiegen Liu
- **Comment**: Page 10. arXiv admin note: text overlap with arXiv:2002.05000,
  arXiv:2103.15061 by other authors
- **Journal**: None
- **Summary**: As an effective way to integrate the information contained in multiple medical images under different modalities, medical image synthesis and fusion have emerged in various clinical applications such as disease diagnosis and treatment planning. In this paper, an invertible and variable augmented network (iVAN) is proposed for medical image synthesis and fusion. In iVAN, the channel number of the network input and output is the same through variable augmentation technology, and data relevance is enhanced, which is conducive to the generation of characterization information. Meanwhile, the invertible network is used to achieve the bidirectional inference processes. Due to the invertible and variable augmentation schemes, iVAN can not only be applied to the mappings of multi-input to one-output and multi-input to multi-output, but also be applied to one-input to multi-output. Experimental results demonstrated that the proposed method can obtain competitive or superior performance in comparison to representative medical image synthesis and fusion methods.



### Regional Adversarial Training for Better Robust Generalization
- **Arxiv ID**: http://arxiv.org/abs/2109.00678v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.00678v2)
- **Published**: 2021-09-02 02:48:02+00:00
- **Updated**: 2021-09-04 01:36:12+00:00
- **Authors**: Chuanbiao Song, Yanbo Fan, Yichen Yang, Baoyuan Wu, Yiming Li, Zhifeng Li, Kun He
- **Comment**: 10 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: Adversarial training (AT) has been demonstrated as one of the most promising defense methods against various adversarial attacks. To our knowledge, existing AT-based methods usually train with the locally most adversarial perturbed points and treat all the perturbed points equally, which may lead to considerably weaker adversarial robust generalization on test data. In this work, we introduce a new adversarial training framework that considers the diversity as well as characteristics of the perturbed points in the vicinity of benign samples. To realize the framework, we propose a Regional Adversarial Training (RAT) defense method that first utilizes the attack path generated by the typical iterative attack method of projected gradient descent (PGD), and constructs an adversarial region based on the attack path. Then, RAT samples diverse perturbed training points efficiently inside this region, and utilizes a distance-aware label smoothing mechanism to capture our intuition that perturbed points at different locations should have different impact on the model performance. Extensive experiments on several benchmark datasets show that RAT consistently makes significant improvement on standard adversarial training (SAT), and exhibits better robust generalization.



### Deep Face Video Inpainting via UV Mapping
- **Arxiv ID**: http://arxiv.org/abs/2109.00681v2
- **DOI**: 10.1109/TIP.2023.3240835
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00681v2)
- **Published**: 2021-09-02 03:00:08+00:00
- **Updated**: 2023-02-13 10:55:16+00:00
- **Authors**: Wenqi Yang, Zhenfang Chen, Chaofeng Chen, Guanying Chen, Kwan-Yee K. Wong
- **Comment**: TIP 2023
- **Journal**: None
- **Summary**: This paper addresses the problem of face video inpainting. Existing video inpainting methods target primarily at natural scenes with repetitive patterns. They do not make use of any prior knowledge of the face to help retrieve correspondences for the corrupted face. They therefore only achieve sub-optimal results, particularly for faces under large pose and expression variations where face components appear very differently across frames. In this paper, we propose a two-stage deep learning method for face video inpainting. We employ 3DMM as our 3D face prior to transform a face between the image space and the UV (texture) space. In Stage I, we perform face inpainting in the UV space. This helps to largely remove the influence of face poses and expressions and makes the learning task much easier with well aligned face features. We introduce a frame-wise attention module to fully exploit correspondences in neighboring frames to assist the inpainting task. In Stage II, we transform the inpainted face regions back to the image space and perform face video refinement that inpaints any background regions not covered in Stage I and also refines the inpainted face regions. Extensive experiments have been carried out which show our method can significantly outperform methods based merely on 2D information, especially for faces under large pose and expression variations. Project page: https://ywq.github.io/FVIP



### AnANet: Modeling Association and Alignment for Cross-modal Correlation Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.00693v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.00693v1)
- **Published**: 2021-09-02 03:42:35+00:00
- **Updated**: 2021-09-02 03:42:35+00:00
- **Authors**: Nan Xu, Junyan Wang, Yuan Tian, Ruike Zhang, Wenji Mao
- **Comment**: None
- **Journal**: None
- **Summary**: The explosive increase of multimodal data makes a great demand in many cross-modal applications that follow the strict prior related assumption. Thus researchers study the definition of cross-modal correlation category and construct various classification systems and predictive models. However, those systems pay more attention to the fine-grained relevant types of cross-modal correlation, ignoring lots of implicit relevant data which are often divided into irrelevant types. What's worse is that none of previous predictive models manifest the essence of cross-modal correlation according to their definition at the modeling stage. In this paper, we present a comprehensive analysis of the image-text correlation and redefine a new classification system based on implicit association and explicit alignment. To predict the type of image-text correlation, we propose the Association and Alignment Network according to our proposed definition (namely AnANet) which implicitly represents the global discrepancy and commonality between image and text and explicitly captures the cross-modal local relevance. The experimental results on our constructed new image-text correlation dataset show the effectiveness of our model.



### FBSNet: A Fast Bilateral Symmetrical Network for Real-Time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.00699v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00699v3)
- **Published**: 2021-09-02 04:16:39+00:00
- **Updated**: 2022-03-06 07:02:14+00:00
- **Authors**: Guangwei Gao, Guoan Xu, Juncheng Li, Yi Yu, Huimin Lu, Jian Yang
- **Comment**: IEEE Transactions on Multimedia, 11 pages, 7 figures, 5 tables
- **Journal**: None
- **Summary**: Real-time semantic segmentation, which can be visually understood as the pixel-level classification task on the input image, currently has broad application prospects, especially in the fast-developing fields of autonomous driving and drone navigation. However, the huge burden of calculation together with redundant parameters are still the obstacles to its technological development. In this paper, we propose a Fast Bilateral Symmetrical Network (FBSNet) to alleviate the above challenges. Specifically, FBSNet employs a symmetrical encoder-decoder structure with two branches, semantic information branch and spatial detail branch. The Semantic Information Branch (SIB) is the main branch with semantic architecture to acquire the contextual information of the input image and meanwhile acquire sufficient receptive field. While the Spatial Detail Branch (SDB) is a shallow and simple network used to establish local dependencies of each pixel for preserving details, which is essential for restoring the original resolution during the decoding phase. Meanwhile, a Feature Aggregation Module (FAM) is designed to effectively combine the output of these two branches. Experimental results of Cityscapes and CamVid show that the proposed FBSNet can strike a good balance between accuracy and efficiency. Specifically, it obtains 70.9\% and 68.9\% mIoU along with the inference speed of 90 fps and 120 fps on these two test datasets, respectively, with only 0.62 million parameters on a single RTX 2080Ti GPU. The code is available at https://github.com/IVIPLab/FBSNet.



### Studying the Effects of Self-Attention for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2109.01486v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01486v1)
- **Published**: 2021-09-02 07:07:16+00:00
- **Updated**: 2021-09-02 07:07:16+00:00
- **Authors**: Adrit Rao, Jongchan Park, Sanghyun Woo, Joon-Young Lee, Oliver Aalami
- **Comment**: ICCV 2021 CVAMD
- **Journal**: None
- **Summary**: When the trained physician interprets medical images, they understand the clinical importance of visual features. By applying cognitive attention, they apply greater focus onto clinically relevant regions while disregarding unnecessary features. The use of computer vision to automate the classification of medical images is widely studied. However, the standard convolutional neural network (CNN) does not necessarily employ subconscious feature relevancy evaluation techniques similar to the trained medical specialist and evaluates features more generally. Self-attention mechanisms enable CNNs to focus more on semantically important regions or aggregated relevant context with long-range dependencies. By using attention, medical image analysis systems can potentially become more robust by focusing on more important clinical feature regions. In this paper, we provide a comprehensive comparison of various state-of-the-art self-attention mechanisms across multiple medical image analysis tasks. Through both quantitative and qualitative evaluations along with a clinical user-centric survey study, we aim to provide a deeper understanding of the effects of self-attention in medical computer vision tasks.



### Learning 3D Mineral Prospectivity from 3D Geological Models Using Convolutional Neural Networks: Application to a Structure-controlled Hydrothermal Gold Deposit
- **Arxiv ID**: http://arxiv.org/abs/2109.00756v2
- **DOI**: 10.1016/j.cageo.2022.105074
- **Categories**: **physics.geo-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.00756v2)
- **Published**: 2021-09-02 07:34:10+00:00
- **Updated**: 2022-01-14 12:20:38+00:00
- **Authors**: Hao Deng, Yang Zheng, Jin Chen, Shuyan Yu, Keyan Xiao, Xiancheng Mao
- **Comment**: None
- **Journal**: None
- **Summary**: The three-dimensional (3D) geological models are the typical and key data source in the 3D mineral prospecitivity modeling. Identifying prospectivity-informative predictor variables from the 3D geological models is a challenging and tedious task. Motivated by the ability of convolutional neural networks (CNNs) to learn the intrinsic features, in this paper, we present a novel method that leverages CNNs to learn 3D mineral prospectivity from the 3D geological models. By exploiting the learning ability of CNNs, the presented method allows for disentangling complex correlation to the mineralization and thus opens a door to circumvent the tedious work for designing the predictor variables. Specifically, to explore the unstructured 3D geological models with the CNNs whose input should be structured, we develop a 2D CNN framework in which the geometry of geological boundary is compiled and reorganized into multi-channel images and fed into the CNN. This ensures an effective and efficient training of CNNs while allowing the prospective model to approximate the ore-forming process. The presented method is applied to a typical structure-controlled hydrothermal deposit, the Dayingezhuang gold deposit, eastern China, in which the presented method was compared with the prospectivity modeling methods using hand-designed predictor variables. The results demonstrate the presented method capacitates a performance boost of the 3D prospectivity modeling and empowers us to decrease work-load and prospecting risk in prediction of deep-seated orebodies.



### Direct PET Image Reconstruction Incorporating Deep Image Prior and a Forward Projection Model
- **Arxiv ID**: http://arxiv.org/abs/2109.00768v1
- **DOI**: 10.1109/TRPMS.2022.3161569
- **Categories**: **physics.med-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.00768v1)
- **Published**: 2021-09-02 08:07:58+00:00
- **Updated**: 2021-09-02 08:07:58+00:00
- **Authors**: Fumio Hashimoto, Kibo Ote
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: IEEE Trans. Radiat. Plasma Med. Sci. 6 (2022) 841
- **Summary**: Convolutional neural networks (CNNs) have recently achieved remarkable performance in positron emission tomography (PET) image reconstruction. In particular, CNN-based direct PET image reconstruction, which directly generates the reconstructed image from the sinogram, has potential applicability to PET image enhancements because it does not require image reconstruction algorithms, which often produce some artifacts. However, these deep learning-based, direct PET image reconstruction algorithms have the disadvantage that they require a large number of high-quality training datasets. In this study, we propose an unsupervised direct PET image reconstruction method that incorporates a deep image prior framework. Our proposed method incorporates a forward projection model with a loss function to achieve unsupervised direct PET image reconstruction from sinograms. To compare our proposed direct reconstruction method with the filtered back projection (FBP) and maximum likelihood expectation maximization (ML-EM) algorithms, we evaluated using Monte Carlo simulation data of brain [$^{18}$F]FDG PET scans. The results demonstrate that our proposed direct reconstruction quantitatively and qualitatively outperforms the FBP and ML-EM algorithms with respect to peak signal-to-noise ratio and structural similarity index.



### Better Self-training for Image Classification through Self-supervision
- **Arxiv ID**: http://arxiv.org/abs/2109.00778v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00778v2)
- **Published**: 2021-09-02 08:24:41+00:00
- **Updated**: 2021-09-09 23:03:50+00:00
- **Authors**: Attaullah Sahito, Eibe Frank, Bernhard Pfahringer
- **Comment**: added link to code repository
- **Journal**: None
- **Summary**: Self-training is a simple semi-supervised learning approach: Unlabelled examples that attract high-confidence predictions are labelled with their predictions and added to the training set, with this process being repeated multiple times. Recently, self-supervision -- learning without manual supervision by solving an automatically-generated pretext task -- has gained prominence in deep learning. This paper investigates three different ways of incorporating self-supervision into self-training to improve accuracy in image classification: self-supervision as pretraining only, self-supervision performed exclusively in the first iteration of self-training, and self-supervision added to every iteration of self-training. Empirical results on the SVHN, CIFAR-10, and PlantVillage datasets, using both training from scratch, and Imagenet-pretrained weights, show that applying self-supervision only in the first iteration of self-training can greatly improve accuracy, for a modest increase in computation time.



### Non-Photorealistic Rendering of Layered Materials: A Multispectral Approach
- **Arxiv ID**: http://arxiv.org/abs/2109.00780v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, I.3.3; I.3.8; I.4.0; I.4.1; I.4.3; I.4.8; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2109.00780v1)
- **Published**: 2021-09-02 08:40:05+00:00
- **Updated**: 2021-09-02 08:40:05+00:00
- **Authors**: Corey Toler-Franklin, Shashank Ranjan
- **Comment**: 15 pages, 35 figures
- **Journal**: None
- **Summary**: We present multispectral rendering techniques for visualizing layered materials found in biological specimens. We are the first to use acquired data from the near-infrared and ultraviolet spectra for non-photorealistic rendering (NPR). Several plant and animal species are more comprehensively understood by multispectral analysis. However, traditional NPR techniques ignore unique information outside the visible spectrum. We introduce algorithms and principles for processing wavelength dependent surface normals and reflectance. Our registration and feature detection methods are used to formulate stylization effects not considered by current NPR methods including: Spectral Band Shading which isolates and emphasizes shape features at specific wavelengths at multiple scales. Experts in our user study demonstrate the effectiveness of our system for applications in the biological sciences.



### Transfer of Pretrained Model Weights Substantially Improves Semi-Supervised Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.00788v2
- **DOI**: 10.1007/978-3-030-64984-5_34
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00788v2)
- **Published**: 2021-09-02 08:58:34+00:00
- **Updated**: 2021-09-09 22:42:22+00:00
- **Authors**: Attaullah Sahito, Eibe Frank, Bernhard Pfahringer
- **Comment**: added link to code and data repo
- **Journal**: In: AI 2020: Advances in Artificial Intelligence. AI 2020 .
  Lecture Notes in Computer Science, vol 12576. Springer, Cham
- **Summary**: Deep neural networks produce state-of-the-art results when trained on a large number of labeled examples but tend to overfit when small amounts of labeled examples are used for training. Creating a large number of labeled examples requires considerable resources, time, and effort. If labeling new data is not feasible, so-called semi-supervised learning can achieve better generalisation than purely supervised learning by employing unlabeled instances as well as labeled ones. The work presented in this paper is motivated by the observation that transfer learning provides the opportunity to potentially further improve performance by exploiting models pretrained on a similar domain. More specifically, we explore the use of transfer learning when performing semi-supervised learning using self-learning. The main contribution is an empirical evaluation of transfer learning using different combinations of similarity metric learning methods and label propagation algorithms in semi-supervised learning. We find that transfer learning always substantially improves the model's accuracy when few labeled examples are available, regardless of the type of loss used for training the neural network. This finding is obtained by performing extensive experiments on the SVHN, CIFAR10, and Plant Village image classification datasets and applying pretrained weights from Imagenet for transfer learning.



### Semi-Supervised Learning using Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.00794v2
- **DOI**: 10.1007/978-3-030-35288-2_47
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.00794v2)
- **Published**: 2021-09-02 09:06:35+00:00
- **Updated**: 2021-09-09 22:22:08+00:00
- **Authors**: Attaullah Sahito, Eibe Frank, Bernhard Pfahringer
- **Comment**: added link of GitHub repository
- **Journal**: In AI 2019: Advances in Artificial Intelligence. AI 2019 . Lecture
  Notes in Computer Science, vol 11919. Springer, Cham
- **Summary**: Neural networks have been successfully used as classification models yielding state-of-the-art results when trained on a large number of labeled samples. These models, however, are more difficult to train successfully for semi-supervised problems where small amounts of labeled instances are available along with a large number of unlabeled instances. This work explores a new training method for semi-supervised learning that is based on similarity function learning using a Siamese network to obtain a suitable embedding. The learned representations are discriminative in Euclidean space, and hence can be used for labeling unlabeled instances using a nearest-neighbor classifier. Confident predictions of unlabeled instances are used as true labels for retraining the Siamese network on the expanded training set. This process is applied iteratively. We perform an empirical study of this iterative self-training algorithm. For improving unlabeled predictions, local learning with global consistency [22] is also evaluated.



### Multi-Modal Zero-Shot Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.00796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00796v1)
- **Published**: 2021-09-02 09:10:39+00:00
- **Updated**: 2021-09-02 09:10:39+00:00
- **Authors**: Razieh Rastgoo, Kourosh Kiani, Sergio Escalera, Mohammad Sabokrou
- **Comment**: arXiv admin note: text overlap with arXiv:2108.10059
- **Journal**: None
- **Summary**: Zero-Shot Learning (ZSL) has rapidly advanced in recent years. Towards overcoming the annotation bottleneck in the Sign Language Recognition (SLR), we explore the idea of Zero-Shot Sign Language Recognition (ZS-SLR) with no annotated visual examples, by leveraging their textual descriptions. In this way, we propose a multi-modal Zero-Shot Sign Language Recognition (ZS-SLR) model harnessing from the complementary capabilities of deep features fused with the skeleton-based ones. A Transformer-based model along with a C3D model is used for hand detection and deep features extraction, respectively. To make a trade-off between the dimensionality of the skeletonbased and deep features, we use an Auto-Encoder (AE) on top of the Long Short Term Memory (LSTM) network. Finally, a semantic space is used to map the visual features to the lingual embedding of the class labels, achieved via the Bidirectional Encoder Representations from Transformers (BERT) model. Results on four large-scale datasets, RKS-PERSIANSIGN, First-Person, ASLVID, and isoGD, show the superiority of the proposed model compared to state-of-the-art alternatives in ZS-SLR.



### Anatomical-Guided Attention Enhances Unsupervised PET Image Denoising Performance
- **Arxiv ID**: http://arxiv.org/abs/2109.00802v2
- **DOI**: 10.1016/j.media.2021.102226
- **Categories**: **physics.med-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.00802v2)
- **Published**: 2021-09-02 09:27:07+00:00
- **Updated**: 2021-09-08 01:53:01+00:00
- **Authors**: Yuya Onishi, Fumio Hashimoto, Kibo Ote, Hiroyuki Ohba, Ryosuke Ota, Etsuji Yoshikawa, Yasuomi Ouchi
- **Comment**: 30 pages, 12 figures
- **Journal**: Med. Image Anal. 74 (2021) 102226
- **Summary**: Although supervised convolutional neural networks (CNNs) often outperform conventional alternatives for denoising positron emission tomography (PET) images, they require many low- and high-quality reference PET image pairs. Herein, we propose an unsupervised 3D PET image denoising method based on an anatomical information-guided attention mechanism. The proposed magnetic resonance-guided deep decoder (MR-GDD) utilizes the spatial details and semantic features of MR-guidance image more effectively by introducing encoder-decoder and deep decoder subnetworks. Moreover, the specific shapes and patterns of the guidance image do not affect the denoised PET image, because the guidance image is input to the network through an attention gate. In a Monte Carlo simulation of [$^{18}$F]fluoro-2-deoxy-D-glucose (FDG), the proposed method achieved the highest peak signal-to-noise ratio and structural similarity (27.92 $\pm$ 0.44 dB/0.886 $\pm$ 0.007), as compared with Gaussian filtering (26.68 $\pm$ 0.10 dB/0.807 $\pm$ 0.004), image guided filtering (27.40 $\pm$ 0.11 dB/0.849 $\pm$ 0.003), deep image prior (DIP) (24.22 $\pm$ 0.43 dB/0.737 $\pm$ 0.017), and MR-DIP (27.65 $\pm$ 0.42 dB/0.879 $\pm$ 0.007). Furthermore, we experimentally visualized the behavior of the optimization process, which is often unknown in unsupervised CNN-based restoration problems. For preclinical (using [$^{18}$F]FDG and [$^{11}$C]raclopride) and clinical (using [$^{18}$F]florbetapir) studies, the proposed method demonstrates state-of-the-art denoising performance while retaining spatial resolution and quantitative accuracy, despite using a common network architecture for various noisy PET images with 1/10th of the full counts. These results suggest that the proposed MR-GDD can reduce PET scan times and PET tracer doses considerably without impacting patients.



### Evaluating the Single-Shot MultiBox Detector and YOLO Deep Learning Models for the Detection of Tomatoes in a Greenhouse
- **Arxiv ID**: http://arxiv.org/abs/2109.00810v1
- **DOI**: 10.3390/s21103569
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.00810v1)
- **Published**: 2021-09-02 09:39:12+00:00
- **Updated**: 2021-09-02 09:39:12+00:00
- **Authors**: Sandro A. Magalhães, Luís Castro, Germano Moreira, Filipe N. Santos, mário Cunha, Jorge Dias, António P. Moreira
- **Comment**: Dataset at DOI:10.25747/pc1e-nk92
- **Journal**: Magalh\~aes, S.A.; Castro, L.; Moreira, G.; Santos, F.N.; Cunha,
  M.; Dias, J.; Moreira, A.P. Evaluating the Single-Shot MultiBox Detector and
  YOLO Deep Learning Models for the Detection of Tomatoes in a Greenhouse.
  Sensors 2021, 21, 3569
- **Summary**: The development of robotic solutions for agriculture requires advanced perception capabilities that can work reliably in any crop stage. For example, to automatise the tomato harvesting process in greenhouses, the visual perception system needs to detect the tomato in any life cycle stage (flower to the ripe tomato). The state-of-the-art for visual tomato detection focuses mainly on ripe tomato, which has a distinctive colour from the background. This paper contributes with an annotated visual dataset of green and reddish tomatoes. This kind of dataset is uncommon and not available for research purposes. This will enable further developments in edge artificial intelligence for in situ and in real-time visual tomato detection required for the development of harvesting robots. Considering this dataset, five deep learning models were selected, trained and benchmarked to detect green and reddish tomatoes grown in greenhouses. Considering our robotic platform specifications, only the Single-Shot MultiBox Detector (SSD) and YOLO architectures were considered. The results proved that the system can detect green and reddish tomatoes, even those occluded by leaves. SSD MobileNet v2 had the best performance when compared against SSD Inception v2, SSD ResNet 50, SSD ResNet 101 and YOLOv4 Tiny, reaching an F1-score of 66.15%, an mAP of 51.46% and an inference time of 16.44 ms with the NVIDIA Turing Architecture platform, an NVIDIA Tesla T4, with 12 GB. YOLOv4 Tiny also had impressive results, mainly concerning inferring times of about 5 ms.



### Built Year Prediction from Buddha Face with Heterogeneous Labels
- **Arxiv ID**: http://arxiv.org/abs/2109.00812v1
- **DOI**: 10.1145/3475720.3484441
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2109.00812v1)
- **Published**: 2021-09-02 09:42:52+00:00
- **Updated**: 2021-09-02 09:42:52+00:00
- **Authors**: Yiming Qian, Cheikh Brahim El Vaigh, Yuta Nakashima, Benjamin Renoust, Hajime Nagahara, Yutaka Fujioka
- **Comment**: None
- **Journal**: None
- **Summary**: Buddha statues are a part of human culture, especially of the Asia area, and they have been alongside human civilisation for more than 2,000 years. As history goes by, due to wars, natural disasters, and other reasons, the records that show the built years of Buddha statues went missing, which makes it an immense work for historians to estimate the built years. In this paper, we pursue the idea of building a neural network model that automatically estimates the built years of Buddha statues based only on their face images. Our model uses a loss function that consists of three terms: an MSE loss that provides the basis for built year estimation; a KL divergence-based loss that handles the samples with both an exact built year and a possible range of built years (e.g., dynasty or centuries) estimated by historians; finally a regularisation that utilises both labelled and unlabelled samples based on manifold assumption. By combining those three terms in the training process, we show that our method is able to estimate built years for given images with 37.5 years of a mean absolute error on the test set.



### Deep Learning-based mitosis detection in breast cancer histologic samples
- **Arxiv ID**: http://arxiv.org/abs/2109.00816v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.00816v1)
- **Published**: 2021-09-02 09:47:43+00:00
- **Updated**: 2021-09-02 09:47:43+00:00
- **Authors**: Michel Halmes, Hippolyte Heuberger, Sylvain Berlemont
- **Comment**: None
- **Journal**: None
- **Summary**: This is the submission for mitosis detection in the context of the MIDOG 2021 challenge. It is based on the two-stage objection model Faster RCNN as well as DenseNet as a backbone for the neural network architecture. It achieves a F1-score of 0.6645 on the Preliminary Test Phase Leaderboard.



### Rotation Invariance and Extensive Data Augmentation: a strategy for the Mitosis Domain Generalization (MIDOG) Challenge
- **Arxiv ID**: http://arxiv.org/abs/2109.00823v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00823v2)
- **Published**: 2021-09-02 10:09:02+00:00
- **Updated**: 2021-09-26 17:33:23+00:00
- **Authors**: Maxime W. Lafarge, Viktor H. Koelzer
- **Comment**: None
- **Journal**: None
- **Summary**: Automated detection of mitotic figures in histopathology images is a challenging task: here, we present the different steps that describe the strategy we applied to participate in the MIDOG 2021 competition. The purpose of the competition was to evaluate the generalization of solutions to images acquired with unseen target scanners (hidden for the participants) under the constraint of using training data from a limited set of four independent source scanners. Given this goal and constraints, we joined the challenge by proposing a straight-forward solution based on a combination of state-of-the-art deep learning methods with the aim of yielding robustness to possible scanner-related distributional shifts at inference time. Our solution combines methods that were previously shown to be efficient for mitosis detection: hard negative mining, extensive data augmentation, rotation-invariant convolutional networks.   We trained five models with different splits of the provided dataset. The subsequent classifiers produced F1-scores with a mean and standard deviation of 0.747+/-0.032 on the test splits. The resulting ensemble constitutes our candidate algorithm: its automated evaluation on the preliminary test set of the challenge returned a F1-score of 0.6828.



### SlowFast Rolling-Unrolling LSTMs for Action Anticipation in Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/2109.00829v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.00829v1)
- **Published**: 2021-09-02 10:20:18+00:00
- **Updated**: 2021-09-02 10:20:18+00:00
- **Authors**: Nada Osman, Guglielmo Camporese, Pasquale Coscia, Lamberto Ballan
- **Comment**: Accepted to EPIC@ICCV 2021
- **Journal**: None
- **Summary**: Action anticipation in egocentric videos is a difficult task due to the inherently multi-modal nature of human actions. Additionally, some actions happen faster or slower than others depending on the actor or surrounding context which could vary each time and lead to different predictions. Based on this idea, we build upon RULSTM architecture, which is specifically designed for anticipating human actions, and propose a novel attention-based technique to evaluate, simultaneously, slow and fast features extracted from three different modalities, namely RGB, optical flow, and extracted objects. Two branches process information at different time scales, i.e., frame-rates, and several fusion schemes are considered to improve prediction accuracy. We perform extensive experiments on EpicKitchens-55 and EGTEA Gaze+ datasets, and demonstrate that our technique systematically improves the results of RULSTM architecture for Top-5 accuracy metric at different anticipation times.



### Stain-Robust Mitotic Figure Detection for the Mitosis Domain Generalization Challenge
- **Arxiv ID**: http://arxiv.org/abs/2109.00853v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00853v2)
- **Published**: 2021-09-02 11:44:42+00:00
- **Updated**: 2021-09-29 14:29:02+00:00
- **Authors**: Mostafa Jahanifar, Adam Shephard, Neda Zamani Tajeddin, R. M. Saad Bashir, Mohsin Bilal, Syed Ali Khurram, Fayyaz Minhas, Nasir Rajpoot
- **Comment**: MIDOG challenge at MICCAI 2021
- **Journal**: None
- **Summary**: The detection of mitotic figures from different scanners/sites remains an important topic of research, owing to its potential in assisting clinicians with tumour grading. The MItosis DOmain Generalization (MIDOG) challenge aims to test the robustness of detection models on unseen data from multiple scanners for this task. We present a short summary of the approach employed by the TIA Centre team to address this challenge. Our approach is based on a hybrid detection model, where mitotic candidates are segmented on stain normalised images, before being refined by a deep learning classifier. Cross-validation on the training images achieved the F1-score of 0.786 and 0.765 on the preliminary test set, demonstrating the generalizability of our model to unseen data from new scanners.



### Generative Models for Multi-Illumination Color Constancy
- **Arxiv ID**: http://arxiv.org/abs/2109.00863v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00863v1)
- **Published**: 2021-09-02 12:24:40+00:00
- **Updated**: 2021-09-02 12:24:40+00:00
- **Authors**: Partha Das, Yang Liu, Sezer Karaoglu, Theo Gevers
- **Comment**: Accepted in International Conference on Computer Vision Workshop
  (ICCVW) 2021
- **Journal**: None
- **Summary**: In this paper, the aim is multi-illumination color constancy. However, most of the existing color constancy methods are designed for single light sources. Furthermore, datasets for learning multiple illumination color constancy are largely missing. We propose a seed (physics driven) based multi-illumination color constancy method. GANs are exploited to model the illumination estimation problem as an image-to-image domain translation problem. Additionally, a novel multi-illumination data augmentation method is proposed. Experiments on single and multi-illumination datasets show that our methods outperform sota methods.



### Real World Robustness from Systematic Noise
- **Arxiv ID**: http://arxiv.org/abs/2109.00864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00864v1)
- **Published**: 2021-09-02 12:25:16+00:00
- **Updated**: 2021-09-02 12:25:16+00:00
- **Authors**: Yan Wang, Yuhang Li, Ruihao Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Systematic error, which is not determined by chance, often refers to the inaccuracy (involving either the observation or measurement process) inherent to a system. In this paper, we exhibit some long-neglected but frequent-happening adversarial examples caused by systematic error. More specifically, we find the trained neural network classifier can be fooled by inconsistent implementations of image decoding and resize. This tiny difference between these implementations often causes an accuracy drop from training to deployment. To benchmark these real-world adversarial examples, we propose ImageNet-S dataset, which enables researchers to measure a classifier's robustness to systematic error. For example, we find a normal ResNet-50 trained on ImageNet can have 1%-5% accuracy difference due to the systematic error. Together our evaluation and dataset may aid future work toward real-world robustness and practical generalization.



### Effect of the output activation function on the probabilities and errors in medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.00903v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.00903v1)
- **Published**: 2021-09-02 12:51:14+00:00
- **Updated**: 2021-09-02 12:51:14+00:00
- **Authors**: Lars Nieradzik, Gerik Scheuermann, Dorothee Saur, Christina Gillmann
- **Comment**: None
- **Journal**: None
- **Summary**: The sigmoid activation is the standard output activation function in binary classification and segmentation with neural networks. Still, there exist a variety of other potential output activation functions, which may lead to improved results in medical image segmentation. In this work, we consider how the asymptotic behavior of different output activation and loss functions affects the prediction probabilities and the corresponding segmentation errors. For cross entropy, we show that a faster rate of change of the activation function correlates with better predictions, while a slower rate of change can improve the calibration of probabilities. For dice loss, we found that the arctangent activation function is superior to the sigmoid function. Furthermore, we provide a test space for arbitrary output activation functions in the area of medical image segmentation. We tested seven activation functions in combination with three loss functions on four different medical image segmentation tasks to provide a classification of which function is best suited in this application scenario.



### FA-GAN: Feature-Aware GAN for Text to Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2109.00907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00907v1)
- **Published**: 2021-09-02 13:05:36+00:00
- **Updated**: 2021-09-02 13:05:36+00:00
- **Authors**: Eunyeong Jeon, Kunhee Kim, Daijin Kim
- **Comment**: ICIP 2021
- **Journal**: None
- **Summary**: Text-to-image synthesis aims to generate a photo-realistic image from a given natural language description. Previous works have made significant progress with Generative Adversarial Networks (GANs). Nonetheless, it is still hard to generate intact objects or clear textures (Fig 1). To address this issue, we propose Feature-Aware Generative Adversarial Network (FA-GAN) to synthesize a high-quality image by integrating two techniques: a self-supervised discriminator and a feature-aware loss. First, we design a self-supervised discriminator with an auxiliary decoder so that the discriminator can extract better representation. Secondly, we introduce a feature-aware loss to provide the generator more direct supervision by employing the feature representation from the self-supervised discriminator. Experiments on the MS-COCO dataset show that our proposed method significantly advances the state-of-the-art FID score from 28.92 to 24.58.



### Impact of Attention on Adversarial Robustness of Image Classification Models
- **Arxiv ID**: http://arxiv.org/abs/2109.00936v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00936v1)
- **Published**: 2021-09-02 13:26:32+00:00
- **Updated**: 2021-09-02 13:26:32+00:00
- **Authors**: Prachi Agrawal, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali Agarwal
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks against deep learning models have gained significant attention and recent works have proposed explanations for the existence of adversarial examples and techniques to defend the models against these attacks. Attention in computer vision has been used to incorporate focused learning of important features and has led to improved accuracy. Recently, models with attention mechanisms have been proposed to enhance adversarial robustness. Following this context, this work aims at a general understanding of the impact of attention on adversarial robustness. This work presents a comparative study of adversarial robustness of non-attention and attention based image classification models trained on CIFAR-10, CIFAR-100 and Fashion MNIST datasets under the popular white box and black box attacks. The experimental results show that the robustness of attention based models may be dependent on the datasets used i.e. the number of classes involved in the classification. In contrast to the datasets with less number of classes, attention based models are observed to show better robustness towards classification.



### SetMargin Loss applied to Deep Keystroke Biometrics with Circle Packing Interpretation
- **Arxiv ID**: http://arxiv.org/abs/2109.00938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.00938v1)
- **Published**: 2021-09-02 13:26:57+00:00
- **Updated**: 2021-09-02 13:26:57+00:00
- **Authors**: Aythami Morales, Julian Fierrez, Alejandro Acien, Ruben Tolosana, Ignacio Serna
- **Comment**: Papper accepted in journal Pattern Recognition
- **Journal**: None
- **Summary**: This work presents a new deep learning approach for keystroke biometrics based on a novel Distance Metric Learning method (DML). DML maps input data into a learned representation space that reveals a "semantic" structure based on distances. In this work, we propose a novel DML method specifically designed to address the challenges associated to free-text keystroke identification where the classes used in learning and inference are disjoint. The proposed SetMargin Loss (SM-L) extends traditional DML approaches with a learning process guided by pairs of sets instead of pairs of samples, as done traditionally. The proposed learning strategy allows to enlarge inter-class distances while maintaining the intra-class structure of keystroke dynamics. We analyze the resulting representation space using the mathematical problem known as Circle Packing, which provides neighbourhood structures with a theoretical maximum inter-class distance. We finally prove experimentally the effectiveness of the proposed approach on a challenging task: keystroke biometric identification over a large set of 78,000 subjects. Our method achieves state-of-the-art accuracy on a comparison performed with the best existing approaches.



### Adversarial Robustness for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2109.00946v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.00946v1)
- **Published**: 2021-09-02 13:45:01+00:00
- **Updated**: 2021-09-02 13:45:01+00:00
- **Authors**: Muhammad Awais, Fengwei Zhou, Hang Xu, Lanqing Hong, Ping Luo, Sung-Ho Bae, Zhenguo Li
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Extensive Unsupervised Domain Adaptation (UDA) studies have shown great success in practice by learning transferable representations across a labeled source domain and an unlabeled target domain with deep models. However, previous works focus on improving the generalization ability of UDA models on clean examples without considering the adversarial robustness, which is crucial in real-world applications. Conventional adversarial training methods are not suitable for the adversarial robustness on the unlabeled target domain of UDA since they train models with adversarial examples generated by the supervised loss function. In this work, we leverage intermediate representations learned by multiple robust ImageNet models to improve the robustness of UDA models. Our method works by aligning the features of the UDA model with the robust features learned by ImageNet pre-trained models along with domain adaptation training. It utilizes both labeled and unlabeled domains and instills robustness without any adversarial intervention or label requirement during domain adaptation training. Experimental results show that our method significantly improves adversarial robustness compared to the baseline while keeping clean accuracy on various UDA benchmarks.



### GAM: Explainable Visual Similarity and Classification via Gradient Activation Maps
- **Arxiv ID**: http://arxiv.org/abs/2109.00951v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.00951v1)
- **Published**: 2021-09-02 13:53:08+00:00
- **Updated**: 2021-09-02 13:53:08+00:00
- **Authors**: Oren Barkan, Omri Armstrong, Amir Hertz, Avi Caciularu, Ori Katz, Itzik Malkiel, Noam Koenigstein
- **Comment**: CIKM 2021
- **Journal**: None
- **Summary**: We present Gradient Activation Maps (GAM) - a machinery for explaining predictions made by visual similarity and classification models. By gleaning localized gradient and activation information from multiple network layers, GAM offers improved visual explanations, when compared to existing alternatives. The algorithmic advantages of GAM are explained in detail, and validated empirically, where it is shown that GAM outperforms its alternatives across various tasks and datasets.



### TrouSPI-Net: Spatio-temporal attention on parallel atrous convolutions and U-GRUs for skeletal pedestrian crossing prediction
- **Arxiv ID**: http://arxiv.org/abs/2109.00953v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.00953v2)
- **Published**: 2021-09-02 13:54:02+00:00
- **Updated**: 2021-09-07 14:28:36+00:00
- **Authors**: Joseph Gesnouin, Steve Pechberti, Bogdan Stanciulescu, Fabien Moutarde
- **Comment**: Accepted to IEEE International Conference on Automatic Face & Gesture
  Recognition 2021 (December 15 - 18, 2021) 7 pages, 2 Figures
- **Journal**: None
- **Summary**: Understanding the behaviors and intentions of pedestrians is still one of the main challenges for vehicle autonomy, as accurate predictions of their intentions can guarantee their safety and driving comfort of vehicles. In this paper, we address pedestrian crossing prediction in urban traffic environments by linking the dynamics of a pedestrian's skeleton to a binary crossing intention. We introduce TrouSPI-Net: a context-free, lightweight, multi-branch predictor. TrouSPI-Net extracts spatio-temporal features for different time resolutions by encoding pseudo-images sequences of skeletal joints' positions and processes them with parallel attention modules and atrous convolutions. The proposed approach is then enhanced by processing features such as relative distances of skeletal joints, bounding box positions, or ego-vehicle speed with U-GRUs. Using the newly proposed evaluation procedures for two large public naturalistic data sets for studying pedestrian behavior in traffic: JAAD and PIE, we evaluate TrouSPI-Net and analyze its performance. Experimental results show that TrouSPI-Net achieved 0.76 F1 score on JAAD and 0.80 F1 score on PIE, therefore outperforming current state-of-the-art while being lightweight and context-free.



### Infrared Image Super-Resolution via Heterogeneous Convolutional WGAN
- **Arxiv ID**: http://arxiv.org/abs/2109.00960v1
- **DOI**: 10.1007/978-3-030-89363-7_35
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.00960v1)
- **Published**: 2021-09-02 14:01:05+00:00
- **Updated**: 2021-09-02 14:01:05+00:00
- **Authors**: Yongsong Huang, Zetao Jiang, Qingzhong Wang, Qi Jiang, Guoming Pang
- **Comment**: To be published in the 18th Pacific Rim International Conference on
  Artificial Intelligence (PRICAI-2021)
- **Journal**: None
- **Summary**: Image super-resolution is important in many fields, such as surveillance and remote sensing. However, infrared (IR) images normally have low resolution since the optical equipment is relatively expensive. Recently, deep learning methods have dominated image super-resolution and achieved remarkable performance on visible images; however, IR images have received less attention. IR images have fewer patterns, and hence, it is difficult for deep neural networks (DNNs) to learn diverse features from IR images. In this paper, we present a framework that employs heterogeneous convolution and adversarial training, namely, heterogeneous kernel-based super-resolution Wasserstein GAN (HetSRWGAN), for IR image super-resolution. The HetSRWGAN algorithm is a lightweight GAN architecture that applies a plug-and-play heterogeneous kernel-based residual block. Moreover, a novel loss function that employs image gradients is adopted, which can be applied to an arbitrary model. The proposed HetSRWGAN achieves consistently better performance in both qualitative and quantitative evaluations. According to the experimental results, the whole training process is more stable.



### Accurate shape and phase averaging of time series through Dynamic Time Warping
- **Arxiv ID**: http://arxiv.org/abs/2109.00978v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, math.ST, stat.CO, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/2109.00978v1)
- **Published**: 2021-09-02 14:29:06+00:00
- **Updated**: 2021-09-02 14:29:06+00:00
- **Authors**: George Sioros, Kristian Nymoen
- **Comment**: 29 pages, 11 figures, submitted to Pattern Recognition (0031-3203)
- **Journal**: None
- **Summary**: We propose a novel time series averaging method based on Dynamic Time Warping (DTW). In contrast to previous methods, our algorithm preserves durational information and the distinctive durational features of the sequences due to a simple conversion of the output of DTW into a time sequence and an innovative iterative averaging process. We show that it accurately estimates the ground truth mean sequences and mean temporal location of landmarks in synthetic and real-world datasets and outperforms state-of-the-art methods.



### Dynamic Scene Novel View Synthesis via Deferred Spatio-temporal Consistency
- **Arxiv ID**: http://arxiv.org/abs/2109.01018v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01018v1)
- **Published**: 2021-09-02 15:29:45+00:00
- **Updated**: 2021-09-02 15:29:45+00:00
- **Authors**: Beatrix-Emőke Fülöp-Balogh, Eleanor Tursman, James Tompkin, Julie Digne, Nicolas Bonneel
- **Comment**: Accompanying video: https://youtu.be/RXK2iv980nU
- **Journal**: None
- **Summary**: Structure from motion (SfM) enables us to reconstruct a scene via casual capture from cameras at different viewpoints, and novel view synthesis (NVS) allows us to render a captured scene from a new viewpoint. Both are hard with casual capture and dynamic scenes: SfM produces noisy and spatio-temporally sparse reconstructed point clouds, resulting in NVS with spatio-temporally inconsistent effects. We consider SfM and NVS parts together to ease the challenge. First, for SfM, we recover stable camera poses, then we defer the requirement for temporally-consistent points across the scene and reconstruct only a sparse point cloud per timestep that is noisy in space-time. Second, for NVS, we present a variational diffusion formulation on depths and colors that lets us robustly cope with the noise by enforcing spatio-temporal consistency via per-pixel reprojection weights derived from the input views. Together, this deferred approach generates novel views for dynamic scenes without requiring challenging spatio-temporally consistent reconstructions nor training complex models on large datasets. We demonstrate our algorithm on real-world dynamic scenes against classic and more recent learning-based baseline approaches.



### Extended Object Tracking Using Sets Of Trajectories with a PHD Filter
- **Arxiv ID**: http://arxiv.org/abs/2109.01019v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2109.01019v1)
- **Published**: 2021-09-02 15:32:12+00:00
- **Updated**: 2021-09-02 15:32:12+00:00
- **Authors**: Jakob Sjudin, Martin Marcusson, Lennart Svensson, Lars Hammarstrand
- **Comment**: 8 pages, 4 figures. Submitted to 24th International Conference on
  Information Fusion
- **Journal**: None
- **Summary**: PHD filtering is a common and effective multiple object tracking (MOT) algorithm used in scenarios where the number of objects and their states are unknown. In scenarios where each object can generate multiple measurements per scan, some PHD filters can estimate the extent of the objects as well as their kinematic properties. Most of these approaches are, however, not able to inherently estimate trajectories and rely on ad-hoc methods, such as different labeling schemes, to build trajectories from the state estimates. This paper presents a Gamma Gaussian inverse Wishart mixture PHD filter that can directly estimate sets of trajectories of extended targets by expanding previous research on tracking sets of trajectories for point source objects to handle extended objects. The new filter is compared to an existing extended PHD filter that uses a labeling scheme to build trajectories, and it is shown that the new filter can estimate object trajectories more reliably.



### Shot boundary detection method based on a new extensive dataset and mixed features
- **Arxiv ID**: http://arxiv.org/abs/2109.01057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01057v1)
- **Published**: 2021-09-02 16:19:24+00:00
- **Updated**: 2021-09-02 16:19:24+00:00
- **Authors**: Alexander Gushchin, Anastasia Antsiferova, Dmitriy Vatolin
- **Comment**: None
- **Journal**: None
- **Summary**: Shot boundary detection in video is one of the key stages of video data processing. A new method for shot boundary detection based on several video features, such as color histograms and object boundaries, has been proposed. The developed algorithm was tested on the open BBC Planet Earth [1] and RAI [2] datasets, and the MSU CC datasets, based on videos used in the video codec comparison conducted at MSU, as well as videos from the IBM set, were also plotted. The total dataset for algorithm development and testing exceeded the known TRECVID datasets. Based on the test results, the proposed algorithm for scene change detection outperformed its counterparts with a final F-score of 0.9794.



### 4D-Net for Learned Multi-Modal Alignment
- **Arxiv ID**: http://arxiv.org/abs/2109.01066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01066v1)
- **Published**: 2021-09-02 16:35:00+00:00
- **Updated**: 2021-09-02 16:35:00+00:00
- **Authors**: AJ Piergiovanni, Vincent Casser, Michael S. Ryoo, Anelia Angelova
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: We present 4D-Net, a 3D object detection approach, which utilizes 3D Point Cloud and RGB sensing information, both in time. We are able to incorporate the 4D information by performing a novel dynamic connection learning across various feature representations and levels of abstraction, as well as by observing geometric constraints. Our approach outperforms the state-of-the-art and strong baselines on the Waymo Open Dataset. 4D-Net is better able to use motion cues and dense image information to detect distant objects more successfully.



### SLIDE: Single Image 3D Photography with Soft Layering and Depth-aware Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2109.01068v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2109.01068v1)
- **Published**: 2021-09-02 16:37:20+00:00
- **Updated**: 2021-09-02 16:37:20+00:00
- **Authors**: Varun Jampani, Huiwen Chang, Kyle Sargent, Abhishek Kar, Richard Tucker, Michael Krainin, Dominik Kaeser, William T. Freeman, David Salesin, Brian Curless, Ce Liu
- **Comment**: ICCV 2021 (Oral); Project page: https://varunjampani.github.io/slide
  ; Video: https://www.youtube.com/watch?v=RQio7q-ueY8
- **Journal**: None
- **Summary**: Single image 3D photography enables viewers to view a still image from novel viewpoints. Recent approaches combine monocular depth networks with inpainting networks to achieve compelling results. A drawback of these techniques is the use of hard depth layering, making them unable to model intricate appearance details such as thin hair-like structures. We present SLIDE, a modular and unified system for single image 3D photography that uses a simple yet effective soft layering strategy to better preserve appearance details in novel views. In addition, we propose a novel depth-aware training strategy for our inpainting module, better suited for the 3D photography task. The resulting SLIDE approach is modular, enabling the use of other components such as segmentation and matting for improved layering. At the same time, SLIDE uses an efficient layered depth formulation that only requires a single forward pass through the component networks to produce high quality 3D photos. Extensive experimental analysis on three view-synthesis datasets, in combination with user studies on in-the-wild image collections, demonstrate superior performance of our technique in comparison to existing strong baselines while being conceptually much simpler. Project page: https://varunjampani.github.io/slide



### Towards disease-aware image editing of chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2109.01071v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01071v2)
- **Published**: 2021-09-02 16:38:24+00:00
- **Updated**: 2021-09-03 04:44:14+00:00
- **Authors**: Aakash Saboo, Sai Niranjan Ramachandran, Kai Dierkes, Hacer Yalim Keles
- **Comment**: None
- **Journal**: None
- **Summary**: Disease-aware image editing by means of generative adversarial networks (GANs) constitutes a promising avenue for advancing the use of AI in the healthcare sector. Here, we present a proof of concept of this idea. While GAN-based techniques have been successful in generating and manipulating natural images, their application to the medical domain, however, is still in its infancy. Working with the CheXpert data set, we show that StyleGAN can be trained to generate realistic chest X-rays. Inspired by the Cyclic Reverse Generator (CRG) framework, we train an encoder that allows for faithfully inverting the generator on synthetic X-rays and provides organ-level reconstructions of real ones. Employing a guided manipulation of latent codes, we confer the medical condition of cardiomegaly (increased heart size) onto real X-rays from healthy patients. This work was presented in the Medical Imaging meets Neurips Workshop 2020, which was held as part of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020) in Vancouver, Canada



### Transformer Networks for Data Augmentation of Human Physical Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.01081v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01081v2)
- **Published**: 2021-09-02 16:47:29+00:00
- **Updated**: 2021-09-04 18:17:05+00:00
- **Authors**: Sandeep Ramachandra, Alexander Hoelzemann, Kristof Van Laerhoven
- **Comment**: 5 pages, 7 figures
- **Journal**: None
- **Summary**: Data augmentation is a widely used technique in classification to increase data used in training. It improves generalization and reduces amount of annotated human activity data needed for training which reduces labour and time needed with the dataset. Sensor time-series data, unlike images, cannot be augmented by computationally simple transformation algorithms. State of the art models like Recurrent Generative Adversarial Networks (RGAN) are used to generate realistic synthetic data. In this paper, transformer based generative adversarial networks which have global attention on data, are compared on PAMAP2 and Real World Human Activity Recognition data sets with RGAN. The newer approach provides improvements in time and savings in computational resources needed for data augmentation than previous approach.



### Cascade RCNN for MIDOG Challenge
- **Arxiv ID**: http://arxiv.org/abs/2109.01085v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01085v2)
- **Published**: 2021-09-02 17:02:50+00:00
- **Updated**: 2021-09-26 21:58:42+00:00
- **Authors**: Salar Razavi, Fariba Dambandkhameneh, Dimitri Androutsos, Susan Done, April Khademi
- **Comment**: Two-page preprint abstract submission for MIDOG challenge, see
  https://imi.thi.de/midog/, three figures
- **Journal**: None
- **Summary**: Mitotic counts are one of the key indicators of breast cancer prognosis. However, accurate mitotic cell counting is still a difficult problem and is labourious. Automated methods have been proposed for this task, but are usually dependent on the training images and show poor performance on unseen domains. In this work, we present a multi-stage mitosis detection method based on a Cascade RCNN developed to be sequentially more selective against false positives. On the preliminary test set, the algorithm scores an F1-score of 0.7492.



### On-target Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2109.01087v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01087v1)
- **Published**: 2021-09-02 17:04:18+00:00
- **Updated**: 2021-09-02 17:04:18+00:00
- **Authors**: Dequan Wang, Shaoteng Liu, Sayna Ebrahimi, Evan Shelhamer, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation seeks to mitigate the shift between training on the \emph{source} domain and testing on the \emph{target} domain. Most adaptation methods rely on the source data by joint optimization over source data and target data. Source-free methods replace the source data with a source model by fine-tuning it on target. Either way, the majority of the parameter updates for the model representation and the classifier are derived from the source, and not the target. However, target accuracy is the goal, and so we argue for optimizing as much as possible on the target data. We show significant improvement by on-target adaptation, which learns the representation purely from target data while taking only the source predictions for supervision. In the long-tailed classification setting, we show further improvement by on-target class distribution learning, which learns the (im)balance of classes from target data.



### The Functional Correspondence Problem
- **Arxiv ID**: http://arxiv.org/abs/2109.01097v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.01097v1)
- **Published**: 2021-09-02 17:18:06+00:00
- **Updated**: 2021-09-02 17:18:06+00:00
- **Authors**: Zihang Lai, Senthil Purushwalkam, Abhinav Gupta
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: The ability to find correspondences in visual data is the essence of most computer vision tasks. But what are the right correspondences? The task of visual correspondence is well defined for two different images of same object instance. In case of two images of objects belonging to same category, visual correspondence is reasonably well-defined in most cases. But what about correspondence between two objects of completely different category -- e.g., a shoe and a bottle? Does there exist any correspondence? Inspired by humans' ability to: (a) generalize beyond semantic categories and; (b) infer functional affordances, we introduce the problem of functional correspondences in this paper. Given images of two objects, we ask a simple question: what is the set of correspondences between these two images for a given task? For example, what are the correspondences between a bottle and shoe for the task of pounding or the task of pouring. We introduce a new dataset: FunKPoint that has ground truth correspondences for 10 tasks and 20 object categories. We also introduce a modular task-driven representation for attacking this problem and demonstrate that our learned representation is effective for this task. But most importantly, because our supervision signal is not bound by semantics, we show that our learned representation can generalize better on few-shot classification problem. We hope this paper will inspire our community to think beyond semantics and focus more on cross-category generalization and learning representations for robotics tasks.



### MitoDet: Simple and robust mitosis detection
- **Arxiv ID**: http://arxiv.org/abs/2109.01485v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01485v2)
- **Published**: 2021-09-02 17:19:08+00:00
- **Updated**: 2022-01-20 13:23:10+00:00
- **Authors**: Jakob Dexl, Michaela Benz, Volker Bruns, Petr Kuritcyn, Thomas Wittenberg
- **Comment**: Corrected typos
- **Journal**: None
- **Summary**: Mitotic figure detection is a challenging task in digital pathology that has a direct impact on therapeutic decisions. While automated methods often achieve acceptable results under laboratory conditions, they frequently fail in the clinical deployment phase. This problem can be mainly attributed to a phenomenon called domain shift. An important source of a domain shift is introduced by different microscopes and their camera systems, which noticeably change the color representation of digitized images. In this method description we present our submitted algorithm for the Mitosis Domain Generalization Challenge, which employs a RetinaNet trained with strong data augmentation and achieves an F1 score of 0.7138 on the preliminary test set.



### Benchmarking the Robustness of Instance Segmentation Models
- **Arxiv ID**: http://arxiv.org/abs/2109.01123v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01123v2)
- **Published**: 2021-09-02 17:50:07+00:00
- **Updated**: 2022-08-10 13:52:51+00:00
- **Authors**: Said Fahri Altindis, Yusuf Dalva, Hamza Pehlivan, Aysegul Dundar
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a comprehensive evaluation of instance segmentation models with respect to real-world image corruptions as well as out-of-domain image collections, e.g. images captured by a different set-up than the training dataset. The out-of-domain image evaluation shows the generalization capability of models, an essential aspect of real-world applications and an extensively studied topic of domain adaptation. These presented robustness and generalization evaluations are important when designing instance segmentation models for real-world applications and picking an off-the-shelf pretrained model to directly use for the task at hand. Specifically, this benchmark study includes state-of-the-art network architectures, network backbones, normalization layers, models trained starting from scratch versus pretrained networks, and the effect of multi-task training on robustness and generalization. Through this study, we gain several insights. For example, we find that group normalization enhances the robustness of networks across corruptions where the image contents stay the same but corruptions are added on top. On the other hand, batch normalization improves the generalization of the models across different datasets where statistics of image features change. We also find that single-stage detectors do not generalize well to larger image resolutions than their training size. On the other hand, multi-stage detectors can easily be used on images of different sizes. We hope that our comprehensive study will motivate the development of more robust and reliable instance segmentation models.



### Domain-Robust Mitotic Figure Detection with Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2109.01124v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01124v2)
- **Published**: 2021-09-02 17:51:51+00:00
- **Updated**: 2021-09-30 05:48:37+00:00
- **Authors**: Youjin Chung, Jihoon Cho, Jinah Park
- **Comment**: 2 pages, 3 figures
- **Journal**: None
- **Summary**: We propose a new training scheme for domain generalization in mitotic figure detection. Mitotic figures show different characteristics for each scanner. We consider each scanner as a 'domain' and the image distribution specified for each domain as 'style'. The goal is to train our network to be robust on scanner types by using various 'style' images. To expand the style variance, we transfer a style of the training image into arbitrary styles, by defining a module based on StarGAN. Our model with the proposed training scheme shows positive performance on MIDOG Preliminary Test-Set containing scanners never seen before.



### NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo
- **Arxiv ID**: http://arxiv.org/abs/2109.01129v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01129v3)
- **Published**: 2021-09-02 17:54:31+00:00
- **Updated**: 2021-10-04 21:00:18+00:00
- **Authors**: Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, Jie Zhou
- **Comment**: To appear in ICCV 2021 (Oral). Project page:
  https://weiyithu.github.io/NerfingMVS/
- **Journal**: None
- **Summary**: In this work, we present a new multi-view depth estimation method that utilizes both conventional reconstruction and learning-based priors over the recently proposed neural radiance fields (NeRF). Unlike existing neural network based optimization method that relies on estimated correspondences, our method directly optimizes over implicit volumes, eliminating the challenging step of matching pixels in indoor scenes. The key to our approach is to utilize the learning-based priors to guide the optimization process of NeRF. Our system firstly adapts a monocular depth network over the target scene by finetuning on its sparse SfM+MVS reconstruction from COLMAP. Then, we show that the shape-radiance ambiguity of NeRF still exists in indoor environments and propose to address the issue by employing the adapted depth priors to monitor the sampling process of volume rendering. Finally, a per-pixel confidence map acquired by error computation on the rendered image can be used to further improve the depth quality. Experiments show that our proposed framework significantly outperforms state-of-the-art methods on indoor scenes, with surprising findings presented on the effectiveness of correspondence-based optimization and NeRF-based optimization over the adapted depth priors. In addition, we show that the guided optimization scheme does not sacrifice the original synthesis capability of neural radiance fields, improving the rendering quality on both seen and novel views. Code is available at https://github.com/weiyithu/NerfingMVS.



### A New Semi-Automated Algorithm for Volumetric Segmentation of the Left Ventricle in Temporal 3D Echocardiography Sequences
- **Arxiv ID**: http://arxiv.org/abs/2109.01132v2
- **DOI**: 10.1007/s13239-021-00547-6
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01132v2)
- **Published**: 2021-09-02 17:55:56+00:00
- **Updated**: 2021-09-03 00:37:42+00:00
- **Authors**: Deepa Krishnaswamy, Abhilash R. Hareendranathan, Tan Suwatanaviroj, Pierre Boulanger, Harald Becher, Michelle Noga, Kumaradevan Punithakumar
- **Comment**: 22 pages, 8 figures
- **Journal**: Cardiovascular Engineering and Technology (2021): 1-14
- **Summary**: Purpose: Echocardiography is commonly used as a non-invasive imaging tool in clinical practice for the assessment of cardiac function. However, delineation of the left ventricle is challenging due to the inherent properties of ultrasound imaging, such as the presence of speckle noise and the low signal-to-noise ratio. Methods: We propose a semi-automated segmentation algorithm for the delineation of the left ventricle in temporal 3D echocardiography sequences. The method requires minimal user interaction and relies on a diffeomorphic registration approach. Advantages of the method include no dependence on prior geometrical information, training data, or registration from an atlas. Results: The method was evaluated using three-dimensional ultrasound scan sequences from 18 patients from the Mazankowski Alberta Heart Institute, Edmonton, Canada, and compared to manual delineations provided by an expert cardiologist and four other registration algorithms. The segmentation approach yielded the following results over the cardiac cycle: a mean absolute difference of 1.01 (0.21) mm, a Hausdorff distance of 4.41 (1.43) mm, and a Dice overlap score of 0.93 (0.02). Conclusions: The method performed well compared to the four other registration algorithms.



### Learning to Prompt for Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2109.01134v6
- **DOI**: 10.1007/s11263-022-01653-1
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.01134v6)
- **Published**: 2021-09-02 17:57:31+00:00
- **Updated**: 2022-10-06 11:36:09+00:00
- **Authors**: Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu
- **Comment**: International Journal of Computer Vision (IJCV), 2022. Update: Adds
  results on the DOSCO (DOmain Shift in COntext) benchmark
- **Journal**: None
- **Summary**: Large pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks. Different from the traditional representation learning that is based mostly on discretized labels, vision-language pre-training aligns images and texts in a common feature space, which allows zero-shot transfer to a downstream task via prompting, i.e., classification weights are synthesized from natural language describing classes of interest. In this work, we show that a major challenge for deploying such models in practice is prompt engineering, which requires domain expertise and is extremely time-consuming -- one needs to spend a significant amount of time on words tuning since a slight change in wording could have a huge impact on performance. Inspired by recent advances in prompt learning research in natural language processing (NLP), we propose Context Optimization (CoOp), a simple approach specifically for adapting CLIP-like vision-language models for downstream image recognition. Concretely, CoOp models a prompt's context words with learnable vectors while the entire pre-trained parameters are kept fixed. To handle different image recognition tasks, we provide two implementations of CoOp: unified context and class-specific context. Through extensive experiments on 11 datasets, we demonstrate that CoOp requires as few as one or two shots to beat hand-crafted prompts with a decent margin and is able to gain significant improvements over prompt engineering with more shots, e.g., with 16 shots the average gain is around 15% (with the highest reaching over 45%). Despite being a learning-based approach, CoOp achieves superb domain generalization performance compared with the zero-shot model using hand-crafted prompts.



### The Power of Points for Modeling Humans in Clothing
- **Arxiv ID**: http://arxiv.org/abs/2109.01137v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2109.01137v2)
- **Published**: 2021-09-02 17:58:45+00:00
- **Updated**: 2021-10-19 08:49:26+00:00
- **Authors**: Qianli Ma, Jinlong Yang, Siyu Tang, Michael J. Black
- **Comment**: In ICCV 2021. Project page: https://qianlim.github.io/POP
- **Journal**: None
- **Summary**: Currently it requires an artist to create 3D human avatars with realistic clothing that can move naturally. Despite progress on 3D scanning and modeling of human bodies, there is still no technology that can easily turn a static scan into an animatable avatar. Automating the creation of such avatars would enable many applications in games, social networking, animation, and AR/VR to name a few. The key problem is one of representation. Standard 3D meshes are widely used in modeling the minimally-clothed body but do not readily capture the complex topology of clothing. Recent interest has shifted to implicit surface models for this task but they are computationally heavy and lack compatibility with existing 3D tools. What is needed is a 3D representation that can capture varied topology at high resolution and that can be learned from data. We argue that this representation has been with us all along -- the point cloud. Point clouds have properties of both implicit and explicit representations that we exploit to model 3D garment geometry on a human body. We train a neural network with a novel local clothing geometric feature to represent the shape of different outfits. The network is trained from 3D point clouds of many types of clothing, on many bodies, in many poses, and learns to model pose-dependent clothing deformations. The geometry feature can be optimized to fit a previously unseen scan of a person in clothing, enabling the scan to be reposed realistically. Our model demonstrates superior quantitative and qualitative results in both multi-outfit modeling and unseen outfit animation. The code is available for research purposes.



### Optimal Target Shape for LiDAR Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.01181v3
- **DOI**: 10.1109/LRA.2021.3138779
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.01181v3)
- **Published**: 2021-09-02 19:18:24+00:00
- **Updated**: 2021-12-21 13:36:38+00:00
- **Authors**: Jiunn-Kai Huang, William Clark, Jessy W. Grizzle
- **Comment**: None
- **Journal**: None
- **Summary**: Targets are essential in problems such as object tracking in cluttered or textureless environments, camera (and multi-sensor) calibration tasks, and simultaneous localization and mapping (SLAM). Target shapes for these tasks typically are symmetric (square, rectangular, or circular) and work well for structured, dense sensor data such as pixel arrays (i.e., image). However, symmetric shapes lead to pose ambiguity when using sparse sensor data such as LiDAR point clouds and suffer from the quantization uncertainty of the LiDAR. This paper introduces the concept of optimizing target shape to remove pose ambiguity for LiDAR point clouds. A target is designed to induce large gradients at edge points under rotation and translation relative to the LiDAR to ameliorate the quantization uncertainty associated with point cloud sparseness. Moreover, given a target shape, we present a means that leverages the target's geometry to estimate the target's vertices while globally estimating the pose. Both the simulation and the experimental results (verified by a motion capture system) confirm that by using the optimal shape and the global solver, we achieve centimeter error in translation and a few degrees in rotation even when a partially illuminated target is placed 30 meters away. All the implementations and datasets are available at https://github.com/UMich-BipedLab/optimal_shape_global_pose_estimation.



### roadscene2vec: A Tool for Extracting and Embedding Road Scene-Graphs
- **Arxiv ID**: http://arxiv.org/abs/2109.01183v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.01183v2)
- **Published**: 2021-09-02 19:21:18+00:00
- **Updated**: 2021-12-30 09:52:18+00:00
- **Authors**: Arnav Vaibhav Malawade, Shih-Yuan Yu, Brandon Hsu, Harsimrat Kaeley, Anurag Karra, Mohammad Abdullah Al Faruque
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, road scene-graph representations used in conjunction with graph learning techniques have been shown to outperform state-of-the-art deep learning techniques in tasks including action classification, risk assessment, and collision prediction. To enable the exploration of applications of road scene-graph representations, we introduce roadscene2vec: an open-source tool for extracting and embedding road scene-graphs. The goal of roadscene2vec is to enable research into the applications and capabilities of road scene-graphs by providing tools for generating scene-graphs, graph learning models to generate spatio-temporal scene-graph embeddings, and tools for visualizing and analyzing scene-graph-based methodologies. The capabilities of roadscene2vec include (i) customized scene-graph generation from either video clips or data from the CARLA simulator, (ii) multiple configurable spatio-temporal graph embedding models and baseline CNN-based models, (iii) built-in functionality for using graph and sequence embeddings for risk assessment and collision prediction applications, (iv) tools for evaluating transfer learning, and (v) utilities for visualizing scene-graphs and analyzing the explainability of graph learning models. We demonstrate the utility of roadscene2vec for these use cases with experimental results and qualitative evaluations for both graph learning models and CNN-based models. roadscene2vec is available at https://github.com/AICPS/roadscene2vec.



### Remote Multilinear Compressive Learning with Adaptive Compression
- **Arxiv ID**: http://arxiv.org/abs/2109.01184v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01184v1)
- **Published**: 2021-09-02 19:24:03+00:00
- **Updated**: 2021-09-02 19:24:03+00:00
- **Authors**: Dat Thanh Tran, Moncef Gabbouj, Alexandros Iosifidis
- **Comment**: 2 figures, 6 tables
- **Journal**: None
- **Summary**: Multilinear Compressive Learning (MCL) is an efficient signal acquisition and learning paradigm for multidimensional signals. The level of signal compression affects the detection or classification performance of a MCL model, with higher compression rates often associated with lower inference accuracy. However, higher compression rates are more amenable to a wider range of applications, especially those that require low operating bandwidth and minimal energy consumption such as Internet-of-Things (IoT) applications. Many communication protocols provide support for adaptive data transmission to maximize the throughput and minimize energy consumption. By developing compressive sensing and learning models that can operate with an adaptive compression rate, we can maximize the informational content throughput of the whole application. In this paper, we propose a novel optimization scheme that enables such a feature for MCL models. Our proposal enables practical implementation of adaptive compressive signal acquisition and inference systems. Experimental results demonstrated that the proposed approach can significantly reduce the amount of computations required during the training phase of remote learning systems but also improve the informational content throughput via adaptive-rate sensing.



### A Reliable, Self-Adaptive Face Identification Framework via Lyapunov Optimization
- **Arxiv ID**: http://arxiv.org/abs/2109.01212v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01212v1)
- **Published**: 2021-09-02 21:01:44+00:00
- **Updated**: 2021-09-02 21:01:44+00:00
- **Authors**: Dohyeon Kim, Joongheon Kim, Jae young Bang
- **Comment**: This paper was presented at ACM Symposium on Operating Systems
  Principles (SOSP) Workshop on AI Systems (AISys), Shanghai, China, October
  2017
- **Journal**: None
- **Summary**: Realtime face identification (FID) from a video feed is highly computation-intensive, and may exhaust computation resources if performed on a device with a limited amount of resources (e.g., a mobile device). In general, FID performs better when images are sampled at a higher rate, minimizing false negatives. However, performing it at an overwhelmingly high rate exposes the system to the risk of a queue overflow that hampers the system's reliability. This paper proposes a novel, queue-aware FID framework that adapts the sampling rate to maximize the FID performance while avoiding a queue overflow by implementing the Lyapunov optimization. A preliminary evaluation via a trace-based simulation confirms the effectiveness of the framework.



### DeepTracks: Geopositioning Maritime Vehicles in Video Acquired from a Moving Platform
- **Arxiv ID**: http://arxiv.org/abs/2109.01235v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.01235v1)
- **Published**: 2021-09-02 22:36:16+00:00
- **Updated**: 2021-09-02 22:36:16+00:00
- **Authors**: Jianli Wei, Guanyu Xu, Alper Yilmaz
- **Comment**: None
- **Journal**: None
- **Summary**: Geopositioning and tracking a moving boat at sea is a very challenging problem, requiring boat detection, matching and estimating its GPS location from imagery with no common features. The problem can be stated as follows: given imagery from a camera mounted on a moving platform with known GPS location as the only valid sensor, we predict the geoposition of a target boat visible in images. Our solution uses recent ML algorithms, the camera-scene geometry and Bayesian filtering. The proposed pipeline first detects and tracks the target boat's location in the image with the strategy of tracking by detection. This image location is then converted to geoposition to the local sea coordinates referenced to the camera GPS location using plane projective geometry. Finally, target boat local coordinates are transformed to global GPS coordinates to estimate the geoposition. To achieve a smooth geotrajectory, we apply unscented Kalman filter (UKF) which implicitly overcomes small detection errors in the early stages of the pipeline. We tested the performance of our approach using GPS ground truth and show the accuracy and speed of the estimated geopositions. Our code is publicly available at https://github.com/JianliWei1995/AI-Track-at-Sea.



### Two Shifts for Crop Mapping: Leveraging Aggregate Crop Statistics to Improve Satellite-based Maps in New Regions
- **Arxiv ID**: http://arxiv.org/abs/2109.01246v1
- **DOI**: 10.1016/j.rse.2021.112488
- **Categories**: **stat.AP**, cs.CV, cs.LG, stat.ML, 62P12 (primary) 62H30, I.4.m; I.m; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2109.01246v1)
- **Published**: 2021-09-02 23:33:03+00:00
- **Updated**: 2021-09-02 23:33:03+00:00
- **Authors**: Dan M. Kluger, Sherrie Wang, David B. Lobell
- **Comment**: This is the revised version of the paper which was submitted to
  Remote Sensing of Environment on May 3, 2021, immediately prior to acceptance
  for publication
- **Journal**: Remote Sensing of Environment (2021). Volume 262
- **Summary**: Crop type mapping at the field level is critical for a variety of applications in agricultural monitoring, and satellite imagery is becoming an increasingly abundant and useful raw input from which to create crop type maps. Still, in many regions crop type mapping with satellite data remains constrained by a scarcity of field-level crop labels for training supervised classification models. When training data is not available in one region, classifiers trained in similar regions can be transferred, but shifts in the distribution of crop types as well as transformations of the features between regions lead to reduced classification accuracy. We present a methodology that uses aggregate-level crop statistics to correct the classifier by accounting for these two types of shifts. To adjust for shifts in the crop type composition we present a scheme for properly reweighting the posterior probabilities of each class that are output by the classifier. To adjust for shifts in features we propose a method to estimate and remove linear shifts in the mean feature vector. We demonstrate that this methodology leads to substantial improvements in overall classification accuracy when using Linear Discriminant Analysis (LDA) to map crop types in Occitanie, France and in Western Province, Kenya. When using LDA as our base classifier, we found that in France our methodology led to percent reductions in misclassifications ranging from 2.8% to 42.2% (mean = 21.9%) over eleven different training departments, and in Kenya the percent reductions in misclassification were 6.6%, 28.4%, and 42.7% for three training regions. While our methodology was statistically motivated by the LDA classifier, it can be applied to any type of classifier. As an example, we demonstrate its successful application to improve a Random Forest classifier.



