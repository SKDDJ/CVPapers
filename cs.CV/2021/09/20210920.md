# Arxiv Papers in cs.CV on 2021-09-20
### DeepStationing: Thoracic Lymph Node Station Parsing in CT Scans using Anatomical Context Encoding and Key Organ Auto-Search
- **Arxiv ID**: http://arxiv.org/abs/2109.09271v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.09271v1)
- **Published**: 2021-09-20 02:32:50+00:00
- **Updated**: 2021-09-20 02:32:50+00:00
- **Authors**: Dazhou Guo, Xianghua Ye, Jia Ge, Xing Di, Le Lu, Lingyun Huang, Guotong Xie, Jing Xiao, Zhongjie Liu, Ling Peng, Senxiang Yan, Dakai Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Lymph node station (LNS) delineation from computed tomography (CT) scans is an indispensable step in radiation oncology workflow. High inter-user variabilities across oncologists and prohibitive laboring costs motivated the automated approach. Previous works exploit anatomical priors to infer LNS based on predefined ad-hoc margins. However, without voxel-level supervision, the performance is severely limited. LNS is highly context-dependent - LNS boundaries are constrained by anatomical organs - we formulate it as a deep spatial and contextual parsing problem via encoded anatomical organs. This permits the deep network to better learn from both CT appearance and organ context. We develop a stratified referencing organ segmentation protocol that divides the organs into anchor and non-anchor categories and uses the former's predictions to guide the later segmentation. We further develop an auto-search module to identify the key organs that opt for the optimal LNS parsing performance. Extensive four-fold cross-validation experiments on a dataset of 98 esophageal cancer patients (with the most comprehensive set of 12 LNSs + 22 organs in thoracic region to date) are conducted. Our LNS parsing model produces significant performance improvements, with an average Dice score of 81.1% +/- 6.1%, which is 5.0% and 19.2% higher over the pure CT-based deep model and the previous representative approach, respectively.



### Interpolation variable rate image compression
- **Arxiv ID**: http://arxiv.org/abs/2109.09280v1
- **DOI**: 10.1145/3474085.3475698
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.09280v1)
- **Published**: 2021-09-20 03:09:41+00:00
- **Updated**: 2021-09-20 03:09:41+00:00
- **Authors**: Zhenhong Sun, Zhiyu Tan, Xiuyu Sun, Fangyi Zhang, Yichen Qian, Dongyang Li, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: Compression standards have been used to reduce the cost of image storage and transmission for decades. In recent years, learned image compression methods have been proposed and achieved compelling performance to the traditional standards. However, in these methods, a set of different networks are used for various compression rates, resulting in a high cost in model storage and training. Although some variable-rate approaches have been proposed to reduce the cost by using a single network, most of them brought some performance degradation when applying fine rate control. To enable variable-rate control without sacrificing the performance, we propose an efficient Interpolation Variable-Rate (IVR) network, by introducing a handy Interpolation Channel Attention (InterpCA) module in the compression network. With the use of two hyperparameters for rate control and linear interpolation, the InterpCA achieves a fine PSNR interval of 0.001 dB and a fine rate interval of 0.0001 Bits-Per-Pixel (BPP) with 9000 rates in the IVR network. Experimental results demonstrate that the IVR network is the first variable-rate learned method that outperforms VTM 9.0 (intra) in PSNR and Multiscale Structural Similarity (MS-SSIM).



### Automatic 3D Ultrasound Segmentation of Uterus Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.09283v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09283v1)
- **Published**: 2021-09-20 03:13:57+00:00
- **Updated**: 2021-09-20 03:13:57+00:00
- **Authors**: Bahareh Behboodi, Hassan Rivaz, Susan Lalondrelle, Emma Harris
- **Comment**: None
- **Journal**: None
- **Summary**: On-line segmentation of the uterus can aid effective image-based guidance for precise delivery of dose to the target tissue (the uterocervix) during cervix cancer radiotherapy. 3D ultrasound (US) can be used to image the uterus, however, finding the position of uterine boundary in US images is a challenging task due to large daily positional and shape changes in the uterus, large variation in bladder filling, and the limitations of 3D US images such as low resolution in the elevational direction and imaging aberrations. Previous studies on uterus segmentation mainly focused on developing semi-automatic algorithms where require manual initialization to be done by an expert clinician. Due to limited studies on the automatic 3D uterus segmentation, the aim of the current study was to overcome the need for manual initialization in the semi-automatic algorithms using the recent deep learning-based algorithms. Therefore, we developed 2D UNet-based networks that are trained based on two scenarios. In the first scenario, we trained 3 different networks on each plane (i.e., sagittal, coronal, axial) individually. In the second scenario, our proposed network was trained using all the planes of each 3D volume. Our proposed schematic can overcome the initial manual selection of previous semi-automatic algorithm.



### TempNet -- Temporal Super Resolution of Radar Rainfall Products with Residual CNNs
- **Arxiv ID**: http://arxiv.org/abs/2109.09289v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09289v2)
- **Published**: 2021-09-20 03:58:52+00:00
- **Updated**: 2022-09-22 04:14:44+00:00
- **Authors**: Muhammed Sit, Bong-Chul Seo, Ibrahim Demir
- **Comment**: None
- **Journal**: None
- **Summary**: The temporal and spatial resolution of rainfall data is crucial for environmental modeling studies in which its variability in space and time is considered as a primary factor. Rainfall products from different remote sensing instruments (e.g., radar, satellite) have different space-time resolutions because of the differences in their sensing capabilities and post-processing methods. In this study, we developed a deep learning approach that augments rainfall data with increased time resolutions to complement relatively lower resolution products. We propose a neural network architecture based on Convolutional Neural Networks (CNNs) to improve the temporal resolution of radar-based rainfall products and compare the proposed model with an optical flow-based interpolation method and CNN-baseline model. The methodology presented in this study could be used for enhancing rainfall maps with better temporal resolution and imputation of missing frames in sequences of 2D rainfall maps to support hydrological and flood forecasting studies.



### Semi-supervised Dense Keypointsusing Unlabeled Multiview Images
- **Arxiv ID**: http://arxiv.org/abs/2109.09299v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09299v1)
- **Published**: 2021-09-20 04:57:57+00:00
- **Updated**: 2021-09-20 04:57:57+00:00
- **Authors**: Zhixuan Yu, Haozheng Yu, Long Sha, Sujoy Ganguly, Hyun Soo Park
- **Comment**: Under review
- **Journal**: None
- **Summary**: This paper presents a new end-to-end semi-supervised framework to learn a dense keypoint detector using unlabeled multiview images. A key challenge lies in finding the exact correspondences between the dense keypoints in multiple views since the inverse of keypoint mapping can be neither analytically derived nor differentiated. This limits applying existing multiview supervision approaches on sparse keypoint detection that rely on the exact correspondences. To address this challenge, we derive a new probabilistic epipolar constraint that encodes the two desired properties. (1) Soft correspondence: we define a matchability, which measures a likelihood of a point matching to the other image's corresponding point, thus relaxing the exact correspondences' requirement. (2) Geometric consistency: every point in the continuous correspondence fields must satisfy the multiview consistency collectively. We formulate a probabilistic epipolar constraint using a weighted average of epipolar errors through the matchability thereby generalizing the point-to-point geometric error to the field-to-field geometric error. This generalization facilitates learning a geometrically coherent dense keypoint detection model by utilizing a large number of unlabeled multiview images. Additionally, to prevent degenerative cases, we employ a distillation-based regularization by using a pretrained model. Finally, we design a new neural network architecture, made of twin networks, that effectively minimizes the probabilistic epipolar errors of all possible correspondences between two view images by building affinity matrices. Our method shows superior performance compared to existing methods, including non-differentiable bootstrapping in terms of keypoint accuracy, multiview consistency, and 3D reconstruction accuracy.



### Feature Correlation Aggregation: on the Path to Better Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.09300v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.09300v1)
- **Published**: 2021-09-20 05:04:26+00:00
- **Updated**: 2021-09-20 05:04:26+00:00
- **Authors**: Jieming Zhou, Tong Zhang, Pengfei Fang, Lars Petersson, Mehrtash Harandi
- **Comment**: None
- **Journal**: None
- **Summary**: Prior to the introduction of Graph Neural Networks (GNNs), modeling and analyzing irregular data, particularly graphs, was thought to be the Achilles' heel of deep learning. The core concept of GNNs is to find a representation by recursively aggregating the representations of a central node and those of its neighbors. The core concept of GNNs is to find a representation by recursively aggregating the representations of a central node and those of its neighbor, and its success has been demonstrated by many GNNs' designs. However, most of them only focus on using the first-order information between a node and its neighbors. In this paper, we introduce a central node permutation variant function through a frustratingly simple and innocent-looking modification to the core operation of a GNN, namely the Feature cOrrelation aGgregation (FOG) module which learns the second-order information from feature correlation between a node and its neighbors in the pipeline. By adding FOG into existing variants of GNNs, we empirically verify this second-order information complements the features generated by original GNNs across a broad set of benchmarks. A tangible boost in performance of the model is observed where the model surpasses previous state-of-the-art results by a significant margin while employing fewer parameters. (e.g., 33.116% improvement on a real-world molecular dataset using graph convolutional networks).



### Learning Versatile Convolution Filters for Efficient Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.09310v1
- **DOI**: 10.1109/TPAMI.2021.3114368
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09310v1)
- **Published**: 2021-09-20 06:07:14+00:00
- **Updated**: 2021-09-20 06:07:14+00:00
- **Authors**: Kai Han, Yunhe Wang, Chang Xu, Chunjing Xu, Enhua Wu, Dacheng Tao
- **Comment**: Accepted by TPAMI. Extended version of NeurIPS 2018 paper
- **Journal**: None
- **Summary**: This paper introduces versatile filters to construct efficient convolutional neural networks that are widely used in various visual recognition tasks. Considering the demands of efficient deep learning techniques running on cost-effective hardware, a number of methods have been developed to learn compact neural networks. Most of these works aim to slim down filters in different ways, \eg,~investigating small, sparse or quantized filters. In contrast, we treat filters from an additive perspective. A series of secondary filters can be derived from a primary filter with the help of binary masks. These secondary filters all inherit in the primary filter without occupying more storage, but once been unfolded in computation they could significantly enhance the capability of the filter by integrating information extracted from different receptive fields. Besides spatial versatile filters, we additionally investigate versatile filters from the channel perspective. Binary masks can be further customized for different primary filters under orthogonal constraints. We conduct theoretical analysis on network complexity and an efficient convolution scheme is introduced. Experimental results on benchmark datasets and neural networks demonstrate that our versatile filters are able to achieve comparable accuracy as that of original filters, but require less memory and computation cost.



### An Optimal Control Framework for Joint-channel Parallel MRI Reconstruction without Coil Sensitivities
- **Arxiv ID**: http://arxiv.org/abs/2109.09738v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2109.09738v2)
- **Published**: 2021-09-20 06:42:42+00:00
- **Updated**: 2022-01-23 23:58:48+00:00
- **Authors**: Wanyu Bian, Yunmei Chen, Xiaojing Ye
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Goal: This work aims at developing a novel calibration-free fast parallel MRI (pMRI) reconstruction method incorporate with discrete-time optimal control framework. The reconstruction model is designed to learn a regularization that combines channels and extracts features by leveraging the information sharing among channels of multi-coil images. We propose to recover both magnitude and phase information by taking advantage of structured convolutional networks in image and Fourier spaces. Methods: We develop a novel variational model with a learnable objective function that integrates an adaptive multi-coil image combination operator and effective image regularization in the image and Fourier spaces. We cast the reconstruction network as a structured discrete-time optimal control system, resulting in an optimal control formulation of parameter training where the parameters of the objective function play the role of control variables. We demonstrate that the Lagrangian method for solving the control problem is equivalent to back-propagation, ensuring the local convergence of the training algorithm. Results: We conduct a large number of numerical experiments of the proposed method with comparisons to several state-of-the-art pMRI reconstruction networks on real pMRI datasets. The numerical results demonstrate the promising performance of the proposed method evidently. Conclusion: We conduct a large number of numerical experiments of the proposed method with comparisons to several state-of-the-art pMRI reconstruction networks on real pMRI datasets. The numerical results demonstrate the promising performance of the proposed method evidently. Significance: By learning multi-coil image combination operator and performing regularizations in both image domain and k-space domain, the proposed method achieves a highly efficient image reconstruction network for pMRI.



### Robust Physical-World Attacks on Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.09320v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09320v1)
- **Published**: 2021-09-20 06:49:52+00:00
- **Updated**: 2021-09-20 06:49:52+00:00
- **Authors**: Xin Zheng, Yanbo Fan, Baoyuan Wu, Yong Zhang, Jue Wang, Shirui Pan
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Face recognition has been greatly facilitated by the development of deep neural networks (DNNs) and has been widely applied to many safety-critical applications. However, recent studies have shown that DNNs are very vulnerable to adversarial examples, raising serious concerns on the security of real-world face recognition. In this work, we study sticker-based physical attacks on face recognition for better understanding its adversarial robustness. To this end, we first analyze in-depth the complicated physical-world conditions confronted by attacking face recognition, including the different variations of stickers, faces, and environmental conditions. Then, we propose a novel robust physical attack framework, dubbed PadvFace, to model these challenging variations specifically. Furthermore, considering the difference in attack complexity, we propose an efficient Curriculum Adversarial Attack (CAA) algorithm that gradually adapts adversarial stickers to environmental variations from easy to complex. Finally, we construct a standardized testing protocol to facilitate the fair evaluation of physical attacks on face recognition, and extensive experiments on both dodging and impersonation attacks demonstrate the superior performance of the proposed method.



### PC$^2$-PU: Patch Correlation and Point Correlation for Effective Point Cloud Upsampling
- **Arxiv ID**: http://arxiv.org/abs/2109.09337v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09337v3)
- **Published**: 2021-09-20 07:40:20+00:00
- **Updated**: 2022-07-12 02:11:19+00:00
- **Authors**: Chen Long, Wenxiao Zhang, Ruihui Li, Hao Wang, Zhen Dong, Bisheng Yang
- **Comment**: Accepted to ACM MM 2022
- **Journal**: None
- **Summary**: Point cloud upsampling is to densify a sparse point set acquired from 3D sensors, providing a denser representation for the underlying surface. Existing methods divide the input points into small patches and upsample each patch separately, however, ignoring the global spatial consistency between patches. In this paper, we present a novel method PC$^2$-PU, which explores patch-to-patch and point-to-point correlations for more effective and robust point cloud upsampling. Specifically, our network has two appealing designs: (i) We take adjacent patches as supplementary inputs to compensate the loss structure information within a single patch and introduce a Patch Correlation Module to capture the difference and similarity between patches. (ii) After augmenting each patch's geometry, we further introduce a Point Correlation Module to reveal the relationship of points inside each patch to maintain the local spatial consistency. Extensive experiments on both synthetic and real scanned datasets demonstrate that our method surpasses previous upsampling methods, particularly with the noisy inputs. The code and data are at \url{https://github.com/chenlongwhu/PC2-PU.git}.



### A novel optical needle probe for deep learning-based tissue elasticity characterization
- **Arxiv ID**: http://arxiv.org/abs/2109.09362v1
- **DOI**: 10.1515/cdbme-2021-1005
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.09362v1)
- **Published**: 2021-09-20 08:29:29+00:00
- **Updated**: 2021-09-20 08:29:29+00:00
- **Authors**: Robin Mieling, Johanna Sprenger, Sarah Latus, Lennart Bargsten, Alexander Schlaefer
- **Comment**: Accepted at CURAC 2021, 2nd Place in the Best Paper Awards
- **Journal**: Current Directions in Biomedical Engineering 7 (2021) 21-25
- **Summary**: The distinction between malignant and benign tumors is essential to the treatment of cancer. The tissue's elasticity can be used as an indicator for the required tissue characterization. Optical coherence elastography (OCE) probes have been proposed for needle insertions but have so far lacked the necessary load sensing capabilities. We present a novel OCE needle probe that provides simultaneous optical coherence tomography (OCT) imaging and load sensing at the needle tip. We demonstrate the application of the needle probe in indentation experiments on gelatin phantoms with varying gelatin concentrations. We further implement two deep learning methods for the end-to-end sample characterization from the acquired OCT data. We report the estimation of gelatin sample concentrations in unseen samples with a mean error of $1.21 \pm 0.91$ wt\%. Both evaluated deep learning models successfully provide sample characterization with different advantages regarding the accuracy and inference time.



### FreeStyleGAN: Free-view Editable Portrait Rendering with the Camera Manifold
- **Arxiv ID**: http://arxiv.org/abs/2109.09378v1
- **DOI**: 10.1145/3478513.3480538
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.09378v1)
- **Published**: 2021-09-20 08:59:21+00:00
- **Updated**: 2021-09-20 08:59:21+00:00
- **Authors**: Thomas Leimkühler, George Drettakis
- **Comment**: Project webpage: https://repo-sam.inria.fr/fungraph/freestylegan/
- **Journal**: ACM Transactions on Graphics (SIGGRAPH Asia 2021)
- **Summary**: Current Generative Adversarial Networks (GANs) produce photorealistic renderings of portrait images. Embedding real images into the latent space of such models enables high-level image editing. While recent methods provide considerable semantic control over the (re-)generated images, they can only generate a limited set of viewpoints and cannot explicitly control the camera. Such 3D camera control is required for 3D virtual and mixed reality applications. In our solution, we use a few images of a face to perform 3D reconstruction, and we introduce the notion of the GAN camera manifold, the key element allowing us to precisely define the range of images that the GAN can reproduce in a stable manner. We train a small face-specific neural implicit representation network to map a captured face to this manifold and complement it with a warping scheme to obtain free-viewpoint novel-view synthesis. We show how our approach - due to its precise camera control - enables the integration of a pre-trained StyleGAN into standard 3D rendering pipelines, allowing e.g., stereo rendering or consistent insertion of faces in synthetic 3D environments. Our solution proposes the first truly free-viewpoint rendering of realistic faces at interactive rates, using only a small number of casual photos as input, while simultaneously allowing semantic editing capabilities, such as facial expression or lighting changes.



### Explaining Convolutional Neural Networks by Tagging Filters
- **Arxiv ID**: http://arxiv.org/abs/2109.09389v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09389v1)
- **Published**: 2021-09-20 09:27:27+00:00
- **Updated**: 2021-09-20 09:27:27+00:00
- **Authors**: Anna Nguyen, Daniel Hagenmayer, Tobias Weller, Michael Färber
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have achieved astonishing performance on various image classification tasks, but it is difficult for humans to understand how a classification comes about. Recent literature proposes methods to explain the classification process to humans. These focus mostly on visualizing feature maps and filter weights, which are not very intuitive for non-experts in analyzing a CNN classification. In this paper, we propose FilTag, an approach to effectively explain CNNs even to non-experts. The idea is that when images of a class frequently activate a convolutional filter, then that filter is tagged with that class. These tags provide an explanation to a reference of a class-specific feature detected by the filter. Based on the tagging, individual image classifications can then be intuitively explained in terms of the tags of the filters that the input image activates. Finally, we show that the tags are helpful in analyzing classification errors caused by noisy input images and that the tags can be further processed by machines.



### Unsupervised Cycle-consistent Generative Adversarial Networks for Pan-sharpening
- **Arxiv ID**: http://arxiv.org/abs/2109.09395v3
- **DOI**: 10.1109/TGRS.2022.3166528
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.09395v3)
- **Published**: 2021-09-20 09:43:24+00:00
- **Updated**: 2022-04-07 07:40:52+00:00
- **Authors**: Huanyu Zhou, Qingjie Liu, Dawei Weng, Yunhong Wang
- **Comment**: 14 pages, 8 figures, and 7 tables. Accepted by TGRS
- **Journal**: None
- **Summary**: Deep learning based pan-sharpening has received significant research interest in recent years. Most of existing methods fall into the supervised learning framework in which they down-sample the multi-spectral (MS) and panchromatic (PAN) images and regard the original MS images as ground truths to form training samples. Although impressive performance could be achieved, they have difficulties generalizing to the original full-scale images due to the scale gap, which makes them lack of practicability. In this paper, we propose an unsupervised generative adversarial framework that learns from the full-scale images without the ground truths to alleviate this problem. We extract the modality-specific features from the PAN and MS images with a two-stream generator, perform fusion in the feature domain, and then reconstruct the pan-sharpened images. Furthermore, we introduce a novel hybrid loss based on the cycle-consistency and adversarial scheme to improve the performance. Comparison experiments with the state-of-the-art methods are conducted on GaoFen-2 and WorldView-3 satellites. Results demonstrate that the proposed method can greatly improve the pan-sharpening performance on the full-scale images, which clearly show its practical value. Codes are available at https://github.com/zhysora/UCGAN.



### Dynamic Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.09396v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.09396v3)
- **Published**: 2021-09-20 09:45:29+00:00
- **Updated**: 2021-09-29 19:19:59+00:00
- **Authors**: Jonas Bokstaller, Costanza Maria Improta
- **Comment**: 3 pages, 5 figures
- **Journal**: None
- **Summary**: The Human-Machine Interaction (HMI) research field is an important topic in machine learning that has been deeply investigated thanks to the rise of computing power in the last years. The first time, it is possible to use machine learning to classify images and/or videos instead of the traditional computer vision algorithms. The aim of this paper is to build a symbiosis between a convolutional neural network (CNN) and a recurrent neural network (RNN) to recognize cultural/anthropological Italian sign language gestures from videos. The CNN extracts important features that later are used by the RNN. With RNNs we are able to store temporal information inside the model to provide contextual information from previous frames to enhance the prediction accuracy. Our novel approach uses different data augmentation techniques and regularization methods from only RGB frames to avoid overfitting and provide a small generalization error.



### Anomaly Detection in Radar Data Using PointNets
- **Arxiv ID**: http://arxiv.org/abs/2109.09401v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2109.09401v1)
- **Published**: 2021-09-20 10:02:24+00:00
- **Updated**: 2021-09-20 10:02:24+00:00
- **Authors**: Thomas Griebel, Dominik Authaler, Markus Horn, Matti Henning, Michael Buchholz, Klaus Dietmayer
- **Comment**: Accepted for presentation at the 2021 IEEE 24th International
  Conference on Intelligent Transportation Systems (ITSC), September 19-22,
  2021, Indianapolis, USA
- **Journal**: None
- **Summary**: For autonomous driving, radar is an important sensor type. On the one hand, radar offers a direct measurement of the radial velocity of targets in the environment. On the other hand, in literature, radar sensors are known for their robustness against several kinds of adverse weather conditions. However, on the downside, radar is susceptible to ghost targets or clutter which can be caused by several different causes, e.g., reflective surfaces in the environment. Ghost targets, for instance, can result in erroneous object detections. To this end, it is desirable to identify anomalous targets as early as possible in radar data. In this work, we present an approach based on PointNets to detect anomalous radar targets. Modifying the PointNet-architecture driven by our task, we developed a novel grouping variant which contributes to a multi-form grouping module. Our method is evaluated on a real-world dataset in urban scenarios and shows promising results for the detection of anomalous radar targets.



### EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow
- **Arxiv ID**: http://arxiv.org/abs/2109.09406v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2109.09406v2)
- **Published**: 2021-09-20 10:07:07+00:00
- **Updated**: 2021-10-26 07:00:59+00:00
- **Authors**: Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang, Zhiliang Yu, Zeyu Chen, Baohua Lai
- **Comment**: accepted by ICCV Workshop
- **Journal**: None
- **Summary**: High-quality training data play a key role in image segmentation tasks. Usually, pixel-level annotations are expensive, laborious and time-consuming for the large volume of training data. To reduce labelling cost and improve segmentation quality, interactive segmentation methods have been proposed, which provide the result with just a few clicks. However, their performance does not meet the requirements of practical segmentation tasks in terms of speed and accuracy. In this work, we propose EdgeFlow, a novel architecture that fully utilizes interactive information of user clicks with edge-guided flow. Our method achieves state-of-the-art performance without any post-processing or iterative optimization scheme. Comprehensive experiments on benchmarks also demonstrate the superiority of our method. In addition, with the proposed method, we develop an efficient interactive segmentation tool for practical data annotation tasks. The source code and tool is avaliable at https://github.com/PaddlePaddle/PaddleSeg.



### Background-Foreground Segmentation for Interior Sensing in Automotive Industry
- **Arxiv ID**: http://arxiv.org/abs/2109.09410v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09410v1)
- **Published**: 2021-09-20 10:12:40+00:00
- **Updated**: 2021-09-20 10:12:40+00:00
- **Authors**: Claudia Drygala, Matthias Rottmann, Hanno Gottschalk, Klaus Friedrichs, Thomas Kurbiel
- **Comment**: None
- **Journal**: None
- **Summary**: To ensure safety in automated driving, the correct perception of the situation inside the car is as important as its environment. Thus, seat occupancy detection and classification of detected instances play an important role in interior sensing. By the knowledge of the seat occupancy status, it is possible to, e.g., automate the airbag deployment control. Furthermore, the presence of a driver, which is necessary for partially automated driving cars at the automation levels two to four can be verified. In this work, we compare different statistical methods from the field of image segmentation to approach the problem of background-foreground segmentation in camera based interior sensing. In the recent years, several methods based on different techniques have been developed and applied to images or videos from different applications. The peculiarity of the given scenarios of interior sensing is, that the foreground instances and the background both contain static as well as dynamic elements. In data considered in this work, even the camera position is not completely fixed. We review and benchmark three different methods ranging, i.e., Gaussian Mixture Models (GMM), Morphological Snakes and a deep neural network, namely a Mask R-CNN. In particular, the limitations of the classical methods, GMM and Morphological Snakes, for interior sensing are shown. Furthermore, it turns, that it is possible to overcome these limitations by deep learning, e.g.\ using a Mask R-CNN. Although only a small amount of ground truth data was available for training, we enabled the Mask R-CNN to produce high quality background-foreground masks via transfer learning. Moreover, we demonstrate that certain augmentation as well as pre- and post-processing methods further enhance the performance of the investigated methods.



### ElasticFace: Elastic Margin Loss for Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.09416v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09416v4)
- **Published**: 2021-09-20 10:31:50+00:00
- **Updated**: 2022-03-21 14:42:26+00:00
- **Authors**: Fadi Boutros, Naser Damer, Florian Kirchbuchner, Arjan Kuijper
- **Comment**: None
- **Journal**: None
- **Summary**: Learning discriminative face features plays a major role in building high-performing face recognition models. The recent state-of-the-art face recognition solutions proposed to incorporate a fixed penalty margin on commonly used classification loss function, softmax loss, in the normalized hypersphere to increase the discriminative power of face recognition models, by minimizing the intra-class variation and maximizing the inter-class variation. Marginal penalty softmax losses, such as ArcFace and CosFace, assume that the geodesic distance between and within the different identities can be equally learned using a fixed penalty margin. However, such a learning objective is not realistic for real data with inconsistent inter-and intra-class variation, which might limit the discriminative and generalizability of the face recognition model. In this paper, we relax the fixed penalty margin constrain by proposing elastic penalty margin loss (ElasticFace) that allows flexibility in the push for class separability. The main idea is to utilize random margin values drawn from a normal distribution in each training iteration. This aims at giving the decision boundary chances to extract and retract to allow space for flexible class separability learning. We demonstrate the superiority of our ElasticFace loss over ArcFace and CosFace losses, using the same geometric transformation, on a large set of mainstream benchmarks. From a wider perspective, our ElasticFace has advanced the state-of-the-art face recognition performance on seven out of nine mainstream benchmarks.



### Improved AI-based segmentation of apical and basal slices from clinical cine CMR
- **Arxiv ID**: http://arxiv.org/abs/2109.09421v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.09421v1)
- **Published**: 2021-09-20 10:48:50+00:00
- **Updated**: 2021-09-20 10:48:50+00:00
- **Authors**: Jorge Mariscal-Harana, Naomi Kifle, Reza Razavi, Andrew P. King, Bram Ruijsink, Esther Puyol-Antón
- **Comment**: *Shared last authors
- **Journal**: None
- **Summary**: Current artificial intelligence (AI) algorithms for short-axis cardiac magnetic resonance (CMR) segmentation achieve human performance for slices situated in the middle of the heart. However, an often-overlooked fact is that segmentation of the basal and apical slices is more difficult. During manual analysis, differences in the basal segmentations have been reported as one of the major sources of disagreement in human interobserver variability. In this work, we aim to investigate the performance of AI algorithms in segmenting basal and apical slices and design strategies to improve their segmentation. We trained all our models on a large dataset of clinical CMR studies obtained from two NHS hospitals (n=4,228) and evaluated them against two external datasets: ACDC (n=100) and M&Ms (n=321). Using manual segmentations as a reference, CMR slices were assigned to one of four regions: non-cardiac, base, middle, and apex. Using the nnU-Net framework as a baseline, we investigated two different approaches to reduce the segmentation performance gap between cardiac regions: (1) non-uniform batch sampling, which allows us to choose how often images from different regions are seen during training; and (2) a cardiac-region classification model followed by three (i.e. base, middle, and apex) region-specific segmentation models. We show that the classification and segmentation approach was best at reducing the performance gap across all datasets. We also show that improvements in the classification performance can subsequently lead to a significantly better performance in the segmentation task.



### Predicting Visual Improvement after Macular Hole Surgery: a Cautionary Tale on Deep Learning with Very Limited Data
- **Arxiv ID**: http://arxiv.org/abs/2109.09463v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09463v2)
- **Published**: 2021-09-20 12:23:04+00:00
- **Updated**: 2021-11-15 03:11:38+00:00
- **Authors**: M. Godbout, A. Lachance, F. Antaki, A. Dirani, A. Durand
- **Comment**: Machine Learning for Health (ML4H) - Extended Abstract
- **Journal**: None
- **Summary**: We investigate the potential of machine learning models for the prediction of visual improvement after macular hole surgery from preoperative data (retinal images and clinical features). Collecting our own data for the task, we end up with only 121 total samples, putting our work in the very limited data regime. We explore a variety of deep learning methods for limited data to train deep computer vision models, finding that all tested deep vision models are outperformed by a simple regression model on the clinical features. We believe this is compelling evidence of the extreme difficulty of using deep learning on very limited data.



### Beyond Semantic to Instance Segmentation: Weakly-Supervised Instance Segmentation via Semantic Knowledge Transfer and Self-Refinement
- **Arxiv ID**: http://arxiv.org/abs/2109.09477v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09477v3)
- **Published**: 2021-09-20 12:31:44+00:00
- **Updated**: 2022-03-29 12:27:08+00:00
- **Authors**: Beomyoung Kim, Youngjoon Yoo, Chaeeun Rhee, Junmo Kim
- **Comment**: CVPR 2022, Accepted
- **Journal**: None
- **Summary**: Weakly-supervised instance segmentation (WSIS) has been considered as a more challenging task than weakly-supervised semantic segmentation (WSSS). Compared to WSSS, WSIS requires instance-wise localization, which is difficult to extract from image-level labels. To tackle the problem, most WSIS approaches use off-the-shelf proposal techniques that require pre-training with instance or object level labels, deviating the fundamental definition of the fully-image-level supervised setting. In this paper, we propose a novel approach including two innovative components. First, we propose a semantic knowledge transfer to obtain pseudo instance labels by transferring the knowledge of WSSS to WSIS while eliminating the need for the off-the-shelf proposals. Second, we propose a self-refinement method to refine the pseudo instance labels in a self-supervised scheme and to use the refined labels for training in an online manner. Here, we discover an erroneous phenomenon, semantic drift, that occurred by the missing instances in pseudo instance labels categorized as background class. This semantic drift occurs confusion between background and instance in training and consequently degrades the segmentation performance. We term this problem as semantic drift problem and show that our proposed self-refinement method eliminates the semantic drift problem. The extensive experiments on PASCAL VOC 2012 and MS COCO demonstrate the effectiveness of our approach, and we achieve a considerable performance without off-the-shelf proposal techniques. The code is available at https://github.com/clovaai/BESTIE.



### On Circuit-based Hybrid Quantum Neural Networks for Remote Sensing Imagery Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.09484v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.ET, quant-ph
- **Links**: [PDF](http://arxiv.org/pdf/2109.09484v2)
- **Published**: 2021-09-20 12:41:50+00:00
- **Updated**: 2021-12-01 10:51:37+00:00
- **Authors**: Alessandro Sebastianelli, Daniela A. Zaidenberg, Dario Spiller, Bertrand Le Saux, Silvia Liberata Ullo
- **Comment**: Submitted to the JSTARS special issue on "Quantum resources for Earth
  Observation" for possible publication. Copyright may be transferred without
  notice, after which this version may no longer be accessible
- **Journal**: None
- **Summary**: This article aims to investigate how circuit-based hybrid Quantum Convolutional Neural Networks (QCNNs) can be successfully employed as image classifiers in the context of remote sensing. The hybrid QCNNs enrich the classical architecture of CNNs by introducing a quantum layer within a standard neural network. The novel QCNN proposed in this work is applied to the Land Use and Land Cover (LULC) classification, chosen as an Earth Observation (EO) use case, and tested on the EuroSAT dataset used as reference benchmark. The results of the multiclass classification prove the effectiveness of the presented approach, by demonstrating that the QCNN performances are higher than the classical counterparts. Moreover, investigation of various quantum circuits shows that the ones exploiting quantum entanglement achieve the best classification scores. This study underlines the potentialities of applying quantum computing to an EO case study and provides the theoretical and experimental background for futures investigations.



### Dyadformer: A Multi-modal Transformer for Long-Range Modeling of Dyadic Interactions
- **Arxiv ID**: http://arxiv.org/abs/2109.09487v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09487v1)
- **Published**: 2021-09-20 12:45:04+00:00
- **Updated**: 2021-09-20 12:45:04+00:00
- **Authors**: David Curto, Albert Clapés, Javier Selva, Sorina Smeureanu, Julio C. S. Jacques Junior, David Gallardo-Pujol, Georgina Guilera, David Leiva, Thomas B. Moeslund, Sergio Escalera, Cristina Palmero
- **Comment**: Accepted to the 2021 ICCV Workshop on Understanding Social Behavior
  in Dyadic and Small Group Interactions
- **Journal**: None
- **Summary**: Personality computing has become an emerging topic in computer vision, due to the wide range of applications it can be used for. However, most works on the topic have focused on analyzing the individual, even when applied to interaction scenarios, and for short periods of time. To address these limitations, we present the Dyadformer, a novel multi-modal multi-subject Transformer architecture to model individual and interpersonal features in dyadic interactions using variable time windows, thus allowing the capture of long-term interdependencies. Our proposed cross-subject layer allows the network to explicitly model interactions among subjects through attentional operations. This proof-of-concept approach shows how multi-modality and joint modeling of both interactants for longer periods of time helps to predict individual attributes. With Dyadformer, we improve state-of-the-art self-reported personality inference results on individual subjects on the UDIVA v0.5 dataset.



### Modeling Annotation Uncertainty with Gaussian Heatmaps in Landmark Localization
- **Arxiv ID**: http://arxiv.org/abs/2109.09533v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09533v2)
- **Published**: 2021-09-20 13:31:58+00:00
- **Updated**: 2021-09-21 12:03:09+00:00
- **Authors**: Franz Thaler, Christian Payer, Martin Urschler, Darko Stern
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org
- **Journal**: None
- **Summary**: In landmark localization, due to ambiguities in defining their exact position, landmark annotations may suffer from large observer variabilities, which result in uncertain annotations. To model the annotation ambiguities of the training dataset, we propose to learn anisotropic Gaussian parameters modeling the shape of the target heatmap during optimization. Furthermore, our method models the prediction uncertainty of individual samples by fitting anisotropic Gaussian functions to the predicted heatmaps during inference. Besides state-of-the-art results, our experiments on datasets of hand radiographs and lateral cephalograms also show that Gaussian functions are correlated with both localization accuracy and observer variability. As a final experiment, we show the importance of integrating the uncertainty into decision making by measuring the influence of the predicted location uncertainty on the classification of anatomical abnormalities in lateral cephalograms.



### Audio-Visual Speech Recognition is Worth 32$\times$32$\times$8 Voxels
- **Arxiv ID**: http://arxiv.org/abs/2109.09536v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09536v1)
- **Published**: 2021-09-20 13:32:19+00:00
- **Updated**: 2021-09-20 13:32:19+00:00
- **Authors**: Dmitriy Serdyuk, Otavio Braga, Olivier Siohan
- **Comment**: 7 pages, 2 figures, 4 tables. A draft for a paper accepted to ASRU
  workshop
- **Journal**: None
- **Summary**: Audio-visual automatic speech recognition (AV-ASR) introduces the video modality into the speech recognition process, often by relying on information conveyed by the motion of the speaker's mouth. The use of the video signal requires extracting visual features, which are then combined with the acoustic features to build an AV-ASR system [1]. This is traditionally done with some form of 3D convolutional network (e.g. VGG) as widely used in the computer vision community. Recently, image transformers [2] have been introduced to extract visual features useful for image classification tasks. In this work, we propose to replace the 3D convolutional visual front-end with a video transformer front-end. We train our systems on a large-scale dataset composed of YouTube videos and evaluate performance on the publicly available LRS3-TED set, as well as on a large set of YouTube videos. On a lip-reading task, the transformer-based front-end shows superior performance compared to a strong convolutional baseline. On an AV-ASR task, the transformer front-end performs as well as (or better than) the convolutional baseline. Fine-tuning our model on the LRS3-TED training set matches previous state of the art. Thus, we experimentally show the viability of the convolution-free model for AV-ASR.



### Parameter Decoupling Strategy for Semi-supervised 3D Left Atrium Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.09596v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09596v2)
- **Published**: 2021-09-20 14:51:42+00:00
- **Updated**: 2021-11-09 04:59:18+00:00
- **Authors**: Xuanting Hao, Shengbo Gao, Lijie Sheng, Jicong Zhang
- **Comment**: ICMV2021 camera ready
- **Journal**: None
- **Summary**: Consistency training has proven to be an advanced semi-supervised framework and achieved promising results in medical image segmentation tasks through enforcing an invariance of the predictions over different views of the inputs. However, with the iterative updating of model parameters, the models would tend to reach a coupled state and eventually lose the ability to exploit unlabeled data. To address the issue, we present a novel semi-supervised segmentation model based on parameter decoupling strategy to encourage consistent predictions from diverse views. Specifically, we first adopt a two-branch network to simultaneously produce predictions for each image. During the training process, we decouple the two prediction branch parameters by quadratic cosine distance to construct different views in latent space. Based on this, the feature extractor is constrained to encourage the consistency of probability maps generated by classifiers under diversified features. In the overall training process, the parameters of feature extractor and classifiers are updated alternately by consistency regularization operation and decoupling operation to gradually improve the generalization performance of the model. Our method has achieved a competitive result over the state-of-the-art semi-supervised methods on the Atrial Segmentation Challenge dataset, demonstrating the effectiveness of our framework. Code is available at https://github.com/BX0903/PDC.



### Fine-Context Shadow Detection using Shadow Removal
- **Arxiv ID**: http://arxiv.org/abs/2109.09609v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09609v2)
- **Published**: 2021-09-20 15:09:22+00:00
- **Updated**: 2021-11-27 01:53:11+00:00
- **Authors**: Jeya Maria Jose Valanarasu, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Current shadow detection methods perform poorly when detecting shadow regions that are small, unclear or have blurry edges. In this work, we attempt to address this problem on two fronts. First, we propose a Fine Context-aware Shadow Detection Network (FCSD-Net), where we constraint the receptive field size and focus on low-level features to learn fine context features better. Second, we propose a new learning strategy, called Restore to Detect (R2D), where we show that when a deep neural network is trained for restoration (shadow removal), it learns meaningful features to delineate the shadow masks as well. To make use of this complementary nature of shadow detection and removal tasks, we train an auxiliary network for shadow removal and propose a complementary feature learning block (CFL) to learn and fuse meaningful features from shadow removal network to the shadow detection network. We train the proposed network, FCSD-Net, using the R2D learning strategy across multiple datasets. Experimental results on three public shadow detection datasets (ISTD, SBU and UCF) show that our method improves the shadow detection performance while being able to detect fine context better compared to the other recent methods.



### Real-Time Trash Detection for Modern Societies using CCTV to Identifying Trash by utilizing Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2109.09611v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09611v2)
- **Published**: 2021-09-20 15:10:26+00:00
- **Updated**: 2021-09-21 17:40:12+00:00
- **Authors**: Syed Muhammad Raza, Syed Muhammad Ghazi Hassan, Syed Ali Hassan, Soo Young Shin
- **Comment**: 15 pages, 6 Figures
- **Journal**: None
- **Summary**: To protect the environment from trash pollution, especially in societies, and to take strict action against the red-handed people who throws the trash. As modern societies are developing and these societies need a modern solution to make the environment clean. Artificial intelligence (AI) evolution, especially in Deep Learning, gives an excellent opportunity to develop real-time trash detection using CCTV cameras. The inclusion of this project is real-time trash detection using a deep model of Convolutional Neural Network (CNN). It is used to obtain eight classes mask, tissue papers, shoppers, boxes, automobile parts, pampers, bottles, and juices boxes. After detecting the trash, the camera records the video of that person for ten seconds who throw trash in society. The challenging part of this paper is preparing a complex custom dataset that took too much time. The dataset consists of more than 2100 images. The CNN model was created, labeled, and trained. The detection time accuracy and average mean precision (mAP) benchmark both models' performance. In experimental phase the mAP performance and accuracy of the improved CNN model was superior in all aspects. The model is used on a CCTV camera to detect trash in real-time.



### Superquadric Object Representation for Optimization-based Semantic SLAM
- **Arxiv ID**: http://arxiv.org/abs/2109.09627v1
- **DOI**: 10.3929/ethz-b-000487527
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09627v1)
- **Published**: 2021-09-20 15:27:56+00:00
- **Updated**: 2021-09-20 15:27:56+00:00
- **Authors**: Florian Tschopp, Juan Nieto, Roland Siegwart, Cesar Cadena
- **Comment**: None
- **Journal**: None
- **Summary**: Introducing semantically meaningful objects to visual Simultaneous Localization And Mapping (SLAM) has the potential to improve both the accuracy and reliability of pose estimates, especially in challenging scenarios with significant view-point and appearance changes. However, how semantic objects should be represented for an efficient inclusion in optimization-based SLAM frameworks is still an open question. Superquadrics(SQs) are an efficient and compact object representation, able to represent most common object types to a high degree, and typically retrieved from 3D point-cloud data. However, accurate 3D point-cloud data might not be available in all applications. Recent advancements in machine learning enabled robust object recognition and semantic mask measurements from camera images under many different appearance conditions. We propose a pipeline to leverage such semantic mask measurements to fit SQ parameters to multi-view camera observations using a multi-stage initialization and optimization procedure. We demonstrate the system's ability to retrieve randomly generated SQ parameters from multi-view mask observations in preliminary simulation experiments and evaluate different initialization stages and cost functions.



### Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2109.09628v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09628v4)
- **Published**: 2021-09-20 15:28:36+00:00
- **Updated**: 2021-11-29 16:15:57+00:00
- **Authors**: Ziyue Feng, Longlong Jing, Peng Yin, Yingli Tian, Bing Li
- **Comment**: Accepted by CoRL2021
- **Journal**: None
- **Summary**: Self-supervised monocular depth prediction provides a cost-effective solution to obtain the 3D location of each pixel. However, the existing approaches usually lead to unsatisfactory accuracy, which is critical for autonomous robots. In this paper, we propose FusionDepth, a novel two-stage network to advance the self-supervised monocular dense depth learning by leveraging low-cost sparse (e.g. 4-beam) LiDAR. Unlike the existing methods that use sparse LiDAR mainly in a manner of time-consuming iterative post-processing, our model fuses monocular image features and sparse LiDAR features to predict initial depth maps. Then, an efficient feed-forward refine network is further designed to correct the errors in these initial depth maps in pseudo-3D space with real-time performance. Extensive experiments show that our proposed model significantly outperforms all the state-of-the-art self-supervised methods, as well as the sparse-LiDAR-based methods on both self-supervised monocular depth prediction and completion tasks. With the accurate dense depth prediction, our model outperforms the state-of-the-art sparse-LiDAR-based method (Pseudo-LiDAR++) by more than 68% for the downstream task monocular 3D object detection on the KITTI Leaderboard. Code is available at https://github.com/AutoAILab/FusionDepth



### FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2109.09658v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09658v3)
- **Published**: 2021-09-20 16:22:49+00:00
- **Updated**: 2021-09-29 12:00:05+00:00
- **Authors**: Karim Lekadir, Richard Osuala, Catherine Gallin, Noussair Lazrak, Kaisar Kushibar, Gianna Tsakou, Susanna Aussó, Leonor Cerdá Alberich, Kostas Marias, Manolis Tsiknakis, Sara Colantonio, Nickolas Papanikolaou, Zohaib Salahuddin, Henry C Woodruff, Philippe Lambin, Luis Martí-Bonmatí
- **Comment**: 47 pages
- **Journal**: None
- **Summary**: The recent advancements in artificial intelligence (AI) combined with the extensive amount of data generated by today's clinical systems, has led to the development of imaging AI solutions across the whole value chain of medical imaging, including image reconstruction, medical image segmentation, image-based diagnosis and treatment planning. Notwithstanding the successes and future potential of AI in medical imaging, many stakeholders are concerned of the potential risks and ethical implications of imaging AI solutions, which are perceived as complex, opaque, and difficult to comprehend, utilise, and trust in critical clinical applications. Despite these concerns and risks, there are currently no concrete guidelines and best practices for guiding future AI developments in medical imaging towards increased trust, safety and adoption. To bridge this gap, this paper introduces a careful selection of guiding principles drawn from the accumulated experiences, consensus, and best practices from five large European projects on AI in Health Imaging. These guiding principles are named FUTURE-AI and its building blocks consist of (i) Fairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness and (vi) Explainability. In a step-by-step approach, these guidelines are further translated into a framework of concrete recommendations for specifying, developing, evaluating, and deploying technically, clinically and ethically trustworthy AI solutions into clinical practice.



### DEM Super-Resolution with EfficientNetV2
- **Arxiv ID**: http://arxiv.org/abs/2109.09661v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.09661v1)
- **Published**: 2021-09-20 16:26:58+00:00
- **Updated**: 2021-09-20 16:26:58+00:00
- **Authors**: Bekir Z Demiray, Muhammed Sit, Ibrahim Demir
- **Comment**: 6 pages, 2 figures, 3 tables
- **Journal**: None
- **Summary**: Efficient climate change monitoring and modeling rely on high-quality geospatial and environmental datasets. Due to limitations in technical capabilities or resources, the acquisition of high-quality data for many environmental disciplines is costly. Digital Elevation Model (DEM) datasets are such examples whereas their low-resolution versions are widely available, high-resolution ones are scarce. In an effort to rectify this problem, we propose and assess an EfficientNetV2 based model. The proposed model increases the spatial resolution of DEMs up to 16times without additional information.



### Trust Your Robots! Predictive Uncertainty Estimation of Neural Networks with Sparse Gaussian Processes
- **Arxiv ID**: http://arxiv.org/abs/2109.09690v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09690v2)
- **Published**: 2021-09-20 17:00:07+00:00
- **Updated**: 2021-09-21 15:27:37+00:00
- **Authors**: Jongseok Lee, Jianxiang Feng, Matthias Humt, Marcus G. Müller, Rudolph Triebel
- **Comment**: 12 pages, 6 figures and 1 table. Accepted at the 5th Conference on
  Robot Learning (CORL 2021), London, UK
- **Journal**: None
- **Summary**: This paper presents a probabilistic framework to obtain both reliable and fast uncertainty estimates for predictions with Deep Neural Networks (DNNs). Our main contribution is a practical and principled combination of DNNs with sparse Gaussian Processes (GPs). We prove theoretically that DNNs can be seen as a special case of sparse GPs, namely mixtures of GP experts (MoE-GP), and we devise a learning algorithm that brings the derived theory into practice. In experiments from two different robotic tasks -- inverse dynamics of a manipulator and object detection on a micro-aerial vehicle (MAV) -- we show the effectiveness of our approach in terms of predictive uncertainty, improved scalability, and run-time efficiency on a Jetson TX2. We thus argue that our approach can pave the way towards reliable and fast robot learning systems with uncertainty awareness.



### Deep Anomaly Generation: An Image Translation Approach of Synthesizing Abnormal Banded Chromosome Images
- **Arxiv ID**: http://arxiv.org/abs/2109.09702v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.2.1 Artificial Intelligence, Applications and Expert Systems,
  Medicine and Science
- **Links**: [PDF](http://arxiv.org/pdf/2109.09702v1)
- **Published**: 2021-09-20 17:16:23+00:00
- **Updated**: 2021-09-20 17:16:23+00:00
- **Authors**: Lukas Uzolas, Javier Rico, Pierrick Coupé, Juan C. SanMiguel, György Cserey
- **Comment**: 8 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Advances in deep-learning-based pipelines have led to breakthroughs in a variety of microscopy image diagnostics. However, a sufficiently big training data set is usually difficult to obtain due to high annotation costs. In the case of banded chromosome images, the creation of big enough libraries is difficult for multiple pathologies due to the rarity of certain genetic disorders. Generative Adversarial Networks (GANs) have proven to be effective in generating synthetic images and extending training data sets. In our work, we implement a conditional adversarial network that allows generation of realistic single chromosome images following user-defined banding patterns. To this end, an image-to-image translation approach based on self-generated 2D chromosome segmentation label maps is used. Our validation shows promising results when synthesizing chromosomes with seen as well as unseen banding patterns. We believe that this approach can be exploited for data augmentation of chromosome data sets with structural abnormalities. Therefore, the proposed method could help to tackle medical image analysis problems such as data simulation, segmentation, detection, or classification in the field of cytogenetics.



### MFEViT: A Robust Lightweight Transformer-based Network for Multimodal 2D+3D Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.13086v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.13086v1)
- **Published**: 2021-09-20 17:19:39+00:00
- **Updated**: 2021-09-20 17:19:39+00:00
- **Authors**: Hanting Li, Mingzhe Sui, Zhaoqing Zhu, Feng Zhao
- **Comment**: 9pages,6 figures,5 tables
- **Journal**: None
- **Summary**: Vision transformer (ViT) has been widely applied in many areas due to its self-attention mechanism that help obtain the global receptive field since the first layer. It even achieves surprising performance exceeding CNN in some vision tasks. However, there exists an issue when leveraging vision transformer into 2D+3D facial expression recognition (FER), i.e., ViT training needs mass data. Nonetheless, the number of samples in public 2D+3D FER datasets is far from sufficient for evaluation. How to utilize the ViT pre-trained on RGB images to handle 2D+3D data becomes a challenge. To solve this problem, we propose a robust lightweight pure transformer-based network for multimodal 2D+3D FER, namely MFEViT. For narrowing the gap between RGB and multimodal data, we devise an alternative fusion strategy, which replaces each of the three channels of an RGB image with the depth-map channel and fuses them before feeding them into the transformer encoder. Moreover, the designed sample filtering module adds several subclasses for each expression and move the noisy samples to their corresponding subclasses, thus eliminating their disturbance on the network during the training stage. Extensive experiments demonstrate that our MFEViT outperforms state-of-the-art approaches with an accuracy of 90.83% on BU-3DFE and 90.28% on Bosphorus. Specifically, the proposed MFEViT is a lightweight model, requiring much fewer parameters than multi-branch CNNs. To the best of our knowledge, this is the first work to introduce vision transformer into multimodal 2D+3D FER. The source code of our MFEViT will be publicly available online.



### BabelCalib: A Universal Approach to Calibrating Central Cameras
- **Arxiv ID**: http://arxiv.org/abs/2109.09704v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09704v3)
- **Published**: 2021-09-20 17:21:57+00:00
- **Updated**: 2021-10-28 14:13:15+00:00
- **Authors**: Yaroslava Lochman, Kostiantyn Liepieshov, Jianhui Chen, Michal Perdoch, Christopher Zach, James Pritts
- **Comment**: None
- **Journal**: None
- **Summary**: Existing calibration methods occasionally fail for large field-of-view cameras due to the non-linearity of the underlying problem and the lack of good initial values for all parameters of the used camera model. This might occur because a simpler projection model is assumed in an initial step, or a poor initial guess for the internal parameters is pre-defined. A lot of the difficulties of general camera calibration lie in the use of a forward projection model. We side-step these challenges by first proposing a solver to calibrate the parameters in terms of a back-projection model and then regress the parameters for a target forward model. These steps are incorporated in a robust estimation framework to cope with outlying detections. Extensive experiments demonstrate that our approach is very reliable and returns the most accurate calibration parameters as measured on the downstream task of absolute pose estimation on test sets. The code is released at https://github.com/ylochman/babelcalib.



### Reconstructing Cosmic Polarization Rotation with ResUNet-CMB
- **Arxiv ID**: http://arxiv.org/abs/2109.09715v2
- **DOI**: 10.1088/1475-7516/2022/01/030
- **Categories**: **astro-ph.CO**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2109.09715v2)
- **Published**: 2021-09-20 17:39:09+00:00
- **Updated**: 2022-01-19 09:19:42+00:00
- **Authors**: Eric Guzman, Joel Meyers
- **Comment**: 11 pages, 7 figures. Updated to match published content. Code
  available from https://github.com/EEmGuzman/resunet-cmb
- **Journal**: JCAP 01 (2022) 030
- **Summary**: Cosmic polarization rotation, which may result from parity-violating new physics or the presence of primordial magnetic fields, converts $E$-mode polarization of the cosmic microwave background (CMB) into $B$-mode polarization. Anisotropic cosmic polarization rotation leads to statistical anisotropy in CMB polarization and can be reconstructed with quadratic estimator techniques similar to those designed for gravitational lensing of the CMB. At the sensitivity of upcoming CMB surveys, lensing-induced $B$-mode polarization will act as a limiting factor in the search for anisotropic cosmic polarization rotation, meaning that an analysis which incorporates some form of delensing will be required to improve constraints on the effect with future surveys. In this paper we extend the ResUNet-CMB convolutional neural network to reconstruct anisotropic cosmic polarization rotation in the presence of gravitational lensing and patchy reionization, and we show that the network simultaneously reconstructs all three effects with variance that is lower than that from the standard quadratic estimator nearly matching the performance of an iterative reconstruction method.



### Multifield Cosmology with Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2109.09747v1
- **DOI**: None
- **Categories**: **astro-ph.CO**, astro-ph.GA, astro-ph.IM, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09747v1)
- **Published**: 2021-09-20 18:00:01+00:00
- **Updated**: 2021-09-20 18:00:01+00:00
- **Authors**: Francisco Villaescusa-Navarro, Daniel Anglés-Alcázar, Shy Genel, David N. Spergel, Yin Li, Benjamin Wandelt, Andrina Nicola, Leander Thiele, Sultan Hassan, Jose Manuel Zorrilla Matilla, Desika Narayanan, Romeel Dave, Mark Vogelsberger
- **Comment**: 11 pages, 7 figures. First paper of a series of four. All 2D maps,
  codes, and networks weights publicly available at
  https://camels-multifield-dataset.readthedocs.io
- **Journal**: None
- **Summary**: Astrophysical processes such as feedback from supernovae and active galactic nuclei modify the properties and spatial distribution of dark matter, gas, and galaxies in a poorly understood way. This uncertainty is one of the main theoretical obstacles to extract information from cosmological surveys. We use 2,000 state-of-the-art hydrodynamic simulations from the CAMELS project spanning a wide variety of cosmological and astrophysical models and generate hundreds of thousands of 2-dimensional maps for 13 different fields: from dark matter to gas and stellar properties. We use these maps to train convolutional neural networks to extract the maximum amount of cosmological information while marginalizing over astrophysical effects at the field level. Although our maps only cover a small area of $(25~h^{-1}{\rm Mpc})^2$, and the different fields are contaminated by astrophysical effects in very different ways, our networks can infer the values of $\Omega_{\rm m}$ and $\sigma_8$ with a few percent level precision for most of the fields. We find that the marginalization performed by the network retains a wealth of cosmological information compared to a model trained on maps from gravity-only N-body simulations that are not contaminated by astrophysical effects. Finally, we train our networks on multifields -- 2D maps that contain several fields as different colors or channels -- and find that not only they can infer the value of all parameters with higher accuracy than networks trained on individual fields, but they can constrain the value of $\Omega_{\rm m}$ with higher accuracy than the maps from the N-body simulations.



### Integrated Construction of Multimodal Atlases with Structural Connectomes in the Space of Riemannian Metrics
- **Arxiv ID**: http://arxiv.org/abs/2109.09808v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09808v3)
- **Published**: 2021-09-20 19:39:10+00:00
- **Updated**: 2022-06-13 18:38:15+00:00
- **Authors**: Kristen M. Campbell, Haocheng Dai, Zhe Su, Martin Bauer, P. Thomas Fletcher, Sarang C. Joshi
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA)
  https://www.melba-journal.org/papers/2022:016.html. arXiv admin note:
  substantial text overlap with arXiv:2103.05730
- **Journal**: None
- **Summary**: The structural network of the brain, or structural connectome, can be represented by fiber bundles generated by a variety of tractography methods. While such methods give qualitative insights into brain structure, there is controversy over whether they can provide quantitative information, especially at the population level. In order to enable population-level statistical analysis of the structural connectome, we propose representing a connectome as a Riemannian metric, which is a point on an infinite-dimensional manifold. We equip this manifold with the Ebin metric, a natural metric structure for this space, to get a Riemannian manifold along with its associated geometric properties. We then use this Riemannian framework to apply object-oriented statistical analysis to define an atlas as the Fr\'echet mean of a population of Riemannian metrics. This formulation ties into the existing framework for diffeomorphic construction of image atlases, allowing us to construct a multimodal atlas by simultaneously integrating complementary white matter structure details from DWMRI and cortical details from T1-weighted MRI. We illustrate our framework with 2D data examples of connectome registration and atlas formation. Finally, we build an example 3D multimodal atlas using T1 images and connectomes derived from diffusion tensors estimated from a subset of subjects from the Human Connectome Project.



### Skin Deep Unlearning: Artefact and Instrument Debiasing in the Context of Melanoma Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.09818v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09818v7)
- **Published**: 2021-09-20 20:00:22+00:00
- **Updated**: 2023-04-27 08:38:48+00:00
- **Authors**: Peter J. Bevan, Amir Atapour-Abarghouei
- **Comment**: 9 pages, ICML 2022
- **Journal**: Proceedings of the 39th International Conference on Machine
  Learning, PMLR 162:1874-1892, 2022
- **Summary**: Convolutional Neural Networks have demonstrated dermatologist-level performance in the classification of melanoma from skin lesion images, but prediction irregularities due to biases seen within the training data are an issue that should be addressed before widespread deployment is possible. In this work, we robustly remove bias and spurious variation from an automated melanoma classification pipeline using two leading bias unlearning techniques. We show that the biases introduced by surgical markings and rulers presented in previous studies can be reasonably mitigated using these bias removal methods. We also demonstrate the generalisation benefits of unlearning spurious variation relating to the imaging instrument used to capture lesion images. Our experimental results provide evidence that the effects of each of the aforementioned biases are notably reduced, with different debiasing techniques excelling at different tasks.



### Well Googled is Half Done: Multimodal Forecasting of New Fashion Product Sales with Image-based Google Trends
- **Arxiv ID**: http://arxiv.org/abs/2109.09824v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09824v5)
- **Published**: 2021-09-20 20:15:08+00:00
- **Updated**: 2022-09-15 12:06:59+00:00
- **Authors**: Geri Skenderi, Christian Joppi, Matteo Denitto, Marco Cristani
- **Comment**: Paper submitted at Wiley Journal of Forecasting
- **Journal**: None
- **Summary**: New fashion product sales forecasting is a challenging problem that involves many business dynamics and cannot be solved by classical forecasting approaches. In this paper, we investigate the effectiveness of systematically probing exogenous knowledge in the form of Google Trends time series and combining it with multi-modal information related to a brand-new fashion item, in order to effectively forecast its sales despite the lack of past data. In particular, we propose a neural network-based approach, where an encoder learns a representation of the exogenous time series, while the decoder forecasts the sales based on the Google Trends encoding and the available visual and metadata information. Our model works in a non-autoregressive manner, avoiding the compounding effect of large first-step errors. As a second contribution, we present VISUELLE, a publicly available dataset for the task of new fashion product sales forecasting, containing multimodal information for 5577 real, new products sold between 2016-2019 from Nunalie, an Italian fast-fashion company. The dataset is equipped with images of products, metadata, related sales, and associated Google Trends. We use VISUELLE to compare our approach against state-of-the-art alternatives and several baselines, showing that our neural network-based approach is the most accurate in terms of both percentage and absolute error. It is worth noting that the addition of exogenous knowledge boosts the forecasting accuracy by 1.5% WAPE wise, revealing the importance of exploiting informative external information. The code and dataset are both available at https://github.com/HumaticsLAB/GTM-Transformer.



### Viewpoint Invariant Dense Matching for Visual Geolocalization
- **Arxiv ID**: http://arxiv.org/abs/2109.09827v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09827v1)
- **Published**: 2021-09-20 20:17:38+00:00
- **Updated**: 2021-09-20 20:17:38+00:00
- **Authors**: Gabriele Berton, Carlo Masone, Valerio Paolicelli, Barbara Caputo
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: In this paper we propose a novel method for image matching based on dense local features and tailored for visual geolocalization. Dense local features matching is robust against changes in illumination and occlusions, but not against viewpoint shifts which are a fundamental aspect of geolocalization. Our method, called GeoWarp, directly embeds invariance to viewpoint shifts in the process of extracting dense features. This is achieved via a trainable module which learns from the data an invariance that is meaningful for the task of recognizing places. We also devise a new self-supervised loss and two new weakly supervised losses to train this module using only unlabeled data and weak labels. GeoWarp is implemented efficiently as a re-ranking method that can be easily embedded into pre-existing visual geolocalization pipelines. Experimental validation on standard geolocalization benchmarks demonstrates that GeoWarp boosts the accuracy of state-of-the-art retrieval architectures. The code and trained models are available at https://github.com/gmberton/geo_warp



### Balanced-MixUp for Highly Imbalanced Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.09850v1
- **DOI**: 10.1007/978-3-030-87240-3_31
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09850v1)
- **Published**: 2021-09-20 21:31:31+00:00
- **Updated**: 2021-09-20 21:31:31+00:00
- **Authors**: Adrian Galdran, Gustavo Carneiro, Miguel A. González Ballester
- **Comment**: Accepted at MICCAI 2021, International Conference on Medical Image
  Computing and Computer Assisted Intervention
- **Journal**: MICCAI 2021
- **Summary**: Highly imbalanced datasets are ubiquitous in medical image classification problems. In such problems, it is often the case that rare classes associated to less prevalent diseases are severely under-represented in labeled databases, typically resulting in poor performance of machine learning algorithms due to overfitting in the learning process. In this paper, we propose a novel mechanism for sampling training data based on the popular MixUp regularization technique, which we refer to as Balanced-MixUp. In short, Balanced-MixUp simultaneously performs regular (i.e., instance-based) and balanced (i.e., class-based) sampling of the training data. The resulting two sets of samples are then mixed-up to create a more balanced training distribution from which a neural network can effectively learn without incurring in heavily under-fitting the minority classes. We experiment with a highly imbalanced dataset of retinal images (55K samples, 5 classes) and a long-tail dataset of gastro-intestinal video frames (10K images, 23 classes), using two CNNs of varying representation capabilities. Experimental results demonstrate that applying Balanced-MixUp outperforms other conventional sampling schemes and loss functions specifically designed to deal with imbalanced data. Code is released at https://github.com/agaldran/balanced_mixup .



### Object Detection in Thermal Spectrum for Advanced Driver-Assistance Systems (ADAS)
- **Arxiv ID**: http://arxiv.org/abs/2109.09854v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09854v2)
- **Published**: 2021-09-20 21:38:55+00:00
- **Updated**: 2021-10-27 22:13:42+00:00
- **Authors**: Muhammad Ali Farooq, Peter Corcoran, Cosmin Rotariu, Waseem Shariff
- **Comment**: This work is carried under EU funded project
  (https://www.heliaus.eu/)
- **Journal**: None
- **Summary**: Object detection in thermal infrared spectrum provides more reliable data source in low-lighting conditions and different weather conditions, as it is useful both in-cabin and outside for pedestrian, animal, and vehicular detection as well as for detecting street-signs & lighting poles. This paper is about exploring and adapting state-of-the-art object detection and classifier framework on thermal vision with seven distinct classes for advanced driver-assistance systems (ADAS). The trained network variants on public datasets are validated on test data with three different test approaches which include test-time with no augmentation, test-time augmentation, and test-time with model ensembling. Additionally, the efficacy of trained networks is tested on locally gathered novel test-data captured with an uncooled LWIR prototype thermal camera in challenging weather and environmental scenarios. The performance analysis of trained models is investigated by computing precision, recall, and mean average precision scores (mAP). Furthermore, the trained model architecture is optimized using TensorRT inference accelerator and deployed on resource-constrained edge hardware Nvidia Jetson Nano to explicitly reduce the inference time on GPU as well as edge devices for further real-time onboard installations.



### Augmenting Depth Estimation with Geospatial Context
- **Arxiv ID**: http://arxiv.org/abs/2109.09879v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09879v1)
- **Published**: 2021-09-20 23:24:17+00:00
- **Updated**: 2021-09-20 23:24:17+00:00
- **Authors**: Scott Workman, Hunter Blanton
- **Comment**: IEEE/CVF International Conference on Computer Vision (ICCV) 2021
- **Journal**: None
- **Summary**: Modern cameras are equipped with a wide array of sensors that enable recording the geospatial context of an image. Taking advantage of this, we explore depth estimation under the assumption that the camera is geocalibrated, a problem we refer to as geo-enabled depth estimation. Our key insight is that if capture location is known, the corresponding overhead viewpoint offers a valuable resource for understanding the scale of the scene. We propose an end-to-end architecture for depth estimation that uses geospatial context to infer a synthetic ground-level depth map from a co-located overhead image, then fuses it inside of an encoder/decoder style segmentation network. To support evaluation of our methods, we extend a recently released dataset with overhead imagery and corresponding height maps. Results demonstrate that integrating geospatial context significantly reduces error compared to baselines, both at close ranges and when evaluating at much larger distances than existing benchmarks consider.



### Estimating and Exploiting the Aleatoric Uncertainty in Surface Normal Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.09881v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09881v1)
- **Published**: 2021-09-20 23:30:04+00:00
- **Updated**: 2021-09-20 23:30:04+00:00
- **Authors**: Gwangbin Bae, Ignas Budvytis, Roberto Cipolla
- **Comment**: ICCV 2021 (oral)
- **Journal**: None
- **Summary**: Surface normal estimation from a single image is an important task in 3D scene understanding. In this paper, we address two limitations shared by the existing methods: the inability to estimate the aleatoric uncertainty and lack of detail in the prediction. The proposed network estimates the per-pixel surface normal probability distribution. We introduce a new parameterization for the distribution, such that its negative log-likelihood is the angular loss with learned attenuation. The expected value of the angular error is then used as a measure of the aleatoric uncertainty. We also present a novel decoder framework where pixel-wise multi-layer perceptrons are trained on a subset of pixels sampled based on the estimated uncertainty. The proposed uncertainty-guided sampling prevents the bias in training towards large planar surfaces and improves the quality of prediction, especially near object boundaries and on small structures. Experimental results show that the proposed method outperforms the state-of-the-art in ScanNet and NYUv2, and that the estimated uncertainty correlates well with the prediction error. Code is available at https://github.com/baegwangbin/surface_normal_uncertainty.



### On the Importance of Distractors for Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.09883v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09883v1)
- **Published**: 2021-09-20 23:35:56+00:00
- **Updated**: 2021-09-20 23:35:56+00:00
- **Authors**: Rajshekhar Das, Yu-Xiong Wang, JoséM. F. Moura
- **Comment**: 17 pages, 5 figures, ICCV 2021
- **Journal**: None
- **Summary**: Few-shot classification aims at classifying categories of a novel task by learning from just a few (typically, 1 to 5) labelled examples. An effective approach to few-shot classification involves a prior model trained on a large-sample base domain, which is then finetuned over the novel few-shot task to yield generalizable representations. However, task-specific finetuning is prone to overfitting due to the lack of enough training examples. To alleviate this issue, we propose a new finetuning approach based on contrastive learning that reuses unlabelled examples from the base domain in the form of distractors. Unlike the nature of unlabelled data used in prior works, distractors belong to classes that do not overlap with the novel categories. We demonstrate for the first time that inclusion of such distractors can significantly boost few-shot generalization. Our technical novelty includes a stochastic pairing of examples sharing the same category in the few-shot task and a weighting term that controls the relative influence of task-specific negatives and distractors. An important aspect of our finetuning objective is that it is agnostic to distractor labels and hence applicable to various base domain settings. Compared to state-of-the-art approaches, our method shows accuracy gains of up to $12\%$ in cross-domain and up to $5\%$ in unsupervised prior-learning settings.



