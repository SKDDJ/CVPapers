# Arxiv Papers in cs.CV on 2021-09-27
### A novel network training approach for open set image recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.12756v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.12756v1)
- **Published**: 2021-09-27 01:47:21+00:00
- **Updated**: 2021-09-27 01:47:21+00:00
- **Authors**: Md Tahmid Hossain, Shyh Wei Teng, Guojun Lu, Ferdous Sohel
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are commonly designed for closed set arrangements, where test instances only belong to some "Known Known" (KK) classes used in training. As such, they predict a class label for a test sample based on the distribution of the KK classes. However, when used under the Open Set Recognition (OSR) setup (where an input may belong to an "Unknown Unknown" or UU class), such a network will always classify a test instance as one of the KK classes even if it is from a UU class. As a solution, recently, data augmentation based on Generative Adversarial Networks(GAN) has been used. In this work, we propose a novel approach for mining a "Known UnknownTrainer" or KUT set and design a deep OSR Network (OSRNet) to harness this dataset. The goal isto teach OSRNet the essence of the UUs through KUT set, which is effectively a collection of mined "hard Known Unknown negatives". Once trained, OSRNet can detect the UUs while maintaining high classification accuracy on KKs. We evaluate OSRNet on six benchmark datasets and demonstrate it outperforms contemporary OSR methods.



### Improving the Thermal Infrared Monitoring of Volcanoes: A Deep Learning Approach for Intermittent Image Series
- **Arxiv ID**: http://arxiv.org/abs/2109.12767v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2109.12767v1)
- **Published**: 2021-09-27 02:31:34+00:00
- **Updated**: 2021-09-27 02:31:34+00:00
- **Authors**: Jeremy Diaz, Guido Cervone, Christelle Wauthier
- **Comment**: 18 pages, 14 figures, submitted to IEEE Transactions on Geoscience
  and Remote Sensing
- **Journal**: None
- **Summary**: Active volcanoes are globally distributed and pose societal risks at multiple geographic scales, ranging from local hazards to regional/international disruptions. Many volcanoes do not have continuous ground monitoring networks; meaning that satellite observations provide the only record of volcanic behavior and unrest. Among these remote sensing observations, thermal imagery is inspected daily by volcanic observatories for examining the early signs, onset, and evolution of eruptive activity. However, thermal scenes are often obstructed by clouds, meaning that forecasts must be made off image sequences whose scenes are only usable intermittently through time. Here, we explore forecasting this thermal data stream from a deep learning perspective using existing architectures that model sequences with varying spatiotemporal considerations. Additionally, we propose and evaluate new architectures that explicitly model intermittent image sequences. Using ASTER Kinetic Surface Temperature data for $9$ volcanoes between $1999$ and $2020$, we found that a proposed architecture (ConvLSTM + Time-LSTM + U-Net) forecasts volcanic temperature imagery with the lowest RMSE ($4.164^{\circ}$C, other methods: $4.217-5.291^{\circ}$C). Additionally, we examined performance on multiple time series derived from the thermal imagery and the effect of training with data from singular volcanoes. Ultimately, we found that models with the lowest RMSE on forecasting imagery did not possess the lowest RMSE on recreating time series derived from that imagery and that training with individual volcanoes generally worsened performance relative to a multi-volcano data set. This work highlights the potential of data-driven deep learning models for volcanic unrest forecasting while revealing the need for carefully constructed optimization targets.



### Autonomy and Perception for Space Mining
- **Arxiv ID**: http://arxiv.org/abs/2109.12109v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.12109v3)
- **Published**: 2021-09-27 02:52:46+00:00
- **Updated**: 2022-04-13 04:09:03+00:00
- **Authors**: Ragav Sachdeva, Ravi Hammond, James Bockman, Alec Arthur, Brandon Smart, Dustin Craggs, Anh-Dzung Doan, Thomas Rowntree, Elijah Schutz, Adrian Orenstein, Andy Yu, Tat-Jun Chin, Ian Reid
- **Comment**: This paper describes our 3rd place and innovation award winning
  solution to the NASA Space Robotics Challenge Phase 2
- **Journal**: None
- **Summary**: Future Moon bases will likely be constructed using resources mined from the surface of the Moon. The difficulty of maintaining a human workforce on the Moon and communications lag with Earth means that mining will need to be conducted using collaborative robots with a high degree of autonomy. In this paper, we describe our solution for Phase 2 of the NASA Space Robotics Challenge, which provided a simulated lunar environment in which teams were tasked to develop software systems to achieve autonomous collaborative robots for mining on the Moon. Our 3rd place and innovation award winning solution shows how machine learning-enabled vision could alleviate major challenges posed by the lunar environment towards autonomous space mining, chiefly the lack of satellite positioning systems, hazardous terrain, and delicate robot interactions. A robust multi-robot coordinator was also developed to achieve long-term operation and effective collaboration between robots.



### Joint Multimedia Event Extraction from Video and Article
- **Arxiv ID**: http://arxiv.org/abs/2109.12776v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2109.12776v1)
- **Published**: 2021-09-27 03:22:12+00:00
- **Updated**: 2021-09-27 03:22:12+00:00
- **Authors**: Brian Chen, Xudong Lin, Christopher Thomas, Manling Li, Shoya Yoshida, Lovish Chum, Heng Ji, Shih-Fu Chang
- **Comment**: To be presented at EMNLP 2021 findings
- **Journal**: None
- **Summary**: Visual and textual modalities contribute complementary information about events described in multimedia documents. Videos contain rich dynamics and detailed unfoldings of events, while text describes more high-level and abstract concepts. However, existing event extraction methods either do not handle video or solely target video while ignoring other modalities. In contrast, we propose the first approach to jointly extract events from video and text articles. We introduce the new task of Video MultiMedia Event Extraction (Video M2E2) and propose two novel components to build the first system towards this task. First, we propose the first self-supervised multimodal event coreference model that can determine coreference between video events and text events without any manually annotated pairs. Second, we introduce the first multimodal transformer which extracts structured event information jointly from both videos and text documents. We also construct and will publicly release a new benchmark of video-article pairs, consisting of 860 video-article pairs with extensive annotations for evaluating methods on this task. Our experimental results demonstrate the effectiveness of our proposed method on our new benchmark dataset. We achieve 6.0% and 5.8% absolute F-score gain on multimodal event coreference resolution and multimedia event extraction.



### Leveraging Multiple CNNs for Triaging Medical Workflow
- **Arxiv ID**: http://arxiv.org/abs/2109.12783v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.12783v1)
- **Published**: 2021-09-27 03:59:23+00:00
- **Updated**: 2021-09-27 03:59:23+00:00
- **Authors**: Lakshmi A. Ghantasala
- **Comment**: 8 pages, 4 figures. Original manuscript and work done completed in
  2019
- **Journal**: None
- **Summary**: High hospitalization rates due to the global spread of Covid-19 bring about a need for improvements to classical triaging workflows. To this end, convolutional neural networks (CNNs) can effectively differentiate critical from non-critical images so that critical cases may be addressed quickly, so long as there exists some representative image for the illness. Presented is a conglomerate neural network system consisting of multiple VGG16 CNNs; the system trains on weighted skin disease images re-labelled as critical or non-critical, to then attach to input images a critical index between 0 and 10. A critical index offers a more comprehensive rating system compared to binary critical/non-critical labels. Results for batches of input images run through the trained network are promising. A batch is shown being re-ordered by the proposed architecture from most critical to least critical roughly accurately.



### High Frame Rate Video Quality Assessment using VMAF and Entropic Differences
- **Arxiv ID**: http://arxiv.org/abs/2109.12785v1
- **DOI**: 10.1109/PCS50896.2021.9477462
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.12785v1)
- **Published**: 2021-09-27 04:08:12+00:00
- **Updated**: 2021-09-27 04:08:12+00:00
- **Authors**: Pavan C Madhusudana, Neil Birkbeck, Yilin Wang, Balu Adsumilli, Alan C. Bovik
- **Comment**: None
- **Journal**: 2021 Picture Coding Symposium (PCS)
- **Summary**: The popularity of streaming videos with live, high-action content has led to an increased interest in High Frame Rate (HFR) videos. In this work we address the problem of frame rate dependent Video Quality Assessment (VQA) when the videos to be compared have different frame rate and compression factor. The current VQA models such as VMAF have superior correlation with perceptual judgments when videos to be compared have same frame rates and contain conventional distortions such as compression, scaling etc. However this framework requires additional pre-processing step when videos with different frame rates need to be compared, which can potentially limit its overall performance. Recently, Generalized Entropic Difference (GREED) VQA model was proposed to account for artifacts that arise due to changes in frame rate, and showed superior performance on the LIVE-YT-HFR database which contains frame rate dependent artifacts such as judder, strobing etc. In this paper we propose a simple extension, where the features from VMAF and GREED are fused in order to exploit the advantages of both models. We show through various experiments that the proposed fusion framework results in more efficient features for predicting frame rate dependent video quality. We also evaluate the fused feature set on standard non-HFR VQA databases and obtain superior performance than both GREED and VMAF, indicating the combined feature set captures complimentary perceptual quality information.



### Machine Learning based Medical Image Deepfake Detection: A Comparative Study
- **Arxiv ID**: http://arxiv.org/abs/2109.12800v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.12800v2)
- **Published**: 2021-09-27 05:10:55+00:00
- **Updated**: 2022-04-06 22:29:25+00:00
- **Authors**: Siddharth Solaiyappan, Yuxin Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep generative networks in recent years have reinforced the need for caution while consuming various modalities of digital information. One avenue of deepfake creation is aligned with injection and removal of tumors from medical scans. Failure to detect medical deepfakes can lead to large setbacks on hospital resources or even loss of life. This paper attempts to address the detection of such attacks with a structured case study. Specifically, we evaluate eight different machine learning algorithms, which including three conventional machine learning methods, support vector machine, random forest, decision tree, and five deep learning models, DenseNet121, DenseNet201, ResNet50, ResNet101, VGG19, on distinguishing between tampered and untampered images.For deep learning models, the five models are used for feature extraction, then fine-tune for each pre-trained model is performed. The findings of this work show near perfect accuracy in detecting instances of tumor injections and removals.



### Effect Of Personalized Calibration On Gaze Estimation Using Deep-Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.12801v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.12801v1)
- **Published**: 2021-09-27 05:14:12+00:00
- **Updated**: 2021-09-27 05:14:12+00:00
- **Authors**: Nairit Bandyopadhyay, Sébastien Riou, Didier Schwab
- **Comment**: None
- **Journal**: None
- **Summary**: With the increase in computation power and the development of new state-of-the-art deep learning algorithms, appearance-based gaze estimation is becoming more and more popular. It is believed to work well with curated laboratory data sets, however it faces several challenges when deployed in real world scenario. One such challenge is to estimate the gaze of a person about which the Deep Learning model trained for gaze estimation has no knowledge about. To analyse the performance in such scenarios we have tried to simulate a calibration mechanism. In this work we use the MPIIGaze data set. We trained a multi modal convolutional neural network and analysed its performance with and without calibration and this evaluation provides clear insights on how calibration improved the performance of the Deep Learning model in estimating gaze in the wild.



### N-shot Palm Vein Verification Using Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.12808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.12808v1)
- **Published**: 2021-09-27 05:47:54+00:00
- **Updated**: 2021-09-27 05:47:54+00:00
- **Authors**: Felix Marattukalam, Waleed H. Abdulla, Akshya Swain
- **Comment**: 5 pages IEEE
- **Journal**: None
- **Summary**: The use of deep learning methods to extract vascular biometric patterns from the palm surface has been of interest among researchers in recent years. In many biometric recognition tasks, there is a limit in the number of training samples. This is because of limited vein biometric databases being available for research. This restricts the application of deep learning methods to design algorithms that can effectively identify or authenticate people for vein recognition. This paper proposes an architecture using Siamese neural network structure for few shot palm vein identification. The proposed network uses images from both the palms and consists of two sub-nets that share weights to identify a person. The architecture performance was tested on the HK PolyU multi spectral palm vein database with limited samples. The results suggest that the method is effective since it has 91.9% precision, 91.1% recall, 92.2% specificity, 91.5%, F1-Score, and 90.5% accuracy values.



### An optimised deep spiking neural network architecture without gradients
- **Arxiv ID**: http://arxiv.org/abs/2109.12813v3
- **DOI**: 10.1109/ACCESS.2022.3200699
- **Categories**: **cs.NE**, cs.CV, I.2.6; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2109.12813v3)
- **Published**: 2021-09-27 05:59:12+00:00
- **Updated**: 2022-05-02 20:45:29+00:00
- **Authors**: Yeshwanth Bethi, Ying Xu, Gregory Cohen, Andre van Schaik, Saeed Afshar
- **Comment**: 18 pages, 6 figures
- **Journal**: IEEE Access, 2022
- **Summary**: We present an end-to-end trainable modular event-driven neural architecture that uses local synaptic and threshold adaptation rules to perform transformations between arbitrary spatio-temporal spike patterns. The architecture represents a highly abstracted model of existing Spiking Neural Network (SNN) architectures. The proposed Optimized Deep Event-driven Spiking neural network Architecture (ODESA) can simultaneously learn hierarchical spatio-temporal features at multiple arbitrary time scales. ODESA performs online learning without the use of error back-propagation or the calculation of gradients. Through the use of simple local adaptive selection thresholds at each node, the network rapidly learns to appropriately allocate its neuronal resources at each layer for any given problem without using a real-valued error measure. These adaptive selection thresholds are the central feature of ODESA, ensuring network stability and remarkable robustness to noise as well as to the selection of initial system parameters. Network activations are inherently sparse due to a hard Winner-Take-All (WTA) constraint at each layer. We evaluate the architecture on existing spatio-temporal datasets, including the spike-encoded IRIS and TIDIGITS datasets, as well as a novel set of tasks based on International Morse Code that we created. These tests demonstrate the hierarchical spatio-temporal learning capabilities of ODESA. Through these tests, we demonstrate ODESA can optimally solve practical and highly challenging hierarchical spatio-temporal learning tasks with the minimum possible number of computing nodes.



### MUTEN: Boosting Gradient-Based Adversarial Attacks via Mutant-Based Ensembles
- **Arxiv ID**: http://arxiv.org/abs/2109.12838v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.12838v1)
- **Published**: 2021-09-27 07:15:01+00:00
- **Updated**: 2021-09-27 07:15:01+00:00
- **Authors**: Yuejun Guo, Qiang Hu, Maxime Cordy, Michail Papadakis, Yves Le Traon
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are vulnerable to adversarial examples, which causes serious threats to security-critical applications. This motivated much research on providing mechanisms to make models more robust against adversarial attacks. Unfortunately, most of these defenses, such as gradient masking, are easily overcome through different attack means. In this paper, we propose MUTEN, a low-cost method to improve the success rate of well-known attacks against gradient-masking models. Our idea is to apply the attacks on an ensemble model which is built by mutating the original model elements after training. As we found out that mutant diversity is a key factor in improving success rate, we design a greedy algorithm for generating diverse mutants efficiently. Experimental results on MNIST, SVHN, and CIFAR10 show that MUTEN can increase the success rate of four attacks by up to 0.45.



### Bayesian deep learning of affordances from RGB images
- **Arxiv ID**: http://arxiv.org/abs/2109.12845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.12845v1)
- **Published**: 2021-09-27 07:39:47+00:00
- **Updated**: 2021-09-27 07:39:47+00:00
- **Authors**: Lorenzo Mur-Labadia, Ruben Martinez-Cantin
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Autonomous agents, such as robots or intelligent devices, need to understand how to interact with objects and its environment. Affordances are defined as the relationships between an agent, the objects, and the possible future actions in the environment. In this paper, we present a Bayesian deep learning method to predict the affordances available in the environment directly from RGB images. Based on previous work on socially accepted affordances, our model is based on a multiscale CNN that combines local and global information from the object and the full image. However, previous works assume a deterministic model, but uncertainty quantification is fundamental for robust detection, affordance-based reason, continual learning, etc. Our Bayesian model is able to capture both the aleatoric uncertainty from the scene and the epistemic uncertainty associated with the model and previous learning process. For comparison, we estimate the uncertainty using two state-of-the-art techniques: Monte Carlo dropout and deep ensembles. We also compare different types of CNN encoders for feature extraction. We have performed several experiments on an affordance database on socially acceptable behaviours and we have shown improved performance compared with previous works. Furthermore, the uncertainty estimation is consistent with the the type of objects and scenarios. Our results show a marginal better performance of deep ensembles, compared to MC-dropout on the Brier score and the Expected Calibration Error.



### A General Gaussian Heatmap Label Assignment for Arbitrary-Oriented Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.12848v4
- **DOI**: 10.1109/TIP.2022.3148874
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.12848v4)
- **Published**: 2021-09-27 07:46:09+00:00
- **Updated**: 2022-01-03 05:41:53+00:00
- **Authors**: Zhanchao Huang, Wei Li, Xiang-Gen Xia, Ran Tao
- **Comment**: 16 pages, 13 figures
- **Journal**: IEEE Transactions on Image Processing 2022
- **Summary**: Recently, many arbitrary-oriented object detection (AOOD) methods have been proposed and attracted widespread attention in many fields. However, most of them are based on anchor-boxes or standard Gaussian heatmaps. Such label assignment strategy may not only fail to reflect the shape and direction characteristics of arbitrary-oriented objects, but also have high parameter-tuning efforts. In this paper, a novel AOOD method called General Gaussian Heatmap Label Assignment (GGHL) is proposed. Specifically, an anchor-free object-adaptation label assignment (OLA) strategy is presented to define the positive candidates based on two-dimensional (2-D) oriented Gaussian heatmaps, which reflect the shape and direction features of arbitrary-oriented objects. Based on OLA, an oriented-bounding-box (OBB) representation component (ORC) is developed to indicate OBBs and adjust the Gaussian center prior weights to fit the characteristics of different objects adaptively through neural network learning. Moreover, a joint-optimization loss (JOL) with area normalization and dynamic confidence weighting is designed to refine the misalign optimal results of different subtasks. Extensive experiments on public datasets demonstrate that the proposed GGHL improves the AOOD performance with low parameter-tuning and time costs. Furthermore, it is generally applicable to most AOOD methods to improve their performance including lightweight models on embedded platforms.



### Deep Structured Instance Graph for Distilling Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2109.12862v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.12862v1)
- **Published**: 2021-09-27 08:26:00+00:00
- **Updated**: 2021-09-27 08:26:00+00:00
- **Authors**: Yixin Chen, Pengguang Chen, Shu Liu, Liwei Wang, Jiaya Jia
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Effectively structuring deep knowledge plays a pivotal role in transfer from teacher to student, especially in semantic vision tasks. In this paper, we present a simple knowledge structure to exploit and encode information inside the detection system to facilitate detector knowledge distillation. Specifically, aiming at solving the feature imbalance problem while further excavating the missing relation inside semantic instances, we design a graph whose nodes correspond to instance proposal-level features and edges represent the relation between nodes. To further refine this graph, we design an adaptive background loss weight to reduce node noise and background samples mining to prune trivial edges. We transfer the entire graph as encoded knowledge representation from teacher to student, capturing local and global information simultaneously. We achieve new state-of-the-art results on the challenging COCO object detection task with diverse student-teacher pairs on both one- and two-stage detectors. We also experiment with instance segmentation to demonstrate robustness of our method. It is notable that distilled Faster R-CNN with ResNet18-FPN and ResNet50-FPN yields 38.68 and 41.82 Box AP respectively on the COCO benchmark, Faster R-CNN with ResNet101-FPN significantly achieves 43.38 AP, which outperforms ResNet152-FPN teacher about 0.7 AP. Code: https://github.com/dvlab-research/Dsig.



### Meta-Aggregator: Learning to Aggregate for 1-bit Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.12872v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.12872v1)
- **Published**: 2021-09-27 08:50:37+00:00
- **Updated**: 2021-09-27 08:50:37+00:00
- **Authors**: Yongcheng Jing, Yiding Yang, Xinchao Wang, Mingli Song, Dacheng Tao
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: In this paper, we study a novel meta aggregation scheme towards binarizing graph neural networks (GNNs). We begin by developing a vanilla 1-bit GNN framework that binarizes both the GNN parameters and the graph features. Despite the lightweight architecture, we observed that this vanilla framework suffered from insufficient discriminative power in distinguishing graph topologies, leading to a dramatic drop in performance. This discovery motivates us to devise meta aggregators to improve the expressive power of vanilla binarized GNNs, of which the aggregation schemes can be adaptively changed in a learnable manner based on the binarized features. Towards this end, we propose two dedicated forms of meta neighborhood aggregators, an exclusive meta aggregator termed as Greedy Gumbel Neighborhood Aggregator (GNA), and a diffused meta aggregator termed as Adaptable Hybrid Neighborhood Aggregator (ANA). GNA learns to exclusively pick one single optimal aggregator from a pool of candidates, while ANA learns a hybrid aggregation behavior to simultaneously retain the benefits of several individual aggregators. Furthermore, the proposed meta aggregators may readily serve as a generic plugin module into existing full-precision GNNs. Experiments across various domains demonstrate that the proposed method yields results superior to the state of the art.



### Wasserstein Patch Prior for Image Superresolution
- **Arxiv ID**: http://arxiv.org/abs/2109.12880v2
- **DOI**: 10.1109/TCI.2022.3199600
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2109.12880v2)
- **Published**: 2021-09-27 09:04:07+00:00
- **Updated**: 2021-12-17 08:56:16+00:00
- **Authors**: Johannes Hertrich, Antoine Houdard, Claudia Redenbach
- **Comment**: None
- **Journal**: IEEE Transactions on Computational Imaging, vol. 8, pp. 693-704,
  2022
- **Summary**: In this paper, we introduce a Wasserstein patch prior for superresolution of two- and three-dimensional images. Here, we assume that we have given (additionally to the low resolution observation) a reference image which has a similar patch distribution as the ground truth of the reconstruction. This assumption is e.g. fulfilled when working with texture images or material data. Then, the proposed regularizer penalizes the $W_2$-distance of the patch distribution of the reconstruction to the patch distribution of some reference image at different scales. We demonstrate the performance of the proposed regularizer by two- and three-dimensional numerical examples.



### Compressive Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2109.12909v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2109.12909v3)
- **Published**: 2021-09-27 09:53:43+00:00
- **Updated**: 2021-12-04 12:22:08+00:00
- **Authors**: Kuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John Canny, Ian Fischer
- **Comment**: NeurIPS 2021. 27 pages, 4 figures. Code and pretrained models at
  https://github.com/google-research/compressive-visual-representations
- **Journal**: None
- **Summary**: Learning effective visual representations that generalize well without human supervision is a fundamental problem in order to apply Machine Learning to a wide variety of tasks. Recently, two families of self-supervised methods, contrastive learning and latent bootstrapping, exemplified by SimCLR and BYOL respectively, have made significant progress. In this work, we hypothesize that adding explicit information compression to these algorithms yields better and more robust representations. We verify this by developing SimCLR and BYOL formulations compatible with the Conditional Entropy Bottleneck (CEB) objective, allowing us to both measure and control the amount of compression in the learned representation, and observe their impact on downstream tasks. Furthermore, we explore the relationship between Lipschitz continuity and compression, showing a tractable lower bound on the Lipschitz constant of the encoders we learn. As Lipschitz continuity is closely related to robustness, this provides a new explanation for why compressed models are more robust. Our experiments confirm that adding compression to SimCLR and BYOL significantly improves linear evaluation accuracies and model robustness across a wide range of domain shifts. In particular, the compressed version of BYOL achieves 76.0% Top-1 linear evaluation accuracy on ImageNet with ResNet-50, and 78.8% with ResNet-50 2x.



### ClipMatrix: Text-controlled Creation of 3D Textured Meshes
- **Arxiv ID**: http://arxiv.org/abs/2109.12922v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2109.12922v1)
- **Published**: 2021-09-27 10:11:35+00:00
- **Updated**: 2021-09-27 10:11:35+00:00
- **Authors**: Nikolay Jetchev
- **Comment**: None
- **Journal**: None
- **Summary**: If a picture is worth thousand words, a moving 3d shape must be worth a million. We build upon the success of recent generative methods that create images fitting the semantics of a text prompt, and extend it to the controlled generation of 3d objects. We present a novel algorithm for the creation of textured 3d meshes, controlled by text prompts. Our method creates aesthetically pleasing high resolution articulated 3d meshes, and opens new possibilities for automation and AI control of 3d assets. We call it "ClipMatrix" because it leverages CLIP text embeddings to breed new digital 3d creatures, a nod to the Latin meaning of the word "matrix" - "mother". See the online gallery for a full impression of our method's capability.



### HarrisZ$^+$: Harris Corner Selection for Next-Gen Image Matching Pipelines
- **Arxiv ID**: http://arxiv.org/abs/2109.12925v6
- **DOI**: 10.1016/j.patrec.2022.04.022
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.12925v6)
- **Published**: 2021-09-27 10:21:03+00:00
- **Updated**: 2022-05-01 09:41:11+00:00
- **Authors**: Fabio Bellavia, Dmytro Mishkin
- **Comment**: None
- **Journal**: Pattern Recognition Letters, 158 (2022) 141-147
- **Summary**: Due to its role in many computer vision tasks, image matching has been subjected to an active investigation by researchers, which has lead to better and more discriminant feature descriptors and to more robust matching strategies, also thanks to the advent of the deep learning and the increased computational power of the modern hardware. Despite of these achievements, the keypoint extraction process at the base of the image matching pipeline has not seen equivalent progresses. This paper presents HarrisZ$^+$, an upgrade to the HarrisZ corner detector, optimized to synergically take advance of the recent improvements of the other steps of the image matching pipeline. HarrisZ$^+$ does not only consists of a tuning of the setup parameters, but introduces further refinements to the selection criteria delineated by HarrisZ, so providing more, yet discriminative, keypoints, which are better distributed on the image and with higher localization accuracy. The image matching pipeline including HarrisZ$^+$, together with the other modern components, obtained in different recent matching benchmarks state-of-the-art results among the classic image matching pipelines. These results are quite close to those obtained by the more recent fully deep end-to-end trainable approaches and show that there is still a proper margin of improvement that can be granted by the research in classic image matching methods.



### Sparse Spatial Transformers for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.12932v3
- **DOI**: 10.1007/s11432-022-3700-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.12932v3)
- **Published**: 2021-09-27 10:36:32+00:00
- **Updated**: 2023-05-10 01:53:11+00:00
- **Authors**: Haoxing Chen, Huaxiong Li, Yaohui Li, Chunlin Chen
- **Comment**: Accepted by SCIENCE CHINA Information Sciences
- **Journal**: None
- **Summary**: Learning from limited data is challenging because data scarcity leads to a poor generalization of the trained model. A classical global pooled representation will probably lose useful local information. Many few-shot learning methods have recently addressed this challenge using deep descriptors and learning a pixel-level metric. However, using deep descriptors as feature representations may lose image contextual information. Moreover, most of these methods independently address each class in the support set, which cannot sufficiently use discriminative information and task-specific embeddings. In this paper, we propose a novel transformer-based neural network architecture called sparse spatial transformers (SSFormers), which finds task-relevant features and suppresses task-irrelevant features. Particularly, we first divide each input image into several image patches of different sizes to obtain dense local features. These features retain contextual information while expressing local information. Then, a sparse spatial transformer layer is proposed to find spatial correspondence between the query image and the full support set to select task-relevant image patches and suppress task-irrelevant image patches. Finally, we propose using an image patch-matching module to calculate the distance between dense local representations, thus determining which category the query image belongs to in the support set. Extensive experiments on popular few-shot learning benchmarks demonstrate the superiority of our method over state-of-the-art methods. Our source code is available at \url{https://github.com/chenhaoxing/ssformers}.



### Optimized Automated Cardiac MR Scar Quantification with GAN-Based Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.12940v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.12940v1)
- **Published**: 2021-09-27 10:46:02+00:00
- **Updated**: 2021-09-27 10:46:02+00:00
- **Authors**: Didier R. P. R. M. Lustermans, Sina Amirrajab, Mitko Veta, Marcel Breeuwer, Cian M. Scannell
- **Comment**: Submitted for peer review
- **Journal**: None
- **Summary**: Background: The clinical utility of late gadolinium enhancement (LGE) cardiac MRI is limited by the lack of standardization, and time-consuming postprocessing. In this work, we tested the hypothesis that a cascaded deep learning pipeline trained with augmentation by synthetically generated data would improve model accuracy and robustness for automated scar quantification.   Methods: A cascaded pipeline consisting of three consecutive neural networks is proposed, starting with a bounding box regression network to identify a region of interest around the left ventricular (LV) myocardium. Two further nnU-Net models are then used to segment the myocardium and, if present, scar. The models were trained on the data from the EMIDEC challenge, supplemented with an extensive synthetic dataset generated with a conditional GAN.   Results: The cascaded pipeline significantly outperformed a single nnU-Net directly segmenting both the myocardium (mean Dice similarity coefficient (DSC) (standard deviation (SD)): 0.84 (0.09) vs 0.63 (0.20), p < 0.01) and scar (DSC: 0.72 (0.34) vs 0.46 (0.39), p < 0.01) on a per-slice level. The inclusion of the synthetic data as data augmentation during training improved the scar segmentation DSC by 0.06 (p < 0.01). The mean DSC per-subject on the challenge test set, for the cascaded pipeline augmented by synthetic generated data, was 0.86 (0.03) and 0.67 (0.29) for myocardium and scar, respectively.   Conclusion: A cascaded deep learning-based pipeline trained with augmentation by synthetically generated data leads to myocardium and scar segmentations that are similar to the manual operator, and outperforms direct segmentation without the synthetic images.



### Fusion-GCN: Multimodal Action Recognition using Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.12946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.12946v1)
- **Published**: 2021-09-27 10:52:33+00:00
- **Updated**: 2021-09-27 10:52:33+00:00
- **Authors**: Michael Duhme, Raphael Memmesheimer, Dietrich Paulus
- **Comment**: 18 pages, 6 figures, 3 tables, GCPR 2021
- **Journal**: None
- **Summary**: In this paper, we present Fusion-GCN, an approach for multimodal action recognition using Graph Convolutional Networks (GCNs). Action recognition methods based around GCNs recently yielded state-of-the-art performance for skeleton-based action recognition. With Fusion-GCN, we propose to integrate various sensor data modalities into a graph that is trained using a GCN model for multi-modal action recognition. Additional sensor measurements are incorporated into the graph representation, either on a channel dimension (introducing additional node attributes) or spatial dimension (introducing new nodes). Fusion-GCN was evaluated on two public available datasets, the UTD-MHAD- and MMACT datasets, and demonstrates flexible fusion of RGB sequences, inertial measurements and skeleton sequences. Our approach gets comparable results on the UTD-MHAD dataset and improves the baseline on the large-scale MMACT dataset by a significant margin of up to 12.37% (F1-Measure) with the fusion of skeleton estimates and accelerometer measurements.



### Text-based Person Search in Full Images via Semantic-Driven Proposal Generation
- **Arxiv ID**: http://arxiv.org/abs/2109.12965v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.12965v2)
- **Published**: 2021-09-27 11:42:40+00:00
- **Updated**: 2023-07-25 09:27:12+00:00
- **Authors**: Shizhou Zhang, De Cheng, Wenlong Luo, Yinghui Xing, Duo Long, Hao Li, Kai Niu, Guoqiang Liang, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Finding target persons in full scene images with a query of text description has important practical applications in intelligent video surveillance.However, different from the real-world scenarios where the bounding boxes are not available, existing text-based person retrieval methods mainly focus on the cross modal matching between the query text descriptions and the gallery of cropped pedestrian images. To close the gap, we study the problem of text-based person search in full images by proposing a new end-to-end learning framework which jointly optimize the pedestrian detection, identification and visual-semantic feature embedding tasks. To take full advantage of the query text, the semantic features are leveraged to instruct the Region Proposal Network to pay more attention to the text-described proposals. Besides, a cross-scale visual-semantic embedding mechanism is utilized to improve the performance. To validate the proposed method, we collect and annotate two large-scale benchmark datasets based on the widely adopted image-based person search datasets CUHK-SYSU and PRW. Comprehensive experiments are conducted on the two datasets and compared with the baseline methods, our method achieves the state-of-the-art performance.



### CT-ICP: Real-time Elastic LiDAR Odometry with Loop Closure
- **Arxiv ID**: http://arxiv.org/abs/2109.12979v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.12979v2)
- **Published**: 2021-09-27 12:08:26+00:00
- **Updated**: 2022-02-24 18:50:34+00:00
- **Authors**: Pierre Dellenbach, Jean-Emmanuel Deschaud, Bastien Jacquet, François Goulette
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Multi-beam LiDAR sensors are increasingly used in robotics, particularly with autonomous cars for localization and perception tasks, both relying on the ability to build a precise map of the environment. For this, we propose a new real-time LiDAR-only odometry method called CT-ICP (for Continuous-Time ICP), completed into a full SLAM with a novel loop detection procedure. The core of this method, is the introduction of the combined continuity in the scan matching, and discontinuity between scans. It allows both the elastic distortion of the scan during the registration for increased precision, and the increased robustness to high frequency motions from the discontinuity.   We build a complete SLAM on top of this odometry, using a fast pure LiDAR loop detection based on elevation image 2D matching, providing a pose graph with loop constraints. To show the robustness of the method, we tested it on seven datasets: KITTI, KITTI-raw, KITTI-360, KITTI-CARLA, ParisLuco, Newer College, and NCLT in driving and high-frequency motion scenarios. Both the CT-ICP odometry and the loop detection are made available online. CT-ICP is currently first, among those giving access to a public code, on the KITTI odometry leaderboard, with an average Relative Translation Error (RTE) of 0.59% and an average time per scan of 60ms on a CPU with a single thread.



### Optimising for Interpretability: Convolutional Dynamic Alignment Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.13004v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.13004v1)
- **Published**: 2021-09-27 12:39:46+00:00
- **Updated**: 2021-09-27 12:39:46+00:00
- **Authors**: Moritz Böhle, Mario Fritz, Bernt Schiele
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2104.00032
- **Journal**: None
- **Summary**: We introduce a new family of neural network models called Convolutional Dynamic Alignment Networks (CoDA Nets), which are performant classifiers with a high degree of inherent interpretability. Their core building blocks are Dynamic Alignment Units (DAUs), which are optimised to transform their inputs with dynamically computed weight vectors that align with task-relevant patterns. As a result, CoDA Nets model the classification prediction through a series of input-dependent linear transformations, allowing for linear decomposition of the output into individual input contributions. Given the alignment of the DAUs, the resulting contribution maps align with discriminative input patterns. These model-inherent decompositions are of high visual quality and outperform existing attribution methods under quantitative metrics. Further, CoDA Nets constitute performant classifiers, achieving on par results to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet. Lastly, CoDA Nets can be combined with conventional neural network models to yield powerful classifiers that more easily scale to complex datasets such as Imagenet whilst exhibiting an increased interpretable depth, i.e., the output can be explained well in terms of contributions from intermediate layers within the network.



### Semi-Supervised Adversarial Discriminative Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2109.13016v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.13016v2)
- **Published**: 2021-09-27 12:52:50+00:00
- **Updated**: 2022-10-19 16:07:17+00:00
- **Authors**: Thai-Vu Nguyen, Anh Nguyen, Nghia Le, Bac Le
- **Comment**: 14 pages, 5 figures
- **Journal**: None
- **Summary**: Domain adaptation is a potential method to train a powerful deep neural network, which can handle the absence of labeled data. More precisely, domain adaptation solving the limitation called dataset bias or domain shift when the training dataset and testing dataset are extremely different. Adversarial adaptation method becoming popular among other domain adaptation methods. Relies on the idea of GAN, adversarial domain adaptation tries to minimize the distribution between training and testing datasets base on the adversarial object. However, some conventional adversarial domain adaptation methods cannot handle large domain shifts between two datasets or the generalization ability of these methods are inefficient. In this paper, we propose an improved adversarial domain adaptation method called Semi-Supervised Adversarial Discriminative Domain Adaptation (SADDA), which can overcome the limitation of other domain adaptation. We also show that SADDA has better performance than other adversarial adaptation methods and illustrate the promise of our method on digit classification and emotion recognition problems.



### Attention Gate in Traffic Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2109.13021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13021v1)
- **Published**: 2021-09-27 12:58:12+00:00
- **Updated**: 2021-09-27 12:58:12+00:00
- **Authors**: Anh Lam, Anh Nguyen, Bac Le
- **Comment**: 18 pages, 2 figures
- **Journal**: None
- **Summary**: Because of increased urban complexity and growing populations, more and more challenges about predicting city-wide mobility behavior are being organized. Traffic Map Movie Forecasting Challenge 2020 is secondly held in the competition track of the Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS). Similar to Traffic4Cast 2019, the task is to predict traffic flow volume, average speed in major directions on the geographical area of three big cities: Berlin, Istanbul, and Moscow. In this paper, we apply the attention mechanism on U-Net based model, especially we add an attention gate on the skip-connection between contraction path and expansion path. An attention gates filter features from the contraction path before combining with features on the expansion path, it enables our model to reduce the effect of non-traffic region features and focus more on crucial region features. In addition to the competition data, we also propose two extra features which often affect traffic flow, that are time and weekdays. We experiment with our model on the competition dataset and reproduce the winner solution in the same environment. Overall, our model archives better performance than recent methods.



### Experience feedback using Representation Learning for Few-Shot Object Detection on Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2109.13027v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13027v1)
- **Published**: 2021-09-27 13:04:53+00:00
- **Updated**: 2021-09-27 13:04:53+00:00
- **Authors**: Pierre Le Jeune, Mustapha Lebbah, Anissa Mokraoui, Hanene Azzag
- **Comment**: 11 pages, 2 figures, accepted at ICMLA 2021 as a short paper
- **Journal**: None
- **Summary**: This paper proposes a few-shot method based on Faster R-CNN and representation learning for object detection in aerial images. The two classification branches of Faster R-CNN are replaced by prototypical networks for online adaptation to new classes. These networks produce embeddings vectors for each generated box, which are then compared with class prototypes. The distance between an embedding and a prototype determines the corresponding classification score. The resulting networks are trained in an episodic manner. A new detection task is randomly sampled at each epoch, consisting in detecting only a subset of the classes annotated in the dataset. This training strategy encourages the network to adapt to new classes as it would at test time. In addition, several ideas are explored to improve the proposed method such as a hard negative examples mining strategy and self-supervised clustering for background objects. The performance of our method is assessed on DOTA, a large-scale remote sensing images dataset. The experiments conducted provide a broader understanding of the capabilities of representation learning. It highlights in particular some intrinsic weaknesses for the few-shot object detection task. Finally, some suggestions and perspectives are formulated according to these insights.



### DOODLER: Determining Out-Of-Distribution Likelihood from Encoder Reconstructions
- **Arxiv ID**: http://arxiv.org/abs/2109.13237v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T07 (Primary) 62D10 (Secondary), I.2.6; I.5.4; G.3
- **Links**: [PDF](http://arxiv.org/pdf/2109.13237v1)
- **Published**: 2021-09-27 14:54:55+00:00
- **Updated**: 2021-09-27 14:54:55+00:00
- **Authors**: Jonathan S. Kent, Bo Li
- **Comment**: 20 pages, 8 figures, Undergraduate Thesis
- **Journal**: None
- **Summary**: Deep Learning models possess two key traits that, in combination, make their use in the real world a risky prospect. One, they do not typically generalize well outside of the distribution for which they were trained, and two, they tend to exhibit confident behavior regardless of whether or not they are producing meaningful outputs. While Deep Learning possesses immense power to solve realistic, high-dimensional problems, these traits in concert make it difficult to have confidence in their real-world applications. To overcome this difficulty, the task of Out-Of-Distribution (OOD) Detection has been defined, to determine when a model has received an input from outside of the distribution for which it is trained to operate.   This paper introduces and examines a novel methodology, DOODLER, for OOD Detection, which directly leverages the traits which result in its necessity. By training a Variational Auto-Encoder (VAE) on the same data as another Deep Learning model, the VAE learns to accurately reconstruct In-Distribution (ID) inputs, but not to reconstruct OOD inputs, meaning that its failure state can be used to perform OOD Detection. Unlike other work in the area, DOODLER requires only very weak assumptions about the existence of an OOD dataset, allowing for more realistic application. DOODLER also enables pixel-wise segmentations of input images by OOD likelihood, and experimental results show that it matches or outperforms methodologies that operate under the same constraints.



### VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2109.13116v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.13116v1)
- **Published**: 2021-09-27 15:06:10+00:00
- **Updated**: 2021-09-27 15:06:10+00:00
- **Authors**: Ekta Sood, Fabian Kögel, Florian Strohm, Prajit Dhar, Andreas Bulling
- **Comment**: CoNLL 2021
- **Journal**: None
- **Summary**: We present VQA-MHUG - a novel 49-participant dataset of multimodal human gaze on both images and questions during visual question answering (VQA) collected using a high-speed eye tracker. We use our dataset to analyze the similarity between human and neural attentive strategies learned by five state-of-the-art VQA models: Modular Co-Attention Network (MCAN) with either grid or region features, Pythia, Bilinear Attention Network (BAN), and the Multimodal Factorized Bilinear Pooling Network (MFB). While prior work has focused on studying the image modality, our analyses show - for the first time - that for all models, higher correlation with human attention on text is a significant predictor of VQA performance. This finding points at a potential for improving VQA performance and, at the same time, calls for further research on neural text attention mechanisms and their integration into architectures for vision and language tasks, including but potentially also beyond VQA.



### An End-to-end Entangled Segmentation and Classification Convolutional Neural Network for Periodontitis Stage Grading from Periapical Radiographic Images
- **Arxiv ID**: http://arxiv.org/abs/2109.13120v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.13120v1)
- **Published**: 2021-09-27 15:28:54+00:00
- **Updated**: 2021-09-27 15:28:54+00:00
- **Authors**: Tanjida Kabir, Chun-Teh Lee, Jiman Nelson, Sally Sheng, Hsiu-Wan Meng, Luyao Chen, Muhammad F Walji, Xioaqian Jiang, Shayan Shams
- **Comment**: 8 pages, 8 figures, 5 tables
- **Journal**: None
- **Summary**: Periodontitis is a biofilm-related chronic inflammatory disease characterized by gingivitis and bone loss in the teeth area. Approximately 61 million adults over 30 suffer from periodontitis (42.2%), with 7.8% having severe periodontitis in the United States. The measurement of radiographic bone loss (RBL) is necessary to make a correct periodontal diagnosis, especially if the comprehensive and longitudinal periodontal mapping is unavailable. However, doctors can interpret X-rays differently depending on their experience and knowledge. Computerized diagnosis support for doctors sheds light on making the diagnosis with high accuracy and consistency and drawing up an appropriate treatment plan for preventing or controlling periodontitis. We developed an end-to-end deep learning network HYNETS (Hybrid NETwork for pEriodoNTiTiS STagES from radiograpH) by integrating segmentation and classification tasks for grading periodontitis from periapical radiographic images. HYNETS leverages a multi-task learning strategy by combining a set of segmentation networks and a classification network to provide an end-to-end interpretable solution and highly accurate and consistent results. HYNETS achieved the average dice coefficient of 0.96 and 0.94 for the bone area and tooth segmentation and the average AUC of 0.97 for periodontitis stage assignment. Additionally, conventional image processing techniques provide RBL measurements and build transparency and trust in the model's prediction. HYNETS will potentially transform clinical diagnosis from a manual time-consuming, and error-prone task to an efficient and automated periodontitis stage assignment based on periapical radiographic images.



### GANiry: Bald-to-Hairy Translation Using CycleGAN
- **Arxiv ID**: http://arxiv.org/abs/2109.13126v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.13126v1)
- **Published**: 2021-09-27 15:39:27+00:00
- **Updated**: 2021-09-27 15:39:27+00:00
- **Authors**: Fidan Samet, Oguz Bakir
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents our computer vision course project called bald men-to-hairy men translation using CycleGAN. On top of CycleGAN architecture, we utilize perceptual loss in order to achieve more realistic results. We also integrate conditional constrains to obtain different stylized and colored hairs on bald men. We conducted extensive experiments and present qualitative results in this paper. Our code and models are available at https://github.com/fidansamet/GANiry.



### Multimodal Integration of Human-Like Attention in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2109.13139v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.13139v1)
- **Published**: 2021-09-27 15:56:54+00:00
- **Updated**: 2021-09-27 15:56:54+00:00
- **Authors**: Ekta Sood, Fabian Kögel, Philipp Müller, Dominike Thomas, Mihai Bace, Andreas Bulling
- **Comment**: None
- **Journal**: None
- **Summary**: Human-like attention as a supervisory signal to guide neural attention has shown significant promise but is currently limited to uni-modal integration - even for inherently multimodal tasks such as visual question answering (VQA). We present the Multimodal Human-like Attention Network (MULAN) - the first method for multimodal integration of human-like attention on image and text during training of VQA models. MULAN integrates attention predictions from two state-of-the-art text and image saliency models into neural self-attention layers of a recent transformer-based VQA model. Through evaluations on the challenging VQAv2 dataset, we show that MULAN achieves a new state-of-the-art performance of 73.98% accuracy on test-std and 73.72% on test-dev and, at the same time, has approximately 80% fewer trainable parameters than prior work. Overall, our work underlines the potential of integrating multimodal human-like and neural attention for VQA



### DAReN: A Collaborative Approach Towards Reasoning And Disentangling
- **Arxiv ID**: http://arxiv.org/abs/2109.13156v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.13156v2)
- **Published**: 2021-09-27 16:10:30+00:00
- **Updated**: 2022-06-30 01:14:40+00:00
- **Authors**: Pritish Sahu, Kalliopi Basioti, Vladimir Pavlovic
- **Comment**: None
- **Journal**: None
- **Summary**: Computational learning approaches to solving visual reasoning tests, such as Raven's Progressive Matrices (RPM), critically depend on the ability to identify the visual concepts used in the test (i.e., the representation) as well as the latent rules based on those concepts (i.e., the reasoning). However, learning of representation and reasoning is a challenging and ill-posed task, often approached in a stage-wise manner (first representation, then reasoning). In this work, we propose an end-to-end joint representation-reasoning learning framework, which leverages a weak form of inductive bias to improve both tasks together. Specifically, we introduce a general generative graphical model for RPMs, GM-RPM, and apply it to solve the reasoning test. We accomplish this using a novel learning framework Disentangling based Abstract Reasoning Network (DAReN) based on the principles of GM-RPM. We perform an empirical evaluation of DAReN over several benchmark datasets. DAReN shows consistent improvement over state-of-the-art (SOTA) models on both the reasoning and the disentanglement tasks. This demonstrates the strong correlation between disentangled latent representation and the ability to solve abstract visual reasoning tasks.



### Visual Anomaly Detection for Images: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2109.13157v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13157v1)
- **Published**: 2021-09-27 16:10:54+00:00
- **Updated**: 2021-09-27 16:10:54+00:00
- **Authors**: Jie Yang, Ruijie Xu, Zhiquan Qi, Yong Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Visual anomaly detection is an important and challenging problem in the field of machine learning and computer vision. This problem has attracted a considerable amount of attention in relevant research communities. Especially in recent years, the development of deep learning has sparked an increasing interest in the visual anomaly detection problem and brought a great variety of novel methods. In this paper, we provide a comprehensive survey of the classical and deep learning-based approaches for visual anomaly detection in the literature. We group the relevant approaches in view of their underlying principles and discuss their assumptions, advantages, and disadvantages carefully. We aim to help the researchers to understand the common principles of visual anomaly detection approaches and identify promising research directions in this field.



### Comparison of Object Detection Algorithms Using Video and Thermal Images Collected from a UAS Platform: An Application of Drones in Traffic Management
- **Arxiv ID**: http://arxiv.org/abs/2109.13185v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13185v1)
- **Published**: 2021-09-27 16:57:09+00:00
- **Updated**: 2021-09-27 16:57:09+00:00
- **Authors**: Hualong Tang, Joseph Post, Achilleas Kourtellis, Brian Porter, Yu Zhang
- **Comment**: Submitted to the 2022 TRB Annual Meeting
- **Journal**: None
- **Summary**: There is a rapid growth of applications of Unmanned Aerial Vehicles (UAVs) in traffic management, such as traffic surveillance, monitoring, and incident detection. However, the existing literature lacks solutions to real-time incident detection while addressing privacy issues in practice. This study explored real-time vehicle detection algorithms on both visual and infrared cameras and conducted experiments comparing their performance. Red Green Blue (RGB) videos and thermal images were collected from a UAS platform along highways in the Tampa, Florida, area. Experiments were designed to quantify the performance of a real-time background subtraction-based method in vehicle detection from a stationary camera on hovering UAVs under free-flow conditions. Several parameters were set in the experiments based on the geometry of the drone and sensor relative to the roadway. The results show that a background subtraction-based method can achieve good detection performance on RGB images (F1 scores around 0.9 for most cases), and a more varied performance is seen on thermal images with different azimuth angles. The results of these experiments will help inform the development of protocols, standards, and guidance for the use of drones to detect highway congestion and provide input for the development of incident detection algorithms.



### Spiking neural networks trained via proxy
- **Arxiv ID**: http://arxiv.org/abs/2109.13208v3
- **DOI**: 10.1109/ACCESS.2022.3187033
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.13208v3)
- **Published**: 2021-09-27 17:29:51+00:00
- **Updated**: 2022-07-30 06:53:00+00:00
- **Authors**: Saeed Reza Kheradpisheh, Maryam Mirsadeghi, Timothée Masquelier
- **Comment**: None
- **Journal**: IEEE Access 10 (2022)
- **Summary**: We propose a new learning algorithm to train spiking neural networks (SNN) using conventional artificial neural networks (ANN) as proxy. We couple two SNN and ANN networks, respectively, made of integrate-and-fire (IF) and ReLU neurons with the same network architectures and shared synaptic weights. The forward passes of the two networks are totally independent. By assuming IF neuron with rate-coding as an approximation of ReLU, we backpropagate the error of the SNN in the proxy ANN to update the shared weights, simply by replacing the ANN final output with that of the SNN. We applied the proposed proxy learning to deep convolutional SNNs and evaluated it on two benchmarked datasets of Fashion-MNIST and Cifar10 with 94.56% and 93.11% classification accuracy, respectively. The proposed networks could outperform other deep SNNs trained with tandem learning, surrogate gradient learning, or converted from deep ANNs. Converted SNNs require long simulation times to reach reasonable accuracies while our proxy learning leads to efficient SNNs with much smaller simulation times. The source codes of the proposed method are publicly available at https://github.com/SRKH/ProxyLearning.



### SAU: Smooth activation function using convolution with approximate identities
- **Arxiv ID**: http://arxiv.org/abs/2109.13210v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2109.13210v1)
- **Published**: 2021-09-27 17:31:04+00:00
- **Updated**: 2021-09-27 17:31:04+00:00
- **Authors**: Koushik Biswas, Sandeep Kumar, Shilpak Banerjee, Ashish Kumar Pandey
- **Comment**: arXiv admin note: text overlap with arXiv:2109.04386
- **Journal**: None
- **Summary**: Well-known activation functions like ReLU or Leaky ReLU are non-differentiable at the origin. Over the years, many smooth approximations of ReLU have been proposed using various smoothing techniques. We propose new smooth approximations of a non-differentiable activation function by convolving it with approximate identities. In particular, we present smooth approximations of Leaky ReLU and show that they outperform several well-known activation functions in various datasets and models. We call this function Smooth Activation Unit (SAU). Replacing ReLU by SAU, we get 5.12% improvement with ShuffleNet V2 (2.0x) model on CIFAR100 dataset.



### Predicting Driver Self-Reported Stress by Analyzing the Road Scene
- **Arxiv ID**: http://arxiv.org/abs/2109.13225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13225v1)
- **Published**: 2021-09-27 17:55:35+00:00
- **Updated**: 2021-09-27 17:55:35+00:00
- **Authors**: Cristina Bustos, Neska Elhaouij, Albert Sole-Ribalta, Javier Borge-Holthoefer, Agata Lapedriza, Rosalind Picard
- **Comment**: None
- **Journal**: None
- **Summary**: Several studies have shown the relevance of biosignals in driver stress recognition. In this work, we examine something important that has been less frequently explored: We develop methods to test if the visual driving scene can be used to estimate a drivers' subjective stress levels. For this purpose, we use the AffectiveROAD video recordings and their corresponding stress labels, a continuous human-driver-provided stress metric. We use the common class discretization for stress, dividing its continuous values into three classes: low, medium, and high. We design and evaluate three computer vision modeling approaches to classify the driver's stress levels: (1) object presence features, where features are computed using automatic scene segmentation; (2) end-to-end image classification; and (3) end-to-end video classification. All three approaches show promising results, suggesting that it is possible to approximate the drivers' subjective stress from the information found in the visual scene. We observe that the video classification, which processes the temporal information integrated with the visual information, obtains the highest accuracy of $0.72$, compared to a random baseline accuracy of $0.33$ when tested on a set of nine drivers.



### TSM: Temporal Shift Module for Efficient and Scalable Video Understanding on Edge Device
- **Arxiv ID**: http://arxiv.org/abs/2109.13227v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.13227v1)
- **Published**: 2021-09-27 17:59:39+00:00
- **Updated**: 2021-09-27 17:59:39+00:00
- **Authors**: Ji Lin, Chuang Gan, Kuan Wang, Song Han
- **Comment**: Journal preprint of arXiv:1811.08383 (TPAMI, 2020). arXiv admin note:
  substantial text overlap with arXiv:1910.00932
- **Journal**: None
- **Summary**: The explosive growth in video streaming requires video understanding at high accuracy and low computation cost. Conventional 2D CNNs are computationally cheap but cannot capture temporal relationships; 3D CNN-based methods can achieve good performance but are computationally intensive. In this paper, we propose a generic and effective Temporal Shift Module (TSM) that enjoys both high efficiency and high performance. The key idea of TSM is to shift part of the channels along the temporal dimension, thus facilitate information exchanged among neighboring frames. It can be inserted into 2D CNNs to achieve temporal modeling at zero computation and zero parameters. TSM offers several unique advantages. Firstly, TSM has high performance; it ranks the first on the Something-Something leaderboard upon submission. Secondly, TSM has high efficiency; it achieves a high frame rate of 74fps and 29fps for online video recognition on Jetson Nano and Galaxy Note8. Thirdly, TSM has higher scalability compared to 3D networks, enabling large-scale Kinetics training on 1,536 GPUs in 15 minutes. Lastly, TSM enables action concepts learning, which 2D networks cannot model; we visualize the category attention map and find that spatial-temporal action detector emerges during the training of classification tasks. The code is publicly available at https://github.com/mit-han-lab/temporal-shift-module.



### PASS: An ImageNet replacement for self-supervised pretraining without humans
- **Arxiv ID**: http://arxiv.org/abs/2109.13228v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2109.13228v1)
- **Published**: 2021-09-27 17:59:39+00:00
- **Updated**: 2021-09-27 17:59:39+00:00
- **Authors**: Yuki M. Asano, Christian Rupprecht, Andrew Zisserman, Andrea Vedaldi
- **Comment**: Accepted to NeurIPS Track on Datasets and Benchmarks 2021. Webpage:
  https://www.robots.ox.ac.uk/~vgg/research/pass/
- **Journal**: None
- **Summary**: Computer vision has long relied on ImageNet and other large datasets of images sampled from the Internet for pretraining models. However, these datasets have ethical and technical shortcomings, such as containing personal information taken without consent, unclear license usage, biases, and, in some cases, even problematic image content. On the other hand, state-of-the-art pretraining is nowadays obtained with unsupervised methods, meaning that labelled datasets such as ImageNet may not be necessary, or perhaps not even optimal, for model pretraining. We thus propose an unlabelled dataset PASS: Pictures without humAns for Self-Supervision. PASS only contains images with CC-BY license and complete attribution metadata, addressing the copyright issue. Most importantly, it contains no images of people at all, and also avoids other types of images that are problematic for data protection or ethics. We show that PASS can be used for pretraining with methods such as MoCo-v2, SwAV and DINO. In the transfer learning setting, it yields similar downstream performances to ImageNet pretraining even on tasks that involve humans, such as human pose estimation. PASS does not make existing datasets obsolete, as for instance it is insufficient for benchmarking. However, it shows that model pretraining is often possible while using safer data, and it also provides the basis for a more robust evaluation of pretraining methods.



### Urban Driver: Learning to Drive from Real-world Demonstrations Using Policy Gradients
- **Arxiv ID**: http://arxiv.org/abs/2109.13333v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.13333v1)
- **Published**: 2021-09-27 20:19:18+00:00
- **Updated**: 2021-09-27 20:19:18+00:00
- **Authors**: Oliver Scheel, Luca Bergamini, Maciej Wołczyk, Błażej Osiński, Peter Ondruska
- **Comment**: CoRL 2021
- **Journal**: None
- **Summary**: In this work we are the first to present an offline policy gradient method for learning imitative policies for complex urban driving from a large corpus of real-world demonstrations. This is achieved by building a differentiable data-driven simulator on top of perception outputs and high-fidelity HD maps of the area. It allows us to synthesize new driving experiences from existing demonstrations using mid-level representations. Using this simulator we then train a policy network in closed-loop employing policy gradients. We train our proposed method on 100 hours of expert demonstrations on urban roads and show that it learns complex driving policies that generalize well and can perform a variety of driving maneuvers. We demonstrate this in simulation as well as deploy our model to self-driving vehicles in the real-world. Our method outperforms previously demonstrated state-of-the-art for urban driving scenarios -- all this without the need for complex state perturbations or collecting additional on-policy data during training. We make code and data publicly available.



### Efficient Computer Vision on Edge Devices with Pipeline-Parallel Hierarchical Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.13356v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2109.13356v2)
- **Published**: 2021-09-27 21:27:41+00:00
- **Updated**: 2021-11-04 19:53:40+00:00
- **Authors**: Abhinav Goel, Caleb Tung, Xiao Hu, George K. Thiruvathukal, James C. Davis, Yung-Hsiang Lu
- **Comment**: Accepted for publication in ASPDAC 2022
- **Journal**: None
- **Summary**: Computer vision on low-power edge devices enables applications including search-and-rescue and security. State-of-the-art computer vision algorithms, such as Deep Neural Networks (DNNs), are too large for inference on low-power edge devices. To improve efficiency, some existing approaches parallelize DNN inference across multiple edge devices. However, these techniques introduce significant communication and synchronization overheads or are unable to balance workloads across devices. This paper demonstrates that the hierarchical DNN architecture is well suited for parallel processing on multiple edge devices. We design a novel method that creates a parallel inference pipeline for computer vision problems that use hierarchical DNNs. The method balances loads across the collaborating devices and reduces communication costs to facilitate the processing of multiple video frames simultaneously with higher throughput. Our experiments consider a representative computer vision problem where image recognition is performed on each video frame, running on multiple Raspberry Pi 4Bs. With four collaborating low-power edge devices, our approach achieves 3.21X higher throughput, 68% less energy consumption per device per frame, and 58% decrease in memory when compared with existing single-device hierarchical DNNs.



### WarpedGANSpace: Finding non-linear RBF paths in GAN latent space
- **Arxiv ID**: http://arxiv.org/abs/2109.13357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13357v1)
- **Published**: 2021-09-27 21:29:35+00:00
- **Updated**: 2021-09-27 21:29:35+00:00
- **Authors**: Christos Tzelepis, Georgios Tzimiropoulos, Ioannis Patras
- **Comment**: Accepted for publication in ICCV 2021
- **Journal**: None
- **Summary**: This work addresses the problem of discovering, in an unsupervised manner, interpretable paths in the latent space of pretrained GANs, so as to provide an intuitive and easy way of controlling the underlying generative factors. In doing so, it addresses some of the limitations of the state-of-the-art works, namely, a) that they discover directions that are independent of the latent code, i.e., paths that are linear, and b) that their evaluation relies either on visual inspection or on laborious human labeling. More specifically, we propose to learn non-linear warpings on the latent space, each one parametrized by a set of RBF-based latent space warping functions, and where each warping gives rise to a family of non-linear paths via the gradient of the function. Building on the work of Voynov and Babenko, that discovers linear paths, we optimize the trainable parameters of the set of RBFs, so as that images that are generated by codes along different paths, are easily distinguishable by a discriminator network. This leads to easily distinguishable image transformations, such as pose and facial expressions in facial images. We show that linear paths can be derived as a special case of our method, and show experimentally that non-linear paths in the latent space lead to steeper, more disentangled and interpretable changes in the image space than in state-of-the art methods, both qualitatively and quantitatively. We make the code and the pretrained models publicly available at: https://github.com/chi0tzp/WarpedGANSpace.



### IGAN: Inferent and Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.13360v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.13360v1)
- **Published**: 2021-09-27 21:48:35+00:00
- **Updated**: 2021-09-27 21:48:35+00:00
- **Authors**: Dr. Luc Vignaud
- **Comment**: 10 pages, 6 figures, accepted for publication in proceedings of NATO
  Specialist Meeting SET-273 RSM on "Multidimensional Radar Imaging for ATR",
  Marseille, France, october 2021
- **Journal**: None
- **Summary**: I present IGAN (Inferent Generative Adversarial Networks), a neural architecture that learns both a generative and an inference model on a complex high dimensional data distribution, i.e. a bidirectional mapping between data samples and a simpler low-dimensional latent space. It extends the traditional GAN framework with inference by rewriting the adversarial strategy in both the image and the latent space with an entangled game between data-latent encoded posteriors and priors. It brings a measurable stability and convergence to the classical GAN scheme, while keeping its generative quality and remaining simple and frugal in order to run on a lab PC. IGAN fosters the encoded latents to span the full prior space: this enables the exploitation of an enlarged and self-organised latent space in an unsupervised manner. An analysis of previously published articles sets the theoretical ground for the proposed algorithm. A qualitative demonstration of potential applications like self-supervision or multi-modal data translation is given on common image datasets including SAR and optical imagery.



